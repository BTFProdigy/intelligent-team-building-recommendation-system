Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 860?868,
Beijing, August 2010
Latent Mixture of Discriminative Experts
for Multimodal Prediction Modeling
Derya Ozkan, Kenji Sagae and Louis-Philippe Morency
USC Institute for Creative Technologies
{ozkan,sagae,morency}@ict.usc.edu
Abstract
During face-to-face conversation, people
naturally integrate speech, gestures and
higher level language interpretations to
predict the right time to start talking or
to give backchannel feedback. In this
paper we introduce a new model called
Latent Mixture of Discriminative Experts
which addresses some of the key issues
with multimodal language processing: (1)
temporal synchrony/asynchrony between
modalities, (2) micro dynamics and (3) in-
tegration of different levels of interpreta-
tion. We present an empirical evaluation
on listener nonverbal feedback prediction
(e.g., head nod), based on observable be-
haviors of the speaker. We confirm the im-
portance of combining four types of mul-
timodal features: lexical, syntactic struc-
ture, eye gaze, and prosody. We show
that our Latent Mixture of Discriminative
Experts model outperforms previous ap-
proaches based on Conditional Random
Fields (CRFs) and Latent-Dynamic CRFs.
1 Introduction
Face-to-face communication is highly interactive.
Even when only one person speaks at a time,
other participants exchange information continu-
ously amongst themselves and with the speaker
through gestures, gaze and prosody. These differ-
ent channels contain complementary information
essential to interpretation and understanding of
human behaviors (Oviatt, 1999). Psycholinguistic
studies also suggest that gesture and speech come
from a single underlying mental process, and they
Pitch
Words
Gaze
TimeP
(no
d)
Look  at listener
Speaker
Listener
Prediction
Figure 1: Example of multimodal prediction
model: listener nonverbal backchannel prediction
based on speaker?s speech and eye gaze. As the
speaker says the word her, which is the end of the
clause (her is also the object of the verb bother-
ing), and lowers the pitch while looking back at
the listener and eventually pausing, the listener
is then very likely to head nod (i.e., nonverbal
backchannel).
are related both temporally and semantically (Mc-
Neill, 1992; Cassell and Stone, 1999; Kendon,
2004).
A good example of such complementarity is
how people naturally integrate speech, gestures
and higher level language to predict when to give
backchannel feedback. Building computational
models of such a predictive process is challeng-
ing since it involves micro dynamics and temporal
relationship between cues from different modali-
ties (Quek, 2003). Figure 1 shows an example of
backchannel prediction where a listener head nod
860
is more likely. For example, a temporal sequence
from the speaker where he/she reaches the end of
segment (syntactic feature) with a low pitch and
looks at the listener before pausing is a good op-
portunity for the listener to give nonverbal feed-
back (e.g., head nod). These prediction models
have broad applicability, including the improve-
ment of nonverbal behavior recognition, the syn-
thesis of natural animations for robots and virtual
humans, the training of cultural-specific nonver-
bal behaviors, and the diagnoses of social disor-
ders (e.g., autism spectrum disorder).
In this paper we introduce a new model
called Latent Mixture of Discriminative Experts
(LMDE) which addresses some of the key issues
with multimodal language processing: (1) tempo-
ral synchrony/asynchrony between modalities, (2)
micro dynamics and (3) integration of different
levels of interpretation. We present an empirical
evaluation on nonverbal feedback prediction (e.g.,
head nod) confirming the importance of combin-
ing different types of multimodal features. We
show that our LMDE model outperforms previ-
ous approaches based Conditional Random Fields
(CRFs) and Latent-Dynamic CRFs.
2 Related Work
Earlier work in multimodal language processing
focused on multimodal dialogue systems where
the gestures and speech may be constrained (John-
ston, 1998; Jurafsky et al, 1998). Most of
the research in multimodal language processing
over the past decade fits within two main trends
that have emerged: (1) recognition of individ-
ual multimodal actions such as speech and ges-
tures (e.g, (Eisenstein et al, 2008; Frampton et
al., 2009; Gravano et al, 2007)), and (2) recog-
nition/summarization of the social interaction be-
tween more than one participants (e.g., meeting
analysis (Heylen and op den Akker, 2007; Moore,
2007; Murray and Carenini, 2009; Jovanovic et
al., 2006)).
The work described in this paper can be seen
from a third intermediate category where multi-
modal cues from one person is used to predict
the social behavior of another participant. This
type of predictive models has been mostly stud-
ied in the context of embodied conversational
agents (Nakano et al, 2003; Nakano et al, 2007).
In particular, backchannel feedback (the nods and
paraverbals such as ?uh-hu? and ?mm-hmm? that
listeners produce as someone is speaking) has re-
ceived considerable interest due to its pervasive-
ness across languages and conversational contexts
and this paper addresses the problem of how to
predict and generate this important class of dyadic
nonverbal behavior.
Several researchers have developed models to
predict when backchannel should happen. In gen-
eral, these results are difficult to compare as they
utilize different corpora and present varying eval-
uation metrics. Ward and Tsukahara (2000) pro-
pose a unimodal approach where backchannels
are associated with a region of low pitch last-
ing 110ms during speech. Models were pro-
duced manually through an analysis of English
and Japanese conversational data. Nishimura
et al (2007) present a unimodal decision-tree
approach for producing backchannels based on
prosodic features. Cathcart et al (2003) propose a
unimodal model based on pause duration and tri-
gram part-of-speech frequency. The model was
constructed by identifying, from the HCRC Map
Task Corpus (Anderson et al, 1991), trigrams
ending with a backchannel. Fujie et al (2004)
used Hidden Markov Models to perform head nod
recognition. In their paper, they combined head
gesture detection with prosodic low-level features
from the same person to determine strongly pos-
itive, weak positive and negative responses to
yes/no type utterances.
In recent years, great research has shown the
strength of latent variable models for natural lan-
guage processing (Blunsom et al, 2008). One of
the most relevant works is that of Eisenstein and
Davis (2007), which presents a latent conditional
model for fusion of multiple modalities (speech
and gestures). One of the key difference of our
work is that we are explicitly modeling the mi-
cro dynamics and temporal relationship between
modalities.
3 Multimodal Prediction Models
Human face-to-face communication is a little like
a dance, in that participants continuously adjust
their behaviors based on verbal and nonverbal dis-
861
plays and signals. A topic of central interest in
modeling such behaviors is the patterning of in-
terlocutor actions and interactions, moment-by-
moment, and one of the key challenges is iden-
tifying the patterns that best predict specific ac-
tions. Thus we are interested in developing pre-
dictive models of communication dynamics that
integrate previous and current actions from all in-
terlocutors to anticipate the most likely next ac-
tions of one or all interlocutors. Humans are good
at this: they have an amazing ability to predict, at
a micro-level, the actions of an interlocutor (Bave-
las et al, 2000); and we know that better predic-
tions can correlate with more empathy and better
outcomes (Goldberg, 2005; Fuchs, 1987).
With turn-taking being perhaps the best-known
example, we now know a fair amount about some
aspects of communication dynamics, but much
less about others. However, recent advances in
machine learning and experimental methods, and
recent findings from a variety of perspectives, in-
cluding conversation analysis, social signal pro-
cessing, adaptation, corpus analysis and model-
ing, perceptual experiments, and dialog systems-
building and experimentation, mean that the time
is ripe to start working towards more comprehen-
sive predictive models.
The study of multimodal prediction models
bring a new series of research challenges:
MULTIMODAL ASYNCHRONY While speech
and gestures seem to come from a single under-
lying mental process (McNeill, 1992), they not
always happen at the same time, making it hard
for earlier multimodal fusion approaches based
on synchrony. A multimodal prediction model
needs to be able to learn automatically the tempo-
ral relationship (and relative importance) between
modalities.
MICRO DYNAMICS The dynamic between mul-
timodal signals should be taken at a micro level
since many of the interactions between speech and
gesture happen at the sub-gesture level or sub-
word level (Quek, 2003). Typical word-based
sampling may not be sufficient and instead a
higher sampling rate should be used.
LIMITED ANNOTATED DATA Given the time re-
quirement to correctly annotate multimodal data,
Figure 2: Latent Mixture of Discriminative Ex-
perts: a new dynamic model for multimodal fu-
sion. In this graphical model, xj represents the
jth multimodal observation, hj is a hidden state
assigned to xj , and yj the class label of xj . Gray
circles are latent variables. The micro dynamics
and multimodal temporal relationships are auto-
matically learned by the hidden states hj during
the learning phase.
most multimodal datasets contain only a limited
number of labeled examples. Since many ma-
chine learning algorithms rely on a large training
corpus, effective training of a predictive model on
multimodal datasets is challenging.
4 Latent Mixture of Discriminative
Experts
In this paper we present a multimodal fusion al-
gorithm, called Latent Mixture of Discriminative
Experts (shown in Figure 2), that addresses the
three challenges discussed in the previous section.
The hidden states of LMDE automatically learn
the temporal asynchrony between modalities. By
using a constant sample rate of 30Hz in our ex-
periments, we can model the micro dynamics of
speech and prosody (e.g., change of intonation
in the middle of a word). And finally, by train-
ing separate experts for each modalities, we im-
prove the prediction performance even with lim-
ited datasets.
The task of our LMDE model is to learn a map-
ping between a sequence of multimodal observa-
tions x = {x1, x2, ..., xm} and a sequence of la-
bels y = {y1, y2, ..., ym}. Each yj is a class la-
bel for the jth frame of a video sequence and is a
member of a set Y of possible class labels, for ex-
ample, Y = {head-nod,other-gesture}.
862
Each frame observation xj is represented by a fea-
ture vector ?(xj) ? Rd, for example, the prosodic
features at each sample. For each sequence, we
also assume a vector of ?sub-structure? variables
h = {h1, h2, ..., hm}. These variables are not ob-
served in the training examples and will therefore
form a set of hidden variables in the model.
Following Morency et al (2007), we define our
LMDE model as follows:
P (y | x, ?) = ?
h
P (y | h, x, ?)P (h | x, ?) (1)
where ? is the model parameters that is to be esti-
mated from training data.
To keep training and inference tractable,
Morency et al (2007) restrict the model to have
disjoint sets of hidden states associated with each
class label. Each hj is a member of a set Hyj
of possible hidden states for the class label yj .
H, the set of all possible hidden states, is defined
to be the union of all Hy sets. Since sequences
which have any hj /? Hyj will by definition have
P (y | h, x, ?) = 0, latent conditional model be-
comes:
P (y | x, ?) = ?
h:?hj?Hyj
P (h | x, ?). (2)
What differentiates our LMDE model from the
original work of Morency et al is the definition of
P (h|x, ?):
P (h| x, ?) =
exp
( ?
l ?l ? Tl(h, x)+?
? ?? ? P?(y|x, ??)
)
Z(x, ?) ,
(3)
whereZ is the partition function and P?(y|x) is
the conditional distribution of the expert indexed
by ?. The expert conditional distributions are de-
fined P?(y|x, ??) using the usual conditional ran-
dom field formulation:
P?(y| x, ??) = exp (
?
k ??,k ? F?,k(y, x))
Z?(x, ??) , (4)
F?,k is defined as
F?,k(y, x) =
m?
j=1
f?,k(yj?1, yj , x, j),
and each feature function f?,k(yj?1, yj , x, j) is
either a state function sk(yj , x, j) or a transition
function tk(yj?1, yj , x, j). State functions sk de-
pend on a single hidden variable in the model
while transition functions tk can depend on pairs
of hidden variables. Tl(h, x), defined in Equa-
tion 3, is a special case, summing only over
the transition feature functions tl(hl?1, hl, x, l).
Each expert ? contains a different subset of
f?,k(yj?1, yj , x, j). These feature functions are
defined in Section 5.2.
4.1 Learning Model Parameters
Given a training set consisting of n labeled se-
quences (xi,yi) for i = 1...n, training is done in
a two step process. First each expert ? is trained
following (Kumar and Herbert., 2003; Lafferty et
al., 2001) objective function to learn the parame-
ter ???:
L(??) =
n?
i=1
logP?(yi | xi, ??)? 12?2 ||??||
2
(5)
The first term in Eq. 5 is the conditional log-
likelihood of the training data. The second term
is the log of a Gaussian prior with variance ?2,
i.e., P (??) ? exp
( 1
2?2 ||??||2
).
Then the marginal probabilities P?(yj =
a | y, x, ???), are computed using belief prop-
agation and used as input for Equation 3. The
optimal parameter ?? was learned using the log-
likelyhood of the conditional probability defined
in Equation 2 (i.e., no regularization).
4.2 Inference
For testing, given a new test sequence x, we want
to estimate the most probable sequence of labels
y? that maximizes our LMDE model:
y? = argmaxy
?
h:?hi?Hyi
P (h | x, ??) (6)
5 Experimental Setup
We evaluate our Latent Mixture of Discrimina-
tive Experts on the multimodal task of predicting
listener nonverbal backchannel (i.e., head nods).
Backchannel feedback (the nods and paraverbals
such as ?uh-hu? and ?mm-hmm? that listeners
863
produce as some is speaking) has received con-
siderable interest due to its pervasiveness across
languages and conversational contexts.
5.1 Dataset
We are using the RAPPORT dataset from (Maat-
man et al, 2005), which contains 47 dyadic inter-
actions between a speaker and a listener. Data is
drawn from a study of face-to-face narrative dis-
course (?quasi-monologic? storytelling). In this
dataset, participants in groups of two were told
they were participating in a study to evaluate a
communicative technology. Subjects were ran-
domly assigned the role of speaker and listener.
The speaker viewed a short segment of a video
clip taken from the Edge Training Systems, Inc.
Sexual Harassment Awareness video. After the
speaker finished viewing the video, the listener
was led back into the computer room, where the
speaker was instructed to retell the stories por-
trayed in the clips to the listener. The listener
was asked to not talk during the story retelling.
Elicited stories were approximately two minutes
in length on average. Participants sat approxi-
mately 8 feet apart. Video sequences were manu-
ally annotated to determine the ground truth head
nod labels. A total of 587 head nods occured over
all video sequences.
5.2 Multimodal Features
This section describes the different multimodal
features used to create our five experts.
PROSODY Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with a
speaker?s prosody (Nishimura et al, 2007; Ward
and Tsukahara, 2000; Cathcart et al, 2003). For
example, Ward and Tsukahara (2000) show that
short listener backchannels (listener utterances
like ?ok? or ?uh-huh? given during a speaker?s ut-
terance) are associated with a lowering of pitch
over some interval. Listener feedback often fol-
lows speaker pauses or filled pauses such as
?um? (see (Cathcart et al, 2003)). Using openS-
MILE (Eyben et al, 2009) toolbox, we extract the
following prosodic features, including standard
linguistic annotations and the prosodic features
suggested by Ward and Tsukhara: downslopes in
pitch continuing for at least 40ms, regions of pitch
lower than the 26th percentile continuing for at
least 110ms (i.e., lowness), drop or rise in energy
of speech (i.e., energy edge), Fast drop or rise in
energy of speech (i.e., energy fast edge), vowel
volume (i.e., vowels are usually spoken softer)
and Pause in speech (i.e., no speech).
VISUAL GESTURES Gestures performed by the
speaker are often correlated with listener feed-
back (Burgoon et al, 1995). Eye gaze, in particu-
lar, has often been implicated as eliciting listener
feedback. Thus, we manually annotate the follow-
ing contextual feature: speaker looking at the lis-
tener.
LEXICAL Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al, 2003). Using the transcrip-
tions, we included all individual words (i.e., uni-
grams) spoken by the speaker during the interac-
tions.
SYNTACTIC STRUCTURE Finally, we attempt
to capture syntactic information that may pro-
vide relevant cues by extracting four types of fea-
tures from a syntactic dependency structure cor-
responding to the utterance. The syntactic struc-
ture is produced automatically using a CRF part-
of-speech (POS) tagger and a data-driven left-to-
right shift-reduce dependency parser (Sagae and
Tsujii, 2007), both trained on POS tags and de-
pendency trees extracted from the Switchboard
section of the Penn Treebank (Marcus et al,
1994), converted to dependency trees using the
Penn2Malt tool1. The four syntactic features are:
? Part-of-speech tags for each word (e.g. noun,
verb, etc.), taken from the output of the POS
tagger
? Grammatical function for each word (e.g.
subject, object, etc.), taken directly from the
dependency labels produced by the parser
? Part-of-speech of the syntactic head of each
word, taken from the dependency links pro-
duced by the parser
? Distance and direction from each word to its
syntactic head, computed from the depen-
dency links produced by the parser
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
864
Figure 3: Baseline Models: a) Conditional Random Fields (CRF), b) Latent Dynamic Conditional
Random Fields(LDCRF), c) CRF Mixture of Experts (no latent variable)
Although our current method for extracting
these features requires that the entire utterance
be available for processing, this provides us with
a first step towards integrating information about
syntactic structure in multimodal prediction mod-
els. Many of these features could in principle be
computed incrementally with only a slight degra-
dation in accuracy, with the exception of features
that require dependency links where a word?s syn-
tactic head is to the right of the word itself. We
leave an investigation that examines only syntac-
tic features that can be produced incrementally in
real time as future work.
5.3 Baseline Models
INDIVIDUAL EXPERTS Our first baseline model
consists of a set of CRF chain models, each
trained with different set of multimodel features
(as described in the previous section). In other
words, only visual, prosodic, lexical or syntactic
features are used to train a single CRF expert. In
one CRF chain model, each gesture class corre-
sponds to a state label. (See Figure 3a).
MULTIMODAL CLASSIFIERS (EARLY FUSION)
Our second baseline consists of two models: CRF
and LDCRF (Morency et al, 2007). To train these
models, we concatenate all multimodal features
(lexical, syntactic, prosodic and visual) in one in-
put vector. Graphical representation of these base-
line models are given in Figure 3.
CRF MIXTURE OF EXPERTS To show the im-
portance of latent variable in our LMDE model,
we trained a CRF-based mixture of discriminative
experts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). The training is performed in two steps.
A graphical representation of a CRF Mixture of
experts is given in the last graph of Figure 3.
5.4 Methodology
We performed held-out testing by randomly se-
lecting a subset of 11 interactions (out of 47) for
the test set. The training set contains the remain-
ing 36 dyadic interactions. All models in this pa-
per were evaluated with the same training and test
sets. Validation of all model parameters (regular-
ization term and number of hidden states) was per-
formed using a 3-fold cross-validation strategy on
the training set. The regularization term was vali-
dated with values 10k, k = ?1..3. Three different
number of hidden states were tested for the LMDE
models: 2, 3 and 4.
The performance is measured by using the F-
measure. This is the weighted harmonic mean
of precision and recall. Precision is the proba-
bility that predicted backchannels correspond to
actual listener behavior. Recall is the probabil-
ity that a backchannel produced by a listener in
our test set was predicted by the model. We use
the same weight for both precision and recall, so-
called F1. During validation we find all the peaks
(i.e., local maxima) from the marginal probabil-
ities. These backchannel hypotheses are filtered
using the optimal threshold from the validation
set. A backchannel (i.e., head nod) is predicted
correctly if a peak happens during an actual lis-
tener backchannel with high enough probability.
The same evaluation measurement is applied to all
models.
The training of all CRFs and LDCRFs were
done using the hCRF library2. The LMDE model
was implemented in Matlab3 based on the hCRF
2http://sourceforge.net/projects/hrcf/
3The source code is available at:
http://projects.ict.usc.edu/multicomp/.
865
Table 1: Comparison of individual experts with
our Latent Mixture of Discriminative Experts
(LMDE).
Expert Precision Recall f1
Lexical 0.1647 0.3305 0.2198
Prosody 0.1396 0.9112 0.2421
Syntactic 0.1833 0.4663 0.2632
POS 0.1935 0.4514 0.2709
Eye Gaze 0.1573 0.1741 0.1653
LMDE 0.2295 0.5677 0.3268
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Recall
Pre
cis
ion
Latent Mixture
Lexical
Prosody
Syntactic
POS
EyeGaze
Figure 4: Comparison of individual experts with
our LMDE model.
library.
6 Results and Discussion
In this section we present the results of our empiri-
cal evaluation designed to test the three main char-
acteristics of the LMDE model: (1) integration of
multiple sources of information, (2) late fusion ap-
proach and (3) latent variable which models the
hidden dynamic between experts. We also present
an analysis of the output probabilities from the
LMDE model and individual experts.
INDIVIDUAL EXPERTS We trained one individ-
ual expert for each feature types: visual, prosodic,
lexical and syntactic features (both part-of speech
and syntactic structure). Precision, recall and F1
values for each individual expert and our LMDE
model are shown in Table 1 and Figure 4.
Pairwise two-tailed t-test comparison between
our LMDE model and individual experts shows a
Table 2: Comparison of our Latent Mixture of
Discriminative Experts (LMDE) with two early
fusion technique (CRF vs LDCRF) and the CRF
Mixture of Experts (Smith et al, 2005).
model Precision Recall f1
LMDE 0.2295 0.5677 0.3268
Early CRF 0.13958 0.9245 0.2425
Early LDCRF 0.1826 0.2484 0.2105
Mixture CRF 0.1502 0.2712 0.1934
significant difference for Lexical, Prosody, Syn-
tactic and Eye gaze, with respective p-values of
0.0037, 0.0379, 0.0400 and 0.0233. Even though
some experts may not perform well individually
(e.g., eye gaze), they can bring important informa-
tion once merged with others. Table 1 shows that
our LMDE model was able to take advantage of
the complementary information from each expert.
LATE FUSION We compare our approach with
two early fusion models: CRF and Latent-
dynamic CRF (see Figure 3). Table 2 summarizes
the results. The CRF model learns direct weights
between input features and the gesture labels. The
LDCRF is able to model more complex dynam-
ics between input features with the latent variable.
We can see that our LMDE model outperforms
both early fusion approaches because of its late
fusion approach. Pairwise two-tailed t-test analy-
sis gives p-values of 0.0481 and 0.0748, for CRF
and LDCRF respectively.
LATENT VARIABLE The CRF Mixture of Ex-
perts (2005) directly merges the expert outputs
while our model uses a latent variable to model the
hidden dynamic between experts (see Figure 3).
Table 2 summarizes the results. Pairwise two-
tailed t-test comparison between these two mod-
els shows a significant difference with a p-value
of 0.0062. This result is important since it shows
that our LMDE model does learn the hidden inter-
action between experts.
MODEL ANALYSIS To understand the multi-
modal integration which happens at the latent
variable level in our LMDE model, Figure 5
shows the output probabilities for all five individ-
ual experts as well as our model. The strength of
the latent variable is to enable different weigting
866
Speak
er inp
ut fea
tures
Exper
ts
Laten
t
Mixtu
re Ground truth labels
Word
s
Gaze ...
Time
(a) (b)
5.6s 7.7s 10.3s
...
15.6s 17.0s 18.7s 20.5s
Figure 5: Output probabilities from LMDE and individual experts for two different sub-sequences. The
gray areas in the graph corresponds to ground truth backchannel feedbacks of the listener.
of the experts at different point in time.
By analyzing the sequence (a), we observe that
both the POS and Syntactic experts learned that
when no words are present (i.e., pause) there is
a high likelihood of backchennel feedback from
the listener (shown at 5.6s and 10.3s). These two
experts are highly weighted (by one of the hid-
den state) during this part of the sequence. Also,
both the Lexical and POS experts learned that the
word ??that?? (and its part-of-speech) are impor-
tant but since the speaker is not looking at the
listener when saying it, the output from LMDE
model is low (see Figure 5, Sequence (a), 7.7s).
By analyzing sequence (b), we see that the Lex-
ical and POS experts learned the importance of the
??and?? at 15.6s and 20.5s. More importantly, we
can see at 17.0s and 18.7s that the influence of
the POS and Syntactic experts have been reduced
in the LMDE output probability. This difference
of weighting shows that a different hidden state is
active during Sequence (b).
7 Conclusion
In this paper we introduced a new model
called Latent Mixture of Discriminative Experts
(LMDE) for learning predictive models of human
communication behaviors. Many of the interac-
tions between speech and gesture happen at the
sub-gesture or sub-word level. LMDE learns au-
tomatically the temporal relationship between dif-
ferent modalities. Since, we train separate experts
for each modality, LMDE is capable of improv-
ing the prediction performance even with limited
datasets.
We evaluated our model on the task of non-
verbal feedback prediction (e.g., head nod). Our
experiments confirm the importance of combin-
ing the four types of multimodal features: lexical,
syntactic structure, eye gaze, and prosody. LMDE
is a generic model that can be applied to a wide
range of problems. As future work, we are plan-
ning to test our model on dialog act classification
and multimodal behavior recognition tasks.
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDECOM).
The content does not necessarily reflect the posi-
tion or the policy of the Government, and no offi-
cial endorsement should be inferred.
References
Anderson, H., M. Bader, E.G. Bard, G. Doherty, S. Garrod,
S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo,
867
H. Thompson, and R. Weinert. 1991. The mcrc map task
corpus. Language and Speech, 34(4):351?366.
Bavelas, J.B., L. Coates, and T. Johnson. 2000. Listeners as
co-narrators. JPSP, 79(6):941?952.
Blunsom, P., T. Cohn, and M. Osborne. 2008. A discrimi-
native latent variable model for statistical machine trans-
lation. In ACL: HLT, pages 200?208.
Burgoon, Judee K., Lesa A. Stern, and Leesa Dillman. 1995.
Interpersonal adaptation: Dyadic interaction patterns.
Cambridge University Press, Cambridge.
Cassell, J. and M. Stone. 1999. Living hand to mouth: Psy-
chological theories about speech and gesture in interactive
dialogue systems. In AAAI.
Cathcart, N., Jean Carletta, and Ewan Klein. 2003. A shal-
low model of backchannel continuers in spoken dialogue.
In EACL, pages 51?58.
Eisenstein, J., R. Barzilay, and R. Davis. 2008. Gestural co-
hesion for topic segmentation. In ACL: HLT, pages 852?
860.
Eisentein, J. and R. Davis. 2007. Conditional modality fu-
sion for coreference. In ACL, pages 352?359.
Eyben, Florian, Martin Wo?llmer, and Bjo?rn Schuller. 2009.
openEAR - Introducing the Munich Open-Source Emo-
tion and Affect Recognition Toolkit. In ACII, pages 576?
581.
Frampton, M., J. Huang, T. Bui, and S. Peters. 2009.
Real-time decision detection in multi-party dialogue. In
EMNLP, pages 1133?1141.
Fuchs, D. 1987. Examiner familiarity effects on test perfor-
mance: implications for training and practice. Topics in
Early Childhood Special Education, 7:90?104.
Fujie, Shinya, Yasuhi Ejiri, Kei Nakajima, Yosuke Mat-
susaka, and Tetsunori Kobayashi. 2004. A conversation
robot using head gesture recognition as para-linguistic in-
formation. In RO-MAN, pages 159?164.
Goldberg, S.B. 2005. The secrets of successful mediators.
Negotiation Journal, 21(3):365?376.
Gravano, A., S. Benus, H. Chavez, J. Hirschberg, and
L. Wilcox. 2007. On the role of context and prosody
in the interpretation and ?okay?. In ACL, pages 800?807.
Heylen, D. and R. op den Akker. 2007. Computing
backchannel distributions in multi-party conversations. In
ACL:EmbodiedNLP, pages 17?24.
Johnston, M. 1998. Multimodal language processing. In
ICSLP.
Jovanovic, N., R. op den Akker, and A. Nijholt. 2006.
Adressee identification in face-to-face meetings. In
EACL.
Jurafsky, D., E. Shriberg, B. Fox, and T. Curl. 1998. Lexical,
prosodic and syntactic cures for dialog acts. In Workshop
on Discourse Relations, pages 114?120.
Kendon, A. 2004. Gesture: Visible Action as Utterance.
Cambridge University Press.
Kumar, S. and M. Herbert. 2003. Discriminative random
fields: A framework for contextual interaction in classifi-
cation. In ICCV.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional
random fields: probabilistic models for segmenting and
labelling sequence data. In ICML.
Maatman, M., J. Gratch, and S. Marsella. 2005. Natural
behavior of a listening agent. In IVA.
Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The penn treebank: anno-
tating predicate argument structure. In ACL:HLT, pages
114?119.
McNeill, D. 1992. Hand and Mind: What Gestures Reveal
about Thought. Univ. Chicago Press.
Moore, P.-Y. Hsueh J. 2007. What decisions have you made:
Automatic decision detection in conversational speech. In
NAACL-HLT, pages 25?32.
Morency, Louis-Philippe, Ariadna Quattoni, and Trevor Dar-
rell. 2007. Latent-dynamic discriminative models for
continuous gesture recognition. In CVPR.
Murray, G. and G. Carenini. 2009. Predicting subjectivity in
multimodal conversations. In EMNLP, pages 1348?1357.
Nakano, Reinstein, Stocky, and Justine Cassell. 2003. To-
wards a model of face-to-face grounding. In ACL.
Nakano, Y., K. Murata, M. Enomoto, Y. Arimoto, Y. Asa,
and H. Sagawa. 2007. Predicting evidence of understand-
ing by monitoring user?s task manipulation in multimodal
conversations. In ACL, pages 121?124.
Nishimura, Ryota, Norihide Kitaoka, and Seiichi Nakagawa.
2007. A spoken dialog system for chat-like conversations
considering response timing. LNCS, 4629:599?606.
Oviatt, S. 1999. Ten myths of multimodal interaction. Com-
munications of the ACM.
Quek, F. 2003. The catchment feature model for multimodal
language analysis. In ICCV.
Sagae, Kenji and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with LRmodels and parser ensem-
bles. In ACL, pages 1044?1050.
Smith, A., T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18?25.
Ward, N. and W. Tsukahara. 2000. Prosodic features which
cue back-channel responses in english and japanese.
Journal of Pragmatics, 23:1177?1207.
868
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 335?340,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Modeling Wisdom of Crowds Using
Latent Mixture of Discriminative Experts
Derya Ozkan and Louis-Philippe Morency
Institute for Creative Technologies
University of Southern California
{ozkan,morency}@ict.usc.edu
Abstract
In many computational linguistic scenarios,
training labels are subjectives making it nec-
essary to acquire the opinions of multiple an-
notators/experts, which is referred to as ?wis-
dom of crowds?. In this paper, we propose a
new approach for modeling wisdom of crowds
based on the Latent Mixture of Discrimina-
tive Experts (LMDE) model that can automat-
ically learn the prototypical patterns and hid-
den dynamic among different experts. Experi-
ments show improvement over state-of-the-art
approaches on the task of listener backchannel
prediction in dyadic conversations.
1 Introduction
In many real life scenarios, it is hard to collect
the actual labels for training, because it is expen-
sive or the labeling is subjective. To address this
issue, a new direction of research appeared in the
last decade, taking full advantage of the ?wisdom of
crowds? (Surowiecki, 2004). In simple words, wis-
dom of crowds enables parallel acquisition of opin-
ions from multiple annotators/experts.
In this paper, we propose a new method to fuse
wisdom of crowds. Our approach is based on the
Latent Mixture of Discriminative Experts (LMDE)
model originally introduced for multimodal fu-
sion (Ozkan et al, 2010). In our Wisdom-LMDE
model, a discriminative expert is trained for each
crowd member. The key advantage of our compu-
tational model is that it can automatically discover
the prototypical patterns of experts and learn the dy-
namic between these patterns. An overview of our
approach is depicted in Figure 1.
We validate our model on the challenging task of
listener backchannel feedback prediction in dyadic
conversations. Backchannel feedback includes the
nods and paraverbals such as ?uh-huh? and ?mm-
hmm? that listeners produce as they are speaking.
Backchannels play a significant role in determining
the nature of a social exchange by showing rapport
and engagement (Gratch et al, 2007). When these
signals are positive, coordinated and reciprocated,
they can lead to feelings of rapport and promote
beneficial outcomes in diverse areas such as nego-
tiations and conflict resolution (Drolet and Morris,
2000), psychotherapeutic effectiveness (Tsui and
Schultz, 1985), improved test performance in class-
rooms (Fuchs, 1987) and improved quality of child
care (Burns, 1984). Supporting such fluid interac-
tions has become an important topic of virtual hu-
man research. In particular, backchannel feedback
has received considerable interest due to its perva-
siveness across languages and conversational con-
texts. By correctly predicting backchannel feed-
back, virtual agent and robots can have stronger
sense of rapport.
What makes backchannel prediction task well-
suited for our model is that listener feedback varies
between people and is often optional (listeners can
always decide to give feedback or not). A successful
computational model of backchannel must be able
to learn these variations among listeners. Wisdom-
LMDE is a generic approach designed to integrate
opinions from multiple listeners.
In our experiments, we validate the performance
of our approach using a dataset of 43 storytelling
dyadic interactions. Our analysis suggests three pro-
335
Latent Mixture of 
Discriminative Experts
h1 h2 h3 hn
y2y1 y3 yn
x1
x1
Wisdom of crowds
(listener backchannel)
Speaker
x1 x2 x3 xnPitch
Words
Gaze Look  at listener
h1
Time
Figure 1: Left: Our approach applied to backchannel prediction: (1) multiple listeners experience the same series of
stimuli (pre-recorded speakers) and (2) a Wisdom-LMDE model is learned using this wisdom of crowds, associating
one expert for each listener. Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF),
b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable)
totypical patterns for backchannel feedback. By
automatically identifying these prototypical pat-
terns and learning the dynamic, our Wisdom-LMDE
model outperforms the previous approaches for lis-
tener backchannel prediction.
1.1 Previous Work
Several researchers have developed models to pre-
dict when backchannel should happen. Ward and
Tsukahara (2000) propose a unimodal approach
where backchannels are associated with a region of
low pitch lasting 110ms during speech. Nishimura et
al. (2007) present a unimodal decision-tree approach
for producing backchannels based on prosodic fea-
tures. Cathcart et al (2003) propose a unimodal
model based on pause duration and trigram part-of-
speech frequency.
Wisdom of crowds was first defined and used in
business world by Surowiecki (2004). Later, it has
been applied to other research areas as well. Raykar
et. al. (2010) proposed a probabilistic approach for
supervised learning tasks for which multiple annota-
tors provide labels but not an absolute gold standard.
Snow et. al. (2008) show that using non-expert la-
bels for training machine learning algorithms can be
as effective as using a gold standard annotation.
In this paper, we present a computational ap-
proach for listener backchannel prediction that ex-
ploits multiple listeners. Our model takes into ac-
count the differences in people?s reactions, and au-
tomatically learns the hidden structure among them.
The rest of the paper is organized as follows. In
Section 2, we present the wisdom acquisition pro-
cess. Then, we describe our Wisdom-LMDE model
in Section 3. Experimentals are presented in Sec-
tion 4. Finally, we conclude with discussions and
future works in Section 5.
2 Wisdom Acquisition
It is known that culture, age and gender affect peo-
ple?s nonverbal behaviors (Linda L. Carli and Loe-
ber, 1995; Matsumoto, 2006). Therefore, there
might be variations among people?s reactions even
when experiencing the same situation. To effi-
ciently acquire responses from multiple listeners, we
employ the Parasocial Consensus Sampling (PCS)
paradigm (Huang et al, 2010), which is based on the
theory that people behave similarly when interact-
ing through a media (e.g., video conference). Huang
et al (2010) showed that a virtual human driven by
PCS approach creates significantly more rapport and
is perceived as more believable than the virtual hu-
man driven by face-to-face interaction data (from ac-
tual listener). This result indicates that the parasocial
paradigm is a viable source of information for wis-
dom of crowds.
In practice, PCS is applied by having participants
watch pre-recorded speaker videos drawn from a
336
Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9
pauselabel:subPOS:NN
POS:NNpauselabel:pmod
pausePOS:NNlabel:nmod
pausePOS:NNlow pitch
pausedirdist:L1low pitch
POS:NNpauselow pitch
Eyebrow updirdist:L8+POS:NN
eye gazedirdist:R1POS:JJ
lownesseye gazepause
Table 1: Most predictive features for each listener from our wisdom dataset. This analysis suggests three prototypical
patterns for backchannel feedback.
dyadic story-telling dataset. In our experiments,
we used 43 video-recorded dyadic interactions from
the RAPPORT1 dataset (Gratch et al, 2006). This
dataset was drawn from a study of face-to-face
narrative discourse (?quasi-monologic? storytelling).
The videos of the actual listeners were manually an-
notated for backchannel feedback. For PCS wis-
dom acquisition, we recruited 9 participants, who
were told to pretend they are an active listener and
press the keyboard whenever they felt like provid-
ing backchannel feedback. This provides us the re-
sponses from multiple listeners all interacting with
the same speaker, hence the wisdom necessary to
model the variability among listeners.
3 Modeling Wisdom of Crowds
Given the wisdom of multiple listeners, our goal is to
create a computational model of backchannel feed-
back. Although listener responses vary among indi-
viduals, we expect some patterns in these responses.
Therefore, we first analyze the most predictive fea-
tures for each listener and search for prototypical
patterns (in Section 3.1). Then, we present our
Wisdom-LMDE that allows to automatically learn
the hidden structure within listener responses.
3.1 Wisdom Analysis
We analyzed our wisdom data to see the most rel-
evant speaker features when predicting responses
from each individual listener. (The complete list of
speaker features are described in Section 4.1.) We
used a feature ranking scheme based on a sparse
regularization technique, as described in (Ozkan and
Morency, 2010). It allows us to identify the speaker
features most predictive of each listener backchan-
nel feedback. The top 3 features for all 9 listeners
are listed in Table 1.
This analysis suggests three prototypical patterns.
For the first 3 listeners, pause in speech and syntac-
1http://rapport.ict.usc.edu/
tic information (POS:NN) are more important. The
next 3 experts include a prosodic feature, low pitch,
which is coherent with earlier findings (Nishimura
et al, 2007; Ward and Tsukahara, 2000). It is inter-
esting to see that the last 3 experts incorporate visual
information when predicting backchannel feedback.
This is in line with Burgoon et al (Burgoon et al,
1995) work showing that speaker gestures are of-
ten correlated with listener feedback. These results
clearly suggest that variations be present among lis-
teners and some prototypical patterns may exist.
Based on these observations, we propose new com-
putational model for listener backchannel.
3.2 Computational Model: Wisdom-LMDE
The goals of our computational model are to au-
tomatically discover the prototypical patterns of
backchannel feedback and learn the dynamic be-
tween these patterns. This will allow the compu-
tational model to accurately predict the responses of
a new listener even if he/she changes her backchan-
nel patterns in the middle of the interaction. It will
also improve generalization by allowing mixtures of
these prototypical patterns.
To achieve these goals, we propose a variant of the
Latent Mixture of Discriminative Experts (Ozkan et
al., 2010) which takes full advantage of the wisdom
of crowds. Our Wisdom-LMDE model is based on
a two step process: a Conditional Random Field
(CRF, see Figure 1a) is learned for each wisdom
listener, and the outputs of these expert models are
used as input to a Latent Dynamic Conditional Ran-
dom Field (LDCRF, see Figure 1b) model, which is
capable of learning the hidden structure within the
experts. In our Wisdom-LMDE, each expert cor-
responds to a different listener from the wisdom of
crowds. More details about training and inference of
LMDE can be found in Ozkan et al (2010).
337
4 Experiments
To confirm the validity of our Wisdom-LMDE
model, we compare its performance with compu-
tational models previously proposed. As motivated
earlier, we focus our experiments on predicting lis-
tener backchannel since it is a well-suited task where
variability exists among listeners.
4.1 Multimodal Speaker Features
The speaker videos were transcribed and annotated
to extract the following features:
Lexical: Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al, 2003). Therefore, we use all
the words (i.e., unigrams) spoken by the speaker.
Syntactic structure: Using a CRF part-of-speech
(POS) tagger and a data-driven left-to-right shift-
reduce dependency parser (Sagae and Tsujii, 2007)
we extract four types of features from a syntactic de-
pendency structure corresponding to the utterance:
POS tags and grammatical function for each word,
POS tag of the syntactic head, distance and direction
from each word to its syntactic head.
Prosody: Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with
a speaker?s prosody (Ward and Tsukahara, 2000;
Nishimura et al, 2007). Following this, we use
downslope in pitch, pitch regions lower than 26th
percentile, drop/rise and fast drop/rise in energy of
speech, vowel volume, pause.
Visual gestures: Gestures performed by the speaker
are often correlated with listener feedback (Burgoon
et al, 1995). Eye gaze, in particular, has often been
implicated as eliciting listener feedback. Thus, we
encode the following contextual features: speaker
looking at listener, smiling, moving eyebrows up
and frowning.
Although our current method for extracting these
features requires that the entire utterance to be avail-
able for processing, this provides us with a first
step towards integrating information about syntac-
tic structure in multimodal prediction models. Many
of these features could in principle be computed in-
crementally with only a slight degradation in accu-
racy, with the exception of features that require de-
pendency links where a word?s syntactic head is to
the right of the word itself. We leave an investiga-
tion that examines only syntactic features that can be
produced incrementally in real time as future work.
4.2 Baseline Models
Consensus Classifier In our first baseline model, we
use consensus labels to train a CRF model, which
are constructed by a similar approach presented
in (Huang et al, 2010). The consensus threshold is
set to 3 (at least 3 listeners agree to give feedback at
a point) so that it contains approximately the same
number of head nods as the actual listener. See Fig-
ure 1 for a graphical representation of CRF model.
CRF Mixture of Experts To show the importance
of latent variable in our Wisdom-LMDE model, we
trained a CRF-based mixture of discriminative ex-
perts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). Similar to our Wisdom-LMDE model,
the training is performed in two steps. A graphical
representation of a CRF Mixture of experts is given
in the Figure 1.
Actual Listener (AL) Classifiers This baseline model
consists of two models: CRF and LDCRF chains
(See Figure 1). To train these models, we use the
labels of the ?Actual Listeners? (AL) from the RAP-
PORT dataset.
Multimodal LMDE In this baseline model, we com-
pare our Wisdom LMDE to a multimodal LMDE,
where each expert refers to one of 5 different set of
multimodal features as presented in (Ozkan et al,
2010): lexical, prosodic, part-of-speech, syntactic,
and visual.
Random Classifier Our last baseline model is a ran-
dom backchannel generator as desribed by Ward
and Tsukahara (2000). This model randomly gener-
ates backchannels whenever some pre-defined con-
ditions in the prosody of the speech is purveyed.
4.3 Methodolgy
We performed hold-out testing on a randomly se-
lected subset of 10 interactions. The training set
contains the remaining 33 interactions. Model pa-
rameters were validated by using a 3-fold cross-
validation strategy on the training set. Regulariza-
338
Table 2: Comparison of our Wisdom-LMDE model with previously proposed models. The last column shows the
paired one tailed t-test results comparing Wisdom LMDE to each model.
tion values used are 10k for k = -1,0,..,3. Numbers
of hidden states used in the LDCRF models were
2, 3 and 4. We use the hCRF library2 for training
of CRFs and LDCRFs. Our Wisdom-LMDE model
was implemented in Matlab based on the hCRF li-
brary. Following (Morency et al, 2008), we use
an encoding dictionary to represent our features.
The performance is measured by using the F-score,
which is the weighted harmonic mean of precision
and recall. A backchannel is predicted correctly if
a peak happens during an actual listener backchan-
nel with high enough probability. The threshold was
selected automatically during validation.
4.4 Results and Discussion
Before reviewing the prediction results, is it impor-
tant to remember that backchannel feedback is an
optional phenomena, where the actual listener may
or may not decide on giving feedback (Ward and
Tsukahara, 2000). Therefore, results from predic-
tion tasks are expected to have lower accuracies as
opposed to recognition tasks where labels are di-
rectly observed (e.g., part-of-speech tagging).
Table 2 summarizes our experiments comparing
our Wisdom-LMDE model with state-of-the-art ap-
proaches for behavior prediction (see Section 4.2).
Our Wisdom-LMDE model achieves the best F1
score. Statistical t-test analysis show that Wisdom-
LMDE is significantly better than Consensus Clas-
sifier, AL Classifier (LDCRF), Multimodel LMDE
and Random Classifier.
The second best F1 score is achieved by CRF
Mixture of experts, which is the only model among
other baseline models that combines different lis-
tener labels in a late fusion manner. This result
2http://sourceforge.net/projects/hrcf/
supports our claim that wisdom of clouds improves
learning of prediction models. CRF Mixture model
is a linear combination of the experts, whereas
Wisdom-LMDE enables different weighting of ex-
perts at different point in time. By using hidden
states, Wisdom-LMDE can automatically learn the
prototypical patterns between listeners.
One really interesting result is that the optimal
number of hidden states in the Wisdom-LMDE
model (after cross-validation) is 3. This is coherent
with our qualitative analysis in Section 3.1, where
we observed 3 prototypical patterns.
5 Conclusions
In this paper, we proposed a new approach called
Wisdom-LMDE for modeling wisdom of crowds,
which automatically learns the hidden structure in
listener responses. We applied this method on
the task of listener backchannel feedback predic-
tion, and showed improvement over previous ap-
proaches. Both our qualitative analysis and exper-
imental results suggest that prototypical patterns ex-
ist when predicting listener backchannel feedback.
The Wisdom-LMDE is a generic model applicable
to multiple sequence labeling tasks (such as emotion
analysis and dialogue intent recognition), where la-
bels are subjective (i.e. small inter-coder reliability).
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDE-COM).
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred.
339
References
Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman.
1995. Interpersonal adaptation: Dyadic interaction
patterns. Cambridge University Press, Cambridge.
M. Burns. 1984. Rapport and relationships: The basis of
child care. Journal of Child Care, 4:47?57.
N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A
shallow model of backchannel continuers in spoken
dialogue. In European Chapter of the Association for
Computational Linguistics. 51?58.
Aimee L. Drolet and Michael W. Morris. 2000. Rap-
port in conflict resolution: Accounting for how face-
to-face contact fosters mutual cooperation in mixed-
motive conflicts. Journal of Experimental Social Psy-
chology, 36(1):26?50.
D. Fuchs. 1987. Examiner familiarity effects on test per-
formance: Implications for training and practice. Top-
ics in Early Childhood Special Education, 7:90?104.
J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella,
M. Morales, R.J. Werf, and L.-P. Morency. 2006. Vir-
tual rapport. Proceedings of International Conference
on Intelligent Virtual Agents (IVA), Marina del Rey,
CA.
Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward
Fast. 2007. Creating rapport with virtual agents. In
IVA.
L. Huang, L.-P. Morency, and J. Gratch:. 2010. Paraso-
cial consensus sampling: combining multiple perspec-
tives to learn virtual human behavior. In Interna-
tional Conference on Autonomous Agents and Multi-
agent Systems (AAMAS).
Suzanne J. LaFleur Linda L. Carli and Christopher C.
Loeber. 1995. Nonverbal behavior, gender, and influ-
ence. Journal of Personality and Social Psychology.
68, 1030-1041.
D. Matsumoto. 2006. Culture and Nonverbal Behav-
ior. The Sage Handbook of Nonverbal Communica-
tion, Sage Publications Inc.
L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predict-
ing listener backchannels: A probabilistic multimodal
approach. In Proceedings of the Conference on Intel-
ligent Virutal Agents (IVA).
Ryota Nishimura, Norihide Kitaoka, and Seiichi Naka-
gawa. 2007. A spoken dialog system for chat-like
conversations considering response timing. Interna-
tional Conference on Text, Speech and Dialog. 599-
606.
D. Ozkan and L.-P. Morency. 2010. Concensus of self-
features for nonverbal behavior analysis. In Human
Behavior Understanding in conjucion with Interna-
tional Conference in Pattern Recognition.
D. Ozkan, K. Sagae, and L.-P. Morency. 2010. La-
tent mixture of discriminative experts for multimodal
prediction modeling. In International Conference on
Computational Linguistics (COLING).
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, Linda Moy, and David Blei. 2010. Learning
from crowds.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18?25.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008.
Cheap and fast - but is it good? Evaluating non-expert
annotations for natural language tasks.
James Surowiecki. 2004. The Wisdom of Crowds: Why
the Many Are Smarter Than the Few and How Col-
lective Wisdom Shapes Business, Economies, Societies
and Nations. Doubleday.
P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why
psychotheraputic engagement fails in the treatment of
asian clients. American Journal of Orthopsychiatry,
55:561?569.
N. Ward and W. Tsukahara. 2000. Prosodic fea-
tures which cue back-channel responses in english and
japanese. Journal of Pragmatics. 23, 1177?1207.
340
