FASIL Email Summarisation System 
 
 
Angelo Dalli, Yunqing Xia, Yorick Wilks 
NLP Research Group 
Department of Computer Science 
University of Sheffield 
{a.dalli, y.xia, y.wilks}@dcs.shef.ac.uk 
 
 
Abstract 
Email summarisation presents a unique set of 
requirements that are different from general 
text summarisation. This work describes the 
implementation of an email summarisation 
system for use in a voice-based Virtual Per-
sonal Assistant developed for the EU FASiL 
Project. Evaluation results from the first inte-
grated version of the project are presented. 
1 Introduction 
Email is one of the most ubiquitous applications used on 
a daily basis by millions of people world-wide, tradi-
tionally accessed over a fixed terminal or laptop com-
puter. In the past years there has been an increasing 
demand for email access over mobile phones. Our work 
has focused on creating an email summarisation service 
that provides quality summaries adaptively and quickly 
enough to cater for the tight constrains imposed by a 
real time text-to-speech system. 
This work has been done as part of the European 
Union FASiL project, which aims to aims to construct a 
conversationally intelligent Virtual Personal Assistant 
(VPA) designed to manage the user?s personal and 
business information through a voice-based interface 
accessible over mobile phones.  
As the quality of life and productivity is to improved 
in an increasingly information dominated society, peo-
ple need access to information anywhere, anytime. The 
Adaptive Information Management (AIM) service in the 
FASiL VPA seeks to automatically prioritise and pre-
sent information that is most pertinent to the mobile 
users and adapt to different user preferences. The AIM 
service is comprised of three main parts: an email sum-
mariser, email categoriser, calendar scheduling/PIM 
interaction and an adaptive prioritisation service that 
optimizes the sequence in which information is pre-
sented, keeping the overall duration of the voice-based 
dialogue to a minimum. 
2 Email Characteristics 
Email Summarisation techniques share many character-
istics with general text summarisation techniques while 
catering for the unique characteristics of email: 
1. short messages usually between 2 to 800 
words in length (after thread-filtering) 
2. frequently do not obey grammatical or con-
ventional stylistic conventions 
3. are a cross between informal mobile text or 
chat styles and traditional writing formats 
4. display unique thread characteristics with 87% 
containing three previous emails or less 
(Fisher and Moody, 2001) 
All these four main characteristics combined to-
gether mean that most document summarisation tech-
niques simply do not work well for email. The voice-
based system also required that summaries be produced 
on demand, with only a short pause allowed for the 
summariser to output a result ? typically a maximum of 
around 1 second per email.  
Another main constraint imposed in the FASiL VPA 
was the presence of two integer parameters ? the pre-
ferred and maximum length of the summary. The 
maximum length constraint had to be obeyed strictly, 
while striving to fit in the summary into the preferred 
length. These performance and size constraints, coupled 
with the four characteristics of email largely determined 
the design of the FASiL Email Summariser. 
2.1 Short Messages 
Email is a form of short, largely informal, written com-
munication that excludes methods that need large 
amounts of words and phrases to work well.  
The main disadvantage is that sometimes the useful 
content of a whole email message is simply a one word 
in case of a yes/no answer to a question or request. The 
summariser exploits this characteristic by filtering out 
threads and other commonly repeated text at the bottom 
of the email text such as standard email text signatures. 
If the resulting text is very short and falls within the 
preferred length of the summary, the message can be 
output in its entirety to users. The short messages also 
make it easier to achieve relevancy in the summaries. 
Inadvertently context is sometimes lost in the sum-
mary due to replies occurring in threaded emails. Also, 
emails containing lots of question-answer pairs can get 
summarised poorly due to the fixed amount of space 
available for the summary. 
2.2 Stylistic Conventions and Grammar 
Email messages often do not follow formal stylistic 
conventions and are may have a substantial level of 
spelling mistakes, abbreviations and other features that 
make text analysis difficult. 
A simple spellchecker using approximate string 
matching and word frequency/occurrence statistics was 
used to match misspelled names automatically.  
Another problem that was encountered was the iden-
tification of sentence boundaries, since more than 10% 
of the emails seen by the summariser frequently had 
missing punctuation and spurious line breaks inserted 
by various different email programs. A set of hand-
coded heuristics managed to produce acceptable results, 
identifying sentence boundaries correctly more than 
90% of the time. 
2.3 Informal and Formal Styles 
Email can often be classified into three categories: in-
formal short messages ? often sent to people whom are 
directly known or with whom there has been a pro-
longed discussion or interaction about a subject, mixed 
formal/informal emails sent to strangers or when re-
questing information or replying to questions, and for-
mal emails that are generally electronic versions of 
formal letter writing. 
The class of emails that cause most problems for 
summarisation purposes are the first two classes of e-
mails. One of the main determining factors for the style 
adopted by people in replying to emails is the amount of 
time that lapses between replies. Generally email gets 
more formal as the time span between replies increases. 
Informal email can also be recognised by excessive 
use of anaphora that need to be resolved properly before 
summarisation can take place. The summariser thus has 
an anaphora resolver that is capable of resolving ana-
phoric references robustly. 
Linguistic theory indicates that as the formality of a 
text increases, the number of words in the deictic cate-
gory will decrease as the number of words in the non-
deictic category increase (and vice-versa). Deictic (or 
anaphoric) word classes include words that have vari-
able meaning whose meaning needs to be resolved 
through the surrounding (usually preceding) context. 
Non-deictic word classes are those words whose mean-
ing is largely context-independent, analogous to predi-
cates in formal logic.  
2.4 Threaded Emails 
Many emails are composed by replying to an original 
email, often including part or whole of the original 
email together with new content, thus creating a thread 
or chain of emails. The first email in the thread will 
potentially be repeated many times over, which might 
mislead the summarisation process. A thread-detection 
filtering tool is used to eliminate unoriginal content in 
the email by comparing the contents of the current email 
with the content of previous emails. A study of over 57 
user?s incoming and outgoing emails found that around 
30% of all emails are threaded. Around 56% of the 
threaded emails contained only one previous email ? i.e. 
a request and reply, and 87% of all emails contained 
only three previous emails apart from the reply (Fisher 
and Moody, 2001). 
Some reply styles also pose a problem when com-
bined with threads. Emails containing a list of questions 
or requests for comments are often edited by the reply-
ing party and answers inserted directly inside the text of 
the original request, as illustrated in Figure 1. 
 
> ? now coming back to the issue 
> of whether to include support for 
> location names in the recogniser 
> I think that we should include 
> this ? your opinions appreciated. 
I agree with this. 
 
Figure 1 Sample Embedded Answer 
 
Figure 1 illustrates the main two difficulties faced by 
the summariser in this situation. While the threaded 
content from the previous reply should be filtered out to 
identify the reply, the reply on its own is meaningless 
without any form of context. The summariser tries to 
overcome this by identifying this style of embedded 
responses when the original content is split into chunks 
or is only partially included in the reply. The text falling 
before the answer is then treated as part of the reply. 
Although this strategy gives acceptable results in some 
cases, more research is needed into finding the optimal 
strategy to extract the right amount of context from the 
thread without either destroying the context or copying 
too much from the original request back into the sum-
mary. 
3 Summarisation Techniques 
Various summarisation techniques were considered in 
the design of the FASiL email summariser. Few opera-
tional email-specific summarisation systems exist, so 
the emphasis was on extracting the best-of-breed tech-
niques from document summarisation systems that are 
applicable to email summarisation. 
3.1 Previous Work 
Many single-document summarisation systems can be 
split according to whether they are extractive or non-
extractive systems. Extractive systems generate summa-
ries by extracting selected segments from the original 
document that are deemed to be most relevant. Non-
extractive systems try to build an abstract representation 
model and re-generate the summary using this model 
and words found in the original document. 
Previous related work on extractive systems in-
cluded the use of semantic tagging and co-
reference/lexical  chains (Saggion et al, 2003; Barzilay 
and Elhadad, 1997; Azzam et al, 1998), lexical occur-
rence/structural statistics (Mathis et al, 1973), discourse 
structure (Marcu, 1998), cue phrases (Luhn, 1958; 
Paice, 1990; Rau et al, 1994), positional indicators 
(Edmunson, 1964) and other extraction methods (Kui-
pec et al, 1995). 
Non-extractive systems are less common ? previous 
related work included reformulation of extracted models 
(McKeown et al, 1999), gist extraction (Berger and 
Mittal, 2000), machine translation-like approaches 
(Witbrock and Mittal, 1999) and generative models (De 
Jong, 1982; Radev and McKeown, 1998; Fum et al,  
1986; Reihmer and Hahn, 1988; Rau et al,  1989). 
A sentence-extraction system was decided for the 
FASiL summariser, with the capability to have phrase-
level extraction in the future. Non-extractive systems 
were not likely to work as robustly and give the high 
quality results needed by the VPA to work as required. 
Another advantage that extractive systems still pose is 
that in general they are more applicable to a wider range 
of arbitrary domains and are more reliable than non-
extractive systems (Teufel, 2003). 
The FASiL summariser uses named entities as an 
indication of the importance of every sentence, and per-
forms anaphora resolution automatically. Sentences are 
selected according to named entity density and also ac-
cording to their positional ranking. 
3.2 Summariser Architecture 
The FASiL Summariser works in conjunction with a 
number of different components to present real-time 
voice-based summaries to users. Figure 2 shows the 
overall architecture of the summariser and its place in 
the FASiL VPA. 
 
 
Figure 2 Summariser and VPA Architecture 
 
An XML-based protocol is used to communicate 
with the Dialogue Manager enabling the system to be 
loosely coupled but to have high cohesion (Sommer-
ville, 1992). 
3.3 Named Entity Recognition 
One of the most important components in the FASiL 
Summariser is the Named Entity Recogniser (NER) 
system. 
The NER uses a very efficient trie-like structure to 
match sub-parts of every name (Gusfield, 1997; 
Stephen, 1994). An efficient implementation enables the 
NER to confirm or reject a word as being a named en-
tity or not in O(n) time. Named entities are automati-
cally classified according to the following list of 11 
classes: 
? Male proper names (M) 
? Female proper names (F) 
? Places (towns, cities, etc.) (P) 
? Locations (upstairs, boardroom, etc.) (L) 
? Male titles (Mr., Esq., etc.) (Mt) 
? Female titles (Ms., Mrs., etc.) (Ft) 
? Generic titles (t) 
? Date and time references (TIME) 
? Male anaphors (Ma) 
? Female anaphors (Fa) 
? Indeterminate anaphors (a)  
 
The gazetteer list for Locations, Titles, and Ana-
phors were compiled manually. Date and time refer-
ences were compiled from data supplied in the IBM 
International Components for Unicode (ICU) project 
(Davis, 2003). Place names were extracted from data 
available online from the U.S. Geological Survey Geo-
graphic Names Information System and the GEOnet 
Names Server (GNS) of the U.S. National Imagery and 
Mapping Agency (USGS, 2003; NIMA, 2003). 
An innovative approach to gathering names for the 
male and female names was adopted using a small cus-
tom-built information extraction system that crawled 
Internet pages to identify likely proper names in the 
texts. Additional hints were provided by the presence of 
anaphora in the same sentence or the following sentence 
as the suspected proper name. The gender of every title 
and anaphora was manually noted and this information 
was used to keep a count of the number of male or fe-
male titles and anaphors associated with a particular 
name. This information enabled the list of names to be 
organised by gender, enabling a rough probability to be 
assigned to suspect words (Azzam et al, 1998; Mitkov, 
2002).  
An Internet-based method that verified the list and 
filtered out likely spelling mistakes and non-existent 
names was then applied to this list, filtering out incor-
rectly spelt names and other features such as online chat 
nicknames (Dalli, 2004). 
A list of over 592,000 proper names was thus ob-
tained by this method with around 284,000 names being 
identified as male and 308,000 names identified as fe-
male. The large size of this list contributed significantly 
to the NER?s resulting accuracy and compares favoura-
bly with previously compiled lists (Stevenson and Gai-
zauskas, 2000). 
3.4 Anaphora Resolution 
Extracting systems suffer from the problem of dangling 
anaphora in summaries. Anaphora resolution is an effec-
tive way of reducing the incoherence in resulting sum-
maries by replacing anaphors with references to the 
appropriate named entities (Mitkov, 2002). This substi-
tution has the direct effect of making the text less con-
text sensitive and implicitly increases the formality of 
the text. 
Cohesion problems due to semantic discontinuities 
where concepts and agents are not introduced are also 
partially solved by placing emphasis on named entities 
and performing anaphora resolution. The major cohe-
sion problem that still has not been fully addressed is 
the coherence of various events mentioned in the text. 
The anaphora resolver is aided by the gender-
categorised named entity classes, enabling it to perform 
better resolution over a wide variety of names. A simple 
linear model is adopted, where the system focuses 
mainly on nominal and clausal antecedents (Cristea et 
al., 2000). The search scope for candidate antecedents is 
set to the current sentence together with the three pre-
ceding sentences as suggested in (Mitkov, 1998) as em-
pirical studies show that more than 85% of all cases are 
handled correctly with this window size (Mitkov, 2002). 
Candidate antecedents being discarded after ten sen-
tences have been processed without the presence of 
anaphora as suggested in (Kameyama, 1997). 
3.5 Sentence Ranking 
After named entity recognition and anaphora resolution, 
the summariser ranks the various sentences/phrases that 
it identifies and selects the best sentences to extract and 
put in the summary. The summariser takes two parame-
ters apart from the email text itself: a preferred length 
and a maximum length. Typical lengths are 160 charac-
ters preferred with 640 characters maximum, which 
compares to the size a mobile text message. 
Ranking takes into account three parameters: named 
entity density and importance of every class, sentence 
position and the preferred and maximum length parame-
ters. 
0
1
2
3
4
5
6
7
8
1 3 5 7 9 11 13 15 17 19
Number of Sentences
Weight
Series1 Series2 Series3
Series4 Series5
 
Figure 3 Positional sentence weight for varying 
summarisation parameters 
 
Positional importance was found to be significant in 
email text since relevant information was often found to 
be in the first few sentences of the email.  
Figure 3 shows how the quadratic positional weight 
function ? changes with position, giving less importance 
to sentences as they occur further from the start (al-
though the weight is always bigger than zero). Different 
kinds of emails were used to calibrate the weight func-
tion. Series 1 (bottom) represents a typical mobile text 
message length summary with a very long message. 
Series 4 and 5 (middle) represent the weight function 
behaviour when the summary maximum length is long 
(approximately more than 1,000 characters), irrelevant 
of the email message length itself. Series 2 and 3 (top) 
represent email messages that fall within the maximum 
length constraints. 
The following ranking function rank(j), where j is 
the sentence number, is used to rank and select excerpts: 
( ) ( )( ) ( ) ( )( )++?= ?
=
? ????
0
,1
i
c iijjrank  
( )? ?( )???
????
? ??+???
?
???
?
+ ?
???? jlength
j
j
1
max  
 
where ? and ? are empirically determined constants, 
? is the preferred summary length, and jmax is the num-
ber of sentences in the email. The NER function ?c 
represents the number of words of type i in sentence j 
and ?(i) gives the weight associated with that type. In 
our case ? equals 10 since there are 11 named entity 
classes. The NER weights ?(i) for every class have 
been empirically determined and optimized. A third 
parameter ? is used to change the values of ? and ? ac-
cording to the maximum and preferred lengths together 
with the email length as shown in Figure 3. 
The first term handles named entity density, the sec-
ond the sentence position and the third biases the rank-
ing towards the preferred length. The sentences are then 
sorted in rank order and the preferred and maximum 
lengths used to determine which sentences to return in 
the summary. 
4 Experimental Results 
The summariser results quality was evaluated against 
manually produced summaries using precision and re-
call, together with a more useful utility-based evaluation 
that uses a fractional model to cater for varying degrees 
of importance for different sentences. 
4.1 Named Entity Recognition Performance 
The performance of the summariser depends signifi-
cantly on the performance of the NER. Speed tests show 
that the NER consistently processes more than 1 million 
wps on a 1.6 GHz machine while keeping resource us-
age to a manageable 300-400 Mb of memory. 
Precision and recall curves were calculated for 100 
emails chosen at random, separated into 10 random 
sample groups from representative subsets of the three 
main types of emails ? short, normal and long emails as 
explained previously. The samples were manually 
marked according to the 11 different named entity 
classes recognised by the NER to act as a comparative 
standard for relevant results. Figures 4 and 5 respec-
tively show the NER precision and recall results. 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Precision
M F P L
Mt Ft t TIME
Ma Fa a  
Figure 4 Precision by Named Entity Class 
 
It is interesting to note that the NER performed 
worst at anaphora identification with an average preci-
sion of 77.5% for anaphora but 96.7% for the rest of the 
named entity classes. 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Recall
M F P L
Mt Ft t TIME
Ma Fa a
 
Figure 5 Recall by Named Entity Class 
 
Figure 6 shows the average precision and recall av-
eraged across all the eleven types of named entity 
classes, for the 10 sample email groups. An average 
precision of 93% was achieved throughout, with 97% 
recall. 
0
0.25
0.5
0.75
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Value
Recall Precision
 
Figure 6 Average Precision and Recall 
 
It is interesting to note that the precision and recall 
curves do not exhibit the commonly observed inverse 
trade-off relationship between precision and recall 
(Buckland and Gey, 1994; Alvarez, 2002). This result is 
explained by the fact that the NER, in this case, can 
actually identify most named entities in the text with 
high precision while neither over-selecting irrelevant 
results nor under-selecting relevant results. 
4.2 Summariser Results Quality 
Quality evaluation was performed by selecting 150 
emails at random and splitting the emails up into 15 
groups of 10 emails at random to facilitate multiple per-
son evaluation. Each sentence in every email was then 
manually ranked using a scale of 1 to 10. For recall and 
precision calculation, any sentence ranked ? 5 was de-
fined as relevant. Figure 7 shows the precision and re-
call values with 74% average precision and 71% aver-
age recall. 
0
0.5
1
1.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Sample Group
Value
Recall Precision
 
Figure 7 Summaries Recall and Precision 
 
A utility-based evaluation was also used to obtain 
more intuitive results than those given by precision and 
recall using the methods reported in (Jing et al, 1998; 
Goldstein et al, 1999; Radev et al, 2000). The average 
score of each summary was compared to the average 
score over infinity expected to be obtained by extracting 
a combination of the first [1..N] sentences at random. 
The summary average score was also compared to the 
score obtained by an averaged pool of 3 human judges. 
Figure 8 shows a comparison between the summariser 
performance and human performance, with the summar-
iser averaging at 86.5% of the human performance, 
ranging from 60% agreement to 100% agreement with 
the gold standard. 
0
0.5
1
1.5
2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Sample Group
Value
Summariser Utility Gold Standard Utility
 
Figure 8 Utility Score Comparison 
In Figure 8 a random extraction system is expected 
to get a score of 1 averaged across an infinite amount of 
runs. The average sentence compression factor for the 
summariser was 42%, exactly the same as the human 
judges? results. The selected emails had an average 
length of 14 sentences, varying from 7 to 27 sentences. 
5 Conclusion and Future Work 
The FASiL Email Summarisation system represents a 
compact summarisation system optimised for email 
summarisation in a voice-based system context.  
The excellent performance in both speed and accu-
racy of the NER component makes it ideal for re-use in 
projects that need high quality real-time identification 
and classification of named entities. 
A future improvement will incorporate a fast POS 
analyser to enable phrase-level extraction to take place 
while improving syntactic coherence. An additional 
improvement will be the incorporation of co-reference 
chain methods to verify email subject lines and in some 
cases suggest more appropriate subject lines. 
The FASiL summariser validates the suitability of 
the combined sentence position and NER-driven ap-
proach towards email summarisation with encouraging 
results obtained. 
Acknowledgments 
This research is funded under the EU FASiL Project, an 
EU grant in Human Language Technology (IST-2001-
38685) (Website: www.fasil.co.uk). 
References 
Alvarez, S. 2002. ?An exact analytical relation among 
recall, precision, and classification accuracy in in-
formation retrieval.? Boston College, Boston, Tech-
nical Report BCCS-02-01. 
Azzam, S., Humphreys, K. and Gaizauskas, R. 1998. 
?Coreference resolution in a multilingual information 
extraction?, Proc. Workshop on Linguistic Corefer-
ence. Granada, Spain. 
Barzilay, R. Elhadad, M. 1997. ?Using Lexical Chains 
for Text Summarization.?, Proc. ACL Workshop on 
Intelligent Scaleable Text Summarization, Madrid, 
Spain. 10-17. 
Berger, L. Mittal, V. 2000. ?OCELOT: A system for 
summarizing web pages?. Carnegie Mellon Univer-
sity. Just Research. Pittsburgh, Pennsylvania. 
Buckland, M. Gey, F. 1994. ?The relationship between 
recall and precision.? J. American Society for Infor-
mation Science, 45(1):12-19. 
Cristea, D., Ide, N., Marcu, D., Tablan, V. 2000. ?An 
empirical investigation of the relation between dis-
course structure and coreference.?, Proc. 19th Int. 
Conf. on Comp. Linguistics (COLING-2000), Saar-
br?cken, Germany. 208-214. 
Dalli, A. 2004. ?An Internet-based method for Verifica-
tion of Extracted Proper Names?. CICLING-2004. 
David, C. 2003. Information Society Statistics: PCs, 
Internet and mobile phone usage in the EU. Euro-
pean Community, Report KS-NP-03-015-EN-N. 
Davis, M. 2003. ?An ICU overview?. Proc. 24th Unicode 
Conference, Atlanta. IBM Corporation, California. 
De Jong, G. 1982. ?An overview of the FRUMP sys-
tem.?, in: Lehnert and Ringle eds., Strategies for 
Natural Language Processing, Lawrence Erlbaum 
Associates, Hillsdale, New Jersey. 149-176. 
Edmunson, H.P. 1964. ?Problems in automatic extract-
ing.?, Comm. ACM, 7, 259-263. 
Fisher, D., Moody, P. 2001. Studies of Automated Col-
lection of Email Records. University of California, 
Irvine, Technical Report UCI-ISR-02-4. 
Fum, D. Guida, G. Tasso, C. 1986. ?Tailoring impor-
tance evaluation to reader?s goals: a contribution to 
descriptive text summarization.? Proc. COLING-86, 
256-259. 
Goldstein, J. Kantrowitz, M. Mittal, V. Carbonell, 
Jaime. 1999. ?Summarizing Text Documents: Sen-
tence Selection and Evaluation Metrics?, Proc. ACM-
SIGIR 1999, Berkeley, California. 
Gusfield, D.  1997.  Algorithms on Strings, Trees and 
Sequences.  Cambridge University Press, Cambridge, 
UK. 
Halliday, M.A.K. 1985. Spoken and written language. 
Oxford University Press, Oxford. 
Jing, H. Barzilay, R. McKeown, K. Elhadad, M. 1998. 
?Summarization Evaluation Methods: Experiments 
and Analysis?, AAAI Spring Symposium on Intelligent 
Text Summarisation, Stanford, California. 
Kameyama, M. 1997. ?Recognising referential links: an 
information extraction perspective.?, Proc. EACL-97 
Workshop on Operational Factors in Practical, Ro-
bust, Anaphora Resolution, Madrid, Spain. 46-53. 
Kuipec, J. Pedersen, J. Chen, F. 1995. ?A Trainable 
Document Summarizer.?, Proc. 18th ACM SIGIR 
Conference, Seattle, Washington. 68-73. 
Luhn, P.H. 1958. ?Automatic creation of literature ab-
stracts?. IBM J. 159-165. 
Marcu, D. 1998. ?To Build Text Summaries of High 
Quality, Nuclearity is not Sufficient.?  Proc. AAAI 
Symposium on Intelligent Text Summarisation, Stan-
ford University, Stanford, California. 1-8. 
Mathis, B.A. Rush, J.E. Young, C.E. 1973. ?Improve-
ment of automatic abstracts by the use of structural 
analysis.?, J. American Society for Information Sci-
ence, 24, 101-109. 
McKeown, K. Klavens, J. Hatzivassiloglou, V. Barzi-
lay, R. Eskin, E. 1999. ?Towards Multidocument 
Summarization by Reformulation: Progress and 
Prospects.?, AAAI Symposium on Intelligent Text 
Summarisation. 
Mitkov, R. 1998. ?Robust pronoun resolution with lim-
ited knowledge.?, Proc. 17th International Confer-
ence on Comp. Linguistics (COLING-1998), 
Montreal, Canada. 869-875. 
Mitkov, R. 2002. Anaphora Resolution. London, Long-
man. 
National Imagery and Mapping Agency (NIMA). 2003. 
GEOnet Names Server (GNS). 
Paice, C. 1990. ?Constructing literature abstracts by 
computer: techniques and prospects.?, Information 
Processing and Management, 26:171-186. 
Radev, D. McKeown, K. 1998. ?Generating Natural 
Language Summaries from Multiple On-Line 
Sources.?, Computational Linguistics, 24(3):469-500. 
Radev, D. Jing, H. Budzikowska, M. 2000. ?Centroid-
based summarization of multiple documents: sen-
tence extraction, utility-based evaluation, user stud-
ies.? in Automatic Summarisation: ANLP/NAACL 
2000 Workshop, New Brunswick, New Jersey. 
Rau, L. Jacobs, P. Zernick, U. 1989. ?Information ex-
traction and text summarization using linguistic 
knowledge acquisition.?, Information Processing and 
Management, 25(4):419-428. 
Rau, L. Brandow, R. Mitze, K. 1994. ?Domain-
Independent Summarization of News.?, in: Summa-
rizing Text for Intelligent Communication, Dagstuhl, 
Germany. 71-75. 
Reimer, U. Hahn, U. 1988. ?Text condensation as 
knowledge base abstraction.? Proc. 4th Conference on 
Artificial Intelligence Applications. 338-344. 
Saggion, H. Bontcheva, K. Cunningham, H. 2003. ?Ro-
bust Generic and Query-based Summarisation?. Proc. 
EACL-2003, Budapest. 
Sommerville, I. 1992. Software Engineering. 4th ed. 
Addison-Wesley. 
Stephen, Graham A. 1994. String Searching Algorithms. 
World Scientific Publishing, Bangor, Gwynedd, UK. 
Stevenson, M. Gaizauskas, R. 2000. ?Using Corpus-
derived Name Lists for Named Entity Recognition, 
Proc. ANLP-2000, Seattle. 
Teufel, S. 2003. ?Information Retrieval: Automatic 
Summarisation?, University of Cambridge. 24-25. 
Witbrock, M. Mittal, V. 1999. ?Ultra Summarization: A 
Statistical Approach to Generating Non-Extractive 
Summaries.?, Just Research, Pittsburgh. 
United States Geological Survey (USGS). 2003. Geo-
graphic Names Information System (GNIS). 
http://geonames.usgs.gov/ 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 29?32,
New York, June 2006. c?2006 Association for Computational Linguistics
Temporal Classification of Text and Automatic Document Dating 
 
 
 Angelo Dalli 
 
 University of Sheffield  
 211, Portobello Street  
 Sheffield, S1 4DP, UK  
 angelo@dcs.shef.ac.uk  
 
 
 
 
Abstract 
Temporal information is presently under- 
utilised for document and text processing 
purposes. This work presents an unsuper-
vised method of extracting periodicity in-
formation from text, enabling time series 
creation and filtering to be used in the 
creation of sophisticated language models 
that can discern between repetitive trends 
and non-repetitive writing pat-terns. The 
algorithm performs in O(n log n) time for 
input of length n. The temporal language 
model is used to create rules based on 
temporal-word associations inferred from 
the time series. The rules are used to 
automatically guess at likely document 
creation dates, based on the assumption 
that natural languages have unique signa-
tures of changing word distributions over 
time. Experimental results on news items 
spanning a nine year period show that the 
proposed method and algorithms are ac-
curate in discovering periodicity patterns 
and in dating documents automatically 
solely from their content. 
1 Introduction 
Various features have been used to classify and 
predict the characteristics of text and related text 
documents, ranging from simple word count mod-
els to sophisticated clustering and Bayesian models 
that can handle both linear and non-linear classes. 
The general goal of most classification research is 
to assign objects from a pre-defined domain (such 
as words or entire documents) to two or more 
classes/categories. Current and past research has 
largely focused on solving problems like tagging, 
sense disambiguation, sentiment classification, 
author and language identification and topic classi-
fication. We introduce an unsupervised method 
that classifies text and documents according to 
their predicted time of writing/creation. The 
method uses a sophisticated temporal language 
model to predict likely creation dates for a docu-
ment, hence dating it automatically. This short pa-
per presents some background information about 
existing techniques and the implemented system, 
followed by a brief explanation of the classifica-
tion and dating method, and finally concluding 
with results and evaluation performed on the LDC 
GigaWord English Corpus (LDC, 2003). 
2 Background 
Temporal information is presently under-utilised 
for document and text processing purposes. Past 
and ongoing research work has largely focused on 
the identification and tagging of temporal expres-
sions, with the creation of tagging methodologies 
such as TimeML/TIMEX (Gaizauskas and Setzer, 
2002; Pustejovsky et al, 2003; Ferro et al, 2004), 
TDRL (Aramburu and Berlanga, 1998) and associ-
ated evaluations such as the ACE TERN competi-
tion (Sundheim et al 2004). 
Temporal analysis has also been applied in 
Question-Answering systems (Pustejovsky et al, 
2004; Schilder and Habel, 2003; Prager et al, 
2003), email classification (Kiritchenko et al
29
0100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
0
100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
1 164 327 490 653 816 979 1142 1305 1468 1631 1794 1957
 
0
500
1000
1500
2000
2500
3000
3500
4000
1 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081
 
 
Figure 1 Effects of applying the temporal periodical algorithm on time series for "January" (top) and "the" (bottom) 
with original series on the left and the remaining time series component after filtering on the right. Y-axis shows 
frequency count and X-axis shows the day number (time). 
 
2004), aiding the precision of Information Re-
trieval results (Berlanga et al, 2001), document 
summarisation (Mani and Wilson, 2000), time 
stamping of event clauses (Filatova and Hovy, 
2001), temporal ordering of events (Mani et al, 
2003) and temporal reasoning from text (Boguraev 
and Ando, 2005; Moldovan et al, 2005). There is 
also a large body of work on time series analysis 
and temporal logic in Physics, Economics and 
Mathematics, providing important techniques and 
general background information. In particular, this 
work uses techniques adapted from Seasonal Auto-
Regressive Integrated Moving Average models 
(SARIMA). SARIMA models are a class of sea-
sonal, non-stationary temporal models based on the 
ARIMA process (defined as a non-stationary ex-
tension of the stationary ARMA model). Non-
stationary ARIMA processes are defined by: 
( ) ( ) ( ) ttd ZBXBB ?? =?1            (1) 
where d is non-negative integer, and ( )X?  
( )X?  polynomials of degrees p and q respec-
tively. The exact parameters for each process (one 
process per word) are determined automatically by 
the system. A discussion of the general SARIMA 
model is beyond the scope of this paper (details 
can be found in Mathematics & Physics publica-
tions). The NLP application of temporal classifica-
tion and prediction to guess at likely document and 
text creation dates is a novel application that has 
not been considered much before, if at all. 
3 Temporal Periodicity Analysis  
We have created a high-performance system that 
decomposes time series into two parts: a periodic 
component that repeats itself in a predictable man-
ner, and a non-periodic component that is left after 
the periodic component has been filtered out from 
the original time series. Figure 1 shows an example 
of the filtering results on time-series of the words 
?January? and ?the?. The time series are based on 
training documents selected at random from the 
GigaWord English corpus. 10% of all the docu-
ments in the corpus were used as training docu-
ments, with the rest being available for evaluation 
and testing. A total of 395,944 time series spanning 
9 years were calculated from the GigaWord cor-
pus. Figure 2 presents pseudo-code for the time 
series decomposition algorithm: 
30
1. Find min/max/mean and standard devia-
tion of time series 
2. Start with a pre-defined maximum win-
dow size (presently set to 366 days) 
3. While window size bigger than 1 repeat 
steps a. to d. below: 
a. Look at current value in time 
series (starting first value) 
b. Do values at positions current, 
current + window size, current + 
2 x window size, etc. vary by 
less than ? standard deviation? 
c. If yes, mark current 
value/window size pair as being 
possible decomposition match 
d. Look at next value in time se-
ries until the end is reached 
e. Decrease window size by one 
4. Select the minimum number of decompo-
sition matches that cover the entire 
time series using a greedy algorithm 
 
Figure 2 Time Series Decomposition Algorithm 
 
The time series decomposition algorithm was 
applied to the 395,944 time series, taking an aver-
age of 419ms per series. The algorithm runs in O(n 
log n) time for a time series of length n. 
The periodic component of the time series is 
then analysed to extract temporal association rules 
between words and different ?seasons?, including 
Day of Week, Week Number, Month Number, 
Quarter, and Year. The procedure of determining if 
a word, for example, is predominantly peaking on 
a weekly basis, is to apply a sliding window of size 
7 (in the case of weekly periods) and determining 
if the periodic time series always spikes within this 
window. Figure 3 shows the frequency distribution 
of the periodic time series component of the days 
of week names (?Monday?, ?Tuesday?, etc.) Note 
that the frequency counts peak exactly on that par-
ticular day of the week. For example, the word 
?Monday? is automatically associated with Day 1, 
and ?April? associated with Month 4. The creation 
of temporal association rules generalises inferences 
obtained from the periodic data. Each association 
rule has the following information: 
? Word ID 
? Period Type (Week, Month, etc.) 
? Period Number and Score Matrix 
The period number and score matrix represent a 
probability density function that shows the likeli-
hood of a word appearing on a particular period 
number. For example, the score matrix for ?Janu-
ary? will have a high score for period 1 (and period 
type set to Monthly). Figure 4 shows some exam-
ples of extracted association rules. The PDF scores 
are shown in Figure 4 as they are stored internally 
(as multiples of the standard deviation of that time 
series) and are automatically normalised during the 
classification process at runtime. Rule generalisa-
tion is not possible in such a straightforward man-
ner for the non-periodic data. The use of non-
periodic data to optimise the results of the temporal 
classification and automatic dating system is not 
covered in this paper. 
4 Temporal Classification and Dating  
The periodic temporal association rules are utilised 
to automatically guess at the creation date of 
documents automatically. Documents are input 
into the system and the probability density func-
tions for each word are weighted and added up. 
Each PDF is weighted according to the inverse 
document frequency (IDF) of each associated 
word. Periods that obtain high score are then 
ranked for each type of period and two guesses per 
period type are obtained for each document. Ten 
guesses in total are thus obtained for Day of Week, 
Week Number, Month Number, Quarter, and Year 
(5 period types x 2 guesses each). 
 
 
Su M T W Th F S 
0 22660 10540 7557 772 2130 3264 11672 
1 12461 37522 10335 6599 1649 3222 3414 
2 3394 18289 38320 9352 7300 2543 2261 
3 2668 4119 18120 36933 10427 5762 2147 
4 2052 2602 3910 17492 36094 9098 5667 
5 5742 1889 2481 2568 17002 32597 7849 
6 7994 7072 1924 1428 3050 14087 21468 
 
       
Av 8138 11719 11806 10734 11093 10081 7782 
St 7357 12711 12974 12933 12308 10746 6930 
 
Figure 3 Days of Week Temporal Frequency Distribu-
tion for extracted Periodic Component 
displayed in a Weekly Period Type format 
 
January 
Week 1 2 3 4 5 
Score 1.48 2.20 3.60 3.43 3.52 
Month 1 Score 2.95 
Quarter 1 Score 1.50 
 
Christmas 
Week 2 5 36 42 44 
Score 1.32 0.73 1.60 0.83 1.32 
31
Week 47 49 50 51 52 
Score 1.32 2.20 2.52 2.13 1.16 
 
Month 1 9 10 11 12 
Score 1.10 0.75 1.63 1.73 1.98 
Quarter 4 Score 1.07 
 
Figure 4 Temporal Classification Rules for Periodic 
Components of "January" and "Christmas" 
5 Evaluation, Results and Conclusion 
The system was trained using 67,000 news items 
selected randomly from the GigaWord corpus. The 
evaluation took place on 678,924 news items ex-
tracted from items marked as being of type ?story? 
or ?multi?. Table 1 presents a summary of results. 
Processing took around 2.33ms per item. 
 
Type Correct Incorrect Avg. 
Error 
DOW 218,899 
(32.24%) 
 460,025 
(67.75%) 
1.89 
days 
Week 24,660 
(3.53%) 
654,264 
(96.36%) 
14.37 
wks 
Month 122,777 
(18.08%) 
556,147 
(81.91%) 
2.57 
mths 
Quarter 337,384 
(49.69%) 
341,540 
(50.30%) 
1.48 
qts 
Year 596,009  
(87.78%) 
82,915 
(12.21%)  
1.74 
yrs 
Combined 422,358 
(62.21%) 
256,566 
(37.79%) 
210 
days 
 
Table 1 Evaluation Results Summary 
 
The actual date was extracted from each news item 
in the GigaWord corpus and the day of week 
(DOW), week number and quarter calculated from 
the actual date. Average errors for each type of 
classifier were calculated automatically. For results 
to be considered correct, the system had to have 
the predicted value ranked in the first position 
equal to the actual value (of the type of period). 
The system results show that reasonable accurate 
dates can be guessed at the quarterly and yearly 
levels. The weekly classifier had the worst per-
formance of all classifiers. The combined classifier 
uses a simple weighted formula to guess the final 
document date using input from all classifiers. The 
weights for the combined classifier have been set 
on the basis of this evaluation. The temporal classi-
fication and analysis system presented in this paper 
can handle any Indo-European language in its pre-
sent form. Further work is being carried out to ex-
tend the system to Chinese and Arabic. Current 
research is aiming at improving the accuracy of the 
classifier by using the non-periodic components 
and improving the combined classification method. 
References  
Aramburu, M. Berlanga, R. 1998. A Retrieval Language 
for Historical Documents. LNCS, 1460, pp. 216-225. 
Berlanga, R. Perez, J. Aramburu, M. Llido, D. 2001. 
Techniques and Tools for the Temporal Analysis of 
Retrieved Information. LNCS, 2113, pp. 72-81. 
Boguraev, B. Ando, R.K. 2005. TimeML-Compliant 
Text Analysis for Temporal Reasoning. IJCAI-2005. 
Ferro, L. Gerber, L. Mani, I. Sundheim, B. Wilson, G. 
2004. TIDES Standard for the Annotation of Tempo-
ral Expressions. The MITRE Corporation. 
Filatova, E. Hovy, E. 2001. Assigning time-stamps to 
event-clauses. Proc. EACL 2001, Toulouse, France. 
Gaizauskas, R. Setzer, A. 2002. Annotation Standards 
for Temporal Information in NL. Proc. LREC 2002. 
Kiritchenko, S. Matwin, S. Abu-Hakima, S. 2004. Email 
Classification with Temporal Features. Proc. IIPWM 
2004, Zakopane, Poland. pp. 523-534. 
Linguistic Data Consortium (LDC). 2003. English Gi-
gaword Corpus. David Graff, ed. LDC2003T05. 
Mani, I. Wilson, G. 2000. Robust temporal processing 
of news. Proc. ACL 2000, Hong Kong. 
Mani, I. Schiffman, B. Zhang, J. 2003. Inferring tempo-
ral ordering of events in news. HLT-NAACL 2003. 
Moldovan, D. Clark, C. Harabagiu, S. 2005. Temporal 
Context Representation and Reasoning. IJCAI-2005. 
Prager, J. Chu-Carroll, J. Brown, E. Czuba, C. 2003. 
Question Answering using predictive annotation. 
Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gai-
zauskas, R. Setzer, A. Katz, G. 2003. TimeML: Ro-
bust Specification of event and temporal expressions 
in text. IWCS-5. 
Pustejovsky, J. Sauri, R. Castano, J. Radev, D. Gai-
zauskas, R. Setzer, A. Sundheim, B. Katz, G. 2004. 
?Representing Temporal and Event Knowledge for 
QA Systems?. New Directions in QA, MIT Press. 
Schilder, F. Habel, C. 2003. Temporal Information Ex-
traction for Temporal QA. AAAI NDQA, pp. 35-44. 
Sundheim, B. Gerber, L. Ferro, L. Mani, I. Wilson, G. 
2004. Time Expression Recognition and Normaliza-
tion (TERN). http://timex2.mitre.org. 
32
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 17?22,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Dating of Documents and Temporal Text Classification 
Angelo Dalli 
NLP Research Group 
University of Sheffield 
United Kingdom 
angelo@dcs.shef.ac.uk 
Yorick Wilks 
NLP Research Group 
University of Sheffield 
United Kingdom 
yorick@dcs.shef.ac.uk 
 
  
 
Abstract 
The frequency of occurrence of words in 
natural languages exhibits a periodic and 
a non-periodic component when analysed 
as a time series. This work presents an 
unsupervised method of extracting perio-
dicity information from text, enabling 
time series creation and filtering to be 
used in the creation of sophisticated lan-
guage models that can discern between 
repetitive trends and non-repetitive writ-
ing patterns. The algorithm performs in 
O(n log n) time for input of length n. The 
temporal language model is used to cre-
ate rules based on temporal-word asso-
ciations inferred from the time series. 
The rules are used to guess automatically 
at likely document creation dates, based 
on the assumption that natural languages 
have unique signatures of changing word 
distributions over time. Experimental re-
sults on news items spanning a nine year 
period show that the proposed method 
and algorithms are accurate in discover-
ing periodicity patterns and in dating 
documents automatically solely from 
their content. 
1 Introduction 
Various features have been used to classify 
and predict the characteristics of text and related 
text documents, ranging from simple word count 
models to sophisticated clustering and Bayesian 
models that can handle both linear and non-linear 
classes. The general goal of most classification 
research is to assign objects from a pre-defined 
domain (such as words or entire documents) to 
two or more classes/categories. Current and past 
research has largely focused on solving problems 
like tagging, sense disambiguation, sentiment 
classification, author and language identification 
and topic classification. In this paper, we intro-
duce an unsupervised method that classifies text 
and documents according to their predicted time 
of writing/creation. The method uses a sophisti-
cated temporal language model to predict likely 
creation dates for a document, hence dating it 
automatically. 
This paper presents the main assumption be-
hind this work together some background infor-
mation about existing techniques and the imple-
mented system, followed by a brief explanation 
of the classification and dating method, and fi-
nally concluding with results and evaluation per-
formed on the LDC GigaWord English Corpus 
(LDC, 2003) together with its implications and 
relevance to temporal-analytical frameworks and 
TimeML applications. 
2 Background and Assumptions 
The main assumption behind this work is that 
natural language exhibits a unique signature of 
varying word frequencies over time. New words 
come into popular use continually, while other 
words fall into disuse either after a brief fad or 
when they become obsolete or archaic. Current 
events, popular issues and topics also affect writ-
ers in their choice of words and so does the time 
period when they create documents. This as-
sumption is implicitly made when people try to 
guess at the creation date of a document ? we 
would expect a document written in Shake-
speare?s time to contain higher frequency counts 
of words and phrases such as ?thou art?, ?be-
twixt?, ?fain?, ?methinks?, ?vouchsafe? and so 
on than would a modern 21st century document. 
Similarly, a document that contains a high fre-
quency of occurrence of the words ?terrorism?, 
?Al Qaeda?, ?World Trade Center?, and so on is 
more likely to be written after 11 September 
2001. New words can also be used to create ab-
solute constraints on the creation dates of docu-
ments, for example, it is highly improbable that a 
17
document containing the word ?blog? was writ-
ten before July 1999 (it was first used in a news-
group in July 1999 as an abbreviation for ?we-
blog?), or a document containing the word 
?Google? to have been written before 1997. 
Words that are now in common use can also be 
used to impose constraints on the creation date; 
for example, the word ?bedazzled? has been at-
tributed to Shakespeare, thus allowing docu-
ments from his time onwards to be identifiable 
automatically. Traditional dictionaries often try 
to record the date of appearance of new words in 
the language and there are various Internet sites, 
such as WordSpy.com, devoted to chronicling 
the appearance of new words and their meanings. 
Our system is building up a knowledge base of 
the first occurrences of various words in different 
languages, enabling more accurate constraints to 
be imposed on the likely document creation date 
automatically. 
Commercial trademarks and company names 
are also useful in dating documents, as their reg-
istration date is usually available in public regis-
tries. Temporal information extracted from the 
documents itself is also useful in dating the docu-
ments ? for example, if a document contains 
many references to the year 2006, it is quite 
likely that the document was written in 2006 (or 
in the last few weeks of December 2005). 
These notions have been used implicitly by re-
searchers and historians when validating the au-
thenticity of documents, but have not been util-
ised much in automated systems. Similar appli-
cations have so far been largely confined to au-
thorship identification, such as (Mosteller and 
Wallace, 1964; Fung, 2003) and the identifica-
tion of association rules (Yarowsky, 1994; 
Silverstein et al, 1997). 
Temporal information is presently under-
utilised for automated document classification 
purposes, especially when it comes to guessing at 
the document creation date automatically. This 
work presents a method of using periodical tem-
poral-frequency information present in docu-
ments to create temporal-association rules that 
can be used for automatic document dating. 
Past and ongoing related research work has 
largely focused on the identification and tagging 
of temporal expressions, with the creation of tag-
ging methodologies such as TimeML/TIMEX 
(Gaizauskas and Setzer, 2002; Pustejovsky et al, 
2003; Ferro et al, 2004), TDRL (Aramburu and 
Berlanga, 1998) and their associated evaluations 
such as the ACE TERN competition (Sundheim 
et al 2004). 
Temporal analysis has also been applied in 
Question-Answering systems (Pustejovsky et al, 
2004; Schilder and Habel, 2003; Prager et al, 
2003), email classification (Kiritchenko et al, 
2004), aiding the precision of Information Re-
trieval results (Berlanga et al, 2001), document 
summarisation (Mani and Wilson, 2000), time 
stamping of event clauses (Filatova and Hovy, 
2001), temporal ordering of events (Mani et al, 
2003) and temporal reasoning from text (Bogu-
raev and Ando, 2005; Moldovan et al, 2005). 
A growing body of related work related to the 
computational treatment of time in language has 
also been building up largely since 2000 (COL-
ING 2000; ACL 2001; LREC 2002; TERQAS 
2002; TANGO 2003, Dagstuhl 2005). 
There is also a large body of work on time se-
ries analysis and temporal logic in Physics, Eco-
nomics and Mathematics, providing important 
techniques and general background information. 
In particular, this work uses techniques adapted 
from Seasonal ARIMA (auto-regressive inte-
grated moving average) models (SARIMA). 
SARIMA models are a class of seasonal, non-
stationary temporal models based on the ARIMA 
process. The ARIMA process is further defined 
as a non-stationary extension of the stationary 
ARMA model. The ARMA model is one of the 
most widely used models when analyzing time 
series, especially in Physics, and incorporate 
both auto-regressive terms and moving average 
terms (Box and Jenkins, 1976). Non-stationary 
ARIMA processes are defined by the following 
equation: 
( ) ( ) ( ) ttd ZBXBB ?? =?1            (1) 
where d is non-negative integer, and ( )X?  
( )X?  polynomials of degrees p and q respec-
tively. The SARIMA extension adds seasonal 
AR and MA polynomials that can handle season-
ally varying data in time series. 
The exact formulation of the SARIMA model 
is beyond the scope of this paper and can be 
found in various mathematics and physics publi-
cations, such as (Chatfield, 2003; Brockwell et 
al., 1991; Janacek, 2001). 
The main drawback of SARIMA modelling 
(and associated models built on the basic ARMA 
model) is that it requires fairly long time series 
before accurate results are obtained. The major-
ity of authors recommend that a time series of at 
least 50 data points is used to build the SARIMA 
model. 
18
0100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
0
100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
 
Time Series for ?January? 
Original (Top Left), Non-Periodic Component (Top 
Right), Periodic Component (Bottom Right) 
0
100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
  
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
1 164 327 490 653 816 979 1142 1305 1468 1631 1794 1957
 
0
500
1000
1500
2000
2500
3000
3500
4000
1 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081
 
 
Time Series for ?The? 
Original (Top Left), Non-Periodic Component (Top 
Right), Periodic Component (Bottom Right) 
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
1 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081
 
 
Figure 1: Effects of applying the temporal periodical algorithm on time series for "January" (top three 
graphs) and "the" (bottom three graphs) with the original series on the left and the remaining time series 
components after filtering on the right. Y-axis shows frequency count and X-axis shows the day number 
(time). 
 
3 Temporal Periodicity Analysis 
We have created a high-performance system that 
decomposes time series into two parts: a periodic 
component that repeats itself in a predictable 
manner, and a non-periodic component that is 
left after the periodic component has been fil-
tered out from the original time series. Figure 1 
shows an example of the filtering results on time-
series of the words ?January? and ?the?. The 
original series is presented together with two se-
ries representing the periodic and non-periodic 
19
components of the original time series. The time 
series are based on training documents selected 
at random from the GigaWord English corpus. 
10% of all the documents in the corpus were 
used as training documents, with the rest being 
available for evaluation and testing. A total of 
395,944 time series spanning 9 years were calcu-
lated from the GigaWord corpus. The availability 
of 9 years of data also mitigated the negative ef-
fects of using short time series in combination 
with SARIMA models (as up to 3,287 data 
points were available for some words, well above 
the 50 data point minimum recommendation). 
Figure 2 presents pseudo-code for the time series 
decomposition algorithm: 
 
1. Find min/max/mean and standard devia-
tion of time series 
2. Start with a pre-defined maximum win-
dow size (set to 366 days in our pre-
sent system) 
3. While window size bigger than 1 re-
peat steps a. to d. below: 
a. Look at current value in time 
series (starting from first 
value) 
b. Do values at positions cur-
rent, current + window size, 
current + 2 x window size, 
etc. vary by less than half a 
standard deviation? 
c. If yes, mark current 
value/window size pair as be-
ing possible decomposition 
match 
d. Look at next value in time se-
ries until the end is reached 
e. Decrease window size by one 
4. Select the minimum number of decompo-
sition matches that cover the entire 
time series using a greedy algorithm 
 
Figure 2: Time Series Decomposition Algorithm 
 
The time series decomposition algorithm was 
applied to the 395,944 time series, taking an av-
erage of 419ms per series. The algorithm runs in 
O(n log n) time for a time series of length n. 
The periodic component of the time series is 
then analysed to extract temporal association 
rules between words and different ?seasons?, 
including Day of Week, Week Number, Month 
Number, Quarter, and Year. The procedure of 
determining if a word, for example, is predomi-
nantly peaking on a weekly basis, is to apply a 
sliding window of size 7 (in the case of weekly 
periods) and determining if the periodic time se-
ries always spikes within this window. Figure 3 
shows the frequency distribution of the periodic 
time series component of the days of week 
names (?Monday?, ?Tuesday?, etc.) Note that the 
frequency counts peak exactly on that particular 
day of the week. Thus, for example, the word 
?Monday? is automatically associated with Day 
1, and ?April? associated with Month 4. 
The creation of temporal association rules 
generalises the inferences obtained from the pe-
riodic data. Each association rule has the follow-
ing information: 
 
? Word ID 
? Period Type (Week, Month, etc.) 
? Period Number and Score Matrix 
 
The period number and score matrix represent 
a probability density function that shows the 
likelihood of a word appearing on a particular 
period number. Thus, for example, the score ma-
trix for ?January? will have a high score for pe-
riod 1 (and period type set to Monthly). Figure 4 
shows some examples of extracted association 
rules. The probability density function (PDF) 
scores are shown in Figure 4 as they are stored 
internally (as multiples of the standard deviation 
of that time series) and are automatically normal-
ised during the classification process at runtime. 
The standard deviation of values in the time se-
ries is used instead of absolute values in order to 
reduce the variance between fluctuations in dif-
ferent time series for words that occur frequently 
(like pronouns) and those that appear relatively 
less frequently. 
Rule generalisation is not possible in such a 
straightforward manner for the non-periodic data. 
In this paper, the use of non-periodic data to op-
timise the results of the temporal classification 
and automatic dating system is not covered. Non-
periodic data may be used to generate specific 
rules that are associated only with particular 
dates or date ranges. Non-periodic data can also 
use information obtained from hapax words and 
other low-frequency words to generate additional 
refinement rules. However, there is a danger that 
relying on rules extracted from non-periodic data 
will simply reflect the specific characteristics of 
the corpus used to train the system, rather than 
the language in general. Ongoing research is be-
ing performed into calculating relevance levels 
for rules extracted from non-periodic data. 
4 Temporal Classification and Auto-
matic Dating 
The periodic temporal association rules are util-
ised to guess automatically the creation date of 
20
documents. Documents are input into the system 
and the probability density functions for each 
word are weighted and added up. Each PDF is 
weighted according to the inverse document fre-
quency (idf) of each associated word. Periods 
that obtain high score are then ranked for each 
type of period and two guesses per period type 
are obtained for each document. Ten guesses in 
total are thus obtained for Day of Week, Week 
Number, Month Number, Quarter, and Year (5 
period types x 2 guesses each). 
 
 
Su M T W Th F S 
0 22660 10540 7557 772 2130 3264 11672 
1 12461 37522 10335 6599 1649 3222 3414 
2 3394 18289 38320 9352 7300 2543 2261 
3 2668 4119 18120 36933 10427 5762 2147 
4 2052 2602 3910 17492 36094 9098 5667 
5 5742 1889 2481 2568 17002 32597 7849 
6 7994 7072 1924 1428 3050 14087 21468 
 
       
Av 8138 11719 11806 10734 11093 10081 7782 
St 7357 12711 12974 12933 12308 10746 6930 
 
Figure 3: Days of Week Temporal Frequency Dis-
tribution for extracted Periodic Component 
displayed in a Weekly Period Type format 
 
January 
Week 1 2 3 4 5 
Score 1.48 2.20 3.60 3.43 3.52 
 
Month 1 Score 2.95 
Quarter 1 Score 1.50 
 
Christmas 
Week 2 5 36 42 44 
Score 1.32 0.73 1.60 0.83 1.32 
Week 47 49 50 51 52 
Score 1.32 2.20 2.52 2.13 1.16 
 
Month 1 9 10 11 12 
Score 1.10 0.75 1.63 1.73 1.98 
 
Quarter 4 Score 1.07 
 
Figure 4: Temporal Classification Rules for Peri-
odic Components of "January" and "Christmas" 
4.1 TimeML Output 
The system can output TimeML compliant 
markup tags using TIMEX that can be used by 
other TimeML compliant applications especially 
during temporal normalization processes. If the 
base anchor reference date for a document is un-
known, and a document contains relative tempo-
ral references exclusively, our system output can 
provide a baseline date that can be used to nor-
malize all the relative dates mentioned in the 
document. The system has been integrated with a 
fine-grained temporal analysis system based on 
TimeML, with promising results, especially 
when processing documents obtained from the 
Internet. 
5 Evaluation, Results and Conclusion 
The system was trained using 67,000 news items 
selected at random from the GigaWord corpus. 
The evaluation took place on 678,924 news items 
extracted from items marked as being of type 
?story? or ?multi? in the GigaWord corpus. Ta-
ble 1 presents a summary of the evaluation re-
sults. Processing took around 2.33ms per item. 
The actual date was extracted from each news 
item in the GigaWord corpus and the day of 
week (DOW), week number and quarter calcu-
lated from the actual date. 
This information was then used to evaluate the 
system performance automatically. The average 
error for each type of classifier was also calcu-
lated automatically. For a result to be considered 
as correct, the system had to have the predicted 
value ranked in the first position equal to the ac-
tual value (of the type of period). 
 
Type Correct Incorrect Avg. 
Error 
DOW 218,899 
(32.24%) 
 460,025 
(67.75%) 
1.89 
days 
Week 24,660 
(3.53%) 
654,264 
(96.36%) 
14.37 
wks 
Month 122,777 
(18.08%) 
556,147 
(81.91%) 
2.57 
mths 
Quarter 337,384 
(49.69%) 
341,540 
(50.30%) 
1.48 
qts 
Year 596,009  
(87.78%) 
82,915 
(12.21%)  
1.74 
yrs 
 
Table 1: Evaluation Results Summary 
 
The system results show that reasonable accurate 
dates can be guessed at the quarterly and yearly 
levels. The weekly classifier had the worst per-
formance of all classifiers, likely as a result of 
weak association between periodical word fre-
quencies and week numbers. Logical/sanity 
checks can be performed on ambiguous results. 
For example, consider a document written on 4 
January 2006 and that the periodical classifiers 
give the following results for this particular 
document: 
? DOW = Wednesday 
? Week = 52 
? Month = January 
21
? Quarter = 1 
? Year = 2006 
These results are typical of the system, as par-
ticular classifiers sometimes get the period incor-
rect. In this example, the weekly classifier incor-
rectly classified the document as pertaining to 
week 52 (at the end of the year) instead of the 
beginning of the year. The system will use the 
facts that the monthly and quarterly classifiers 
agree together with the fact that week 1 follows 
week 52 if seen as a continuous cycle of weeks 
to correctly classify the document as being cre-
ated on a Wednesday in January 2006. 
The capability to automatically date texts and 
documents solely from its contents (without any 
additional external clues or hints) is undoubtedly 
useful in various contexts, such as the forensic 
analysis of undated instant messages or emails 
(where the Day of Week classifier can be used to 
create partial orderings), and in authorship iden-
tification studies (where the Year classifier can 
be used to check that the text pertains to an ac-
ceptable range of years). 
The temporal classification and analysis sys-
tem presented in this paper can handle any Indo-
European language in its present form. Further 
work is being carried out to extend the system to 
Chinese and Arabic. Evaluations will be carried 
out on the GigaWord Chinese and GigaWord 
Arabic corpora for consistency. Current research 
is aiming at improving the accuracy of the classi-
fier by using the non-periodic components and 
integrating a combined classification method 
with other systems.  
References 
Aramburu, M. Berlanga, R. 1998. A Retrieval Lan-
guage for Historical Documents. Springer Verlag 
LNCS, 1460, pp. 216-225. 
Berlanga, R. Perez, J. Aramburu, M. Llido, D. 2001. 
Techniques and Tools for the Temporal Analysis of 
Retrieved Information. Springer Verlag LNCS, 
2113, pp. 72-81. 
Boguraev, B. Ando, R.K. 2005. TimeML-Compliant 
Text Analysis for Temporal Reasoning. IJCAI-
2005, pp. 997-1003. 
Box, G. Jenkins, G. 1976. Time Series Analysis:  
Forecasting and Control, Holden-Day. 
Brockwell, P.J. Fienberg, S. Davis, R. 1991. Time 
Series: Theory and Methods. Springer-Verlag. 
Chatfield, C. 2003. The Analysis of Time Series. CRC 
Press. 
Ferro, L. Gerber, L. Mani, I. Sundheim, B. Wilson, G. 
2004. TIDES Standard for the Annotation of Tem-
poral Expressions. The MITRE Corporation. 
Filatova, E. Hovy, E. 2001. Assigning time-stamps to 
event-clauses. Proc. EACL 2001, Toulouse. 
Fung, G. 2003. The Disputed Federalist Papers: SVM 
Feature Selection via Concave Minimization. New 
York City, ACM Press. 
Gaizauskas, R. Setzer, A. 2002. Annotation Standards 
for Temporal Information in NL. LREC 2002. 
Janacek, G. 2001. Practical Time Series. Oxford U.P. 
Kiritchenko, S. Matwin, S. Abu-Hakima, S. 2004. 
Email Classification with Temporal Features. 
Proc. IIPWM 2004, Zakopane, Poland. Springer 
Verlag Advances in Soft Computing, pp. 523-534. 
Linguistic Data Consortium (LDC). 2003. English 
Gigaword Corpus. David Graff, ed. LDC2003T05. 
Mani, I. Wilson, G. 2000. Robust temporal processing 
of news. Proc. ACL 2000, Hong Kong. 
Mani, I. Schiffman, B. Zhang, J. 2003. Inferring tem-
poral ordering of events in news. Proc. HLT-
NAACL 2003, Edmonton, Canada. 
Moldovan, D. Clark, C. Harabagiu, S. 2005. Tempo-
ral Context Representation and Reasoning. IJCAI-
2005, pp. 1099-1104. 
Mosteller, F. Wallace, D. 1964. Inference and Dis-
puted Authorship: Federalist. Addison-Wesley. 
Prager, J. Chu-Carroll, J. Brown, E. Czuba, C. 2003. 
Question Answering using predictive annotation. 
In Advances in Question Answering, Hong Kong. 
Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gai-
zauskas, R. Setzer, A. Katz, G. 2003. TimeML: 
Robust Specification of event and temporal expres-
sions in text. IWCS-5. 
Pustejovsky, J. Sauri, R. Castano, J. Radev, D. Gai-
zauskas, R. Setzer, A. Sundheim, B. Katz, G. 2004. 
?Representing Temporal and Event Knowledge for 
QA Systems?. New Directions in QA, MIT Press. 
Schilder, F. Habel, C. 2003. Temporal Information 
Extraction for Temporal QA. AAAI Spring Symp., 
Stanford, CA. pp. 35-44. 
Silverstein, C. Brin, S. Motwani, R. 1997. Beyond 
Market Baskets: Generalizing Association Rules to 
Dependence Rules. Data Mining and Knowledge 
Discovery. 
Sundheim, B. Gerber, L. Ferro, L. Mani, I. Wilson, G. 
2004. Time Expression Recognition and Normali-
zation (TERN). MITRE, Northrop Grumman, 
SPAWAR. http://timex2.mitre.org. 
Yarowsky, D. 1994. Decision Lists For Lexical Am-
biguity Resolution: Application to Accent Restora-
tion in Spanish and French. ACL 1994. 
22
Adaptation of the F-measure to Cluster Based Lexicon Quality 
Evaluation 
 
 
Angelo Dalli 
NLP Research Group 
Department of Computer Science 
University of Sheffield 
a.dalli@dcs.shef.ac.uk 
 
 
Abstract 
An external lexicon quality measure 
called the L-measure is derived from the 
F-measure (Rijsbergen, 1979; Larsen and 
Aone, 1999). The typically small sample 
sizes available for minority languages and 
the evaluation of Semitic language lexi-
cons are two main factors considered. 
Large-scale evaluation results for the 
Maltilex Corpus are presented (Rosner et 
al., 1999). 
1 Introduction 
Computational Lexicons form a fundamental 
component of any NLP system. Unfortunately, 
good quality lexicons are hard to create and 
maintain. The labour intensive process of lexicon 
creation is further compounded when minority 
languages are concerned. Inevitably, computa-
tional lexicons for minor languages tend to be 
quite small when compared to computational 
lexicons available for more common languages 
such as English. 
The Maltilex Corpus is used in this paper to 
evaluate a cluster based lexicon quality measure 
adapted from the F-measure. The Maltilex Cor-
pus is the first large-scale computational lexicon 
for Maltese (Rosner et al, 1999). The choice of 
Maltese as the evaluation language presented 
some additional problems due to the Semitic 
morphology and grammar of Maltese (Mifsud, 
1995). An innovative approach to lexicon crea-
tion using an automated technique called the 
Lexicon Structuring Technique (LST) was used 
to create an initial computational lexicon from a 
wordlist (Dalli, 2002a). LST decreased the 
amount of work that is normally required to cre-
ate a lexicon from scratch by adapting a number 
of clustering, alignment, and approximate match-
ing techniques to produce a set of clusters con-
taining related wordforms. Lexicon clusters are 
thus analogous to lemmas in more traditional 
lexicons. 
This approach has many advantages for a lan-
guage having a Semitic morphology and gram-
mar due to the large number of wordforms that 
can be derived for a single lemma. Instead of 
processing every wordform individually, the 
whole cluster can be treated as a single entity, 
reducing processing requirements significantly. 
The close relationship of this lexicon defini-
tion and standard clustering systems (with lem-
mas corresponding to clusters), enabled the re-
use of cluster quality evaluation measures to the 
task of lexicon quality evaluation. There are two 
main ways of evaluating cluster quality which are 
summarised in (Steinbach et al, 1999 pg. 6) as 
follows: 
 
? Internal Quality Measure ? Clusters are 
compared without reference to external 
knowledge against some predefined set of 
desirable qualities. 
? External Quality Measure ? Clusters are 
compared to known external classes. 
Internal quality measures are not always desir-
able, since their very existence implies that better 
quality can be achieved by applying an internal 
quality measure in conjunction with some opti-
misation technique. An internal quality measure 
for cluster-based lexicons was not available ei-
ther. 
The two main external quality measures appli-
cable lexicon quality evaluation tasks are entropy 
(Shannon, 1948) and the F-measure (van 
Rijsbergen, 1979; Larsen and Aone, 1999). 
Entropy based quality measures assert that the 
best entropy that can be obtained is when each 
cluster contains the optimal number of members. 
In our context this corresponds to having clusters 
(corresponding to lemmas) that contain exactly 
all the wordforms associated with that cluster. 
The class distribution of the data is calculated by 
considering the probability of every member be-
longing to some class. The entropy of every clus-
ter j is calculated using the standard entropy 
formula 
( )
( )
E j p p
ij ij
i
= ?
?
log  where p
ij
 de-
notes the probability that a member of cluster j 
belongs to class i. The total entropy is then calcu-
lated as  
( )
E
n
n E j
j
j
m
*
= ?
=
?
1
1
 where n
j
 is the 
size of cluster j, m the number of clusters, and n 
the total number of data points. 
The F-measure treats every cluster as a query 
and every class as the desired result set for a 
query. The recall and precision values for each 
given class are then calculated using information 
retrieval concepts. The F-measure of cluster j and 
class i is given by 
( )
( ) ( )
( ) ( )
F i j
r i j p i j
r i j p i j
,
, ,
, ,
=
? ?
+
2
 
where r denotes recall and p the precision. Recall 
is defined as  ( )r i j
n
n
ij
i
, =  and precision is de-
fined as  
( )
p i j
n
n
ij
j
, =
 where n
ij
 is the number of 
class i members in cluster j, while n
j
 and n
i
 are 
the sizes of cluster j and class i respectively. The 
overall F-measure for the entire data set of size n 
is given by 
( )
[ ]
F
n
n
F i j
i
i
*
max ,=
?
. 
2 Lexicon Quality Measure 
Computational lexicons have an additional do-
main-specific external quality measure available 
in the form of existing non-computational lan-
guage dictionaries. Dictionaries can be used to 
compare the results generated by the automated 
system against those produced by human experts. 
Generally it can be assumed that reputable 
printed dictionaries are of a very high quality and 
thus provide a gold standard for comparison. For 
some languages, especially minority languages, 
the only available quality data would be in 
printed dictionary form. Unfortunately most non-
computational dictionaries are not amenable to 
automated analysis techniques since the process 
of re-inputting and re-structuring data into a 
computational dictionary format is generally so 
labour intensive that it becomes too expensive. 
Additionally, since every cluster and class 
correspond to a lemma, the number of classes to 
be considered is expected to number in the thou-
sands. This would make a straightforward appli-
cation of the F-measure an overly long process. 
A modified statistical sampling technique based 
on the F-measure that gives results that are ap-
proximately as good as the full application of the 
F-measure and that caters for the particular nu-
ances of lexicon quality evaluation is thus 
needed. 
The L-measure is such a new measure based 
on the F-measure that attempts to measure the 
quality of a given lexicon in relation to other ex-
isting lexicons that are possibly non-
computational lexicons (i.e. human compiled 
language dictionaries), taking into consideration 
that a full population analysis may not be practi-
cal under most circumstances. 
 
2.1 Lexicon Extraction from Dictionaries 
The L-Measure works by comparing two lexi-
cons, one derived from a gold standard represen-
tation in the form of human compiled dictionaries 
and the other being a computational lexicon 
whose quality is being assessed. In order to avoid 
confusion, formal definitions of the terms dic-
tionary, lexicon and wordlists are now presented. 
A dictionary D is formally modeled as a se-
quence <t
1
 .. t
h
> of tuples of the form (l, def) 
where l denotes a lemma (i.e. a dictionary head-
word in a more traditional sense) and def is a 5-
tuple (m, r, c, i, o) with m containing morpho-
logical information that enables members of the 
lemma to be inferred or generated, r a set of rela-
tions to other lemmas, c a description of the dif-
ferent contexts where the lemma may be 
normally used, i containing meta-information 
about lemma l itself, and o an object containing 
additional information (such as etymology, ex-
amples of common use, etc.) Since multiple en-
tries of the same headword may be present in D 
the sequence is not injective, i.e. the sequence 
can contain duplicate elements. 
The main two differences between a dictionary 
and a lexicon are that different types of informa-
tion are stored about every lemma in the def 
component, and secondly, that a lexicon has an 
injective sequence of tuples (i.e. a sequence that 
does not have duplicates and where the exact or-
der is important) while a dictionary does not 
(since a dictionary does not need to force a 
headword to have one unique entry, especially in 
the case of printed dictionaries that often have the 
same headword appearing in multiple top-level 
entries). 
A dictionary D can be thus transformed into a 
lexicon L, denoted by L = lex(D), by filtering the 
tuple sequence <t
1
 .. t
h
> making up D to include 
only the l components of every tuple. The filtered 
sequence is then transformed into an injective 
sequence of unique lemmas <l
1
 .. l
u
>, satisfying 
the requirements for a lexicon. Appropriate trans-
formations have to be defined to transform the 
def component from dictionary to lexicon format.   
The sequence of lemmas is then expanded to a 
canonical wordlist W. A canonical wordlist W is 
a sequence <w
1
 .. w
u
> of sets of strings generated 
from a lexicon L, denoted by W = can(L), by list-
ing all possible instances of every lemma in the 
lexicon (i.e. all possible wordforms of a particu-
lar lemma), in effect creating a full form lexicon.  
The canonical wordlist W thus has u sets of 
strings corresponding to u lemmas in the lexicon. 
The particular lemma used to generate a word-
form w is obtained by the operator lem(w). The 
sequence of lemmas used to generate W is de-
noted as lemmas(W). The union of two wordlists 
W
1
 ? W
2
 is defined to be the union of all sets of 
strings in both wordlists, 
i.e.
jiji
yxWWWyWx ?=?????
2121
,  
provided that lem(x
i
) = lem(y
j
) ? lem(x
j
) ?     
lemmas(W
2
) ? lem(y
j
) ? lemmas(W
1
) holds. 
This definition ensures maximum coverage of 
the resulting canonical wordlist. An empty or null 
canonical wordlist results if no pair of strings 
obey the previously stated condition while the 
union of a wordlist with a null wordlist is the 
original wordlist itself.  
Similarly the intersection of two wordlists W
1
 
? W
2
 is defined to be the union of all sets of 
strings in both wordlists that have corresponding 
lemmas appearing in both wordlists, i.e. 
? ? ? ? ? = ?x W y W W W x y
i j i j1 2 1 2
,  
provided that lem(x
i
) = lem(y
j
) holds. 
Note that this definition is concerned mainly 
with the lemmas and their associated wordforms 
themselves. Since lexicons are not just a list of 
lemmas and wordforms, other linguistic annota-
tions will have to be evaluated using other tech-
niques appropriate to the particular linguistic 
annotations added to the lemma entries. 
 
2.2 L-Measure Definition 
Given a lexicon L and a set of dictionaries D = 
{D
1
 .. D
k
} transform the set of dictionaries D into 
a set of lexicons L' = {L
1
 .. L
k
} using the lex 
transformation on every dictionary, thus 
( )
U
k
i
DlexL
1
'= . Define W as the canonical word-
list obtained from L, W = can(L) and W' as the 
canonical wordlist obtained from L', 
( )
U
k
i
LcanW
1
'=  under canonical wordlist union.  
Define Y to be the canonical wordlist of words 
common to both W and W', Y = W ? W'. The 
sample size S used for the L-measure is defined 
as ?.|lemmas(Y)| where ? is some value in the 
range (0..1) that controls the random sample size. 
Typically ? should be set to somewhere between 
0.01 and 0.1. It is expected that the sample size 
will be large enough to assume that the sample is 
representative of the whole population. 
The L-measure of a lemma j in lemmas(W) and 
lemma i in lemmas(Y) is given by 
( )
( ) ( )
( ) ( )
L i j
r i j p i j
r i j p i j
,
, ,
, ,
=
? ?
+
2
 where r denotes re-
call and p is the precision. Recall is defined as 
( )r i j
n
n
ij
i
, =  and precision is defined as 
( )
p i j
n
n
ij
j
, =
 where n
ij
 is the number of lemma i 
members in lemma j, while n
j
 and n
i
 are the sizes 
of lemma j and lemma i respectively. The overall 
L-measure for the entire sample of size n is given 
by
( )
[ ]
L
n
n
L i j
i
i
*
max ,=
?
. L
*
 is always in the 
range [0..1] and is proportional to the lexicon 
quality, with an L
*
  score of 1 representing a per-
fect quality lexicon with respect to the lexicon 
being used as a standard. 
Y is used instead of W' since lexical word cov-
erage is largely determined by the quality of the 
corpus used to create the lexicon. While this kind 
of analysis might be useful in determining the 
coverage of a lexicon the L-measure is oriented 
towards measuring quality rather than quantity, 
independently of the corpus that was used to cre-
ate the lexicon. 
 
3 Results 
The L-measure has been used to measure the 
quality of the Maltilex Computational Lexicon in 
relation to existing paper based dictionaries. The 
most comprehensive dictionary of Maltese was 
used to produce L', the comparison standard lexi-
con (Aquilina, 1987-1990). The capability of the 
L-measure to work with a statistical sample made 
a manual analysis of results possible without hav-
ing L' in digital form. 
The value for the sample size S was deter-
mined through a parameter ? that was set to 0.01, 
meaning that 1% of all lemmas in the Maltilex 
Computational Lexicon were covered by the sta-
tistical sample. Since around 63,000 lemmas ex-
ist in the combined lexicon the sample size S was 
determined to be 630. The set of 630 lemmas 
chosen at random from the Maltilex Corpus con-
tained a total of 5,887 wordforms taken from the 
combined lexicon. 
The precision and recall for the samples were 
calculated individually to obtain the individual L-
measure for a range of lemmas. A fully worked 
out example of the calculation of the L-measure 
for the lemma missier (father) is given. Lem-
mas in the Maltilex Computational Lexicon are 
aligned automatically using a technique adopted 
from bioinformatics and hence the presentation 
of the wordforms in their aligned format (Dalli, 
2000b; Gusfield, 1997). 
The lemma missier (the Maltese word for fa-
ther with the cluster showing different forms like 
my father, your father, etc.) taken from the 
Maltilex Computational Lexicon, which repre-
sents lemma i, contains seven members as dis-
played below: 
 
m i s s ie r _ _ _ _ _ _  _ _ _ 
m i s s ie r e k _ _ _ _  _ _ _ 
m i s s ie r _ _ _ n _ a  _ _ _ 
m i s s ie r _ k o m _ _  _ _ _ 
m i s s i  r i _ _ _ j ie t n a 
m i s s ie r i _ _ _ _ _  _ _ _ 
m i s s ie r _ h o m _ _  _ _ _ 
 
The lemma missier, taken from Aquilina?s Dic-
tionary, which represents lemma j, can be used to 
generate the following ten members as displayed 
below: 
 
m i s s ie r _ _ _ _ _ _  _ _ _ 
m i s s ie r e k _ _ _ _  _ _ _ 
m i s s ie r _ _ _ n _ a  _ _ _ 
m i s s ie r _ k o m _ _  _ _ _ 
m i s s i  r i _ _ _ j ie t n a 
m i s s ie r i _ _ _ _ _  _ _ _ 
m i s s ie r a _ _ _ _ _  _ _ _ 
m i s s ie r _ _ u _ _ _  _ _ _ 
m i s s ie r _ h o m _ _  _ _ _ 
m i s s i  r i _ _ _ j ie t _ _ 
 
For this example, n
j
 and n
i
 are thus equal to 10 
and 7 respectively. Recall and precision values 
are calculated as ( ) 1
7
7
', ==missiermissierr
 
( ) 7.0
10
7
', ==missiermissierp respectively. 
 
The L-measure for the lemma missier is 
( ) 8235.0
7.1
4.1
7.01
7.012
', ==
+
??
=missiermissierL  
The overall L-measure for the entire sample of 
5,887 wordforms is given by 
( )[ ]
?
=
i
i
jiL
n
L ,max
5887
*
. The contribution of 
the lemma missier to the final L
*
 score is thus 
given by 8235.0
5887
7
= 0.000979226. A high 
precision floating point library was used to repre-
sent the individual contribution values since these 
are generally very small. Figures 1 and 2 show 
the precision and recall curves for the whole 
sample respectively.  
 
0
0.2
0.4
0.6
0.8
1
1.2
1 32 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621
Lemmas
 
Figure 1 Precision 
0
0.2
0.4
0.6
0.8
1
1.2
1 25 49 73 97 121 145 169 193 217 241 265 289 313 337 361 385 409 433 457 481 505 529 553 577 601 625
Lemmas
 
Figure 2 Recall 
0
0.2
0.4
0.6
0.8
1
1.2
1 32 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621
Lemmas
 
Figure 3 Precision and Recall Trends 
 
 
Figure 3 shows moving average trendlines for 
precision and recall (precision is shown in a bold 
line on top, recall is the fainter line underneath). 
The average precision was 0.91748 and the aver-
age rate of recall was 0.661359. 
 
0
0.2
0.4
0.6
0.8
1
1.2
1 55 109 163 217 271 325 379 433 487 541 595
Figure 4 Individual L-Measure Values 
0
0.2
0.4
0.6
0.8
1
1.2
1 55 109 163 217 271 325 379 433 487 541 595
Figure 5 Individual L-Measure Values Trend 
 
Figure 4 shows the individual L-measure val-
ues for the sample. The values displayed in Fig-
ure 4 are those used to calculate the final L
*
 
value. Figure 5 shows the moving average trend-
line for the individual L-measure values. 
The average individual L-measure was 
0.707256882 while the average individual 
contribution of a lemma to the L
*
 value was 
0.000748924. The variance in the L-measure in-
dividual values was 0.065504369.  
The correlation between the L-measure and 
precision was 0.163665769 while the correlation 
between the L-measure and recall was 
0.922214452. 
The overall L
*
 score for the Maltilex Computa-
tional Lexicon was 0.4718. This score is quite 
intuitive when the various problems in the exist-
ing Maltese corpus used to create the Computa-
tional Lexicon are considered. This score means 
that the number of wordforms that are stored or 
that can be generated by the current lexicon 
needs to be expanded by around 53% in order to 
match the quality of the lexicon underlying 
Aquilina?s dictionary (Aquilina, 1987-1990). 
4 Conclusion 
The L-measure is a useful evaluation metric that 
can be used to measure the quality of a computa-
tional lexicon based on clustering concepts. The 
small data sample required by L-measure to give 
meaningful results makes it a practical measure 
to use in a variety of situations where massive 
amounts of data might not be available. This 
makes L-measure ideal for use in the evaluation 
of Language Resources for minority languages 
and also for quick benchmark studies that evalu-
ate the quality of a computational lexicon as it is 
being created. 
Compared with the F-measure, the L-measure 
will give highly similar results using less data. 
Naturally the validity of the L-measure results 
depends on the choice of the ? value, which in 
turn determines the sample size. 
The lemma/cluster based approach of the L-
measure is suitable for the evaluation of Semitic 
language lexicons that often prove problematic to 
evaluation techniques based on English or Ro-
mance languages. 
The L-measure also has potential future appli-
cations in the comparison and evaluation of dif-
ferent lexicons. The individual L-measure scores 
can also be used to identify areas of similarities 
and differences between different lexicons 
quickly. 
The L-measure can also be adapted to other 
areas of Computational Linguistics as long as the 
concept of a cluster and some means of determin-
ing its precision and recall exist. Minimal 
changes are needed to adapt the L-measure to 
other domains making future adaptations likely. 
Acknowledgment 
This work has been made possible with the col-
laboration of the Maltilex Project at the Univer-
sity of Malta. 
References  
Angelo Dalli. 2002a. Computational Lexicon for Mal-
tese. M.Sc. Dissertation. Department of Computer 
Science and AI, University of Malta, Malta. 
Angelo Dalli. 2002b. Biologically Inspired Lexicon 
Structuring Technique. HLT2002, San Diego, Cali-
fornia.   
Bjorner Larsen and Chinatsu Aone. 1999. Fast and 
Effective Text Mining Using Linear-time Docu-
ment Clustering. KDD-99, San Diego, California. 
C. Van Rijsbergen. 1979. Information Retrieval, 2nd 
ed. Butterworth, London. 
Claude E. Shannon. 1948. A mathematical theory of 
communication. Bell System Technical Journal 27: 
379-423, 623-656. 
Dan Gusfield. 1997. Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cam-
bridge, UK. 
Joseph Aquilina. 1987-1990. Maltese-English Dic-
tionary. Midsea Books, 2 Volumes, Valletta, Malta. 
Manwel Mifsud. 1995. Loan verbs in Maltese a de-
scriptive and comparative study. Studies in Semitic 
languages and linguistics, Brill, Leiden. 
Michael Rosner et. al. 1999. Linguistic and Computa-
tional Aspects of Maltilex. ATLAS Symposium, Tu-
nis. 
Michael Steinbach, George Karypis, and Vipin 
Kumar. 1999. A comparison of document cluster-
ing techniques, University of Minnesota, Technical 
Report 00-034. 
 
 
 
