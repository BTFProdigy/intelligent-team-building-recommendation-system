Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 973?981,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning to Predict Code-Switching Points
Thamar Solorio and Yang Liu
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080, USA
tsolorio,yangl@hlt.utdallas.edu
Abstract
Predicting possible code-switching points can
help develop more accurate methods for au-
tomatically processing mixed-language text,
such as multilingual language models for
speech recognition systems and syntactic an-
alyzers. We present in this paper exploratory
results on learning to predict potential code-
switching points in Spanish-English. We
trained different learning algorithms using a
transcription of code-switched discourse. To
evaluate the performance of the classifiers, we
used two different criteria: 1) measuring pre-
cision, recall, and F-measure of the predic-
tions against the reference in the transcrip-
tion, and 2) rating the naturalness of artifi-
cially generated code-switched sentences. Av-
erage scores for the code-switched sentences
generated by our machine learning approach
were close to the scores of those generated by
humans.
1 Introduction
Multilingual speakers often switch back and forth
between languages when speaking or writing,
mostly in informal settings. The mixing of lan-
guages involves very elaborated patterns and forms
and we usually use the term Code-Switching (CS)
to encompass all of them (Lipski, 1978). Before the
Internet era, CS was mainly used in its spoken form.
But with so many different informal interaction set-
tings, such as chats, forums, blogs, and web sites
like Myspace and Facebook, CS is being used more
and more in written form. For English and Spanish,
CS has taken a step further. It has become a hall-
mark of the chicano culture as it is evident by the
growing number of chicano writers publishing work
in Spanish-English CS.
We have not completely discovered the process
of human language acquisition, especially dual lan-
guage acquisition. Findings in linguistics, soci-
olinguistics, and psycholinguistics show that the
production of code-switched discourse requires a
very sophisticated knowledge of the languages be-
ing mixed. Some theories suggest bilingual speak-
ers might have a third grammar for processing this
type of discourse. The general agreement regarding
CS is that switches do not take place at random and
instead it is possible to identify rules that bilingual
speakers adhere to.
Understanding the CS process can lead to accu-
rate methods for the automatic processing of bilin-
gual discourse, and corpus-driven studies about CS
can also inform linguistic theories. In this paper we
present exploratory work on learning to predict CS
points using a machine learning approach. Such an
approach can be used to reduce perplexity of lan-
guage models for bilingual discourse. We believe
that CS behavior can be learned by a classifier and
the results presented in this paper support our belief.
One of the difficult aspects of trying to predict
CS points is how to evaluate the performance of
the learner since switching is intrinsically motivated
and there are no forced switches (Sankoff, 1998b).
Therefore, standard classification measures for this
task such as precision, recall, F-measure, or ac-
curacy, are not the best approach for measuring
the effectiveness of a CS predictor. To comple-
973
ment the evaluation of our approach, we designed a
task involving human judgements on the naturalness
of automatically generated code-switched sentences.
Both evaluations yielded encouraging results.
The next section discusses theories explaining
the CS production process. Then in Section 3 we
present our framework for learning to predict CS
points. Section 4 discusses the empirical evaluation
of the classifiers compared to the human reference.
In Section 5 we present results of human evalua-
tions on automatically generated code-switched sen-
tences. Section 6 describes previous work related to
the processing of code-switched text. Finally, we
conclude in Section 7 with a summary of our find-
ings and directions for future work.
2 Bilingual Discourse
The combination of languages can be considered
to be a continuous spectrum where on each end of
the spectrum we have one of the standard languages
and no blending. As one moves closer to the mid-
dle of the spectrum the amount and complexity of
the blending pattern increases. The blending pattern
most widely known, and studied, is code-switching,
which refers to the mixing of words from two lan-
guages, but the words themselves do not suffer any
syntactic or phonological alterations. The CS points
can lie at sentence boundaries, but very often we
will also observe CS inside sentences. According to
(Sankoff, 1998b; Poplack, 1980; Lipski, 1978) when
CS is used inside a sentence, it can only happen at
syntactic boundaries shared by both languages, and
the resulting monolingual fragments will conform to
the grammar of the corresponding language. In this
CS theory the relationship between both languages
is symmetric ?lexical items from one language can
be replaced by the corresponding items in the sec-
ond language and vice versa. Another prevalent lin-
guistic theory argues the contrary: there is an asym-
metric relation where the changes can occur only in
one direction, which reflects the existence of a Ma-
trix Language (ML), the dominant language, and an
Embedded Language (EL), or subordinate language
(Joshi, 1982). The Matrix Language Frame model,
proposed and extended by Scotton-Myers, supports
this asymmetric relation theory. This formalism pre-
scribes that content morphemes can come from the
ML or the EL, whereas late system morphemes,
the elements that indicate grammatical relations, can
only be provided by the ML (Myers-Scotton, 1997).
Until an empirical evaluation is carried out on
large representative samples of discourse involving
a large number of different speakers, and different
language-pairs, the production of CS discourse will
not be explained satisfactorily. The goal of this work
is to move closer to a better understanding of CS by
learning from corpora to predict possible CS points.
3 Learning When To Code-Switch
3.1 The English-Spanish Code-Switched Data
Set
We recorded a conversation among three English-
Spanish bilingual speakers that code-switch regu-
larly when speaking to each other. The conversa-
tion lasts for about 40 minutes (?8k words, 922
sentences). It was manually transcribed and anno-
tated with Part-of-Speech (POS) tags. A total of
239 switches were identified manually. English is
the predominant language used, with a total of 576
monolingual sentences. We refer to this transcrip-
tion as the Spanglish data set. We are currently in the
process of collecting new transcriptions of this con-
versation in order to measure inter annotator agree-
ment.
3.2 Approach
Machine learning algorithms have proven to be sur-
prisingly good at language processing tasks, in-
cluding optical character recognition, text classifica-
tion, named entity extraction, and many more. The
premise of our paper is that machine learning al-
gorithms can also be successful at learning how to
code-switch as well as humans. At the very least
we want to provide encouraging evidence that this
is possible. To the best of our knowledge, there is
no previous work related to the problem of auto-
matically predicting CS points. Our machine learn-
ing framework then is inspired by existing theories
of CS and existing work on part-of-speech tagging
code-switched text (Solorio and Liu, 2008).
In our approach, each word boundary is a poten-
tial point for switching ? an instance of the learning
task. It should be noted that we can only rely on the
history of words preceding potential CS points in or-
974
Feature id Description
1 Word
2 Language id
3 Gold-standard POS tag
4 BIO chunk
5 English Tree Tagger POS
6 English Tree Tagger prob
7 English Tree Tagger lemma
8 Spanish Tree Tagger POS
9 Spanish Tree Tagger prob
10 Spanish Tree Tagger lemma
Table 1: Features explored in learning to predict CS
points.
der to extract meaningful features. Otherwise, if we
look also into the future, we could just do language
identification to extract the CS points. However, our
goal is to provide methods that can be used in real
time applications, where we do not have access to
observations beyond the point of interest. Another
restriction we imposed on the method is related to
the size of the context used. A sentence can be code-
switched in different ways, with all different ver-
sions adhering to the CS ?grammar?. The number
of permissible CS sentences grows almost exponen-
tially with the length of the sentence1. By limiting
the length of the context to at most two words we
are trying to avoid some sort of over fitting by hav-
ing the model making assumptions over the interac-
tion of the two languages that will be too weak, or
speaker-dependent.
Previous studies have identified several socio-
pragmatic functions of code-switching. The most
common include direct quotation, emphasis, clari-
fication, parenthetical comments, tags, and trigger
switches. Other characteristics relevant to CS be-
havior are the topic being discussed, the speakers
involved, the setting where the conversation is tak-
ing place, and the level of familiarity between the
speakers. Having encoded information regarding the
CS function and the aforementioned relevant factors
might help in predicting upcoming CS points. How-
ever, annotating this information in the transcription
can be time consuming and very often this informa-
1Almost exponentially because not all sentences will be con-
sidered grammatical.
tion is not readily available. Therefore, at the ex-
pense of making this task even more difficult, we de-
cided against trying to include this type of informa-
tion and include only lexical and syntactic features,
to evaluate a practical and cost effective method for
this task. Table 1 shows the list of features. All
of these features are associated with word wn, the
word immediately preceding boundary n. Feature 1
is the word form2. Feature 2 is language identifica-
tion. If the production of CS discourse adheres to
the matrix language frame model, then knowledge
of the language can potentially be a good source
of information. Feature 3 is the gold-standard POS
tag. We also include as a feature the position of
the word relative to the phrase constituent using a
Beginning-Inside-Outside (BIO) scheme. For in-
stance, the word at the beginning of the verb phrase
will be labeled as B, the following words inside this
verb phrase will be tagged as I, and words that were
not identified as part of a phrase constituent were
labeled as O. This chunking information was ex-
tracted using the English and Spanish versions of
FreeLing3. We did not measure accuracy on the
chunking information. Features 5 to 9 were gener-
ated by tagging the Spanglish conversation using the
Spanish and the English versions of the Tree Tagger
(Schmid, 1994). Attributes 5 to 7 are extracted from
the English version, which include the POS tag, the
confidence, and the lemma for that word. Similarly,
features 8 to 10 were taken from the Spanish mono-
lingual tree tagger. Features from the monolingual
taggers will have some noisy labels when tagging
fragments of the other language. However, consider-
ing that our feature set is small we want to explore if
adding these features, which include the lemmas and
probability estimates, can contribute to the learning
task.
We also explored using a larger context. In this
case, we extract the same features shown in Table
1 for the two words preceding the word boundary,
resulting in 20 attributes representing each instance.
Evaluation for this task is not straightforward.
Within a sentence, there are several CS points that
will result in a natural sounding code-switched sen-
tence, but none of these CS points are mandatory.
2Strictly speaking these should be called tokens, not words
since punctuation marks are considered as well.
3http://garraf.epsevg.upc.es/freeling/
975
CS has a lot to do with the speaker?s preferences,
the topic being discussed, and the background of the
participants involved. Using the standard approach
for measuring performance of classifiers can be mis-
leading, especially if the reference data set is small
and/or has only a small number of speakers. It is un-
realistic to just consider F-measure, or accuracy, as
truthfully reflecting how well the learners generalize
to the task. Therefore, we evaluated the classifier?s
performance using two different criteria, which are
discussed in the next sections.
4 Evaluation 1: Using the Reference Data
Set
This is the standard evaluation of machine learning
classifiers. We randomly divided the data into sen-
tences and grouped them into 10 subsets to perform
a cross-validation. Tables 2 and 3 show results for
Naive Bayes (NB) and Value Feature Interval (VFI)
(Demiroz and Guvenir, 1997). Using WEKA (Wit-
ten and Frank, 1999), we experimented with differ-
ent subsets of the attributes and two context win-
dows: using only the preceding word and using the
previous two words. The results presented here are
overall averages of 10-fold cross validation. We also
report standard deviations. It should be noted that
the Spanglish data set is highly imbalanced, around
96% of the instances belong to the negative class.
Therefore, our comparisons are based on Precision,
Recall, and F-measure, leaving accuracy aside, since
a weak classifier predicting that all instances belong
to the negative class will reach an accuracy of 96%.
The performance measures shown on Tables 2 and
3 show that NB outperforms VFI in most of the con-
figurations tested. In particular, NB yields the best
results when using a 1 word context with no lexical
forms nor lemmas as attributes (see Table 2 row 3).
This is a fortunate finding ?for most practical prob-
lems there will always be words in the test set that
have not been observed in the training set. For our
small Spanglish data set that will certainly be the
case. In contrast, VFI achieves higher F-measures
when using a context of two words and all the fea-
tures are used.
Analyzing the predictions of the learners we noted
that the NB classifier is heavily biased by the lan-
guage attribute, close to 80% of the positive predic-
tions made by NB are after seeing a word in Span-
ish. This preference seems to support the assump-
tion of the asymmetry between the two languages
and the existence of an ML4. This however is not
the case for VFI, only a little over 50% of the posi-
tive predictions belong to this scenario. Another in-
teresting finding is the learner?s tendency to predict
a code-switch after observing words like ?Yeah?,
?anyway?, ?no?, and ?shower?. The first two seem
to fit the pattern of idiomatic expressions. Accord-
ing to Montes-Alcala? this type of CS includes lin-
guistic routines and fillers that are difficult to trans-
late accurately (Montes-Alcala?, 2007), which might
be the case of ?anyway?, and unconscious changes,
which can explain the case of ?Yeah?. The case
of ?shower? and ?no? are more difficult to explain,
they might be overfitting patterns from the learners.
We also found out that VFI learned to predict that
a CS will take place right after seeing the sequence
of words le dije (I said). This sequence of words is
frequently used when the speaker is about to quote
his/herself, and this quotation is one of the well-
documented CS functions (Montes-Alcala?, 2007).
A greedy search approach for attribute selection
using WEKA showed that out of the 20 attributes
(when using a two word context), the subset with
the highest predictive value included the language
identification for word wn?1 and wn?2, the confi-
dence threshold from the English tagger for word
wn?2, the lemma from the Spanish Tree tagger for
wn?1, and the lexical form of the word wn?1. We
expected the chunk information to be useful and this
does not seem to be the case. Another unexpected
outcome is that higher F-measures are reached by
adding features generated by the monolingual Tree
taggers. Even though these features are noisy, they
still carry useful information.
We only show results from NB and VFI. Initial
experiments with a subset of the data showed that
these algorithms were the most promising for this
task. They both yielded higher F-measures, even
when compared against Support Vector Machines
(SVMs), C4.5, and neural networks. On this ex-
periment all the discriminative classifiers reached
a classification accuracy close to 96%, but an F-
4We remind the reader that in this paper ML stands for Ma-
trix Language.
976
Features Used
English Spanish Naive Bayes
Word Lang POS BIO Tree tagger Tree tagger
C Form id tag chunk POS Prob Lem POS Prob Lem P R F1
1 X X X 0.09(0.01) 0.01(0.00) 0.02(0.00)
1 X X X X 0.23(0.01) 0.32(0.02) 0.27(0.02)
1* X X X X X X X 0.19(0.00) 0.53(0.00) 0.28(0.00)
1 X X X X X X X X X X 0.18(0.00) 0.59(0.00) 0.27(0.00)
2 X X X 0.13(0.00) 0.35(0.00) 0.19(0.00)
2 X X X X 0.16(0.00) 0.46(0.00) 0.23(0.00)
2 X X X X X X X 0.14(0.00) 0.55(0.01) 0.23(0.00)
2 X X X X X X X X X X 0.16(0.00) 0.59(0.01) 0.25(0.00)
Table 2: Prediction results of CS points with NB using different features. Column C indicates the size of the context
used, 1 indicates a 1 word context, and 2 indicates two words preceding the word boundary. Columns P, R, and
F1, show precision, recall, and F-measure, respectively. Numbers in parenthesis show standard deviations. The row
marked with a ?*? shows the configuration used for the generation of CS sentences presented in Section 5.
measure on the positive class of around 0%. NB
and VFI estimate predictions for each class sepa-
rately, which makes them robust to imbalanced data
sets. In addition, generative models are known to
be better for smaller data sets since they reach their
higher asymptotic error much faster than discrimi-
native models (Ng and Jordan, 2002). This might
explain why Naive Bayes outperformed strong clas-
sifiers such as SVMs by a large margin.
The overall prediction performance is not very
high. However, we should remark that for this par-
ticular task expecting a high F-measure is unrealis-
tic. Consider for example, a case where the learners
predict a CS point where the speaker decided not to
switch, this does not imply that particular point is
not a good CS point. And similarly, if the classifier
missed an existing CS point in the reference data set
the resulting sentence might still be grammatical and
natural sounding. This motivated the use of an alter-
native evaluation, which we discuss below.
5 Evaluation 2: Using Human Evaluators
The goal of this evaluation is to explore how humans
perceive our automatically generated CS sentences,
and in particular, how do they compare to the orig-
inal sentences and to the randomly generated ones.
We selected 30 spontaneous and naturally occurring
CS sentences from different sources. Some of them
were selected from the Spanglish Times Magazine5,
some others from blogs found in (Montes-Alcala?,
2007). Other sentences were taken from a paper
discussing CS on e-mails (Montes-Alcala?, 2005).
All of the sentences are true occurrences of writ-
ten CS, from speakers different from the ones in the
Spanglish data set. The sentences were translated
to standard English and Spanish and were manually
aligned. We will use this parallel set of sentences
to predict CS points with our models. Based on the
model predictions we will generate code-switched
sentences by combining monolingual fragments.
It should be noted that the Spanglish data set is
a transcription of spoken CS. In contrast, this new
evaluation set contains only written CS. Recent stud-
ies suggest written CS will adhere to the rules of
spoken CS (Montes-Alcala?, 2005), but there is still
some controversy on this issue. From our perspec-
tive, both samples come from informal conversa-
tional interactions. It is expected that both will have
similar patterns and therefore will provide a good
source for our evaluation.
5.1 Automatically Generated Code-Switching
Sentences
In this subsection we describe how to generate code-
switched sentences randomly and with the learned
models described in the previous sections. For the
5http://www.spanglishtimes.com/
977
Features Used
English Spanish Voting Feature Intervals
Word Lang POS BIO Tree tagger Tree tagger
C Form id tag chunk POS Prob Lem POS Prob Lem P R F1
1 X X X 0.12(0.00) 0.68(0.00) 0.21(0.00)
1 X X X X 0.12(0.00) 0.65(0.01) 0.20(0.00)
1* X X X X X X X 0.12(0.00) 0.72(0.01) 0.21(0.00)
1 X X X X X X X X X X 0.13(0.00) 0.65(0.00) 0.22(0.00)
2 X X X 0.13(0.00) 0.60(0.00) 0.21(0.00)
2 X X X X 0.15(0.00) 0.52(0.01) 0.23(0.00)
2 X X X X X X X 0.13(0.00) 0.68(0.00) 0.22(0.00)
2 X X X X X X X X X X 0.15(0.00) 0.51(0.00) 0.24(0.00)
Table 3: Prediction results of CS points with VFI using different features. The notation on this table is the same as in
Table 2
classifier-based approach, we POS tagged each par-
allel set of sentences, with the monolingual English
and Spanish Tree Taggers, and we extracted the
same set of features described shown in Table 1. We
decided to train the models with a context size of
one word, even though both learners reached higher
F-measures when using a two-word context. This
decision was based on the observation that having a
two-word context will pose restrictions on possible
CS points, since we would not be able to switch un-
less we have inserted into the sentence at least two
tokens from the same language.
We trained the NB and VFI models with the Span-
glish data set (using features 2?6, 8, and 9, see Ta-
ble 1) and generated CS predictions for each paral-
lel file. A code-switched sentence is generated by
adding the first token of the sentence in language 1
(L1), and continue adding more tokens from L1 until
a CS point is found. When a CS prediction is found,
the following tokens are selected from the second
language (L2), and we continue adding tokens from
L2 until the classifier has predicted a change. Differ-
ent versions of the sentences are generated by chang-
ing the definition of L1 and L2.
For the randomly generated CS sentences, switch-
ing decisions are made randomly with a probability
proportional to the positive predictions made by the
classifiers (in this case NB). That is, for the Spanish
sentences switch points are predicted randomly with
a 30% chance of switching while for English switch
points are predicted with a 10% chance.
Generator Average Score
Human 3.64
NB 3.33
Random 2.68
VFI 2.50
Table 4: Average score of 18 judges over the set of 28
code-switched sentences rated.
In total we generated 180 CS sentences: 30 sen-
tences per generator scheme (we have three genera-
tors: NB, VFI, and random), and two versions from
each generator corresponding to the two possible
configurations of L1-L2 (Spanish-English, English-
Spanish). We noticed that in some cases same sen-
tences are generated by different methods and some-
times there are no switches. We narrowed down the
sentences by randomly choosing the combination of
L1-L2 for each generator. This reduced the num-
ber of sentences from having 6 versions, to having
only 3 versions of each sentence. From the resulting
30 sets, we removed 2 sets because one or more of
the generator schemes produced a monolingual sen-
tence. Therefore, we used 28 sets for human evalua-
tions.
5.2 Human Evaluation Results
We had a total of 18 subjects participating in the ex-
periment. All of them identified themselves as be-
ing able to read and write Spanish and English, and
the majority of them said to have used CS at least
978
some times. We showed to the human subjects the
28 sets of sentences. This time we included the orig-
inal version of the sentence. Therefore, each judge
was given 4 versions of each of the 28 code-switched
sentences: the one generated from NB predictions,
the one from VFI, the randomly generated, and the
original one. Then we asked them to rate each sen-
tence with a number from 1 to 5 indicating how nat-
ural and human-like the sentence sounds. A rating
of 5 means that they strongly agree, 4 means they
agree, 3 not sure, 2 disagree, 1 strongly disagree.
The average results are presented in Table 4. The
sentences generated by NB were scored consider-
ably higher than those from VFI and random, and
closer to the human sentences. According to the
paired t-test the difference between the NB score and
the random one is significant (p=0.01). However the
average score for VFI is lower than random. More
experiments are needed to see if by choosing the set-
ting where VFI had the highest F-measure would
make a difference in this respect. Overall the sub-
jects rated the human-generated CS sentences lower
than what we were expecting, although it is clear that
they consider these sentences more natural sound-
ing than the rest. This low rating might be related to
the attitude several evaluators expressed toward CS.
In the evaluation form we asked the judges to ex-
press their opinion on CS and several of them indi-
cated feelings along the lines of ?we shouldn?t code-
switch?.
There are several ways in which two parallel sen-
tences can be combined in CS, and possibly several
will sound natural, but from our results, it is clear
that the NB algorithm was indeed able to generate
a human-like CS behavior that was successfully dif-
ferentiated from randomly-generated sentences.
By looking at the set of automatically generated
code-switched sentences, we realized that the ma-
jority of the sentences are grammatical and natural
sounding. We believe that for a large number of the
sentences it would be hard for a human to distin-
guish the sentences that were automatically gener-
ated from the human-generated ones. One of the
give away clues is when a multi-word expression
is CS, or a tag line. Table 5 shows three examples
from the sentences evaluated. In the table there is
an example in sentence 1c where the noun phrase is
code-switched, the sentence is grammatical accord-
ing to Spanish rules, but it sounds very odd to have
the noun carta followed by the adjective in English,
?astrological?. Other interesting features are present
in example 3 where for the same noun phrase ?pro-
duce section? we have both, the female marking de-
terminer la and the masculine el. The same thing
happens for the noun phrase ?check-out line?. We
would need to have a larger occurrence of these in-
stances in our test set to determine if on average one
form is preferred over the other.
In another experiment, we measured the predic-
tion performance of NB and VFI on the 30 code-
switched sentences used in this part of the evalua-
tion. The best results, an F-measure of 0.418, were
achieved by NB when a context of 1 word was used,
and no words, nor lemmas were included as features.
This is the same setting used for the generation pro-
cess. In contrast, VFI reached an F-measure of 0.351
on this same setting. 30 sentences represent a very
small dataset but the results are very promising since
the speakers are different in the training and testing
dataset. Moreover, these results support the claim
that written and spoken CS obey similar rules.
6 Related Work
There is little prior work on computational linguis-
tic approaches to code-switched discourse. Most
of the previous work includes formalisms to pars-
ing and generating mixed sentences, for example for
Marathi and English (Joshi, 1982), or Hindi and En-
glish (Goyal et al, 2003). Sankoff proposed a pro-
duction model of bilingual discourse that accounts
for the equivalence constraint and the unpredictabil-
ity of code-switching (Sankoff, 1998a). His real-
time production model draws on the alternation of
fragments from two virtual monolingual sentences.
But no statistical assessment has been conducted on
real corpora.
Another related work deals with language iden-
tification on English-Maltese code-switched SMS
messages (Rosner and Farrugia, 2007). What the au-
thors found to work best for language identification
in this noisy domain is a combination of a bigram
Hidden Markov Model, trained on language tran-
sitions, and a trigram character Markov Model for
handling unknown words.
979
1a. Naive Bayes:
By unlocking the information in your astrological chart, puedo ver la respuesta! Ask me!
1b. VFI:
Puedo ver la answer by unlocking the information in your carta astrolo?gica! Ask me !
1c. Random:
By unlocking the information de tu carta astrological, I can see the answer! Ask me !
1d. Human:
By unlocking the information in your astrological chart, puedo ver the answer! Pregu?ntame!
1e. English version:
By unlocking the information in your astrological chart, I can see the answer! Ask me!
2a. Naive Bayes:
Pero siendo this a new year, es tiempo de empezar de nuevo que no?
2b. VFI:
But this being a new year, es tiempo de empezar over isn?t it ?
2c. Random:
But this being a new an?o, it?s tiempo to start over isn?t it?
2d. Human:
Pero this being a new year, it?s a time to start over que no?
2e. English version:
But this being a new year, it?s time to start over isn?t it?
3a. Naive Bayes:
Juan confirmed me that it was very obvious, y no solamente en el produce section, en la check-out line as well.
3b. VFI:
Me confirmo? Juan que it was very obvious, y no solamente en el produce section, tambie?n en la check-out line.
3c. Random:
Juan confirmed que fue very obvious, y not solamente en el a?rea de produce, in the check-out line as well.
3d. Human:
Me confirmo? Juan que fue muy obvio, y no solamente en la produce section, tambie?n en el check-out line.
3e. English version:
Juan confirmed me that it was very obvious, and not only on the produce section, in the check-out line as well.
Table 5: Examples of automatically generated CS sentences.
7 Conclusions
We presented preliminary results on learning to pre-
dict CS points with machine learning. One of the
possible applications of our method involves fine-
tuning the weights in a multilingual language model,
for instance, as part of a speech recognizer for Span-
glish. With this in mind, we restricted the possible
features in the learning scenario allowing only lexi-
cal and syntactic features that could be automatically
generated from the text. Empirical evaluations on
a Spanglish conversation showed that Naive Bayes
and VFI can predict with acceptable F-measures
possible CS points, considering the difficulty of the
task. Prediction of CS points can help improve mul-
tilingual language models.
Evaluation of our approach cannot be done based
only on the gold-standard set since there is no sin-
gle right answer in this task. Therefore, we comple-
mented the evaluation by involving judgements from
bilingual speakers. We generated CS sentences by
taking the predictions from the classifiers to merge
parallel sentences. On average, the sentences gen-
erated from the NB model were rated closer to the
original sentences, and a lot higher than the ones
from a random generator. Most of the sentences
sounded human-like. But because the process is au-
tomatic we did find some awkward constructions,
for example plural vs singular noun-verb agreement,
or multi-word phrases that were code-switched in
the middle. Perhaps a multi-word recognition fea-
ture could improve results.
One of the advantages of technological develop-
ment and economic globalization is that more peo-
ple from different regions of the world with differ-
ent cultures, and therefore, different languages will
980
be in closer contact. As a result, code-switching will
become more popular. It is important to start ad-
dressing this type of bilingual communication from
a computational linguistics point of view. This work
is one of the few attempts to fill the gap.
Some directions for future work include: explor-
ing the extent to which our results can be improved
by including a multi-word expression recognition
system. We also want to investigate the integration
of our approach to multilingual language models and
move beyond CS to address other deeper linguistic
phenomena. Lastly, we would like to explore similar
approaches in other popular language combinations.
Acknowledgements
This research is supported by the National Science
Foundation under grant 0812134. We are grateful to
Ray Mooney, Melissa Sherman and the three anony-
mous reviewers for insightful comments and sug-
gestions. Special thanks to the human judges that
helped with the sentence evaluations.
References
G. Demiroz and H. A. Guvenir. 1997. Classification by
voting feature intervals. In European Conference on
Machine Learning, ECML-97, pages 85?92.
P. Goyal, Manav R. Mital, A. Mukerjee, Achla M. Raina,
D. Sharma, P. Shukla, and K. Vikram. 2003. A
bilingual parser for Hindi, English and code-switching
structures. In Computational Linguistics for South
Asian Languages ?Expanding Synergies with Europe,
EACL-2003 Workshop, Budapest, Hungary.
A. Joshi. 1982. Processing of sentences with intrasenten-
tial code-switching. In Ja?n Horecky?, editor, COLING-
82, pages 145?150, Prague, July.
J. Lipski. 1978. Code-switching and the problem of
bilingual competence. In M. Paradis, editor, Aspects
of bilingualism, pages 250?264. Hornbeam.
C. Montes-Alcala?. 2005. Ma?ndame un e-mail: cam-
bio de co?digos espan?ol-ingle?s online. In Luis Ortiz
and Manel Lacorte, editors, In Contacto y contextos
lingu???sticos: El espan?ol en los Estados Unidos y en
contacto con otras lenguas. Iberoamericana/Vervuert.
C. Montes-Alcala?. 2007. Blogging in two languages:
Code-switching in bilingual blogs. In Jonathan
Holmquist, Augusto Lorenzino, and Lotfi Sayahi, edi-
tors, In Selected Proc. of the Third Workshop on Span-
ish Sociolinguistics, pages 162?170, Somerville, MA.
Cascadilla Proceedings Project.
C. Myers-Scotton. 1997. Duelling Languages: Gram-
matical Structure in Codeswitching. Oxford Univer-
sity Press, 2nd edition.
A. Ng and M. Jordan. 2002. On discriminative vs. gen-
erative classifiers: A comparison of logistic regression
and Naive Bayes. In Advances in Neural Information
Processing Systems (NIPS) 15. MIT Press.
S. Poplack. 1980. Sometimes I?ll start a sentence in
Spanish y termino en espan?ol: toward a typology of
code-switching. Linguistics, 18(7/8):581?618.
M. Rosner and P. J. Farrugia. 2007. A tagging algorithm
for mixed language identification in a noisy domain.
In INTERSPEECH 2007, pages 190?193, Antwerp,
Belguim, August.
D. Sankoff. 1998a. A formal production-based expla-
nation of the facts of code-switching. Bilingualism,
Language and Cognition, (1):39?50.
D. Sankoff. 1998b. The production of code-mixed dis-
course. In 36th ACL, volume I, pages 8?21, Montreal,
Quebec, Canada, August.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, September.
T. Solorio and Y. Liu. 2008. Part-of-speech tagging
for English-Spanish code-switched text. In EMNLP-
2008, Honolulu, Hawai, October.
I. H. Witten and E. Frank. 1999. Data Mining, Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann.
981
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1051?1060,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Part-of-Speech Tagging for English-Spanish Code-Switched Text
Thamar Solorio and Yang Liu
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080, USA
tsolorio,yangl@hlt.utdallas.edu
Abstract
Code-switching is an interesting linguistic
phenomenon commonly observed in highly
bilingual communities. It consists of mixing
languages in the same conversational event.
This paper presents results on Part-of-Speech
tagging Spanish-English code-switched dis-
course. We explore different approaches to
exploit existing resources for both languages
that range from simple heuristics, to language
identification, to machine learning. The best
results are achieved by training a machine
learning algorithm with features that combine
the output of an English and a Spanish Part-
of-Speech tagger.
1 Introduction
Worldwide the percentage of bilingual speakers is
fairly large, and it keeps increasing at a high rate.
In the U.S., 18% of the total population speaks a
language other than English at home, the major-
ity of which speaks Spanish (U.S. Census Bureau,
2003). A significant percentage of this Spanish-
English bilingual population code-switch between
the two languages in what is often referred as Span-
glish, the mix of Spanish and English. Spanish
and English are not the only occurrence of language
mixtures. Examples of other popular combinations
include Arabic dialects, French and German, Span-
ish and Catalan, Maltese and English, and English
and French. Typically when there are linguistic bor-
ders, or when the country has more than one official
language, we can find instances of code-switching.
Despite the wide use of code-switched discourse
among bilinguals, this linguistic phenomenon has
received little attention in the fields of Natural Lan-
guage Processing and Computational Linguistics.
Part-of-Speech (POS) tagging is a well studied prob-
lem in these fields. For languages such as English,
German, Spanish, and Chinese there are several dif-
ferent POS taggers that reach high accuracies, espe-
cially in news text corpora. However, to our knowl-
edge, there is no previous work on developing a POS
tagger for text with mixes of languages.
In this paper we present results on the problem
of POS tagging English-Spanish code-switched dis-
course by taking advantage of existing taggers for
both languages. This rationale follows the evi-
dence from studies of code-switching on different
language pairs, which have shown code-switching
to be grammatical according to both languages be-
ing switched. We use different heuristics to combine
POS tag information from existing monolingual tag-
gers. We also explore the use of different language
identification methods to select POS tags from the
appropriate monolingual tagger. However, the best
results are achieved by a machine learning approach
using features generated by the monolingual POS
taggers.
The next section presents the facts about code-
switching, including some previous work done
mainly in linguistics. Then in Section 3 we dis-
cuss previous work related to the automated pro-
cessing of code-switched discourse. In Section 4
we describe the English-Spanish code-switched data
set gathered for the experimental evaluation. Sec-
tion 5 presents the heuristics-based approaches for
POS tagging that we explored. In Section 6 we
describe our machine learning approach and show
1051
results on POS tagging code-switched text. An in
depth analysis of results is presented in Section 7,
and we conclude this paper with a summary of the
findings and directions for future work in Section 8.
2 Rules of Code-switching
In the linguistic, sociolinguistic, psychology, and
psycholinguistic literature, bilingualism and the in-
herent phenomena it exhibits have been studied
for nearly a century (Espinosa, 1917; Ervin and
Osgood, 1954; Gumperz, 1964; Gumperz and
Hernandez-Chavez, 1971; Gumperz, 1971; Sankoff,
1968; Lipski, 1978). Despite the numerous previ-
ous studies of linguistic characteristics of bilingual-
ism, there is no clear consensus on the terminol-
ogy related to language alternation patterns in bilin-
gual speakers. The alternation of languages within
a sentence is known as code-mixing, but it has
also been referred as intrasentential code-switching,
and intrasentential alternation (Poplack, 1980; Gros-
jean, 1982; Ardila, 2005). Alternation across sen-
tence boundaries is known as intersentential code-
switching, or just code-switching. In the rest of this
paper we will refer to the mixing of languages as
code-switching. When necessary, we will differen-
tiate the type of code-switching by referring to al-
ternations within sentences as intrasentential code-
switching and alternations across sentence bound-
aries as intersentential code-switching.
Linguistic phenomena in bilingual speakers have
been analyzed on different language pairs, includ-
ing English-French, English-Dutch, Finish-English,
Arabic-French, and Spanish-English, to name a few.
There is a general agreement that code-switched pat-
terns are not generated randomly; according to these
studies, they follow specific grammatical rules. Fur-
thermore, some studies suggest that, if these rules
are violated, the resulting discourse will sound un-
natural (Toribio, 2001b; Toribio, 2001a). The fol-
lowing shows the rules governing code-switching
discourse described in several studies (Poplack,
1980; Poplack, 1981; Sankoff, 1981; Sankoff,
1998a).
? Switches can take place only between full word
boundaries. This is also known as the free mor-
pheme constraint.
? Monolingual constructs within the sentence
will follow the grammatical rules of the mono-
lingual fragment.
? Permissible switch points are those that do not
violate the order of adjacent constituents on
both sides of the switch point of either of the
languages. This is called the equivalence con-
straint.
Although these rules are somewhat controversial,
and most of the studies on this area have been con-
ducted on small samples, we cannot ignore the fact
that patterns bearing the above rules have emerged in
different bilingual communities with different back-
grounds.
3 Automated Processing of Code-Switched
Discourse
A previous work related to the processing of code-
switched text deals with language identification
on English-Maltese code-switched SMS messages
(Rosner and Farrugia, 2007). In addition to deal-
ing with intrasentential code-switching, they have
to deal with text where misspellings and ad hoc
word contractions abound. What Rosner and Far-
rigua have found to work best for language identi-
fication in this noisy domain is a combination of a
bigram Hidden Markov Model, trained on language
transitions, and a trigram character Markov Model
for handling unknown words. In another related
work, Franco and Solorio present preliminary results
on training a language model for Spanish-English
code-switched text (Franco and Solorio, 2007). To
evaluate their language model, they asked a human
subject to judge sentences generated by a PCFG in-
duced from training data and the language model.
However, they only used one human judge.
Regarding the automated POS tagging and pars-
ing of code-mixed utterances there is little prior
work. To the best of our knowledge, there is no
parser, nor POS tagger, currently available for the
syntactic analysis of this type of discourse. There
are theoretical approaches that propose formalisms
to represent the structure of code-switched utter-
ances and describe a framework for parsing and gen-
erating mixed sentences, for example for Marathi
and English (Joshi, 1982), or Hindi and English
(Goyal et al, 2003). Sankoff proposed a production
model of bilingual discourse that accounts for the
1052
equivalence constraint and the unpredictability of
code-switching (Sankoff, 1998a; Sankoff, 1998b).
His real-time production model draws on the alter-
nation of fragments from two virtual monolingual
sentences. It also accounts for other types of code-
switching such as repetition-translation and inser-
tional code-switching. But no statistical assessment
has been conducted on real corpora.
Our goal is to develop a POS tagger for code-
switched utterances, which is the first step of the
syntactic analysis of any language. Among the chal-
lenges we face is the lack of a representative sam-
ple of code-mixed discourse. Most POS taggers are
built using large collections, usually at least a mil-
lion words, such as the Brown corpus (Kucera and
Francis, 1967), the Wall Street Journal corpus (Paul
and Baker, 1992), or the Switchboard corpus (God-
frey et al, 1992). Currently, there is no annotation
of code-switched text of comparable size. But in
contrast to the lack of linguistic resources available
for Spanish-English code-mixed discourse, English
and Spanish have sufficient resources, especially En-
glish. Thus, rather than starting from scratch, we
will draw on existing taggers for both languages,
which will reduce the amount of code-switched data
needed. Some examples of POS taggers that per-
form reasonably well on monolingual text of each
language can be found in (Brants, 2000; Brill, 1992;
Carreras and Padro?, 2002; Charniak, 1993; Ratna-
parkhi, 1996; Schmid, 1994). However, these tools
are designed to work on monolingual text, therefore
if applied as they are to code-switched text, their ac-
curacy will decrease by a large margin. In the fol-
lowing sections we will explore different methods
for combining monolingual taggers.
4 Data Set
Data collections that have instances of Spanish-
English code-switching, Spanglish for short, are not
easily found since code-switching is primarily used
in spoken form. To gather data we recorded a con-
versation among three staff members of a southwest
university in the U.S. The three speakers come from
a highly bilingual background and code-switch reg-
ularly when speaking among themselves, or other
bilingual speakers.
This recording session has around 39 minutes of
Table 1: Excerpts taken from the Spanglish data set.
Spanglish English Translation
(a)Entonces le dio? el
virus y no se lo atendio?
and the virus spread
through his body.
(a)Then he got the
virus and he didn?t re-
ceive treatment and the
virus spread through
his body.
(b)Cuando yo lo vi he
looked pretty bad.
(b)When I saw him he
looked pretty bad.
(c)I think she was
taller than he was.
(c)I think she was taller
than he was.
Y un cara?cter muy
bonito tambie?n ella.
And a very nice char-
acter she as well.
Very easy going. Very easy going.
continuous speech (922 sentences, about 8k words)
and was transcribed and annotated with POS tags by
a human annotator. The annotations were later re-
vised by a different annotator but no inter-annotator
agreement was measured. The POS tag set used in
the annotation is the combination of the tag sets from
the English and the Spanish Tree Taggers (see Sec-
tion 5). The vocabulary of the transcription has a to-
tal of 1,516 different word forms1. In the conversa-
tion a total of 239 switches were identified manually,
out of which 129 are intrasentential code-switches,
and the rest are intersentential. English is the pre-
dominant language used, with a total of 6,020 tokens
and 576 monolingual sentences. In contrast, the
transcription has close to 2k tokens in Spanish. Ta-
ble 1 shows examples of code-switching taken from
the recorded conversation; (a) and (b) are instances
of intrasentential code-switching, and (c) shows in-
tersentential code-switching.
5 Rule-based Methods for Exploiting
Existing Resources
In this section we present several heuristics-based
methods for POS tagging code-switched text. First,
we describe the monolingual taggers used in this
work. Then we present the different approaches ex-
plored and contrast their performance.
1This transcription and the audio file are freely available for
research purposes by contacting the first author.
1053
5.1 Monolingual Taggers
We used the Tree Tagger (Schmid, 1994) for this
work because of the following considerations:
1. It has both English and Spanish versions. The
English tagger uses a slightly modified version of
the Penn Treebank tag set and was trained and eval-
uated on different portions of the Penn Treebank,
reaching a POS tagging accuracy of 96.36%. The
Spanish one uses a different tagset with 75 different
POS tags2 and was trained on the Spanish CRATER
corpus.
2. The transition probabilities are estimated using a
modified version of the ID3 decision tree algorithm
(Quinlan, 1986), which provides more freedom to
learn contextual cues than n-grams.
3. Both taggers include a special tag for foreign
words, PE for Spanish and FW for English. We do
not expect this tag to identify correctly all foreign
words, but when available this information will be
exploited.
4. The Tree tagger generates probability estimates
on the tags that can be used as features.
5. Finally, when the tagger fails to lemmatize a
word it outputs the special token ?unknown?. This
information can be used as a hint of words that do
not belong to that particular language.
5.2 Heuristic-based Systems
For all heuristics the complete Spanglish data set
was given to both taggers as a single text, then the
final tag for each word was selected from the output
of the taggers according to the different heuristics.
Table 2 shows the tagging accuracies of the different
heuristics we explored, which are explained below.
1. Using the monolingual tagger. Here we simply
give the Spanglish text to the Spanish and the
English tree tagger. We expect from both taggers a
performance degradation due to the inclusion of for-
eign words in code-switching, as compared against
their accuracy on monolingual texts. Another
complicating factor to keep in mind is that we are
dealing with spoken language. Hesitations, fillers,
disfluencies, and interruption points, such as Umm,
Mmmhmm, and Uh-huh, are frequently observed in
2The authors were unable to identify the source of the Span-
ish tagset.
Table 2: Accuracy on POS tagging Spanglish text using
simple heuristics for combining the output of the English
and Spanish tagger.
Heuristic Accuracy (%)
1 Spanish Tree Tagger 25.99
English Tree Tagger 54.59
2 Highest prob tag or English 51.51
Highest prob tag or Spanish 49.16
3 Prob + special tags + lemmas 64.27
4 Dictionary-based Language Id 86.03
Character 5-grams Language Id 81.46
Human Language Id 89.72
speech and it is well known that they complicate
the POS tagging task. The tagging accuracy from
using the individual taggers is rather low, 26% for
the Spanish tagger and 54% for the English one.
The large difference between the two taggers can be
attributed to the fact that the majority of the words
in the corpus are in English.
2. Using confidence thresholds. The Tree Tagger
can output probabilities for each tag, showing the
confidence of the tagger on each particular tag. To
use this information we choose for each word the tag
from the tagger with the highest confidence. When
there is a tie we use either the English or the Spanish
tag. Table 2 shows the results for the two cases. The
?Highest prob tag or English? heuristic gives an ac-
curacy of 51%, which is almost as accurate as using
only the English tagger. The ?Highest prob tag or
Spanish? achieves an accuracy of 49%, which is an
improvement over using only the monolingual Span-
ish tagger, but it is still below the accuracy of the En-
glish monolingual tagger. This is also possibly due
to the task being easier for the English tagger.
3. Combining confidence thresholds with knowl-
edge from special tags and lemmas. This heuristic
uses confidence thresholds combined with decisions
based on the special tags, described in Section 5.1,
and the unknown lemmas found. Let POSE(wi)
and POSS(wi) be the POS tags assigned to word
wi by the English and Spanish tagger respectively;
and let ProbE(wi) and ProbS(wi) be the confi-
dence scores of POS tags for word wi computed by
the English and Spanish tree taggers, respectively.
For each word wi in the text, the final POS tag,
1054
POSF (wi), will be assigned as follows:
1. If POSE(wi) = FW , then POSF (wi) ?
POSS(wi)
2. Else if POSS(wi) = PE, then POSF (wi) ?
POSE(wi)
3. Else if POSE(wi) = ?unknown?, then
POSF (wi)? POSS(wi)
4. Else if POSS(wi) = ?unknown?, then
POSF (wi)? POSE(wi)
5. Else if ProbE(wi) > ProbS(wi), then
POSF (wi)? POSE(wi)
6. Else POSF (wi)? POSS(wi)
This heuristic performs better than the other meth-
ods explored so far, yielding an accuracy of 64.27%.
It seems that knowledge of the taggers can be used
to improve results. However, POS tagging accuracy
is still poor.
4. Selecting POS tags based on automated language
identification. We used two different strategies for
automatically identifying the language at the word
level. One is based on dictionary look-up and the
other is character-based language models. For the
first approach, every word in the text is searched in
the English and Spanish dictionaries. If a word is
found in the English dictionary, then we identify that
word as belonging to English and the POS tags from
the English tagger are used for that word and the
following ones, until a word is found in the Span-
ish dictionary. Similarly, for a word not found in
the English dictionary, but found in the Spanish dic-
tionary, we use the Spanish tags until an English
word is found. Note that this simple heuristic will
always label words that belong to both languages as
English, which is also the case for words not found
in either dictionary. This dictionary-based method
has a language identification accuracy of 94% on the
Spanglish corpus.
The character language models were trained on
the Agence France Presse (AFP) portions of the Gi-
gaword for English and Spanish, respectively. For
each of the words in the Spanglish corpus, we first
decide its language by choosing the one with the
lowest perplexity, calculated using character n-gram
language models, then we use the corresponding
POS tag. We experimented with different language
model orders, with n ranging from 2 to 6, and found
that we achieve the highest accuracy, 81.46%, on
POS tagging using a 5-gram language model. This
5-gram method reached a language identification ac-
curacy of 85% for the Spanglish corpus. However,
the language identification method using dictionary
look-up achieved the best POS tagging result so far:
86.03%. The Spanglish conversation is dominated
by every-day language that is easily found in dic-
tionaries, while the text used to train the charac-
ter based n-gram language models includes vocab-
ulary that is not commonly used in conversations.
This can explain why the simple dictionary look-
up approach yielded better results for our corpus.
Performing manual identification of the language
and sending to the appropriate tagger just the corre-
sponding fragments yields a very high POS tagging
accuracy, 89.72%. This shows that it is important
to deal with the language switches for boosting ac-
curacy. However relying on human annotated lan-
guage tags would be expensive and for some tasks
unfeasible.
6 Machine Learning for POS Tagging
Code-Switched Discourse
From Table 2 we can see that, with the exception of
the language identification heuristic, accuracies are
low for the previous experiments. However, we be-
lieve that we can improve results further by using
Machine Learning (ML) algorithms trained specif-
ically for this task. In this section we describe the
ML setting and present a comparison of the differ-
ent algorithms we tested.
6.1 Approach
The key point is that the features selected for de-
scribing the learning instances are the output from
the English and the Spanish taggers. This scheme
is similar to a stacked classifier approach (Wolpert,
1992), where the final classifier takes as input the
predictions made by the different learners on the first
pass and is trained to select the right tag from them,
or a different one if the right answer is not available.
The gold-standard POS tags are used as the
class label, and instances in this learning task are
described by the following attributes:
1055
1. The word (word)
2. English POS tag (Et)
3. English POS tagger lemma (El)
4. English POS tagger confidence (Ep)
5. Spanish POS tag (St)
6. Spanish POS tagger lemma (Sl)
7. Spanish POS tagger confidence (Sp)
Feature 1 is just the lexical word form as it ap-
pears in the transcript. Features 2 to 4 are generated
by the English Tree tagger, while features 5 to 7 are
generated by the Spanish Tree tagger. Thus all fea-
tures are automatically extracted.
6.2 Results
We evaluated experimentally the idea of using ML
with different learning algorithms in WEKA (Wit-
ten and Frank, 1999). We selected some of the most
widely known algorithms, including Support Vector
Machines (SVM) with a polynomial kernel of ex-
ponent one (Scho?lkopf and Smola, 2002), Weka?s
modified version of Quinlan?s C4.5 (J48) (Quinlan,
1986), Additive Logistic Regression with Decision
Stumps (Logit Boost) (Friedman et al, 1998) and
Naive Bayes. The only parameter we modified was
for J48 ?we enabled the option for reducing error
pruning.
Table 3: POS tagging accuracy of Spanglish text with
different Machine Learning algorithms. Oracle shows
the accuracy achieved when always selecting the right
POS tag from the output of both Tree Taggers. Language
Id shows accuracy of identifying the language and then
choosing the output of the corresponding tagger.
ML Algorithms Mean Accuracy (%) Variance
Naive Bayes 88.50 1.9280
SVM 93.48 1.2784
Logit Boost 93.19 1.4437
J48 91.11 2.1527
Oracle 90.31 -
Language Id 85.80 -
Table 3 shows the average accuracy of 10-fold
cross-validation for each classifier together with the
variance. SVM and Logit Boost performed the best
and the difference between the two algorithms is not
significant according to the paired t-test (P-value =
0.1). For comparison, we show the accuracy of the
10 20 30 40 50 60 70 80 90
80
82
84
86
88
90
92
94
96
98
100
Percentage of training data used
A
cc
ur
ac
y
Figure 1: Effect of different amounts of training data on
accuracy
language identification approach together with the
oracle accuracy. The oracle is the accuracy achieved
when always selecting the right POS tag, when it
is available, from the output of both Tree Taggers.
We did not expect the oracle?s accuracy to be an up-
per bound on the accuracy for the ML learning al-
gorithm. Our intuition is that the ML algorithm can
be trained to identify when the taggers have made
a mistake and what the right answer should be. As
the results show, the ML approach can indeed out-
perform the oracle, and the language identification
method.
In Figure 1 we show the effect of the amount
of training data on the accuracy using Logit Boost.
We selected Logit Boost for this and the follow-
ing experiments since its accuracy is comparable to
SVMs but it is computationally less expensive. We
randomly partitioned the transcription into 10 sub-
groups. Then we used one subgroup as the test set
and the rest for training. Starting with one subgroup
in the training set, we incrementally added one sub-
group to the training set and evaluate the tagging
performance of the test set. We repeated this pro-
cess several times, choosing randomly a new test set
each time. The percentages shown are the average
over all the experiments. With only 10% of the sen-
tences for training we are reaching very good accu-
racy already, as high as that from the strategy based
on language identification. The curve flattens after
1056
Table 4: Accuracy of Logit Boost with different subsets
of attributes. ?X? marks attributes included. Et, El, Ep,
and St, Sl, and Sp are the POS tag, lemma and confi-
dence output by the English and the Spanish POS tagger,
respectively.
word Et El Ep St Sl Sp Accuracy
X X X X ? ? ? 88.80
? X X X ? ? ? 86.22
X ? ? ? X X X 78.59
? ? ? ? X X X 65.28
X X X ? X X ? 92.95
X X ? X X ? X 92.53
X X ? ? X ? ? 91.22
X X X ? ? X ? 89.76
X ? X X ? X X 77.08
? ? X ? ? X ? 74.18
X X ? ? ? ? ? 85.76
X ? ? ? ? ? ? 71.17
? ? ? X ? ? X 24.96
X X X X X ? ? 92.55
X X X X ? ? X 88.89
X ? X ? X ? ? 78.74
X X X X ? X ? 89.62
? X ? ? X X ? 90.76
? ? X X ? X X 75.94
X ? X ? X X X 80.24
X ? ? X X X X 79.13
60% of the training data is used. We do not gain
much by adding more training data after this.
Results shown in Table 3 demonstrate that POS
tagging can be learned effectively based on the at-
tributes described in Subsection 6.1, even if we are
not explicitly adding contextual information. To de-
termine the extent to which each attribute is con-
tributing to the learning task, we performed another
set of experiments where we selected different sub-
sets of the attributes. Table 4 shows the results with
Logit Boost. Overall, the attributes taken from the
English POS tagger are more valuable for this learn-
ing task. If we only take the word form and the fea-
tures from the English Tree tagger (first row in Ta-
ble 4) we are reaching an accuracy that outperforms
all heuristics. Still, there is some valuable infor-
mation provided by the Spanish POS tagger output
since the highest accuracy is achieved by including
the Spanish-based attributes in combination with the
English-based ones. Surprisingly, we can manage to
outperform the oracle by using only three attributes:
the lexical word form and the POS tags from the
English and Spanish tagger (see row 7 in table), or
the POS tags from the monolingual taggers together
with the lemma from the Spanish tagger (see row 4
from bottom to top). We also experimented adding
as an attribute the output of the language identifica-
tion method, but found no significant changes in the
accuracy.
7 Discussion
We analyzed the different results gathered through
the experiments and we present here the most rele-
vant insights.
The first discovery, is that a lot of the errors made
by the oracle, and the other methods as well, are due
to the difficulties inherent in dealing with sponta-
neous speech where fillers, interruption points, hes-
itations, and the like abound. About as much as
20% of the errors made by the oracle are due to
these features. Another roughly 20% is due to un-
known tokens in the transcription, such as mum-
bling, slang words such as ?gonna? and ?wanna?,
or other sounds unintelligible for the human tran-
scriber. For the rest of the analysis we decided to
ignore these types of mistakes for all methods and
focus only on the remaining mistakes. In the case
of the oracle we are left with 445 erroneously POS
tagged words. From those, about 50%, or 233 to
be exact, are errors in sentences with code-switches.
We consider this to be a strong indication of the
complexity that intrasentential switches add to the
task of POS tagging. For the taggers, these sentences
are incomplete, or ill-formed, since they have frag-
ments with foreign words and thus, they fail to iden-
tify them. The rest of the oracle mistakes can not be
attributable to a single cause. Some are fragmented
sentences, and some are due to errors inherent of the
tagger, but nothing is particulary salient about them.
The language identification methods share, of
course, the same mistakes made by the oracle, plus
342 more, for a total of 787 (in the case of the
dictionary-based language identification). The chal-
lenge of POS tagging code-switched text is more ev-
ident for this method. Out of the mistakes made by
the language identification method, 540 lie in sen-
tences with code-switching, that is, nearly 70% of
the mistakes. For 307 of these mistakes the right
1057
POS tag was available from one of the taggers.
Some typical examples of these errors are words that
belong to both languages, such as ?a?, ?no?, ?me?
and ?con?.
The ML approach outperformed both the lan-
guage identification method and the oracle. Analyz-
ing the predictions made by SVM we verified that
out of the 445 errors made by the oracle, SVM cor-
rectly tagged 223, the majority of which are words in
sentences with code-switching (142 words). When
compared against the errors from the method based
on language identification, SVM correctly tagged
481 words out of the 787, 374 of which are words
in sentences with code-switches. In summary, the
ML approach is more robust to code-switched sen-
tences. Note that we did find some errors made by
the ML approach that are not shared by the oracle
or the language identification method, a total of 105.
Some of these mistakes are due to inconsistencies
on the human-annotated tags. For instance, in most
cases slang words such as ?gonna? and ?wanna? are
labeled as unknown words, but we found that these
words were labeled as verbs in a few cases. Not sur-
prisingly this caused the ML algorithm to fail, since
these class labels were misleading. The majority of
the mistakes, however, seem to be due to systematic
mistakes by the POS taggers.
One last remark is regarding our decision to find a
method for successfully exploiting the existing tag-
gers for POS tagging Spanglish text. Our origi-
nal motivation came from the lack of linguistic re-
sources to process Spanglish text. However, we did
train from scratch a sequential model for POS tag-
ging Spanglish, namely Conditional Random Fields
(CRFs) (Lafferty et al, 2001). We used MALLET
(McCallum, 2002) for this experiment and the same
training/testing partitions used in the experiment re-
ported in Table 3. The CRF POS tagger was trained
using capitalization information and the previous to-
ken as context. The average accuracy of this CRF
was 81%, which is lower than the language identifi-
cation heuristic. We believe that this low accuracy is
due to the lack of a representative sample of anno-
tated Spanglish. It will be interesting to see if when
more data becomes available the ML algorithms still
yield the best results.
8 Conclusions
Code-switching is a fresh and exciting research area
that has received little attention in the language pro-
cessing community. Research on this topic has many
interesting applications, including automatic speech
recognition, machine translation, and computer as-
sisted language learning. In this paper we present
preliminary work towards developing a POS tagger
for English-Spanish code-switched text that, to the
best of our knowledge, is the first effort towards this
end.
We explored different heuristics for taking advan-
tage of existing linguistic resources for English and
Spanish with unimpressive results. A simple word-
level language identification strategy outperformed
all heuristics tested. But the best results, even bet-
ter than the oracle, were achieved by using machine
learning using the output of monolingual POS tag-
gers as input features.
In the error analysis we showed that most of
the mistakes made by the language identification
method, and the oracle itself, occur in sentences with
intrasentential code-switching, showing the diffi-
culty of the task. In contrast, our machine learning
approach was less sensitive to the complexity of this
alternation pattern.
There is still a lot of work to do in this area. Our
ongoing efforts include gathering a larger corpus,
with different speakers and conversational styles, as
well as written forms of code-switching from blogs
and Internet forums. In addition, we are exploring
the use of context information. The features we are
currently using to represent each word do not take
into account the context surrounding the word. We
want to test if by using contextual features we can
further improve our results.
In this study we focused on code-switching, but
borrowing is another complex language alternation
pattern that we want the POS tagger to handle. We
are working on developing a special method for
identification and morphological analysis of borrow-
ings. This method will help increase the accuracy of
the POS tagger.
Spanish-English is not the only popular combi-
nation of languages. An interesting line of future
work would be to explore if the method presented
here can be adapted to different language combi-
1058
nations. Moreover, multilingual communities will
code-switch among more than two codes and this
poses fascinating research challenges as well.
Acknowledgements
We are grateful to Ray Mooney, Melissa Sher-
man and the four anonymous reviewers for insight-
ful comments and suggestions. Special thanks to
Brenda Medina and Nicolle Whitman for helping
with some experiments. This research is supported
by the National Science Foundation under grant
0812134.
References
Ardila, A. (2005) Spanglish: an anglicized dialect. His-
panic Journal of Behavioral Sciences, 27(1): 60?81.
Brants, T. (2000) TnT - a statistical part-of-speech tagger.
In Sixth Applied Natural Language Processing Confer-
ence ANLP-2000 Seatle, WA.
Brill, E. (1990) A simple rule-based part-of-speech tag-
ger. In 3rd Conference on Applied Natural Language
Processing, pp. 152?155. Trento, Italy.
Carreras, X. and Padro?, L. (2002) A flexible distributed
architecture for natural language analyzers. In Third
International Conference on Language Resources and
Evaluation, LREC-02, pp. 1813?1817. Las Palmas de
Gran Canaria, Spain.
Charniak, E. (1993) Equations for part-of-speech tag-
ging. In 11th National Conference on AI, pp. 784?789.
Ervin, S. and Osgood, C. (1954) Second language learn-
ing and bilingualism. Journal of abnormal and social
phsychology, Supplement 49, pp. 139?146.
Espinosa, A. (1917) Speech mixture in New Mexico: the
influence of English language on New Mexican Span-
ish. In H. Stevens and H. Bolton, (eds.), The Pacific
Ocean in History, pp. 408?428.
Franco, J.C. and Solorio T. (2007) Baby-steps towards
building a Spanglish language model. In 8th Interna-
tional Conference on Computational Linguistics and
Intelligent Text Processing, CICLing-2007, pp. 75?84.
Mexico City, Mexico.
Friedman, J., Hastie, T., and Tibshirani, R. (1998) Addi-
tive logistic regression: a statistical view of boosting.
Technical Report, Stanford University.
Godfrey, J., Holliman, E. and McDaniel, J. (1992)
Switchboard: Telephone speech corpus for research
development. In ICASSP, pp. 517?520, San Francisco,
CA, USA.
Goyal, P., Mital, Manav R., Mukerjee, A., Raina, Achla
M., Sharma, D., Shukla, P. and Vikram, K. (2003) A
bilingual parser for Hindi, English and code-switching
structures. In Computational Linguistics for South
Asian Languages ?Expanding Synergies with Europe,
EACL-2003 Workshop, Budapest, Hungary.
Grosjean, F. (1982) Life with Two Languages: An Intro-
duction to Bilingualism. Harvard University Press.
Gumperz, J. J. and Hernandez-Chavez, E. (1971) Cogni-
tive aspects of bilingual communication. Oxford Uni-
versity Press, London.
Gumperz, J. J. (1964) Linguistic and social interaction in
two communities. In John J. Gumperz (ed.), Language
in social groups, pp. 151?176, Stanford. Stanford Uni-
versity Press.
Gumperz, J. J. (1971) Bilingualism, bidialectism and
classroom interaction. In Language in social groups,
pp. 311?339, Stanford. Stanford University Press.
Joshi, A. K. (1982) Processing of sentences with in-
trasentential code-switching. In Ja?n Horecky? (ed.),
COLING-82, pp. 145?150, Prague.
Kucera, H. and Francis, W. N. (1967) Computational
analysis of present-day American English. Brown Uni-
versity Press.
Lafferty, J. and McCallum, A. and Pereira F. (2001) Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In 18th ICML,
pp. 282?289. MA, USA.
Lipski, J. M. (1978) Code-switching and the problem of
bilingual competence. In M. Paradis (ed.), Aspects of
bilingualism, pp. 250?264, Columbia, SC. Hornbeam.
McCallum, A. (2002) MALLET: A Machine Learning
for Language Toolkit. Retrieved January 7, 2008 from
http://mallet.cs.umass.edu
Paul, D. B. and Baker, J. M. (1992) The design of the
Wall Street Journal-based CSR corpus. In HLT?91:
workshop on speech and Natural Language pp. 357?
362, Morristown, NJ, USA.
Poplack, S., Sankoff, D. and Miller, C. (1988) The social
correlates and linguistic processes of lexical borrowing
and assimilation. Linguistics, 26(1): 47?104.
Poplack, S. (1980) Sometimes I?ll start a sentence in
Spanish y termino en espan?ol: toward a typology of
code-switching. Linguistics, 18(7/8): 581?618.
Poplack, S. (1981) Syntactic structure and social function
of code-switching. In R. Duran (ed.), Latino discourse
and communicative behavior, pp. 169?184, Norwood,
NJ. Ablex.
Quinlan, J. R. (1986) Induction of decision trees. Ma-
chine Learning, 1: 81?106.
Ratnaparkhi, A. (1996) A maximum entropy model
for part-of-speech tagging. In EMNLP, pp. 133?142,
Philadelphia, PA, May.
Rosner, M. and Farrugia, P. (2007) A tagging algorithm
for mixed language identification in a noisy domain.
1059
In INTERSPEECH 2007, pp. 190?193, Antwerp, Bel-
gium.
Sankoff, D. (1968) Social aspects of multilingualism in
New Guinea. Ph.D. thesis, McGill University.
Sankoff, D. (1981) A formal grammar for codeswitching.
Papers in Linguistics: International Journal of Human
communications, 14(1): 3?46.
Sankoff, D. (1998a) A formal production-based explana-
tion of the facts of code-switching. Bilingualism, Lan-
guage and Cognition, 1: 39?50. Cambridge University
Press.
Sankoff, D. (1998b) The production of code-mixed dis-
course. In 36th ACL, volume I, pp. 8?21, Montreal,
Quebec, Canada.
Schmid, H. (1994) Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, Manchester,
UK.
Scho?lkopf, B. and Smola, A. J. (2002) Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization and Beyond. MIT Press.
Toribio, A. J. (2001a) Accessing Spanish-English code-
switching competence. International Journal of Bilin-
gualism, 5(4):403?436.
Toribio, A. J. (2001b) On the emergence of bilingual
code-switching competence. Bilingual Language and
Cognition, 4(3):203?231.
U.S. Census Bureau. (2003) Language use and En-
glish speaking ability: 2000. Retrieved October 30,
2006 from http://www.census.gov/prod/
2003pubs/c2kbr-29.pdf.
Witten, I. H. and Frank, E. (1999) Data Mining, Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann.
Wolpert, D. H. (1992) Stacked Generalization. Neural
Networks, 5(2):241?259.
1060
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 46?55,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Corpus-Based Approach for the Prediction of Language Impairment in
Monolingual English and Spanish-English Bilingual Children
Keyur Gabani and Melissa Sherman and Thamar Solorio and Yang Liu
Department of Computer Science
The University of Texas at Dallas
keyur,mesh,tsolorio,yangl@hlt.utdallas.edu
Lisa M. Bedore and Elizabeth D. Pen?a
Department of Communication Sciences and Disorders
The University of Texas at Austin
lbedore,lizp@mail.utexas.edu
Abstract
In this paper we explore a learning-based ap-
proach to the problem of predicting language
impairment in children. We analyzed sponta-
neous narratives of children and extracted fea-
tures measuring different aspects of language
including morphology, speech fluency, lan-
guage productivity and vocabulary. Then, we
evaluated a learning-based approach and com-
pared its predictive accuracy against a method
based on language models. Empirical re-
sults on monolingual English-speaking chil-
dren and bilingual Spanish-English speaking
children show the learning-based approach is
a promising direction for automatic language
assessment.
1 Introduction
The question of how best to identify children with
language disorders is a topic of ongoing debate.
One common assessment approach is based on cut-
off scores from standardized, norm-referenced lan-
guage assessment tasks. Children scoring at the
lower end of the distribution, typically more than
1.25 or 1.5 Standard Deviations (SD) below the
mean, are identified as having language impair-
ment (Tomblin et al, 1997). This cutoff-based
approach has several well-documented weaknesses
that may result in both over- and under-identification
of children as language impaired (Plante and Vance,
1994). Recent studies have suggested considerable
overlap between children with language impairment
and their typically developing cohorts on many of
these tasks (e.g., (Pen?a et al, 2006b; Spaulding et
al., 2006)). In addition, scores and cutoffs on stan-
dardized tests depend on the distribution of scores
from the particular samples used in normalizing the
measure. Thus, the validity of the measure for chil-
dren whose demographic and other socioeconomic
characteristics are not well represented in the test?s
normative sample is a serious concern. Finally, most
norm-referenced tests of language ability rely heav-
ily on exposure to mainstream language and expe-
riences, and have been found to be biased against
children from families with low parental education
and socioeconomic status, as well as children from
different ethnic backgrounds (Campbell et al, 1997;
Dollaghan and Campbell, 1998).
This paper aims to develop a reliable and auto-
matic method for identifying the language status of
children. We propose the use of different lexico-
syntactic features, typically used in computational
linguistics, in combination with features inspired
by current assessment practices in the field of lan-
guage disorders to train Machine Learning (ML) al-
gorithms. The two main contributions of this pa-
per are: 1) It is one step towards developing a re-
liable and automatic approach for language status
prediction in English-speaking children; 2) It pro-
vides evidence showing that the same approach can
be adapted to predict language status in Spanish-
English bilingual children.
2 Related Work
2.1 Monolingual English-Speaking Children
Several hypotheses exist that try to explain the gram-
matical deficits of children with Language Impair-
46
ment (LI). Young children normally go through a
stage where they use non-finite forms of verbs in
grammatical contexts where finite forms are re-
quired (Wexler, 1994). This is referred as the op-
tional infinitive stage. The Extended Optional Infini-
tive (EOI) theory (Rice and Wexler, 1996) suggests
that children with LI exhibit the use of a ?young?
grammar for an extended period of time, where
tense, person, and number agreement markers are
omitted.
In contrast to the EOI theory, the surface account
theory (Leonard et al, 1997) assumes that chil-
dren with LI have reduced processing capabilities.
This deficit affects the perception of low stress mor-
phemes, such as -ed, -s, be and do, resulting in an
inconsistent use of these verb morphemes.
Spontaneous narratives are considered as one of
the most ecologically valid ways to measure com-
municative competence (Botting, 2002). They rep-
resent various aspects involved in children?s every-
day communication. Typical measures for sponta-
neous language samples include Mean Length of
Utterance (MLU) in words, Number of Different
Words (NDW), and errors in grammatical morphol-
ogy. Assessment approaches compare children?s
performance on these measures against expected
performance. As mentioned in Section 1, these cut-
off based methods raise questions concerning accu-
racy and bias. Manually analyzing the narratives is
also a very time consuming task. After transcribing
the sample, clinicians need to code for the differ-
ent clinical markers and other morphosyntactic in-
formation. This can take up to several hours for each
child making it infeasible to analyze a large number
of samples.
2.2 Bilingual Spanish-English Speaking
Children
Bilingual children face even more identification
challenges due to their dual language acquisition.
They can be mistakenly labeled as LI due to: 1) the
inadequate use of translations of assessment tools;
2) an over reliance on features specific to English; 3)
a lack of appropriate expectations about how the lan-
guages of a bilingual child should develop (Bedore
and Pen?a, 2008); 4) or the use of standardized
tests where the normal distribution used to compare
language performance is composed of monolingual
children (Restrepo and Gutie?rrez-Clellen, 2001).
Spanish speaking children with LI show differ-
ent clinical markers than English speaking children
with LI. As mentioned above, English speakers have
problems with verb morphology. In contrast, Span-
ish speakers have been found to have problems with
noun morphology, in particular in the use of articles
and clitics (Restrepo and Gutie?rrez-Clellen, 2001;
Jacobson and Schwartz, 2002; Bedore and Leonard,
2005). Bedore and Leonard (2005) also found dif-
ferences in the error patterns of Spanish and related
languages such as Italian. Spanish-speakers tend to
both omit and substitute articles and clitics, while
the dominant errors for Italian-speakers are omis-
sions.
3 Our Approach
We use language models (LMs) in our initial inves-
tigation, and later explore more complex ML algo-
rithms to improve the results. Our ultimate goal is
to discover a highly accurate ML method that can be
used to assist clinicians in the task of LI identifica-
tion in children.
3.1 Language Models for Predicting Language
Impairment
LMs are statistical models used to estimate the prob-
ability of a given sequence of words. They have been
explored previously for clinical purposes. Roark et
al. (2007) proposed cross entropy of LMs trained
on Part-of-Speech (POS) sequences as a measure of
syntactic complexity with the aim of determining
mild cognitive impairment in adults. Solorio and
Liu (2008) evaluated LMs on a small data set in a
preliminary trial on LI prediction.
The intuition behind using LMs is that they can
identify atypical grammatical patterns and help dis-
criminate the population with potential LI from
the Typically Developing (TD) one. We use LMs
trained on POS tags rather than on words. Using
POS tags can address the data sparsity issue in LMs,
and place less emphasis on the vocabulary and more
emphasis on the syntactic patterns.
We trained two separate LMs using POS tags
from the transcripts of TD and LI children, respec-
tively. The language status of a child is predicted
using the following criterion:
47
d(s) =
{ LI if (PPTD(s) > PPLI(s))
TD otherwise
where s represents a transcript from a child, and
PPTD(s) and PPLI(s) are the perplexity values
from the TD and LI LMs, respectively. We used the
SRI Language Modeling Toolkit (Stolcke, 2002) for
training the LMs and calculating perplexities.
3.2 Machine Learning for Predicting Language
Impairment
Although LMs have been used successfully on dif-
ferent human language processing tasks, they are
typically trained and tested on language samples
larger than what is usually collected by clinicians for
the purpose of diagnosing a child with potential LI.
Clinicians make use of additional information be-
yond children?s speech, such as parent and teacher
questionnaires and test scores on different language
assessment tasks. Therefore in addition to using
LMs for children language status prediction, we ex-
plore a machine learning classification approach that
can incorporate more information for better predic-
tion. We aim to identify effective features for this
task and expect this information will help clinicians
in their assessment.
We consider various ML algorithms for the clas-
sification task, including Naive Bayes, Artificial
Neural Networks (ANNs), Support Vector Ma-
chines (SVM), and Boosting with Decision Stumps.
Weka (Witten and Frank, 1999) was used in our ex-
periments due to its known reliability and the avail-
ability of a large number of algorithms. Below we
provide a comprehensive list of features that we ex-
plored for both English and Spanish-English tran-
scripts. We group these features according to the
aspect of language they focus on. Features specific
to Spanish are discussed in Section 5.2.
1. Language productivity
(a) Mean Length of Utterance (MLU) in
words
Due to a general deficit of language abil-
ity, children with LI have been found to
produce language samples with a shorter
MLU in words because they produce
grammatically simpler sentences when
compared to their TD peers.
(b) Total number of words
This measure is widely used when build-
ing language profiles of children for diag-
nostic and treatment purposes.
(c) Degree of support
In spontaneous samples of children?s
speech, it has been pointed out that chil-
dren with potential LI need more encour-
agement from the investigator (Wetherell
et al, 2007) than their TD peers. A sup-
port prompt can be a question like ?What
happened next?? We count the number of
utterances, or turns, of the investigator in-
terviewing the child for this feature.
2. Morphosyntactic skills
(a) Ratio of number of raw verbs to the total
number of verbs
As mentioned previously, children with LI
omit tense markers in verbs more often
than their TD cohorts. For example:
...the boy look into the hole but didn?t
find...
Hence, we include the ratio of the number
of raw verbs to the total number of verbs
as a feature.
(b) Subject-verb agreement
Research has shown that English-speaking
children with LI have difficulties mark-
ing subject-verb agreement (Clahsen and
Hansen, 1997; Schu?tze and Wexler, 1996).
An illustration of subject-verb disagree-
ment is the following:
...and he were looking behind the rocks
As a way of capturing this information
in the machine learning setting, we con-
sider various bigrams of POS tags: noun
and verb, noun and auxiliary verb, pro-
noun and verb, and pronoun and auxiliary
verb. These features are included in a bag-
of-words fashion using individual counts.
Also, we allow a window between these
pairs to capture agreement between sub-
48
ject and verb that may have modifiers in
between.
(c) Number of different POS tags
This feature is the total number of differ-
ent POS tags in each transcript.
3. Vocabulary knowledge
We use the Number of Different Words (NDW)
to represent vocabulary knowledge of a child.
Although such measures can be biased against
children from different backgrounds, we expect
this possible negative effect to decrease as a re-
sult of having a richer pool of features.
4. Speech fluency
Repetitions, revisions, and filled pauses have
been considered indicators of language learn-
ing difficulties (Thordardottir and Weismer,
2002; Wetherell et al, 2007). In this work
we include as features (a) the number of fillers,
such as uh, um, er; and (b) the number of disflu-
encies (abandoned words) found in each tran-
script.
5. Perplexities from LMs
As mentioned in Section 3.1 we trained LMs of
order 1, 2, and 3 on POS tags extracted from
TD and LI children. We use the perplexity val-
ues from these models as features. Addition-
ally, differences in perplexity values from LI
and TD LMs for different orders are used as
features.
6. Standard scores
A standard score, known as a z-score, is the dif-
ference between an observation and the mean
relative to the standard deviation. For this fea-
ture group, we first find separate distributions
for the MLU in words, NDW and total num-
ber of utterances for the TD and LI populations.
Then, for each transcript, we compute the stan-
dard scores based on each of these six distribu-
tions. This represents how well the child is per-
forming relative to the TD and LI populations.
Note that a cross validation setup was used to
obtain the distribution for the TD and LI chil-
dren for training. This is also required for the
LM features above.
4 Experiments with Monolingual Children
4.1 The Monolingual English Data Set
Our target population for this work is children with
an age range of 3 to 6 years old. However, currently
we do not have any monolingual data sets readily
available to test our approach in this age range. In
the field of communication disorders data sharing
is not a common practice due to the sensitive con-
tent of the material in the language samples of chil-
dren, and also due to the large amount of effort and
time it takes researchers to collect, transcribe, and
code the data before they can begin their analysis.
To evaluate our approach we used a dataset from
CHILDES (MacWhinney, 2000) that includes nar-
ratives from English-speaking adolescents with and
without LI with ages ranging between 13 and 16
years old. Even though the age range is outside the
range we are interested in, we believe that this data
set can still be helpful in exploring the feasibility of
our approach as a first step.
This data set contains 99 TD adolescents and 19
adolescents who met the LI profile at one point in
the duration of the study. There are transcripts from
each child for two tasks: a story telling and a spon-
taneous personal narrative. The first task is a picture
prompted story telling task using the wordless pic-
ture book, ?Frog, Where Are You?? (Mayer, 1969).
In this story telling task children first look at the
story book ?to develop a story in memory? and then
are asked to narrate the story. This type of elicitation
task encourages the use of past tense constructions,
providing plenty of opportunities for extracting clin-
ical markers. In the spontaneous personal narrative
task, the child is asked to talk about a person who an-
noys him/her the most and describe the most annoy-
ing features of that person. This kind of spontaneous
personal narrative encourages the participant for the
use of third person singular forms (-s). Detailed in-
formation of this data set can be found in (Wetherell
et al, 2007).
We processed the transcripts using the CLAN
toolkit (MacWhinney, 2000). MOR and POST from
CLAN are used for morphological analysis and POS
tagging of the children?s speech. We decided to use
these analyzers since they are customized for chil-
dren?s speech.
49
Story telling Personal narrative
Method P (%) R (%) F1 (%) P (%) R (%) F1 (%)
Baseline 28.57 10.53 15.38 33.33 15.79 21.43
1-gram LMs 41.03 84.21 55.17 34.21 68.42 45.61
2-gram LMs 75.00 47.37 58.06 55.56 26.32 35.71
3-gram LMs 80.00 21.05 33.33 87.50 36.84 51.85
Table 1: Evaluation of language models on the monolingual English data set.
Story telling Personal narrative
Algorithm P (%) R (%) F1 (%) P (%) R (%) F1 (%)
Naive Bayes 38.71 63.16 48.00 34.78 42.11 38.10
Bayesian Network 58.33 73.68 65.12 28.57 42.11 34.04
SVM 76.47 68.42 72.22 47.06 42.11 44.44
ANNs 62.50 52.63 57.14 50.00 47.37 48.65
Boosting 70.59 63.16 66.67 69.23 47.37 56.25
Table 2: Evaluation of machine learning algorithms on the monolingual English data set.
4.2 Results with Monolingual
English-Speaking Children
The performance measures we use are: precision
(P), recall (R), and F-measure (F1). Here the LI cat-
egory is the positive class and the TD category is the
negative class.
Table 1 shows the results of leave-one-out-cross-
validation (LOOCV) obtained from the LM ap-
proach for the story telling and spontaneous personal
narrative tasks. It also shows results from a base-
line method that predicts language status by using
standard scores on measures that have been asso-
ciated with LI in children (Dollaghan, 2004). The
three measures we used for the baseline are: MLU
in words, NDW, and total number of utterances pro-
duced. To compute this baseline we estimate the
mean and standard deviation of these measures us-
ing LOOCV with the TD population as our norma-
tive sample. The baseline predicts that a child has
LI if the child scores more than 1.25 SD below the
mean on at least two out of the three measures.
Although LMs yield different results for the story
telling and personal narrative tasks, they both pro-
vide consistently better results than the baseline. For
the story telling task the best results, in terms of the
F1 measure, are achieved by a bigram LM (F1 =
58.06%) while for the personal narrative the highest
F1 measure (51.85%) is from the trigram LM. If we
consider precision, both tasks have the same increas-
ing pattern when increasing LM orders. However for
recall that is not the case. In the story telling task,
recall decreases at the expense of higher precision,
but for the personal narrative task, the trigram LM
reaches a better trade-off between precision and re-
call, which yields a high F1 measure. We also evalu-
ated 4-gram LMs, but results did not improve, most
likely because we do not have enough data to train
higher order LMs.
The results for different ML algorithms are shown
in Table 2, obtained by using all features described
in Section 3.2. The feature based approach us-
ing ML algorithms outperformed using only LMs
on both tasks. For the story telling task, SVM
with a linear kernel achieves the best results (F1 =
72.22%), while Boosting with Decision Stumps pro-
vides the best performance (F1 = 56.25%) for the
personal narrative task.
4.3 Feature and Error Analysis
The ML results shown above use the entire feature
set described in Subsection 3.2. The next question
we ask is the effectiveness of different features for
this task. The datasets we are using in our evalua-
tion are very small, especially considering the num-
ber of positive instances. This prevents us from hav-
ing a separate subset of the data for parameter tun-
ing or feature selection. Therefore, we performed
additional experiments to evaluate the usefulness of
individual features. Figure 1 shows the F1 measures
50
 0
 20
 40
 60
 80
 100
1 2 3 4 5 6
F-m
eas
ure
 (%
)
Features
Story TellingPersonal Narrative
Figure 1: Discriminating power of different groups of
features. The numbers on the x-axis correspond to the
feature groups in Section 3.2.
when using different feature groups. The numbers
on the x-axis correspond to the feature groups de-
scribed in Section 3.2. The F1 measure value for
each of the features is the highest value obtained by
running different ML algorithms for classification.
We noticed that for the story telling task, using
perplexity values from LMs (group 5) as a feature
in the ML setting outperforms the LM threshold ap-
proach by a large margin. It seems that having the
perplexity values as well as the perplexity differ-
ences from all the LMs of different orders in the ML
algorithm provides a better estimation of the target
concept.
Only the standard scores (group 6) yield a higher
F1 measure for the personal narrative task than the
story telling one. The majority of the features (5
out of 6 groups) provide higher F1 measures for the
story telling task, which explains the significantly
better results on this task over the personal narrative
in our learning approach. This is consistent with pre-
vious work contrasting narrative genre stating that
the restrictive setting of a story retell is more reveal-
ing of language difficulties than spontaneous narra-
tives, where the subjects have more control on the
content and style (Wetherell et al, 2007).
We also performed some error analysis for some
of the transcripts that were consistently misidenti-
fied by different ML algorithms. In the story telling
task, we find that some LI transcripts are misclassi-
fied as TD because they (1) have fewer fillers, dis-
fluencies, and degree of support; (2) are similar to
the TD transcripts, which is depicted by the perplex-
ity values for these transcripts; or (3) contain higher
MLU in words as compared to their LI peers. Some
of the reasons for classifying transcripts in the TD
category as LI are shorter MLU in words as com-
pared to other TD peers, large number of fillers, and
excessive repetitions of words and phrases unlike the
other TD children. These factors are consistent with
the effective features that we found from Figure 1.
For the personal narrative task, standard scores
(group 6) and language productivity (group 1) have
an important role in classification, as shown in Fig-
ure 1. The TD transcripts that are misidentified have
lower standard scores and MLU in words than those
of their TD peers.
We believe that another source of noise in the
transcripts comes from the POS tags themselves.
For instance, we found that many verbs in present
tense for third person singular are tagged as plural
nouns, which results in a failure to capture subject-
verb agreement.
Lastly, according to the dataset description, chil-
dren in the LI category met the LI criteria at one
stage in their lifetime and some of these children
also had, or were receiving, some educational sup-
port in the school environment at the time of data
collection. This support for children with LI is
meant to improve their performance on language
related tasks, making the automatic classification
problem more complicated. This also raises the
question about the reference label (TD or LI) for
each child in the data set we used. The details about
which children received interventions are not speci-
fied in the dataset description.
5 Experiments with Bilingual Children
In this section we generalize the approach to a
Spanish-English bilingual population. In adapting
the approach to our bilingual population we face two
challenges: first, what shows to be promising for
a monolingual and highly heterogeneous population
may not be as successful in a bilingual setting where
we expect to have a large variability of exposure to
each language; second, there is a large difference
in the mean age of the monolingual setting and that
of our bilingual one. This age difference will result
in different speech patterns. Younger children pro-
51
duce more ill-formed sentences since they are still
in a language acquisition phase. Lastly, the clini-
cal markers in adolescents are geared towards prob-
lems at the pragmatic and discourse levels, while at
younger ages they focus more on syntax and mor-
phology.
For dealing with the first challenge we are extract-
ing language-specific features and hope that by look-
ing at both languages we can reach a good discrim-
ination performance. For the second challenge, our
feature engineering approach has been focused on
younger children from the beginning. We are aiming
to capture the type of morphosyntactic patterns that
can identify LI in young children. In addition, the
samples in the bilingual population are story retells,
and our feature setting showed to be a good match
for this task. Therefore, we expect our approach to
capture relevant classification patterns, even in the
presence of noisy utterances.
5.1 The Bilingual Data Set
The transcripts for the bilingual LI task come from
an on-going longitudinal study of language impair-
ment in Spanish-English speaking children (Pen?a et
al., 2006a). The children in this study were enrolled
in kindergarten with a mean age of about 70 months.
Of the 59 children, 6 were identified as having a
possible LI by an expert in communication disor-
ders, while 53 were identified as TD. Six of the TD
children were excluded due to missing information,
yielding a total of 47 TD children.
Each child told a series of stories based on Mercer
Mayer?s wordless picture books (Mayer, 1969). Two
stories were told in English and two were told in
Spanish, for a total of four transcripts per child. The
books used for English were ?A Boy, A Dog, and
A Frog? and ?Frog, Where Are You?? The books
used for Spanish retelling were ?Frog on His Own?
and ?Frog Goes to Dinner.? The transcripts for each
separate language were combined, yielding one in-
stance per language for each child.
An interesting aspect of the bilingual data is that
the children mix languages in their narratives. This
phenomenon is called code-switching. At the begin-
ning of a retelling session, the interviewer encour-
ages the child to speak the target language if he/she
is not doing so. Once the child begins speaking the
correct language, any code-switching thereafter is
not corrected by the interviewer. Due to this, the En-
glish transcripts contain Spanish utterances and vice
versa. We believe that words in the non-target lan-
guage help contribute to a more accurate language
development profile. Therefore, in our work we de-
cided to keep these code-switched elements. A com-
bined lexicon approach was used to tag the mixed-
language fragments. If a word does not appear in the
target language lexicon, we apply the POS tag from
the non-target language.
5.2 Spanish-Specific Features
Many structural differences exist between Spanish,
a Romance language, and English, a Germanic lan-
guage. Spanish is morphologically richer than En-
glish. It contains a larger number of different verb
conjugations and it uses a two gender system for
nouns, adjectives, determiners, and participles. A
Spanish-speaking child with LI will have difficulties
with different grammatical elements, such as articles
and clitics, than an English-speaking child (Bedore
and Pen?a, 2008). These differences indicate that the
Spanish feature set will need to be tailored towards
the Spanish language.
To account for Spanish-specific patterns we in-
cluded new POS bigrams as features. To capture
the use of correct and incorrect gender and num-
ber marking morphology, we added noun-adjective,
determiner-noun, and number-noun bigrams to the
list of morphosyntactic features.
5.3 Results on Bilingual Children
Results are shown for the baseline and LM threshold
approach for the bilingual data set in Table 3. The
baseline is computed from the same measures as the
monolingual dataset (MLU in words, NDW, and to-
tal utterances).
Compared to Table 1, the values in Table 3
are generally lower than on the monolingual story
telling task. In this inherently difficult task, the bilin-
gual transcripts are more disfluent than the monolin-
gual ones. This could be due to the age of the chil-
dren or their bilingual status. Recent studies on psy-
cholinguistics and language production have shown
that bilingual speakers have both languages active
at speech production time (Kroll et al, 2008) and
it is possible that this may cause interference, espe-
cially in children still in the phase of language acqui-
52
English Spanish
Method P (%) R (%) F1 (%) P (%) R (%) F1 (%)
Baseline 20.00 16.66 18.18 16.66 16.66 16.66
1-gram LMs 40.00 33.33 36.36 17.64 50.00 26.08
2-gram LMs 50.00 33.33 40.00 33.33 16.66 22.22
3-gram LMs 100.00 33.33 50.00 0.00 0.00 -
Table 3: Evaluation of language models on Bilingual Spanish-English data set.
sition. In addition, the LMs in the monolingual task
were trained using more instances per class, possibly
yielding better results.
There are some different patterns between using
the English and Spanish transcripts. In English,
the unigram models provide the least discriminative
value, and the bigram and trigram models improve
discrimination. We also evaluated higher order n-
grams, but did not obtain any further improvement.
We found that the classification accuracy of the LM
approach was influenced by two children with LI
who were consistently marked as LI due to a greater
perplexity value from the TD LM. A further analysis
shows that these children spoke mostly Spanish on
the ?English? tasks yielding larger perplexities from
the TD LM, which was trained from mostly English.
In contrast, the LI LM was created with transcripts
containing more Spanish than the TD one, and thus
test transcripts with a lot of Spanish do not inflate
perplexity values that much.
For Spanish, unigram LMs provide some discrim-
inative usefulness, and then the bigram performance
decreases while the trigram model provides no dis-
criminative value. One reason for this may be that
the Spanish LMs have a larger vocabulary. In the
Spanish LMs, there are 2/3 more POS tags than in
the English LM. This size difference dramatically
increases the possible bigrams and trigrams, there-
fore increasing the number of parameters to esti-
mate. In addition, we are using an ?off the shelf?
POS tagger (provided by CLAN) and this may add
noise in the feature extraction process. Since we do
not have gold standard annotations for these tran-
scripts, we cannot measure the POS tagging accu-
racy. A rough estimate based on manually revis-
ing one transcript in each language showed a POS
tagging accuracy of 90% for English and 84% for
Spanish. Most of the POS tagger errors involve
verbs, nouns and pronouns. Thus while the accu-
 0
 20
 40
 60
 80
 100
1 2 3 4 5 6
F-m
eas
ure
 (%
)
Features
EnglishSpanish
Combined
Figure 2: Discriminating power of different groups of
features for the bilingual population. The numbers on the
x-axis correspond to the feature groups in Section 3.2.
racy might not seem that low, it can still have a ma-
jor impact on our approach since it involves the POS
categories that are more relevant for this task.
Table 4 shows the results from various ML algo-
rithms. In addition to predicting the language status
with the English and Spanish samples separately, we
also combined the English and Spanish transcripts
together for each child, and used all the features
from both languages in order to allow a prediction
based on both samples. The best F1 measure for this
task (60%) is achieved by using the Naive Bayes al-
gorithm with the combined Spanish-English feature
set. This is an improvement over both the separate
English and Spanish trials. The Naive Bayes algo-
rithm provided the best discrimination for the En-
glish (54%) and Combined data sets and Boosting
and SVM provided the best discrimination for the
Spanish set (18%).
5.4 Feature Analysis
Similar to the monolingual dataset, we performed
additional experiments exploring the contribution
of different groups of features. We tested the six
53
English Spanish Combined
Algorithm P (%) R (%) F1 (%) P (%) R (%) F1 (%) P (%) R (%) F1 (%)
ANNs 66.66 33.33 44.44 0.00 0.00 - 100.00 16.66 28.57
SVM 14.28 16.66 15.38 20.00 16.66 18.18 66.66 33.33 44.44
Naive Bayes 60.00 50.00 54.54 0.00 0.00 - 75.00 50.00 60.00
Logistic Regression 25.00 16.66 20.00 - 0.00 - 50.00 33.33 40.00
Boosting 50.00 33.33 40.00 20.00 16.66 18.18 66.66 33.33 44.44
Table 4: Evaluation of machine learning algorithms on the Bilingual Spanish-English data set.
groups of features described in Section 3.2 sepa-
rately. Overall, the combined LM perplexity val-
ues (group 5) provided the best discriminative value
(F1 = 66%). The LM perplexity values performed
the best for English. It even outperformed using all
the features in the ML algorithm, suggesting some
feature selection is needed for this task.
The morpohsyntactic skills (group 2) provided the
best discriminative value for the Spanish language
features, and performed better than the complete
feature set for Spanish. Within group 2, we evalu-
ated different POS bigrams for the Spanish and En-
glish sets and observed that most of the bigram com-
binations by themselves are usually weak predictors
of language status. In the Spanish set, out of all of
the lexical combinations, only the determiner-noun,
noun-verb, and pronoun-verb categories provided
some discriminative value. The determiner-noun
category captured the correct and incorrect gender
marking between the two POS tags. The noun-verb
and pronoun-verb categories covered the correct and
incorrect usage of subject-verb combinations. In-
terestingly enough, the pronoun-verb category per-
formed well by itself, yielding an F1 measure of
54%. There are also some differences in the frequen-
cies of bigram features in the English and Spanish
data sets. For example, there is no noun-auxiliary
POS pattern in Spanish, and the pronoun-auxiliary
bigram appears less frequently in Spanish than in
English because in Spanish the use of personal pro-
nouns is not mandatory since the verb inflection will
disambiguate the subject of the sentence.
The vocabulary knowledge feature (group 3) did
not provide any discriminative value for any of the
language tasks. This may be because bilingual chil-
dren receive less input for each language than a
monolingual child learning one language, or due to
the varied vocabulary acquisition rate in our bilin-
gual population.
6 Conclusions and Future Work
In this paper we present results on the use of LMs
and ML techniques trained on features representing
different aspects of language gathered from spon-
taneous speech samples for the task of assisting
clinicians in determining language status in chil-
dren. First, we evaluate our approach on a monolin-
gual English-speaking population. Next, we show
that this ML approach can be successfully adapted
to a bilingual Spanish-English population. ML al-
gorithms provide greater discriminative power than
only using a threshold approach with LMs.
Our current efforts are devoted to improving pre-
diction accuracy by refining our feature set. We are
working on creating a gold standard corpus of chil-
dren?s transcripts annotated with POS tags. This
data set will help us improve accuracy on our POS-
based features. We are also exploring the use of
socio-demographic features such as the educational
level of parents, the gender of children, and enroll-
ment status on free lunch programs.
Acknowledgments
This work was supported by NSF grant 0812134,
and by grant 5 UL1 RR024982 from NCRR, a com-
ponent of NIH. We also thank the three NAACL re-
viewers for insightful comments on the submitted
version of this paper.
References
Lisa M. Bedore and Laurence B. Leonard. 2005. Verb
inflections and noun phrase morphology in the sponta-
neous speech of Spanish-speaking children with spe-
cific language impairment. Applied Psycholinguistics,
26(2):195?225.
54
Lisa M. Bedore and Elizabeth D. Pen?a. 2008. Assess-
ment of bilingual children for identification of lan-
guage impairment: Current findings and implications
for practice. International Journal of Bilingual Edu-
cation and Bilingualism, 11(1):1?29.
Nicola Botting. 2002. Narrative as a tool for the assess-
ment of linguistic and pragmatic impairments. Child
Language Teaching and Therapy, 18(1):1?21.
Thomas Campbell, Chris Dollaghan, Herbert Needle-
man, and Janine Janosky. 1997. Reducing bias in lan-
guage assessment: Processing-dependent measures.
Journal of Speech, Language, and Hearing Research,
40(3):519?525.
Harald Clahsen and Detlef Hansen. 1997. The grammat-
ical agreement deficit in specific language impairment:
Evidence from therapy experiments. In Myrna Gop-
nik, editor, The Inheritance and Innateness of Gram-
mar, chapter 7. Oxford University Press, New York.
Christine A. Dollaghan and Thomas F. Campbell. 1998.
Nonword repetition and child language impairment.
Journal of Speech, Language, and Hearing Research,
41(5):1136?1146.
Christine A. Dollaghan. 2004. Taxometric analyses of
specific language impairment in 3- and 4-year-old chil-
dren. Journal of Speech, Language, and Hearing Re-
search, 47(2):464?475.
Peggy F. Jacobson and Richard G. Schwartz. 2002.
Morphology in incipient bilingual Spanish-speaking
preschool children with specific language impairment.
Applied Psycholinguistics, 23(1):23?41.
Judith F. Kroll, Chip Gerfen, and Paola E. Dussias. 2008.
Laboratory designs and paradigms: Words, sounds,
sentences. In L. Wei and M. G. Moyer, editors, The
Blackwell Guide to Research Methods in Bilingualism
and Multilingualism, chapter 7. Blackwell Pub.
Laurence B. Leonard, Julia A. Eyer, Lisa M. Bedore,
and Bernard G. Grela. 1997. Three accounts of
the grammatical morpheme difficulties of English-
speaking children with specific language impairment.
Journal of Speech, Language, and Hearing Research,
40(4):741?753.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum, Mahwah, NJ.
Mercer Mayer. 1969. Frog, where are you? Dial Press.
Elizabeth D. Pen?a, Lisa M. Bedore, Ronald B. Gillam,
and Thomas Bohman. 2006a. Diagnostic markers
of language impairment in bilingual children. Grant
awarded by the NIDCD, NIH.
Elizabeth D. Pen?a, Tammie J. Spaulding, and Elena
Plante. 2006b. The composition of normative groups
and diagnostic decision making: Shooting ourselves
in the foot. American Journal of Speech-Language
Pathology, 15(3):247?254.
Elena Plante and Rebecca Vance. 1994. Selection
of preschool language tests: A data-based approach.
Language, Speech, and Hearing Services in Schools,
25(1):15?24.
Mar??a Adelaida Restrepo and Vera F. Gutie?rrez-Clellen.
2001. Article use in Spanish-speaking children with
specific language impairment. Journal of Child Lan-
guage, 28(2):433?452.
Mabel L. Rice and Kenneth Wexler. 1996. Toward tense
as a clinical marker of specific language impairment
in English-speaking children. Journal of Speech and
Hearing Research, 39(6):1239?1257.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for de-
tecting mild cognitive impairment. In Proceedings of
the Workshop on BioNLP 2007, pages 1?8. ACL.
Carson T. Schu?tze and Kenneth Wexler. 1996. Subject
case licensing and English root infinitives. In Proceed-
ings of the 20th Annual Boston University Conference
on Language Development. Cascadilla Press.
Thamar Solorio and Yang Liu. 2008. Using language
models to identify language impairment in Spanish-
English bilingual children. In Proceedings of the
Workshop on BioNLP 2008, pages 116?117. ACL.
Tammie J. Spaulding, Elena Plante, and Kimberly A.
Farinella. 2006. Eligibility criteria for language im-
pairment: Is the low end of normal always appropri-
ate? Language, Speech, and Hearing Services in
Schools, 37(1):61?72.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Elin T. Thordardottir and Susan Ellis Weismer. 2002.
Content mazes and filled pauses on narrative language
samples of children with specific language impair-
ment. Brain and Cognition, 48(2-3):587?592.
J. Bruce Tomblin, Nancy L. Records, Paula Buckwal-
ter, Xuyang Zhang, Elaine Smith, and Marlea O?Brien.
1997. Prevalence of specific language impairment in
kindergarten children. Journal of Speech, Language,
and Hearing Research, 40(6):1245?1260.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007. Narrative in adolescent specific
language impairment (SLI): a comparison with peers
across two different narrative genres. International
Journal of Language and Communication Disorders,
42:583?605(23).
Kenneth Wexler. 1994. Optional infinitives. In David
Lightfoot and Norbert Hornstein, editors, Verb Move-
ment. Cambridge University Press.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
55
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 116?117,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Language Models to Identify Language Impairment in
Spanish-English Bilingual Children
Thamar Solorio
Department of Computer Science
The University of Texas at Dallas
tsolorio@hlt.utdallas.edu
Yang Liu
Department of Computer Science
The University of Texas at Dallas
yangl@hlt.utdallas.edu
1 Introduction
Children diagnosed with Specific Language Impair-
ment (SLI) experience a delay in acquisition of cer-
tain language skills, with no evidence of hearing im-
pediments, or other cognitive, behavioral, or overt
neurological problems (Leonard, 1991; Paradis et
al., 2005/6). Standardized tests, such as the Test for
Early Grammatical Impairment, have shown to have
great predictive value for assessing English speaking
monolingual children. Diagnosing bilingual chil-
dren with SLI is far more complicated due to the
following factors: lack of standardized tests, lack of
bilingual clinicians, and more importantly, the lack
of a deep understanding of bilingualism and its im-
plications on language disorders. In addition, bilin-
gual children often exhibit code-switching patterns
that will make the assessment task even more chal-
lenging. In this paper, we present preliminary re-
sults from using language models to help discrim-
inating bilingual children with SLI from Typically-
Developing (TD) bilingual children.
2 Our Approach
We believe that statistical inference can assist in
the problem of accurately discriminating language
patterns indicative of SLI. In this work, we use
Language Models (LMs) for this task since they are
a powerful statistical measure of language usage
and have been successfully used to solve a variety
of NLP problems, such as text classification, speech
recognition, hand-writing recognition, augmenta-
tive communication for the disabled, and spelling
error detection (Manning and Schu?tze, 1999).
LMs estimate the probability of a word sequence
W = ?w1, ...wk? as follows (using the chain rule):
p(W ) = ?ki=1 p(wi|w1, . . . , wi?1)
which can be approximated using an N-gram as:
p(W ) ? ?ki=1 p(wi|wi?N+1, wi?N+2, ..., wi?1)
Since in our problem we are interested in differ-
entiating syntactic patterns, we will train the LMs
on Part-of-Speech (POS) patterns instead of words.
Using a 3-gram we have:
p(T ) = ?ki=1 p(ti|ti?2, ti?1)
where T = ?t1, t2, ..., tk? is the sequence of POS
tags assigned to the sequence of words W .
The intuition is that the language patterning of an
SLI child will differ from those of TD children at
two different levels: one is at the syntactic level,
and the second one is at the interaction between
both languages in patterns such as code-switching.
Given that the tagset for each language is differ-
ent, by using the POS tags we will incorporate into
the model the syntactic structure together with the
switch points across languages.
We train two LMs with the POS sequences: MT ,
with data from the TD children and MI , with data
from the SLI bilingual children. Once both LMs are
trained, then we can use them to make predictions
over new speech samples of bilingual children. To
determine whether an unobserved speech sample is
likely to belong to a child suffering from SLI, we
will measure the perplexity of the two LMs over the
POS patterns of this new speech sample. We make
the final decision using a threshold:
d(s) =
{
SLI if (PPT (s) ? PPI(s)) > 0
TD otherwise
116
where PPT (s) is the perplexity of the model MT
over the sample s, and PPI(s) is the perplexity of
the model MI over the same sample s. In other
words, if the perplexity of the LM trained on syn-
tactic patterns of children with SLI is smaller than
that of the LM trained on POS patterns of TD chil-
dren, then we will predict that the sample belongs to
a child with SLI.
In a related work, (Roark et al, 2007) explored
the use of cross entropy of LMs trained on POS tags
as a measure of syntactic complexity. Their results
were inconsistent across language tasks, which may
be due to the meaning attached to cross entropy in
this setting. Unlikely patterns are a deviation from
what is expected; they are not necessarily complex
or syntactically rich.
3 Preliminary Results
We empirically evaluated our approach using tran-
scripts that were made available by a speech pathol-
ogist in our team. The TD samples were comprised
of 5 males and 4 females between 48 and 72 months
old. The children were identified as being bilingual
by their parents, and according to parental report,
these children live in homes where Spanish is spo-
ken an average of 46.3% of the time. Language
samples of SLI bilinguals were collected from chil-
dren being served in the Speech and Hearing Clinic
at UTEP. The samples are from two females aged
53 and 111 months. The clients were diagnosed
with language impairment after diagnostic evalua-
tions which were conducted in Spanish. The tran-
scriptions were POS tagged with the bilingual tagger
developed by (Solorio et al, 2008).
Table 1 shows the preliminary results using cross
validation. With the decision threshold outlined
above, out of the 9 TD children, the models were
able to discriminate 7 as TD; from the 2 SLI chil-
dren both were correctly identified as SLI. Although
the results presented above are not conclusive due to
the very small size corpora at hand, they look very
promising. Stronger conclusions can be drawn once
we collect more data.
4 Final Remarks
This paper presents very promising preliminary re-
sults on the use of LMs for discriminating patterns
Table 1: Perplexity and final output of the LMs for the
discrimination of SLI and TD.
Sample PPT (s) PPI(s) d(s)
TD1 14.73 23.12 TD
TD2 11.37 16.17 TD
TD3 18.35 36.58 TD
TD4 30.23 22.27 SLI
TD5 9.42 15.50 TD
TD6 17.37 36.75 TD
TD7 20.32 33.19 TD
TD8 16.40 24.47 TD
TD9 24.35 23.71 SLI
SLI1 20.21 19.10 SLI
SLI2 19.70 12.43 SLI
average TD 18.06 25.75 TD
average SLI 19.95 15.76 SLI
indicative of SLI in Spanish-English bilingual chil-
dren. As more data becomes available, we expect
to gather stronger evidence supporting our method.
Our current efforts involve collecting more samples,
as well as evaluating the accuracy of LMs on mono-
lingual children with and without SLI.
Acknowledgements
Thanks to Bess Sirmon Fjordbak for her contribution to
the project and the three anonymous reviewers for their
useful comments.
References
L. B. Leonard. 1991. Specific language impairment as
a clinical category. Language, Speech, and Hearing
Services in Schools, 22:66?68.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press.
J. Paradis, M. Crago, and F. Genesee. 2005/6. Domain-
general versus domain-specific accounts of specific
language impairment: Evidence from bilingual chil-
drens acquisition of object pronouns. Language Ac-
quisition, 13:33?62.
B. Roark, M. Mitchell, and K. Hollingshead. 2007. Syn-
tactic complexity measures for detecting mild cogni-
tive impairment. In BioNLP 2007: Biological, trans-
lational, and clinical language processing, pages 1?8,
Prague, June. ACL.
T. Solorio, Y. Liu, and B. Medina. 2008. Part-of-speech
tagging English-Spanish code-switched text. Submit-
ted to Natural Language Engineering.
117
Proceedings of NAACL HLT 2007, Companion Volume, pages 45?48,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Filter-Based Approach to Detect End-of-Utterances from Prosody in
Dialog Systems
Olac Fuentes, David Vera and Thamar Solorio
Computer Science Department
University of Texas at El Paso
El Paso, TX 79968
Abstract
We propose an efficient method to detect
end-of-utterances from prosodic information in
conversational speech. Our method is based
on the application of a large set of binary and
ramp filters to the energy and fundamental fre-
quency signals obtained from the speech sig-
nal. These filter responses, which can be com-
puted very efficiently, are used as input to a
learning algorithm that generates the final de-
tector. Preliminary experiments using data ob-
tained from conversations show that an accu-
rate classifier can be trained efficiently and that
good results can be obtained without requiring
a speech recognition system.
1 Introduction
While there have been improvements and a significant
number of methods introduced into the realm of dialog-
based systems, there are aspects of these methods which
can be further improved upon. One such aspect is end-
of-utterance (EOU) detection, which consists of automat-
ically determining when a user has finished his/her turn
and is waiting to receive an answer from the system. Cur-
rent dialog-based systems use a simple pause threshold,
which commonly results in either unnecessary long wait-
ing times or interruptions from the system when the user
makes a pause in the middle of an utterance. These prob-
lems can annoy and discourage users using even simple
dialog systems.
Most previous methods aimed at improving upon
pause thresholds for detecting end-of-utterances use
spectral energy measures (Hariharan et al, 2001; Jia and
Xu, 2002). Other methods use prosodic features with
(Ferrer et al, 2002) and without speech recognition sys-
tems (Ferrer et al, 2003) in conjunction with decision
trees to determine end-of-utterances as quickly as possi-
ble. For this and related problems, the choice of features
is critical. Most common is to use a fixed inventory of
features, chosen based on the linguistics literature and
past experience (Shriberg and Stolcke, 2004). Recently
we have experimented with alternative approaches, in-
cluding features hand-tailored to specific discrimination
problems (Ward and Al Bayyari, 2006) and random ex-
ploration of the feature space (Solorio et al, 2006). In
this paper we explore yet another approach, using a large
battery of very simple and easy to evaluate features.
In this paper we present a method to improve the ac-
curacy that can be obtained in end-of-utterance detection
that uses prosodic information only, without a speech rec-
ognizer. We adapt and extend a filter-based approach
originally proposed in computer graphics (Crow, 1984)
and later exploited successfully in computer vision (Viola
and Jones, 2001) and music retrieval (Ke et al, 2005).
Our approach consists of applying simple filters, which
can be computed in constant time, in order to generate
attributes to be used by a learning algorithm. After the
attributes have been generated, we test different learning
algorithms to detect end-of-utterances. Our results show
that the features yield good results in combination with
several of the classifiers, with the best result being ob-
tained with bagging ensembles of decision trees.
2 Method
The first stage in our system is to extract prosodic infor-
mation from the raw audio signal. Using the audio anal-
ysis tool Didi, the log energy and fundamental frequency
signals are extracted from the source sound wave. After
computing log energy and pitch, we apply a large set of
filters in the time domain to the energy and pitch signals
in order to generate attributes suitable for classification.
We compute the filter responses for both signals at every
time step using three types of filters, each applied at many
different times scales.
The first filter type, shown in Figure 1a), is a two-step
binary filter, split approximately in half. The first half
of the filter consists of a sequence of 1?s. The second
half consists of -1?s. The second filter type is a three-step
binary filter (as shown in Figure 1b)), split in approximate
thirds alternating between 1 and -1. Finally, the third filter
is an upward slope ranging from -1 to 1.
Although simple, these filters, in particular when they
are applied at multiple scales, can characterize most of
the prosodic features that are known to be relevant in
identifying dialog phenomena including raises and falls
in pitch and pauses of different lengths.
The response of any of these filters over the signal at
any time is given by the dot product of the filter and signal
45
5 10 15 20 25 30 35 40?1
?0.8
?0.6
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
a) Type I filter
5 10 15 20 25 30 35 40 45 50 55 60?1
?0.8
?0.6
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
b) Type II filter
5 10 15 20 25 30 35 40?1
?0.8
?0.6
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
c) Type III filter
Figure 1: The three types of filters, the first two being
binary only having values -1 or 1, and the last having an
upward slope from -1 to 1.
window of the same length. Computing this dot product
is slow, especially over larger time window sizes. This
cost is even greater when many filter responses are taken
over the course of the entire signal length.
Given the large number of filters and the size of a nor-
mal audio signal, the straightforward dot-product-based
computation of the filter responses is prohibitively expen-
sive. Fortunately, it is possible to device methods to com-
pute these responses efficiently, as explained in the next
subsection.
2.1 Efficient Filter Computation
This constant time computation of binary filters for two-
dimensional signals was first presented by Crow (Crow,
1984) in the field of computer graphics and later ap-
plied successfully in computer vision (Viola and Jones,
2001). Here we show how that can be adapted to one-
dimensional signals and extended to the case of non-
binary filters, such as ramps.
Let s be the signal corresponding to either the log en-
ergy or the fundamental frequency. Let f be a filter of
size n (in arbitrary time units) and let k be the time in-
stant for which we want to compute the filter response.
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5x 104?3
?2.5
?2
?1.5
?1
?0.5
0
0.5
1
1.5
2
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5x 104?80
?60
?40
?20
0
20
40
60
80
Figure 2: Energy, with mean subtracted, and its corre-
sponding integral signal.
The filter response F is given by
F (s, f, k) =
n?1?
i=0
sk+i ? fi
The standard computation of F takes O(n) operations;
however, for the special case of binary filters like the ones
shown in figures 1a) and 1b), we can compute this re-
sponse in constant time with some preprocessing as fol-
lows. Let I be the integral signal, where each element of
I is given by
Ij =
j?
i=0
si
It can be seen that
k?
i=j
sj = Ik ? Ij?1
Thus this summation can be computed with two ac-
cesses to memory, after pre-computing and storing the
values of I in an array. Figure 2 shows an example of a
signal (with its mean subtracted) and the corresponding
integral signal.
Consider a binary filter f such as the one shown in
1a), f = {1n/2,?1n/2}, that is, f consists of n/2 ones
followed by n/2 negative ones. Then the filter response
of a signal can then be computed in constant time using
three references to the integral signal I:
F (s, f, k) = 2Ik+n/2?1 ? Ik?1 ? Ik+n?1
Similarly, the response to a filter like the one shown in
Figure 1 b), given by f = {1n/3,?1n/3, 1n/3} can be
computed with four memory references.
F (s, f, k) = Ik+n?1? 2Ik+2n/3?1+2Ik+n/3?1? Ik?1
46
The third filter is an upward ramp ranging from -1 to
1. Whereas the binary filters are simple to calculate us-
ing look-up values, and their application to 1-dimensional
signals is a simple adaptation to the 2-D algorithm, a
ramp is more difficult and requires separate preprocess-
ing for filters of different lengths. Regardless, it is still
possible to compute its response in constant time after
preprocessing.
We define a ramp filter of length n as f = {?1, 2n?1 ?
1, 4n?1 ? 1, ..., 1? 2n?1 , 1}. The response to this filter is
F (s, f, k) =
n?1?
i=0
sk+i ? fi
=
n?1?
i=0
( 2in? 1 ? 1)sk+i
= 2n? 1
n?1?
i=0
isk+i ?
n?1?
i=0
sk+i
= 2n? 1
n?1?
i=0
i sk+i ? (In?1 ? Ik?1)
Let ?n?1i=0 i sk+i be denoted by Ank. Clearly, if Ank
can be computed in constant time, then F (s, f, k) can
also be computed in constant time. This can be done, with
a little preprocessing, as follows. Let An0 be computed
in the standard (O(n) time) way,
An0 =
n?1?
i=0
isi
Then to compute values of Ank for k > 0 we first
observe that
Ank = sk+1 + 2sk+2 + . . .+ (n? 1)sk+n?1
and
Ank+1 = sk+2 + 2sk+3 + . . .+ (n? 1)sk+n
From this, we can see that
Ank+1 = Ank ? sk+1 ? sk+2 ? . . .
?sk+n?1 + (n? 1)sk+n
= Ank ?
k+n?1?
i=k+1
si + (n? 1)sk+n
= Ank ? (Ik+n?1 ? Ik) + (n? 1)sk+n
Thus after pre-computing vectors I and An, which
takes linear time in the size of the signal, we can compute
any filter response in constant time. However, while we
can derive all binary-filter responses from vector I , com-
puting ramp-filter responses requires the pre-computation
of a separate An for every filter length n. Nevertheless,
this cost is small compared to the cost of computing a dot
product for every time instant in the input signal.
The integral signal representations are computed from
the two prosodic feature signals, and filter features are
calculated along the timeframe of the signal. Once the fil-
ter responses are obtained, they are used as attributes for
machine learning algorithms, which are used to generate
the final classifiers. The data is then used to train several
learning algorithms, as implemented in Weka (Witten and
Frank, 2005).
3 Experimental Results
Experiments were conducted to test the end-of-utterance
system on pre-recorded conversations, measuring preci-
sion and recall of positive classifications. The conver-
sations used contained speech by both male and female
users to compare the robustness among different vocal
range frequencies.
3.1 Data
The training set of data is derived from about 22 min-
utes of conversations, with the audio split such that each
speaker is on a separate voice channel (see (Hollingsed,
2006)). In 17-minutes worth of these, volunteers were
asked to list eight exits on the east side of El Paso on
Interstate 10. This provided a clear way to measure end-
of-utterances as if a system were prompting users for in-
put. This set of conversations contained a large number of
turn-switches, which also simulated voice-portal systems
well. For most of the time in this set, the same person
(a female) is conducting the quiz. However, the speakers
taking the quiz have distinctly different voices and are
mixed in gender.
Five minutes of the training set were taken from a
casual conversation also containing a male and female
speaker combination. The speakers in this conversa-
tion are different from the speakers in the other dataset.
Adding these data balances the training set, reducing the
probability of the system learning only the specific quiz
format used in much of the training data.
Didi was used to extract the prosodic features, and the
filter responses were computed for each of the three fil-
ter types, in sizes ranging from 200ms to 3 seconds in
increments of 50ms, totaling 342 features per time in-
stance. The class was set to 0 or 1, signaling non-end-
of-utterances and a confirmed end-of-utterances, respec-
tively. 992 instances were created for the experiments,
split equally in two between positive examples of end-of-
utterances, and randomly selected negative examples for
both channels in the source audio.
47
Table 1: Experimental results of using different classifiers and
averaging ten ten-fold-cross-validation evaluations with random
seeds per classifier.
Recall Precision F-Measure
Dec.Stump 0.623 0.705 0.660
Dec.Table 0.768 0.799 0.783
C4.5 0.792 0.800 0.796
Boost(DS) 0.792 0.820 0.806
Bag(REP) 0.850 0.833 0.841
Bag(C4.5) 0.786 0.797 0.791
All instances used for training were randomly cho-
sen. The positive examples were chosen from human-
determined end-of-utterance intervals, which ranged
from the time instant a valid end-of-utterance was
recorded to a point either 1.5 seconds after that instant or
a start-of-utterance that occurred prior to that time. The
negative examples were randomly chosen such that no
time instance was chosen prior to the 3-second-mark of
the audio file used and none was within a marked end-of-
utterance interval.
3.2 Results
Six combinations of classifiers were generated using the
Weka data mining tool. Each of these classifier combina-
tions was tested using 10-fold cross-validation. The re-
sults reflect the average of ten such cross-validation runs,
each using a different random seed. The final classifier
combinations used are Weka?s implementations of deci-
sion stumps, decision tables, C4.5 (Quinlan, 1993) and
ensembles of decision stumps using boosting and C4.5
and reduced error pruning (REP) decision trees (Quinlan,
1987) using bagging.
The experiments performed yield interesting results.
Table 1 shows that, with the exception of decision
stumps, which are perhaps too simple for this task, all
classifiers performed well, which shows that our filters
produce suitable features for classification. The best re-
sults were obtained using bagging and REP trees, but re-
sults for other methods yield similar precision and recall.
It is almost certain that better results can be obtained
using these methods if bleeding across channels in the
audio streams was reduced. The F0 features do a good job
of filtering out possible mistakes in the system due to the
way the frequencies are calculated. However, bleeding
can still mislead the classifiers into perceiving an end-of-
utterance from another speaker.
4 Conclusions and Future Work
We have shown a new filter-based method for detect-
ing end-of-utterances in conversation using only basic
prosodic information. We adapted and extended previ-
ously described methods for fast computation of filter re-
sponses, which allows our system to be trained quickly
and easily permits real-time performance. Preliminary
experiments in the task of classifying windows in dialog
recordings as being end-of-utterances or not have yielded
very promising results using standard classification algo-
rithms, with an f-measure of 0.84.
Present and future work includes evaluating the
method as a component of a real-time dialog system,
where its usefulness at decreasing waiting time can be
tested. We are also working on methods for feature se-
lection and compression to obtain further speedup, and
finally we are experimenting with larger datasets.
Acknowledgement: The authors would like to thank
NSF for partially supporting this work under grants IIS-
0415150 and 0080940.
References
F. C. Crow. 1984. Summed-area tables for texture mapping.
In Proceedings of the 11th Annual Conference on Computer
Graphics and Interactive Techniques.
L. Ferrer, E. Shriberg, and A. Stolcke. 2002. Is the speaker
done yet? Faster and more accurate end-of-utterance detec-
tion using prosody. In Proceedings of ICSLP.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-based
approach to end-of-utterance detection that does not require
speech recognition. In Proceedings of IEEE ICASSP.
R. Hariharan, J. Hakkinen, and K. Laurila. 2001. Robust end-
of-utterance detection for real-time speech recognition appli-
cations. In Proceedings of IEEE ICASSP.
T. K. Hollingsed. 2006. Responsive behavior in tutorial spoken
dialogues. Master?s thesis, University of Texas at El Paso.
C. Jia and B. Xu. 2002. An improved entropy-based endpoint
detection algorithm. In Proceedings of ISCSLP.
Y. Ke, D. Hoiem, and R. Sukthankar. 2005. Computer vision
for music identification. In Proceedings of IEEE CVPR.
J. R. Quinlan. 1987. Simplifying decision trees. International
Journal of Man-Machine Studies, 27.
J. R. Quinlan. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufman.
E. Shriberg and A. Stolcke. 2004. Direct modeling of prosody:
An overview of applications in automatic speech processing.
In Proceedings of ICSP.
T. Solorio, O. Fuentes, N. Ward, and Y. Al Bayyari. 2006.
Prosodic feature generation for back-channel prediction. In
Proceedings of Interspeech.
P. Viola and M. Jones. 2001. Rapid object detection using a
boosted cascade of simple features. In Proceedings of IEEE
CVPR.
N. Ward and Y. Al Bayyari. 2006. A case study in the identi-
fication of prosodic cues to turn-taking: Back-channeling in
Arabic. In Proceedings of Interspeech.
I. H. Witten and E. Frank. 2005. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufman.
48
Improving Name Discrimination: A Language Salad Approach
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
Roxana Angheluta
Attentio SA
B-1030 Brussels, Belgium
roxana@attentio.com
Zornitsa Kozareva
Dept. de Lenguajes y Sistemas Informa?ticos
University of Alicante
03080 Alicante, Spain
zkozareva@dlsi.ua.es
Thamar Solorio
Department of Computer Science
University of Texas at El Paso
El Paso, TX 79902 USA
tsolorio@utep.edu
Abstract
This paper describes a method of discrim-
inating ambiguous names that relies upon
features found in corpora of a more abun-
dant language. In particular, we discrim-
inate ambiguous names in Bulgarian, Ro-
manian, and Spanish corpora using infor-
mation derived from much larger quan-
tities of English data. We also mix to-
gether occurrences of the ambiguous name
found in English with the occurrences of
the name in the language in which we are
trying to discriminate. We refer to this as
a language salad, and find that it often re-
sults in even better performance than when
only using English or the language itself
as the source of information for discrimi-
nation.
1 Introduction
Name ambiguity is a problem that is increasing
in complexity and scope as online information
sources grow and expand their coverage. Like
words, names are often ambiguous and can refer
to multiple underlying entities or concepts. Web
searches for names can often return results asso-
ciated with multiple people or organizations in a
disorganized and unclear fashion. For example,
the top 10 results of a Google search for George
Miller includes a mixture of entries for two dif-
ferent entities, one a psychology professor from
Princeton University and the other the director of
the film Mad Max.1
Name discrimination takes some number of
contexts that include an ambiguous name, and di-
vides them into groups or clusters, where the con-
1Search conducted January 4, 2006.
texts in each cluster should ideally refer to the
same underlying entity (and each cluster should
refer to a different entity). Thus, if we are given
10,000 contexts that include the name John Smith,
we would want to divide those contexts into clus-
ters corresponding to each of the different under-
lying entities that share that name.
We have developed an unsupervised method of
name discrimination (Pedersen et al, 2005). We
have shown the method to be language indepen-
dent (Pedersen et al, 2006), which is to say we
can apply it to English contexts as easily as we
can apply it to Romanian or French. However,
we have observed that there are situations where
the number of contexts in which an ambiguous
name occurs is relatively small, perhaps because
the name itself is unusual, or because the quantity
of data available for language is limited in general.
These problems of scarcity can make it difficult to
apply these methods and discriminate ambiguous
names, especially in languages with fewer online
resources.
This paper presents a method of name discrim-
ination is based on using a larger number of con-
texts in English that include an ambiguous name,
and applying information derived from these con-
texts to the discrimination of that name in another
language, where there are many fewer contexts.
We also show that mixing English contexts with
the contexts to be discriminated can result in a
performance improvement over only using the En-
glish or the original contexts alone.
2 Discrimination by Clustering Contexts
Our method of name discrimination is described in
more detail in (Pedersen et al, 2005), but in gen-
eral is based on an unsupervised approach to word
sense discrimination introduced by (Purandare and
25
Pedersen, 2004), which builds upon earlier work
in word sense discrimination, including (Schu?tze,
1998) and (Pedersen and Bruce, 1997).
Our method treats each occurrence of an am-
biguous name as a context that is to be clustered
with other contexts that also include the same
name. In this paper, each context consists of about
50 words, where the ambiguous name is generally
in the middle of the context. The goal is to cluster
similar contexts together, based on the presump-
tion that the occurrences of a name that appear
in similar contexts will refer to the same underly-
ing entity. This approach is motivated by both the
distributional hypothesis (Harris, 1968) and the
strong contextual hypothesis (Miller and Charles,
1991).
2.1 Feature Selection
The contexts to be clustered are represented by
lexical features which may be selected from either
the contexts being clustered, or from a separate
corpus. In this paper we use both approaches. We
cluster the contexts based on features identified in
those very same contexts, and we also cluster the
contexts based on features identified in a separate
set of data (in this case English). We explore the
use of a mixed feature selection strategy where we
identify features both from the data to be clustered
and the separate corpus of English text. Thus, our
feature selection data may come from one of three
sources: the contexts to be clustered (which we
will refer to as the evaluation contexts), English
contexts which include the same name but are not
to be clustered, and the combination of these two
(our so-called Language Salad or Mix).
The lexical features we employ are bigrams,
that is consecutive words that occur together in the
corpora from which we are identifying features. In
this work we identify bigram features using Point-
wise Mutual Information (PMI). This is defined as
the log of the ratio of the observed frequency with
which the two words occur together in the feature
selection data, to the expected number of times
the two words would occur together in a corpus if
they were independent. This expected value is es-
timated simply by taking the product of the num-
ber of times the two words occur individually, and
dividing this by the total number of bigrams in the
feature selection data. Thus, larger values of PMI
indicate that the observed frequency of the bigram
is greater than would be expected if the two words
were independent.
In these experiments we take the top 500 ranked
bigrams that occur five or more times in the feature
selection data. We also exclude any bigram from
consideration that is made up of one or two stop
words, which are high frequency function words
that have been specified in a manually created list.
Note that with smaller numbers of contexts (usu-
ally 200 or fewer), we lower the frequency thresh-
old to two or more.
In general PMI is known to have a bias towards
pairs of words (bigrams) that occur a small num-
ber of times and only with each other. In this work
that is a desirable quality, since that will tend to
identify pairs of words that are very strongly as-
sociated with each other and also provide unique
discriminating information.
2.2 Context Representation
Once the bigram features have been identified,
then the contexts to be clustered are represented
using second order co-occurrences that are de-
rived from those bigrams. In general a second
order co-occurrence is a pair of words that may
not occur with each other, but that both occur fre-
quently with a third word. For example, garden
and fire may not occur together often, but both
commonly occur with hose. Thus, garden hose
and fire hose represent first order co?occurrences,
and garden and fire represent a second order co?
occurrence.
The process of creating the second order repre-
sentation has several steps. First, the bigram fea-
tures identified by PMI (the top ranked 500 bi-
grams that have occurred 5 or more times in the
feature selection data) are used to create a word
by word co?occurrence matrix. The first word in
each bigram represents a row in the matrix, and the
second word in each bigram represents a column.
The cells in the matrix contain the PMI scores.
Note that this matrix is not symmetric, and that
there are many words that only occur in either a
row or a column (and not both) because they tend
to occur as the first or second word in a bigram.
For example, President might tend to be a first
word in a bigram (e.g., President Clinton, Presi-
dent Putin), whereas last names will tend to be the
second word.
Once the co?occurrence matrix is created, then
the contexts to be clustered can be represented.
Each word in the context is checked to see if it
26
has a corresponding row (i.e., vector) in the co?
occurrence matrix. If it does, that word is replaced
in the context by the row from the matrix, so that
the word in the context is now represented by the
vector of words with which it occurred in the fea-
ture selection data. If a word does not have a corre-
sponding entry in the co?occurrence matrix, then
it is simply removed from the context. After all
the words in the context are checked, then all of
the vectors that are selected are averaged together
to create a vector representation of the context.
Then these contexts are clustered into a pre?
specified number of clusters using the k?means
algorithm. Note that we are currently develop-
ing methods to automatically select the number of
clusters in the data (e.g., (Pedersen and Kulkarni,
2006)), although we have not yet applied them to
this particular work.
3 The Language Salad
In this paper, we explore the creation of a second
order representation for a set of evaluation con-
texts using three different sets of feature selection
data. The co?occurrence matrix may be derived
from the evaluation contexts themselves, or from
a separate set of contexts in a different language,
or from the combination of these two (the Salad or
Mix).
For example, suppose we have 100 Romanian
evaluation contexts that include an ambiguous
name, and that same name also occurs 10,000
times in an English language corpus.2 Our goal
is to cluster the 100 Romanian contexts, which
contain all the information that we have about the
name in Romanian. While we could derive a sec-
ond order representation of the contexts, the re-
sulting co?occurrence matrix would likely be very
small and sparse, and insufficient for making good
discrimination decisions. We could instead rely
on first order features, that is look for frequent
words or bigrams that occur in the evaluation con-
texts, and try and find evaluation contexts that
share some of the same words or phrases, and clus-
ter them based on this type of information. How-
ever, again, the small number of contexts available
would likely result in very sparse representations
for the contexts, and unreliable clustering results.
Thus, our method is to derive a co?occurrence
matrix from a language for which we have many
2We assume that the names either have the same spelling
in both languages, or that translations are readily available.
occurrences of the ambiguous name, and then use
that co?occurrence matrix to represent the evalua-
tion contexts. This relies on the fact that the eval-
uation contexts will contain at least a few names
or words that are also used in the larger corpus (in
this case English). In general, we have found that
while this is not always true, it is often the case.
We have also experimented with combining the
English contexts with the evaluation contexts, and
building a co?occurrence matrix based on this
combined or mixed collection of contexts. This
is the language salad that we refer to, a mixture of
contexts in two different languages that are used to
derive a representation of the evaluation contexts.
4 Experimental Data
We use data in four languages in these experi-
ments, Bulgarian, English, Romanian, and Span-
ish.
4.1 Raw Corpora
The Romanian data comes from the 2004 archives
of the newspaper Adevarul (The Truth)3. This is a
daily newspaper that is among the most popular in
Romania. While Romanian normally has diacrit-
ical markings, this particular newspaper does not
include those in their online edition, so the alpha-
bet used was the same as English.
The Bulgarian data is from the Sega 2002 news
corpus, which was originally prepared for the
CLEF competition.4 This is a corpus of news arti-
cles from the Newspaper Sega5, which is based in
Sofia, Bulgaria. The Bulgarian text was translit-
erated (phonetically) from Cyrillic to the Roman
alphabet. Thus, the alphabet used was the same
as English, although the phonetic transliteration
leads to fewer cognates and borrowed English
words that are spelled exactly the same as in En-
glish text.
The Spanish corpora comes from the Spanish
news agency EFE from the year 1994 and 1995.
This collection was used in the Question Answer-
ing Track at CLEF-2003, and also for CLEF-2005.
This text is represented in Latin-1, and includes
the usual accents that appear in Spanish.
The English data comes from the GigaWord
corpus (2nd edition) that is distributed by the Lin-
guistic Data Consortium. This consists of more
3http://www.adevarulonline.ro/arhiva
4http://www.clef-campaign.org
5http://www.segabg.com
27
than 2 billion words of newspaper text that comes
from five different news sources between the years
1994 and 2004. In fact, we subdivide the English
data into three different corpora, where one is from
2004, another from 2002, and the third from 1994-
95, so that for each of the evaluation languages
(Bulgarian, Spanish, and Romanian) we have an
English corpus from the same time period.
4.2 Evaluation Contexts
Our experimental data consists of evaluation con-
texts derived from the Bulgarian, Romanian, and
Spanish corpora mentioned above. We also have
English corpora that includes the same ambiguous
names as found in the evaluation contexts.
In order to quickly generate a large volume of
experimental data, we created evaluation contexts
from the corpora for each of our four languages
by conflating together pairs of well known names
or places, and that are generally not highly am-
biguous (although some might be rather general).
For example, one of the pairs of names we con-
flate is George Bush and Tony Blair. To do that,
every occurrence of both of these names is con-
verted to an ambiguous form (GB TB, for exam-
ple), and the discrimination task is to cluster these
contexts such that their original and correct name
is re?discovered. We retain a record of the orig-
inal name for each occurrence, so as to evaluate
the results of our method. Of course we do not use
this information anywhere in the process outside
of evaluation.
The following pairs of names were conflated in
all four of the languages: George Bush-Tony Blair,
Mexico-India, USA-Paris, Ronaldo-David Beck-
ham (2002 and 2004), Diego Maradona-Roberto
Baggio (1994-95 only), and NATO-USA. Note
that some of these names have different spellings
in some of our languages, so we look for and con-
flate the native spelling of the names in the differ-
ent language corpora. These pairs were selected
because they occur in all four of our languages,
and they represent name distinctions that are com-
monly of interest, that is they represent ambiguity
in names of people and places. With these pairs
we are also following (Nakov and Hearst, 2003)
who suggest that if one is introducing ambiguity
by creating pseudo?words or conflating names,
then these words should be related in some way
(in order to avoid the creation of very sharp or ob-
vious sense distinctions).
4.3 Discussion
For each of the three evaluation languages (Bul-
garian, Romanian, and Spanish) we have contexts
for five different name conflate pairs that we wish
to discriminate. We have corresponding English
contexts for each evaluation language, where the
dates of both are approximately the same. This
temporal consistency between the evaluation lan-
guage and English is important because the con-
texts in which a name is used may change over
time. In 1994, for example, Tony Blair was not
yet Prime Minister of England (he became PM in
1997), and references to George Bush most likely
refer to the US President who served from 1988
until 1992, rather than the current US President
(who began his term in office in 2001). In 1994
the current (as of 2006) US President had just been
elected governor of Texas, and was not yet a na-
tional figure. This points out that George Bush is
an example of an ambiguous name, but our ob-
servation has been that in the 2002 and 2004 data
(Romanian and Bulgarian) nearly all occurrences
are associated with the current president, and that
most of the occurrences in 1994-95 (Spanish) re-
fer to the former US President. This illustrates
an important point: it is necessary to consider the
perspective represented by the different corpora.
There is little reason to expect that news articles
from Spain in 1994 and 1995 would focus much
attention on the newly elected governor of Texas
in the United States.
Tables 1, 2, and 3 show the number of contexts
that have been collected for each name conflate
pair. For example, in Table 1 we see that there are
746 Bulgarian contexts that refer to either Mex-
ico or India, and that of these 51.47% truly re-
fer to Mexico, and 48.53% to India. There are
149,432 English contexts that mention Mexico or
India, and the Mix value shown is simply the sum
of the number of Bulgarian and English contexts.
In general these tables show that the English
contexts are much larger in number, however,
there are a few exceptions with the Spanish data.
This is because the EFE corpus is relatively large
as compared to the Bulgarian and Romanian cor-
pora, and provides frequency counts that are in
some cases comparable to those in the English cor-
pus.
28
5 Experimental Methodology
For each of the three evaluation languages (Bul-
garian, Romanian, Spanish) there are five name
conflate pairs. The same name conflate pairs
are used for all three languages, except for
Diego Maradona-Roberto Baggio which is only
used with Spanish, and Ronaldo-David Beckham,
which is only used with Bulgarian and Romanian.
This is due to the fact that in 1994-95 (the era
of the Spanish data) neither Ronaldo nor David
Beckham were as famous as they later became, so
they were mentioned somewhat less often than in
the 2002 and 2004 corpora. The other four name
conflate pairs are used in all of the languages.
For each name conflate pair we create a second
order representation using three different sources
of features selection data: the evaluation contexts
themselves, the corresponding English contexts,
and then the mix of the evaluation contexts and the
English contexts (the Mix). The objective of these
experiments is to determine which of these sources
of feature selection data results in the highest F-
Measure, which is the harmonic mean of the pre-
cision and recall of an experiment.
The precision of each experiment is the num-
ber of evaluation contexts clustered correctly, di-
vided by the number of contexts that are clustered.
The clustering algorithm may choose not to assign
every context to a cluster, which is why that de-
nominator may not be the same as the number of
evaluation contexts. The recall of each experiment
is the the number of correctly clustered evaluation
contexts divided by the total number of evaluation
contexts. Note that for each of the three variations
for each name conflate pair experiment exactly the
same evaluation language contexts are being dis-
criminated, all that is changing in each experiment
is the source of the feature selection data. Thus the
F-measures for a name conflate pair in a particular
language can be compared directly. Note however
that the F-measures across languages are harder to
compare directly, since different evaluation con-
texts are used, and different English contexts are
used as well.
There is a simple baseline that can be used as a
point of comparison, and that is to place all of the
contexts for each name conflate pair into one clus-
ter, and say that there is no ambiguity. If that is
done, then the resulting F-Measure will be equal
to the majority percentage of the true underlying
entity as shown in Tables 1, 2, and 3. For exam-
ple, for Bulgarian, if the 746 Bulgarian contexts
for Mexico and India are all put into the same clus-
ter, the resulting F-Measure would be 51.47%, be-
cause we would simply assign all the contexts in
the cluster to the more common of the two entities,
which is Mexico in this case.
6 Experimental Results
Tables 1, 2, and 3 show the results for our exper-
iments, language by language. Each table shows
the results for the 15 experiments done for each
language: five name conflate pairs, each with
three different sources of feature selection data.
The row labeled with the name of the evalua-
tion language reports the F-Measure for the eval-
uation contexts (whose number of occurrences is
shown in the far right column) when the fea-
ture selection data is the evaluation contexts them-
selves. The rows labeled English and Mix report
the F-Measures obtained for the evaluation con-
texts when the feature selection data is the English
contexts, or the Mix of the English and evaluation
contexts.
6.1 Bulgarian Results
The Bulgarian results are shown in Table 1. Note
that the number of contexts for English is consid-
erably larger than for Bulgarian for all five name
conflate pairs. The Bulgarian and English data
came from 2002 news reports.
The Mix of feature selection data results in the
best performance for three of the five name con-
flate pairs: George Bush - Tony Blair, Ronaldo -
David Beckham, and NATO - USA. For remain-
ing two name conflate pairs, just using the Bul-
garian evaluation contexts results in the highest F-
Measure (Mexico-India, USA-Paris).
We believe that this may be partially due to the
fact that the two cases where Bulgarian leads to the
best results are for very general or generic underly-
ing entities: Mexico and India, and then the USA
and Paris. In both cases, contexts that mention
these entities could be discussing a wide range of
topics, and the larger volumes of English data may
simply overwhelm the process with a huge num-
ber of second order features. In addition, it may
be that the English and Bulgarian corpora contain
different content that reflects the different interests
of the original readership of this text. For example,
news that is reported about India might be rather
different in the United States (the source of most
29
Table 1: Bulgarian Results (2002): Feature Selec-
tion Data, F-Measure, and Number of Contexts
George Bush (73.43) - Tony Blair (26.57)
Mix 68.37 11,570
Bulgarian 55.78 651
English 36.15 10,919
Mexico (51.47) - India (48.53)
Bulgarian 70.97 746
Mix 55.01 150,178
English 48.15 149,432
USA (79.53) - Paris (20.47)
Bulgarian 58.67 3,283
Mix 51.68 56,044
English 49.66 52,761
Ronaldo (61.25) - David Beckham (38.75)
Mix 64.88 8,649
Bulgarian 52.75 320
English 48.11 8,329
NATO (87.37) - USA (12.63)
Mix 75.44 54,193
Bulgarian 65.92 3,770
English 60.44 50,423
of the English data) than in Bulgaria. Thus, the
use of the English corpora might not have been
as helpful in those cases where the names to be
discriminated are more global figures. For exam-
ple, Tony Blair and George Bush are probably in
the news in the USA and Bulgaria for many of the
same reasons, thus the underlying content is more
comparable than that of the more general entities
(like Mexico and India) that might have much dif-
ferent content associated with them.
We observed that Bulgarian tends to have fewer
cognates or shared names with English than do
Romanian and English. This is due to the fact
that the Bulgarian text is transliterated. This may
account for the fact that the English-only results
for Bulgarian are very poor, and it is only in com-
bination with the Bulgarian contexts that the En-
glish contexts show any positive effect. This sug-
gests that there are only a few words in the Bulgar-
ian contexts that also occur in English, but those
that do have a positive impact on clustering per-
formance.
6.2 Romanian Results
The Romanian results are shown in Table 2. The
Romanian and English contexts come from 2004.
Table 2: Romanian Results (2004): Feature Selec-
tion Data, F-Measure, and Number of Contexts
Tony Blair (72.00) - George Bush (28.00)
English 64.23 11,616
Mix 54.31 11,816
Romanian 50.75 200
India (53.66) - Mexico (46.34)
Romanian 50.93 82
English 47.30 88,247
Mix 42.55 88,329
USA (60.29) - Paris (39.71)
English 59.05 45,346
Romanian 58.76 700
Mix 57.91 46,046
David Beckham (55.56) - Ronaldo (44.44)
Mix 81.00 4,365
English 70.85 4,203
Romanian 52.47 162
NATO (58.05) - USA (41.95)
Mix 60.48 43,508
Romanian 51.20 1,168
English 38.91 42,340
The Mix of Romanian and English contexts for
feature selection results in improvements for two
of the five pairs (David Beckham - Ronaldo, and
NATO - USA). The use of English contexts only
provides the best results for two other pairs (Tony
Blair - George Bush, and USA - Paris, although in
the latter case the difference in the F-Measures that
result from the three sources of data is minimal).
There is one case (Mexico-India) where using the
Romanian contexts as feature selection data re-
sults in a slightly better F-measure than when us-
ing English contexts.
The improvement that the Mix shows for David
Beckham-Ronaldo is significant, and is perhaps
due to fact that in both English and Romanian text,
the content about Beckham and Ronaldo is simi-
lar, making it more likely that the mix of English
and Romanian contexts will be helpful. However,
it is also true that the Mix results in a significant
improvement for NATO-USA, and it seems likely
that the local perspective in Romania and the USA
would be somewhat different on these two entities.
However, NATO-USA has a relatively large num-
ber of contexts in Romanian as well as English, so
perhaps the difference in perspective had less of
an impact in those cases where the number of Ro-
30
Table 3: Spanish Results (1994-95): Feature Se-
lection Data, F-Measure, and Number of Contexts
George Bush (75.58) - Tony Blair (24.42)
Mix 78.59 2,353
Spanish 64.45 1,163
English 54.29 1,190
D. Maradona (51.55) - R. Baggio (48.45)
English 67.65 1,588
Mix 61.35 3,594
Spanish 60.70 2,006
India (92.34) - Mexico (7.66)
English 72.76 19,540
Spanish 66.57 2,377
Mix 61.54 21,917
USA (62.30) - Paris (37.70)
Spanish 69.31 1,000
English 64.30 17,344
Mix 59.40 18,344
NATO (63.86) - USA (36.14)
Spanish 62.04 2,172
Mix 58.47 27,426
English 56.00 25,254
manian contexts is much smaller (as is the case for
Beckham and Ronaldo).
6.3 Spanish Results
The Spanish results are shown in Table 3. The
Spanish and English contexts come from 1994-
1995, which puts them in a slightly different his-
torical era than the Bulgarian and Romanian cor-
pora.
Due to this temporal difference, we used Diego
Maradona and Roberto Baggio as a conflated pair,
rather than David Beckham and Ronaldo, who
were much younger and somewhat less famous at
that time. Also, Ronaldo is a highly ambiguous
name in Spanish, as it is a very common first name.
This is true in English text as well, although casual
inspection of the English text from 2002 and 2004
(where the Ronaldo-Beckham pair was included
experimentally) reveals that Ronaldo the soccer
player tends to occur more so than any other single
entity named Ronaldo, so while there is a bit more
noise for Ronaldo, there is not really a significant
ambiguity.
For the Spanish results we only note one pair
(George Bush - Tony Blair) where the Mix of En-
glish and Spanish results in the best performance.
This again suggests that the perspective of the
Spanish and English corpora were similar with re-
spect to these entities, and their combination was
helpful. In two other cases (Maradona-Baggio,
India-Mexico) English only contexts achieve the
highest F-Measure, and then in the two remaining
cases (USA-Paris, NATO-USA) the Spanish con-
texts are the best source of features.
Note that for Spanish we have reasonably large
numbers of contexts (as compared to Bulgarian
and Romanian). Given that, it is especially inter-
esting that English-only contexts are the most ef-
fective in two of five cases. This suggests that this
approach may have merit even when the evalua-
tion language does not suffer from problems of ex-
treme scarcity. It may simply be that the informa-
tion in the English corpora provides more discrim-
inating information than does the Spanish, and that
it is somewhat different in content than the Span-
ish, otherwise we would expect the Mix of English
and Spanish contexts to do better than being most
accurate for just one of five cases.
7 Discussion
Of the 15 name conflate experiments (five pairs,
three languages), in only five cases did the use of
the evaluation contexts as a source of feature se-
lection data result in better F-Measure scores than
did either using the English contexts alone or as a
Mix with the evaluation language contexts. Thus,
we conclude that there is a clear benefit to using
feature selection data that comes from a different
language than the one for which discrimination is
being performed.
We believe that this is due to the volume of
the English data, as well as to the nature of the
name discrimination task. For example, a per-
son is often best described or identified by observ-
ing the people he or she tends to associate with,
or the places he or she visits, or the companies
with which he or she does business. If we ob-
serve that George Miller and Mel Gibson occur
together, then it seems we can safely infer that
George Miller the movie director is being referred
to, rather than George Miller the psychologist and
father of WordNet.
This argument might suggest that first order
co?occurrences would be sufficient to discrimi-
nate among the names. That is, simply group the
evaluation contexts based on the features that oc-
cur within them, and essentially cluster evaluation
31
contexts based on the number of features they have
in common with other evaluation contexts. In fact,
results on word sense discrimination (Purandare
and Pedersen, 2004) suggest that first order rep-
resentations are more effective with larger number
of context than second order methods. However,
we see examples in these results that suggests this
may not always be the case. In the Bulgarian re-
sults, the largest number of Bulgarian contexts are
for NATO-USA, but the Mix performs quite a bit
better than Bulgarian only. In the case of Roma-
nian, again NATO-USA has the largest number of
contexts, but the Mix still does better than Roma-
nian only. And in Spanish, Mexico-India has the
largest number of contexts and English-only does
better. Thus, even in cases where we have an abun-
dant number of evaluation contexts, the indirect
nature of the second order representation provides
some added benefit.
We believe that the perspective of the news or-
ganizations providing the corpora certainly has an
impact on the results. For example, in Romanian,
the news about David Beckham and Ronaldo is
probably much the same as in the United States.
These are international figures that are both ex-
ternal to countries where the news originates, and
there is no reason to suppose there would be a
unique local perspective represented by any of the
news sources. The only difference among them
might be in the number of contexts available. In
this situation, the addition of the English contexts
may provide enough additional information to im-
prove discrimination performance in another lan-
guage.
For example, in the 162 Romanian contexts
for Ronaldo-Beckham, there is one occurrence of
Posh, which was the stage name of Beckham?s
wife Victoria. This is below our frequency cut-
off threshold for feature selection, so it would be
discarded when using Romanian?only contexts.
However, in the English contexts Posh is men-
tioned 6 times, and is included as a feature. Thus,
the one occurrence of Posh in the Romanian cor-
pus can be well represented by information found
in the English contexts, thus allowing that Roma-
nian context to be correctly discriminated.
8 Conclusions
This paper shows that a method of name discrim-
ination based on second order context representa-
tions can take advantage of English contexts, and
the mix of English and evaluation contexts, in or-
der to perform more accurate name discrimination.
9 Acknowledgments
This research is supported by a National Sci-
ence Foundation Faculty Early CAREER Devel-
opment Award (#0092784). All of the experiments
in this paper were carried out with version 0.71
SenseClusters package, which is freely available
from http://senseclusters.sourceforge.net.
References
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
P. Nakov and M. Hearst. 2003. Category-based pseu-
dowords. In Companion Volume to the Proceedings
of HLT-NAACL 2003 - Short Papers, pages 67?69,
Edmonton, Alberta, Canada, May 27 - June 1.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
T. Pedersen and A. Kulkarni. 2006. Selecting the
r?ightn?umber of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy, April.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005.
Name discrimination by clustering similar contexts.
In Proceedings of the Sixth International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 220?231, Mexico City, February.
T. Pedersen, A. Kulkarni, R. Angheluta, Z. Kozareva,
and T. Solorio. 2006. An unsupervised language in-
dependent method of name discrimination using sec-
ond order co-occurrence features. In Proceedings
of the Seventh International Conference on Intelli-
gent Text Processing and Computational Linguistics,
pages 208?222, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
32
A Language Independent Method for Question Classification
Thamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2
Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez1
1Language Technologies Group, Computer Science Department
National Institute of Astrophysics, Optics and Electronics
72840 Tonantzintla, Puebla,
Mexico
2Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
Espan?a
{thamy,mapco,mmontesg,villasen,allopez}@inaoep.mx
Abstract
Previous works on question classification are
based on complex natural language process-
ing techniques: named entity extractors,
parsers, chunkers, etc. While these ap-
proaches have proven to be effective they
have the disadvantage of being targeted to a
particular language. We present here a sim-
ple approach that exploits lexical features
and the Internet to train a classifier, namely
a Support Vector Machine. The main fea-
ture of this method is that it can be applied
to different languages without requiring ma-
jor modifications. Experimental results of
this method on English, Italian and Span-
ish show that this approach can be a prac-
tical tool for question answering systems,
reaching a classification accuracy as high as
88.92%.
1 Introduction
Open-domain Question Answering (QA) sys-
tems are concerned with the problem of trying
to answer questions from users posed in nat-
ural language. What makes these systems a
very complex and interesting research area is
that the answers they retrieve must be concise,
as opposed to traditional search engines that in
response to a user query retrieve a list of docu-
ments believed to contain the answer. More-
over, current evaluation environments of QA
systems, such as TREC QA track (Voorhees,
2001) and CLEF (Peters et al, 2003), restrict
the size of the answers to a maximum of 50
bytes. Given the complexity involved in this
problem, traditional approaches to QA take a
divide-and-conquer strategy, where the prob-
lem is divided into several less complex subtasks
that combined lead to the resolution of the ques-
tions. An important subtask of a QA system
is question analysis, since it can provide useful
clues for identifying potential answers in a large
collection of texts. For instance, Question Clas-
sification is concerned with assigning semantic
classes to questions. This semantic classifica-
tion can be used to reduce the search space of
possible answers, i.e. if we can determine that
the question Who is the Italian Prime Minis-
ter? belongs to the semantic category PER-
SON, then we only need to look for instances of
type PERSON as possible answers. Clearly, the
advantage of such classification relies on hav-
ing the ability of extracting from the documents
such instances. In other words, a good question
classification module may be useless if we lack
an accurate named entity extractor for the doc-
ument collection.
Results of the error analysis of an open-
domain QA system showed that 36.4% of the
errors were generated by the question classifi-
cation module (Moldovan et al, 2003). Thus
it is not surprising that an increasing interest
has arisen aimed at developing accurate ques-
tion classifiers (Zhang and Lee, 2003; Li and
Roth, 2002; Suzuki et al, 2003). However, most
of these approaches are targeted to the English
language. Besides, the machine learning algo-
rithms used are trained on features extracted by
natural language processing tools that are lan-
guage dependent, and for some languages these
tools are not available. This implies that if we
want to reproduce the results of these methods
in a different language we need first to solve the
problem of making available the appropriate an-
alyzers in the given language.
We present here a flexible method for ques-
tion classification. We claim that the method
is language-independent since no complex nat-
ural language processing tools are needed; we
use plain lexical features that can be extracted
automatically from the questions. A machine
learning algorithm that has proven to perform
well over high dimensional data, is trained on
prefixes of words and on additional attribute in-
formation gathered automatically from the In-
ternet. The method was evaluated experimen-
tally, achieving high accuracy on questions in
three different languages: English, Italian and
Spanish.
The next section briefly summarizes some of
the previous approaches for question classifica-
tion. Section 3 presents the learning scenario
of this work, together with a brief introduction
to Support Vector Machines (SVM). Section 4
shows our experimental results and we conclude
with a discussion of this work and ideas for fu-
ture research in Section 5.
2 Related Work
Most approaches to question classification are
based on handcrafted rules (Voorhees, 2001).
It is not until recently that machine learn-
ing techniques are being used to tackle the
problem of question classification. In (Zhang
and Lee, 2003) they present a new method
for question classification using Support Vector
Machines. They compared accuracy of SVM
against Nearest Neighbors, Naive Bayes, De-
cision Trees and Sparse Network of Winnows
(SNoW), with SVM producing the best results.
In their work, Zhang and Sun Lee improve accu-
racy by introducing a tree kernel function that
allows to represent the syntactic structure of
questions. Their experimental results show that
SVM using this tree kernel function achieves an
accuracy of 90%, however, a parser is needed in
order to acquire the syntactic information.
Li and Roth reported a hierarchical approach
for question classification based on the SNoW
learning architecture (Li and Roth, 2002). This
hierarchical classifier discriminates among 5
coarse classes, which are then refined into 50
more specific classes. The learners are trained
using lexical and syntactic features such as pos
tags, chunks and head chunks together with two
semantic features: named entities and seman-
tically related words. They reported question
classification accuracy of 98.80% for a coarse
classification, using 5,500 instances for training.
A different approach, used for Japanese ques-
tion classification, is that of Suzuki et al
(Suzuki et al, 2003). They used SVM whith
a new kernel function, called Hierarchical Di-
rected Acyclic Graph, which allows the use of
structured data. They experimented with 68
question types and compared performance of us-
ing bag-of-words against using more elaborated
combinations of attributes, namely named en-
tities and semantic information. Their best re-
sults, an accuracy of 94.8% at the first level of
the hierarchy, were obtained when using SVM
trained on bag-of-words together with named
entities and semantic information.
The idea of using the Internet in a QA sys-
tem is not new. What is new, however, is that
we are using the Internet to obtain values for
features in our question classification process,
as opposed to previous approaches where the
redundancy of information available on the In-
ternet has been used in the answer extraction
process (Brill et al, 2002; Lin et al, 2002; Katz
et al, 2003).
3 Learning Question Classifiers
Question classification is very similar to text
classification. One thing they have in common
is that in both cases we need to assign a class,
from a finite set of possible classes, to a natural
language text. Another similarity is attribute
information; what has been used as attributes
for text classification can also be extracted and
used in question classification. Finally, in both
cases we have high dimensional attributes: if we
want to use the bag-of-words approach, we will
face the problem of having very large attribute
sets.
An important difference is that question clas-
sification introduces the problem of dealing with
short sentences, compared with text documents,
and thus we have less information available on
each question instance. This is the reason why
question classification approaches are trying to
use other information (e.g. chunks and named
entities) besides the words within the questions.
However, the main disadvantage of relying on
semantic analyzers, named entity taggers and
the like, is that for some languages these tools
are not yet well developed. Plus, most of them
are very sensitive to changes in the domain of
the corpus; and even if these tools are accu-
rate, in some cases acquiring one for a partic-
ular language may be a difficult task. This is
our prime motivation for searching for differ-
ent, more easier to gather, information to solve
the question classification problem. Our learn-
ing scenario considers as attribute information
prefixes of words in combination with attributes
whose values are obtained from the Internet.
These Internet based attributes are targeted to
extract evidence of the possible semantic class
of the question.
The next subsection will explain how the In-
ternet is used to extract attributes for our ques-
tion classification problem. In subsection 3.2
we present a brief description of Support Vec-
tor Machines, the learning algorithm used on
our experiments.
3.1 Using Internet
As Kilgarriff and Grefenstette wrote, the In-
ternet is a fabulous linguists? playground (Kil-
garriff and Grefenstette, 2003). It has become
the greatest information source available world-
wide, and although English is the dominant
language represented on the Internet it is very
likely that one can find information in almost
any desired language. Considering this, and the
fact that the texts are written in natural lan-
guage, we believe that new methods that take
advantage of this large corpus must be devised.
In this work we propose using the Internet in or-
der to acquire information that can be used as
attributes in our classification problem. This at-
tribute information can be extracted automat-
ically from the web and the goal is to provide
an estimate about the possible semantic class of
the question.
The procedure for gathering this information
from the web is as follows: we use a set of heuris-
tics to extract from the question a word w, or
set of words, that will complement the queries
submitted for the search. We then go to a search
engine, in this case Google, and submit queries
using the word w in combination with all the
possible semantic classes for our purpose. For
instance, for the question Who is the President
of the French Republic? we extract the word
President using our heuristics, and run 5 queries
in the search engine, one for each possible class.
These queries take the following form:
? ?President is a person?
? ?President is a place?
? ?President is a date?
? ?President is a measure?
? ?President is an organization?
We count the number of results returned by
Google for each query and normalize them by
their sum. The resultant numbers are the val-
ues for the attributes used by the learning algo-
rithm. As can be seen, it is a very straightfor-
ward approach, but as the experimental results
will show, this information gathered from the
Internet is quite useful. In Table 1 we present
the figures obtained from Google for the ques-
tion presented above, column Results show the
number of hits returned by the search engine
and in column Normalized we present the num-
ber of hits normalized by the total of all results
returned for the different queries.
An additional advantage of using the Internet
is that by approximating the values of attributes
in this way, we take into account words or en-
tities belonging to more than one class (poly-
semy).
Now that we have introduced the use of the
Internet in this work, we continue describing the
set of heuristics that we use in order to perform
the web search.
3.1.1 Heuristics
We begin by eliminating from the questions
all words that appear in our stop list. This
stop list contains the usual items: articles,
prepositions and conjunctions plus all the
interrogative adverbs and all lexical forms
of the verb ?to be?. The remaining words
are sent to the search engine in combina-
tion with the possible semantic classes, as
described above. If no results are returned
for any of the semantic classes we then start
eliminating words from right to left until the
search engine returns results for at least one
of the semantic categories. As an example
consider the question posed previously: Who
is the President of the French Republic? we
eliminate the words from the stop list and
then formulate queries for the remaining
words. These queries are of the following form:
?President French Republic is a si? where s ?
{Person,Organization, P lace,Date,Measure}.
The search engine did not return any results
for this query, so we start eliminating words
from right to left. The query is now like this:
?President French is a si? and given that
again we have no results returned we finally
formulate the last possible query: ?President is
a si? which returns results for all the semantic
classes except for Date.
Being heuristics, we are aware that in some
cases they do not work well. Nevertheless,
for the vast majority of the cases they pre-
sented surprisingly good results, in the three
Query Results Normalized
?President is a person? 259 0.8662
?President is a place? 9 0.0301
?President is an organization? 11 0.0368
?President is a measure? 20 0.0669
?President is a date? 0 0
Table 1: Example of using the Internet to extract features for question classification
Class Number of Instances
Person 91
Organization 41
Measure 103
Date 64
Object 12
Other 54
Place 85
Table 2: Distribution of semantic classes for the
DISEQuA corpus
languages, as shown in the experimental eval-
uation.
3.2 Support Vector Machines
Given that Support Vector Machines have
proven to perform well over high dimensionality
data they have been successfully used in many
natural language related applications, such as
text classification (Joachims, 1999; Joachims,
2002; Tong and Koller, 2001) and named entity
recognition (Mitsumori et al, 2004; Solorio and
Lo?pez, 2004). This technique uses geometrical
properties in order to compute the hyperplane
that best separates a set of training examples
(Stitson et al, 1996). When the input space
is not linearly separable SVM can map, by us-
ing a kernel function, the original input space
to a high-dimensional feature space where the
optimal separable hyperplane can be easily cal-
culated. This is a very powerful feature, be-
cause it allows SVM to overcome the limitations
of linear boundaries. They also can avoid the
over-fitting problems of neural networks as they
are based on the structural risk minimization
principle. The foundations of these machines
were developed by Vapnik, for more informa-
tion about this algorithm we refer the reader to
(Vapnik, 1995; Scho?lkopf and Smola, 2002).
4 Experimental Evaluation
4.1 Data sets
The data set used in this work consists of the
questions provided in the DISEQuA Corpus
(Magnini et al, 2003). Such corpus was made
up of simple, mostly short, straightforward and
factual queries that sound naturally sponta-
neous, and arisen from a real desire to know
something about a particular event or situation.
The DISEQuA Corpus contains 450 questions,
each one formulated in four languages: Dutch,
English, Italian and Spanish. The questions
are classified into seven categories: Person, Or-
ganization, Measure, Date, Object, Other and
Place. The experiments performed in this work
used the English, Italian and Spanish versions
of these questions.
4.2 Experiments
In the experiments performed in this work we
used the evaluation technique 10-fold cross-
validation which consists of randomly dividing
the data into 10 equally-sized subgroups and
performing 10 different experiments. We sep-
arated nine groups together with their original
classes as the training set, the remaining group
was considered the test set. Each experiment
consists of ten runs of the procedure described
above, and the overall average are the results
reported here.
In our experiments we used the WEKA imple-
mentation of SVM (Witten and Frank, 1999).
In this setting multi-class problems are solved
using pairwise classification. The optimization
algorithm used for training the support vec-
tor classifier is an implementation of Platt?s se-
quential minimal optimization algorithm (Platt,
1999). The kernel function used for mapping
the input space was a polynomial of exponent
one.
The most common approach to question clas-
sification is bag-of-words, so we decided to com-
pare results of using bag-of-words against using
just prefixes of the words in the questions. In
Language Words Prefix-5 Prefix-4 Internet
ENGLISH 81.77% 81.32% 80.21% 67.77%
ITALIAN 88.03% 87.59% 88.70% 60.79%
SPANISH 79.90% 81.45% 76.97% 68.86%
Table 3: Experimental results of accuracy when training SVM with words, prefixes and Internet-
based attributes
order to choose an appropriate prefix size we
compute the average length of the words in the
three languages used in this work. For English
the average length of words is 4.62, for Italian
is 4.8 while for Spanish the average length is
4.75. So we decided to experiment with pre-
fixes of size 4 and 5. In Table 3 we can see a
comparison of classification accuracy of train-
ing SVM using all the words in the questions,
using prefixes of size 4 and 5 and using only
the Internet-based attributes. As we can see for
English the best results were obtained when us-
ing words as attributes, although the difference
between using just prefixes and using words is
not so large. For Spanish however, the best re-
sults were achieved when using prefixes of size
5. This can be due to the fact that some of
the interrogative words, that by themselves can
define the semantic class of questions in this
language, such as Cua?ndo (When) and Cua?nto
(How much) could be considered as the same
prefix of size 4 i.e. Cua?n. But if we consider
prefixes of size 5, then these two words will form
two different prefixes: Cua?nd and Cua?nt, thus
reducing the loss of information, as oppose to
using prefixes of size 4. For Italian language
the best results were obtained from using pre-
fixes of size 4. And for the three languages the
Internet-based attributes had rather low accu-
racies, the lowest being for Italian. When we
analyzed the results computed for Italian, us-
ing our Internet-based attributes, we realized
that in many cases we could not get any re-
sults to the queries. One plausible explanation
for this lack of information, is that the num-
ber of Italian documents available on Internet
is much smaller than for English and Spanish.
Estimates reported in (Kilgarriff and Grefen-
stette, 2003) show that for Italian the web size
in words is 1,845,026,000; while for English and
Spanish the web sizes are 76,598,718,000 and
2,658,631,000 respectively. Thus our method
was not able to extract as much information as
for the other two languages.
4.3 Combining Internet-based
Attributes with Lexical Features
Results presented in the previous subsection
show how by using just lexical information we
can train SVM and achieve high accuracies in
the three languages. But our goal is to discover
the usefulness of using Internet in order to ex-
tract attributes for question classification. We
performed other experiments combining the lex-
ical attributes with the Internet information in
order to discover if we can further improve ac-
curacy. Table 4 show experimental results of
this attribute combination and Figure 1 shows
a graphical representation of these results.
It is interesting to note that for English and
Spanish we did gain accuracy when using the
Internet features in all the cases. In contrast,
for Italian classification accuracy was decreased
when incorporating Internet-based attributes to
words and prefixes of size 5. We believe that this
drop in accuracy for Italian may be due to the
weakly supported information extracted from
the Internet, Table 3 shows that SVM trained
only on the coefficients from the Internet per-
formed worse for Italian. It is not surprising
that adding this rather sparse information to
the attributes in the Italian language did not
produce an advantage in the classifiers perfor-
mance.
5 Conclusions
We have presented here experimental results of
a language independent question classification
method. The method is claimed to be lan-
guage independent since the features used as
attributes in the learning task can be extracted
from the questions in a fully automated manner;
we do not use semantic or syntactic informa-
tion because otherwise we will be restricted to
work on languages for which we do have parsers
that can extract this information. We believe
that this method can be successfully applied
to other languages, such as Romanian, French,
Portuguese and Catalan, that share the mor-
phologic characteristics of the three languages
Language Words+Internet Prefix-5 + Internet Prefix-4 + Internet
ENGLISH 82.88% 82.66% 83.55%
ITALIAN 87.34% 86.93% 88.92%
SPANISH 83.43% 84.09% 81.45%
Table 4: Experimental results combining Internet-valued attributes with words and prefixes
Inte
rne
t
Wo
rds
Pre
fix-
5
Pre
fix-
4
Wo
rds
+In
ter
ne
t
Pre
fix-
5+
Inte
rne
t
Pre
fix-
4+
Inte
rne
t
60
65
70
75
80
85
90
Spanish
English
Italian
Figure 1: Graphical comparison of question classification accuracies
tested here.
Comparing our results with those of previous
works we can say that our method is promis-
ing. For instance Zhang and Sun Lee (Zhang
and Lee, 2003) reported an accuracy of 90% for
English questions, while Li and Roth (Li and
Roth, 2002) achieved 98.8% accuracy. How-
ever, they used a training set of 5,500 questions
and a test set of 500 questions, while in our ex-
periments we used for training 405 for each 45
test questions (10-fold-cross-validation). When
Zhang and Sun Lee used only 1,000 questions
for training they achieved an accuracy of 80.2%.
It is well known that machine learning algo-
rithms perform better when a bigger training
set is available, so it is expected that experi-
ments of our method with a larger training set
will provide improved results.
As future work we plan to investigate active
learning with SVM for this problem. Given that
manually labelling questions is a very time con-
suming task, active learning can provide a faster
approach to build accurate question classifiers.
Instead of randomly selecting question instances
to label manually and then provide them to the
learner, the learner can analyze the unlabeled
instances and select for labelling the instances
that seem more relevant to the task.
Another interesting line for future work is ex-
ploring the advantage of using mixed languages
corpora lo learn question classification. The
Romance languages, for instance, such as Ital-
ian, French and Spanish have stems in common.
Then it is feasible that questions for several lan-
guages may help to train a classifier for a differ-
ent language. The advantage of this idea will be
the availability of larger corpora for languages
for which a large enough corpus is not available,
counting in favor of languages that are under-
represented on the Internet. We could circum-
vent this lack of presence on the Internet for
some languages by using information available
on other, more well represented, languages.
6 Acknowledgements
We would like to thank CONACyT for par-
tially supporting this work under grants 166934,
166876 and U39957-Y, and Secretar??a de Estado
de Educacio?n y Universidades de Espan?a.
References
E. Brill, S. Dumais, and M. Banko. 2002. An
analysis of the AskMSR question-answering
system. In 2002 Conference on Empirical
Methods in Natural Language Processing.
T. Joachims. 1999. Transductive inference for
text classification using support vector ma-
chines. In Proceedings of the Sixteenth In-
ternational Conference on Machine Learning
(ICML), pages 200?209. Morgan Kaufmann.
T. Joachims. 2002. Learning to Classify Text
using Support Vector Machines: Methods
Theory and Algorithms, volume 668 of The
Kluwer International Series in Engineering
and Computer Science. Kluwer Academic
Publishers.
B. Katz, J. Lin, D. Loreto, W. Hilde-
brandt, M. Bilotti, S. Felshin, A. Fernandes,
G. Marton, and F. Mora. 2003. Integrat-
ing web-based and corpus-based techniques
for question answering. In Twelfth Text RE-
trieval Conference (TREC 2003), Gaithers-
burg, Maryland, November.
A. Kilgarriff and G. Grefenstette. 2003. Intro-
duction to the special issue on the web as cor-
pus. Computational Linguistics, 29(3):333?
347.
X. Li and D. Roth. 2002. Learning question
classifiers. In COLING?02.
J. Lin, A. Fernandes, B. Katz, G. Marton,
and S. Tellex. 2002. Extracting answers
from the web using knowledge annotation and
knowledge mining techniques. In Eleventh
Text REtrieval Conference (TREC 2002),
Gaithersburg, Maryland, November.
B. Magnini, S. Romagnoli, A. Vallin, J. Her-
rera, A. Pen?as, V. Peinado, F. Verdejo, and
M. de Rijke. 2003. Creating the DISEQuA
corpus: a test set for multilingual question
answering. In Carol Peters, editor, Working
Notes for the CLEF 2003 Workshop, Trond-
heim, Norway, August.
T. Mitsumori, S. Fation, M. Murata, K. Doi,
and H. Doi. 2004. Boundary correction of
protein names adapting heuristic rules. In
Alexander Gelbukh, editor, Fifth Interna-
tional Conference on Intelligent Text Process-
ing and Computational Linguistics, CICLing
2004, volume 2945 of Lecture Notes in Com-
puter Science, pages 172?175. Springer.
D. Moldovan, M. Pas?ca, S. Harabagiu, and
M. Surdeanu. 2003. Performance issues and
error analysis in an open-domain question
answering system. ACM Trans. Inf. Syst.,
21(2):133?154.
C. Peters, M. Braschler, J. Gonzalo, and
M. Kluck, editors. 2003. Advances in
Cross-Language Information Retrieval, Third
Workshop of the Cross-Language Evaluation
Forum, CLEF 2002. Rome, Italy, September
19-20, 2002. Revised Papers, volume 2785 of
Lecture Notes in Computer Science. Springer.
J. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal op-
timization. In Advances in Kernel Meth-
ods ?Support Vector Learning, (B. Scho?lkopf,
C.J.C. Burges, A.J. Smola, eds.), pages 185?
208, Cambridge, Massachusetts. MIT Press.
B. Scho?lkopf and A. J. Smola. 2002. Learning
with Kernels: Support Vector Machines, Reg-
ularization, Optimization and Beyond. MIT
Press.
T. Solorio and A. Lo?pez Lo?pez. 2004. Learn-
ing named entity classifiers using support vec-
tor machines. In Alexander Gelbukh, editor,
Fifth International Conference on Intelligent
Text Processing and Computational Linguis-
tics, CICLing 2004, volume 2945 of Lecture
Notes in Computer Science, pages 158?167.
Springer.
M. O. Stitson, J. A. E. Wetson, A. Gammer-
man, V. Vovk, and V. Vapnik. 1996. Theory
of support vector machines. Technical Report
CSD-TR-96-17, Royal Holloway University of
London, England, December.
J. Suzuki, H. Taira, Y. Sasaki, and E. Maeda.
2003. Question classification using HDAG
kernel. In Workshop on Multilingual Summa-
rization and Question Answering 2003, pages
61?68.
S. Tong and D. Koller. 2001. Support vector
machine active learning with applications to
text classification. Journal of Machine Learn-
ing Research, 2:45?66.
V. Vapnik. 1995. The Nature of Statisti-
cal Learning Theory. Number ISBN 0-387-
94559-8. Springer, N.Y.
E. Voorhees. 2001. Overview of the TREC
2001 question answering track. In Proceed-
ings of the 10th Text REtrieval Conference
(TREC01), NIST, pages 157?165, Gaithers-
burg, MD.
I. H. Witten and E. Frank. 1999. Data Mining,
Practical Machine Learning Tools and Tech-
niques with Java Implementations. The Mor-
gan Kaufmann Series in Data Management
Systems. Morgan Kaufmann.
D. Zhang and W. Sun Lee. 2003. Question
classification using support vector machines.
In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 26?32, Toronto, Canada. ACM Press.
Proceedings of the ACL Student Research Workshop, pages 25?30,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Exploiting Named Entity Taggers in a Second Language
Thamar Solorio
Computer Science Department
National Institute of Astrophysics, Optics and Electronics
Luis Enrique Erro #1, Tonantzintla, Puebla
72840, Mexico
Abstract
In this work we present a method for
Named Entity Recognition (NER). Our
method does not rely on complex linguis-
tic resources, and apart from a hand coded
system, we do not use any language-
dependent tools. The only information
we use is automatically extracted from the
documents, without human intervention.
Moreover, the method performs well even
without the use of the hand coded system.
The experimental results are very encour-
aging. Our approach even outperformed
the hand coded system on NER in Span-
ish, and it achieved high accuracies in Por-
tuguese.
1 Introduction
Given the usefulness of Named Entities (NEs) in
many natural language processing tasks, there has
been a lot of work aimed at developing accurate
named entity extractors (Borthwick, 1999; Velardi et
al., 2001; Are?valo et al, 2002; Zhou and Su, 2002;
Florian, 2002; Zhang and Johnson, 2003). Most ap-
proaches however, have very low portability, they
are designed to perform well over a particular collec-
tion or type of document, and their accuracies will
drop considerably when used in different domains.
The reason for this is that many NE extractor sys-
tems rely heavily on complex linguistic resources,
which are typically hand coded, for example regu-
lar expressions, grammars, gazetteers and the like.
Adapting a system of this nature to a different col-
lection or language requires a lot of human effort,
involving tasks such as rewriting the grammars, ac-
quiring new dictionaries, searching trigger words,
and so on. Even if one has the human resources and
the time needed for the adaptation process, there are
languages that lack the linguistic resources needed,
for instance, dictionaries are available in electronic
form for only a handful of languages. We believe
that, by using machine learning techniques, we can
adapt an existing hand coded system to different do-
mains and languages with little human effort.
Our goal is to present a method that will facilitate
the task of increasing the coverage of named entity
extractor systems. In this setting, we assume that
we have available an NE extractor system for Span-
ish, and we want to adapt it so that it can perform
NER accurately in documents from a different lan-
guage, namely Portuguese. It is important to empha-
size here that we try to avoid the use of complex and
costly linguistic tools or techniques, besides the ex-
isting NER system, given the language restrictions
they pose. Although, we do need a corpus of the
target language. However, we consider the task of
gathering a corpus much easier and faster than that
of developing linguistic tools such as parsers, part-
of-speech taggers, grammars and the like.
In the next section we present some recent work
related to NER. Section 3 describes the data sets
used in our experiments. Section 4 introduces our
approach to NER, and we conclude in Section 5 giv-
ing a brief discussion of our findings and proposing
research lines for future work.
25
2 Related Work
There has been a lot of work on NER, and there is a
remarkable trend towards the use of machine learn-
ing algorithms. Hidden Markov Models (HMM) are
a common choice in this setting. For instance, Zhou
and Su trained HMM with a set of attributes combin-
ing internal features such as gazetteer information,
and external features such as the context of other
NEs already recognized (Zhou and Su, 2002). (Bikel
et al, 1997) and (Bikel et al, 1999) are other exam-
ples of the use of HMMs.
Previous methods for increasing the coverage
of hand coded systems include that of Borthwick,
he used a maximum entropy approach where he
combined the output of three hand coded systems
with dictionaries and other orthographic information
(Borthwick, 1999). He also adapted his system to
perform NER in Japanese achieving impressive re-
sults.
Spanish resources for NER have been used pre-
viously to perform NER on a different language.
Carreras et al presented results of a NER system
for Catalan using Spanish resources (Carreras et al,
2003a). They explored several methods for build-
ing NER for Catalan. Their best results are achieved
using cross-linguistic features. In this method the
NER system is trained on mixed corpora and per-
forms reasonably well on both languages. Our work
follows Carreras et al approach, but differs in that
we apply directly the NER system for Spanish to
Portuguese and train a classifier using the output and
the real classes.
In (Petasis et al, 2000) a new method for automat-
ing the task of extending a proper noun dictionary is
presented. The method combines two learning ap-
proaches: an inductive decision-tree classifier and
unsupervised probabilistic learning of syntactic and
semantic context. The attributes selected for the ex-
periments include POS tags as well as morphologi-
cal information whenever available.
One work focused on NE recognition for Span-
ish is based on discriminating among different kinds
of named entities: core NEs, which contain a trig-
ger word as nucleus, syntactically simple weak
NEs, formed by single noun phrases, and syntacti-
cally complex named entities, comprised of complex
noun phrases. Are?valo and colleagues focused on
the first two kinds of NEs (Are?valo et al, 2002). The
method is a sequence of processes that uses simple
attributes combined with external information pro-
vided by gazetteers and lists of trigger words. A
context free grammar, manually coded, is used for
recognizing syntactic patterns.
3 Data sets
In this paper we report results of experimenting with
two data sets. The corpus in Spanish is that used
in the CoNLL 2002 competitions for the NE extrac-
tion task. This corpus is divided into three sets: a
training set consisting of 20,308 NEs and two differ-
ent sets for testing, testa which has 4,634 NEs and
testb with 3,948 NEs, the former was designated to
tune the parameters of the classifiers (development
set), while testb was designated to compare the re-
sults of the competitors. We performed experiments
with testa only.
For evaluating NER on Portuguese we used the
corpus provided by ?HAREM: Evaluation contest
on named entity recognition for Portuguese?. This
corpus contains newspaper articles and consists of
8,551 words with 648 NEs.
4 Two-step Named Entity Recognition
Our approach to NER consists in dividing the prob-
lem into two subproblems that are addressed sequen-
tially. We first solve the problem of determining
boundaries of named entities, we called this process
Named Entity Delimitation (NED). Once we have
determined which words belong to named entities,
we then get to the task of classifying the named en-
tities into categories, this process is what we called
Named Entity Classification (NEC). We explain the
two procedures in the following subsections.
4.1 Named Entity Delimitation
We used the BIO scheme for delimiting named enti-
ties. In this approach each word in the text is labeled
with one out of three possible classes: The B tag is
assigned to words believed to be the beginning of a
NE, the I tag is for words that belong to an entity
but that are not at the beginning, and the O tag is for
all words that do not satisfy any of the previous two
conditions.
26
Table 1: An example of the attributes used in the
learning setting for NER in Spanish. The fragment
presented in the table, ?El Eje?rcito Mexicano puso
en marcha el Plan DN-III?, translates as ?The Mex-
ican Army launched the DN-III plan?
Internal Features External Features
Word Caps Position POS tag BIO tag Class
El 3 1 DA O O
Eje?rcito 2 2 NC B B
Mexicano 2 3 NC I I
puso 2 4 VM O O
en 2 5 SP O O
marcha 2 6 NC O O
el 3 7 DA O O
Plan 2 8 NC B B
DN-III 3 9 NC I I
In our approach, NED is tackled as a learning
task. The features used as attributes are automati-
cally extracted from the documents and are used to
train a machine learning algorithm. We used a mod-
ified version of C4.5 algorithm (Quinlan, 1993) im-
plemented within the WEKA environment (Witten
and Frank, 1999).
For each word we combined two types of fea-
tures: internal and external; we consider as inter-
nal features the word itself, orthographic informa-
tion and the position in the sentence. The external
features are provided by the hand coded NER system
for Spanish, these are the Part-of-Speech tag and the
BIO tag. Then, the attributes for a given word w are
extracted using a window of five words anchored in
the word w, each word described by the internal and
external features mentioned previously.
Within the orthographic information we consider
6 possible states of a word. A value of 1 in this at-
tribute means that the letters in the word are all cap-
italized. A value of 2 means the opposite: all letters
are lower case. The value 3 is for words that have the
initial letter capitalized. 4 means the word has dig-
its, 5 is for punctuation marks and 6 refers to marks
representing the beginning and end of sentences.
The hand coded system used in this work was de-
veloped by the TALP research center (Carreras and
Padro?, 2002). They have developed a set of NLP an-
alyzers for Spanish, English and Catalan that include
practical tools such as POS taggers, semantic ana-
lyzers and NE extractors. This NER system is based
on hand-coded grammars, lists of trigger words and
gazetteer information.
In contrast to other methods we do not perform bi-
nary classifications, as (Carreras et al, 2003b), thus
we do not build specialized classifiers for each of the
tags. Our classifier learns to discriminate among the
three classes and assigns labels to all the words, pro-
cessing them sequentially. In Table 1 we present an
example taken from the data used in the experiments
where internal and external features are extracted for
each word in a sentence.
4.1.1 Experimental Results
For all results reported here we show the overall
average of several runs of 10-fold cross-validation.
We used common measures from information re-
trieval: precision, recall and F1 and we present re-
sults from individual classes as we believe it is im-
portant in a learning setting such as this, where
nearly 90% of the instances belong to one class.
Table 2 presents comparative results using the
Spanish corpus. We show four different sets of re-
sults, the first ones are from the hand coded sys-
tem, they are labeled NER system for Spanish. Then
we present results of training a classifier with only
the internal features described above, these results
are labeled Internal features. In a third experiment
we trained the classifier using only the output of the
NER system, these are under column External fea-
tures. Finally, the results of our system are presented
in column labeled Our method. We can see that even
though the NER system performs very well by it-
self, by training the C4.5 algorithm on its outputs we
improve performance in all the cases, with the ex-
ception of precision for class B. Given that the hand
coded system was built for this collection, it is very
encouraging to see our method outperforming this
system. In Table 3 we show results of applying our
method to the Portuguese corpus. In this case the
improvements are much more impressive, particu-
larly for class B, in all the cases the best results are
obtained from our technique. This was expected as
we are using a system developed for a different lan-
guage. But we can see that our method yields very
competitive results for Portuguese, and although by
using only the internal features we can outperform
the hand coded system, by combining the informa-
tion using our method we can increase accuracies.
27
Table 2: Comparison of results for Spanish NE delimitation
NER system for Spanish Internal features External features Our method
Class P R F1 P R F1 P R F1 P R F1
B 92.8 89.3 91.7 87.1 89.3 88.2 93.9 91.5 92.7 93.5 92.9 93.2
I 84.3 85.2 84.7 89.5 77.1 82.9 87.8 87.8 85.7 90.6 87.4 89.0
O 98.6 98.9 98.8 98.1 98.9 98.5 98.7 99 98.9 98.9 99.2 99.1
overall 91.9 91.1 91.7 91.5 88.4 89.8 93.4 92.7 92.4 94.3 93.1 93.7
Table 3: Experimental results for NE delimitation in Portuguese
NER system for Spanish Internal features External features Our method
Class P R F1 P R F1 P R F1 P R F1
B 60.0 68.8 64.1 82.4 85.8 84.1 75.9 81.0 78.4 82.1 87.8 84.9
I 64.5 73.3 68.6 80.1 76.8 78.4 73.8 70.3 72.0 80.9 77.8 79.3
O 97.2 95.5 96.4 98.7 98.5 98.6 98.1 97.7 97.9 98.8 98.4 98.6
overall 73.9 79.2 76.3 87.0 87.0 87.0 82.6 83.0 82.7 87.2 88.0 87.6
From the results presented above, it is clear that
the method can perform NED in Spanish and Por-
tuguese with very high accuracy. Another insight
suggested by these results is that in order to perform
NED in Portuguese we do not need an existing NED
system for Spanish, the internal features performed
well by themselves, but if we have one available,
we can use the information provided by it to build
a more accurate NED method.
4.2 Named Entity Classification
As mentioned previously, we build our NE classi-
fiers using the output of a hand coded system. Our
assumption is that by using machine learning algo-
rithms we can improve performance of NE extrac-
tors without a considerable effort, as opposed to that
involved in extending or rewriting grammars and
lists of trigger words and gazetteers. Another as-
sumption underlying this approach is that of believ-
ing that the misclassifications of the hand coded sys-
tem for Spanish will not affect the learner. We be-
lieve that by having available the correct NE classes
in the training corpus, the learner will be capable of
generalizing error patterns that will be used to as-
sign the correct NE. If this assumption holds, learn-
ing from other?s mistakes, the learner will end up
outperforming the hand coded system.
In order to build a training set for the learner, each
instance is described with the same attributes as for
the NED task described in section 4.1, with the addi-
tion of a new attribute. Since NEC is a more difficult
task, we consider useful adding as attribute the suf-
fix of each word. Then, for each instance word we
consider its suffix, with a maximum size of 5 char-
acters.
Another important difference between this clas-
sification task and NED relies in the set of target
values. For the Spanish corpus the possible class
values are the same as those used in CoNLL-2002
competition task: person, organization, location and
miscellaneous. However, for the Portuguese corpus
we have 10 possible classes: person, object, quan-
tity, event, organization, artifact, location, date, ab-
straction and miscellaneous. Thus the task of adapt-
ing the system for Spanish to perform NEC in Por-
tuguese is much more complex than that of NED
given that the Spanish system only discerns the four
NE classes defined on the CoNLL-2002. Regardless
of this, we believe that the learner will be capable
of achieving good accuracies by using the other at-
tributes in the learning task.
4.2.1 Experimental Results
Similarly to the NED case we trained C4.5 clas-
sifiers for the NEC task, results are presented in Ta-
bles 4 and 5. Again, we perform comparisons be-
tween the hand coded system and the use of different
subsets of attributes. For the case of Spanish NEC,
we can see in Table 4, that our method using internal
and external features presents the best results. The
improvements are impressive, specially for the NE
class Miscellaneous where the hand coded system
achieved an F measure below 1 while our system
achieved an F measure of 56.7. In the case of NEC
in Portuguese the results are very encouraging. The
28
Table 4: NEC performance on the Spanish development set
NER system for Spanish Internal features External features Our method
Class P R F1 P R F1 P R F1 P R F1
Per 84.7 93.2 88.2 94.0 62.9 75.3 88.3 93.1 90.6 88.2 95.4 91.7
Org 78.7 88.7 82.9 61.7 90.0 73.2 77.7 91.9 84.2 83.4 89.0 86.1
Loc 78.7 76.2 76.9 78.4 65.1 71.2 80.3 80.3 80.3 82.0 82.5 82.2
Misc 24.9 .004 .008 75.5 42.0 54.0 52.9 23.4 33.5 71.6 46.9 56.7
overall 66.7 64.5 62.0 77.4 65.0 68.4 74.8 72.1 72.1 81.3 78.4 79.1
hand coded system performed poorly but by training
a C4.5 algorithm results are improved considerably,
even for the classes that the hand coded system was
not capable of recognizing. As expected, the exter-
nal features did not solve the NEC by themselves but
contribute for improving the performance. This, and
the results from using only internal features, suggest
that we do not need complex linguistic resources in
order to achieve good results. Additionally, we can
see that for some cases the classifiers were not able
of performing an accurate classification, as in the
case of classes object and miscellaneous. This may
be due to a poor representation of the classes in the
training set, for instance the class object has only 4
instances. We believe that if we have more instances
available the learners will improve these results.
5 Conclusions
Named entities have a wide usage in natural lan-
guage processing tasks. For instance, it has been
shown that indexing NEs within documents can help
increase precision of information retrieval systems
(Mihalcea and Moldovan, 2001). Other applications
of NEs are in Question Answering (Mann, 2002;
Pe?rez-Coutin?o et al, 2004) and Machine Translation
(Babych and Hartley, 2003). Thus it is important to
have accurate NER systems, but these systems must
be easy to port and robust, given the great variety of
documents and languages for which it is desirable to
have these tools available.
In this work we have presented a method for per-
forming named entity recognition. The method uses
a hand coded system and a set of lexical and or-
thographic features to train a machine learning al-
gorithm. Apart from the hand coded system our
method does not require any language dependent
features, we do not make use of lists of trigger
words, neither we use any gazetteer information.
The only information used in this approach is auto-
matically extracted from the documents, without hu-
man intervention. Yet, the results presented here are
very encouraging. We were able to achieve good ac-
curacies for NEC in Portuguese, where we needed to
classify NEs into 10 possible classes, by exploiting
a hand-coded system for Spanish targeted to only 4
classes. This achievement gives evidence of the flex-
ibility of our method. Additionally we outperform
the hand coded system on NER in Spanish. Thus,
our method has shown to be robust and easy to port
to other languages. The only requirement for using
our method is a tokenizer for languages that do not
separate words with white spaces, the rest can be
used pretty straightforward.
We are interested in exploring the use of this
method to perform NER in English, we would like
to determine to what extent our system is capable
of achieving competitive results without the use of
language dependent resources, such as dictionaries
and lists of words. Another research direction is the
adaptation of this method to cross language NER.
We are very interested in exploring if, by training
a classifier with mixed language corpora, we can
perform NER in more than one language simulta-
neously.
References
Montse Are?valo, Xavier Carreras, Llu??s Ma`rquez, Toni
Mart??, Llu??s Padro?, and Maria Jose? Simon. 2002.
A proposal for wide-coverage Spanish named en-
tity recognition. Sociedad Espan?ola para el Proce-
samiento del Lenguaje Natural, (28):63?80, May.
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition. In Proceedings of the EACL 2003
Workshop on MT and Other Language Technology
Tools, pages 1?8.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high perfor-
29
Table 5: NEC performance on the Portuguese set
NER system for Spanish Internal features External features Our method
Class P R F1 P R F1 P R F1 P R F1
Pessoa (Person) 34.8 72.5 46.6 49.1 92.0 64.0 46.9 64.6 54.4 45.5 91.1 60.7
Coisa (Object) 0 0 0 0 0 0 0 0 0 0 0 0
Valor (Quantity) 0 0 0 82.1 47.1 59.8 74.6 69.1 71.8 77.6 76.5 77.0
Acontecimento (Event) 0 0 0 33.3 21.4 26.1 14.3 7.1 9.5 50.0 21.4 30.0
Organizac?a?o (Organization) 41.4 38.4 39.3 70.7 56.9 63.1 45.7 56.9 50.7 79.3 49.2 60.8
Obra (Artifact) 0 0 0 76.6 64.3 69.9 29.4 8.9 13.7 74.4 57.1 64.6
Local (Location) 52.5 16.5 24.8 72.6 32.6 45.0 43.6 38.5 40.9 67.4 32.1 43.5
Tempo (Date) 0 0 0 74.0 86.6 79.8 85.5 83.9 84.7 87.0 83.9 85.5
Abstracc?a?o (Abstraction) 0 0 0 82.1 41.8 55.4 22.2 3.6 6.3 79.3 41.8 54.8
Variado (Miscellaneous) 0 0 0 1 15.4 26.7 0 0 0 1 15.4 26.7
overall 12.8 12.7 11.0 54.1 45.8 48.9 36.2 33.2 33.2 56.1 46.8 50.3
mance learning name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 194?201.
Daniel M. Bikel, Richard Schwartz, and Ralph
Weischedel. 1999. An algorithm that learns what?s in
a name. Machine Learning, Special Issue on Natural
Language Learning, 34(1?3):211?231, February.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University, New York, September.
Xavier Carreras and Llu??s Padro?. 2002. A flexible dis-
tributed architecture for natural language analyzers. In
Proceedings of LREC?02, Las Palmas de Gran Ca-
naria, Spain.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?. 2003a.
Named entity recognition for Catalan using Spanish
resources. In 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL?03), Budapest, Hungary, April.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?. 2003b.
A simple named entity extractor using adaboost. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 152?155. Edmonton,
Canada.
Radu Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002, pages 175?178. Taipei, Taiwan.
Gideon S. Mann. 2002. Fine-grained proper noun
ontologies for question answering. In SemaNet?02:
Building and Using Semantic Networks, Taipei, Tai-
wan.
Rada Mihalcea and Dan Moldovan. 2001. Document
indexing using named entities. Studies in Informatics
and Control, 10(1), January.
Manuel Pe?rez-Coutin?o, Thamar Solorio, Manuel Montes
y Go?mez, Aurelio Lo?pez Lo?pez, and Luis Villasen?or
Pineda. 2004. Question answering for Spanish
based on lexical and context annotation. In Christian
Lema??tre, Carlos Reyes, and Jesu?s A. Gonza?lez, edi-
tors, Advances in Artificial Intelligence ? IBERAMIA
2004, Lecture Notes in Artificial Intelligence 3315,
pages 325?333, Puebla, Mexico, November. Springer.
Georgios Petasis, Alessandro Cucchiarelli, Paola Velardi,
Georgios Paliouras, Vangelis Karkaletsis, and Con-
stantine D. Spyropoulos. 2000. Automatic adaptation
of proper noun dictionaries through cooperation of ma-
chine learning and probabilistic methods. In Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 128?135. ACM Press.
J. R. Quinlan. 1993. C4.5: Programs for machine learn-
ing. San Mateo, CA: Morgan Kaufmann.
Thamar Solorio. 2005. Improvement of Named Entity
Tagging by Machine Learning. Ph.D. thesis, Insti-
tuto Nacional de Astrof??sica, ?Optica y Electro?nica, To-
nantzintla, Puebla, Mexico, (to appear).
Paola Velardi, Paolo Fabriani, and Michel Missikoff.
2001. Using text processing techniques to automati-
cally enrich a domain ontology. In Proceedings of the
international conference on Formal Ontology in Infor-
mation Systems, pages 270?284. ACM Press.
Ian H. Witten and Eibe Frank. 1999. Data Mining, Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. The Morgan Kaufmann Series
in Data Management Systems. Morgan Kaufmann.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 204?207. Edmonton,
Canada.
Guodong Zhou and Jian Su. 2002. Named entity recog-
nition using an HMM-based chunk tagger. In Proceed-
ings of ACL?02, pages 473?480.
30
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1228?1237, Dublin, Ireland, August 23-29 2014.
Cross-Topic Authorship Attribution: Will Out-Of-Topic Data Help?
Upendra Sapkota and Thamar Solorio
The University of Alabama at Birmingham
1300 University Boulevard
Birmingham, AL 35294, USA
{upendra,solorio}@cis.uab.edu
Manuel Montes-y-G
?
omez
Instituto Nacional de Astrof??sica
Optica y Electr?onica
Puebla, Mexico
mmontesg@ccc.inaoep.mx
Steven Bethard
The University of Alabama at Birmingham
1300 University Boulevard
Birmingham, AL 35294, USA
bethard@cis.uab.edu
Paolo Rosso
NLE Lab - PRHLT Research Center
Universitat Polit`ecnica de Val`encia
Valencia, Spain
prosso@dsic.upv.es
Abstract
Most previous research on authorship attribution (AA) assumes that the training and test data
are drawn from same distribution. But in real scenarios, this assumption is too strong. The goal
of this study is to improve the prediction results in cross-topic AA (CTAA), where the training
data comes from one topic but the test data comes from another. Our proposed idea is to build
a predictive model for one topic using documents from all other available topics. In addition
to improving the performance of CTAA, we also make a thorough analysis of the sensitivity to
changes in topic of four most commonly used feature types in AA. We empirically illustrate that
our proposed framework is significantly better than the one trained on a single out-of-domain
topic and is as effective, in some cases, as same-topic setting.
1 Introduction
Authorship Attribution is the problem of identifying who, from a number of given candidate authors,
wrote the given piece of text. The authorship attribution task can be viewed as a multi-class single-label
text classification task where each author indicates a class. However, the purpose of AA is to model each
author?s writing style. AA methods have a wide range of applications, including Forensic Linguistics (spam
filtering (de Vel et al., 2001), verifying the authorship of threatening emails), cybercrimes (identifying
authors of malicious code and defending against pedophiles), and plagiarism detection (Stamatatos, 2011).
The AA methods can be useful in applied areas such as law and journalism where the identification
of the true author of a piece of text (such as a ransom note) may be able to save lives or help prosecute
offenders. One of the outstanding problems in AA studies is the unrealistic assumption that the samples of
both known and unknown authorship are drawn from the same distribution. This assumption considerably
simplifies the AA task but also limits the practical usability of the methods. In practical scenarios usually
the documents under investigation are from a different domain than that of the training documents. We
feel the need to advance the way AA methods are designed so that the bridge between domains will
be minimized to obtain the optimum performance. Therefore, we try to improve the performance of
cross-topic AA (CTAA), one of the dimensions of cross-domain AA (CDAA) where training and test data
come from different topics.
In this paper, we focus on one of the outstanding research questions on AA: Can we reliably predict
the author of a document written in one topic with a predictive model developed using documents from
other topics? We hypothesize that the addition of training data even if it comes from a topic different
than that of the test data improves cross-topic AA performance. To test the hypothesis, we compare
the performance of our proposed model trained on documents from all available out-of-topic data with
two models, one trained on single out-of-topic data and another trained on the same topic (intra-topic)
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1228
data. We also compare the performance of using four widely used features in AA to demonstrate their
discriminative power in intra-topic and cross-topic AA. The contributions of this study are as follows:
? We propose a new method to identify the author of a document on a topic using a predictive model
trained on examples from different topics. The successful results attained indicate that authors
maintain a consistent style across topics.
? This is the first comprehensive study showing empirically which widely used features in AA are
effective for cross-topic AA. We demonstrate that character n-grams are a strong discriminator among
authors in CTAA and that lexical features are less effective in CTAA than they are for intra-topic AA.
? We empirically illustrate that having the same amount of training documents from multiple topics is
significantly better than having documents from a single topic. It shows that topic variety in training
documents improves the performance of CTAA.
? We also demonstrate that across all genres, adding an extra topic to the training data gives a character
n-gram model a greater boost in performance than to a stop-word, a stylistic or a lexical model. This
is true regardless of the topics on which the model is trained.
? Our proposed methodology is simple to implement suggesting that our findings on cross-topic AA
will be generalizable to other classification problems too.
The paper is organized as follows. Section 2 describes two cross-topic datasets while Section 3 describes
the methodology for our experiments. Section 4 describes different features while Section 5 presents
the experimental setup. We present the evaluation and analysis in Sections 6 and 7. In Section 8, we
describe previous studies on cross-topic AA. Finally, Section 9 presents our conclusions and some future
directions.
2 Cross-Topic Datasets
Although several corpora are available for traditional AA, we need datasets containing documents from a
number of authors from different domains (different topics, different genres). We need many topics to be
able to test cross-topic performance, and many genres to ensure that our findings are robust across different
styles of text. Obtaining such corpora is a challenging task since most authorship attribution studies focus
on a single domain. We have found two datasets that meet our criteria, one having both cross-topic and
cross-genre flavor, and the other having only cross-topic flavor. The first corpus contains communication
samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six
topics (Catholic Church, Gay Marriage, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex
Discrimination), which we call dataset 1. This dataset was obtained from Goldstein-Stewart et al. (2009).
Using this dataset, it is possible to see how the performance of cross-topic AA changes across different
genres.
Another corpus is composed of texts published in The Guardian daily newspaper written by 13 authors
in one genre on four topics (dataset 2) due Stamatatos et al. (2013). It contains opinion articles (comments)
about World, U.K., Culture, and Politics. Table 1 shows some statistics about the datasets.
Corpus #authors #genres #topics
avg avg avg
#docs/author #sentences/doc #words/doc
Dataset 1 21 6 6 36 31.7 600
Dataset 2 13 1 4 64 53 1034
Table 1: Some statistics about dataset 1 and dataset 2.
In dataset 1, the average document length is almost half the average document length in dataset 2, while
the number of authors is almost twice as that in dataset 2. Also, in dataset 1, there is only one document
written by an author on each topic on each genre. However, there are, on average, 16 documents per author
per topic on each genre in dataset 2. Overall, dataset 1 seems more challenging and resembles more a
realistic scenario of forensic investigations where very few short documents per author might be available.
1229
3 Methodology
To answer our research question and test our hypothesis, we designed three training scenarios. First
of all, to demonstrate the complexity of cross-topic tasks, we compare the performance between two
training conditions: Intra-Topic (IT), and Single Cross-Topic (SCT). Once we show that it is important to
solve this CTAA problem, we design one more training condition based on our proposed idea, Multiple
Cross-Topics (MCT) and compare its performance with the IT and the SCT scenarios.
Intra-Topic (IT) In this scenario, all the documents in both the training and test data belong to the same
topic. Although this is a strong assumption that does not hold true in most of the realistic scenarios, we
examine AA under such conditions in order to be able to compare it with our proposed methods.
Single Cross-Topic (SCT) In this setting, the test data consists of documents from a single topic while
the AA model is trained using documents belonging to another topic different than the topic of the test
data, but from the same genre. For example, in dataset 1, for ?Chat? genre, a model could be trained
on a topic ?Gay Marriage? and tested on the topic ?Legalization of Marijuana?. We experiment on all
combinations of test/train topics, i.e., for each test topic, we train separately on each of the remaining
topics.
Multiple Cross-Topics (MCT) Unlike in SCT and IT scenarios, here for each test topic, we train
on documents from all available topics other than the one used for testing. Our assumption is that
authors somehow maintain their unique writeprints across different topics. Therefore, even though the
additional data comes from a topic different than that of the test data, we expect to see improvements in
the performance of cross-topic AA.
In the SCT scenario, since there is a mismatch between the training and test topic, we expect to obtain
experimental results worst than that of the IT scenario. However, we expect that the performance of
cross-topic AA using our proposed MCT scenario will be better than SCT in all the cases.
4 Features
The choice of features depends greatly on the type of classification problem. Previous research has
explored various types of features that can discriminate among the candidate authors. Stylistic features,
character-level and word-level n-grams are the most frequently and successfully used features (Houvardas
and Stamatatos, 2006; Zheng et al., 2006; Frantzeskou et al., 2007; Abbasi and Chen, 2008; Luyckx
and Daelemans, 2011; Koppel et al., 2011). We consider four of the most widely used features. Our
goal behind exploring four different types of features is to understand which features are the best for
cross-topic AA.
Lexical Features. Bag-of-words is one of the commonly used document representations that uses
single-content words as document features. Authorship attribution approaches using a bag-of-words
representation have been found to be effective (Diederich et al., 2003; Kaster et al., 2005; Zhao and
Zobel, 2005; Coyotl-Morales et al., 2006). We call bag-of-words the lexical features since we exclude
stop-words.
Stop-Words. Stop-words carry no or very little semantic meaning of the texts, however, their use
indicates the presence of certain syntactic structures. Although, these words are excluded in the topic-
based text classification tasks due to lack of any semantic information in them, we believe these features
will be effective in cross-domain AA as hinted by previous work (Goldstein-Stewart et al., 2009). Typically,
words such as articles, prepositions, and conjunctions are considered as stop-words. We use a list of stop
words publicly available for download (www.webconfs.com/stop-words.php).
Stylistic Features. Previous research has shown stylistic features to be effective in AA (Stamatatos,
2006; Bhargava et al., 2013). We use 13 stylistic features: number of sentences, number of tokens per
sentence, number of punctuations per sentence, number of emoticons per document, percentage of words
without vowel, percentage of contractions, percentage of total alphabetic characters, percentage of two
consecutive punctuations, percentage of three consecutive punctuations, percentage of upper case words,
1230
total parenthesis count, percentage of sentence initial words with first letter capitalized, and percentage of
words without vowel.
Character n-grams. An n-gram is a sequence of n-contiguous characters. These features capture both
the thematic as well as stylistic information of the texts, and hence have been proven to be very effective
in previous AA studies (Keselj et al., 2003; Peng et al., 2003; Escalante et al., 2011). Since these features
carry stylistic choices of the authors, we believe they will be stable across domains.
5 Experimental Settings
Following the training scenarios discussed previously in Section 3, we performed a set of experiments.
We used 643 predefined stop-words. We considered as lexical features all words that were not stop words,
and were among the 3,500 most frequent words occurring at least twice in the training data. We used
3,500 most frequent character 3-grams occurring at least six times in the training data.
Since dataset 1 is already balanced across authors, we used all the documents from this dataset. However,
dataset 2 was originally imbalanced, therefore we chose at most ten documents per author to avoid a highly
skewed distribution. In order to create a corpus like in the realistic scenarios of forensic investigations
such as tweets, SMS, and emails, we chunked each selected document by sentence boundaries into five
new short documents. This shortening of the documents increases the complexity of the task but enhances
the practical applicability of our methods. We use these chunked versions for evaluating our proposed
method. Splitting the documents in this way has been used in the past to deal with the lack of more
documents per author(Luyckx and Daelemans, 2011; Koppel and Winter, 2014).
We obtained the performance measures using support vector machines (SVMs) implemented in Weka
(Witten and Frank, 2005) with default parameters. We considered using SVMs because preliminary results
showed this algorithm outperformed other reasonable alternatives. We used prediction accuracy as the
performance measure to evaluate different training scenarios. Rather than just comparing the accuracies,
we make most of the decisions based on statistical significance computed using two-tailed t-tests with
95% confidence interval.
All the experiments for cross-topic settings are carried out by controlling the genre. In the IT scenario,
we computed the accuracy on each test topic using stratified 10-fold cross-validation. In the SCT scenario,
for each test topic, prediction accuracy was computed by training separately on each remaining topic and
averaging performances. We computed the accuracy on each test topic in the MCT scenario by withholding
one topic as test topic and training on all other topics. For each training scenario, we computed one single
score for each genre by averaging the accuracies across all test topics belonging to that genre.
6 Experimental Results and Evaluation
In this section, we report results and analysis on different experiments we carried out. We will start
by showing empirically the challenge of cross-topic AA. Then, we will show results of our proposed
approach.
6.1 Is Cross-Topic AA More Difficult than Intra-Topic AA?
Genre
Lexical Features Stop-words Stylistic Features Character n-grams
IT SCT IT-SCT IT SCT IT-SCT IT SCT IT-SCT IT SCT IT-SCT
Chat 25.71 13.11 96.11
?
19.21 16.54 16.14
?
41.90 27.49 34.39
?
39.21 27.56 42.27
?
Essay 26.58 5.92 348.99
?
16.80 11.77 42.74
?
15.66 14.56 7.02 30.90 13.28 132.68
?
Email 19.80 6.22 218.33
?
16.43 12.67 29.68
?
25.29 24.4 3.52 24.94 14.52 71.76
?
Phone Interview 37.62 10.29 265.6
?
33.49 18.00 86.06
?
33.02 16.16 51.06
?
56.99 25.46 123.84
?
Blog 22.18 6.32 250.95
?
15.37 11.25 36.62
?
13.16 11.31 14.06
?
25.38 12.03 110.97
?
Discussion 23.37 11.64 100.77
?
23.37 16.31 43.29
?
30.99 15.8 49.02
?
40.69 25.28 60.96
?
Table 2: Comparison of AA performance on IT and SCT scenarios on dataset 1. For each feature type, the
IT and SCT columns indicate the accuracy (%) while the IT-SCT column is the relative gain of IT over
SCT. For each genre, bold figures represent the best accuracy. Statistical significance is indicated by
?
in
positive direction and by
[
in negative direction.
1231
First of all, we want to understand if the cross-topic problem is more difficult than the intra-topic
problem of AA. We compared the performance of the IT and the SCT scenarios using four types of
features on various genres of dataset 1 as shown in Table 2. We clearly observed that for each genre, and
for each feature type, the performance of the IT scenario is better than the SCT scenario and the difference
is statistically significant. The only exceptions are ?Email? and ?Essay? genres for stylistic features. This
is a strong indication that irrespective of the type of domain as well as the features considered, cross-topic
AA is much more difficult than intra-topic AA.
6.2 Does Our Proposed Method Improve CTAA Performance?
We target to answer: Can we reliably predict the author of a document written in one topic with a predictive
model developed using documents from multiple other topics? We carry out various experiments and
compare the performance of our proposed MCT scenario with that of IT and SCT scenarios separately.
Although, comparing MCT with only SCT would be enough to answer our research question and test our
hypothesis, we are also interested in gaining more insights about cross-topic AA and understanding how
it compares to IT, the simplest case of AA.
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 25.71 13.11 33.02 28.43
?
151.87
?
Essay 26.58 5.92 12.64 -52.45
[
113.51
?
Email 19.80 6.22 11.87 -40.05
[
90.84
?
Phone Interview 37.62 10.29 20.95 -44.31
[
103.6
?
Blog 22.18 6.32 13.15 -40.71 108.07
?
Discussion 23.37 11.64 25.26 8.09 117.01
?
(a) Lexical Features
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 19.21 16.54 33.49 74.34
?
102.48
?
Essay 16.80 11.77 22.06 31.31
?
97.08
?
Email 16.43 12.67 24.97 51.98
?
116.06
?
Phone Interview 33.49 18,00 38.89 16.12 115.67
?
Blog 15.37 11.25 20.43 32.92 81.6
?
Discussion 23.37 16.31 32.59 39.45
?
99.82
?
(b) Stop-words
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 41.90 27.49 37.62 -10.21 36.85
?
Essay 15.66 14.56 23.36 49.17
?
60.44
?
Email 25.29 24.4 33.12 30.96
?
35.74
?
Phone Interview 33.02 16.16 23.49 -28.86 45.36
?
Blog 13.16 11.31 15.67 26.29
?
38.55
?
Discussion 30.99 15.8 24.33 -21.49 53.99
?
(c) Stylistic Features
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 39.21 27.56 57.46 46.54
?
108.49
?
Essay 30.9 13.28 36.66 18.64 176.05
?
Email 24.94 14.52 36.53 46.47
?
151.58
?
Phone Interview 56.99 25.46 56.35 -1.12 121.33
?
Blog 25.38 12.03 33.41 31.64 177.72
?
Discussion 40.69 25.28 49.91 22.66
?
97.43
?
(d) Character n-grams
Table 3: Performance of lexical, stop-words, stylistic, and character n?gram features on dataset 1. The
SCT, IT and MCT columns indicate the accuracy (%) while the MCT-SCT and MCT-IT columns present
the relative gain of MCT over the other scenario. Statistical significance is indicated by
?
in positive
direction and by
[
in negative direction.
MCT-SCT columns on Table 3 illustrate the statistical significance of MCT over SCT in a positive
direction for all the genres. Using any type of feature in any genre, it is possible to significantly improve
the performance of CTAA by training a machine learning algorithm using documents from all available
out-of-domain topics. This serves as evidence to confirm our hypothesis and answer our research question
that documents written in one topic can be reliably predicted with a model developed using documents
from multiple other topics. This indicates that authors maintain a consistent writing style across topics.
In the MCT-IT column in Table 3(a), we can seen that the IT is significantly better than the MCT in
three genres, while the MCT is better than the IT in only one. This is because lexical features directly
capture the choices of authors in a certain thematic area, and hence they yield a good performance in the
intra-topic setting. However, we observed contrasting and interesting patterns using stop-words, stylistic
features, and character n-grams (MCT-IT column of Tables 3(b), 3(c), and 3(d)). MCT was better than IT,
and the difference was significantly better, in 10 genres, while IT performance was significantly better
than MCT in none of the genres. This is a very interesting finding as we observed that the cross-topic AA
problem can be solved as effectively as the intra-topic AA problem using these features and a variety of
topics.
Also using dataset 2, we found that for each type of feature, MCT is better than SCT, and the difference
is statistically significant as shown in Table 4. This is another supporting evidence to our hypothesis. The
small gain of IT over MCT suggests that our proposed approach is competitive even with the IT scenario.
1232
Feature Type IT SCT MCT MCT-IT MCT-SCT
Lexical Features 63.98 21.46 38.62 -39.64
[
79.96
?
Stop-words 45.01 31.66 41.21 -8.44 30.16
?
Stylistic Features 32.85 27.46 32.17 -2.07 17.15
?
Character n-grams 75.08 45.87 64.54 -14.04
[
40.7
?
Table 4: Performance of four types of features on three different training scenarios on dataset 2. For each
feature type, the SCT, IT and MCT columns indicate the accuracy (%) while the MCT-SCT and MCT-IT
columns present the relative gain of MCT over the other scenario. Statistical significance is indicated by
?
in positive direction and by
[
in negative direction.
6.3 Sensitivity of Features to Changes in Topic
We also want to demonstrate the behavior of four different feature types to changes in topic. We want to
test if lexical features favor intra-topic AA and character n-grams favor cross-topic AA. Unlike lexical
features, character n-grams carry stylistic choices of authors, and hence are expected to be robust across
topics. In Table 2, for each genre, the relative gain of IT over SCT using lexical features is highest
compared to that of stop-words, stylistic features, and character n-grams, thereby indicating that lexical
features are more effective for ITAA than for CTAA. It is also apparent in Table 2 that the gain of
characters n-grams is always better than that of stop-words and stylistic features. While looking at the
performance on the SCT scenario using four features, it is observed that character n-grams give the best
performance, while stop-words and stylistic features give the second best performance, which leaves
lexical features at the bottom. This is because the first three features are topic-independent and hence
were able to better discriminate among authors in cross-topic scenarios than lexical features. However,
overall, character n-grams have the highest discriminative power in both IT and SCT, which confirms
findings of earlier research (Stamatatos, 2013).
In Table 3, character n-grams, when compared to lexical features, stop-words, and stylistic features,
yield the highest average relative gain on MCT over the SCT scenario (138.77%, vs 114.15% for lexical
features, 97.41% for stop-words, 46.55% for stylistic features). Also, comparing the prediction accuracies
of all four features separately in SCT, IT, and MCT scenarios, it is observed that character n-grams score
best in most of the genres on each training scenario. This confirms that character n-grams have higher
discriminative power in cross-topic AA than stop-words, stylistic features and lexical features.
For cross-topic AA, we observed that the accuracy across the board is not high. It is because the CTAA
task is harder than other single domain classification tasks since the topics of the test data are fully disjoint
with the topics of the training data. On top of that, the shorter document length makes it more challenging.
The current system might not be production quality, but our findings will enable better models in the
future that hopefully will be accurate enough to solve CTAA problems more effectively.
6.4 Cross-Topic AA with Varying Number of Training Topics
For traditional AA, it has been shown that around 10,000 word-tokens per author suffice as a ?reliable
minimum for an authorial set? (Burrows, 2007). In our study, we have as few as 600 word-tokens per
author, much less than the minimum size requirement stated by previous research. In this section, we look
at how performance improves with increase in amount of training data by adding additional topics.
To explore this, we experimented by training on documents from all possible combinations of topics. In
dataset 1, there are a total of six topics. Therefore, for each test topic, we experiment separately using
one, two, three, four, and five topics for training. When measuring performance on k training topics, we
gather all possible combinations of training on k of the five topics and then average the performance
across all these combinations. For example, if we use two topics for training, then for each test topic, there
are
(
5
2
)
= 10 possible training combinations that we then average to get a final score. We illustrate the
results in Figure 1 for four genres using four types of features. Irrespective of the genres, topics, and types
of features used, CTAA performance improves gradually with addition of more data. In most genres, this
improvement seems to be almost linear with the number of topics trained on, suggesting that gathering
more out-of-topic data should continue to improve the performance. We also observed that the character
1233
(a) Genre = Discussion (b) Genre = Phone Interview
(c) Genre = Essay (d) Genre = Blog
Figure 1: Effect of training on varying number of topics in CTAA using lexical, stop-words, stylistic, and
character n-gram features on dataset 1.
n-grams are the most effective author discriminator in cross-topic AA.
We performed a deeper analysis of the effect of individual topics, which is shown in Table 5. We took
an initial topic as training data and then paired it with each of the other topics as additional training data
and measured the average performance gain from the addition of the second topic. It is shown that across
all genres, adding a second topic to the training data gives a character n-gram model greatest boost in
performance than to a stop word or a stylistic or a lexical model. This is true regardless of the topics on
which the model is trained. We do not observe negative transfer as in transfer learning (Pan and Yang,
2010) because in cross topic AA authors maintain styles across topics.
Initial Topic
Genre = Chat Genre = Email
Lexical Stop-words Stylistic Character n-grams Lexical Stop-words Stylistic Character n-grams
Sex Discrimination 5.85 5.57 1.67 10.33 2.24 7.29 8.86 9.72
Legalization of Marijuana 7.86 7.76 1.57 12.19 2.91 3.32 5.21 7.39
Catholic Church 6.24 8.76 6.24 14.33 2.41 4.48 3.59 5.22
Privacy Rights 5.9 4.66 1.9 14.05 2.97 6.45 4.6 10.06
War in Iraq 8.1 7.95 3.48 15.57 3.96 7.58 2.99 7.79
Gay Marriage 7.19 5.85 7.19 10.29 2.57 4.31 1.98 6.82
Table 5: Average performance gain from adding an additional topic as training data across different initial
topics on dataset 1. Each value is the average accuracy gain after adding the second topic.
7 Is it Just ?More Data? that is Helping or is ?Diversity? Relevant?
The quantity of training data was not controlled in the experiments presented in Section 6, therefore,
we performed some additional experiments where we did control for this. In Table 6, we present the
comparison of SCT and MCT scenarios using the same amount of training data to understand whether
the performance improvement in the MCT scenario is due to diversity or due to the fact of adding more
data. We use dataset 1 to make this comparison. For the SCT scenario, for each test topic, we averaged
1234
performance over three random samplings, where in each sampling we randomly selected four documents
per author in each training topic. For the MCT scenario, for each test topic, we averaged performance
Lexical Features Stop-words Stylistic Features Character n-grams
Genre SCT MCT MCT-SCT SCT MCT MCT-SCT SCT MCT MCT-SCT SCT MCT MCT-SCT
Chat 12.24 13.94 13.89
?
14.37 16.35 13.78
?
26.52 28.52 7.54
?
24.39 25.17 3.2
?
Essay 9.11 11.3 24.04
?
12.43 14.12 13.6
?
21.35 22.93 7.4
?
18.37 19.58 6.59
?
Discussion 9.65 10.52 9.02
?
12.93 13.7 5.96
?
19.57 20.85 6.54
?
19.84 21.48 8.27
?
Email 8.84 9.98 12.9
?
12.48 13.91 11.46
?
20.89 21.92 4.93
?
17.91 20.76 15.91
?
Phone Interview 8.94 10.84 21.25
?
14.65 17.67 20.61
?
19.73 20.94 6.13
?
18.84 26.35 39.86
?
Blog 8.45 9.66 14.32
?
12.78 14.05 9.94
?
18.53 19.62 5.88
?
17.58 19.95 13.48
?
Table 6: Comparison of MCT and SCT scenarios on controlled training data using four types of features
on dataset 1. For each feature type, the SCT and MCT columns indicate the accuracy (%) while the
MCT-SCT columns present the relative gain of MCT over the SCT. Statistical significance is indicated by
?
in positive direction and by
[
in negative direction.
over three random samplings, where in each sampling we randomly selected four training topics. For
each selection of four training topics, we averaged performance over three random samplings where in
each sampling we randomly selected one document per author in each training topic. Thus, we ended up
with the same number of documents for training both models. Even with the same amount of training
data, training on documents from different topics is better than training on documents from a single topic,
with statistically significant performance gains ranging from 3.2% to 39.86% as shown in Table 6. This
demonstrates that data from a diverse set of topics will still give a boost in performance and is always
significantly better than using data from the same topic.
8 Related Work
The majority of the work in authorship attribution deals with single-domain datasets. However, there
have been a handful of studies that add some cross-topic flavor in the AA task (Mikros and Argiri, 2007;
Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013). Mikros et al. (2007) concluded that
many stylometric variables are actually discriminating topic rather than author and their use in AA should
be done carefully. However, the study was performed on a single corpus containing only two authors
in two topics that raises questions on reliability of their conclusions. Stamatatos (2013) illustrated the
effectiveness of character n-grams in cross-topic AA. It was also shown in that study that avoiding rare
features is effective in both intra-topic and cross-topic AA. However, all these conclusions came from
training an SVM classifier in only one fixed topic. In contrast, in our paper, we draw our conclusions from
all possible training/testing combinations rather than fixing in advance the training topic.
Goldstein-Stewart et al. (2009) also carried out some cross-topic experiments by concatenating the texts
of an author from different genres. This experimental setting results in a corpus where each test document
contains a mix of genres, which is not representative of real world AA problems. Still, to provide some
comparisons to the work of Goldstein-Stewart et al. (2009), we concatenated all the texts in dataset 1
produced by an individual on a single topic, across all genres to produce one document per author on each
topic. We compare our results with those reported in the paper under same training/testing conditions. We
withheld one topic and trained on documents from the other five topics.
Test Topic Lexical Stop-words Stylistic Character n-grams Stop-words + Character n-grams Previous Work
Sex Discrimination 66.67 76.19 33.33 95.24 95.24 95
Catholic Church 76.19 95.24 38.10 95.24 100 95
Gay Marriage 80.95 80.95 42.86 90.48 90.48 95
Legalization of Marijuana 52.38 66.67 33.33 95.24 100 100
Privacy Rights 42.86 52.38 28.57 95.24 90.48 100
War in Iraq 57.14 71.43 38.10 100 100 81
Average 62.7 73.81 35.72 95.24 96.03 94.33
Table 7: Comparing performance of our work with previous work in the same training/testing setting. The
results in the last column were obtained from Goldstein-Stewart et al.(2009). For each test topic, the bold
figure represents the best performance.
1235
The last column of Table 7 presents the results obtained by using the combination of stop-words and 88
Linguistic Inquiry and Word Count (LIWC) features as reported in Goldstein-Stewart et al. (2009). We
observed that the combination of character n-grams and stop-words, on average, performs better than
those reported in the paper. On this fixed training/testing scenario, we see better accuracies, as high as
100%, across the board. This is because, in this experiment, each training sample on average was ? 25
times longer than the training sample in our chunked versions. This illustrates that authorship attribution
of short documents, as in our chunked versions, is a challenging task, but we believe it resembles a more
realistic scenario of forensic investigations.
9 Conclusions and Future Work
In this research, we presented the first comprehensive study with rigorous analysis on cross-topic AA.
Although previous work had hinted some of our findings, it was based on very limited experiments (using
only one fixed topic for training). We investigated CTAA using all possible combinations of topics to draw
more robust and stable conclusions. We first illustrated the difficulty of cross-topic AA by comparing its
performance with intra-topic AA using different types of features. We demonstrated that a framework
trained on documents belonging to thematic areas different than that of the documents under investigation
statistically improves the performance of cross-topic AA. This improves the ability of the model to find the
authors of documents belonging to a new topic not present during the training of the model. By controlling
the training data, we demonstrated that training on diverse topics is better than training on a single topic
confirming that MCT not only benefits from more data but also from a thematic variety. We also showed a
statistical analysis that lexical features are closer to the thematic area and hence were an effective author
discriminator in intra-topic attribution. Similarly, character n-grams prove to be a very powerful feature
especially in a condition where training and test documents come from different thematic areas. Although
intra-topic AA is easier than cross-topic AA, our proposed model for CTAA achieves performance close
or in some cases, better than that of an intra-topic AA model. Another interesting conclusion of our study
is that addition of more training data from any topic, no matter how distant or close it is with the topic of
documents under investigation, improves the performance of CTAA for all types of features. We believe
that our contribution to cross-topic AA will be generalizable to other classification problems too.
In the future, we plan to explore the cross-genre problem of AA that is critical for tasks like linking
user accounts across emails, blogs, and other social media. Our proposed CTAA approach can be directly
applied to the cross-genre problem but we may discover different feature behavior in this scenario. We
also plan to explore domain adaptation and transfer learning techniques to solve CDAA problems.
Acknowledgements
This research was partially supported by ONR grant N00014-12-1-0217, NSF award 1254108, and NSF
award 1350360. It was also supported in part by the CONACYT grant 134186 and the WIQ-EI IRSES
project (grant no. 269180) within the FP 7 Marie Curie.
References
A. Abbasi and H. Chen. 2008. Writeprints: A stylometric approach to identity-level identification and similarity
detection in cyberspace. ACM Trans. Inf. Syst., 26(2):7:1?7:29, April.
M. Bhargava, P. Mehndiratta, and K. Asawa. 2013. Stylometric analysis for authorship attribution on twitter. In
Big Data Analytics, volume 8302 of Lecture Notes in Computer Science, pages 37?47. Springer International
Publishing.
J. Burrows. 2007. All the way through: Testing for authorship in different frequency strata. Literary & Linguistic
Computing, 22:27 ? 47.
R. Mar??a Coyotl-Morales, L. Villase?nor Pineda, M. Montes-y G?omez, and P. Rosso. 2006. Authorship attribution
using word sequences. In Proceedings of the 11th Iberoamerican Conference on Progress in Pattern Recogni-
tion, Image Analysis and Applications, CIARP?06, pages 844?853, Berlin, Heidelberg. Springer-Verlag.
1236
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001. Multi-topic e-mail authorship attribution forensics.
In Proceedings of the Workshop on Data Mining for Security Applications, 8th ACM Conference on Computer
Security.
J. Diederich, J. Kindermann, E. Leopold, and G. Paass. 2003. Authorship attribution with support vector machines.
Applied Intelligence, 19:109?123, May.
H. J. Escalante, T. Solorio, and M. Montes-y G?omez. 2011. Local histograms of character n-grams for author-
ship attribution. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 288?298, Portland, Oregon, USA, June. Association for Computational
Linguistics.
G. Frantzeskou, E. Stamatatos, S. Gritzalis, and C. E. Chaski. 2007. Identifying authorship by byte-level n-grams:
The source code author profile (SCAP) method. Journal of Digital Evidence, 6(1).
J. Goldstein-Stewart, R. Winder, and R. Evans Sabin. 2009. Person identification from text and speech genre
samples. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics, EACL ?09, pages 336?344, Stroudsburg, PA, USA. Association for Computational Linguistics.
J. Houvardas and E. Stamatatos. 2006. N-gram feature selection for authorship identification. In J. Euzenat and
J. Domingue, editors, AIMSA 2006, volume 4183 of LNAI, pages 77?86.
A. Kaster, S. Siersdorfer, and G. Weikum. 2005. Combining text and linguistic document representations for
authorship attribution. In SIGIR Workshop: Stylistic Analysis of Text for Information Access, pages 27?35.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-gram based author profiles for authorship attribution. In
Proceedings of the Pacific Association for Computational Linguistics, pages 255?264.
M. Koppel and Y. Winter. 2014. Determining if two documents are written by the same author. Journal of the
Association for Information Science and Technology, 65(1):178?187.
M. Koppel, J. Schler, and S. Argamon. 2011. Authorship attribution in the wild. Language Resources and
Evaluation, 45:83?94.
K. Luyckx and W. Daelemans. 2011. The effect of author set size and data size in authorship attribution. Literary
and Linguistic Computing, 26(1):35?55.
G. K. Mikros and E. K. Argiri. 2007. Investigating topic influence in authorship attribution. In Proceedings of
the International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection,
pages 29?35.
S. Jialin Pan and Q. Yang. 2010. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng.,
22(10):1345?1359, October.
F. Peng, D. Schuurmans, V. Keselj, and S. Wang. 2003. Language independent authorship attribution using char-
acter level language models. In 10th Conference of the European Chapter of the Association for Computational
Linguistics, EACL, pages 267?274.
A. I. Schein, J. F. Caver, R. J. Honaker, and C. H. Martell. 2010. Author attribution evaluation with novel topic
cross-validation. In The 2010 International Conference on Knowledge Discovery and Information Retrieval,
Valencia, Spain, October.
E. Stamatatos. 2006. Authorship attribution based on feature set subspacing ensembles. International Journal on
Artificial Intelligence tools, 15(5):823?838.
E. Stamatatos. 2011. Plagiarism detection using stopword n-grams. Journal of the American Society for Informa-
tion Science and Technology, 62(12):2512?2527.
E. Stamatatos. 2013. On the robustness of authorship attribution based on character n-gram features. Journal of
Law & Policy, 21(2):421 ? 439.
I. H. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan
Kauffmann, 2nd edition.
Y. Zhao and J. Zobel. 2005. Effective and scalable authorship attribution using function words. In Proceedings of
2nd Asian Information Retrieval Symposium, volume 3689 of LNCS, pages 174?189, Jeju Island, Korea.
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A framework for authorship identification of online messages:
Writing-style features and classification techniques. J. Am. Soc. Inf. Sci. Technol., 57(3):378?393, February.
1237
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288?298,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Local Histograms of Character N -grams for Authorship Attribution
Hugo Jair Escalante
Graduate Program in Systems Eng.
Universidad Auto?noma de Nuevo Leo?n,
San Nicola?s de los Garza, NL, 66450, Me?xico
hugo.jair@gmail.com
Thamar Solorio
Dept. of Computer and Information Sciences
University of Alabama at Birmingham,
Birmingham, AL, 35294, USA
solorio@cis.uab.edu
Manuel Montes-y-Go?mez
Computer Science Department, INAOE,
Tonantzintla, Puebla, 72840, Me?xico
Department of Computer and Information Sciences,
University of Alabama at Birmingham,
Birmingham, AL, 35294, USA
mmontesg@cis.uab.edu
Abstract
This paper proposes the use of local his-
tograms (LH) over character n-grams for au-
thorship attribution (AA). LHs are enriched
histogram representations that preserve se-
quential information in documents; they have
been successfully used for text categorization
and document visualization using word his-
tograms. In this work we explore the suitabil-
ity of LHs over n-grams at the character-level
for AA. We show that LHs are particularly
helpful for AA, because they provide useful
information for uncovering, to some extent,
the writing style of authors. We report experi-
mental results in AA data sets that confirm that
LHs over character n-grams are more help-
ful for AA than the usual global histograms,
yielding results far superior to state of the art
approaches. We found that LHs are even more
advantageous in challenging conditions, such
as having imbalanced and small training sets.
Our results motivate further research on the
use of LHs for modeling the writing style of
authors for related tasks, such as authorship
verification and plagiarism detection.
1 Introduction
Authorship attribution (AA) is the task of deciding
whom, from a set of candidates, is the author of a
given document (Houvardas and Stamatatos, 2006;
Luyckx and Daelemans, 2010; Stamatatos, 2009b).
There is a broad field of application for AA meth-
ods, including spam filtering (de Vel et al, 2001),
fraud detection, computer forensics (Lambers and
Veenman, 2009), cyber bullying (Pillay and Solorio,
2010) and plagiarism detection (Stamatatos, 2009a).
Therefore, the development of automated AA tech-
niques has received much attention recently (Sta-
matatos, 2009b). The AA problem can be natu-
rally posed as one of single-label multiclass clas-
sification, with as many classes as candidate au-
thors. However, unlike usual text categorization
tasks, where the core problem is modeling the the-
matic content of documents (Sebastiani, 2002), the
goal in AA is modeling authors? writing style (Sta-
matatos, 2009b). Hence, document representations
that reveal information about writing style are re-
quired to achieve good accuracy in AA.
Word and character based representations have
been used in AA with some success so far (Houvar-
das and Stamatatos, 2006; Luyckx and Daelemans,
2010; Plakias and Stamatatos, 2008b). Such rep-
resentations can capture style information through
word or character usage, but they lack sequential in-
formation, which can reveal further stylistic infor-
mation. In this paper, we study the use of richer
document representations for the AA task. In partic-
ular, we consider local histograms over n-grams at
the character-level obtained via the locally-weighted
bag of words (LOWBOW) framework (Lebanon et
al., 2007).
Under LOWBOW, a document is represented by a
set of local histograms, computed across the whole
document but smoothed by kernels centered on dif-
ferent document locations. In this way, document
288
representations preserve both word/character usage
and sequential information (i.e., information about
the positions in which words or characters occur),
which can be more helpful for modeling the writ-
ing style of authors. We report experimental re-
sults in an AA data set used in previous studies un-
der several conditions (Houvardas and Stamatatos,
2006; Plakias and Stamatatos, 2008b; Plakias and
Stamatatos, 2008a). Results confirm that local his-
tograms of character n-grams are more helpful for
AA than the usual global histograms of words or
character n-grams (Luyckx and Daelemans, 2010);
our results are superior to those reported in re-
lated works. We also show that local histograms
over character n-grams are more helpful than lo-
cal histograms over words, as originally proposed
by (Lebanon et al, 2007). Further, we performed
experiments with imbalanced and small training
sets (i.e., under a realistic AA setting) using the
aforementioned representations. We found that the
LOWBOW-based representation resulted even more
advantageous in these challenging conditions. The
contributions of this work are as follows:
? We show that the LOWBOW framework can be
helpful for AA, giving evidence that sequential in-
formation encoded in local histograms is useful for
modeling the writing style of authors.
? We propose the use of local histograms over
character-level n-grams for AA. We show that
character-level representations, which have proved
to be very effective for AA (Luyckx and Daelemans,
2010), can be further improved by adopting a local
histogram formulation. Also, we empirically show
that local histograms at the character-level are more
helpful than local histograms at the word-level for
AA.
? We study several kernels for a support vector ma-
chine AA classifier under the local histograms for-
mulation. Our study confirms that the diffusion ker-
nel (Lafferty and Lebanon, 2005) is the most ef-
fective among those we tried, although competitive
performance can be obtained with simpler kernels.
? We report experimental results that are superior to
state of the art approaches (Plakias and Stamatatos,
2008b; Plakias and Stamatatos, 2008a), with im-
provements ranging from 2%?6% in balanced data
sets and from 14%? 30% in imbalanced data sets.
2 Related Work
AA can be faced as a multiclass classifica-
tion task with as many classes as candidate au-
thors. Standard classification methods have been
applied to this problem, including support vec-
tor machine (SVM) classifiers (Houvardas and Sta-
matatos, 2006) and variants thereon (Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a),
neural networks (Tearle et al, 2008), Bayesian clas-
sifiers (Coyotl-Morales et al, 2006), decision tree
methods (Koppel et al, 2009) and similarity based
techniques (Keselj et al, 2003; Lambers and Veen-
man, 2009; Stamatatos, 2009b; Koppel et al, 2009).
In this work, we chose an SVM classifier as it has
reported acceptable performance in AA and because
it will allow us to directly compare results with pre-
vious work that has used this same classifier.
A broad diversity of features has been used to rep-
resent documents in AA (Stamatatos, 2009b). How-
ever, as in text categorization (Sebastiani, 2002),
word-based and character-based features are among
the most widely used features (Stamatatos, 2009b;
Luyckx and Daelemans, 2010). With respect to
word-based features, word histograms (i.e., the bag-
of-words paradigm) are the most frequently used
representations in AA (Zhao and Zobel, 2005;
Argamon and Levitan, 2005; Stamatatos, 2009b).
Some researchers have gone a step further and
have attempted to capture sequential information
by using n-grams at the word-level (Peng et al,
2004) or by discovering maximal frequent word se-
quences (Coyotl-Morales et al, 2006). Unfortu-
nately, because of computational limitations, the lat-
ter methods cannot discover enough sequential in-
formation from documents (e.g., word n-grams are
often restricted to n ? {1, 2, 3}, while full se-
quential information would be obtained with n ?
{1 . . . D} where D is the maximum number of
words in a document).
With respect to character-based features, n-grams
at the character level have been widely used in AA
as well (Plakias and Stamatatos, 2008b; Peng et
al., 2003; Luyckx and Daelemans, 2010). Peng et
al. (2003) propose the use of language models at the
n-gram character-level for AA, whereas Keselj et
al. (2003) build author profiles based on a selection
of frequent n-grams for each author. Stamatatos and
co-workers have studied the impact of feature se-
lection, with character n-grams, in AA (Houvardas
and Stamatatos, 2006; Stamatatos, 2006a), ensem-
ble learning with character n-grams (Stamatatos,
2006b) and novel classification techniques based
289
on characters at the n-gram level (Plakias and Sta-
matatos, 2008a).
Acceptable performance in AA has been reported
with character n-gram representations. However,
as with word-based features, character n-grams are
unable to incorporate sequential information from
documents in their original form (in terms of the
positions in which the terms appear across a doc-
ument). We believe that sequential clues can be
helpful for AA because different authors are ex-
pected to use different character n-grams or words
in different parts of the document. Accordingly,
in this work we adopt the popular character-based
and word-based representations, but we enrich them
in a way that they incorporate sequential informa-
tion via the LOWBOW framework. Hence, the pro-
posed features preserve sequential information be-
sides capturing character and word usage informa-
tion. Our hypothesis is that the combination of se-
quential and frequency information can be particu-
larly helpful for AA.
The LOWBOW framework has been mainly used
for document visualization (Lebanon et al, 2007;
Mao et al, 2007), where researchers have used in-
formation derived from local histograms for dis-
playing a 2D representation of document?s con-
tent. More recently, Chasanis et al (2009) used
the LOWBOW framework for segmenting movies
into chapters and scenes. LOWBOW representa-
tions have also been applied to discourse segmen-
tation (AMIDA, 2007) and have been suggested for
text summarization (Das and Martins, 2007). How-
ever, to the best of our knowledge the use of the
LOWBOW framework for AA has not been studied
elsewhere. Actually, the only two references using
this framework for text categorization are (Lebanon
et al, 2007; AMIDA, 2007). The latter can be due to
the fact that local histograms provide little gain over
usual global histograms for thematic classification
tasks. In this paper we show that LOWBOW rep-
resentations provide important improvements over
global histograms for AA; in particular, local his-
tograms at the character-level achieve the highest
performance in our experiments.
3 Background
This section describes preliminary information on
document representations and pattern classification
with SVMs.
3.1 Bag of words representations
In the bag of words (BOW) representation, docu-
ments are represented by histograms over the vo-
cabulary1 that was used to generate a collection of
documents; that is, a document i is represented as:
di = [xi,1, . . . , xi,|V |] (1)
where V is the vocabulary and |V | is the number of
elements in V , di,j = xi,j is a weight that denotes
the contribution of term j to the representation of
document i; usually xi,j is related to the occurrence
(binary weighting) or the weighted frequency of oc-
currence (e.g., the tf-idf weighting scheme) of the
term j in document i.
3.2 Locally-weighted bag-of-words
representation
Instead of using the BOW framework directly, we
adopted the LOWBOW framework for document
representation (Lebanon et al, 2007). The underly-
ing idea in LOWBOW is to compute several local
histograms per document, where these histograms
are smoothed by a kernel function, see Figure 1.
The parameters of the kernel specify the position of
the kernel in the document (i.e., where the local his-
togram is centered) and its scale (i.e., to what extent
it is smoothed). In this way the sequential informa-
tion in the document is preserved together with term
usage statistics.
Let Wi = {wi,1, . . . , wi,Ni}, denote the terms
(in order of appearance) in document i where Ni
is the number of terms that appear in document i
and wi,j ? V is the term appearing at position
j; let vi = {vi,1, . . . , vi,Ni} be the set of indexes
in the vocabulary V of the terms appearing in Wi,
such that vi,j is the index in V of the term wi,j ;
let t = [t1, . . . , tNi ] be a set of (equally spaced)
scalars that determine intervals, with 0 ? tj ? 1 and?Ni
j=1 tj = 1, such that each tj can be associated to
a position in Wi. Given a kernel smoothing function
Ks?,? : [0, 1] ? R with location parameter ? and
scale parameter ?, where ?kj=1 Ks?,?(tj) = 1 and
1In the following we will refer to arbitrary vocabularies,
which can be formed with terms from either words or character
n-grams.
290
Figure 1: Diagram of the process for obtaining local
histograms. Terms (wi) appearing in different posi-
tions (1, . . . , N ) of the document are weighted according
to the locations (?1, . . . , ?k) of the smoothing function
K?,?(x). Then, the term position weighting is combined
with term frequency weighting for obtaining local his-
tograms over the terms in the vocabulary (1, . . . , |V |).
? ? [0, 1]. The LOWBOW framework computes a
local histogram for each position ?j ? {?1, . . . , ?k}
as follows:
dlji,{vi,1,...,vi,Ni} = di,{vi,1,...,vi,Ni} ?K
s
?j ,?(t) (2)
where dli,vj :vj 6?vi = const, a small constant value,
and di,j is defined as above. Hence, a set dl{1,...,k}i
of k local histograms are computed for each doc-
ument i. Each histogram dlji carries information
about the distribution of terms at a certain position
?j of the document, where ? determines how the
nearby terms to ?j influence the local histogram
j. Thus, sequential information of the document is
considered throughout these local histograms. Note
that when ? is small, most of the sequential informa-
tion is preserved, as local histograms are calculated
at very local scales; whereas when ? ? 1, local his-
tograms resemble the traditional BOW representa-
tion.
Under LOWBOW documents can be represented
in two forms (Lebanon et al, 2007): as a single his-
togram dLi = const ?
?k
j=1 dlji (hereafter LOW-
BOW histograms) or by the set of local histograms
itself dl{1,...,k}i . We performed experiments with
both forms of representation and considered words
and n-grams at the character-level as terms (c.f. Sec-
tion 5). Regarding the smoothing function, we con-
sidered the re-normalized Gaussian pdf restricted to
[0, 1]:
Ks?,?(x) =
?
?
?
N (x;?,?)
?( 1??? )??(??? ) if x ? [0, 1]
0 otherwise
(3)
where ?(x) is the cumulative distribution function
for a Gaussian with mean 0 and standard deviation 1,
evaluated at x, see (Lebanon et al, 2007) for further
details.
3.3 Support vector machines
Support vector machines (SVMs) are pattern classi-
fication methods that aim to find an optimal sepa-
rating hyperplane between examples from two dif-
ferent classes (Shawe-Taylor and Cristianini, 2004).
Let {xi, yi}N be pairs of training patterns-outputs,
where xi ? Rd and y ? {?1, 1}, with d the di-
mensionality of the problem. SVMs aim at learn-
ing a mapping from training instances to outputs.
This is done by considering a linear function of the
form: f(x) = Wx + b, where parameters W and b
are learned from training data. The particular linear
function considered by SVMs is as follows:
f(x) =
?
i
?iyiK(xi, x)? b (4)
that is, a linear function over (a subset of) training
examples, where ?i is the weight associated with
training example i (those for which ?i > 0 are the so
called support vectors) and yi is the label associated
with training example i, K(xi, xj) is a kernel2 func-
tion that aims at mapping the input vectors, (xi, xj),
into the so called feature space, and b is a bias
term. Intuitively, K(xi, xj) evaluates how similar
instances xi and xj are, thus the particular choice of
kernel is problem dependent. The parameters in ex-
pression (4), namely ?{1,...,N} and b, are learned by
using exact optimization techniques (Shawe-Taylor
and Cristianini, 2004).
2One should not confuse the kernel smoothing function,
Ks?,?(x), defined in Equation (3) with the Mercer kernel in
Equation (4), as the former acts as a smoothing function and
the latter acts as a similarity function.
291
4 Authorship Attribution with LOWBOW
Representations
For AA we represent the training documents of
each author using the framework described in Sec-
tion 3.2, thus each document of each candidate au-
thor is either a LOWBOW histogram or a bag of lo-
cal histograms (BOLH). Recall that LOWBOW his-
tograms are an un-weighted sum of local histograms
and hence can be considered a summary of term us-
age and sequential information; whereas the BOLH
can be seen as term occurrence frequencies across
different locations of the document.
For both types of representations we consider an
SVM classifier under the one-vs-all formulation for
facing the AA problem. We consider SVM as base
classifier because this method has proved to be very
effective in a large number of applications, including
AA (Houvardas and Stamatatos, 2006; Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a);
further, since SVMs are kernel-based methods, they
allow us to use local histograms for AA by consid-
ering kernels that work over sets of histograms.
We build a multiclass SVM classifier by con-
sidering the pairs of patterns-outputs associated to
documents-authors. Where each pattern can be ei-
ther a LOWBOW histogram or the set of local his-
tograms associated with the corresponding docu-
ment, and the output associated to each pattern is
a categorical random variable (outputs) that asso-
ciates the representation of each document to its cor-
responding author y1,...,N ? {1, . . . , C}, with C
the number of candidate authors. For building the
multiclass classifier we adopted the one-vs-all for-
mulation, where C binary classifiers are built and
where each classifier fi discriminates among exam-
ples from class i (positive examples) and the rest
j : j ? {1, . . . , C}, j 6= i; despite being one of the
simplest formulations, this approach has shown to
obtain comparable and even superior performance to
that obtained by more complex formulations (Rifkin
and Klautau, 2004).
For AA using LOWBOW histograms, we con-
sider a linear kernel since it has been success-
fully applied to a wide variety of problems (Shawe-
Taylor and Cristianini, 2004), including AA (Hou-
vardas and Stamatatos, 2006; Plakias and Sta-
matatos, 2008b). However, standard kernels can-
not work for input spaces where each instance is de-
scribed by a set of vectors. Therefore, usual kernels
are not applicable for AA using BOLH. Instead, we
rely on particular kernels defined for sets of vectors
rather than for a single vector. Specifically, we con-
sider kernels of the form (Rubner et al, 2001; Grau-
man, 2006):
K(P,Q) = exp (? D(P,Q)
2
?
) (5)
where D(P,Q) is the sum of the distances between
the elements of the bag of local histograms asso-
ciated to author P and the elements of the bag of
histograms associated with author Q; ? is the scale
parameter of K. Let P = {p1, . . . , pk} and Q =
{q1, . . . , qk} be the elements of the bags of local
histograms for instances P and Q, respectively, Ta-
ble 1 presents the distance measures we consider for
AA using local histograms.
Kernel Distance
Diffusion D(P,Q) = ?kl=1 arccos
(??pl ?
?ql?
)
EMD D(P,Q) = EMD(P,Q)
Eucidean D(P,Q) =
??k
l=1(pl ? ql).2
?2 D(P,Q) =
??k
l=1
(pl?ql)2
(pl+ql)
Table 1: Distance functions used to calculate the kernel
defined in Equation (5).
Diffusion, Euclidean, and ?2 kernels compare lo-
cal histograms one to one, which means that the lo-
cal histograms calculated at the same locations are
compared to each other. We believe that for AA
this is advantageous as it is expected that an author
uses similar terms at similar locations of the docu-
ment. The Earth mover?s distance (EMD), on the
other hand, is an estimate of the optimal cost in tak-
ing local histograms from Q to local histograms in
P (Rubner et al, 2001); that is, this measure com-
putes the optimal matching distance between local
histograms from different authors that are not neces-
sarily computed at similar locations.
5 Experiments and Results
For our experiments we considered the data set used
in (Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a). This corpus is a subset of the
RCV1 collection (Lewis et al, 2004) and comprises
292
documents authored by 10 authors. All of the docu-
ments belong to the same topic. Since this data set
has predefined training and testing partitions, our re-
sults are comparable to those obtained by other re-
searchers. There are 50 documents per author for
training and 50 documents per author for testing.
We performed experiments with LOWBOW3 rep-
resentations at word and character-level. For the ex-
periments with words, we took the top 2,500 most
common words used across the training documents
and obtained LOWBOW representations. We used
this setting in agreement with previous work on
AA (Houvardas and Stamatatos, 2006). For our
character n-gram experiments, we obtained LOW-
BOW representations for character 3-grams (only
n-grams of size n = 3 were used) considering
the 2, 500 most common n-grams. Again, this set-
ting was adopted in agreement with previous work
on AA with character n-grams (Houvardas and
Stamatatos, 2006; Plakias and Stamatatos, 2008b;
Plakias and Stamatatos, 2008a; Luyckx and Daele-
mans, 2010). All our experiments use the SVM im-
plementation provided by Canu et al (2005).
5.1 Experimental settings
In order to compare our methods to related works
we adopted the following experimental setting. We
perform experiments using all of the training doc-
uments per author, that is, a balanced corpus (we
call this setting BC). Next we evaluate the perfor-
mance of classifiers over reduced training sets. We
tried balanced reduced data sets with: 1, 3, 5 and
10 documents per author (we call this configura-
tion RBC). Also, we experimented with reduced-
imbalanced data sets using the same imbalance rates
reported in (Plakias and Stamatatos, 2008b; Plakias
and Stamatatos, 2008a): we tried settings 2 ? 10,
5? 10, and 10? 20, where, for example, setting 2-
10 means that we use at least 2 and at most 10 doc-
uments per author (we call this setting IRBC). BC
setting represents the AA problem under ideal con-
ditions, whereas settings RBC and IRBC aim at em-
ulating a more realistic scenario, where limited sam-
ple documents are available and the whole data set is
highly imbalanced (Plakias and Stamatatos, 2008b).
3We used LOWBOW code of G. Lebanon and Y. Mao avail-
able from http://www.cc.gatech.edu/?ymao8/lowbow.htm
5.2 Experimental results in balanced data
We first compare the performance of the LOWBOW
histogram representation to that of the traditional
BOW representation. Table 2 shows the accuracy
(i.e., percentage of documents in the test set that
were associated to its correct author) for the BOW
and LOWBOW histogram representations when us-
ing words and character n-grams information. For
LOWBOW histograms, we report results with three
different configurations for ?. As in (Lebanon et al,
2007), we consider uniformly distributed locations
and we varied the number of locations that were in-
cluded in each setting. We denote with k the number
of local histograms. In preliminary experiments we
tried several other values for k, although we found
that representative results can be obtained with the
values we considered here.
Method Parameters Words Characters
BOW - 78.2% 75.0%
LOWBOW k = 2;? = 0.2 75.8% 72.0%
LOWBOW k = 5;? = 0.2 77.4% 75.2%
LOWBOW k = 20;? = 0.2 77.4% 75.0%
Table 2: Authorship attribution accuracy for the BOW
representation and LOWBOW histograms. Column 2
shows the parameters we used for the LOWBOW his-
tograms; columns 3 and 4 show results using words and
character n-grams, respectively.
From Table 2 we can see that the BOW repre-
sentation is very effective, outperforming most of
the LOWBOW histogram configurations. Despite a
small difference in performance, BOW is advanta-
geous over LOWBOW histograms because it is sim-
pler to compute and it does not rely on parameter
selection. Recall that the LOWBOW histogram rep-
resentations are obtained by the combination of sev-
eral local histograms calculated at different locations
of the document, hence, it seems that the raw sum of
local histograms results in a loss of useful informa-
tion for representing documents. The worse perfor-
mance was obtained when k = 2 local histograms
are considered (see row 3 in Table 2). This re-
sult is somewhat expected since the larger the num-
ber of local histograms, the more LOWBOW his-
tograms approach the BOW formulation (Lebanon
et al, 2007).
We now describe the AA performance obtained
when using the BOLH formulation; these results
293
are shown in Table 3. Most of the results from
this table are superior to those reported in Table 2,
showing that bags of local histograms are a better
way to exploit the LOWBOW framework for AA.
As expected, different kernels yield different results.
However, the diffusion kernel outperformed most of
the results obtained with other kernels; confirming
the results obtained by other researchers (Lebanon
et al, 2007; Lafferty and Lebanon, 2005).
Kernel Euc. Diffusion EMD ?2
Words
Setting-1 78.6% 81.0% 75.0% 75.4%
Setting-2 77.6% 82.0% 76.8% 77.2%
Setting-3 79.2% 80.8% 77.0% 79.0%
Characters
Setting-1 83.4% 82.8% 84.4% 83.8%
Setting-2 83.4% 84.2% 82.2% 84.6%
Setting-3 83.6% 86.4% 81.0% 85.2%
Table 3: Authorship attribution accuracy when using bags
of local histograms and different kernels for word-based
and character-based representations. The BC data set is
used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,
respectively.
On average, the worse kernel was that based on
the earth mover?s distance (EMD), suggesting that
the comparison of local histograms at different loca-
tions is not a fruitful approach (recall that this is the
only kernel that compares local histograms at differ-
ent locations). This result evidences that authors use
similar word/character distributions at similar loca-
tions when writing different documents.
The best performance across settings and kernels
was obtained with the diffusion kernel (in bold, col-
umn 3, row 9) (86.4%); that result is 8% higher
than that obtained with the BOW representation and
9% better than the best configuration of LOWBOW
histograms, see Table 2. Furthermore, that result
is more than 5% higher than the best reported re-
sult in related work (80.8% as reported in (Plakias
and Stamatatos, 2008b)). Therefore, the consid-
ered local histogram representations over character
n-grams have proved to be very effective for AA.
One should note that, in general, better per-
formance was obtained when using character-level
rather than word-level information. This confirms
the results already reported by other researchers
that have used character-level and word-level infor-
mation for AA (Houvardas and Stamatatos, 2006;
Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a; Peng et al, 2003). We believe this
can be attributed to the fact that character n-grams
provide a representation for the document at a finer
granularity, which can be better exploited with local
histogram representations. Note that by considering
3-grams, words of length up to three are incorpo-
rated, and usually these words are function words
(e.g., the, it, as, etc.), which are known to be in-
dicative of writing style. Also, n-gram information
is more dense in documents than word-level infor-
mation. Hence, the local histograms are less sparse
when using character-level information, which re-
sults in better AA performance.
True author
AC AS BL DL JM JG MM MD RS TN
88 2 0 0 0 0 0 0 0 0
10 98 0 0 0 0 0 0 0 0
0 0 68 0 40 0 0 0 0 0
0 0 0 80 0 0 0 0 0 4
0 0 12 2 42 0 0 2 0 0
0 0 0 0 0 100 0 0 0 2
2 0 2 0 0 0 100 0 0 0
0 0 18 0 18 0 0 98 0 0
0 0 0 2 0 0 0 0 100 4
0 0 0 16 0 0 0 0 0 90
Table 4: Confusion matrix (in terms of percentages) for
the best result in the BC corpus (i.e., last row, column 3
in Table 3). Columns show the true author for test docu-
ments and rows show the authors predicted by the SVM.
Table 4 shows the confusion matrix for the setting
that reached the best results (i.e., column 3, last row
in Table 3). From this table we can see that 8 out
of the 10 authors were recognized with an accuracy
higher or equal to 80%. For these authors sequential
information seems to be particularly helpful. How-
ever, low recognition performance was obtained for
authors BL (B. K. Lim) and JM (J. MacArtney).
The SVM with BOW representation of character n-
grams achieved recognition rates of 40% and 50%
for BL and JM respectively. Thus, we can state that
sequential information was indeed helpful for mod-
eling BL writing style (improvement of 28%), al-
though it is an author that resulted very difficult to
model. On the other hand, local histograms were not
very useful for identifying documents written by JM
(made it worse by ?8%). The largest improvement
(38%) of local histograms over the BOW formula-
tion was obtained for author TN (T. Nissen). This
294
result gives evidence that TN uses a similar distri-
bution of words in similar locations across the doc-
uments he writes. These results are interesting, al-
though we would like to perform a careful analysis
of results in order to determine for what type of au-
thors it would be beneficial to use local histograms,
and what type of authors are better modeled with a
standard BOW approach.
5.3 Experimental results in imbalanced data
In this section we report results with RBC and
IRBC data sets, which aim to evaluate the perfor-
mance of our methods in a realistic setting. For
these experiments we compare the performance of
the BOW, LOWBOW histogram and BOLH repre-
sentations; for the latter, we considered the best set-
ting as reported in Table 3 (i.e., an SVM with dif-
fusion kernel and k = 20). Tables 5 and 6 show
the AA performances when using word and charac-
ter information, respectively.
We first analyze the results in the RBC data set
(recall that for this data set we consider 1, 3, 5, 10,
and 50, randomly selected documents per author).
From Tables 5 and 6 we can see that BOW and
LOWBOW histogram representations obtained sim-
ilar performance to each other across the different
training set sizes, which agree with results in Table 2
for the BC data sets. The best performance across
the different configurations of the RBC data set was
obtained with the BOLH formulation (row 6 in Ta-
bles 5 and 6). The improvements of local histograms
over the BOW formulation vary across different set-
tings and when using information at word-level and
character-level. When using words (columns 2-6
in Table 5) the differences in performance are of
15.6%, 6.2%, 6.8%, 2.9%, 3.8% when using 1, 3, 5,
10 and 50 documents per author, respectively. Thus,
it is evident that local histograms are more beneficial
when less documents are considered. Here, the lack
of information is compensated by the availability of
several histograms per author.
When using character n-grams (columns 2-6 in
Table 6) the corresponding differences in perfor-
mance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%,
when using 1, 3, 5, 10, and 50 documents per au-
thor, respectively. In this case, the larger improve-
ment was obtained when 50 documents per author
are available; nevertheless, one should note that re-
sults using character-level information are, in gen-
eral, significantly better than those obtained with
word-level information; hence, improvements are
expected to be smaller.
When we compare the results of the BOLH for-
mulation with the best reported results elsewhere
(c.f. last row 6 in Tables 5 and 6) (Plakias and Sta-
matatos, 2008b), we found that the improvements
range from 14% to 30.2% when using character n-
grams and from 1.2% to 26% when using words.
The differences in performance are larger when less
information is used (e.g., when 5 documents are
used for training) and we believe the differences
would be even larger if results for 1 and 3 documents
were available. These are very positive results; for
example, we can obtain almost 71% of accuracy, us-
ing local histograms of character n-grams when a
single document is available per author (recall that
we have used all of the test samples for evaluating
the performance of our methods).
We now analyze the performance of the different
methods when using the IRBC data set (columns 7-
9 in Tables 5 and 6). The same pattern as before can
be observed in experimental results for these data
sets as well: BOW and LOWBOW histograms ob-
tained comparable performance to each other and
the BOLH formulation performed the best. The
BOLH formulation outperforms state of the art ap-
proaches by a considerable margin that ranges from
10% to 27%. Again, better results were obtained
when using character n-grams for the local his-
tograms. With respect to RBC data sets, the BOLH
at the character-level resulted very robust to the re-
duction of training set size and the highly imbal-
anced data.
Summarizing, the results obtained in RBC and
IRBC data sets show that the use of local histograms
is advantageous under challenging conditions. An
SVM under the BOLH representation is less sen-
sitive to the number of training examples available
and to the imbalance of data than an SVM using
the BOW representation. Our hypothesis for this
behavior is that local histograms can be thought of
as expanding training instances, because for each
training instance in the BOW formulation we have
k?training instances under BOLH. The benefits of
such expansion become more notorious as the num-
ber of available documents per author decreases.
295
WORDS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 36.8% 57.1% 62.4% 69.9% 78.2% 62.3% 67.2% 71.2%
LOWBOW 37.9% 55.6% 60.5% 69.3% 77.4% 61.1% 67.4% 71.5%
Diffusion kernel 52.4% 63.3% 69.2% 72.8% 82.0% 66.6% 70.7% 74.1%
Reference - - 53.4% 67.8% 80.8% 49.2% 59.8% 63.0%
Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms. We report
results for the BOW, LOWBOW histogram and BOLH representations. For reference (last row), we also include the
best result reported in (Plakias and Stamatatos, 2008b), when available, for each configuration.
CHARACTER N-GRAMS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 65.3% 71.9% 74.2% 76.2% 75.0% 70.1% 73.4% 73.1%
LOWBOW 61.9% 71.6% 74.5% 73.8% 75.0% 70.8% 72.8% 72.1%
Diffusion kernel 70.7% 78.3% 80.6% 82.2% 86.4% 77.8% 80.5% 82.2%
Reference - - 50.4% 67.8% 76.6% 49.2% 59.8% 63.0%
Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.
6 Conclusions
We have described the use of local histograms (LH)
over character n-grams for AA. LHs are enriched
histogram representations that preserve sequential
information in documents (in terms of the positions
of terms in documents); we explored the suitabil-
ity of LHs over n-grams at the character-level for
AA. We showed evidence supporting our hypothe-
sis that LHs are very helpful for AA; we believe that
this is due to the fact that LOWBOW representations
can uncover, to some extent, the writing preferences
of authors. Our experimental results showed that
LHs outperform traditional bag-of-words formula-
tions and state of the art techniques in balanced,
imbalanced, and reduced data sets. The improve-
ments were larger in reduced and imbalanced data
sets, which is a very positive result as in real AA
applications one often faces highly imbalanced and
small sample issues. Our results are promising and
motivate further research on the use and extension
of the LOWBOW framework for related tasks (e.g.
authorship verification and plagiarism detection).
As future work we would like to explore the use
of LOWBOW representations for profile-based AA
and related tasks. Also, we would like to develop
model selection strategies for learning what combi-
nation of hyperparameters works better for modeling
each author.
Acknowledgments
We thank E. Stamatatos for making his data set
available. Also, we are grateful for the thought-
ful comments of L. A. Barro?n and those of the
anonymous reviewers. This work was partially sup-
ported by CONACYT under project grants 61335,
and CB-2009-134186, and by UAB faculty develop-
ment grant 3110841.
References
AMIDA. 2007. Augmented multi-party interaction
with distance access. Available from http://www.
amidaproject.org/, AMIDA Report.
S. Argamon and S. Levitan. 2005. Measuring the useful-
ness of function words for authorship attribution. In
Proceedings of the Joint Conference of the Association
for Computers and the Humanities and the Association
for Literary and Linguistic Computing, Victoria, BC,
Canada.
S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotoma-
monjy. 2005. SVM and kernel methods Matlab tool-
box. Perception Systmes et Information, INSA de
Rouen, Rouen, France.
V. Chasanis, A. Kalogeratos, and A. Likas. 2009. Movie
segmentation into scenes and chapters using locally
weighted bag of visual words. In Proceedings of the
ACM International Conference on Image and Video
Retrieval, pages 35:1?35:7, Santorini, Fira, Greece.
ACM Press.
R. M. Coyotl-Morales, L. Villasen?or-Pineda, M. Montes-
y-Go?mez, and P. Rosso. 2006. Authorship attribu-
tion using word sequences. In Proceedings of 11th
296
Iberoamerican Congress on Pattern Recognition, vol-
ume 4225 of LNCS, pages 844?852, Cancun, Mexico.
Springer.
D. Das and A. Martins. 2007. A survey on au-
tomatic text summarization. Available from:
http://www.cs.cmu.edu/?nasmith/LS2/
das-martins.07.pdf, Literature Survey for the
Language and Statistics II course at Carnegie Mellon
University.
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001.
Multitopic email authorship attribution forensics. In
Proceedings of the ACM Conference on Computer Se-
curity - Workshop on Data Mining for Security Appli-
cations, Philadelphia, PA, USA.
K. Grauman. 2006. Matching Sets of Features for Ef-
ficient Retrieval and Recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proceedings
of the 12th International Conference on Artificial In-
telligence: Methodology, Systems, and Applications,
volume 4183 of LNCS, pages 77?86, Varna, Bulgaria.
Springer.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of the Pacific Association for Compu-
tational Linguistics, pages 255?264, Halifax, Canada.
M. Koppel, J. Schler, and S. Argamon. 2009. Computa-
tional methods in authorship attribution. Journal of the
American Society for Information Science and Tech-
nology, 60:9?26.
J. Lafferty and G. Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6:129?163.
M. Lambers and C. J. Veenman. 2009. Forensic author-
ship attribution using compression distances to pro-
totypes. In Computational Forensics, Lecture Notes
in Computer Science, Volume 5718. ISBN 978-3-642-
03520-3. Springer Berlin Heidelberg, 2009, p. 13, vol-
ume 5718 of LNCS, pages 13?24. Springer.
G. Lebanon, Y. Mao, and J. Dillon. 2007. The locally
weighted bag of words framework for document rep-
resentation. Journal of Machine Learning Research,
8:2405?2441.
D. Lewis, T. Yang, and F. Rose. 2004. RCV1: A new
benchmark collection for text categorization research.
Journal of Machine Learning Research, 5:361?397.
K. Luyckx and W. Daelemans. 2010. The effect of au-
thor set size and data size in authorship attribution.
Literary and Linguistic Computing, pages 1?21, Au-
gust.
Y. Mao, J. Dillon, and G. Lebanon. 2007. Sequential
document visualization. IEEE Transactions on Visu-
alization and Computer Graphics, 13(6):1208?1215.
F. Peng, D. Shuurmans, V. Keselj, and S. Wang. 2003.
Language independent authorship attribution using
character level language models. In Proceedings of the
10th conference of the European chapter of the Associ-
ation for Computational Linguistics, volume 1, pages
267?274, Budapest, Hungary.
F. Peng, D. Shuurmans, and S. Wang. 2004. Augmenting
naive Bayes classifiers with statistical language mod-
els. Information Retrieval Journal, 7(1):317?345.
S. R. Pillay and T. Solorio. 2010. Authorship attribution
of web forum posts. In Proceedings of the eCrime Re-
searchers Summit (eCrime), 2010, pages 1?7, Dallas,
TX, USA. IEEE.
S. Plakias and E. Stamatatos. 2008a. Author identifi-
cation using a tensor space representation. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence, volume 178, pages 833?834, Patras,
Greece. IOS Press.
S. Plakias and E. Stamatatos. 2008b. Tensor space mod-
els for authorship attribution. In Proceedings of the 5th
Hellenic Conference on Artificial Intelligence: Theo-
ries, Models and Applications, volume 5138 of LNCS,
pages 239?249, Syros, Greece. Springer.
R. Rifkin and A. Klautau. 2004. In defense of one-vs-all
classification. Journal of Machine Learning Research,
5:101?141.
Y. Rubner, C. Tomasi, J. Leonidas, and J. Guibas. 2001.
The earth mover?s distance as a metric for image re-
trieval. International Journal of Computer Vision,
40(2):99?121.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
E. Stamatatos. 2006a. Authorship attribution based on
feature set subspacing ensembles. International Jour-
nal on Artificial Intelligence Tools, 15(5):823?838.
E. Stamatatos. 2006b. Ensemble-based author identifi-
cation using character n-grams. In Proceedings of the
3rd International Workshop on Text-based Information
Retrieval, pages 41?46, Riva del Garda, Italy.
E. Stamatatos. 2009a. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceed-
ings of the 3rd International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse,
PAN?09, pages 38?46, Donostia-San Sebastian, Spain.
E. Stamatatos. 2009b. A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538?
556.
M. Tearle, K. Taylor, and H. Demuth. 2008. An
algorithm for automated authorship attribution using
neural networks. Literary and Linguist Computing,
23(4):425?442.
297
Y. Zhao and J. Zobel. 2005. Effective and scalable au-
thorship attribution using function words. In Proceed-
ings of 2nd Asian Information Retrieval Symposium,
volume 3689 of LNCS, pages 174?189, Jeju Island,
Korea. Springer.
298
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 275?281,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UABCoRAL: A Preliminary study for Resolving the Scope of Negation
Binod Gyawali, Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, Alabama, USA
{bgyawali,solorio}@cis.uab.edu
Abstract
This paper describes our participation in the
closed track of the *SEM 2012 Shared Task
of finding the scope of negation. To perform
the task, we propose a system that has three
components: negation cue detection, scope of
negation detection, and negated event detec-
tion. In the first phase, the system creates a
lexicon of negation signals from the training
data and uses the lexicon to identify the nega-
tion cues. Then, it applies machine learning
approaches to detect the scope and negated
event for each negation cue identified in the
first phase. Using a preliminary approach, our
system achieves a reasonably good accuracy
in identifying the scope of negation.
1 Introduction
All human language samples, either written or spo-
ken, contain some information in negated form. In
tasks such as information retrieval, sometimes, we
should consider only the positive information of an
event and disregard its negation information, and
vice versa. For example, while searching for the pa-
tients with diabetes, we should not include a patient
who has a clinical report saying No symptoms of di-
abetes were observed. Thus, finding the negation
and its scope is important in tasks where the nega-
tion and assertion information need to be treated dif-
ferently. However, most of the systems developed
for processing natural language data do not consider
negations present in the sentences. Although various
works (Morante et al, 2008; Morante and Daele-
mans, 2009; Li et al, 2010; Councill et al, 2010;
Apostolova et al, 2011) have dealt with the identifi-
cation of negations and their scope in sentences, this
is still a challenging task.
The first task in *SEM 2012 Shared
Task (Morante and Blanco, 2012) is concerned
with finding the scope of negation. The task
includes identifying: i) negation cues, ii) scope of
negation, and iii) negated event for each negation
present in the sentences. Negation cue is a word,
part of a word, or a combination of words that
carries the negation information. Scope of negation
in a sentence is the longest group of words in
the sentence that is influenced by the negation
cue. Negated event is the shortest group of words
that is actually affected by the negation cue. In
Example (1) below, word no is a negation cue, the
discontinuous word sequences ?I gave him? and
?sign of my occupation? are the scopes, and ?gave?
is the negated event.
(1) I [gave] him no sign of my occupation.
In this paper, we propose a system to detect the
scope of negation for the closed track of *SEM 2012
Shared Task. Our system uses a combination of
a rule based approach, and a machine learning ap-
proach. We use a rule based approach to create a
lexicon of all the negation words present in the train-
ing data. Then we use this lexicon to detect the
negation cues present in the test data. We do a pre-
liminary analysis of finding the scope of negation
and the negated events by applying a machine learn-
ing approach, and using basic features created from
the words, lemmas, and parts-of-speech (POS) tags
of words in the sentences. The F-measure scores
275
achieved by our system are about 85% for negation
cue detection, 65% in full scope identification, 48%
in negated event detection, and 39% in identifying
full negation. Our error analysis shows that the use
of lexicon is not very appropriate to detect the nega-
tion cues. We also describe the challenges in identi-
fying the scope and the negated events.
2 Problem Description
The *SEM 2012 shared task competition provided
three data sets: training, development, and test data
set. Each sentence in each data set is split into
words. The dataset contains the information such
as lemma, part of speech, and other syntactic infor-
mation of each word. Each sentence of training and
development data is annotated with negation cues,
scopes and negated events. Using the training and
the development data, the task is to identify negation
cues, scopes and negated events in all unannotated
sentences of the test data.
Sentence
tokens
Negation
cue
Scope Negated
event
I - I -
am - am -
not not - -
sure - sure sure
whether - whether -
I - I -
left - left -
it - it -
here - here -
Table 1: An example of negation cue, scope and the
negated event
A sentence can contain more than one negation
cue. Negation cues in the data set can be i) a sin-
gle word token such as n?t, nowhere, ii) a contin-
uous sequence of two or more words, such as no
more, by no means or iii) two or more discontinu-
ous words such as ..neither...nor... A negation cue
is either a part or same as its corresponding nega-
tion word. This corresponding negation word is re-
ferred as a negation signal in the remaining sections
of the paper. For example, for a negation signal
unnecessary, the negation cue is un, and similarly,
for a negation signal needless, the negation cue is
less.
Scope of a negation in a sentence can be a con-
tinuous sequence of words or a discontinuous set
of words in the sentence. Scope of negation some-
times includes the negation word. A negation word
may not have a negated event. Presence of a negated
event in a sentence depends upon the facts described
by the sentence. Non-factual sentences such as in-
terrogative, imperative, and conditional do not con-
tain negated events. Morante and Daelemans (2012)
describe the details of the negation cue, scope, and
negated event, and the annotation guidelines. An ex-
ample of the task is shown in Table 1.
3 System Description
We decompose the system to identify the scope of
negation into three tasks. They are:
1. Finding the negation cue
2. Finding the scope of negation
3. Finding the negated event
The scope detection and the negated event de-
tection tasks are dependent on the task of finding
the negation cue. But the scope detection and the
negated event detection tasks are independent of
each other.
We identify the negation cues present in the test
data based on a lexicon of negation signals that
are present in the training and the development
data. The tasks of identifying scope of negation and
negated event are modeled as classification prob-
lems. To identify scope and negated event, we train
classifiers with the instances created from the train-
ing data provided. We create test instances from the
test data annotated with negation cues predicted by
our cue detection component. Due to the use of test
data annotated by our cue detection component, the
false negative rate in predicting the negation cues is
propagated to the scope detection as well as negated
event detection components. The details of all the
three components are described in the subsections
below.
3.1 Identifying the negation cue
In this task, we identify all the negation cues present
in the sentences. We group the negation cues under
three types depending upon how they are present in
the data. They are: single word cues, continuous
276
multiword cues, and discontinuous multiword cues.
All the cues present in the training and development
datasets are shown in Table 2.
Cue types Cues
Single word
cues
absence, dis, except, fail, im, in, ir, less, n?t,
neglected, neither, never, no, nobody, none,
nor, not, nothing, nowhere, prevent, refused,
save, un, without
Continuous
multiword
cues
no more, rather than, by no means, nothing
at all, on the contrary, not for the world
Discontinuous
multiword
cues
neither nor, no nor, not not
Table 2: Negation cues present in training and develop-
ment data
In the training and development data, multiword
negation cues account for only 1.40% of the total
negation cues. At this stage, we decided to focus
on identifying the single word negation cues. The
system first creates a lexicon that contains the pairs
of negation cues and their corresponding negation
signals for all the single word negation cues present
in the training and the development datasets. In or-
der to identify a negation cue in the test set, the sys-
tem searches all the words in the sentences of the
test data that match the negation signals of the lexi-
con. For each word that matches, it assigns the cor-
responding cue of the signal from the lexicon as its
negation cue.
3.2 Identifying the scope of negation
We apply a machine learning technique to identify
the scope of negation. For each negation cue present
in a sentence, we create the problem instances as the
tuple of the negation signal and each word present
in the same sentence. To create the instances, we
use only those sentences having at least one nega-
tion. For training, we create instances from the train-
ing data, but we consider only those words that are
within a window of size 20 from the negation signal
and within the sentence boundary. We restricted the
words to be within the window in order to minimize
the problem of imbalanced data. This window was
chosen following our observation that only 1.26%
of the scope tokens go beyond the 20 word win-
dow from the negation signal. Including the words
beyond this window causes a major increase in the
negative instances resulting in a highly imbalanced
training set. While creating test instances, we do not
restrict the words by window size. This restriction is
not done in order to include all the words of the sen-
tences in the test instances. An instance is labeled
as positive if the word used to create the instance is
the scope of the negation signal; else it is labeled as
negative.
We extract 10 features to identify the scope of
negation as follows:
1. Negation signal in the tuple
2. Lemma of the negation signal
3. POS tag of the negation signal
4. Word in the tuple
5. Lemma of the word in the tuple
6. POS tag of the word in the tuple
7. Distance between the negation signal and the
word in terms of number of words
8. Position of the word from the negation signal
(left, right)
9. Whether a punctuation character (?,?, ?:?,?;?) ex-
ists between the word and the negation signal
10. Sequence of POS tags in between the negation
signal and the word
After the classification, if an instance is predicted
as positive, the word used to create the instance is
considered as the scope of the negation signal. If a
negation signal has prefix such as ?dis?, ?un?, ?in?,
?ir?, or ?im?, the scope of negation includes only the
part of word (signal) excluding the prefix. Thus, for
each negation signal having these prefix, we remove
the prefix from the signal and consider the remain-
ing part of it as the scope, regardless of whether the
classifier classifies the instance pair as positive or
negative.
277
3.3 Identifying the negated event
The task of identifying the negated event is simi-
lar to the task of identifying the scope of negation.
The process of creating the instances for this task
is almost the same to that of finding the scope of
negation, except that, we limit the window size to
4 words from the negation signal. 4.24% of the
negated events lie away from the 4 word window.
Beyond this window, the events are very sparse and
a small increment in the window size leads to abrupt
increase in negative instances and creates an imbal-
ance in the data. The 4 word window size was se-
lected based on the best result obtained among var-
ious experiments performed with different window
sizes greater than and equal to 4. The same rule
applies while creating instances for training data as
well as test data. We use only nine features in this
step, excluding the 9th feature used in the scope de-
tection. We also apply the same rule of mapping the
negation signals starting with ?dis?, ?un?, ?in?, ?ir?,
and ?im? to the negated event as in the previous step.
4 Experimental Settings
We evaluated our system only on the test data of the
shared task. For the machine learning tasks, we used
the SVM light classifier (Joachims, 1999) with 4th
degree polynomial kernel and other default param-
eters. The identification of cues, scopes, negated
events, and full negation are evaluated on the basis
of the F-measures. We also use ?B? variant for cues,
scopes, negated events and the full negation for eval-
uation. The precision of ?B? variant is calculated as
the ratio of true positives to the system count. Iden-
tification of cues and negated events are measured
independent of any other steps. But the identifica-
tion of the scopes is measured depending upon the
correct identification of cues in three different ways
as follows:
i) scopes (cue match): the cue has to be correct
for the scope to be correct
ii) scopes (no cue match): the system must iden-
tify part of the cue for the scope to be correct
iii) scope tokens (no cue match): a part of the sys-
tem identified cue must overlap with the gold stan-
dard cue for the scope tokens to be correct
The F1 score of the full negation detection was
used to rank the systems of the participants. The
details about the evaluation measures can be found
in Morante and Blanco (2012).
5 Results Analysis
The results obtained by our system over the test data
are shown in Table 3. The results obtained by each
component, and their analysis are described in the
subsections below.
5.1 Identifying the negation cues
The system is able to achieve an 85.77% F1 score in
the task of identifying the negation cues using a sim-
ple approach based on the lexicon of the negation
signals. Because of the system?s inability to iden-
tify multiword negation cues, it could not detect the
multiword cues such as ..neither..nor.., ..absolutely
nothing.., ..far from.., ..never more.., that account for
3.5% of the total negation cues present in the test
data.
The accuracy of the system is limited by the cov-
erage of the lexicon. Due to the low coverage of the
lexicon, the system fails to identify signals such as
ceaseless, discoloured, incredulity, senseless,
and unframed that are present only in the test data.
These signals account for 4.5% of the total negation
signals present in the test data. Some words such
as never, nothing, not, n?t, no, and without are
mostly present as the negation signals in the data.
But these words are not always the negation signals.
The phrase no doubt is present nine times in the test
data, but the word no is a negation signal in only
four of them. This accounts for 1.89% error in the
negation cue detection. The word save is present
once as a negation signal in the training data, but it
is never a negation signal in the test data. Therefore,
our lexicon based system invariably predicts two oc-
currences of save in the test data as negation signals.
5.2 Identifying the scope of negation
The system achieves 63.46% F1 score in identifying
scopes with cue match, 64.76% F1 score in identify-
ing scopes with no cue match, and 76.23% F1 score
in identifying scope tokens with no cue match. The
results show that our system has a higher precision
than recall in identifying the scope. As mentioned
278
gold system tp fp fn precision (%) recall (%) F1 (%)
Cues 264 284 226 37 38 85.93 85.61 85.77
Scopes (cue match) 249 239 132 35 117 79.04 53.01 63.46
Scopes (no cue match) 249 239 132 35 113 79.53 54.62 64.76
Scope tokens (no cue match) 1805 1456 1243 213 562 85.37 68.86 76.23
Negated (no cue match) 173 104 65 35 104 65.00 38.46 48.33
Full negation 264 284 73 37 191 66.36 27.65 39.04
Cues B 264 284 226 37 38 79.58 85.61 82.48
Scopes B (cue match) 249 239 132 35 117 55.23 53.01 54.10
Scopes B (no cue match) 249 239 132 35 113 56.90 54.62 55.74
Negated B (no cue match) 173 104 65 35 104 62.50 38.46 47.62
Full negation B 264 284 73 37 191 25.70 27.65 26.64
Total sentences: 1089
Negation sentences: 235
Negation sentences with errors: 172
% Correct sentences: 81.73
% Correct negation sentences: 26.81
Table 3: Results of the system
earlier, the negation cues identified in the first task
are used to identify the scope of negation and the
negated events. Using the test data with 15% error
in negation cues as the input to this component and
some of the wrong predictions of the scope by this
component led to a low recall value in the scope de-
tection.
The results show that the system works well when
a negation signal has fewer scope tokens and when
the scope tokens are closer to the negation signal.
There are some cases where the system could not
identify the scope tokens properly. It is unable to de-
tect the scope tokens that are farther in distance from
the negation signals. The system is not performing
well in predicting the discontinuous scopes. When
a negation cue has discontinuous scope, mostly the
system predicts one sequence of words correctly but
could not identify the next sequence. In sentence
(2) in the example below, the underlined word se-
quences are the discontinuous scopes of the nega-
tion cue not. In the sentence, our system predicts
only the second sequence of scope, but not the first
sequence. In some cases, our system does not have a
good coverage of scope tokens. In sentence (3), the
underlined word sequence is the scope of the signal
no, but our system detects only at ninety was hard-
ship as its scope. These inabilities to detect the full
scope have led to have a higher accuracy in predict-
ing the partial scope tokens (76.23%) than predicting
the full scope (64.76%).
(2) the box is a half pound box of honeydew to-
bacco and does not help us in any way
(3) ...a thermometer at ninety was no hardship
(4) ...I cannot see anything save very vague
indications
Analyzing the results, we see that the error in pre-
dicting the scope of the negation is high when the
scope is distributed in two different phrases. In the
example (2) above, does not help us in any way is
a single verb phrase and all the scope within the
phrase is correctly identified by our system. The
box being a separate phrase, it is unable to identify
it. However, in some cases such as example (4), the
system could not identify any scope tokens for nega-
tion cue not.
Some of the findings of previous works have
shown that the features related to syntactic path are
helpful in identifying the scope of negation. Li et
al. (2010) used the syntactic path from the word to
the negation signal and showed that this helped to
improve the accuracy of scope detection. Similarly,
work by Councill et al (2010) showed that the ac-
curacy of scope detection could be increased using
the features from the dependency parse tree. In our
experiment, there was a good improvement in the
scope detection rate when we included ?sequence
of POS tags? between the negation signal and the
word as a feature. This improvement after including
the sequence of POS tags feature and its consistency
279
with the previous works implies that adding path re-
lated features might help to improve the accuracy in
scope detection.
5.3 Identifying the negated event
We are able to achieve an F1 score of 48.33% in pre-
dicting the negated events, which is the lowest score
among all three components. As in the scope de-
tection task, error in negation cue detection led to
lower the recall rate of the negated event detection
system. The accuracy of full negation is based on
the correct identification of the negation cues, scope
and the negated events of all the negations present
in the sentences. The output shows that there are
many cases where negation cues and the scope are
correctly identified but there is an error in identify-
ing the negated events. The higher error in predict-
ing the negated events led to reduce the score of full
negation and achieve an F1 score of 39.04%.
Our system is unable to detect some negated
events even though they are adjacent to the nega-
tion signal. This shows that the use of simple fea-
tures extracted from words, lemmas, and POS tags
is not enough to predict the negated events properly.
Adding features related to words in left and right of
the negation signal and the path feature may help to
improve the detection of negated events.
In order to analyze the impact of error in the nega-
tion cue detection component upon the scope and
negated event detection components, we performed
an experiment using the gold standard negation cues
to detect the scope and the negated events. F1 scores
achieved by this system are 73.1% in full scope de-
tection, 54.87% in negated event detection, 81.46%
in scope tokens detection, and 49.57% in full nega-
tion detection. The result shows that there is al-
most 10% increment in the F1 score in all the com-
ponents. Thus, having an improved cue detection
component greatly helps to improve the accuracy of
scope and negated event detection components.
6 Discussion and Conclusion
In this paper we outline a combination of a rule
based approach and a machine learning approach to
identify the negation cue, scope of negation, and the
negated event. We show that applying a basic ap-
proach of using a lexicon to predict the negation cues
achieves a considerable accuracy. However, our sys-
tem is unable to identify the negation cues such as
never, not, nothing, n?t, and save that can appear
as a negation signal as well as in other non-negated
contexts. It also cannot cover the negation cues of
the signals that are not present in the training data.
Moreover, in order to improve the overall accuracy
of the scope and negated event detection, we need an
accurate system to detect the negation cues since the
error in the negation cue detection propagates to the
next steps of identifying the scope and the negated
event. It is difficult to identify the scope of nega-
tions that are farther in distance from the negation
signal. Detecting the tokens of the scope that are
discontinuous is also challenging.
As future work, we would like to extend our task
to use a machine learning approach instead of the
lexicon of negation signals to better predict the nega-
tion cues. The system we presented here uses a pre-
liminary approach without including any syntactic
information to detect the scope and negated events.
We would also incorporate syntactic information to
identify the scope and negated events in our future
work. To improve the accuracy of identifying the
scope and the negated events, adding other features
related to the neighbor words of the negation signal
might be helpful. In our tasks, we limit the scope
and negated event instances by the window size in
order to avoid imbalance data problem. Another in-
teresting work to achieve better accuracy could be to
use other approaches of imbalanced dataset classifi-
cation instead of limiting the training instances by
the window size.
References
Emilia Apostolova, Noriko Tomuro, and Dina Demner-
Fushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 283?287, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
280
cessing, NeSp-NLP ?10, pages 51?59, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in ker-
nel methods: support sector searning, pages 169?184.
MIT Press, Cambridge, MA, USA.
Junhui Li, Guodong Zhou, Hongling Wang, and Qiaom-
ing Zhu. 2010. Learning the scope of negation via
shallow semantic parsing. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10, pages 671?679, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
Shared Task: Resolving the Scope and Focus of Nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics (*SEM 2012),
Montreal, Canada.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning, CoNLL
?09, pages 21?29, Stroudsburg, PA, USA.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), Istanbul.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 715?724. Association for Compu-
tational Linguistics.
281
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 86?90,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Modelling Fixated Discourse in Chats with Cyberpedophiles
Dasha Bogdanova
University of
Saint Petersburg
dasha.bogdanova
@gmail.com
Paolo Rosso
NLE Lab. - ELiRF,
Univ. Polite?cnica de Valencia
prosso@dsic.upv.es
Thamar Solorio
University of
Alabama at Birmingham
solorio@cis.uab.edu
Abstract
The ability to detect deceptive statements in
predatory communications can help in the iden-
tification of sexual predators, a type of deception
that is recently attracting the attention of the re-
search community. Due to the intention of a pe-
dophile of hiding his/her true identity (name, age,
gender and location) its detection is a challenge.
According to previous research, fixated discourse
is one of the main characteristics inherent to the
language of online sexual predation. In this pa-
per we approach this problem by computing sex-
related lexical chains spanning over the conversa-
tion. Our study shows a considerable variation in
the length of sex-related lexical chains according
to the nature of the corpus, which supports our
belief that this could be a valuable feature in an
automated pedophile detection system.
1 Introduction
Child sexual abuse is not a rare problem. The statisti-
cal analysis by the National Incident-Based Reporting
System data (FBI, 1995) revealed that in the majority
of all sexual assaults (67%) the victims were under-
age (Snyder, 2000). Child sexual abuse and pedophilia
are related to each other and both are of great social
concern. On the one hand, law enforcement is work-
ing on prosecuting and preventing child sexual abuse.
On the other hand, psychologists and mental special-
ists are investigating the phenomenon of pedophilia.
Even though pedophilia has been studied from differ-
ent research perspectives, it remains to be a very im-
portant problem that requires further research.
The widespread availability of the Internet, and the
anonymity enabled by it has brought about new forms
of crime. According to the research conducted by
Mitchell (2001), 19% of children have been sexually
approached over the Internet. However, only 10% of
such cases were reported to the police. Attempts to so-
licit children have become common in chat rooms, but
manual monitoring of each conversation is impossible,
due to the massive amount of data and privacy issues.
Therefore, development of reliable tools for detecting
pedophilia in social media is of great importance.
Another related issue is that Internet makes it very
easy to provide false personal information. There-
fore, many online sexual predators create false profiles
where they hide their identity and age. Thus, detec-
tion of online sexual predation also involves age and
gender detection in chats.
From the Natural Language Processing (NLP) per-
spective, there are additional challenges to this prob-
lem because of the chat data specificity. Chat conver-
sations are very different, not only from the written
text, but also from other types of Internet communi-
cation, such as blogs and forums. Since online chat-
ting usually involves very fast typing, mistakes, mis-
spellings, and abbreviations occur frequently in chats.
Moreover, specific slang (e.g. ?kewl? is used instead
of ?cool? and ?asl? stands for ?age/sex/location?) and
character flooding (e.g. greeeeeat!) are used. There-
fore, modern NLP tools often fail to provide accurate
processing of chat language.
Previous research on cyberpedophiles reports that
they often copy juveniles? behavior (Egan et al, 2011),
in particular, they often use colloquialisms and emoti-
cons. Other important characteristics reported previ-
ously include the unwillingness of the predator to step
out of the sex-related conversation, even if the poten-
tial victim wants to change the topic. This is called
fixated discourse (Egan et al, 2011). In this paper
we present preliminary experiments on modelling this
phenomenon. To approach the problem we apply lex-
ical chaining techniques. The experiments show the
difference in the length of sex-related lexical chains
between different datasets. We believe this fact could
be then utilized in detecting pedophiles.
The following section overviews related work on the
topic. Section 3 briefly describes previous research
on pedophiles, the language of online sexual preda-
tion and the fixated discourse phenomenon in partic-
ular. Our approach to modelling fixated discourse is
presented in Section 4. We describe the data set used
in the experiments in Section 5, followed by prelim-
inary experiments presented in Section 6. We finally
draw some conclusions and plans for future work in
Section 7.
86
2 Related Work
The problem of detecting pedophiles in social media
is difficult and relatively novel. New ways of meet-
ing new friends are offered: chatting with webcam
(http://chatroulette.com/) or picking another user at
random and let you have a one-on-one chat with each
other (http://omegle.com/) in a completely anonymous
way.
Some chat conversations with online sexual preda-
tors are available at www.perverted-justice.com. The
site is run by adult volunteers who enter chat rooms
as juveniles (usually 12-15 year old) and if they are
sexually solicited by adults, they work with the po-
lice to prosecute this. Related to the problem of pe-
dophile detection in social media, a study of Perverted
Justice Foundation revealed that since 2007, they have
been working on identifying sex offenders on Myspace
and in 2008, they expanded that effort to Facebook.
The results are sadly staggering in terms of sex of-
fenders that have misused the two social media: Mys-
pace (period 2007- 2010) and Facebook (2008-2010)
deleted respectively 10,746 and 2,800 known sex of-
fenders. Although both social media have been helpful
and responsive towards removing danger users from
their communities, an automatic identification of sex
offenders would certainly help and make the process
faster.
Only few attempts to automatic detection of on-
line sexual predation have been done. Pendar (2007)
proved that it is possible to distinguish between preda-
tor and pseudo-victim with quite high accuracy. The
experiments were conducted on perverted-justice data.
The authors used a kNN classifier to distinguish be-
tween lines written by predators and the lines posted
by pseudo-victims. As features they used word uni-
grams, bigrams and trigrams.
Another attempt has been done by McGhee et al
(2011). They manually annotated the chat lines from
perverted-justice.com with the following labels:
1. Exchange of personal information
2. Grooming
3. Approach
4. None of the above listed classes
In order to distinguish between these types of lines
they used both a rule-based and a machine learn-
ing (kNN) classification approach. Their experiments
showed that the machine learning approach provides
better results and achieves up to 83% accuracy.
Another research work closely related to detection
of cyberpedophilia has been carried by Peersman et
al. (?). As it was already mentioned, pedophiles often
create false profiles and pretend to be younger or of
another gender. Moreover, they try to copy children?s
behaviour. Therefore, there is a need to detect age and
gender in chat conversation. Peersman et al (?) have
analyzed chats from Belgium Netlog social network.
Discrimination between those who are older than 16
from those who are younger based on Support Vector
Machine classification yields 71.3% accuracy. The ac-
curacy is even higher with increasing the gap between
the age groups (e.g. the accuracy of classifying those
who are less than 16 from those who are older than
25 is 88.2%). They have also investigated the issues of
the minimum required dataset. Their experiments have
shown that with 50% of the original dataset the accu-
racy remains almost the same and with only 10% it is
still much better than random baseline performance.
3 Profiling the Pedophile
Pedophilia is a ?disorder of adult personality and be-
haviour? which is characterized by sexual interest in
prepubescent children (International statistical classifi-
cation of diseases and related health problems, 1988).
Even though solicitation of children is not a medi-
cal diagnosis, Abel and Harlow (2001) reported that
88% of child sexual abuse cases are committed by pe-
dophiles. Therefore, we believe that understanding be-
haviour of pedophiles could help detecting and pre-
venting online sexual predation. Even though online
sexual offender is not always a pedophile, in this paper
we use these terms as synonyms.
3.1 Predator?s Linguistic Behavior
The language sexual offenders use was analyzed by
Egan et al (2011). The authors considered the chats
published at www.perverted-justice.com. The analysis
of the chats revealed several characteristics of preda-
tors? language:
? Fixated discourse. Predators impose a sex-related
topic on the conversation and dismiss attempts
from the pseudo-victim to switch topics.
? Implicit/explicit content. On the one hand, preda-
tors shift gradually to the sexual conversation,
starting with more ordinary compliments. On the
other hand, conversation then becomes overtly re-
lated to sex. They do not hide their intentions.
? Offenders often understand that what they are do-
ing is not moral.
? They transfer responsibility to the victim.
? Predators often behave as children, copying the
language: colloquialisms often appear in their
messages.
? They try to minimize the risk of being prosecuted:
they ask to delete chat logs and warn victims not
to tell anyone about the talk, though they finally
stop being cautious and insist on meeting offline.
87
In this paper we consider only the first charac-
teristic: fixated discourse. The conversation below,
taken from perverted-justice.com, illustrates fixated
discourse: the predator almost ignores what the victim
says and comes back to the sex-related conversation:
Predator: licking dont hurt
Predator: its like u lick ice cream
Pseudo-victim: do u care that im 13 in march
and not yet? i lied a little bit b4
Predator: its all cool
Predator: i can lick hard
4 Our Approach
We believe that lexical chains are appropriate to model
the fixated discourse of the predators chats.
4.1 Lexical Chains
A lexical chain is a sequence of semantically related
terms (Morris and Hirst, 1991). It has applications
in many tasks including Word Sense Disambiguation
(WSD) (Galley and McKeown, 2003) and Text Sum-
marization (Barzilay and Elhadad, 1997).
To estimate semantic similarity we used
two metrics: the similarity of Leacock and
Chodorow (Leacock and Chodorow, 2003), and that
of Resnik (Resnik, 1995). Leacock and Chodorow?s
semantic similarity measure is defined as:
SimL&Ch(c1, c2) = ?log
length(c1, c2)
2 ? depth
where length(c1, c2) is the length of the shortest path
between the concepts c1 and c2 and depth is depth of
the taxonomy.
The semantic similarity measure that was proposed
by Resnik (Resnik, 1995) relies on the Information
Content concept:
IC(c) = ?logP (c)
where P(c) is the probability of encountering the
concept c in a large corpus. Thus, Resnik?s similarity
measure is defined as follows:
SimResnik(c1, c2) = IC(lcs(c1, c2))
where lcs(c1, c2) is the least common subsumer of
c1 and c2.
4.2 Modelling Fixated Discourse
To model the fixated discourse phenomenon, we esti-
mate the length of the longest sex-related lexical chain
in a text. In particular, we start the construction of a
chain with an anchor word ?sex? in the first WordNet
meaning: ?sexual activity, sexual practice, sex, sex ac-
tivity (activities associated with sexual intercourse)?.
Then we continue the chain construction process until
the end of the text. We compare the relative lengths (in
percentage to the total number of words) of the con-
structed chains: we believe that the presence of a long
sex-related lexical chain in a text indicates fixated dis-
course.
5 Data
Pendar (2007) has summarized the possible types of
chat interactions with sexually explicit content:
1. Predator/Other
(a) Predator/Victim (victim is underage)
(b) Predator/Volunteer posing as a children
(c) Predator/Law enforcement officer posing as
a child
2. Adult/Adult (consensual relationship)
The most interesting from our research point of view
is data of the type 1(a), but obtaining such data is not
easy. However, the data of type 1(b) is freely avail-
able at the web site www.perverted-justice.com (PJ).
For our study, we have extracted chat logs from the
perverted-justice website. Since the victim is not real,
we considered only the chat lines written by predators.
As the negative dataset, we need data of type 2.
Therefore, we have downloaded cybersex chat logs
available at www.oocities.org/urgrl21f/. The archive
contains 34 one-on-one cybersex logs. We have sep-
arated lines of different authors, thereby obtaining 68
files.
We have also used a subset of the NPS chat cor-
pus (Forsythand and Martell, 2007), though it is not
of type 2, we believe it will make a good comparison.
We have extracted chat lines only for those adult au-
thors who had more than 30 lines written. Finally the
NPS dataset consisted of 65 authors.
6 Experiments
We carried out preliminary experiments on estimating
the length of lexical chains with sexually related con-
tent in PJ chats, and compare our results with the cor-
pora described above. Our goal is to explore the fea-
sibility of including fixated discourse as a feature in
pedophile detection.
We used Java WordNet Similarity library (Hope,
2008), which is a Java implementation of Perl Word-
net:Similarity (Pedersen et al, 2008). The average
length of the longest lexical chains (with respect to the
total number of words in a document) found for dif-
ferent corpora are presented in Table 1 and Table 2.
As we expected, sex-related lexical chains in the NPS
corpus are much shorter regardless of the similarity
metric used. The chains in the cybersex corpus are
even longer than in PJ corpus. This is probably due
88
Threshold
0.5 0.7
mean st.dev. mean st.dev.
PJ 12.21 3.63 9.3 5.68
Cybersex 18.28 16.8 9.98 12.76
NPS 5.66 5.9 2.42 4.77
Table 1: Average length of the longest lexical chain (percent-
age in the total number of words) computed with Leacock
and Chodorow semantic similarity.
Threshold
0.5 0.7
mean st.dev. mean st.dev.
PJ 8.24 4.51 6.68 5.06
Cybersex 12.04 15.86 9.13 11.64
NPS 0.67 0.96 0.41 0.66
Table 2: Average length of the longest lexical chain (per-
centage in the total number of words) computed with Resnik
semantic similarity.
to the fact that whilst both corpora contain conver-
sations about sex, cyberpedophiles are switching to
this topic gradually, whereas cybersex logs are entirely
sex-related.
7 Conclusions and Future Work
Detection of online sexual predation is a problem of
great importance. In this small scale study we have
focused on modelling fixated discourse using lexical
chains as a potential feature in the automated detec-
tion of online sex predators. The preliminary experi-
ments revealed that the lengths of sex-related lexical
chains vary with the nature of the corpus, with the pe-
dophiles logs having longer lexical chains than chat
logs not related to sex, while the cybersex chat logs
had the longest sex-related lexical chains of the three
corpora.
As it was mentioned in Section 1, chat language
is very informal and has a lot of abbreviations, slang
words, mistakes etc. Hence a fair amount of words
used there do not appear in WordNet and, therefore,
can not be included into the lexical chains. For exam-
ple, the word ?ssex? is obviously related and should
appear in the chain, though because of the different
spelling it is not found in WordNet and, therefore, is
not included into the chain. We plan to add a normal-
ization step prior to computing lexical chains. We have
used only one anchor word (?sex?) to start the lexical
chain. But several other words could also be good can-
didate for this.
Fixated discourse is not only about keeping the sex-
ual topic throughout all the conversation, it is also
about unwillingness to step out of the sexual conver-
sation and ignoring victim?s attempts to do it. There-
fore, the chat lines of the pseudo-victim should be an-
Figure 1: Average length of lexical chains calculated with
Leacock and Chodorow semantic similarity
Figure 2: Average length of lexical chains calculated with
Resnik semantic similarity
alyzed as well in order to find out if there were failed
attempts to switch the topic. This may also help to dis-
tinguish predation from cybersex conversation, since
in the cybersex conversation both participants want to
follow the topic. However, during this preliminary ex-
periments we have not yet considered this. Moreover,
perverted-justice is run by volunteers posing as poten-
tial victims. It is then possible that the volunteers? be-
havior differ from the responses of real children (Egan
et al, 2011). Their goal is to build a legal case against
the pedophile and, therefore, they are more willing to
provoke the predator than to avoid sex-related conver-
sation.
Another way to distinguish cybersex fixed topic
from the predator?s unwillingness to step out of it is
could be to use emotion classification based on the
Leary Rose model proposed by Vaassen and Daele-
mans (Vaassen and Daelemans, 2011). Their approach
is based on Interpersonal Circumplex suggested by
Leary (Leary, 1957). This is a model of interpersonal
communication that reflects whether one of the par-
ticipants is dominant and whether the participants are
cooperative. It was already mentioned that cyberpe-
dophiles tend to be dominant. Therefore, we believe
that the Leary Rose model can be useful in detecting
online sexual predation.
89
Once the model of fixated discourse is improved,
we plan to use it as an additional feature to detect pe-
dophiles in social media.
Acknowledgements
The first author was partially supported by a Google
Research Award and by a scholarship from the Uni-
versity of St. Petersburg. The second author
was supported by WIQ-EI IRSES project (grant no.
269180) from the European Commission, within the
FP 7 Marie Curie People, the MICINN research
project TEXT-ENTERPRISE 2.0 TIN2009-13391-
C04-03(Plan I+D+i), and the VLC/CAMPUS Micro-
cluster on Multimodal Interaction in Intelligent Sys-
tems. The last author was partially supported by the
UPV program PAID-02-11, award no. 1932.
References
Gene G. Abel and Nora Harlow. The Abel and Harlow
child molestation prevention study. Philadelphia, Xlibris,
2001.
Regina Barzilay and Michael Elhadad. Using lexical chains
for text summarization. In Proceedings of the Intelligent
Scalable Text Summarization Workshop, 1997.
Vincent Egan, James Hoskinson, and David Shewan. Per-
verted justice: A content analysis of the language used by
offenders detected attempting to solicit children for sex.
Antisocial Behavior: Causes, Correlations and Treat-
ments, 2011.
Eric N Forsythand and Craig H Martell. Lexical and dis-
course analysis of online chat dialog. International Con-
ference on Semantic Computing ICSC 2007, pages 19?26,
2007.
Michel Galley and Kathleen McKeown. Improving word
sense disambiguation in lexical chaining. In Proceedings
of IJCAI-2003, 2003.
David Hope. Java wordnet similarity library.
http://www.cogs.susx.ac.uk/users/drh21.
Claudia Leacock and Martin Chodorow. C-rater: Automated
scoring of short-answer questions. Computers and the
Humanities, 37(4):389?405, 2003.
Timothy Leary. Interpersonal diagnosis of personality; a
functional theory and methodology for personality evalu-
ation. Oxford, England: Ronald Press, 1957.
India McGhee, Jennifer Bayzick, April Kontostathis, Lynne
Edwards, Alexandra McBride and Emma Jakubowski.
Learning to identify Internet sexual predation. Interna-
tional Journal on Electronic Commerce 2011.
Kimberly J. Mitchell, David Finkelhor, and Janis Wolak.
Risk factors for and impact of online sexual solicitation
of youth. Journal of the American Medical Association,
285:3011?3014, 2001.
Jane Morris and Graeme Hirst. Lexical cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17(1):21?43, 1991.
Federal Bureau of Investigation. Nibrs flatfile tape master
record descriptions. 1995.
Ted Pedersen, Siddharth Patwardhan, Jason Michelizzi,
and Satanjeev Banerjee. Wordnet:similarity. http://wn-
similarity.sourceforge.net/.
Nick Pendar. Toward spotting the pedophile: Telling vic-
tim from predator in text chats. pages 235?241, Irvine,
California, 2007.
Philip Resnik. Using information content to evaluate seman-
tic similarity in a taxonomy. In IJCAI, pages 448?453,
1995.
Howard N. Snyder. Sexual assault of young children as re-
ported to law enforcement: Victim, incident, and offender
characteristics. a nibrs statistical report. Bureau of Justice
Statistics Clearinghouse, 2000.
Frederik Vaassen and Walter Daelemans. Automatic emo-
tion classification for interpersonal communication. In
Proceedings of the 2nd Workshop on Computational Ap-
proaches to Subjectivity and Sentiment Analysis (WASSA
2.011), pages 104?110. Association for Computational
Linguistics, 2011.
World health organization, international statistical classi-
fication of diseases and related health problems: Icd-10
section f65.4: Paedophilia. 1988.
90
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 176?184,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Grading the Quality of Medical Evidence
Binod Gyawali, Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham, AL, USA
{bgyawali,solorio}@cis.uab.edu
Yassine Benajiba
Clinical Decision Support Solutions Department
Philips Research North America, Briarcliff Manor, NY, USA
yassine.benajiba@philips.com
Abstract
Evidence Based Medicine (EBM) is the prac-
tice of using the knowledge gained from the
best medical evidence to make decisions in
the effective care of patients. This medi-
cal evidence is extracted from medical docu-
ments such as research papers. The increas-
ing number of available medical documents
has imposed a challenge to identify the ap-
propriate evidence and to access the quality
of the evidence. In this paper, we present
an approach for the automatic grading of ev-
idence using the dataset provided by the 2011
Australian Language Technology Association
(ALTA) shared task competition. With the
feature sets extracted from publication types,
Medical Subject Headings (MeSH), title, and
body of the abstracts, we obtain a 73.77%
grading accuracy with a stacking based ap-
proach, a considerable improvement over pre-
vious work.
1 Introduction
?Evidence Based Medicine (EBM) is the conscien-
tious, explicit, and judicious use of current best evi-
dence in making decisions about the care of individ-
ual patients? (Sackett et al, 1996). EBM requires to
identify the best evidence, understand the method-
ology and strength of the approaches reported in
the evidence, and bring relevant findings into clin-
ical practice. Davidoff et al (1995) express EBM in
terms of five related ideas. Their ideas imply that
the conclusions should be derived based on the best
evidence available, the clinical decisions should be
made based on the conclusions derived, and the per-
formance of the clinical decisions should be evalu-
ated constantly. Thus, physicians practicing EBM
should be constantly aware of the new ideas and
the best methodologies available based on the most
recent literature. But the amount of clinical docu-
ments available is increasing everyday. For exam-
ple, Pubmed, a service of the US National Library of
Medicine contains more than 21 million citations for
biomedical literature from MEDLINE, life science
journals, and online books (last updated on Decem-
ber 7, 2011) 1. The abundance of digital informa-
tion makes difficult the task of evaluating the quality
of results presented and the significance of the con-
clusions drawn. Thus, it has become an important
task to grade the quality of evidence so that the most
significant evidence is incorporated into the clinical
practices.
There are several scale systems available to grade
medical evidence. Some of them are: hierarchy
of evidence proposed by Evans (2003), Grading of
Recommendations Assessment, Development, and
Evaluation (GRADE) scale by GRADE (2004), and
Strength of Recommendation Taxonomy (SORT)
scale by Ebell et al (2004). The SORT scale ad-
dresses the quality, quantity, and consistency of evi-
dence and proposes three levels of ratings: A, B, and
C. Grade A is recommended based on the consistent,
good-quality patient-oriented evidence, grade B is
based on the inconsistent or limited-quality patient-
oriented evidence, and grade C is based on consen-
sus, disease-oriented evidence, usual practice, ex-
pert opinion or case studies.
1http://www.ncbi.nlm.nih.gov/books/NBK3827/
176
The Australasian Language Technology Associa-
tion (ALTA) 2011 organized the shared task compe-
tition2 to build an automatic evidence grading sys-
tem for EBM based on the SORT grading scale. We
carry out our experiments using the data set provided
for the competition and compare the accuracy of
grading the evidence by applying basic approaches
and an ensemble (stacking) based approach of clas-
sification. We show that the later approach can
achieve 73.77% of grading accuracy, a significant
improvement over the basic approaches. We further
extend our experiments to show that, using feature
sets generated from the method and conclusion sec-
tions of the abstracts helps to obtain higher accuracy
in evidence grading than using a feature set gener-
ated from the entire body of the abstracts.
2 Related Work
To the best of our knowledge, automatic evidence
grading based on a grading scale was initiated by
Sarker et al (2011). Their work was based on the
SORT scale to grade the evidence using the corpus
developed by Molla-Aliod (2010). They showed
that using only publication types as features could
yield an accuracy of 68% while other information
like publication types, journal names, publication
years, and article titles could not significantly help
to improve the accuracy of the grading. Molla-Aliod
and Sarker (2011) worked on the evidence grading
problem of 2011 ALTA shared task and achieved
an accuracy of 62.84% using three sequential clas-
sifiers, each trained by one of the following feature
sets: word n-grams from the abstract, publication
types, and word n-grams from the title. They ap-
plied a three way classification approach where the
instances classified as A or C were removed from
the test set and labeled as such, while instances
classified as B were passed to the next classifier in
the pipeline. They repeated this process until they
reached the end of three sequential classifiers.
Most of the EBM related work is focused on ei-
ther the identification of important statements from
the medical abstracts or the classification of med-
ical abstracts to facilitate the retrieval of impor-
tant documents. Work by Demner-Fushman et al
(2006), Dawes et al (2007), Kim et al (2011) au-
2http://www.alta.asn.au/events/sharedtask2011
tomatically identify the key statements in the med-
ical abstracts and classify them into different levels
that are considered important for EBM practitioners
in making decisions. Kilicoglu et al (2009) worked
on recognizing the clinically important medical ab-
stracts using an ensemble learning method (stack-
ing). They used different combinations of feature
vectors extracted from documents to classify the ev-
idence into relevant or non relevant classes. They
approached the problem as a binary classification
problem without using any grading scales.
Systematic Reviews (SRs) are very important
to support EBM. Creating and updating SRs is
highly inefficient and needs to identify the best evi-
dence. Cohen et al (2010) used a binary classifica-
tion system to identify the documents that are most
likely to be included in creating and updating SRs.
In this work, we grade the quality of evidence
based on the SORT scale, that is different from most
of the existing works related to classification of ab-
stracts and identification of key statements of ab-
stracts. We work on the same problem as by Molla-
Aliod and Sarker (2011) but, we undertake the prob-
lem with a different approach and use different sets
of features.
3 Dataset
We use the data of 2011 ALTA shared task compe-
tition that contains three different sets: training, de-
velopment and test set. The number of evidence in-
stances present in each set is shown in Table 1. Each
data set consists of instances with grades A, B, or C
based on the SORT scale. The distribution of evi-
dence grades is shown in Table 2.
Data Set No. of Evidence Instances
Training Set 677
Development Set 178
Test Set 183
Table 1: Evidence per data set
The evidence instances were obtained from the
corpus developed by Molla-Aliod and Santiago-
Martinez (2011). The corpus was generated based
on the question and the evidence based answer for
the question along with SOR grade obtained from
the ?Clinical Inquiries? section of the Journal of
177
Grades Training
set (%)
Development
set (%)
Test set
(%)
A 31.3 27.0 30.6
B 45.9 44.9 48.6
C 22.7 28.1 20.8
Table 2: Evidence distribution per grade
Family Practice (JFP). A sample question from the
JFP Clinical Inquiries section is ?How does smoking
in the home affect children with asthma??. Each ev-
idence contains at least one or more publications de-
pending upon from which publications the evidence
was generated. Each publication is an XML file con-
taining information such as abstract title, abstract
body, publication types, and MeSH terms. Each
publication is assigned at least one publication type
and zero or more MeSH terms. The MeSH terms
vocabulary 3 is developed and maintained by the
National Library of Medicine and is used in rep-
resentation, indexing and retrieval of medical doc-
uments. Some of the medical document retrieval
work emphasizes the use of MeSH terms in the ef-
ficient retrieval of documents (Trieschnigg et al,
2009; Huang et al, 2011). MeSH terms are also
used in document summarization (Bhattacharya et
al., 2011).
Figure 1: Sample data file
Each data set contains an additional grade file
with the information related to the evidence in-
stances, their grades, and the publications. A sam-
ple of the file is shown in Figure 1. The first column
contains the evidence id, the second column contains
the grades A, B, or C of the evidence based on the
SORT scale, and the remaining columns show the
publication id of each publication in the evidence.
3http://www.nlm.nih.gov/mesh
The problem in this task is to analyze the publica-
tions in each evidence provided and classify them
into A, B or C.
The dataset available for our research has ab-
stracts in two different formats. One of them con-
tains abstracts divided into sections: background,
objective, method, result, and conclusion. The other
format contains abstracts with all the information in
a single block without any sections. A sample of an
abstract having only four sections in the given data
is shown below:
Objectives: To determine the effectiveness of a muscle
strengthening program compared to a stretching program in
women with fibromyalgia (FM).
Methods: Sixty-eight women with FM were randomly as-
signed to a 12 week, twice weekly exercise program consisting
of either muscle strengthening or stretching. Outcome measures
included muscle strength (main outcome variable), flexibility,
weight, body fat, tender point count, and disease and symptom
severity scales.
Results: No statistically significant differences between
groups were found on independent t tests. Paired t tests revealed
twice the number of significant improvements in the strengthen-
ing group compared to the stretching group. Effect size scores
indicated that the magnitude of change was generally greater in
the strengthening group than the stretching group.
Conclusions: Patients with FM can engage in a specially
tailored muscle strengthening program and experience an im-
provement in overall disease activity, without a significant exer-
cise induced flare in pain. Flexibility training alone also results
in overall improvements, albeit of a lesser degree.
In the abstract above, we see that the approaches
applied for the study are described in the method
section, and the outcome and its effectiveness are
described in the conclusion section.
4 Proposed Methodology
In this paper we propose a system to identify the
correct grade of an evidence given publications in
the evidence. We deal with the problem of evi-
dence grading as a classification problem. In evi-
dence grading, basic approaches have been shown
to have poor performance. Molla-Aliod and Sarker
(2011) showed that a basic approach of using simple
bag-of-word features and a Naive Bayes classifier
achieved 45% accuracy and proposed a sequential
approach to improve the accuracy at each step. Our
preliminary studies of applying the simple classifi-
cation approach also showed similar results. Here,
we propose a stacking based approach (Wolpert,
178
1992) of evidence grading. Stacking based approach
builds a final classifier by combining the predictions
made by multiple classifiers to improve the predic-
tion accuracy. It involves two steps. In the first step,
multiple base-level classifiers are trained with dif-
ferent feature sets extracted from a dataset and the
classifiers are used to predict the classes of a sec-
ond dataset. Then, a higher level classifier is trained
using the predictions made by the base-level clas-
sifiers on the second dataset and used to predict the
classes of the actual test data. In this approach, base-
level classifiers are trained independent of each other
and allowed to predict the classes. Based on the
predictions made by these base-level classifiers, the
higher level classifier learns from those predictions
and makes a new prediction that is the final class.
Our stacking based approach of classification uses
five feature sets. In the first step of classification, we
train five classifiers using different feature sets per
classifier and use the classifiers to predict the grades
of the development dataset. Thus, at the end of the
first step, five different predictions on the develop-
ment dataset are obtained. In the second step, a new
classifier is trained using the grades predicted by the
five classifiers as features. This new classifier is then
used to predict the grades of the test dataset.
5 Features
We extracted six sets of features from the publica-
tions to perform our experiments. They are as fol-
lows:
1. Publication types
2. MeSH terms
3. Abstract title
4. Abstract body
5. Abstract method section
6. Abstract conclusion section
For feature set 1, we extracted 30 distinct publi-
cation types from the training data. For the MeSH
terms feature set, we selected 452 unique MeSH
terms extracted from the training data. The publi-
cations contained the descriptor name of the MeSH
terms having an attribute ?majortopicyn? with value
?Y? or ?N?. As MeSH terms feature set, we selected
only those MeSH term descriptor names having ma-
jortopicyn=?Y?.
We extracted the last four sets of features from
the title, body, method, and conclusion sections of
the abstracts. Here, the body of an abstract means
the whole content of the abstract, that includes back-
ground, objective, method, result, and conclusion
sections. We applied some preprocessing steps to
generate these feature sets. We also applied a feature
selection technique to reduce the number of features
and include only the high informative features from
these feature sets. The details about preprocess-
ing and feature selection techniques are described in
Section 6.
We performed all the experiments on the basis of
evidence, i.e. we created a single feature vector per
evidence. If an evidence contained more than one
publication, we generate its features as the union of
the features extracted from all its publications.
The grades of the evidence in the SORT scale
are based on the quality of evidence, basis of ex-
periments, the methodologies used, and the types of
analysis done. Grades also depend upon the effec-
tiveness of the approach used in the experiments.
The method section of an abstract contains the in-
formation related to the basis of the experiments,
such as randomized controlled trails, systematic re-
view, cohort studies, and the methods used in their
research. The conclusion section of the abstract
usually contains the assertion statements about how
strongly the experiment supports the claims. Anal-
ysis of the contents of abstracts shows that the in-
formation needed for grading on SORT scale is typ-
ically available in the method and conclusion sec-
tions, more than in the other sections of the abstracts.
Thus, we used the method and conclusion sections
of the abstracts to generate two different feature sets
so that only the features more likely to be important
in grading using the SORT rating would be included.
Separating method and conclusion sections of
the abstracts
In order to extract features from the method and con-
clusion sections, we should separate them from the
body of abstracts, which is a challenging task for
those abstracts without section headers. Of the to-
tal number of abstracts, more than one-third of the
abstracts do not contain the section headers. In or-
der to separate these sections, we used a very simple
approach based on the number of sentences present
179
in the method and conclusion sections, and the body
of the abstracts. We used the following information
to separate the method and conclusion sections from
these abstracts: i) Order of sections in the abstracts,
ii) Average number of sentences in the method and
conclusion sections of the abstracts having sections,
and iii) Average number of sentences in the entire
body of the abstracts not having sections. All the ab-
stracts having section headers contained the sections
in the same order: background, objective, method,
result and conclusion. From the available training
dataset, we calculated:
i. The average number of sentences in the method
(4.14) and conclusion (2.11) sections of the abstracts di-
vided into sections
ii. The average number of sentences (8.78) of the ab-
stracts not having sections
Based on these values, we fragmented the ab-
stracts that do not have the section headers and sepa-
rated the method and conclusion sections from them.
Table 3 shows how the method and conclusion sec-
tions of those abstracts were generated. For exam-
ple, the fourth row of the table says that, if an ab-
stract without section headers has 6, 7 or 8 sentences
(let it be n), then the 3rd, 4th and 5th sentences were
considered as the method section, and the nth sen-
tence was considered as the conclusion section.
Total sentences in
Abstracts(n)
Method Conclusion
1 None 1
2 or 3 1 n
4 or 5 2 and 3 n
6 or 7 or 8 2, 3 and 4 n
More than 8 3, 4 and 5 n-1 and n
Table 3: Selecting method and conclusion of the abstracts
having a single block
6 Experiments and Results
This section describes the two sets of experiments
performed to compare the performance of the stack-
ing based approach and the effectiveness of the base-
level classifiers used. The first set of experiments
was done to provide a baseline comparison against
our stacking based approach. The second set con-
sists of five experiments to evaluate different con-
figurations of stack based classifiers. The basic ap-
proach of classification implies the use of a single
classifier trained by using a single feature vector.
We applied preprocessing steps to generate fea-
ture sets from the title, body, method and conclusion
sections of the abstracts. The preprocessing steps
were: detecting sentences using OpenNLP Sentence
Detector4, stemming words in each sentence using
Porter Stemmer (Porter, 1980), changing the sen-
tences into lower-case, and removing punctuation
characters from the sentences. After the preprocess-
ing step, we generated features from the unigrams,
bigrams and trigrams in each part. We removed
those features from the feature sets that contained
the stopwords listed by Pubmed5 or contained any
token having a length less than three characters. To
remove the less informative features, we calculated
the information gain of the features in the training
data using Weka (Hall et al, 2009) and selected only
the top 500 high informative features for each fea-
ture set. We used the Weka SVM classifier for all the
experiments. Based on the best result obtained af-
ter a series of experiments run with different kernel
functions and regularization parameters, we chose
the SVM classifier with a linear kernel and regular-
ization parameter equals 1 for all the experiments.
We used a binary weight for all the features.
6.1 First set of experiments
In the first set, we performed nine experiments using
the basic classification approach and one experiment
using the stacking based approach. The details of
the experiments and the combinations of the features
used in them are as shown in Table 4.
The first six experiments in the table were imple-
mented by applying a basic approach of classifica-
tion and each using only a single set of features. Ex-
periments 7, 8, and 9 were similar to the first six
experiments except, they used more than one set of
features to create the feature vector. Each feature in
the experiments 7, 8, and 9 encode the section of its
origin. For example, if feature abdomen is present
in method as well as conclusion sections, it is rep-
resented as two distinct features conc abdomen and
method abdomen. In experiment 10, we applied
4http://incubator.apache.org/opennlp
5http://www.ncbi.nlm.nih.gov/books/NBK3827
/table/pubmedhelp.T43/?report=objectonly)
180
the stacking approach of classification using five
base-level classifiers. The base-level classifiers in
this experiment are the basic classifiers used in ex-
periments 1 to 5.
Exp.
No.
Features used Exp. type
1. Publication types
Basic approach
2. MeSH terms
3. Abstract title
4. Abstract method
5. Abstract conclusion
6. Abstract body
7.
Publication types,
MeSH terms
8.
Publication types,
MeSH terms,
Abstract title,
Abstract body
9.
Publication types,
MeSH terms,
Abstract title,
Abstract method,
Abstract conclusion
10.
Publication types
Stacking based
approach
MeSH terms
Abstract title
Abstract method
Abstract conclusion
Table 4: Experiments to compare basic approaches to a
stacking based approach
Figure 2 shows the results of the 10 experiments
described in Table 4 in the same order, from 1st to
10th place and the result of the experiment by Molla-
Aliod and Sarker (2011). The results show that
the stacking based approach gives the highest ac-
curacy (73.77%), outperforming all the basic ap-
proaches applying any combination of feature sets.
The stacking based approach outperforms the base-
line of a single layered classification approach (Exp
9) that uses all the five sets of features. Molla-Aliod
and Sarker (2011) showed that a simple approach of
using a single classifier and bag-of-words features
could not achieve a good accuracy (45.9%) and pro-
posed a new approach of using a sequence of classi-
fiers to achieve a better result. Similar to their simple
approach, our basic approaches could not achieve
good results, but their performance is comparable
to Molla-Aliod and Sarker (2011)?s baseline system.
The result of our stacking based approach shows that
our approach has a better accuracy than the sequen-
cial classification approach (62.84%) proposed by
Figure 2: Comparison of accuracy of basic approaches to
a stacking based approach. X-axis shows the experiments
and Y-axis shows the accuracy of the experiments. The
first nine experiments are based on the basic approach
and the tenth experiment is based on the stacking based
approach.
Molla-Aliod and Sarker (2011).
Our stacking based approach works on two lev-
els. In the first level, the base-level classifiers pre-
dict the grades of the evidence. In the next level,
these predictions are used to train a new classifier
that learns from the predictions to identify the grades
correctly. Moreover, the five feature sets used in our
experiments were unrelated to each other. For ex-
ample, the features present in MeSH headings were
different from the features used in publication types,
and similarly, the features present in the method sec-
tion of the abstract were different from the features
present in the conclusion section. Each base-level
classifier trained by one of these feature sets is spe-
cialized in that particular feature set. Thus, using
the predictions made by these specialized base-level
classifiers to train a higher level classifier helps to
better predict the grades, this cannot be achieved by
a single classifier trained by a set of features (Exp.
1, 2, 3, 4, 5, 6), or a group of different feature sets
(Exp. 7, 8, 9).
6.2 Second set of experiments
In the second set of experiments, we compared five
experiments performed varying the base-level clas-
sifiers used in our stack based approach. Experi-
ments 1 and 2 were performed using a single base-
level classifier, that means that the second classifier
is trained on only one feature. Experiments 3 and 4
were performed by using four base-level classifiers,
and experiment 5 was performed using five base-
181
level classifiers. The 5th experiment in this set is
same as the 10th experiment in the first set. The de-
tails about the feature sets used in each experiment
are shown in Table 5.
Exp.
No.
Features used No. of Base level
classifiers
1.
Publication types,
1
MeSH terms,
Abstract title,
Abstract body
2.
Publication types,
1
MeSH terms,
Abstract title,
Abstract method,
Abstract conclusion
3.
Publication types
4
MeSH terms
Abstract title
Abstract body
4.
Publication types
4
MeSH terms
Abstract title
Abstract method,
Abstract conclusion
5.
Publication types
5
MeSH terms
Abstract title
Abstract method
Abstract conclusion
Table 5: Experiments to compare stacking based ap-
proach
Figure 3 shows the accuracy of the five experi-
ments shown in Table 5 in the same order. It shows
that the accuracy of 1st and 2nd experiments is lower
than the accuracy of 3rd, 4th, and 5th experiments.
In these two experiments, a feature vector generated
from the prediction of a single base-level classifier
is used to train the higher level classifier, that is not
sufficient to make a correct decision.
Experiments 3, 4, and 5 show a considerable im-
provement in the accuracy of the grading. Compar-
ing the results of experiments 3 and 4, we see that
the 4th experiment has higher accuracy than the 3rd
one. The difference between these experiments was
the use of features from the method and conclusion
sections of the abstracts in the 4th experiment, while
using features from the entire body of abstracts in
the 3rd experiment. The higher accuracy in the 4th
experiment shows that the method and conclusion
sections of the experiment contain high informative
text that is important for evidence grading, while
Figure 3: Comparison of accuracy of the stacking based
approaches. X-axis shows the experiments and Y-axis
shows the accuracy of the experiments. 1st and 2nd ex-
periments use only one base-level classifier, 3rd and 4th
experiment are based on four base-level classifiers and
5th one uses five base-level classifiers.
the body of abstracts may contain some information
that is not relevant to the task. The same analysis
can also be inferred from the results of experiment
8 and 9 in the first set of experiments. The high-
est accuracy obtained in the 5th experiment of apply-
ing 5 base-level classifiers shows that identifying the
sections of the abstracts containing high informative
features and using a sufficient number of base-level
classifiers can help to achieve a good accuracy in ev-
idence grading.
7 Error Analysis
The result obtained by the stacking based approach
(5th experiment in Table 5) using five base-level clas-
sifiers gave a higher error rate in predicting grades
A and C, compared to the error rate in predict-
ing grade B. Most of the error is the misclassifica-
tion of A to C and vice versa. One of the possi-
ble reasons of this might be due to the use of the
feature set extracted from the conclusion section.
Among the five base-level classifiers used in the ex-
periment, the one trained by the features extracted
from the conclusion sections has the lowest accu-
racy (5th experiment in Figure 2). We evaluated the
text contained in the conclusion section of the ab-
stracts in our dataset. The section mostly contains
the assertion statements having the words showing
strong positive/negative meanings. Conclusion of A
grade evidence mostly contains the information that
strongly asserts the claim (e.g. emollient treatment
182
significantly reduced the high-potency topical cor-
ticosteroid consumption in infants with AD), while
that of C grade evidence is not strong enough to as-
sert the claim (e.g. PDL therapy should be consid-
ered among the better established approaches in the
treatment of warts, although data from this trial sug-
gest that this approach is probably not superior). It
seems that the problem might be because of not pro-
cessing the negations appropriately. So, in order to
preserve some negation information present in the
conclusion sections, we performed another experi-
ment by merging words no, not, nor with their suc-
cessor word to create a single token from the two
words. This approach still could not reduce the mis-
classification. Thus, the simple approach of extract-
ing unigram, bigram, and trigram features from the
conclusion section might not be sufficient and might
need to include higher level analysis related to as-
sertion/certainty of the statements to reduce the mis-
classification of the evidence.
Other possible reasons of the misclassification
of the evidence might be the imbalanced data set.
Our dataset (Table 2) contains higher number of in-
stances with grade B than those with grades A and C.
Moreover, the number of publications per evidence
is not uniform, that ranges from 1 to 8 publications
per evidence in the test data. Analyzing the results,
we found that misclassification of evidence having
only one publication is higher than that of the evi-
dence having more than one publication. If an ev-
idence contains only one publication, the features
of the evidence extracted from a single publication
might not be sufficient to accurately grade the evi-
dence and might lead to misclassification.
In order to evaluate the appropriateness of our
approach in extracting the method and conclusion
sections, we performed a manual inspection of ab-
stracts. We could not revise all the abstracts to ver-
ify the approach. Thus, we randomly selected 25
abstracts without section headers from the test data
and viewed the content in them. We found that the
conclusion section was appropriately extracted in al-
most all abstracts, while the selection of method sec-
tion was partially effective. Our approach was based
on the assumption that all the abstracts having many
sentences have all the sections (background, objec-
tive, method, result, and conclusion). But we found
that the abstracts do not follow the same format, and
the start sentence of the method section is not con-
sistent. Even a long abstract might sometimes start
with the method section, and sometimes the objec-
tive section might not be present in the abstracts.
This could lead to increase the error in our grading
system.
8 Conclusion
This paper presents an approach of grading the med-
ical evidence applying a stacking based classifier
using the features from publication types, MeSH
terms, abstract body, and method, and conclusion
sections of the abstracts. The results show that
this approach achieves an accuracy of 73.77%, that
is significantly better than the previously reported
work. Here, we present two findings: 1) We show
that the stacking based approach helps to obtain a
better result in evidence grading than the basic ap-
proach of classification. 2) We also show that the
method and conclusion sections of the abstracts con-
tain important information necessary for evidence
grading. Using the feature sets generated from these
two sections helps to achieve a higher accuracy than
by using the feature set generated from the entire
body of the abstracts.
In this work, all the information available in the
method and conclusion sections of the abstracts is
treated with equal weight. Evidence grading should
not depend upon specific disease names and syn-
dromes, but should be based on how strong the facts
are presented. We would like to extend our ap-
proach by removing the words describing specific
disease names, disease syndromes, and medications,
and giving higher weight to the terms that describe
the assertion of the statements. In our current work,
we apply a simple approach to extract the method
and conclusion sections from the abstracts not hav-
ing sections. Improving the approach by using a ma-
chine learning algorithm that can more accurately
extract the sections might help to increase the accu-
racy of grading. Including the information about the
strength of assertions made in the conclusion sec-
tions could also help in boosting the accuracy. Fu-
ture work would also include testing the effective-
ness of our approach on other diverse data sets hav-
ing complex structures of the evidence, or on a dif-
ferent grading scale.
183
References
Sanmitra Bhattacharya, Viet HaThuc, and Padmini Srini-
vasan. 2011. Mesh: a window into full text for doc-
ument summarization. Bioinformatics, 27(13):i120?
i128.
Aaron M. Cohen, Kyle Ambert, and Marian McDon-
agh. 2010. A Prospective Evaluation of an Au-
tomated Classification System to Support Evidence-
based Medicine and Systematic Review. AMIA Annu
Symp Proc., 2010:121 ? 125.
Frank Davidoff, Brian Haynes, Dave Sackett, and
Richard Smith. 1995. Evidence based medicine.
BMJ, 310(6987):1085?1086, 4.
Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,
Arlene Greenberg, and Jian-Yun Nie. 2007. The iden-
tification of clinically important elements within med-
ical journal abstracts: Patient-Population-Problem,
Exposure-Intervention, Comparison, Outcome, Dura-
tion and Results (PECODR). Informatics in Primary
Care, 15(1):9?16.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2006. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion, 13(1):52 ? 60.
M. H. Ebell, J. Siwek, B. D. Weiss, S. H. Woolf, J. Sus-
man, B. Ewigman, and M. Bowman. 2004. Strength
of recommendation taxonomy (SORT): a patient-
centered approach to grading evidence in the medi-
cal literature. American Family Physician, 69(3):548?
56+.
David Evans. 2003. Hierarchy of evidence: a frame-
work for ranking evidence evaluating healthcare inter-
ventions. Journal of Clinical Nursing, 12(1):77?84.
GRADE. 2004. Grading quality of evidence and strength
of recommendations. BMJ, 328(7454):1490, 6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1).
Minlie Huang, Aurlie Nvol, and Zhiyong Lu. 2011. Rec-
ommending MeSH terms for annotating biomedical
articles. Journal of the American Medical Informat-
ics Association, 18(5):660?667.
Halil Kilicoglu, Dina Demner-Fushman, Thomas C Rind-
flesch, Nancy L Wilczynski, and R Brian Haynes.
2009. Towards Automatic Recognition of Scientif-
ically Rigorous Clinical Research Evidence. Jour-
nal of the American Medical Informatics Association,
16(1):25?31.
Su Kim, David Martinez, Lawrence Cavedon, and Lars
Yencken. 2011. Automatic classification of sentences
to support Evidence Based Medicine. BMC Bioinfor-
matics, 12(Suppl 2):S5.
Diego Molla-Aliod and Maria Elena Santiago-Martinez.
2011. Development of a Corpus for Evidence Based
Medicine Summarisation. In Proceedings of the
Australasian Language Technology Association Work-
shop.
Diego Molla-Aliod and Abeed Sarker. 2011. Automatic
Grading of Evidence: the 2011 ALTA Shared Task.
In Proceedings of Australasian Language Technology
Association Workshop, pages 4?8.
Diego Molla-Aliod. 2010. A Corpus for Evidence
Based Medicine Summarisation. In Proceedings of the
Australasian Language Technology Association Work-
shop, volume 8.
MF Porter. 1980. An algorithm for sufx stripping. Pro-
gram, 14(3):130?137.
David L Sackett, William M C Rosenberg, J A Muir Gray,
R Brian Haynes, and W Scott Richardson. 1996. Ev-
idence based medicine: what it is and what it isn?t.
BMJ, 312(7023):71?72, 1.
Abeed Sarker, Diego Molla-Aliod, and Cecile Paris.
2011. Towards automatic grading of evidence. In Pro-
ceedings of LOUHI 2011 Third International Work-
shop on Health Document Text Mining and Informa-
tion Analysis, pages 51?58.
Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciska
de Jong, Wessel Kraaij, and Dietrich Rebholz-
Schuhmann. 2009. MeSH Up: effective MeSH text
classification for improved document retrieval. Bioin-
formatics, 25(11):1412?1418.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241 ? 259.
184
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 110?118,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
On the Impact of Sentiment and Emotion Based Features in
Detecting Online Sexual Predators
Dasha Bogdanova
University of
Saint Petersburg
dasha.bogdanova
@gmail.com
Paolo Rosso
NLE Lab - ELiRF
Universitat
Polite`cnica de Vale`ncia
prosso@dsic.upv.es
Thamar Solorio
CoRAL Lab
University of
Alabama at Birmingham
solorio@cis.uab.edu
Abstract
According to previous work on pedophile psy-
chology and cyberpedophilia, sentiments and
emotions in texts could be a good clue to de-
tect online sexual predation. In this paper, we
have suggested a list of high-level features, in-
cluding sentiment and emotion based ones, for
detection of online sexual predation. In partic-
ular, since pedophiles are known to be emo-
tionally unstable, we were interested in inves-
tigating if emotion-based features could help
in their detection. We have used a corpus of
predators? chats with pseudo-victims down-
loaded from www.perverted-justice.com and
two negative datasets of different nature: cy-
bersex logs available online and the NPS chat
corpus. Naive Bayes classification based on
the proposed features achieves accuracies of
up to 94% while baseline systems of word and
character n-grams can only reach up to 72%.
1 Introduction
Child sexual abuse and pedophilia are both problems
of great social concern. On the one hand, law en-
forcement is working on prosecuting and preventing
child sexual abuse. On the other hand, psycholo-
gists and mental specialists are investigating the phe-
nomenon of pedophilia. Even though the pedophilia
has been studied from different research points, it re-
mains to be a very important problem which requires
further research, especially from the automatic de-
tection point of view.
Previous studies report that in the majority of
cases of sexual assaults the victims are under-
aged (Snyder, 2000). On the Internet, attempts
to solicit children have become common as well.
Mitchell (2001) found out that 19% of children have
been sexually approached online. However, manual
monitoring of each conversation is impossible, due
to the massive amount of data and privacy issues. A
good alternative is the development of reliable tools
for detecting pedophilia in online social media is of
great importance.
In this paper, we address the problem of detecting
pedophiles with natural language processing (NLP)
techniques. This problem becomes even more chal-
lenging because of the chat data specificity. Chat
conversations are very different not only from the
written text but also from other types of social media
interactions, such as blogs and forums, since chat-
ting in the Internet usually involves very fast typing.
The data usually contains a large amount of mis-
takes, misspellings, specific slang, character flood-
ing etc. Therefore, accurate processing of this data
with automated syntactic analyzers is rather chal-
lenging.
Previous research on pedophilia reports that the
expression of certain emotions in text could be help-
ful to detect pedophiles in social media (Egan et al,
2011). Following these insights we suggest a list
of features, including sentiments as well as other
content-based features. We investigate the impact
of these features on the problem of automatic detec-
tion of online sexual predation. Our experimental
results show that classification based on such fea-
tures discriminates pedophiles from non-pedophiles
with high accuracy.
The remainder of the paper is structured as fol-
lows: Section 2 overviews related work on the topic,
110
Section 3 outlines the profile of a pedophile based on
the previous research. Our approach to the problem
of detecting pedophiles in social media on the ba-
sis of high-level features is presented in Section 4.
Experimental data is described in Section 5. We
show the results of the conducted experiments in
Section 6; they are followed by discussion and plans
for future research in Section 7. We finally draw
some conclusions in Section 8.
2 Related Research
The problem of automatic detection of pedophiles
in social media has been rarely addressed so far. In
part, this is due to the difficulties involved in hav-
ing access to useful data. There is an American
foundation called Perverted Justice (PJ). It investi-
gates cases of online sexual predation: adult volun-
teers enter chat rooms as juveniles (usually 12-15
year old) and if they are sexually solicited by adults,
they work with the police to prosecute the offenders.
Some chat conversations with online sexual preda-
tors are available at www.perverted-justice.com and
they have been the subject of analysis of recent re-
search on this topic.
Pendar (2007) experimented with PJ data. He sep-
arated the lines written by pedophiles from those
written by pseudo-victims and used a kNN classi-
fier based on word n-grams to distinguish between
them.
Another related research has been carried out by
McGhee et al (2011). The chat lines from PJ were
manually classified into the following categories:
1. Exchange of personal information
2. Grooming
3. Approach
4. None of the listed above classes
Their experiments have shown that kNN classifi-
cation achieves up to 83% accuracy and outperforms
a rule-based approach.
As it was already mentioned, pedophiles often
create false profiles and pretend to be younger or
of another gender. Moreover, they try to copy
children?s behavior. Automatically detecting age
and gender in chat conversations could then be the
first step in detecting online predators. Peersman
et al (2011) have analyzed chats from the Bel-
gium Netlog social network. Discrimination be-
tween those who are older than 16 from those who
are younger based on a Support Vector Machine
classification yields 71.3% accuracy. The accuracy
is even higher when the age gap is increased (e.g.
the accuracy of classifying those who are less than
16 from those who are older than 25 is 88.2%). They
have also investigated the issues of the minimum
amount of training data needed. Their experiments
have shown that with 50% of the original dataset the
accuracy remains almost the same, and with only
10% it is still much better than the random baseline
performance.
NLP techniques were as well applied to capture
child sexual abuse data in P2P networks (Panchenko
et al, 2012). The proposed text classification system
is able to predict with high accuracy if a file contains
child pornography by analyzing its name and textual
description.
Our work neither aims at classification of chat
lines into categories as it was done by McGhee et
al. (2011) nor at discriminating between victim and
predator as it was done by Pendar (2007), but at dis-
tinguishing between pedophile?s and not pedophile?s
chats, in particular, by utilizing clues provided by
psychology and sentiment analysis.
3 Profiling the Pedophile
Pedophilia is a ?disorder of adult personality and be-
havior? which is characterized by sexual interest in
prepubescent children (International statistical clas-
sification of diseases and related health problems,
1988). Even though solicitation of children is not a
medical diagnosis, Abel and Harlow (2001) reported
that 88% of child sexual abuse cases are commit-
ted by pedophiles. Therefore, we believe that under-
standing behavior of pedophiles could help to detect
and prevent online sexual predation. Even though an
online sexual offender is not always a pedophile, in
this paper we use these terms as synonyms.
Previous research reports that about 94% of sex-
ual offenders are males. With respect to female sex-
ual molesters, it is reported, that they tend to be
young and, in these cases, men are often involved
as well (Vandiver and Kercher, 2004). Sexual as-
111
sault offenders are more often adults (77%), though
in 23% of cases children are solicited by other juve-
niles.
Analysis of pedophiles? personality characterizes
them with feelings of inferiority, isolation, lone-
liness, low self-esteem and emotional immaturity.
Moreover, 60%-80% of them suffer from other psy-
chiatric illnesses (Hall and Hall, 2007). In general,
pedophiles are less emotionally stable than mentally
healthy people.
3.1 Profile of the Online Sexual Predator
Hall and Hall (2007) noticed that five main types
of computer-based sexual offenders can be distin-
guished: (1) the stalkers, who approach children in
chat rooms in order to get physical access to them;
(2) the cruisers, who are interested in online sexual
molestation and not willing to meet children offline;
(3) the masturbators, who watch child pornography;
(4) the networkers or swappers, who trade informa-
tion, pornography, and children; and (5) a combi-
nation of the four types. In this study we are in-
terested in detecting stalkers (type (1)) and cruisers
(type (2)).
The language sexual offenders use was analyzed
by Egan et al (2011). The authors considered the
chats available from PJ. The analysis of the chats
revealed several characteristics of predators? lan-
guage:
? Implicit/explicit content. On the one hand,
predators shift gradually to the sexual conversa-
tion, starting with more ordinary compliments:
Predator: hey you are really cute
Predator: u are pretty
Predator: hi sexy
On the other hand, the conversa-
tion then becomes overtly related to
sex. They do not hide their intentions:
Predator: can we have sex?
Predator: you ok with sex with me and
drinking?
? Fixated discourse. Predators are not willing to
step aside from the sexual conversation. For
example, in this conversation the predator al-
most ignores the question of pseudo-victim and
comes back to the sex-related conversation:
Predator: licking dont hurt
Predator: its like u lick ice cream
Pseudo-victim: do u care that im 13 in
march and not yet? i lied a little bit b4
Predator: its all cool
Predator: i can lick hard
? Offenders often understand that what they are
doing is not moral:
Predator: i would help but its not moral
? They transfer responsibility to the victim:
Pseudo-victim: what ya wanta do when u
come over
Predator: whatever?movies, games, drink,
play around?it?s up to you?what would you
like to do?
Pseudo-victim: that all sounds good
Pseudo-victim: lol
Predator: maybe get some sexy pics of you
:-P
Predator: would you let me take pictures of
you? of you naked? of me and you playing?
:-D
? Predators often behave as children, copying
their linguistic style. Colloquialisms appear of-
ten in their messages:
Predator: howwwww dy
...
Predator: i know PITY MEEEE
? They try to minimize the risk of being prose-
cuted: they ask to delete chat logs and warn
victims not to tell anyone about the talk:
112
Predator: don?t tell anyone we have been
talking
Pseudo-victim: k
Pseudo-victim: lol who would i tell? no
one?s here.
Predator: well I want it to be our secret
? Though they finally stop being cautious and in-
sist on meeting offline:
Predator: well let me come see you
Pseudo-victim: why u want 2 come
over so bad?
Predator: i wanna see you
In general Egan et al (Egan et al, 2011) have
found online solicitation to be more direct, while in
real life children seduction is more deceitful.
4 Our Approach
We address the problem of automatic detection of
online sexual predation. While previous studies
were focused on classifying chat lines into differ-
ent categories (McGheeet al, 2011) or distinguish-
ing between offender and victim (Pendar, 2007), in
this work we address the problem of detecting sex-
ual predators.
We formulate the problem of detecting pedophiles
in social media as the task of binary text categoriza-
tion: given a text (a set of chat lines), the aim is to
predict whether it is a case of cyberpedophilia or not.
4.1 Features
On the basis of previous analysis of pedophiles? per-
sonality (described in previous section), we consider
as features those emotional markers that could un-
veil a certain degree of emotional instability, such
as feelings of inferiority, isolation, loneliness, low
self-esteem and emotional immaturity.
On the one hand, pedophiles try to be nice with a
victim and make compliments, at least in the begin-
ning of a conversation. Therefore, the use of posi-
tive words is expected. On the other hand, as it was
described earlier, pedophiles tend to be emotionally
unstable and prone to lose temper, hence they might
start using words expressing anger and negative lex-
icon. Other emotions can be as well a clue to detect
pedophiles. For example, offenders often demon-
strate fear, especially with respect to being prose-
cuted, and they often lose temper and express anger:
Pseudo-victim: u sad didnt car if im 13. now u car.
Predator: well, I am just scared about being in
trouble or going to jail
Pseudo-victim: u sad run away now u say no. i
gues i dont no what u doin
Predator: I got scared
Predator: we would get caugth sometime
In this example pseudo-victim is not answering:
Predator: hello
Predator: r u there
Predator:
Predator: thnx a lot
Predator: thanx a lot
Predator:
Predator: u just wast my time
Predator: drive down there
Predator: can u not im any more
Here the offender is angry because the pseudo-
victim did not call him:
Predator: u didnt call
Predator: i m angry with u
Therefore, we have decided to use markers of
basic emotions as features. At the SemEval 2007
task on ?Affective Text? (Strapparava and Mihal-
cea, 2007) the problem of fine-grained emotion an-
notation was defined: given a set of news titles,
the system is to label each title with the appropri-
ate emotion out of the following list: ANGER, DIS-
GUST, FEAR, JOY, SADNESS, SURPRISE. In this
research work we only use the percentages of the
markers of each emotion.
We have also borrowed several features from
McGhee et al (2011):
? Percentage of approach words. Approach
words include verbs such as come and meet and
such nouns as car and hotel.
? Percentage of relationship words. These words
refer to dating (e.g. boyfriend, date).
113
? Percentage of family words. These words are
the names of family members (e.g. mum, dad,
brother).
? Percentage of communicative desensitization
words. These are explicit sexual terms offend-
ers use in order to desensitize the victim (e.g.
penis, sex).
? Percentage of words expressing sharing infor-
mation. This implies sharing basic information,
such as age, gender and location, and sending
photos. The words include asl, pic.
Since pedophiles are known to be emotionally un-
stable and suffer from psychological problems, we
consider features reported to be helpful to detect
neuroticism level by Argamon et al (2009). In par-
ticular, the features include percentages of personal
and reflexive pronouns and modal obligation verbs
(have to, has to, had to, must, should, mustn?t, and
shouldn?t).
We consider the use of imperative sentences and
emoticons to capture the predators tendencies to
be dominant and copy childrens? behaviour respec-
tively.
The study of Egan et al (Egan et al, 2011) has
revealed several recurrent themes that appear in PJ
chats. Among them, fixated discourse: the unwill-
ingness of the predator to change the topic. In (Bog-
danova et al, 2012) we present experiments on mod-
eling the fixated discourse. We have constructed lex-
ical chains (Morris and Hirst, 1991) starting with
the anchor word ?sex? in the first WordNet mean-
ing: ?sexual activity, sexual practice, sex, sex activ-
ity (activities associated with sexual intercourse)?.
We have finally used as a feature the length of the
lexical chain constructed with the Resnik similarity
measure (Resnik, 1995) with the threshold = 0.7.
The full list of features is presented in Table 1.
5 Datasets
Pendar (2007) has summarized the possible types of
chat interactions with sexually explicit content:
1. Predator/Other
(a) Predator/Victim (victim is underaged)
(b) Predator/Volunteer posing as a children
(c) Predator/Law enforcement officer posing
as a child
2. Adult/Adult (consensual relationship)
The most interesting from our research point of
view is data of the type 1a, but obtaining such
data is not easy. However, the data of the type 1b
is freely available at the web site www.perverted-
justice.com. For our study, we have extracted chat
logs from the perverted-justice website. Since the
victim is not real, we considered only the chat lines
written by predators.
Since our goal is to distinguish sex related chat
conversations where one of the parties involved is a
pedophile, the ideal negative dataset would be chat
conversations of type 2 (consensual relations among
adults) and the PJ data will not meet this condition
for the negative instances. We need additional chat
logs to build the negative dataset. We used two neg-
ative datasets in our experiments: cybersex chat logs
and the NPS chat corpus.
We downloaded the cybersex chat logs available
at www.oocities.org/urgrl21f/. The archive contains
34 one-on-one cybersex logs. We have separated
lines of different authors, thereby obtaining 68 files.
We have also used the subset the of NPS chat cor-
pus (Forsythand and Martell, 2007), though it is not
of type 2. We have extracted chat lines only for those
adult authors who had more than 30 lines written.
Finally the dataset consisted of 65 authors. From
each dataset we have left 20 files for testing.
6 Experiments
To distinguish between predators and not predators
we used a Naive Bayes classifier, already success-
fully utilized for analyzing chats by previous re-
search (Lin, 2007). To extract positive and nega-
tive words, we used SentiWordNet (Baccianella et
al., 2010). The features borrowed from McGhee et
al. (2011), were detected with the list of words au-
thors made available for us. Imperative sentences
were detected as affirmative sentences starting with
verbs. Emoticons were captured with simple regular
expressions.
Our dataset is imbalanced, the majority of the chat
logs are from PJ. To make the experimental data
more balanced, we have created 5 subsets of PJ cor-
114
Feature Class Feature Example Resource
Emotional Positive Words cute, pretty SentiWordNet
Markers Negative Words dangerous, annoying (Baccianella et al, 2010)
JOY words happy, cheer WordNet-Affect
SADNESS words bored, sad (Strapparava and
ANGER words annoying, furious Valitutti, 2004)
SURPRISE words astonished, wonder
DISGUST words yucky, nausea
FEAR words scared, panic
Features borrowed Approach words meet, car McGhee et al (2011)
from McGhee Relationship nouns boyfriend, date
et al (2011) Family words mum, dad
Communicative desensitization words sex. penis
Information words asl, home
Features helpful Personal pronouns I, you Argamon et al (2009)
to detect Reflexive pronouns myself, yourself
neuroticism level Obligation verbs must, have to
Features derived Fixated Discourse see in Section 3.1 Bogdanova et al (2012)
from pedophile?s
psychological profile
Other Emoticons 8), :(
Imperative sentences Do it!
Table 1: Features used in the experiments.
pus, each of which contained chat lines from 60 ran-
domly selected predators.
For the cybersex logs, half of the chat sessions
belong to the same author. We used this author for
training, and the rest for testing, in order to prevent
the classification algorithm from learning to distin-
guish this author from pedophiles.
For comparison purposes, we experimented with
several baseline systems using low-level features
based on n-grams at the word and character level,
which were reported as useful features by related re-
search (Peersman et al, 2011). We trained naive
Bayes classifiers using word level unigrams, bi-
grams and trigrams. We also trained naive Bayes
classifiers using character level bigrams and tri-
grams.
The classification results are presented in Tables 2
and 3. The high-level features outperform all the
low-level ones in both the cybersex logs and the NPS
chat datasets and achieve 94% and 90% accuracy on
these datasets respectively.
Cybersex chat logs are data of type 2 (see previ-
ous section), they contain sexual content and, there-
fore, share same of the same vocabulary with the
perverted-justice data, whilst the NPS data gener-
ally is not sex-related. Therefore, we expected low-
level features to provide better results on the NPS
data. The experiments have shown that, except for
the character bigrams, all low-level features consid-
ered indeed work worse in case of cybersex logs
(see the average rows in both tables). The aver-
age accuracy in this case varies between 48% and
58%. Surprisingly, low-level features do not work
as good as we expected in case of the NPS chat
dataset: bag of words provides only 61% accuracy.
Among other low-level features, character trigrams
provide the highest accuracy of 72%, which is still
much lower than the one of the high-level features
(90%). The high-level features yield a lower accu-
racy (90% accuracy) on the PJ-NPS dataset than in
the case of PJ-cybersex logs (94% accuracy). This is
probably due to the data diversity: cybersex chat is
a very particular type of a conversation, though NPS
chat corpora can contain any type of conversations
up to sexual predation.
115
Accuracy
High-level Bag of Term Term Character Character
features words bigrams trigrams bigrams trigrams
Run 1 0.93 0.38 0.55 0.60 0.73 0.78
Run 2 0.95 0.40 0.50 0.53 0.75 0.45
Run 3 0.95 0.70 0.45 0.53 0.48 0.50
Run 4 0.98 0.43 0.53 0.53 0.50 0.38
Run 5 0.90 0.50 0.48 0.53 0.45 0.50
Average 0.94 0.48 0.50 0.54 0.58 0.52
Table 2: Results of Naive Bayes classification applied to perverted-justice data and cybersex chat logs.
Accuracy
High-level Bag of Term Term Character Character
features words bigrams trigrams bigrams trigrams
Run 1 0.93 0.73 0.60 0.60 0.68 0.75
Run 2 0.95 0.68 0.53 0.53 0.48 0.45
Run 3 0.95 0.58 0.53 0.53 0.48 0.85
Run 4 0.98 0.53 0.53 0.53 0.23 0.80
Run 5 0.90 0.53 0.53 0.53 0.25 0.75
Average 0.92 0.61 0.54 0.54 0.42 0.72
Table 3: Results of Naive Bayes classification applied to perverted-justice data and NPS chats.
7 Discussion and Future Work
We have conducted experiments on detecting pe-
dophiles in social media with a binary classification
algorithm. In the experiments we used two negative
datasets of different nature: the first one is more ap-
propriate, it contains one-on-one cybersex conversa-
tions, while the second dataset is extracted from the
NPS chat corpus and contains logs from chat rooms,
and, therefore, is less appropriate since the conver-
sations are not even one on one.
It is reasonable to expect that in the case of the
negative data consisting of cybersex logs, distin-
guishing cyberpedophiles is a harder task, than in the
case of the NPS data. The results obtained with the
baseline systems support this assumption: we obtain
higher accuracy for the NPS chats in all but character
bi-grams. The interesting insight from these results
is that our proposed higher-level features are able to
boost accuracy to 94% on the seemingly more chal-
lenging task.
Our error analysis showed that the NPS logs mis-
classified with the high-level features are also mis-
classified by the baseline systems. These instances
either share the same lexicon or are about the same
topics. Therefore they are more similar to cyberpe-
dophiles training data than the training data of the
NPS corpus, which is very diverse. These examples
are taken from misclassified NPS chat logs:
User: love me like a bomb baby come on get it on
...
User: ryaon so sexy
User: you are so anal
User: obviously i didn?t get it
User: just loosen up babe
...
User: i want to make love to him
User: right field wrong park lol j/k
User: not me i put them in the jail lol
User: or at least tell the cops where to go to get the
bad guys lol
In the future we plan to further investigate the
misclassified data. The feature extraction we have
implemented does not use any word sense disam-
biguation. This can as well cause mistakes since
the markers are not just lemmas but words in par-
ticular senses, since for example the lemma ?fit?
can be either a positive marker (?a fit candidate?)
or negative (?a fit of epilepsy?), depending on the
116
context. Therefore we plan to employ word sense
disambiguation techniques during the feature extrac-
tion phase.
So far we have only seen that the list of fea-
tures we have suggested provides good results.
They outperform all thelow-level features consid-
ered. Among those low-level features, character tri-
grams provide the best results on the NPS data (72%
accuracy), though on the cybersex logs they achieve
only 54%. We plan to merge low-level and high-
level features in order to see if this could improve
the results.
In the future we plan also to explore the impact of
each high-level feature. To better understand which
ones carry more discriminative power and if we can
reduce the number of features. All these experi-
ments will be done employing naive Bayes as well
as Support Vector Machines as classifiers.
8 Conclusions
This paper presents some results of an ongoing re-
search project on the detection of online sexual pre-
dation, a problem the research community is inter-
ested in, as the PAN task on Sexual Predator Identi-
fication1 suggests.
Following the clues given by psychological re-
search, we have suggested a list of high-level fea-
tures that should take into account the level of emo-
tional instability of pedophiles, as well as their feel-
ings of inferiority, isolation, loneliness, low self-
esteem etc. We have considered as well such low-
level features as character bigrams and trigrams and
word unigrams, bigrams and trigrams. The Naive
Bayes classification based on high-level features
achieves 90% and 94% accuracy when using NPS
chat corpus and the cybersex chat logs as a nega-
tive dataset respectively, whereas low-level features
achieve only 42%-72% and 48%-58% accuracy on
the same data.
Acknowledgements
The research of Dasha Bogdanova was carried out
during the 3-month internship at the Universitat
Polite`cnica de Vale`ncia (scholarship of the Univer-
sity of St.Petersburg). Her research was partially
1http://pan.webis.de/
supported by Google Research Award. The collab-
oration with Thamar Solorio was possible thanks
to her one-month research visit at the Universi-
tat Polite`cnica de Vale`ncia (program PAID-PAID-
02-11 award n. 1932). The research work of
Paolo Rosso was done in the framework of the Eu-
ropean Commission WIQ-EI IRSES project (grant
no. 269180) within the FP 7 Marie Curie People,
the MICINN research project TEXT-ENTERPRISE
2.0 TIN2009-13391-C04-03(Plan I+D+i), and the
VLC/CAMPUS Microcluster on Multimodal Inter-
action in Intelligent Systems.
References
Gene G. Abel and Nora Harlow. The Abel and Har-
low child molestation prevention study. Philadelphia,
Xlibris, 2001.
Shlomo Argamon, Moshe Koppel, James Pennebaker,
and Jonathan Schler. Automatically profiling the au-
thor of an anonymous text. Communications of the
ACM, 52 (2):119?123, 2009.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. the Sev-
enth International conference on Language Resources
and Evaluation, 2010.
Regina Barzilay and Michael Elhadad. Using lexical
chains for text summarization. In Proceedings of
the Intelligent Scalable Text Summarization Workshop,
1997.
Dasha Bogdanova, Paolo Rosso, Thamar Solorio. Mod-
elling Fixated Discourse in Chats with Cyberpe-
dophiles. Proceedings of the Workshop on Compu-
tational Approaches to Deception Detection, EACL,
2012.
Vincent Egan, James Hoskinson, and David Shewan.
Perverted justice: A content analysis of the language
used by offenders detected attempting to solicit chil-
dren for sex. Antisocial Behavior: Causes, Correla-
tions and Treatments, 2011.
Eric N Forsythand and Craig H Martell. Lexical and dis-
course analysis of online chat dialog. International
Conference on Semantic Computing ICSC 2007, pages
19?26, 2007.
Michel Galley and Kathleen McKeown. Improving word
sense disambiguation in lexical chaining. In Proceed-
ings of IJCAI-2003, 2003.
Ryan C. W. Hall and Richard C. W. Hall. A profile
of pedophilia: Definition, characteristics of offenders,
recidivism, treatment outcomes, and forensic issues.
Mayo Clinic Proceedings, 2007.
117
David Hope. Java wordnet similarity library.
http://www.cogs.susx.ac.uk/users/drh21.
Claudia Leacock and Martin Chodorow. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):389?405, 2003.
Timothy Leary. Interpersonal diagnosis of personality;
a functional theory and methodology for personality
evaluation. Oxford, England: Ronald Press, 1957.
Jane Lin. Automatic author profiling of online chat logs.
PhD thesis, 2007.
India McGhee, Jennifer Bayzick, April Kontostathis,
Lynne Edwards, Alexandra McBride and Emma
Jakubowski. Learning to identify Internet sexual pre-
dation. International Journal on Electronic Commerce
2011.
Kimberly J. Mitchell, David Finkelhor, and Janis Wolak.
Risk factors for and impact of online sexual solicita-
tion of youth. Journal of the American Medical Asso-
ciation, 285:3011?3014, 2001.
Jane Morris and Graeme Hirst. Lexical cohesion com-
puted by thesaural relations as an indicator of the struc-
ture of text. Computational Linguistics, 17(1):21?43,
1991.
Ted Pedersen, Siddharth Patwardhan, Jason Miche-
lizzi, and Satanjeev Banerjee. Wordnet:similarity.
http://wn-similarity.sourceforge.net/.
Claudia Peersman, Walter Daelemans, and Leona Van
Vaerenbergh. Predicting age and gender in online so-
cial networks. In Proceedings of the 3rd Workshop on
Search and Mining User-Generated Contents, 2011.
Nick Pendar. Toward spotting the pedophile: Telling vic-
tim from predator in text chats. In Proceedings of
the International Conference on Semantic Computing,
pages 235?241, Irvine, California, 2007.
Alexander Panchenko, Richard Beaufort, Cedrick Fairon.
Detection of Child Sexual Abuse Media on P2P Net-
works: Normalization and Classification of Associated
Filenames. In Proceedings of the LREC Workshop on
Language Resources for Public Security Applications,
2012.
Philip Resnik. Using information content to evaluate se-
mantic similarity in a taxonomy. In IJCAI, pages 448?
453, 1995.
Howard N. Snyder. Sexual assault of young children as
reported to law enforcement: Victim, incident, and of-
fender characteristics. a nibrs statistical report. Bureau
of Justice Statistics Clearinghouse, 2000.
Carlo Strapparava and Rada Mihalcea. Semeval-2007
task 14: affective text. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, Se-
mEval?07, pages 70?74, 2007.
Carlo Strapparava and Alessandro Valitutti. Wordnet-
affect: an affective extension of wordnet. In Proceed-
ings of the 4th International Conference on Language
Re-sources and Evaluation, 2004.
Frederik Vaassen and Walter Daelemans. Automatic
emotion classification for interpersonal communica-
tion. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 104?110. Association
for Computational Linguistics, 2011.
Donna M. Vandiver and Glen Kercher. Offender and vic-
tim characteristics of registered female sexual offend-
ers in Texas: A proposed typology of female sexual
offenders. Sex Abuse, 16:121?137, 2004
World health organization, international statistical clas-
sification of diseases and related health problems: Icd-
10 section f65.4: Paedophilia. 1988.
118
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 59?68,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Case Study of Sockpuppet Detection in Wikipedia
Thamar Solorio and Ragib Hasan and Mainul Mizan
The University of Alabama at Birmingham
1300 University Blvd.
Birmingham, AL 35294, USA
{solorio,ragib,mainul}@cis.uab.edu
Abstract
This paper presents preliminary results of using
authorship attribution methods for the detec-
tion of sockpuppeteering in Wikipedia. Sock-
puppets are fake accounts created by malicious
users to bypass Wikipedia?s regulations. Our
dataset is composed of the comments made
by the editors on the talk pages. To overcome
the limitations of the short lengths of these
comments, we use an voting scheme to com-
bine predictions made on individual user en-
tries. We show that this approach is promising
and that it can be a viable alternative to the
current human process that Wikipedia uses to
resolve suspected sockpuppet cases.
1 Introduction
Collaborative projects in social media have become
very popular in recent years. A very successful ex-
ample of this is Wikipedia, which has emerged as the
world?s largest crowd-sourced encyclopaedia. This
type of decentralized collaborative processes are ex-
tremely vulnerable to vandalism and malicious be-
havior. Anyone can edit articles in Wikipedia and/or
make comments in article discussion pages. Reg-
istration is not mandatory, but anyone can register
an account in Wikipedia by providing only little in-
formation about themselves. This ease of creating
an identity has led malicious users to create mul-
tiple identities and use them for various purposes,
ranging from block evasion, false majority opinion
claims, and vote stacking. This is an example of
the multi aliasing problem known as ?The Sybil At-
tack? (Douceur, 2002). Unfortunately, Wikipedia
does not provide any facility to detect such multi-
ple identities. The current process is carried out by
humans, is very time consuming, and final resolu-
tion to cases of multiple identities is based on human
intuition. A smart sockpuppet can therefore evade
detection by using multiple IP addresses, modifying
writing style, and changing behavior. Also, a mali-
cious user can create sleeper accounts that perform
benign edits from time to time, but are used for sock-
puppetry when needed. Identifying such accounts
as sockpuppets is not obvious as these accounts may
have a long and diverse edit history.
Sockpuppets are a prevalent problem in Wikipedia,
there were close to 2,700 unique suspected cases
reported in 2012. In this paper, we present a small
scale study of automated detection of sockpuppets
based on machine learning. We approach this
problem from the point of view of authorship attri-
bution (AA), where the task consists of analyzing a
written document to predict the true author. If we
can successfully model the editors? unique writing
style from their comments, then we can use this
information to link the sockpuppet accounts to their
corresponding puppeteer. We focus on the content
from the talk pages since the articles edited on
Wikipedia have a fixed and very uniform style. In
contrast, we have observed that editors write in a
more free-form style during discussions carried out
on the talk pages. Our results show that a two-stage
process for the task can achieve promising results.
The contributions of this study are as follows:
? We present encouraging preliminary results on
using authorship attribution approaches for un-
59
covering real sockpuppet cases in Wikipedia. To
the best of our knowledge, we are the first to
tackle this problem.
? We identify novel features that have high dis-
criminative power and are suitable for this task,
where the input text is very short. These features
can be helpful in other social media settings, as
there are many shared characteristics across this
genre.
The rest of the paper is organized as follows:
in Section 2, we provide a detailed discussion on
Wikipedia?s editing environment and culture. In Sec-
tion 3, we talk about authorship attribution and re-
lated work. Then in Section 4, we present our de-
tailed approach. In Sections 5, 6, and 7, we discuss
the data set, experimental setup, and results, respec-
tively. Finally, we present an overall discussion and
future directions in Sections 8 and 9.
2 Background
In Wikipedia, whenever a user acts in bad faith, van-
dalizes existing articles, or creates spurious articles,
that user is banned from editing new content. The
ban can last for some hours, to days, and in some
cases it can be permanent. Sometimes, a banned user
creates a new account to circumvent the ban, or edits
Wikipedia without signing in.
These extra accounts or IP addresses, from which
logged out edits are made, are called sockpuppets.
The primary (oldest) account is called the sockpup-
peteer. Whenever an editor is suspected to be a sock-
puppet of another editor, a sockpuppet investigation
case is filed against those accounts. Any editor can
file a case, but the editor must provide supporting evi-
dence as well. Typical evidence includes information
about the editing actions related to those accounts,
such as the articles, the topics, vandalism patterns,
timing of account creation, timing of edits, and voting
pattern in disagreements.
Sometime after the case is filed, an administrator
will investigate the case. An administrator is an editor
with privileges to make account management deci-
sions, such as banning an editor. If the administrator
is convinced that the suspect is a sockpuppet, he de-
clares the verdict as confirmed. He also issues bans
to the corresponding accounts and closes the case.
If an administrator cannot reach a verdict on a case,
he asks for a check user to intervene. Check users
are higher privileged editors, who have access to pri-
vate information regarding editors and edits, such as
the IP address from which an editor has logged in.
Other interested editors in the case, or the original
editor who filed the case can also ask for a check
user to intervene. The check user will review the ev-
idence, as well as private information regarding the
case, and will try to establish the connection between
the sockpuppet and puppeteer. Then the check user
will rule on the case. Finally, another administrator
will look at the check user report and issue a final
verdict. During the process, the accused editors, both
the puppeteer and the sockpuppet, can submit evi-
dence in their favor. But this additional evidence is
not mandatory.
The current process to resolve suspected cases of
sockpuppets has several disadvantages. We have al-
ready mentioned the first one. Because it is a manual
process, it is time consuming and expensive. Perhaps
a more serious weakness is the fact that relaying on
IP addresses is not robust, as simple counter mea-
sures can fool the check users. An alternative to this
process could be an automated framework that re-
lies on the analysis of the comments to link editor
accounts, as we propose in this paper.
3 Related Work
Modern approaches to AA typically follow a text
classification framework where the classes are the
set of candidate authors. Different machine learning
algorithms have been used, including memory-based
learners (Luyckx and Daelemans, 2008a; Luyckx
and Daelemans, 2010), Support Vector Machines
(Escalante et al, 2011), and Probabilistic Context
Free Grammars (Raghavan et al, 2010).
Similarity-based approaches have also been suc-
cessfully used for AA. In this setting, the training
documents from the same author are concatenated
into a single file to generate profiles from author-
specific features. Then authorship predictions are
based on similarity scores. (Keselj et al, 2003; Sta-
matatos, 2007; Koppel et al, 2011) are examples of
successful examples of this approach.
Previous research has shown that low-level fea-
tures, such as character n-grams are very powerful
60
discriminators of writing styles. Although, enriching
the models with other types of features can boost
accuracy. In particular, stylistic features (punctuation
marks, use of emoticons, capitalization information),
syntactic information (at the part-of-speech level and
features derived from shallow parsing), and even se-
mantic features (bag-of-words) have shown to be
useful.
Because of the difficulties in finding data from
real cases, most of the published work in AA eval-
uates the different methods on data collections that
were gathered originally for other purposes. Exam-
ples of this include the Reuters Corpus (Lewis et al,
2004) that has been used for benchmarking different
approaches to AA (Stamatatos, 2008; Plakias and
Stamatatos, 2008; Escalante et al, 2011) and the
datasets used in the 2011 and 2012 authorship identi-
fication competitions from the PAN Workshop series
(Argamon and Juola, 2011; Juola, 2012). Other re-
searchers have invested efforts in creating their own
AA corpus by eliciting written samples from subjects
participating in their studies (Luyckx and Daelemans,
2008b; Goldstein-Stewart et al, 2008), or crawling
though online websites (Narayanan et al, 2012).
In contrast, in this paper we focus on data from
Wikipedia, where there is a real need to identify if
the comments submitted by what appear to be dif-
ferent users, belong to a sockpuppeteer. Data from
real world scenarios like this make solving the AA
problem an even more urgent and practical matter,
but also impose additional challenges to what is al-
ready a difficult problem. First, the texts analyzed in
the Wikipedia setting were generated by people with
the actual intention of deceiving the administrators
into believing they are indeed coming from differ-
ent people. With few exceptions (Afroz et al, 2012;
Juola and Vescovi, 2010), most of the approaches to
AA have been evaluated with data where the authors
were not making a conscious effort to deceive or dis-
guise their own identities or writeprint. Since there
has been very little research done on deception detec-
tion, it is not well understood how AA approaches
need to be adapted for these situations, or what kinds
of features must be included to cope with deceptive
writing. However, we do assume this adds a com-
plicating factor to the task, and previous research
has shown considerable decreases in AA accuracy
when deception is present (Brennan and Greenstadt,
2009). Second, the length of the documents is usu-
ally shorter for the Wikipedia comments than that of
other collections used. Document length will clearly
affect the prediction performance of AA approaches,
as the shorter documents will contain less informa-
tion to develop author writeprint models and to make
an inference on attribution. As we will describe later,
this prompted us to reframe our solution in order to
circumvent this short document length issue. Lastly,
the data available is limited, there is an average of 80
entries per user in the training set from the collection
we gathered, and an average of 8 messages in the test
set, and this as well limits the amount of evidence
available to train author models. Moreover, the test
cases have an average of 8 messages. This is a very
small amount of texts to make the final prediction.
4 Approach
In our framework, each comment made by a user is
considered a ?document? and therefore, each com-
ment represents an instance of the classification task.
There are two steps in our method. In the first step,
we gather predictions from the classifier on each com-
ment. Then in the second step we take the predictions
for each comment and combine them in a majority
voting schema to assign final decisions to each ac-
count.
The two step process we just described helps us
deal with the challenging length of the individual
comments. It is also an intuitive approach, since what
we need to determine is if the account belongs to the
sockpuppeteer. The ruling is at the account-level,
which is also consistent with the human process. In
the case of a positive prediction by our system, we
take as a confidence measure on the predictions the
percentage of comments that were individually pre-
dicted as sockpuppet cases.
4.1 Feature Engineering
In this study, we have selected typical features of
authorship attribution, as well as new features we
collected from inspecting the data by hand. In total,
we have 239 features that capture stylistic, grammati-
cal, and formatting preferences of the authors. The
features are described below.
Total number of characters: The goal of this
feature is to model the author?s behavior of writing
61
long wordy texts, or short comments.
Total number of sentences: We count the total
number of sentences in the comments. While this fea-
ture is also trying to capture some preferences regard-
ing the productivity of the author?s comments, it can
tell us more about the author?s preference to organize
the text in sentences. Some online users tend to write
in long sentences and thus end up with a smaller num-
ber of sentences. To fragment the comments into sen-
tences, we use the Lingua-EN-Sentence-0.25 from
www.cpan.org (The Comprehensive Perl Archive
Network). This off-the-shelf tool prevents abbrevia-
tions to be considered as sentence delimiters.
Total number of tokens: We define a token as
any sequence of consecutive characters with no white
spaces in between. Tokens can be words, numbers,
numbers with letters, or with punctuation, such as
apple, 2345, 15th, and wow!!!. For this feature we
just count how many tokens are in the comment.
Words without vowels: Most English words have
one or more vowels. The rate of words without vow-
els can also be a giveaway marker for some authors.
Some words without vowels are try, cry, fly, myth,
gym, and hymn.
Total alphabet count: This feature consists of
the count of all the alphabetic characters used by the
author in the text.
Total punctuation count: Some users use punctu-
ation marks in very unique ways. For instance, semi-
colons and hyphens show noticeable differences in
their use, some people avoid them completely, while
others might use them in excess. Moreover, the use
of commas is different in different parts of the world,
and that too can help identify the author.
Two/three continuous punctuation count: Se-
quences of the same punctuation mark are often used
to emphasize or to add emotion to the text, such as
wow!!!, and really??. Signaling emotion in written
text varies greatly for different authors. Not every-
one displays emotions explicitly or feels comfortable
expressing them in text. We believe this could also
help link users to sockpuppet cases.
Total contraction count: Contractions are used
for presenting combined words such as don?t, it?s,
I?m, and he?s. The contractions, or the spelled-out-
forms are both correct grammatically. Hence, the use
of contraction is somewhat a personal writing style
attribute. Although the use of contractions varies
across different genres, in social media they are com-
monly used.
Parenthesis count: This is a typical authorship at-
tribution feature that depicts the rate at which authors
use parenthesis in their comments.
All caps letter word count: This is a feature
where we counted the number of tokens having all
upper case letters. They are either abbreviations, or
words presented with emphasis. Some examples are
USA, or ?this is NOT correct?.
Emoticons count: Emoticons are pictorial rep-
resentations of feelings, especially facial expres-
sions with parenthesis, punctuation marks, and letters.
They typically express the author?s mood. Some com-
monly used emoticons are :) or :-) for happy face, :(
for sad face, ;) for winking, :D for grinning, <3 for
love/heart, :O for being surprised, and :P for being
cheeky/tongue sticking out.
Happy emoticons count: As one of the most
widely used emoticons, happy face was counted as a
specific feature. Both :) and :-) were counted towards
this feature.
Sentence count without capital letter at the be-
ginning: Some authors start sentences with numbers
or small letters. This feature captures that writing
style. An example can be ?1953 was the year, ...? or,
?big, bald, and brass - all applies to our man?.
Quotation count: This is an authorship attribu-
tion feature where usage of quotation is counted as
a feature. When quoting, not everyone uses the quo-
tation punctuation and hence quotation marks count
may help discriminate some writers from others.
Parts of speech (POS) tags frequency: We took
a total of 36 parts of speech tags from the Penn Tree-
bank POS (Marcus et al, 1993) tag set into considera-
tion. We ignored all tags related to punctuation marks
as we have other features capturing these characters.
Frequency of letters: We compute the frequency
of each of the 26 English letters in the alphabet. The
count is normalized by the total number of non-white
characters in the comment. This contributed 26 fea-
tures to the feature set.
Function words frequency: It has been widely
acknowledged that the rate of function words is a
good marker of authorship. We use a list of function
words taken from the function words in (Zheng et
al., 2006). This list contributed 150 features to the
feature set.
62
All the features described above have been used
in previous work on AA. Following are the features
that we found by manually inspecting the Wikipedia
data set. All the features involving frequency counts
are normalized by the length of the comment.
Small ?i? frequency: We found the use of small
?i? in place of capital ?I? to be common for some
authors. Interestingly, authors who made this mistake
repeated it quite often.
Full stop without white space frequency: Not
using white space after full stop was found quite
frequently, and authors repeated it regularly.
Question frequency: We found that some authors
use question marks more frequently than others. This
is an idiosyncratic feature as we found some authors
abuse the use of question marks for sentences that do
not require question marks, or use multiple question
marks where one question mark would suffice.
Sentence with small letter frequency: Some au-
thors do not start a sentence with the first letter cap-
italized. This behavior seemed to be homogeneous,
meaning an author with this habit will do it almost
always, and across all of its sockpuppet accounts.
Alpha, digit, uppercase, white space, and tab
frequency: We found that the distribution of these
special groups of characters varies from author to
author. It captures formatting preferences of text
such as the use of ?one? and ?zero? in place of ?1?
and ?0?, and uppercase letters for every word.
?A?, and an error frequency: Error with usage
of ?a?, and ?an? was quite common. Many authors
tend to use ?a? in place of ?an?, and vice versa. We
used a simple rate of all ?a? in front of words starting
with vowel, or ?an? in front of words starting with
consonant.
?he?, and ?she? frequency: Use of ?he?, or ?she?
is preferential to each author. We found that the use of
?he?, or ?she? by any specific author for an indefinite
subject is consistent across different comments.
5 Data
We collected our data from cases filed by real users
suspecting sockpupeteering in the English Wikipedia.
Our collection consists of comments made by the
accused sockpuppet and the suspected puppeteer in
various talk pages. All the information about sock-
puppet cases is freely available, together with infor-
Class Total Avg. Msg.
Train
Avg. Mesg.
Test
Sockpuppet 41 88.75 8.5
Non-sockpuppet 36 77.3 7.9
Table 1: Distribution of True/False sockpuppet cases in
the experimental data set. We show the average number
of messages in train and test partitions for both classes.
mation about the verdict from the administrators. For
the negative examples, we also collected comments
made by other editors in the comment threads of the
same talk pages. For each comment, we also col-
lected the time when the comment was posted as
an extra feature. We used this time data to investi-
gate if non-authorship features can contribute to the
performance of our model, and to compare the perfor-
mance of stylistic features and external user account
information.
Our dataset has two types of cases: confirmed
sockpuppet, and rejected sockpuppet. The confirmed
cases are those where the administrators have made fi-
nal decisions, and their verdict confirmed the case as
a true sockpuppet case. Alternatively, for the rejected
sockpuppet cases, the administrator?s verdict exoner-
ates the suspect of all accusations. The distribution
of different cases is given in Table 1.
Of the cases we have collected, one of the notable
puppeteers is ?-Inanna-?. This editor was active in
Wikipedia for a considerable amount of time, from
December 2005 to April 2006. He also has a number
of sockpuppet investigation cases against him. Ta-
ble 2 shows excerpts from comments made by this
editor on the accounts confirmed as sockpuppet. We
highlight in boldface the features that are more no-
ticeable as similar patterns between the different user
accounts.
An important aspect of our current evaluation
framework is the preprocessing of the data. We
?cleansed? the data by removing content that was
not written by the editor. The challenge we face is
that Wikipedia does not have a defined structure for
comments. We can get the difference of each modifi-
cation in the history of a comment thread. However,
not all modifications are comments. Some can be
reverts (changing content back to an old version), or
updates. Additionally, if an editor replies to more
than one part of a thread in response to multiple com-
63
Comment from the sockpuppeteer: -Inanna-
Mine was original and i have worked on it more than 4 hours.I have changed
it many times by opinions.Last one was accepted by all the users(except for
khokhoi).I have never used sockpuppets.Please dont care Khokhoi,Tombseye
and Latinus.They are changing all the articles about Turks.The most important
and famous people are on my picture.
Comment from the sockpuppet: Altau
Hello.I am trying to correct uncited numbers in Battle of Sarikamis and Crimean
War by resources but khoikhoi and tombseye always try to revert them.Could
you explain them there is no place for hatred and propagandas, please?
Comment from the others: Khoikhoi
Actually, my version WAS the original image. Ask any other user. Inanna?s
image was uploaded later, and was snuck into the page by Inanna?s sockpuppet
before the page got protected. The image has been talked about, and people
have rejected Inanna?s image (see above).
Table 2: Sample excerpt from a single sockpuppet case. We show in boldface some of the stylistic features shared
between the sockpuppeter and the sockpuppet.
System P R F A (%)
B-1 0.53 1 0.69 53.24
B-2 0.53 0.51 0.52 50.64
Our System 0.68 0.75 0.72 68.83
Table 3: Prediction performance for sockpuppet detec-
tion. Measures reported are Precision (P), Recall (R),
F-measure (F), and Accuracy (A). B-1 is a simple baseline
of the majority class and B-2 is a random baseline.
ments, or edits someone else?s comments for any
reason, there is no fixed structure to distinguish each
action. Hence, though our initial data collector tool
gathered a large volume of data, we could not use all
of it as the preprocessing step was highly involved
and required some manual intervention.
6 Experimental Setting
We used Weka (Witten and Frank, 2005) ? a widely
recognized free and open source data-mining tool, to
perform the classification. For the purpose of this
study, we chose Weka?s implementation of Support
Vector Machine (SVM) with default parameters.
To evaluate in a scenario similar to the real setting
in Wikipedia, we process each sockpuppet case sepa-
rately, we measure prediction performance, and then
aggregate the results of each case. For example, we
take data from a confirmed sockpuppet case and gen-
erate the training and test instances. The training data
comes from the comments made by the suspected
sockpuppeteer, while the test data comes from the
comments contributed by the sockpuppet account(s).
We include negative samples for these cases by col-
lecting comments made on the same talk pages by
editors not reported or suspected of sockpuppeteer-
ing. Similarly, to measure the false positive ratio of
our approach, we performed experiments with con-
firmed non-sockpuppet editors that were also filed as
potential sockpuppets in Wikipedia.
7 Results
The results of our experiments are shown in Table 3.
For comparison purposes we show results of two
simple baseline systems. B-1 is the trivial classifier
that predicts every case as sockpuppet (majority). B-
2 is the random baseline (coin toss). However as seen
in the table, both baseline systems are outperformed
by our system that reached an accuracy of 68%. B-1
reached an accuracy of 53% and B-2 of 50%.
For the miss-classified instances of confirmed
sockpuppet cases, we went back to the original com-
ment thread and the investigation pages to find out
the sources of erroneous predictions for our system.
We found investigation remarks for 4 cases. Of these
4 cases, 2 cases were tied on the predictions for the
individual comments. We flip a coin in our system
to break ties. From the other 2 cases, one has the
neutral comment from administrators: ?Possible?,
which indicates some level of uncertainty. The last
one has comments that indicate a meat puppet. A
meat puppet case involves two different real people
64
where one is acting under the influence of the other.
A reasonable way of taking advantage of the current
system is to use the confidence measure to make pre-
dictions of the cases where our system has the highest
confidence, or higher than some threshold, and let
the administrators handle those cases that are more
difficult for an automated approach.
We have also conducted an experiment to rank our
feature set with the goal of identifying informative
features. We used information gain as the ranking
metric. A snapshot of the top 30 contributing fea-
tures according to information gain is given in Ta-
ble 4. We can see from the ranking that some of the
top-contributing features are idiosyncratic features.
Such features are white space frequency, beginning
of the sentence without capital letter, and no white
space between sentences. We can also infer from
Table 4 that function word features (My, me, its, that,
the, I, some, be, have, and since), and part of speech
tags (VBG-Verb:gerund or present participle, CD-
Cardinal number, VBP-Verb:non-3rd person singular
present, NNP-Singular proper noun, MD-Modal, and
RB-Adverb) are among the most highly ranked fea-
tures. Function words have been identified as highly
discriminative features since the earliest work on au-
thorship attribution.
Finally, we conducted experiments with two edit
timing features for 49 cases. These two features are
edit time of the day in a 24 hour clock, and edit
day of the week. We were interested in exploring if
adding these non-stylistic features could contribute
to classification performance. To compare perfor-
mance of these non-authorship attribution features,
we conducted the same experiments without these
features. The results are shown in Table 5. We can
see that average confidence of the classification, as
well as F-measure goes up with the timing features.
These timing features are easy to extract automati-
cally, therefore they should be included in an auto-
mated approach like the one we propose here.
8 Discussion
The experiments presented in the previous section are
encouraging. They show that with a relatively small
set of automatically generated features, a machine
learning algorithm can identify, with a reasonable per-
formance, the true cases of sockpuppets in Wikipedia.
Features
Whitespace frequency
Punctuation count
Alphabet count
Contraction count
Uppercase letter frequency
Total characters
Number of tokens
me
my
its
that
Beginning of the sentence without capital letter ?
VBG-Verb:gerund or present participle
No white space between sentences ?
the
Frequency of L
I
CD-Cardinal number
Frequency of F
VBP-Verb:non-3rd person singular present
Sentence start with small letter ?
some
NNP-Singular proper noun
be
Total Sentences
MD-Modal
? mark frequency
have
since
RB-Adverb
Table 4: Ranking of the top 30 contributing features for the
experimental data using information gain. Novel features
from our experiment are denoted by ?.
Features used Confidence F-measure
All + timing features 84.04% 0.72
All - timing features 78.78% 0.69
Table 5: Experimental result showing performance of the
method with and without timing features for the problem
of detecting sockpuppet cases. These results are on a
subset of 49 cases.
65
72 74 76 78 80 82 84 86Confidence in %
0.65
0.66
0.67
0.68
0.69
0.70
0.71
0.72
0.73
F-m
ea
su
re
a
b
c
d
e
f
g
Figure 1: A plot of confidence in % for successful cases vs. F-measure for the system where we remove one feature
group at a time. Here marker a) represents performance of the system with all the features. Markers b) timing features, c)
part of speech tags, d) idiosyncratic features, e) function words, f) character frequencies, and g) AA features, represent
performance of the system when the specified feature group is removed.
Since falsely accusing someone of using a sockpup-
pet could lead to serious credibility loss for users,
we believe a system like ours could be used as a first
pass in resolving the suspected sockpuppet cases, and
bring into the loop the administrators for those cases
where the certainty is not high.
To further investigate the contribution of different
groups of features in our feature set, we conducted
additional experiments where we remove one feature
group at a time. Our goal is to see which feature
group causes larger decreases in prediction perfor-
mance when it is not used in the classification. We
split our feature set into six groups, namely timing
features, parts of speech tags, idiosyncratic features,
function words, character frequencies, and author-
ship attribution features. In Figure 1, we show the
result of the experiments. From the figure, we ob-
serve that function words are the most influential
features as both confidence, and F-measure showed
the largest drop when this group was excluded. The
idiosyncratic features that we have included in the
feature set showed the second largest decrease in pre-
diction performance. Timing features, and part of
speech tags have similar drops in F-measure but they
showed a different degradation pattern on the con-
fidence: part of speech tags caused the confidence
to decrease by a larger margin than the timing fea-
tures. Finally, character frequencies, and authorship
attribution features did not affect F-measure much,
but the confidence from the predictions did decrease
considerably with AA features showing the second
largest drop in confidence overall.
9 Conclusion and Future Directions
In this paper, we present a first attempt to develop an
automated detection method of sockpuppets based
solely on the publicly available comments from the
suspected users. Sockpuppets have been a bane for
Wikipedia as they are widely used by malicious users
to subvert Wikipedia?s editorial process and consen-
sus. Our tool was inspired by recent work on the
popular field of authorship attribution. It requires no
additional administrative rights (e.g., the ability to
view user IP addresses) and therefore can be used
by regular users or administrators without check user
rights. Our experimental evaluation with real sock-
66
puppet cases from the English Wikipedia shows that
our tool is a promising solution to the problem.
We are currently working on extending this study
and improving our results. Specific aspects we would
like to improve include a more robust confidence
measure and a completely automated implementation.
We are aiming to test our system on all the cases filed
in the history of the English Wikipedia. Later on, it
would be ideal to have a system like this running in
the background and pro-actively scanning all active
editors in Wikipedia, instead of running in a user
triggered mode. Another useful extension would
be to include other languages, as English is only
one of the many languages currently represented in
Wikipedia.
Acknowledgements
This research was supported in part by ONR grant
N00014-12-1-0217. The authors would like to thank
the anonymous reviewers for their comments on a
previous version of this paper.
References
S. Afroz, M. Brennan, and R. Greenstadt. 2012. Detecting
hoaxes, frauds, and deception in writing style online. In
Proceedings of the 2012 IEEE Symposium on Security
and Privacy (S&P), pages 461?475. IEEE, May.
Shlomo Argamon and Patrick Juola. 2011. Overview of
the international authorship identification competition
at PAN-2011. In Proceedings of the PAN 2011 Lab Un-
covering Plagiarism, Authorship, and Social Software
Misuse, held in conjunction with the CLEF 2011 Con-
ference on Multilingual and Multimodal Information
Access Evaluation, Amsterdam.
M. Brennan and R. Greenstadt. 2009. Practical attacks
against authorship recognition techniques. In Proceed-
ings of the Twenty-First Innovative Applications of Ar-
tificial Intelligence Conference.
John Douceur. 2002. The Sybil attack. In Peter Dr-
uschel, Frans Kaashoek, and Antony Rowstron, editors,
Peer-to-Peer Systems, volume 2429 of Lecture Notes
in Computer Science, pages 251?260. Springer Berlin /
Heidelberg.
H. J. Escalante, T. Solorio, and M. Montes-y Go?mez.
2011. Local histograms of character n-grams for au-
thorship attribution. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 288?298,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Jade Goldstein-Stewart, Kerri A. Goodwin, Roberta Evans
Sabin, and Ransom K. Winder. 2008. Creating and
using a correlated corpus to glean communicative com-
monalities. In Proceedings of LREC-2008, the Sixth
International Language Resources and Evaluation Con-
ference.
P. Juola and D. Vescovi. 2010. Empirical evaluation of
authorship obfuscation using JGAAP. In Proceedings
of the 3rd ACM workshop on Artificial Intelligence and
Security, pages 14?18. ACM.
Patrick Juola. 2012. An overview of the traditional author-
ship attribution subtask. In PAN 2012 Lab, Uncovering
Plagiarism, Authorship and Social Software Misuse,
held in conjunction with CLEF 2012.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003.
N-gram based author profiles for authorship attribution.
In Proceedings of the Pacific Association for Computa-
tional Linguistics, pages 255?264.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011. Authorship attribution in the wild. Language
Resources and Evaluation, 45:83?94.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for text
categorization research. J. Mach. Learn. Res., 5:361?
397, December.
Kim Luyckx and Walter Daelemans. 2008a. Authorship
attribution and verification with many authors and lim-
ited data. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 513?520, Manchester, UK, August.
Kim Luyckx and Walter Daelemans. 2008b. Personae: a
corpus for author and personality prediction from text.
In Proceedings of LREC-2008, the Sixth International
Language Resources and Evaluation Conference.
Kim Luyckx and Walter Daelemans. 2010. The effect
of author set size and data size in authorship attribu-
tion. Literary and Linguistic Computing, pages 1?21,
August.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Comput. Linguist.,
19(2):313?330, June.
A. Narayanan, H. Paskov, N.Z. Gong, J. Bethencourt,
E. Stefanov, E.C.R. Shin, and D. Song. 2012. On the
feasibility of internet-scale author identification. In
Proceedings of the 33rd conference on IEEE Sympo-
sium on Security and Privacy, pages 300?314. IEEE.
S. Plakias and E. Stamatatos. 2008. Tensor space models
for authorship attribution. In Proceedings of the 5th
Hellenic Conference on Artificial Intelligence: Theo-
ries, Models and Applications, volume 5138 of LNCS,
pages 239?249, Syros, Greece.
67
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
48th Annual Meeting of the ACL 2010, pages 38?42,
Uppsala, Sweden, July. Association for Computational
Linguistics.
E. Stamatatos. 2007. Author identification using imbal-
anced and limited training texts. In Proceedings of the
18th International Workshop on Database and Expert
Systems Applications, DEXA ?07, pages 237?241, Sept.
E. Stamatatos. 2008. Author identification: Using text
sampling to handle the class imbalance problem. Infor-
mation Processing and Managemement, 44:790?799.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques. Morgan
Kauffmann, 2nd edition.
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang.
2006. A framework for authorship identification of
online messages: Writing-style features and classifica-
tion techniques. Journal of the American Society for
Information Science and Technology, 57(3):378?393.
68
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 224?231,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Native Language Identification: a Simple n-gram Based Approach
Binod Gyawali and Gabriela Ramirez and Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, Alabama, USA
{bgyawali,gabyrr,solorio}@cis.uab.edu
Abstract
This paper describes our approaches to Na-
tive Language Identification (NLI) for the NLI
shared task 2013. NLI as a sub area of au-
thor profiling focuses on identifying the first
language of an author given a text in his sec-
ond language. Researchers have reported sev-
eral sets of features that have achieved rel-
atively good performance in this task. The
type of features used in such works are: lex-
ical, syntactic and stylistic features, depen-
dency parsers, psycholinguistic features and
grammatical errors. In our approaches, we se-
lected lexical and syntactic features based on
n-grams of characters, words, Penn TreeBank
(PTB) and Universal Parts Of Speech (POS)
tagsets, and perplexity values of character of
n-grams to build four different models. We
also combine all the four models using an en-
semble based approach to get the final result.
We evaluated our approach over a set of 11 na-
tive languages reaching 75% accuracy.
1 Introduction
Recently, a growing number of applications are tak-
ing advantage of author profiling to improve their
services. For instance, in security applications (Ab-
basi and Chen, 2005; Estival et al, 2007) to help
limit the search space of, for example, the author of
an email threat, or in marketing where the demog-
raphy information about customers is important to
predict behaviors or to develop new products.
Particularly, author profiling is a task of identi-
fying several demographic characteristics of an au-
thor from a written text. Demographic groups can be
identified by age, gender, geographic origin, level of
education and native language. The idea of identi-
fying the native language based on the manner of
speaking and writing a second language is borrowed
from Second Language Acquisition (SLA), where
this is known as language transfer. The theory of
language transfer says that the first language (L1)
influences the way that a second language (L2) is
learned (Ahn, 2011; Tsur and Rappoport, 2007).
According to this theory, if we learn to identify what
is being transfered from one language to another,
then it is possible to identify the native language of
an author given a text written in L2. For instance,
a Korean native speaker can be identified by the er-
rors in the use of articles a and the in his English
writings due to the lack of similar function words in
Korean. As we see, error identification is very com-
mon in automatic approaches, however, a previous
analysis and understanding of linguistic markers are
often required in such approaches.
In this paper we investigate if it is possible to build
native language classifiers that are not based on the
analysis of common grammatical errors or in deeper
semantic analysis. On the contrary, we want to find
a simple set of features related to n-grams of words,
characters, and POS tags that can be used in an ef-
fective way. To the best of our knowledge, almost
all the works related to L1 identification use fine
grained POS tags, but do not look into whether a
coarse grained POS tagset could help in their work.
Here, we explore the use of coarse grained Univer-
sal POS tags with 12 POS categories in the NLI task
and compare the result with the fine grained Penn
TreeBank (PTB) POS tags with 36 POS categories.
224
Moreover, we also investigate how the system works
when perplexity values are used as features in iden-
tifying native languages. Using an ensemble based
approach that combines four different models built
by various combinations of feature sets of n-grams
of words, characters, and POS tags, and perplexity
values, we identify the native language of the author,
over 11 different languages, with an accuracy close
to 80% and 75% in development and test dataset re-
spectively.
2 Related Work
The first known work about native language identifi-
cation appears in 2005 (Koppel et al, 2005). In their
study, the authors experimented with three types of
features, i.e. function words, letter n-grams, er-
rors and idiosyncrasies. But their analysis was fo-
cused on the identification of common errors. They
found that using a combination of all the features in
a Support Vector Machine (SVM), they can obtain
an accuracy of 80% in the classification of 5 differ-
ent native languages. As in this first study, analyz-
ing errors is common in native language identifica-
tion methods, since it is a straightforward adapta-
tion of how this task is performed in SLA. For in-
stance, Wong and Dras (2009) investigate the use
of error types such as disagreement on subject-verb
and noun-number, as well as misuse of determin-
ers to show that error analysis is helpful in this task.
But their results could not outperform the results ob-
tained by Koppel et al (2005). They also suggested
that analyzing other types of errors might help to im-
prove their approach. In the same path, Jarvis et al
(2012) investigate a larger variety of errors, for ex-
ample lexical words and phrase errors, determiner
errors, spelling errors, adjective order errors and er-
rors in the use of punctuation marks, among others.
But they also could not achieve results comparable
to the previous results in this task.
Since language transfer occurs when grammati-
cal structures from a first language determine the
grammatical structures of a second language, the in-
clusion of function words and dependency parsers
as features seem to be helpful to find such trans-
fers as well as error types (Tetreault et al, 2012;
Brooke and Hirst, 2011; Wong et al, 2012). It
is common that the analysis of the structure of
certain grammatical patterns is also informative to
find the use or misuse of well-established gram-
matical structures (e.g. to distinguish between the
use of verb-subject-object, subject-verb-object, and
subject-object-verb), in such cases n-grams of POS
tags can be used. Finally, according to Tsur and
Rappoport (2007), the transfer of phonemes is use-
ful in identifying the native language. Even though
the phonemes are usually speech features, the au-
thors suggest that this transfer can be captured by
the use of character n-grams in the text. Character
n-grams have been proved to be a good feature in
author profiling as well since they also capture hints
of style, lexical information, use of punctuation and
capitalization.
In sum, there are varieties of feature types used
in native language identification, most of them com-
bine three to nine types. Each type aims to capture
specific information such as lexical and syntactic in-
formation, structural information, idiosyncrasies, or
errors.
3 Shared Task Description
The Native Language Identification (NLI) shared
task focuses on identifying the L1 of an author based
on his writing in a second language. In this case,
the second language is English. The shared task had
three sub-tasks: one closed training and two open
training. The details about the tasks are described
by Tetreault et al (2013). For each subtask, the par-
ticipants were allowed to submit up to five runs. We
participated in the closed training sub-task and sub-
mitted five runs.
The data sets provided for the shared task were
generated from the TOEFL corpus (Blanchard et al,
2013) that contains 12, 100 English essays. The
corpus comprised 11 native languages (L1s): Ara-
bic (ARA), Chinese (CHI), French (FRE), German
(GER), Hindi (HIN), Italian (ITA), Japanese (JPN),
Korean (KOR), Spanish (SPA), Telugu (TEL), and
Turkish (TUR), each containing 1100 essays. The
corpus was divided into training, development, and
test datasets with 9900, 1100, and 1100 essays re-
spectively. Each L1 contained an equal number of
essays in each dataset.
225
Feature Sets N-grams
Error rates for top k features
500 800 1000 3000 6000
Character n-grams
2 grams 78.27 77.64 77.18 75.82 -
3 grams 78.55 60.55 64.27 43.73 44.36
Word n-grams
2 grams 66.55 58.36 55.64 44.91 38.73
3 grams 75.55 69.18 76.36 67.09 54.18
PTB POS n-grams
2 grams 69.73 76.73 69.55 72.09 -
3 grams 72.82 72.45 67.27 56.18 62.27
Universal POS n-grams
2 grams 85.36 - - - -
3 grams 78.1818 79.55 72.36 85.27 -
Table 1: Error rates in L1 identification using various feature sets with different number of features
4 General System Description
In this paper we describe two sets of experiments.
We performed a first set of experiments to evaluate
the accuracy of different sets of features in order to
find the best selection. This set was also intended to
determine the threshold of the number of top fea-
tures in each set needed to obtain a good perfor-
mance in the classification task. These experiments
are described in Section 5.
In the second set, we performed five different ex-
periments for five runs. Four of the five models
used different combinations of feature sets to train
the classifier. The major goal of these experiments
was to find out how good the results achieved can
be by using lower level lexical and shallow syntactic
features. We also compared the accuracy obtained
by using the fine grained POS tags and the coarse
grained POS tags. In one of these experiments, we
used perplexity values as features to see how effec-
tive these features can be in NLI tasks. Finally, the
fifth experiment was an ensemble based approach
where we applied a voting scheme to the predictions
of the four approaches to get the final result. The de-
tails of these experiments are described in Section 6.
In our experiments, we trained the classifier using
the training dataset, and using the model we tested
the accuracy on the development and test dataset.
We used an SVM multiclass classifier (Crammer and
Singer, 2002) with default parameter settings for the
machine learning tasks. We used character n-grams,
word n-grams, Parts of Speech (POS) tag n-grams,
and perplexity of character trigrams as features. For
all the features except perplexity, we used a TF-IDF
weighting scheme. To reduce the number of fea-
tures, we selected only the top k features based on
the document frequency in the training data.
The provided dataset contained all the sentences
in the essays tokenized by using ETS?s proprietary
tokenizers. For the POS tags based features, we
used two tagsets: Penn TreeBank (PTB) and Uni-
versal POS tags. For PTB POS tags, we tagged the
text with the Stanford parser (Klein and Manning,
2003). In order to tag the sentences with Universal
POS tags, we mapped the PTB POS tags to universal
POS tags using the mapping described by Petrov et
al. (2011).
We also used perplexity values from language
models in our experiments. To generate the lan-
guage models and compute perplexity, we used the
SRILM toolkit (Stolcke et al, 2011). We used train-
ing data to generate the language models and train
the classifier. Finally, all the sentences were con-
verted into lower case before finding the word and
character n-grams.
5 Feature Sets Evaluation
We performed a series of experiments using a sin-
gle feature set per experiment in order to find the
best combinations of features to use in classification
models. All of the feature sets were based on n-
grams. We ranked the n-grams by their frequencies
on the training set and then used the development set
to find out the best top k features in the training set.
We used the values of k as 500, 800, 1000, 3000,
and 6000 for this set of experiments. The error rates
of these experiments are shown in Table 1. Since the
total number of features in character bigrams, PTB
226
Exp-W2,3PTB3C3 Exp-W2,3Univ3C3 Exp ClassBased Exp Perplexity Exp Ensemble
L1 P R F1 P R F1 P R F1 P R F1 P R F1
ARA 90.7 68.0 77.7 87.1 54.0 66.7 72.2 70.0 71.1 70.8 51.0 59.3 90.9 70.0 79.1
CHI 79.0 83.0 81.0 57.9 84.0 68.6 75.0 78.0 76.5 71.7 66.0 68.8 78.4 87.0 82.5
FRE 91.5 75.0 82.4 75.7 81.0 78.3 92.8 64.0 75.7 71.2 74.0 72.5 90.8 79.0 84.5
GRE 86.0 92.0 88.9 77.5 86.0 81.5 84.2 85.0 84.6 63.8 83.0 72.2 88.3 91.0 89.7
HIN 67.3 66.0 66.7 70.0 63.0 66.3 66.3 63.0 64.6 52.3 45.0 48.4 70.2 66.0 68.0
ITA 72.3 94.0 81.7 76.9 83.0 79.8 66.4 89.0 76.1 65.3 77.0 70.6 74.6 94.0 83.2
JPN 86.6 71.0 78.0 76.0 76.0 76.0 64.3 81.0 71.7 51.7 60.0 55.6 85.2 75.0 79.8
KOR 78.3 83.0 80.6 65.0 80.0 71.7 68.1 64.0 66.0 55.1 49.0 51.9 78.8 82.0 80.4
SPA 72.3 68.0 70.1 90.9 50.0 64.5 65.4 68.0 66.7 58.5 38.0 46.1 74.5 70.0 72.2
TEL 68.4 80.0 73.7 66.9 83.0 74.1 68.2 75.0 71.4 53.4 71.0 60.9 69.2 81.0 74.7
TUR 77.9 81.0 79.4 84.0 63.0 72.0 83.3 55.0 66.3 69.5 66.0 67.7 81.8 81.0 81.4
Overall 78.3 73.0 72.0 61.8 79.6
Table 2: L1 identification accuracy in development data
POS bigrams, Universal POS bigrams, and Univer-
sal POS trigrams were 1275, 1386, 144, and 1602
respectively, some fields in the table are blank.
A trivial baseline for this task is to classify all the
instances to a single class, which gives 9.09% ac-
curacy. The table above shows that the results ob-
tained in all cases is better than the baseline. In five
cases, better results were obtained when using the
top 3000 or 6000 features compared to other feature
counts. In the case of the character trigram feature
set, though the result using top 3000 features is bet-
ter than the others, the difference is very small com-
pared to the experiment using top 6000 features. The
accuracy obtained by using top 3000 features in PTB
POS tags is 6% higher than that with top 6000 fea-
tures. In case of Universal POS tags trigrams, better
results were obtained with top 1000 features.
Results show that bigram and trigram feature sets
of words give higher accuracy compared to bigrams
and trigrams of characters and POS tags. Comparing
the results of n-grams of two different POS tagsets,
the results obtained when using the PTB tagset are
better than those when using the Universal tagsets.
In the case of character, PTB POS tag, and Univer-
sal POS tag bigram feature sets, the overall accu-
racy is less than 30%. Based on these results, we de-
cided to use the following sets of features: trigrams
of characters and POS tags (PTB and Universal) and
bigrams of words in our experiments below.
6 Final Evaluation
We submitted five runs for the task based on five
classifiers. We named the experiments based on the
features used and the approaches used for feature se-
lection. Details about the experiments and their re-
sults are described below.
1. Exp-W2,3PTB3C3: In this experiment, we
used bigrams at the word level, and trigrams at
the word, character level, as well as PTB POS
tag trigrams as feature sets. We selected these
feature sets based on the accuracies obtained
in the experiments described in Section 5. We
tried to use a consistent number of features in
each feature set. As seen in Table 1, though
the results obtained by using top 3000 and 6000
features are better in equal number of cases (2
and 2), the difference in accuracies when us-
ing 6000 features is higher than that when us-
ing 3000 features. Thus, we decided to use the
top 6000 features in all the four feature sets.
2. Exp-W2,3Univ3C3: The PTB POS tagset con-
tains 36 fine grained POS categories while the
Universal POS tagset contains only 12 coarse
POS categories. In the second experiment, we
tried to see how the performance changes when
using coarse grained Universal POS categories
instead of fine grained PTB POS tags. Thus,
we performed the second experiment with the
same settings as the first experiment except we
used Universal POS tags instead of PTB POS
tags. Since the total number of Universal POS
227
Exp-W2,3PTB3C3 Exp-W2,3Univ3C3 Exp ClassBased Exp Perplexity Exp Ensemble
L1 P R F1 P R F1 P R F1 P R F1 P R F1
ARA 74.3 55.0 63.2 90.9 50.0 64.5 67.9 74.0 70.8 54.3 44.0 48.6 79.7 63.0 70.4
CHI 76.2 80.0 78.0 65.9 81.0 72.6 74.5 73.0 73.7 69.3 61.0 64.9 80.2 81.0 80.6
FRE 86.4 70.0 77.3 75.8 75.0 75.4 90.6 58.0 70.7 54.5 54.0 54.3 85.7 72.0 78.3
GRE 83.2 89.0 86.0 79.1 91.0 84.7 82.7 86.0 84.3 65.2 86.0 74.1 87.6 92.0 89.8
HIN 63.7 65.0 64.4 64.5 69.0 66.7 59.6 56.0 57.7 60.0 54.0 56.8 67.0 67.0 67.0
ITA 62.5 90.0 73.8 70.0 84.0 76.4 61.4 86.0 71.7 52.5 64.0 57.7 62.5 90.0 73.8
JPN 85.7 72.0 78.3 67.2 78.0 72.2 62.1 87.0 72.5 52.6 50.0 51.3 81.9 77.0 79.4
KOR 75.0 75.0 75.0 60.3 73.0 66.1 68.1 62.0 64.9 52.6 50.0 51.3 72.8 75.0 73.9
SPA 60.0 57.0 58.5 81.1 43.0 56.2 57.6 57.0 57.3 55.6 45.0 49.7 67.1 57.0 61.6
TEL 75.3 67.0 70.9 70.0 77.0 73.3 71.7 71.0 71.4 66.1 74.0 69.8 73.0 73.0 73.0
TUR 66.4 79.0 72.1 79.0 64.0 70.7 80.6 50.0 61.7 61.4 51.0 55.7 72.4 76.0 74.1
Accuracy 72.6 71.4 69.1 58.6 74.8
Table 3: L1 identification accuracy in test data
trigrams was only 1602, we replaced 6000 PTB
POS trigrams with 1602 Universal POS tri-
grams.
3. Exp ClassBased: The difference in this exper-
iment from the first one lies in the process of
feature selection. Instead of selecting the top k
features from the whole training data, the se-
lection was done considering the top m fea-
tures for each L1 class present in the training
dataset, i.e., we first selected the top m features
from each L1 class and combined them for a
total of p where p is greater than or equal to
m and k. After a number of experiments per-
formed with different combinations of features
to train the classifier and testing on the develop-
ment dataset, we obtained the best result using
character trigrams, PTB POS tag bigrams and
trigrams, and word bigrams feature sets with
3000, 1000, 1000, and 6000 features from each
L1 respectively. This makes the total number
of features in character trigrams, POS tag bi-
grams, POS tag trigrams, and word bigrams as
3781, 1278, 1475, and 15592 respectively.
4. Exp Perplexity: In this experiment, we used
the perplexity values as the features that were
computed from character trigram language
models. Language models define the proba-
bility distribution of a sequence of tokens in
a given text. We used perplexity values since
these have been successfully used in some au-
thorship attribution tasks (Sapkota et al, 2013).
5. Exp Ensemble: In the fifth experiment, we
used an ensemble based approach with our
above mentioned four different models. We
allowed each of the four models to have two
votes. The first vote is a weighted voting
schema in which the models were ranked ac-
cording to their results in the development
dataset and the weight for each model was
given by wc = 1/rank(c), where rank(c) is
the position of c in the ranked list. The final
output was based on the second vote that used
a majority voting schema. In the second vote,
the output of the first voting schema was also
used along with the output of four models.
The results obtained by the above mentioned five
experiments on the development and test datasets are
shown in Tables 2 and 3 respectively. The tables
show that the results obtained in the development
dataset are better than those in the test dataset for
all the approaches. In both datasets, we achieved the
best results using the ensemble based approach, i.e.
79.2% and 74.8% accuracies in the development and
test dataset respectively. Considering the accuracies
of individual L1s, this approach achieved the high-
est accuracy in 10 L1s in the development dataset
and in 7 L1s in the test dataset. Our system has the
best accuracy for German in both development and
test dataset. The other classes with higher accura-
cies in both datasets are French and Chinese. In both
datasets, our system had the lowest accuracy for the
Hindi and Spanish classes. Arabic and Telugu have
228
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
ARA 63 2 1 0 6 8 1 5 6 4 4
CHI 2 81 0 1 2 1 5 4 0 0 4
FRE 2 0 72 7 1 11 0 0 4 0 3
GER 0 2 2 92 1 1 0 0 1 0 1
HIN 2 2 0 0 67 2 0 2 3 19 3
ITA 0 0 2 2 0 90 0 0 3 0 3
JPN 3 3 1 1 0 3 77 9 1 1 1
KOR 1 7 1 0 0 0 8 75 4 1 3
SPA 1 1 3 0 2 25 1 4 57 0 6
TEL 1 0 0 0 21 0 1 0 3 73 1
TUR 4 3 2 2 0 3 1 4 3 2 76
Table 4: Confusion Matrix
3rd and 4th lowest accuracies.
Besides the ensemble based approach, the sec-
ond best result was obtained by the first experiment
(Exp W2,3PTB3C3). Comparing the overall accura-
cies of the first and second (Exp-W2,3Univ3C3) ex-
periments, though the difference between them does
not seem very high in the test dataset, there is a dif-
ference of more than 5% in the development dataset.
In the test dataset, the second experiment has the
best results among all the approaches for classes
Italian and Telugu, and has better results than the
first experiment for classes Arabic and Hindi. The
difference in the approaches used in the first and sec-
ond experiments was the use of n-grams of different
POS tagsets. The use of coarse grained Universal
POS tagset features generalizes the information and
loses the discriminating features that the fine grained
PTB POS tagset features captures. For instance, the
PTB POS tagset differentiates verbs into six cate-
gories while the Universal POS tagset has only one
category for that grammatical class. Because of this,
the fine grained POS tagset seems better for identify-
ing the native languages than using a coarse grained
POS tagset in most of the cases. More studies are
needed to analyze the cases where Universal POS
tagset works better than the fine grained PTB POS
tagset.
The difference in accuracies obtained between the
first experiment (Exp W2,3PTB3C3) and the third
experiment (Exp ClassBased) is more than 6% in
the development dataset and more than 3% in the test
dataset. In the test dataset, the third experiment has
the highest accuracy for Arabic class and has better
accuracy than the first experiment for Telugu class.
The difference between these approaches was the
feature selection approach used to create the feature
vector. The results show that in most of the cases se-
lecting the features from the whole dataset achieves
better accuracy in identifying native languages com-
pared to using the stratified approach of selecting the
features from individual classes. The main reason
behind using the class based feature selection was
that we tried to capture some features that are specif-
ically present in one class and not in others. Since all
the texts in our dataset were about one of the eight
prompts, and we have a balanced dataset, there was
no benefit of doing the class based feature selection
approach.
The fourth experiment (Exp Perplexity) using
perplexity values as features did not achieve accu-
racy comparable to the first three experiments. Be-
cause of the time constraint, we calculated perplex-
ity based on only character trigram language mod-
els. Though the result we achieved is not promis-
ing, this approach could be an interesting work in fu-
ture experiments where we could use other language
models or the combination of various language mod-
els to compute the perplexity.
7 Error Analysis
The confusion matrix of the results obtained in the
test dataset by using the ensemble based approach
is shown in Table 4. The table shows the German
class has the best accuracy with only a small number
of texts of German mispredicted to other languages,
while 7 texts of French class are mispredicted as
German. The German language is rich in morpohol-
ogy and shares a common ancestor with English. It
also has a different grammatical structure from the
229
other languages in the task. The features we used
in our experiments are shallow syntactic and lexical
features, which could discriminate the writing styles
and the structure of the German class texts, thus hav-
ing a higher prediction accuracy.
The table shows that French, Italian, and Spanish
classes seem to be confused with each other. Though
the misclassification rate of texts in the Italian class
is considerably low, a good number of texts in the
French and Spanish classes are misclassified as Ital-
ian. The highest number of documents mispredicted
is from Spanish to Italian, i.e. 25 texts of Span-
ish class are mispredicted as Italian. These three
languages fall under the same language family i.e.
Indo-European/Romance and have a similar gram-
matical features. The grammatical structure is a par-
ticular example of the high rate of misclassification
among these classes. While English language is very
strict in the order of words (Subject-Verb-Object),
Spanish, Italian and French allow more flexibility.
For instance, in Spanish, the phrases ?the car red?
(el auto rojo) and ?the red car? (el rojo auto) are
both correct although the later is a much less com-
mon construction. In this scenario, it is easy to see
that the n-grams of words and POS tags are benefi-
cial to distinguish them from English, but these n-
grams might be confusing to identify the differences
among these three languages since the patterns of
language transfer might be similar.
Though Hindi and Telugu languages do not fall
under the same language family, they are highly con-
fused with each other. After Spanish to Italian, the
second highest number of misclassified texts is from
Telugu to Hindi. Similarly, 19 texts from the class
Hindi are mispredicted as Telugu. Both of these lan-
guages are spoken in India. Hindi is the National
and official language of India, while Telugu is an of-
ficial language in some states of India. Moreover,
English is also one of the official languages. So, it
is very likely that the speakers are exposed to the
same English dialect and therefore their language
transfer patterns might be very similar. This might
have caused our approach of lexical and syntactic
features to be unable to capture enough information
to identify the differences between the texts of these
classes.
Texts from Arabic class are equally misclassified
to almost all the other classes, while misclassifica-
tion to Arabic do not seem that high. Texts of the
Japanese, Korean, Chinese classes seem to be con-
fused with each other, but the confusion does not
seem very high thus having a good accuracy rate.
8 Conclusion and Future Work
In this paper, we describe our approaches to Na-
tive Language identification for the NLI Shared Task
2013. We present four different models for L1 iden-
tification, three of them using various combinations
of n-gram features at the word, character and POS
tag levels and a fourth one using perplexity values as
features. Results show that all these approaches give
a good accuracy in L1 identification. We achieved
the best result among these by using the combina-
tion of character, words, and PTB POS tags. Fi-
nally, we applied an ensemble based approach over
the results of the four different models that gave the
highest overall accuracy of 79.6% and 74.8% in the
development and test dataset respectively.
In our approaches, we use simple n-grams and do
not consider grammatical errors in L1 identification.
We would like to expand our approach by using the
errors such as misspelled words and subject-verb,
and noun-number disagreements as features. More-
over, in our current work of using perplexity values,
the result seems good but is not promising. In this
approach, we used the perplexity values based on
only character trigram language models. We would
like to incorporate other word and character n-gram
language models to calculate perplexity values in
our future work.
Acknowledgements
We would like to thank the organizers of NLI shared
task 2013. We would also like to thank CONACyT
for its partial support of this work under scholarship
310473.
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying
authorship analysis to Arabic web content. In Pro-
ceedings of the 2005 IEEE international conference
on Intelligence and Security Informatics, ISI?05, pages
183?197, Berlin, Heidelberg. Springer-Verlag.
Charles S. Ahn. 2011. Automatically Detecting Authors?
230
Native Language. Master?s thesis, Naval Postgraduate
School, Monterey, CA.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Koby Crammer and Yoram Singer. 2002. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Scott Jarvis, Yves Bestgen, Scott A. Crossley, Syl-
viane Granger, Magali Paquot, Jennifer Thewissen,
and Danielle McNamara. 2012. The Comparative
and Combined Contributions of n-Grams, Coh-Metrix
Indices and Error Types in the L1 Classification of
Learner Texts. In Scott Jarvis and Scott A. Crosley,
editors, Approaching Language Transfer through Text
Classification, pages 154?177. Multilingual Matters.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Upendra Sapkota, Thamar Solorio, Manuel Montes-y
Go?mez, and Paolo Rosso. 2013. The use of orthogo-
nal similarity relations in the prediction of authorship.
In Computational Linguistics and Intelligent Text Pro-
cessing, pages 463?475. Springer.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, pages 9?16, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
231
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 89?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploring word class n-grams to measure
language development in children
Gabriela Ram??rez de la Rosa and Thamar Solorio
University of Alabama at Birmingham
Birmingham, AL 35294, USA
gabyrr,solorio@cis.uab.edu
Manuel Montes-y-Go?mez
INAOE
Sta. Maria Tonantzintla, Puebla, Mexico
mmontesg@ccc.inaoep.mx
Yang Liu
The University of Texas at Dallas
Richardson, TX 75080, USA
yangl@hlt.utdallas.edu
Aquiles Iglesias
Temple University
Philadelphia, PA 19140, USA
iglesias@temple.edu
Lisa Bedore and Elizabeth Pen?a
The University of Texas at Austin
Austin, TX 78712, USA
lbedore,lizp@mail.utexas.edu
Abstract
We present a set of new measures designed
to reveal latent information of language
use in children at the lexico-syntactic
level. We used these metrics to analyze
linguistic patterns in spontaneous narra-
tives from children developing typically
and children identified as having a lan-
guage impairment. We observed signif-
icant differences in the z-scores of both
populations for most of the metrics. These
findings suggest we can use these metrics
to aid in the task of language assessment
in children.
1 Introduction
The analysis of spontaneous language samples is
an important task across a variety of fields. For in-
stance, in language assessment this task can help
to extract information regarding language profi-
ciency (e.g. is the child typically developing or
language impaired). In second language acqui-
sition, language samples can help determine if
a child?s proficiency is similar to that of native
speakers.
In recent years, we have started seeing a grow-
ing interest in the exploration of NLP techniques
for the analysis of language samples in the clinical
setting. For example, Sahakian and Snyder (2012)
propose a set of linguistic measures for age pre-
diction in children that combines three traditional
measures from language assessment with a set of
five data-driven measures from language samples
of 7 children. A common theme in this emerg-
ing line of research is the study of the syntax in
those language samples. For instance, to annotate
data to be used in the study of language develop-
ment (Sagae et al, 2005), or to build models to
map utterances to their meaning, similar to what
children do during the language acquisition stage
(Kwiatkowski et al, 2012). In addition, language
samples are also used for neurological assessment,
as for example in (Roark et al, 2007; Roark et
al., 2011) where they explored features such as
Yngve and Frazier scores, together with features
derived from automated parse trees to model syn-
tactic complexity and surprisal. Similar features
are used in the classification of language samples
to discriminate between children developing typ-
ically and children suffering from autism or lan-
guage impairment (Prud?hommeaux et al, 2011).
In a similar line of research, machine learning and
features inspired by NLP have been explored for
the prediction of language status in bilingual chil-
dren (Gabani et al, 2009; Solorio et al, 2011).
More recent work has looked at the feasibility of
scoring coherence in story narratives (Hassanali et
al., 2012a) and also on the inclusion of coherence
89
as an additional feature to boost prediction accu-
racy of language status (Hassanali et al, 2012b).
The contribution of our work consists on new
metrics based on n-grams of Part of Speech (POS)
tags for assessing language development in chil-
dren that combine information at the lexical and
syntactic levels. These metrics are designed to
capture the lexical variability of specific syntac-
tic constructions and thus could help to describe
the level of language maturity in children. For in-
stance, given two lists of examples of the use of
determiner + noun: ?the dog, the frog, the tree?
and ?this dog, a frog, these trees? we want to be
able to say that the second one has more lexical
variability than the first one for that grammatical
pattern.
Our approach to compute these new metrics
does not require any special treatment on the tran-
scripts or special purpose parsers beyond a POS
tagger. On the contrary, we provide a set of mea-
sures that in addition to being easy to interpret by
practitioners, are also easy to compute.
2 Background and Motivation
To establish language proficiency, clinical re-
searchers and practitioners rely on a variety of
measures, such as number of different words,
type-token ratio, distribution of part-of-speech
tags, and mean length of sentences and words per
minute (Lu, 2012; Yoon and Bhat, 2012; Chen and
Zechner, 2011; Yang, 2011; Miller et al, 2006), to
name a few. Most of these metrics can be cate-
gorized as low-level metrics since they only con-
sider rates of different characteristics at the lexi-
cal level. These measures are helpful in the so-
lution of several problems, for example, building
automatic scoring models to evaluate non-native
speech (Chen and Zechner, 2011). They can also
be used as predictors of the rate of growth of En-
glish acquisition in specific populations, for in-
stance, in typically developing (TD) and language
impaired (LI) bilingual children (Rojas and Igle-
sias, 2012; Gutie?rrez-Clellen et al, 2012). Among
the most widely used metrics are mean length of
utterance (MLU), a measure of syntactic complex-
ity (Bedore et al, 2010), and measures of lexi-
cal productivity, such as the number of different
words (NDW) and the child?s ratio of functional
words to content words (F/C) (Sahakian and Sny-
der, 2012).
MLU, NDW, F/C and some other low-level
measures have demonstrated to be valuable in the
assessment of language ability considering that
practitioners often only need to focus on produc-
tivity, diversity of vocabulary, and sentence or-
ganization. Although useful, these metrics only
provide superficial measures of the children?s lan-
guage skills that fail to capture detailed lexico-
syntactic information. For example, in addition to
knowing that a child is able to use specific verb
forms in the right context, such as, third person
singular present tense or regular past tense, knowl-
edge about what are the most common patterns
used by a child, or how many different lexical
forms for noun + verb are present in the child?s
speech is needed because answering these ques-
tions provides more detailed information about the
status of grammatical development. To fill in this
need, we propose a set of measures that aim to cap-
ture language proficiency as a function of lexical
variability in syntactic patterns. We analyze the
information provided by our proposed metrics on
a set of spontaneous story retells and evaluate em-
pirically their potential use in language status pre-
diction.
3 Proposed measures
To present the different metrics we propose in this
study we begin with the definition of the following
concepts:
A syntactic pattern p is an n-gram of part-of-
speech tags denoted as p = ?t1 t2 ... tn?, where
ti indicates the part-of-speech tag corresponding
to the word at position i. For simplicity we use
tpi to indicate the tag at position i from pattern p.
Two examples of syntactic patterns of length two
are ?DT NN? and ?DT JJ? 1.
A lexical form f is an n-gram of words. It is de-
fined as f = ?w1 w2 ... wn?, where wi is the word
at position i. Similarly to the previous definition,
we use wfi to indicate the word at position i in a
lexical form f .
A lexical form f corresponds to a syntactic
pattern p if and only if |f | is equal to |p| and
?ktag(w
f
k ) = t
p
k, where tag() is a function that re-
turns the part-of-speech of its argument. The set of
lexical forms in a given transcript corresponding to
a syntactic pattern p is denoted by LF p. Two ex-
amples of lexical forms from the syntactic pattern
?DT NN? are ?the cat? and ?the frog?.
1We use the Penn Treebank POS tagset
90
DT the (62), a (17), all (8), no(2), that (1)
NN frog (16), boy(7), dog (6), boat (4), name (3), place (2), house (2), water (2), rabbit (2), noise (2), stick (1), tree
(1), bye(1), floor (1), um (1), baby (1), forest (1), room (1), foot (1), rock (1), squirrel (1), back (1), rabb (1),
card (1), one (1), present (1), dress (1), box (1), family (1)
VBD saw (7), dropped (4), said (4), started (4), looked (3), kicked (3), called (3), found (2), took (2), got (2), jumped
(2), heard (2), thought (1), turned (1), fell (1), waked (1), stood (1), wa (1), touched (1), told (1), scared (1), tur
(1), haded (1), opened (1), shh (1)
DT NN the frog (3), the dog (2), the place (2), the water (2), the boat (2), a noise (2), the forest (1), the rock (1), a tree
(1), a present (1), a um (1), the card (1), the box (1), the rabb (1), the floor (1), the back (1), no one (1)
DT VBD all started (2), all heard (1)
Table 1: Example of 5 syntactic patterns with their lists of lexical forms and the number of repetitions
of each of them. This information corresponds to an excerpt of an example transcript. DT is the part-of-
speech tag for determiner, NN for noun, and VBD for verb in past tense.
The bag-of-words associated to a syntactic pat-
tern p is denoted as W p. This set is composed
of all the words from the lexical forms that corre-
spond to the syntactic pattern p. It is formally de-
fined as follows: W p = {w|w ? f, f ? LF p}.
For example, the bag-of-words of the syntactic
pattern ?DT NN? with lexical forms ?the cat? and
?the frog? is {the, cat, frog}.
Table 1 shows five syntactic patterns of a tran-
script?s fragment. For each syntactic pattern in the
transcript we show the list of its lexical forms and
their frequency. We will use this example in the
description of the measures in the following sub-
sections.
3.1 Number of different lexical forms
(NDLF)
Analogous to the number of different words
(NDW), where words in the transcript are consid-
ered atomic units, we propose a metric where the
atomic units are lexical forms. Then, we measure
the number of different lexical forms used for each
syntactic pattern in the transcript. Formally, given
a syntactic pattern p and its set of lexical forms
LF p, the number of different lexical forms is com-
puted as follows:
NDLF(p) = |LF p| (1)
This measure gives information about the num-
ber of different ways the child can combine words
in order to construct a fragment of a speech that
corresponds to a specific grammatical pattern. Re-
search in language assessment has shown that
when children are in the early acquisition stages
of certain grammatical constructions they will use
the patterns as ?fixed expressions?. As children
master these constructions they are able to use
these grammatical devices in different contexts,
but also with different surface forms. Thereby, we
could use this measure to discriminate the syntac-
tic patterns the child has better command of from
those that might still be problematic and used in-
frequently or with a limited combination of sur-
face forms. For example, from the information
on Table 1 we see that NDLF(DT NN) = 17, and
NDLF(DT VBD) = 2. This seems to indicate that
the child has a better command of the grammatical
construction determiner + noun (DT NN) and can
thus produce more different lexical forms of this
pattern than determiner + verb (DT + VBD). But
also, we may use this measure to identify rare pat-
terns, that are unlikely to be found in a typically
developing population.
3.2 Lexical forms distribution (LFdist)
Following the idea of lexical forms as atomic
units, NDLF allows to know the different lexical
forms present in the transcripts. But we do not
know the distribution of use of each lexical form
for a specific syntactic pattern. In other words,
NDLF tells us the different surface forms observed
for each syntactic pattern, but it does not measure
the frequency of use of each of these lexical forms,
nor whether each of these forms are used at similar
rates. We propose to use LFdist to provide infor-
mation about the distribution of use for LF p, the
set of lexical forms observed for the syntactic pat-
tern p. We believe that uniform distributions can
be indicative of syntactic structures that the child
has mastered, while uneven distributions can re-
veal structures that the child has only memorized
(i.e. the child uses a fixed and small set of lex-
ical forms). To measure this distribution we use
the entropy of each syntactic pattern. In particu-
lar, given a syntactic pattern p and its set of lexical
forms LF p, the lexical form distribution is com-
puted as follows:
91
LFdist(p) = ?
?
fi?LF p
prob(fi) log prob(fi)
(2)
where
prob(fi) =
count(fi)
?
fk?LF p count(fk)
(3)
and count() is a function that returns the fre-
quency of its argument. Larger values of LFdist
indicate a greater difficulty in the prediction of
the lexical form that is being used under a spe-
cific grammatical pattern. For instance, in the ex-
ample of Table 1, LFdist(DT VBD) = 0.91 and
LFdist(DT NN) = 3.97. This indicates that the
distribution in the use of lexical forms for deter-
miner + noun is more uniform than the use of
lexical forms for determiner + verb, which im-
plies that for determiner + verb there are some
lexical forms that are more frequently used than
others2. Syntactic patterns with small values of
LFdist could flag grammatical constructions the
child does not feel comfortable manipulating and
thus might still be in the acquisition stage of lan-
guage learning.
3.3 Lexical variation (LEX)
Until now we are considering lexical forms as
atomic units. This could lead to overestimating
the real lexical richness in the sample, in particu-
lar for syntactic patterns of length greater than 1.
To illustrate this consider the syntactic pattern p =
?DT NN? and suppose we have the following set
of lexical forms for p = {?the frog?, ?a frog?, ?a
dog?, ?the dog?}. The value for NDLF (p) = 4.
But how many of these eight words are in fact dif-
ferent? That is the type of distinction we want to
make with the next proposed measure: LEX, that
is also an adaptation of type-token ratio (Lu, 2012)
used in the area of communication disorders but
computed over each grammatical pattern. For this
example, we want to be able to find that the lex-
ical variation of ?DT NN? is 0.5 (because there
are only four different words out of eight). For-
mally, given a syntactic pattern p, its set of lexical
forms LF p, and the bag-of-words W p, the lexical
variation is defined as shown in Equation 4.
2We recognize that this is an oversimplification of the en-
tropy measure since the number of outcomes will most likely
be different for each syntactic pattern.
LEX(p) =
|W p|
|LF p| ? n
(4)
Note that |LF p| = NDLF(p), and n is the
length of the syntactic pattern p. In Table 1 the lex-
ical variation of the pattern ?determiner + noun?
(DT+NN) is equal to 0.58 ( 2017?2 ), and for deter-
miner + verb (DT+VBD) is equal to 0.75 ( 32?2 ).
That means 58% of total words used under the pat-
tern ?DT+NN? are different, in comparison with
the 75% for ?DT+VBD?. In general, the closer the
value of LEX is to 1, there is less overlap between
the words in the lexical forms for that pattern.
Our hypothesis behind this measure is that for the
same syntactic pattern TD children may have less
overlap of words than children with LI, e.g. less
overlap indicates the use of a more diverse set of
words.
3.4 Lexical use of syntactic knowledge
(LexSyn)
With LEX we hope to accomplish the character-
ization of lexical richness of syntactic patterns
assuming that each part-of-speech has a similar
number of possible lexical forms. We assume as
well that less overlap in the words used for the
same grammatical pattern represents a more devel-
oped language than that with more overlap. How-
ever the definition of LEX overlooks a well known
fact about language: different word classes have
a different range of possibilities as their lexical
forms. Consider open class items, such as nouns
and verbs, where the lexicon is large and keeps
growing. In contrast, closed class items, such as
prepositions and determiners are fixed and have a
very small number of lexical forms. Therefore it
seems unfair to assign equal weight to the overlap
of words for these different classes. To account
for this phenomenon, we propose a new measure
that includes the information about the syntactic
knowledge that the child shows for each part of
speech. That is, we weigh the level of overlap
for specific grammatical constructions according
to the lexicon for the specific word classes in-
volved. Since we limit our analysis to the language
sample at hand, we define the ceiling of the lexi-
cal richness of a specific word class to be the to-
tal number of different surface forms found in the
transcript. In particular, given a syntactic pattern
p = ?t1 t2 ... tn?, with its set of lexical forms
LF p, the lexical use of syntactic knowledge is de-
fined as:
92
LexSyn(p) =
1
n
n?
i=1
|wfi |f ? LF
p|
NDLF(tpi )
(5)
where the numerator is the size of the set of
words in the i-th position in all the lexical forms.
Note that this measure does not make sense for
syntactic patterns of length < 2. Instead, syn-
tactic patterns of length 1 were used to identify
the syntactic knowledge of the child by using the
NDLF of each POS in p. In the example of Ta-
ble 1, LexSyn(DT NN) = 0.59. This value corre-
sponds to the sum of the number of different de-
terminers used in position 1 for LF p divided by
the total number of different determiners that this
child produced in the sample (for this case, the
number of determiners that this child produced is
given by NDLF(DT), that is 5), plus the number
of different nouns used under this syntactic pat-
tern over the total number of nouns produced by
the child (NDLF(NN)=29). The complete calcula-
tion of LexSyn(DT NN) = 12 ?(
3
5+
17
29) = 0.59.
This contrasts with the value of LexSyn for the pat-
tern ?determiner + verb?, LexSyn(DT VBD) =
1
2 ? (
1
5 +
2
25) = 0.14 that seems to indicate that the
child has more experience combining determiners
and nouns than determiners and verbs. Perhaps
this child has had limited exposure to other pat-
terns combining determiner and verb, or this pat-
tern is at a less mature stage in the linguistic reper-
toire of the child.
Children with LI tend to exhibit a less devel-
oped command of syntax than their TD cohorts.
Syntactic patterns with large values of LexSyn
show a high versatility in the use of those syntactic
patterns. However, since the syntactic reference is
taken from the same child, this versatility is rela-
tive only to what is observed in that single tran-
script. For instance, suppose that the total num-
ber of different determiners observed in the child?s
transcript is 1. Then any time the child uses that
determiner in a syntactic pattern, the knowledge of
this class, according to our metric, will be 100%,
which is correct, but this might not be enough to
determine if the syntactic knowledge of the child
for this grammatical class corresponds to age ex-
pectations for a typically developing child. In or-
der to improve the measurement of the lexical use
of syntactic knowledge we propose the measure
LexSynEx, that instead of using the information
of the same child to define the coverage of use for
a specific word class, it uses the information ob-
served for a held out set of transcripts from TD
children. This variation allows the option of mov-
ing the point of reference to a specific cohort, ac-
cording to what is needed.
4 Data set
The data used in this research is part of an ongoing
study of language impairment in Spanish-English
speaking children (Pen?a et al, 2003). From this
study we used a set of 175 children with a mean
age of about 70 months. Language status of these
children was determined via expert judgment by
three bilingual certified speech-language pathol-
ogists. At the end of the data collection period,
the experts reviewed child records in both lan-
guages including language samples, tests proto-
cols, and parent and teacher questionnaire data.
They made independent judgments about chil-
dren?s lexical, morphosyntactic, and narrative per-
formance in each language. Finally, they made an
overall judgment about children?s language abil-
ity using a 6 point scale (severely language im-
paired to above normal impairment). If at least two
examiners rated children?s language ability with
mild, moderate or severe impairment they were as-
signed to the LI group. Percent agreement among
the three examiners was 90%. As a result of this
process, 20 children were identified by the clinical
researchers as having LI, while the remaining 155
were identified as typically developing (TD).
The transcripts were gathered following stan-
dard procedures for collection of spontaneous lan-
guage samples in the field of communication dis-
orders. Using a wordless picture book, the chil-
dren were asked to narrate the story. The two
books used were ?A boy, a dog, and a frog? (Mayer,
1967) and ?Frog, where are you?? (Mayer, 1969).
For each child in the sample, 4 transcripts of story
narratives were collected, 2 in each language. In
this study we use only the transcripts where En-
glish was the target language.
5 Procedure
The purpose of the following analysis is to inves-
tigate the different aspects in the child?s language
that can be revealed by the proposed metrics. All
our measures are based on POS tags. We used the
Charniak parser (Charniak, 2000) to generate the
POS tags of the transcripts. For all the results re-
ported here we removed the utterances from the
interrogators and use all utterances by the chil-
93
dren. From the 155 TD instances, we randomly se-
lected 20, that together with the 20 instances with
LI form the test set. The remaining 135 TD in-
stances were used as the normative population, our
training set.
After the POS tagging process, we extracted the
set of syntactic patterns with length equal to 1, 2, 3
and 4 that appear in at least 80% of the transcripts
in the training set. The 80% threshold was chosen
with the goal of preserving the content that is most
likely to represent the TD population.
6 Analysis of the proposed measures and
implications
Figure 1 shows 5 plots corresponding to each of
our proposed measures. Each graph shows a com-
parison between the average values of the TD and
the LI populations. The x-axis in the graphs rep-
resents all the syntactic patterns gathered from the
training set that appeared on the test data, and the
y-axis represents the difference in the z-score val-
ues of each measure from the test set. The x-axis
is sorted in descending order according to the z-
score differences between values of TD and LI.
The most relevant discovery is that NDFL,
LFdist, LexSyn and LexSynEx show a wider gap
in the z-scores between the TD and LI popula-
tions for most of the syntactic patterns analyzed.
This difference is easy to note visually as most of
the TD patterns tend to have larger values, while
the ones for children with LI have lower scores.
Therefore, it seems our measures are indeed cap-
turing relevant information that characterizes the
language of the TD population.
Analyzing LEX from Figure 1, we see that most
of the LEX values are positive, for both TD and
LI instances, and we cannot observe marked dif-
ferences between them. That might be a con-
sequence of assuming all word classes can have
an equivalent number of different lexical forms.
Once we weigh each POS tag in the pattern by the
word forms the child has used (as in LexSyn and
LexSynEx), noticeable differences across the two
groups emerge. When we include syntactic knowl-
edge of a group of children (as in LexSynEx), those
similarities disappear. This behavior highlights the
need for a combined lexico-syntactic measure that
can describe latent information about language us-
age in children.
For building an intervention plan that helps to
improve child language skills, practitioners could
LFdist
verb (3rd person singular present)
verb (past tense) + personal pronoun
personal pronoun + auxiliary verb + adverb
verb (gerund)
NDLF
there + auxiliary verb
personal pronoun + auxiliary verb + adverb
adjective + noun
verb (3rd person singular present)
LexSyn
verb (past tense) + personal pronoun
personal pronoun + verb (past tense) + personal pronoun
personal pronoun + auxiliary verb + adverb
there + auxiliary verb
LexSynEx
personal pronoun + auxiliary verb + adverb
personal pronoun + verb (past tense) + personal pronoun
verb (past tense) + personal pronoun
there + auxiliary verb
Table 2: List of syntactic patterns with the biggest
difference between LI and TD in 4 measures:
LFdist, NDLF, and LexSyn and LexSynEx.
use the knowledge of specific grammatical con-
structions that need to be emphasized ?those that
seem to be problematic for the LI group. These
structures can be identified by pulling the syntac-
tic patterns with the largest difference in z-scores
from the TD population. Table 2 shows a list of
syntactic patterns with small values for LI and the
largest differences between LI and TD instances
in the test set. As the table indicates, most of the
syntactic patterns have length greater than 1. This
is not surprising since we aimed for developing
measures of higher-order analysis that can com-
plement the level of information provided by com-
monly used metrics in language assessment (as in
the case of MLU, NDW or F/C). The table also
shows that while each measure identifies a differ-
ent subset of syntactic patterns as relevant, some
syntactic patterns emerge in all the metrics. For
instance, personal pronoun + auxiliary verb + ad-
verb and there + auxiliary verb. This repetition
highlights the importance of those grammatical
constructions. But the differences also show that
the metrics complement each other. In general,
the syntactic patterns in the list represent complex
grammatical constructions where children with LI
are showing a less advanced command of language
use.
Table 3 shows some statistics about the lexical
forms present under pronoun + verb (3rd person
singular present) + verb (gerund or present par-
ticiple) (PP VBZ VBG) in all our data set. The last
94
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(a) NDLF
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(b) LFdist
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(c) LEX
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(d) LexSyn
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(e) LexSynEx
Figure 1: Performance comparison of the proposed measures for the TD and LI groups. Each data point
represents the difference in z-scores between the average values of the TD and LI instances in the test
set.
row in that table presents an example of the lexi-
cal forms used by two children. Note that for the
child with LI, there is only one lexical form: he is
touching. On the other hand, the TD child is using
the grammatical pattern with six different surface
forms. Clinical practitioners can take this infor-
mation and design language tasks that emphasize
the use of ?PP VBZ VBG? constructions.
6.1 Analysis of correlations among measures
To analyze the level of overlap between our mea-
sures we computed correlation coefficients among
them. The results are shown in Table 4.
The results from the correlation analysis are not
that surprising. They show that closely related
measures are highly to moderately correlated. For
instance, LEX and eLEX have a correlation of
TD LI
number of PP 6 5
number of VBZ 3 2
number of VBG 7 4
Example (instances: she is putting he is touching
td-0156 and li-3022) she is going
he is pushing
she is looking
she is carrying
she is playing
Table 3: Statistics of the surface forms for the
grammatical pattern PP VBZ VBG.
0.69, and LexSynEx and LexSyn have a correla-
tion of 0.61. NDLF and LFdist showed a posi-
tive correlation score of 0.81. This high correla-
tion hints to the fact that as the number of lexical
forms increases, so does the gap between their fre-
95
LFdist NDLF LEX eLEX LexSyn LexSynEx
LFdist 1.00
NDLF 0.81 1.00
LEX -0.53 -0.31 1.00
eLEX -0.54 -0.43 0.69 1.00
LexSyn 0.07 0.02 -0.23 -0.10 1.00
LexSynEx -0.02 -0.03 -0.08 -0.03 0.61 1.00
Table 4: Correlation matrix for the proposed metrics.
quency of use. While this may be a common phe-
nomenon of language use, it does not have a neg-
ative effect since the same effect will be observed
in both groups of children and we care to see the
differences in performance between a TD and an
LI population.
For all other pairs of measures, the correlation
scores were in the range of [?0.5, 0.1]. It was in-
teresting to note that LexSyn showed the lowest
correlation with the rest of the measures (between
[?0.11, 0.01]).
Correlation coefficients between our metrics
and MLU, NDW, and F/C were computed sepa-
rately for syntactic patterns of different lengths.
However all the different matrices showed the
same correlation patterns. We found a high cor-
relation between MLU and NDW, but low cor-
relation with all our proposed measures, except
for one case: NDW and LexSyn seemed to be
highly correlated (?-0.7). Interestingly, we noted
that despite the high correlation between MLU and
NDW, MLU and LexSyn showed weak correlation
(?-0.4). Overall, the findings from this analysis
support the use of our metrics as complimentary
measures for child language assessment.
7 Conclusions and future work
We proposed a set of new measures that were de-
veloped to characterize the lexico-syntactic vari-
ability of child language. Each measure aims to
find information that is not captured by traditional
measures used in communication disorders.
Our study is still preliminary in nature and re-
quires an in depth evaluation and analysis with a
larger pool of subjects. However the results pre-
sented are encouraging. The set of experiments
we discussed showed that TD and LI children have
significant differences in performance according
to our metrics and thus these metrics can be used to
enrich models of language trajectories in child lan-
guage acquisition. Another potential use of met-
rics similar to those proposed here is the design of
targeted intervention practices.
The scripts to compute the metrics as described
in this paper are available to the research commu-
nity by contacting the authors. However, the sim-
plicity of the metrics makes it easy for anyone to
implement, and it certainly makes it easy for clin-
ical researchers to interpret.
Our proposed metrics are a contribution to the
set of already known metrics for language assess-
ment. The goal of these new metrics is not to
replace existing ones, but to complement what is
already available with concise information about
higher-order syntactic constructions in the reper-
toire of TD children.
We are interested in evaluating the use of our
metrics in a longitudinal study. We believe they
are a promising framework to represent language
acquisition trajectories.
Acknowledgments
This research was partially funded by NSF under
awards 1018124 and 1017190. The first author
also received partial funding from CONACyT.
References
Lisa M. Bedore, Elizabeth D. Pen?a, Ronald B. Gillam,
and Tsung-Han Ho. 2010. Language sample mea-
sures and language ability in Spanish-English bilin-
gual kindergarteners. Journal of Communication
Disorders, 43:498?510.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
722?731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
96
Keyur Gabani, Melissa Sherman, Thamar Solorio,
Yang Liu, Lisa M. Bedore, and Elizabeth D. Pen?a.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
NAACL ?09, pages 46?55, Stroudsburg, PA, USA.
Association for Computational Linguistics.
V. Gutie?rrez-Clellen, G. Simon-Cereijido, and
M. Sweet. 2012. Predictors of second language
acquisition in Latino children with specific language
impairment. American Journal of Speech Language
Pathology, 21(1):64?77.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012a. Coherence in child language narra-
tives: A case study of annotation and automatic pre-
diction of coherence. In Proceedings of 3rd Work-
shop on Child, Computer and Interaction (WOCCI
2012).
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012b. Evaluating NLP features for auto-
matic prediction of language impairment using child
speech transcripts. In Interspeech.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. 2012. A probabilis-
tic model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 234?244, Avignon, France. Associa-
tion for Computational Linguistics.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Language Journal, 96(2):190?208.
Mercer Mayer. 1967. A boy, a dog, and a frog. Dial
Press.
Mercer Mayer. 1969. Frog, where are you? Dial
Press.
Jon F. Miller, John Heilmann, Ann Nockerts, Aquiles
Iglesias, Leah Fabiano, and David J. Francis. 2006.
Oral language and reading in bilingual children.
Learning Disabilities Research and Practice, 21:30?
43.
Elizabeth D. Pen?a, Lisa M. Bedore, Ronald B. Gillam,
and Thomas Bohman. 2003. Diagnostic markers
of language impairment in bilingual children. Grant
awarded by the NIDCH, NIH.
Emily T. Prud?hommeaux, Brian Roark, Lois M.
Black, and Jan van Santen. 2011. Classification of
atypical language in autism. In Proceedings of the
2nd Workshop on Cognitive Modeling and Compu-
tational Linguistics, pages 88?96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Biologi-
cal, translational, and clinical language processing,
pages 1?8, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transcations on Au-
dio, Speech, and Language Processing, 19(7):2081?
2090, September.
Rau?l Rojas and Aquiles Iglesias. 2012. The language
growth of Spanish-speaking English language learn-
ers. Child Development.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005. Automatic measurement of syntactic devel-
opment in child language. In Proceedings of the
43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL ?05, pages 197?204,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sam Sahakian and Benjamin Snyder. 2012. Automat-
ically learning measures of child language develop-
ment. In ACL, pages 95?99. The Association for
Computational Linguistics.
Thamar Solorio, Melissa Sherman, Y. Liu, Lisa
Bedore, Elizabeth Pen?a, and A. Iglesias. 2011. An-
alyzing language samples of Spanish-English bilin-
gual children for the automated prediction of lan-
guage dominance. Natural Language Engineering,
pages 367?395.
Charles Yang. 2011. A statistical test for grammar.
In Proceedings of the 2nd Workshop on Cognitive
Modeling and Computational Linguistics, pages 30?
38, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In EMNLP-CoNLL, pages 600?
608. Association for Computational Linguistics.
97
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 111?115,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Latent Dirichlet Allocation for Child Narrative Analysis
Khairun-nisa Hassanali and Yang Liu
The University of Texas at Dallas
Richardson, TX, USA
nisa,yangl@hlt.utdallas.edu
Thamar Solorio
University of Alabama at Birmingham
Birmingham, AL, USA
solorio@uab.edu
Abstract
Child language narratives are used for lan-
guage analysis, measurement of language
development, and the detection of lan-
guage impairment. In this paper, we ex-
plore the use of Latent Dirichlet Alloca-
tion (LDA) for detecting topics from nar-
ratives, and use the topics derived from
LDA in two classification tasks: automatic
prediction of coherence and language im-
pairment. Our experiments show LDA is
useful for detecting the topics that corre-
spond to the narrative structure. We also
observed improved performance for the
automatic prediction of coherence and lan-
guage impairment when we use features
derived from the topic words provided by
LDA.
1 Introduction
Language sample analysis is a common technique
used by speech language researchers to measure
various aspects of language development. These
include speech fluency, syntax, semantics, and co-
herence. For such analysis, spontaneous narratives
have been widely used. Narrating a story or a per-
sonal experience requires the narrator to build a
mental model of the story and use the knowledge
of semantics and syntax to produce a coherent nar-
rative. Children learn from a very early age to nar-
rate stories. The different processes involved in
generating a narrative have been shown to provide
insights into the language status of children.
There has been some prior work on child lan-
guage sample analysis using NLP techniques. Sa-
hakian and Snyder (2012) used a set of linguistic
features computed on child speech samples to cre-
ate language metrics that included age prediction.
Gabani et al (2011) combined commonly used
measurements in communication disorders with
several NLP based features for the prediction of
Language Impairment (LI) vs. Typically Develop-
ing (TD) children. The features they used included
measures of language productivity, morphosyntac-
tic skills, vocabulary knowledge, sentence com-
plexity, probabilities from language models, stan-
dard scores, and error patterns. In their work, they
explored the use of language models and machine
learning methods for the prediction of LI on two
types of child language data: spontaneous and nar-
rative data.
Hassanali et al (2012a) analyzed the use of
coherence in child language and performed auto-
matic detection of coherence from child language
transcripts using features derived from narrative
structure such as the presence of critical narrative
components and the use of narrative elements such
as cognitive inferences and social engagement de-
vices. In another study, Hassanali et al (2012b)
used several coherence related features to auto-
matically detect language impairment.
LDA has been used in the field of narrative anal-
ysis. Wallace et al (2012) adapted LDA to the task
of multiple narrative disentanglement, in which
the aim was to tease apart narratives by assigning
passages from a text to the subnarratives that they
belong to. They achieved strong empirical results.
In this paper, we explore the use of LDA for
child narrative analysis. We aim to answer two
questions: Can we apply LDA to children nar-
ratives to identify meaningful topics? Can we
represent these topics automatically and use them
for other tasks, such as coherence detection and
language impairment prediction? Our results are
promising. We found that using LDA topic model-
ing can infer useful topics, and incorporating fea-
tures derived from such automatic topics improves
the performance of coherence classification and
language impairment detection over the previously
reported results.
111
Coherence Scale TD LI Total
Coherent 81 6 87
Incoherent 18 13 31
Total 99 19 118
Table 1: Number of TD and LI children on a 2-
scale coherence level
2 Data
For the purpose of the experiments, we used the
Conti-Ramsden dataset (Wetherell et al, 2007a;
Wetherell et al, 2007b) from the CHILDES
database (MacWhinney, 2000). This dataset con-
sists of transcripts belonging to 118 adolescents
aged 14 years. The adolescents were given the
wordless picture story book ?Frog, where are
you?? and asked to narrate the story based on the
pictures. The storybook is about the adventures of
a boy who goes searching for his missing pet frog.
Even though our goal is to perform child narrative
analysis, we used this dataset from adoloscents
since it was publicly available, and was annotated
for language impairment and coherence. Of the
118 adolescents, 99 adolescents belonged to the
TD group and 19 adolescents belonged to the lan-
guage impaired group. Hassanali et al (2012a)
annotated this dataset for coherence. A transcript
was annotated as coherent, as long as there was no
difficulty in understanding the narrative, and in-
coherent otherwise. Table 1 gives the TD and LI
distribution on a 2-scale coherence level. Figure
1 shows an example of a transcript produced by a
TD child.
Figure 1: Sample transcript from a TD child
3 Narrative Topic Analysis Using LDA
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) has been used in NLP to model topics within
a collection of documents. In this study, we use
LDA to detect topics in narratives. Upon exam-
ining the transcripts, we observed that each topic
was described in about 3 to 4 utterances. We there-
fore segmented the narratives into chunks of 3 ut-
terances, with the assumption that each segment
corresponds roughly to one topic.
We used the software by Blei et al1 to perform
LDA. Prior to performing LDA, we removed the
stop words from the transcripts. We chose ? to
be 0.8 and K to be 20, where ? is the parameter
of the Dirichlet prior on the per-document topic
distributions and K denotes the number of topics
considered in the model.
We chose to use the transcripts of TD children
for generating the topics, because the transcripts of
TD children have fewer disfluencies, incomplete
utterances, and false starts. As we can observe
from Table 1, a higher percentage of TD children
produced coherent narratives when compared to
children with LI.
Table 2 gives the topic words for the top 10
topics extracted using LDA. The topics in Table
2 were manually labeled after examination of the
topic words extracted using LDA. We found that
some of the topics extracted by LDA corresponded
to subtopics. For example, searching for the frog
in the house has subtopics of the boy searching
for the frog in room and the dog falling out of the
window, which were part of the topics covered by
LDA. The subtopics are marked in italics in Table
2.
The following narrative components were iden-
tified as important features for the automatic pre-
diction of coherence by Hassanali et al (2012a).
1. Instantiation: introduce the main characters
of the story: the boy, the frog, and the dog,
and the frog goes missing
2. 1st episode: search for the frog in the house
3. 2nd episode: search for the frog in the tree
4. 3rd episode: search for the frog in the hole in
the ground
5. 4th episode: search for the frog near the rock
6. 5th episode: search for the frog behind the
log
7. Resolution: boy finds the frog in the river and
takes a frog home
Upon examining the topics extracted by LDA, we
observed that all the components mentioned above
1http://www.cs.princeton.edu/ blei/lda-c/index.html
112
Topic
No
Topic Words Used by TD Population Topic Described
1 went,frog,sleep,glass,put,caught,jar,yesterday,out,house Introduction
2 frog,up,woke,morning,called,gone,escaped,next,kept,realized Frog goes missing
3 window,out,fell,dog,falls,broke,quickly,opened,told,breaking Dog falls out of window
4 tree,bees,knocked,running,popped,chase,dog,inside,now,flying Dog chases the bees
5 deer,rock,top,onto,sort,big,up,behind,rocks,picked Deer behind the rock
6 searched,boots,room,bedroom,under,billy,even, floor,tilly,tried Search for frog in room
7 dog,chased,owl,tree,bees,boy,came,hole,up,more Boy is chased by owl from a
tree with beehives
8 jar,gone,woke,escaped,night,sleep,asleep,dressed,morning,frog Frog goes missing
9 deer,top,onto,running,ways,up,rocks,popped,suddenly,know Boy runs into the deer
10 looking,still,dog,quite,cross,obviously,smashes,have,annoyed Displeasure of boy with dog
Table 2: Top 10 topic words extracted by LDA on the story telling task. Subtopics are shown in italics.
were present in these topics. Many of the LDA
topics corresponded to a picture or two in the sto-
rybook.
4 Using LDA Topics for Coherence and
Language Impairment Classification
We extended the use of LDA for two tasks,
namely: the automatic evaluation of coherence
and the automatic evaluation of language impair-
ment. For the experiments below, we used the
WEKA toolkit (Hall et al, 2009) and built sev-
eral models using the naive Bayes, Bayesian net
classifier, Logistic Regression, and Support Vec-
tor Machine (SVM) classifier. Of all these classi-
fiers, the naive Bayes classifier performed the best,
and we report the results using the naive Bayes
classifier in Tables 3 and 4. We performed all the
experiments using leave-one-out cross-validation,
wherein we excluded the test transcript that be-
longed to a TD child from the training set when
generating topics using LDA.
4.1 Automatic Evaluation of Coherence
We treat the automatic evaluation of coherence
as a classification task. A transcript could either
be classified as coherent or incoherent. We use
the results of Hassanali et al (2012a) as a base-
line. They used the presence of narrative episodes,
and the counts of narrative quality elements such
as cognitive inferences and social engagement de-
vices as features in the automatic prediction of co-
herence. We add the features that we automati-
cally extracted using LDA.
We checked for the presence of at least six of
the ten topic words or their synonyms per topic in
a window of 3 utterances. If the topic words were
present, we took this as a presence of a topic; oth-
erwise we denoted it as an absence of a topic. In
total, there were 20 topics that we extracted using
LDA, which is higher compared to the 8 narrative
structure topics that were annotated for by Has-
sanali et al (2012a).
Table 3 gives the results for the automatic clas-
sification of coherence. As we observe in Table
3, there is an improvement in performance over
the baseline. We attribute this to the inclusion of
subtopics that were extracted using LDA.
4.2 Automatic Evaluation of Language
Impairment
We extended the use of LDA to create a summary
of the narratives. For the purpose of generating the
summary, we considered only the narratives gen-
erated by TD children in the training set. We gen-
erated a summary, by choosing 5 utterances cor-
responding to each topic that was generated using
LDA, thereby yielding a summary that consisted
of 100 utterances.
We observed that different words were used to
represent the same concept. For example, ?look?
and ?search? were used to represent the concept
of searching for the frog. Since the narration was
based on a picture storybook, many of the children
used different terms to refer to the same animal.
For example, ?the deer? in the story has been inter-
preted to be ?deer?, ?reindeer?, ?moose?, ?stag?,
?antelope? by different children. We created an
extended topic vocabulary using Wordnet to in-
clude words that were semantically similiar to the
topic keywords. In addition, for an utterance to be
113
Feature Set
Coherent Incoherent Accuracy
(%)Precision Recall F-1 Precision Recall F-1
Narrative (Hassanali et al,
2012a) (baseline)
0.869 0.839 0.854 0.588 0.645 0.615 78.814
Narrative + automatic topic
features
0.895 0.885 0.89 0.688 0.71 0.699 83.898
Table 3: Automatic classification of coherence on a 2-scale coherence level
in the summary, we put in the additional constraint
that neighbouring utterances within a window of
3 utterances also talk about the same topic. We
used this summary for constructing unigram and
bigram word features for the automatic prediction
of LI.
The features we constructed for the prediction
of LI were as follows:
1. Bigrams of the words in the summary
2. Presence or absence of the words in the sum-
mary regardless of the position
3. Presence or absence of the topics detected by
LDA in the narratives
4. Presence or absence of the topic words that
were detected using LDA
We used both the topics detected and the pres-
ence/absence of topic words as features since the
same topic word could be used across several top-
ics. For example, the words ?frog?, ?dog?, ?boy?,
and ?search? are common across several topics.
We refer to the above features as ?new features?.
Table 4 gives the results for the automatic pre-
diction of LI using different features. As we can
observe, the performance improves to 0.872 when
we add the new features to Gabani?s and the nar-
rative structure features. When we use the new
features by themselves to predict language impair-
ment, the performance is the worst. We attribute
this to the fact that other feature sets are richer
since these features take into account aspects such
as syntax and narrative structure.
We performed feature analysis on the new fea-
tures to see what features contributed the most.
The top scoring features were the presence or ab-
sence of the topics detected by LDA that corre-
sponded to the introduction of the narrative, the
resolution of the narrative, the search for the frog
in the room, and the search for the frog behind
the log. The following bigram features generated
from the summary contributed the most: ?deer
Feature P R F-1
Gabani?s (Gabani et
al., 2011)
0.824 0.737 0.778
Narrative (Hassanali et
al., 2012a)
0.385 0.263 0.313
New features 0.308 0.211 0.25
Narrative + Gabani?s 0.889 0.842 0.865
Narrative + Gabani?s +
new features
0.85 0.895 0.872
Table 4: Automatic classification of language im-
pairment
rock?, ?lost frog?, and ?boy hole?. Using a subset
of these best features did not improve the perfor-
mance when we added them to the narrative fea-
tures and Gabani?s features.
5 Conclusions
In this paper, we explored the use of LDA in the
context of child language analysis. We used LDA
to extract topics from child language narratives
and used these topic keywords to create a sum-
mary of the narrative and an extended vocabu-
lary. The topics extracted using LDA not only
covered the main components of the narrative but
also covered subtopics too. We then used the LDA
topic words and the summary to create features
for the automatic prediction of coherence and lan-
guage impairment. Due to higher coverage of the
LDA topics as compared to manual annotation, we
found an increase in performance of both auto-
matic prediction of coherence and language im-
pairment with the addition of the new features. We
conclude that the use of LDA to model topics and
extract summaries is promising for child language
analysis.
Acknowledgements
This research is supported by NSF awards IIS-
1017190 and 1018124.
114
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Keyur Gabani, Thamar Solorio, Yang Liu, Khairun-
nisa Hassanali, and Christine A. Dollaghan. 2011.
Exploring a corpus-based approach for detect-
ing language impairment in monolingual English-
speaking children. Artificial Intelligence in
Medicine, 53(3):161?170.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012a. Coherence in child language nar-
ratives: A case study of annotation and automatic
prediction of coherence. In Proceedings of WOCCI
2012 - 3rd Workshop on Child, Computer and Inter-
action.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012b. Evaluating NLP features for au-
tomatic prediction of language impairment using
child speech transcripts. In Proceedings of INTER-
SPEECH.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, Volume I: Transcription for-
mat and programs. Lawrence Erlbaum Associates.
Sam Sahakian and Benjamin Snyder. 2012. Automat-
ically learning measures of child language develop-
ment. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 95?99. Association
for Computational Linguistics.
Bryon C. Wallace. 2012. Multiple narrative disentan-
glement: Unraveling infinite jest. In Proceeding of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1?10.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007a. Narrative in adolescent specific
language impairment (SLI): a comparison with peers
across two different narrative genres. International
Journal of Language & Communication Disorders,
42(5):583?605.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007b. Narrative skills in adolescents
with a history of SLI in relation to non-verbal IQ
scores. Child Language Teaching and Therapy,
23(1):95.
115
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Overview for the First Shared Task on
Language Identification in Code-Switched Data
Thamar Solorio
Dept. of Computer Science
University of Houston
Houston, TX, 77004
solorio@cs.uh.edu
Elizabeth Blair, Suraj Maharjan, Steven Bethard
Dept. of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, AL, 35294
{eablair,suraj,bethard}@uab.edu
Mona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdi
Dept. of Computer Science
George Washington University
Washington, DC 20052
{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.edu
Julia Hirschberg and Alison Chang
Dept. of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
ayc2135@columbia.edu
Pascale Fung
Dept. of Electronic & Computer Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
pascale@ece.ust.hk
Abstract
We present an overview of the first shared
task on language identification on code-
switched data. The shared task in-
cluded code-switched data from four lan-
guage pairs: Modern Standard Arabic-
Dialectal Arabic (MSA-DA), Mandarin-
English (MAN-EN), Nepali-English (NEP-
EN), and Spanish-English (SPA-EN). A to-
tal of seven teams participated in the task
and submitted 42 system runs. The evalua-
tion showed that language identification at
the token level is more difficult when the
languages present are closely related, as in
the case of MSA-DA, where the prediction
performance was the lowest among all lan-
guage pairs. In contrast, the language pairs
with the higest F-measure where SPA-EN
and NEP-EN. The task made evident that
language identification in code-switched
data is still far from solved and warrants
further research.
1 Introduction
The main goal of this language identification shared
task is to increase awareness of the outstanding
challenges in the automated processing of Code-
Switched (CS) data and motivate more research in
this direction. We define CS broadly as a commu-
nication act, whether spoken or written, where two
or more languages are being used interchangeably.
In its spoken form, CS has probably been around
ever since different languages first came in contact.
Linguists have studied this phenomenon since the
mid 1900s. In contrast, the Natural Language Pro-
cessing (NLP) community has only recently started
to pay attention to CS, with the earliest work in
this area dating back to Joshi?s theoretical work
proposing an approach to parsing CS data (Joshi,
1982) based on the Matrix and Embedded language
framework. With the wide-spread use of social me-
dia, CS is now being used more and more in written
language and thus we are seeing an increase in pub-
lished papers dealing with CS. We are specifically
interested in intrasentential code switched phenom-
ena. As a result of this task, we have successfully
created the first set of annotated data for several
language pairs with a coherent set of labels across
the languages. As the shared task results show,
CS poses new research questions that warrant new
NLP approaches, and thus we expect to see a sig-
nificant increase in NLP work in the coming years
addressing CS phenomena in data.
The shared task covers four language pairs and
is focused on social media data. We provided par-
ticipants with annotated data from Twitter for the
62
language pairs: Modern Standard Arabic-Arabic
dialects (MSA-DA), Mandarin-English (MAN-
EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).
These language pairs represent a good variety in
terms of language typology and relatedness among
pairs. They also cover languages with different rep-
resentation in terms of number of speakers world
wide. Participants were asked to make predictions
on unseen Twitter data for each language pair. We
also provided participants with test data from a
?surprise genre? with the objective of assessing the
robustness of language identification systems to
genre variation.
2 Task Description
The task consists of labeling each token/word in
the input file with one of six labels: lang1, lang2,
other, ambiguous, mixed, and named entities NE.
The lang1, lang2 labels refer to the two languages
addressed in the subtask, for example for the lan-
guage pair MSA-DA, lang1 would be an MSA and
lang2 is DA. The other category is a label used to
tag all punctuation marks, emoticons, numbers, and
similar tokens that do not represent actual words in
any of the given languages. The ambiguous label
is for instances where it is not possible to assign
a language with certainty, for example, a lexical
form that belongs to both languages, appearing in a
context that does not indicate one language over the
other. The mixed category is for words composed
of CS morphemes, such as the word snapchateando
?to chat? from SPA-EN, the word overai from NEP-
EN, or the word hayqwlwn
1
?they will say?, from
MSA-DA, where the ?ha? is a DA future morpheme
and the stem ?yqwlwn? is MSA.The NE label is
included in this task in an effort to allow for a more
focused analysis of CS data with the exclusion of
proper nouns. NEs have a very different behavior
than most other words in a language vocabulary
and thus from our perspective they need to be iden-
tified to be handled properly.
Table 1 shows Twitter examples taken from the
training data. The annotation guidelines are posted
on the workshop website
2
. We post the ones used
for SPA-EN as for the other language pairs the only
differences are the examples provided.
1
We use Buckwalter transliteration scheme http://
www.qamus.org/transliteration.htm
2
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Language Pair Example
MSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.
HAfZ AlmyrAzy ElY qnAp drym llHdyv
En >wlwyAt Alvwrp fy AlmrHlp Al-
HAlyp wqDyp tSHyH msAr Alvwrp
Al<ElAmy
(Today O?Clock 11 I will be
[a ]guest[ of] Mr. Hafez
AlMirazi on Channel Dream
to talk about [the ]priorities[ of]
the revolution in the stage the current
and [the ]issue[ of] correcting
[the ]path[ of] the revolution Media)
NEP-EN My car at the workshop for a much
needed repairs... ABA pocket khali
hune bho
(My car at the workshop for a much
needed repairs. . . now my pocket will
be empty)
SPA-EN Por primera vez veo a @username ac-
tually being hateful! it was beautiful:)
(For the first time I get to see @user-
name actually being hateful! it was
beautiful:)
Table 1: Examples of Twitter data used in the
shared task.
3 Related Work
In the past, most language identification research
has been done at the document level. Some re-
searchers, however, have developed methods to
identify languages within multilingual documents
(Singh and Gorla, 2007; Nguyen and Do?gru?oz,
2013; King and Abney, 2013). Their test data
comes from a variety of sources, including web
pages, bilingual forum posts, and jumbled data
from monolingual sources, but none of them are
trained on code-switched data, opting instead for a
monolingual training set per language. This could
prove to be a problem when working on code-
switched data, particularly in shorter samples such
as social media data, as the code-switching context
is not present in training material.
One system tackled both the problems of code-
switching and social media in language and code-
switched status identification (Lignos and Marcus,
2013). Lignos and Marcus gathered millions of
monolingual tweets in both English and Spanish in
order to model the two languages, and used crowd-
sourcing to annotate tens of thousands of Span-
ish tweets, approximately 11% of which contained
code-switched content. This system was able to
achieve 96.9% word-level accuracy and a 0.936
F-measure in identifying code-switched tweets.
The issue still stands that relatively little code-
switching data, such as that used in Lignos and
63
Marcus? research, is readily available. Even in
their data, the percentage of code-switched tweets
was barely over a tenth of the total test data. There
have been other corpora built, particularly for other
language pairs such as Mandarin-English (Li et
al., 2012; Lyu et al., 2010), but the amount of data
available and the percentage of code-switching data
within that data are not up to the standards of other
areas of the natural language processing field. With
this in mind, we sought to provide corpora for mul-
tiple language pairs, each with a better distribution
of code-switching phenomena.
4 Data Sets
Most of the data for the shared task comes form
Twitter. However, we also collected and annotated
data from other social media sources, including
Facebook, web forums, and blogs. These additional
sources of data were used as the surprise data. In
this section we describe briefly the corpora curated
for the shared task.
Language-pair Training Test Surprise
MAN-EN 1000 313 n/a
MSA-DA 5,838 2332, 1,777 12,017
NEP-EN 9,993 3,018 (2,874) 1,087
SPA-EN 11,400 3,060 (1,626) 1,102
Table 2: Statistics of the shared task data sets
per language pairs. The numbers are according to
what was actually annotated, numbers in parenthe-
sis show what the participating systems were able
to crawl from Twitter. The Surprise genre comes
from various sources, other than Twitter.
Table 2 shows some statistics about the differ-
ent datasets used in this task. We strive to provide
dataset sizes that would allow a robust analysis of
results. However, an unexpected challenge was
the rate at which tweets became unavailable. Dif-
ferent language pairs had different attrition rates
with SPA-EN being the most affected language and
MSA-DA and NEP-EN the least affected. Note
that we provided two test datasets for MSA-DA.
Since we separated the data on a per user basis, the
first test set had a highly skewed distribution. The
second test set was distributed to participants to
allow a comparison with a data set having a class
distribution more similar to the training set.
4.1 SPA-EN data
Developing the corpus involved two primary steps:
locating code-switching tweets and using crowd-
sourcing to annotate their tokens with language
tags. A small portion of the tweets were annotated
in-lab and this was used as the gold data for quality
control in the crowdsourcing annotation.
To avoid biasing the data used in this task, we
used a two step process to select the tweets: first we
identified CS tweets by doing a keyword search on
Twitter?s API. We selected a few frequently used
English words and restricted the search to tweets
identified by Twitter as Spanish from users in Cali-
fornia and Texas. An additional set of tweets was
then collected by using frequent Spanish words in
an all English tweet, from users in the same loca-
tions. We filtered these tweets to remove tweets
containing URLs, duplicates, spam tweets and
retweets.
In-lab annotators labeled the filtered tweets using
the guidelines referenced above. From this set of
labeled data we then ranked the users in this set by
the percentage of CS tweets. We selected the 12
most prolific CS users and then pulled all of their
available tweets. These 12 users contributed the
tweets used in the shared task. The tweets were
labeled using CrowdFlower
3
. After analyzing the
number and content distribution of the tweets, the
SPA-EN data was split into a 11,400 tweet training
set and a 3,014 tweet test set.
The SPA-EN Surprise Genre (SPA-EN-SG) in-
cluded Facebook comments from the Veteranas
community
4
and the Chicanas community
5
and
blog data from the Albino Bean
6
. Data was col-
lected using Python scripts that implemented the
Beautiful Soup library and the third-party Python
Facebook SDK (for Blogger and Facebook respec-
tively). Post and comment IDs were used to iden-
tify Facebook posts, and URLs were used to iden-
tify Blogger posts. The collected posts were format-
ted to match those collected from Twitter. In-lab
annotators were used to annotate approximately 1K
tokens. All the data we collected in this manner
was released as surprise data to all participants.
4.2 NEP-EN data
The collection of NEP-EN data followed a simi-
lar approach to that of SPA-EN. We first focused
on finding users that switched frequently between
3
http://www.crowdflower.com/
4
https://www.facebook.com/
VeteranaPinup
5
https://www.facebook.com/pages/
Chicanas/444483772293893
6
http://thealbinobean.blogspot.com/
64
Nepali and English. In addition, the users must
not be using Devnagari script as done by Nepalese
to write Nepali, but must have used its Roman-
ized form. We started by manually reading tweets
from some of our Nepali friends. We then crawled
their followers who corresponded with them using
code-switched tweets or replies. We found that
a lot of these users were regular code-switchers
themselves. We repeated the same process with the
followers and collected nearly 30 such users. We
then collected about 2,000 tweets each from these
users using the Twitter API. We filtered out all the
retweets and the tweets with URLs, following the
same process that was used for SPA-EN.
For the surprise test data, we crawled code-
switched data from Facebook comments and posts.
We found that most Nepalese comments had a rich
amount of code-switched data. However, we could
not crawl their data because of privacy issues. Nev-
ertheless, we could crawl data from public Face-
book pages. We identified some public Nepali Face-
book pages where anyone could comment. These
pages include FM, news and public figures? public
Facebook pages. We crawled the latest 10 feeds
from these public pages using the Facebook API
and gathered about 12,000 comments and posts for
the shared task.
Initially, we sought out help from Nepali gradu-
ate students at the University of Alabama at Birm-
ingham to annotate 100 tweets (1739 tokens). We
gave the same annotation file to two annotators to
do the annotation. We found that they agreed with
an accuracy of 95.34%. These tweets were then re-
viewed and used as initial gold data in Crowdflower
to annotate the first 1000 tweets. The annotation
job was enabled only in Nepal and Bhutan. We
disabled India, even though people living in some
regions of India (Darjeeling, Sikkim) also speak
and write in Nepali, as most spammers were com-
ing from India. We then ran two batches of 5000
tweets and one batch of 3000 tweets along with the
initial 1,000 tweets as the gold data. This NEP-EN
data was then split into a 9,993 tweet training set
and a 2,874 tweet test set. No Twitter user appeared
in both sets.
4.3 MAN-EN data
The MAN-EN tweets were collected from Twitter
with the Twitter API. Users were selected from
lists of most followed Twitter accounts in Taiwan
(where Mandarin Chinese is the official language).
These users? tweets were checked for Mandarin En-
glish bilingualism and added to our data collection
if they contained both languages.
The next round of usernames came from the
lists of users that our original top accounts were
following. The tweets written by this new set of
users were then examined for Mandarin English
code switching and stored as data if they matched
the criteria.
The jieba tokenizer
7
was used to segment the
Mandarin sections of the tweets and compute off-
sets of each segment. We format the code switch-
ing tweets into columns including language type,
labels, and offsets. Named entities were labeled
manually by a single annotator.
The data was split by user into 1000 tweets for
training and 313 for testing. No MAN-EN surprise
data for the current shared task.
4.4 MSA-DA data
For the MSA-DA language pair, we selected Egyp-
tian Arabic (EGY) as the Arabic dialect. We har-
vested data from two social media sources: Twitter
[TWT] and Blog commentaries [COM]. The TWT
data served as the main gold standard data for the
task where we provided fully annotated data for
Training/Tuning and Test. We provided two TWT
data sets for the test data that exemplified different
tag distributions. The COM data set comprised
only test data and it served as the Arabic surprise
data set.
To reduce the potential of TWT data attrition
from users deleting their accounts or tweets, we
selected tweets that are less prone to deletion and/or
change. Thereby we harvested tweets by a select
set of Egyptian Public Figures. The percentage
of deleted tweets and deactivated accounts among
those users is significantly lower if we compare it
to the tweets crawled from random Egyptian users.
We used the ?Tweepy? library to crawl the time-
lines of 12 Public Figures. Similar to other lan-
guage pairs, we excluded all re-tweets, tweets with
URLs, tweets mentioning other users, and tweets
containing Latin characters. We accepted 9,947
tweets, for each we extracted the tweet-id and user-
id. Using these IDs, we retrieved the tweets text,
tokenized it and assigned character offsets. To guar-
antee consistency and avoid any misalignment is-
sues, we compiled the full pipeline into the ?Arabic
Tweets Token Assigner? package which is made
7
https://github.com/fxsjy/jieba
65
available through the workshop website
8
.
For COM, we selected 6723 commentaries (half
MSA and half DA) from ?youm7?
9
commen-
taries provided by the Arabic Online Commentary
Dataset (Zaidan and Callison-Burch, 2011). The
COM data set was processed (12017 total tokens)
using the same pipeline created for the task. We
also provided the participants with the data format-
ted with character offsets to maintain consistency
across data sets in the Arabic subtask.
The annotation of MSA-DA language pair data
is based on two sets of guidelines. The first set
is a generic set of guidelines for code switching
in general across different language pairs. These
guidelines provide the overarching framework for
annotating code switched data on the morpholog-
ical, lexical, syntactic, and pragmatic levels. The
second set of guidelines is language pair specific.
We created the guidelines for the Arabic language
specifically. We enlisted the help of 3 annotators
in addition to a super annotator, hence resulting
in 4 annotators overall for the whole collection of
the data. All the annotators are native speakers
of Egyptian Arabic with excellent proficiency in
MSA. The super annotator only annotated 10% of
the overall data and served as the adjudicator. The
annotation process was iterative with several repe-
titions of the cycle of training, annotation, revision,
adjudication until we approached a stable Inter An-
notator Agreement (IAA) of over 90% pairwise
agreement.
5 Survey of Shared Task Systems
We received submissions from seven different
teams. Each participating system had the freedom
to submit responses to any of the language pairs
covered in the shared task. All seven participants
submitted system responses for SPA-EN, making
this language pair the most popular in this shared
task and MAN-EN the least popular.
All but one participating system used a machine
learning algorithm or language models, or even a
combination of both, as part of their configuration.
A couple of the participating systems used hand-
crafted rules of some sort, either at the intermediate
steps or as the final post-processing step. We also
observed a good number of systems using exter-
nal resources, in the form of labeled monolingual
8
http://emnlp2014.org/workshops/
CodeSwitch/call.html
9
An Egyptian newspaper, www.youm7.com
corpora, language specific gazetteers, off the shelf
tools (NE recognizers, language id systems, or mor-
phological analyzers) and even unsupervised data
crawled from the same users present in the data
sets provided. Affixes were also used in some form
by different systems.
The architecture of the different systems ranged
from a simple approach based on frequencies of
character n-grams combined in a rule-based system,
to more complex approaches using word embed-
dings, extended Markov Models, and CRF autoen-
coders. The majority of the systems that partici-
pated in more than one language pair did little to no
customization to account for the morphological dif-
ferences of the specific language pairs beyond lan-
guage specific parameter-tuning, which probably
reflects participants? goal to develop a multilingual
id system.
Due to the presence of the NE label, several
systems included a component for NE recognition
where there was one available for the specific lan-
guage. In addition, many systems also included
case information. One unexpected finding from
the shared task was that no participating system
tried to embed in their models some form of lin-
guistic theory or framework about CS. Only one
system made an explicit reference to CS theories
(Chittaranjan et al., 2014) in their motivation to use
contextual information, which can be considered
as a loose embedding of CS theory. While system
performance was competitive (see next section),
there is still room for improvement and perhaps
some of that improvement can come out of adding
this kind of knowledge into the models. Lastly, we
were surprised to see that not all systems made use
of character encoding information, even though for
Mandarin-English that would have been a strong
indicator. In Table 3 we present a summary high-
lighting some of the design choices of participating
systems.
6 Results
We used the following evaluation metrics: Accu-
racy, Precision, Recall, and F-measure. We use
F-measure to provide a ranking of systems. In the
evaluation at the tweet level we use the standard
f-measure. For the evaluation at the token level
we use instead the average weighted f-measure to
account for the highly imbalanced distribution of
classes.
To provide a fair evaluation, we only scored pre-
66
System
Machine
Learning
Rules Case
Character
Encoding
External Resources LM Affixes Context
(Chittaranjan et al., 2014)
CRF
4
4 dbpedia dumps, online sources
? 3
(Shrestha, 2014) 4
4 spell checker
(Jain and Bhat, 2014)
CRF
4
4 English dictionary
4 4 ? 2
(King et al., 2014)
eMM
ANERgazet, TwitterNLP, Stan-
ford NER
4 4 4
(Bar and Dershowitz, 2014)
SVM
4
Illocution Twitter Lexicon,
monolingual corpora (NE lists)
4 4 ? 2
(Lin et al., 2014)
CRF
4
4
Hindi-Nepali Wikipedia, JRC,
CoNLL 2003 shared task, lang
id predictors: cld2 and ldig
4 4
(Barman et al., 2014)
kNN, SVM
4 4
BNC, LexNorm
4 ? 1
Table 3: Comparison of shared task participating system algorithm choices. CRF stands for Conditional
Random Fields, SVM for Support Vector Machines and LM for Language Models.
dictions on tweets submitted by all teams. All
systems were compared to a simple lexicon-based
baseline. The lexicon was gathered from the train-
ing data for classes lang1 and lang2 only. Emoti-
cons, punctuation marks, usernames and URLs are
by default tagged as other. In the case of a tie or a
new token, the baseline system assigns the majority
class for that language pair.
Figure 1 shows prediction performance on the
Twitter test data for each language pair at the tweet
level. The system predictions for this task are taken
directly from the individual token predictions in
the following manner: if the system predictions for
the same tweet contain at least one tag from each
language (lang1 and lang2), the tweet is labeled
as code-switched, otherwise it is labeled as mono-
lingual. As illustrated, each language pair shows
different patterns. Comparing the systems that par-
ticipated in all language pairs, there is no clear
winner across the board. However, (Chittaranjan et
al., 2014) was in the top three places in at least one
test file for each language pair. Table 4 shows the
results at the token level by label. Here again the
figures show F-measure per class label and the last
column is the weighted average f-measure (Avg-F).
One of the few general trends on these results is
that most participating systems were not able to
correctly identify the minority classes ?ambiguous?
and ?other?. There are only few instances of these
labels in the training set and some test sets did not
have one of these classes present. The impact on
final system performance from these classes is not
significant. However, to study CS patterns we will
need to have these labels identified properly.
The MAN-EN pair received four system re-
sponses and all four of them reached an F-measure
>80% and outperformed the simple baseline by a
considerable margin. We expected this language
pair to be the easiest one for the shared task since
each language uses a different encoding script. A
very rough but accurate distinction between Man-
darin and English could be achieved by looking
at the character encoding. However, according to
the system descriptions provided, not all systems
used encoding information. The best performing
systems for MAN-EN are (King et al., 2014) and
(Chittaranjan et al., 2014). The former slightly
outperformed the latter at the Tweet level (see Fig-
ure 1a) task while the opposite was true at the token
level (see Table 4 rows 4 and 5).
In the case of SPA-EN, all seven systems out-
performed the simple baseline. The best perform-
ing system in all SPA-EN tasks was (Bar and
Dershowitz, 2014). This system achieved an F-
measure of 82.2%, 2.9 percentage points above the
second best system (Lin et al., 2014) on the tweet
level task (see Figure 1(d)). In the token level
evaluation, (Bar and Dershowitz, 2014) reached
an Avg. F-measure of 94%. This top performing
system uses a sequential classification approach
where the labels from the preceding words are used
as features in the model. Another design choice
that might have given the edge to this system is the
fact that their model combines character- and word-
based language models in what the authors call
?intra- and inter-word level? features. Both types
of language models are trained on large amounts
of monolingual data and NE lists, which again pro-
vides additional knowledge that other systems are
not exploiting. For instance, the NE lexicons might
account for the best results in the NE class in both
the Twitter data and the Surprise genre (see Table 4
last row for SPA-EN and second to last for SPA-
EN Surprise). Most systems showed considerable
67
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Chittaranjan
et al., 2014)
(King et
al., 2014)
0.6
0.7
0.8
0.9
1
0.838
0.888
0.892
0.894
F
-
m
e
a
s
u
r
e
Baseline
(a) MAN-EN
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Elfardy et
al., 2014)
(Lin et
al., 2014)
0.1
0.2
0.3
0.4
0.152
0.118
0.048
0.044
0.095
0.196
0.260
0.338
0.360
0.417
F
-
m
e
a
s
u
r
e
Baseline Test1
Baseline Test2
(b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2
(King et
al., 2014)
(Lin et
al., 2014)
(Jain and
Bhat, 2014)
(Shrestha,
2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
0.8
0.9
1
0.952
0.962
0.972
0.974
0.975
0.977
F
-
m
e
a
s
u
r
e
Baseline
(c) NEP-EN
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.6
0.7
0.8
0.9
1
0.634
0.703
0.753
0.754
0.783
0.793
0.822
F
-
m
e
a
s
u
r
e
Baseline
(d) SPA-EN
Figure 1: Prediction results on language identification at the tweet level. This is a binary task to distinguish
between a monolingual and a CS tweet. We show performance of participating systems using F-measure
as the evaluation metric. The solid line shows the lexicon baseline performance.
differences in prediction performance in both gen-
res. In all cases the Avg. F-measure was higher
on the Twitter test data than on the surprise genre.
Although the surprise genre is too small to draw
strong conclusions, all language pairs with surprise
genre test data showed a decrease in performance
of around 10%.
We analyzed system outputs and found some
consistent sources of error. Lexical forms that exist
in both languages were frequently mislabeled by
68
most systems. For example the word for ?he? was
frequently mislabeled by at least one system. In
most of the cases systems were predicting EN as
label when the target language was SPA. Cases like
this were even more prone to errors when these
words fell in the CS point, as in this tweet: ni el
header he hecho (I haven?t even done the header).
Tweets like this one, with just one token from the
other language, were difficult for most systems.
Named entities were also frequent sources of error,
especially when they were spelled with lower cases
letters.
By far the hardest language pair in this shared
task was MSA-DA, as anticipated. Especially when
considering the typological similarities between
MSA and DA. This is mainly due to the fact that
DA and MSA are close variants of one another and
hence they share considerable amount of lexical
items. The shared lexical items could be simple
cognates of one another, or faux amis where they
are homographs or homophones, but have com-
pletely different meaning. Both categories con-
stitute a significant challenge. Accordingly, the
baseline system had the lowest performance from
all language pairs in both test sets. We note chal-
lenges in this language pair on each linguistic level
where CS occurs especially for the shared lexical
items.
On the phonological level, DA writers tend to
mimic the MSA script for DA words even if they
are pronounced differently. For example: ?heart? is
pronounced in DA Alob and in MSA as qalob but
commonly written in MSA as ?qalob? in DA data.
Also many phonological differences are in short
vowels that are underspecified in written Arabic,
adding another layer of ambiguity.
On the morphological level, there is no avail-
able morphological analyzer able to recognize such
shared words and hence they are mostly misclassi-
fied. Language identification for MSA-DA CS text
highly depends on the context. Typically some Ara-
bic variety word serves as a marker for a context
switch such as mElh$ for DA or mn? for MSA. But
if shared lexical items are used, it is challenging
to identify the Arabic variant. An example from
the training data is qlb meaning either heart as a
noun or change as a verb in the phrase lw qlb mjrm,
corresponding to ?If the heart of a criminal? or ?if
he changes into a criminal?. These challenges ren-
der language identification for CS MSA-DA data
far from solved as evident by the fact that the high-
est scoring system reached an F-measure of only
41.7% in Test2 for CS identification. Moreover,
this is the only language pair where at least one
system was not able to outperform the baseline and
in the case of Test2 only one system (Lin et al.,
2014) outperformed the baseline.
Most teams did well for the NEP-EN shared task,
and all teams outperformed the baseline. The rea-
son for the high performance might be the high
number of codeswitched tweets in the training and
test data for NEP-EN (much higher than other lan-
guage pairs). This allowed systems to have more
samples of CS instances. The other reason for good
performance by most participants in both evalua-
tions might be that Nepali and English are two very
different languages. The structure of the words and
syntax of word formation are very different. We
suspect, for instance, that there is a much lower
overlap of character n-grams in this language pair
than in SPA-EN, which makes for an easier task. At
the Tweet level, system performance ranged over
a small set of values, the lowest F-measure was
95.2% while the highest was 97.7%. Looking at
the numbers in Table 4, we can see that even NE
recognition seemed to be a much easier task for this
language pair than for SPA-EN (compare results
for the NE category in both SPA-EN sets to those
of both NEP-EN data sets). The best performing
system for the Twitter test data is (Barman et al.,
2014) with an F-measure of 97.7%. The results
trend in the surprise genre is not consistent with
what we observed for the Twitter test data. The
top ranked system for Twitter sunk to the 4th place
with an F-measure or 59.6%, a considerable drop
of almost 40 percentage points. In this case, the
overall numbers indicate a much wider difference
in the genres than what we observed for other lan-
guages, such as SPA-EN, for example. We should
note that the class distribution in the surprise data is
considerably different from what the models used
for training, and from that of the test data as well.
In the Twitter data there was a larger number of CS
tweets than monolingual ones, while in the surprise
genre the majority class was monolingual. This
will account for a good portion of the differences
in performance. But here as well, the small number
of labeled instances makes it hard to draw strong
conclusions.
69
Test Set System lang1 lang2 NE other ambiguous mixed Avg-F
MAN-EN
Baseline 0.9 0.47 0 0.29 - 0 0.761
(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871
(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886
(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884
(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892
MSA-DA Test 1
(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720
Baseline 0.92 0.06 0 0.89 0 - 0.819
(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898
(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909
(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922
(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936
MSA-DA Test 2
Baseline 0.54 0.27 0 0.94 0 0 0.385
(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477
(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513
(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580
(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777
(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799
MSA-DA Surprise
(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467
(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626
(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654
(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778
(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801
NEP-EN
Baseline 0.67 0.76 0 0.61 - 0 0.678
(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707
(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917
(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942
(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944
(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948
(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959
NEP-EN Surprise
(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712
(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761
(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796
(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850
(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853
(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855
SPA-EN
Baseline 0.72 0.56 0 0.75 0 0 0.704
(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873
(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905
(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913
(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921
(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923
(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926
(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940
SPA-EN Surprise
(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778
(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811
(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816
(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823
(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824
(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828
(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839
Table 4: Performance results on language identification at the token level. A ?-? indicates there were no
tokens of this class in the test set. We ranked systems using weighted averaged f-measure (Avg-F). The ?*?
marks the system by (Elfardy et al., 2014). This system was not considered in the ranking for the shared
task as it was developed by co-organizers of the task.
7 Lessons Learned
Among the things we want to improve for future
shared tasks is the issue of data loss due to re-
moval of tweets or users deleting their accounts.
We decided to use Twitter data to have a relevant
corpus. However, the trade-off is the lack of rights
to distribute the data ourselves. This is not just a
burden for the participants. It is an awful waste of
resources as the data that was expensive to gather
and label is not being used beyond the small group
of researchers involved in the creation of the cor-
pus. This will deter us from using Twitter data for
future shared tasks, at least until a better solution
is identified.
70
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Elfardy et
al., 2014)
0
0.1
0.2
0.3
0.170
0.194
0.222
0.276
0.277
F
-
m
e
a
s
u
r
e
(a) MSA-DA Surprise Genre Results
(Chittaranjan
et al., 2014)
(Jain and
Bhat, 2014)
(Barman et
al., 2014)
(King et
al., 2014)
(Shrestha,
2014)
(Lin et
al., 2014)
0.4
0.5
0.6
0.7
0.554
0.571
0.596
0.604
0.632
0.702
F
-
m
e
a
s
u
r
e
(b) NEP-EN Surprise Genre Results
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.4
0.5
0.6
0.7
0.8
0.633
0.640
0.704
0.710
0.725
0.727
0.753
F
-
m
e
a
s
u
r
e
(c) SPA-EN Surprise Genre Results
Figure 2: Prediction results on language identification at the document level for the surprise genre. This
is a binary task to distinguish between a monolingual and a code-switched text. We show performance of
participating systems using F-measure as the evaluation metric.
Using crowdsourcing for annotating the data is a
cheap and easy way for generating resources. But
we found out that even when following best prac-
tices for quality control, there was a substantial
amount of noise in the gold data. We plan to con-
tinue working on refining the annotation guidelines
and quality control processes to reduce the amount
of noise in gold annotations.
8 Conclusion
This is the first shared task on language identifica-
tion in CS data. Yet, the response was quite positive
as we received 42 system runs from seven different
teams, plus submissions for MSA-AD from a sub
group of the task organizers (Elfardy et al., 2014).
The systems presented are overall robust and with
interesting differences from one another. Although
we did not see a single system ranking in the top
places across all language pairs and tasks, we did
see systems showing robust performance indicat-
ing some level of language independence. But the
results are not consistent at the tweet/document
level. The language pair that proved to be the most
difficult for the task was MSA-DA, where the lexi-
con baseline system was hard to beat even with an
F-measure of 47.1%.
This shared task showed that language identifica-
tion in code-switched data is still an open problem
that warrants further investigation. Perhaps in the
near future we will see systems that embed some
form of linguistic theory about CS and maybe that
would result in more accurate predictions.
Our goal is to support new research addressing
CS data. Discussions about the challenge for the
next shared task are already underway. One pos-
sibility might be parsing. We plan to investigate
the challenges in parsing CS data and we will start
by exploring the hardships in manually annotating
CS with syntactic information. We would also like
to explore the possibility of classifying CS points
according to their socio-pragmatic role.
71
Acknowledgments
We would like to thank all shared task partici-
pants. We also thank Brian Hester and Mohamed
Elbadrashiny for their invaluable support in the
development of the gold standard data and analy-
sis of results. We also thank the in-lab annotators
and the CrowdFlower contributors. This work was
partly funded by NSF under awards 1205475 and
1205556.
References
Kfir Bar and Nachum Dershowitz. 2014. Tel Aviv Uni-
versity system description for the code-switching
workshop shared task. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Utsab Barman, Joachim Wagner, Grzegorz Chrupala,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. A framework to label
code-mixed sentences in social media. In Proceed-
ings of the First Workshop on Computational Ap-
proaches to Code-Switching, Doha, Qatar, October.
ACL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying code switching in
informal Arabic text. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Naman Jain and Riyaz Ahmad Bhat. 2014. Language
identification in codeswitching scenario. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
A. Joshi. 1982. Processing of sentences with in-
trasentential code-switching. In J?an Horeck?y, editor,
COLING-82, pages 145?150, Prague, July.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Levi King, Eric Baucom, Tim Gilmanov, Sandra
K?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-
drigues. 2014. The IUCL+ system: Word-level
language identification via extended Markov models.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
Mandarin-English code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL An-
thology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2014. The CMU submission for the shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching, Doha,
Qatar, October. ACL.
D.C. Lyu, T.P. Tan, E. Chng, and H. Li. 2010. SEAME:
a Mandarin-English code-switching speech corpus
in South-East Asia. In INTERSPEECH, volume 10,
pages 1986?1989.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 857?862, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Prajwol Shrestha. 2014. An incremental approach for
language identification in codeswitched text. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identifi-
cation of languages and encodings in a multilingual
document. In Proceedings of ACL-SIGWAC?s Web
As Corpus3, Belgium.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.
Association for Computational Linguistics.
72
