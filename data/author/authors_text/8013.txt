Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 141?148, New York City, June 2006. c?2006 Association for Computational Linguistics
A Context Pattern Induction Method for Named Entity Extraction
Partha Pratim Talukdar
CIS Department
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Thorsten Brants
Google, Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043
brants@google.com
Mark Liberman Fernando Pereira
CIS Department
University of Pennsylvania
Philadelphia, PA 19104
{myl,pereira}@cis.upenn.edu
Abstract
We present a novel context pattern in-
duction method for information extrac-
tion, specifically named entity extraction.
Using this method, we extended several
classes of seed entity lists into much larger
high-precision lists. Using token member-
ship in these extended lists as additional
features, we improved the accuracy of a
conditional random field-based named en-
tity tagger. In contrast, features derived
from the seed lists decreased extractor ac-
curacy.
1 Introduction
Partial entity lists and massive amounts of unla-
beled data are becoming available with the growth
of the Web as well as the increased availability of
specialized corpora and entity lists. For example,
the primary public resource for biomedical research,
MEDLINE, contains over 13 million entries and is
growing at an accelerating rate. Combined with
these large corpora, the recent availability of entity
lists in those domains has opened up interesting op-
portunities and challenges. Such lists are never com-
plete and suffer from sampling biases, but we would
like to exploit them, in combination with large un-
labeled corpora, to speed up the creation of infor-
mation extraction systems for different domains and
languages. In this paper, we concentrate on explor-
ing utility of such resources for named entity extrac-
tion.
Currently available entity lists contain a small
fraction of named entities, but there are orders of
magnitude more present in the unlabeled data1. In
this paper, we test the following hypotheses:
i. Starting with a few seed entities, it is possible
to induce high-precision context patterns by ex-
ploiting entity context redundancy.
ii. New entity instances of the same category can
be extracted from unlabeled data with the in-
duced patterns to create high-precision exten-
sions of the seed lists.
iii. Features derived from token membership in the
extended lists improve the accuracy of learned
named-entity taggers.
Previous approaches to context pattern induc-
tion were described by Riloff and Jones (1999),
Agichtein and Gravano (2000), Thelen and Riloff
(2002), Lin et al (2003), and Etzioni et al (2005),
among others. The main advance in the present
method is the combination of grammatical induction
and statistical techniques to create high-precision
patterns.
The paper is organized as follows. Section 2 de-
scribes our pattern induction algorithm. Section 3
shows how to extend seed sets with entities extracted
by the patterns from unlabeled data. Section 4 gives
experimental results, and Section 5 compares our
method with previous work.
1For example, based on approximate matching, there is an
overlap of only 22 organizations between the 2403 organiza-
tions present in CoNLL-2003 shared task training data and the
Fortune-500 list.
141
2 Context Pattern Induction
The overall method for inducing entity context pat-
terns and extending entity lists is as follows:
1. Let E = seed set, T = text corpus.
2. Find the contexts C of entities in E in the cor-
pus T (Section 2.1).
3. Select trigger words from C (Section 2.2).
4. For each trigger word, induce a pattern automa-
ton (Section 2.3).
5. Use induced patterns P to extract more entities
E? (Section 3).
6. Rank P and E? (Section 3.1).
7. If needed, add high scoring entities in E? to E
and return to step 2. Otherwise, terminate with
patterns P and extended entity list E ? E? as
results.
2.1 Extracting Context
Starting with the seed list, we first find occurrences
of seed entities in the unlabeled data. For each such
occurrence, we extract a fixed number W (context
window size) of tokens immediately preceding and
immediately following the matched entity. As we
are only interested in modeling the context here, we
replace all entity tokens by the single token -ENT-.
This token now represents a slot in which an entity
can occur. Examples of extracted entity contexts are
shown in Table 1. In the work presented in this pa-
pers, seeds are entity instances (e.g. Google is a seed
for organization category).
increased expression of -ENT- in vad mice
the expression of -ENT- mrna was greater
expression of the -ENT- gene in mouse
Table 1: Extracted contexts of known genes with
W = 3.
The set of extracted contexts is denoted by C . The
next step is to automatically induce high-precision
patterns containing the token -ENT- from such ex-
tracted contexts.
2.2 Trigger Word Selection
To induce patterns, we need to determine their starts.
It is reasonable to assume that some tokens are more
specific to particular entity classes than others. For
example, in the examples shown above, expression
can be one such word for gene names. Whenever
one comes across such a token in text, the proba-
bility of finding an entity (of the corresponding en-
tity class) in its vicinity is high. We call such start-
ing tokens trigger words. Trigger words mark the
beginning of a pattern. It is important to note that
simply selecting the first token of extracted contexts
may not be a good way to select trigger words. In
such a scheme, we would have to vary W to search
for useful pattern starts. Instead of that brute-force
technique, we propose an automatic way of select-
ing trigger words. A good set of trigger words is
very important for the quality of induced patterns.
Ideally, we want a trigger word to satisfy the follow-
ing:
? It is frequent in the set C of extracted contexts.
? It is specific to entities of interest and thereby
to extracted contexts.
We use a term-weighting method to rank candi-
date trigger words from entity contexts. IDF (In-
verse Document Frequency) was used in our experi-
ments but any other suitable term-weighting scheme
may work comparably. The IDF weight fw for a
word w occurring in a corpus is given by:
fw = log
( N
nw
)
where N is the total number of documents in the
corpus and nw is the total number of documents con-
taining w. Now, for each context segment c ? C , we
select a dominating word dc given by
dc = argmaxw?c fw
There is exactly one dominating word for each
c ? C . All dominating words for contexts in C form
multiset M . Let mw be the multiplicity of the dom-
inating word w in M . We sort M by decreasing mw
and select the top n tokens from this list as potential
trigger words.
142
Selection criteria based on dominating word fre-
quency work better than criteria based on simple
term weight because high term weight words may
be rare in the extracted contexts, but would still be
misleadingly selected for pattern induction. This can
be avoided by using instead the frequency of domi-
nating words within contexts, as we did here.
2.3 Automata Induction
Rather than using individual contexts directly, we
summarize them into automata that contain the most
significant regularities of the contexts sharing a
given trigger word. This construction allows us to
determine the relative importance of different con-
text features using a variant of the forward-backward
algorithm from HMMs.
2.3.1 Initial Induction
For each trigger word, we list the contexts start-
ing with the word. For example, with ?expression?
as the trigger word, the contexts in Table 1 are re-
duced to those in Table 2. Since ?expression? is a
left-context trigger word, only one token to the right
of -ENT- is retained. Here, the predictive context
lies to the left of the slot -ENT- and a single to-
ken is retained on the right to mark the slot?s right
boundary. To model predictive right contexts, the to-
ken string can be reversed and the same techniques
as here applied on the reversed string.2
expression of -ENT- in
expression of -ENT- mrna
expression of the -ENT- gene
Table 2: Context segments corresponding to trigger
word ?expression?.
Similar contexts are prepared for each trigger
word. The context set for each trigger word is then
summarized by a pattern automaton with transitions
that match the trigger word and also the wildcard
-ENT- . We expect such automata to model the po-
sition in context of the entity slot and help us extract
more entities of the same class with high precision.
2Experiments reported in this paper use predictive left con-
text only.
10
11
12
of
of
of
the
the
a
...
...
a
Figure 1: Fragment of a 1-reversible automaton
We use a simple form of grammar induction to
learn the pattern automata. Grammar induction tech-
niques have been previously explored for informa-
tion extraction (IE) and related tasks. For instance,
Freitag (1997) used grammatical inference to im-
prove precision in IE tasks.
Context segments are short and typically do not
involve recursive structures. Therefore, we chose to
use 1-reversible automata to represent sets of con-
texts. An automaton A is k-reversible iff (1) A is
deterministic and (2) Ar is deterministic with k to-
kens of lookahead, where Ar is the automaton ob-
tained by reversing the transitions of A. Wrapper in-
duction using k-reversible grammar is discussed by
Chidlovskii (2000).
In the 1-reversible automaton induced for each
trigger word, all transitions labeled by a given token
go to the same state, which is identified with that
token. Figure 1 shows a fragment of a 1-reversible
automaton. Solan et al (2005) describe a similar au-
tomaton construction, but they allow multiple transi-
tions between states to distinguish among sentences.
Each transition e = (v,w) in a 1-reversible au-
tomaton A corresponds to a bigram vw in the con-
texts used to create A. We thus assign each transition
the probability
P (w|v) = C(v,w)?w?C(v,w?)
where C(v,w) is the number of occurrences of the
bigram vw in contexts for W . With this construc-
tion, we ensure words will be credited in proportion
to their frequency in contexts. The automaton may
overgenerate, but that potentially helps generaliza-
tion.
143
2.3.2 Pruning
The initially induced automata need to be pruned
to remove transitions with weak evidence so as to
increase match precision.
The simplest pruning method is to set a count
threshold c below which transitions are removed.
However, this is a poor method. Consider state 10 in
the automaton of Figure 2, with c = 20. Transitions
(10, 11) and (10, 12) will be pruned. C(10, 12)  c
but C(10, 11) just falls short of c. However, from
the transition counts, it looks like the sequence ?the
-ENT-? is very common. In such a case, it is not
desirable to prune (10, 11). Using a local threshold
may lead to overpruning.
We would like instead to keep transitions that are
used in relatively many probable paths through the
automaton. The probability of path p is P (p) =
?
(v,w)?p P (w|v). Then the posterior probability of
edge (v,w) is
P (v,w) =
?
(v,w)?p P (p)
?
p P (p)
,
which can be efficiently computed by the forward-
backward algorithm (Rabiner, 1989). We can now
remove transitions leaving state v whose posterior
probability is lower than pv = k(maxw P (v,w)),
where 0 < k ? 1 controls the degree of pruning,
with higher k forcing more pruning. All induced and
pruned automata are trimmed to remove unreachable
states.
10
11
12
of
of
of
the
the
an
 (98)
13a
an
... (40)
... (7)
(5)
(80)
(18)
(40)(20)
(20)
(20)
(2)
-ENT-
Figure 2: Automaton to be pruned at state 10. Tran-
sition counts are shown in parenthesis.
3 Automata as Extractor
Each automaton induced using the method described
in Sections 2.3-2.3.2 represents high-precision pat-
terns that start with a given trigger word. By scan-
ning unlabeled data using these patterns, we can ex-
tract text segments which can be substituted for the
slot token -ENT-. For example, assume that the in-
duced pattern is ?analyst at -ENT- and? and that
the scanned text is ?He is an analyst at the Univer-
sity of California and ...?. By scanning this text us-
ing the pattern mentioned above, we can figure out
that the text ?the University of California? can sub-
stitute for ?-ENT-?. This extracted segment is a
candidate extracted entity. We now need to decide
whether we should retain all tokens inside a candi-
date extraction or purge some tokens, such as ?the?
in the example.
One way to handle this problem is to build a
language model of content tokens and retain only
the maximum likelihood token sequence. However,
in the current work, the following heuristic which
worked well in practice is used. Each token in the
extracted text segment is labeled either keep (K) or
droppable (D). By default, a token is labeled K. A
token is labeled D if it satisfies one of the droppable
criteria. In the experiments reported in this paper,
droppable criteria were whether the token is present
in a stopword list, whether it is non-capitalized, or
whether it is a number.
Once tokens in a candidate extraction are labeled
using the above heuristic, the longest token sequence
corresponding to the regular expression K[D K]?K is
retained and is considered a final extraction. If there
is only one K token, that token is retained as the fi-
nal extraction. In the example above, the tokens are
labeled ?the/D University/K of/D California/K?, and
the extracted entity will be ?University of Califor-
nia?.
To handle run-away extractions, we can set a
domain-dependent hard limit on the number of to-
kens which can be matched with ?-ENT-?. This
stems from the intuition that useful extractions are
not very long. For example, it is rare that a person
name longer than five tokens.
3.1 Ranking Patterns and Entities
Using the method described above, patterns and
the entities extracted by them from unlabeled data
are paired. But both patterns and extractions vary
in quality, so we need a method for ranking both.
Hence, we need to rank both patterns and entities.
This is difficult given that there we have no nega-
144
tive labeled data. Seed entities are the only positive
instances that are available.
Related previous work tried to address this prob-
lem. Agichtein and Gravano (2000) seek to extract
relations, so their pattern evaluation strategy consid-
ers one of the attributes of an extracted tuple as a
key. They judge the tuple as a positive or a negative
match for the pattern depending on whether there are
other extracted values associated with the same key.
Unfortunately, this method is not applicable to entity
extraction.
The pattern evaluation mechanism used here is
similar in spirit to those of Etzioni et al (2005) and
Lin et al (2003). With seeds for multiple classes
available, we consider seed instances of one class
as negative instances for the other classes. A pat-
tern is penalized if it extracts entities which belong
to the seed lists of the other classes. Let pos(p) and
neg(p) be respectively the number of distinct pos-
itive and negative seeds extracted by pattern p. In
contrast to previous work mentioned above, we do
not combine pos(p) and neg(p) to calculate a single
accuracy value. Instead, we discard all patterns p
with positive neg(p) value, as well as patterns whose
total positive seed (distinct) extraction count is less
than certain threshold ?pattern. This scoring is very
conservative. There are several motivations for such
a conservative scoring. First, we are more interested
in precision than recall. We believe that with mas-
sive corpora, large number of entity instances can
be extracted anyway. High accuracy extractions al-
low us to reliably (without any human evaluation)
use extracted entities in subsequent tasks success-
fully (see Section 4.3). Second, in the absence of
sophisticated pattern evaluation schemes (which we
are investigating ? Section 6), we feel it is best to
heavily penalize any pattern that extracts even a sin-
gle negative instance.
Let G be the set of patterns which are retained
by the filtering scheme described above. Also, let
I(e, p) be an indicator function which takes value 1
when entity e is extracted by pattern p and 0 other-
wise. The score of e, S(e), is given by
S(e) = ?p?GI(e, p)
This whole process can be iterated by includ-
ing extracted entities whose score is greater than or
equal to a certain threshold ?entity to the seed list.
4 Experimental Results
For the experiments described below, we used 18
billion tokens (31 million documents) of news data
as the source of unlabeled data. We experimented
with 500 and 1000 trigger words. The results pre-
sented were obtained after a single iteration of the
Context Pattern Induction algorithm (Section 2).
4.1 English LOC, ORG and PER
For this experiment, we used as seed sets subsets of
the entity lists provided with CoNLL-2003 shared
task data.3 Only multi-token entries were included
in the seed lists of respective categories (location
(LOC), person (PER) & organization (ORG) in this
case). This was done to partially avoid incorrect
context extraction. For example, if the seed entity is
?California?, then the same string present in ?Uni-
versity of California? can be incorrectly considered
as an instance of LOC. A stoplist was used for drop-
ping tokens from candidate extractions, as described
in Section 3. Examples of top ranking induced pat-
terns and extracted entities are shown in Table 9.
Seed list sizes and experimental results are shown
in Table 3. The precision numbers shown in Table 3
were obtained by manually evaluating 100 randomly
selected instances from each of the extended lists.
Category Seed
Size
Patterns
Used
Extended
Size
Precision
LOC 379 29 3001 70%
ORG 1597 276 33369 85%
PER 3616 265 86265 88%
Table 3: Results of LOC, ORG & PER entity list ex-
tension experiment with ?pattern = 10 set manually.
The overlap4 between the induced ORG list and
the Fortune-500 list has 357 organization names,
which is significantly higher than the seed list over-
lap of 22 (see Section 1). This shows that we have
been able to improve coverage considerably.
4.2 Watch Brand Name
A total of 17 watch brand names were used as
seeds. In addition to the pattern scoring scheme
3A few locally available entities in each category were also
added. These seeds are available upon request from the authors.
4Using same matching criteria as in Section 1.
145
of Section 3.1, only patterns containing sequence
?watch? were finally retained. Entities extracted
with ?entity = 2 are shown in Table 5. Extraction
precision is 85.7%.
Corum, Longines, Lorus, Movado, Accutron, Au-
demars Piguet, Cartier, Chopard, Franck Muller,
IWC, Jaeger-LeCoultre, A. Lange & Sohne, Patek
Philippe, Rolex, Ulysse, Nardin, Vacheron Con-
stantin
Table 4: Watch brand name seeds.
Rolex Fossil Swatch
Cartier Tag Heuer Super Bowl
Swiss Chanel SPOT
Movado Tiffany Sekonda
Seiko TechnoMarine Rolexes
Gucci Franck Muller Harry Winston
Patek Philippe Versace Hampton Spirit
Piaget Raymond Weil Girard Perregaux
Omega Guess Frank Mueller
Citizen Croton David Yurman
Armani Audemars Piguet Chopard
DVD DVDs Chinese
Breitling Montres Rolex Armitron
Tourneau CD NFL
Table 5: Extended list of watch brand names after
single iteration of pattern induction algorithm.
This experiment is interesting for several reasons.
First, it shows that the method presented in this pa-
per is effective even with small number of seed in-
stances. From this we conclude that the unambigu-
ous nature of seed instances is much more important
than the size of the seed list. Second, no negative
information was used during pattern ranking in this
experiment. This suggests that for relatively unam-
biguous categories, it is possible to successfully rank
patterns using positive instances only.
4.3 Extended Lists as Features in a Tagger
Supervised models normally outperform unsuper-
vised models in extraction tasks. The downside of
supervised learning is expensive training data. On
the other hand, massive amounts of unlabeled data
are readily available. The goal of semi-supervised
learning to combine the best of both worlds. Recent
research have shown that improvements in super-
vised taggers are possible by including features de-
rived from unlabeled data (Miller et al, 2004; Liang,
2005; Ando and Zhang, 2005). Similarly, automati-
cally generated entity lists can be used as additional
features in a supervised tagger.
System F1 (Precision, Recall)
Florian et al (2003),
best single, no list
89.94 (91.37, 88.56)
Zhang and Johnson
(2003), no list
90.26 (91.00, 89.53)
CRF baseline, no list 89.52 (90.39, 88.66)
Table 6: Baseline comparison on 4 categories (LOC,
ORG, PER, MISC) on Test-a dataset.
For this experiment, we started with a conditional
random field (CRF) (Lafferty et al, 2001) tagger
with a competitive baseline (Table 6). The base-
line tagger was trained5 on the full CoNLL-2003
shared task data. We experimented with the LOC,
ORG and PER lists that were automatically gener-
ated in Section 4.1. In Table 7, we show the accuracy
of the tagger for the entity types for which we had
induced lists. The test conditions are just baseline
features with no list membership, baseline plus seed
list membership features, and baseline plus induced
list membership features. For completeness, we also
show in Table 8 accuracy on the full CoNLL task
(four entity types) without lists, with seed list only,
and with the three induced lists. The seed lists (Sec-
tion 4.1) were prepared from training data itself and
hence with increasing training data size, the model
overfitted as it became completely reliant on these
seed lists. From Tables 7 & 8 we see that incor-
poration of token membership in the extended lists
as additional membership features led to improve-
ments across categories and at all sizes of training
data. This also shows that the extended lists are of
good quality, since the tagger is able to extract useful
evidence from them.
Relatively small sizes of training data pose inter-
esting learning situation and is the case with practi-
cal applications. It is encouraging to observe that the
list features lead to significant improvements in such
cases. Also, as can be seen from Table 7 & 8, these
lists are effective even with mature taggers trained
on large amounts of labeled data.
5Standard orthographic information, such as character n-
grams, capitalization, tokens in immediate context, chunk tags,
and POS were used as features.
146
Training Data Test-a Test-b
(Tokens) No List Seed List Unsup. List No List Seed List Unsup. List
9268 68.16 70.91 72.82 60.30 63.83 65.56
23385 78.36 79.21 81.36 71.44 72.16 75.32
46816 82.08 80.79 83.84 76.44 75.36 79.64
92921 85.34 83.03 87.18 81.32 78.56 83.05
203621 89.71 84.50 91.01 84.03 78.07 85.70
Table 7: CRF tagger F-measure on LOC, ORG, PER extraction.
Training Data Test-a Test-b
(Tokens) No List Seed List Unsup. List No List Seed List Unsup. List
9229 68.27 70.93 72.26 61.03 64.52 65.60
204657 89.52 84.30 90.48 83.17 77.20 84.52
Table 8: CRF tagger F-measure on LOC, ORG, PER and MISC extraction.
5 Related Work
The method presented in this paper is similar in
many respects to some of the previous work on
context pattern induction (Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Lin et al, 2003; Et-
zioni et al, 2005), but there are important differ-
ences. Agichtein and Gravano (2000) focus on rela-
tion extraction while we are interested in entity ex-
traction. Moreover, Agichtein and Gravano (2000)
depend on an entity tagger to initially tag unlabeled
data whereas we do not have such requirement. The
pattern learning methods of Riloff and Jones (1999)
and the generic extraction patterns of Etzioni et al
(2005) use language-specific information (for exam-
ple, chunks). In contrast, the method presented here
is language independent. For instance, the English
pattern induction system presented here was applied
on German data without any change. Also, in the
current method, induced automata compactly repre-
sent all induced patterns. The patterns induced by
Riloff and Jones (1999) extract NPs and that deter-
mines the number of tokens to include in a single
extraction. We avoid using such language dependent
chunk information as the patterns in our case include
right6 boundary tokens thus explicitly specifying the
slot in which an entity can occur. Another interest-
ing deviation here from previous work on context
pattern induction is the fact that on top of extending
6In case of predictive left context.
seed lists at high precision, we have successfully in-
cluded membership in these automatically generated
lexicons as features in a high quality named entity
tagger improving its performance.
6 Conclusion
We have presented a novel language-independent
context pattern induction method. Starting with a
few seed examples, the method induces in an unsu-
pervised way context patterns and extends the seed
list by extracting more instances of the same cat-
egory at fairly high precision from unlabeled data.
We were able to improve a CRF-based high quality
named entity tagger by using membership in these
automatically generated lists as additional features.
Pattern and entity ranking methods need further
investigation. Thorough comparison with previ-
ously proposed methods also needs to be carried out.
Also, it will be interesting to see whether the fea-
tures generated in this paper complement some of
the other methods (Miller et al, 2004; Liang, 2005;
Ando and Zhang, 2005) that also generate features
from unlabeled data.
7 Acknowledgements
We thank the three anonymous reviewers as well as
Wojciech Skut, Vrishali Wagle, Louis Monier, and
Peter Norvig for valuable suggestions. This work is
supported in part by NSF grant EIA-0205448.
147
Induced LOC Patterns
troops in -ENT-to
Cup qualifier against -ENT-in
southern -ENT-town
war - torn -ENT-.
countries including -ENT-.
Bangladesh and -ENT-,
England in -ENT-in
west of -ENT-and
plane crashed in -ENT-.
Cup qualifier against -ENT-,
Extracted LOC Entities
US
United States
Japan
South Africa
China
Pakistan
France
Mexico
Israel
Pacific
Induced PER Patterns
compatriot -ENT-.
compatriot -ENT-in
Rep. -ENT-,
Actor -ENT-is
Sir -ENT-,
Actor -ENT-,
Tiger Woods , -ENT-and
movie starring -ENT-.
compatriot -ENT-and
movie starring -ENT-and
Extracted PER Entities
Tiger Woods
Andre Agassi
Lleyton Hewitt
Ernie Els
Serena Williams
Andy Roddick
Retief Goosen
Vijay Singh
Jennifer Capriati
Roger Federer
Induced ORG Patterns
analyst at -ENT-.
companies such as -ENT-.
analyst with -ENT-in
series against the -ENT-tonight
Today ?s Schaeffer ?s Option Activity Watch features -ENT-(
Cardinals and -ENT-,
sweep of the -ENT-with
joint venture with -ENT-(
rivals -ENT-Inc.
Friday night ?s game against -ENT-.
Extracted ORG Entities
Boston Red Sox
St. Louis Cardinals
Chicago Cubs
Florida Marlins
Montreal Expos
San Francisco Giants
Red Sox
Cleveland Indians
Chicago White Sox
Atlanta Braves
Table 9: Top ranking LOC, PER, ORG induced pattern and extracted entity examples.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
Proceedings of ACL-2005. Ann Arbor, USA.
Boris Chidlovskii. 2000. Wrapper generation by k-
reversible grammar induction. ECAI Workshop on
Machine Learning for Information Extraction.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web - an exper-
imental study. Artificial Intelligence Journal.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
Dayne Freitag. 1997. Using grammatical inference to
improve precision in information extraction. In ICML-
97 Workshop on Automata Induction, Grammatical In-
ference, and Language Acquisition, Nashville.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
ICML 2001.
Percy Liang. 2005. Semi-supervised learning for natural
language. MEng. Thesis, MIT.
Winston Lin, Roman Yangarber, and Ralph Grishman.
2003. Bootstrapped learning of semantic classes from
positive and negative examples. In Proceedings of
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proceedings of HLT-NAACL 2004.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
Proc. of IEEE, 77, 257?286.
Ellen Riloff and Rosie Jones. 1999. Learning Dictio-
naries for Information Extraction by Multi-level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. In Proceedings of National Academy of Sci-
iences. 102:11629-11634.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of EMNLP 2002.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In Proceedings of CoNLL-2003.
148
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1051?1055,
Prague, June 2007. c?2007 Association for Computational Linguistics
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 and
Kuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira1
1CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
2L2F ? INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
javg@l2f.inesc-id.pt
Abstract
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
1 Introduction
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al, 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al, 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
2 Parsing Challenges
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
1While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
1051
pchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
Table 1: Official labeled (l) and other unlabeled (ul)
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al, 1993),
so there is no complex structure to NPs. Ku?bler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider theWSJ
phrase ?the New York State Insurance Department?.
The annotation indicates a flat structure, where ev-
2We measured these drops on several other dependency
parsers and found similar results.
3For example, the phrase ?(R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).?
ery token is headed by ?Department?. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For ?the detoxi-
cation enzyme glutathione transferase P1-1?, ?en-
zyme? is the head of the NP, ?P1-1? is the head of
?transferase?, and ?transferase? is the head of ?glu-
tathione?. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun ?P1-1?, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase ?Howard Mosher, president and
chief executive officer,? has ?Mosher? as the head
of ?Howard? and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase ?glutathione
transferase P1-1? is an appositive indicating which
?enzyme? is meant. However, since there are no
commas, the parser thinks ?P1-1? is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
1052
hurt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al, 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domain?s annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
3 Adaptation Approaches
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
3.1 Features
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
3.2 Diversity
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
1053
tions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al, 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
3.3 Target Focused Learning
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each feature?s value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
4 Future Directions
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
5 Acknowledgments
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Ku?bler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
1054
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
1055
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582?590,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
Partha Pratim Talukdar?
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Joseph Reisinger?
University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Deepak Ravichandran
Google Inc.
Mountain View, CA 94043
deepakr@google.com
Rahul Bhagat?
USC Information Sciences Institute
Marina Del Rey, CA 90292
rahul@isi.edu
Fernando Pereira
Google Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
1 Introduction
1.1 Motivation
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding andWeb search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al, 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
?Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
1.2 Contributions
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al, 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas?ca, 2008) is extended in two
ways:
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
582
instances such as Office of War Information),
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas?ca
(2008), while retaining similar precision levels.
2 First Phase Extractors
To show Adsorption?s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas?ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al, 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
2.1 Extraction from Free Text
Van Durme and Pas?ca (2008) produce an open-
domain set of instance clusters C ? C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C ? ? C, C ? 6= C containing
instances with label L is less thanK. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
2.2 Extraction from Structured Text
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al, 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T ? T consists of one or more columns. Each
column g ? T consists of a set of candidate in-
stances i ? g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C ? C as
MC(g)
def
= {i ? I(C) : i ? g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its ?-unique class
coverage, that is, the set of classes that have at least
? unique seeds in g,
Q(g;?)
def
= {C ? C : |MC(g)| ? ?}.
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
score(C, g;?)
def
= |MC(g)|
? ?? ?
seed matches
?
class coherence
? ?? ?
|MC(g)|
|
?
C??Q(g;?) I(C
?)|
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
583
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas?ca, 2008).
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
score(i, C;?)
def
=
1
ZC
?
g?T,T?T
score(C, g;?)
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
3 Graph-Based Extraction
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
? Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
? Extractions frommultiple sources, such asWeb
queries, Web tables, and text patterns can be
represented in a single graph.
? Graphs make explicit the potential paths of in-
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V,E,W ), where V is the set
of nodes, E is the set of edges and W : E ? R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C,w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node?s class is
hypothesized to be semantically similar to the label?s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
584
bob dylan
musician
0.95
johnny cash
0.87
singer
0.73
billy joel
0.82
0.75
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al, 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V,E,W ) and computes for each node v a la-
bel distribution Lv that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al, 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V,Er,Wr) be the edge-reversed version of the
original graph G = (V,E,W ) where (a, b) ?
Er iff (b, a) ? E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q ? V . To es-
timate Lq for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
1. With probability pcontv , continue the ran-
dom walk to a neighbor of v.
2. With probability pabndv , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
3. With probability pinjv , stop the random
walk and emit a label L from Iv.
Lq is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions Lv to assign a probability not
only to each label in C but also to the dummy
label ?, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G? = (V ?, E?,W ?) as
follows. For each v ? V , we define an ini-
tial distribution Iv = L?, where L? is the
dummy distribution with L?(?) = 1, repre-
senting lack of label information for v. In addi-
tion, let Vs ? V be the set of nodes for which
we have some actual label knowledge, and let
V ? = V ? {v? : v ? Vs}, E? = E ? {(v?, v) :
v ? Vs}, and W ?(v?, v) = 1 for v ? Vs,
W ?(u, v) = W (u, v) for u, v ? V . Finally,
let Iv? (seed labels) specify the knowledge about
possible labels for v ? Vs. Less formally, the
v? nodes in G? serve to inject into the graph the
prior label distributions for each v ? Vs.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
585
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al
(2008) to set the random walk probabilities:
? Let cv =
log ?
log(? + expH(v)) where H(v) =
?
?
u puv ? log(puv) with puv =
W (u,v)
P
u
? W (u
? ,v)
.
H(v) can be interpreted as the entropy of v?s
neighborhood. Thus, cv is lower if v has many
neighbors. We set ? = 2.
? jv = (1 ? cv) ?
?
H(v) if Iv 6= L> and 0
otherwise.
? Then let
zv = max(cv + jv, 1)
pcontv = cv/zv
pinjv = jv/zv
pabndv = 1? p
cont
v ? p
abnd
v
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al (2008).
3The number of iterations was set to 10 in the experiments
reported in this paper.
Algorithm 1 Adsorption Algorithm.
Input: G? = (V
?
, E
?
,W ?), Iv (?v ? V ?).
Output: Distributions {Lv : v ? V }.
1: Lv = Iv ?v ? V
?
2:
3: repeat
4: Nv =
?
u W (u, v)
5: Dv = 1Nv
?
u W (u, v)Lu ?v ? V
?
6: for all v ? V
?
do
7: Lv = pcontv ?Dv +p
inj
v ? Iv +pabndv ?L
>
8: end for
9: until convergence
4 Experiments
4.1 Data
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas?ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set ? = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
4.2 Graph Creation
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
586
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
Table 2: Classes and seeds used to initialize Adsorption.
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
5 Evaluation
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion?s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption?s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
Book Publishers Federal Agencies NFL Players Scientific Journals Mammals20
40
60
80
100
 
 Adsorption A8
Book
Publishers
Federal
Agencies
NFL
Players
Scientific
Journals
Mammals
A8 Adsorption
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
5.1 Instance Precision
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation ?i is a C? was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sante?, kidney and blood pressure research,
american journal of physiology?cell physiology
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
5.2 Class Label Recall
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
588
MRR MRR # found
Method (full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
lower MRR scores compared to Adsorption. This
result highlights Adsorption?s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
6 Related Work
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
7 Conclusion
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
Acknowledgments
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
References
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49?56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
589
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207?227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050?1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using theWeb. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
590
BioNLP 2007: Biological, translational, and clinical language processing, pages 129?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Code Assignment to Medical Text
Koby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim Talukdar
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA
{crammer|mdredze|kuzman|partha}@seas.upenn.edu
Steven Carroll
Division of Oncology, The Children?s Hospital of Philadelphia, Philadelphia, PA
carroll@genome.chop.edu
Abstract
Code assignment is important for handling
large amounts of electronic medical data in
the modern hospital. However, only expert
annotators with extensive training can as-
sign codes. We present a system for the
assignment of ICD-9-CM clinical codes to
free text radiology reports. Our system as-
signs a code configuration, predicting one or
more codes for each document. We com-
bine three coding systems into a single learn-
ing system for higher accuracy. We compare
our system on a real world medical dataset
with both human annotators and other auto-
mated systems, achieving nearly the maxi-
mum score on the Computational Medicine
Center?s challenge.
1 Introduction
The modern hospital generates tremendous amounts
of data: medical records, lab reports, doctor notes,
and numerous other sources of information. As hos-
pitals move towards fully electronic record keeping,
the volume of this data only increases. While many
medical systems encourage the use of structured in-
formation, including assigning standardized codes,
most medical data, and often times the most impor-
tant information, is stored as unstructured text.
This daunting amount of medical text creates
exciting opportunities for applications of learning
methods, such as search, document classification,
data mining, information extraction, and relation ex-
traction (Shortliffe and Cimino, 2006). These ap-
plications have the potential for considerable bene-
fit to the medical community as they can leverage
information collected by hospitals and provide in-
centives for electronic record storage. Much of the
data generated by medical personnel is unused past
the clinical visit, often times because there is no way
to simply and quickly apply the wealth of informa-
tion. Medical NLP holds the promise of both greater
care for individual patients and enhanced knowledge
about health care.
In this work we explore the assignment of ICD-9-
CM codes to clinical reports. We focus on this prac-
tical problem since it is representative of the type
of task faced by medical personnel on a daily ba-
sis. Many hospitals organize and code documents
for later retrieval using different coding standards.
Often times, these standards are extremely complex
and only trained expert coders can properly perform
the task, making the process of coding documents
both expensive and unreliable since a coder must se-
lect from thousands of codes a small number for a
given report. An accurate automated system would
reduce costs, simplify the task for coders, and create
a greater consensus and standardization of hospital
data.
This paper addresses some of the challenges asso-
ciated with ICD-9-CM code assignment to clinical
free text, as well as general issues facing applica-
tions of NLP to medical text. We present our auto-
mated system for code assignment developed for the
Computational Medicine Center?s challenge. Our
approach uses several classification systems, each
with the goal of predicting the exact code configu-
ration for a medical report. We then use a learning
129
system to combine our predictions for superior per-
formance.
This paper is organized as follows. First, we ex-
plain our task and difficulties in detail. Next we de-
scribe our three automated systems and features. We
combine the three approaches to create a single su-
perior system. We evaluate our system on clinical
reports and show accuracy approaching human per-
formance and the challenge?s best score.
2 Task Overview
The health care system employs a large number of
categorization and classification systems to assist
data management for a variety of tasks, including
patient care, record storage and retrieval, statistical
analysis, insurance, and billing. One of these sys-
tems is the International Classification of Diseases,
Ninth Revision, Clinical Modification (ICD-9-CM)
which is the official system of assigning codes to di-
agnoses and procedures associated with hospital uti-
lization in the United States. 1 The coding system
is based on World Health Organization guidelines.
An ICD-9-CM code indicates a classification of a
disease, symptom, procedure, injury, or information
from the personal history. Codes are organized hier-
archically, where top level entries are general group-
ings (e.g. ?diseases of the respiratory system?) and
bottom level codes indicate specific symptoms or
diseases and their location (e.g. ?pneumonia in as-
pergillosis?). Each specific, low-level code consists
of 4 or 5 digits, with a decimal after the third. Higher
level codes typically include only 3 digits. Overall,
there are thousands of codes that cover a broad range
of medical conditions.
Codes are assigned to medical reports by doc-
tors, nurses and other trained experts based on com-
plex coding guidelines (National Center for Health
Statistics, 2006). A particular medical report can be
assigned any number of relevant codes. For exam-
ple, if a patient exhibits a cough, fever and wheez-
ing, all three codes should be assigned. In addi-
tion to finding appropriate codes for each condition,
complex rules guide code assignment. For exam-
ple, a diagnosis code should always be assigned if a
diagnosis is reached, a diagnosis code should never
1http://www.cdc.gov/nchs/about/otheract/
icd9/abticd9.htm
be assigned when the diagnosis is unclear, a symp-
tom should never be assigned when a diagnosis is
present, and the most specific code is preferred. This
means that codes that seem appropriate to a report
should be omitted in specific cases. For example,
a patient with hallucinations should be coded 780.1
(hallucinations) but for visual hallucinations, the
correct code is 368.16. The large number of codes
and complexity of assignment rules make this a diffi-
cult problem for humans (inter-annotator agreement
is low). Therefore, an automated system that sug-
gested or assigned codes could make medical data
more consistent.
These complexities make the problem difficult
for NLP systems. Consider the task as multi-class,
multi-label. For a given document, many codes may
seem appropriate but it may not be clear to the algo-
rithm how many to assign. Furthermore, the codes
are not independent and different labels can inter-
act to either increase or decrease the likelihood of
the other. Consider a report that says, ?patient re-
ports cough and fever.? The presence of the words
cough and fever indicate codes 786.2 (cough) and
780.6 (fever). However, if the report continues to
state that ?patient has pneumonia? then these codes
are dropped in favor of 486 (pneumonia). Further-
more, if the report then says ?verify clinically?, then
the diagnosis is uncertain and only codes 786.2 and
780.6 apply. Clearly, this is a challenging problem,
especially for an automated system.
2.1 Corpus
We built and evaluated our system in accordance
with the Computational Medicine Center?s (CMC)
2007 Medical Natural Language Processing Chal-
lenge.2 Since release of medical data must strictly
follow HIPAA standards, the challenge corpus un-
derwent extensive treatment for disambiguation,
anonymization, and careful scrubbing. A detailed
description of data preparation is found in Compu-
tational Medicine Center (2007). We describe the
corpus here to provide context for our task.
The training corpus is comprised of 978 radiolog-
ical reports taken from real medical records. A test
corpus contains 976 unlabeled documents. Radiol-
ogy reports have two text fields, clinical history and
2www.computationalmedicine.org/challenge
130
impression. The physician ordering the x-ray writes
the clinical history, which contains patient informa-
tion for the radiologist, including history and current
symptoms. Sometimes a guess as to the diagnosis
appears (?evaluate for asthma?). The descriptions
are sometimes whole sentences and other times sin-
gle words (?cough?). The radiologist writes the im-
pression to summarize his or her findings. It con-
tains a short analysis and often times a best guess as
to the diagnosis. At times this field is terse, (?pneu-
monia? or ?normal kidneys?) and at others it con-
tains an entire paragraph of text. Together, these two
fields are used to assign ICD-9-CM codes, which
justify a certain procedure, possibly for reimburse-
ment by the insurance company.
Only a small percentage of ICD-9-CM codes ap-
pear in the challenge. In total, the reports include 45
different codes arranged in 94 configurations (com-
binations). Some of these codes appear frequently,
while others are rare, appearing only a single time.
The test set is restricted so that each configuration
appears at least once in the training set, although
there is no further guarantee as to the test set?s distri-
bution over codes. Therefore, in addition to a large
number of codes, there is variability in the amount
of data for each code. Four codes have over 100
examples each and 24 codes have 10 or fewer doc-
uments, with 10 of these codes having only a single
document.
Since code annotation is a difficult task, each doc-
ument in the corpus was evaluated by three expert
annotators. A gold annotation was created by tak-
ing the majority of the annotators; if two of the three
annotators provided a code, that code is used in the
gold configuration. This approach means that a doc-
ument?s configuration may be a construction of mul-
tiple annotators and may not match any of the three
annotators exactly. Both the individual and the ma-
jority annotations are included with the training cor-
pus.
While others have attempted ICD-9 code classi-
fication, our task differs in two respects (Section 7
provides an overview of previous work). First, pre-
vious work has used discharge reports, which are
typically longer with more text fields. Second, while
most systems are evaluated as a recommendation
system, offering the top k codes and then scoring
recall at k, our task is to provide the exact configu-
ration. The CMC challenge evaluated systems using
an F1 score, so we are penalized if we suggest any
label that does not appear in the majority annotation.
To estimate task difficulty we measured the inter-
annotator score for the training set using the three
annotations provided. We scored two annotations
with the micro average F1, which weighs each code
assignment equally (see Section 5 for details on
evaluation metrics). If an annotator omitted a code
and included an extra code, he or she is penalized
with a false positive (omitting a code) and a false
negative (adding an extra code). We measured anno-
tators against each other; the average f-measure was
74.85 (standard deviation of .06). These scores were
low since annotators chose from an unrestricted set
of codes, many of which were not included in the fi-
nal majority annotation. However, these scores still
indicate the human accuracy for this task using an
unrestricted label set. 3
3 Code Assignment System
We developed three automated systems guided by
our above analysis. First, we designed a learning
system that used natural language features from the
official code descriptions and the text of each re-
port. It is general purpose and labels all 45 codes
and 94 configurations (labels). Second, we built a
rule based system that assigned codes based on the
overlap between the reports and code descriptions,
similar to how an annotator may search code de-
scriptions for appropriate labels. Finally, a special-
ized system aimed at the most common codes imple-
mented a policy that mimics the guidelines a medical
staffer would use to assign these codes.
3.1 Learning System
We begin with some notational definitions. In what
follows, x denotes the generic input document (ra-
diology report), Y denotes the set of possible label-
ings (code configurations) of x, and y?(x) the cor-
rect labeling of x. For each pair of document x
and labeling y ? Y , we compute a vector-valued
feature representation f(x, y). A linear model is
3We also measured each annotator with the majority codes,
taking the average score (87.48), and the best annotator with
the majority label (92.8). However, these numbers are highly
biased since the annotator influences the majority labeling. We
observe that our final system still exceeds the average score.
131
given by a weight vector w. Given this weight vec-
tor w, the score w ? f(x, y) ranks possible labelings
of x, and we denote by Yk,w(x) the set of k top
scoring labelings for x. For some structured prob-
lems, a factorization of f(x, y) is required to enable
a dynamic program for inference. For our problem,
we know all the possible configurations in advance
(there are 94 of them) so we can pick the highest
scoring y ? Y by trying them all. For each docu-
ment x and possible labeling y, we compute a score
using w and the feature representation f(x, y). The
top scoring y is output as the correct label. Section
3.1.1 describes our feature function f(x, y) while
Section 3.1.2 describes how we find a good weight
vector w.
3.1.1 Features
Problem representation is one of the most impor-
tant aspects of a learning system. In our case, this
is defined by the set of features f(x, y). Ideally we
would like a linear combination of our features to ex-
actly specify the true labeling of all the instances, but
we want to have a small total number of features so
that we can accurately estimate their values. We sep-
arate our features into two classes: label specific fea-
tures and transfer features. For simplicity, we index
features by their name. Label specific features are
only present for a single label. For example, a simple
class of label specific features is the conjunction of a
word in the document with an ICD-9-CM code in the
label. Thus, for each word we create 94 features, i.e.
the word conjoined with every label. These features
tend to be very powerful, since weights for them can
encode very specific information about the way doc-
tors talk about a disease, such as the feature ?con-
tains word pneumonia and label contains code 486?.
Unfortunately, the cost of this power is that there are
a large number of these features, making parameter
estimation difficult for rare labels. In contrast, trans-
fer features can be present in multiple labels. An
example of a transfer feature might be ?the impres-
sion contains all the words in the code descriptions
of the codes in this label?. Transfer features allow us
to generalize from one label to another by learning
things like ?if all the words of the label description
occur in the impression, then this label is likely? but
have the drawback that we cannot learn specific de-
tails about common labels. For example, we cannot
learn that the word ?pneumonia? in the impression
is negatively correlated with the code cough. The
inclusion of both label specific and transfer features
allows us to learn specificity where we have a large
number of examples and generality for rare codes.
Before feature extraction we normalized the re-
ports? text by converting it to lower case and by
replacing all numbers (and digit sequences) with a
single token ?NUM?. We also prepared a synonym
dictionary for a subset of the tokens and n-grams
present in the training data. The synonym dictionary
was based onMeSH4, the Medical Subject Headings
vocabulary, in which synonyms are listed as terms
under the same concept. All ngrams and tokens
in the training data which had mappings defined in
the synonym dictionary were then replaced by their
normalized token; e.g. all mentions of ?nocturnal
enuresis? or ?nighttime urinary incontinence? were
replaced by the token ?bedwetting?. Additionally,
we constructed descriptions for each code automati-
cally from the official ICD-9-CM code descriptions
in National Center for Health Statistics (2006). We
also created a mapping between code and code type
(diagnosis or symptom) using the guidelines.
Our system used the following features. The de-
scriptions of particular features are in quotes, while
schemes for constructing features are not.
? ?this configuration contains a disease code?,
?this configuration contains a symptom code?,
?this configuration contains an ambiguous
code? and ?this configuration contains both dis-
ease and symptom codes?.5
? With the exception of stop-words, all words of
the impression and history conjoined with each
label in the configuration; pairs of words con-
joined with each label; words conjoined with
pairs of labels. For example, ?the impression
contains ?pneumonia? and the label contains
codes 786.2 and 780.6?.
? A feature indicating when the history or im-
pression contains a complete code description
4www.nlm.nih.gov/mesh
5We included a feature for configurations that had both dis-
ease and symptom codes because they appeared in the training
data, even though coding guidelines prohibit these configura-
tions.
132
for the label; one for a word in common with
the code description for one of the codes in the
label; a common word conjoined with the pres-
ence of a negation word nearby (?no?, ?not?,
etc.); a word in common with a code descrip-
tion not present in the label. We applied similar
features using negative words associated with
each code.
? A feature indicating when a soft negation word
appears in the text (?probable?, ?possible?,
?suspected?, etc.) conjoined with words that
follow; the token length of a text field (?im-
pression length=3?); a conjunction of a feature
indicating a short text field with the words in
the field (?impression length=1 and ?pneumo-
nia? ?)
? A feature indicating each n-gram sequence that
appears in both the impression and clinical his-
tory; the conjunction of certain terms where
one appears in the history and the other in the
impression (e.g. ?cough in history and pneu-
monia in impression?).
3.1.2 Learning Technique
Using these feature representations, we now learn
a weight vector w that scores the correct labelings
of the data higher than incorrect labelings. We used
a k-best version of the MIRA algorithm (Crammer,
2004; McDonald et al, 2005). MIRA is an online
learning algorithm that for each training document
x updates the weight vector w according to the rule:
wnew = argmin
w
?w ? wold?
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y)
where L(y?(x), y) is a measure of the loss of label-
ing y with respect to the correct labeling y?(x). For
our experiments, we set k to 30 and iterated over the
training data 10 times. Two standard modifications
to this approach also helped. First, rather than using
just the final weight vector, we average all weight
vectors. This has a smoothing effect that improves
performance on most problems. The second modifi-
cation is the introduction of slack variables:
wnew = argmin
w
?w ? wold? + ?
?
i
?i
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y) ? ?i
?i ? {1 . . . k} : ?i ? 0.
We used a ? of 10?3 in our experiments.
The most straightforward loss function is the 0/1
loss, which is one if y does not equal y?(x) and zero
otherwise. Since we are evaluated based on the num-
ber of false negative and false positive ICD-9-CM
codes assigned to all the documents, we used a loss
that is the sum of the number of false positive and the
number of false negative labels that y assigns with
respect to y?(x).
Finally, we only used features that were possi-
ble for some labeling of the test data by using only
the test data to construct our feature alphabet. This
forced the learner to focus on hypotheses that could
be used at test time and resulted in a 1% increase in
F-measure in our final system on the test data.
3.2 Rule Based System
Since some of the configurations appear a small
number of times in our corpus (some only once),
we built a rule based system that requires no train-
ing. The system uses a description of the ICD-9-CM
codes and their types, similar to the list used by our
learning system (Section 3.1.1). The code descrip-
tions include between one and four short descrip-
tions, such as ?reactive airway disease?, ?asthma?,
and ?chronic obstructive pulmonary disease?. We
treat each of these descriptions as a bag of words.
For a given report, the system parses both the clini-
cal history and impression into sentences, using ?.?
as a sentence divider. Each sentence is the checked
to see if all of the words in a code description appear
in the sentence. If a match is found, we set a flag
corresponding to the code. However, if the code is
a disease, we search for a negation word in the sen-
tence, removing the flag if a negation word is found.
Once all code descriptions have been evaluated, we
check if there are any flags set for disease codes. If
so, we remove all symptom code flags. We then emit
a code corresponding to each set flag. This simple
system does not enforce configuration restrictions;
133
we may predict a code configuration that does not
appear in our training data. Adding this restriction
improved precision but hurt recall, leading to a slight
decrease in F1 score. We therefore omitted the re-
striction from our system.
3.3 Automatic Coding Policies
As we described in Section 2, enforcing coding
guidelines can be a complex task. While a learning
system may have trouble coding a document, a hu-
man may be able to define a simple policy for cod-
ing. Since some of the most frequent codes in our
dataset have this property, we decided to implement
such an automatic coding policy. We selected two
related sets of codes to target with a rule based sys-
tem, a set of codes found in pneumonia reports and
a set for urinary tract infection/reflux reports.
Reports related to pneumonia are the most com-
mon in our dataset and include codes for pneumo-
nia, asthma, fever, cough and wheezing; we handle
them with a single policy. Our policy is as follows:
? Search for a small set of keywords (e.g.
?cough?, ?fever?) to determine if a code should
be applied.
? If ?pneumonia? appears unnegated in the im-
pression and the impression is short, or if it oc-
curs in the clinical history and is not preceded
by phrases such as ?evaluate for? or ?history
of?, apply pneumonia code and stop.
? Use the same rule to code asthma by looking
for ?asthma? or ?reactive airway disease?.
? If no diagnosis is found, code all non-negated
symptoms (cough, fever, wheezing).
We selected 80% of the training set to evaluate in the
construction of our rules. We then ran the finished
system on both this training set and the held out 20%
of the data. The system achieved F1 scores of 87%
on the training set and 84% on the held out data for
these five codes. The comparable scores indicates
that we did not over-fit the training data.
We designed a similar policy for two other related
codes, urinary tract infection and vesicoureteral re-
flux. We found these codes to be more complex as
they included a wide range of kidney disorders. On
these two codes, our system achieved 78% on the
train set and 76% on the held out data. Overall, au-
tomatically applying our two policies yielded high
confidence predictions for a significant subset of the
corpus.
4 Combined System
Since our three systems take complimentary ap-
proaches to the problem, we combined them to im-
prove performance. First, we took our automatic
policy and rule based systems and cascaded them; if
the automatic policy system does not apply a code,
the rule based system classifies the report. We used
a cascaded approach since the automatic policy sys-
tem was very accurate when it was able to assign
a code. Therefore, the rule based system defers to
the policy system when it is triggered. Next, we in-
cluded the prediction of the cascaded system as a
feature for our learning system. We used two fea-
ture rules: ?cascaded-system predicted exactly this
label? and ?cascaded-system predicted one of the
codes in this label?. As we show, this yielded our
most accurate system. While we could have used a
meta-classifier to combine the three systems, includ-
ing the rule based systems as features to the learning
system allowed it to learn the appropriate weights
for the rule based predictions.
5 Evaluation Metric
Evaluation metrics for this task are often based on
recommendation systems, where the system returns
a list of the top k codes for selection by the user. As
a result, typical metrics are ?recall at k? and aver-
age precision (Larkey and Croft, 1995). Instead, our
goal was to predict the exact configuration, returning
exactly the number of codes predicted to be on the
report. The competition used a micro-averaged F1
score to evaluate predictions. A contingency table
(confusion matrix) is computed by summing over
each predicted code for each document by predic-
tion type (true positive, false positive, false negative)
weighing each code assignment equally. F1 score
is computed based on the resultant table. If specific
codes or under-coding is favored, we can modify our
learning loss function as described in Section 3.1.2.
A detailed treatment of this evaluation metric can be
found in Computational Medicine Center (2007).
134
System Precision Recall F1
BL 61.86 72.58 66.79
RULE 81.9 82.0 82.0
CASCADE 86.04 84.56 85.3
LEARN 85.5 83.6 84.6
CASCADE+LEARN 87.1 85.9 86.5
Table 1: Performance of our systems on the provided
labeled training data (F1 score). The learning sys-
tems (CASCADE+LEARN and LEARN ) were eval-
uated on ten random split of the data while RULE
was evaluated on all of the training data. We include
a simple rule based system (BL ) as a baseline.
6 Results
We evaluated our systems on the labeled training
data of 978 radiology reports. For each report, each
system predicted an exact configuration of codes
(i.e. one of 94 possible labels). We score each sys-
tem using a micro-averaged F1 score. Since we only
had labels for the training data, we divided the data
using an 80/20 training test split and averaged results
over 10 runs for our learning systems. We evaluated
the following systems:
? RULE : The rule based system based on ICD-
9-CM code descriptions (Section 3.2).
? CASCADE : The automatic code policy system
(Section 3.3) cascaded with RULE (Section 4).
? LEARN : The learning system with both label
specific and transfer features (Section 3.1).
? CASCADE+LEARN : Our combined system
that incorporates CASCADE predictions as a
feature to LEARN (Section 4).
For a baseline, we built a simple system that ap-
plies the official ICD-9-CM code descriptions to find
the correct labels (BL ). For each code in the train-
ing set, the system generates text-segments related to
it. During testing, for each new document, the sys-
tem checks if any text-segment (as discovered dur-
ing training) appears in the document. If so, the cor-
responding code is predicted. The results from our
four systems and baseline are shown in Table 1.
System Train Test
CASCADE 85.3 84
CASCADE+LEARN 86.5 87.60
Average - 76.6
Best - 89.08
Table 2: Performance of two systems on the train
and test data. Results obtained from the web sub-
mission interface were rounded. Average and Best
are the average and best f-measures of the 44 sub-
mitted systems (standard deviation 13.40).
Each of our systems easily beats the baseline, and
the average inter-annotator score for this task. Ad-
ditionally, we were able to evaluate two of our sys-
tems on the test data using a web interface as pro-
vided by the competition. The test set contains 976
documents (about the same as the training set) and
is drawn the from same distribution as the training
data. Our test results were comparable to perfor-
mance on the training data, showing that we did
not over-fit to the training data (Table 2). Addi-
tionally, our combined system (CASCADE+LEARN
) achieved a score of 87.60%, beating our training
data performance and exceeding the average inter-
annotator score. Out of 44 submitted systems, the
average score on test data was 76.7% (standard devi-
ation of 13.40) and the maximum score was 89.08%.
Our system scored 4th overall and was less than
1.5% behind the best system. Overall, in comparison
with our baselines and over 40 systems, we perform
very well on this task.
7 Related Work
There have been several attempts at ICD-9-CM
code classification and related problems for med-
ical records. The specific problem of ICD-9-CM
code assignment was studied by Lussier et al (2000)
through an exploratory study. Larkey and Croft
(1995) designed classifiers for the automatic assign-
ment of ICD-9 codes to discharge summaries. Dis-
charge summaries tend to be considerably longer
than our data and contain multiple text fields. Ad-
ditionally, the number of codes per document has
a larger range, varying between 1 and 15 codes.
Larkey and Croft use three classifiers: K-nearest
neighbors, relevance feedback, and bayesian inde-
135
pendence. Similar to our approach, they tag items
as negated and try to identify diagnosis and symp-
tom terms. Additionally, their final system combines
all three models. A direct comparison is not possi-
ble due to the difference in data and evaluation met-
rics; they use average precision and recall at k. On
a comparable metric, ?principal code is top candi-
date?, their best system achieves 59.9% accuracy. de
Lima et al (1998) rely on the hierarchical nature of
medical codes to design a hierarchical classification
scheme. This approach is likely to help on our task
as well but we were unable to test this since the lim-
ited number of codes removes any hierarchy. Other
approaches have used a variety of NLP techniques
(Satomura and Amaral, 1992).
Others have used natural language systems for the
analysis of medical records (Zweigenbaum, 1994).
Chapman and Haug (1999) studied radiology re-
ports looking for cases of pneumonia, a goal sim-
ilar to that of our automatic coding policy system.
Meystre and Haug (2005) processed medical records
to harvest potential entries for a medical problem
list, an important part of electronic medical records.
Chuang et al (2002) studied Charlson comorbidi-
ties derived from processing discharge reports and
chest x-ray reports and compared them with admin-
istrative data. Additionally, Friedman et al (1994)
applies NLP techniques to radiology reports.
8 Conclusion
We have presented a learning system that processes
radiology reports and assigns ICD-9-CM codes.
Each of our systems achieves results comparable
with an inter-annotator baseline for our training data.
A combined system improves over each individ-
ual system. Finally, we show that on test data un-
available during system development, our final sys-
tem continues to perform well, exceeding the inter-
annotator baseline and achieving the 4th best score
out of 44 systems entered in the CMC challenge.
9 Acknowledgements
We thank Andrew Lippa for his extensive medical
wisdom. Dredze is supported by an NDSEG fel-
lowship; Ganchev and Talukdar by NSF ITR EIA-
0205448; and Crammer by DARPA under Contract
No. NBCHD03001. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or the De-
partment of Interior-National Business Center (DOI-
NBC).
References
W.W. Chapman and P.J. Haug. 1999. Comparing expert sys-
tems for identifying chest x-ray reports that support pneu-
monia. In AMIA Symposium, pages 216?20.
JH Chuang, C Friedman, and G Hripcsak. 2002. A com-
parison of the charlson comorbidities derived from medical
language processing and administrative data. AMIA Sympo-
sium, pages 160?4.
Computational Medicine Center. 2007. The
computational medicine center?s 2007 med-
ical natural language processing challenge.
http://computationalmedicine.org/challenge/index.php.
Koby Crammer. 2004. Online Learning of Complex Categorial
Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-
son. 1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical Infor-
matics Association, 1:161?74.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical re-
port, University of Massachusetts at Amherst, Amherst, MA.
YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Au-
tomating icd-9-cm encoding using medical language pro-
cessing: A feasibility study.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In HLT/EMNLP.
Stephane Meystre and Peter J Haug. 2005. Automation of a
problem list using natural language processing. BMC Medi-
cal Informatics and Decision Making.
National Center for Health Statistics. 2006. Icd-
9-cm official guidelines for coding and reporting.
http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.
Y Satomura and MB Amaral. 1992. Automated diagnostic in-
dexing by natural language processing. Medical Informat-
ics, 17:149?163.
Edward H. Shortliffe and James J. Cimino, editors. 2006.
Biomedical Informatics: Computer Applications in Health
Care and Biomedicine. Springer.
P. Zweigenbaum. 1994. Menelas: an access system for medical
records using natural language. Comput Methods Programs
Biomed, 45:117?20.
136
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 833?838,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Learning and Inference in a Large Knowledge-base
using Latent Syntactic Cues
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{mg1,ppt,bkisiel,tom.mitchell}@cs.cmu.edu
Abstract
Automatically constructed Knowledge Bases
(KBs) are often incomplete and there is a gen-
uine need to improve their coverage. Path
Ranking Algorithm (PRA) is a recently pro-
posed method which aims to improve KB cov-
erage by performing inference directly over
the KB graph. For the first time, we demon-
strate that addition of edges labeled with la-
tent features mined from a large dependency
parsed corpus of 500 million Web documents
can significantly outperform previous PRA-
based approaches on the KB inference task.
We present extensive experimental results val-
idating this finding. The resources presented
in this paper are publicly available.
1 Introduction
Over the last few years, several large scale Knowl-
edge Bases (KBs) such as Freebase (Bollacker et
al., 2008), NELL (Carlson et al, 2010), and YAGO
(Suchanek et al, 2007) have been developed. Each
such KB consists of millions of facts (e.g., (Tiger
Woods, playsSport, Golf )) spanning over multiple
relations. Unfortunately, these KBs are often incom-
plete and there is a need to increase their coverage of
facts to make them useful in practical applications.
A strategy to increase coverage might be to per-
form inference directly over the KB represented as a
graph. For example, if the KB contained the follow-
ing facts, (Tiger Woods, participatesIn, PGA Tour))
and (Golf, sportOfTournament, PGA Tour), then by
putting these two facts together, we could potentially
infer that (Tiger Woods, playsSport, Golf ). The
Figure 1: Example demonstrating how lexicalized syn-
tactic edges can improve connectivity in the KB enabling
PRA (Lao and Cohen, 2010) to discover relationships be-
tween Alex Rodriguez and World Series. Edges with la-
tent labels can improve inference performance by reduc-
ing data sparsity. See Section 1.1 for details.
recently proposed Path Ranking Algorithm (PRA)
(Lao and Cohen, 2010) performs such inference by
automatically learning semantic inference rules over
the KB (Lao et al, 2011). PRA uses features based
off of sequences of edge types, e.g., ?playsSport,
sportOfTournament?, to predict missing facts in the
KB.
PRA was extended by (Lao et al, 2012) to per-
form inference over a KB augmented with depen-
dency parsed sentences. While this opens up the
possibility of learning syntactic-semantic inference
rules, the set of syntactic edge labels used are
just the unlexicalized dependency role labels (e.g.,
nobj, dobj, etc., without the corresponding words),
thereby limiting overall expressitivity of the learned
inference rules. To overcome this limitation, in this
paper we augment the KB graph by adding edges
with more expressive lexicalized syntactic labels
(where the labels are words instead of dependen-
833
cies). These additional edges, e.g., (Alex Rodriguez,
?plays for?, NY Yankees), are mined by extracting
600 million Subject-Verb-Object (SVO) triples from
a large corpus of 500m dependency parsed docu-
ments, which would have been prohibitively expen-
sive to add directly as in (Lao et al, 2012). In order
to overcome the explosion of path features and data
sparsity, we derive edge labels by learning latent em-
beddings of the lexicalized edges. Through exten-
sive experiments on real world datasets, we demon-
strate effectiveness of the proposed approach.
1.1 Motivating Example
In Figure 1, the KB graph (only solid edges) is dis-
connected, thereby making it impossible for PRA to
discover any relationship between Alex Rodriguez
and World Series. However, addition of the two
edges with SVO-based lexicalized syntactic edges
(e.g., (Alex Rodriguez, plays for, NY Yankees)) re-
stores this inference possibility. For example, PRA
might use the edge sequence ??plays for?, team-
PlaysIn? as evidence for predicting the relation in-
stance (Alex Rodriguez, athleteWonChampionship,
World Series). Unfortunately, such na??ve addition
of lexicalized edges may result in significant data
sparsity, which can be overcome by mapping lexi-
calized edge labels to some latent embedding (e.g.,
(Alex Rodriguez, LatentFeat#5, NY Yankees) and
running PRA over this augmented graph. Using la-
tent embeddings, PRA could then use the following
edge sequence as a feature in its prediction models:
?LatentFeat#5, teamPlaysIn?. We find this strategy
to be very effective as described in Section 4.
2 Related Work
There is a long history of methods using suface-level
lexical patterns for extracting relational facts from
text corpora (Hearst, 1992; Brin, 1999; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002;
Etzioni et al, 2004). Syntactic information in the
form of dependency paths have been explored in
(Snow et al, 2006; Suchanek et al, 2006). A
method of latent embedding of relation instances
for sentence-level relation extraction was shown in
(Wang et al, 2011). However, none of this prior
work makes explicit use of the background KBs as
we explore in this paper.
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010) has been used previously to perform inference
over graph-structured KBs (Lao et al, 2011), and to
learn formation of online communities (Settles and
Dow, 2013). In (Lao et al, 2012), PRA is extended
to perform inference over a KB using syntactic in-
formation from parsed text. In contrast to these pre-
vious PRA-based approaches where all edge labels
are either KB labels or at surface-level, in this pa-
per we explore using latent edge labels in addition
to surface-level labels in the graph over which PRA
is applied. In particular, we focus on the problem of
performing inference over a large KB and learn la-
tent edge labels by mining dependency syntax statis-
tics from a large text corpus.
Though we use Principal Components Analysis
(PCA) for dimensionality reduction for the experi-
ments in this paper, this is by no means the only
choice. Various other dimensionality reduction tech-
niques, and in particular, other verb clustering tech-
niques (Korhonen et al, 2003), may also be used.
OpenIE systems such as Reverb (Etzioni et al,
2011) also extract verb-anchored dependency triples
from large text corpus. In contrast to such ap-
proaches, we focus on how latent embedding of
verbs in such triples can be combined with explicit
background knowledge to improve coverage of ex-
isting KBs. This has the added capability of infer-
ring facts which are not explicitly mentioned in text.
The recently proposed Universal Schema (Riedel
et al, 2013) also demonstrates the benefit of us-
ing latent features for increasing coverage of KBs.
Key differences between that approach and ours in-
clude our use of syntactic information as opposed to
surface-level patterns in theirs, and also the ability
of the proposed PRA-based method to generate use-
ful inference rules which is beyond the capability of
the matrix factorization approach in (Riedel et al,
2013).
3 Method
3.1 Path Ranking Algorithm (PRA)
In this section, we present a brief overview of the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010), building on the notations in (Lao et al, 2012).
Let G = (V,E, T ) be the graph, where V is the set
of vertices, E is the set of edges, and T is the set of
edge types. For each edge (v1, t, v2) ? E, we have
834
v1, v2 ? V and t ? T . LetR ? T be the set of types
predicted by PRA. R could in principal equal T , but
in this paper we restrict prediction to KB relations,
while T also includes types derived from surface text
and latent embeddings. Let pi = ?t1, t2, . . . , tw? be
a path type of length w over graph G, where ti ? T
is the type of the ith edge in the path. Each such
path type is also a feature in the PRA model. For
a given source and target node pair s, t ? V , let
P (s ? t;pi) be the value of the feature pi specify-
ing the probability of reaching node t starting from
node s and following a path constrained by path type
pi. We approximate these probabilities using random
walks. A value of 0 indicates unreachability from s
to t using path type pi.
Let B = {pi1, . . . , pim} be the set of all features
(path types). The score that relation r holds between
node s and node t is given by the following function:
ScorePRA(s, t, r) =
?
pi?B
P (s? t;pi) ?rpi
where ?rpi is the weight of feature pi in class r ? R.
Feature Selection: The set B of possible path
types grows exponentially in the length of the paths
that are considered. In order to have a manageable
set of features to compute, we first perform a feature
selection step. The goal of this step is to select for
computation only those path types that commonly
connect sources and targets of relation r. We per-
form this feature selection by doing length-bounded
random walks from a given list of source and tar-
get nodes, keeping track of how frequently each path
type leads from a source node to a target node. The
most common m path types are selected for the set
B.
Training: We perform standard logistic regres-
sion with L2 regularization to learn the weights ?rpi.
We follow the strategy in (Lao and Cohen, 2010) to
generate positive and negative training instances.
3.2 PRAsyntactic
In this section, we shall extend the knowledge graph
G = (V,E, T ) from the previous section with an
augmented graph G
?
= (V,E
?
, T
?
), where E ? E
?
and T ? T
?
, with the set of vertices unchanged.
In order to get the edges in E
?
? E, we first
collect a set of Subject-Verb-Object (SVO) triples
D = {(s, v, o, c)} from a large dependency parsed
text corpus, with c ? R+ denoting the frequency
of this triple in the corpus. The additional edge
set is then defined as Esyntactic = E
?
? E =
{(s, v, o) | ?(s, v, o, c) ? D, s, o ? V }. We de-
fine S = {v | ?(s, v, o) ? Esyntactic} and set
T
?
= T ? S. In other words, for each pair of
directly connected nodes in the KB graph G, we add
an additional edge between those two nodes for each
verb which takes the NPs represented by two nodes
as subjects and objects (or vice versa) as observed in
a text corpus. In Figure 1, (Alex Rodriguez, ?plays
for?, NY Yankees) is an example of such an edge.
PRA is then applied over this augmented graph
G
?
, over the same set of prediction types R as be-
fore. We shall refer to this version of PRA as
PRAsyntactic. For the experiments in this paper, we
collected |D| = 600 million SVO triples1 from the
entire ClueWeb corpus (Callan et al, 2009), parsed
using the Malt parser (Nivre et al, 2007) by the
Hazy project (Kumar et al, 2013).
3.3 PRAlatent
In this section we construct G
??
= (V,E??, T
??
),
another syntactic-information-induced extension of
the knowledge graph G, but instead of using the sur-
face forms of verbs in S (see previous section) as
edge types, we derive those edges types T
??
based
on latent embeddings of those verbs. We note that
E ? E
??
, and T ? T
??
.
In order to learn the latent or low dimensional em-
beddings of the verbs in S, we first define QS =
{(s, o) | ?(s, v, o, c) ? D, v ? S}, the set of
subject-object tuples in D which are connected by
at least one verb in S. We now construct a matrix
X|S|?|QS | whose entry Xv,q = c, where v ? S, q =
(s, o) ? QS , and (s, v, o, c) ? D. After row normal-
izing and centering matrix X , we apply PCA on this
matrix. Let A|S|?d with d << |QS | be the low di-
mensional embeddings of the verbs in S as induced
by PCA. We use two strategies to derive mappings
for verbs from matrix A.
? PRAlatentc : The verb is mapped to concatena-
tion of the k2 most positive columns in the row
in A that corresponds to the verb. Similarly, for
the most negative k2 columns.
1This data and other resources from the paper are publicly
available at http://rtw.ml.cmu.edu/emnlp2013 pra/.
835
Precision Recall F1
PRA 0.800 0.331 0.468
PRAsyntactic 0.804 0.271 0.405
PRAlatentc 0.885 0.334 0.485
PRAlatentd 0.868 0.424 0.570
Table 1: Comparison of performance of different variants
of PRA micro averaged across 15 NELL relations. We
find that use of latent edge labels, in particular the pro-
posed approach PRAlatentd , significantly outperforms
other approaches. This is our main result. (See Section 4)
? PRAlatentd : The verb is mapped to disjunction
of top-k most positive and negative columns in
the row in A that corresponds to the verb.
4 Experiments
We compared the various methods using 15 NELL
relations. For each relation, we split NELL?s known
relation instances into 90% training and 10% testing.
For each method, we then selected 750 path features
and trained the model, as described in Section 3, us-
ing GraphChi (Kyrola et al, 2012) to perform the
random walk graph computations. To evaluate the
model, we took all source nodes in the testing data
and used the model to predict target nodes. We re-
port the precision and recall (on the set of known tar-
get nodes) of the set of predictions for each model
that are above a certain confidence threshold. Be-
cause we used strong regularization, we picked for
our threshold a model score of 0.405, correspond-
ing to 60% probability of the relation instance being
true; values higher than this left many relations with-
out any predictions. Table 1 contains the results.
As can be seen in the table, PRAsyntactic on av-
erage performs slightly worse than PRA. While
the extra syntactic features are very informative for
some relations, they also introduce a lot of spar-
sity, which makes the model perform worse on other
relations. When using latent factorization meth-
ods to reduce the sparsity of the syntactic features,
we see a significant improvement in performance.
PRAlatentc has a 45% reduction in precision er-
rors vs. PRA while maintaining the same recall,
and PRAlatentd reduces precision errors by 35%
while improving recall by 27%. Section 4.1 con-
tains some qualitative analysis of how sparsity is re-
duced with the latent methods. As a piece quanti-
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
PRAPRAsyntacticPRAlatentcPRAlatentd
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
PRAPRAsyntacticPRAlatentcPRAlatentd
Figure 2: Precision (y axis) - Recall (x axis) plots for the
relations cityLiesOnRiver (top) and athletePlaysForTeam
(bottom). PRAlatentd (rightmost plot), the proposed ap-
proach which exploits latent edge labels, outperforms
other alternatives.
tative analysis, there were 908 possible path types
found in the feature selection step with PRA on the
relation cityLiesOnRiver (of which we then selected
750). For PRAsyntactic, there were 73,820, while
PRAlatentc had 47,554 and PRAlatentd had 58,414.
Table 2 shows F1 scores for each model on
each relation, and Figure 2 shows representative
Precision-Recall plots for two NELL relations. In
both cases, we find that PRAlatentd significantly
outperforms other baselines.
4.1 Discussion
While examining the model weights for each of
the methods, we saw a few occasions where sur-
face relations and NELL relations combined to form
interpretable path types. For example, in ath-
letePlaysForTeam, some highly weighted features
took the form of ?athletePlaysSport, ?(sport) played
by (team)??. A high weight on this feature would
bias the prediction towards teams that are known to
play the same sport as the athlete.
For PRA, the top features for the best performing
relations are path types that contain a single edge
836
PRA PRAsyntactic PRAlatentc PRAlatentd
animalIsTypeOfAnimal 0.52 0.50 0.47 0.53
athletePlaysForTeam 0.22 0.21 0.56 0.64
athletePlaysInLeague 0.81 0.75 0.73 0.74
cityLiesOnRiver 0.05 0 0.07 0.31
cityLocatedInCountry 0.15 0.20 0.45 0.55
companyCeo 0.29 0.18 0.25 0.35
countryHasCompanyOffice 0 0 0 0
drugHasSideEffect 0.96 0.95 0.94 0.94
headquarteredIn 0.31 0.11 0.41 0.64
locationLocatedWithinLocation 0.40 0.38 0.38 0.41
publicationJournalist 0.10 0.06 0.10 0.16
roomCanContainFurniture 0.72 0.70 0.71 0.73
stadiumLocatedInCity 0.53 0 0.13 0.67
teamPlaysAgainstTeam 0.47 0.24 0.26 0.21
writerWroteBook 0.59 0.62 0.73 0.80
Table 2: F1 performance of different variants of PRA for all 15 relations tested.
which is a supertype or subtype of the relation be-
ing predicted. For instance, for the relation ath-
letePlaysForTeam (shown in Figure 2), the highest-
weighted features in PRA are athleteLedSport-
sTeam (more specific than athletePlaysForTeam)
and personBelongsToOrganization (more general
than athletePlaysForTeam). For the same rela-
tion, PRAsyntactic has features like ?scored for?,
?signed?, ?have?, and ?led?. When using a latent
embedding of these verb phrases, ?signed?, ?have?,
and ?led? all have the same representation in the la-
tent space, and so it seems clear that PRAlatent gains
a lot by reducing the sparsity inherent in using sur-
face verb forms.
For cityLiesOnRiver, where PRA does not per-
form as well, there is no NELL relation that is an im-
mediate supertype or subtype, and so PRA does not
have as much evidence to use. It finds features that,
e.g., are analogous to the statement ?cities in the
same state probably lie on the same river?. Adding
lexical labels gives the model edges to use like ?lies
on?, ?runs through?, ?flows through?, ?starts in?
and ?reaches?, and these features give a significant
boost in performance to PRAsyntactic. Once again,
almost all of those verb phrases share the same latent
embedding, and so PRAlatent gains another signifi-
cant boost in performance by combining them into a
single feature.
5 Conclusion
In this paper, we introduced the use of latent lexi-
cal edge labels for PRA-based inference over knowl-
edge bases. We obtained such latent edge labels
by mining a large dependency parsed corpus of
500 million web documents and performing PCA
on the result. Through extensive experiments on
real datasets, we demonstrated that the proposed ap-
proach significantly outperforms previous state-of-
the-art baselines.
Acknowledgments
We thank William Cohen (CMU) for enlightening
conversations on topics discussed in this paper. We
thank the ClueWeb project (CMU) and the Hazy
Research Group (http://hazy.cs.wisc.edu/hazy/) for
their generous help with data sets; and to the anony-
mous reviewers for their constructive comments.
This research has been supported in part by DARPA
(under contract number FA8750-13-2-0005), and
Google. Any opinions, findings, conclusions and
recommendations expressed in this paper are the au-
thors? and do not necessarily reflect those of the
sponsors.
837
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM conference on Digital
libraries.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web.
J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009.
Clueweb09 data set. boston.lti.cs.cmu.edu.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr, and Tom M Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In AAAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
itall:(preliminary results). In Proceedings of WWW.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second generation.
In Proceedings of IJCAI.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational Linguistics.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of ACL.
Arun Kumar, Feng Niu, and Christopher Re?. 2013.
Hazy: making it easier to build and maintain big-data
analytics. Communications of the ACM, 56(3):40?49.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin. 2012.
Graphchi: Large-scale graph computation on just a pc.
In Proceedings of the 10th USENIX Symposium on Op-
erating Systems Design and Implementation (OSDI),
pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Burr Settles and Steven Dow. 2013. Let?s get together:
the formation and success of online creative collabora-
tions. In Proceedings of CHI.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Fabian M Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of KDD.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of WWW.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation top-
ics. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1426?1436. Association for Computational Linguis-
tics.
838
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 397?406,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incorporating Vector Space Similarity in Random Walk Inference over
Knowledge Bases
Matt Gardner
Carnegie Mellon University
mg1@cs.cmu.edu
Partha Talukdar
?
Indian Institute of Science
ppt@serc.iisc.in
Jayant Krishnamurthy
Carnegie Mellon University
jayantk@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
tom@cs.cmu.edu
Abstract
Much work in recent years has gone into
the construction of large knowledge bases
(KBs), such as Freebase, DBPedia, NELL,
and YAGO. While these KBs are very
large, they are still very incomplete, ne-
cessitating the use of inference to fill in
gaps. Prior work has shown how to make
use of a large text corpus to augment ran-
dom walk inference over KBs. We present
two improvements to the use of such large
corpora to augment KB inference. First,
we present a new technique for combin-
ing KB relations and surface text into a
single graph representation that is much
more compact than graphs used in prior
work. Second, we describe how to incor-
porate vector space similarity into random
walk inference over KBs, reducing the fea-
ture sparsity inherent in using surface text.
This allows us to combine distributional
similarity with symbolic logical inference
in novel and effective ways. With exper-
iments on many relations from two sepa-
rate KBs, we show that our methods sig-
nificantly outperform prior work on KB
inference, both in the size of problem our
methods can handle and in the quality of
predictions made.
1 Introduction
Much work in recent years has gone into the
construction of large knowledge bases, either
by collecting contributions from many users,
as with Freebase (Bollacker et al., 2008) and
?
Research carried out while at the Machine Learning
Department, Carnegie Mellon University.
DBPedia (Mendes et al., 2012), or automat-
ically from web text or other resources, as
done by NELL (Carlson et al., 2010) and
YAGO (Suchanek et al., 2007). These knowl-
edge bases contain millions of real-world enti-
ties and relationships between them. However,
even though they are very large, they are still
very incomplete, missing large fractions of possi-
ble relationships between common entities (West
et al., 2014). Thus the task of inference over
these knowledge bases, predicting new relation-
ships simply by examining the knowledge base it-
self, has become increasingly important.
A promising technique for inferring new re-
lation instances in a knowledge base is random
walk inference, first proposed by Lao and Cohen
(2010). In this method, called the Path Ranking
Algorithm (PRA), the knowledge base is encoded
as a graph, and random walks are used to find
paths that connect the source and target nodes of
relation instances. These paths are used as features
in a logistic regression classifier that predicts new
instances of the given relation. Each path can be
viewed as a horn clause using knowledge base re-
lations as predicates, and so PRA can be thought
of as a kind of discriminatively trained logical in-
ference.
One major deficiency of random walk inference
is the connectivity of the knowledge base graph?
if there is no path connecting two nodes in the
graph, PRA cannot predict any relation instance
between them. Thus prior work has introduced the
use of a text corpus to increase the connectivity of
the graph used as input to PRA (Lao et al., 2012;
Gardner et al., 2013). This approach is not without
its own problems, however. Whereas knowledge
base relations are semantically coherent and dif-
ferent relations have distinct meanings, this is not
397
true of surface text. For example, ?The Nile flows
through Cairo? and ?The Nile runs through Cairo?
have very similar if not identical meaning. Adding
a text corpus to the inference graph increases con-
nectivity, but it also dramatically increases feature
sparsity.
We introduce two new techniques for making
better use of a text corpus for knowledge base in-
ference. First, we describe a new way of incor-
porating the text corpus into the knowledge base
graph that enables much more efficient process-
ing than prior techniques, allowing us to approach
problems that prior work could not feasibly solve.
Second, we introduce the use of vector space sim-
ilarity in random walk inference in order to reduce
the sparsity of surface forms. That is, when fol-
lowing a sequence of edge types in a random walk
on a graph, we allow the walk to follow edges that
are semantically similar to the given edge types,
as defined by some vector space embedding of the
edge types. If a path calls for an edge of type
?flows through?, for example, we accept other
edge types (such as ?runs through?) with probabil-
ity proportional to the vector space similarity be-
tween the two edge types. This lets us combine
notions of distributional similarity with symbolic
logical inference, with the result of decreasing the
sparsity of the feature space considered by PRA.
We show with experiments using both the NELL
and Freebase knowledge bases that this method
gives significantly better performance than prior
approaches to incorporating text data into random
walk inference.
2 Graph Construction
Our method for knowledge base inference, de-
scribed in Section 3, performs random walks over
a graph to obtain features for a logistic regression
classifier. Prior to detailing that technique, we first
describe how we produce a graph G = (N , E ,R)
from a set of knowledge base (KB) relation in-
stances and a set of surface relation instances ex-
tracted from a corpus. Producing a graph from
a knowledge base is straightforward: the set of
nodes N is made up of the entities in the KB; the
set of edge types R is the set of relation types in
the KB, and the typed edges E correspond to re-
lation instances from the KB, with one edge of
type r connecting entity nodes for each (n
1
, r, n
2
)
triple in the KB. Less straightforward is how to
construct a graph from a corpus, and how to con-
nect that graph to the KB graph. We describe our
methods for each of those below.
To create a graph from a corpus, we first prepro-
cess the corpus to obtain a collection of surface
relations, such as those extracted by open infor-
mation extraction systems like OLLIE (Mausam et
al., 2012). These surface relations consist of a pair
of noun phrases in the corpus, and the verb-like
connection between them (either an actual verb,
as done by Talukdar et al. (2012), a dependency
path, as done by Riedel et al. (2013), or OpenIE
relations (Mausam et al., 2012)). The verb-like
connections are naturally represented as edges in
the graph, as they have a similar semantics to the
knowledge base relations that are already repre-
sented as edges. We thus create a graph from these
triples exactly as we do from a KB, with nodes cor-
responding to noun phrase types and edges corre-
sponding to surface relation triples.
So far these two subgraphs we have created
are entirely disconnected, with the KB graph con-
taining nodes representing entities, and the sur-
face relation graph containing nodes representing
noun phrases, with no edges between these noun
phrases and entities. We connect these two graphs
by making use of the ALIAS relation in the KB,
which links entities to potential noun phrase ref-
erents. Each noun phrase in the surface relation
graph is connected to those entity nodes which the
noun phrase can possibly refer to according to the
KB. These edges are not the output of an entity
linking system, as done by Lao et al. (2012), but
express instead the notion that the noun phrase can
refer to the KB entity. The use of an entity linking
system would certainly allow a stronger connec-
tion between noun phrase nodes and entity nodes,
but it would require much more preprocessing and
a much larger graph representation, as each men-
tion of each noun phrase would need its own node,
as opposed to letting every mention of the same
noun phrase share the same node. This graph rep-
resentation allows us to add tens of millions of sur-
face relations to a graph of tens of millions of KB
relations, and perform all of the processing on a
single machine.
As will be discussed in more detail in Section 4,
we also allow edge types to optionally have an as-
sociated vector that ideally captures something of
the semantics of the edge type.
Figure 1 shows the graph constructions used in
our experiments on a subset of KB and surface re-
398
KB Relations:
(Monongahela, RIVERFLOWSTHROUGHCITY, Pittsburgh)
(Pittsburgh, ALIAS, ?Pittsburgh?)
(Pittsburgh, ALIAS, ?Steel City?)
(Monongahela, ALIAS, ?Monongahela River?)
(Monongahela, ALIAS, ?The Mon?)
Surface Relations:
(?The Mon?, ?flows through?, ?Steel City?)
(?Monongahela River?, ?runs through?, ?Pittsburgh?)
Embeddings:
?flows through?: [.2, -.1, .9]
?runs through?: [.1, -.3, .8]
(a) An example data set.
(c) An example graph that replaces surface relations with a
cluster label, as done by Gardner et al. (2013). Note, how-
ever, that the graph structure differs from that prior work;
see Section 5.
(b) An example graph that combines a KB and surface rela-
tions.
(d) An example graph that uses vector space representations
of surface edges, as introduced in this paper.
Figure 1: Example graph construction as used in the experiments in this paper. A graph using only KB
edges is simply a subset of these graphs containing only the RIVERFLOWSTHROUGHCITY edge, and is
not shown.
lations. Note that Figures 1b and 1c are shown as
rough analogues of graphs used in prior work (de-
scribed in more detail in Section 5), and we use
them for comparison in our experiments.
3 The Path Ranking Algorithm
We perform knowledge base inference using the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010). We begin this section with a brief overview
of PRA, then we present our modification to the
PRA algorithm that allows us to incorporate vector
space similarity into random walk inference.
PRA can be thought of as a method for exploit-
ing local graph structure to generate non-linear
feature combinations for a prediction model. PRA
generates a feature matrix over pairs of nodes in
a graph, then uses logistic regression to classify
those node pairs as belonging to a particular rela-
tion.
More formally, given a graph G with nodes N ,
edges E , and edge labelsR, and a set of node pairs
(s
i
, t
i
) ? D, one can create a connectivity matrix
where rows correspond to node pairs and columns
correspond to edge lables. PRA augments this
matrix with additional columns corresponding to
sequences of edge labels, called path types, and
changes the cell values from representing the pres-
ence of an edge to representing the specificity of
the connection that the path type makes between
the node pair.
Because the feature space considered by PRA
is so large (the set of all possible edge label se-
quences, with cardinality
?
l
i=1
|R|
i
, assuming a
bound l on the maximum path length), the first
step PRA must perform is feature selection, which
is done using random walks over the graph. The
second step of PRA is feature computation, where
each cell in the feature matrix is computed using
a constrained random walk that follows the path
type corresponding to the feature. We now explain
each of these steps in more detail.
Feature selection finds path types pi that are
likely to be useful in predicting new instances of
the relation represented by the input node pairs .
These path types are found by performing random
walks on the graph G starting at the source and
target nodes in D, recording which paths connect
some source node with its target. The edge se-
quences are ranked by frequency of connecting a
source node to a corresponding target node, and
the top k are kept.
Feature computation. Once a set of path types
399
is selected as features, the next step of the PRA
algorithm is to compute a value for each cell in the
feature matrix, corresponding to a node pair and a
path type. The value computed is the probability
of arriving at the target node of a node pair, given
that a random walk began at the source node and
was constrained to follow the path type: p(t|s, pi).
Once these steps have been completed, the re-
sulting feature matrix can be used with whatever
model or learning algorithm is desired; in this and
prior work, simple logistic regression has been
used as the prediction algorithm.
4 Vector space random walks
Our modifications to PRA are confined entirely to
the feature computation step described above; fea-
ture selection (finding potentially useful sequences
of edge types) proceeds as normal, using the sym-
bolic edge types. When computing feature val-
ues, however, we allow a walk to follow an edge
that is semantically similar to the edge type in the
path, as defined by Euclidean distance in the vec-
tor space.
More formally, consider a path type pi. Re-
call that pi is a sequence of edge types <
e
1
, e
2
, . . . , e
l
>, where l is the length of the path;
we will use pi
i
to denote the i
th
edge type in the
sequence. To compute feature values, PRA begins
at some node and follows edges of type pi
i
until
the sequence is finished and a target node has been
reached. Specifically, if a random walk is at a node
n with m outgoing edge types {e
1
, e
2
, . . . , e
m
},
PRA selects the edge type from that set which
matches pi
i
, then selects uniformally at random
from all outgoing edges of that type. If there is
no match in the set, the random walk restarts from
the original start node.
We modify the selection of which edge type to
follow. When a random walk is at a node n with
m outgoing edge types {e
1
, e
2
, . . . , e
m
}, instead
of selecting only the edge type that matches pi
i
,
we allow the walk to select instead an edge that
is close to pi
i
in vector space. For each edge type
at node n, we select the edge with the following
probability:
p(e
j
|pi
i
) ? exp(??v(e
j
) ?v(pi
i
)), ?j, 1 ? j ? m
where v(?) is a function that returns the vector
representation of an edge type, and ? is a spiki-
ness parameter that determines how much weight
to give to the vector space similarity. As ? ap-
proaches infinity, the normalized exponential ap-
proximates a delta function on the closest edge
type to pi
i
, in {e
1
, e
2
, . . . , e
m
}. If pi
i
is in the set
of outgoing edges, this algorithm converges to the
original PRA.
However, if pi
i
is not in the set of outgoing edge
types at a node and all of the edge types are very
dissimilar to pi
i
, this algorithm (with ? not close to
infinity) will lead to a largely uniform distribution
over edge types at that node, and no way for the
random walk to restart. To recover the restart be-
havior of the original PRA, we introduce an addi-
tional restart parameter?, and add another value to
the categorical distribution before normalization:
p(restart|pi
i
) ? exp(? ? ?)
When this restart type is selected, the random
walk begins again, following pi
1
starting at the
source node. With ? set to a value greater than the
maximum similarity between (non-identical) edge
type vectors, and ? set to infinity, this algorithm
exactly replicates the original PRA.
Not all edge types have vector space representa-
tions: the ALIAS relation cannot have a meaning-
ful vector representation, and we do not use vec-
tors to represent KB relations, finding that doing
so was not useful in practice (which makes intu-
itive sense: KB relations are already latent repre-
sentations themselves). While performing random
walks, if pi
i
has no vector representation, we fall
back to the original PRA algorithm for selecting
the next edge.
We note here that when working with vector
spaces it is natural to try clustering the vectors to
reduce the parameter space. Each path type pi is
a feature in our model, and if two path types dif-
fer only in one edge type, and the differing edge
types have very similar vectors, the resultant fea-
ture values will be essentially identical for both
path types. It seems reasonable that running a
simple clustering algorithm over these path types,
to reduce the number of near-duplicate features,
would improve performance. We did not find this
to be the case, however; all attempts we made to
use clustering over these vectors gave performance
indistinguishable from not using clustering. From
this we conclude that the main issue hindering per-
formance when using PRA over these kinds of
graphs is one of limited connectivity, not one of
too many parameters in the model. Though the
400
feature space considered by PRA is very large, the
number of attested features in a real graph is much
smaller, and it is this sparsity which our vector
space methods address.
5 Related Work
Knowledge base inference. Random walk infer-
ence over knowledge bases was first introduced by
Lao and Cohen (2010). This work was improved
upon shortly afterward to also make use of a large
corpus, by representing the corpus as a graph and
connecting it to the knowledge base (Lao et al.,
2012). Gardner et al. (2013) further showed that
replacing surface relation labels with a represen-
tation of a latent embedding of the relation led
to improved prediction performance. This result
is intuitive: the feature space considered by PRA
is exponentially large, and surface relations are
sparse. The relations ?[river] flows through [city]?
and ?[river] runs through [city]? have near iden-
tical meaning, and both should be very predic-
tive for the knowledge base relation RIVERFLOW-
STHROUGHCITY. However, if one of these rela-
tions only appears in the training data and the other
only appears in the test data, neither will be useful
for prediction. Gardner et al. (2013) attempted to
solve this issue by finding a latent symbolic repre-
sentation of the surface relations (such as a cluster-
ing) and replacing the edge labels in the graph with
these latent representations. This makes it more
likely for surface relations seen in training data to
also be seen at test time, and naturally improved
performance.
This representation, however, is still brittle, as
it is still a symbolic representation that is prone to
mismatches between training and test data. If the
clustering algorithm used is too coarse, the fea-
tures will not be useful, and if it is too fine, there
will be more mismatches. Also, verbs that are on
the boundaries of several clusters are problematic
to represent in this manner. We solve these prob-
lems by modifying the PRA algorithm to directly
use vector representations of edge types during the
random walk inference.
These two prior techniques are the most directly
related work to what we present in this paper, and
we compare our work to theirs.
Graph construction. In addition to the incor-
poration of vector space similarity into the PRA
algorithm, the major difference between our work
and the prior approaches mentioned above is in the
construction of the graph used by PRA. We con-
trast our method of graph construction with these
prior approaches in more detail below.
Lao et al. (2012) represent every word of ev-
ery sentence in the corpus as a node in the graph,
with edges between the nodes representing depen-
dency relationships between the words. They then
connect this graph to the KB graph using a simple
entity linking system (combined with coreference
resolution). The resultant graph is enormous, such
that they needed to do complex indexing on the
graph and use a cluster of 500 machines to per-
form the PRA computations. Also, as the edges
represent dependency labels, not words, with this
graph representation the PRA algorithm does not
have access to the verbs or other predicative words
that appear in the corpus, which frequently express
relations. PRA only uses edge types as feature
components, not node types, and so the rich infor-
mation contained in the words is lost. This graph
construction also would not allow the incorpora-
tion of vector space similarity that we introduced,
as dependency labels do not lend themselves well
to vector space representations.
Gardner et al. (2013) take an approach very sim-
ilar to the one presented in Section 2, preprocess-
ing the corpus to obtain surface relations. How-
ever, instead of creating a graph with nodes rep-
resenting noun phrases, they added edges from
the surface relations directly to the entity nodes
in the graph. Using the ALIAS relation, as we do,
they added an edge between every possible con-
cept pair that could be represented by the noun
phrases in a surface relation instance. This leads
to some nonsensical edges added to the graph,
and if the ALIAS relation has high degree (as it
does for many common noun phrases in Freebase),
it quickly becomes unscalable?this method of
graph construction runs out of disk space when
attempting to run on the Freebase experiments in
Section 6. Also, in conflating entity nodes in the
graph with noun phrases, they lose an important
distinction that turns out to be useful for predic-
tion, as we discuss in Section 6.4.
1
1
Recent notions of ?universal schema? (Riedel et al.,
2013) also put KB entities and noun phrases into the same
conceptual space, though they opt for using noun phrases in-
stead of the KB entities used by Gardner et al. In general
this is problematic, as it relies on some kind of entity linking
system as preprocessing, and cannot handle common noun
references of proper entities without losing information. Our
method, and that of Lao et al., skirts this issue entirely by not
trying to merge KB entities with noun phrases.
401
Other related work. Also related to the present
work is recent research on programming lan-
guages for probabilistic logic (Wang et al., 2013).
This work, called ProPPR, uses random walks to
locally ground a query in a small graph before per-
forming propositional inference over the grounded
representation. In some sense this technique is
like a recursive version of PRA, allowing for more
complex inferences than a single iteration of PRA
can make. However, this technique has not yet
been extended to work with large text corpora, and
it does not yet appear to be scalable enough to han-
dle the large graphs that we use in this work. How
best to incorporate the work presented in this pa-
per with ProPPR is an open, and very interesting,
question.
Examples of other systems aimed at reason-
ing over common-sense knowledge are the CYC
project (Lenat, 1995) and ConceptNet (Liu and
Singh, 2004). These common-sense resources
could easily be incorporated into the graphs we
use for performing random walk inference.
Lines of research that seek to incorporate dis-
tributional semantics into traditional natural lan-
guage processing tasks, such as parsing (Socher
et al., 2013a), named entity recognition (Passos et
al., 2014), and sentiment analysis (Socher et al.,
2013b), are also related to what we present in this
paper. While our task is quite different from these
prior works, we also aim to combine distributional
semantics with more traditional methods (in our
case, symbolic logical inference), and we take in-
spiration from these methods.
6 Experiments
We perform both the feature selection step and the
feature computation step of PRA using GraphChi,
an efficient single-machine graph processing li-
brary (Kyrola et al., 2012). We use MAL-
LET?s implementation of logistic regression, with
both L1 and L2 regularization (McCallum, 2002).
To obtain negative evidence, we used a closed
world assumption, treating any (source, target)
pair found during the feature computation step as
a negative example if it was not given as a positive
example. We tuned the parameters to our methods
using a coarse, manual grid search with cross vali-
dation on the training data described below. The
parameters we tuned were the L1 and L2 regu-
larization parameters, how many random walks to
perform in the feature selection and computation
NELL Freebase
Entities 1.2M 20M
Relation instances 3.4M 67M
Total relation types 520 4215
Relation types tested 10 24
Avg. instances/relation 810 200
SVO triples used 404k 28M
Table 1: Statistics of the data used in our experi-
ments.
steps of PRA, and spikiness and restart parameters
for vector space walks. The results presented were
not very sensitive to changes in these parameters.
6.1 Data
We ran experiments on both the NELL and Free-
base knowledge bases. The characteristics of these
knowledge bases are shown in Table 1. The Free-
base KB is very large; to make it slightly more
manageable we filtered out relations that did not
seem applicable to relation extraction, as well as a
few of the largest relations.
2
This still left a very
large, mostly intact KB, as can be seen in the ta-
ble. For our text corpus, we make use of a set of
subject-verb-object triples extracted from depen-
dency parses of ClueWeb documents (Talukdar et
al., 2012). There are 670M such triples in the
data set, most of which are completely irrelevant to
the knowledge base relations we are trying to pre-
dict. For each KB, we filter the SVO triples, keep-
ing only those which can possibly connect training
and test instances of the relations we used in our
experiments. The number of SVO triples kept for
each KB is also shown in Table 1. We obtained
vector space representations of these surface rela-
tions by running PCA on the SVO matrix.
We selected 10 NELL relations and 24 Free-
base relations for testing our methods. The NELL
relations were hand-selected as the relations with
the largest number of known instances that had a
reasonable precision (the NELL KB is automati-
cally created, and some relations have low preci-
sion). We split the known instances of these rela-
tions into 75% training and 25% testing, giving on
average about 650 training instances and 160 test
2
We removed anything under /user, /common, /type (ex-
cept for the relation /type/object/type), /base, and /freebase,
as not applicable to our task. We also removed relations deal-
ing with individual music tracks, book editions, and TV epid-
sodes, as they are very large, very specific, and unlikely to be
useful for predicting the relations in our test set.
402
instances for each relation.
The 24 Freebase relations were semi-randomly
selected. We first filtered the 4215 relations based
on two criteria: the number of relation instances
must be between 1000 and 10000, and there must
be no mediator in the relation.
3
Once we selected
the relations, we kept all instances of each rela-
tion that had some possible connection in the SVO
data.
4
This left on average 200 instances per rela-
tion, which we again split 75%-25% into training
and test sets.
6.2 Methods
The methods we compare correspond to the graphs
shown in Figure 1. The KB method uses the orig-
inal PRA algorithm on just the KB relations, as
presented by Lao and Cohen (2010). KB + SVO
adds surface relations to the graph (Figure 1b). We
present this as roughly analogous to the methods
introduced by Lao et al. (2012), though with some
significant differences in graph representation, as
described in Section 5. KB + Clustered SVO fol-
lows the methods of Gardner et al. (2013), but us-
ing the graph construction introduced in this pa-
per (Figure 1c; their graph construction techniques
would have made graphs too large to be feasible
for the Freebase experiments). KB + Vector SVO
is our method (Figure 1d).
6.3 Evaluation
As evaluation metrics, we use mean average pre-
cision (MAP) and mean reciprocal rank (MRR),
following recent work evaluating relation extrac-
tion performance (West et al., 2014). We test sig-
nificance using a paired permutation test.
The results of these experiments are shown in
Table 2 and Table 3. In Table 4 we show average
precision for every relation tested on the NELL
KB, and we show the same for Freebase in Table 5.
6.4 Discussion
We can see from the tables that KB + Vector SVO
(the method presented in this paper) significantly
outperforms prior approaches in both MAP and
3
A mediator in Freebase is a reified relation in-
stance meant to handle n-ary relations, for instance
/film/performance. PRA in general, and our implementation
of it in particular, needs some modification to be well-suited
to predicting relations with mediators.
4
We first tried randomly selecting instances from these re-
lations, but found that the probability of selecting an instance
that benefited from an SVO connection was negligible. In or-
der to make use of the methods we present, we thus restricted
ourselves to only those that had a possible SVO connection.
Method MAP MRR
KB 0.193 0.635
KB + SVO 0.218 0.763
KB + Clustered SVO 0.276 0.900
KB + Vector SVO 0.301 0.900
Table 2: Results on the NELL knowledge base.
The bolded line is significantly better than all other
results with p < 0.025.
Method MAP MRR
KB 0.278 0.614
KB + SVO 0.294 0.639
KB + Clustered SVO 0.326 0.651
KB + Vector SVO 0.350 0.670
Table 3: Results on the Freebase knowledge base.
The bolded line is significantly better than all other
results with p < 0.0002.
MRR. We believe that this is due to the reduction
in feature sparsity enabled by using vector space
instead of symbolic representations (as that is the
only real difference between KB + Clustered SVO
and KB + Vector SVO), allowing PRA to make
better use of path types found in the training data.
When looking at the results for individual relations
in Table 4 and Table 5, we see that KB + Vector
SVO outperforms other methods on the majority
of relations, and it is a close second when it does
not.
We can also see from the results that mean av-
erage precision seems a little low for all meth-
ods tested. This is because MAP is computed as
the precision of all possible correct predictions in
a ranked list, where precision is counted as 0 if
the correct prediction is not included in the list.
In other words, there are many relation instances
in our randomly selected test set that are not in-
ferrable from the knowledge base, and the low re-
call hurts the MAP metric. MRR, which judges the
precision of the top prediction for each relation,
gives us some confidence that the main issue here
is one of recall, as MRR is reasonably high, es-
pecially on the NELL KB. As further evidence, if
we compute average precision for each query node
(instead of for each relation), excluding queries for
which the system did not return any predictions,
MAP ranges from .29 (KB) to .45 (KB + Vector
SVO) on NELL (with around 30% of queries hav-
ing no prediction), and from .40 (KB) to .49 (KB +
403
Relation KB KB + SVO KB + Clustered SVO KB + Vector SVO
ActorStarredInMovie 0.000 0.032 0.032 0.037
AthletePlaysForTeam 0.200 0.239 0.531 0.589
CityLocatedInCountry 0.126 0.169 0.255 0.347
JournalistWritesForPublication 0.218 0.254 0.291 0.319
RiverFlowsThroughCity 0.000 0.001 0.052 0.076
SportsTeamPositionForSport 0.217 0.217 0.178 0.180
StadiumLocatedInCity 0.090 0.156 0.275 0.321
StateHasLake 0.000 0.000 0.000 0.000
TeamPlaysInLeague 0.934 0.936 0.947 0.939
WriterWroteBook 0.144 0.179 0.195 0.202
Table 4: Average precision for each relation tested on the NELL KB. The best performing method on
each relation is bolded.
Relation KB KB + SVO KB + C-SVO KB + V-SVO
/amusement parks/park/rides 0.000 0.009 0.004 0.013
/architecture/architect/structures designed 0.072 0.199 0.257 0.376
/astronomy/constellation/contains 0.004 0.017 0.000 0.008
/automotive/automotive class/examples 0.003 0.001 0.002 0.006
/automotive/model/automotive class 0.737 0.727 0.742 0.768
/aviation/airline/hubs 0.322 0.286 0.298 0.336
/book/literary series/author s 0.798 0.812 0.818 0.830
/computer/software genre/software in genre 0.000 0.001 0.001 0.001
/education/field of study/journals in this discipline 0.001 0.003 0.003 0.001
/film/film/rating 0.914 0.905 0.914 0.905
/geography/island/body of water 0.569 0.556 0.580 0.602
/geography/lake/basin countries 0.420 0.361 0.409 0.437
/geography/lake/cities 0.111 0.134 0.177 0.175
/geography/river/cities 0.030 0.038 0.045 0.066
/ice hockey/hockey player/hockey position 0.307 0.243 0.222 0.364
/location/administrative division/country 0.989 0.988 0.991 0.989
/medicine/disease/symptoms 0.061 0.078 0.068 0.067
/medicine/drug/drug class 0.169 0.164 0.135 0.157
/people/ethnicity/languages spoken 0.134 0.226 0.188 0.223
/spaceflight/astronaut/missions 0.010 0.186 0.796 0.848
/transportation/bridge/body of water spanned 0.534 0.615 0.681 0.727
/tv/tv program creator/programs created 0.164 0.179 0.163 0.181
/visual art/art period movement/associated artists 0.044 0.040 0.046 0.037
/visual art/visual artist/associated periods or movements 0.276 0.295 0.282 0.290
Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on
each relation is bolded. For space considerations, ?Clustered SVO? is shortened to ?C-SVO? and ?Vector
SVO? is shortened to ?V-SVO? in the table header.
404
Vector SVO) on Freebase, (where 21% of queries
gave no prediction). Our methods thus also im-
prove MAP when calculated in this manner, but it
is not an entirely fair metric,
5
so we use standard
MAP to present our main results.
One interesting phenomenon to note is
a novel use of the ALIAS relation in some
of the relation models. The best exam-
ple of this was found with the relation
/people/ethnicity/languages spoken. A
high-weighted feature when adding surface
relations was the edge sequence <ALIAS, ALIAS
INVERSE>. This edge sequence reflects the
fact that languages frequently share a name
with the group of people that speaks them (e.g.,
Maori, French). And because PRA can gen-
erate compositional features, we also find the
following edge sequence for the same relation:
</people/ethnicity/included in group,
ALIAS, ALIAS INVERSE>. This feature captures
the same notion that languages get their names
from groups of people, but applies it to subgroups
within an ethnicity. These features would be
very difficult, perhaps impossible, to include in
systems that do not distinguish between noun
phrases and knowledge base entities, such as
the graphs constructed by Gardner et al. (2013),
or typical relation extraction systems, which
generally only work with noun phrases after
performing a heuristic entity linking.
7 Conclusion
We have offered two main contributions to the task
of knowledge base inference. First, we have pre-
sented a new technique for combining knowledge
base relations and surface text into a single graph
representation that is much more compact than
graphs used in prior work. This allowed us to ap-
ply methods introduced previously to much larger
problems, running inference on a single machine
over the entire Freebase KB combined with tens of
millions of surface relations. Second, we have de-
scribed how to incorporate vector space similarity
into random walk inference over knowledge bases,
reducing the feature sparsity inherent in using sur-
face text. This allows us to combine distributional
similarity with symbolic logical inference in novel
and effective ways. With experiments on many
5
MAP is intended to include some sense of recall, but ex-
cluding queries with no predictions removes that and opens
the metric to opportunistic behavior.
relations from two separate knowledge bases, we
have shown that our methods significantly outper-
form prior work on knowledge base inference.
The code and data used in the ex-
periments in this paper are available at
http://rtw.ml.cmu.edu/emnlp2014 vector space pra/.
Acknowledgments
This research has been supported in part by
DARPA under contract number FA8750-13-2-
0005, by NSF under grant 31043,18,1121946, and
by generous support from Yahoo! and Google.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of SIGMOD.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using latent
syntactic cues. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin.
2012. Graphchi: Large-scale graph computation
on just a pc. In Proceedings of the 10th USENIX
Symposium on Operating Systems Design and Im-
plementation (OSDI), pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Hugo Liu and Push Singh. 2004. Conceptnet: a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534. Asso-
ciation for Computational Linguistics.
405
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Pablo N. Mendes, Max Jakob, and Christian Bizer.
2012. Dbpedia for nlp: A multilingual cross-domain
knowledge base. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC?12).
Alexandre Passos, Vineet Kumar, and Andrew Mc-
Callum. 2014. Lexicon infused phrase embed-
dings for named entity resolution. arXiv preprint
arXiv:1404.5367.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Acquiring temporal constraints be-
tween relations. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 992?1001. ACM.
William Yang Wang, Kathryn Mazaitis, and
William W. Cohen. 2013. Programming with
personalized pagerank: A locally groundable
first-order probabilistic logic. In Proceedings of the
22Nd ACM International Conference on Conference
on Information &#38; Knowledge Management,
CIKM ?13, pages 2129?2138, New York, NY, USA.
ACM.
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In WWW.
406
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1473?1481,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Experiments in Graph-based Semi-Supervised Learning Methods for
Class-Instance Acquisition
Partha Pratim Talukdar?
Search Labs, Microsoft Research
Mountain View, CA 94043
partha@talukdar.net
Fernando Pereira
Google, Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
Graph-based semi-supervised learning
(SSL) algorithms have been successfully
used to extract class-instance pairs from
large unstructured and structured text col-
lections. However, a careful comparison
of different graph-based SSL algorithms
on that task has been lacking. We com-
pare three graph-based SSL algorithms
for class-instance acquisition on a variety
of graphs constructed from different do-
mains. We find that the recently proposed
MAD algorithm is the most effective. We
also show that class-instance extraction
can be significantly improved by adding
semantic information in the form of
instance-attribute edges derived from
an independently developed knowledge
base. All of our code and data will be
made publicly available to encourage
reproducible research in this area.
1 Introduction
Traditionally, named-entity recognition (NER) has
focused on a small number of broad classes such
as person, location, organization. However, those
classes are too coarse to support important ap-
plications such as sense disambiguation, seman-
tic matching, and textual inference in Web search.
For those tasks, we need a much larger inventory
of specific classes and accurate classification of
terms into those classes. While supervised learn-
ing methods perform well for traditional NER,
they are impractical for fine-grained classification
because sufficient labeled data to train classifiers
for all the classes is unavailable and would be very
expensive to obtain.
? Research carried out while at the University of Penn-
sylvania, Philadelphia, PA, USA.
To overcome these difficulties, seed-based in-
formation extraction methods have been devel-
oped over the years (Hearst, 1992; Riloff and
Jones, 1999; Etzioni et al, 2005; Talukdar et
al., 2006; Van Durme and Pas?ca, 2008). Start-
ing with a few seed instances for some classes,
these methods, through analysis of unstructured
text, extract new instances of the same class. This
line of work has evolved to incorporate ideas from
graph-based semi-supervised learning in extrac-
tion from semi-structured text (Wang and Cohen,
2007), and in combining extractions from free
text and from structured sources (Talukdar et al,
2008). The benefits of combining multiple sources
have also been demonstrated recently (Pennac-
chiotti and Pantel, 2009).
We make the following contributions:
? Even though graph-based SSL algorithms
have achieved early success in class-instance
acquisition, there is no study comparing dif-
ferent graph-based SSL methods on this task.
We address this gap with a series of experi-
ments comparing three graph-based SSL al-
gorithms (Section 2) on graphs constructed
from several sources (Metaweb Technolo-
gies, 2009; Banko et al, 2007).
? We investigate whether semantic informa-
tion in the form of instance-attribute edges
derived from an independent knowledge
base (Suchanek et al, 2007) can improve
class-instance acquisition. The intuition be-
hind this is that instances that share attributes
are more likely to belong to the same class.
We demonstrate that instance-attribute edges
significantly improve the accuracy of class-
instance extraction. In addition, useful class-
attribute relationships are learned as a by-
product of this process.
? In contrast to previous studies involving pro-
1473
prietary datasets (Van Durme and Pas?ca,
2008; Talukdar et al, 2008; Pennacchiotti
and Pantel, 2009), all of our experiments use
publicly available datasets and we plan to re-
lease our code1.
In Section 2, we review three graph-based
SSL algorithms that are compared for the class-
instance acquisition task in Section 3. In Section
3.6, we show how additional instance-attribute
based semantic constraints can be used to improve
class-instance acquisition performance. We sum-
marize the results and outline future work in Sec-
tion 4.
2 Graph-based SSL
We now review the three graph-based SSL algo-
rithms for class inference over graphs that we have
evaluated.
2.1 Notation
All the algorithms compute a soft assignment of
labels to the nodes of a graph G = (V,E,W ),
where V is the set of nodes with |V | = n, E is
the set of edges, and W is an edge weight ma-
trix. Out of the n = nl + nu nodes in G, nl
nodes are labeled, while the remaining nu nodes
are unlabeled. If edge (u, v) 6? E, Wuv = 0.
The (unnormalized) Laplacian, L, ofG is given by
L = D?W , whereD is an n?n diagonal degree
matrix with Duu =
?
vWuv. Let S be an n ? n
diagonal matrix with Suu = 1 iff node u ? V is
labeled. That is, S identifies the labeled nodes in
the graph. C is the set of labels, with |C| = m
representing the total number of labels. Y is the
n ? m matrix storing training label information,
if any. Y? is an n ?m matrix of soft label assign-
ments, with Y?vl representing the score of label l
on node v. A graph-based SSL computes Y? from
{G,SY }.
2.2 Label Propagation (LP-ZGL)
The label propagation method presented by Zhu
et al (2003), which we shall refer to as LP-ZGL
in this paper, is one of the first graph-based SSL
methods. The objective minimized by LP-ZGL is:
min
Y?
?
l?C
Y? >l LY?l, s.t. SYl = SY?l (1)
1
http://www.talukdar.net/datasets/class inst/
where Y?l of size n ? 1 is the lth column of Y? .
The constraint SY = SY? makes sure that the su-
pervised labels are not changed during inference.
The above objective can be rewritten as:
?
l?C
Y? >l LY?l =
?
u,v?V,l?C
Wuv(Y?ul ? Y?vl)
2
From this, we observe that LP-ZGL penalizes any
label assignment where two nodes connected by a
highly weighted edge are assigned different labels.
In other words, LP-ZGL prefers smooth labelings
over the graph. This property is also shared by the
two algorithms we shall review next. LP-ZGL has
been the basis for much subsequent work in the
graph-based SSL area, and is still one of the most
effective graph-based SSL algorithms.
2.3 Adsorption
Adsorption (Baluja et al, 2008) is a graph-based
SSL algorithm which has been used for open-
domain class-instance acquisition (Talukdar et al,
2008). Adsorption is an iterative algorithm, where
label estimates on node v in the (t+ 1)th iteration
are updated using estimates from the tth iteration:
Y? (t+1)v ? p
inj
v ?Yv+p
cont
v ?B
(t)
v +p
abnd
v ?r (2)
where,
B(t)v =
?
u
Wuv
?
u? Wu?v
Y? (t)u
In (2), pinjv , p
cont
v , and p
abnd
v are three proba-
bilities defined on each node v ? V by Ad-
sorption; and r is a vector used by Adsorption
to express label uncertainty at a node. On each
node v, the three probabilities sum to one, i.e.,
pinjv + p
cont
v + p
abnd
v = 1, and they are based on
the random-walk interpretation of the Adsorption
algorithm (Talukdar et al, 2008). The main idea
of Adsorption is to control label propagation more
tightly by limiting the amount of information that
passes through a node. For instance, Adsorption
can reduce the importance of a high-degree node
v during the label inference process by increas-
ing pabndv on that node. For more details on these,
please refer to Section 2 of (Talukdar and Cram-
mer, 2009). In contrast to LP-ZGL, Adsorption
allows labels on labeled (seed) nodes to change,
which is desirable in case of noisy input labels.
1474
2.4 Modified Adsorption (MAD)
Talukdar and Crammer (2009) introduced a modi-
fication of Adsorption called MAD, which shares
Adsorption?s desirable properties but can be ex-
pressed as an unconstrained optimization problem:
min
Y?
?
l?C
[
?1
(
Yl ? Y?l
)>
S
(
Yl ? Y?l
)
+
?2Y?
>
l L
?
Y?l + ?3
?
?
?
?
?
?Y?l ?Rl
?
?
?
?
?
?
2
]
(3)
where ?1, ?2, and ?3 are hyperparameters; L
?
is the Laplacian of an undirected graph derived
from G, but with revised edge weights; and R is
an n ? m matrix of per-node label prior, if any,
with Rl representing the lth column of R. As in
Adsorption, MAD allows labels on seed nodes to
change. In case of MAD, the three random-walk
probabilities, pinjv , p
cont
v , and p
abnd
v , defined by
Adsorption on each node are folded inside the ma-
trices S,L
?
, andR, respectively. The optimization
problem in (3) can be solved with an efficient iter-
ative algorithm described in detail by Talukdar and
Crammer (2009).
These three algorithms are all easily paralleliz-
able in a MapReduce framework (Talukdar et al,
2008; Rao and Yarowsky, 2009), which makes
them suitable for SSL on large datasets. Addition-
ally, all three algorithms have similar space and
time complexity.
3 Experiments
We now compare the experimental performance
of the three graph-based SSL algorithms reviewed
in the previous section, using graphs constructed
from a variety of sources described below. Fol-
lowing previous work (Talukdar et al, 2008), we
use Mean Reciprocal Rank (MRR) as the evalua-
tion metric in all experiments:
MRR =
1
|Q|
?
v?Q
1
rv
(4)
where Q ? V is the set of test nodes, and rv is the
rank of the gold label among the labels assigned to
node v. Higher MRR reflects better performance.
We used iterative implementations of the graph-
based SSL algorithms, and the number of itera-
tions was treated as a hyperparameter which was
tuned, along with other hyperparameters, on sep-
arate held-out sets, as detailed in a longer version
of this paper. Statistics of the graphs used during
experiments in this section are presented in Table
1.
3.1 Freebase-1 Graph with Pantel Classes
Table ID: people-person
Name Place of Birth Gender
? ? ? ? ? ? ? ? ?
Isaac Newton Lincolnshire Male
Bob Dylan Duluth Male
Johnny Cash Kingsland Male
? ? ? ? ? ? ? ? ?
Table ID: film-music contributor
Name Film Music Credits
? ? ? ? ? ?
Bob Dylan No Direction Home
? ? ? ? ? ?
Figure 1: Examples of two tables from Freebase,
one table is from the people domain while the
other is from the film domain.
0.5
0.575
0.65
0.725
0.8
23 x 2 23 x 10
Freebase-1 Graph, 23 Pantel Classes
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Amount of Supervision (# classes x seeds per class)
LP-ZGL Adsorption MAD
Figure 3: Comparison of three graph transduction
methods on a graph constructed from the Freebase
dataset (see Section 3.1), with 23 classes. All re-
sults are averaged over 4 random trials. In each
group, MAD is the rightmost bar.
Freebase (Metaweb Technologies, 2009)2 is
a large collaborative knowledge base. The
knowledge base harvests information from many
open data sets (for instance Wikipedia and Mu-
sicBrainz), as well as from user contributions. For
our current purposes, we can think of the Freebase
2http://www.freebase.com/
1475
Graph Vertices Edges Avg. Min. Max.
Deg. Deg. Deg.
Freebase-1 (Section 3.1) 32970 957076 29.03 1 13222
Freebase-2 (Section 3.2) 301638 2310002 7.66 1 137553
TextRunner (Section 3.3) 175818 529557 3.01 1 2738
YAGO (Section 3.6) 142704 777906 5.45 0 74389
TextRunner + YAGO (Section 3.6) 237967 1307463 5.49 1 74389
Table 1: Statistics of various graphs used in experiments in Section 3. Some of the test instances in the
YAGO graph, added for fair comparison with the TextRunner graph in Section 3.6, had no attributes in
YAGO KB, and hence these instance nodes had degree 0 in the YAGO graph.
Bob Dylan
film-music_contributor-name
Johnny 
Cash
people-person-name
Isaac Newton
Bob Dylan
film-music_contributor-name
Johnny 
Cash
people-person-name
Isaac Newton
has_attribute:albums
(a) (b)
Figure 2: (a) Example of a section of the graph constructed from the two tables in Figure 1. Rectangular
nodes are properties, oval nodes are entities or cell values. (b) The graph in part (a) augmented with
an attribute node, has attribue:albums, along with the edges incident on it. This results is additional
constraints for the nodes Johnny Cash and Bob Dylan to have similar labels (see Section 3.6).
dataset as a collection of relational tables, where
each table is assigned a unique ID. A table con-
sists of one or more properties (column names)
and their corresponding cell values (column en-
tries). Examples of two Freebase tables are shown
in Figure 1. In this figure, Gender is a property
in the table people-person, and Male is a corre-
sponding cell value. We use the following process
to convert the Freebase data tables into a single
graph:
? Create a node for each unique cell value
? Create a node for each unique property name,
where unique property name is obtained by
prefixing the unique table ID to the prop-
erty name. For example, in Figure 1, people-
person-gender is a unique property name.
? Add an edge of weight 1.0 from cell-value
node v to unique property node p, iff value
v is present in the column corresponding to
property p. Similarly, add an edge in the re-
verse direction.
By applying this graph construction process on
the first column of the two tables in Figure 1, we
end up with the graph shown in Figure 2 (a). We
note that even though the resulting graph consists
of edges connecting nodes of different types: cell
value nodes to property nodes; the graph-based
SSL methods (Section 2) can still be applied on
such graphs as a cell value node and a property
node connected by an edge should be assigned
same or similar class labels. In other words, the la-
bel smoothness assumption (see Section 2.2) holds
on such graphs.
We applied the same graph construction pro-
cess on a subset of the Freebase dataset consist-
ing of topics from 18 randomly selected domains:
astronomy, automotive, biology, book, business,
1476
chemistry, comic books, computer, film, food, ge-
ography, location, people, religion, spaceflight,
tennis, travel, and wine. The topics in this subset
were further filtered so that only cell-value nodes
with frequency 10 or more were retained. We call
the resulting graph Freebase-1 (see Table 1).
Pantel et al (2009) have made available
a set of gold class-instance pairs derived
from Wikipedia, which is downloadable from
http://ow.ly/13B57. From this set, we selected
all classes which had more than 10 instances
overlapping with the Freebase graph constructed
above. This resulted in 23 classes, which along
with their overlapping instances were used as the
gold standard set for the experiments in this sec-
tion.
Experimental results with 2 and 10 seeds (la-
beled nodes) per class are shown in Figure 3. From
the figure, we see that that LP-ZGL and Adsorp-
tion performed comparably on this dataset, with
MAD significantly outperforming both methods.
3.2 Freebase-2 Graph with WordNet Classes
0.25
0.285
0.32
0.355
0.39
192 x 2 192 x 10
Freebase-2 Graph, 192 WordNet Classes
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Amount of Supervision (# classes x seeds per class)
LP-ZGL Adsorption MAD
Figure 4: Comparison of graph transduction meth-
ods on a graph constructed from the Freebase
dataset (see Section 3.2). All results are averaged
over 10 random trials. In each group, MAD is the
rightmost bar.
To evaluate how the algorithms scale up, we
construct a larger graph from the same 18 domains
as in Section 3.1, and using the same graph con-
struction process. We shall call the resulting graph
Freebase-2 (see Table 1). In order to scale up the
number of classes, we selected all Wordnet (WN)
classes, available in the YAGO KB (Suchanek et
al., 2007), that had more than 100 instances over-
lapping with the larger Freebase graph constructed
above. This resulted in 192 WN classes which we
use for the experiments in this section. The reason
behind imposing such frequency constraints dur-
ing class selection is to make sure that each class
is left with a sufficient number of instances during
testing.
Experimental results comparing LP-ZGL, Ad-
sorption, and MAD with 2 and 10 seeds per class
are shown in Figure 4. A total of 292k test nodes
were used for testing in the 10 seeds per class con-
dition, showing that these methods can be applied
to large datasets. Once again, we observe MAD
outperforming both LP-ZGL and Adsorption. It is
interesting to note that MAD with 2 seeds per class
outperforms LP-ZGL and adsorption even with 10
seeds per class.
3.3 TextRunner Graph with WordNet
Classes
0.15
0.2
0.25
0.3
0.35
170 x 2 170 x 10
TextRunner Graph, 170 WordNet Classes
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Amount of Supervision (# classes x seeds per class)
LP-ZGL Adsorption MAD
Figure 5: Comparison of graph transduction meth-
ods on a graph constructed from the hypernym tu-
ples extracted by the TextRunner system (Banko
et al, 2007) (see Section 3.3). All results are aver-
aged over 10 random trials. In each group, MAD
is the rightmost bar.
In contrast to graph construction from struc-
tured tables as in Sections 3.1, 3.2, in this section
we use hypernym tuples extracted by TextRun-
ner (Banko et al, 2007), an open domain IE sys-
tem, to construct the graph. Example of a hyper-
nym tuple extracted by TextRunner is (http, proto-
col, 0.92), where 0.92 is the extraction confidence.
To convert such a tuple into a graph, we create a
node for the instance (http) and a node for the class
(protocol), and then connect the nodes with two
1477
directed edges in both directions, with the extrac-
tion confidence (0.92) as edge weights. The graph
created with this process from TextRunner out-
put is called the TextRunner Graph (see Table 1).
As in Section 3.2, we use WordNet class-instance
pairs as the gold set. In this case, we considered
all WordNet classes, once again from YAGO KB
(Suchanek et al, 2007), which had more than 50
instances overlapping with the constructed graph.
This resulted in 170 WordNet classes being used
for the experiments in this section.
Experimental results with 2 and 10 seeds per
class are shown in Figure 5. The three methods
are comparable in this setting, with MAD achiev-
ing the highest overall MRR.
3.4 Discussion
If we correlate the graph statistics in Table 1 with
the results of sections 3.1, 3.2, and 3.3, we see
that MAD is most effective for graphs with high
average degree, that is, graphs where nodes tend
to connect to many other nodes. For instance,
the Freebase-1 graph has a high average degree
of 29.03, with a corresponding large advantage
for MAD over the other methods. Even though
this might seem mysterious at first, it becomes
clearer if we look at the objectives minimized
by different algorithms. We find that the objec-
tive minimized by LP-ZGL (Equation 1) is under-
regularized, i.e., its model parameters (Y? ) are not
constrained enough, compared to MAD (Equation
3, specifically the third term), resulting in overfit-
ting in case of highly connected graphs. In con-
trast, MAD is able to avoid such overfitting be-
cause of its minimization of a well regularized ob-
jective (Equation 3). Based on this, we suggest
that average degree, an easily computable struc-
tural property of the graph, may be a useful indica-
tor in choosing which graph-based SSL algorithm
should be applied on a given graph.
Unlike MAD, Adsorption does not optimize
any well defined objective (Talukdar and Cram-
mer, 2009), and hence any analysis along the lines
described above is not possible. The heuristic
choices made in Adsorption may have lead to its
sub-optimal performance compared to MAD; we
leave it as a topic for future investigation.
3.5 Effect of Per-Node Class Sparsity
For all the experiments in Sections 3.1, 3.2, and
3.6, each node was allowed to have a maximum
of 15 classes during inference. After each update
0.3
0.33
0.36
0.39
0.42
5 15 25 35 45
Effect of Per-node Sparsity Constraint
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Maximum Allowed Classes per Node
Figure 6: Effect of per node class sparsity (maxi-
mum number of classes allowed per node) during
MAD inference in the experimental setting of Fig-
ure 4 (one random split).
on a node, all classes except for the top scoring
15 classes were discarded. Without such sparsity
constraints, a node in a connected graph will end
up acquiring all the labels injected into the graph.
This is undesirable for two reasons: (1) for ex-
periments involving a large numbers of classes (as
in the previous section and in the general case of
open domain IE), this increases the space require-
ment and also slows down inference; (2) a partic-
ular node is unlikely to belong to a large num-
ber of classes. In order to estimate the effect of
such sparsity constraints, we varied the number
of classes allowed per node from 5 to 45 on the
graph and experimental setup of Figure 4, with 10
seeds per class. The results for MAD inference
over the development split are shown in Figure
6. We observe that performance can vary signifi-
cantly as the maximum number of classes allowed
per node is changed, with the performance peak-
ing at 25. This suggests that sparsity constraints
during graph based SSL may have a crucial role to
play, a question that needs further investigation.
3.6 TextRunner Graph with additional
Semantic Constraints from YAGO
Recently, the problem of instance-attribute extrac-
tion has started to receive attention (Probst et al,
2007; Bellare et al, 2007; Pasca and Durme,
2007). An example of an instance-attribute pair
is (Bob Dylan, albums). Given a set of seed
instance-attribute pairs, these methods attempt to
extract more instance-attribute pairs automatically
1478
0.18
0.23
0.28
0.33
0.38
LP-ZGL Adsorption MAD
170 WordNet Classes, 2 Seeds per Class
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Algorithms
TextRunner Graph
YAGO Graph
TextRunner + YAGO Graph
0.3
0.338
0.375
0.413
0.45
LP-ZGL Adsorption MAD
170 WordNet Classes, 10 Seeds per Class
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Algorithms
TextRunner Graph
YAGO Graph
TextRunner + YAGO Graph
Figure 7: Comparison of class-instance acquisition performance on the three different graphs described
in Section 3.6. All results are averaged over 10 random trials. Addition of YAGO attributes to the
TextRunner graph significantly improves performance.
YAGO Top-2 WordNet Classes Assigned by MAD
Attribute (example instances for each class are shown in brackets)
has currency wordnet country 108544813 (Burma, Afghanistan)
wordnet region 108630039 (Aosta Valley, Southern Flinders Ranges)
works at wordnet scientist 110560637 (Aage Niels Bohr, Adi Shamir)
wordnet person 100007846 (Catherine Cornelius, Jamie White)
has capital wordnet state 108654360 (Agusan del Norte, Bali)
wordnet region 108630039 (Aosta Valley, Southern Flinders Ranges)
born in wordnet boxer 109870208 (George Chuvalo, Fernando Montiel)
wordnet chancellor 109906986 (Godon Brown, Bill Bryson)
has isbn wordnet book 106410904 (Past Imperfect, Berlin Diary)
wordnet magazine 106595351 (Railway Age, Investors Chronicle)
Table 2: Top 2 (out of 170) WordNet classes assigned by MAD on 5 randomly chosen YAGO attribute
nodes (out of 80) in the TextRunner + YAGO graph used in Figure 7 (see Section 3.6), with 10 seeds per
class used. A few example instances of each WordNet class is shown within brackets. Top ranked class
for each attribute is shown in bold.
from various sources. In this section, we ex-
plore whether class-instance assignment can be
improved by incorporating new semantic con-
straints derived from (instance, attribute) pairs. In
particular, we experiment with the following type
of constraint: two instances with a common at-
tribute are likely to belong to the same class. For
example, in Figure 2 (b), instances Johnny Cash
and Bob Dylan are more likely to belong to the
same class as they have a common attribute, al-
bums. Because of the smooth labeling bias of
graph-based SSL methods (see Section 2.2), such
constraints are naturally captured by the methods
reviewed in Section 2. All that is necessary is the
introduction of bidirectional (instance, attribute)
edges to the graph, as shown in Figure 2 (b).
In Figure 7, we compare class-instance acqui-
sition performance of the three graph-based SSL
methods (Section 2) on the following three graphs
(also see Table 1):
TextRunner Graph: Graph constructed
from the hypernym tuples extracted by Tex-
tRunner, as in Figure 5 (Section 3.3), with
175k vertices and 529k edges.
YAGO Graph: Graph constructed from the
(instance, attribute) pairs obtained from the
YAGO KB (Suchanek et al, 2007), with 142k
nodes and 777k edges.
TextRunner + YAGO Graph: Union of the
1479
two graphs above, with 237k nodes and 1.3m
edges.
In all experimental conditions with 2 and 10
seeds per class in Figure 7, we observe that the
three methods consistently achieved the best per-
formance on the TextRunner + YAGO graph. This
suggests that addition of attribute based seman-
tic constraints from YAGO to the TextRunner
graph results in a better connected graph which
in turn results in better inference by the graph-
based SSL algorithms, compared to using either
of the sources, i.e., TextRunner output or YAGO
attributes, in isolation. This further illustrates
the advantage of aggregating information across
sources (Talukdar et al, 2008; Pennacchiotti and
Pantel, 2009). However, we are the first, to the
best of our knowledge, to demonstrate the effec-
tiveness of attributes in class-instance acquisition.
We note that this work is similar in spirit to the
recent work by Carlson et al (2010) which also
demonstrates the benefits of additional constraints
in SSL.
Because of the label propagation behavior,
graph-based SSL algorithms assign classes to all
nodes reachable in the graph from at least one
of the labeled instance nodes. This allows us
to check the classes assigned to nodes corre-
sponding to YAGO attributes in the TextRunner
+ YAGO graph, as shown in Table 2. Even
though the experiments were designed for class-
instance acquisition, it is encouraging to see that
the graph-based SSL algorithm (MAD in Table
2) is able to learn class-attribute relationships,
an important by-product that has been the fo-
cus of recent studies (Reisinger and Pasca, 2009).
For example, the algorithm is able to learn that
works at is an attribute of the WordNet class word-
net scientist 110560637, and thereby its instances
(e.g. Aage Niels Bohr, Adi Shamir).
4 Conclusion
We have started a systematic experimental com-
parison of graph-based SSL algorithms for class-
instance acquisition on a variety of graphs con-
structed from different domains. We found that
MAD, a recently proposed graph-based SSL algo-
rithm, is consistently the most effective across the
various experimental conditions. We also showed
that class-instance acquisition performance can be
significantly improved by incorporating additional
semantic constraints in the class-instance acqui-
sition process, which for the experiments in this
paper were derived from instance-attribute pairs
available in an independently developed knowl-
edge base. All the data used in these experiments
was drawn from publicly available datasets and we
plan to release our code3 to foster reproducible
research in this area. Topics for future work in-
clude the incorporation of other kinds of semantic
constraint for improved class-instance acquisition,
further investigation into per-node sparsity con-
straints in graph-based SSL, and moving beyond
bipartite graph constructions.
Acknowledgments
We thank William Cohen for valuable discussions,
and Jennifer Gillenwater, Alex Kulesza, and Gre-
gory Malecha for detailed comments on a draft of
this paper. We are also very grateful to the authors
of (Banko et al, 2007), Oren Etzioni and Stephen
Soderland in particular, for providing TextRunner
output. This work was supported in part by NSF
IIS-0447972 and DARPA HRO1107-1-0029.
References
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008.
Video suggestion and discovery for youtube: taking
random walks through the view graph. Proceedings
of WWW-2008.
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. Procs. of IJCAI.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira,
M. Liberman, A. McCallum, and M. Dredze. 2007.
Lightly-Supervised Attribute Extraction. NIPS 2007
Workshop on Machine Learning for Web Search.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka Jr,
and T.M. Mitchell. 2010. Coupled Semi-Supervised
Learning for Information Extraction. In Proceed-
ings of the Third ACM International Conference on
Web Search and Data Mining (WSDM), volume 2,
page 110.
O. Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web - an
experimental study. Artificial Intelligence Journal.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Fourteenth International
3
http://www.talukdar.net/datasets/class inst/
1480
Conference on Computational Linguistics, Nantes,
France.
Metaweb Technologies. 2009. Freebase data dumps.
http://download.freebase.com/datadumps/.
P. Pantel, E. Crestan, A. Borkovsky, A.M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity
and entity set expansion. Proceedings of EMNLP-
09, Singapore.
M. Pasca and Benjamin Van Durme. 2007. What you
seek is what you get: Extraction of class attributes
from query logs. In IJCAI-07. Ferbruary, 2007.
M. Pennacchiotti and P. Pantel. 2009. Entity Ex-
traction via Ensemble Semantics. Proceedings of
EMNLP-09, Singapore.
K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu.
2007. Semi-supervised learning of attribute-value
pairs from product descriptions. In IJCAI-07, Fer-
bruary, 2007.
D. Rao and D. Yarowsky. 2009. Ranking and Semi-
supervised Classification on Large Scale Graphs Us-
ing Map-Reduce. TextGraphs.
J. Reisinger and M. Pasca. 2009. Bootstrapped extrac-
tion of class attributes. In Proceedings of the 18th
international conference on World wide web, pages
1235?1236. ACM.
E. Riloff and R. Jones. 1999. Learning dictionar-
ies for information extraction by multi-level boot-
strapping. In Proceedings of the 16th National Con-
ference on Artificial Intelligence (AAAI-99), pages
474?479, Orlando, Florida.
F.M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: a core of semantic knowledge. In Proceed-
ings of the 16th international conference on World
Wide Web, page 706. ACM.
P. P. Talukdar and Koby Crammer. 2009. New regular-
ized algorithms for transductive learning. In ECML-
PKDD.
P. P. Talukdar, T. Brants, F. Pereira, and M. Liberman.
2006. A context pattern induction method for named
entity extraction. In Tenth Conference on Computa-
tional Natural Language Learning, page 141.
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichan-
dran, R. Bhagat, and F. Pereira. 2008. Weakly-
Supervised Acquisition of Labeled Class Instances
using Graph Random Walks. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 581?589.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using the Web.
Data Mining, 2007. ICDM 2007. Seventh IEEE In-
ternational Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
1481
Proceedings of the ACL 2010 Conference Short Papers, pages 377?381,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Data Representation using Inference-Driven Metric
Learning
Paramveer S. Dhillon
CIS Deptt., Univ. of Penn.
Philadelphia, PA, U.S.A
dhillon@cis.upenn.edu
Partha Pratim Talukdar?
Search Labs, Microsoft Research
Mountain View, CA, USA
partha@talukdar.net
Koby Crammer
Deptt. of Electrical Engg.
The Technion, Haifa, Israel
koby@ee.technion.ac.il
Abstract
We initiate a study comparing effective-
ness of the transformed spaces learned by
recently proposed supervised, and semi-
supervised metric learning algorithms
to those generated by previously pro-
posed unsupervised dimensionality reduc-
tion methods (e.g., PCA). Through a va-
riety of experiments on different real-
world datasets, we find IDML-IT, a semi-
supervised metric learning algorithm to be
the most effective.
1 Introduction
Because of the high-dimensional nature of NLP
datasets, estimating a large number of parameters
(a parameter for each dimension), often from a
limited amount of labeled data, is a challenging
task for statistical learners. Faced with this chal-
lenge, various unsupervised dimensionality reduc-
tion methods have been developed over the years,
e.g., Principal Components Analysis (PCA).
Recently, several supervised metric learning al-
gorithms have been proposed (Davis et al, 2007;
Weinberger and Saul, 2009). IDML-IT (Dhillon et
al., 2010) is another such method which exploits
labeled as well as unlabeled data during metric
learning. These methods learn a Mahalanobis dis-
tance metric to compute distance between a pair
of data instances, which can also be interpreted as
learning a transformation of the input data, as we
shall see in Section 2.1.
In this paper, we make the following contribu-
tions:
Even though different supervised and semi-
supervised metric learning algorithms have
recently been proposed, effectiveness of the
transformed spaces learned by them in NLP
? Research carried out while at the University of Penn-
sylvania, Philadelphia, PA, USA.
datasets has not been studied before. In
this paper, we address that gap: we com-
pare effectiveness of classifiers trained on the
transformed spaces learned by metric learn-
ing methods to those generated by previ-
ously proposed unsupervised dimensionality
reduction methods. We find IDML-IT, a
semi-supervised metric learning algorithm to
be the most effective.
2 Metric Learning
2.1 Relationship between Metric Learning
and Linear Projection
We first establish the well-known equivalence be-
tween learning a Mahalanobis distance measure
and Euclidean distance in a linearly transformed
space of the data (Weinberger and Saul, 2009). Let
A be a d?d positive definite matrix which param-
eterizes the Mahalanobis distance, dA(xi, xj), be-
tween instances xi and xj , as shown in Equation
1. Since A is positive definite, we can decompose
it as A = P>P , where P is another matrix of size
d? d.
dA(xi, xj) = (xi ? xj)
>A(xi ? xj) (1)
= (Pxi ? Pxj)
>(Pxi ? Pxj)
= dEuclidean(Pxi, Pxj)
Hence, computing Mahalanobis distance pa-
rameterized by A is equivalent to first projecting
the instances into a new space using an appropriate
transformation matrix P and then computing Eu-
clidean distance in the linearly transformed space.
In this paper, we are interested in learning a better
representation of the data (i.e., projection matrix
P ), and we shall achieve that goal by learning the
corresponding Mahalanobis distance parameterA.
We shall now review two recently proposed
metric learning algorithms.
377
2.2 Information-Theoretic Metric Learning
(ITML): Supervised
Information-Theoretic Metric Learning (ITML)
(Davis et al, 2007) assumes the availability of
prior knowledge about inter-instance distances. In
this scheme, two instances are considered simi-
lar if the Mahalanobis distance between them is
upper bounded, i.e., dA(xi, xj) ? u, where u
is a non-trivial upper bound. Similarly, two in-
stances are considered dissimilar if the distance
between them is larger than certain threshold l,
i.e., dA(xi, xj) ? l. Similar instances are rep-
resented by set S, while dissimilar instances are
represented by set D.
In addition to prior knowledge about inter-
instance distances, sometimes prior information
about the matrix A, denoted by A0, itself may
also be available. For example, Euclidean dis-
tance (i.e., A0 = I) may work well in some do-
mains. In such cases, we would like the learned
matrixA to be as close as possible to the prior ma-
trix A0. ITML combines these two types of prior
information, i.e., knowledge about inter-instance
distances, and prior matrix A0, in order to learn
the matrix A by solving the optimization problem
shown in (2).
min
A0
Dld(A,A0) (2)
s.t. tr{A(xi ? xj)(xi ? xj)
>} ? u,
?(i, j) ? S
tr{A(xi ? xj)(xi ? xj)
>} ? l,
?(i, j) ? D
whereDld(A,A0) = tr(AA
?1
0 )? log det(AA
?1
0 )
?n, is the LogDet divergence.
To handle situations where exactly solving the
problem in (2) is not possible, slack variables may
be introduced to the ITML objective. To solve this
optimization problem, an algorithm involving re-
peated Bregman projections is presented in (Davis
et al, 2007), which we use for the experiments re-
ported in this paper.
2.3 Inference-Driven Metric Learning
(IDML): Semi-Supervised
Notations: We first define the necessary notations.
Let X be the d ? n matrix of n instances in a
d-dimensional space. Out of the n instances, nl
instances are labeled, while the remaining nu in-
stances are unlabeled, with n = nl+nu. Let S be
a n ? n diagonal matrix with Sii = 1 iff instance
xi is labeled. m is the total number of labels. Y
is the n?m matrix storing training label informa-
tion, if any. Y? is the n?m matrix of estimated la-
bel information, i.e., output of any classifier, with
Y?il denoting score of label l at node i. .
The ITML metric learning algorithm, which we
reviewed in Section 2.2, is supervised in nature,
and hence it does not exploit widely available un-
labeled data. In this section, we review Infer-
ence Driven Metric Learning (IDML) (Algorithm
1) (Dhillon et al, 2010), a recently proposed met-
ric learning framework which combines an exist-
ing supervised metric learning algorithm (such as
ITML) along with transductive graph-based la-
bel inference to learn a new distance metric from
labeled as well as unlabeled data combined. In
self-training styled iterations, IDML alternates be-
tween metric learning and label inference; with
output of label inference used during next round
of metric learning, and so on.
IDML starts out with the assumption that ex-
isting supervised metric learning algorithms, such
as ITML, can learn a better metric if the number
of available labeled instances is increased. Since
we are focusing on the semi-supervised learning
(SSL) setting with nl labeled and nu unlabeled
instances, the idea is to automatically label the
unlabeled instances using a graph based SSL al-
gorithm, and then include instances with low as-
signed label entropy (i.e., high confidence label
assignments) in the next round of metric learning.
The number of instances added in each iteration
depends on the threshold ?1. This process is con-
tinued until no new instances can be added to the
set of labeled instances, which can happen when
either all the instances are already exhausted, or
when none of the remaining unlabeled instances
can be assigned labels with high confidence.
The IDML framework is presented in Algo-
rithm 1. In Line 3, any supervised metric
learner, such as ITML, may be used as the
METRICLEARNER. Using the distance metric
learned in Line 3, a new k-NN graph is constructed
in Line 4 , whose edge weight matrix is stored in
W . In Line 5 , GRAPHLABELINF optimizes over
the newly constructed graph, the GRF objective
(Zhu et al, 2003) shown in (3).
min
Y? ?
tr{Y?
?>LY?
?
}, s.t. S?Y? = S?Y?
?
(3)
where L = D ?W is the (unnormalized) Lapla-
1During the experiments in Section 3, we set ? = 0.05
378
Algorithm 1: Inference Driven Metric Learn-
ing (IDML)
Input: instancesX , training labels Y , training
instance indicator S, label entropy threshold ?,
neighborhood size k
Output: Mahalanobis distance parameter A
1: Y? ? Y , S? ? S
2: repeat
3: A? METRICLEARNER(X, S?, Y? )
4: W ? CONSTRUCTKNNGRAPH(X,A, k)
5: Y?
?
? GRAPHLABELINF(W, S?, Y? )
6: U ? SELECTLOWENTINST(Y?
?
, S?, ?)
7: Y? ? Y? + UY?
?
8: S? ? S? + U
9: until convergence (i.e., Uii = 0, ?i)
10: return A
cian, and D is a diagonal matrix with Dii =?
jWij . The constraint, S?Y? = S?Y?
?
, in (3)
makes sure that labels on training instances are not
changed during inference. In Line 6, a currently
unlabeled instance xi (i.e., S?ii = 0) is consid-
ered a new labeled training instance, i.e., Uii = 1,
for next round of metric learning if the instance
has been assigned labels with high confidence in
the current iteration, i.e., if its label distribution
has low entropy (i.e., ENTROPY(Y?
?
i:) ? ?). Fi-
nally in Line 7, training instance label information
is updated. This iterative process is continued till
no new labeled instance can be added, i.e., when
Uii = 0 ?i. IDML returns the learned matrix A
which can be used to compute Mahalanobis dis-
tance using Equation 1.
3 Experiments
3.1 Setup
Dataset Dimension Balanced
Electronics 84816 Yes
Books 139535 Yes
Kitchen 73539 Yes
DVDs 155465 Yes
WebKB 44261 Yes
Table 1: Description of the datasets used in Sec-
tion 3. All datasets are binary with 1500 total in-
stances in each.
Description of the datasets used during experi-
ments in Section 3 are presented in Table 1. The
first four datasets ? Electronics, Books, Kitchen,
and DVDs ? are from the sentiment domain and
previously used in (Blitzer et al, 2007). WebKB
is a text classification dataset derived from (Sub-
ramanya and Bilmes, 2008). For details regard-
ing features and data pre-processing, we refer the
reader to the origin of these datasets cited above.
One extra preprocessing that we did was that we
only considered features which occurred more 20
times in the entire dataset to make the problem
more computationally tractable and also since the
infrequently occurring features usually contribute
noise. We use classification error (lower is better)
as the evaluation metric. We experiment with the
following ways of estimating transformation ma-
trix P :
Original2: We set P = I , where I is the
d ? d identity matrix. Hence, the data is not
transformed in this case.
RP: The data is first projected into a lower
dimensional space using the Random Pro-
jection (RP) method (Bingham and Mannila,
2001). Dimensionality of the target space
was set at d
?
= logn
2log 1
, as prescribed in
(Bingham and Mannila, 2001). We use the
projection matrix constructed by RP as P . 
was set to 0.25 for the experiments in Sec-
tion 3, which has the effect of projecting the
data into a much lower dimensional space
(84 for the experiments in this section). This
presents an interesting evaluation setting as
we already run evaluations in much higher di-
mensional space (e.g., Original).
PCA: Data instances are first projected into
a lower dimensional space using Principal
Components Analysis (PCA) (Jolliffe, 2002)
. Following (Weinberger and Saul, 2009), di-
mensionality of the projected space was set
at 250 for all experiments. In this case, we
used the projection matrix generated by PCA
as P .
ITML: A is learned by applying ITML (see
Section 2.2) on the Original space (above),
and then we decompose A as A = P>P to
obtain P .
2Note that ?Original? in the results tables refers to orig-
inal space with features occurring more than 20 times. We
also ran experiments with original set of features (without
any thresholding) and the results were worse or comparable
to the ones reported in the tables.
379
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 31.3? 0.9 42.5? 1.0 46.4? 2.0 33.0? 1.0 30.7?0.7
Books 37.5? 1.1 45.0? 1.1 34.8? 1.4 35.0? 1.1 32.0?0.9
Kitchen 33.7? 1.0 43.0? 1.1 34.0? 1.6 30.9? 0.7 29.0?1.0
DVDs 39.0? 1.2 47.7? 1.2 36.2? 1.6 37.0? 0.8 33.9?1.0
WebKB 31.4? 0.9 33.0? 1.0 27.9? 1.3 28.9? 1.0 25.5?1.0
Table 2: Comparison of SVM % classification errors (lower is better), with 50 labeled instances (Sec.
3.2). nl=50. and nu = 1450. All results are averaged over ten trials. All hyperparameters are tuned on a
separate random split.
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 27.0? 0.9 40.0? 1.0 41.2? 1.0 27.5? 0.8 25.3?0.8
Books 31.0? 0.7 42.9? 0.6 31.3? 0.7 29.9? 0.5 27.7?0.7
Kitchen 26.3? 0.5 41.9? 0.7 27.0? 0.9 26.1? 0.8 24.8?0.9
DVDs 34.7? 0.4 46.8? 0.6 32.9? 0.8 34.0? 0.8 31.8?0.9
WebKB 25.7? 0.5 31.1? 0.5 24.9? 0.6 25.6? 0.4 23.9?0.4
Table 3: Comparison of SVM % classification errors (lower is better), with 100 labeled instances (Sec.
3.2). nl=100. and nu = 1400. All results are averaged over ten trials. All hyperparameters are tuned on
a separate random split.
IDML-IT: A is learned by applying IDML
(Algorithm 1) (see Section 2.3) on the Orig-
inal space (above); with ITML used as
METRICLEARNER in IDML (Line 3 in Al-
gorithm 1). In this case, we treat the set of
test instances (without their gold labels) as
the unlabeled data. In other words, we essen-
tially work in the transductive setting (Vap-
nik, 2000). Once again, we decompose A as
A = P>P to obtain P .
We also experimented with the supervised
large-margin metric learning algorithm (LMNN)
presented in (Weinberger and Saul, 2009). We
found ITML to be more effective in practice than
LMNN, and hence we report results based on
ITML only. Each input instance, x, is now pro-
jected into the transformed space as Px. We
now train different classifiers on this transformed
space. All results are averaged over ten random
trials.
3.2 Supervised Classification
We train a SVM classifier, with an RBF kernel, on
the transformed space generated by the projection
matrix P . SVM hyperparameter, C and RBF ker-
nel bandwidth, were tuned on a separate develop-
ment split. Experimental results with 50 and 100
labeled instances are shown in Table 2, and Ta-
ble 3, respectively. From these results, we observe
that IDML-IT consistently achieves the best per-
formance across all experimental settings. We also
note that in Table 3, performance difference be-
tween ITML and IDML-IT in the Electronics and
Kitchen domains are statistically significant.
3.3 Semi-Supervised Classification
In this section, we trained the GRF classifier (see
Equation 3), a graph-based semi-supervised learn-
ing (SSL) algorithm (Zhu et al, 2003), using
Gaussian kernel parameterized by A = P>P to
set edge weights. During graph construction, each
node was connected to its k nearest neighbors,
with k treated as a hyperparameter and tuned on
a separate development set. Experimental results
with 50 and 100 labeled instances are shown in
Table 4, and Table 5, respectively. As before, we
experimented with nl = 50 and nl = 100. Once
again, we observe that IDML-IT is the most effec-
tive method, with the GRF classifier trained on the
data representation learned by IDML-IT achieving
best performance in all settings. Here also, we ob-
serve that IDML-IT achieves the best performance
across all experimental settings.
380
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 47.9? 1.1 49.0? 1.2 43.2? 0.9 34.9? 0.5 34.0?0.5
Books 50.0? 1.0 49.4? 1.0 47.9? 0.7 42.1? 0.7 40.6?0.7
Kitchen 49.8? 1.1 49.6? 0.9 48.6? 0.8 31.1? 0.5 30.0?0.5
DVDs 50.1? 0.5 49.9? 0.7 49.4? 0.6 42.1? 0.4 41.2?0.5
WebKB 33.1? 0.4 33.1? 0.3 33.1? 0.3 30.0? 0.4 28.7?0.5
Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 50 and nu = 1450. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 43.5? 0.7 47.2? 0.8 39.1? 0.7 31.3? 0.2 30.8?0.3
Books 48.3? 0.5 48.9? 0.3 43.3? 0.4 35.2? 0.5 33.3?0.6
Kitchen 45.3? 0.6 48.2? 0.5 41.0? 0.7 30.7? 0.6 29.9?0.3
DVDs 48.6? 0.3 49.3? 0.5 45.9? 0.5 42.6? 0.4 41.7?0.3
WebKB 33.4? 0.4 33.4? 0.4 33.4? 0.3 30.4? 0.5 28.6?0.7
Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 100 and nu = 1400. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
4 Conclusion
In this paper, we compared the effectiveness
of the transformed spaces learned by recently
proposed supervised, and semi-supervised metric
learning algorithms to those generated by previ-
ously proposed unsupervised dimensionality re-
duction methods (e.g., PCA). To the best of our
knowledge, this is the first study of its kind in-
volving NLP datasets. Through a variety of ex-
periments on different real-world NLP datasets,
we demonstrated that supervised as well as semi-
supervised classifiers trained on the space learned
by IDML-IT consistently result in the lowest clas-
sification errors. Encouraged by these early re-
sults, we plan to explore further the applicability
of IDML-IT in other NLP tasks (e.g., entity classi-
fication, word sense disambiguation, polarity lexi-
con induction, etc.) where better representation of
the data is a pre-requisite for effective learning.
Acknowledgments
Thanks to Kuzman Ganchev for providing detailed
feedback on a draft of this paper. This work
was supported in part by NSF IIS-0447972 and
DARPA HRO1107-1-0029.
References
E. Bingham and H. Mannila. 2001. Random projec-
tion in dimensionality reduction: applications to im-
age and text data. In ACM SIGKDD.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In
ACL.
J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon.
2007. Information-theoretic metric learning. In
ICML.
P. S. Dhillon, P. P. Talukdar, and K. Crammer. 2010.
Inference-driven metric learning for graph construc-
tion. Technical Report MS-CIS-10-18, CIS Depart-
ment, University of Pennsylvania, May.
IT Jolliffe. 2002. Principal component analysis.
Springer verlag.
A. Subramanya and J. Bilmes. 2008. Soft-Supervised
Learning for Text Classification. In EMNLP.
V.N. Vapnik. 2000. The nature of statistical learning
theory. Springer Verlag.
K.Q. Weinberger and L.K. Saul. 2009. Distance metric
learning for large margin nearest neighbor classifica-
tion. The Journal of Machine Learning Research.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In ICML.
381
Tutorial Abstracts of ACL 2012, page 6,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Graph-based Semi-Supervised Learning Algorithms for NLP
Amar Subramanya
Google Research
asubram@google.com
Partha Pratim Talukdar
Carnegie Mellon University
ppt@cs.cmu.edu
Abstract
While labeled data is expensive to prepare, ever in-
creasing amounts of unlabeled linguistic data are
becoming widely available. In order to adapt to
this phenomenon, several semi-supervised learning
(SSL) algorithms, which learn from labeled as well
as unlabeled data, have been developed. In a sep-
arate line of work, researchers have started to real-
ize that graphs provide a natural way to represent
data in a variety of domains. Graph-based SSL al-
gorithms, which bring together these two lines of
work, have been shown to outperform the state-of-
the-art in many applications in speech processing,
computer vision and NLP. In particular, recent NLP
research has successfully used graph-based SSL al-
gorithms for PoS tagging (Subramanya et al, 2010),
semantic parsing (Das and Smith, 2011), knowledge
acquisition (Talukdar et al, 2008), sentiment anal-
ysis (Goldberg and Zhu, 2006) and text categoriza-
tion (Subramanya and Bilmes, 2008).
Recognizing this promising and emerging area of re-
search, this tutorial focuses on graph-based SSL al-
gorithms (e.g., label propagation methods). The tu-
torial is intended to be a sequel to the ACL 2008
SSL tutorial, focusing exclusively on graph-based
SSL methods and recent advances in this area, which
were beyond the scope of the previous tutorial.
The tutorial is divided in two parts. In the first
part, we will motivate the need for graph-based SSL
methods, introduce some standard graph-based SSL
algorithms, and discuss connections between these
approaches. We will also discuss how linguistic data
can be encoded as graphs and show how graph-based
algorithms can be scaled to large amounts of data
(e.g., web-scale data).
Part 2 of the tutorial will focus on how graph-based
methods can be used to solve several critical NLP
tasks, including basic problems such as PoS tagging,
semantic parsing, and more downstream tasks such
as text categorization, information acquisition, and
sentiment analysis. We will conclude the tutorial
with some exciting avenues for future work.
Familiarity with semi-supervised learning and
graph-based methods will not be assumed, and the
necessary background will be provided. Examples
from NLP tasks will be used throughout the tutorial
to convey the necessary concepts. At the end of this
tutorial, the attendee will walk away with the follow-
ing:
? An in-depth knowledge of the current state-of-
the-art in graph-based SSL algorithms, and the
ability to implement them.
? The ability to decide on the suitability of
graph-based SSL methods for a problem.
? Familiarity with different NLP tasks where
graph-based SSL methods have been success-
fully applied.
In addition to the above goals, we hope that this tu-
torial will better prepare the attendee to conduct ex-
citing research at the intersection of NLP and other
emerging areas with natural graph-structured data
(e.g., Computation Social Science).
Please visit http://graph-ssl.wikidot.com/ for details.
References
Dipanjan Das and Noah A. Smith. 2011. Semi-supervised
frame-semantic parsing for unknown predicates. In Proceed-
ings of the ACL: Human Language Technologies.
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing stars when
there aren?t many stars: graph-based semi-supervised learn-
ing for sentiment categorization. In Proceedings of the Work-
shop on Graph Based Methods for NLP.
Amarnag Subramanya and Jeff Bilmes. 2008. Soft-supervised
text classification. In EMNLP.
Amarnag Subramanya, Slav Petrov, and Fernando Pereira.
2010. Graph-based semi-supervised learning of structured
tagging models. In EMNLP.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira.
2008. Weakly supervised acquisition of labeled class in-
stances using graph random walks. In EMNLP.
6
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489?499,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Interpretable Semantic Vectors from a Joint Model of Brain- and Text-
Based Meaning
Alona Fyshe
1
, Partha P. Talukdar
1
, Brian Murphy
2
, Tom M. Mitchell
1
1
Machine Learning Department, Carnegie Mellon University
2
School of Electronics, Electrical Engineering and Computer Science
Queen?s University Belfast
[afyshe,partha.talukdar,tom.mitchell]@cs.cmu.edu
brian.murphy@qub.ac.uk
Abstract
Vector space models (VSMs) represent
word meanings as points in a high dimen-
sional space. VSMs are typically created
using a large text corpora, and so repre-
sent word semantics as observed in text.
We present a new algorithm (JNNSE) that
can incorporate a measure of semantics
not previously used to create VSMs: brain
activation data recorded while people read
words. The resulting model takes advan-
tage of the complementary strengths and
weaknesses of corpus and brain activation
data to give a more complete representa-
tion of semantics. Evaluations show that
the model 1) matches a behavioral mea-
sure of semantics more closely, 2) can
be used to predict corpus data for unseen
words and 3) has predictive power that
generalizes across brain imaging technolo-
gies and across subjects. We believe that
the model is thus a more faithful represen-
tation of mental vocabularies.
1 Introduction
Vector Space Models (VSMs) represent lexical
meaning by assigning each word a point in high di-
mensional space. Beyond their use in NLP appli-
cations, they are of interest to cognitive scientists
as an objective and data-driven method to discover
word meanings (Landauer and Dumais, 1997).
Typically, VSMs are created by collecting word
usage statistics from large amounts of text data and
applying some dimensionality reduction technique
like Singular Value Decomposition (SVD). The
basic assumption is that semantics drives a per-
son?s language production behavior, and as a result
co-occurrence patterns in written text indirectly
encode word meaning. The raw co-occurrence
statistics are unwieldy, but in the compressed
VSM the distance between any two words is con-
ceived to represent their mutual semantic similar-
ity (Sahlgren, 2006; Turney and Pantel, 2010), as
perceived and judged by speakers. This space then
reflects the ?semantic ground truth? of shared lex-
ical meanings in a language community?s vocab-
ulary. However corpus-based VSMs have been
criticized as being noisy or incomplete representa-
tions of meaning (Glenberg and Robertson, 2000).
For example, multiple word senses collide in the
same vector, and noise from mis-parsed sentences
or spam documents can interfere with the final se-
mantic representation.
When a person is reading or writing, the se-
mantic content of each word will be necessarily
activated in the mind, and so in patterns of ac-
tivity over individual neurons. In principle then,
brain activity could replace corpus data as input
to a VSM, and contemporary imaging techniques
allow us to attempt this. Functional Magnetic Res-
onance Imaging (fMRI) and Magnetoencephalog-
raphy (MEG) are two brain activation recording
technologies that measure neuronal activation in
aggregate, and have been shown to have a pre-
dictive relationship with models of word mean-
ing (Mitchell et al, 2008; Palatucci et al, 2009;
Sudre et al, 2012; Murphy et al, 2012b).
1
If brain activation data encodes semantics, we
theorized that including brain data in a model of
semantics could result in a model more consistent
with semantic ground truth. However, the inclu-
sion of brain data will only improve a text-based
model if brain data contains semantic information
not readily available in the corpus. In addition,
if a semantic test involves another subject?s brain
activation data, performance can improve only if
the additional semantic information is consistent
across brains. Of course, brains differ in shape,
size and in connectivity, so additional information
encoded in one brain might not translate to an-
1
For more details on fMRI and MEG, see Section 4.2
489
other. Furthermore, different brain imaging tech-
nologies measure very different correlates of neu-
ronal activity. Due to these differences, it is possi-
ble that one subject?s brain activation data cannot
improve a model?s performance on another sub-
ject?s brain data, or for brain data collected using
a different recording technology. Indeed, inter-
subject models of brain activation is an open re-
search area (Conroy et al, 2013), as is learning the
relationship between recording technologies (En-
gell et al, 2012; Hall et al, 2013). Brain data
can also be corrupted by many types of noise (e.g.
recording room interference, movement artifacts),
another possible hindrance to the use of brain data
in VSMs.
VSMs are interesting from both engineering
and scientific standpoints. In this work we fo-
cus on the scientific question: Can the inclusion
of brain data improve semantic representations
learned from corpus data? What can we learn from
such a model? From an engineering perspective,
brain activation data will likely never replace text
data. Brain activation recordings are both expen-
sive and time consuming to collect, whereas tex-
tual data is vast and much of it is free to download.
However, from a scientific perspective, combining
text and brain data could lead to more consistent
semantic models, in turn leading to a better un-
derstanding of semantics and semantic modeling
generally.
In this paper, we leverage both kinds of data to
build a hybrid VSM using a new matrix factor-
ization method (JNNSE). Our hypothesis is that
the noise of brain and corpus derived statistics
will be largely orthogonal, and so the two data
sources will have complementary strengths as in-
put to VSMs. If this hypothesis is correct, we
should find that the resulting VSM is more suc-
cessful in modeling word semantics as encoded in
human judgements, as well as separate corpus and
brain data that was not used in the derivation of the
model. We will show that our method:
1. creates a VSM that is more correlated to an
independent measure of word semantics.
2. produces word vectors that are more pre-
dictable from the brain activity of different
people, even when brain data is collected
with a different recording technology.
3. predicts corpus representations of withheld
words more accurately than a model that does
not combine data sources.
4. directly maps semantic concepts onto the
brain by jointly learning neural representa-
tions.
Together, these results suggest that corpus and
brain activation data measure semantics in com-
patible and complimentary ways. Our results
are evidence that a joint model of brain- and
text-based semantics may be closer to seman-
tic ground truth than text-only models. Our
findings also indicate that there is additional se-
mantic information available in brain activation
data that is not present in corpus data, and that
there are elements of semantics currently lack-
ing in text-based VSMs. We have made avail-
able the top performing VSMs created with brain
and text data (http://www.cs.cmu.edu/
?
afyshe/papers/acl2014/).
In the following sections we will review NNSE,
and our extension, JNNSE. We will describe the
data used and the experiments to support our posi-
tion that brain data is a valuable source of semantic
information that compliments text data.
2 Non-Negative Sparse Embedding
Non-Negative Sparse Embedding (NNSE) (Mur-
phy et al, 2012a) is an algorithm that produces
a latent representation using matrix factorization.
Standard NNSE begins with a matrix X ? R
w?c
made of c corpus statistics for w words. NNSE
solves the following objective function:
argmin
A,D
w
?
i=1
?
?
X
i,:
?A
i,:
?D
?
?
2
+ ?
?
?
A
?
?
1
(1)
subject to: D
i,:
D
T
i,:
? 1,? 1 ? i ? ` (2)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? ` (3)
The solution will find a matrix A ? R
w?`
that is
sparse, non-negative, and represents word seman-
tics in an `-dimensional latent space. D ? R
`?c
gives the encoding of corpus statistics in the la-
tent space. Together, they factor the original cor-
pus statistics matrix X in a way that minimizes
the reconstruction error. TheL
1
constraint encour-
ages sparsity in A; ? is a hyperparameter. Equa-
tion 2 constrains D to eliminate solutions where
A is made arbitrarily small by making D arbi-
trarily large. Equation 3 ensures that A is non-
negative. We may increase ` to give more dimen-
sional space to represent word semantics, or de-
crease ` for more compact representations.
490
The sparse and non-negative representation in
A produces a more interpretable semantic space,
where interpretability is quantified with a behav-
ioral task (Chang et al, 2009; Murphy et al,
2012a). To illustrate the interpretability of NNSE,
we describe a word by selecting the word?s top
scoring dimensions, and selecting the top scoring
words in those dimensions. For example, the word
chair has the following top scoring dimensions:
1. chairs, seating, couches;
2. mattress, futon, mattresses;
3. supervisor, coordinator, advisor.
These dimensions cover two of the distinct mean-
ings of the word chair (furniture and person of
power).
NNSE?s sparsity constraint dictates that each
word can have a non-zero score in only a few di-
mensions, which aligns well to previous feature
elicitation experiments in psychology. In feature
elicitation, participants are asked to name the char-
acteristics (features) of an object. The number of
characteristics named is usually small (McRae et
al., 2005), which supports the requirement of spar-
sity in the learned latent space.
3 Joint Non-Negative Sparse Embedding
We extend NNSEs to incorporate an additional
source of data for a subset of the words in X ,
and call the approach Joint Non-Negative Sparse
Embeddings (JNNSEs). The JNNSE algorithm
is general enough to incorporate any new infor-
mation about the a word w, but for this study
we will focus on brain activation recordings of
a human subject reading single words. We
will incorporate either fMRI or MEG data, and
call the resulting models JNNSE(fMRI+Text) and
JNNSE(MEG+Text) and refer to them generally
as JNNSE(Brain+Text). For clarity, from here
on, we will refer to NNSE as NNSE(Text), or
NNSE(Brain) depending on the single source of
input data used.
Let us order the rows of the corpus data X so
that the first 1 . . . w
?
rows have both corpus statis-
tics and brain activation recordings. Each brain
activation recording is a row in the brain data ma-
trix Y ? R
w
?
?v
where v is the number of features
derived from the recording. For MEG recordings,
v =sensors ? time points= 306? 150. For fMRI
v = grey-matter voxels =' 20, 000 depending on
the brain anatomy of each individual subject. The
new objective function is:
argmin
A,D
(c)
,D
(b)
w
?
i=1
?
?
X
i,:
?A
i,:
?D
(c)
?
?
2
+
w
?
?
i=1
?
?
Y
i,:
?A
i,:
?D
(b)
?
?
2
+ ?
?
?
A
?
?
1
(4)
subject to: D
(c)
i,:
D
(c)
i,:
T
? 1, ? 1 ? i ? ` (5)
D
(b)
i,:
D
(b)
i,:
T
? 1,? 1 ? i ? ` (6)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? `
(7)
We have introduced an additional constraint on the
rows 1 . . . w
?
, requiring that some of the learned
representations in A also reconstruct the brain ac-
tivation recordings (Y ) through representations in
D
(b)
? R
`?v
. Let us use A
?
to refer to the brain-
constrained rows of A. Words that are close in
?brain space? must have similar representations in
A
?
, which can further percolate to affect the rep-
resentations of other words in A via closeness in
?corpus space?.
With A or D fixed, the objective function for
NNSE(Text) and JNNSE(Brain+Text) is convex.
However, we are solving forA andD, so the prob-
lem is non-convex. To solve for this objective, we
use the online algorithm of Section 3 from Mairal
et al (Mairal et al, 2010). This algorithm is
guaranteed to converge, and in practice we found
that JNNSE(Brain+Text) converged as quickly as
NNSE(Text) for the same `. We used the SPAMS
package
2
to solve, and set ? = 0.025. This al-
gorithm was a very easy extension to NNSE(Text)
and required very little additional tuning.
We also consider learning shared representa-
tions in the case where data X and Y contain the
effects of known disjoint features. For example,
when a person reads a word, the recorded brain
activation data Y will contain the physiological
response to viewing the stimulus, which is unre-
lated to the semantics of the word. These sig-
nals can be attributed to, for example, the num-
ber of letters in the word and the number of white
pixels on the screen (Sudre et al, 2012). To ac-
count for such effects in the data, we augment
A
?
with a set of n fixed, manually defined fea-
tures (e.g. word length) to create A
?
percept
?
R
w?(`+n)
. D
(b)
? R
(`+n)?v
is used withA
?
percept
,
2
SPAMS Package: http://spams-devel.gforge.inria.fr/
491
to reconstruct the brain data Y . More gener-
ally, one could instead allocate a certain num-
ber of latent features specific to X or Y, both of
which could be learned, as explored in some re-
lated work (Gupta et al, 2013). We use 11 per-
ceptual features that characterize the non-semantic
features of the word stimulus (for a list, see sup-
plementary material at http://www.cs.cmu.
edu/
?
afyshe/papers/acl2014/).
The JNNSE algorithm is advantageous in that
it can handle partially paired data. That is, the
algorithm does not require that every row in X
also have a row in Y . Fully paired data is a re-
quirement of many other approaches (White et al,
2012; Jia and Darrell, 2010). Our approach al-
lows us to leverage the semantic information in
corpus data even for words without brain activa-
tion recordings.
JNNSE(Brain+Text) does not require brain data
to be mapped to a common average brain, which
is often the case when one wants to generalize be-
tween human subjects. Such mappings can blur
and distort data, making it less useful for subse-
quent prediction steps. We avoid these mappings,
and instead use the fact that similar words elicit
similar brain activation within a subject. In the
JNNSE algorithm, it is this closeness in ?brain
space? that guides the creation of the latent space
A. Leveraging intra-subject distance measures
to study inter-subject encodings has been studied
previously (Kriegeskorte et al, 2008a; Raizada
and Connolly, 2012), and has even been used
across species (humans and primates) (Kriegesko-
rte et al, 2008b).
Though we restrict ourselves to using one sub-
ject per JNNSE(Brain+Text) model, the JNNSE
algorithm could easily be extended to include
data from multiple brain imaging experiments by
adding a new squared loss term for additional
brain data.
3.1 Related Work
Perhaps the most well known related approach
to joining data sources is Canonical Correlation
Analysis (CCA) (Hotelling, 1936), which has been
applied to brain activation data in the past (Rus-
tandi et al, 2009). CCA seeks two linear trans-
formations that maximally correlate two data sets
in the transformed form. CCA requires that the
data sources be paired (all rows in the corpus data
must have a corresponding brain data), as corre-
lation between points is integral to the objective.
To apply CCA to our data we would need to dis-
card the vast majority of our corpus data, and use
only the 60 rows of X with corresponding rows
in Y. While CCA holds the input data fixed and
maximally correlates the transformed form, we
hold the transformed form fixed and seek a solu-
tion that maximally correlates the reconstruction
(AD
(c)
or A
?
D
(b)
) with the data (X and Y respec-
tively). This shift in error compensation is what
allows our data to be only partially paired. While
a Bayesian formulation of CCA can handle miss-
ing data, our model has missing data for> 97% of
the full w ? (v + c) brain and corpus data matrix.
To our knowledge, this extreme amount of missing
data has not been explored with Bayesian CCA.
One could also use a topic model style formula-
tion to represent this semantic representation task.
Supervised topic models (Blei and McAuliffe,
2007) use a latent topic to generate two observed
outputs: words in a document and a categorical la-
bel for the document. The same idea could be ap-
plied here: the latent semantic representation gen-
erates the observed brain activity and corpus statis-
tics. Generative and discriminative models both
have their own strengths and weaknesses, gener-
ative models being particularly strong when data
sources are limited (Ng and Jordan, 2002). Our
task is an interesting blend of data-limited and
data-rich problem scenarios.
In the past, various pieces of additional informa-
tion have been incorporated into semantic models.
For example, models with behavioral data (Sil-
berer and Lapata, 2012) and models with visual
information (Bruni et al, 2011; Silberer et al,
2013) have both shown to improve semantic rep-
resentations. Other works have correlated VSMs
built with text or images with brain activation
data (Murphy et al, 2012b; Anderson et al, 2013).
To our knowledge, this work is the first to integrate
brain activation data into the construction of the
VSM.
4 Data
4.1 Corpus Data
The corpus statistics used here are the download-
able vectors from Fyshe et al (2013)
3
. They
are compiled from a 16 billion word subset of
ClueWeb09 (Callan and Hoy, 2009) and contain
two types of corpus features: dependency and doc-
ument features, found to be complimentary for
3
http://www.cs.cmu.edu/
?
afyshe/papers/
conll2013/
492
most tasks. Dependency statistics were derived
by dependency parsing the corpus and compil-
ing counts for all dependencies incident on the
word. Document statistics are word-document
co-occurrence counts. Count thresholding was
applied to reduce noise, and positive pointwise-
mutual-information (PPMI) (Church and Hanks,
1990) was applied to the counts. SVD was ap-
plied to the document and dependency statistics
and the top 1000 dimensions of each type were
retained. We selected the rows corresponding to
noun-tagged words (approx. 17000 words).
4.2 Brain Activation Data
We have MEG and fMRI data at our disposal.
MEG measures the magnetic field caused by many
thousands of neurons firing together, and has good
time resolution (1000 Hz) but poor spatial reso-
lution. fMRI measures the change in blood oxy-
genation that results from differential neural ac-
tivity, and has good spatial resolution but poor
time resolution (0.5-1 Hz). We have fMRI data
and MEG data for 18 subjects (9 in each imaging
modality) viewing 60 concrete nouns (Mitchell et
al., 2008; Sudre et al, 2012). The 60 words span
12 word categories (animals, buildings, tools, in-
sects, body parts, furniture, building parts, uten-
sils, vehicles, objects, clothing, food). Each of the
60 words was presented with a line drawing, so
word ambiguity is not an issue. For both record-
ing modalities, all trials for a particular word were
averaged together to create one training instance
per word, with 60 training instances in all for each
subject and imaging modality. More preprocess-
ing details appear in the supplementary material.
5 Experimental Results
Here we explore several variations of JNNSE and
NNSE formulations. For a comparison of the
models used, see Table 1.
5.1 Correlation to Behavioral Data
To test if our joint model of Brain+Text is closer
to semantic ground truth we compared the latent
representation A learned via JNNSE(Brain+Text)
or NNSE(Text) to an independent behavioral mea-
sure of semantics. We collected behavioral data
for the 60 nouns in the form of answers to 218
semantic questions. Answers were gathered with
Mechanical Turk. The full list of questions ap-
pear in the supplementary material. Some exam-
ple questions are:?Is it alive??, and ?Can it bend??.
Mechanical Turk users were asked to respond to
each question for each word on a scale of 1-5. At
least 3 respondents answered each question and
the median score was used. This gives us a se-
mantic representation of each of the 60 words in
a 218-dimensional behavioral space. Because we
required answers to each of the questions for all
words, we do not have the problems of sparsity
that exist for feature production norms from other
studies (McRae et al, 2005). In addition, our an-
swers are ratings, rather than binary yes/no an-
swers.
For a given value of ` we solve the NNSE(Text)
and JNNSE(Brain+Text) objective function as de-
tailed in Equation 1 and 4 respectively. We com-
pared JNNSE(Brain+Text) and NNSE(Text) mod-
els by measuring the correlation of all pairwise
distances in JNNSE(Brain+Text) and NNSE(Text)
space to the pairwise distances in the 218-
dimensional semantic space. Distances were
calculated using normalized Euclidean distance
(equivalent in rank-ordering to cosine distance,
but more suitable for sparse vectors). Figure 1
shows the results of this correlation test. The er-
ror bars for the JNNSE(Brain+Text) models rep-
resent a 95% confidence interval calculated using
the standard error of the mean (SEM) over the 9
person-specific JNNSE(Brain+Text) models. Be-
cause there is only one NNSE(Text) model for
each dimension setting, no SEM can be calculated,
but it suffices to show that the NNSE(Text) corre-
lation does not fall into the 95% confidence inter-
val of the JNNSE(Brain+Text) models. The SVD
matrix for the original corpus data has correlation
0.4279 to the behavioral data, also below the 95%
confidence interval for all JNNSE models. The re-
sults show that a model that incorporates brain ac-
tivation data is more faithful to a behavioral mea-
sure of semantics.
5.2 Word Prediction from Brain Activation
We now show that the JNNSE(Brain+Text) vec-
tors are more consistent with independent sam-
ples of brain activity collected from different sub-
jects, even when recorded using different record-
ing technologies. As previously mentioned, be-
cause there is a large degree of variation between
brains and because MEG and fMRI measure very
different correlates of neuronal activity, this type
of generalization has proven to be very challeng-
ing and is an open research question in the neuro-
science community.
The output A of the JNNSE(Brain+Text) or
493
Table 1: A Comparison of the models explored in this paper, and the data upon which they operate.
Model Name Section(s) Text Data Brain Data Withheld Data
NNSE(Text) 2, 5 X x -
NNSE(Brain) 2, 5.2.1, 5.3 x X -
JNNSE(Brain+Text) 3, 5 X X -
JNNSE(Brain+Text): Dropout task 5.2.2 X X subset of brain data
JNNSE(Brain+Text): Predict corpus 5.3 X X subset of text data
250 500 10000.4
0.42
0.44
0.46
0.48
0.5
Correlation of Semantic Question Distances to JNNSE(fMRI)
Number of Latent Dimensions
Cor
rela
tion
 
 JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 1: Correlation of JNNSE(Brain+Text) and
NNSE(Text) models with the distances in a se-
mantic space constructed from behavioral data.
Error bars indicate SEM.
NNSE(Text) algorithm can be used as a VSM,
which we use for the task of word prediction from
fMRI or MEG recordings. A JNNSE(Brain+Text)
created with a particular human subject?s data is
never used in the prediction framework with that
same subject. For example, if we use fMRI data
from subject 1 to create a JNNSE(fMRI+Text), we
will test it with the remaining 8 fMRI subjects, but
all 9 MEG subjects (fMRI and MEG subjects are
disjoint).
Let us call the VSM learned with
JNNSE(Brain+Text) or NNSE(Text) the se-
mantic vectors. We can train a weight matrix W
that predicts the semantic vector a of a word from
that word?s brain activation vector x: a = Wx.
W can be learned with a variety of methods, we
will use L
2
regularized regression. One can also
train regressors that predict the brain activation
data from the semantic vector: x = Wa, but we
have found this to give lower predictive accuracy.
Note that we must re-train our weight matrix W
for each subject (instead of re-using D
(b)
from
Equation 4) because testing always occurs on a
different subject, and the brain activation data is
not inter-subject aligned.
We train ` independent L
2
regularized regres-
sors to predict the `-dimensional vectors a =
{a
1
. . . a
`
}. The predictions are concatenated
to produce a predicted semantic vector: a? =
{a?
1
, . . . , a?
`
}. We assess word prediction perfor-
mance by testing if the model can differentiate be-
tween two unseen words, a task named 2 vs. 2 pre-
diction (Mitchell et al, 2008; Sudre et al, 2012).
We choose the assignment of the two held out se-
mantic vectors (a
(1)
,a
(2)
) to predicted semantic
vectors (a?
(1)
, a?
(2)
) that minimizes the sum of the
two normalized Euclidean distances. 2 vs. 2 ac-
curacy is the percentage of tests where the correct
assignment is chosen.
The 60 nouns fall into 12 word categories.
Words in the same word category (e.g. screw-
driver and hammer) are closer in semantic space
than words in different word categories, which
makes some 2 vs. 2 tests more difficult than oth-
ers. We choose 150 random pairs of words (with
each word represented equally) to estimate the dif-
ficulty of a typical word pair, without having to
test all
(
60
2
)
word pairs. The same 150 random
pairs are used for all subjects and all VSMs. Ex-
pected chance performance on the 2 vs. 2 test is
50%.
Results for testing on fMRI data in the
2 vs. 2 framework appear in Figure 2.
JNNSE(fMRI+Text) data performed on aver-
age 6% better than the best NNSE(Text), and
exceeding even the original SVD corpus represen-
tations while maintaining interpretability. These
results generalize across brain activity recording
types; JNNSE(MEG+Text) performs as well as
JNNSE(fMRI+Text) when tested on fMRI data.
The results are consistent when testing on MEG
data: JNNSE(MEG+Text) or JNNSE(fMRI+Text)
outperforms NNSE(Text) (see Figure 3).
494
250 500 1000
64
66
68
70
72
74
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on fMRI data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 2: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
fMRI data. Models created with one subject?s
fMRI data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
250 500 1000
66
68
70
72
74
76
78
80
82
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on MEG data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 3: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
MEG data. Models created with one subject?s
MEG data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
NNSE(Text) performance decreases as the
number of latent dimension increases. This im-
plies that without the regularizing effect of brain
activation data, the extra NNSE(Text) dimensions
are being used to overfit to the corpus data, or
possibly to fit semantic properties not detectable
with current brain imaging technologies. How-
ever, when brain activation data is included, in-
creasing the number of latent dimensions strictly
increases performance for JNNSE(fMRI+Text).
JNNSE(MEG+Text) has peak performance with
500 latent dimensions, with ? 1% decrease in
performance at 1000 latent dimensions. In previ-
ous work, the ability to decode words from brain
activation data was found to improve with added
latent dimensions (Murphy et al, 2012a). Our
results may differ because our words are POS
tagged, and we included only nouns for the final
NNSE(Text) model. We found that with the orig-
inal ? = 0.05 setting from Murphy et al (Mur-
phy et al, 2012a) produced vectors that were too
sparse; four of the 60 test words had all-zero vec-
tors (JNNSE(Brain+Text) models did have any all-
zero vectors). To improve the NNSE(Text) vectors
for a fair comparison, we reduced ? = 0.025, un-
der which NNSE(Text) did not produce any all-
zero vectors for the 60 words.
Our results show that brain activation data con-
tributes additional information, which leads to an
increase in performance for the task of word pre-
diction from brain activation data. This suggests
that corpus-only models may not capture all rel-
evant semantic information. This conflicts with
previous studies which found that semantic vec-
tors culled from corpus statistics contain all of the
semantic information required to predict brain ac-
tivation (Bullinaria and Levy, 2013).
5.2.1 Prediction from a Brain-only Model
How much predictive power does the corpus data
provide to this word prediction task? To test
this, we calculated the 2 vs. 2 accuracy for a
NNSE(Brain) model trained on brain activation
data only. We train NNSE(Brain) with one sub-
ject?s data and use the resulting vectors to calculate
2 vs. 2 accuracy for the remaining subjects. We
have brain data for only 60 words, so using ` ? 60
latent dimensions leads to an under-constrained
system and a degenerate solution wherein only one
latent dimension is active for any word (and where
the brain data can be perfectly reconstructed). The
degenerate solution makes it impossible to gen-
eralize across words and leads to performance at
chance levels. An NNSE(MEG) trained on MEG
data gave maximum 2 vs. 2 accuracy of 67% when
` = 20. The reduced performance may be due to
the limited training data and the low SNR of the
data, but could also be attributed to the lack of cor-
pus information, which provides another piece of
semantic information.
495
5.2.2 Effect on Rows Without Brain Data
It is possible that some JNNSE(Brain+Text) di-
mensions are being used exclusively to fit brain
activation data, and not the semantics represented
in both brain and corpus data. If a particular
dimension j is solely used for brain data, the
sparsity constraint will favor solutions that sets
A
(i,j)
= 0 for i > w
?
(no brain data constraint),
and A
(i,j)
> 0 for some 0 ? i ? w
?
(brain data
constrained). We found that there were no such
dimensions in the JNNSE(Brain+Text). In fact for
the ` = 1000 JNNSE(Brain+Text), all latent di-
mensions had greater than ? 25% non-zero en-
tries, which implies that all dimensions are being
shared between the two data inputs (corpus and
brain activation), and are used to reconstruct both.
To test that the brain activation data is truly in-
fluencing rows of A not constrained by brain acti-
vation data, we performed a dropout test. We split
the original 60 words into two 30 word groups (as
evenly as possible across word categories). We
trained JNNSE(fMRI+Text) with 30 words, and
tested word prediction with the remaining 8 sub-
jects and the other 30 words. Thus, the training
and testing word sets are disjoint. Because of the
reduced size of the training data, we did see a drop
in performance, but JNNSE(fMRI+Text) vectors
still gave word prediction performance 7% higher
than NNSE(Text) vectors. Full results appear in
the supplementary material.
5.3 Predicting Corpus Data
Here we ask: can an accurate latent representa-
tion of a word be constructed using only brain
activation data? This task simulates the scenario
where there is no reliable corpus representation of
a word, but brain data is available. This scenario
may occur for seldom-used words that fall below
the thresholds used for the compilation of corpus
statistics. It could also be useful for acronym to-
kens (lol, omg) found in social media contexts
where the meaning of the token is actually a full
sentence.
We trained a JNNSE(fMRI+Text) with brain
data for all 60 words, but withhold the corpus data
for 30 of the 60 words (as evenly distributed as
possible amongst the 12 word categories). The
brain activation data for the 30 withheld words
will allow us to create latent representations in
A for withheld words. Simultaneously, we will
learn a mapping from the latent representation to
the corpus data (D
(c)
). This task cannot be per-
Table 2: Mean rank accuracy over 30 words
using corpus representations predicted by a
JNNSE(MEG+Text) model trained with some
rows of the corpus data withheld. Significance
is calculated using Fisher?s method to combine p-
values for each of the subject-dependent models.
Latent Dim size Rank Accuracy p-value
250 65.30 < 10
?19
500 67.37 < 10
?24
1000 63.47 < 10
?15
formed with a NNSE(Text) model because one
cannot learn a latent representation of a word with-
out data of some kind. This further emphasizes the
impact of brain imaging data, which will allow us
to generalize to previously unseen words in corpus
space.
We use the latent representations in A for each
of the words without corpus data and the mapping
to corpus space D
(c)
to predict the withheld cor-
pus data in X . We then rank the withheld rows of
X by their distance to the predicted row of X and
calculate the mean rank accuracy of the held out
words. Results in Table 2 show that we can recre-
ate the withheld corpus data using brain activation
data. Peak mean rank accuracy (67.37) is attained
at ` = 500 latent dimensions. This result shows
that neural semantic representations can create a
latent representation that is faithful to unseen cor-
pus statistics, providing further evidence that the
two data sources share a strong common element.
How much power is the remaining corpus data
supplying in scenarios where we withhold cor-
pus data? To answer this question, we trained an
NNSE(Brain) model on 30 words of brain activa-
tion, and then trained a regressor to predict cor-
pus data from those latent brain-only representa-
tions. We use the trained regressor to predict the
corpus data for the remaining 30 words. Peak per-
formance is attained at ` = 10 latent dimensions,
giving mean rank accuracy of 62.37, significantly
worse than the model that includes both corpus
and brain activation data (67.37).
5.4 Mapping Semantics onto the Brain
Because our method incorporates brain data into
an interpretable semantic model, we can directly
map semantic concepts onto the brain. To do
this, we examined the mappings from the latent
space to the brain space via D
(b)
. We found that
the most interpretable mappings come from mod-
496
!"#$%&'()
(a) D
(b)
matrix, subject P3, dimension with top words bath-
room, balcony, kitchen. MNI coordinates z=-12 (left) and z=-18
(right). Fusiform is associated with shelter words.
!"#$%&'$()*+
!(&%&'$()*+
(b) D
(b)
matrix; subject P1; dimension with top words ankle,
elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Pre-
and post-central areas are activated for body part words.
!"#$%&'(#)*+"#,$%
(c) D
(b)
matrix; subject P1; dimension with top scoring words
buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24
(right). Pars opercularis is believed to be part of the gustatory
cortex, which responds to food related words.
Figure 4: The mappings (D
(b)
) from latent se-
mantic space (A) to brain space (Y ) for fMRI and
words from three semantic categories. Shown are
representations of the fMRI slices such that the
back of the head is at the top of the image, the
front of the head is at the bottom.
els where the perceptual features had been scaled
down (divided by a constant factor), which en-
courages more of the data to be explained by
the semantic features in A. Figure 4 shows the
mappings (D
(b)
) for dimensions related to shel-
ter, food and body parts. The red areas align
with areas of the brain previously known to be
activated by the corresponding concepts (Mitchell
et al, 2008; Just et al, 2010). Our model
has learned these mappings in an unsupervised
setting by relating semantic knowledge gleaned
from word usage to patterns of activation in the
brain. This illustrates how the interpretability of
JNNSE can allow one to explore semantics in
the human brain. The mappings for one subject
are available for download (http://www.cs.
cmu.edu/
?
afyshe/papers/acl2014/).
6 Future Work and Conclusion
We are interested in pursuing many future projects
inspired by the success of this model. We would
like to extend the JNNSE algorithm to incorporate
data from multiple subjects, multiple modalities
and multiple experiments with non-overlapping
words. Including behavioral data and image data
is another possibility.
We have explored a model of semantics that in-
corporates text and brain activation data. Though
the number of words for which we have brain acti-
vation data is comparatively small, we have shown
that including even this small amount of data has
a positive impact on the learned latent representa-
tions, including for words without brain data. We
have provided evidence that the latent representa-
tions are closer to the neural representation of se-
mantics, and possibly, closer to semantic ground
truth. Our results reveal that there are aspects of
semantics not currently represented in text-based
VSMs, indicating that there may be room for im-
provement in either the data or algorithms used to
create VSMs. Our findings also indicate that using
the brain as a semantic test can separate models
that capture this additional semantic information
from those that do not. Thus, the brain is an im-
portant source of both training and testing data.
Acknowledgments
This work was supported in part by NIH un-
der award 5R01HD075328-02, by DARPA under
award FA8750-13-2-0005, and by a fellowship to
Alona Fyshe from the Multimodal Neuroimag-
ing Training Program funded by NIH awards
T90DA022761 and R90DA023420.
References
Andrew J Anderson, Elia Bruni, Ulisse Bordignon,
Massimo Poesio, and Marco Baroni. 2013. Of
words , eyes and brains : Correlating image-based
distributional semantic models with neural represen-
tations of concepts. In Proceedings of the Confer-
ence on Empirical Methods on Natural Language
Processing.
David M Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In Advances in Neural Information
Processing Systems, pages 1?22.
497
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP 2011 Geometrical Mod-
els for Natural Language Semantics (GEMS).
John A Bullinaria and Joseph P Levy. 2013. Limiting
factors for mapping corpus-based semantic repre-
sentations to brain activity. PloS one, 8(3):e57191,
January.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09
Dataset.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2009. Reading
Tea Leaves : How Humans Interpret Topic Models.
In Advances in Neural Information Processing Sys-
tems, pages 1?9.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Bryan R Conroy, Benjamin D Singer, J Swaroop Gun-
tupalli, Peter J Ramadge, and James V Haxby. 2013.
Inter-subject alignment of human cortical anatomy
using functional connectivity. NeuroImage, 81:400?
11, November.
Andrew D Engell, Scott Huettel, and Gregory Mc-
Carthy. 2012. The fMRI BOLD signal tracks elec-
trophysiological spectral perturbations, not event-
related potentials. NeuroImage, 59(3):2600?6,
February.
Alona Fyshe, Partha Talukdar, Brian Murphy, and Tom
Mitchell. 2013. Documents and Dependencies : an
Exploration of Vector Space Models for Semantic
Composition. In Computational Natural Language
Learning, Sofia, Bulgaria.
Arthur M Glenberg and David a Robertson. 2000.
Symbol Grounding and Meaning: A Compari-
son of High-Dimensional and Embodied Theories
of Meaning. Journal of Memory and Language,
43(3):379?401, October.
Sunil Kumar Gupta, Dinh Phung, Brett Adams, and
Svetha Venkatesh. 2013. Regularized nonnegative
shared subspace learning. Data Mining and Knowl-
edge Discovery, 26(1):57?97.
Emma L Hall, Si?an E Robson, Peter G Morris, and
Matthew J Brookes. 2013. The relationship be-
tween MEG and fMRI. NeuroImage, November.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Yangqing Jia and Trevor Darrell. 2010. Factorized La-
tent Spaces with Structured Sparsity. In Advances in
Neural Information Processing Systems, volume 23.
Marcel Adam Just, Vladimir L Cherkassky, Sandesh
Aryal, and Tom M Mitchell. 2010. A neuroseman-
tic theory of concrete noun representation based on
the underlying brain codes. PloS one, 5(1):e8622,
January.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008a. Representational similarity analysis
- connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2(November):4,
January.
Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff,
Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky,
Keiji Tanaka, and Peter A Bandettin. 2008b. Match-
ing Categorical Object Representations in Inferior
Temporal Cortex of Man and Monkey. Neuron,
60(6):1126?1141.
TK Landauer and ST Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review, 1(2):211?240.
Julien Mairal, Francis Bach, J Ponce, and Guillermo
Sapiro. 2010. Online learning for matrix factor-
ization and sparse coding. The Journal of Machine
Learning Research, 11:19?60.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547?59,
November.
Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L Malave, Robert A
Mason, and Marcel Adam Just. 2008. Pre-
dicting human brain activity associated with the
meanings of nouns. Science (New York, N.Y.),
320(5880):1191?5, May.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012a. Learning Effective and Interpretable Se-
mantic Models using Non-Negative Sparse Embed-
ding. In Proceedings of Conference on Computa-
tional Linguistics (COLING).
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012b. Selecting Corpus-Semantic Models for Neu-
rolinguistic Decoding. In First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 114?123, Montreal, Quebec, Canada.
Andrew Y. Ng and Michael I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. In Ad-
vances in neural information processing systems,
volume 14.
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau,
and Tom M Mitchell. 2009. Zero-Shot Learning
with Semantic Output Codes. Advances in Neural
Information Processing Systems, 22:1410?1418.
Rajeev D S Raizada and Andrew C Connolly. 2012.
What Makes Different People?s Representations
Alike : Neural Similarity Space Solves the Problem
of Across-subject fMRI Decoding. Journal of Cog-
nitive Neuroscience, 24(4):868?877.
498
Indrayana Rustandi, Marcel Adam Just, and Tom M
Mitchell. 2009. Integrating Multiple-Study
Multiple-Subject fMRI Datasets Using Canonical
Correlation Analysis. In MICCAI 2009 Workshop:
Statistical modeling and detection issues in intra-
and inter-subject functional MRI data analysis.
Magnus Sahlgren. 2006. The Word-Space Model Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words. Doctor
of philosophy, Stockholm University.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of Semantic Representation with Vi-
sual Attributes. In Association for Computational
Linguistics 2013, Sofia, Bulgaria.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking Neural Coding of Per-
ceptual and Semantic Features of Concrete Nouns.
NeuroImage, 62(1):463?451, May.
Peter D Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning : Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Martha White, Yaoliang Yu, Xinhua Zhang, and Dale
Schuurmans. 2012. Convex multi-view subspace
learning. In Advances in Neural Information Pro-
cessing Systems, pages 1?14.
499
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114?123,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Corpus-Semantic Models for Neurolinguistic Decoding
Brian Murphy
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
brianmurphy@cmu.edu
Partha Talukdar
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
ppt@cs.cmu.edu
Tom Mitchell
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
tom.mitchell@cs.cmu.edu
Abstract
Neurosemantics aims to learn the mapping
between concepts and the neural activity
which they elicit during neuroimaging ex-
periments. Different approaches have been
used to represent individual concepts, but
current state-of-the-art techniques require
extensive manual intervention to scale to
arbitrary words and domains. To over-
come this challenge, we initiate a system-
atic comparison of automatically-derived
corpus representations, based on various
types of textual co-occurrence. We find
that dependency parse-based features are
the most effective, achieving accuracies
similar to the leading semi-manual ap-
proaches and higher than any published
for a corpus-based model. We also find
that simple word features enriched with
directional information provide a close-to-
optimal solution at much lower computa-
tional cost.
1 Introduction
The cognitive plausibility of computational
models of word meaning has typically been
tested using behavioural benchmarks, such as
identification of synonyms among close asso-
ciates (the TOEFL task for language learners,
see e.g. Landauer and Dumais, 1997); emulating
elicited judgments of pairwise similarity (such as
Rubenstein and Goodenough, 1965); judgments
of category membership (e.g. Battig and Mon-
tague, 1969); and word priming effects (Lund
and Burgess, 1996). Mitchell et al (2008) in-
troduced a new task in neurosemantic decoding
? using models of semantics to learn the map-
ping between concepts and the neural activity
which they elicit during neuroimaging experi-
ments. This was achieved with a linear model
which used training data to find neural basis im-
ages that correspond to the assumed semantic
dimensions (for instance, one such basis image
might be the activity of the brain for words rep-
resenting animate concepts), and subsequently
used these general patterns and known seman-
tic dimensions to infer the fMRI activity that
should be elicited by an unseen stimulus con-
cept. Follow-on work has experimented with
other neuroimaging modalities (Murphy et al,
2009), and with a range of semantic models in-
cluding elicited property norms (Chang et al,
2011), corpus derived models (Devereux and
Kelly, 2010; Pereira et al, 2011) and structured
ontologies (Jelodar et al, 2010).
The current state-of-the-art performance on
this task is achieved using models that are hand-
tailored in some respect, whether using manual
annotation tasks (Palatucci et al, 2009), use of
a domain-appropriate curated corpus (Pereira
et al, 2011), or selection of particular collocates
to suit the concepts to be described (Mitchell
et al, 2008). While these approaches are clearly
very successful, it is questionable whether they
are a general solution to describe the vari-
ous parts-of-speech and semantic domains that
make up a speaker?s vocabulary. The Mitchell
et al (2008) 25-verb model would probably have
to be extended to describe the lexicon at large,
and it is unclear whether such a compact model
could be maintained. While Wikipedia (Pereira
et al, 2011) has very broad and increasing cov-
114
erage, it is possible that it will remain inad-
equate for specialist vocabularies, or for less-
studied languages. And while the method used
by Palatucci et al (2009) distributes the anno-
tation task efficiently by crowd-sourcing, it still
requires that appropriate questions are compiled
by researchers, a task that is both difficult to
perform in a systematic way, and which may not
generalize to more abstract concepts.
In this paper we examine a representative set
of corpus-derived models of meaning, that re-
quire no manual intervention, and are applicable
to any syntactic and semantic domain. We con-
centrate on which types of basic corpus pattern
perform well on the neurosemantic decoding
task: LSA-style word-region co-occurrences,
and various HAL-style word-collocate features
including raw tokens, POS tags, and a full de-
pendency parse. Otherwise a common feature
extraction and preprocessing pipeline is used: a
co-occurrence frequency cutoff, application of a
frequency normalization weighting, and dimen-
sionality reduction with SVD.
The following section describes how the brain
activity data was gathered and processed; the
construction of several corpus-derived models
of meaning; and the regression-based meth-
ods used to predict one from the other, evalu-
ated with a brain-image matching task (Mitchell
et al, 2008). In section 3 we report the re-
sults, and in the Conclusion we discuss both the
practical implications, and what this works sug-
gests for the cognitive plausibility of distribu-
tional models of meaning.
2 Methods
2.1 Brain activity features
The dataset used here is that described in detail
in (Mitchell et al, 2008) and released publicly1
in conjunction with the NAACL 2010 Work-
shop on Computational Neurolinguistics (Mur-
phy et al, 2010). Functional MRI (fMRI) data
was collected from 9 participants while they per-
formed a property generation task. The stimuli
were line-drawings, accompanied by their text
1http://www.cs.cmu.edu/afs/cs/project/theo-
73/www/science2008/data.html
label, of everyday concrete concepts, with 5 ex-
emplars of each of 12 semantic classes (mam-
mals, body parts, buildings, building parts,
clothes, furniture, insects, kitchen utensils, mis-
cellaneous functional artifacts, work tools, veg-
etables, and vehicles). Stimuli remained on
screen for three seconds, and each was each pre-
sented six times, in random order, to give a total
of 360 image presentations in the session.
The fMRI images were recorded with 3.0T
scanner at 1 second intervals, with a spatial reso-
lution of 3x3x6mm. The resulting data was pre-
processed with the SPM package (Friston et al,
2007); the blood-oxygen-level response was ap-
proximated by taking a boxcar average over a
sequence of brain images in each trial; percent
signal change was calculated relative to rest pe-
riods, and the data from each of the six repeti-
tions of each stimulus were averaged to yield a
single brain image for each concept. Finally, a
grey-matter anatomical mask was used to select
only those voxels (three-dimensional pixels) that
overlap with cortex, yielding approximately 20
thousand features per participant.
2.2 Models of semantics
Our objective is to compare current semantic
representations that get state-of-the-art perfor-
mance on the neuro-semantics task with repre-
sentative distributional models of semantics that
can be derived from arbitrary corpora, using
varying degrees of linguistic preprocessing. A
series of candidate models were selected to rep-
resent the variety of ways in which basic textual
features can be extracted and represented, in-
cluding token co-occurrence in a small local win-
dow, dependency parses of whole sentences, and
document co-occurrence, among others. Other
parameters were kept fixed in a way that the
literature suggests would be neutral to the var-
ious models, and so allow a fair comparison
among them (Sahlgren, 2006; Bullinaria and
Levy, 2007; Turney and Pantel, 2010).
All textual statistics were gathered from a set
of 50m English-language web-page documents
consisting of 16 billion words. Where a fixed
text window was used, we chose an extent of
?4 lower-case tokens either side of the target
115
word of interest, which is in the mid-range of
optimal values found by various authors (Lund
and Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Positive pointwise-mutual-information (1,2) was
used as an association measure to normalize
the observed co-occurrence frequency p(w, f) for
the varying frequency of the target word p(w)
and its features p(f). PPMI up-weights co-
occurrences between rare words, yielding posi-
tive values for collocations that are more com-
mon than would be expected by chance (i.e. if
word distributions were independent), and dis-
cards negative values that represent patterns of
co-occurrences that are rarer than one would ex-
pect by chance. It has been shown to perform
well generally, with both word- and document-
level statistics, in raw and dimensionality re-
duced forms (Bullinaria and Levy, 2007; Turney
and Pantel, 2010).2
PPMIwf =
{
PMIwf if PMIwf > 0
0 otherwise
(1)
PMIwf = log
(
p(w, f)
p(w)p(f)
)
(2)
A frequency threshold is commonly applied
for three reasons: low-frequency co-occurrence
counts are more noisy; PMI is positively bi-
ased towards hapax co-occurrences; and due
to Zipfian distributions a cut-off dramatically
reduces the amount of data to be processed.
Many authors use a threshold of approximately
50-100 occurrences for word-collocate models
(Lund and Burgess, 1996; Lin, 1998; Rapp,
2003). Since Bullinaria and Levy (2007) find
improving performance with models using pro-
gressively lower cutoffs we explored two cut-offs
of 20 and 50 which equate to low co-occurrences
thresholds of 0.00125 or 0.003125 per million re-
spectively; for the word-region model we chose
a threshold of 2 occurrences of a target term in
a document, to keep the input features to a rea-
sonable dimensionality (Bradford, 2008).
After applying these operations to the input
data from each model, the resulting dimension-
2Preliminary analyses confirmed that PPMI per-
formed as well or better than alternatives including log-
likelihood, TF-IDF, and log-entropy.
ality ranged widely, from about 500 thousand,
to tens of millions. A singular value decompo-
sition (SVD) was applied to identify the 1000
dimensions within each model with the great-
est explanatory power, which also has the ef-
fect of combining similar dimensions (such as
synonyms, inflectional variants, topically simi-
lar documents) into common components, and
discarding more noisy dimensions in the data.
Again there is variation in the number of di-
mension that authors use: here we experiment
with 300 and 1000. For decomposition we used
a sparse SVD method, the Implicitly Restarted
Arnoldi Method (Lehoucq et al, 1998; Jones
et al, 2001), which was coherent with the PPMI
normalization used, since a zero value repre-
sented both negative target-feature associations,
and those that were not observed or fell below
the frequency cut-off. We also streamlined the
task by reducing the input data C (of n target
words by m co-occurrence features) to a square
matrix CCT of size n ? n, taking advantage of
the equality of their left singular vectors U. For
SVD to generalize well over the many input fea-
tures, it is also important to have more training
cases that the small set of 60 concrete nouns
used in our evaluation task. Consequently we
gathered all statistics over a set of the 40,000
most frequent word-forms found in the Ameri-
can National Corpus (Nancy Ide and Keith Su-
derman, 2006), which should approximate the
scale and composition of the vocabulary of a
university-educated speaker of English (Nation
and Waring, 1997), and over 95% of tokens typ-
ically encountered in English.
2.2.1 Hand-tailored benchmarks
The state-of-the-art models on this brain ac-
tivity prediction task are both hand-tailored.
Mitchell et al (2008) used a model of seman-
tics based on co-occurrence in the Google 1T 5-
gram corpus of English (Brants and Franz, 2006)
with a small set of 25 Verbs chosen to rep-
resent everyday sensory-motor interaction with
concrete objects, such as see, move, listen. We
recreated this using our current parameters (web
document corpus, co-occurrence frequency cut-
off, PPMI normalization). The second hand-
116
tailored dataset we used was a set of Elicited
Properties inspired by the 20 Questions game,
and gathered using Mechanical Turk (Palatucci
et al, 2009; Palatucci, 2011). Multiple infor-
mants were asked to answer one or more of 218
questions ?related to size, shape, surface prop-
erties, and typical usage? such as Do you see
it daily?, Is it wild?, Is it man-made? with a
scalar response ranging from 1 to 5. The re-
sulting responses were then averaged over infor-
mants, and then the values of each question were
grouped into 5 bins, giving all dimensions simi-
lar mean and variance.
2.2.2 Word-Region Model
Latent Semantic Analysis (Deerwester et al,
1990; Landauer and Dumais, 1997), and its
probabilistic cousins (Blei et al, 2003; Grif-
fiths et al, 2007), express the meaning of a
word as a distribution of co-occurrence across
a set of documents, or other text-regions such
as paragraphs. This word-region matrix in-
stantiates the assumption that words that share
a topical domain (such as medicine, entertain-
ment, philosophy) would be expected to appear
in similar sub-sets of text-regions. In such a
model, the nearest neighbors of a target word
are syntagmatically related (i.e. appear along-
side each other), and for judge might include
lawyer, court, crime, or prison.
The Document model used here is loosely
based on LSA, taking the frequency of occur-
rence of each of our 40,000 vocabulary words
in each of 50 million documents as its input
data, and it follows Bullinaria and Levy (2007);
Turney and Pantel (2010) in using PPMI as a
normalization function. We have not investi-
gated variations on the decomposition algorithm
in any detail, such as those using non-negative
matrix factorization, probabilistic LSA or LDA
topic models, as the objective in this paper is
to provide a direct comparison between the dif-
ferent types of basic collocation information en-
coded in corpora, rather than evaluate the best
algorithmic means for discovering latent dimen-
sions in those co-occurrences. Nor have we eval-
uated performance on a more structured corpus
input (Pereira et al, 2011). However prelimi-
nary tests with the Wikipedia corpus, and with
LDA, using the Gensim package (Rehurek and
Sojka, 2010) yielded similar performances.
2.2.3 Word-Collocate Models
Word-collocate models make a complemen-
tary assumption to that of the document model:
that words with closely-related categorical or
taxonomic properties should appear in the same
position of similar sentences. In a basic word-
collocate model, based on a word-word co-
occurrence matrix, the nearest neighbors of
judge might be athlete, singer, or fire-fighter,
reflecting paradigmatic relatedness (i.e. substi-
tutability). Word-collocate models are further
differentiated by the amount of linguistic anno-
tation attached to word features, ranging from
simple word-form features in a fixed-width win-
dow around the concept word, to more elaborate
word sequence patterns and parses including
parts of speech and dependency relation tags.
Among these alternatives, we might expect a
dependency model to be the most powerful. In-
tuitively, the information that John is sentient
is similarly encoded in the text John likes cake
and John seems to really really like cake, and a
suitably effective parser should be able to gen-
eralize over this variation, to extract the same
dependency relationship of John-subject-like. In
contrast a narrow window-based model might
exclude informative features (such as like in the
second example), while including presumably
uninformative ones (such as really). However
parsers have the disadvantage of being computa-
tionally expensive (meaning that they typically
are applied to smaller corpora) and usually in-
troduce some noise through their errors. Conse-
quently, simpler window-based models have of-
ten been found to be as effective.
The most basic model considered is the
Word-Form model, in which all lower-case to-
kens (word forms and punctuation) found within
four positions left and right of the target word
are recorded, yielding simple features such as
{john, likes}. It may also be termed a ?flat?
model in contrast to those which assign a vari-
able weight to collocates, progressively lower as
one moves further than the target position (e.g.
117
Lund et al, 1995). We did not use a stop-list, as
Bullinaria and Levy (2007) found co-occurrence
with very high frequency words also to be infor-
mative for semantic tasks. We also expect that
the subsequent steps of normalizing with PPMI,
reduction with SVD, and use of regularised re-
gression should be able to recognize when such
high-frequency words are not informative and
then discount these, without the need for such
assumptions upfront.
The Stemmed model is a slight variation on
the Word-Form model, where the same statistics
are aggregated after applying Lancaster stem-
ming (Paice, 1990; Loper and Bird, 2002).
The Directional model, inspired by Schu?tze
and Pedersen (1993), is also derived from the
word-form model, but differentiates between co-
occurrence to the left or to the right of the target
word, with features such as {john L, cake R}.
The Part-of-Speech model (Kanejiya et al,
2003; Widdows, 2003) replaces each lower-
case word-token with its part-of-speech disam-
biguated form (e.g. likes VBZ, cake NN ). These
annotations were extracted from the full depen-
dency parse described below.
The Sequence model draws on a range of
work that uses word sequence patterns (Lin and
Pantel, 2001; Almuhareb and Poesio, 2004; Ba-
roni et al, 2010), and may also be considered an
approximation of models that use shallow syn-
tactic analysis (Grefenstette, 1994; Curran and
Moens, 2002). All distinct token sequences up
to length 4 either side of the target word were
counted.
Finally the Dependency model uses a full
dependency parse, which might be considered
the most informed representation of the word-
collocate relationships instantiated in corpus
sentences, and this approach has been used by
several authors (Lin, 1998; Pado? and Lapata,
2007; Baroni and Lenci, 2010). The features
used are pairs of dependency relation and lex-
eme corresponding to each edge linked to a tar-
get word of interest (e.g. likes subj ). The parser
used here was Malt, which achieves accuracies of
85% when deriving labelled dependencies on En-
glish text (Hall et al, 2007). The features pro-
duced by this module are much more limited,
to those words that have a direct dependency
relation with the word of interest.
2.3 Linear Learning Model
A linear regression model will allow us to eval-
uate how well a given model of word semantics
can be used to predict brain activity. We fol-
low the analysis in Mitchell et al (2008) and
subsequently adopted by several other research
groups (see Murphy et al, 2010). For each par-
ticipant and selected fMRI feature (i.e. each
voxel, which records the time-course of neural
activity at a fixed location in the brain), we train
a model where the level of activation of the latter
(the blood oxygenation level) in response to dif-
ferent concepts is approximated by a regularised
linear combination of their semantic features:
f = C? + ?||?||2 (3)
where f is the vector of activations of a spe-
cific fMRI feature for different concepts, the ma-
trix C contains the values of the semantic fea-
tures for the same concepts, ? is the vector of
weights we must learn for each of those (corpus-
derived) features, and ? tunes the degree of reg-
ularisation. We can illustrate this with a toy
example, containing several stimulus concepts
and their attributes on three semantic dimen-
sions: cat (+animate, -big, +moving); phone
(-animate, -big, -moving); elephant (+animate,
+big, +moving); skate-board (-animate, -big,
+moving). After training over all the voxels in
our fMRI data with this simple semantic model,
we can derive whole brain images that are typ-
ical of each of the semantic dimensions. The
power of the model is its ability to predict ac-
tivity for concepts that were not in the training
set ? for instance the brain activation elicited by
the word car might be approximated by combin-
ing the images see for -animate, +big, +moving,
even though this combination of properties was
not observed during training.
The linear model was estimated with a
least squared errors method and L2 regularisa-
tion, selecting the lambda parameter from the
range 0.0001 to 5000 using Generalized Cross-
Validation (see Hastie et al, 2011, p.244). The
118
activation of each fMRI voxel in response to a
new concept that was not in the training data
was predicted by a ?-weighted sum of the val-
ues on each semantic dimension, building a pic-
ture of expected the global neural activity re-
sponse for an arbitrary concept. Again follow-
ing Mitchell et al (2008) we use a leave-2-out
paradigm in which a linear model for each neu-
ral feature is trained in turn on all concepts mi-
nus 2, having selected the 500 most stable voxels
in the training set using the same correlational
measure across stimulus presentations. For each
of the 2 left-out concepts, we predict the global
neural activation pattern, as just described. We
then try to correctly match the predicted and
observed activations, by measuring the cosine
distance between the model-generated estimate
of fMRI activity and the that observed in the ex-
periment. If the sum of the matched cosine dis-
tances is lower than the sum of the mismatched
distances, we consider the prediction successful
? otherwise as failed. At chance levels, expected
matching accuracy is 50%, and significant per-
formance above chance can be estimated using
the binomial test, once variance had been veri-
fied over independent trials (i.e. where no single
stimulus concept is shared between pairs).
3 Results
Table 1 shows the main results of the leave-
two-out brain-image matching task. They show
the mean classification performance over 1770
word pairs (60 select 2) by 9 participants. All of
these classification accuracies are highly signif-
icant at p  0.001 over test trials (binomial,
chance 50%, n=1770*9) and p < 0.001 over
words (binomial, chance 50%, n=60). There
were some significant differences between mod-
els when making inferences over trials, but for
the small set of words used here it is not possible
to make firm conclusions about the superiority
of one model over the other, that could be confi-
dently expected to generalize to other stimuli or
experiments. However, we do achieve classifica-
tion accuracies that are as high, or higher than
any previously published (Palatucci et al, 2009;
Pereira et al, 2011), while models based on very
Semantic Models Features Accuracy
25 Verbs 25 78.5
Elicited Properties 218 83.5
Document (f2) 1000 76.2
Word Form 1000 80.0
Stemmed 1000 76.2
Direction 1000 80.2
Part-of-Speech 1000 80.0
Sequence 1000 78.5
Dependency 1000 83.1
Table 1: Brain activity prediction accuracy on leave-
2-out pair-matching task. A frequency cutoff of 20
was used for all 1000 dimensional models.
Semantic Models 300 Feats. 1000 Feats.
Document (f2) 79.9 76.2
Word Form 78.1 80.0
Stemmed 77.9 76.2
Direction 80.0 80.2
Part-of-Speech 77.9 80.0
Sequence 72.9 78.5
Dependency 81.6 83.1
Table 2: Effect of SVD dimensionality in the leave-
2-out pair-matching setting; frequency cutoff of 20.
different basic features (directional word-forms;
dependency relations; document co-occurrence)
yield very similar performance.
3.1 Effect of Number of Dimensions
Here we evaluate what effect the number of SVD
dimensions used has on the final performance
of various semantic models. Experimental re-
sults comparing 300 and 1000 dimensions are
presented in Table 2, all based on a frequency
cutoff of 20. We observe that performance im-
proves in 5 out of 7 semantic models compared,
with the highest performance achieved by the
Dependency model when 1000 SVD dimensions
were used.
3.2 Effect of Frequency Cutoff
In this section, we evaluate what effect frequency
cutoff has on the brain prediction accuracy of
various semantic models. From the results in
Table 3, we observe only marginal changes as
the frequency cutoff varied from 20 to 50. This
suggests that the semantic models of this set of
119
Semantic Models Cutoff = 50 Cutoff = 20
Document (f2) 79.9 79.9
Word Form 78.5 78.1
Stemmed 78.2 77.9
Direction 80.8 80.0
Part-of-Speech 77.5 77.9
Sequence 74.4 72.9
Dependency 81.3 81.6
Table 3: Effect of frequency cutoff in the leave-2-out
pair-matching setting; 300 SVD dimensions.
words are not very sensitive to variations in the
frequency cutoff under current experimental set-
tings, and do not benefit clearly from the de-
crease in sparsity and increase in noise that a
lower threshold produces.
3.3 Information Overlap Analysis
To verify that the models are in fact substan-
tially different, we performed a follow-on analy-
sis that measured the informational overlap be-
tween the corpus-derived models. Given two
models A and B, both with dimensionality 40
thousand words by 300 SVD dimensions, we can
evaluate the extent to which A (used as the
predictor semantic representation) contains the
information encoded in B (the explained rep-
resentation). As shown in (4), for each SVD
component c, we take the left singular vector
bc as a dependent variable and fit it with a lin-
ear model, using the matrix A (all left singu-
lar vectors) as independent variables. The ex-
plained variance for this column is weighted by
its squared singular value s2c in B, and the sum of
these component-wise variances gives the total
variance explained R2A?B.
R2A?B =
300?
c=1
s2c?
s2c
RA?bc (4)
Figure 1 indicates that the first three models,
which are all derived from token occurrences in a
?4 window, are close to identical. The sequence
and document models are relatively dissimilar,
and the dependency model occupies a middle
ground, with some similarity to all the models.
It is also interesting to note that the among the
first cluster of word-form derived models, the
Figure 1: Informational Overlap between Corpus-
Derived Datasets, in R2
directional one has the highest similarity to the
dependency model.
4 Conclusion
The main result of this study was that we
achieved classification accuracies as high as
any published, and within a fraction of a per-
centage point of the human benchmark 20
Questions data, using completely unsupervised,
data-driven models of semantics based on a large
random sample of web-text. The most linguisti-
cally informed among the models (and so, per-
haps the most psychologically plausible), based
on dependency parses, is the most successful.
Still the performance of sometimes radically dif-
ferent models, from Document-based (syntag-
matic) and Word-Form-based (paradigmatic), is
surprisingly similar. One reason for this may be
that we have reached a ceiling in performance
on the fMRI data, due to its inherent noise ? in
this regard it is interesting to note that an at-
tempt to classify individual concepts using this
data directly, without an intervening model of
semantics, also achieves about 80% (though on a
different task, Shinkareva et al, 2008). Another
possible explanation is that both methods reveal
equivalent sets of underlying semantic dimen-
sions, but figure 1 suggests not. Alternatively,
it may be that the small set of 60 words exam-
ined here may be as well-distinguished by means
120
of their taxonomic differences, as by their top-
ical differences, a suggestion supported by the
results in Pereira et al (2011, see Figure 2A).
From the perspective of computational effi-
ciency however, some of the models have clearer
advantages. The Dependency and Part-of-
Speech models are processing-intensive, since
the broad vocabulary considered requires that
the very large quantities of text pass through
a parsing or tagging pipeline (though these
tasks can be parallelized). The Sequence and
Document models conversely require very large
amounts of memory to store all their features
during SVD. In comparison, the Direction model
is impressive, as it achieves close to optimal per-
formance, despite being very cheap to produce
in terms of processor time and memory foot-
print. Its relatively superior performance may
be due to the relatively fixed word-order of En-
glish, making it a good approximation of a De-
pendency model. For instance, given the nar-
row ?4 token windows used here, the Direction
features shaky Left and donate Right (relative
to a target noun) are probably nearly identical
to the Dependency features shaky Adj and do-
nate Subj. The Sequence model might also be
seen as an approximate Dependency model, but
one with the addition of more superficial colloca-
tions such as ?fish and chips? or ?Judge Judy?,
which are less relevant to our semantic task.
The evidence for the influence of the scal-
ing parameters (number of SVD dimensions,
frequency cutoff) is mixed: cut-off appears to
have little effect either way, and increasing the
number of dimensions can help or hinder (com-
pare the Sequence and Document models). We
can speculate that the Document model is al-
ready ?saturated? with 300 dimensions/topics,
but that the other models based on properties
have a higher inherent dimensionality. It may
also be a lower cut-off and higher dimensional-
ity would show clearer benefits over a larger set
of semantic/syntactic domains, including lower-
frequency words (the lowest frequency work in
the set of 60 used here was igloo, which has an
incidence of 0.3 per million words in the ANC).
PPMI appears to be both effective, and par-
simonious with assumptions one might make
about conceptual representations, where it
would be cognitively onerous and unnecessary
to encode all negative features (such as the facts
that dogs do not have wheels, are not commu-
nication events, and do not belong in the avi-
ation domain). But while SVD is certainly ef-
fective in dealing with the pervasive synonymy
and polysemy seen in corpus-feature sets, it is
less clear that it reveals psychologically plausi-
ble dimensions of meaning. Alternatives such as
non-negative matrix factorization (Lee and Se-
ung, 1999) or Latent Dirichlet Allocation (Blei
et al, 2003) might extract more readily inter-
pretable dimensions; or alternative regularisa-
tion methods such as Elastic Nets, Lasso (Hastie
et al, 2011), or Network Regularisation (Sandler
et al, 2009) might even be capable of identifying
meaningful clusters of features when learning di-
rectly on co-occurrence data. Finally, we should
consider whether more derived datasets could be
used as input data in place of the basic corpus
features used here, such as the full facts learned
by the NELL system (Carlson et al, 2010), or
crowd-sourced data which can be easily gathered
for any word (e.g. association norms, Kiss et al,
1973), though different algorithmic means would
be needed to deal with their extreme degree of
sparsity.
The results also suggest a series of follow-on
analyses. A priority should be to test these
models against a wider range of neuroimaging
data modalities (e.g. MEG, EEG) and stim-
ulus sets, including abstract kinds (see Mur-
phy et al 2012, for a preliminary study), and
parts-of-speech beyond nouns. It may be that a
putative complementarity between word-region
and word-collocate models is only revealed when
we look at a broader sample of the human
lexicon. And beyond establishing what infor-
mational content is required to make semantic
distinctions, other factorisation methods (e.g.
sparse or non-negative decompositions) could be
applied to yield more interpretable dimensions.
Other classification tasks might also be more
sensitive for detecting differences between mod-
els, such as the test of word identification among
a set by rank accuracy, as used in (Shinkareva
et al, 2008).
121
References
Almuhareb, A. and Poesio, M. (2004). Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP, pages 158?165.
Baroni, M. and Lenci, A. (2010). Distributional
Memory : A General Framework for Corpus-Based
Semantics. Computational Linguistics, 36(4):673?
721.
Baroni, M., Murphy, B., Barbu, E., and Poesio, M.
(2010). Strudel: A corpus-based semantic model
based on properties and types. Cognitive Science,
34(2):222?254.
Battig, W. F. and Montague, W. E. (1969). Cate-
gory Norms for Verbal Items in 56 Categories: A
Replication and Extension of the Connecticut Cat-
egory Norms. Journal of Experimental Psychology
Monographs, 80(3):1?46.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003).
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3(4-5):993?1022.
Bradford, R. B. (2008). An empirical study of re-
quired dimensionality for large-scale latent seman-
tic indexing applications. Proceeding of the 17th
ACM conference on Information and knowledge
mining CIKM 08, pages 153?162.
Brants, T. and Franz, A. (2006). Web 1T 5-gram
Version 1.
Bullinaria, J. A. and Levy, J. P. (2007). Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B.,
Jr., E. R. H., and Mitchell, T. M. (2010). To-
ward an Architecture for Never-Ending Language
Learning. Artificial Intelligence, 2(4):3?3.
Chang, K.-m. K., Mitchell, T., and Just, M. A.
(2011). Quantitative modeling of the neural repre-
sentation of objects: how semantic feature norms
can account for fMRI activation. NeuroImage,
56(2):716?727.
Curran, J. R. and Moens, M. (2002). Improvements
in automatic thesaurus extraction. In SIGLEX,
pages 59?66.
Deerwester, S., Dumais, S., Landauer, T., Furnas,
G., and Harshman, R. (1990). Indexing by La-
tent Semantic Analysis. Journal of the American
Society of Information Science, 41(6):391 ? 407.
Devereux, B. and Kelly, C. (2010). Using fMRI ac-
tivation to conceptual stimuli to evaluate meth-
ods for extracting conceptual representations from
corpora. In Murphy, B., Korhonen, A., and
Chang, K. K.-M., editors, 1st Workshop on Com-
putational Neurolinguistics.
Friston, K. J., Ashburner, J. T., Kiebel, S. J.,
Nichols, T. E., and Penny, W. D. (2007). Statis-
tical Parametric Mapping: The Analysis of Func-
tional Brain Images, volume 8. Academic Press.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Kluwer, Dordrecht.
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B.
(2007). Topics in semantic representation. Psy-
chological Review, 114(2):211?244.
Hall, J., Nilsson, J., Nivre, J., Eryigit, G., Megyesi,
B., Nilsson, M., and Saers, M. (2007). Single Malt
or Blended? A Study in Multilingual Parser Op-
timization. CoNLL Shared Task Session, pages
933?939.
Hastie, T., Tibshirani, R., and Friedman, J. (2011).
The Elements of Statistical Learning, volume 18 of
Springer Series in Statistics. Springer, 5th edition.
Jelodar, A. B., Alizadeh, M., and Khadivi, S. (2010).
WordNet Based Features for Predicting Brain Ac-
tivity associated with meanings of nouns. In Mur-
phy, B., Korhonen, A., and Chang, K. K.-M., ed-
itors, 1st Workshop on Computational Neurolin-
guistics, pages 18?26.
Jones, E., Oliphant, T., Peterson, P., and Et Al.
(2001). SciPy: Open source scientific tools for
Python.
Kanejiya, D., Kumar, A., and Prasad, S. (2003).
Automatic evaluation of students? answers using
syntactically enhanced LSA. Building educational
applications, NAACL, 2:53?60.
Kiss, G. R., Armstrong, C., Milroy, R., and Piper, J.
(1973). An associative thesaurus of English and its
computer analysis. In Aitken, A. J., Bailey, R. W.,
and Hamilton-Smith, N., editors, The Computer
and Literary Studies. Edinburgh University Press.
Landauer, T. and Dumais, S. (1997). A solution to
Plato?s problem: the latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Lee, D. D. and Seung, H. S. (1999). Learning the
parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?91.
Lehoucq, R. B., Sorensen, D. C., and Yang, C.
(1998). Arpack users? guide: Solution of large
scale eigenvalue problems with implicitly restarted
Arnoldi methods. SIAM.
122
Lin, D. (1998). Automatic Retrieval and Clustering
of Similar Words. In COLING-ACL, pages 768?
774.
Lin, D. and Pantel, P. (2001). DIRT ? discovery
of inference rules from text. Proceedings of the
seventh ACM SIGKDD international conference
on Knowledge discovery and data mining KDD 01,
datamining:323?328.
Loper, E. and Bird, S. (2002). {NLTK}: The natu-
ral language toolkit. In ACL Workshop, volume 1,
pages 63?70. Association for Computational Lin-
guistics.
Lund, K. and Burgess, C. (1996). Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
Lund, K., Burgess, C., and Atchley, R. (1995). Se-
mantic and associative priming in high dimen-
sional semantic space. In Proceedings of the 17th
Cognitive Science Society Meeting, pages 660?665.
Mitchell, T. M., Shinkareva, S. V., Carlson, A.,
Chang, K.-M., Malave, V. L., Mason, R. A., and
Just, M. A. (2008). Predicting Human Brain Ac-
tivity Associated with the Meanings of Nouns. Sci-
ence, 320:1191?1195.
Murphy, B., Baroni, M., and Poesio, M. (2009). EEG
responds to conceptual stimuli and corpus seman-
tics. In Proceedings of EMNLP, pages 619?627.
ACL.
Murphy, B., Korhonen, A., and Chang, K. K.-
M., editors (2010). Proceedings of the 1st Work-
shop on Computational Neurolinguistics, NAACL-
HLT, Los Angeles. ACL.
Murphy, B., Talukdar, P., and Mitchell, T. (2012).
Comparing Abstract and Concrete Conceptual
Representations using Neurosemantic Decoding.
In NAACL Workshop on Cognitive Modelling and
Computational Linguistics.
Nancy Ide and Keith Suderman (2006). The Amer-
ican National Corpus First Release. Proceedings
of the Fifth Language Resources and Evaluation
Conference (LREC).
Nation, P. and Waring, R. (1997). Vocabulary size,
text coverage and word lists. In Schmitt, N. and
McCarthy, M., editors, Vocabulary Description ac-
quisition and pedagogy, pages 6?19. Cambridge
University Press.
Pado?, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199.
Paice, C. D. (1990). Another stemmer. SIGIR Fo-
rum, 24(3):56?61.
Palatucci, M., Hinton, G., Pomerleau, D., and
Mitchell, T. M. (2009). Zero-Shot Learning with
Semantic Output Codes. Advances in Neural In-
formation Processing Systems, 22:1?9.
Palatucci, M. M. (2011). Thought Recognition: Pre-
dicting and Decoding Brain Activity Using the
Zero-Shot Learning Model. PhD thesis, Carnegie
Mellon University.
Pereira, F., Detre, G., and Botvinick, M. (2011).
Generating Text from Functional Brain Images.
Frontiers in Human Neuroscience, 5:1?11.
Rapp, R. (2003). Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the
Ninth Machine Translation Summit, pp:315?322.
Rehurek, R. and Sojka, P. (2010). Software Frame-
work for Topic Modelling with Large Corpora. In
New Challenges, LREC 2010, pages 45?50. ELRA.
Rubenstein, H. and Goodenough, J. B. (1965). Con-
textual correlates of synonymy. Communications
of the ACM, 8(10):627?633.
Sahlgren, M. (2006). The Word-Space Model: Using
distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stock-
holm University.
Sandler, T., Talukdar, P. P., Ungar, L. H., and
Blitzer, J. (2009). Regularized Learning with Net-
works of Features. Advances in Neural Informa-
tion Processing Systems 21, 4:1401?1408.
Schu?tze, H. and Pedersen, J. (1993). A Vector Model
for syntagmatic and paradigmatic relatedness. In
Making Sense of Words Proceedings of the 9th
Annual Conference of the University of Waterloo
Centre for the New OED and Text Research, pages
104?113.
Shinkareva, S. V., Mason, R. A., Malave, V. L.,
Wang, W., Mitchell, T. M., and Just, M. A.
(2008). Using fMRI Brain Activation to Iden-
tify Cognitive States Associated with Perception
of Tools and Dwellings. PloS ONE, 3(1).
Turney, P. D. and Pantel, P. (2010). From Frequency
to Meaning: Vector Space Models of Semantics.
Artificial Intelligence, 37(1):141?188.
Widdows, D. (2003). Unsupervised methods for de-
veloping taxonomies by combining syntactic and
statistical information. In NAACL, pages 197?
204. Association for Computational Linguistics.
123
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 307?315,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Crowdsourced Comprehension:
Predicting Prerequisite Structure in Wikipedia
Partha Pratim Talukdar
Machine Learning Department
Carnegie Mellon University
ppt@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
The growth of open-access technical publica-
tions and other open-domain textual informa-
tion sources means that there is an increas-
ing amount of online technical material that
is in principle available to all, but in prac-
tice, incomprehensible to most. We propose
to address the task of helping readers com-
prehend complex technical material, by us-
ing statistical methods to model the ?prereq-
uisite structure? of a corpus ? i.e., the se-
mantic impact of documents on an individual
reader?s state of knowledge. Experimental re-
sults using Wikipedia as the corpus suggest
that this task can be approached by crowd-
sourcing the production of ground-truth labels
regarding prerequisite structure, and then gen-
eralizing these labels using a learned classifier
which combines signals of various sorts. The
features that we consider relate pairs of pages
by analyzing not only textual features of the
pages, but also how the containing corpora is
connected and created.
1 Introduction and Motivation
Nicholas Carr has argued in his recent popular book
?The Shallows? that existing Internet technologies
encourage ?shallow? processing of recent and pop-
ular information, at the expense of ?deeper?, con-
templative study of less immediately-accessible in-
formation (Carr, 2011) . While Carr?s hypothesis is
difficult to formalize rigorously, it seems intuitively
plausible. For instance, user-generated content from
Twitter and Facebook is mainly comprised of short,
shallow snippets of information. Most current re-
search in AI (and more broadly in computer science)
does not seem likely to reverse this trend: e.g., work
in crowdsourcing has concentrated on tasks that can
be easily decomposed into small pieces, and much
current NLP research aims at facilitating short-term
?shallow? goals, such as answering well-formulated
questions (e.g., (Kwok et al, 2001)) and extracting
concrete facts (e.g., (Etzioni et al, 2006; Yates et al,
2007; Carlson et al, 2010)). This raises the ques-
tion: what can AI do to facilitate deep, contempla-
tive study?
In this paper we address one aspect of this larger
goal. Specifically, we consider automation of a
novel task?using AI methods to facilitate the ?deep
comprehension? of complex technical material. We
conjecture that the primary reason that technical
documents are difficult to understand is lack of mod-
ularity: unlike a self-contained document written for
a general reader, technical documents require cer-
tain background knowledge to comprehend?while
that background knowledge may also be available in
other on-line documents, determining the proper se-
quence of documents that a particular reader should
study is difficult.
We thus formulate the problem of comprehending
technical material as a probabilistic planning prob-
lem, where reading a document is an operator that
will probabilistically change the state of knowledge
K(u, t) of a user u at time t, in a manner that de-
pends on u?s prior knowledge K(u, t ? 1). Solving
this task requires, among other things, understand-
ing the effect of reading individual documents d ?
specifically, the concepts that are explained by d,
and the concepts that are prerequisites for compre-
hending d. This paper addresses this problem. In
particular, we consider predicting whether one page
in Wikipedia is a prerequisite of another.
More generally, we define the ?prerequisite struc-
307
CRF
Statistical_model
Graphical_model
Discriminative_model Markov_random_field
Parameter_learning
Maximum_likelihood
Hidden_Markov_Model
Viterbi_algorithm
random_variable
probability_distribution
variable_mathematics
Conditional_independence
Gradient_descent
Baum_Welch_algorithmMarkov_chain
Inference dynamic_programming Expectional_maximization_algorithm
Figure 1: The prerequisite structure rooted at the page ?Conditional Random Fields?, omitting nodes that would
already be known a typical CS graduate student.
Variable (Mathematics) Random Variable Probability Distribution
Conditional Independence Statistical Model Graphical Model
Discriminative Model Markov Random Field
Gradient Descent Parameter Learning Maximum Likelihood
Inference Dynamic Programming Viterbi Algorithm
Markov Chain Expectation Maximization Algorithm Baum Welch Algorithm
Hidden Markov Model CRF
Figure 2: A plan for comprehending ?Conditional Random Fields? (to be read left-to-right, top-to-bottom). Horizontal
lines indicate breaks between independent sections of the subgraph.
ture? for a corpus as a graph, where nodes are con-
cepts to comprehend, and a directed edge d ? d?
corresponds to the assertion ?understanding d? is a
prerequisite to understanding d?. For Wikipedia, we
assume a one-to-one correspondence between doc-
ument titles and concepts explicated by (i.e., post-
conditions of) these documents. Figure 2 presents
a small example of a prerequisite structure, and in-
dicates how it might be used to construct a plan for
comprehending a specific concept.
Focusing on Wikipedia has several advantages.
First, it is densely linked, and hence a document d
will likely be linked directly to any prerequisite page
d?. (However, not all hyperlinks will indicate a pre-
requisite.) Second, Wikipedia?s standardized format
makes textual analysis easier. Finally, there is a great
deal of social information available about how docu-
ments are used by the Wikipedia community. These
properties make it easy for us to explore the infor-
mativeness of different types of information with re-
spect to predicting prerequisite structure.
Our overall plan for producing a prerequisite
structure for a corpus is first, to use crowdsourc-
ing approaches to obtain a subset of the prerequisite
structure; and second, to extrapolate this structure
to the entire corpus using machine learning. Below,
we first describe datasets that we have collected,
based on five technical concepts in Wikipedia from
five different fields. We then outline the specifics of
our procedure for annotating prerequisite structure,
using Amazon?s Mechanical Turk, and demonstrate
that meaningful signals about prerequisite structure
can be obtained using a classifier that exploits sev-
eral sources: graph analysis of Wikipedia?s link
graph; graph analysis of a bipartite graph relating
Wikipedia pages to Wikipedians that have edited
these pages; and textual analysis. We complete our
experimental analysis of the prerequisite-structure
prediction task by discussing and evaluating the de-
gree to which prerequisite-structure prediction is
domain-independent, and the degree to which differ-
ent subareas of Wikipedia (e.g., biology vs computer
science) require different predictors.
After discussing related work, we return in the
concluding remarks to the overarching goal of fa-
cilitating comprehension, and discuss the relation-
308
Target Concept #Nodes #Edges #Edits
Global Warming 19,170 501,608 1,490,967
Meiosis 19,811 444,100 880,684
Newton?s Laws of Motion 15,714 436,035 795,988
Parallel Postulate 14,966 363,462 858,785
Public-key cryptography 16,695 371,104 1,003,181
Table 1: Target concepts used in the experiments.
ship of the current study to these goals. Specifi-
cally we note that facilitating comprehension also
requires understanding a user?s goals, and her initial
state of knowledge, in addition to understanding the
prerequisite structure of the corpus. We also discuss
the relationship between planning and prerequisite-
structure prediction and suggest that use of appro-
priately robust planning methods may lead to good
comprehension plans, even with imperfectly pre-
dicted prerequisite structure.
2 Experiments
As discussed above, we focus in this paper
on predicting prerequisite structure in Wikipedia.
While most Wikipedia pages are accessible to a
general reader, there are many pages that de-
scribe technical concepts, such as ?conditional
random fields?, ?cloud radiative forcing?, and
?Corticotropin-releasing factor?. Most of these tech-
nical pages are not self-contained: for instance,
to read and comprehend the page on ?conditional
random fields?, one will have to first understand
?graphical model?, and so on, as suggested by Fig-
ure 1. In this section, we evaluate the following
questions:
? Can we train a statistical classifier for prereq-
uisite classification in a target domain, where
the classifier is trained on out of domain (i.e.,
non-target domain) data annotated using Ama-
zon Mechanical Turk service?
? What are the effects of different types of signals
on the performance of such a classifier?
? How does out of domain training compare to in
domain training?
2.1 Experimental Setup
For our experiments, we choose five targets from
differing areas for experimentation, listed in Table 1.
Several of the techniques we used are based on graph
analysis. The full graphs associated with Wikipedia
are unwieldy to use for experimentation because of
their size: therefore, for each target concept, we ex-
tracted a moderate-sized low-conductance subgraph
of Wikipedia?s link graph containing the target, us-
ing a variant of the PageRank-Nibble algorithm (An-
dersen et al, 2006).1. As parameters we used ? =
0.15 and  = 10?7, yielding graphs with approx-
imately 15-20,000 nodes and 350-500,000 edges
each. We also collected the edit history for each
page in every subgraph forming a second graph for
each sub-domain 2. On average, each page from
these subgraphs had been edited about 20 times, by
about 8 unique editors. Details are given in Table 1.
For classification, we used a Maximum Entropy
(MaxEnt) classifier. Given a pair of Wikipedia pages
x = (d, d
?
) connected by a directed edge (hyperlink)
from d to d
?
, the classifier will predict with probabil-
ity p(+1|x) whether the main concept in page d
?
is
a prerequisite for the main concept in page d. The
classifier has the form
p(y|x) =
exp(w ? ?(x, y))
?
y??Y exp(w ? ?(x, y
?))
, y ? Y = {?1,+1}
where ?(x, y) is a feature function which represents
the pair of pages x = (d, d
?
) in a high dimensional
space, and w is the parameter vector of the classifier
which is estimated from training data. We use the
Mallet package3 to train and evaluate classifiers. For
the experiments in this paper, we shall exploit the
following types of features:
WikiHyperlinks: Features include the random
walk with restart (RWR) score (Tong et al,
2006) of the target concept page d
?
starting
from the source page d. Additional features
include the PageRank score of the target and
source pages.
1Specifically, we used the ?ApproximatePageRank? method
from (Andersen et al, 2006) to find a set of nodes S containing
a low-conductance subgraph, but did not prune S to find the
lowest-conductance subgraph of it with a ?sweep?. The version
of Wikipedia?s link graph we used was DBPedia?s version 3.7
(Auer et al, 2007)
2Specifically, a bipartite graph connecting pages and editors.
We used a version of Wikipedia?s edit history extracted by other
researchers (Leskovec et al, 2010), discarding edits marked as
?minor? by the editor.
3Mallet package: http://mallet.cs.umass.edu/
309
Domain
Time (s) / Worker
# HITs ?
Evaluation / HIT
Meiosis 38 3 400 0.50
Public-key Cryp. 26 3 200 0.63
Parallel Postulate 41 3 200 0.55
Newton?s Laws 20 5 400 0.47
Global Warming 14 5 400 0.56
Average 27.8 - - 0.54
Table 2: Statistics about the Gold-standard data prepared
using Amazon Mechanical Turk. Also shown are the
averaged ? statistics-based inter-annotator agreement in
each domain. The last row corresponds to the ? value
averaged across all five domains.
WikiEdits: This includes one feature?the
analogous RWR score on the graph of edit in-
formation.
WikiPageContent: Features in this category
are derived from the contents of the two
Wikipedia pages d and d
?
. Examples include:
the category identity of the source page; the
category identity of the target page; whether
the titles of d
?
and d are mentioned in the first
sentence of d; the name of the first section in d
which contains a link to d
?
; whether there is any
overlap in categories between the two pages;
whether d is also linked from d
?
; and the log of
the number of times d? is linked form d. We use
the JWPL library (http://jwpl.googlecode.com)
for efficient and structured access to Wikipedia
pages from a recent dump obtained on Jan 4,
2012.
2.1.1 Gold-standard Annotation from
Mechanical Turk4
In order to evaluate different prerequisite classi-
fication systems and also to train the MaxEnt clas-
sifier, we collected gold prerequisite decisions us-
ing Amazon Mechanical Turk (AMT). Since prepar-
ing annotated gold data for entire graphs in Table 1
would be prohibitively expensive, we used the fol-
lowing strategy to sample a smaller subgraph from
the larger domain-specific subgraph, which in turn
will be used for training and evaluation purposes.
Preliminary investigation suggested that most of the
pages in the prerequisite structure rooted at a target
4Amazon Mechanical Turk: http://mturk.amazon.com
concept d are connected to d via many short hyper-
link paths. Hence, for each target domain, we first
selected the top 20 nodes with highest RWR scores,
relative to the target concept, in the subgraph for that
target concept (as listed in Table 1.) We then sam-
pled a total of 400 edges from these selected nodes,
with outgoing edges from a node sampled with a fre-
quency proportional to its RWR score. Thus, using
this strategy, we selected up to 400 pairs of pages
(d, d
?
), where each pair has a hyperlink from d to d?.
Classification of a pair of hyperlinked Wikipedia
pages (d, d
?
) into one of the four following classes
constituted a Human Intelligence Task (HIT): (1) d
?
is a prerequisite of d; (2) d is a prerequisite of d
?
; (3)
the two pages are unrelated; (4) Don?t know. Sub-
sequently, based on the feedback from the workers,
a fifth option was also added: the two concepts are
related, but they don?t have any prerequisite relation-
ship between them. Based on the available workers
and turnaround time, the number of assignments per
HIT (i.e., number of unique workers assigned to a
particular HIT) was either 3 or 5; and the number
of HITs used was either 200 or 400. Depending on
the hardness of domain and availability of workers
opting to work on a domain, reward per HIT assign-
ment was varied from $0.02 (for Global Warming
and Newton?s Laws) to $0.08 (for Public-key Cryp-
tography, Meiosis and Parallel Postulate). This data
collection stage spanning all five domains was com-
pleted in about a week at a total cost of $278. Statis-
tics about the data are presented in Table 25.
Starting with the AMT data collected as above,
we next created a binary-labeled training dataset,
where each instance corresponds to a pair of pages.
We ignored all ?Don?t Know? labels, treated option
(1) above as vote for the corresponding prerequisite
edge, and treated all other options as votes against.
We then assigned the final label for a node pair using
majority vote (breaking ties arbitrarily).
2.1.2 Consistency of labels
In contrast to standard setup of gold data prepara-
tion where a single annotator is guaranteed to pro-
vide feedback on every instance, the situation in
case of Mechanical Turk-based annotation is differ-
ent, as the workers are at liberty to choose the HITs
(or instances) they want to work on. This makes
5The dataset is available upon request from the authors.
310
40
50
60
70
80
Meiosis Public Key Para. Postulate Newton?s Law Global Warming Average
Performance Comparison for Prerequisite Prediction
A
c
c
u
r
a
c
y
Random Baseline MaxEnt Classifier
Figure 3: Comparison of performance between the Max-
Ent classifier (right bar in each group) against a random
baseline (left bar in each group) in all five domains. On
average, the MaxEnt classifier results in an 8.6% absolute
improvement in accuracy.
standard ? statistics-based inter-annotator computa-
tion (Fleiss, 1981) inapplicable in the current set-
ting. We circumvented this problem by first select-
ing all workers with at least 100 feedbacks, and then
computing pairwise ? statistics between all pairs of
these frequent workers. These ? statistics were aver-
aged across each domain, and also averaged across
all domains. The results, also shown in Table 2,
show moderate agreement (recall that ? = 0 indi-
cates no correlation). We are encouraged to observe
that moderate level of agreement is possible even in
this setting, where there is no control over worker
background and quality. We next explore whether
this level of agreement is sufficient to train statisti-
cal classifiers.
2.2 Prerequisite Classification
In this section, we explore whether it is possible to
train a MaxEnt classifier to determine prerequisite
structure in a target domain, with the training per-
formed in ?leave one domain out? manner, where
the training data originates from domains other than
the target domain. For example, for classifications in
the target domain, say ?Global Warming?, we train
the classifier with annotated data from the remaining
four domains (or whatever domains are available).
We note that training on ?out of domain?, if it is
successful, has several benefits. First, a successful
training strategy in this setup removes any need to
have labeled data in each target domain of interest,
which is particularly relevant as labeled data is ex-
pensive to prepare. Second, a classifier trained just
once can be repeatedly used across multiple domains
without requiring retraining.
Accuracies of MaxEnt classifiers trained using the
?leave one domain out? strategy are shown in Fig-
ure 3; we report the test accuracy on each target do-
main, as well as the average across domains. Perfor-
mance of a random classifier is presented as a base-
line. Classes in the train and test sets were balanced
by oversampling the minority class. From Figure 3,
we observe that it is indeed possible to train pre-
requisite classifiers in an out of domain setting, us-
ing data from the Amazon Mechanical Turk service;
on average, the classifier outperforms the random
baseline with 8.6% absolute improvement in classi-
fication accuracy. We also experimented with other
rule-based classifiers6, and in all cases, the trained
MaxEnt classifier outperformed these baselines. Al-
though more sophisticated training strategies and
more clever feature engineering would likely yield
further improvements, we find it encouraging that
even a relatively straightforward classification tech-
nology along with a basic set of features was able to
achieve significant improvement in performance on
the novel task of prerequisite prediction.
2.3 Feature Ablation Experiments
The MaxEnt classifier evaluated in the previous
section had access to all three types of features:
WikiEdits, WikiHyperLinks, and WikiPageContent,
as described in the beginning of this section. In or-
der to evaluate the contribution of each such sig-
nal, we created ablated versions of the full Max-
Ent classifier which uses only one of these three
subsets. We call these thee variants: MaxEnt-
WikiEdits, MaxEnt-WikiHyperLinks, and MaxEnt-
WikiPageContent, respectively. Average accuracies
across all five domains comparing these three vari-
ants, in comparison to the Random baseline and
the full classifier (MaxEnt-Full, as in previous sec-
tion) are presented in Table 3. From this, we ob-
serve that all three variants perform better than the
random baseline, with maximum gains achieved
by the MaxEnt-WikiPageContent classifier, which
uses page content-based features exclusively. We
6For example, classify d
?
as a prerequisite for d if d
?
is
linked from the first paragraph in d.
311
System Accuracy
Random 50.22
MaxEnt-WikiEdits 51.62
MaxEnt-WikiHyperlinks 52.70
MaxEnt-WikiPageContent 57.84
MaxEnt-Full 58.82
Table 3: Comparison of accuracies (averaged across all
five domains) of the full MaxEnt classifier with its ablated
versions which use a subset of the features, and also the
random baseline. The full classifier, which exploits all
three types of signals (viz., WikiEdits, WikiHyperlinks,
and WikiPageContent) achieves the highest performance.
Domain
Wiki- Wiki- WikiPage-
All
Edits HyperLinks Content
Meiosis 5.4 2.4 0.3 1
Public-key
-0.7 -1.8 15.1 17.1
Crypto.
Parallel
3.1 6.1 11.7 14.7
Postulate
Newton?s
-0.2 6.2 3.9 3.9
Laws
Global
-7.7 0.1 5.8 6.8
Warming
Table 4: Accuracy gains (absolute) relative to the Ran-
dom baseline achieved by the full MaxEnt classifier as
well as its ablated versions trained with three different
subsets of the full classifier. Positive gains are marked in
bold.
also note that the full classifier MaxEnt-Full, is
able to effectively combine three types of signals
improving performance even further. In Table 4,
we present a per-domain breakdown of the gains
achieved by these four classifiers over the random
baseline. From this, we observe that the MaxEnt-
WikiEdits classifier outperforms the random base-
line only in 2 out of 5 domains. This might be due
to the fact that the MaxEnt-WikiEdits uses uses only
one feature?the RWR score of the target page rela-
tive to the source page on the Wikipedia edits graph.
We hope that use of more discriminating features
should further help this classifier. From Table 4, we
also observe that MaxEnt-WikiHyperLinks is able to
outperform the random baseline in 4 out of 5 cases,
and the MaxEnt-WikiPageContent (as well as the
full classifier) outperforms the random baseline in
all 5 domains, sometimes with large gains (as in the
case of Public-key Cryptography domain).
40
50
60
70
80
Meiosis Public Key Para. Postulate Newton?s Laws Global Warming Average
Effect of Out of Domain vs In Domain Training
A
c
c
u
r
a
c
y
Out of Domain Training In Domain Training
Figure 4: Accuracy comparison of out of domain (left bar
in each group) and in domain training (right bar in each
group) for the five domains. From this we observe that
good generalization performance is possible even when
there is no in domain training data available.
2.4 Effect of Out of Domain Training
All the classifiers evaluated in previous sections
were trained in an out of domain setting, i.e., the
training data originated from domains outside the
domain in which the classifier is applied and eval-
uated. This has several benefits, as noted above. An
alternative and more standard way to train classi-
fiers is to have the training and evaluation data be
from the same domain (below, the in-domain set-
ting). While such a classifier will require labeled
training from each domain of interest, it is nonethe-
less of interest to compare in-domain and out-of-
domain learning. If there are substantive differences,
this could be used to improve prerequisite-structure
predictor in a subdomain (e.g., biology), or may
suggest alternative training methods (e.g., involving
transfer learning).
Motivated by this, for each domain, we com-
pare the performances of the out-of-domain and in-
domain classification performances. The results are
shown in Figure 4. On average, we observe that the
out-of-domain classifier is able to achieve 93% of
the performance of the in-domain classifier. We note
that this is encouraging for domain-independent
prerequisite-structure prediction, as this suggests
that for the prerequisite classification task, close to
optimal (i.e., in-domain performance) is possible
when the classifiers are trained in an out-of-domain
setting.
312
3 Related Work
We believe the task of prerequisite structure predic-
tion to be novel; however, it is clearly related to a
number of other well-studied research problems.
In light of our emphasis on Wikipedia, a con-
nection can be drawn between identifying prerequi-
sites and measuring the semantic relatedness of con-
cepts using Wikipedia?s link structure (Yeh et al,
2009). We consider here a related but narrower
question, namely whether an inter-page link will im-
prove comprehension for a specific reader.
In the area of intelligent tutoring and educational
data mining, recent research has looked at enriching
textbooks with authoritative web content (Agrawal
et al, 2010). Also, the problem of detecting pre-
requisite structure from differential student perfor-
mance on tests has been considered (e.g., (Pavlik
et al, 2008; Vuong et al, 2011)). Our proposal con-
siders discovering prerequisite structure from text,
rather than from exercises, and relies on different
signals.
Research in adaptive hypermedia (surveyed else-
where (Chen and Magoulas, 2005)) has goals similar
to ours. Most adaptive hypermedia systems operate
in narrow domains, which precludes use of some of
the crowd-based signals we consider here. In this lit-
erature, a distinction is often made between ?adapt-
ability? (the ability for a user to modify a presenta-
tion of hypermedia) and ?adaptivity? (the ability of
a system to adapt to a user?s needs.) In this frame-
work, our project focuses on adding ?adaptivity? to
existing corpora via a prerequisite structure, and our
principle contribution to this area is identifying tech-
niques that learn to combine textual features and so-
cial, crowd-based signals in order to usefully guide
comprehension.
Another related area is data-mining logs of Web
usage, as surveyed by Pierrakos et al(Pierrakos
et al, 2003). Our focus here is on facilitating a
particular type of Web usage, comprehension, rather
than more commonly-performed tasks like site nav-
igation and purchasing.
A number of ?open education? resources exist, in
which information can be organized into sharable
modules with known prerequisites between them
(e.g., Connexions (Baraniuk, 2008)). We focus here
on discovering prerequisite structure with machine-
learning methods rather than simply encoding it.
Similarly, a Wikimedia project7 has developed in-
frastructure allowing a user to manually assemble
Wikipedia pages into e-books. Our focus is on guid-
ing the process of finding and ordering the sections
of these books, not the infrastructure for generating
them. We also note that one widely-used way for
complex technical concepts to be broadly commu-
nicated is by writers or teams of writers, and pre-
vious researchers have investigated understanding
how collaborative writers work (Noe?l and Robert,
2004), and even developed tools for collaborative
writing (Zheng et al, 2006). Our work focuses on
tools to empower readers, rather than writers.
4 Conclusion
In this paper, we motivated the goal of ?crowdsourc-
ing? the task of helping readers comprehend com-
plex technical material, by using machine learning
to predict prerequisite structure from not only docu-
ment text, but also crowd-generated data such as hy-
perlinks and edit logs. While it is not immediately
obvious that this task is feasible, our experiments
suggest that relatively reliable features to predict
prerequisite structure exist, and can be successfully
combined using standard machine learning methods.
To achieve the broader goal of facilitating com-
prehension, predicting prerequisite structure is not
enough. Another important subproblem is using pre-
dicted prerequisites to build a feasible plan. As part
of ongoing work, we are exploring use of modern
optimization methods (such as Integer Linear Pro-
gramming) to compute ?reading plans? that mini-
mize a weighted linear combination of expected user
effort and probability of plan ?failure?8.
We also plan to explore another major subprob-
lem associated with facilitating comprehension?
personalizing a reading plan. Clearly, even if d? is
a prerequisite for d, a user interested in d need not
first read a page explaining d?, if she already under-
stands d?; instead, a reading plan based on prereq-
uisite structure should be adjusted based on what is
believed about the user?s prior knowledge state. In
7See http://en.labs.wikimedia.org/wiki/Wiki to print, the
?Wiki to Print? project.
8A plan ?failure? means that the plan not actually satisfy all
necessary prerequisites, leading to imperfect comprehension on
the part of the reader after she executes the plan.
313
the context of Wikipedia comprehension, one possi-
ble signal for predicting an individuals? prior knowl-
edge is the Wikipedia edit log: if we assume that
editors tend to edit things they know, the edit log
indicates which concepts tend to be jointly known,
and hence collaborative-filtering methods might be
able to more completely predict a user?s knowledge
given partial information about her knowledge?just
as collaborative-filtering is often used now to extrap-
olate user preference?s from knowledge of others?
joint preferences.
Besides contributing to the goal of facilitating
comprehension, we believe that the specific problem
of predicting prerequisite structure in Wikipedia is
a task of substantial independent interest. Prereq-
uisite structure can be thought of as a sort of ex-
planatory discourse structure, which is overlaid on
a hyperlink graph; hence, scaling up our methods
and applying them to all of Wikipedia would iden-
tify a canonical broad-coverage instance of such ex-
planatory discourse. This could be re-used for other
tasks much as lexical resources like WordNet are:
for instance, consider identifying explanatory dis-
course in an external technical text (e.g., a textbook)
by soft-matching it to the Wikipedia structure, us-
ing existing techniques to match the external text to
Wikipedia (Agrawal et al, 2010; Mihalcea and Cso-
mai, 2007; Milne and Witten, 2008).
Acknowledgments
This research has been supported in part by DARPA
(under contract number FA8750-09-C-0179), and
Google. Any opinions, findings, conclusions and
recommendations expressed in this paper are the au-
thors? and do not necessarily reflect those of the
sponsors. We are thankful to the anonymous review-
ers for their constructive comments
References
Agrawal, R., Gollapudi, S., Kenthapadi, K., Srivastava,
N., and Velu, R. (2010). Enriching textbooks through
data mining. In Proceedings of the First ACM Sympo-
sium on Computing for Development, page 19. ACM.
Andersen, R., Chung, F., and Lang, K. (2006). Local
graph partitioning using pagerank vectors. In Founda-
tions of Computer Science, 2006. FOCS?06. 47th An-
nual IEEE Symposium on, pages 475?486. IEEE.
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyga-
niak, R., and Ives, Z. (2007). Dbpedia: A nucleus
for a web of open data. In Aberer, K., Choi, K.-S.,
Noy, N., Allemang, D., Lee, K.-I., Nixon, L., Golbeck,
J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber,
G., and Cudr-Mauroux, P., editors, The Semantic Web,
volume 4825 of Lecture Notes in Computer Science,
pages 722?735. Springer Berlin / Heidelberg.
Baraniuk, R. (2008). Challenges and opportunities for
the open education movement: A Connexions case
study, pages 229?246. MIT Press, Cambridge, Mas-
sachusetts.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hr-
uschka Jr, E., and Mitchell, T. (2010). Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Conference on Artificial Intelligence
(AAAI), pages 1306?1313.
Carr, N. (2011). The shallows: What the Internet is doing
to our brains. WW Norton & Co Inc.
Chen, S. and Magoulas, G. (2005). Adaptable and adap-
tive hypermedia systems. IRM Press.
Etzioni, O., Banko, M., and Cafarella, M. (2006). Ma-
chine reading. In Proceedings of the National Confer-
ence on Artificial Intelligence.
Fleiss, J. (1981). The measurement of interrater agree-
ment. Statistical methods for rates and proportions,
2:212?236.
Kwok, C., Etzioni, O., and Weld, D. (2001). Scaling
question answering to the web. ACM Transactions on
Information Systems (TOIS), 19(3):242?262.
Leskovec, J., Huttenlocher, D., and Kleinberg, J. (2010).
Governance in social media: a case study of the
wikipedia promotion process. In AAAI International
Conference on Weblogs and Social Media (ICWSM
?10). AAAI.
Mihalcea, R. and Csomai, A. (2007). Wikify!: linking
documents to encyclopedic knowledge. In CIKM, vol-
ume 7, pages 233?242.
Milne, D. and Witten, I. (2008). Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509?518. ACM.
Noe?l, S. and Robert, J. (2004). Empirical study on
collaborative writing: What do co-authors do, use,
and like? Computer Supported Cooperative Work
(CSCW), 13(1):63?89.
Pavlik, P., Cen, H., Wu, L., and Koedinger, K. (2008).
Using item-type performance to covariance to improve
the skill acquisition of an existing tutor. In Proc. of
the 1st International Conference on Educational Data
Mining.
Pierrakos, D., Paliouras, G., Papatheodorou, C., and Spy-
ropoulos, C. (2003). Web usage mining as a tool for
314
personalization: A survey. User Modeling and User-
Adapted Interaction, 13(4):311?372.
Tong, H., Faloutsos, C., and Pan, J.-Y. (2006). Fast ran-
dom walk with restart and its applications. In Proceed-
ings of the Sixth International Conference on Data
Mining, ICDM ?06.
Vuong, A., Nixon, T., and Towle, B. (2011). A method
for finding prerequisites within a curriculum. In Proc.
of the 4th International Conference on Educational
Data Mining.
Yates, A., Cafarella, M., Banko, M., Etzioni, O., Broad-
head, M., and Soderland, S. (2007). Textrunner: Open
information extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
Yeh, E., Ramage, D., Manning, C., Agirre, E., and Soroa,
A. (2009). Wikiwalk: random walks on wikipedia
for semantic relatedness. In Proceedings of the 2009
Workshop on Graph-based Methods for Natural Lan-
guage Processing, pages 41?49. Association for Com-
putational Linguistics.
Zheng, Q., Booth, K., and McGrenere, J. (2006). Co-
authoring with structured annotations. In Proceedings
of the SIGCHI conference on Human Factors in com-
puting systems, pages 131?140. ACM.
315
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 84?93,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Documents and Dependencies: an Exploration of
Vector Space Models for Semantic Composition
Alona Fyshe, Partha Talukdar, Brian Murphy and Tom Mitchell
Machine Learning Department &
Center for the Neural Basis of Cognition
Carnegie Mellon University, Pittsburgh
{afyshe|ppt|bmurphy|tom.mitchell}@cs.cmu.edu
Abstract
In most previous research on distribu-
tional semantics, Vector Space Models
(VSMs) of words are built either from
topical information (e.g., documents in
which a word is present), or from syntac-
tic/semantic types of words (e.g., depen-
dency parse links of a word in sentences),
but not both. In this paper, we explore the
utility of combining these two representa-
tions to build VSM for the task of seman-
tic composition of adjective-noun phrases.
Through extensive experiments on bench-
mark datasets, we find that even though
a type-based VSM is effective for seman-
tic composition, it is often outperformed
by a VSM built using a combination of
topic- and type-based statistics. We also
introduce a new evaluation task wherein
we predict the composed vector represen-
tation of a phrase from the brain activity of
a human subject reading that phrase. We
exploit a large syntactically parsed corpus
of 16 billion tokens to build our VSMs,
with vectors for both phrases and words,
and make them publicly available.
1 Introduction
Vector space models (VSMs) of word semantics
use large collections of text to represent word
meanings. Each word vector is composed of fea-
tures, where features can be derived from global
corpus co-occurrence patterns (e.g. how often a
word appears in each document), or local corpus
co-occurrence patterns patterns (e.g. how often
two words appear together in the same sentence,
or are linked together in dependency parsed sen-
tences). These two feature types represent dif-
ferent aspects of word meaning (Murphy et al,
2012c), and can be compared with the paradig-
matic/syntagmatic distinction (Sahlgren, 2006).
Global patterns give a more topic-based mean-
ing (e.g. judge might appear in documents also
containing court and verdict). Certain local pat-
terns give a more type-based meaning (e.g. the
noun judge might be modified by the adjective
harsh, or be the subject of decide, as would related
and substitutable words such as referee or con-
ductor). Global patterns have been used in Latent
Semantic Analysis (Landauer and Dumais, 1997)
and LDA Topic models (Blei et al, 2003). Local
patterns based on word co-occurrence in a fixed
width window were used in Hyperspace Analogue
to Language (Lund and Burgess, 1996). Subse-
quent models added increasing linguistic sophisti-
cation, up to full syntactic and dependency parses
(Lin, 1998; Pado? and Lapata, 2007; Baroni and
Lenci, 2010).
In this paper we systematically explore the util-
ity of a global, topic-based VSM built from what
we call Document features, and a local, type-based
VSM built from Dependency features. Our Doc-
ument VSM represents each word w by a vector
where each feature is a specific document, and the
feature value is the number of mentions of word
w in that document. Our Dependency VSM rep-
resents word w with a vector where each feature
is a dependency parse link (e.g., the word w is the
subject of the verb ?eat?), and the feature value is
the number of instances of this dependency fea-
ture for word w across a large text corpus. We
also consider a third Combined VSM in which
the word vector is the concatenation of its Doc-
ument and Dependency features. All three mod-
els subsequently normalize frequencies using pos-
itive pointwise mutual-information (PPMI), and
84
are dimensionality reduced using singular value
decomposition (SVD). This is the first systematic
study of the utility of Document and Dependency
features for semantic composition. We construct
all three VSMs (Dependencies, Documents, Com-
bined) using the same text corpus and preprocess-
ing pipeline, and make the resulting VSMs avail-
able for download (http://www.cs.cmu.
edu/?afyshe/papers/conll2013/). To
our knowledge, this is the first freely available
VSM that includes entries for both words and
adjective-noun phrases, and it is built from a much
larger corpus than previously shared resources (16
billion words, 50 million documents). Our main
contributions include:
? We systematically study complementarity of
topical (Document) and type (Dependency)
features in Vector Space Model (VSM)
for semantic composition of adjective-noun
phrases. To the best of our knowledge, this is
one of the first studies of this kind.
? Through extensive experiments on standard
benchmark datasets, we find that a VSM built
from a combination of topical and type fea-
tures is more effective for semantic compo-
sition, compared to a VSM built from Docu-
ment and Dependency features alone.
? We introduce a novel task: to predict the vec-
tor representation of a composed phrase from
the brain activity of human subjects reading
that phrase.
? We explore two composition methods, addi-
tion and dilation, and find that while addition
performs well on corpus-only tasks, dilation
performs best on the brain activity task.
? We build our VSMs, for both phrases and
words, from a large syntactically parsed text
corpus of 16 billion tokens. We also make
the resulting VSM publicly available.
2 Related Work
Mitchell and Lapata (2010) explored several
methods of combining adjective and noun vec-
tors to estimate phrase vectors, and compared
the similarity judgements of humans to the sim-
ilarity of their predicted phrase vectors. They
found that for adjective-noun phrases, type-based
models outperformed Latent Dirichlet Allocation
(LDA) topic models. For the type-based mod-
els, multiplication performed the best, followed
by weighted addition and a dilation model (for de-
tails on composition functions see Section 4.2).
However, Mitchell and Lapata did not combine
the topic- and type-based models, an idea we ex-
plore in detail in this paper.
Baroni and Zamparelli (2010) extended the typ-
ical vector representation of words. Their model
used matrices to represent adjectives, while nouns
were represented with column vectors. The vec-
tors for nouns and adjective-noun phrases were
derived from local word co-occurrence statistics.
The matrix to represent the adjective was esti-
mated with partial least squares regression where
the product of the learned adjective matrix and
the observed noun vector should equal the ob-
served adjective-noun vector. Socher et al (2012)
also extended word representations beyond sim-
ple vectors. Their model assigns each word a vec-
tor and a matrix, which are composed via an non-
linear function (e.g. tanh) to create phrase rep-
resentations consisting of another vector/matrix
pair. This process can proceed recursively, follow-
ing a parse tree to produce a composite sentence
meaning. Other general semantic composition
frameworks have been suggested, e.g. (Sadrzadeh
and Grefenstette, 2011) who focus on the opera-
tional nature of composition, rather than the rep-
resentations that are supplied to the framework.
Here we focus on creating word representations
that are useful for semantic composition.
Turney (2012) published an exploration of the
impact of domain- and function-specific vector
space models, analogous to the topic and type
meanings encoded by our Document and Depen-
dency models respectively. In Turney?s work,
domain-specific information was represented by
noun token co-occurrence statistics within a lo-
cal window, and functional roles were repre-
sented by generalized token/part-of-speech co-
occurrence patterns with verbs - both of which
are relatively local and shallow when compared
with this work. Similar local context-based fea-
tures were used to cluster phrases in (Lin and Wu,
2009). Though the models discussed here are
not entirely comparable to it, a recent comparison
suggested that broader, deeper features such as
ours may result in representations that are superior
for tasks involving neural activation data (Murphy
et al, 2012b).
85
In contrast to the composite model in (Griffiths
et al, 2005), in this paper we explore the com-
plementarity of semantics captured by topical in-
formation and syntactic/semantic types. We fo-
cus on learning VSMs (involving both words and
phrases) for semantic composition, and use more
expressive dependency-based features in our type-
based VSM. A comparison of vector-space repre-
sentations was recently published (Blacoe and La-
pata, 2012), in which the authors compared sev-
eral methods of combining single words vectors
to create phrase vectors. They found that the best
performance for adjective-noun composition used
point-wise multiplication and a model based on
type-based word co-occurrence patterns.
3 Creating a Vector-Space
To create the Dependency vectors, a 16 billion
word subset of ClueWeb09 (Callan and Hoy,
2009) was dependency parsed using the Malt
parser (Hall et al, 2007). Dependency statistics
were then collected for a predetermined list of
target words and adjective-noun phrases, and for
arbitrary adjective-noun phrases observed in the
corpus. The list was composed of the 40 thou-
sand most frequent single tokens in the Ameri-
can National Corpus (Ide and Suderman, 2006),
and a small number of words and phrases used
as stimuli in our brain imaging experiments. Ad-
ditionally, we included any phrase found in the
corpus whose maximal token span matched the
PoS pattern J+N+, where J and N denote adjec-
tive and noun PoS tags respectively. For each
unit (i.e., word or phrase) in this augmented list,
counts of all unit-external dependencies incident
on the head word were aggregated across the cor-
pus, while unit-internal dependencies were ig-
nored. Each token was appended with its PoS tag,
and the dependency edge label was also included.
This resulted in the extraction of 498 million de-
pendency tuples. For example, the dependency tu-
ple (a/DT, NMOD, 27-inch/JJ television/NN,14),
indicates that a/DT was found as a child of 27-
inch/JJ television/NN with a frequency of 14 in
the corpus.
To create Document vectors, word-document
co-occurrence counts were taken from the same
subset of Clueweb, which covered 50 million doc-
uments. We applied feature-selection for compu-
tational efficiency reasons, ranking documents by
the number of target word/phrase types they con-
tained and choosing the top 10 million.
A series of three additional filtering steps
selected target words/phrases, and Docu-
ment/Dependency features for which there was
adequate data.1 First, a co-occurrence frequency
cut-off was used to reduce the dimensionality
of the matrices, and to discard noisy estimates.
A cutoff of 20 was applied to the dependency
counts, and of 2 to document counts. Positive
pointwise-mutual-information (PPMI) was used
as an association measure to normalize the
observed co-occurrence frequency for the varying
frequency of the target word and its features,
and to discard negative associations. Second, the
target list was filtered to the 57 thousand words
and phrases which had at least 20 non-?stop
word? Dependency co-occurrence types, where
a ?stop word? was one of the 100 most frequent
Dependency features observed (so named be-
cause the dependencies were largely incident on
function words). Third, features observed for
no more than one target were removed, as were
empty target entries. The result was a Document
co-occurrence matrix of 55 thousand targets by
5.2 million features (total 172 million non-zero
entries), and a Dependency matrix of 57 thousand
targets by 1.25 million features (total 35 million
non-zero entries).
A singular value decomposition (SVD) matrix
factorization was computed separately on the De-
pendency and Document statistics matrices, with
1000 latent dimensions retained. For this step
we used Python/Scipy implementation of the Im-
plicitly Restarted Arnoldi method (Lehoucq et al,
1998; Jones et al, 2001). This method is com-
patible with PPMI normalization, since a zero
value represents both negative target-feature asso-
ciations, and those that were not observed or fell
below the frequency cut-off. To combine Docu-
ment and Dependency information, we concate-
nate vectors.
4 Experiments
To evaluate how Document and Dependency di-
mensions can interact and compliment each other,
1In earlier experiments with more than 500 thousand
phrasal entries, we found that the majority of targets were
dominated by non-distinctive stop word co-occurrences, re-
sulting in semantically vacuous representations.
86
Table 1: The nearest neighbors of three queries under three VSMs: all 2000 dimensions (Deps & Docs);
1000 Document dimensions (Docs); 1000 Dependency dimensions (Deps).
Query Deps & Docs Docs Deps
beautiful/JJ wonderful/JJ wonderful/JJ lovely/JJ
lovely/JJ fantastic/JJ gorgeous/JJ
excellent/JJ unspoiled/JJ wonderful/JJ
dog/NN cat/NN dogs/NNS cat/NN
dogs/NNS vet/NN the/DT dog/NN
pet/NN leash/NN dogs/NNS
bad/JJ publicity/NN negative/JJ publicity/NN fast/JJ cash/NN loan/NN negative/JJ publicity/NN
bad/JJ press/NN small/JJ business/NN loan/NN bad/JJ press/NN
unpleasantness/NN important/JJ cities/NNS unpleasantness/NN
Concrete Cats Mixed Cats Concrete Sim Mixed Sim Mixed Related0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1Performance of Documents and Dependency Dimensions for Single Word Tasks
Task
Per
form
anc
e
 
 Docs onlyDeps onlyDocs & Deps
Figure 1: Performance of VSMs for single word
behavioral tasks as we vary Document and Depen-
dency inclusion.
we can perform a qualitative comparison between
the nearest neighbors (NNs) of words and phrases
in the three VSMs ? Dependency, Document, and
Combined (Dependency & Document). Results
appear in Table 1. Note that single words and
phrases can be neighbors of each other, demon-
strating that our VSMs can generalize across syn-
tactic types. In the Document VSM, we get more
topically related words as NNs (e.g., vet and leash
for dog); and in the Dependency VSM, we see
words that might substitute for one another in a
sentence (e.g., gorgeous for beautiful). The two
feature sets can work together to up-weight the
most suitable NNs (as in beautiful), or help to
drown out noise (as in the NNs for bad publicity
in the Document VSM).
4.1 Judgements of Word Similarity
As an initial test of the informativeness of Doc-
ument and Dependency features, we evaluate
the representation of single words. Behavioral
judgement benchmarks have been widely used to
evaluate vector space representations (Lund and
Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Here we used five such tests. Two tests are catego-
rization tests, where we evaluate how well an au-
tomatic clustering of our word vectors correspond
to pre-defined word categories. The first ?Con-
crete Categories? test-set consists of 82 nouns,
each assigned to one of 10 concrete classes (Battig
and Montague, 1969). The second ?Mixed Cat-
egories? test-set contains 402 nouns in a range
of 21 concrete and abstract classes from Word-
Net (Almuhareb and Poesio, 2004; Miller et al,
1990). Both categorization tests were performed
with the Cluto clustering package (Karypis, 2003)
using cosine distances. Success was measured as
percentage purity over clusters based on their plu-
rality class, with chance performance at 10% and
5% respectively for the ?Concrete Categories? and
?Mixed Categories? tests.
The remaining three tests use group judgements
of similarity: the ?Concrete Similarity? set of
65 concrete word pairs (Rubenstein and Goode-
nough, 1965); and two variations on the Word-
Sim353 test-set (Finkelstein et al, 2002), par-
titioned into subsets corresponding to strict at-
tributional similarity (?Mixed Similarity?, 203
noun pairs), and broader topical ?relatedness?
(?Mixed Relatedness?, 252 noun pairs) (Agirre et
al., 2009). Performance on these benchmarks is
Spearman correlation between the aggregate hu-
man judgements and pairwise cosine distances of
word vectors in a VSM.
The results in Figure 1 show that the Depen-
dency VSM substantially outperforms the Docu-
ment VSM when predicting human judgements of
strict attributional (categorial) similarity (?Simi-
larity? as opposed to ?Relatedness?) for concrete
nouns. Conversely the Document VSM is compet-
87
Figure 2: The performance of three phrase representations for predicting the behavioral phrasal similar-
ity scores from Mitchell and Lapata (2010). The highest correlation is 0.5033 and uses 25 Document
dimensions, 600 Dependency dimensions and the addition composition function.
itive for less concrete word types, and for judge-
ments of broader topical relatedness.
4.2 Judgements of Phrase Similarity
We also evaluated our system on behavioral data
of phrase similarity judgements gathered from 18
human informants. The adjective-noun phrase
pairs are divided into 3 groups: high, medium
and low similarity (Mitchell and Lapata, 2010).
For each pair of phrases, informants rated phrase
similarity on a Likert scale of 1-7. There are 36
phrase pairs in each of the three groups for a to-
tal of 108 phrase pairs. Not all of the phrases oc-
curred frequently enough in our corpus to pass our
thresholds, and so were omitted from our analy-
sis. In several cases we also used pluralizations
of the test phrases (e.g.?dark eyes?) where the
singular form was not found in our VSM. After
these changes we were left with 28, 24 and 28
in the high, medium and low groups respectively.
In total we have 80 observed vectors for the 108
phrase pairs. These adjective-noun phrases were
included in the list of targets, so their statistics
were gathered in the same way as for single words.
This does not impact results for composed vectors,
as all of the single words in the phrases do appear
in our VSMs. A full list of the phrase pairs can be
found in Mitchell and Lapata (2010).
To evaluate, we used three different representa-
tions of phrases. For phrase pairs that passed our
thresholds, we can test the similarity of observed
representations by comparing the VSM represen-
tation of the phrase (no composition function).
For all 108 phrase pairs we can test the composed
phrase representations, derived by applying addi-
tion and dilation operations to word vectors. Mul-
tiplication is not used as SVD representations in-
clude negative values, and so the product of two
negative values would be positive.
Addition is the element-wise sum of two se-
mantic feature vectors saddi = sadji +snouni , where
snouni , sadji , and saddi are the ith element of the
noun, adjective, and predicted phrase vectors, re-
spectively. Dilation of two semantic feature vec-
tors sadj and snoun is calculated by first decom-
posing the noun into a component parallel to the
adjective (x) and a component perpendicular to
the adjective (y) so that snoun = x + y. Dilation
then enhances the adjective component by multi-
plying it by a scalar (?): sdilate = ?x+y. This can
be viewed as taking the representation of the noun,
and up-weighting the elements it shares with the
adjective, which is coherent with the notion of co-
composition (Pustejovsky, 1995). Previous work
(Mitchell and Lapata, 2010) tuned the ? parame-
ter (? = 16.7). We use that value here, though
further optimization might increase performance.
For our evaluation we calculated the cosine dis-
tance between pairs of phrases in the three dif-
ferent representation spaces: observed, addition
and dilation. Results for a range of dimension-
ality settings appear in Figure 2. In the observed
space, we maximized performance when we in-
88
cluded all 1000 of the Document and 350 Depen-
dency dimensions. For consistency the y axis in
Figure 2 extends only to 100 Document dimen-
sions: changes beyond 100 dimensions for ob-
served vectors were minimal. By design, SVD
will tend to use lower dimensions to represent the
strongest signals in the input statistics, which typ-
ically originate in the types of targets that are most
frequent ? in this case single words. We have ob-
served that less frequent and noisier counts, as
might be found for many phrases, are displaced
to the higher dimensions. Consistent with this ob-
servation, maximum performance occurs using a
high number of dimensions (correlation of 0.37 to
human judgements of phrase similarity).
Interestingly, using the single word vectors to
predict the phrase vectors via the addition function
gives the best correlation of any of the represen-
tations, outperforming even the observed phrase
representations. When using 25 Document di-
mensions and 600 Dependency dimensions the
correlation is 0.52, compared to the best per-
formance of 0.51 using Dependency dimensions
only. We speculate that the advantage of com-
posed vectors over observed vectors is due to
sparseness and resulting noise/variance in the ob-
served phrase vectors, as phrases are necessarily
less frequent than their constituent words.
The dilation composition function performs
slightly worse than addition, but shows best per-
formance at the same point as addition. Here, the
highest correlation (0.46) is substantially lower
than that attained by addition, and uses 25 dimen-
sions of the Document, and 600 dimensions of the
Dependency VSM.
To summarize, without documents, {observed,
addition and dilation} phrase vectors have maxi-
mal correlations {0.37, 0.51 and 0.46}. With doc-
uments, {observed, addition and dilation} phrase
vectors have maximal correlations {0.37, 0.52 and
0.50}. Our results using the addition function
(0.52) outperform the results in two previous stud-
ies (Mitchell and Lapata, 2010; Blacoe and Lap-
ata, 2012): (0.46 and 0.48 respectively). This is
evidence that a VSM built from a larger corpus,
and with both Document and Dependency infor-
mation can yield superior results.
4.3 Composed vs Observed Phrase Vectors
Next we tested how well our representations and
semantic composition functions could predict the
observed vector statistics for phrases from the
vectors of their component words. Again, we
explored addition and dilation composition func-
tions. For testing we have 13, 575 vectors for
which both the adjective and noun passed our
thresholds. We predicted a composed phrase vec-
tor using the statistics of the single words and
one of the two composition functions (addition
or dilation). We then sorted the list of observed
phrase vectors by their distance to the composed
phrase vector and recorded the position of the
corresponding observed vector in the list. From
this we calculated percentile rank, the percent of
phrases that are further from the predicted vec-
tor than the observed vector. Percentile rank is:
100 ? (1 ? ?rank/N) where ?rank is the aver-
age position of the correct observed vector in the
sorted list and N = 13, 575 is the size of the list.
Figure 3 shows the changes in percentile rank
in response to varying dimensions of Documents
and Dependencies for the addition function. Di-
lation results are not shown, but the pattern of
performance is very similar. In general, when
one includes more Document dimensions, the per-
centile rank increases. For both the dilation and
addition composition functions the peak perfor-
mance is with 750 Dependency dimensions and
1000 Document dimensions. Dilation?s peak per-
formance is 97.87; addition peaks at 98.03 per-
centile rank. As in Section 4.2, we see that the
accurate representation of phrases requires higher
SVD dimensions.
To evaluate when composition fails, we ex-
amined the cases where the percentile rank was
< 25%. Amongst these words we found an over-
representation of operational adjectives like ?bet-
ter? and ?more?. As observed previously, it is
possible that such adjectives could be better rep-
resented with a matrix or function (Socher et al,
2012; Baroni and Zamparelli, 2010). Composi-
tion may also be failing when the adjective-noun
phrase is non-compositional (e.g. lazy susan); fil-
tering such phrases could improve performance.
4.4 Brain Activity Data
Here we explore the relationship between the neu-
ral activity observed when a person reads a phrase,
89
100 250 500 750 100093
93.5
94
94.5
95
95.5
96
96.5
97
97.5
98
Number of Dependency Dimensions
Per
cen
tile 
Ran
k
Percentile Rank for Varing Doc. and Dep. Dimensions (Addition)
 
 
0 Doc Dims25501005007501000
Figure 3: The percentile rank of observed phrase
vectors compared to vectors created using the ad-
dition composition function.
and our predicted composed VSM for that phrase.
We collected brain activity data using Magnetoen-
cephalography (MEG). MEG is a brain imaging
method with much higher temporal resolution (1
ms) than fMRI (?2 sec). Since words are natu-
rally read at a rate of about 2 per second, MEG is a
better candidate for capturing the fast dynamics of
semantic composition in the brain. Some previous
work has explored adjective-noun composition in
the brain (Chang et al, 2009), but used fMRI and
corpus statistics based only on co-occurrence with
5 hand-selected verbs.
Our MEG data was collected while 9 partici-
pants viewed 38 phrases, each repeated 20 times
(randomly interleaved). The stimulus nouns were
chosen because previous research had shown them
to be decodable from MEG recordings, and the ad-
jectives were selected to modulate their most de-
codable semantic properties (e.g. edibility, ma-
nipulability) (Sudre et al, 2012). The 8 adjec-
tives selected are (?big?, ?small?, ?ferocious?,
?gentle?, ?light?, ?heavy?, ?rotten?, ?tasty?), and
the 6 nouns are (?dog?, ?bear?, ?tomato?, ?car-
rot?, ?hammer?, ?shovel?). The words ?big? and
?small? are paired with every noun, ?ferocious?
and ?gentle? with animals, ?light? and ?heavy?
with tools and ?rotten? and ?tasty? with foods.
We also included the words ?the? and the word
?thing? as semantically neutral fillers, to present
each of the words in a condition without seman-
tic modulation. In total there are 38 phrases (e.g.
?rotten carrot?, ?big hammer?).
In the MEG experiment, the adjective and
paired noun were each shown for 500ms, with a
300ms interval between them, and there were 3
Figure 4: Results for predicting composed phrase
vectors (addition [4a] and dilation [4b]) from
MEG recordings. Results shown are the aver-
age over 9 subjects viewing 38 adjective-noun
phrases. This is the one task on which dilation
outperforms addition.
(a) Addition composition function results.
(b) Dilation composition function results.
seconds in total time between the onset of subse-
quent phrases. Data was preprocessed to maxi-
mize the signal/noise ratio as is common practice
? see Gross et al, (2012). The 20 repeated trials
for each phrase were averaged together to create
one average brain image per phrase.
To determine if the recorded MEG data can be
used to predict our composed vector space rep-
resentations, we devised the following classifica-
tion framework.2 The training data is comprised
of the averaged MEG signal for each of the 38
phrases for one subject, and the labels are the 38
phrases. We use our VSMs and composition func-
tions to form a mapping of the 38 phrases to com-
2Predicting brain activity from VSM representations is
also possible, but provides additional challenges, as parts of
the observed brain activity are not driven by semantics.
90
posed semantic feature vectors w ? {s1 . . . sm}.
The mapping allows us to use Zero Shot Learn-
ing (Palatucci et al, 2009) to predict novel phrases
(not seen during training) from a MEG record-
ing. This is a particularly attractive characteris-
tic for the task of predicting words, as there are
many words and many more phrases in the En-
glish language, and one cannot hope to collect
MEG recordings for all of them.
Formally, let us define the semantic represen-
tation of a phrase w as semantic feature vector
~sw = {s1...sm}, where the semantic space has
dimensionm that varies depending on the number
of Document and/or Dependency dimensions we
include. We utilize the mapping w ? {s1 . . . sm}
to train m independent functions f1(X) ?
s?1, . . . , fm(X) ? s?m where s? represents the
value of a predicted composed semantic feature.
We combine the output of f1 . . . fm to create the
final predicted semantic vector ~s? = {s?1 . . . s?m}.
We use cosine distance to quantify the distance be-
tween true and predicted semantic vectors.
To measure performance we use the 2 vs. 2 test.
For each test we withhold two phrases and train
regressors on the remaining 36. We use the re-
gressors f and MEG data from the two held out
phrases to create two predicted semantic vectors.
We then choose the assignment of predicted se-
mantic vectors (~s?i and ~s?j) to true semantic vec-
tors (~si and ~sj) that minimizes the sum of cosine
distances. If we choose the correct assignment
(~s?i 7? ~si and ~s?j 7? ~sj) we mark the test as cor-
rect. 2 vs. 2 accuracy is the number of 2 vs. 2
tests with correct assignments divided by the total
number of tests. There are (38 choose 2) = 703
distinct 2 vs. 2 tests, and we evaluate on the subset
for which neither the adjective nor noun are shared
(540 pairs). Chance performance is 0.50.
For each f we trained a regressor with L2
penalty. We tune the regularization parame-
ter with leave-one-out-cross-validation on training
data. We train regressors using the first 800 ms of
MEG signal after the noun stimulus appears, when
we assume semantic composition is taking place.
Results appear in Figure 4. The best perfor-
mance (2 vs. 2 accuracy of 0.9440) is achieved
with dilation, 800 dimensions of Dependencies
and zero Document dimensions. When we use
the addition composition function, optimal per-
formance is 0.9212, at 600 Dependency and zero
Document dimensions. Note, however, that the
parameter search here was much coarser that in
Sections 4.2 and 4.3, due to the computation re-
quired. We used a finer grid around the peaks in
performance for addition and dilation and found
minimal improvement (?0.5%) with the addition
of a small number of Document dimensions.
It is intriguing that this neurosemantic task is
the only task for which dilation outperforms addi-
tion. All other composition tasks explored in this
study were concerned with matching composed
word vectors to observed or composed word vec-
tors, whereas here we are interested in matching
composed word vectors to observed brain activity.
Perhaps the brain works in a manner more akin to
the emphasis of elements as modeled by dilation,
rather than a summing of features. Further work
is required to fully understand this phenomenon,
but this is surely a thought-provoking result.3
5 Conclusion
We have performed a systematic study of comple-
mentarity of topical (Document) and type (Depen-
dency) features in Vector Space Model (VSM) for
semantic composition of adjective-noun phrases.
To the best of our knowledge, this is one of the
first such studies of this kind. Through experi-
ments on multiple real world benchmark datasets,
we demonstrated the benefit of combining topic-
and type-based features in a VSM. Additionally,
we introduced a novel task of predicting vec-
tor representations of composed phrases from the
brain activity of human subjects reading those
phrases. We exploited a large syntactically parsed
corpus to build our VSM models, and make them
publicly available. We hope that the findings and
resources from this paper will serve to inform fu-
ture work on VSMs and semantic composition.
Acknowledgment
We are thankful to the anonymous reviewers for their con-
structive comments. We thank CMUs Parallel Data Labo-
ratory (PDL) for making the OpenCloud cluster available,
Justin Betteridge (CMU) for his help with parsing the corpus,
and Yahoo! for providing the M45 cluster. This research has
been supported in part by DARPA (under contract number
FA8750-13-2-0005), NIH (NICHD award 1R01HD075328-
01), Keck Foundation (DT123107), NSF (IIS0835797), and
Google. Any opinions, findings, conclusions and recommen-
dations expressed in this paper are the authors and do not
necessarily reflect those of the sponsors.
3No pun intended.
91
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, and Marius Pas. 2009. A study on similarity and
relatedness using distributional and WordNet-based ap-
proaches. Proceedings of NAACL-HLT 2009.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP, pages 158?165.
Marco Baroni and Alessandro Lenci. 2010. Distributional
memory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1183?1193. Association
for Computational Linguistics.
W F Battig and W E Montague. 1969. Category Norms
for Verbal Items in 56 Categories: A Replication and Ex-
tension of the Connecticut Category Norms. Journal of
Experimental Psychology Monographs, 80(3):1?46.
William Blacoe and Mirella Lapata. 2012. A Comparison of
Vector-based Representations for Semantic Composition.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 546?556, Jeju
Island, Korea.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine Learning
Research, 3(4-5):993?1022.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09 Dataset.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Kai-min Chang, Vladimir L. Cherkassky, Tom M Mitchell,
and Marcel Adam Just. 2009. Quantitative modeling of
the neural representation of adjective-noun phrases to ac-
count for fMRI activation. In Proceedings of the Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 638?646.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: the concept revisited.
ACM Transactions on Information Systems, 20(1):116?
131.
Thomas L Griffiths, Mark Steyvers, David M Blei, and
Joshua B Tenenbaum. 2005. Integrating topics and syn-
tax. Advances in neural information processing systems,
17.
Joachim Gross, Sylvain Baillet, Gareth R. Barnes,
Richard N. Henson, Arjan Hillebrand, Ole Jensen, Karim
Jerbi, Vladimir Litvak, Burkhard Maess, Robert Oost-
enveld, Lauri Parkkonen, Jason R. Taylor, Virginie van
Wassenhove, Michael Wibral, and Jan-Mathijs Schoffe-
len. 2012. Good-practice for conducting and reporting
MEG research. NeuroImage, October.
J Hall, J Nilsson, J Nivre, G Eryigit, B Megyesi, M Nilsson,
and M Saers. 2007. Single Malt or Blended? A Study
in Multilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLPCoNLL 2007,
volume s. 19-33, pages 933?939. Association for Compu-
tational Linguistics.
Nancy Ide and Keith Suderman. 2006. The American Na-
tional Corpus First Release. Proceedings of the Fifth Lan-
guage Resources and Evaluation Conference (LREC).
Eric Jones, Travis Oliphant, Pearu Peterson, and others.
2001. SciPy: Open source scientific tools for Python.
George Karypis. 2003. CLUTO: A Clustering Toolkit.
Technical Report 02-017, Department of Computer Sci-
ence, University of Minnesota.
T Landauer and S Dumais. 1997. A solution to Plato?s prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological
Review, 104(2):211?240.
R B Lehoucq, D C Sorensen, and C Yang. 1998. Arpack
users? guide: Solution of large scale eigenvalue problems
with implicitly restarted Arnoldi methods. SIAM.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for
discriminative learning. In Proceedings of the ACL.
Dekang Lin. 1998. Automatic Retrieval and Clustering of
Similar Words. In COLING-ACL, pages 768?774.
K Lund and C Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and Computers, 28:203?
208.
George A Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduction
to WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive science,
34(8):1388?429, November.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012a.
Comparing Abstract and Concrete Conceptual Represen-
tations using Neurosemantic Decoding. In NAACL Work-
shop on Cognitive Modelling and Computational Linguis-
tics.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012b.
Selecting Corpus-Semantic Models for Neurolinguistic
Decoding. In First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pages 114?123, Montreal,
Quebec, Canada.
Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell.
2012c. Learning Effective and Interpretable Semantic
Models using Non-Negative Sparse Embedding. In Inter-
national Conference on Computational Linguistics (COL-
ING 2012), Mumbai, India.
S Pado? and M Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161?199.
92
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau, and
Tom M Mitchell. 2009. Zero-Shot Learning with Se-
mantic Output Codes. Advances in Neural Information
Processing Systems, 22:1410?1418.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
Reinhard Rapp. 2003. Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the Ninth
Machine Translation Summit, pp:315?322.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633, October.
Mehrnoosh Sadrzadeh and Edward Grefenstette. 2011. A
Compositional Distributional Semantics Two Concrete
Constructions and some Experimental Evaluations. Lec-
ture Notes in Computer Science, 7052:35?47.
Magnus Sahlgren. 2006. The Word-Space Model: Using dis-
tributional analysis to represent syntagmatic and paradig-
matic relations between words in high-dimensional vector
spaces. Dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositionality
through Recursive Matrix-Vector Spaces. In Conference
on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila We-
hbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell.
2012. Tracking Neural Coding of Perceptual and Seman-
tic Features of Concrete Nouns. NeuroImage, 62(1):463?
451, May.
Peter D Turney. 2012. Domain and Function : A Dual-Space
Model of Semantic Relations and Compositions. Journal
of Artificial Intelligence Research, 44:533?585.
93
