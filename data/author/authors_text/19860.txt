Coling 2010: Poster Volume, pages 409?417,
Beijing, August 2010
Recognizing Relation Expression between Named Entities based on
Inherent and Context-dependent Features of Relational words
Toru Hirano?, Hisako Asano?, Yoshihiro Matsuo?, Genichiro Kikui?
?NTT Cyber Space Laboratories, NTT Corporation
?Innovative IP Architecture Center, NTT Communications Corporation
hirano.tohru@lab.ntt.co.jp
hisako.asano@ntt.com
{matsuo.yoshihiro,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a supervised learn-
ing method to recognize expressions that
show a relation between two named en-
tities, e.g., person, location, or organiza-
tion. The method uses two novel fea-
tures, 1) whether the candidate words in-
herently express relations and 2) how the
candidate words are influenced by the past
relations of two entities. These features
together with conventional syntactic and
contextual features are organized as a tree
structure and are fed into a boosting-based
classification algorithm. Experimental re-
sults show that the proposed method out-
performs conventional methods.
1 Introduction
Much attention has recently been devoted to us-
ing enormous amount of web text covering an ex-
ceedingly wide range of domains as a huge knowl-
edge resource with computers. To use web texts as
knowledge resources, we need to extract informa-
tion from texts that are merely sequences of words
and convert them into a structured form. Although
extracting information from texts as a structured
form is difficult, relation extraction is a way that
makes it possible to use web texts as knowledge
resources.
The aim of relation extraction is to extract se-
mantically related named entity pairs, X and Y ,
and their relation, R, from a text as a struc-
tured form [X , Y , R]. For example, the triple
[Yukio Hatoyama, Japan, prime minister] would
be extracted from the text ?Yukio Hatoyama is the
prime minister of Japan?. This extracted triple
provides important information used in informa-
tion retrieval (Zhu et al, 2009) and building an
ontology (Wong et al, 2010).
It is possible to say that all named entity pairs
that co-occur within a text are semantically related
in some way. However, we define that named en-
tity pairs are semantically related if they satisfy
either of the following rules:
? One entity is an attribute value of the other.
? Both entities are arguments of a predicate.
Following the above definition, explicit and im-
plicit relations should be extracted. An explicit re-
lation means that there is an expression that shows
the relation between a named entity pair in a given
text, while an implicit relation means that there is
no such expression. For example, the triple [Yukio
Hatoyama, Kunio Hatoyama, brother] extracted
from the text ?Yukio Hatoyama, the Democratic
Party, is Kunio Hatoyama?s brother? is an explicit
relation. In contrast, the triple [Yukio Hatoyama,
the Democratic Party, member] extracted from the
same text is an implicit relation because there is
no expression showing the relation (e.g. member)
between ?Yukio Hatoyama? and ?the Democratic
Party? in the text.
Extracting triples [X , Y , R] from a text in-
volves two tasks. One is detecting semantically
related pairs from named entity pairs that co-occur
in a text and the other is determining the rela-
tion between a detected pair. For the former task,
various supervised learning methods (Culotta and
Sorensen, 2004; Zelenko et al, 2003; Hirano et
al., 2007) and bootstrapping methods (Brin, 1998;
Pantel and Pennacchiotti, 2006) have been ex-
plored to date. In contrast, for the latter task,
409
only a few methods have been proposed so far
(Hasegawa et al, 2004; Banko and Etzioni, 2008;
Zhu et al, 2009). We therefore addressed the
problem of how to determine relations between a
given pair.
We used a three-step approach to address this
problem. The first step is to recognize an expres-
sion that shows explicit relations between a given
named entity pair in a text. If no such expression
is recognized, the second step is to estimate the
relationship that exists between a given named en-
tity pair that has an implicit relation. The last step
is to identify synonyms of the relations that are
recognized or estimated in the above steps. In this
paper, we focus on the first step. The task is se-
lecting a phrase from the text that contains a re-
lation expression linking a given entity pair and
outputting the expression as one showing the rela-
tionship between the pair.
In our preliminary experiment, it was found
that using only structural features of a text, such
as syntactic or contextual features, is not good
enough for a number of examples. For instance,
the two Japanese sentences shown in Figure 1
have the same syntactic structure but (a) contains a
relation expression and (b) does not. We therefore
assume there are clues for recognizing relation
expressions other than conventional syntactic and
contextual information. In this paper, we propose
a supervised learning method that includes two
novel features of relational words as well as con-
ventional syntactic and contextual features. The
novel features of our method are:
Inherent Feature: Some words are able to ex-
press the relations between named entities
and some are not. Thus, it would be useful to
know the words that inherently express these
relations.
Context-dependent Feature: There are a num-
ber of typical relationships that change as
time passes, such as ?dating? ? ?engage-
ment? ? ?marriage? between persons. Fur-
thermore, present relations are influenced by
the past relations of a given named entity
pair. Thus, it would be useful to know the
past relations between a given pair and how
the relations change as time passes.
In the rest of this paper, Section 2 references re-
lated work, Section 3 outlines our method?s main
features and related topics, Section 4 describes our
experiments and experimental results, and Section
5 briefly summarizes key points and future work
to be done.
2 Related Work
The ?Message Understanding Conference? and
?Automatic Content Extraction? programs have
promoted relational extraction. The task was stud-
ied so as to extract predefined semantic relations
of entity pairs in a text. Examples include the
supervised learning method cited in (Kambhatla,
2004; Culotta and Sorensen, 2004; Zelenko et al,
2003) and the bootstrapping method cited in (Pan-
tel and Pennacchiotti, 2006; Agichtein and Gra-
vano, 2000). Recently, open information extrac-
tion (Open IE), a novel domain-independent ex-
traction paradigm, has been suggested (Banko and
Etzioni, 2008; Hasegawa et al, 2004). The task is
to detect semantically related named entity pairs
and to recognize the relation between them with-
out using predefined relations.
Our work is a kind of open IE, but our approach
differs from that of previous studies. Banko
(2008) proposed a supervised learning method us-
ing conditional random fields to recognize the re-
lation expressions from words located between a
given pair. Hasegawa (2004) also proposed a rule-
based method that selects all words located be-
tween a given pair as a relation expression if a
given named entities appear within ten words. The
point of these work is that they selected relation
expressions only from the words located between
Osaka Fucho
01
-nosaka ucho
01
-noKacho02-noacho02-no
Yumei
04
-desu.u ei
04
-desu.Suzuki
03
-san-wauzuki
03
-san- a
D
DD
Osaka Fucho
05
-nosaka ucho
05
-noSoumukyoku06-noou ukyoku06-no
Yumei
08
-desu.u ei
08
-desu.Suzuki
07
-san-wauzuki
07
-san- a
D
DD
(a)Mr.Suzuki
03
, a manager
02
of Osaka Prefectural Government
01
, is famous
04
.(b)Mr.Suzuki
07
, administration office
06
in Osaka PrefecturalGovernment
05
, is famous
08
.
(a) (b)
Figure 1: Same syntactic examples
410
given entities in the text, because as far as English
texts are concerned, 86% of the relation expres-
sions of named entity pairs appear between the
pair (Banko and Etzioni, 2008). However, our tar-
get is Japanese texts, in which only 26% of entity
pair relation expressions appear between the pair.
Thus, it is hard to incorporate previous approaches
into a Japanese text.
To solve the problem, our task was to select a
phrase from the entire text that would include a
relation expression for connecting a given pair.
3 Recognizing Relation Expressions
between Named Entities
To recognize the relation expression for a given
pair, we need to select a phrase that includes an
expression that shows the relation between a given
entity pair from among all noun and verb phrases
in a text. Actually, there are two types of candi-
date phrases in this case. One is from a sentence
that contains a given pair (intra-sentential), and
the other is from a sentence that does not (inter-
sentential). For example, the triple [Miyaji21,
Ishii22, taiketsu12] extracted from the following
text is inter-sentential.
(S-1) Chumokoku11-no taiketsu12-ga
mamonaku13 hajimaru14.
(The showcase11 match12 will start14 soon13.)
(S-2) Ano Miyaji21-to Ishii22-toiu
kanemochi23-niyoru yume24-no
kikaku25.
(The dream24 event25 between the rich mens23,
Miyaji21 and Ishii22.)
According to our annotated data shown in Ta-
ble 2, 53% of the semantically-related named en-
tity pairs are intra-sentential and 12% are inter-
sentential. Thus, we first select a phrase from
those in a sentence that contains a given pair, and
if no phrase is selected, select one from the rest of
the sentences in a text.
We propose a supervised learning method that
uses two novel features of relational words as
well as conventional syntactic and contextual fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classifica-
tion algorithm (Kudo and Matsumoto, 2004). The
highest-scoring phrase is then selected if the score
exceeds a given threshold. Finally, the head of the
selected phrase is output as the relation expression
of a given entity pair.
The method consists of four parts: preprocess-
ing (POS tagging and parsing), feature extraction,
classification, and selection. In this section, we
describe the idea behind using our two novel fea-
tures and how they are implemented to recognize
the relation expressions of given pairs. Before
that, we will describe our proposed method?s con-
ventional features.
3.1 Conventional Features
Syntactic feature
To recognize the intra-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative syntactic structure that consists
of given entities and their relation expression. For
example, there is a structure for which the com-
mon parent phrase of the given pair, X = ?Ha-
toyama Yukio32? and Y = ?Hatoyama Kunio33?,
has the relation expression, R = ?ani34? in the
Japanese sentence S-3. Figure 2 shows the depen-
dency tree of sentence S-3.
(S-3) Minshuto31-no Hatoyama Yukio32-wa
Hatoyama Kunio33-no ani34-desu.
(Yukio Hatoyama32, the Democratic Party31,
is Kunio Hatoyama33?s brother34.)
To use a discriminative structure for each can-
didate, we make a minimum tree that consists of
given entities and the candidate where each phrase
is represented by a case marker ?CM?, a depen-
dency type ?DT?, an entity class, and the string
and POS of the candidate (See Figure 3).
Minshuto
31
-noinshuto
31
-no
Hatoyama Yukio
32
-waatoya a ukio
32
- a
Ani
34
-desu.ni
34
-desu.
Hatoyama Kunio
33
-noatoya a unio
33
-noD
D D
Figure 2: Dependency tree of sentence S-3
411
X:person:person
Phrasehrase
PhrasehraseCandidateandidatePhrasehrase
Y:person:person
CM:wa: a DT:D:
STR:Ani
34
: ni
34
POS:Noun: ounCM:?: DT:O:CM:no:no DT:D: Inh:1Inh:1C
rank
:1
rank
:1C
prob
:0.23
prob
:0.23
Figure 3: Intra-sentential feature tree
Contextual Feature
To recognize the inter-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative contextual structure that con-
sists of given entities and their relation expression.
Here, we use a Salient Referent List (SRL) to ob-
tain contextual structure. The SRL is an empirical
sorting rule proposed to identify the antecedent
of (zero) pronouns (Nariyama, 2002), and Hirano
(2007) proposed a way of applying SRL to rela-
tion detection. In this work, we use this way to
apply SRL to recognize inter-sentential relation
expressions.
We applied SRL to each candidate as follows.
First, from among given entities and the candi-
date, we choose the one appearing last in the text
as the root of the tree. We then append noun
phrases, from the chosen one to the beginning of
the text, to the tree depending on case markers,
?wa? (topicalised subject), ?ga? (subject), ?ni?
(indirect object),?wo? (object), and ?others?, with
the following rules. If there are nodes of the same
case marker already in the tree, the noun phrase
is appended as a child of the leaf node of them.
In other cases, the noun phrase is appended as a
child of the root node. For example, we get the
SRL tree shown in Figure 4 with the given entity
pair, X = ?Miyaji21? and Y = ?Ishii22?, and the
candidate, ?taiketsu12?, with the text (S-1, S-2).
To use a discriminative SRL structure, we make
a minimum tree that consists of given entities and
the candidate where each phrase is represented by
an entity class, and the string and POS of the can-
didate (See Figure 5). In this way, there is a prob-
lem when the candidate is a verb phrase, because
ga: Taiketsu
12
ga: aiketsu
12
Ishii
22
Ishii
22
others: Miyaji
21
others: iyaji
21
others: Chumoku
11
others: hu oku
11
Figure 4: Salient referent list tree
only noun phrases are appended to the SRL tree.
If the candidate is a verb phrase, we cannot make
a minimum tree that consists of given entities and
the candidate.
To solve this problem, a candidate verb phrase
is appended to the feature tree using a syntactic
structure. In a dependency tree, almost all verb
phrases have some parent or child noun phrases
that are in the SRL tree. Thus, candidate verb
phrases are appended as offspring of these noun
phrases represented syntactically as ?parent? or
?child?. For example, when given the entity pair,
X = ?Miyaji21? and Y = ?Ishii22?, and the can-
didate, ?hajimaru14? from the text (S-1, S-2), a
feature tree cannot be made because the candi-
date is not in an SRL tree. By extending the way
the syntactic structure is used, ?hajimaru14? has a
child node ?taiketsu12?, which is in an SRL tree,
and this makes it possible to make the feature tree
shown in Figure 6.
3.2 Proposed Features
To recognize intra-sentential or inter-sentential re-
lation expressions for given pairs, we assume
there are clues other than syntactic and contex-
tual information. Thus, we propose inherent and
SRL:gaL:ga Candidateandidate
Y:person:person
X:person:personSRL:othersL:othersSTR:Taiketsu
12
: aiketsu
12
POS:Noun: ounInh:1Inh:1 C
rank
:1
rank
:1 C
prob
:0.23
prob
:0.23
Figure 5: Inter-sentential feature tree
412
SRL:gaL:ga
Dep:Childep: hild Candidateandidate
Y:person:person X:person:personSRL:othersL:othersSTR:Hajimaru
14
: aji aru
14
POS:Verb: erbInh:0Inh:0 C
rank
:2
rank
:2 C
prob
:0.00
prob
:0.00
Figure 6: Extended inter-sentential feature tree
context-dependent features of relational words.
Inherent Feature of Relational words
Some words are able to express the relations be-
tween named entities and some are not. For exam-
ple, the word ?mother? can express a relation, but
the word ?car? cannot. If there were a list of words
that could express relations between named enti-
ties, it would be useful to recognize the relation
expression of a given pair. As far as we know,
however, no such list exists in Japanese. Thus,
we estimate which words are able to express rela-
tions between entities. Here, we assume that al-
most all verbs are able to express relations, and
accordingly we focus on nouns.
When the relation expression, R, of an entity
pair, X and Y , is a noun, it is possible to say ?Y is
R of X? or ?Y is X?s R?. Here, we can say noun
R takes an argument X . In linguistics, this kind
of noun is called a relational noun. Grammatically
speaking, a relational noun is a simple noun, but
because its meaning describes a ?relation? rather
than a ?thing?, it is used to describe relations just
as prepositions do. To estimate which nouns are
able to express the relations between named enti-
ties, we use the characteristics of relational nouns.
In linguistics, many researchers describe the rela-
tionship between possessives and relational nouns
(Chris, 2008). Thus, we use the knowledge that
in the patterns ?B of A? or ?A?s B?, if word B is
a relational noun, the corresponding word A be-
longs to a certain semantic category. In contrast,
if word B is not a relational noun, the correspond-
ing word A belongs to many semantic categories
(Tanaka et al, 1999). Figure 7 shows scattering
of the semantic categories of ?mother? and ?car?
Semantic categoriesRelative
 Frequency
Semantic categoriesRelative
 Frequency
Figure 7: Scattering of semantic category of
?mother? (left) and ?car? (right).
acquired by the following way.
First, we acquired A and B using the patterns
?A no B?1 from a large Japanese corpus, then
mapped words A into semantic categories C= {
c1, c2, ? ? ? , cm } using a Japanese lexicon (Ikehara
et al, 1999). Next, for each word B, we calcu-
lated a scattering score Hc(B) using the semantic
category of corresponding words A. Finally, we
estimated whether a word is a relational noun by
using k-NN estimation with positive and negative
examples. As estimated results, ?Inh:1? shows
that it is a relational noun and ?Inh:0? shows that
it is not. In both cases, the result is appended to
the feature tree as a child of the candidate node
(See Figure 3, 5, or 6).
Hc(B) = ?
?
c?C
P (c|B)logmP (c|B)
P (c|B) = freq(c,B)freq(B)
In our experiments, we acquired 55,412,811
pairs of A and B from 1,698,798 newspaper ar-
ticles and 10,499,468 weblog texts. As training
data, we used the words of relation expressions as
positive examples and other words as negative ex-
amples.
Context-dependent Feature of Relational
words
There are a number of typical relationships that
change as time passes, such as ?dating? ? ?en-
gagement? ? ?marriage? between persons. Fur-
thermore, present relations are affected by the past
relations of a given named entity pair. For in-
stance, if the past relations of a given pair are ?dat-
ing? and ?engagement? and one of the candidates
is ?marriage?, ?marriage? would be selected as the
relation expression of the given pair. Therefore, if
1
?B of A? or ?A?s B? in English.
413
Pair of entity class rm rn PT (rn|rm) Count(rm, rn)
dating 0.050 102
?person,person? dating marriage 0.050 101
engagement 0.040 82
marriage 0.157 786
?person,person? engagement engagement 0.065 325
wedding 0.055 276
president 0.337 17,081
?person,organization? vice president vice president 0.316 16,056
CEO 0.095 4,798
fellow 0.526 61
?person,organization? researcher manager 0.103 12
member 0.078 9
alliance 0.058 8,358
?organization,organization? alliance accommodated 0.027 3,958
acquisition 0.027 3,863
mutual consultation 0.022 2,670
?location,location? neighbour support 0.015 1,792
visit 0.012 1,492
war 0.077 78,170
?location,location? war mutual consultation 0.015 15,337
support 0.010 10,226
Table 1: Examples of calculated relation trigger model between entity classes defined by IREX
we know the past relations of the given pair and
the typical relational change that occurs as time
passes, it would be useful to recognize the rela-
tion expression of a given pair.
In this paper, we represent typical relational
changes that occur as time passes by a simple re-
lation trigger model PT (rn|rm). Note that rm
is a past relation and rn is a relation affected by
rm. This model disregards the span between rn
and rm. To make the trigger model, we automat-
ically extract triples [X , Y , R] from newspaper
articles and weblog texts, which have time stamps
of the document creation. Using these triples with
time stamps for each entity pair, we sort rela-
tions in order of time and count pairs of present
and previous relations. For example, if we ex-
tract ?dating? occurring for an entity pair on Jan-
uary 10, 1998, ?engagement? occurring on Febru-
ary 15, 2001, and ?marriage? occurring on De-
cember 24, 2001, the pairs ?dating, engagement?,
?dating, marriage?, and ?engagement, marriage?
are counted. The counted score is then summed
up by the pair of entity class and the trigger model
is calculated by the following formula.
PT (rn|rm) =
Count(rm, rn)?
rn Count(rm, rn)
For the evaluation, we extracted triples by
named entity recognition (Suzuki et al, 2006), re-
lation detection (Hirano et al, 2007), and the pro-
posed method using the inherent features of rela-
tional words described in Section 3.2. A total of
10,463,232 triples were extracted from 8,320,042
newspaper articles and weblog texts with time
stamps made between January 1, 1991 and June
30, 2006. As examples of the calculated relation
trigger model, Table 1 shows the top three proba-
bility relations rn of several relations rm between
Japanese standard named entity classes defined
in the IREX workshop2. For instance, the rela-
tion ?fellow? has the highest probability of being
changed from the relation ?researcher? between
person and organization as time passes.
2http://nlp.cs.nyu.edu/irex/
414
To obtain the past relations of a given pair in
the input text, we again used the triples with time
stamps extracted as above. The only relations we
use as past relations, Rm = {rm1 , rm2 , ? ? ? , rmk},
are those of a given pair whose time stamps are
older than the input text. Finally, we calcu-
lated probabilities with the following formula us-
ing the past relations Rm and the trigger model
PT (rn|rm).
PT (rn|Rm) = max{PT (rn|rm1),
PT (rn|rm2), ? ? ? , PT (rn|rmk)}
Using this calculated probability, we ranked
candidates and appended the rank ?Crank? and
the probability score ?Cprob? to the feature tree
as a child of the candidate node (See Figure 3,
5, or 6). For example, if the past relations Rm
were ?dating? and ?engagement? and candidates
were ?marriage?, ?meeting?, ?eating?, or ?drink-
ing?, the candidates probabilities were calculated
and ranked as ?marriage? (Cprob:0.15, Crank:1),
?meeting? (Cprob:0.08, Crank:2), etc.
3.3 Classification Algorithms
Several structure-based learning algorithms have
been proposed so far (Collins and Duffy, 2002;
Suzuki et al, 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s
boosting-based algorithm using sub-trees as fea-
tures, which is implemented as a BACT system.
Given a set of training examples each of which
is represented as a tree labeling whether the can-
didate is the relation expression of a given pair or
not, the BACT system learns that a set of rules
is effective in classifying. Then, given a test in-
stance, the BACT system classifies using a set of
learned rules.
4 Experiments
We conducted experiments using texts from
Japanese newspaper articles and weblog texts to
test the proposed method for both intra- and inter-
sentential tasks. In the experiments, we compared
the following methods:
Conventional Features: trained by conventional
syntactic features for intra-sentential tasks as
Relation Types #
Explicit Intra-sentential 9,178Inter-sentential 2,058
Implicit 5,992
Total 17,228
Table 2: Details of the annotated data
described in Section 3.1, and contextual fea-
tures for inter-sentential tasks as described in
Section 3.1.
+Inherent Features: trained by conventional
features plus inherent features of relational
words described in Section 3.2.
++Context-dependent FeaturesTM: trained
by conventional and inherent features plus
context-dependent features of relational
words with the trigger model described in
Section 3.2.
++Context-dependent FeaturesCM: trained
by conventional and inherent features
plus context-dependent features of rela-
tional words with a cache model. We
evaluated this method to compare it with
Context-dependent FeaturesTM to show the
effectiveness of the proposed trigger model.
The cache model is a simple way to use past
relations in which the probability PC(rcand)
calculated by the following formula and the
rank based on the probability is appended to
every candidate feature tree.
PC(rcand) =
|rcand in past relations|
|past relations|
4.1 Settings
We used 6,200 texts from Japanese newspapers
and weblogs dated from January 1, 2004 to June
30, 2006, manually annotating the semantic rela-
tions between named entities for experiment pur-
poses. There were 17,228 semantically-related
entity pairs as shown in Table 2. In an intra-
sentential experiment, 17,228 entity pairs were
given, but only 9,178 of them had relation expres-
sions. In contrast, in an inter-sentential experi-
ment, 8,050 entity pairs excepted intra-sentential
415
Precision Recall F
Conventional Features 63.5? (3,436/5,411) 37.4? (3,436/9,178) 0.471
+Inherent Features 67.2? (4,036/6,001) 43.9? (4,036/9,178) 0.531
++Context-dependent FeaturesTM 70.7? (4,460/6,312) 48.6? (4,460/9,178) 0.576
++Context-dependent FeaturesCM 67.5? (4,042/5,987) 44.0? (4,042/9,178) 0.533
Table 3: Experimental results of intra-sentential
Precision Recall F
Conventional Features 70.1? (579/825) 28.1? (579/2,058) 0.401
+Inherent Features 77.1? (719/932) 34.9? (719/2,058) 0.480
++Context-dependent FeaturesTM 75.2? (794/1,055) 38.5? (794/2,058) 0.510
++Context-dependent FeaturesCM 74.3? (732/985) 35.5? (732/2,058) 0.481
Table 4: Experimental result of inter-sentential
were given, but only 2,058 of them had relation
expressions.
We conducted five-fold cross-validation over
17,228 entity pairs so that sets of pairs from a sin-
gle text were not divided into the training and test
sets. In the experiments, all features were auto-
matically acquired using a Japanese POS tagger
(Fuchi and Takagi, 1998) and dependency parser
(Imamura et al, 2007).
4.2 Results
Tables 3 and 4 show the performance of several
methods for intra-sentential and inter-sentential.
Precision is defined as the percentage of cor-
rect relation expressions out of recognized ones.
Recall is the percentage of correct relation ex-
pressions from among the manually annotated
ones. The F measure is the harmonic mean of
precision and recall.
A comparison with the Conventional Fea-
tures and Inherent Features method for intra-
/inter-sentential tasks indicates that the proposed
method using inherent features of relational words
improved intra-sentential tasks F by 0.06 points
and inter-sentential tasks F by 0.08 points. Us-
ing a statistical test (McNemar Test) demonstrably
showed the proposed method?s effectiveness.
A comparison with the Inherent Features and
Context-dependent FeaturesTM method showed
that the proposed method using context-dependent
features of relational words improved intra-/inter-
sentential task performance by 0.045 and 0.03
points, respectively. McNemar test results also
showed the method?s effectiveness.
To further compare the usage of context-
dependent features, trigger models, and cache
models, we also used Context-dependent
FeaturesCM method for comparison. Tables
3 and 4 show that our proposed trigger model
performed better than the cache model, and
McNemar test results showed that there was a
significant difference between the models. The
reason the trigger model performed better than
the cache model is that the trigger model correctly
recognized the relation expressions that did not
appear in the past relations of a given pair. Thus,
we can conclude that using typical relationships
that change as time passes helps to recognize
relation expressions between named entities.
5 Conclusion
We proposed a supervised learning method that
employs inherent and context-dependent features
of relational words and uses conventional syntac-
tic or contextual features to improve both intra-
and inter-sentential relation expression recogni-
tion. Our experiments demonstrated that the
method improves the F measure and thus helps
to recognize relation expressions between named
entities.
In future work, we plan to estimate implicit re-
lations between named entities and to identify re-
lational synonyms.
416
References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM conference
on Digital libraries, pages 85?94.
Banko, Michele and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting on Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 28?36.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, pages 172?183.
Chris, Barker, 2008. Semantics: An international
handbook of natural language meaning, chap-
ter Possessives and relational nouns. Walter De
Gruyter Inc.
Collins, Michael and Nigel Duffy. 2002. Convolution
kernels for natural language. Advances in Neural
Information Processing Systems, 14:625?632.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 423?429.
Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese
morphological analyzer using word co-occurrence
- jtag. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, volume 1, pages 409?413.
Hasegawa, Takaaki, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 415?422.
Hirano, Toru, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting semantic relations between named
entities in text using contextual features. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 157?160.
Ikehara, Satoru, Masahiro Miyazaki, Satoru Shirai,
Akio Yoko, Hiromi Nakaiwa, Kentaro Ogura, Masa-
fumi Oyama, and Yoshihiko Hayashi. 1999. Ni-
hongo Goi Taikei (in Japanese). Iwanami Shoten.
Imamura, Kenji, Genichiro Kikui, and Norihito Ya-
suda. 2007. Japanese dependency parsing using se-
quential labeling for semi-spoken language. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 225?228.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 178?181.
Kudo, Taku and Yuji Matsumoto. 2004. A boosting
algorithm for classification of semi-structured text.
In Proceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing, pages
301?308.
Nariyama, Shigeko. 2002. Grammar for ellipsis res-
olution in japanese. In Proceedings of the 9th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 135?
145.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 113?120.
Suzuki, Jun, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical directed acyclic
graph kernel: Methods for structured natural lan-
guage data. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics, pages 32?39.
Suzuki, Jun, Erik McDermott, and HIdeki Isozaki.
2006. Training conditional random fields with mul-
tivariate evaluation measures. In Proceedings of the
43th Annual Meeting on Association for Computa-
tional Linguistics.
Tanaka, Shosaku, Yoichi Tomiura, and Toru Hitaka.
1999. Classification of syntactic categories of
nouns by the scattering of semantic categories (in
japanese). Transactions of Information Processing
Society of Japan, 40(9):3387?3396.
Wong, Wilson, Wei Liu, and Mohammed Bennamoun.
2010. Acquiring semantic relations using the web
for constructing lightweight ontologies. In Proceed-
ings of the 13th Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
Zhu, Jun, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and
Ji-Rong Wen. 2009. Statsnowball: a statistical ap-
proach to extracting entity relationships. In Pro-
ceedings of the 18th international conference on
World Wide Web, pages 101?110.
417
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1773?1782, Dublin, Ireland, August 23-29 2014.
Morphological Analysis for Japanese Noisy Text
Based on Character-level and Word-level Normalization
SAITO Itsumi, SADAMITSU Kugatsu, ASANO Hisako and MATSUO Yoshihiro
NTT Media Intelligence Laboratories
{saito.itsumi, sadamitsu.kugatsu,
asano.hisako, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
Social media texts are often written in a non-standard style and include many lexical variants
such as insertions, phonetic substitutions, abbreviations that mimic spoken language. The nor-
malization of such a variety of non-standard tokens is one promising solution for handling noisy
text. A normalization task is very difficult to conduct in Japanese morphological analysis because
there are no explicit boundaries between words. To address this issue, in this paper we propose a
novel method for normalizing and morphologically analyzing Japanese noisy text. We generate
both character-level and word-level normalization candidates and use discriminative methods to
formulate a cost function. Experimental results show that the proposed method achieves accept-
able levels in both accuracy and recall for word segmentation, POS tagging, and normalization.
These levels exceed those achieved with the conventional rule-based system.
1 Introduction
Social media texts attract a lot of attention in the fields of information extraction and text mining. Al-
though texts of this type contain a lot of information, such as one?s reputation or emotions, they often
contain non-standard tokens (lexical variants) that are considered out-of-Vocabulary (OOV) terms. We
define an OOV as a word that does not exist in the dictionary. Texts in micro-blogging services such
as Twitter are particularly apt to contain words written in a non-standard style, e.g., by lengthening
them (?goooood? for ?good?) or abbreviating them (?thinkin? ? for ?thinking?). This is also seen in the
Japanese language, which has standard word forms and variants of them that are often used in social
media texts. To take one word as an example, the standard form is???? (oishii, ?It is delicious?) and
its variants include ???????(oishiiiii), ??? (?oishii), and ????(oishii), where the un-
derlined characters are the differences from the standard form. Such non-standard tokens often degrade
the accuracy of existing language processing systems, which are trained using a clean corpus.
Almost all text normalization tasks for languages other than Japanese (e.g., English), aim to replace
the non-standard tokens that are explicitly segmented using the context-appropriate standard words (Han
et al. (2012), Han and Baldwin (2011), Hassan and Menezes (2013), Li and Liu (2012), Liu et al. (2012),
Liu et al. (2011), Pennell and Liu (2011), Cook and Stevenson (2009), Aw et al. (2006)). On the other
hand, the problem is more complicated in Japanese morphological analysis because Japanese words are
not segmented by explicit delimiters. In traditional Japanese morphological analysis, word segmentation
and part-of-speech (POS) tagging are simultaneously estimated. Therefore, we have to simultaneously
analyze normalization, word segmentation, and POS tagging to estimate the normalized form using the
context information. For example, the input ??????? ???(pan-keiki oishiiii, ?This pancake
tastes good?) written in the standard form is????????? (pan-keiki oishii). The result obtained
with the conventional Japanese morphological analyzer MeCab (Kudo (2005)) for this input is????
? (pancake, noun)/? ?? (unk)/? (unk)/? (unk)/, where slashes indicate the word segmentations and
?unk? means an unknown word. As this result shows, Japanese morphological analyzers often fail to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1773
correctly estimate the word segmentation if there are unknown words, so the pipeline method (e.g., first
estimating the word segmentations and then estimating the normalization forms) is unsuitable.
Moreover, Japanese has several writing scripts, the main ones being Kanji, Hiragana, and Katakana.
Each word has its own formal written script (e.g., ??? (kyoukasyo, ?textbook?) as formally written
in Kanji), but in noisy text, there are many words that are intentionally written in a different script
(e.g., ?????? (kyoukasyo, ?textbook?) is the Hiragana form of???). These tokens written in
different script also degrade the performance of existing systems because dictionaries basically include
only the standard script. Unlike the character-level variation we described above, this type of variation
occurs on a word?level one. Therefore, there are both character-level and word-level non-standard
tokens in Japanese informal written text. Several normalization approaches have been applied to Japanese
text. Sasano et al. (2013) and Oka et al. (2011) introduced simple character level derivational rules for
Japanese morphological analysis that are used to normalize specific patterns of non-standard tokens, such
as for word lengthening and lower-case substitution. Although these approaches handle Japanese noisy
text fairly effectively, they can handle only limited kinds of non-standard tokens.
We propose a novel method of normalization in this study that can handle both character- and word-
level lexical variations in one model. Since it automatically extracts character-level transformation pat-
terns in character-level normalization, it can handle many types of character-level transformations. It
uses two steps (character- and word-level) to generate normalization candidates, and then formulates a
cost function of the word sequences as a discriminative model. The contributions this research makes
can be summarized by citing three points. First, the proposed system can analyze a wider variety of
non-standard token patterns than the conventional system by using our two-step normalization candidate
generation algorithms. Second, it can largely improve the accuracy of Japanese morphological analysis
for non-standard written text by simultaneously performing the normalization and morphological analy-
ses. Third, it can automatically extract character alignments and in so doing reduces the cost of manually
creating many types of transformation patterns. The rest of this paper is organized as follows. Section 2
describes the background to our research, including Japanese traditional morphological analysis, related
work, and data collection methods. Section 3 introduces the proposed approach, which includes lattice
generation and formulation, as a discriminative model. Section 4 discusses experiments we performed
and our analyses of the experimental results. Section 5 concludes the paper with a brief summary and a
mention of future work.
2 Background
2.1 Japanese Morphological Analysis
Many approaches to joint word segmentation and POS tagging including Japanese Morphological anal-
ysis can be interpreted as re-ranking while using a word lattice (Kaji and Kitsuregawa (2013)). There
are two points to consider in the analysis procedure: how to generate the word lattice and how to formu-
late the cost of each path. In Japanese morphological analysis, the dictionary-based approach has been
widely used to generate the word lattice (Kudo et al. (2004), Kurohashi et al. (1994)). In a traditional
approach, an optimal path is sought by using the sum of the two types of costs for the path: the cost
for a candidate word that reflects the word?s occurrence probability, and the cost for a pair of adjacent
POS that reflects the probability of an adjacent occurrence of the pair (Kudo et al. (2004), Kurohashi et
al. (1994)). A greater cost means less probability. The Viterbi algorithm is usually used for finding the
optimal path.
2.2 Related Work
Several studies have been conducted on Japanese morphological analysis in the normalized form. The
approach proposed by Sasano et al. (2013) aims to develop heuristics to flexibly search by using a simple,
manually created derivational rule. Their system generates normalized character sequence based on the
derivational rule, and adding new nodes that are generated from normalized character sequence when
generating the word lattice using dictionary lookup. Figure 1 presents an example of this approach.
If the non-standard written sentence ??????? (suugoku tanoshii, ?It is such fun?) is input, the
1774
Figure 1: Example of Japanese morphological analysis and normalization
type non-standard form standard form
(1) Insertion ??????? (arigatoou) ????? (arigatou, ?Thank you?)
(2) Deletion ?? (samu) ??? (samui, ?cold?)
(3) Substitution with phonetic variation ???? (kawaee) ???? (kawaii, ?cute?)
(4) Substitution with lowercases and uppercases ????? (arigatou) ????? (arigatou, ?Thank you?)
(5) Hiragana substitution ????? (aidei) ID (aidei, ?identification card?)
(6) Katakana substitution ????? (arigatou) ????? (arigatou, ?Thank you?)
(7) Any combination of (1) to (6) ????? (kaunta) ????? (kaunta, ?counter?)
???? (attsui) ??? (atsui, ?hot?)
Table 1: Types of non-standard tokens and examples of annotated data
traditional dictionary-based system generates Nodes that are described using solid lines, as shown in Fig.
1. Since ?????? (suugoku, ?such?) and ????? (tanoshii, ?fun?) are OOVs, the traditional system
cannot generate the correct word segments or POS tags. However, their system generates additional
nodes for the OOVs, shown as broken line rectangles in Fig. 1. In this case, derivational rules that
substitute ??? with ?null? and ??? (i) with ??? (i) are used and the system can generate the standard
forms ????? (sugoku, ?such?) and ????? (tanoshii, ?fun?) and their POS tags. If we can generate
sufficiently appropriate rules, these approaches seem to be effective. However, there are many types of
derivational patterns in SNS text and it is difficult to cover all of them by hand. Moreover, it becomes a
serious problem how to set the path cost for appropriately re-ranking the word lattice when the number
of candidates increases. Our approach is also based on the dictionary-based approach, however, our
approach is significantly dissimilar from their approach in two ways. First, we automatically generate
derivational patterns (we call them transformation tables) based on the character-level alignment between
non-standard tokens and their standard forms. Compared to generating the rules by hand, our approach
can generate broad coverage rules. Second, we use discriminative methods to formulate a cost function.
Jiang et al. (2008), Kaji and Kitsuregawa (2013) introduce several features to appropriately re-rank the
added nodes. This enables our system to perform well even when the number of candidates increases.
On the other hand, several studies have applied a statistical approach. For example, Sasaki et al.
(2013) proposed a character-level sequential labeling method for normalization. However, it handles
only one-to-one character transformations and does not take the word-level context into account. The
proposed method can handle many-to-many character transformations and takes word-level context into
account, so the scope for handling non-standard tokens is different. Many studies have been done on text
normalization for English; for example Han and Baldwin (2011) classifies whether or not OOVs are non-
standard tokens and estimates standard forms on the basis of contextual, string, and phonetic similarities.
In these studies it was assumed that clear word segmentations existed. However, since Japanese is an
unsegmented language the normalization problem needs to be treated as a joint normalization, word
segmentation, and POS tagging problem.
2.3 Data Collection and Analysis of Non-standard Tokens
In previous studies (Hassan and Menezes (2013), Ling et al. (2013), Liu et al. (2011)), the researchers
proposed unsupervised ways to extract non-standard tokens and their standard forms. For Japanese text,
however, it is very difficult to extract word pairs in an unsupervised way because there is no clear word
segmentation. To address this problem we first extracted non-standard tokens from Twitter text and blog
1775
Figure 2: Structure of proposed system
Figure 3: Example of candidate generation
text and manually annotated their standard (dictionary) forms. In total, we annotated 4808 tweets and
8023 blog text sentences. Table 1 lists the types of non-standard tokens that we targeted in this study
and examples of the annotated data. Types (1), (2), (3) and (4) are similar to English transform patterns.
Types (5) and (6) are distinctive patterns in Japanese. As previously mentioned Japanese has several
kinds of scripts, the main ones being Kanji, Hiragana, and Katakana. These scripts can be used to write
the same word in several ways. For example, the dictionary entry ?? (sensei, ?teacher?) can also
be written in Hiragana form ???? (sensei) or Katakana form ???? (sensei). Most words are
normally written in the standard form, but in informal written text (e.g., Twitter text), these same words
are often written in a non-standard form. In examining Twitter data for such non-standard tokens, we
found that 55.0% of them were types (1) to (3) in Table 1, 4.5% were type (4), 20.1% were types (5)
to (6), 2.7% were type (7), and the rest did not fall under any of these types since they were the result
of dialects, typos, and other factors. In other words, a large majority of the non-standard tokens fell
under types (1) to (7). We excluded those that did not as targets in this study because our proposed
method cannot easily handle them. Types (1) to (4) occur at character-level and so can be learned from
character-level alignment, but types (5) to (6) occur at word-level and it is inefficient to learn them on
a character?level basis. Accordingly, we considered generating candidates and features on two levels:
character-level and word-level.
3 Proposed Method
3.1 Overview of Proposed System
We showed the structure of the proposed system in Fig. 2. Our approach adds possible normalization
candidates to a word lattice and finds the best sequence using a Viterbi decoder based on a discriminative
model. We introduced several features that can be used to appropriately evaluate the confidence of the
added nodes as normalization candidates. We generate normalization candidates as indicated in Fig. 3.
1776
Figure 4: Example of character alignment
We describe the details in the following section.
3.2 Character-level Lattice
3.2.1 Character Alignment between Non-standard Tokens and Their Normalized Forms
We have to create a character-level transformation table to generate the character-level lattice. We used
the joint multigram model proposed by Sittichai et al. (2007) to create the transformation table because
this model can handle many-to-many character alignments between two character sequences. In ob-
serving non-standard tokens and their standard forms, we find there are not only one-to-one character
transformations but also many-to-many character transformations. Furthermore, unlike in translation,
there is no character reordering so the problems that arise are similar to those in transliteration. Accord-
ingly, we adopted a joint multigram model that is widely used for transliteration problems. The optimal
alignment can be formulated as q? = arg max
q?K
d
?
q?q
p(q) , where d is a pair of non-standard tokens
and its standard form (e.g., d is?????? (arigatoou), ????? (arigatou). Here, q is a partial
character alignment in d (e.g., q is ????, ???), q is the character alignment q set in d (e.g., q of
path 1 in Fig. 4 is {(??, ??), (??, ??), (??, ??), (????, ???)}. K
d
is the possible character
alignment sequence candidates generated from d. We generate n-best optimal path for K
d
in this study.
The maximum likelihood training can be performed using the EM algorithm derivated in Bisani and Ney
(2008) and Kubo et al. (2011) to estimate p(q). p(q) can be formulated as follow:
p(q) = ?
q
/
?
q?Q
?
q
(1)
?
q
=
?
d?D
?
q?K
d
p(q)n
q
(q) =
?
d?D
?
q?K
d
?
q?q
p?(q)
?
q?K
d
?
q?q
p?(q)
n
q
(q),
and where D is the number of the d pair, Q is the set of q, and n
q
(q) is the count of q that occurred in
q. In our system, we allow for standard form deletions (i.e., mapping of a non-standard character to a
null standard character) but not non-standard token deletions. Since we use this alignment as the trans-
formation table when generating a character-level lattice, the lattice size becomes unnecessarily large
if we allow for non-standard form deletions. In the calculation step of the EM algorithm, we calculate
the expectation (partial counts) ?
q
of each alignment in the E-step, calculate the joint probability p(q)
that maximizes the likelihood function in the M-step as described before, and repeat these steps until
convergence occurs. p?(q) indicates the result of p(q) calculated in the previous step over the iteration.
When generating the character-level lattice, we used alignments that were expected to exceed a prede-
fined threshold. We used ?
q
(q = (c
t
, c
v
)) and r(c
t
, c
v
) as thereshold, where c
t
and c
v
are the partial
character sequence of non-standard token and it?s standard form respectively. r(c
t
, c
v
) is calculated by
r(c
t
, c
v
) = ?
q
/n
c
v
., where n
c
v
is the number of occurrences of c
v
in the training data. We set the thresh-
old ?
q thres
= 0.5 , and r(c
t
, c
v
)
thres
= 0.0001 in this study. We also used r(c
t
, c
v
) as a feature of cost
1777
function in subsection. 3.4.2. When calculating initial value, we set p(c
t
, c
v
) high if the character c
t
and
c
v
are the same character and the length of each character is 1. We also give the limitation that a Kanji
character does not change to a different character and is aligned with same character in the calculation
step of the character alignment.
3.2.2 Generation of Character-level Lattice Based on Transformation Table
First, repetitions of more than one letter of ???, ???, ?-?, and ??? are reduced back to one letter (e.g.,
???????? (arigatooooou, ?Thank you?) is reduced to ?????? (arigatoou)) for the
input text. In addition, repetitions of more than three letters other than ???, ???, ?-?, and ??? are
reduced back to three letters (e.g.,???????? (uresiiiiiii, ?I?m happy?) is reduced back to??
???? (uresiiii)). These preprocessing rules are inspired by Han and Baldwin (2011) and determined
by taking the Japanese characteristics into consideration. We also used these rules when we estimated the
alignments of the non-standard tokens and their standard forms. Next, we generate the character-level
normalization candidates if they match the key transformation table in the input text. For example, if the
transformation table contains (q, logp(q))= (??? (yoo), ?? (you)?, -8.39), (?? (o), ? (o)?, -7.56),
and the input text includes the character sequence ???? ? (tyoo), we generate a new sequence ?????
(tyou) and ????? (tyoo). In other words, we add new nodes ???? (you) and ??? (o) in the position
of ??? ? (yoo) and ??? (o), respectively (see Fig. 3).
3.3 Generation of Word-level Lattice
We generate the word lattice based on the generated character-level lattice using dictionary lookup. We
exploit dictionary lookup by using the possible character sequence of the character-level lattice while
the traditional approach exploits it by using only the input character sequence. For example, we exploit
dictionary lookup for character sequences such as ???? ????? (tyoo kawaii) and ?????????
(tyou kawaii) and ????????? (chiyou kawaii) and ???? ????? (tyoo kawaii) (see Fig. 3)
Furthermore, we use the phonetic information of the dictionary to generate the normalization candi-
dates for Hiragana and Katakana substitution. For example, assume ??? (tyou, ?super?) and ??????
(kawaii, ?cute?) are the dictionary words. Then, if the input text contains the character sequences ???
?? (tyo) (which is written in Hiragana) and ?????? (kawaii) (which is written in Katakana), we add
??? (tyo, ?super?) and ?????? (kawaii, ?cute?) to the word lattice as the normalization candidates
since the two character sequences are pronounced identically. By using this two-step algorithm, we can
handle any combinational derivational patterns, such as Katakana substitutions or substitutions of lower-
cases like ?????? (kawaii)? ?????? (kawaii)? ?????? (kawaii, ?cute?) (see Fig. 3). Note
that we filtered candidates on the basis of a predefined threshold to prevent the generation of unneces-
sary candidates. The threshold was defined on the basis of the character sequence cost of normalization,
which is described in subsection 3.4.2. Furthermore, we limited the number of character transformations
to two per word.
3.4 Decoder
3.4.1 Objective Function
The decoder selects the optimal sequence y? from L(s) when given the candidate set L(s) for sentence
s. This is formulated as y? = arg min
y?L(s)
w ? f(y) (Jiang et al. (2008), Kaji and Kitsuregawa (2013)), where
y? is the optimal path, L(s) is the lattice created for sentence s, and w ? f(y) is the dot product between
weight vector w and feature vector f(y). The optimal path is selected according to the w ? f(y) value.
3.4.2 Features
The proposed lattice generation algorithm generates a lattice larger than that generated in traditional
dictionary-based lattice generation. Therefore, we need to introduce an appropriate normalization cost
into the objective function. We listed the features we used in Table 2. Let w
i
be the ith word candidate
and p
i
be the POS tag of w
i
. p
i?1
andw
i?1
are adjacent POS tag and word respectively. We also used the
word unigram cost f
w
i
p
i
, the cost for a pair of adjacent POS f
p
i?1
,p
i
that are quoted from MeCab (Kudo,
1778
Name Feature
Word unigram cost f
w
i
p
i
POS bi-gram cost f
p
i?1
,p
i
Word-POS bi-gram cost ?logp
w
i?1
p
i?1
,w
i
p
i
Character sequence cost log(p?
s
/p
?
t
i
)
where, p?
x
= p
1/length(x)
x
, p
x
=
?
n
j=1
p(c
j
|c
j?1
j?5
), x ? {s, t
i
}
Character transformation cost ?
trans
i
? (?logr(c
t
, c
v
))
Hiragana substitution cost ?
h
i
? f
w
i
p
i
Katakana substitution cost ?
k
i
? f
w
i
p
i
Table 2: Feature list of the decoder. ?
trans
i
is 1 if w
i
is generated by character transformation, otherwise
0. ?
h
i
is 1 ifw
i
is generated by Hiragana substitution, otherwise 0. ?
k
i
is 1 ifw
i
is generated by Katakana
substitution, otherwise 0.
2005), and five additional types of costs. These are the word-pos bi-gram cost ?logp
w
i?1
p
i?1
,w
i
p
i
of a
blog corpus; the character transformation cost ?
trans
i
?(?logr(c
t
, c
v
)), which is calculated in Section3.2,
for nodes generated by character transformation; the Hiragana substitution cost ?
h
i
? f
w
i
p
i
for nodes
generated by Hiragana substitution; the Katakana substitution cost ?
k
i
? f
w
i
p
i
for nodes generated by
Katakana substitution; and the character sequence cost log(p?
s
/p
?
t
i
) for all the normalized nodes. The
character sequence cost reflects the character sequence probability of the normalization candidates. Here,
s and t
i
are input string and transformed string respectively. (e.g., In Fig. 3, for the normalized node
?????? (cute, adjective), s is ???? ????? and t
i
is ???? ?????). Then p
s
and p
t
i
are
calculated by using the character 5-gram of a blog corpus, which is formulated by p
s
= p(c
1
? ? ? c
n
) =
?
n
j=1
p(c
j
|c
j?1
j?5
), where c
j
is the j th character of character sequence s. p?
t
i
and p?
s
are normalized by
using the length of each string s and t
i
as p?
t
i
= p
1/length(t
i
)
t
i
. We set the threshold (p?
s
/p
?
t
i
)
thres
= 1.5
for generating a Hiragana or Katakana normalization candidate in this study. Since all those features can
be factorized, the optimal path is searched for by using the Viterbi algorithm.
3.4.3 Training
We formulated the objective function for tuning weights w by using Eq. 2. The weights w are trained
by using the minimum error rate training (MERT) Machery et al. (2008). We defined the error function
as the differences between the reference word segmentations and the POS tags of the reference sequence
y
ref
and the system output arg min
y?L(s)
w ? f(y).
w? = arg min
w?W
N
?
i=1
error(y
ref
, arg min
y?L(s)
w ? f(y)) (2)
4 Experiments
4.1 Dataset and Estimated Transformation Table
We conducted experiments to confirm the effectiveness of the proposed method, in which we annotated
corpora of a Japanese blog and Twitter. The Twitter corpus was split into three parts: the training, devel-
opment, and test sets. The test data comprised 300 tweets, development data comprised 500 sentences
and the training data comprised 4208 tweets. We randomly selected the test data which contained at least
one non-standard token. The test data comprised 4635 words, 403 words of them are non-standard token
and are orthographically transformed into normalized form and POS tags. The blog corpus comprised
8023 sentences and all of them were used as training data. Training data was used for extracting char-
acter transformation table and development data was used for estimating parameters of discriminative
model. We used the IPA dictionary provided by MeCab to generate the word-level lattice and extracted
the dictionary-based features. We itemized the estimated character transformation patterns in Table 3.
There were 5228 transformation patterns that were learned from the training data and we used 3268 of
them, which meets the predefined condition. The learned patterns cover most of the previously pro-
1779
non-standard
character c
t
standard
character c
v
logp(q)
non-standard
character c
t
standard
character c
v
logp(q)
? null -4.233 ?? (ssu) ?? (desu) -5.999
??(maa) ?? (maa) -5.059 ?? (doo) ?? (dou) -6.210
??(syo) ??? (syou) -5.211 ?? (nee) ?? (nai) -6.232
?? (daro) ??? (darou) -5.570 ??(rya) ?? (reha) -6.492
?(ttsu) null -5.648 ?? (ten) ?? (teru) -6.633
?? (nto) ??? (ntou) -5.769 ?? (yuu) ?? (iu) -6.660
?(wa) ? (wa) -5.924 ?? (nan) ?? (nano) -6.706
Table 3: Example of character-level transformation table
posed rules. In addition, our method can learn more of the variational patterns that are difficult to create
manually.
4.2 Baseline and Evaluation Metrics
We compared the five methods listed in Table 4 in our experiments. Traditional means that which gen-
erates no normalization candidates and only uses the word cost and the cost for a pair of adjacent POS,
so we can consider it as a traditional Japanese morphological analysis. We compared three baselines,
Baseline1, Baseline2 and Baseline3. Baseline1 is the conventional rule-based method (considering in-
sertion of long sound symbols and lowercases, and substitution with long sound symbols and lower-
cases), which was proposed by Sasano et al. (2013). In Baseline2, 3, and Proposed, we basically use
the proposed discriminative model and features, but there are several differences. Baseline2 only gen-
erates character-level normalization candidates. Baseline3 uses our two-step normalization candidate
generation algorithms, but the character transformation cost of all the normalization candidates that are
generated by character normalization is the same. Proposed generates the character-level and Hiragana
and Katakana normalization candidates and use all features we proposed.
We evaluated each method on the basis of precision and recall and the F-value for the overall system
accuracy. Since Japanese morphological analysis simultaneously estimates the word segmentation and
POS tagging, we have to check whether or not our system is negatively affected by anything other than the
non-standard tokens. We also evaluated the recall with considering only normalized words. That value
directly reflects the performance of our normalization method. We registered emoticons that occurred in
the test data in the dictionary so that they would not negatively affect the systems? performance.
4.3 Results and Discussion
The results are classified in Table 4. As the table shows, the proposed methods performed statistically
significantly better than the baselines and the traditional method in both precision and recall (p < 0.01),
where the precision was greatly improved. This indicates that our method can not only correctly analyze
the non-standard tokens, but can also reduce the number of wrong words generated. Baseline1 also
improved the accuracy and recall compared to the traditional method, but the effect was limited. When
we compare Proposed with Baseline2, we find the F-value is improved when we take the Hiragana
and Katakana substitution into consideration. Baseline3 also improved the F-value but its performance is
inferior to proposed method.This proves that even if we can generate sufficient normalization candidates,
the results worsen if the weight parameter of each normalization candidate is not appropriately tuned. The
column of ?recall?? in Table 4 specifies the improvement rates of the non-standard tokens. The proposed
methods improve about seven times when using Baseline1 while preventing degradation. These results
prove that we have to generate appropriate and sufficient normalization candidates and appropriately tune
the cost of each candidate to improve both the precision and recall.
We show examples of the system output in Table 5. In the table, slashes indicate the position of the
estimated word segmentations and the words that were correctly analyzed are written in bold font. Exam-
ples (1) to (5) are examples improved by using the proposed method. Examples (6) to (7) are examples
that were not improved and example (8) is an example that was degraded. Examples (1) to (3) include
phonetic variations and example (4) is a Hiragana substitution. Example (5) is a combinational trans-
1780
word segmentation word segmentation and POS tag
method precision recall F-value precision recall F-value recall?
Traditional 0.716 0.826 0.767 0.683 0.788 0.732 -
Rule based (BL1??) 0.753 0.833 0.791 0.717 0.794 0.754 0.092
Proposed 0.856 0.883 0.869 0.822 0.849 0.835 0.667
- without Hiragana and Katakana normalization (BL2) 0.834 0.875 0.854 0.798 0.838 0.818 0.509
- character transformation cost is fixed (BL3) 0.838 0.865 0.851 0.807 0.834 0.821 0.533
? considering only normalized words, ?? BL:baseline
Table 4: Results of precision and recall of test data
input traditional proposed gold standard
(1)???(adii) ? (a)/? (di)/? ??? (atsui) ??? (atsui, ?hot?)
(2)???(sugee) ?? (suge)/? ??? (sugoi) ??? (sugoi, ?great?)
(3)????? (gommeen) ? (go)/?/? (me)/?/? (n)/ ??? (gomen) ??? (gomen, ?I?m sorry?)
(4)????(hitsuyou) ?? (hitsu)/?? (you) ?? (hitsuyou) ?? (hitsuyou, ?necessary?)
(5)?????(daichuki) ? (da)/?? (ichi)/?(yu)/? (ki)/ ??? (daisuki) ??? (daisuki, ?like very much?)
(6)?????(oseee) ?? (ose)/?? (ee)/? (e) ?? (ose) ??? (osoi, ?slow?)
(7)????? (kanwaii) ?? (kan)/? (wa)/?? (ii) ?? (kanwa)/?? (ii) ???? (kawaii, ?cute?)
(8)??? (inai) ? (i)/?? (nai) ?? (inai) ?/?? (i/nai, ?absent?)
Table 5: System output examples
formation pattern of a phonetic variation and Hiragana substitution. We can see our system can analyze
such variational non-standard tokens for all these examples. Two types of errors were identified. The first
occurred as the result of a lack of a character transformation pattern and the second was search errors.
Example (6) shows an example of a case in which our system couldn?t generate correct normalization
candidate because there was not corresponding character transformation pattern, even though there was
a similar phonetic transformation pattern. To ensure there will be no lack of transformation patterns,
we should either increase the parallel corpus size to enable the learning of more patterns or derive new
transformation patterns from the learned patterns. Example (7) shows an example of a case in which a
normalized candidate was generated but a search failed to locate it. Example (8) shows an example of a
case in which the result was degraded. Our system can control the degradation well, but there are several
degradation caused by normalization. We will need to develop a more complicated model or introduce
other features into the current model to reduce the number of search errors.
5 Conclusion and Future Work
We introduced a text normalization approach into joint Japanese morphological analysis and showed that
our two-step lattice generation algorithm and formulation using discriminative methods outperforms the
previous method. In future work, we plan to extend this approach by introducing an unsupervised or
semi-supervised parallel corpus extraction for learning character alignments to generate more patterns
at a reduced cost. We also plan to improve our model?s structure and features and implement it with a
decoding method to reduce the number of search errors. In addition, we should consider adding other
types of unknown words (such as named entities) to the morphological analysis system to improve its
overall performance.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization.
Proceedings of the COLING/ACL on Main Conference Poster Sessions, pages 33?40.
Maximilian Bisani and Hermann Ney. 2008. Joint-sequence models for grapheme-to-phoneme conversion.
Speech Commun., 50(5):434?451, May.
Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. Proceedings
of the Workshop on Computational Approaches to Linguistic Creativity, pages 71?78.
1781
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies - Volume 1, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for
microblogs. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, pages 421?432.
Hany Hassan and Arul Menezes. 2013. Social text normalization using contextual graph random walks. Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1577?1586, August.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word lattice reranking for chinese word segmentation and part-of-
speech tagging. Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1,
pages 385?392.
Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Efficient word lattice generation for joint word segmentation
and pos tagging in japanese. Proceedings of the Sixth International Joint Conference on Natural Language
Processing, pages 153?161.
Keigo Kubo, Hiromichi Kawanami, Hiroshi Saruwatari, and Kiyohiro Shikano. 2011. Unconstrained many-to-
many alignment for automatic pronunciation annotation. In Proc. of APSIPA ASC.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to japanese
morphological analysis. In Proc. of EMNLP, pages 230?237.
T. Kudo. 2005. Mecab : Yet another part-of-speech and morphological analyzer. http://mecab.sourceforge.net/.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of japanese
morphological analyzer juman. In Proc. of The International Workshop on Sharable Natural Language Re-
sources, page 22?38.
Chen Li and Yang Liu. 2012. Improving text normalization using character-blocks based models and system
combination. Proceedings of COLING 2012, pages 1587?1602.
Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
October.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution? normaliz-
ing text messages without pre-categorization nor supervision. Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pages 71?76, June.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-coverage normalization system for social media language.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1035?1044.
W Machery, F J Och, and I Uszkoreit J Thayer. 2008. Lattice-based minimum error rate training for statistical
machine translation. In Proc. of EMNLP, 1:725?734.
Teruaki Oka, Mamoru Komachi, Toshinobu Ogiso, and Yuji Matsumoto. 2011. Handling orthographic variations
in morphological analysis for near-modern japanese (in japanese). In Proc. of The 27th Annual Conference of
the Japanese Society for Articial Intelligence.
Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of sms
abbreviations. Proceedings of 5th International Joint Conference on Natural Language Processing, pages 974?
982, November.
Akira Sasaki, Junta Mizuno, Naoaki Okazaki, and Kentaro Inui. 2013. Normalization of text in microblogging
based on machine learning(in japanese) (in japanese). In Proc. of The 27th Annual Conference of the Japanese
Society for Articial Intelligence.
Ryohei Sasano, Sadao Kurohashi, and Manabu Okumura. 2013. A simple approach to unknown word process-
ing in japanese morphological analysis. Proceedings of the Sixth International Joint Conference on Natural
Language Processing, pages 162?170.
Jiampojamarn Sittichai, Kondrak Grzegorz, and Sherif Tarek. 2007. Applying many-to-many alignments and
hidden markov models to letter-to-phoneme conversion. In Proc. of The Conference of the North American
Chapter of the Association for Computational Linguistics, pages 372?379.
1782
