Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 794?805, Dublin, Ireland, August 23-29 2014.
Automatic Feature Selection for Agenda-Based Dependency Parsing
Miguel Ballesteros
Natural Language Processing Group
Universitat Pompeu Fabra
Barcelona, Spain
miguel.ballesteros@upf.edu
Bernd Bohnet
School of Computer Science
University of Birmingham
Birmingham, United Kingdom
bohnetb@cs.bham.ac.uk
Abstract
In this paper we present an in-depth study on automatic feature selection for beam-search depen-
dency parsers. The search strategy is inherited from the one implemented in MaltOptimizer, but
searches in a much larger set of feature templates that could lead to a higher number of combina-
tions. Our models provide results that are on par with models trained with a larger set of feature
templates, and this implies that our models provide faster training and parsing times. Moreover,
the results establish the state of the art for some of the languages.
1 Introduction
Finding an optimal and accurate set of feature templates is crucial when training statistical parsers; in
fact it is essential when building any machine learning system (Smith, 2011). In dependency parsing, the
features are based on the linguistic information that is annotated within the words and the information
that is being calculated during the parsing process. Researchers normally tend to include a large set of
feature templates in their machine learning models, following the idea that more is always better; however
some recent research on feature selection for transition-based parsing (Ballesteros, 2013; Ballesteros and
Nivre, 2014) and graph-based parsing (He et al., 2013) have shown that more features are not always
better, at least in the case of dependency parsing; models containing more features are always slower in
parsing and training time and they do not always provide better results.
This indicates that a smart feature template selection could be the key in the trade-off for finding an
accurate and fast feature model for a given parsing model. On the one hand, we want a parser that should
provide the best results possible, while on the other hand, we want a parser that should provide the results
in the fastest way possible. For practical applications, a fast model is crucial.
In this paper, we report the results of feature selection experiments that we carried out with the in-
tention of obtaining accurate and faster feature models, for the transition-based Mate parser with and
without graph-based completion models. The Mate parser is a beam search parser that uses a hash kernel
for training, joint part-of-speech tagging, morphological tagging and dependency parsing. As a result of
this research, we provide a framework that allows to find an optimal feature template set for the Mate
parser (Bohnet et al., 2013). Moreover, our models provide some of the highest results ever reported for
a set of treebanks.
The paper is organized as follows. Section 2 describes related work including the used agenda-based
dependency parser. This section depicts the feature templates that can be used by a transition-based or
a graph-based parser. Section 3 describes the feature selection algorithm that we implemented for our
experiments. Section 4 shows the experimental set-up. Section 5 reports the main results of our experi-
ments. Section 6 provides the parsing times and memory requirements. Finally, Section 7 concludes.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
794
Transition Condition
LEFT-ARC
d
([?|i, j], B,?)? ([?|j], B,?[(j, i)?A, ?(j, i)=d]) i 6= 0
RIGHT-ARC
d
([?|i, j], B,?)? ([?|i], B,?[(i, j)?A, ?(i, j)=d])
SHIFT
p,m,l
(?, [i|?],?)? ([?|i], ?,?[pi(i)=p, ?(i)=m,?(i)= l])
SWAP ([?|i, j], ?,?)? ([?|j], [i|?],?) 0 < i < j
Figure 1: Transition set for joint morphological and syntactic analysis. The stack ? is represented as a
list with its head to the right (and tail ?) and the buffer B as a list with its head to the left (and tail ?).
2 Related Work
2.1 Mate Parser
For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs
joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing.
The parser employs a number of techniques that lead to very competitive accuracy such as beam-search
with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set,
a graph-based completion model that adds scores for tree parts which a transition-based parser would not
be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes
into account second and third order factors and obtains a score as soon as the tree parts are completed.
The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et
al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the
two models. The drawback of such a large feature set is a huge impact on the speed. Important research
questions include (1) whether the number of features could be reduced to speed up the parser and (2)
whether languages dependent feature sets would be beneficiary.
2.2 Features in transition-based dependency parsing
Every transition-based parser uses two data structures: (1) a buffer that contains at the beginning of the
parsing process all words of the sentence that have to be parsed, and (2) a stack.
The Mate parser that we used in our experiment follows Nivre?s arc-standard parsing algorithm plus
the SWAP transition to build non-projective dependency trees. Figure 1 depicts the transition system
formally; the SHIFT transition removes the first node from the buffer and puts it on the stack. The
LEFT-ARC
d
transition introduces a labeled dependency edge between the top element on the stack and
the second element of the stack with the label d. The second top element is removed from the stack.
The RIGHT-ARC
d
transition introduces a labeled dependency edge between the second element on the
stack and the top element with the label d while the top element is removed from the stack. The SWAP
transition swaps the position of the topmost nodes of the stack and the buffer.
A classifier selects transitions based on the feature templates that are composed of stack elements,
buffer elements, the already created parse, and the transition sequence. For instance, if the parser contains
the feature template LEMMA(S
1
), it means that it may use the lemma of the word that is in the first
position of the stack in any parsing state in order to select the best parsing action.
2.3 Features in graph-based dependency parsing
A Graph-based dependency parser performs an exhaustive search over trees of the words of a sentence.
Frequently, dynamic programming techniques are used to find the optimal tree for each span, considering
candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to
decide among alternative spans. The typical feature models are based on combinations of edges (as
known as, factors). A factor consists either of a single edge, two or three edges; which are called
first order, second and third order factors, respectively. The later are employed in more advanced and
recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo
and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the
795
vertexes involved in the factors. A feature template of a second order factor is composed of properties
drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted
as POS(H)+POS(D)+POS(C). In our experiments, we use in addition to the transition-based model, a
completion model that uses graph-based feature templates with up to third order factors to re-score the
beam.
2.4 Feature Selection
There has been some recent research on trying to manually find better feature models for dependency
parsers, such as Nivre et al. (2006), Hall et al. (2007), Hall (2008), Zhang and Nivre (2011), and
Agirre et al. (2011). There is also research on automatic feature selection in the case of transition-based
dependency parsing, a good example is MaltOptimizer (Ballesteros and Nivre, 2014) which implements
a search for the best feature model that it can find, following acquired previous experience and deep
linguistic knowledge (Hall et al., 2007; Nivre and Hall, 2010); Nilsson and Nugues (2010) also tried to
search for optimal feature sets in the case of transition-based parsing, starting from a reduced test set
using the concept of topological neighbors. Finally, He He et al. (2013) also tried automatic feature
selection but for a graph-based parsing algorithm, where they pruned the feature space, removing unused
features, in a first-order graph-based dependency parser, providing models that are equally accurate and
faster.
Zhang and Nivre (2011) pointed out that two different parsers based on the same algorithm may
need different feature templates since other design aspects of a parser might have an influence on the
usefulness of feature templates such as the learning technique or the use of beam search.
3 Feature Selection Algorithm
As in MaltOptimizer (Ballesteros and Nivre, 2014), our feature selection algorithm starts with a default
feature set that is based on the MaltParser?s default feature model for an arc-standard parsing algorithm
1
,
it first tests whether the features that are in the default model are actually useful, which means that
whenever we remove any of the features of the default set, the accuracy is still the same (or better).
Let F = {F
1
, . . . , F
n
} be the full set of features,
let M(X) be the evaluation metric for feature set X,
and let ? be the threshold.
1 X ? ?
2 while X 6= F
3 B ? 0
4 Y ? ?
5 for each X
i
? F \X
6 if M(X ? {X
i
}) + ? > B then
7 B ?M(X ? {X
i
})
8 Y ? X ? {X
i
}
9 if M(X) > B then
10 return X
11 else
12 X ? Y
13 return X
Figure 2: Algorithm for forward feature selection.
After that, one by one, the algorithm tries to
add feature templates to the feature set. For each
additional feature template a parser is trained for
testing and if the accuracy is higher than the ac-
curacy of the previous step plus a ? (threshold)
then the feature in question is added to the fea-
ture set. The selection process continues until
all features have been tested, and therefore each
feature has been either added or rejected. Most
of the feature selection is based on the forward
selection algorithm shown in Figure 2, although
there is also a bit of backward selection from the
default set.
The feature selection algorithm only has the
training set as an input, and it splits it into train-
ing and development to validate the outcomes of
the experiments.
2
After the feature selection, we
run the parser model on a held-out test set to measure its performance.
The feature selection is pruned following similar strategies to MaltOptimizer; there are features that are
deeply related and the system tries to avoid unnecessary tests when some features happen to be excluded.
For instance, the algorithm will not try to select the third position of the buffer for the part-of-speech, if
the second position was excluded by the feature selection algorithm.
1
http://www.maltparser.org/userguide.html
2
It makes a 80/20 division; 80% for training, 20% for development.
796
4 Experimental Set-Up
In order to set up the experiments for the feature selection algorithm, we carried out a series of tests
based on the parser settings. From these experiments, we obtained the best parser settings, the threshold
that provides the best results given a development set, and the best scoring method and some additional
configurations, that gave us reliable results and a fast outcome.
We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank
5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with
the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).
3
English: We used the WSJ section
of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the
labeling rules of Nivre (2006).
4
German: We used Tiger Treebank (Brants et al., 2002) in the improved
dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency
Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000;
Boguslavsky et al., 2002).
4.1 Parser settings
As outlined in Section 3, our feature selection experiments require the training of a large number of
parsing models and applying these to the development set.
5
Therefore, we aimed to find a training setup
for the parser that provided fast training times while maintaining a realistic training and optimization
scenario.
A major factor for the time usage is the beam size. The beam contains the alternative syntactic struc-
tures that are considered in the parsing process, and thus it requires more time and memory while it
normally provides better results. The parser uses two additional small beams to store the differently
tagged syntactic structures and morphological structures, for the joint models. We explored a number of
configurations and assessed the parsing performance by carrying out a set of experiments on the Penn
Treebank and the training settings of Bohnet et al. (2013);
6
the results are shown in Table 1.
transition-based model
beam 1 3 5 8 12 20 30 40 50
LAS 88.00 89.71 90.10 90.19 90.26 90.09 90.29 90.46 90.41
POS 96.88 97.02 97.03 97.00 96.94 96.95 97.02 96.92 97.00
TT 4 7 8 9 11 14 16 20 21
transition-based and graph-based completion model
beam 1 3 5 8 12 20 30 40 50
LAS 77.49 88.92 90.13 90.55 90.49 90.62 90.97 90.96 90.75
POS 96.71 96.93 96.97 96.97 96.97 97.05 96.99 97.00 97.04
TT 2 9 11 14 20 32 35 40 48
Table 1: Labeled Accuracy Score (LAS) in percent, Part-of-Speech tag accuracy POS in percent and
training time (TT) in milliseconds per sentence. The parser was applied on the development set and
trained over the Penn Treebank.
The table provides an overview of this preliminary experiment. The upper part of the table shows the
performance when only using the transition-based model. The accuracy improvements are small when
the beam-size becomes larger than 5. Even when we compared the results with the results of a beam size
of 30, we observed only a small accuracy improvement. Further, we observe with a larger beam size a
saturation where the accuracy does not improve and the parsing results show a small variance.
3
Training: 001?815, 1001?1136. Development: 886?931, 1148?1151. Test: 816?885, 1137?1147.
4
Training: 02-21. Development: 24. Test: 23.
5
All this experiments were carried out on a CPU Intel Xeon 3.4 Ghz with 6 cores.
6
We used 25 training iterations and we took the accuracy scores from the last iteration, we used the join parser, the two best
part-of-speech tags and morphological tags. The threshold for the inclusion of part-of-speech tags was set to 0.25 and that of
the morphological tagger to 0.1. We selected a beam size for the alternative POS tags and morphological tags of 4.
797
English German
? LAS UAS POS # LAS UAS POS MOR #
0.05 90.17 91.39 97.00 40 90.57 92.81 97.89 90.45 41
0.02 90.24 91.52 97.04 54 90.83 93.00 98.01 90.55 49
0.01 90.17 91.45 96.90 54 90.90 92.95 97.98 90.69 60
0.00 90.43 91.71 97.00 57 90.89 92.98 97.94 90.59 68
-0.01 90.26 91.47 97.06 69 90.92 93.09 98.02 90.72 79
-0.02 90.27 91.52 97.05 77 91.27 93.37 98.17 90.84 93
-0.05 90.49 91.66 97.01 98 91.02 93.11 98.11 90.69 116
-? 90.37 91.65 96.98 188 90.77 93.00 98.14 89.56 188
Figure 3: Accuracy scores depending on the threshold ?.
The feature selection starts with a default feature set that includes 20 features (cf. Section 3), and
all these features are derived from the default feature models for MaltParser (Nivre et al., 2007)
7
. In
total, the feature selection algorithm, for the transition-based model, may select 188 features. In Table 1
we show the training time (TT). We used this table to selected the optimal settings for the beam. After
considering the trade-off between accuracy and speed, we selected for the feature selection a beam size of
8, since it obtains 90.19 LAS which is close to the highest accuracy score 90.46 and with this beam size
the parser is fast. For a parser trained with all feature templates, the average parsing time per sentence is
9 milliseconds. With 20-60 features, we obtained a parsing time of 2-5 milliseconds per sentence, which
is a faster and more optimal setting for the feature selection. Moreover, with a beam size of 40, we get
parsing times that ranged depending on the number of features from 12 to 50 milliseconds per sentence,
this is impracticable for feature selection experiments.
4.2 Selecting an Optimal Threshold
Feature templates are selected when they provide a higher accuracy compared to the previous feature
set plus a threshold ?. To determine an optimal ? for the feature selection, we carried out a series of
experiments with different ? values. As a first step, we ran the feature selection algorithm starting from
0.05 and reducing the value stepwise to -0.05 (testing 0.05, 0.02, 0.01, 0.0, -0.01, -0.02, -0.05) with the
intention of obtaining accuracy scores for all these settings. Table 3 shows the scores for our experiments
on the development set for the English and German treebanks. We obtained an optimal trade-off between
score and number of features with a ? of 0.0. With higher thresholds, such as 0.02 or 0.05, the feature
selection algorithm was very restrictive, and resulted in lower accuracy scores. This indicates that there
are several features that are not included that could contribute to a higher accuracy; for instance, in the
German case, we see that the algorithm only selects 41 features. Moreover, the accuracy for English with
a ? of 0.0 is even higher compared with the results obtained when all features were included (cf. last
row: ??). For German, we see a highest accuracy score with threshold of?0.02. We might get the best
accuracy with this threshold when applied to the test set; however, the downside of this threshold is that
the algorithm selected 25 more feature templates, which leads to a slower parser.
Figure 4 illustrates the accuracy gain depending on the number of features included. The development
set of these graphs consist of 20% of the original training set. A negative ? leads to the inclusion of more
features, which seem to provide even slightly higher results while including much more features. This
outcome is not fully supported by the results from the development sets for English where we observed
slightly lower results for a ? of -0.02 compared to 0.0.
To determine the optimal threshold ? for a language would come with a high computational cost, we
carried out these experiments for English and German which show only small differences in accuracy
in the threshold range around 0. Therefore, we adopted 0.0 as threshold for our further experiments on
other languages as well, cf. Table 4.
7
http://maltparser.org
798
87	 ?
89	 ?
91	 ?
0	 ? 25	 ? 50	 ? 75	 ? 100	 ?
0.0	 ? 	 ?-??	 ?0.02	 ?
	 ?0.02	 ? 	 ?-??	 ?0.05	 ?
Figure 4: Selected features (x-axis) vs Labeled Accuracy Score (y-axis). Features: transition-based
English German
LAS UAS POS # LAS UAS POS MOR #
LAS 90.34 91.71 97.04 54 90.89 92.98 97.94 90.59 68
LUMP 90.38 91.57 97.09 55 90.82 92.88 98.11 90.65 53
PMLAS 90.12 91.38 97.02 40 89.27 91.66 98.01 90.66 31
Table 2: Experiments with evaluation metrics with a ? of 0.0 on the development sets. Features:
transition-based. The morphology results are only shown for German, because the English treebank
does not contain separate morphological features.
4.3 Selecting the Best Scoring Method
We carried out a number of experiments to determine the best criterion for the inclusion of features into
the model. We tested several evaluation measures that compute the results of each model, that are LAS
[labeled attachment score], LUMP
8
[(labeled attachment score + unlabeled attachment score + mor-
phology accuracy + part-of-speech accuracy)/4] and PMLAS
9
[labeled attachment score, morphology
accuracy and part-of-speech accuracy]. Table 2 shows the results of the feature selection for English and
German for all these scoring methods. We finally selected LAS as our scoring method given that it pro-
vides the best results for German and competitive results (at the same level) for English. LUMP is very
similar, however, it seems a bit more restrictive than LAS. Moreover, PMLAS was the most restrictive
measure, allowing only 31 features for German and 40 for English, which is the reason why there is a
significant lower accuracy for the models selected with PMLAS.
Finally, it is worth mentioning that we explored an alternative criterion for the inclusion of features
into the set. We explored the possibility to include only features that show a statistical significant im-
provement. However, this criterion is too strict as only very few features showed a statistical significant
improvement on its own.
4.4 Selection of Feature Templates of the Graph-based Completion Model
The graph-based completion model re-scores the beam incrementally and leads to a higher accuracy.
We tried to select the graph-based feature templates of the completion model after the selection of the
8
LMP [(labeled attachment score + morphology accuracy + part-of-speech accuracy)/3] would have been another alternative.
However, we wanted to give the syntax still a higher weight in the feature selection process.
9
See (Bohnet et al., 2013)
799
transition-based feature templates. This approach could not reach the accuracy gain shown by Bohnet
and Kuhn (2012). We attempted to compensate this by starting the selection procedure from the default
set with the intention of maximizing potential accuracy gains. However, this procedure did not lead to a
better accuracy when later combined with the selected transition-based feature templates. We tried also
to relax the threshold to -0.02 in order to include more features and to achieve a higher accuracy. Since
this leads to better results, we performed the feature selection for the graph-based completion model with
this setting.
4.5 Morphology for English
The Penn Treebank is annotated with part-of-speech tags that include morphological features such as
NNS (plural noun) or VBD (verb past tense). The corpus does not include separate morphological features.
Splitting up these features could be useful because: (1) the parser might be able to generalize better when
we use the word categories separated from morphological features, and (2) we might take advantage
of the ability of the parser to predict morphology and part-of-speech based on the interaction with the
syntax. Table 3 summarizes the results. Our transition-based parsing model shows only small differences
between the scores for the original POS tag set and the tag set that separates the category and morphology.
transition-based model
LAS UAS POS MOR POS&MOR
baseline dev 90.13 91.44 ? ? 96.97
separate dev 90.11 91.26 97.66 98.81 97.08
baseline test 92.11 93.16 ? ? 97.41
separate test 92.07 93.09 97.88 97.93 97.35
transition-based model with completion model
baseline test 92.41 93.35 ? ? 97.41
separate test 92.53 93.49 97.85 98.89 97.28
Table 3: Experiments on Penn Treebank with separate representation of word category and morphology.
The results of the transition-based model, including the graph-based model shows some larger differ-
ences
The labeled and unlabeled accuracy scores are not statistically significant and we concluded that (1)
and (2) do not probably hold. Splitting up the morphology is a neutral operation in terms of labeled
and unlabeled accuracy scores; however, it is worth noting that our results with the separate test for the
completion model is more competitive, providing an improvement of 0.14 UAS.
5 Experiments: Feature Selection
We applied the feature selection algorithm with the parameters determined in the previous sections on
the corpora of Chinese, English, German, Hungarian and Russian, and we applied the outcome to parse
the held-out test sets with a beam size of 40 and 25 training iterations. Table 4 shows the accuracy scores
and the number of features selected for each language. The threshold for inclusion of the feature was set
to 0, cf. section 4.
The first row (Full) shows the accuracy scores for the full set of features, that includes all 188 feature
templates of the transition-based feature set. The second row gives the accuracy scores that have been
obtained with the reduced feature set gained by the feature selection algorithm described in Section 3.
For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar-
ian and Russian higher labeled and unlabeled accuracy scores. The scores for German are very similar
to the ones obtained with the full set and the scores for English are slightly worse. In the case of the
transition-based parser with graph-based completion model, the results are the same for Chinese, and
slightly worse for the rest of the languages, with the parser at least twice as fast. It is worth noting that
the number of feature templates is reduced by 2/3 across all languages which leads to a much faster
parsing and training time, thus freeing up a huge amount of main memory.
800
German Hungarian Russian
LAS UAS POS MOR # LAS UAS POS MOR # LAS UAS POS MOR #
Transition-based features
Full 91.39 93.39 97.96 90.36 188 87.67 90.38 97.83 96.39 188 86.73 92.24 98.88 94.66 188
Select 91.34 93.36 97.88 90.48 68 87.94 90.51 97.87 96.38 71 87.21 92.40 98.88 94.74 64
Transition-based and graph-based features
Full+Cmp 91.77 93.63 98.14 90.77 326 88.88 91.33 97.84 96.41 326 87.66 92.84 98.82 94.56 326
Sel+Cmp 91.81 93.72 97.85 90.44 206 88.67 91.16 97.83 96.39 209 87.93 93.01 98.89 94.73 202
Sel+Sel 91.60 93.61 97.85 90.39 91 88.40 90.50 97.86 96.39 97 87.57 92.76 98.88 94.59 75
Chinese English
LAS UAS POS # LAS UAS POS #
Transition-based features
Full 77.81 81.13 94.11 188 92.13 93.18 97.40 188
Select 78.04 81.20 94.17 56 91.89 92.93 97.38 57
Transition-based and graph-based features
Full+Cmp 78.34 81.46 94.19 326 92.41 93.35 97.41 326
Sel+Cmp 78.74 81.86 94.13 197 92.22 93.19 97.37 195
Sel+Sel 78.74 81.77 94.28 67 92.08 93.05 97.44 74
Table 4: Labeled attachment score (LAS), unlabeled attachment score (UAS), part-of-speech accuracy
(POS) and morphology accuracy (MOR) per language and model. The first two rows refer only to
transition-based features while the last two rows include transition-based and graph-based features. Full
refers to a model with all transition-based features. Select refers to a model with selected transition-based
features. Full+Cmp refers to a model with all transition-based features and all graph-based features.
Sel+Cmp refers to a model with selected transition-based features and all graph-based features. Sel+Sel
refers to a model with selected transition-based features and selected graph-based features. The English
and Chinese accuracy scores exclude punctuation marks.
More about parsing time, training time and memory requirements is depicted in Section 6. A compar-
ison with state of the art results as shown in the Tables 5a to 5d reveal that the parser with the selected
features of the transition-based, and graph-based model are on an equal level for Chinese, Russian and
Hungarian with state-of-the-art results. With the selected transition-based and the full graph-based fea-
ture templates, the results for these languages surpass current state-of-the-art results.
6 Time and Memory Requirements
The number of feature templates has a serious impact on training time, parsing time and the amount of
main memory required. The feature selection may have huge impact on the speed of a parser. Therefore,
we measure the actual time and memory usage by applying the parser on the English test set of the Penn
Treebank. This was done with different parsing models, and for each model, test runs were performed
with an increasing number of CPU cores. Figure 6 shows an overview of the results.
The parsing model with all transition- and graph-based features takes on one CPU core 0.085 seconds
per sentence (cf. Figure 6, line with rhombus). In contrast, the parser with selected transition-based
features parses a sentence in less than half of the time (0.042 seconds, line with crosses). The parsing
accuracy is only 0.42 percentage points worse (93.35 vs. 92.93 UAS). When we compare the first parsing
model with the model with selected transition-based and graph-based features, we observe a parsing time
of 0.066 seconds per sentence and a small accuracy difference of only 0.27.
If we use six CPU cores then parsing time decreases drastically to 0.016 seconds per sentence for the
selected transition-based feature model, 0.023 for the selected transition- and graph-based feature model
and to 0.05 seconds per sentence for the model with all features (which is much slower). Our experiments
801
Parser UAS LAS POS
McDonald et al. (2005a) 90.9
McDonald and Pereira (2006) 91.5
Huang and Sagae (2010) 92.1
Koo and Collins (2010) 93.04
Zhang and Nivre (2011) 92.9
Martins et al. (2010) 93.26
Bohnet and Nivre (2012) 93.38 92.44 97.33
this work (sel. trans.& sel. cmpl.) 93.05 92.08 97.44
this work (P&M cf. Table 3) 93.49 92.53 ?
Koo et al. (2008) ? 93.16
Carreras et al. (2008) ? 93.5
Suzuki et al. (2009) ? 93.79
(a) Accuracy scores for WSJ-PTB. Results marked with ? use
additional information sources and are not directly comparable
to the others.
Parser UAS POS
MSTParser1 75.56 93.51
MSTParser2 77.73 93.51
Li et al. (2011) 3rd-order 80.60 92.80
Hatori et al. (2011) HS 79.60 94.01
Hatori et al. (2011) ZN 81.20 93.94
this work (sel. trans.) 81.20 94.17
this work (sel. trans.+ sel. cmp.) 81.77 94.28
(b) Accuracy scores for the Chinese treebank converted with
the head rules of Zhang and Clark (2008). MSTParser results
from Li et al. (2011). UAS scores from Li et al. (2011) and Ha-
tori et al. (2011) recalculated from the separate accuracy scores
for root words and non-root words.
Parser UAS LAS POS
Farkas et al. (2012) 90.1 87.2
Bohnet et al. (2013) 91.3 88.9 98.1
this work (sel. trans. & sel. cmpl.) 90.50 88.40 97.83
this work (sel. trans. & full cmpl.) 91.16 88.67 97.86
(c) State of the art comparison for Hungarian. The table shows
that we can reach state of the art performance with less features.
Parser UAS LAS POS
Boguslavsky et al. (2011) 90.0 86.0
Bohnet et al. (2013) 92.8 87.6 98.5
this work (sel. trans. & sel. cmp.) 92.76 87.57 98.89
this work (sel. trans. & full cmp.) 93.01 87.93 98.88
(d) State of the art comparison for Russian.
Figure 5: Comparison with state of the art results.
0	 ?
0,025	 ?
0,05	 ?
0,075	 ?
0,1	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ?
seco
nds	 ? 	 ?
CPU	 ?Cores	 ?
(1)	 ?all	 ?graph	 ?&	 ?tr.	 ?features	 ?	 ?
(2)	 ?selected	 ?tr.	 ?&	 ?all.	 ?graph	 ?features	 ?
(3)	 ?selected	 ?graph	 ?&	 ?tr.	 ?features	 ?
(4	 ?)selected	 ?tr.	 ?features	 ?
id trans. features graph features # features UAS
(1) all all 75.8 M 93.35
(2) selected all 43.5 M 93.22
(3) selected selected 22.1 M 93.08
(4) selected none 17.8 M 92.93
Figure 6: Parsing Time in relation to CPU cores and number of features in the hash kernel in millions.
demonstrate that we can double the parsing speed and maintain a very high parsing accuracy.
7 Conclusions
In this paper, we have presented the first feature selection algorithm for agenda-based dependency pars-
ing. Our algorithm could be directly used out of the box,
10
and applied to a new data set or language to
get an optimized feature model for a agenda-based parser such as the Mate tools.
11
Our feature selection algorithm provides models with even higher accuracy for Chinese and Russian,
cf.Table 4. For the remaining languages the models provide accuracy scores that are comparable to
the ones obtained by models including a larger set of feature templates. For all languages, the feature
models gained via feature selection are faster and require less memory, which make them very useful
for practical applications. We conclude that feature models obtained with the feature selection algorithm
10
The source code and the feature models found for each language are available at https://code.google.com/p/
mate-tools/
11
https://code.google.com/p/mate-tools/wiki/ParserAndModels
802
often provide a comparable accuracy level while they are considerable faster. Finally, our model for
English with the separated morphology tag-set provides one of the best results reported with 93.49 UAS.
Additionally, the feature selection algorithms for this setting shows competitive results with a largely
reduced number of feature templates, and thus less parsing time and lower memory requirements. The
parser is faster (almost double) and provides 93.05 UAS which is also among the best results.
Acknowledgments
This research project was supported by funding of the European Union (PCIG13-GA-2013-618143).
References
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and Joakim Nivre. 2011. Improving Dependency Parsing
with Semantic Classes. In The 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL), pages 699?703, Portland, USA.
Miguel Ballesteros and Joakim Nivre. 2014. MaltOptimizer: Fast and Effective Parser Optimization. Natural
Language Engineering.
Miguel Ballesteros. 2013. Effective morphological feature selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,
pages 53?60.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grigoriev, Leonid Kreidlin, and Nadezhda Frid. 2000. Depen-
dency treebank for Russian: Concept, tools, types of information. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING), pages 987?991.
Igor Boguslavsky, Ivan Chardin, Svetlana Grigorieva, Nikolai Grigoriev, Leonid Iomdin, Leonid Kreidlin, and
Nadezhda Frid. 2002. Development of a dependency treebank for Russian and its possible applications in NLP.
In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages
852?856.
Igor Boguslavsky, Leonid Iomdin, Victor Sizov, Leonid Tsinman, and Vadim Petrochenkov. 2011. Rule-based
dependency parser refined by empirical and corpus statistics. In Proceedings of the International Conference
on Dependency Linguistics, pages 318?327.
Bernd Bohnet and Jonas Kuhn. 2012. The best of bothworlds - a graph-based completion model for transition-
based parsers. In Proceedings of the 15th Conference of the European Chapter of the Association for Computa-
tional Linguistics (EACL), pages 77?87.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1455?1465, Jeju Island,
Korea, July. Association for Computational Linguistics.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich?ard Farkas, Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Association for Computational
Linguistics(TACL), 1:415?428.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. TIGER treebank. In
Proceedings of the 1st Workshop on Treebanks and Linguistic Theories (TLT), pages 24?42.
Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language
Learning (CoNLL), pages 9?16.
Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the
CoNLL Shared Task at the 2007 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-CONLL), pages 957?961.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the
42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 112?119.
803
Rich?ard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 15th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 55?65.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ulsen Eryi?git, Be?ata Megyesi, Mattias Nilsson, and Markus Saers. 2007.
Single Malt or Blended? A Study in Multilingual Parser Optimization. In Proceedings of the CoNLL Shared
Task at the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CONLL), pages 933?939.
Johan Hall. 2008. Transition-Based Natural Language Parsing with Dependency and Constituency Representa-
tions. Ph.D. thesis, V?axj?o University.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint pos tagging and
dependency parsing in chinese. pages 1216?1224.
He He, Hal Daum?e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In EMNLP,
pages 1455?1464.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1077?1086.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics (ACL), pages 1?11.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 595?603.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for
chinese POS tagging and dependency parsing. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1180?1191.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo parsers: Dependency
parsing by approximate variational inference. In Proceedings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 34?44.
Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL), pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),
pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji?c. 2005b. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the Human Language Technology Conference and the Conference
on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 523?530.
Peter Nilsson and Pierre Nugues. 2010. Automatic Discovery of Feature Sets for Dependency Parsing. In Pro-
ceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 824?832.
Joakim Nivre and Johan Hall. 2010. A quick guide to MaltParser optimization. Technical report, maltparser.org.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsen Eryi?git, and Svetoslav Marinov. 2006. Labeled pseudo-projective
dependency parsing with support vector machines. In Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 221?225.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13:95?135.
Joakim Nivre. 2006. Inductive Dependency Parsing. Springer.
Wolfgang Seeker and Jonas Kuhn. 2012. Making ellipses explicit in dependency conversion for a german treebank.
pages 3132?3139.
Noah A. Smith. 2011. Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies.
Morgan and Claypool, May.
804
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 551?560.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 562?571.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 188?193,
Portland, Oregon, USA.
805
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1402?1413, Dublin, Ireland, August 23-29 2014.
Deep-Syntactic Parsing
Miguel Ballesteros
1
, Bernd Bohnet
2
, Simon Mille
1
, Leo Wanner
1,3
1
Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain
2
School of Computer Science, University of Birmingham, United Kingdom
3
Catalan Institute for Research and Advanced Studies (ICREA)
1,3
{name.lastname}@upf.edu
2
bohnetb@cs.bham.ac.uk
Abstract
?Deep-syntactic? dependency structures that capture the argumentative, attributive and coordi-
native relations between full words of a sentence have a great potential for a number of NLP-
applications. The abstraction degree of these structures is in-between the output of a syntactic
dependency parser (connected trees defined over all words of a sentence and language-specific
grammatical functions) and the output of a semantic parser (forests of trees defined over indi-
vidual lexemes or phrasal chunks and abstract semantic role labels which capture the argument
structure of predicative elements, dropping all attributive and coordinative dependencies). We
propose a parser that delivers deep syntactic structures as output.
1 Introduction
Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per
force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions
and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and
Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simpli-
fication, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the
source and the target structures. Therefore, the use of more abstract ?syntactico-semantic? structures
seems more appropriate. Following Mel??cuk (1988), we call these structures deep-syntactic structures
(DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic
Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to ab-
stract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame
stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attribu-
tive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank
and Semantic Frame structures are not always connected, may contain either individual lexical items or
phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or
sentential). In other words, they constitute incomplete structures that drop not only idiosyncratic, func-
tional but also meaningful elements of a given sentence and often contain dependencies between chunks
rather than individual tokens. Therefore, we propose to put on the research agenda the task of deep-
syntactic parsing and show how a DSyntS is obtained from a SSynt dependency parse using data-driven
tree transduction in a pipeline with a syntactic parser.
1
In Section 2, we introduce SSyntSs and DSyntSs
and discuss the fundamentals of SSyntS?DSyntS transduction. Section 3 describes the experiments that
we carried out on Spanish material, and Section 4 discusses their outcome. Section 5 summarizes the
related work, before in Section 6 some conclusions and plans for future work are presented.
2 Fundamentals of SSyntS?DSyntS transduction
Before we set out to discuss the principles of the SSyntS?DSynt transduction, we must specify the
DSyntSs and SSyntSs as used in our experiments.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The term ?tree transduction? is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension
of finite state transduction (Aho, 1972) to trees.
1402
2.1 Defining SSyntS and DSyntS
SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value
structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels.
The features of the node labels in SSyntSs are lex
ssynt
, and ?syntactic grammemes? of the value of
lex
ssynt
, i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for
verbs. The value of lex
ssynt
can be any (either full or functional) lexical item; in graphical representations
of SSyntSs, usually only the value of lex
ssynt
is shown. The edge labels of a SSyntS are grammatical
functions ?subj?, ?dobj?, ?det?, ?modif?, etc. In other words, SSyntSs are syntactic structures of the kind
as encountered in the standard dependency treebanks; cf., e.g., dependency version of the Penn TreeBank
(Johansson and Nugues, 2007) for English, Prague Dependency Treebank for Czech (Haji?c et al., 2006),
Ancora for Spanish (Taul?e et al., 2008), Copenhagen Dependency Treebank for Danish (Buch-Kromann,
2003), etc. In formal terms that we need for the outline of the transduction below, a SSyntS is defined as
follows:
Definition 1 (SSyntS) An SSyntS of a language L is a quintuple T
SS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over all lexical items L of L, the set of syntactic grammemes G
synt
, and the set of grammatical
functions R
gr
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L,
? ?
r
s
?a
assigns to each a ? A an r ? R
gr
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
synt
.
The features of the node labels in DSyntSs as worked with in this paper are lex
dsynt
and ?seman-
tic grammemes? of the value of lex
dsynt
, i.e., number and determination for nouns and tense, aspect,
mood and voice for verbs.
2
In contrast to lex
ssynt
in SSyntS, DSyntS?s lex
dsynt
can be any full, but
not a functional lexeme. In accordance with this restriction, in the case of look after a person, AFTER
will not appear in the corresponding DSyntS; it is a functional (or governed) preposition (so are TO or
BY, in Figure 1).
3
In contrast, AFTER in leave after the meeting is a full lexeme; it will remain in the
DSyntS because there it has its own meaning of ?succession in time?. The edge labels of a DSyntS are
language-independent ?deep-syntactic? relations I,. . . ,VI, ATTR, COORD, APPEND. ?I?,. . . ,?VI? are
argument relations, analogous to A0, A1, etc. in the PropBank annotation. ?ATTR? subsumes all (cir-
cumstantial) ARGM-x PropBank relations as well as the modifier relations not captured by the PropBank
and FrameNet annotations. ?COORD? is the coordinative relation as in: John-COORD?and-II?Mary,
publish-COORD?or-II?perish, and so on. APPEND subsumes all parentheticals, interjections, direct
addresses, etc., as, e.g., in Listen, John!: listen-APPEND?John. DSyntSs thus show a strong similarity
with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated;
(ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique
ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv)
they are connected.
4
A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al.,
2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows:
Definition 2 (DSyntS) An DSyntS of a language L is a quintuple T
DS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over the full lexical items L
d
of L, the set of semantic grammemes G
sem
, and the set of deep-
syntactic relations R
dsynt
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L
d
,
? ?
r
s
?a
assigns to each a ? A an r ? R
dsynt
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
sem
.
Consider in Figure 1 an example for an SSyntS and its corresponding DSyntS.
2
Most of the grammemes have a semantic and a surface interpretation; see (Mel??cuk, 2013).
3
Functional lexemes also include auxiliaries (e.g. HAVE, or BE when it is not a copula), and definite and indefinite deter-
miners (THE, A); see Figure 1).
4
Our DSyntSs are thus DSyntSs as used in the Meaning-Text Theory (Mel??cuk, 1988), only that our DSyntSs do not
disambiguate lexical items and do not use lexical functions (Mel??cuk, 1996).
1403
(a) almost 1.2 million jobs have been created by the state thanks to their endeavours
restr
quant
quant
subj
analyt perf
analyt pass
agent
adv
prepos
det
obl obj
prepos
det
(b) almost 1.2 million job create state thanks their endeavour
ATTR
ATTR
ATTR
II
I
ATTR
II
I
Figure 1: An SSyntS (a) and its corresponding DSyntS (b)
2.2 Fleshing out the SSyntS?DSyntS transduction
It is clear that the SSyntS and DSyntS of the same sentence are not isomorphic. The following corre-
spondences between the SSyntS S
ss
and DSyntS S
ds
of a sentence need to be taken into account during
SSyntS?DSyntS transduction:
(i) a node in S
ss
is a node in S
ds
;
(ii) a relation in S
ss
corresponds to a relation in S
ds
;
(iii) a fragment of the S
ss
tree corresponds to a single node in S
ds
;
(iv) a relation with a dependent node in S
ss
is a grammeme in S
ds
;
(v) a grammeme in S
ss
is a grammeme in S
ds
;
(vi) a node in S
ss
is conflated with another node in S
ds
; and
(vii) a node in S
ds
has no correspondence in S
ss
.
The grammeme correspondences (iv) and (v) and the ?pseudo? correspondences in (vi) and (vii)
5
are
few or idiosyncratic and are best handled in a rule-based post-processing stage. The main task of the
SSyntS?DSyntS transducer is thus to cope with the correspondences (i)?(iii). For this purpose, we can
view both SSyntS and DSyntS as vectors indexed in terms of two-dimensional matrices I = N ?N (N
being the set of nodes of a given tree 1, . . . ,m), with I(i, j) = ?(n
i
, n
j
), if n
i
, n
j
? N and (n
i
, n
j
) ? A
and I(i, j) = 0 otherwise (where ??(n
i
, n
j
)? is the function that assigns to an edge a relation label and
i, j = 1, . . . ,m; i 6= j are nodes of the tree). That is, for a given SSyntS, the matrix I(i, j) contains in
the cells (i, j), i, j = 1, . . . ,m, the names of the SSynt-relations between the nodes n
i
and n
j
, and ?0?
otherwise, while for a given DSyntS, the cells of its matrix I
D
contain DSyntS-relations.
Starting from the matrix I
S
of a given SSyntS, the task is therefore to obtain the matrix I
D
of the
corresponding DSyntS, that is, to identify correspondences between i/j, (i, j) and groups of (i, j) of
I
S
with i
?
/j
?
and (i
?
, j
?
) of I
D
; see (i)?(iii) above. In other words, the task consists in identifying and
removing all functional lexemes, and attach correctly the remaining nodes between them.
6
As a ?token chain?surface-syntactic tree? projection, this task can be viewed as a classification task.
However, while the former is isomorphic, we know that the SSyntS?DSyntS projection is not. In order
to approach the task to an isomorphic projection (and thus simplify its modelling), it is convenient to
interpret SSyntS and the targeted DSyntS as collections of hypernodes:
Definition 3 (Hypernode) Given a SSyntS S
s
with its index matrix I
S
(a DSyntS S
d
with its index matrix
I
D
), a node partition p (with |p |? 1) of I
S
(I
D
) is a hypernode h
s
i
(h
d
i
) iff p corresponds to a partition
p
?
(with |p
?
|? 1) of S
d
(S
s
).
In this way, the SSyntS?DSyntS correspondence boils down to a correspondence between individual
hypernodes and between individual arcs, and the transduction embraces the following three (classifica-
tion) subtasks: 1. Hypernode identification, 2. DSynt tree construction, and 3. DSynt arc labeling, which
are completed by a post-processing stage.
5
(vi) covers, e.g., reflexive verb particles such as se in Spanish, which are conflated in the DSyntS with the verb:
se?aux refl dir-conocer vs. CONOCERSE ?know each other?; (vii) covers, e.g., the zero subject in pro-drop languages (which
is absent in the SSyntS and present in the DSyntS).
6
What is particularly challenging is the identification of functional prepositions: based on the information found in the
corpus only, our system must decide if a given preposition is a full or a functional lexeme. That is, we do not resort to any
external lexical resources.
1404
1. Hypernode identification. The hypernode identification consists of a binary classification of the
nodes of a given SSyntS as nodes that form a hypernode of cardinality 1 (i.e., nodes that have a one-
to-one correspondence to a node in the DSyntS) vs. nodes that form part of a hypernode of cardinality
> 1. In practice, hypernodes of type one will be formed by: 1) noun nodes that do not govern determiner
or functional preposition nodes, 2) full verb nodes that are not governed by any auxiliary verb nodes
and that do not govern any functional preposition node, adjective nodes, adverbial nodes, and semantic
preposition nodes. Hypernodes of type two will be formed by: 1) noun nodes + determiner / func-
tional preposition nodes they govern, 2) verb nodes + auxiliary nodes they are governed by + functional
preposition nodes they govern.
2. DSynt tree reconstruction. The outcome of the hypernode identification stage is thus the set H
s
=
H
s
|p|=1
?H
s
|p|>1
of hypernodes of two types. With this set at hand, we can define an isomorphy function
? : H
s
? H
d
|p|=1
(with h
d
? H
d
|p|=1
consisting of n
d
? N
ds
, i.e., the set of nodes of the target DSyntS).
? is the identity function for h
s
? H
s
|p|=1
. For h
s
? H
s
|p|>1
, ? maps the functional nodes in h
s
onto
grammemes (attribute-value pairs) of the lexically meaningful node in h
d
and identifies the lexically
meaningful node as head. Some of the dependencies of the obtained nodes n
d
? N
ds
can be recovered
from the dependencies of their sources. Due to the projection of functional nodes to grammemes (which
can be also seen as node removal), some dependencies will be also missing and must be introduced.
Algorithm 1 recalculates the dependencies for the target DSyntS S
d
, starting from the index matrix I
S
of
SSyntS S
s
to obtain a connected tree.
Algorithm 1: DSyntS tree reconstruction
for ?n
i
? N
d
do
if ?n
j
: (n
j
, n
i
) ? S
s
? ?(n
j
) ? N
d
then
(n
j
, n
i
)? S
d
// the equivalent of the head node of n
i
is included in DSyntS
else if ?n
j
, n
a
: (n
j
, n
i
) ? S
s
? ?(n
j
) 6? N
d
?
?(n
a
) ? N
d
then
//n
a
is the first ancestor of n
j
that has an equivalent in DSyntS
//the equivalent of the head node of n
i
is not included in DSyntS, but the ancestor n
a
is
(n
a
, n
i
)? S
d
else
//the equivalent of the head node of n
i
is not included in DSyntS, but several ancestors of it are
n
b
:= BestHead(n
i
, S
s
, S
d
)
(n
b
, n
i
)? S
d
endfor
BestHead recursively ascends S
s
from a given node n
i
until it encounters one or several head nodes
n
d
? N
ds
. In case of several encountered head nodes, the one which governs the highest frequency
dependency is returned.
3. Label Classification. The tree reconstruction stage produces a ?hybrid? connected dependency tree
S
s?d
with DSynt nodes N
ds
, and arcs A
s
labelled by SSynt relation labels, i.e., an index matrix we
can denote as I
?
, whose cells (i, j) contain SSynt labels for all n
i
, n
j
? N
ds
: (n
i
, n
j
) ? A
s
and
?0? otherwise. The next and last stage of SSynt-to-DSyntS transduction is thus the projection of SSynt
relation labels of S
s?d
to their corresponding DSynt labels, or, in other words, the mapping of I
?
to I
D
of the target DSyntS.
4. Postprocessing. As mentioned in Section 2, there is a limited number of idiosyncratic correspon-
dences between elements of SSyntS and DSyntS (the correspondences (iv?vii) which can be straight-
forwardly handled by a rule-based postprocessor because (a) they are non-ambiguous, i.e., a ? b, c ?
d ? a = b ? c = d, and (b) they are few. Thus, only determiners and auxiliaries in SSyntS map onto a
grammeme in DSyntS, both SSyntS and DSyntS count with less than a dozen grammemes, etc.
3 Experiments
In order to validate the outlined SSyntS?DSyntS transduction and to assess its performance in combi-
nation with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of
1405
experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2.
JointPoS TaggerSSynt parser
SSynt?DSyntTransducerPlainSentences
DSyntTreebankSSyntTreebank
SSyntS DSynS
Figure 2: Setup of a deep-syntactic parser
For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et
al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS
treebank the semantically and information structure influenced relation tags to obtain an annotation gran-
ularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)).
Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and
86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641
tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank).
To obtain the SSyntS, we use Bohnet and Nivre (2012)?s transition-based parser, which combines
lemmatization, PoS tagging, and syntactic dependency parsing?tuned and trained on the respective sets
of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set.
POS LEMMA LAS UAS
96.14 91.10 78.64 86.49
Table 1: Results of Bohnet and Nivre?s surface-syntactic parser on the development set
In what follows, we first present the realization of the SSyntS?DSyntS transducer and then the real-
ization of the baseline.
3.1 SSyntS?DSyntS transducer
As outlined in Section 2.2, the SSyntS?DSyntS transducer is composed of three submodules and a post-
processing stage:
1. Hypernode identification. For the hypernode identification, we trained a binary polynomial (degree
2) SVM from LIBSVM (Chang and Lin, 2001). The SVM allows both features related to the processed
node and higher-order features, which can be related to the head node of the processed node or to its
sibling nodes. After several feature selection trials, we chose the following features for each node n:
? lemma or stem of the label of n,
? label of the relation between n and its head,
? surface PoS of n?s label (the SSynt and DSyntS treebanks distinguish between surface and deep
PoS),
? label of the relation between n?s head to its own head,
? surface PoS of the label of n?s head node.
After an optimization round of the parameters available in the SVM implementation, the hypernode
identification achieved over the gold development set 99.78% precision and 99.02% recall (and thus
99.4% F1). That is, only very few hypernodes are not identified correctly. The main error source are
governed prepositions: the classifier has to learn when to assign a preposition an own hypernode (i.e.,
when it is lexically meaningful) and when it should be included into the hypernode of the governor (i.e.,
when it is functional). Our interpretation is that the features we use for this task are appropriate, but
that the training data set is too small. As a result, some prepositions are erroneously left out from or
introduced into the DSyntS.
1406
2. Tree reconstruction. The implementation of the tree reconstruction module shows an unlabelled
dependency attachment precision of 98.18% and an unlabelled dependency attachment recall of 97.43%
over the gold development set. Most of the errors produced by this module have their origin in the
previous module, i.e., hypernode identification. When a node has been incorrectly removed, the module
errs in the attachment because it cannot use the node in question as the destination or the origin of a
dependency, as it is the case in the gold-standard annotation:
Gold-standard: ser como e?ne
be like letter-n
II
II
Predicted: ser e?ne
II
When a node has erroneously not been removed, no dependencies between its governor and its depen-
dent can be established since DSyntS must remain a tree (which gives the same LAS and UAS errors as
when a node has been erroneously removed):
Gold-standard: y Michael Jackson
II
Predicted: y a Michael Jackson
and to Michael Jackson
II
II
3. Relation label classification. For relation label classification, we use a multiclass linear SVM. The
label classification depends on the concrete annotation schemata of the SSyntS and DSyntS treebanks
on which the parser is trained. Depending on the schemata, some DSynt relation labels may be easier to
derive from the original SSyntS relation labels than others. Table 2 lists all SSynt relation labels that have
a straightforward mapping to DSyntS relation labels in the used treebanks, i.e., neither their dependent
nor their governor are removed, and the SSyntS label always maps to the same DSynt label.
SSynt DSynt
abbrev ATTR
abs pred ATTR
adv ATTR
adv mod ATTR
agent I
appos ATTR
attr ATTR
aux phras ?
aux refl dir II
SSynt DSynt
aux refl indir III
bin junct ATTR
compl1 II
compl2 III
compl adnom ATTR
coord COORD
copul II
copul clitic II
copul quot II
SSynt DSynt
dobj clitic II
dobj quot II
elect ATTR
juxtapos APPEND
modal II
modif ATTR
num junct COORD
obj copred ATTR
prepos II
SSynt DSynt
prepos quot II
prolep APPEND
quant ATTR
quasi coord COORD
quasi subj I
relat ATTR
restr ATTR
sequent ATTR
subj I
subj copred ATTR
Table 2: Straightforward SSynt to DSyntS mappings
Table 3 shows SSyntS relation?DSyntS relation label correspondences that are not straightforward.
SSynt DepRel
A
Mapping to DSynt
analyt fut remove Gov and Dep; add tense=FUT
analyt pass remove Gov; invert I and II; add voice=PASS
analyt perf remove Gov; add tense=PAST
analyt progr remove Gov; add tem constituency=PROGR
aux refl lex remove Dep; add se at the end of Gov?s lemma
aux refl pass remove Dep; invert I and II; add voice=PASS
compar remove Dep if conjunction
compar /coord /sub conj remove Dep if governed preposition
det
IF Dep=el?un THEN remove Dep; add definiteness=DEF/INDEF
IF Dep=possessive THEN DepRel ATTR?I?II?III
IF Dep=other THEN DepRel ATTR
dobj remove Dep if governed preposition
iobj remove Dep if governed preposition; DepRel II?III?IV?V?VI
iobj clitic DepRel II?III?IV?V?VI
obl compl remove Dep if governed preposition; DepRel I?II?III?IV?V?VI
obl obj remove Dep if governed preposition; DepRel II?III?IV?V?VI
punc ?
punc init ?
Table 3: Complex SSynt to DSynt mappings
1407
The final set of features selected for label classification includes: (i) lemma of the dependent node, (ii)
dependency relation to the head of the dependent node, (iii) dependency relation label of the head node
to its own head, (iv) dependency relation to the head of the sibling nodes of the dependent node, if any.
After an optimization round of the parameter set of the SVM-model, relation labelling achieved
94.00% label precision and 93.28% label recall on the development set. The recall is calculated con-
sidering all the nodes that are included in the gold standard. The error sources for relation labelling
were mostly the dependencies that involved possessives and the various types of objects (see Table
3) due to their differing valency. For instance, the relation det in su?det?coche ?his/her car? and
su?det?llamada ?his/her phone call? have different correspondences in DSyntS: su?ATTR?coche
vs. su?I?llamada. That is, the DSyntS relation depends on the lexical properties of the governor.
7
Once again, more training data is needed in order to classify better those cases.
4. Postprocessing In the postprocessing stage for Spanish, the following rules capture non-ambiguous
correspondences between elements of the SSynt-index matrix I
S
= N
s
?N
s
and DSyntS index matrix
I
D
= N
d
?N
d
, with n
s
? N
s
and n
d
? N
d
, and n
s
and n
d
corresponding to each other (we do not list
here identity correspondences such as between the number grammemes of n
s
and n
d
):
? if n
s
is dependent of analyt pass or analyt refl pass relation, then the voice grammeme in n
d
is
PASS;
? if n
s
is dependent of analyt progr, then the voice grammeme in n
d
is PROGR;
? if n
s
is dependent of analyt refl lex, then add the particle -SE as suffix of node label (word) of d
d
;
? if any of the children of n
s
is labelled by one of the tokens UN ?a
masc
?, UNA ?a
fem
?, UNOS
?some
masc
? or UNAS ?some
fem
?, then the definiteness grammeme in n
d
INDEF, otherwise it is
DEF;
? if the n
s
label is a finite verb and n
s
does not govern a subject relation, then add to I
?
the relation
n
d
? I?n
?
d
, with n
?
d
being a newly introduced node.
3.2 Baseline
As point of reference for the evaluation of the performance of our SSyntS?DSyntS transducer, we use a
rule-based baseline that carries out the most direct transformations extracted from Tables 2 and 3. The
baseline detects hypernodes by directly removing all the nodes that we are sure need to be removed, i.e.
punctuation and auxiliaries. The nodes that are only potentially to be removed, i.e., all dependents of
DepRels that have a possibly governed preposition or conjunction in Table 3, are left in the DSyntS. The
new relation labels in the DSyntS are obtained by selecting the label that is most likely to substitute the
SSyntS relation label according to classical grammar studies. The rules of the rule-based baseline look
as follows:
1 if (deprel==abbrev) then deep deprel=ATTR
2 if (deprel==obl obj) then deep deprel=II
. . .
n if (deprel==punc) then remove(current node)
4 Results and Discussion
Let us look in this section at the performance figures of the SSyntS parser, the SSyntS?DSyntS trans-
ducer, and the sentence?DSyntS pipeline obtained in the experiments.
4.1 SSyntS?DSyntS transducer results
In Table 4, the performance of the subtasks of the SSyntS?DSyntS transducer is contrasted to the per-
formance of the baselines; the evaluation of the postprocessing subtask is not included because the one-
to-one projection of SSyntS elements to DSyntS guarantees an accuracy of 100% of the operations
performed. The transducer has been applied to the gold standard test set, which is the held-out test set,
with gold standard PoS tags, lemmas and dependency trees. It outputs in total 5610 nodes; the rule-based
baseline outputs 8653 nodes. As mentioned in Section 3, our gold standard includes 5641 nodes.
7
Note that lexemes are not generalized: a verb and its corresponding noun (e.g., construct/construction) are considered
distinct lexemes.
1408
Hyper-Node Detection
Measure Rule-based Baseline Tree Transducer
p 64.31 (5565/8653) 99.79 (5598/5610)
r 98.65 (5565/5641) 99.24 (5598/5641)
F1 77.86 99.51
Attachment and Labelling
Measure Rule-based Baseline Tree Transducer
LAP 50.02 (4328/8653) 91.07 (5109/5610)
UAP 53.05 (4590/8653) 98.32 (5516/5610)
LA-P 57.66 (4989/8653) 92.37 (5182/5610)
LAR 76.72 (4328/5641) 90.57 (5109/5641)
UAR 81.37 (4590/5641) 97.78 (5516/5641)
LA-R 88.44 (4989/5641) 91.86 (5182/5641)
Table 4: Performance of the SSyntS?DSyntS transducer and of the rule-based baseline over the gold-
standard held-out test set (LAP: labelled attachment precision, UAP: unlabelled attachment precision, LA-P: label assign-
ment precision, LAR: labelled attachment recall, UAR: Unlabelled attachment recall and LA-R: Label assignment recall)
Our data-driven SSyntS?DSyntS transducer is much better than the baseline with respect to all eval-
uation measures.
8
The transducer relies on distributional patterns identified in the training data set, and
makes thus use of information that is not available for the rule-based baseline, which studies one node
at a time. However, the rule-based baseline results also show that transduction that would remove a few
nodes would provide results close to a 100% recall for the hypernode detection because a DSynt tree is a
subtree of the SSynt tree (if we ignore the nodes introduced by post-processing). This is also evidenced
by the labeled and attachment recall scores. The results of the transducer on the test and development
sets are quite comparable. The hypernode detection is even better on the test set. The label accuracy
suffers most from using unseen data during the development of the system. The attachment figures are
approximately equivalent on both sets.
4.2 Results of deep-syntactic parsing
Let us consider now the performance of the complete DSynt parsing pipeline (PoS-tagger+surface-
dependency parser? SSyntS?DSyntS transducer) on the held-out test set. Table 5 displays the figures
of the Bohnet and Nivre parser. The figures are in line with the performance of state-of-the-art parsers
for Spanish (Mille et al., 2012).
POS LEMMA LAS UAS
96.05 92.10 81.45 88.09
Table 5: Performance of Bohnet and Nivre?s joint PoS-tagger+dependency parser trained on Ancora-UPF
Table 6 shows the performance of the pipeline when we feed the output of the syntactic parser to the
rule-based baseline SSyntS?DSyntS module and the tree transducer. We observe a clear error propaga-
tion from the dependency parser (which provides 81.45% LAS) to the SSyntS?DSyntS transducer, which
loses in tree quality more than 18%.
Hyper-Node Detection
Measure Baseline Tree Transducer
p 63.87 (5528/8655) 97.07 (5391/5554)
r 98.00 (5528/5641) 95.57 (5391/5641)
F1 77.33 96.31
Labelling and Attachment
Measure Baseline Tree Transducer
LAP 38.75 (3354/8655) 68.31 (3794/5554)
UAP 44.69 (3868/8655) 77.31 (4294/5554)
LA-P 49.66 (4298/8655) 80.47 (4469/5554)
LAR 59.46 (3354/5641) 67.26 (3794/5641)
UAR 68.57 (3868/5641) 76.12 (4294/5641)
LA-R 76.19 (4298/5641) 79.22 (4469/5641)
Table 6: Performance of the deep-syntactic parsing pipeline
5 Related Work
To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As
semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented
structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received
considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007
8
We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too
weak to be used as baseline.
1409
(Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and
semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Haji?c et al., 2009).
The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued
with predicate identification, argument identification, argument labeling, and word sense disambigua-
tion; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly
all arguments to select the best combination was applied. Some of the systems were based on integrated
syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu??s et al.,
2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform
structural changes?as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS.
Klime?s (2006)?s parser removes nodes (producing tectogrammatical structures as in the Prague Depen-
dency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier
works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997).
However, this is not to say that the idea of the surface?surface syntax?deep syntax pipeline is new.
It goes back at least to Curry (1961) and is implemented in a number of more recent works; see, e.g., (de
Groote, 2001; Klime?s, 2006; Bojar et al., 2008).
6 Conclusions and Future Work
We have presented a deep-syntactic parsing pipeline which consists of a state-of-the-art dependency
parser and a novel SSyntS?DSyntS transducer. The obtained DSyntSs can be used in different applica-
tions since they abstract from language-specific grammatical idiosyncrasies of the SSynt structures as
produced by state-of-the art dependency parsers, but still avoid the complexities of genuine semantic
analysis.
9
DSyntS-treebanks needed for data-driven applications can be bootstrapped by the pipeline.
If required, a SSyntS?DSyntS structure pair can be also mapped to a pure predicate-argument graph
such as the DELPH-IN structure (Oepen, 2002) or to an approximation thereof (as the Enju conversion
(Miyao, 2006), which keeps functional nodes), to an DRS (Kamp and Reyle, 1993), or to a PropBank
structure. On the other hand, DSyntS-treebanks can be used for automatic extraction of deep grammars.
As shown by Cahill et al. (2008), automatically obtained resources can be of an even better quality than
manually-crafted resources. In this context, especially research in the context of CCGs (Hockenmeier,
2003; Clark and Curran, 2007) and TAGs (Xia, 1999) should be also mentioned.
To validate our approach with languages other than Spanish, we carried out an experiment on a Chi-
nese SSyntS-DSyntS Treebank (training the DSynt-transducer on the outcome of the SSynt-parser). The
results over predicted input showed an accuracy of about 75%, i.e., an accuracy comparable to the accu-
racy achieved for Spanish. We are also investigating multilingual approaches, such as the one proposed
by McDonald et al. (2013).
In the future, we will carry out further in-depth feature engineering for the task of DSynt-parsing. It
proved to be crucial in semantic role labelling and dependency parsing (Che et al., 2009; Ballesteros and
Nivre, 2012); we expect it be essential for our task as well. Furthermore, we will join surface syntactic
and deep-syntactic parsing we kept so far separate; see, e.g., (Zhang and Clark, 2008; Llu??s et al., 2013;
Bohnet and Nivre, 2012) for analogous proposals. Further research is required here since although joint
models avoid error propagation from the first stage to the second, overall, pipelined models still proved
to be competitive; cf. the outcome of CoNLL shared tasks.
The deep-syntactic parser described in this paper is available for downloading at https://code.
google.com/p/deepsyntacticparsing/.
Acknowledgements
This work has been supported by the European Commission under the contract number FP7-ICT-610411.
Many thanks to the three anonymous COLING reviewers for their very helpful comments and sugges-
tions.
9
The motivation to work with DSyntS instead of SSyntS is thus similiar to the motivation of the authors of the Abstract
Meaning Representation (AMR) for Machine Translation (Banarescu et al., 2013), only that AMRs are considerably more
semantic than DSyntSs.
1410
References
Alfred V. Aho. 1972. The theory of parsing, translation and, compiling. Prentice Hall, Upper Saddle River, NJ.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOptimizer: A System for MaltParser Optimization. In Proceed-
ings of the Eighth International Conference on Language Resources and Evaluation (LREC 12).
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop & Interoperability with Discourse, pages 178?186, Sofia, Bulgaria.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In EMNLP-CoNLL.
O. Bojar, S. Cinkov?a, and J. Pt?a?cek. 2008. Towards English-to-Czech MT via Tectogrammatical Layer. The
Prague Bulletin of Mathematical Linguistics, 90:57?68.
Mathias Buch-Kromann. 2003. The Danish dependency treebank and the dtag treebank tool. In 2nd Workshop on
Treebanks and Linguistic Theories (TLT), Sweden, pages 217?220.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Genabith, and Andy Way. 2008. Wide-
coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics,
34(1):81?124.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: A Library for Support Vector Machines. Software available
at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL 2009): Shared Task, pages 49?54, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
R. Curry. 1961. Some logical aspects of grammatical structure. In R. Jakobson, editor, Structure of Language and
Its Mathematical Aspects, pages 56?68. American Mathematical Society, Providence, RI.
Ph. de Groote. 2001. Towards abstract categorial grammar. In Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL).
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato. 2002. The FrameNet database and software tools. In
Proceedings of the Third International Conference on Language Resources and Evaluation, volume IV, Las
Palmas. LREC, LREC.
A. Gesmundo, J. Henderson, P. Merlo, and I.Titov. 2009. Latent variable model of synchronous syntactic-semantic
parsing for multiple languages. In CoNLL 2009 Shared Task., Conf. on Computational Natural Language
Learning, pages 37?42, Boulder, Colorado, USA.
Jan Haji?c, Jarmila Panevov?a, Eva Haji?cov?a, Petr Sgall, Petr Pajas, Jan
?
St?ep?anek, Ji?r?? Havelka, Marie Mikulov?a,
and Zden
?
k
?
Zabokrtsk?y. 2006. Prague Dependency Treebank 2.0. Linguistic Data Consortium, Philadelphia.
Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s M`arquez,
Adam Meyers, Joakim Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue,
and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared
Task, pages 1?18.
J. Hockenmeier. 2003. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics (ACL), pages 359?366, Sapporo, Japan.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and Dan Flickinger. 2012. Who did what to whom? a contrastive
study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop, pages
2?11, Jeju, Republic of Korea, July. Association for Computational Linguistics.
1411
R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for english. In J. Nivre, H.-J.
Kaalep, K. Muischnek, and M. Koit, editors, Proceedings of NODALIDA 2007, pages 105?112, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic?semantic analysis with PropBank and
NomBank. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages
183?187, Manchester, United Kingdom.
H. Kamp and U. Reyle. 1993. From Discourse to Logic. Kluwer Academic Publishers, Dordrecht, NL.
R.T. Kasper and W.C. Rounds. 1986. A logical semantics for feature structures. In Proceedings of the 24th annual
meeting on Association for Computational Linguistics, pages 257?266.
V?aclav Klime?s. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, UFAL,
MFF UK, Prague, Czech Republic.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic
dependencies. Transactions of the Association for Computational Linguistics, pages 219?230.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92?97.
Igor Mel??cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.
Igor Mel??cuk. 1996. Lexical functions: A tool for the description of lexical relations in the lexicon. In L. Wanner,
editor, Lexical functions in lexicography and natural language processing, pages 37?102. Benjamins Academic
Publishers, Amsterdam.
Igor Mel??cuk. 2013. Semantics: From meaning to text, Volume 2. Benjamins Academic Publishers, Amsterdam.
Simon Mille, Alicia Burga, Gabriela Ferraro, and Leo Wanner. 2012. How does the granularity of an annotation
scheme influence dependency parsing performance? In Conference on Computational Linguistics, COLING
2012.
Simon Mille, Alicia Burga, and Leo Wanner. 2013. AnCora-UPF: A Multi-Level Annotation of Spanish . In
Proceedings of the Second International Conference on Dependency Linguistics (DEPLING 2013).
Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development
and Feature Forest Model. Ph.D. thesis, University of Tokyo.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 915?932.
Stephan Oepen. 2002. Collaborative Language Engineering: A Case Study in Efficient Grammar-based Process-
ing. Stanford Univ Center for the Study.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank. Computational Linguistics,
31:71?106.
Owen Rambow and Aravind Joshi. 1997. A formal look at dependency grammar and phrase structure grammars,
with special consideration of word-order phenomena. In L. Wanner, editor, Recent Trends in Meaning-Text
Theory, pages 167?190. Benjamins Academic Publishers, Amsterdam.
W.C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4(3):257?287.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s M`arquez, and Joakim Nivre. 2008. The conll 2008
shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Language Learning, pages 159?177.
M. Taul?e, M. Ant`onia Mart??, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for Catalan and
Spanish. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08), Marrakech,
Morocco, may. European Language Resources Association (ELRA).
J.W. Thatcher. 1970. Generalized sequential machine maps. Journal of Computer and System Sciences, 4(4):339?
367.
1412
F. Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural
Language Processing Pacific Rim Symposium, pages 398?403, Beijing, China.
Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Pro-
ceedings of ACL-08: HLT, pages 888?896, Columbus, Ohio, June. Association for Computational Linguistics.
1413
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 58?62,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
MaltOptimizer: An Optimization Tool for MaltParser
Miguel Ballesteros
Complutense University of Madrid
Spain
miballes@fdi.ucm.es
Joakim Nivre
Uppsala University
Sweden
joakim.nivre@lingfil.uu.se
Abstract
Data-driven systems for natural language
processing have the advantage that they can
easily be ported to any language or domain
for which appropriate training data can be
found. However, many data-driven systems
require careful tuning in order to achieve
optimal performance, which may require
specialized knowledge of the system. We
present MaltOptimizer, a tool developed to
facilitate optimization of parsers developed
using MaltParser, a data-driven dependency
parser generator. MaltOptimizer performs
an analysis of the training data and guides
the user through a three-phase optimization
process, but it can also be used to perform
completely automatic optimization. Exper-
iments show that MaltOptimizer can im-
prove parsing accuracy by up to 9 percent
absolute (labeled attachment score) com-
pared to default settings. During the demo
session, we will run MaltOptimizer on dif-
ferent data sets (user-supplied if possible)
and show how the user can interact with the
system and track the improvement in pars-
ing accuracy.
1 Introduction
In building NLP applications for new languages
and domains, we often want to reuse components
for tasks like part-of-speech tagging, syntactic
parsing, word sense disambiguation and semantic
role labeling. From this perspective, components
that rely on machine learning have an advantage,
since they can be quickly adapted to new settings
provided that we can find suitable training data.
However, such components may require careful
feature selection and parameter tuning in order to
give optimal performance, a task that can be dif-
ficult for application developers without special-
ized knowledge of each component.
A typical example is MaltParser (Nivre et al
2006), a widely used transition-based dependency
parser with state-of-the-art performance for many
languages, as demonstrated in the CoNLL shared
tasks on multilingual dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al 2007). Malt-
Parser is an open-source system that offers a wide
range of parameters for optimization. It imple-
ments nine different transition-based parsing al-
gorithms, each with its own specific parameters,
and it has an expressive specification language
that allows the user to define arbitrarily complex
feature models. Finally, any combination of pars-
ing algorithm and feature model can be combined
with a number of different machine learning al-
gorithms available in LIBSVM (Chang and Lin,
2001) and LIBLINEAR (Fan et al 2008). Just
running the system with default settings when
training a new parser is therefore very likely to
result in suboptimal performance. However, se-
lecting the best combination of parameters is a
complicated task that requires knowledge of the
system as well as knowledge of the characteris-
tics of the training data.
This is why we present MaltOptimizer, a tool
for optimizing MaltParser for a new language
or domain, based on an analysis of the train-
ing data. The optimization is performed in three
phases: data analysis, parsing algorithm selec-
tion, and feature selection. The tool can be run
in ?batch mode? to perform completely automatic
optimization, but it is also possible for the user to
manually tune parameters after each of the three
phases. In this way, we hope to cater for users
58
without specific knowledge of MaltParser, who
can use the tool for black box optimization, as
well as expert users, who can use it interactively
to speed up optimization. Experiments on a num-
ber of data sets show that using MaltOptimizer for
completely automatic optimization gives consis-
tent and often substantial improvements over the
default settings for MaltParser.
The importance of feature selection and param-
eter optimization has been demonstrated for many
NLP tasks (Kool et al 2000; Daelemans et al
2003), and there are general optimization tools for
machine learning, such as Paramsearch (Van den
Bosch, 2004). In addition, Nilsson and Nugues
(2010) has explored automatic feature selection
specifically for MaltParser, but MaltOptimizer is
the first system that implements a complete cus-
tomized optimization process for this system.
In the rest of the paper, we describe the opti-
mization process implemented in MaltOptimizer
(Section 2), report experiments (Section 3), out-
line the demonstration (Section 4), and conclude
(Section 5). A more detailed description of Malt-
Optimizer with additional experimental results
can be found in Ballesteros and Nivre (2012).
2 The MaltOptimizer System
MaltOptimizer is written in Java and implements
an optimization procedure for MaltParser based
on the heuristics described in Nivre and Hall
(2010). The system takes as input a training
set, consisting of sentences annotated with depen-
dency trees in CoNLL data format,1 and outputs
an optimized MaltParser configuration together
with an estimate of the final parsing accuracy.
The evaluation metric that is used for optimiza-
tion by default is the labeled attachment score
(LAS) excluding punctuation, that is, the percent-
age of non-punctuation tokens that are assigned
the correct head and the correct label (Buchholz
and Marsi, 2006), but other options are available.
For efficiency reasons, MaltOptimizer only ex-
plores linear multiclass SVMs in LIBLINEAR.
2.1 Phase 1: Data Analysis
After validating that the data is in valid CoNLL
format, using the official validation script from
the CoNLL-X shared task,2 the system checks the
1http://ilk.uvt.nl/conll/#dataformat
2http://ilk.uvt.nl/conll/software.html#validate
minimum Java heap space needed given the size
of the data set. If there is not enough memory
available on the current machine, the system in-
forms the user and automatically reduces the size
of the data set to a feasible subset. After these ini-
tial checks, MaltOptimizer checks the following
characteristics of the data set:
1. Number of words/sentences.
2. Existence of ?covered roots? (arcs spanning
tokens with HEAD = 0).
3. Frequency of labels used for tokens with
HEAD = 0.
4. Percentage of non-projective arcs/trees.
5. Existence of non-empty feature values in the
LEMMA and FEATS columns.
6. Identity (or not) of feature values in the
CPOSTAG and POSTAG columns.
Items 1?3 are used to set basic parameters in the
rest of phase 1 (see below); 4 is used in the choice
of parsing algorithm (phase 2); 5 and 6 are rele-
vant for feature selection experiments (phase 3).
If there are covered roots, the system checks
whether accuracy is improved by reattaching
such roots in order to eliminate spurious non-
projectivity. If there are multiple labels for to-
kens with HEAD=0, the system tests which label
is best to use as default for fragmented parses.
Given the size of the data set, the system sug-
gests different validation strategies during phase
1. If the data set is small, it recommends us-
ing 5-fold cross-validation during subsequent op-
timization phases. If the data set is larger, it rec-
ommends using a single development set instead.
But the user can override either recommendation
and select either validation method manually.
When these checks are completed, MaltOpti-
mizer creates a baseline option file to be used as
the starting point for further optimization. The
user is given the opportunity to edit this option
file and may also choose to stop the process and
continue with manual optimization.
2.2 Phase 2: Parsing Algorithm Selection
MaltParser implements three groups of transition-
based parsing algorithms:3 (i) Nivre?s algorithms
(Nivre, 2003; Nivre, 2008), (ii) Covington?s algo-
rithms (Covington, 2001; Nivre, 2008), and (iii)
3Recent versions of MaltParser contains additional algo-
rithms that are currently not handled by MaltOptimizer.
59
Figure 1: Decision tree for best projective algorithm.
Figure 2: Decision tree for best non-projective algo-
rithm (+PP for pseudo-projective parsing).
Stack algorithms (Nivre, 2009; Nivre et al 2009)
Both the Covington group and the Stack group
contain algorithms that can handle non-projective
dependency trees, and any projective algorithm
can be combined with pseudo-projective parsing
to recover non-projective dependencies in post-
processing (Nivre and Nilsson, 2005).
In phase 2, MaltOptimizer explores the parsing
algorithms implemented in MaltParser, based on
the data characteristics inferred in the first phase.
In particular, if there are no non-projective depen-
dencies in the training set, then only projective
algorithms are explored, including the arc-eager
and arc-standard versions of Nivre?s algorithm,
the projective version of Covington?s projective
parsing algorithm and the projective Stack algo-
rithm. The system follows a decision tree consid-
ering the characteristics of each algorithm, which
is shown in Figure 1.
On the other hand, if the training set con-
tains a substantial amount of non-projective de-
pendencies, MaltOptimizer instead tests the non-
projective versions of Covington?s algorithm and
the Stack algorithm (including a lazy and an eager
variant), and projective algorithms in combination
with pseudo-projective parsing. The system then
follows the decision tree shown in Figure 2.
If the number of trees containing non-
projective arcs is small but not zero, the sys-
tem tests both projective algorithms and non-
projective algorithms, following the decision trees
in Figure 1 and Figure 2 and picking the algorithm
that gives the best results after traversing both.
Once the system has finished testing each of the
algorithms with default settings, MaltOptimizer
tunes some specific parameters of the best per-
forming algorithm and creates a new option file
for the best configuration so far. The user is again
given the opportunity to edit the option file (or
stop the process) before optimization continues.
2.3 Phase 3: Feature Selection
In the third phase, MaltOptimizer tunes the fea-
ture model given all the parameters chosen so far
(especially the parsing algorithm). It starts with
backward selection experiments to ensure that all
features in the default model for the given pars-
ing algorithm are actually useful. In this phase,
features are omitted as long as their removal does
not decrease parsing accuracy. The system then
proceeds with forward selection experiments, try-
ing potentially useful features one by one. In this
phase, a threshold of 0.05% is used to determine
whether an improvement in parsing accuracy is
sufficient for the feature to be added to the model.
Since an exhaustive search for the best possible
feature model is impossible, the system relies on
a greedy optimization strategy using heuristics de-
rived from proven experience (Nivre and Hall,
2010). The major steps of the forward selection
experiments are the following:4
1. Tune the window of POSTAG n-grams over
the parser state.
2. Tune the window of FORM features over the
parser state.
3. Tune DEPREL and POSTAG features over
the partially built dependency tree.
4. Add POSTAG and FORM features over the
input string.
5. Add CPOSTAG, FEATS, and LEMMA fea-
tures if available.
6. Add conjunctions of POSTAG and FORM
features.
These six steps are slightly different depending
on which algorithm has been selected as the best
in phase 2, because the algorithms have different
parsing orders and use different data structures,
4For an explanation of the different feature columns such
as POSTAG, FORM, etc., see Buchholz and Marsi (2006) or
see http://ilk.uvt.nl/conll/#dataformat
60
Language Default Phase 1 Phase 2 Phase 3 Diff
Arabic 63.02 63.03 63.84 65.56 2.54
Bulgarian 83.19 83.19 84.00 86.03 2.84
Chinese 84.14 84.14 84.95 84.95 0.81
Czech 69.94 70.14 72.44 78.04 8.10
Danish 81.01 81.01 81.34 83.86 2.85
Dutch 74.77 74.77 78.02 82.63 7.86
German 82.36 82.36 83.56 85.91 3.55
Japanese 89.70 89.70 90.92 90.92 1.22
Portuguese 84.11 84.31 84.75 86.52 2.41
Slovene 66.08 66.52 68.40 71.71 5.63
Spanish 76.45 76.45 76.64 79.38 2.93
Swedish 83.34 83.34 83.50 84.09 0.75
Turkish 57.79 57.79 58.29 66.92 9.13
Table 1: Labeled attachment score per phase and with
comparison to default settings for the 13 training sets
from the CoNLL-X shared task (Buchholz and Marsi,
2006).
but the steps are roughly equivalent at a certain
level of abstraction. After the feature selection
experiments are completed, MaltOptimizer tunes
the cost parameter of the linear SVM using a sim-
ple stepwise search. Finally, it creates a complete
configuration file that can be used to train Malt-
Parser on the entire data set. The user may now
continue to do further optimization manually.
3 Experiments
In order to assess the usefulness and validity of
the optimization procedure, we have run all three
phases of the optimization on all the 13 data sets
from the CoNLL-X shared task on multilingual
dependency parsing (Buchholz and Marsi, 2006).
Table 1 shows the labeled attachment scores with
default settings and after each of the three opti-
mization phases, as well as the difference between
the final configuration and the default.5
The first thing to note is that the optimization
improves parsing accuracy for all languages with-
out exception, although the amount of improve-
ment varies considerably from about 1 percentage
point for Chinese, Japanese and Swedish to 8?9
points for Dutch, Czech and Turkish. For most
languages, the greatest improvement comes from
feature selection in phase 3, but we also see sig-
5Note that these results are obtained using 80% of the
training set for training and 20% as a development test set,
which means that they are not comparable to the test results
from the original shared task, which were obtained using the
entire training set for training and a separate held-out test set
for evaluation.
nificant improvement from phase 2 for languages
with a substantial amount of non-projective de-
pendencies, such as Czech, Dutch and Slovene,
where the selection of parsing algorithm can be
very important. The time needed to run the op-
timization varies from about half an hour for the
smaller data sets to about one day for very large
data sets like the one for Czech.
4 System Demonstration
In the demonstration, we will run MaltOptimizer
on different data sets and show how the user can
interact with the system while keeping track of
improvements in parsing accuracy. We will also
explain how to interpret the output of the system,
including the final feature specification model, for
users that are not familiar with MaltParser. By re-
stricting the size of the input data set, we can com-
plete the whole optimization procedure in 10?15
minutes, so we expect to be able to complete a
number of cycles with different members of the
audience. We will also let the audience contribute
their own data sets for optimization, provided that
they are in CoNLL format.6
5 Conclusion
MaltOptimizer is an optimization tool for Malt-
Parser, which is primarily aimed at application
developers who wish to adapt the system to a
new language or domain and who do not have
expert knowledge about transition-based depen-
dency parsing. Another potential user group con-
sists of researchers who want to perform compar-
ative parser evaluation, where MaltParser is often
used as a baseline system and where the use of
suboptimal parameter settings may undermine the
validity of the evaluation. Finally, we believe the
system can be useful also for expert users of Malt-
Parser as a way of speeding up optimization.
Acknowledgments
The first author is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-
01 Project), Universidad Complutense de Madrid
and Banco Santander Central Hispano (GR58/08
Research Group Grant). He is under the support
of the NIL Research Group (http://nil.fdi.ucm.es)
from the same university.
6The system is available for download under an open-
source license at http://nil.fdi.ucm.es/maltoptimizer
61
References
Miguel Ballesteros and Joakim Nivre. 2012. MaltOp-
timizer: A System for MaltParser Optimization. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC).
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
149?164.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vec-
tor Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Michael A. Covington. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95?102.
Walter Daelemans, Ve?ronique Hoste, Fien De Meul-
der, and Bart Naudts. 2003. Combined optimiza-
tion of feature selection and algorithm parameters
in machine learning of language. In Nada Lavrac,
Dragan Gamberger, Hendrik Blockeel, and Ljupco
Todorovski, editors, Machine Learning: ECML
2003, volume 2837 of Lecture Notes in Computer
Science. Springer.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Anne Kool, Jakub Zavrel, and Walter Daelemans.
2000. Simultaneous feature selection and param-
eter optimization for memory-based natural lan-
guage processing. In A. Feelders, editor, BENE-
LEARN 2000. Proceedings of the Tenth Belgian-
Dutch Conference on Machine Learning, pages 93?
100. Tilburg University, Tilburg.
Peter Nilsson and Pierre Nugues. 2010. Automatic
discovery of feature sets for dependency parsing. In
COLING, pages 824?832.
Joakim Nivre and Johan Hall. 2010. A quick guide
to MaltParser optimization. Technical report, malt-
parser.org.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 2216?2219.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task of EMNLP-CoNLL 2007, pages 915?
932.
Joakim Nivre, Marco Kuhlmann, and Johan Hall.
2009. An improved oracle for dependency parsing
with online reordering. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT?09), pages 73?76.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:513?553.
Joakim Nivre. 2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP
(ACL-IJCNLP), pages 351?359.
Antal Van den Bosch. 2004. Wrapped progressive
sampling search for optimizing learning algorithm
parameters. In Proceedings of the 16th Belgian-
Dutch Conference on Artificial Intelligence.
62
Going to the Roots of Dependency Parsing
Miguel Ballesteros?
Complutense University of Madrid
Joakim Nivre??
Uppsala University
Dependency trees used in syntactic parsing often include a root node representing a dummy
word prefixed or suffixed to the sentence, a device that is generally considered a mere technical
convenience and is tacitly assumed to have no impact on empirical results. We demonstrate that
this assumption is false and that the accuracy of data-driven dependency parsers can in fact be
sensitive to the existence and placement of the dummy root node. In particular, we show that
a greedy, left-to-right, arc-eager transition-based parser consistently performs worse when the
dummy root node is placed at the beginning of the sentence (following the current convention
in data-driven dependency parsing) than when it is placed at the end or omitted completely.
Control experiments with an arc-standard transition-based parser and an arc-factored graph-
based parser reveal no consistent preferences but nevertheless exhibit considerable variation in
results depending on root placement. We conclude that the treatment of dummy root nodes in
data-driven dependency parsing is an underestimated source of variation in experiments and
may also be a parameter worth tuning for some parsers.
1. Introduction
It is a lesson learned in many studies on natural language processing that choosing the
right linguistic representation can be crucial for obtaining high accuracy on a given task.
In constituency-based parsing, for example, adding or deleting nodes in syntactic trees
can have a substantial impact on the performance of a statistical parser. In dependency
parsing, the syntactic representations used offer less opportunity for transformation,
given that the nodes of a dependency tree are basically determined by the tokens of
the input sentence, except for the possible addition of a dummy word acting as the
root of the tree. In this article, we show that even this seemingly trivial modification
can make a difference, and that the exact placement of the dummy root node can
have a significant impact on the accuracy of a given parser. This suggests that the
placement of the dummy root is a parameter worth tuning for certain parsing systems
as well as a source of variation to be taken into account when interpreting experimental
results.
? Universidad Complutense de Madrid, Departamento de Ingenier??a del Software e Inteligencia Artificial,
C/ Prof. Jose? Garc??a Santesmases, s/n, 28040 Madrid, Spain. E-mail: miballes@fdi.ucm.es.
?? Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
Submission received: 25 July 2012; revised submission received: 13 October 2012; accepted for publication:
19 October 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
2. Dependency Graphs
Dependency-based approaches to syntactic parsing assume that the syntactic structure
of a sentence can be analyzed in terms of binary dependency relations between lexical
units, units that in the simplest case are taken to correspond directly to the tokens
of the sentence. It is very natural to represent this structure by a directed graph,
with nodes representing input tokens and arcs representing dependency relations.
In addition, we can add labels to arcs in order to distinguish different dependency
types or grammatical functions (e.g., subject, object, adverbial). We call such a graph a
dependency graph.
Dependency graphs are normally assumed to satisfy certain formal constraints,
such as the single-head constraint, which forbids more than one incoming arc to a node,
and the acyclicity constraint, ruling out cyclic graphs. Many dependency theories and
annotation schemes further require that the graph should be a tree, with a unique root
token on which all other tokens are transitively dependent, whereas other frameworks
allow more than one token to be a root in the sense of not having any incoming arc. A
simple and elegant way of reconciling such cross-framework differences and arriving
at a single formalization of dependency structures is to add a dummy root node, a special
node that does not correspond to any input token, and to require that the dependency
graph is a tree rooted at this node. The original tree constraint can then be enforced
by requiring that the special node has exactly one child, but not all frameworks need to
enforce this constraint. An additional advantage of adding a dummy root node is that its
outgoing arcs can be labeled to indicate the functional status of what would otherwise
simply be unlabeled root nodes. With a slight misuse of terminology, we call such labels
informative root labels.1
Because the dummy root node does not correspond to an input token, it has no well-
defined position in the node sequence defined by the word order of a sentence and could
in principle be inserted anywhere (or nowhere at all). One option that can be found in
the literature is to insert it at the end of this sequence, but the more common convention
in contemporary research on dependency parsing is to insert it at the beginning, hence
treating it as a dummy word prefixed to the sentence. This is also the choice implicitly
assumed in the CoNLL data format, used in the CoNLL shared tasks on dependency
parsing in 2006 and 2007 (Buchholz and Marsi 2006; Nivre et al 2007) and the current
de facto standard for exchange of dependency annotated data.
The question that concerns us here is whether the use of a dummy root node is just
a harmless technicality permitting us to treat different dependency theories uniformly,
and whether its placement in the input sequence is purely arbitrary, or whether both of
these choices may in fact have an impact on the parsing accuracy that can be achieved
with a given parsing model. In order to investigate this question empirically, we define
three different types of dependency graphs that differ only with respect to the existence
and placement of the dummy root node:
1. None: Only nodes corresponding to tokens are included in the graph.
2. First: A dummy root node is added as the first token in the sentence.
3. Last: A dummy root node is added as the last token in the sentence.
1 For example, in the Prague Dependency Treebank, which allows multiple children of the dummy root
node, the label may indicate whether the child functions as a main predicate, as the head of a coordinate
structure, or as final punctuation.
6
Ballesteros and Nivre Going to the Roots of Dependency Parsing
Figure 1 illustrates the three types of dependency graphs with examples taken from
the Penn Treebank of English (Marcus, Santorini, and Marcinkiewicz 1993) converted
to dependency structure using the procedure described in Nivre (2006), and the Prague
Dependency Treebank of Czech (Hajic? et al 2001; Bo?hmova? et al 2003). In the former
case, it is assumed that the dummy root node always has exactly one child, with a
dummy dependency label ROOT. In the latter case, the dummy root node may have
several children and these children have informative root labels indicating their func-
tion (Pred and AuxK in the example). Note also that the Czech dependency graph of
type None is not a tree, but a forest, because it consists of two disjoint trees.
3. Experiments
In order to test the hypothesis that the existence and placement of the dummy root node
can have an impact on parsing accuracy, we performed an experiment using two widely
used data-driven dependency parsers, MaltParser (Nivre, Hall, and Nilsson 2006) and
MSTParser (McDonald 2006), and all the 13 data sets from the CoNLL 2006 shared
task on multilingual dependency parsing (Buchholz and Marsi 2006) as well as the
English Penn Treebank converted to Stanford dependencies (de Marneffe, MacCartney,
and Manning 2006). We created three different versions of each data set, corresponding
to the representation types None, First, and Last, and used them to evaluate MaltParser
with two different transition systems?arc-eager (Nivre 2003) and arc-standard (Nivre
2004)?and MSTParser with the arc-factored non-projective algorithm (McDonald
et al 2005). The results are shown in Table 1.
When creating the data sets, we took the original version from the CoNLL-X shared
task as None, because it does not include the dummy root node as an explicit input
token. In this representation, the tokens of a sentence are indexed from 1 to n and the
dependency graph is specified by giving each word a head index ranging from 0 to n,
where 0 signifies that the token is not a dependent on any other token in the sentence.
The First version was created by adding an extra token at the beginning of the sentence
with index 1 and head index 0, increasing all other token and head indices by 1, meaning
that all tokens that previously had a head index of 0 would now be attached to the new
Economic1
 

NMOD
news2
 

SBJ
had3 little4
 

NMOD
effect5
 

OBJ
on6
 

NMOD
financial7
 

NMOD
markets8
 

PMOD
.9

 
P
Z1
 

AuxP
nich2
 

Atr
je3 jen4
 

AuxZ
jedna5
 

Sb
na6
 

AuxP
kvalitu7

 
Adv
.8
ROOT1 Economic2
 

NMOD
news3
 

SBJ
had4
 

ROOT
little5
 

NMOD
effect6
 

OBJ
on7
 

NMOD
financial8
 

NMOD
markets9
 

PMOD
.10

 
P
ROOT1 Z2
 

AuxP
nich3
 

Atr
je4
 

Pred
jen5
 

AuxZ
jedna6
 

Sb
na7
 

AuxP
kvalitu8

 
Adv
.9
 

AuxK
ROOT10Economic1
 

NMOD
news2
 

SBJ
had3
 

ROOT
little4
 

NMOD
effect5
 

OBJ
on6
 

NMOD
financial7
 

NMOD
markets8
 

PMOD
.9

 
P
ROOT9Z1
 

AuxP
nich2
 

Atr
je3
 

Pred
jen4
 

AuxZ
jedna5
 

Sb
na6
 

AuxP
kvalitu7

 
Adv
.8
 

AuxK
Figure 1
Dependency graph types None (top), First (middle), and Last (bottom) for an English
sentence from the Penn Treebank (left) and a Czech sentence taken from the Prague
Dependency Treebank (right). (Gloss of Czech sentence: Z/Out-of nich/them je/is jen/
only jedna/one-FEM-SG na/to kvalitu/quality ./. = ?Only one of them concerns quality.?)
7
Computational Linguistics Volume 39, Number 1
Table 1
Experimental results for arc-eager (AE), arc-standard (AS), and maximum spanning tree
parsing (MST) on all the CoNLL-X data sets plus the English Penn Treebank converted to
Stanford dependencies with three different dependency graph types (None, First, Last).
Evaluation metrics are labeled attachment score (LAS), unlabeled attachment score (UAS), root
attachment (or no attachment in the case of None) measured as recall (RR) and precision (RP).
Scores in bold are best in their column (per language); scores in italic are not comparable to the
rest because of informative arc labels that cannot be predicted with the None representation.
AE AS MST
Language Type LAS UAS RR RP LAS UAS RR RP LAS UAS RR RP
Arabic
None 60.00 75.17 74.24 69.52 60.48 77.29 81.69 80.60 66.73 78.96 84.07 83.78
First 63.63 74.57 84.75 73.75 64.93 77.09 83.73 81.79 66.41 78.32 83.39 90.44
Last 64.15 74.97 73.56 68.24 65.29 77.31 82.71 81.06 66.41 78.32 78.31 87.83
Bulgarian
None 85.76 90.80 94.22 90.80 85.06 90.33 91.96 91.96 86.30 91.64 98.24 98.24
First 84.64 89.83 90.20 87.56 85.12 90.33 91.71 91.71 86.32 91.28 97.49 97.49
Last 85.76 90.78 94.22 90.58 85.16 90.33 91.96 91.96 86.14 91.28 97.24 97.24
Chinese
None 85.13 89.68 93.63 88.42 85.25 90.08 93.06 93.06 86.88 90.82 94.33 94.33
First 84.59 89.09 92.25 89.55 85.23 90.10 92.82 92.82 86.54 90.68 94.33 94.33
Last 85.15 89.70 93.63 88.42 85.17 90.00 92.82 92.82 86.36 90.52 93.87 93.97
Czech
None 68.30 81.14 80.51 74.61 68.36 81.96 87.85 73.35 76.70 85.98 82.20 80.83
First 72.98 79.96 83.33 72.66 74.88 82.52 86.44 88.95 77.04 86.34 85.88 84.92
Last 73.96 81.16 81.07 75.53 74.28 81.78 87.01 73.16 77.68 86.70 89.55 89.55
Danish
None 82.36 87.88 91.33 88.06 81.64 87.86 92.88 93.17 83.39 89.46 92.57 91.44
First 80.60 86.59 86.69 82.11 81.66 87.86 92.88 93.17 83.97 89.84 94.74 94.94
Last 82.38 87.94 91.64 88.36 81.52 87.74 92.88 92.31 83.43 89.42 92.26 92.26
Dutch
None 71.09 74.51 65.56 66.08 70.67 74.43 69.84 72.53 79.05 83.49 79.77 79.92
First 70.81 74.41 72.18 57.88 71.07 75.23 64.20 82.09 78.91 83.43 74.32 83.59
Last 71.05 74.51 65.76 66.67 70.65 74.45 69.84 72.38 78.25 82.95 75.10 85.21
English
None 88.63 90.47 88.70 81.75 88.15 90.07 87.83 86.42 87.55 89.91 87.70 87.70
First 88.00 90.04 83.99 85.42 88.04 89.95 86.20 86.24 87.63 90.00 90.20 90.17
Last 88.57 90.46 88.58 81.73 88.16 90.07 87.91 86.61 87.69 90.06 88.45 88.38
German
None 83.85 86.64 94.68 85.14 84.31 87.22 93.84 93.84 85.64 89.54 97.76 97.76
First 83.29 86.08 89.64 90.40 84.37 87.24 93.84 93.84 85.74 89.66 97.48 97.48
Last 83.93 86.72 94.68 85.14 84.35 87.22 93.84 93.84 85.34 89.50 97.76 97.76
Japanese
None 89.85 92.10 92.74 85.20 90.15 92.30 92.53 85.42 90.45 93.02 93.38 88.47
First 88.79 91.27 88.15 89.20 89.13 91.57 87.83 90.94 90.83 93.36 92.85 91.58
Last 89.77 92.12 92.96 84.56 90.01 92.28 92.64 85.60 90.47 93.06 92.21 90.66
Portuguese
None 79.12 88.60 92.01 85.76 78.32 87.78 90.28 90.28 84.87 89.74 89.58 89.58
First 83.77 88.36 87.85 86.35 83.45 87.82 90.62 90.62 85.19 90.26 91.67 91.67
Last 84.17 88.62 92.01 85.76 83.47 87.80 90.62 90.62 84.89 89.26 90.62 90.31
Spanish
None 78.64 82.51 83.25 76.28 77.88 81.69 81.73 81.73 79.40 83.57 79.70 78.50
First 78.14 82.15 79.70 73.36 77.64 81.51 81.22 81.22 79.20 83.41 83.76 84.18
Last 78.64 82.49 83.76 76.39 77.72 81.55 80.71 81.12 79.48 83.53 84.77 83.50
Swedish
None 83.49 89.60 93.32 90.07 82.65 89.42 92.03 91.56 81.36 88.29 89.97 90.21
First 83.13 89.29 91.77 89.47 82.53 89.38 91.77 91.77 81.76 88.59 91.52 91.75
Last 83.59 89.70 93.32 90.07 82.65 89.36 91.77 91.30 81.66 88.35 92.03 92.03
Slovene
None 64.25 79.56 73.98 63.46 63.67 79.28 75.26 67.35 71.44 82.47 79.08 75.98
First 67.73 77.84 71.94 64.83 69.40 79.42 73.47 78.69 71.72 82.67 79.34 79.34
Last 69.98 79.62 75.00 64.05 69.42 79.28 75.26 67.66 71.64 82.33 76.79 79.21
Turkish
None 56.66 72.18 90.29 92.25 57.00 72.06 90.59 92.13 58.49 74.55 93.47 86.88
First 56.48 71.86 88.77 93.00 56.80 72.12 89.68 94.86 58.59 74.59 92.56 94.28
Last 56.64 72.16 90.14 91.95 56.88 72.10 90.59 92.13 58.89 74.83 93.02 94.45
Average
None 76.94 84.35 86.32 81.24 76.69 84.41 87.24 85.24 79.88 86.53 88.70 87.40
First 77.61 83.67 85.09 81.11 78.16 84.44 86.17 88.48 79.99 86.60 89.25 90.44
Last 78.41 84.35 86.45 81.25 78.20 84.38 87.18 85.18 79.88 86.44 88.71 90.17
8
Ballesteros and Nivre Going to the Roots of Dependency Parsing
dummy root token. The Last version was created by adding an extra token at the end
of the sentence with index n+1, and changing every head index that previously was 0
to n+1. In both First and Last, we made sure that the new dummy token had a unique
word form and unique values for all other features, so that it could not be mistaken for
any real word. For First and Last, we applied an inverse transformation to the parser
output before evaluation.
Both MaltParser and MSTParser by default add a dummy root node at the be-
ginning of the sentence internally before parsing, so we had to modify the parsers
so that they only considered arcs involving nodes corresponding to input tokens. For
MaltParser this only required setting a flag that makes the parser start with an empty
stack instead of a stack containing an extra dummy root node.2 For MSTParser, we
modified the parser implementation so that it extracts a maximum spanning tree that
is still rooted in an extra dummy root node but where the score of a tree is based only
on the scores of arcs connecting real token nodes. Finally, because MaltParser with the
arc-eager and arc-standard transition systems can only construct projective dependency
graphs, we projectivized all training sets before training the MaltParser models using
the baseline pseudo-projective transformation of Nivre and Nilsson (2005).3 Except for
these modifications, all parsers were run with out-of-the-box settings.
3.1 Deterministic Arc-Eager Parsing
The arc-eager transition-based parser first described in Nivre (2003) parses a sentence
in a single pass from left to right, using a stack to store partially processed tokens
and greedily choosing the highest-scoring parsing action at each point. The arc-eager
property entails that every arc in the output graph is added at the earliest possible
opportunity, which means that right-dependents are attached to their head before they
have found their own right-dependents. This can be an advantage because the early
attachment neither implies nor precludes the later addition of right-dependents, but
it can also be a drawback because it forces the parser to make an early commitment
about right-dependents. In this context, it is especially relevant that the addition of a
dummy root node at the beginning of the sentence (First) forces the parser to make
an early commitment regarding dependents of this root node. By contrast, if a dummy
root node is added at the end of a sentence (Last), decisions regarding root dependents
will be postponed until the end. Similarly, if no root node is added (None), then these
decisions will not be explicitly modeled at all, meaning that whatever nodes remain
on the stack after parsing will be treated as root dependents.
As can be seen from Table 1, the arc-eager parser performs consistently worse under
the First condition, with an average unlabeled attachment score (UAS) of 83.67 over the
14 languages, to be compared with 84.35 for None and Last. The difference in accuracy
between First and None/Last ranges from 0.10 for Dutch to 1.72/1.78 for Slovene, and
the difference in means is highly statistically significant (p < 0.001, Wilcoxon signed-
rank test). The difference between None and Last is never greater than 0.20 (and very
far from statistically significant), indicating that either postponing or excluding root
attachment decisions leads to very similar performance for the arc-eager parser. The
2 The exact commandline flag is -allow root false. A side effect of this flag is that all unattached tokens
get the dummy label ROOT, meaning that informative root labels cannot be predicted for representations
of type None. See http://maltparser.org for more information.
3 This is done with the MaltParser flag -pp baseline.
9
Computational Linguistics Volume 39, Number 1
same pattern is found for labeled attachment score (LAS), but here we can only directly
compare First and Last because the Arabic, Czech, Portuguese, and Slovene data sets
contain informative root labels that cannot be predicted under the None condition (cf.
footnote 2). The difference in means between First and Last is 0.80 and again highly
statistically significant (p < 0.001, Wilcoxon signed-rank test). A closer look at the root
accuracy suggests that most of the difference stems from a lower recall on root depen-
dents with the First representation, but this pattern is not completely consistent across
languages and Arabic, Czech, and Dutch actually have higher recall. For Czech and
Dutch this is accompanied by lower precision, but for Arabic the First representation
actually gives the best recall and precision of root dependents (but nevertheless the
worst overall attachment score). It is probably significant that the Arabic data set has
the longest sentences with the root word often appearing early in the sentence. Hence,
an early commitment to root attachment is more likely to be correct in this case, even if
it is more error prone in general.
3.2 Deterministic Arc-Standard Parsing
The arc-standard transition-based parser first described in Nivre (2004) is similar to the
arc-eager parser in that it parses a sentence in a single pass from left to right, using
a stack to store partially processed tokens and greedily choosing the highest-scoring
parsing action at each point. It differs by postponing the attachment of right-dependents
until the complete subtree under the dependent itself has been built. As a consequence,
the dependency tree is built strictly bottom?up, which means that attachment to a
dummy root node will always happen at the end, regardless of whether the dummy
root node is positioned at the beginning or at the end of the sentence. There is therefore
no reason to expect that the placement of the dummy root node should have the same
impact as for the arc-eager parser.
Looking at the results for the arc-standard parser in Table 1 confirms this expecta-
tion, with the three conditions giving very similar mean UAS (84.41 for None, 84.44 for
First, 84.38 for Last) and none of the differences being statistically significant. For LAS,
we can again only directly compare First and Last, but there is practically no difference
in the means here either (78.16 for First vs. 78.20 for Last). Nevertheless, it is worth
noting that, for individual languages, differences in scores can be quite substantial.
Thus, for Dutch, the First condition outperforms the None/Last condition by 0.80/0.78
in UAS and 0.40/0.42 in LAS. Conversely, for Japanese, the None/Last conditions are
better than First by 0.73/0.71 in UAS and 1.02/0.88 in LAS. Although the general trend
is that None and Last give the most similar results, just as for the arc-eager parser,
there are also cases like Chinese where None and First are both slightly better than
Last. Zooming in on root accuracy, we see a clear gain in precision (and marginal drop
in recall) for the First representation, which is probably an effect of the arc-standard
strategy where the attachment of right-dependents often have to be delayed whereas
left-dependents can be attached eagerly.
3.3 Maximum Spanning Tree Parsing
The maximum spanning tree parser described in McDonald et al (2005) uses a very
different parsing model compared with the two transition-based parsers. Instead of
scoring individual parsing actions, it scores all possible dependency arcs in the sentence
and then uses exact inference to extract the highest-scoring complete dependency tree
10
Ballesteros and Nivre Going to the Roots of Dependency Parsing
under an arc-factored model, where the score of each tree is the sum of the scores of
its component arcs. Because the parsing algorithm does not impose any ordering at all
on different attachments, we would expect even less impact from the placement of the
dummy root node than for the deterministic arc-standard parser.
The results in Table 1 do not quite confirm this expectation. The mean UAS varies
from 86.44 for the Last condition to 86.60 for the First condition, and the mean LAS
is 79.88 for None and Last but 79.99 for First. Although none of these differences is
statistically significant on the aggregate level, differences can be quite substantial for
individual languages, with First outperforming Last by a whole percentage point in
UAS for Portuguese (but only 0.30 in LAS) and None outperforming both First and Last
by 0.64 for Arabic (and 0.32 in LAS). The fact that LAS differences tend to be smaller
than UAS differences can probably be explained by the fact that MSTParser uses a two-
stage approach, where the second labeling stage is the same for all three conditions.
With respect to root accuracy, the most interesting observation is that both First and
Last seem to given higher precision than None, which suggests that it is an advantage
to represent root attachments explicitly so that features over these arcs can contribute to
the overall score of a parse tree. It is also worth noting that these features are different
for First and Last, despite the arc-factored model, because of the so-called ?in-between
features? that record the part-of-speech tags of words occurring between the head
and the dependent of an arc, which in turn explains why these two conditions do not
always give the same results.
4. Discussion
The main conclusion we draw from this experiment is that the addition of a dummy
word prefixed or suffixed to a sentence is not a mere technical convenience without
impact on empirical results. Whether we include a dummy word representing the root
of the dependency tree and, if so, where we place this word in the sequence of input
tokens, can have a non-negligible effect on parsing accuracy for different parsers?in
some cases resulting in statistically significant differences with a magnitude of several
percentage points according to standard evaluation metrics.
The nature and magnitude of the impact definitely depends on the parsing model
used. Whereas the deterministic arc-eager parser gives consistently worse results with a
dummy root node positioned at the beginning of the sentence, neither the deterministic
arc-standard parser nor the maximum spanning tree parser has any clear preference in
this respect. Although the overall patterns emerging when averaging over many data
sets can largely be explained in this way, there is also considerable variation across data
sets that we do not yet fully understand, however. Zooming in on root accuracy has
allowed us to start forming hypotheses, such as the impact of long sentences in com-
bination with predominantly head-initial structures for Arabic, but a full exploration
of the interaction of parsing models and language-specific properties is clearly outside
the scope of this article and has to be left for future research. Another limitation of
the current study is that it only examines three different parsers, and although this
is clearly sufficient to prove the existence of the phenomenon it will be interesting to
see whether the same patterns can be found if we examine more recent state-of-the-art
methods, going from deterministic parsing to beam search for transition-based parsing
and from arc-factored to higher-order models for graph-based parsing. In this context,
it is also relevant to mention previous work, such as Hall et al (2007) and Attardi
and Dell?Orletta (2009), which have tried to improve parsing accuracy by switching or
11
Computational Linguistics Volume 39, Number 1
combining parsing directions, which implicitly has the effect of changing the position
of the root node (if present).
In conclusion, we believe there may be two methodological lessons to learn from
our experiments. The first is that, for certain parsing models, the existence and place-
ment of the dummy root node is in fact a parameter worth tuning for best performance.
Thus, for the deterministic arc-eager parser, it seems that we can obtain higher parsing
accuracy by placing the dummy root node at the end of the sentence (or omitting
it completely) instead of placing it at the beginning in the sentence, as is currently
the norm in data-driven dependency parsing. The second lesson is that, because the
differences observed between different conditions are sometimes at least as large as
the differences considered significant when comparing different parsing models, the
status of the dummy root node may be an underestimated source of variation and a
variable that needs to be controlled for in experimental evaluations. The current practice
of consistently placing the root node at the beginning of the sentence is one way of
ensuring comparability of results, but given the arbitrariness of this decision together
with our experimental results, it may be worth exploring other representations as well.
Acknowledgments
Thanks to Ryan McDonald, Yoav Goldberg,
and three anonymous reviewers for useful
comments, and to Ryan also for help in
modifying MSTParser. Miguel Ballesteros is
funded by the Spanish Ministry of Education
and Science (TIN2009-14659-C03-01 Project).
References
Attardi, Giuseppe and Felice Dell?Orletta.
2009. Reverse revision and linear tree
combination for dependency parsing. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT),
pages 261?264, Boulder, CO.
Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,
and Barbora Hladka?. 2003. The Prague
Dependency Treebank: A three-level
annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using
Parsed Corpora. Kluwer, pages 103?127.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the 10th Conference on Computational
Natural Language Learning (CoNLL),
pages 149?164, New York, NY.
de Marneffe, Marie-Catherine, Bill
MacCartney, and Christopher D.
Manning. 2006. Generating typed
dependency parses from phrase
structure parses. In Proceedings of
the 5th International Conference on
Language Resources and Evaluation
(LREC), pages 449?454, Genoa.
Hajic?, Jan, Barbora Vidova Hladka, Jarmila
Panevova?, Eva Hajic?ova?, Petr Sgall, and
Petr Pajas. 2001. Prague Dependency
Treebank 1.0. LDC, 2001T10.
Hall, Johan, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias
Nilsson, and Markus Saers. 2007. Single
malt or blended? A study in multilingual
parser optimization. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007,
pages 933?939, Prague.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
McDonald, Ryan. 2006. Discriminative
Learning and Spanning Tree Algorithms
for Dependency Parsing. Ph.D. thesis,
University of Pennsylvania.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
the Human Language Technology Conference
and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing.
In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT),
pages 149?160, Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing.
In Proceedings of the Workshop on
Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 50?57,
Barcelona.
12
Ballesteros and Nivre Going to the Roots of Dependency Parsing
Nivre, Joakim. 2006. Inductive Dependency
Parsing. Springer, Berlin.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2006. Maltparser: A data-driven
parser-generator for dependency parsing.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 2216?2219,
Genoa.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 99?106,
Ann Arbor, MI.
13

First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282?287,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of
Negation
Jorge Carrillo de Albornoz, Laura Plaza, Alberto D??az and Miguel Ballesteros
Universidad Complutense de Madrid
C/ Prof. Jose? Garc??a Santesmases, s/n
28040 Madrid (Spain)
{jcalbornoz,lplazam,albertodiaz,miballes}@fdi.ucm.es
Abstract
This paper presents one of the two contribu-
tions from the Universidad Complutense de
Madrid to the *SEM Shared Task 2012 on Re-
solving the Scope and Focus of Negation. We
describe a rule-based system for detecting the
presence of negations and delimitating their
scope. It was initially intended for process-
ing negation in opinionated texts, and has been
adapted to fit the task requirements. It first
detects negation cues using a list of explicit
negation markers (such as not or nothing), and
infers other implicit negations (such as affixal
negations, e.g, undeniable or improper) by us-
ing semantic information from WordNet con-
cepts and relations. It next uses the informa-
tion from the syntax tree of the sentence in
which the negation arises to get a first approxi-
mation to the negation scope, which is later re-
fined using a set of post-processing rules that
bound or expand such scope.
1 Introduction
Detecting negation is important for many NLP tasks,
as it may reverse the meaning of the text affected
by it. In information extraction, for instance, it is
obviously important to distinguish negated informa-
tion from affirmative one (Kim and Park, 2006). It
may also improve automatic indexing (Mutalik et
al., 2001). In sentiment analysis, detecting and deal-
ing with negation is critical, as it may change the
polarity of a text (Wiegand et al, 2010). How-
ever, research on negation has mainly focused on the
biomedical domain, and addressed the problem of
detecting if a medical term is negated or not (Chap-
man et al, 2001), or the scope of different negation
signals (Morante et al, 2008).
During the last years, the importance of process-
ing negation is gaining recognition by the NLP re-
search community, as evidenced by the success of
several initiatives such as the Negation and Spec-
ulation in Natural Language Processing workshop
(NeSp-NLP 2010)1 or the CoNLL-2010 Shared
Task2, which aimed at identifying hedges and their
scope in natural language texts. In spite of this, most
of the approaches proposed so far deal with negation
in a superficial manner.
This paper describes our contribution to the
*SEM Shared Task 2012 on Resolving the Scope
and Focus of Negation. As its name suggests, the
task aims at detecting the scope and focus of nega-
tion, as a means of encouraging research in negation
processing. In particular, we participate in Task 1:
scope detection. For each negation in the text, the
negation cue must be detected, and its scope marked.
Moreover, the event or property that is negated must
be recognized. A comprehensive description of the
task may be found in (Morante and Blanco, 2012).
For the sake of clarity, it is important to define
what the organization of the task understands by
negation cue, scope of negation and negated event.
The words that express negation are called negation
cues. Not and no are common examples of such
cues. Scope is defined as the part of the mean-
ing that is negated, and encloses all negated con-
cepts. The negated event is the property that is
1http://www.clips.ua.ac.be/NeSpNLP2010/
2www.inf.u-szeged.hu/rgai/conll2010st/
282
negated by the cue. For instance, in the sentence:
[Holmes] did not [say anything], the scope is en-
closed in square brackets, the negation cue is under-
lined and the negated event is shown in bold. More
details about the annotation of negation cues, scopes
and negated events may be found in (Morante and
Daelemans, 2012).
The system presented to the shared task is an
adaptation of the one published in (Carrillo de Al-
bornoz et al, 2010), whose aim was to detect and
process negation in opinionated text in order to im-
prove polarity and intensity classification. When
classifying sentiments and opinions it is important
to deal with the presence of negations and their ef-
fect on the emotional meaning of the text affected by
them. Consider the sentence (1) and (2). Sentence
(1) expresses a positive opinion, whereas that in sen-
tence (2) the negation word not reverses the polarity
of such opinion.
(1) I liked this hotel.
(2) I didn?t like this hotel.
Our system has the main advantage of being sim-
ple and highly generic. Even though it was origi-
nally conceived for treating negations in opinionated
texts, a few simple modifications have been suffi-
cient to successfully address negation in a very dif-
ferent type of texts, such as Conan Doyle stories. It
is rule-based and does not need to be trained. It also
uses semantic information in order to automatically
detect the negation cues.
2 Methodology
As already told, the UCM-I system is a modified ver-
sion of the one presented in (Carrillo de Albornoz
et al, 2010). Next sections detail the modifications
performed to undertake the present task.
2.1 Detecting negation cues
Our previous work was focused on explicit nega-
tions (i.e., those introduced by negation tokens such
as not, never). In contrast, in the present work
we also consider what we call implicit negations,
which includes affixal negation (i.,e., words with
prefixes such as dis-, un- or suffixes such as -less;
e.g., impatient or careless), inffixal negation (i.e.,
pointlessness, where the negation cue less is in the
middle of the noun phrase). Note that we did not
Table 1: Examples of negation cues.
Explicit negation cues
no not non nor
nobody never nowhere ...
Words with implicit negation cues
unpleasant unnatural dislike impatient
fearless hopeless illegal ...
have into account these negation cues when ana-
lyzing opinionated texts because these words them-
selves usually appear in affective lexicons with their
corresponding polarity values (i.e., impatient, for in-
stance, appears in SentiWordNet with a negative po-
larity value).
In order to detect negation cues, we use a list of
predefined negation signals, along with an automatic
method for detecting new ones. The list has been
extracted from different previous works (Councill et
al., 2010; Morante, 2010). This list also includes the
most frequent contracted forms (e.g., don?t, didn?t,
etc.). The automated method, in turn, is intended
for discovering in text new affixal negation cues. To
this end, we first find in the text all words with pre-
fixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suf-
fix -less that present the appropriate part of speech.
Since not all words with such affixes are negation
cues, we use semantic information from WordNet
concepts and relations to decide. In this way, we re-
trieve from WordNet the synset that correspond to
each word, using WordNet::SenseRelate (Patward-
han et al, 2005) to correctly disambiguate the mean-
ing of the word according to its context, along with
all its antonym synsets. We next check if, after re-
moving the affix, the word exists in WordNet and
belongs to any of the antonym synsets. If so, we
consider the original word to be a negation cue (i.e.,
the word without the affix has the opposite meaning
than the lexical item with the affix).
Table 1 presents some examples of explicit nega-
tion cues and words with implicit negation cues. For
space reasons, not all cues are shown. We also con-
sider common spelling errors such as the omission
of apostrophes (e.g., isnt or nt). They are not likely
to be found in literary texts, but are quite frequent in
user-generated content.
This general processing is, however, improved
with two rules:
283
Table 2: Examples of false negation cues.
no doubt without a doubt not merely not just
not even not only no wonder ...
1. False negation cues: Some negation words
may be also used in other expressions with-
out constituting a negation, as in sentence (3).
Therefore, when the negation token belongs
to such expressions, this is not processed as a
negation. Examples of false negation cues are
shown in Table 2.
(3) ... the evidence may implicate not only your
friend Mr. Stapleton but his wife as well.
2. Tag questions: Some sentences in the cor-
pora present negative tag questions in old En-
glish grammatical form, as it may shown in
sentences (4) and (5). We have implemented a
specific rule to deal with this type of construc-
tions, so that they are not treated as negations.
(4) You could easily recognize it , could you not?.
(5) But your family have been with us for several
generations , have they not?
2.2 Delimiting the scope of negation
The scope of a negation is determined by using the
syntax tree of the sentence in which the negation
arises, as generated by the Stanford Parser.3 To this
end, we find in the syntax tree the first common an-
cestor that encloses the negation token and the word
immediately after it, and assume all descendant leaf
nodes to the right of the negation token to be af-
fected by it. This process may be seen in Figure
1, where the syntax tree for the sentence: [Watson
did] not [solve the case] is shown. In this sentence,
the method identifies the negation token not and as-
sumes its scope to be all descendant leaf nodes of the
common ancestor of the words not and solve (i.e.,
solve the case).
This modeling has the main advantage of being
highly generic, as it serves to delimit the scope of
negation regardless of what the negated event is (i.e.,
the verb, the subject, the object of the verb, an ad-
jective or an adverb). As shown in (Carrillo de Al-
3http://nlp.stanford.edu/software/lex-parser.shtml
Figure 1: Syntax tree of the sentence: Watson did not
solve the case.
bornoz et al, 2010), it behaves well when determin-
ing the scope of negation for the purpose of classi-
fying product reviews in polarity classes. However,
we have found that this scope is not enough for the
present task, and thus we have implemented a set of
post-processing rules to expand and limit the scope
according to the task guidelines:
1. Expansion to subject. This rule expands the
negation scope in order to include the subject of
the sentence within it. In this way, in sentence
(6) the appropriate rule is fired to include ?This
theory? within the negation scope.
(6) [This theory would] not [work].
It must be noted that, for polarity classifica-
tion purposes, we do not consider the subject
of the sentence to be part of this scope. Con-
sider, for instance, the sentence: The beauti-
ful views of the Eiffel Tower are not guaranteed
in all rooms. According to traditional polarity
classification approaches, if the subject is con-
sidered as part of the negation scope, the polar-
ity of the positive polar expression ?beautiful?
should be changed, and considered as negative.
2. Subordinate boundaries. Our original nega-
tion scope detection method works well with
coordinate sentences, in which negation cues
scope only over their clause, as if a ?boundary?
exists between the different clauses. This oc-
curs, for instance, in the sentence:
284
Table 3: List of negation scope delimiters.
Tokens POS
so, because, if, while
INuntil, since, unless
before, than, despite IN
what, whose WP
why, where WRB
however RB
?,?, - , :, ;, (, ), !, ?, . -
(7) [It may be that you are] not [yourself lumi-
nous], but you are a conductor of light.
It also works properly in subordinate sentences,
when the negation occurs in the subordinate
clause, as in: You can imagine my surprise
when I found that [there was] no [one there].
However, it may fail in some types of subor-
dinate sentences, where the scope should be
limited to the main clause, but our model pre-
dict both clauses to be affected by the negation.
This is the case for the sentences where the de-
pendent clause is introduced by the subordinate
conjunctions in Table 3. An example of such
type of sentence is (8), where the conjunction
token because introduces a subordinate clause
which is out of the negation scope. To solve this
problem, the negation scope detection method
includes a set of rules to delimit the scope in
those cases, using as delimiters the conjunc-
tions in Table 3. Note that, since some of these
delimiters are ambiguous, their part of speech
tags are used to disambiguate them.
(8) [Her father] refused [to have anything to do
with her] because she had married without his
consent.
3. Prepositional phrases: Our original method
also fails to correctly determine the negation
scope when the negated event is followed by
a prepositional phrase, as it may be seen in
Figure 2, where the syntax tree for the sen-
tence: [There was] no [attempt at robbery] is
shown. Note that, according to our original
model, the phrase ?at robbery? does not belong
to the negation scope. This is an error that was
not detected before, but has been fixed for the
present task.
Figure 2: Syntax tree for the sentence: There was no at-
tempt at robbery.
2.3 Finding negated events
We only consider a single type of negated events,
so that, when a cue word contains a negative affix,
the word after removing the affix is annotated as the
negated event. In this way, ?doubtedly? is correctly
annotated as the negated event in sentence (9). How-
ever, the remaining types of negated events are rele-
gated to future work.
(9) [The oval seal is] undoubtedly [a plain
sleeve-link].
3 Evaluation Setup
The data collection consists of a development set, a
training set, and two test sets of 787, 3644, 496 and
593 sentences, respectively from different stories by
Conan Doyle (see (Morante and Blanco, 2012) for
details). Performance is measured in terms of recall,
precision and F-measure for the following subtasks:
? Predicting negation cues.
? Predicting both the scope and cue.
? Predicting the scope, the cue does not need to
be correct.
? Predicting the scope tokens, where not a full
scope match is required.
? Predicting negated events.
? Full evaluation, which requires all elements to
be correct.
285
Table 4: Results for the development set.
Metric Pr. Re. F-1
Cues 92.55 86.13 89.22
Scope (cue match) 86.05 44.05 58.27
Scope (no cue match) 86.05 44.05 58.27
Scope tokens (no cue match) 88.05 59.05 70.69
Negated (no cue match) 65.00 10.74 18.43
Full negation 74.47 20.23 31.82
4 Evaluation Results
The results of our system when evaluated on the de-
velopment set and the two test sets (both jointly and
separately), are shown in Tables 4, 5, and 6.
It may be seen from these tables that our sys-
tem behaves quite well in the prediction of negation
cues subtask, achieving around 90% F-measure in
all data sets, and the second position in the com-
petition. Performance in the scope prediction task,
however, is around 60% F-1, and the same results
are obtained if the correct prediction of cues is re-
quired (Scope (cue match)). This seems to indicate
that, for all correct scope predictions, our system
have also predicted the negation cues correctly. Ob-
viously these results improve for the Scope tokens
measure, achieving more than 77% F-1 for the Card-
board data set. We also got the second position in
the competition for these three subtasks. Concerning
detection of negated events, our system gets poor re-
sults, 22.85% and 19.81% F-1, respectively, in each
test data set. These results affect the performance
of the full negation prediction task, where we get
32.18% and 32.96% F-1, respectively. Surprisingly,
the result in the test sets are slightly better than those
in the development set, and this is due to a better be-
havior of the WordNet-based cue detection method
in the formers than in the later.
5 Discussion
We next discuss and analyze the results above.
Firstly, and regarding detection of negation cues, our
initial list covers all explicit negations in the devel-
opment set, while the detection of affixal negation
cues using our WordNet-based method presents a
precision of 100% but a recall of 53%. In particu-
lar, our method fails when discovering negation cues
such as unburned, uncommonly or irreproachable,
where the word after removing the affix is a derived
form of a verb or adjective.
Secondly, and concerning delimitation of the
scope, our method behaves considerably well. We
have found that it correctly annotates the negation
scope when the negation affects the predicate that
expresses the event, but sometimes fails to include
the subject of the sentence in such scope, as in:
[I know absolutely] nothing [about the fate of this
man], where our method only recognizes as the
negation scope the terms about the fate of this man.
The results have also shown that the method fre-
quently fails when the subject of the sentence or the
object of an event are negated. This occurs, for
instance, in sentences: I think, Watson, [a brandy
and soda would do him] no [harm] and No [woman
would ever send a reply-paid telegram], where we
only point to ?harm? and ?woman? as the scopes.
We have found a further category of errors in the
scope detection tasks, which concern some types
of complex sentences with subordinate conjunctions
where our method limits the negation scope to the
main clause, as in sentence: [Where they came from,
or who they are,] nobody [has an idea] , where our
method limits the scope to ?has an idea?. However,
if the negation cue occurs in the subordinate clause,
the method behaves correctly.
Thirdly, with respect to negated event detection,
as already told our method gets quite poor results.
This was expected, since our system was not orig-
inally designed to face this task and thus it only
covers one type of negated events. Specifically,
it correctly identifies the negated events for sen-
tences with affixal negation cues, as in: It is most
improper, most outrageous, where the negated event
is ?proper?. However, it usually fails to identify
these events when the negation affects the subject
of the sentence or the object of an event.
6 Conclusions and Future Work
This paper presents one of the two contributions
from the Universidad Complutense de Madrid to the
*SEM Shared Task 2012. The results have shown
that our method successes in identifying negation
cues and performs reasonably well when determin-
ing the negation scope, which seems to indicate that
a simple unsupervised method based on syntactic in-
formation and a reduced set of post-processing rules
286
Table 5: Results for the test sets (jointly).
Metric Gold System Tp Fp Fn Precision Recall F-1
Cues 264 278 241 29 23 89.26 91.29 90.26
Scopes (cue match) 249 254 116 24 133 82.86 46.59 59.64
Scopes (no cue match) 249 254 116 24 133 82.86 46.59 59.64
Scope tokens (no cue match) 1805 1449 1237 212 568 85.37 68.53 76.03
Negated (no cue match) 173 33 22 11 151 66.67 12.72 21.36
Full negation 264 278 57 29 207 66.28 21.59 32.57
Table 6: Results for the Cardboard and Circle test sets.
Metric
Cardboard set Circle set
Pr. Re. F-1 Pr. Re. F-1
Cues 90.23 90.23 90.23 88.32 92.37 90.30
Scope (cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope (no cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope tokens (no cue match) 84.91 72.08 77.97 85.96 64.50 73.70
Negated (no cue match) 66.67 13.79 22.85 66.67 11.63 19.81
Full negation 68.29 21.05 32.18 64.44 22.14 32.96
is a viable approach for dealing with negation. How-
ever, detection of negated events is the main weak-
ness of our approach, and this should be tackled in
future work. We also plan to improve our method
for detecting affixal negations to increment its recall,
by using further WordNet relations such as ?derived
from adjective?, and ?pertains to noun?, as well as
to extend this method to detect infixal negations.
Acknowledgments
This research is funded by the Spanish Ministry of
Science and Innovation (TIN2009-14659-C03-01)
and the Ministry of Education (FPU program).
References
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerva?s. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the 14th Conference on Computational Natural
Language Learning (CoNLL 2010), pages 153?161.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34:301?310.
Isaac Councill, Ryan McDonald, and Leonid Velikovich.
2010. What?s great and what?s not: learning to classify
the scope of negation for improved sentiment analysis.
In Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, pages 51?59.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. ACM Trans. on Asian Language Infor-
mation Processing, 5(1):44?60.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the 1st Joint Conference on
Lexical and Computational Semantics (*SEM 2012).
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715?724.
Roser Morante. 2010. Descriptive Analysis of Negation
Cues in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. A quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. SenseRelate::TargetWord: a generalized
framework for word sense disambiguation. In Pro-
ceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 73?76.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68.
287
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 288?293,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UCM-2: a Rule-Based Approach to Infer the Scope of Negation via
Dependency Parsing
Miguel Ballesteros, Alberto D??az, Virginia Francisco,
Pablo Gerva?s, Jorge Carrillo de Albornoz and Laura Plaza
Natural Interaction Based on Language Group
Complutense University of Madrid
Spain
{miballes, albertodiaz, virginia}@fdi.ucm.es,
pgervas@sip.ucm.es, {jcalbornoz, lplazam}@fdi.ucm.es
Abstract
UCM-2 infers the words that are affected by
negations by browsing dependency syntactic
structures. It first makes use of an algo-
rithm that detects negation cues, like no, not
or nothing, and the words affected by them
by traversing Minipar dependency structures.
Second, the scope of these negation cues is
computed by using a post-processing rule-
based approach that takes into account the in-
formation provided by the first algorithm and
simple linguistic clause boundaries. An initial
version of the system was developed to handle
the annotations of the Bioscope corpus. For
the present version, we have changed, omitted
or extended the rules and the lexicon of cues
(allowing prefix and suffix negation cues, such
as impossible or meaningless), to make it suit-
able for the present task.
1 Introduction
One of the challenges of the *SEM Shared Task
(Morante and Blanco, 2012) is to infer and classify
the scope and event associated to negations, given
a training and a development corpus based on Co-
nan Doyle stories (Morante and Daelemans, 2012).
Negation, simple in concept, is a complex but essen-
tial phenomenon in any language. It turns an affir-
mative statement into a negative one, changing the
meaning completely. We believe therefore that be-
ing able to handle and classify negations we would
be able to improve several text mining applications.
Previous to this Shared Task, we can find several
systems that handle the scope of negation in the state
of the art. This is a complex problem, because it re-
quires, first, to find and capture the negation cues,
and second, based on either syntactic or semantic
representations, to identify the words that are di-
rectly (or indirectly) affected by these negation cues.
One of the main works that started this trend in natu-
ral language processing was published by Morante?s
team (2008; 2009), in which they presented a ma-
chine learning approach for the biomedical domain
evaluating it on the Bioscope corpus.
In 2010, a Workshop on Negation and Spec-
ulation in Natural Language Processing (Morante
and Sporleder, 2010) was held in Uppsala, Swe-
den. Most of the approaches presented worked in
the biomedical domain, which is the most studied in
negation detection.
The system presented in this paper is a modifica-
tion of the one published in Ballesteros et al (2012).
This system was developed in order to replicate (as
far as possible) the annotations given in the Bio-
scope corpus (Vincze et al, 2008). Therefore, for
the one presented in the task we needed to modify
most of the rules to make it able to handle the more
complex negation structures in the Conan Doyle cor-
pus and the new challenges that it represents. The
present paper has the intention of exemplifying the
problems of such a system when the task is changed.
Our system presented to the Shared Task is based
on the following properties: it makes use of an algo-
rithm that traverses dependency structures, it classi-
fies the scope of the negations by using a rule-based
approach that studies linguistic clause boundaries
and the outcomes of the algorithm for traversing
dependency structures, it applies naive and simple
288
solutions to the problem of classifying the negated
event and it does not use the syntactic annotation
provided in the Conan Doyle corpus (just in an ex-
ception for the negated event annotation).
In Section 2 we describe the algorithms that we
propose for inferring the scope of negation and the
modifications that we needed to make to the previ-
ous version. In Section 3 we discuss the evaluation
performed with the blind test set and development
set and the error analysis over the development set.
Finally, in Section 4 we give our conclusions and
suggestions for future work.
2 Methodology
Our system consists of two algorithms: the first one
is capable of inferring words affected by the negative
operators (cues) by traversing dependency trees and
the second one is capable of annotating sentences
within the scope of negations. This second algo-
rithm is the one in which we change the behaviour in
a deeper way. The first one just serves as a consult-
ing point in some of the rules of the second one. By
using the training set and development set provided
to the authors we modified, omitted or changed the
old rules when necessary.
The first algorithm which traverses a dependency
tree searching for negation cues to determine the
words affected by negations, was firstly applied (at
an earlier stage) to a very different domain (Balles-
teros et al, 2010) obtaining interesting results. At
that time, the Minipar parser (Lin, 1998) was se-
lected to solve the problem in a simple way with-
out needing to carry out several machine learning
optimizations which are well known to be daunting
tasks. We also selected Minipar because at that mo-
ment we only needed unlabelled parsing.
Therefore, our system consists of three different
modules: a static negation cue lexicon, an algorithm
that from a parse given by Minipar and the nega-
tion cue lexicon produces a set of words affected
by the negations, and a rule-based system that pro-
duces the annotation of the scope of the studied sen-
tence. These components are described in the fol-
lowing sections.
In order to annotate the sentence as it is done in
the Conan Doyle corpus, we also developed a post-
processing system that makes use of the outcomes
of the initial system and produces the expected out-
put. Besides this, we also generate a very naive rule-
based approach to handle the problem of annotating
the negated event.
It is worth to mention that we did not make
use of the syntactic annotation provided in the Co-
nan Doyle corpus, our input is the plain text sen-
tence. Therefore, the system could work without the
columns that are included in the annotation, just with
the word forms. We only make use of the annota-
tion when we annotate the negated event, checking
the part-of-speech tag to ascertain whether the cor-
responding word is a verb or not. The system could
work without these columns but only the results of
the negated event would be affected.
2.1 Negation Cue Lexicon
The lexicon containing the negation cues is static. It
can be extended indefinitely but it has the restriction
that it does not learn and it does not grow automat-
ically when applying it to a different domain. The
lexicon used in the previous system (Ballesteros et
al., 2012) was also static but it was very small com-
pared to the one employed by the present system,
just containing less than 20 different negation cues.
Therefore, in addition to the previous lexicon, we
analysed the training set and development sets and
extracted 153 different negation cues (plus the ones
already present in the previous system). We stored
these cues in a file that feeds the system when it
starts. Table 1 shows a small excerpt of the lexicon.
not no neither..nor
unnecessary unoccupied unpleasant
unpractical unsafe unseen
unshaven windless without
Table 1: Excerpt of the lexicon
2.2 Affected Wordforms Detection Algorithm
The algorithm that uses the outcomes of Minipar is
the same employed in (Ballesteros et al, 2012) with-
out modifications. It basically traverses the depen-
dency structures and returns for each negation cue a
set of words affected by the cue.
The algorithm takes into account the way of han-
dling main verbs by Minipar, in which these verbs
289
appear as heads and the auxiliary verbs are depen-
dants of them. Therefore, the system first detects the
nodes that contain a word which is a negation cue,
and afterwards it does the following:
? If the negation cue is a verb, such as lack, it is
marked as a negation cue.
? If the negation cue is not a verb, the algorithm
marks the main verb (if it exists) that governs
the structure as a negation cue.
For the rest of nodes, if a node depends directly
on any of the ones previously marked as negation
cue, the system marks it as affected. The negation is
also propagated until finding leaves, so wordforms
that are not directly related to the cues are detected
too.
Finally, by using all of the above, the algorithm
generates a list of words affected by each negation
cue.
2.3 Scope Classification Algorithm
This second algorithm is the one that has suffered
the deepest modifications from the first version. The
previous version handled the annotation as it is done
in the Bioscope corpus. The algorithm works as fol-
lows:
? The system opens a scope when it finds a new
negation cue detected by the affected word-
forms detection algorithm. In Bioscope, only
the sentences in passive voice include the sub-
ject inside the scope. However, the Conan
Doyle corpus does not contain this exception
always including the subject in the scope when
it exists. Therefore, we modified the decision
that fires this rule, and we apply the way of an-
notating sentences in passive voice for all the
negation cues, either passive or active voice
sentences.
Therefore, for most of the negation cues the
system goes backward and opens the scope
when it finds the subject involved or a marker
that indicates another statement, like a comma.
There are some exceptions to this, such as
scopes in which the cue is without or nei-
ther...nor. For them the system just opens the
scope at the cue.
? The system closes a scope when there are no
more wordforms to be added, i.e.:
? It finds words that indicate another state-
ment, such as but or because.
? No more words in the output of the first
algorithm.
? End of the sentence.
? We also added a new rule that can handle the
negation cues that are prefix or suffix of another
word, such as meaning-less: if the system finds
a cue word like this, it then annotates the suffix
or prefix as the cue (such as less) and the rest of
the word as part of the scope. Note that the Af-
fected Wordforms Detection algorithm detects
the whole word as a cue word.
2.4 Negated Event Handling
In order to come up with a solution that could pro-
vide at least some results in the negated event han-
dling, we decided to do the following:
? When the cue word contains a negative prefix
or a negative suffix, we annotate the word as
the negated event.
? When the cue word is either not or n?t and the
next word is a verb, according to the part-of-
speech annotation of the Conan Doyle corpus,
we annotate the verb as the negated event.
2.5 Post-Processing Step
The post-processing step basically processes the an-
notated sentence with Bioscope style, (we show
an example for clarification: <scope>There is
<cue>no</cue> problem</scope>). It tokenizes
the sentences, in which each token is a word or a
wordform, after that, it does the following:
? If the token contains the string <scope>, the
system just starts a new scope column reserv-
ing three new columns and it puts the word in
the first free ?scope? column. Because it means
that there is a new scope for the present sen-
tence.
? If the token is between a <cue> annotation, the
system puts it in the corresponding free ?cue?
column of the scope already opened.
290
? If the token is annotated as ?negated event?, the
system just puts the word in the last column of
the scope already opened.
Note that these three rules are not exclusive and
can be fired for the same token, but in this case they
are fired in the same order as they are presented.
3 Results and Discussion
In this section we first show the evaluation results
and second the error analysis after studying the re-
sults on the development set.
3.1 Results
In this section we show the results obtained in two
different tables: Table 2 shows the results of the sys-
tem with the test set, Table 3 shows the results of the
system with the development set.
As we can observe, the results for the develop-
ment set are higher than the ones obtained for the
test set. The reason is simple, we used the develop-
ment set (apart from the training set) to modify the
rules and to make the system able to annotate the
sentences of the test set.
Note that our system only detects some of the
negation cues (around 72% F1 and 76% F1, respec-
tively, for the test and development sets). We there-
fore believe that one of the main drawbacks of the
present system is the static lexicon of cues. In the
previous version, due to the simplicity of the task,
this was not an issue. However, it is worth noting
that once the negation is detected the results are not
that bad, we show a high precision in most of the
tasks. But the recall suffers due to the coverage of
the lexicon.
It is also worth noting that for the measure Scope
tokens, which takes into account the tokens included
in the scope but not a full scope match, our system
provides interesting outcomes (around 63% F1 and
73% F1, respectively), showing that it is able to an-
notate the tokens in a similar way. We believe that
this fact evidences that the present system comes
from a different kind of annotation and a different
domain, and the extension or modification of such a
system is a complex task.
We can also observe that the negated events re-
sults are very low (around 17.46% F1 and 22.53%
F1, respectively), but this was expected because by
using our two rules we are only covering two cases
and moreover, these two cases are not always behav-
ing in the same way in the corpora.
3.2 Error Analysis
In this section we analyse the different errors of our
system with respect to the development set. This set
contains 787 sentences, of which 144 are negation
sentences containing 168 scopes, 173 cues and 122
negation events.
With respect to the negation cue detection we
have obtained 58 false negatives (fn) and 16 false
positives (fp). These results are not directly derived
from the static lexicon of cues. The main problem is
related with the management of sentences with more
than one scope. The majority of the errors have been
produced because in some cases all the cues are as-
signed to all the scopes detected in the same sen-
tence, generating fp, and in other cases the cues of
the second and subsequent scopes are ignored, gen-
erating fn. The first case occurs in sentences like
(1), no and without are labelled as cues in the two
scopes. The second case occurs in sentences like
(2), where neither the second scope nor the second
cue are labelled. In sentence (3) un is labelled as
cue two times (unbrushed, unshaven) but within the
same scope, generating a fp in the first scope and a
fn in the second one.
? (1) But no [one can glance at your toilet and at-
tire without [seeing that your disturbance dates
from the moment of your waking .. ?]]
? (2) [You do ]n?t [mean] - . [you do] n?t [mean
that I am suspected] ? ?
? (3) Our client smoothed down [his] un[brushed
hair] and felt [his] un[shaven chin].
We also found false negatives that occur in multi
word negation cues as by no means, no more and
rather than.
A different kind of false positives is related to
modality cues, dialogue elements and special cases
(Morante and Blanco, 2012). For example, no in (4),
not in (5) and save in (6).
? (4) ? You traced him through the telegram , no
[doubt]., ? said Holmes .
291
Test set gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 235 170 39 94 81.34 64.39 71.88
Scopes(cue match): 249 233 96 47 153 67.13 38.55 48.98
Scopes(no cue match): 249 233 96 48 152 66.90 38.96 49.24
Scope tokens(no cue match): 1805 2096 1222 874 583 58.30 67.70 62.65
Negated(no cue match): 173 81 36 42 134 46.15 21.18 29.03
Full negation: 264 235 29 39 235 42.65 10.98 17.46
Table 2: Test set results.
Development gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 173 161 115 16 58 87.79 66.47 75.66
Scopes(cue match): 168 160 70 17 98 80.46 41.67 54.90
Scopes(no cue match): 168 160 70 17 98 80.46 41.67 54.90
Scope tokens(no cue match): 1348 1423 1012 411 336 71.12 75.07 73.04
Negated(no cue match): 122 71 35 31 82 53.03 29.91 38.25
Full negation: 173 161 24 16 149 60.00 13.87 22.53
Table 3: Development set results.
? (5) ? All you desire is a plain statement , [is it]
not ? ?.
? (6) Telegraphic inquiries ... that [Marx knew]
nothing [of his customer save that he was a
good payer] .
We can also find problems with affixal negations,
that is, bad separation of the affix and root of the
word. For example, in (7) dissatisfied was erro-
neously divided in di- and ssatisfied. Again, it is
derived from the use of a static lexicon.
? (7) He said little about the case, but from
that little we gathered that [he also was not
dis[satisfied] at the course of events].
Finally, we could also find cases that may be due
to annotation errors. For example, incredible is not
annotated as negation cue in (8). The annotation of
this cue we think is inconsistent, it appears 5 times
in the training corpus, 2 times is labelled as cue, but
3 times is not. According to the context in this sen-
tence, incredible means not credible.
? (8) ?Have just had most incredible and
grotesque experience.
With respect to the full scope detection, most of
the problems are due again to the management of
sentences with more than one scope. We have ob-
tained 98 fn and 17 fp. Most of the problems are
related with affixal negations, as in (9), in which all
the words are included in the scope, which accord-
ing to the gold standard is not correct.
? (9) [Our client looked down with a rueful face
at his own] un[conventional appearance].
With respect to the scope tokens detection, the
results are higher, around 73% F1 in scope tokens
compared to 55% in full match scopes. The reason
is because our system included tokens for the ma-
jority of scopes, increasing the recall until 75% but
lowering the precision due to the inclusion of more
fp.
4 Conclusions and Future Work
In this paper we presented our participation in the
SEM-Shared Task, with a modification of a rule-
based system that was designed to be used in a dif-
ferent domain. As the main conclusion we could say
that modifying such a system to perform in a differ-
ent type of texts is complicated. However, taking
into account this fact, and the results obtained, we
are tempted to say that our system presents compet-
itive results.
292
We believe that the present system has a lot of
room for improvement: (i) improve the manage-
ment of sentences with more than one scope modify-
ing the scope classification algorithm and the post-
processing step, (ii) replacing the dependency parser
with a state-of-the-art parser in order to get higher
performance, or (iii) proposing a different way of
getting a reliable lexicon of cues, by using a seman-
tic approach that informs if the word has a negative
meaning in the context of the sentence. Again, this
could be achieved by using one of the parsers pre-
sented in the ConLL 2008 Shared Task (Surdeanu et
al., 2008).
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-01
Project).
References
Miguel Ballesteros, Rau?l Mart??n, and Bele?n D??az-Agudo.
2010. Jadaweb: A cbr system for cooking recipes. In
Proceedings of the Computing Cooking Contest of the
International Conference of Case-Based Reasoning.
Miguel Ballesteros, Virginia Francisco, Alberto D??az,
Jesu?s Herrera, and Pablo Gerva?s. 2012. Inferring the
scope of negation in biomedical documents. In Pro-
ceedings of the 13th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLING 2012), New Delhi. Springer.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems, Granada.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics (*SEM 2012),
Montreal, Canada.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning, CoNLL
?09, pages 21?29, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC). Istanbul, Turkey.
Roser Morante and Caroline Sporleder, editors. 2010.
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, Uppsala, Swe-
den.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 715?724, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the Twelfth Conference on Natural Language Learn-
ing, pages 159?177, Manchester, United Kingdom.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
293
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 63?70,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Effective Morphological Feature Selection
with MaltOptimizer at the SPMRL 2013 Shared Task
Miguel Ballesteros
Natural Language Processing Group
Pompeu Fabra University.
Barcelona, Spain
miguel.ballesteros@upf.edu
Abstract
The inclusion of morphological features pro-
vides very useful information that helps to en-
hance the results when parsing morphologi-
cally rich languages. MaltOptimizer is a tool,
that given a data set, searches for the opti-
mal parameters, parsing algorithm and opti-
mal feature set achieving the best results that
it can find for parsers trained with MaltParser.
In this paper, we present an extension of Mal-
tOptimizer that explores, one by one and in
combination, the features that are geared to-
wards morphology. From our experiments
in the context of the Shared Task on Parsing
Morphologically Rich Languages, we extract
an in-depth study that shows which features
are actually useful for transition-based pars-
ing and we provide competitive results, in a
fast and simple way.
1 Introduction
Since the CoNLL Shared Tasks on Syntactic Depen-
dency parsing (Buchholz and Marsi, 2006; Nivre et
al., 2007), the number of treebanks and new pars-
ing methods have considerably increased. Thanks to
that, it has been observed that parsing morphologi-
cally rich languages (henceforth, MRLs) is a chal-
lenge because these languages include multiple lev-
els of information that are difficult to classify and,
therefore, to parse. This is why there has been recent
research in this direction, with for instance a Special
Issue in Computational Linguistics (Tsarfaty et al,
2012b).
MaltOptimizer (Ballesteros and Nivre, 2012b;
Ballesteros and Nivre, 2012a) is a system that is ca-
pable of providing optimal settings for training mod-
els with MaltParser (Nivre et al, 2006a), a freely
available transition-based parser generator. MaltOp-
timizer, among other things, performs an in-depth
feature selection, selecting the attributes that help
to achieve better parsing results. In this paper ?
and in this participation in the Shared Task on Pars-
ing Morphologically Rich Languages (Seddah et al,
2013) ? we present an extension of MaltOptimizer
that performs a deeper search over the morpholog-
ical features that are somewhat one of the keys to
parsing MRLs. Instead of lumping all morphosyn-
tactic features together, we define a different field for
each individual feature (case, number, gender, etc.).
Hence, we are able to extract a study that shows
which features are actually useful for parsing MRLs
with MaltParser.
The new SPMRL-MaltOptimizer imple-
mentation is available for download at
http://nil.fdi.ucm.es/maltoptimizer/spmrl.html.
It is worth noting that it can be applied to any
treebank in CoNLL data format.1
The rest of the paper is organized as follows. Sec-
tion 2 describes MaltOptimizer. Section 3 shows
how we modified MaltOptimizer to make it able to
perform a more complete morphological feature se-
lection. Section 4 describes the experiments that we
carried out with the data sets of the Shared Task on
Parsing Morphologically Rich Languages. Section
5 reports the results of the experiments and the con-
clusions that we can extract. Section 6 discusses re-
lated work on MaltOptimizer and parsing morpho-
logically rich languages. And finally, Section 7 con-
1http://ilk.uvt.nl/conll/#dataformat
63
cludes.
2 MaltOptimizer
MaltOptimizer is a system written in Java that im-
plements a full optimization procedure for Malt-
Parser based on the experience acquired from pre-
vious experiments (Hall et al, 2007; Nivre and
Hall, 2010). MaltOptimizer attempts to find the best
model that it can find, but it does not guarantee that
the outcome is the best model possible because of
the difficulty of exploring all the possibilities that are
provided by the parameters, parsing algorithms and
different feature windows. The optimization proce-
dure is divided in 3 different phases, as follows:
1. Data analysis and initial optimization.
2. Parsing algorithm selection.
3. Feature selection and LIBLINEAR optimiza-
tion.
MaltOptimizer divides the treebank into a train-
ing set and a held-out test set for evaluation. In the
first phase, MaltOptimizer makes an analysis of the
treebank in order to set up the rest of the optimiza-
tion, and it attempts the optimization with some gen-
eral parameters, such as the way of handling covered
roots.2 After that, it tests the parsing algorithms that
are available in MaltParser by selecting the one that
provides best results in default settings. In the third
phase, it explores a wide range of features that are
based on previous parsing steps and/or the informa-
tion annotated in the treebanks. Finally, it also ex-
plores the single hyper-parameter (c) of the LIBLIN-
EAR classifier.
In the next Section, we present how we updated
MaltOptimizer for our participation in the Shared
Task of parsing MRLs.
3 Morphological Feature Exploration
The CoNLL data format contains several columns
of information that help to perform the dependency
parsing of a sentence. One of the columns is the
FEATS column that normally contains a set of mor-
phological features, which is normally of the format
a=x|b=y|c=z. At the time of writing, the available
2A covered root is a root node covered by a dependency arc.
version of MaltOptimizer explores the features in-
cluded in this column as a single feature, by lumping
all morphosyntactic features in the MaltParser clas-
sifier, and by splitting the information but including
all of them at the same time without making any dis-
tinctions. This is what MaltParser allows by using
the standard CoNLL format, which contains the fol-
lowing information per column.
1. ID: Identifier.
2. FORM: Word form.
3. LEMMA: Lemma or stemmed version of the
word.
4. CPOSTAG: Coarse-grained part-of-speech
tag.
5. POSTAG: Fine-grained part-of-speech tag.
6. FEATS: Morphosyntactic features (e.g., case,
number, tense, etc.). It is normally of the for-
mat a=x|b=y|c=z.
7. HEAD: Head node.
8. DEPREL: Dependency relation to head.
9. PHEAD: Projective head node.
10. PDEPREL: Projective dependency relation to
head.
However, MaltParser also provides the option of
parsing new data formats that are derived from the
original CoNLL format. Therefore, there is the pos-
sibility to add new columns that may contain use-
ful information for parsing. The new MaltOptimizer
implementation automatically generates a new data
format and a new data set. It creates new columns
that only contain the information of a single feature
which is included in the FEATS column.
Figure 1 shows two versions of a sentence anno-
tated in the French treebank from the Shared Task.
The one shown above is in the standard CoNLL for-
mat, and the one shown below is the extended format
generated by MaltOptimizer in which the FEATS
column has been divided in 10 different columns.
64
1 En en P P mwehead=ADV+|pred=y 4 mod
2 tout tout D DET g=m|n=s|s=ind|pred=y 1 dep_cpd
3 cas cas N NC g=m|s=c|pred=y 1 dep_cpd
4 est ?tre V V m=ind|n=s|p=3|t=pst 0 root
5 -il il CL CLS g=m|n=s|p=3|s=suj 4 suj
6 plus plus ADV ADV _ 7 mod
7 nuanc nuanc A ADJ g=m|n=s|s=qual 4 ats
8 . . PONCT PONCT s=s 4 ponct
1 En en P P _ _ _ _ _ _ ADV+ y _ _ 4 mod
2 tout tout D DET ind m s _ _ _ _ y _ _ 1 dep_cpd
3 cas cas N NC c m _ _ _ _ _ y _ _ 1 dep_cpd
4 est ?tre V V _ _ s 3 ind pst _ _ _ _ 0 root
5 -il il CL CLS suj m s 3 _ _ _ _ _ _ 4 suj
6 plus plus ADV ADV _ _ _ _ _ _ _ _ _ _ 7 mod
7 nuanc nuanc A ADJ qual m s _ _ _ _ _ _ _ 4 ats
8 . . PONCT PONCT s _ _ _ _ _ _ _ _ _ 4 ponct
Figure 1: A sentence from the French treebank in the standard (above) and complex (below) formats. The projective
columns have been removed for simplicity.
4 Experiments
With the intention of both assessing the usefulness
of the new MaltOptimizer implementation and test-
ing which features are useful for each targeted lan-
guage, we carried out a series of experiments over
the data sets from the Shared Task on Parsing MRLs
(Seddah et al, 2013). We run the new MaltOpti-
mizer implementation for all the data sets provided
by the Shared Task organizers and we run Malt-
Parser with the model suggested. Therefore, we had
36 different runs, 4 for each language (gold and pre-
dicted scenarios with 5k treebanks, and gold and
predicted scenarios with full treebanks).
In order to have a comparable set of results, we
performed all the optimization processes with the
smaller versions of the treebanks (5k) and both op-
timization and training steps with both the small
and larger version for all languages. Each MaltOp-
timizer run took approximately 3-4 hours for opti-
mization (the running time also depends on the size
of the set of morphological features, or other param-
eters, such as the number of dependency relations)
and it takes around 20 extra minutes to get the final
model with MaltParser. These estimates are given
with an Intel Xeon server with 8 cores, 2.8GHz and
a heap space of, at least, 8GB.
5 Results and Discussion
Table 1 shows the results for gold-standard input
while Table 2 shows the results for the provided pre-
dicted inputs for the best model that the new Mal-
tOptimizer implementation can find (Dev-5k, Dev,
Test-5k and Test) and a baseline, which is Malt-
Parser in default settings (Malt-5k and Malt) on the
test sets. The first conclusion to draw is that the dif-
ference between gold and predicted inputs is nor-
mally of 2 points, however for some languages such
as French the drop reaches 6 points. It is also ev-
idenced that, as shown by Ballesteros and Nivre
(2012a), some languages benefit more from the fea-
ture selection phase, while others achieve higher im-
provements by selecting a different parsing algo-
rithm.
In general terms, almost all languages bene-
fit from having an accurate stemmed version of
the word in the LEMMA column, providing very
substantial improvements when accurately selecting
this feature. Another key feature, for almost all lan-
guages, is the grammatical CASE that definitely en-
hances the performance; we can therefore conclude
that it is essential for MRLs. Both aspects evidence
the lexical challenge of parsing MRLs without using
this information.
There is a positive average difference comparing
with the MaltParser baseline of 4.0 points training
over the full treebanks and predicted scenario and
5.6 points training over the full treebanks and gold
scenario. It is therefore evident how useful MaltOp-
timizer is when it can perform an in-depth morpho-
logical feature exploration. In the following subsec-
tions we explain the results for each targeted lan-
guage, giving special emphasis to the ones that turn
out to be more meaningful.
5.1 Arabic
For Arabic, we used the shared task Arabic data
set, originally provided by the LDC (Maamouri et
65
Language Default Phase 1 Phase 2 Phase 3 Diff Dev-5k Dev Malt-5k Malt Test-5k Test
Arabic 83.48 83.49 83.49 87.95 4.47 85.98 87.60 80.36 82.28 85.30 87.03
Basque 67.05 67.33 67.45 79.89 13.30 80.35 81.65 67.13 69.19 81.40 82.07
French 77.96 77.96 78.27 85.24 7.28 85.19 86.30 78.16 79.86 84.93 85.71
German 79.90 81.09 84.85 87.70 7.80 87.32 90.40 76.64 79.98 83.59 86.96
Hebrew 76.78 76.80 79.37 80.17 3.39 79.83 79.83 76.61 76.61 80.03 80.03
Hungarian 70.37 71.11 71.98 81.91 11.54 80.69 80.74 71.27 72.34 82.37 83.14
Korean 87.22 87.22 87.22 88.94 1.72 86.52 90.20 81.69 88.43 83.74 89.39
Polish 75.52 75.58 79.28 80.27 4.75 81.58 81.91 76.64 77.70 79.79 80.49
Swedish 76.75 76.75 78.91 79.76 3.01 74.85 74.85 75.73 75.73 77.67 77.67
Table 1: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task
on PMRLs in the gold scenario on the held-out test set for optimization. The first columns shows results per phase
(the procedure of each phase is briefly described in Section 2) on the held-out sets for evaluation. The Dev-5k and
Dev columns report labeled attachment score on the development sets. The columns Malt and Malt-5k report results
of MaltParser in default settings on the test sets. And the columns, Test-5k and Test report results for the best model
found by SPMRL-MaltOptimizer on the test sets.
Language Default Phase 1 Phase 2 Phase 3 Diff Dev-5k Dev Malt-5k Malt Test-5k Test
Arabic 83.20 83.21 83.21 85.68 2.48 80.35 82.28 78.30 80.36 79.64 81.90
Basque 68.80 69.33 69.89 77.24 8.44 78.12 79.46 68.12 70.11 77.59 78.58
French 77.43 77.43 77.63 79.42 1.99 77.65 79.33 76.54 77.98 77.56 79.00
German 78.69 79.87 82.58 83.97 5.28 83.39 86.63 74.81 77.81 79.22 82.75
Hebrew 76.29 76.31 79.01 79.67 3.38 73.40 73.40 69.97 69.97 73.01 73.01
Hungarian 68.26 69.12 69.96 78.71 10.45 76.82 77.62 69.08 70.15 79.00 79.63
Korean 80.08 80.08 80.08 81.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65
Polish 74.43 74.49 76.93 78.41 3.98 80.61 80.83 75.29 75.63 79.50 80.49
Swedish 74.53 74.53 76.51 77.66 3.13 72.90 72.90 73.21 73.21 75.82 75.82
Table 2: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task on
PMRLs in the predicted scenario on the held-out test set for optimization. The columns of this table report the results
in the same way as Table 1 but using predicted inputs.
al., 2004), specifically its SPMRL 2013 dependency
instance, derived from the Columbia Catib Tree-
bank (Habash and Roth, 2009; Habash et al, 2009),
extended according to the SPMRL 2013 extension
scheme (Seddah et al, 2013).
For the gold input, the most useful feature is, by
far, DASHTAG3 with an improvement of 2 points.
CASE is also very useful, as it is for most of the
languages, with 0.67 points. Moreover, SUBCAT
(0.159) and CAT (0.129) provide improvements as
well.
In the pred scenario, there is no DASHTAG, and
this allows other features to rise, for instance, CASE
(0.66), CPOSTAG (0.12), GENDER (0.08), SUB-
CAT (0.07) and CAT (0.06) provide improvements.
Finally it is worth noting that the TED accuracy
3DASHTAG comes from the original constituent data, when
a DASHTAG was present in a head node label, this feature was
kept in the Catib corpus.
(Tsarfaty et al, 2011) for the lattices is 0.8674 with
the full treebanks and 0.8563 with 5k treebanks,
which overcomes the baseline in more than 0.06
points, this shows that MaltOptimizer is also useful
under TED evaluation constraints.
5.2 Basque
The improvement provided by the feature selection
for Basque (Aduriz et al, 2003) is really high. It
achieves almost 13 points improvement with the
gold input and around 8 points with the predicted
input. The results in the gold scenario are actually
a record if we also consider the experiments per-
formed over the treebanks of the CoNLL Shared
Tasks (Ballesteros and Nivre, 2012a). One of the
reasons is the treatment of covered roots that is opti-
mized during the first phase of optimization. This
corpus has multiple root labels, ROOT being the
most common one and the one selected by MaltOp-
66
timizer as default.
For the gold input, the CPOSTAG and LEMMA
columns turn out to be very useful, providing an
improvement of 2.5 points and slightly less than 1
point respectively, MaltOptimizer selects them all
over the more central tokens over the stack and the
buffer. The Basque treebank contains a very big
set of possible features in the FEATS column, how-
ever only some of them provide significant improve-
ments, which evidences the usefulness of selecting
them one by one. The most useful feature with a
huge difference is KASE (or CASE) that provides
5.9 points by itself. MaltOptimizer fills out all the
available positions of the stack and the buffer with
this feature. Another useful feature is ERL [type of
subordinated sentence], providing almost 0.8 points.
Moreover, NUMBER (0.3), NORK2 (0.15), ASP
[aspect] (0.09), NOR1 (0.08), and NMG (0.06) pro-
vide slighter, but significant, improvements as well.4
Surprisingly, the predicted input provides bet-
ter results in the first 2 phases, which means that
for some reason MaltParser is able to parse better
by using just the predicted POS column, however,
the improvement achieved by MaltOptimizer dur-
ing Phase 3 are (just) a bit more than 7 points. In
this case, the CPOSTAG column is less useful, pro-
viding only 0.13 points, while the LEMMA (1.2) is
still very useful. CASE provides 4.5 points, while
NUM (0.17), ASP (0.13) and ADM (0.11) provide
improvements as well.
5.3 French
For French (Abeille? et al, 2003) there is a huge dif-
ference between the results with gold input and the
results with predicted input. With gold input, the
feature selection provides a bit less than 8 points
while there is just an improvement of around 2
points with predicted input. In this case, the lack
of quality in the predicted features is evident. It is
also interesting that the lexical column, FORM, pro-
vides a quite substantial improvement when Mal-
tOptimizer attempts to modify it, which is some-
thing that does not happen with the rest of lan-
guages.
For the gold input, apart from LEMMA that pro-
vides around 0.7 points, the most useful feature is
4NORK2, NOR1 and NMG are auxiliaries case markers.
MWEHEAD [head of a multi word expression, if
exists] that does not exist in the predicted scenario.
MWEHEAD provides more than 4 points; this fact
invites us to think that a predicted version of this
feature would be very useful for French, if possi-
ble. PRED [automatically predicted] (0.8), G [gen-
der] (0.6), N [number] (0.2) and S [subcat] (0.14)
are also useful.
In the predicted scenario, the CPOSTAG column
provides some improvements (around 0.1) while the
LEMMA is less useful than the one in the gold sce-
nario (0.2). The morphological features that are use-
ful are S [subcat] (0.3) and G [gender] (0.3).
5.4 German
For German (Brants et al, 2002) the results are more
or less in the average. For the gold input, LEMMA
is the best feature providing around 0.8 points; from
the morphological features the most useful one is, as
expected, CASE with 0.58 points. GENDER (0.16)
and NUMBER (0.16) are also useful.
In the predicted scenario, CASE is again very use-
ful (0.67). Other features, such as, NUMBER (0.10)
and PERSON (0.10) provide improvements, but as
we can observe a little bit less than the improve-
ments provided in the gold scenario.
5.5 Hebrew
For the Hebrew (Sima?an et al, 2001; Tsarfaty,
2013) treebank, unfortunately we did not see a lot
of improvements by adding the morphological fea-
tures. For the gold input, only CPOSTAG (0.08)
shows some improvements, while the predicted sce-
nario shows improvements for NUM (0.08) and PER
(0.08). It is worth noting that the TED accuracy
(Tsarfaty et al, 2011) for the lattices is 0.8305 which
is ranked second.
This outcome is different from the one obtained
by Goldberg and Elhadad (2010), but it is also true
that perhaps by selecting a different parsing algo-
rithm it may turn out different, because two parsers
may need different features, as shown by Zhang and
Nivre (2012). This is why, it would be very interest-
ing to perform new experiments with MaltOptimizer
by testing different parsing algorithms included in
MaltParser with the Hebrew treebank.
67
5.6 Hungarian
The Hungarian (Vincze et al, 2010) results are
also very consistent. During the feature selection
phase, MaltOptimizer achieves an improvement of
10 points by the inclusion of morphological features.
This also happens in the initial experiments per-
formed with MaltOptimizer (Ballesteros and Nivre,
2012a), by using the Hungarian treebank of the
CoNLL 2007 Shared Task. The current Hungarian
treebank presents covered roots and multiple root la-
bels and this is why we also get substantial improve-
ments during Phase 1.
For the gold input, as expected the LEMMA col-
umn is very useful, providing more than 1.4 points,
while MaltOptimizer selects it all over the available
feature windows. The best morphological feature
is again CASE providing an improvement of 5.7
points just by itself, in a similar way as in the ex-
periments with Basque. In this case, the SUBPOS
[grammatical subcategory] feature that is included
in the FEATS column is also very useful, provid-
ing around 1.2 points. Other features that are useful
are NUMP [number of the head] (0.2), NUM [num-
ber of the current token] (0.16), DEF [definiteness]
(0.11) and DEG [degree] (0.09).
In the predicted scenario, we can observe a sim-
ilar behavior for all features. MOOD provides 0.4
points while it does not provide improvements in the
gold scenario. The results of the SUBPOS feature
are a bit lower in this case (0.5 points), which evi-
dences the quality lost by using predicted inputs.
5.7 Korean
As Korean (Choi, 2013) is the language in which
our submission provided the best results comparing
to other submissions, it is interesting to dedicate a
section by showing its results. For the 5k input, our
model provides the best results of the Shared Task,
while the results of the model trained over the full
treebank qualified the second.
For the gold input, the most useful feature is
CPOSTAG providing around 0.6 points. Looking
into the morphological features, CASE, as usual, is
the best feature with 0.24 points, AUX-Type (0.11),
FNOUN-Type (0.08) are also useful.
In the predicted scenario, MaltOptimizer per-
forms similarly, having CPOSTAG (0.35) and CASE
(0.32) as most useful features. ADJ-Type (0.11) and
PUNCT-Type (0.06) are also useful. The results of
the features are a bit lower with the predicted input,
with the exception of CASE which is better.
5.8 Polish
Polish (S?widzin?ski and Wolin?ski, 2010) is one of the
two languages (with Swedish) in which our model
performs with the worst results.
In the gold scenario only the LEMMA (0.76)
shows some substantial improvements during the
optimization process; unfortunately, the morpholog-
ical features that are extracted when MaltOptimizer
generates the new complex data format did not fire.
For the predicted input, LEMMA (0.66) is again
the most useful feature, but as happened in the gold
scenario, the rest of the features did not fire during
the feature selection.
5.9 Swedish
As happened with Polish, the results for Swedish
(Nivre et al, 2006b) are not as good as we could ex-
pect; however we believe that the information shown
in this paper is useful because MaltOptimizer detects
which features are able to outperform the best model
found so far and the model trained with MaltParser
in default settings by a bit less than 2 points in the
predicted scenario and more than 2 points in the gold
scenario.
For the gold scenario only two features are ac-
tually useful according to MaltOptimizer, MaltOp-
timizer shows improvements by adding GENDER
(0.22) and PERFECTFORM (0.05).
For the predicted input, MaltOptimizer shows im-
provements by adding DEGREE (0.09), GENDER
(0.08) and ABBRV (0.06). However, as we can see
the improvements for Swedish are actually lower
compared to the rest of languages.
6 Related Work
There has been some recent research making use of
MaltOptimizer. For instance, Seraji et al (2012)
used MaltOptimizer to get optimal models for pars-
ing Persian. Tsarfaty et al (2012a) worked with
MaltOptimizer and Hebrew by including the opti-
mization for presenting new ways of evaluating sta-
tistical parsers. Mambrini and Passarotti (2012),
68
Agirre et al (2012), Padro? et al (2013) and Balles-
teros et al (2013) applied MaltOptimizer to test dif-
ferent features of Ancient Greek, Basque and Span-
ish (the last 2) respectively; however at that time
MaltOptimizer did not allow the FEATS column to
be divided. Finally, Ballesteros et al (2012) applied
MaltOptimizer for different parsing algorithms that
are not included in the downloadable version show-
ing that it is also possible to optimize different pars-
ing algorithms.
7 Conclusions
This new MaltOptimizer implementation helps the
developers to adapt MaltParser models to new lan-
guages in which there is a rich set of features. It
shows which features are able to make a change in
the parsing results and which ones are not, in this
way, it is possible to focus annotation effort for the
purpose of parsing. We clearly observe that MaltOp-
timizer outperforms very substantially the results
shown in the baseline, which is MaltParser in default
settings, and it is also nice to see that the improve-
ments provided by MaltOptimizer for the morpho-
logical features are actually very high, if we com-
pare to the ones obtained by MaltOptimizer for the
corpora of the CoNLL shared tasks (Ballesteros and
Nivre, 2012a).
It is worth noting that the experiments with Mal-
tOptimizer do not take so long. The time needed to
perform the optimization is actually very short if we
compare to the efforts needed to achieve results in
the same range of accuracy by careful manual op-
timization. The MaltOptimizer process was sped
up following heuristics derived from deep proven
experience (Nivre and Hall, 2010), which means
that there are several combinations that are untested;
however, it is worth noting that these heuristics re-
sulted in similar performance to more exhaustive
search for a big set of languages (Ballesteros, 2013).
From the feature study shown in Section 5, we ex-
pect that it could be useful for people doing parsing
research and interested in parsing MRLs. Finally,
comparing our submission with the results of other
teams, we believe that we provide a fast and effec-
tive parser optimization for parsing MRLs, having
competitive results for most of the languages.
Acknowledgments
I would like to thank Koldo Gojenola who initially
gave me the idea presented in this paper. I am also
very thankful to Joakim Nivre for his constant help
and support. Finally, special thanks to the organizers
Djame? Seddah, Reut Tsarfaty and Sandra Ku?bler.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT), pages 201?204.
Eneko Agirre, Aitziber Atutxa, and Kepa Sarasola. 2012.
Contribution of complex lexical information to solve
syntactic ambiguity in Basque. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING 2012), Mumbai, India, 12/2012.
Miguel Ballesteros and Joakim Nivre. 2012a. MaltOp-
timizer: A System for MaltParser Optimization. In
Proceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC 2012).
Miguel Ballesteros and Joakim Nivre. 2012b. Mal-
tOptimizer: An Optimization Tool for MaltParser. In
Proceedings of the System Demonstration Session of
the Thirteenth Conference of the European Chapter of
the Association for Computational Linguistics (EACL
2012).
Miguel Ballesteros, Carlos Go?mez-Rodr??guez, and
Joakim Nivre. 2012. Optimizing Planar and 2-
Planar Parsers with MaltOptimizer. Procesamiento del
Lenguaje Natural, 49, 09/2012.
Miguel Ballesteros, Simon Mille, and Alicia Burga.
2013. Exploring Morphosyntactic Annotation Over a
Spanish Corpus for Dependency Parsing . In Proceed-
ings of the Second International Conference on De-
pendency Linguistics (DEPLING 2013).
Miguel Ballesteros. 2013. Exploring Automatic Feature
Selection for Transition-Based Dependency Parsing.
Procesamiento del Lenguaje Natural, 51.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
69
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 149?164.
Jinho D. Choi. 2013. Preparing Korean Data for the
Shared Task on Parsing Morphologically Rich Lan-
guages. ArXiv e-prints, September.
Yoav Goldberg and Michael Elhadad. 2010. Easy first
dependency parsing of modern hebrew. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
SPMRL ?10, pages 103?107, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task of EMNLP-CoNLL 2007, pages 933?939.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools.
Francesco Mambrini and Marco Carlo Passarotti. 2012.
Will a Parser Overtake Achilles? First experiments on
parsing the Ancient Greek Dependency Treebank. In
Proceedings of the Eleventh International Workshop
on Treebanks and Linguistic Theories (TLT11).
Joakim Nivre and Johan Hall. 2010. A quick guide
to MaltParser optimization. Technical report, malt-
parser.org.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC), pages 2216?2219.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915?932.
Muntsa Padro?, Miguel Ballesteros, Hector Mart??nez, and
Bernd Bohnet. 2013. Finding dependency pars-
ing limits over a large spanish corpus. In IJCNLP,
Nagoya, Japan. Association for Computational Lin-
guistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-
dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, and Alina Wro?blewska.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morphologi-
cally rich languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
Mojgan Seraji, Bea?ta Megyesi, and Joakim Nivre. 2012.
Dependency parsers for persian. In Proceedings of
10th Workshop on Asian Language Resources, at 24th
International Conference on Computational Linguis-
tics (COLING 2012). ACL Anthology.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference
(TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Reut Tsarfaty, Joakim Nivre, and Evelina Anders-
son. 2011. Evaluating dependency parsing: Robust
and heuristics-free cross-annotation evaluation. In
EMNLP, pages 385?396, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical
parsing. In EACL, pages 44?54.
Reut Tsarfaty, Djame? Seddah, Sandra Kuebler, and
Joakim Nivre. 2012b. Parsing Morphologically Rich
Languages: Introduction to the Special Issue. Compu-
tational Linguistics, November.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic
Scheme of Stanford Dependencies. Proceedings of
ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
Yue Zhang and Joakim Nivre. 2012. Analyzing the effect
of global learning and beam-search on transition-based
dependency parsing. In COLING, pages 1391?1400.
70
Proceedings of the 8th International Natural Language Generation Conference, pages 108?112,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Classifiers for data-driven deep sentence generation
Miguel Ballesteros1, Simon Mille1 and Leo Wanner2,1
1NLP Group, Department of Information and Communication Technologies
Pompeu Fabra University, Barcelona
2Catalan Institute for Research and Advanced Studies (ICREA)
<fname>.<lname>@upf.edu
Abstract
State-of-the-art statistical sentence gener-
ators deal with isomorphic structures only.
Therefore, given that semantic and syntac-
tic structures tend to differ in their topol-
ogy and number of nodes, i.e., are not iso-
morphic, statistical generation saw so far
itself confined to shallow, syntactic gener-
ation. In this paper, we present a series
of fine-grained classifiers that are essen-
tial for data-driven deep sentence genera-
tion in that they handle the problem of the
projection of non-isomorphic structures.
1 Introduction
Deep data-driven (or stochastic) sentence gener-
ation needs to be able to map abstract seman-
tic structures onto syntactic structures. This has
been a problem so far since both types of struc-
tures differ in their topology and number of nodes
(i.e., are non-isomorphic). For instance, a truly
semantic structure will not contain any functional
nodes,1 while a surface-syntactic structure or a
chain of tokens in a linearized tree will con-
tain all of them. Some state-of-the-art propos-
als use a rule-based module to handle the projec-
tion between non-isomorphic semantic and syn-
tactic structures/chains of tokens, e.g., (Varges and
Mellish, 2001; Belz, 2008; Bohnet et al., 2011),
and some adapt the semantic structures to be iso-
morphic with syntactic structures (Bohnet et al.,
2010). In this paper, we present two alternative
stochastic approaches to the projection between
non-isomorphic structures, both based on a cas-
cade of Support Vector Machine (SVM) classi-
fiers.2 The first approach addresses the projection
as a generic non-isomorphic graph transduction
1See, for instance, (Bouayad-Agha et al., 2012).
2Obviously, other machine learning techniques could also
be used.
problem in terms of four classifiers for 1. identi-
fication of the (non-isomorphic) correspondences
between fragments of the source and target struc-
ture, 2. generation of the nodes of the target struc-
ture, 3. generation of the dependencies between
corresponding fragments of the source and target
structure, and 4. generation of the internal depen-
dencies in all fragments of the target structure.
The second approach takes advantage of the lin-
guistic knowledge about the projection of the in-
dividual linguistic token types. It replaces each
of the above four classifiers by a set of classifiers,
with each single classifier dealing with only one
individual linguistic token type (verb, noun, ad-
verb, etc.) or with a configuration thereof. As will
be seen, the linguistic knowledge pays off: the sec-
ond approach achieves considerably better results.
Since our goal is to address the challenge of the
projection of non-isomorphic structures, we focus,
in what follows, on this task. That is, we do not
build a complete generation pipeline until the sur-
face. This could be done, for instance, by feed-
ing the output obtained from the projection of a
semantic onto a syntactic structure to the surface
realizer described in (Bohnet et al., 2010).
2 The Task
The difference in the linguistic abstraction of se-
mantic and syntactic structures leads to diver-
gences that impede the isomorphy between the
two and make the mapping between them a chal-
lenge for statistical generation. Let us, before we
come to the implementation, give some theoretical
details on these structures as we picture them and
on the possible approaches to the projection of a
semantic structure to a syntactic one.
2.1 The Notion of semantic and syntactic
structures
As semantic structure, we assume a shallow se-
mantic representation that is very similar to the
108
PropBank (Babko-Malaya, 2005) and deep anno-
tations as used in the Surface Realisation Shared
Task (Belz et al., 2011): the deep-syntactic layer
of the AnCora-UPF corpus (Mille et al., 2013).
Deep-syntactic structures (DSyntSs) do not
contain any punctuation and functional nodes, i.e.,
governed prepositions and conjunctions, auxil-
iaries and determiners.3
As syntactic structure (in the terminology
of Ancora-UPF: surface-syntactic structures,
SSyntSs), we assume dependency trees in which
the nodes are labeled by open or closed class
lexemes and the edges by grammatical function
relations of the type subject, oblique object,
adverbial, modifier, etc.; cf.4 See Figure 1 for a
contrastive illustration of DSyntS and SSyntS.
rumor want new song be successful
I
II
ATTR I II
a rumor wants that the new song will be successful
det subj dobj
conj
detmodif subj analyt fut copul
Figure 1: DSyntS (above) and SSyntS (below) of
an English Sentence.
Note, however, that the proposal outlined be-
low for the projection of non-isomorphic struc-
tures is trainable on any multi-layered treebanks
where different layers are not isomorphic.
2.2 Projection of DSyntSs onto SSyntSs
In order to project a DSyntS onto its correspond-
ing SSyntS in the course of sentence generation,
the following types of actions need to be per-
formed:
1. Project each node in the DSyntS onto its SSynS-
correspondence. This correspondence can be a
single node, as, e.g., successful? successful, or a
subtree (hypernode, known as syntagm in linguis-
tics), as, e.g., song ? the song ?DT NN? (where
?DT? is a determiner and ?NN? a noun) or be
? that will be ?IN VAUX VB? (where ?IN? is a
preposition, ?VAUX? an auxiliary and ?VB? a full
verb). In formal terms, we assume any SSyntS-
correspondence to be a hypernode with a cardinal-
ity ? 1.
2. Generate the correct lemma for the nodes in
3For more details on the SSyntS, see (Mille et al., 2013).
4DSyntSs and their corresponding SSyntSs are stored in
the 14-column CoNLL?08 format.
SSyntS that do not have a 1:1 correspondence in
the SSyntS (as ?DT?, ?IN? and ?VAUX? above).
3. Establish the dependencies within the individ-
ual SSyntS-hypernodes.
4. Establish the dependencies between the
SSyntS-hypernodes (more precisely, between the
nodes of different SSyntS-hypernodes) to obtain a
connected SSyntS-tree.
3 Classifiers
As mentioned in the Introduction, the realization
of the actions 1.? 4. can be approached either in
terms of 4 generic classifiers (Section 3.1) or in
terms of 4 sets of fine-grained (micro) classifiers
(Section 3.2) that map one representation onto an-
other. As also mentioned above, we realize both
approaches as Support Vector Machines (SVMs).
3.1 Generic classifier approach
Each of the generic classifiers deals with one of
the following tasks.
a. Hypernode Identification: Given a deep
syntactic node nd from the DSyntS, the system
must find the shape of the surface hypernode (=
syntagm) that corresponds to nd in the SSyntS.
The hypernode identification SVM uses the fol-
lowing features:
POS of nd, POS of nd?s head, voice,
temp. constituency, finiteness, tense, lemma of
nd, and nd?s dependencies.
In order to simplify the task, we define the shape
of a surface hypernode as a list of surface PoS-
tags. This list contains the PoS of each of the lem-
mas within the hypernode and a tag that signals the
original deep node; for instance:
[ VB(deep), VAUX, IN]
b. Lemma Generation. Once the hypernodes
of the SSyntS under construction have been pro-
duced, the functional nodes that have been newly
introduced in the hypernodes must be assigned a
lemma. The lemma generation SVM uses the fol-
lowing features of the deep nodes nd in the hyper-
nodes:
? finiteness, ? definiteness, ? PoS of nd, ? lemma
of nd, ? PoS of the head of nd
to select the most likely lemma.
c. Intra-hypernode Dependency Generation.
Given a hypernode and its lemmas provided by
the two previous stages, the dependencies (i.e., the
dependency attachments and dependency labels)
between the elements of the hypernode must be
109
determined (and thus also the governor of the hy-
pernode). For this task, the intra-hypernode de-
pendency generation SVM uses the following fea-
tures:
? lemmas included in the hypernode, ? PoS-tags
of the lemmas in the hypernode, ? voice of the
head h of the hypernode, ? deep dependency re-
lation to h.
[ VB(deep), VAUX, IN]
analyt fut prepos
Figure 2: Internal dependency within a hypernode.
d. Inter-hypernode Dependency Generation.
Once the individual hypernodes have been con-
verted into connected dependency subtrees, the
hypernodes must be connected between each
other, such that we obtain a complete SSyntS. The
inter-hypernode dependency generation SVM uses
the following features of a hypernode ss:
? the internal dependencies of ss, ? the head of
ss, ? the lemmas of ss, ? the PoS of the depen-
dent of the head of ss in DSyntS
to determine for each hypernode its governor.
[ VB(deep), VAUX, IN] [ NN(deep), DT]
subj
Figure 3: Surface dependencies between two hy-
pernodes.
3.2 Implementation of sets of micro
classifiers
In this alternative approach, a single classifier is
foreseen for each kind of input. Thus, for the
hypernode identification module, for each deep
PoS tag (which can be one of the following four:
?N? (noun), ?V? (verb), ?Adv? (adverb), ?A? (ad-
jective)), a separate multi-class classifier is de-
fined. For instance, in the case of ?N?, the N-
classifier will use the above features to assign
to the a DSynt-node with PoS ?N? the most ap-
propriate (most likely) hypernode?in this case,
[NN(deep), DT]. In a similar way, in the case of
the lemma generation module, for each surface
PoS tag, a separate classifier is defined. Thus,
the DT-classifier would pick for the hypernode
[NN(deep), DT] the most likely lemma for the DT-
node (optimally, a determiner).
For the intra-hypernode attachments module,
for each kind of hypernode, dynamically a sepa-
rate classifier is generated.5 In the case of the hy-
5This implies that the number of classifiers varies depend-
ing on the training set, in the intra-hypernode dependency
generation there are 108 SVMs.
pernode [ VB(deep), VAUX, IN], the correspond-
ing classifier will create a link between the prepo-
sition and the auxiliary, and between the auxiliary
and the verb, with respectively the preposition and
the auxiliary as heads because it is the best link
that it can find; cf. Figure 2 for illustration.
Finally, for the inter-hypernode attachments
module, for each hypernode with a distinct in-
ternal dependency pattern, a separate classifier is
dynamically derived (for our treebank, we ob-
tained 114 different SVM classifiers because it
also takes into account hypernodes with just one
token). For instance, the classifier for the hypern-
ode [ NN(deep), DT] is most likely to identify as
its governor VAUX in the hypernode [ VB(deep),
VAUX, IN]; cf. Figure 3.
4 Experiments and Results
In this section, we present the performance of the
two approaches to DSyntS?SSyntS projection on
the DSyntS- and SSynt-layers of the AnCora-UPF
treebank (Mille et al., 2013).6 Table 1 displays
the results for the generic classifier for all tasks
on the development and the test set, while Table
2 displays the results obtained through the sets of
micro classifiers.
Dev.set # %
Hypernode identification 3131/3441 90.99
Lemma generation 818/936 87.39
Intra-hypernode dep. generation 545/798 68.30
Inter-hypernode dep. generation 2588/3055 84.71
Test set # %
Hypernode identification 5166/5887 87.75
Lemma generation 1822/2084 87.43
Intra-hypernode dep. generation 1093/1699 64.33
Inter-hypernode dep. generation 4679/5385 86.89
Table 1: Results of the evaluation of the generic
classifiers for the non-isomorphic transduction.
The results show that for hypernode identifica-
tion and inter-hypernode dependency generation,
the results of both types of classifiers are compara-
ble, be it on the development set or on the test set.
However, thanks to the micro classifiers, with the
same features, the lemma generation model based
on micro classifiers improves by 4 points and the
intra-hypernode dependency generation by nearly
6Following a classical machine learning set-up, we di-
vided the treebank into: (i) a development set (219 sen-
tences, 3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank); (ii) a training set (3036 sentences,
57665 tokens in the DSyntS treebank and 86984 tokens in
the SSyntS treebank); and a (iii) a held-out test for evalua-
tion (258 sentences, 5641 tokens in the DSyntS treebank and
8955 tokens in the SSyntS treebank).
110
Dev.set # %
Hypernode identification 3133/3441 91.05
Lemma generation 851/936 90.92
Intra-hypernode dep. generation 767/798 96.12
Inter-hypernode dep. generation 2574/3055 84.26
Test set # %
Hypernode identification 5169/5886 87.82
Lemma generation 1913/2084 91.79
Intra-hypernode dep. generation 1630/1699 95.94
Inter-hypernode dep. generation 4648/5385 86.31
Table 2: Results of the evaluation of the micro
classifiers for the non-isomorphic transduction.
30 points. This means that the intra-hypernode de-
pendency generation task is too sparse to be real-
ized as a single classifier. The micro classifiers
are in this case binary, i.e., 2:1, or unary, i.e., 1:1
classifiers, which implies a tremendous reduction
of the search space (and thus higher accuracy). In
contrast, the single classifier is a multi-class clas-
sifier that must decide among more than 60 pos-
sible classes. Although most of these 60 classes
are diferentiated by features, the differentiation
is not perfect. In the case of lemma generation,
we observe a similar phenomenon. In this case,
the micro-classifiers are multi-class classifiers that
normally have to cope with 5 different classes
(lemmas in this case), while the unique classi-
fier has to cope with around 60 different classes
(or lemmas). Hypernode identification and inter-
hypernode dependency generation are completely
guided by the input; thus, it seems that they do not
err in the same way.
Although the micro classifier approach leads
to significantly better results, we believe that it
can still be improved. First, the introduction of
prepositions causes most errors in hypernode de-
tection and lemma generation: when a preposition
should be introduced or not and which preposi-
tion should be introduced depends exclusively on
the sub-categorization frame of the governor of
the deep node. A treebank of a limited size as
used in our experiments simply does not contain
subcategorization patterns of all predicative lexi-
cal items (especially of nouns)?which would be
crucial. Thus, in the test set evaluation, out of the
171 lemma errors 147 are prepositions and out of
the 717 errors on hypernode identification, more
than 500 are due to nouns and preposition. The in-
crease of the size of the treebank would therefore
be an advantage.
Second, in the case of inter-hypernode depen-
dency, errors are due to the labels of the dependen-
cies more than to the attachements, and are quite
distributed over the different types of configura-
tions. The generation of these dependencies suf-
fers from the fact that the SSyntS tag-set is very
fine-grained. For instance, there are 9 different
types of verbal objects in SSyntS,7 which capture
very specific syntactic properties of Spanish, such
as ?can the dependent can be replaced by a clitic
pronoun? Can the dependent be moved away from
its governor? Etc. This kind of information is not
of a high relevance for generation of well-formed
text. Using a more reduced (more coarse-grained)
SSyntS tag set would definitely improve the qual-
ity of the projection.
5 Related work
There is an increasing amount of work on sta-
tistical sentence generation; see, e.g., (Bangalore
and Rambow, 2000; Langkilde-Geary, 2002; Fil-
ippova and Strube, 2008). However, hardly any
addresses the problem of the projection between
non-isomorphic semantic and syntactic structures.
In general, structure prediction approaches use
a single classifier model (Smith, 2011). But
see, e.g., (Carreras et al., 2008), who use dif-
ferent models to predict each part of the triplet
for spinal model pruning, and (Bjo?rkelund et al.,
2010; Johansson and Nugues, 2008), who use
a set of classifiers for predicate identification in
the context of semantic role labelling. Amalgam
(Corston-Oliver et al., 2002), which maps a logi-
cal input onto sentences with intermediate syntac-
tic (phrase-based) representation, uses language-
specific decision trees in order to predict when to
introduce auxiliaries, determiners, cases, etc.
6 Conclusions
We presented two alternative classifier approaches
to deep generation that cope with the projection
of non-isomorphic semantic and syntactic struc-
tures and argued that the micro classifier approach
is more adequate. In spite of possible improve-
ments presented in Section 4, each set of micro
classifiers achieves results above 86% on the test
set. For intra-hypernode dependency generation,
it even reaches 95.94% .
Acknowledgments
This work has been partially funded by the Euro-
pean Commission under the contract number FP7-
ICT-610411.
7There are 47 SSynt dependencies in total, to compare to
the 7 dependencies in the DSyntS.
111
References
Olga Babko-Malaya, 2005. Propbank Annotation
Guidelines.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING),
pages 42?48, Saarbru?cken, Germany.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first Surface Realisation Shared Task: Overview and
evaluation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation (ENLG), pages
217?226, Nancy, France.
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Journal of Natural Lan-
guage Engineering, 14(4):431?455.
A. Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic
dependency parser. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics : Demonstration Volume (COLING), pages 33?
36, Beijing, China.
Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-
cia Burga. 2010. Broad coverage multilingual
deep sentence generation with a stochastic multi-
level realizer. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), pages 98?106, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. StuMaBa: From deep representation
to surface. In Proceedings of the Generation Chal-
lenges Session at the 13th European Workshop on
Natural Language Generation (ENLG), pages 232?
235, Nancy, France.
Nadjet Bouayad-Agha, Gerard Casamayor, Simon
Mille, and Leo Wanner. 2012. Perspective-oriented
generation of football match summaries: Old tasks,
new challenges. ACM Transactions on Speech and
Language Processing, 9(2):3:1?3:31.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the per-
ceptron for efficient, feature-rich parsing. In Pro-
ceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL), pages 9?16,
Manchester, UK.
Simon Corston-Oliver, Michael Gamon, Eric Ringger,
and Robert Moore. 2002. An overview of Amal-
gam: A machine-learned generation module. In
Proceedings of the 2nd International Natural Lan-
guage Generation Conference (INLG), pages 33?40,
New-York, NY, USA.
Katja Filippova and Michael Strube. 2008. Sen-
tence fusion via dependency graph compression.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 177?185, Honolulu, Hawaii.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based Semantic Role Labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 69?78, Honolulu, Hawaii.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of
the 2nd International Natural Language Generation
Conference (INLG), pages 17?24, New-York, NY,
USA. Citeseer.
Simon Mille, Alicia Burga, and Leo Wanner. 2013.
AnCora-UPF: A multi-level annotation of Spanish.
In Proceedings of the 2nd International Conference
on Dependency Linguistics (DepLing), pages 217?
226, Prague, Czech Republic.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Synthesis Lectures on Human Language Technolo-
gies. Morgan and Claypool.
Sebastian Varges and Chris Mellish. 2001. Instance-
based Natural Language Generation. In Proceed-
ings of the 2nd Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 1?8, Pittsburgh, PA, USA.
112
