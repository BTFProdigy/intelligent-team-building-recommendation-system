Automatic Corpus-Based Thai Word Extraction 
with the C4.5 Learning Algorithm 
VIRACH SORNLERTLAMVANICH, TANAPONG POTIPITI AND THATSANEE 
CHAROENPORN 
National Electronics and Computer Technology Centel, 
National Science and Technology Development Agency, 
Ministry of Science and Technology Environntent, 
22 '~1 Floor Gypsum Metiw)olitan Tower 539/2 Sriayudhya Rd. Rajthevi Bangkok 10400 ThailatM 
Email: virach@nectec.or.th, tanapong@nectec.or.th, thatsanee@nectec.or.th 
Abstract 
"Word" is difficult to define in the languages that 
do not exhibit explicit word boundary, such as 
Thai. Traditional methods on defining words for 
this kind of languages have to depend on human 
judgement which bases on unclear criteria o1" 
procedures, and have several limitations. This 
paper proposes an algorithm for word extraction 
from Thai texts without borrowing a hand from 
word segmentation. We employ the c4.5 learning 
algorithm for this task. Several attributes uch as 
string length, frequency, nmtual information and 
entropy are chosen for word/non-word 
determination. Our experiment yields high 
precision results about 85% in both training and 
test corpus. 
1 In t roduct ion  
in the Thai language, there is no explicit word 
boundary; this causes a lot of problems in Thai 
language processing including word 
segmentation, information retrieval, machine 
translation, and so on. Unless there is regularity in 
defining word entries, Thai language processing 
will never be effectively done. The existing Thai 
language processing tasks mostly rely on the 
hand-coded dictionaries to acquire the information 
about words. These manually created ictionaries 
have a lot of drawbacks. First, it cannot deal with 
words that are not registered in the dictionaries. 
Second, because these dictionaries are manually 
created, they will never cover all words that occur 
in real corpora. This paper, therefore, proposes an 
automatic word-extraction algorithm, which 
hopefully can overcome this Thai language- 
processing barrier. 
An essential and non-trivial task for the 
languages that exhibit inexplicit word boundary 
such as Thai, Japanese, and many other Asian 
languages undoubtedly is the task in identifying 
word boundary. "Word", generally, means a unit 
of expression which has universal intuitive 
recognition by native speakers. Linguistically, 
word can be considered as the most stable unit 
which has little potential to rearrangement and is 
uninterrupted as well. "Uninterrupted" here 
attracts our lexical knowledge bases so much. 
There are a lot of uninterrupted sequences of 
words functioning as a single constituent of a 
sentence. These uninterrupted strings, of course 
are not the lexical entries in a dictionary, but each 
occurs in a very high frequency. The way to point 
out whether they are words or not is not 
distinguishable even by native speakers. Actually, 
it depends on individual judgement. For example, 
a Thai may consider 'oonfila~mu' (exercise) a whole 
word, but another may consider 'n~n~m~' as a 
compound: 'oon' (take)+ 'filg~' (power)+ 'too' (body). 
Computationally, it is also difficult to decide 
where to separate a string into words. Even 
though it is reported that the accuracy of recent 
word segmentation using a dictionary and some 
heuristic methods is in a high level. Currently, 
lexicographers can make use of large corpora and 
show the convincing results from the experiments 
over corpora. We, therefore, introduce here a new 
efficient method for consistently extracting and 
identifying a list of acceptable Thai words. 
2 Previous Works  
Reviewing the previous works on Thai word 
extraction, we found only the work of 
Sornlertlamvanich and Tanaka (1996). They 
employed the fiequency of the sorted character n- 
grams to extract Thai open compounds; the strings 
that experienced a significant change of 
occurrences when their lengths are extended. This 
algorithm reports about 90% accuracy of Thai 
802 
open compound extraction. However, the 
algorithm emphasizes on open compotmd 
extraction and has to limit tile range of n-gram to 
4-20 grams for the computational reason. This 
causes limitation in the size of corpora and 
efficiency in the extraction. 
The other works can be found in the 
research on the Japanese language. Nagao et al 
(1994) has provided an effective method to 
construct a sorted file that facilitates the 
calculation of n-gram data. But their algorithm did 
not yield satisfactory accuracy; there were many 
iuwflid substrings extracted. The following work 
(lkehara et al, 1995) improved the sorted file to 
avoid repeating in counting strings. The extraction 
cesult was better, but the determination of the 
longest strings is always made consecutively from 
left to right. If an erroneous tring is extracted, its 
errors will propagate through the rest of the input 
:~trings. 
:3 Our Approach 
3.1 The C4.5 Learning Algorithm 
Decision tree induction algorithms have been 
successfully applied for NLP problems such as 
sentence boundary dismnbiguation (Pahner et al 
1997), parsing (Magerman 1995) and word 
segmentation (Mekuavin et al 1997). We employ 
the c4.5 (Quinhln 1993) decision tree induction 
program as the learning algorithm for word 
extraction. 
The induction algorithm proceeds by 
evaluating content of a series of attributes and 
iteratively building a tree fiom the attribute values 
with the leaves of the decision tree being the value 
of the goal attribute. At each step of learning 
procedure, the evolving tree is branched on the 
attribute that pal-titions tile data items with the 
highest information gain. Branches will be added 
until all items in the training set arc classified. To 
reduce the effect of overfitting, c4.5 prunes the 
entire decision tree constructed. It recursively 
examines each subtree to determine whether 
replacing it with a leaf or brauch woukt reduce 
expected error rate. This pruning makes the 
decision tree better in dealing with tile data 
different froul tile training data. 
3.2 Attributes 
We treat the word extraction problem as the 
problem of word/nou-word string disambiguation. 
The next step is to identify the attributes that are 
able to disambiguate word strings flom non-word 
strings. The attributes used for the learning 
algorithm are as follows. 
3.2.1 Left Mutual hfomlation and Right Mutual 
h{fbrmation 
Mutual information (Church et al 1991) of 
random variable a and b is the ratio of probability 
that a and b co-occur, to tile indepeudent 
probability that a and b co-occur. High mutual 
information indicates that a and b co-occur lnore 
than expected by chance. Our algorithm employs 
left and right mutual information as attributes in 
word extraction procedure. Tile left mutual 
information (Lm), and right mutual information 
(Rm) of striug ayz are defined as: 
Lm(xyz)  - 
Rm(xyr.)  - 
p(xyz.) 
p(x)p(yz) 
p(xy~.) 
p ( ,y )p (z )  
where 
x is the leftmost character ofayz 
y is the lniddle substring ol'ayz 
is the rightmost character of :tlVz 
p( ) is tile probability function. 
If xyz is a word, both Lm(xyz) and Rm(~yz) should 
be high. On the contra W, if .rye is a non-word 
string but consists of words and characters, either 
of its left or right mutual information or both lnust 
be low. For example, 'ml~qn~" ( n'(a Thai alphabet) 
'fl~anq'(The word means appear in Thai.) ) must 
have low left mutual information. 
3.2.2 Left Entropy and Right Entropy 
Eutropy (Shannon 1948) is the information 
measuring disorder of wu'iables. The left and right 
entropy is exploited as another two attributes in 
our word extraction. Left entropy (Le), and right 
entropy (Re) of stringy are defined as: 
803 
Le(y) = - Z p(xy I Y)' Iog2p(xYlY) 
V.r~ A 
Re(y) = - Z p(yz l y ) " log 2 p(yz l y ) 
Vz~A 
where 
y is the considered string, 
A is the set of all alphabets 
x, z is any alphabets in A. 
I fy  is a word, the alphabets that come before and 
aflery should have varieties or high entropy. If y 
is not a complete word, either of its left or right 
entropy, or both must be low. For example, 'ahan' 
is not a word but a substring of word 'O~3n~l' 
(appear). Thus the choices of the right adjacent 
alphabets to '~qn' must be few and the right 
entropy of 'ahw, when the right adjacent alphabet 
is '~', must be low. 
3.2.3 Frequency 
It is obvious that the iterative occurrences of 
words must be higher than those of non-word 
strings. String frequency is also useful 
information for our task. Because the string 
frequency depends on the size of corpus, we 
normalize the count of occurrences by dividing by 
the size of corpus and multiplying by the average 
value of Thai word length: 
F(s) = N(s).Avl 
Sc 
where 
s is the considered string 
N(s) is the number of the occurrences 
of s in corpus 
Sc is the size of corpus 
Avl is the average Thai word length. 
We employed the frequency value as another 
attribute for the c4.5 learning algorithm. 
3.2.4 Length 
Short strings are more likely to happen by chance 
than long strings. Then, short and long strings 
should be treated ifferently in the disambiguation 
process. Therefore, string length is also used as an 
attribute for this task. 
3.2.5 Functional Words 
Functional words such as '~' (will) and '~' (then) 
are frequently used in Thai texts. These functional 
words are used often enough to mislead the 
occurrences of string patterns. To filter out these 
noisy patterns from word extraction process, 
discrete attribute Func(s): 
Func(s) : 1 if string s contains 
fnnctional words, 
= 0 if otherwise, 
is applied. 
3.2.6 First Two and Last Two Characters 
A very useful process for our disambiguation is to 
check whether the considered string complies with 
Thai spelling rules or not. We employ the words 
in the Thai Royal Institute dictionary as spelling 
examples for the first and last two characters. 
Then we define attributes Fc(s)and Lc(s) for 
this task as follows. 
N(s, s2*) 
Fc(s )  - 
ND 
N(*s,,_l  s,, ) Lc( s ) - 
ND 
where s is the considered string and 
S .= S IS2 . . .Sn_ IS  n 
N(sls2* ) is the number of words in 
the dictionary that begin with s~s 2 
N(*s,_ls,,) is the nmnber of 
words in the dictionary that 
end with s,,_~s,, 
ND is the number of words in 
the dictionary. 
3.3 Applying C4.5 to Thai Word Extraction 
The process of applying c4.5 to our word 
extraction problem is shown in Figure 1. Firstly, 
we construct a training set for the c4.5 learning 
algorithm. We apply Yamamoto et al(1998)'s 
algorithm to extract all strings from a plain and 
unlabelled I-MB corpus which consists of 75 
articles from various fields. For practical and 
reasonable purpose, we select only the 2-to-30- 
character strings that occur more than 2 times, 
804 
Extracting Strings 
from 
the Training 
Corpus 
Computing the\] 
Attributes I 
Value J 
iTagging the 
Strings 1 
'qV 
i Extracting Strings 
from 
the Test Corpus 
~ t ~  the 
Attributes 
Value 
J -  --We r ~  
1 Extraction 
Figure. 1 : Overview o1' the Process 
Re > 1.78 / ,  
-2~Lm 14233--:. / is notaword ' 
\ 
/ \ \  
Y//" \~  N 
.2" Func= 0 "> s nota wor 
i s  a word  
Figure 2: Exanlple of the Decision tree 
have positive right and left entropy, and conform 
to simple Thai spelling rules. To this step, we get 
about 30,000 strings. These strings are lnalmally 
tagged as words or non-word strings. The strings' 
statistics explained above are calculated for each 
string. Then the strings' attributes and tags are 
used as the training example for the learning 
algorithln. The decision tree is then constructed 
from the training data. 
In order to test the decision tree, another 
plain I-MB corpus (the test corpus), which 
consists of 72 articles fi'om various fields, is 
employed. All strings in the test corpus are 
extracted and filtered out by the same process as 
used in the training set. After the filtering process, 
we get about 30,000 strings to be tested. These 
30,000 strings are manually tagged in order that 
the precision and recall of the decision tree can be 
evaluated. The experimental results will be 
discussed in the next section. 
4 Exper imental  Results 
4.1 The Results 
To measure the accuracy of the algorithln, we 
consider two statistical values: precision and 
recall. The precision of our algorithm is 87.3% for 
the training set and 84.1% for the test set. The 
recall of extraction is 56% in both training and 
test sets. We compare the recall of our word 
extraction with the recall from using the Thai 
Royal Institute dictionary (RID). The recall froln 
our approach and from using RID are comparable 
and our approach should outperform the existing 
dictionary for larger corpora. Both precision and 
recall fiom training and test sets are quite close. 
This indicates that the created decision tree is 
robust for unseen data. Table 3 also shows that 
more than 30% of the extracted words are not 
found in RID. These would be the new entries for 
the dictionary. 
Table 1 : The precision of word extraction 
No. of strings 
extracted by the 
decision tree 
Training 1882 
Set (100%) 
'lest Set 1815 
(100%) 
No. of No. of non- 
words word strings 
extracted extracted 
1643 239 
(87.3%) (12.7%) 
1526 289 
(84.1%) (15.9%) 
Table 2: Tile recall of word extraction 
Training 
Set 
Test Set 
No. of words 
that ill 30,000 
strings 
extracted 
No. of words 
extracted by 
the decision 
t ree  
No. of words 
in corpus that 
are found 
RID 
2933 1643 1833 
(100%) (56.0%) (62.5%) 
2720 1526 1580 
(100%) (56.1%) (58.1%) 
805 
Table 3: Words extracted 
No. of words 
extracted by 
the decision 
tree 
by the decision 
No. of words 
extracted by 
the decision 
tree which is 
inRID 
tree and RID 
No. of words 
extracted by 
the decision 
tree which is 
not in RID 
Training 1643 1082 561 
Set (100.0%) (65.9%) (34.1%) 
Test Set 1526 1046 480 
(100.1%) (68.5%) (31.5%) 
4.2 The Relationship of Accuracy, Occurrence 
and Length 
In this section, we consider the relationship of the 
extraction accuracy to the string lengths and 
occurrences. Figure 2 and 3 depict that both 
precision and recall have tendency to increase as 
string occurrences are getting higher. This implies 
that the accuracy should be higher for larger 
corpora. Similarly, in Figure 4 and 5, the accuracy 
tends to be higher in longer strings. The new 
created words or loan words have tendency to be 
long. Our extraction, then, give a high accuracy 
and very useful for extracting these new created 
words. 
T ra in  in  g 
. . . . . . .  T cs t  
, r 1 r I I I I 
2 6 10  14  18  22  26  3O 34  3 \ [{  
0 ccur rcncc  (x  I O0  ) 
Figurc 3: Prccision-Occurrence R lationship 
lOO 
Z .  
~4o - -T ra in ing  
2o . . . . . .  Test 
o r r r T T 1 T T ? ? 
2 6 10  14  18  22  26  30  34  38  
Occurrence (xl00) 
Figure 4: Recall-Occurrence Relationship 
lOO 
"~ 40  r, 
2O 
0 
120 
I T raining 
. . . . . .  Tcst  
T r E r i r ~ i 
1 3 5 7 9 11  13  15  17  
Length (No. of characters) 
Figure 5: Precision-Length Relationship 
I 90  
I 8o 
1 70 
i 60 
50 i!40? 
~" 30  
20  
. . . . . .  Test  
lO 
0 ? i i 
1 3 5 7 ? 11 13  15  17  
\ [ , cng lh  (No .  of  characters )  
Figure 6: Prccision-Length P,elationship 
5 Conclusion 
In this paper, we have applied the c4.5 learning 
algorithm for the task of Thai word extraction. 
C4.5 can construct a good decision tree for 
word/non-word disambiguation. The learned 
attributes, which are mutual information, entropy, 
word frequency, word length, functional words, 
first two and last two characters, can capture 
useful information for word extraction. Our 
approach yields about 85% and 56% in precision 
and recall measures respectively, which is 
comparable to employing an existing dictionary. 
The accuracy should be higher in larger corpora. 
Our future work is to apply this algorithm with 
larger corpora to build a corpus-based Thai 
dictionary. And hopefully, out" approach should be 
successful for other non-word-boundary 
languages. 
Acknowledgement 
Special thanks to Assistant Professor Mikio 
Yamamoto for providing the useful program to 
extract all substrings from the corpora in linear 
time. 
806 
References 
Church, K.W., Robert L. and Mark L.Y. 
(1991) A Status Report on ACL/DCL. 
Proceedings of 7 a' Annual Co#(ference of 
the UW Centre New OED attd Text 
Reseatrh: Using Corpora, pp. 84-91 
Ikehara, S., Shirai, S. and Kawaoka, T. (1995) 
Automatic Extraction of Uninterrupted 
Collocations by n-gran~ Statistics. Piwceeding q\[ 
The fitwt Annual Meeting of the Association for 
Natural Language Processing, pp. 313-316 (in 
Japancse) 
Magerman, D.M. (1995) Statistical decision-tree 
models for parsing., hwceeding of 33rd 
Amtual Meeting of Association for Computational 
Linguistics 
Meknavin, S., Charoenpornsawat, P. and Kijsirikul, B. 
(1997) Feature-based Thai Word Segmentation. 
Proceeding of the Natural Language Processing 
Pacific Rim Symposium 1997, pp. 35-46 
Nagao, M. and Mort, S. (1994) A New Method of N- 
gram Statistics for Large Number of n and 
Automatic Extraction of Words and Phrases fl'om 
Large Text l)ata of Japanese. Proceeding of 
COLING 94, Vol. 1, pp. 611-15 
Pahner, D.D. and Hearst M.A. (1997) Adaptive 
Multilingual Sentence Boundary Disambiguation. 
ComputationalLinguistics Vol.27, pp. 241-267 
Quinhm, J.R. (1993) C4.5 Programs for Machine 
Learning.Morgan Publishers San Mated, 
California, 302 p. 
Shannon, C.E. (1948) A Mathematical Theory of 
CommunicatiomJ. Bell System Technical Jolu'nal 
27, pp. 379-423 
Sornlertlamvanich, V. and Tanaka, H. (1996) The 
Automatic Extraction of Open Compounds from 
Text. Proceeding o\[ COLING 96 Vol. 2, pp. 1143- 
1146 
Yamamoto, M. and Church, K.W. (1998) Using Suffix 
Arrays to Compare Term Frequency and 
Document Frequency for All Substrings in Corpus. 
Proceeding of Sixth Workshop on Veo' Large 
Corpora pp. 27-37 
807 
Improving Translation Quality of Rule-based Machine Translation  
 
Paisarn Charoenpornsawat, Virach Sornlertlamvanich and Thatsanee Charoenporn 
Information Research and Development Division  
National Electronics and Computer Technology Center 
112 Thailand Science Park, Paholyothin Rd., 
Klong 1, Klong Luang, Pathumthani 12120  
THAILAND 
{paisarn, virach, thatsanee}@nectec.or.th 
 
Abstract  
This paper proposes machine learning 
techniques, which help disambiguate word 
meaning. These methods focus on considering 
the relationship between a word and its 
surroundings, described as context information 
in the paper. Context information is produced 
from rule-based translation such as part-of-
speech tags, semantic concept, case relations and 
so on. To automatically extract the context 
information, we apply machine learning 
algorithms which are C4.5, C4.5rule and 
RIPPER. In this paper, we test on ParSit, which 
is an interlingual-based machine translation for 
English to Thai. To evaluate our approach, an 
verb-to-be is selected because it has increased in 
frequency and it is quite difficult to be translated 
into Thai by using only linguistic rules. The 
result shows that the accuracy of C4.5, C4.5rule 
and RIPPER are 77.7%, 73.1% and 76.1% 
respectively whereas ParSit give accuracy only 
48%.  
Introduction 
Machine translation has been developed for 
many decades. Many approaches have been 
proposed such as rule-based, statistic-based [5], 
and example-based approaches [3, 6, 11]. 
However, there is no machine learning technique 
that meets human?s requirement. Each technique 
has its own advantages and disadvantages. 
Statistic-based, example-based and corpus-based 
approaches were recently proposed. A rule-
based approach is the first strategy pursued by 
research in the field of machine translation. 
Rules are written from linguistic knowledge by 
human. The strength is that it can deeply analyze 
in both syntax and semantic levels. However, the 
weak points of this model are 1) it requires much 
linguistic knowledge. 2) it is impossible to write 
rules that cover all a language. In many years 
ago, a statistic-based and an example-based were 
proposed. These approaches do not require 
linguistic knowledge, but they need large size of 
bilingual corpus. A statistic-based approach uses 
statistic of bilingual corpus and language model. 
The advantage is that it may be able to produce 
suitable translations even if a given sentence is 
not similar to any sentences in a training corpus. 
In contrast, an example-based can produce 
appropriate translations in case of a given 
sentence must similar to any sentences in a 
training data. Nevertheless, a statistic-based 
approach cannot translate idioms and phrases 
that reflect long-distance dependency. 
To improve quality of a rule-based 
machine translation, we have to modify/add 
some generation rules or analysis rules. This 
method requires much linguistic knowledge and 
we cannot guarantee that accuracy will be better. 
For example, in case of modifying some rules, it 
does not only change incorrect sentences to 
correct sentences furthermore they may effect on 
correct sentences too. The common errors of 
machine translation can be classified into two 
main groups. One is choosing incorrect meaning 
and the other is incorrect ordering. In our 
experiments, we select ParSit in evaluation. 
ParSit is English-to-Thai machine translation by 
using an interlingual-based approach [8]. An 
interlingual-based approach is a kind of rule-
based machine translation. The statistics of 
incorrect meaning and incorrect ordering in 
ParSit are 81.74% and 18.26% respectively. 
Therefore, in this paper, we address on choosing 
a correct meaning. We use context information, 
words and part-of-speech tags, in classifying the 
correct meaning. This paper, we apply machine 
learning algorithms, C4.5, C4.5rule, and 
RIPPER, to automatically extract words and 
part-of-speech tags.  
We develop a computer system for sentence translation 
Syntax & Semantic analysis for English
 Parsit 
1. A Rule-Based Approach: Case Study 
ParSit: English to Thai Machine 
Translation. develop agent proposeobject In this section, we will briefly describe a rule-
based machine translation. Each rule-based 
machine translation has its own mythology in 
translation. Hence in this paper, we select ParSit 
as a case study. ParSit is English to Thai 
machine translation using an interlingual-based 
approach. ParSit consists of four modules that 
are a syntax analysis module, a semantic 
analysis module, a semantic generation module, 
and a syntax generation module. An example of 
ParSit translation is shown in figure 1. 
system translationwe
modifier object
computer sentence
Interlingual tree 
Syntax & Semantic generation for Thai
 In figure 1, the English sentence, ?We 
develop a computer system for sentence 
translation.?, input into ParSit. Both syntax and 
semantic analysis modules analyze the sentence 
and then transform into the interlingual tree 
which is shown in Figure 1. In the interlingual 
tree shows the relationship between words such 
as 1) ?We? is an agent of ?develop? 2) ?system? 
is an object of ?develop? 3)  ?computer? is 
modifier of  ?system? and so on. Finally, Thai 
sentence, ?????????????????????????????????????
??????, is generated from the interlingual tree 
by the syntax and semantic generation modules. 
?????? ????? ???? ??????????? ????? ?????? ?????? 
Figure 1: ParSit translation process. 
o Generating over words. 
This is the house in which she lives. 
Incorrect: ?????????????????????????????????? 
Correct:  ????????????????????????? 
 
o Using an incorrect word. 
The news that she died was a great 
shock. 
 The errors of translation from ParSit can 
be classified into two main groups. One is 
incorrect meaning and the other is incorrect 
ordering. The incorrect meaning also can be 
reclassified into three categories; 1). missing 
some words 2). generating over words 3). using 
incorrect word The examples of errors are 
shown below. 
Incorrect: ??????????????????????????????
???????? 
Correct:  ?????????????????????????????????
???????? 
 
 ? Incorrect ordering errors. ? Incorrect meaning errors. He is wrong to leave. 
o Missing some words. Incorrect: ?????????????? 
The city is not far from here 
Correct:     ?????????????? Incorrect: ?????????????????  
 Correct:      ????????????????????? 
   
We evaluated ParSit by using 770-
English-sentence corpus that is designed by 
Japan Electronic Industry Development 
Association (JEIDA). This corpus has the 
characteristics for testing in word level such 
as concept mismatching, word absence and 
etc. and sentence level such as grammar and 
modifier misplacement. The statistics of 
ParSit errors are shown in Table 1. 
 
 Table 1. Statistics of ParSit Error 
Incorrect meaning errors 
M (%) G (%) U (%) 
Incorrect ordering 
errors (%) 
16.71 13.31 51.42 18.26 
 
In table 1, M, G and U mean missing 
some word errors, generating over word errors 
and using incorrect word errors respectively. 
According to Table 1, ParSit makes many 
errors in choosing incorrect meaning (81.74%). 
In this paper, we focus on solving the problem 
of choosing incorrect meaning. To decide what 
is the correct meaning of a word, we propose to 
use context information around that word. 
Context information that we use will be 
described in the next section.  
2 Applying Machine Learning Technique 
2.1  Context Information 
There are many kinds of context information 
that useful to decide the appropriate meaning of 
a word such as grammatical rules, collocation 
words, context words, semantic concept and etc. 
Context information is derived from a rule-base 
machine translation. Words and their part-of-
speech tags are the simplest information, which 
are produced from English analysis module. In 
this paper, we use words and/or part-of-speech 
tags around a target word in deciding a word 
meaning. 
2.2 Machine Learning  
In this section, we will briefly descript three 
machine leaning techniques, C4.5, C4.5rule and 
RIPPER. 
2.2.1 C4.5 & C4.5Rule 
C4.5, decision tree, is a traditional classifying 
technique that proposed by Quinlan [7]. C4.5 
have been successfully applied in many NLP 
problems such as word extraction [9] and 
sentence boundary disambiguation [2]. So in this 
paper, we employ C4.5 in our experiments. 
The induction algorithm proceeds by 
evaluation content of series of attributes and 
iteratively building a tree from the attribute 
values with the leaves of the decision tree being 
the valued of the goal attribute. At each step of 
learning procedure, the evolving tree is branched 
on the attribute that partitions the data items 
with the highest information gain. Branches will 
be added until all items in the training set are 
classified. To reduce the effect of overfitting, 
C4.5 prunes the entire decision tree constructed. 
It recursively examines each subtree to 
determine whether replacing it with a leaf or 
branch would reduce expected error rate. This 
pruning makes the decision tree better in dealing 
with the data different from training data. 
In C4.5 version 8, it provides the other 
technique, which is extended from C4.5 called 
C4.5rule. C4.5rule extracts production rules 
from an unpruned decision tree produced by 
C4.5, and then improves process by greedily 
deletes or adds single rules in an effort to reduce 
description length. So in this paper we also 
employ both techniques of C4.5 and C4.5rule. 
2.2.2 RIPPER 
RIPPER [10] is the one of the famous machine 
learning techniques applying in NLP problems 
[4], which was  proprosed by William W. 
Cohen. On his experiment [10] shows that 
RIPPER is more efficient than C4.5 on noisy 
data and it scales nearly linearly with the 
number of examples in a dataset. So we decide 
to choose RIPPER in evaluating and comparing 
results with C4.5 and C4.5rule. 
RIPPER is a propositional rule learning 
algorithm that constructs a ruleset which 
classifies the training data [11]. A rule in the 
constructed ruleset is represented in the form of 
a conjunction of conditions:  
if T1 and T2 and ... Tn then class Cx. 
T1 and T2 and ... Tn is called the body of the rule. 
Cx is a target class to be learned; it can be a 
positive or negative class. A condition Ti tests 
for a particular value of an attribute, and it takes 
one of four forms: An = v,  Ac ? ?, Ac ? ?  and  v 
3 Overview of The System 
Figure 2 : Overview of the system 
Machine learning 
Translated sentence with 
improving quality 
Rules or tree 
from training 
data 
Rules or tree from training data 
Input sentence 
Rule-based MT 
(ParSit) 
Translated sentence Context information 
(words and POS) 
Correct the translated 
sentence by human 
Machine learning 
Rule-based MT 
(ParSit)
Context information 
(words and POS)Translated sentence 
Input sentence In this section, we will describe the process of 
our system in Figure 2. First, input a source 
sentence into rule-based MT and then use syntax 
and semantic rules for analysing the sentence. At 
this step, rule-based MT gives various kinds of 
word information. In this experiment we used 
only words and part-of-speech tags. After 
analysing, rule-based MT generates a sentence 
into target language. Next, the translated 
sentence from rule-based MT and the context 
information are parsed into machine learning. 
Machine learning requires a rule set or a 
decision tree, which are generated from a 
training set, to decide what is the appropriate 
meaning of a word. 
 In training module (Figure 3), we parse 
English sentences with part-of-speech tags, 
which are given by ParSit, and assign the correct 
meaning by linguists into machine learning 
module. The machine learning will learn and 
produces a rule set or a decision tree for 
disambiguating word meaning. The process of 
training is shown in Figure 3. 
 
4 Preliminary Experiments & Results. 
 
To evaluate our approach, we should test on a 
word, which frequently occurred in normal text 
and has several meanings. According to the 
statistics of word usage from 100M-word British 
National Corpus, verb-to-be occurred more than 
thee million times, and translation of verb-to-be  
into Thai is quite difficult by using only 
linguistic rules. Therefore our experiment, we 
test our approach on verb-to-be. 
? As, where An is a nominal attribute and v is a 
legal value for An; or Ac is a continuous variable 
and ? is some value for Ac that occurs in the 
training data; or As is a set-value attribute and v 
is a value that is an element of As. In fact, a 
condition can include negation. A set-valued 
attribute is an attribute whose value is a set of 
strings. The primitive tests on a set-valued 
attribute As are of the form ?v ? As?. When 
constructing a rule, RIPPER finds the test that 
maximizes information gain for a set of 
examples S efficiently, making only a single 
pass over S for each attribute. All symbols v, that 
appear as elements of attribute A for some 
training examples, are considered by RIPPER. 
Figure 3 : The training module 
 In the experiment, we use 3,200 English 
sentences from Japan Electronic Dictionary 
Research Institute (EDR). EDR corpus is 
collected from news, novel and journal. Then 
our linguists manually assigned the suitable 
meaning of verb-to-be in Thai. In training and 
testing steps, we divided data into two groups. 
The first is 700 sentences for testing and the 
other is for training. We use various sizes of a 
training data set and different sizes of context 
information.  
 Table 2, 3 and 4 are the result from 
C4.5, C4.5rule and RIPPER respectively. The 
series in columns represent the number of 
training sentences. The row headers show the 
types of context information that Pos?n, 
Word?n and P&W?n mean part-of-speech tags, 
words and part-of-speech tags and words with 
the window size is n.  
 
Table 2. The results from C4.5 
 
 100 500 1 K 1.5K 2K 2.5K 
Pos?1 67.1 69.8 69.8 69.8 69.8 69.8 
Pos?2 67.1 69.8 69.8 69.8 69.8 69.8 
Pos?3 67.1 69.8 69.8 69.8 69.8 69.8 
Word?1 55.5 63.2 73.1 74.2 75.5 75.4 
Word?2 57.7 64.6 71.7 72.7 75.5 77.3 
Word?3 57.8 65.3 71.3 73.1 75.4 77.7 
P&W?1 55.5 68.6 71.1 71.3 71.8 71.8 
P&W?2 57.7 68.6 71.3 70.4 71.8 71.8 
P&W?3 57.8 68.6 71.3 69.6 71.3 71.9 
 
Table 3: The results from C4.5rule 
 
 100 500 1 K 1.5K 2K 2.5K 
Pos?1 69.8 71.3 76.3 77.3 76.0 73.1 
Pos?2 69.8 77.5 76.7 76.9 76.3 73.1 
Pos?3 69.2 77.2 76.2 76.8 70.1 73.1 
Word?1 54.9 73.1 63.4 63.6 67.2 71.1 
Word?2 56.3 73.5 73.5 72.5 64.7 70.6 
Word?3 56.3 72.2 72.5 72.3 76.8 70.6 
P&W?1 54.9 77.2 63.4 68.4 69.2 71.1 
P&W?2 56.8 76.7 73.5 68.0 70.5 70.6 
P&W?3 56.8 69.6 64.3 61.8 71.5 71.1 
 
Table 4: The results from RIPPER. 
 
 100 500 1 K 1.5K 2K 2.5K 
Pos?1 70.2 70.9 73.3 71.7 72.1 76.1 
Pos?2 69.4 71.0 69.2 70.2 70.8 72.1 
Pos?3 69.2 71.0 69.6 71.3 76.9 70.6 
Word?1 63.1 69.8 67.2 72.1 72.9 71.1 
Word?2 55.3 67.7 66.8 74.0 72.2 70.6 
Word?3 58.0 70.5 66.8 71.7 72.3 70.6 
P&W?1 72.7 73.9 73.3 73.5 73.4 76.1 
P&W?2 57.7 72.3 69.2 73.5 72.2 72.1 
P&W?3 62.0 70.4 69.6 72.1 72.6 70.6 
 
According to the result from C4.5 in 
Table 2, with data size is not more than 500 
sentences, C4.5 makes good accuracy by using 
only part-of-speech tags with any window sizes. 
In case of a training data set is equal or more 
than 1000 sentences, considering only words 
give the best accuracy and the suitable window 
size is depend on the size of training data set. In 
Table 3, C4.5rule gives high accuracies on 
considering only part-of-speech tags with any 
window sizes. In table 4, RIPPER produces high 
accuracies by investigating only one word and 
one part-of-speech tag before and after verb-to-
be words. 
Conclusion 
C4.5, C4.5rule and RIPPER have efficiency in 
extracting context information from a training 
corpus. The accuracy of these three machine 
learning techniques is not quite different, and 
RIPPER gives the better results than C4.5 and 
C4.5rule do in a small train set. The appropriate 
context information depends on machine 
learning algorithms. The suitable context 
information giving high accuracy in C4.5, 
C4.5rule and RIPPER are ?3 words around a 
target word, part-of-speech tags with any 
window sizes and ?1 word and part-of-speech 
tag respectively 
This can prove that our approach has a 
significant in improving a quality of translation. 
The advantages of our method are 1) adaptive 
model, 2) it can apply to another languages, and 
3). It is not require linguistic knowledge.  
In future experiment, we will include 
other machine learning techniques such as 
Winnow[1] and increase other context 
information such as semantic, grammar. 
Acknowledgements 
Special thanks to Mr. Sittha Phaholphinyo for 
marking up the correct meaning of verb-to-be 
words and Mr. Danoopon Nanongkhai, intern 
student from Computer Engineering 
Department, Kasertsart University, for his help 
in testing the experiments. 
References  
[1] Andrew R. Golding and Dan Roth. 1999. 
A Winnow-Based Approach to Context Sensitive 
Spelling Correction, Machine Learning, Special 
issue on Machine Learning and Natural Language 
Processing, Volume 34, pp. 107-130. 
 
[2] David D. Palmer Marti A. Hearst 1994. Adaptive 
Sentence Boundary Disambiguation. In the 
Proceedings of the Fourth ACL Conference on 
Applied Natural Language Processing, Stuttgart. 
 
[3] Michael Carl. 1999: Inducing Translation 
Templates for Example-Based Machine Translation, 
In the Proceeding of MT-Summit VII, Singapore. 
 
[4] Paisarn Charoenpornsawat., Boonserm Kijsirikul. 
and Surapant Meknavin. 1998. Feature-based Thai 
Unknown Word Boundary Identification Using 
Winnow. In Proceedings of the 1998 IEEE Asia-
Pacific Conference on Circuits and Systems 
(APCCAS?98).  
 
[5] Peter F. Brown, John Cocke and etc. statiscal 
approach to machine translation. Computational 
linguistics 16, 1990  
 
[6] Ralf D. Brown 1996. Example-Based Machine 
Translation  in the PanGloss System. In 
Proceedings of the Sixteenth International 
Conference on Computational Linguistics, Page 
169-174, Copenhagen, Denmark. 
[7] Ross Quinlan. 1993. C4.5: Programs for Machine 
Learning Morgan Kauffman. 
 
[8] Virach Sornlertlamvanich and Wantanee 
Phantachat 1993. Interlingual Expression for Thai 
Language. Technical report. Linguistic and 
Knowledge Engineering Laboratory, National 
Electronics and Computer Technology Center, 
Thailand.  
 
[9] Virach sornlertlamvanich, Tanapong Potipiti and 
Thatsanee Charoenporn. 2000. Automatic Corpus-
Based Thai Word Extraction with the C4.5 Leaning 
Algorithm. Proceedings of the 18th International 
Conference on Computational Linguistics 
(COLING2000), Saarbrucken, Germany. 
 
[10] William W. Cohen. 1995 Fast effective rule 
induction, In Proceedings of the Twelfth 
International Conference on Machine Learning, 
Lake Taho, California, Morgan Kauffman. 
  
[11] Ying Zhang, Ralf D. Brown, and Robert E. 
Frederking, 2001. Adapting an Example-Based 
Translation System to Chinese. In Proceedings of 
Human Language Technology Conference 2001 p. 
7-10. San Diego, California, March 18-21, 2001. 
Computational Linguistics, Computational 
Linguistics, 11/1, pp. 18?27.  
 
Abstract
The rapid growth of Internet Technology,
especially user friendliness approach, helps
increase the number of Internet users and the
amount of information in the cyberspace.
There is a countless amount of information in
languages. This has spread developments of
MT systems. The focus of our approach is to
increase the reusability of those MT systems
by using Cross System machine translation.
Using natural language as an intermediate
language, such as English, will help us use the
information in Internet qualitatively. In this
paper, we point out some problems that may
cause the efficiency to decrease when a
sentence is translated from a second language
to a third language. A novel method is
proposed to solve this problem.
 
1. Introduction
Machine Translation (MT) is an automatic
system that provides an ability to convert a
message written in one language (source
language: SL) to another (target language:
TL)[1]. The interlingua approach [2,3], a
methodology of constructing an intermediate
language, is a dominant approach in
standalone system to support multi-language.
Many products such as, SYSTRAN [4],
BESTILAND [5], are implemented using this
approach. Interlingua approach is helpful for
a central server, but it is difficult to complete
concepts in Interlingua.
The rapid growth of Internet Technology,
especially user friendliness approach, helps
increase the population of users who access
the Internet and the amount of information in
the cyberspace. With the increasing amount of
online information and the rapid growth of
non-English speaking Internet hosts, it is
becoming increasingly important to offer
users universal access to valuable information
resources in different languages. The
European Multilingual Information Retrieval
(EMIR) project [6], the MULINEX project[7],
the TwentyOne project[8], and the
cross-language retrieval track in TREC[9]
conference all reflect people?s interest in
A Cross System Machine Translation
Thepchai Supnithi, Virach Sornlertlamvanich, Thatsanee Charoenporn
Information Research and Development Division
National Electronics and Computer Technology Center
112 Thailand Science Park, Paholyothin Rd.,
Klong 1, Klong Luang, Pathumthani 12120
THAILAND
{thepchai , virach ,thatsanee}@nectec.or.th
providing interoperability among different
language processing environments and
multilingual information retrieval.
Distributed system technology plays an
important role to enable us to manage
information from various places. This makes
it unnecessary to access only the central
server. It helps machine translation
developers to work individually. Yasuhara
[10] wrote that many machine translation
systems were developed, especially from local
language to English, and the language has an
important role as an intermediate language.
Our paper tries to apply a distributed
technique by using English language, which is
mostly used by non-English speakers as a
second language to be an intermediate
language. Our approach is not aimed to show
that it is better than the interlingua approach,
but it is another solution for us to use existing
resources in cyberspace. We hope that it is
possible to help developers build the
machine translation that will support
all languages taking into account of
cost, quantity, and time consumption.
In section 2, we show cross system
MT approach. In section 3, an
example of our approach is given. In
section 4 we illustrate drawbacks of
this technique and give an example
about how to examine these problems.
2. Cross System MT
The major significance of Asian
languages is the variation of languages
in the region; most of which use their
own unique set of characters. In terms of
grammar, some (Thai, Laotian, Japanese,
Chinese, etc.) do not indicate word boundary,
some (Thai, Laotian, etc.) do not inflect while
others(Japanese, Korean, etc.) provide
particles to indicate the word grammatical
function, some are not distinguishable
between sentences and phrases, etc. These are
the basic difficulties that interest the
researchers in the field of machine translation
and the application.
Due to these varieties, it is difficult to build
an MT system that supports all languages
taking into account of cost, quantity, and time
consumption. Cross system machine
translation approach is, therefore, an essential
concept that helps reduce these problems by
reusing the large amount of information
existing in Internet.
Figure 1 shows an idea of our cross system
machine translation approach. Since the
Text Processing Common Platform
VisualizationRepresentation Extraction Retrieval Summarization MT Mining
English
Language Processing
Chinese
Language Processing
Japanese
Language Processing
French
Language Processing
Korean
Language Processing
Myanmar
Language Processing
Vietnam
Language Processing
Indonesia
Language Processing
Thai
Language Processing
??
Language Processing
??
Language Processing
MT
MT MT MT MT
MT MT MT
MTMT
e-Content Dictionary e-Content Dictionary e-Content Dictionary e-Content Dictionary
e-Content Dictionary e-Content Dictionary
e-Content Dictionary
e-Content Dictionary e-Content Dictionary e-Content Dictionary e-Content Dictionary
Figure 1.Cross System Architecture 
???? ?? ??????? ?? ??
???? ?? ??????? ?? ???Computer Computer?


?EnglishEnglish???????
TE??ET System JE??EJ System
S
earching
Word transfer
Web transfer
1 2 3
4
567
TE??ET System: Thai-English MT System
JE??EJ System : Japanese-English MT System
?? ?? ?? ??
?? ?? ?? ??
S
earching
technology of building MT can be transferred
from us to other countries in this region and
we know that English is broadly used as a
bridge to communicate among different
languages. It is simpler for a local developer
to build an MT system from his/her local
language to English (L1??E). If all
countries have their own Ln??E MT system,
sharing English as an intermediate
representation language reduces problems
shown above. Moreover, there are many
different ways to develop a MT system. Our
approach is to encapsulate the type differences
among MT systems. Thus we can decrease the
gap among languages by connecting the MT
system of each local language.
Our cross system MT also offers a good
infrastructure for many future applications
such as e-commerce, digital archive,
e-publishing, and so on as shown in figure 1.
Next we show an example of the usage of a
cross system MT.
3. A Usage Sample of
Cross System MT
This chapter shows an
application of using our
cross system MT. Figure 2
shows an example of our
expected application tool
for information retrieval.
We have two MT systems
in our workgroup, a
bilingual Thai??English
MT system and Japanese
??English MT system.
When a user starts to search by input a
keyword in Thai, such as a word ??????????? ?
[kom pyu ter]?(step1). The word
?????????????will be sent to the Thai?English
MT system to translate into
?computer?(step2). The word ?computer?
will be sent to the English?Japanese MT
system to translate into ?     
?(step3). The word ?? will
be used as a keyword to search for Japanese
web pages by a Japanese search engine
(step4). The result of Japanese web pages
from the search engine will be sent to
Japanese ? English MT system to translate
into English web pages (step 5). The result of
English homepages will again be sent to
English ? Thai MT system to translate into
Thai pages (step 6). Finally, the output of the
workgroup is web pages that contain the
keyword????????????? (step 7). These web pages
are selected from Japanese web pages.
This approach helps us to develop a MT
system that supports all languages taking into
Figure 2.Cross System Information Retrieval 
account of cost, quantity, and time
consumption. If each pair of languages can be
translated perfectly, it should produce a
satisfactory result for cross system technique.
There is, however, a major problem that we
have to consider about the efficiency due to
the fact that the efficiency of each pair of
machine translation is not completed. The
more languages we include in our system, the
less efficient the system becomes. In order to
find out the solution for this problem, we
show the linguistic problems and an example
for solving those problems in the next section.
4. Problems and Solution to Improve
Efficiency from Cross System Approach
The cross system MT approach seems to
be another solution to develop an MT system
that is possible to connect to other languages.
However, it has a major problem of efficiency
decreasing. When we consider the efficiency
of translation from the SL to TL, we find that
the machine translation cannot be transferred
completely. Section 4.1 gives the problem that
is possible in linguistics and in section 4.2 our
approach to transfer information from the first
SL to TL in order to examine these problems
is described.
4.1 Linguistic Problems
Manisara Meechoonuk and Somporn
Rakchonlatee [11] evaluated the result of
machine translation developed in Thailand,
they define the linguistic problems as shown
in table 1. In the investigation, they show that
the result from MT that is perfect translation is
about 29%, comprehensible translation is
about 55%, and incomprehensible translation
for the remaining . They also state that
?Mismatch Concept? is about 34% found and
is the most common linguistic problems .
These linguistic problems cause the
Table 1. List of Linguistic Problems and Meaning
Linguistics Problems Meaning
Mismatch Concept Inappropriate concept is selected
Misplaced Modifiers Wrong position of words, phrases or modifiers in TL resulting in
distortion of meaning
Inappropriate Literal Translation An inappropriate translation that follows closely the form of SL.
It can be categorized into 1) part of speech, 2) order, 3) idiom.
Addition of words or phrases Some words in TL that are not stated in SL are added.
Omission of words The meaning of a word or words when translating from SL to TL
is/are leaved out.
Insufficient definitions of idioms, two
word verbs, and phrasal verbs
The scope or number of words in electronic storage is either
limited or inaccurate according to the meanings of words in SL
Translation which does not conform to
Target language grammar
A difference sentence structure in TL that may cause an
incomprehensible translation.
Implicit in both SL and TL The implied meaning of a word in the SL is not expressed clearly
or fully in TL
Active in SL but passive in TL The participles appear in SL as active forms but are translated
into passive forms in TL
Insufficient Dictionary Definitions The scope or number of words in the electronic data dictionary is
limited
Different Semantic Segmentation
between SL and TL
Using difference marker, such as punctuation or space in SL and
TL may cause the incomprehensible translation
Specific in SL but generic in TL A specific word in SL is referred as a general meaning in TL
incomplete translation. An MT system cannot
correctly translate from second language to
third language if the result of translation from
the MT system from first language to second
language is not perfect . We, however, find
that ?Insufficient definitions of idioms,
two-word verbs, and phrasal verbs? and
?Insufficient Dictionary Definitions?
problems cannot be fixed by the cross
language system because of the lack of
information before the translation in SL.
We examine this problem by adding the
information from the first language together
with the result of second language. When the
MT system translates from the second
language to the third one , it can request
additional information that is attached from
the first language as a reference. Next we
show some examples of using our method.
4.2 Examples of Information Transfer
In section 4.1, we give linguistic problems
that cause a decrease of efficiency of
translation. In order to increase the efficiency,
we illustrate how to give the additional
information in order to help the translation
when the second language functions as a
source language. Our approach is that the
information we receive from the first
language is the most appropriate information.
If we can add additional information from the
first language as much as we can, it will help
us increase the efficiency of translation. We
use an XML as a language to transfer from
first language to other languages.
For example, we have two MT systems, a
Thai??English MT system and
English??Japanese MT system.
(1) Looking at the first sentence,
??????????? (dek duum ya)?
means ?A child drinks a medicine?.
But it is translated into ?A child drinks a
drug? by the Thai?English MT system.
The problem of this sentence is classified as
a ?Mismatch Concept? problem. A word
????has several meanings, such as medicine,
drug, cure, tonic and so on. For this problem
we can add all concepts as a reference as
follows.
?A child <AGT> drinks<? > a drug
<OBJ:c#drug, c#medicine, c#pill,
c#tonic> ?
This will help the second MT system not to
translate ???? as ?drug?, but refer all concepts
of ???? before the translation. The result of
translation should be, ?? by
English?Japanese MT system.
(2) Looking at another sentence in
Japanese.
? 	
  
Constructing Taxonomy of Numerative Classifiers for Asian Languages
Kiyoaki Shirai
JAIST
kshirai@jaist.ac.jp
Takenobu Tokunaga
Tokyo Inst. of Tech.
take@cl.cs.titech.ac.jp
Chu-Ren Huang
Academia Sinica
churenhuang@gmail.com
Shu-Kai Hsieh
National Taiwan Normal Univ.
shukai@gmail.com
Tzu-Yi Kuo
Academia Sinica
ivykuo@gate.sinica.edu.tw
Virach Sornlertlamvanich
TCL, NICT
virach@tcllab.org
Thatsanee Charoenporn
TCL, NICT
thatsanee@tcllab.org
Abstract
Numerative classifiers are ubiquitous in
many Asian languages. This paper pro-
poses a method to construct a taxonomy
of numerative classifiers based on a noun-
classifier agreement database. The taxon-
omy defines superordinate-subordinate rela-
tion among numerative classifiers and rep-
resents the relations in tree structures. The
experiments to construct taxonomies were
conducted for evaluation by using data from
three different languages: Chinese, Japanese
and Thai. We found that our method was
promising for Chinese and Japanese, but in-
appropriate for Thai. It confirms that there
really is no hierarchy among Thai classifiers.
1 Introduction
Many Asian languages do not mark grammatical
numbers (singular/plural) in noun form, but use nu-
merative classifiers together with numerals instead
when describing the number of nouns. Numerative
classifiers (hereafter ?classifiers?) are used with a
limited group of nouns, in particular material nouns.
In English, for example: ?three pieces of paper?. In
Asian languages these classifiers are ubiquitous and
used with common nouns. Therefore the number of
classifiers is much larger than in Western languages.
An agreement between nouns and classifiers is also
necessary, i.e., a certain noun specifies possible clas-
sifiers. The agreement is determined based on var-
ious aspects of a noun, such as its meaning, shape,
pragmatic aspect and so on.
This paper proposes a method to automati-
cally construct a taxonomy of numerative classi-
fiers for Asian languages. The taxonomy defines
superordinate-subordinate relations between classi-
fiers. For instance, the Japanese classifier ?? (to?)?
is used for counting big animals such as elephants
and tigers, while ?? (hiki)? is used for all animals.
Since ??? can be considered more general than ?
??, ??? is the superordinate classifier of ???, rep-
resented as ???  ??? in this paper. The taxon-
omy represents such superordinate-subordinate rela-
tions between classifiers in the form of a tree struc-
ture. A taxonomy of classifiers would be fundamen-
tal knowledge for natural language processing. In
addition, it will be useful for language learners, be-
cause learning usage of classifiers is rather difficult,
especially for Western language speakers.
We evaluate the proposed method by using the
data of three Asian languages: Chinese, Japanese
and Thai.
2 Noun-classifier agreement database
First, let us introduce usages of classifiers in Asian
languages. In the following examples, ?CL? stands
for classifier.
? Chinese: yi-ju
(CL)
dian-hua
(telephone)
? ? ? a telephone
? Japanese: inu
(dog)
2 hiki
(CL)
? ? ? 2 dogs
? Thai: nakrian
(student)
3 khon
(CL)
? ? ? 3 students
397
As mentioned earlier, the agreement between nouns
and classifiers is observed. For instance, the
Japanese classifier ?hiki? in the above example
agrees with only animals. The agreement is also
found in Chinese and Thai.
The proposed method to construct a classifier tax-
onomy is based on agreement between nouns and
classifiers. First we prepare a collection of pairs
(n, c) of a noun n and a classifier c which agrees
with n for a language. The statistics of our Chinese,
Japanese, and Thai database are summarized in Ta-
ble 1.
Table 1: Noun-classifier agreement database
Chinese Japanese Thai
No. of (n,c) pairs 28,202 9,582 9,618
No. of nouns (type) 10,250 4,624 8,224
No. of CLs (type) 205 331 608
The Japanese database was built by extracting
noun-classifier pairs from a dictionary (Iida, 2004)
which enumerates nouns and their corresponding
classifiers. The Chinese database was derived from
a dictionary (Huang et al, 1997). The Thai database
consists of a mixture of two kinds of noun-classifier
pairs: 8,024 nouns and their corresponding classi-
fiers from a dictionary of a machine translation sys-
tem (CICC, 1995) and 200 from a corpus. The pairs
from the corpus were manually checked for their va-
lidity.
3 Proposed Method
3.1 Extracting superordinate-subordinate
relations of classifiers
We extracted superordinate-subordinate classifier
pairs based on inclusive relations of sets of nouns
agreeing with those classifiers. Suppose that Nk is
a set of nouns that agrees with a classifier ck. If Ni
subsumes Nj (Ni ? Nj), we can estimate that ci
subsumes cj (ci  cj). For instance, in our Japanese
database, the classifier ?? (ten)? agrees with shops
such as ?drug store?, ?kiosk? and ?restaurant?, and
these nouns also agree with ?? (ken)?, since ??? is
a classifier which agrees with any kind of building.
Thus, we can estimate the relation ???  ???.
Given a certain classifier cj , ci satisfying the fol-
lowing two conditions (1) and (2) is considered as a
N
j
N
i
Figure 1: Relation of sets of nouns agreeing with
classifiers
superordinate classifier of cj .
|Ni| > |Nj | (1)
IR(ci, cj) ? Tir
where IR(ci, cj)
def
=
|N
i
?N
j
|
|N
j
|
(2)
Condition (1) requires that a superordinate classifier
agrees with more nouns than a subordinate classifier.
IR(ci, cj) is an inclusion ratio representing to what
extent nouns in Nj are also included in Ni (the ratio
of the light gray area to the area of the small circle
in Figure 1).
Condition (2) means that if IR(ci, cj) is greater
than a certain threshold T
ir
, we estimate a
superordinate-subordinate relation between ci and
cj . The basic idea is that superordinate-subordinate
relations are extracted when Nj is a proper subset
of Ni, i.e. IR(ci, cj) = 1, but this is too strict. In
order to extract more relations, we loosen this condi-
tion such that relations are extracted when IR(ci, cj)
is large enough. If we set Tir lower, more relations
can be acquired, but they may be less reliable.
Table 2: Extraction of superordinate-subordinate re-
lations
Chinese Japanese Thai
T
ir
0.7 0.6 0.6
No. of extracted relations 251 322 239
No. of CLs not in 36 76 395
the extracted relations (18%) (23%) (61%)
Table 2 shows the results of our experiments to
extract superordinate-subordinate relations of classi-
fiers. The threshold T
ir
was determined in an ad hoc
manner for each language. The numbers of extracted
superordinate-subordinate relations are shown in the
second row in the table. Manual inspection of the
sampled relations revealed that many reasonable re-
lations were extracted. The objective evaluation of
these extracted relations will be discussed in 4.2.
398
The third row in Table 2 indicates the numbers of
classifiers which were not included in the extracted
superordinate-subordinate relations with its ratio to
the total number of classifiers in the database in
parentheses. We found that no relation is extracted
for a large number of Thai classifiers.
3.2 Constructing structure
The structure of a taxonomy is constructed based
on a set of superordinate-subordinate relations be-
tween classifiers. Currently we adopt a very naive
approach to construct structures, i.e., starting from
the most superordinate classifiers as roots, we ex-
tend trees downward to less general classifiers by
using the extracted superordinate-subordinate rela-
tions. Note that since there is more than one classi-
fier that does not have any superordinate classifiers,
we will have a set of trees rather than a single tree.
When constructing structures, redundant relations
are ignored in order to make the structures as concise
as possible. A relation is considered redundant if the
relation can be inferred by using other relations and
transitivity of the relations. The formal definition of
redundant relations is given below:
ca  cb is redundant iff ?cm : ca  cm, cm  cb
Statistics of constructed structures for each lan-
guage are shown in Table 3. More than 50 iso-
lated structures (trees) were obtained for Chinese
and Japanese, while more than 100 for Thai. We ob-
tained several large structures, the largest containing
45, 85 and 23 classifiers for Chinese, Japanese and
Thai, respectively. As indicated in the fifth row in
Table 3, however, many structures consisting of only
2 classifiers were also constructed.
Table 3: Construction of structures
Chinese Japanese Thai
No. of structures 52 54 102
No. of CLs in a structure
Average 4.9 6.3 3.3
Maximum 45 85 23
Max. depth of structures 4 3 3
No. of structures with 2 CLs 18 24 54
4 Discussion
In this section, we will discuss the results of our
experiments. First 4.1 discusses appropriateness of
our method for the three languages. Then we eval-
uate our method in more detail. The evaluation of
extracted superordinate-subordinate relations is de-
scribed in 4.2, and the evaluation of structures in 4.3.
4.1 Comparison of different languages
According to the results of our experiments, the
proposed method seems promising for Chinese and
Japanese, but not for Thai. From the Thai data,
no relation was obtained for about 60% of classi-
fiers (Table 2), and many small fragmented struc-
tures were created (Table 3).
This is because of the characteristic that nouns
and classifiers are strongly coupled in Thai, i.e.,
many classifiers agree with only one noun. In our
Thai database, 252 (41.5%) classifiers agree with
only one noun. This means that the overlap between
two noun sets Ni and Nj can be quite small, making
the inclusion ratio IR(ci, cj) very small. Out basic
idea is that we can extract superordinate-subordinate
relations between two classifiers when the overlap of
their corresponding noun sets is large. However, this
assumption does not hold in Thai classifiers. The
above facts suggest that there seems to be no hierar-
chical taxonomy of classifiers in Thai.
4.2 Evaluation of extracted relations
4.2.1 Analysis of Nouns in Nj \ Ni
As explained in 3.1, our method extracts a relation
ci  cj even when Ni does not completely subsume
Nj . We analysed nouns in the relative complement
of Ni in Nj (Nj \Ni), i.e., the dark gray area in Fig-
ure 1. The relation ci  cj implies that all nouns
which are countable with a subordinate classifier cj
are also countable with its superordinate classifier ci,
but there is no guarantee of this for nouns in Nj \Ni,
since we loosened the condition as in (2) by intro-
ducing a threshold.
To see to what extent nouns in Nj \ Ni agree
with ci as well, we manually verified the agreement
of nouns in Nj \ Ni and ci for all extracted rela-
tions ci  cj . The verification was done by native
speakers of each language. Results of the valida-
tion are summarized in Table 4. For Japanese and
Chinese, multiple judges verified the results. When
judgments conflicted, we decided the final decision
by a discussion of two judges for Japanese, and by
majority voting for Chinese. The 4th and 5th rows
399
in Table 4 show the agreement of judgments. The
?Agreement ratio? is the ratio of cases that judg-
ments agree. Since three judges verified nouns for
Chinese, we show the average of the agreement ra-
tios for two judges out of the three. The agreement
ratio and Cohen?s ? is relatively high for Japanese,
but not for Chinese. We found many uncertain cases
for Chinese nouns. For example, ?? (wei)? is a clas-
sifier used when counting people with honorific per-
spective. However, judgement if ??? can modify
nouns such as ?political prisoner? or ?local villain?
is rather uncertain.
Table 4: Analysis of nouns in Nj \ Ni
Chinese Japanese Thai
No. of nouns in N
j
\N
i
1,650 579 43
No. of nouns countable 1,195 241 24
with c
i
as well 72% 42% 56%
No. of judges 3 2 1
Agreement ratio 0.677 0.936 ?
Cohen?s ? 0.484 0.868 ?
Table 4 reveals that a considerable number of
nouns in Nj \ Ni are actually countable with ci,
meaning that our databases do not include noun-
classifier agreement exhaustively.
4.2.2 Reliability of relations ??
Based on the analysis in 4.2.1, we evaluate ex-
tracted superordinate-subordinate relations. We de-
fine the reliability R of the relation ci  cj as
R(ci  cj) =
|Ni ? Nj |+ |NCj,i|
|Nj |
, (3)
where, NCj,i is a subset of Nj \ Ni consisting of
nouns which are manually judged to agree with ci.
We can consider that the more strictly this statement
holds, the more reliable the extracted relations will
be.
Figure 2 shows the relations between the thresh-
old T
ir
and both the number of extracted relations
and their reliability. The horizontal axis indicates
the threshold T
ir
in (2). The bar charts indicate the
number of extracted relations, while the line graphs
indicate the averages of reliability of all extracted re-
lations. Of course, if we set T
ir
lower, we can extract
more relations at the cost of their reliability. How-
ever, even when T
ir
is set to the lowest value, the
averages of reliability are relatively high, i.e. 0.98
(Chinese), 0.91 (Japanese) and 0.99 (Thai). Thus
we can conclude that the extracted superordinate-
subordinate relations are reliable enough.
4.3 Evaluation of structures
As in ordinary ontologies, we will assume that prop-
erties of superordinate classifiers can be inherited to
their subordinate classifiers. In other words, a clas-
sifier taxonomy suggests transitivity of agreement
with nouns over superordinate-subordinate relations
as
c
1
 c
2
? c
2
 c
3
? c
1
 c
3
.
In order to evaluate the structures of our taxonomy,
we verify the validity of transitivity.
First, we extracted all pairs of classifiers having
an ancestor-descendant relation from our classifier
taxonomy. Hereafter we denote ancestor-descendant
pairs of classifiers as (ca, cd), where ca is an ances-
tor and cd an descendant. The path from ca to cd on
the taxonomy can be represented as
c
0
(= ca)  c1  ...  cn(= cd). (4)
We denote a superordinate-subordinate relation de-
rived by transitivity as
?
, such as c
0
?
 cn. Among
all ancestor-descendant relations, we extracted ones
with a path length of more than one, or n > 1
in (4). Then we compare R(ca
?
 cd), the re-
liability of a relation derived by transitivity, with
R(ci  ci+1) (0 ? i < n), the reliability of di-
rect relations in the path from ca to cd. If these are
comparable, we can conclude that transitivity in the
taxonomy is valid.
Table 5 shows the results of the analysis of transi-
tivity. As indicated in the column ?all? in Table 5, 78
and 86 ancestor-descendant pairs (ca, cd) were ex-
tracted from the Chinese and Japanese classifier tax-
onomy, respectively. In contrast, only 6 pairs were
extracted from the Thai taxonomy, since each struc-
ture of the Thai taxonomy is rather small as we al-
ready discussed with Table 3. Thus we have omit-
ted further analysis of Thai. The extracted ancestor-
descendant pairs of classifiers are then classified into
three cases, (A), (B) and (C). Their numbers are
shown in the last three rows in Table 5, where mini
and maxi denote the minimum and maximum of re-
liability among all direct relations R(ci  ci+1) in
the path from ca to cd.
400
Chinese Japanese Thai
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
Figure 2: Reliability of extracted superordinate-subordinate relations
Table 5: Verification of transitivity
Chinese Japanese
all direct indirect all direct indirect
No. of (c
a
, c
d
) 78 58 20 86 55 31
Average of R(c
a
?
c
d
) 0.88 0.98 0.61 0.77 0.93 0.48
(A) min
i
> R(c
a
?
c
d
) 16 (21%) 4 (7%) 12 (60%) 24 (28%) 3 (5%) 21 (68%)
(B) min
i
? R(c
a
?
c
d
) < max
i
39 (50%) 34 (59%) 5 (25%) 27 (31%) 24 (44%) 3 (9%)
(C) max
i
? R(c
a
?
c
d
) 23 (29%) 20 (34%) 3 (15%) 35 (41%) 28 (51%) 7 (23%)
In case (A), reliability of a relation derived by
transitivity, R(ca
?
 cd), is less than that of any di-
rect relations, R(ci  ci+1). In case (B), reliability
of a transitive relation is comparable with that of di-
rect relations, i.e. R(ca
?
 cd) is greater or equal to
mini and less than maxi. In case (C), the transitive
relation is more reliable than direct relations.
The average of the reliability of ca
?
 cd is rela-
tively high, 0.88 for Chinese and 0.77 for Japanese.
We also found that more than 70% of derived rela-
tions (case (B) and case (C)) are comparable to or
greater than direct relations. The above facts indi-
cate transitivity on our structural taxonomy is valid
to some degree.
From a different point of view, we divided pairs
of (ca, cd) into two other cases, ?direct? and ?indi-
rect? as shown in the columns of Table 5. The ?di-
rect? case includes the relations which are also ex-
tracted by our method. Note that such relations are
discarded as redundant ones. On the other hand, the
?indirect? case includes the relations which can not
be extracted from the database but only inferred by
using transitivity on the taxonomy. That is, they are
truly new relations. In order to calculate reliability
of ?indirect? cases, we performed additional manual
validation of nouns in Nd\Na.
However, the average of R(ca
?
 cd) in ?in-
direct? cases is not so high for both Chinese and
Japanese, as a large amount of pairs are classi-
fied into case (A). Thus it is not effective to infer
new superordinate-subordinate relations by transi-
tivity. Since we currently only adopted a very naive
method to construct a classifier taxonomy, more so-
phisticated methods should be explored in order to
prevent inferring irrelevant relations.
5 Related Work
Bond (2000) proposed a method to choose an appro-
priate classifier for a noun by referring its seman-
tic class. This method is implemented in a sentence
generation module of a machine translation system.
Similar attempts to generate both Japanese and Ko-
rean classifiers were also reported (Paik and Bond,
2001). Bender and Siegel (2004) implemented a
HPSG that handles several intricate structures in-
cluding Japanese classifiers. Matsumoto (1993)
reported his close analysis of Japanese classi-
fiers based on prototype semantics. Sornlertlam-
vanich (1994) presented an algorithm for selecting
an adequate classifier for a noun by using a cor-
pus. Their research can be regarded as a method to
construct a noun-classifier agreement database au-
401
tomatically from corpora. We used databases de-
rived from dictionaries except for a small number
of noun-classifier pairs in Thai, because we believe
dictionaries provide more reliable and stable infor-
mation than corpora, and in addition they were avail-
able and on hand. Note that we are not concerned
with frequencies of noun-classifier coocurrence in
this study. Huang (1998) proposed a method to
construct a noun taxonomy based on noun-classifier
agreement that is very similar to ours, but aims at
developing a taxonomy for nouns rather than one for
classifiers. There has not been very much work on
building resources concerning noun-classifier agree-
ment. To our knowledge, this is the first attempt to
construct a classifier taxonomy.
6 Conclusion
This paper proposed a method to construct a tax-
onomy of numerative classifiers based on a noun-
classifier agreement database. First, superordinate-
subordinate relations of two classifiers are extracted
by measuring the overlap of two sets of nouns agree-
ing with each classifier. Then these relations are
used as building blocks to build a taxonomy of
tree structures. We conducted experiments to build
classifier taxonomies for three languages: Chinese,
Japanese and Thai. The effectiveness of our method
was evaluated by measuring reliability of extracted
relations, and verifying validity of transitivity in the
taxonomy. We found that extracted relations are re-
liable, and the transitivity in the taxonomy relatively
valid. Relations inferred by transitivity, however, are
less reliable than those directly derived from noun-
classifier agreement.
Future work includes investigating a way to en-
large classifier taxonomies. Currently, not all clas-
sifiers are included in our taxonomy, and it con-
sists of a set of fragmented structures. A more so-
phisticated method to build a large taxonomy in-
cluding more classifiers should be examined. Our
method should also be refined in order to make
superordinate-subordinate relations inferred by the
transitivity more reliable. We are now investigat-
ing a stepwise method to construct taxonomies that
prefers more reliable relations, i.e. an initial tax-
onomy is built with a small number of highly reli-
able relations, and is then expanded with less reli-
able ones.
Acknowledgment
This research was carried out through financial sup-
port provided under the NEDO International Joint
Research Grant Program (NEDO Grant).
References
Emily M. Bender and Melanie Siegel. 2004. Imple-
menting the syntax of Japanese numeral classifiers. In
Proceedings of the the First International Joint Con-
ference on Natural Language Processing, pages 398?
405.
Francis Bond and Kyonghee Paik. 2000. Reusing an on-
tology to generate numeral classifiers. In Proceedings
of the COLING, pages 90?96.
CICC. 1995. CICC Thai basic dictionary. (developed by
Center of the International Cooperation for Computer-
ization).
Chu-Ren Huang, Keh-Jian Chen, and Chin-Hsiung Lai,
editors. 1997. Mandarin Daily News Dictionary of
Measure Words. Mandarin Daily News Publisher.
Chu-Ren Huang, Keh-jiann Chen, and Zhao-ming Gao.
1998. Noun class extraction from a corpus-based col-
location dictionary: An integration of computational
and qualitative approaches. In Quantitative and Com-
putational Studies of Chinese Linguistics, pages 339?
352.
Asako Iida. 2004. Kazoekata no Ziten (Dictionary for
counting things). Sho?gakukan. (in Japanese).
Yo Matsumoto. 1993. The Japanese numeral classifiers:
A study of semantic categories and lexical organiza-
tion. Linguistics, 31:667?713.
Kyonghee Paik and Francis Bond. 2001. Multilin-
gual generation of numeral classifiers using a common
ontology. In Proceedings of the 19th International
Conference on Computer Processing of Oriental Lan-
guages (ICCPOL), pages 141?147.
Virach Sornlertlamvanich, Wantanee Pantachat, and
Surapant Meknavin. 1994. Classifier assignment by
corpus-based approach. In Proceedings of the COL-
ING, pages 556?561.
402
Synset Assignment for Bi-lingual Dictionary with Limited Resource 
 
       Virach Sornlertlamvanich  
Thatsanee Charoenporn  
Chumpol Mokarat 
Thai Computational Linguistics Lab.  
NICT Asia Research Center, 
Thailand Science Park,  
Pathumthani, Thailand 
{virach,thatsanee,chumpol}@tcllab.org 
Hitoshi Isahara 
National Institute of Information 
and Communications Technology 
3-5 Hikaridai, Seika-cho, soraku-gaun, 
Kyoto, Japan 619-0289 
isahara@nict.go.jp 
 
Hammam Riza  
IPTEKNET, Agency for the Assess-
ment and Application of Technology,     
Jakarta Pusat 10340, Indonesia  
hammam@iptek.net.id 
 
Purev Jaimai  
Center for Research on Language 
Processing, National University of 
Mongolia, Ulaanbaatar, Mongolia  
purev@num.edu.mn 
 
Abstract 
This paper explores an automatic WordNet 
synset assignment to the bi-lingual diction-
aries of languages having limited lexicon 
information. Generally, a term in a bi-
lingual dictionary is provided with very 
limited information such as part-of-speech, 
a set of synonyms, and a set of English 
equivalents. This type of dictionary is 
comparatively reliable and can be found in 
an electronic form from various publishers. 
In this paper, we propose an algorithm for 
applying a set of criteria to assign a synset 
with an appropriate degree of confidence to 
the existing bi-lingual dictionary. We show 
the efficiency in nominating the synset 
candidate by using the most common lexi-
cal information. The algorithm is evaluated 
against the implementation of Thai-
English, Indonesian-English, and Mongo-
lian-English bi-lingual dictionaries. The 
experiment also shows the effectiveness of 
using the same type of dictionary from dif-
ferent sources.  
1 Introduction 
The Princeton WordNet (PWN) (Fellbaum, 1998) 
is one of the most semantically rich English lexical 
databases that are widely used as a lexical knowl-
edge resource in many research and development 
topics. The database is divided by part of speech 
into noun, verb, adjective and adverb, organized in 
sets of synonyms, called synset, each of which 
represents ?meaning? of the word entry.  
Though WordNet was already used as a starting 
resource for developing many language WordNets, 
the construction of the WordNet for any languages 
can be varied according to the availability of the 
language resources. Some were developed from 
scratch, and some were developed from the combi-
nation of various existing lexical resources. Span-
ish and Catalan WordNets, for instance, are auto-
matically constructed using hyponym relation, 
monolingual dictionary, bilingual dictionary and 
taxonomy (Atserias et al, 1997). Italian WordNet 
(Magnini et al, 1994) is semi-automatically con-
structed from definition in monolingual dictionary, 
bilingual dictionary, and WordNet glosses. Hun-
garian WordNet uses bilingual dictionary, mono-
lingual explanatory dictionary, and Hungarian the-
saurus in the construction (Proszeky et al, 2002), 
etc. 
673
This paper presents a new method particularly to 
facilitate the WordNet construction by using the 
existing resources having only English equivalents 
and the lexical synonyms. Our proposed criteria 
and algorithm for application are evaluated by im-
plementing to Asian languages which occupy quite 
different language phenomena in terms of gram-
mars and word unit. 
To evaluate our criteria and algorithm, we use 
the PWN version 2.1 containing 207,010 senses 
classified into adjective, adverb, verb, and noun. 
The basic building block is a ?synset? which is 
essentially a context-sensitive grouping of syno-
nyms which are linked by various types of relation 
such as hyponym, hypernymy, meronymy, anto-
nym, attributes, and modification. Our approach is 
conducted to assign a synset to a lexical entry by 
considering its English equivalent and lexical 
synonyms. The degree of reliability of the assign-
ment is defined in terms of confidence score (CS) 
based on our assumption of the membership of the 
English equivalent in the synset. A dictionary from 
different source is also a reliable source to increase 
the accuracy of the assignment because it can ful-
fill the thoroughness of the list of English equiva-
lent and the lexical synonyms. 
The rest of this paper is organized as follows: 
Section 2 describes our criteria for synset assign-
ment. Section 3 provides the results of the experi-
ments and error analysis on Thai, Indonesian, and 
Mongolian. Section 4 evaluates the accuracy of the 
assignment result, and the effectiveness of the 
complimentary use of a dictionary from different 
sources. Section 5 shows a collaborative interface 
for revising the result of synset assignment. And 
Section 6 concludes our work. 
2 Synset Assignment 
A set of synonyms determines the meaning of a 
concept. Under the situation of limited resources 
on a language, English equivalent word in a bi-
lingual dictionary is a crucial key to find an 
appropriate synset for the entry word in question. 
The synset assignment criteria described in this 
Section relies on the information of English 
equivalent and synonym of a lexical entry, which 
is most commonly encoded in a bi-lingual 
dictionary. 
Synset Assignment Criteria 
Applying the nature of WordNet which introduces 
a set of synonyms to define the concept, we set up 
four criteria for assigning a synset to a lexical entry. 
The confidence score (CS) is introduced to 
annotate the likelihood of the assignment. The 
highest score, CS=4, is assigned to the synset that 
is evident to include more than one English 
equivalent of the lexical entry in question. On the 
contrary, the lowest score, CS=1, is assigned to 
any synset that occupies only one of the English 
equivalents of the lexical entry in question when 
multiple English equivalents exist. 
The details of assignment criteria are elaborated 
as in the followings. Li denotes the lexical entry, Ej 
denotes the English equivalent, Sk denotes the syn-
set, and ? denotes the member of a set: 
Case 1: Accept the synset that includes more 
than one English equivalent with confidence score 
of 4. 
Figure 1 simulates that a lexical entry L0 has two 
English equivalents of E0 and E1. Both E0 and E1 
are included in a synset of S1. The criterion implies 
that both E0 and E1 are the synset for L0 which can 
be defined by a greater set of synonyms in S1. 
Therefore the relatively high confidence score, 
CS=4, is assigned for this synset to the lexical en-
try. 
 
Figure 1. Synset assignment with SC=4 
Example: 
L0:  
E0: aim  E1: target 
S0: purpose, intent, intention, aim, design 
S1: aim, object, objective, target 
S2: aim 
In the above example, the synset, S1, is assigned 
to the lexical entry, L0, with CS=4. 
Case 2: Accept the synset that includes more 
than one English equivalent of the synonym of the 
lexical entry in question with confidence score of 3.  
In case that Case 1 fails in finding a synset that 
includes more than one English equivalent, the 
English equivalent of a synonym of the lexical en-
try is picked up to investigate. 
L0 
E0 
S0 ?
 
S1 
?
 
E1 
?
 
S2 
?
 
674
 Figure 2. Synset assignment with SC=3 
Figure 2 simulates that an English equivalent of 
a lexical entry L0 and its synonym L1 are included 
in a synset S1. In this case the synset S1 is assigned 
to both L0 and L1 with CS=3. The score in this case 
is lower than the one assigned in Case 1 because 
the synonym of the English equivalent of the lexi-
cal entry is indirectly implied from the English 
equivalent of the synonym of the lexical entry. The 
newly retrieved English equivalent may not be dis-
torted. 
Example: 
L0: 	
  L1: 
 
E0: stare  E1: gaze 
S0: gaze, stare S1: stare 
In the above example, the synset, S0, is assigned 
to the lexical entry, L0, with CS=3. 
Case 3: Accept the only synset that includes the 
only one English equivalent with confidence score 
of 2. 
 
Figure 3. Synset assignment with SC=2 
Figure 3 simulates the assignment of CS-2 when 
there is only one English equivalent and there is no 
synonym of the lexical entry. Though there is no 
any English equivalent to increase the reliability of 
the assignment, in the same time there is no 
synonym of the lexical entry to distort the relation. 
In this case, the only one English equivalent shows 
it uniqueness in the translation that can maintain a 
degree of the confidence. 
Example: 
L0:           E0: obstetrician     
S0: obstetrician, accoucheur 
In the above example, the synset, S0, is assigned 
to the lexical entry, L0, with CS=2. 
Case 4: Accept more than one synset that in-
cludes each of the English Equivalent with confi-
dence score of 1. 
Case 4 is the most relax rule to provide some re-
lation information between the lexical entry and a 
synset. Figure 4 simulates the assignment of CS=1 
to any relations that do not meet the previous crite-
ria but the synsets that include one of the English 
equivalent of the lexical entry. 
 
Figure 4. Synset assignment with SC=1 
Example: 
L0: 
 
E0: hole  E1: canal 
S0: hole, hollow   
S1: hole, trap, cakehole, maw, yap, gop 
S2: canal, duct, epithelial duct, channel 
In the above example, each synset, S0, S1, and S2 
is assigned to lexical entry L0, with CS=1. 
3 Experiment results 
We applied the synset assignment criteria to a 
Thai-English dictionary (MMT dictionary) (CICC, 
1995) with the synset from WordNet 2.1. To com-
pare the ratio of assignment for Thai-English dic-
tionary, we also investigate the synset assignment 
of Indonesian-English and Mongolian-English dic-
tionaries. 
 WordNet (synset) T-E Dict (entry) 
 total assigned total assigned 
Noun 145,103 18,353 (13%) 43,072
11,867 
(28%)
Verb 24,884 1,333 (5%) 17,669
2,298 
(13%)
Adjective 31,302 4,034 (13%) 18,448
3,722 
(20%)
Adverb 5,721 737 (13%) 3,008
1,519 
(51%)
total 207,010 24,457 (12%) 82,197
19,406 
(24%)
Table 1. Synset assignment to T-E dictionary 
In our experiment, there are only 24,457 synsets 
from 207,010 synsets, which is 12% of the total 
number of the synset that can be assigned to Thai 
lexical entries. Table 1 shows the successful rate in 
assigning synset to Thai-English dictionary. About 
24 % of Thai lexical entries are found with the 
English equivalents that meet one of our criteria.  
Going through the list of unmapped lexical en-
try, we can classify the errors into three groups:- 
1. Compound 
The English equivalent is assigned in a com-
L0 E0 
S0 ?
 
S1 
?
 
E1 
?
 
S2 
?
 
L1 
L0 E0 S0 
?
 
L0 
E0 
S0 ?
 
S1 
?
 
E1 
S2 
?
 
675
pound, especially in case that there is no an 
appropriate translation to represent exactly 
the same sense. For example, 
L: 		Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 13?18,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
KUI: an ubiquitous tool for collective intelligence development 
Thatsanee Charoenporn, Virach Sornlertlamvanich 
and Kergrit Robkop 
Thai Computational Linguistics Laboratory 
NICT Asia Research Center, Thailand 
{virach,thatsanee,kergrit}@tcllab.org 
Hitoshi Isahara 
National Institute for 
Communications Tech-
nology (NICT), Japan 
ishara@nict.go.jp
 
 
Abstract 
Collective intelligence is the capability for 
a group of people to collaborate in order to 
achieve goals in a complex context than its 
individual member. This common concept 
increases topic of interest in many sciences 
including computer science where com-
puters are bring about as group support 
elements. This paper presents a new plat-
form, called Knowledge Unifying Initiator 
(KUI) for knowledge development which 
enables connection and collaboration 
among individual intelligence in order to 
accomplish a complex mission. KUI is a 
platform to unify the various thoughts fol-
lowing the process of thinking, i.e., initiat-
ing the topic of interest, collecting the 
opinions to the selected topics, localizing 
the opinions through the translation or cus-
tomization and posting for public hearing 
to conceptualize the knowledge. The proc-
ess of thinking is done under the selectional 
preference simulated by voting mechanism 
in case that many alternatives occur. By 
measuring the history of participation of 
each member, KUI adaptively manages the 
reliability of each member?s opinion and 
vote according to the estimated Ex-
pertScore. 
1 Introduction 
The Internet is a must for forming an online com-
munity in the present day. Many tools have been 
developed to support such an online community 
work. For instance, SourceForge.net (http://www. 
sourceforge.net) facilitates project based Open 
Source software development. Open Source soft-
ware developers deploy SourceForge.net to an-
nounce their initiation, to call for participation, to 
distribute their works and to receive feedbacks. 
SourceForge.net is said to be the largest Open 
Source software development community. 
Wiki.org (http://www.wiki.org) facilitates a data-
base for creating and editing Web page content. It 
keeps the history of the online editing works which 
allows multiple authoring. Wiki is especially de-
rived for several online collaborative works such 
as wikipedia, wikitionary, wikibooks, etc. In addi-
tion, PhpWiki is one of the derived works of wiki 
as a handy software tool for managing the organ-
izational documentation. This collaborative work-
ing environment has changed our working style to 
a more efficient manner. In the same time, the 
flood of information under the open collaborative 
works is now challenging us for an efficient man-
agement system. The disorder of the information 
causes difficulties in the requirement of the sys-
tematic maintenance for retrieval, extraction, or 
even summarization from the stored information. 
To understand the intention of an article (or a solu-
tion), we not only rely on the trace or the history of 
editing, but we also constantly recall the back-
ground of our decision in producing the article (or 
the solution). 
Why don't we organize the information in the 
development process beforehand rather than limit-
ing our capability in making use of the un-
structured information? Google (http://www. 
google.com) successfully responds our needs in 
looking for documents from the WWW. However, 
the results from the search can simply over a mil-
lion sites and just some tens out of which are 
13
viewed for the search. This most powerful search-
ing tool does not digest the information to meet 
final our requirement. It only thoroughly shows the 
results of the related document. 
Back to the principle of collective intelligent 
(Smith, 1994; Johnson et al, 1998; Levy, 1997) in 
which ?two minds are better than one?, mountains 
of knowledge are contributed by this internet 
community. But the most intelligence is the intelli-
gence of knowledge connections in which new 
technologies can take part in helping individuals to 
think and develop their concept collectively. 
We proposed and developed KUI (Knowledge 
Unifying Initiator) (KUI, 2006; Sornlertlamvanich, 
2006) to be a Knowledge User Interface (KUI) for 
online collaborative work to help community to 
think and to develop things together. KUI is a plat-
form to unify the various thoughts following the 
process of thinking, i.e., initiating the topic of in-
terest, collecting the opinions to the selected top-
ics, localizing the opinions through the translation 
or customization and finally posting for public 
hearing to conceptualize the knowledge. The proc-
ess of thinking is done under the selectional prefer-
ence simulated by voting mechanism in case that 
many alternatives occur. 
2 Collaborative tool for managing collec-
tive intelligence 
We developed KUI (Knowledge Unifying Initia-
tor) for being a knowledge development supporting 
tool of a web community. Actually, KUI is a plat-
form to unify various thoughts created by follow-
ing process of thinking, i.e., (1) new task, to allow 
a participant to initiate a task, (2) opinion, to allow 
a participant to post his own opinion, (3) localiza-
tion, to allow a participant to bring in a new 
knowledge into the community by translation, and 
(4) public-hearing, to allow a participant to post a 
draft of concept for conceptualizing the knowl-
edge. The process of thinking is done under the 
selectional preference simulated by voting mecha-
nism in case that many alternatives occur. 
In this section, we describe the concept behind 
KUI, the knowledge development process, and the 
features in KUI. 
2.1 What is KUI? 
KUI or Knowledge Unifying Initiator is a GUI for 
knowledge engineering, in other words Knowledge 
User Interface (KUI). It provides a web interface 
accessible for pre-registered members only for the 
accountability reason. An online registration is of-
fered to manage the account by profiling the login 
participant in making contribution to the commu-
nity. A contributor can comfortably move around 
in the virtual space from desk to desk to participate 
in a particular task. A login member will be as-
signed to a desk when a participation task is de-
fined. Members can then participate in the chat 
group of the same desk. A desk functions as a 
meeting place for collaborative work that needs 
some discussion through the chat function, or al-
low a contributor to work individually by using the 
message slot to record each own opinion. The 
working space can be expanded by closing the un-
necessary frames so that the contributor can con-
centrate on a particular task. All working topics 
can also be statistically viewed through the pro-
vided tabs. These tabs help contributors to under-
stand KUI in the aspects of the current status of 
contribution and the available tasks. A web com-
munity can be formed to create a domain specific 
knowledge efficiently through the features pro-
vided by KUI. These KUI features fulfill the proc-
ess of human thought to record the knowledge. 
In addition, KUI also provides a KUI look up 
function for viewing the composed knowledge. It 
is equipped with a powerful search and statistical 
browse in many aspects. Moreover, the chat log is 
provided to learn about the intention of the knowl-
edge composers. We frequently want to know 
about the background of the solution for better un-
derstanding or to remind us about the decision, but 
we cannot find one. To avoid the repetition of a 
mistake, we systematically provide the chat log to 
keep the trace of discussion or the comments to 
show the intention of knowledge composers. 
2.2 Knowledge Development in KUI 
Adopting the concept of Open Source software 
development, we will be possibly able to develop a 
framework for domain specific knowledge devel-
opment under the web community environment. 
Sharing and collaboration are the considerable fea-
tures of the framework. The knowledge will be 
finally shared among the communities by receiving 
the consensus from the participants in each step. 
To facilitate the knowledge development, the proc-
ess is deliberated into four steps (Sornlertlam-
vanich, 2006). 
14
New Task 
A new task (Topic of interest) can be posted to 
draw intention from participants. The only selected 
tasks by a major vote will then be proceed for fur-
ther discussion in the requested type of task i.e., 
Opinion Poll, Localization or Public-Hearing. 
 
 
 
   Figure 1. Process of knowledge development 
 
Opinion Poll 
The selected task is posted to call for opinions 
from the participants in this step. Opinion poll is 
conducted to get the population of each opinion. 
The result of the opinion poll provides the variety 
of opinions that reflects the current thought of the 
communities together with the consensus to the 
opinions. 
 
Localization 
Translation is a straightforward implementation of 
the localization. Collaborative translation helps 
producing the knowledge in multiple languages in 
the most efficient way. Multi-lingual texts are gen-
erated in this type of task. 
 
Public-Hearing 
The result of discussion will be revised and con-
firmed by gathering the opinions to develop the 
final draft of the proposal. Suggestions for revision 
are ranked according to the vote. The author may 
consider the weight of suggestion to make decision 
on the final revision. 
The developed knowledge is started from post-
ing 'New Task', participants express their supports 
by casting a vote. Upon a threshold the    'New 
Task' is selected for conducting a poll on 'Opinion', 
or introducing to the community by  'Localization', 
or posting a draft for 'Public-Hearing' to gather 
feedbacks from the community. The transition 
from 'Opinion' to either 'Localization' or 'Public-
Hearing' occurs when the 'Opinion' has a concrete 
view for implementation. The discussion in 'Local-
ization' and 'Public-Hearing' is however inter-
changeable due to purpose of implementation 
whether to adopt the knowledge to the local com-
munity or to get feedbacks from the community. 
The knowledge creating is managed in 4 differ-
ent categories corresponding to the stage of knowl-
edge. Each individual in the community casts a 
vote to rank the appropriateness of solutions at 
each category. The community can then form the 
community knowledge under the 'Selectional Pref-
erence' background. 
Topic 
     of  
Interest 
Localization 
Opinion 
Public Hear-
ing 
2.3 Features in KUI 
These KUI features fulfill the process of hu-
man thought to record the knowledge. 
 
Poll-based Opinion or Public-Hearing 
A contributor may choose to work individually by 
posting an opinion e.g. localization, suggestion 
etc., or join a discussion desk to conduct 'Public-
Hearing' with others on the selected topic. The dis-
cussion can be conducted via the provided 'Chat' 
frame before concluding an opinion. Any opinions 
or suggestions are committed to voting. Opinions 
can be different but majority votes will cast the 
belief of the community. These features naturally 
realize the online collaborative works to create the 
knowledge. 
 
Individual or Group Work 
Thought may be formed individually or though a 
concentrated discussion. KUI facilitates a window 
for submitting an opinion and another window for 
submitting a chat message. Each suggestion can be 
cast through the 'Opinion' window marked with a 
degree of its confidence. By working individually, 
comments to a suggestion can be posted to mark its 
background to make it more understanding. On the 
other hand, when working as a group, discussions 
among the group participants will be recorded. The 
discussion can be resumed at any points to avoid 
the iterating words. 
 
Record of Intention 
The intention of each opinion can be reminded by 
the recorded comments or the trace of discussions. 
Frequently, we have to discuss again and again on 
the result that we have already agreed. Misinterpre-
15
tation of the previous decision is also frequently 
faced when we do not record the background of 
decision. Record of intention is therefore necessary 
in the process of knowledge creation. The knowl-
edge interpretation also refers to the record of in-
tention to obtain a better understanding. 
 
Selectional Preference 
Opinions can be differed from person to person 
depending on the aspects of the problem. It is not 
always necessary to say what is right or what is 
wrong. Each opinion should be treated as a result 
of intelligent activity. However, the majority ac-
cepted opinions are preferred at the moment. Ex-
periences could tell the preference via vote casting. 
The dynamically vote ranking will tell the selec-
tional preference of the community at each mo-
ment 
3 KUI for Collective Intelligent Develop-
ment 
Related to the principle of KUI and its features, 
KUI provide many collaborative tools or applica-
tion as followings. 
  
Translating 
Translating is a type of text for language expert 
group contribution. Since the existing knowledge 
in one language is invaluable to other language 
communities. Translating such knowledge will 
help bridging the different language communities. 
It will also bring the individual to an unlimited in-
formation space beyond the language barrier. Con-
tribution in term and phrase translation is to create 
a multi-lingual terminology and an aligned multi-
lingual corpus.  
KUI-Translating Room facilitates an individual 
to view either the current translation tasks in the 
task list or the discussion forum of each translating 
task. Online lookup is also provided to consult a 
term translation.  
Individual participated in KUI-Translating can 
cast a vote for a new task, a vote for multiple tasks 
is allowed, select a topic to participate in the dis-
cussion forum, translate the existing terms into 
your own language, chat with your friends to find 
the best translation, cast a vote to your favorite 
translation, invite assistants to your own initiated 
private task, and propose a new task for commu-
nity voting as well.  
Polling 
Opinion Poll is conducted for getting the popula-
tion of each opinion. The result of the opinion poll 
shows the variety of opinions that reflects the cur-
rent thought of the communities together with the 
consensus to the opinions.  
Similar to KUI-Translating, an individual can 
view the current polling task in the task list as well 
as the discussion forum of each polling task via 
KUI-Polling. And current result of polling can be 
view via online lookup function.    
 
Public-Hearing 
Public Hearing is a way to make a complete docu-
ment from the draft. The result from discussion 
will be received and confirmed by gathering the 
opinions to reflect in the final document. Voting of 
the opinion will help the author to select the ap-
propriate opinion of the community. 
An individual can view the current public hear-
ing tasks in the task list as well as the discussion 
forum of each public hearing task via KUI-Polling. 
And current result of polling can be view via 
online lookup function.    
 
 
Figure 2. KUI-Translating page 
 
Writing 
Writing your document online will keep your 
document in access anywhere and anytime. Indi-
vidual does not have to carry all the documents 
with him/her. Only online, one can work on it. 
Sharing the editing online will also support the 
collaborative work.  
16
With KUI-Writing, individual can create or im-
port a new document, edit the existing document, 
chat with friends about the document, and save or 
export the document.  
 
Correspondent to other collaborative tools, all of 
KUI-application provides function to cast a vote 
for either a new task or multiple tasks. Individual 
can select a topic to participate or post new topic, 
chat with others, invite assistants to his/her own 
initiated task, and so on. 
 
The majority vote will select the best solution 
for the collaborative task. 
4 ExpertScore 
KUI heavily depends on members? voting score to 
produce a reliable result. Therefore, we introduce 
an adjustable voting score to realize a self-
organizing system. Each member is initially pro-
vided a default value of voting score equals to one. 
The voting score is increased according to Ex-
pertScore which is estimated by the value of Ex-
pertise, Contribution, and Continuity of the par-
ticipation history of each member. Expertise is a 
composite score of the accuracy of opinion and 
vote, as shown in Equation 1. Contribution is a 
composite score of the ratio of opinion and vote 
posting comparing to the total, as shown in Equa-
tion 2. Continuity is a regressive function based on 
the assumption that the absence of participation of 
a member will gradually decrease its ExpertScore 
to one after a year (365 days) of the absence, as 
shown in Equation 3. 
 
)3(
365
1
)2(
)(
)(
)(
)(
)1(
)(
)(
)(
)(
4
???
???
??=
+=
+=
???????????????
?
??
D
Continuity
TotalVotecount
Votecount
onTotalOpinicount
Opinioncount
onContributi
Votecount
BestVotecount
Opinioncount
nBestOpiniocount
Expertise
??
??
  
 
Where, 
1=+++ ????  
D is number of recent absent date 
 
As a result, the ExpertScore can be estimated by 
Equation 4. 
 
???
???
?
???
???
?
++
+
?
???
?
???
? ??
???
??=
)(
)(
)(
)(
)(
)(
)(
)(
365
1
4
TotalVotecount
Votecount
onTotalOpinicount
Opinioncount
Votecount
BestVotecount
Opinioncount
nBestOpiniocount
D
eExpertScor
??
??
                          
???(4) 
 
The value of ExperScore is ranged between 1 to 
365 according to the accuracy and the rate of con-
tribution of each member. This means that reliable 
members are rewarded better score for each vote. 
However, the expertise of the member is decreased 
according to the continuity of the participation. By 
means of the ExpertScore, we can rank the opin-
ions precisely and yield reliable results, especially 
for the results produced by an online community. 
 
 
Figure 3. KUI-Polling page 
5 Application Show Case 
KUI for Collaborative Translation Task 
In this collaborative text translation, individual 
participants of different mother language work 
online as a virtual group by using KUI. There are 
several translation task required the collaborative 
translation such as Asian WordNet (originally from 
WordNet (Miller, 1995; http://wordnet. prince-
ton.edu/), Medical Translation, OSS Glossary and 
so on. And some are ready for individual use for 
example NICT?s Japanese ? English News Articles 
Alignment, Open Office Glossary, Swadesh List, 
Technical Term Dictionary. 
The volunteer participants are to translate the 
English text into their native languages, by using 
KUI. They act as a virtual group and participate in 
the translation via this web interface. With differ-
ent backgrounds and degrees of translation abili-
ties, they, therefore, can discuss or exchange their 
opinion while translating each utterance. The 
17
communication is not only for getting to know 
each other, but also for better understanding of the 
utterance before translation. Figure 4 shows the 
participation work flow. 
 
 
                Figure 4. Participant work flow 
 
 
 
Figure 5. Lookup page of Asian WordNet 
 
 
6 Conclusion 
We proposed an efficient online collaborative 
framework in producing and maintaining knowl-
edge according to the principle of collective intel-
ligent. KUI was designed to support an open web 
community by introducing a voting system and a 
mechanism to realize the function of selectional 
preference. It was efficiently introduced to encour-
age the communication among individuals from 
different background. KUI was also proved to sup-
port the collaborative work in producing many 
kinds of tasks. The translated text, an example, will 
be voluntarily maintained by the online partici-
pants under the selectional preference based on the 
voting function. Correspondent to collective intel-
ligent collaborative tool, KUI enables to connect 
and collaborate among individual intelligence in 
order to accomplish a complex mission. Of course, 
?two minds are better than one?.  
 
Acknowledgment 
 
Thanks to KUI community for the invaluable con-
tribution to this project. 
References 
http://www.google.com 
http://www.sourceforge.net 
http://www.wiki.org 
 
N. Johnson, S. Rasmussen, C. Joslyn, L. Rocha, S. 
Smith and M. Kantor. Symbiotic Intelligence: 
Self-organizing Knowledge on Distributed Net-
works Driven by Human Interaction, Int. Con-
ference on Artificial Life, Boston. 1998. 
 
KUI. http://www.tcllab.org/kui/ (2006) 
 
Levy. Collective Intelligence: Mankind?s Emerg-
ing World in Cyberspace, New York, 1997. 
 
G. A. Miller. WordNet: A Lexical Databases for 
English. Communications of the ACM, 39-41, 
November, 1995 
 
J.B. Smith.  Collective Intelligence in Computer-
Based Collaboration. Erlbaum, New York, 
1994. 
V. Sornlertlamvanich. KUI: The OSS-Styled 
Knowledge Development System. Handbook of 
The 7th AOSS Symposium, Kuala Lumpur, Ma-
laysia, 2006. 
 
WordNet. http://wordnet.princeton.edu/ 
18
Enhanced Tools for Online Collaborative Language Resource  
Development 
Virach Sornlertlamvanich  
Thatsanee Charoenporn  
Suphanut Thayaboon 
Chumpol Mokarat 
Thai Computational Linguistics Lab.  
NICT Asia Research Center, 
Pathumthani, Thailand 
{virach, thatsanee, suphanut,  
chumpol}@tcllab.org 
Hitoshi Isahara 
National Institute of Information 
and Communications Technology 
3-5 Hikaridai, Seika-cho, soraku-gaun, 
Kyoto, Japan 619-0289 
isahara@nict.go.jp 
 
Abstract 
This paper reports our recent work of tool 
development for language resource con-
struction. To make a revision of Asian 
WordNet which is automatically generated 
by using the existing English translation 
dictionary, we propose an online collabora-
tive tool which can organize multiple trans-
lations. To support the work of syntactic 
dependency tree annotation, we develop an 
editing suite which integrates the utilities 
for word segmentation, POS tagging and 
dependency tree into a sequence of editing. 
1 Introduction 
Though WordNet was already used as a starting 
resource for developing many language WordNets, 
the constructions of the WordNet for languages 
can be varied according to the availability of the 
language resources. Some were developed from 
scratch, and some were developed from the combi-
nation of various existing lexical resources. 
This paper presents an online collaborative tool 
particularly to facilitate the construction of the 
Asian WordNet which is automatically generated 
by using the existing resources having only Eng-
lish equivalents and the lexical synonyms. 
In addition, to support the work of syntactic de-
pendency tree annotation, we develop an editing 
suite which integrates the utilities for word seg-
mentation, POS tagging and dependency tree. The 
tool is organized in 4 steps, namely, sentence se-
lection, word segmentation, POS tagging, and syn-
tactic dependency tree annotation. 
The rest of this paper is organized as follows: 
Section 2 describes the collaborative interface for 
revising the result of synset translation. Section 3 
describes the tool for annotating Thai syntactic 
dependency tree corpus. And, Section 4 concludes 
our work. 
2 Collaborative Tools for Asian WordNet 
Construction 
There are some efforts in developing Wordnets for 
some of Asian languages, e.g. Chinese, Japanese, 
Korean, and Hindi. The number of languages that 
have been successfully developed their Wordnets 
is still limited to some active research in the area. 
However, the extensive development of Wordnet 
in other languages is of the efforts to support the 
NLP research and implementation. It is not only to 
facilitate the implementation of NLP applications 
for the language, but also provide an inter-linkage 
among the Wordnets for different languages to de-
velop multi-lingual applications. 
We adopt the proposed criteria for automatic 
synset assignment for Asian languages which has 
limited language resources. Based on the result 
from the above synset assignment algorithm, we 
introduce KUI (Knowledge Unifying Initiator), 
(Sornlertlamvanich et al, 2007) to establish an 
online collaborative work in refining the WorNets. 
KUI is a community software tool which allows 
registered members including language experts to 
revise and vote for the synset assignment. The sys-
tem manages the synset assignment according to 
the preferred score obtained from the revision 
process. As a result, the community-based Word-
Nets will be accomplished and exported into the 
original form of WordNet database. Via the synset 
The 6th Workshop on Asian Languae Resources, 2008
105
ID assigned in the WordNet, the system can gener-
ate a cross language WordNet. Through this effort, 
a translated version of Asian WordNet can be es-
tablished. 
Table 1 shows a record of WordNet displayed 
for translation in KUI interface. English entry to-
gether with its part-of-speech, synset, and gloss are 
provided if exists. The members will examine the 
assigned lexical entry and decide whether to vote 
for it or propose a new translation. 
Car 
[Options] 
POS : NOUN 
Synset : auto, automobile, machine, motorcar 
Gloss : a motor vehicle with four wheels; usually propelled 
by an internal combustion engine; 
Table 1. A record for a synset 
 
Figure 1. KUI interface (www.tcllab.org/kui) 
Figure 1 illustrates the translation page of KUI 
In the working area, the login member can partici-
pate in proposing a new translation or voting for 
the preferred translation to revise the synset as-
signment. Statistics of the progress as well as many 
useful functions such as item search, chat, and list 
of online participants are also provided to under-
stand the progress of work and to work online with 
other members. 
3 Tool for Constructing a Syntactic De-
pendency Tree Annotated Corpus 
The tool is organized in 4 steps, namely, sentence 
selection, word segmentation, POS tagging, and 
syntactic dependency tree annotation, shown in 
Figure 2. Sentence segmentation is yet another 
crucial issue for the Thai language. We, however, 
will not discuss about the issue in this work. The 
input is already a list of sentences provided for an-
notator to select. 
 
Figure 2. Syntactic Dependency Tree Annotation 
3.1 POS Annotation 
The result from the automatic word segmentation 
and POS tagging program is generated with alter-
native POSs for revision. A dropdown list of POSs 
is provided for annotator to correct the POS. Since 
word segmentation is processed together with POS 
tagging, the annotator is also provided a GUI to 
merge or to divide the proposed word unit. 
 
3.2 Syntactic Dependency Tree Annotation 
The result from POS annotation in Section 3.1 is 
passed to define the syntactic dependency between 
words. The dependency is assigned to form a 
phrase and a sentence respectively. The final out-
put will be marked in the XML manner and shown 
as a tree for confirmation. 
 
4 Conclusion 
Our current work on the web-based collaborative 
tool for Asian WordNet construction and tool for 
Syntactic Dependency Tree Annotation are devel-
oped as an open platform for online contribution. 
A user-friendly interface and self-organizing utili-
ties are intentionally prepared to support the online 
collaborative work. 
References 
Virach Sornlertlamvanich, Thatsanee Charoenporn, 
Kergit Robkop, and Hitoshi Isahara. 2007. Collabo-
rative Platform for Multilingual Resource Develop-
ment and Intercultural Communication, IWIC2007, 
Springer, LNCS4568:91-102.
The 6th Workshop on Asian Languae Resources, 2008
106
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 139?144,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Thai WordNet Construction 
 
 
Sareewan Thoongsup1 
Kergrit Robkop1 
Chumpol Mokarat1 
Tan Sinthurahat1 
1 Thai Computational Linguistics Lab. 
NICT Asia Research Center, Thailand 
{sareewan, kergrit,   
Chumpol, tan, thatsanee, 
virach}@tcllab.org 
Thatsanee Charoenporn 1,2 
Virach Sornlertlamvanich 1,2 
Hitoshi Isahara 3 
2National Electronics and Computer 
Technology Center Thailand, Thailand 
3National Institute of Information and 
Communications Technology, Japan 
isahara@nict.go.jp 
 
  
 
Abstract 
This paper describes semi-automatic construc-
tion of Thai WordNet and the applied method 
for Asian wordNet. Based on the Princeton 
WordNet, we develop a method in generating 
a WordNet by using an existing bi-lingual dic-
tionary. We align the PWN synset to a bi-
lingual dictionary through the English equiva-
lent and its part-of-speech (POS), automati-
cally. Manual translation is also employed af-
ter the alignment. We also develop a web-
based collaborative workbench, called KUI 
(Knowledge Unifying Initiator), for revising 
the result of synset assignment and provide a 
framework to create Asian WordNet via the 
linkage through PWN synset.     
1 Introduction 
The Princeton WordNet (PWN) (Fellbuam, 
1998) is one of the most semantically rich Eng-
lish lexical banks widely used as a resource in 
many research and development. WordNet is a 
great inspiration in the extensive development of 
this kind of lexical database in other languages. 
It is not only an important resource in imple-
menting NLP applications in each language, but 
also in inter-linking WordNets of different lan-
guages to develop multi-lingual applications to 
overcome the language barrier. There are some 
efforts in developing WordNets of some lan-
guages (Atserias and et al, 1997; Vossen, 1997; 
Farrers and et al, 1998; Balkova and et al, 2004; 
Isahara and et al, 2008). But the number of lan-
guages that have been successfully developed  
 
 
 
their WordNets is still limited to some active 
research in this area. This paper, however, is the 
one of that attempt.  
This paper describes semi-automatic construc-
tion of Thai WordNet and the applied method for 
Asian WordNet. Based on the Princeton Word-
Net, we develop a method in generating a 
WordNet by using an existing bi-lingual diction-
ary. We align the PWN synset to a bi-lingual 
dictionary through the English equivalent and its 
part-of-speech (POS), automatically. Manual 
translation is also employed after the alignment. 
We also develop a web-based collaborative 
workbench, called KUI (Knowledge Unifying 
Initiator), for revising the result of synset as-
signment and provide a framework to create 
Asian WordNet via the linkage through PWN 
synset. 
The rest of this paper is organized as follows: 
Section 2 describes how we construct the Thai 
WordNet, including approaches, methods, and 
some significant language dependent issues ex-
perienced along the construction. Section 3 pro-
vides the information on Asian WordNet con-
struction and progress. And Section 4 concludes 
our work.  
2 Thai WordNet Construction Proce-
dure 
Different approaches and methods have been 
applied in constructing WordNet of many lan-
guages according to the existing lexical re-
sources. This section describes how Thai Word-
Net is constructed either approach or method. 
139
2.1 Approaches 
To build language WordNet from scratch, two 
approaches were brought up into the discussion: 
the merge approach and the expand approach.  
The merge approach is to build the taxonomies 
of the language; synsets and relations, and then 
map to the PWN by using the English equivalent 
words from existing bilingual dictionaries.  
The expand approach is to map or translate lo-
cal words directly to the PWN's synsets by using 
the existing bilingual dictionaries. 
Employing the merge approach, for Thai as an 
example, we will completely get synsets and re-
lations for the Thai language. But it is time and 
budget consuming task and require a lot of 
skilled lexicographers as well, while less time 
and budget is used when employing the expand 
approach to get a translated version of WordNet. 
But some particular Thai concepts which do not 
occur in PWN will not exist in this lexicon. 
Comparing between these two approaches, the 
Thai WordNet construction intended to follow 
the expand approach by this following reasons; 
 
z Many languages have developed their 
own WordNet using the PWN as a 
model, so we can link Thai lexical data-
base to those languages.  
z The interface for collaboration for other 
languages can be easily developed. 
2.2 Methods 
As presented above, we follow the expand ap-
proach to construct the Thai WordNet by trans-
lating the synsets in the PWN to the Thai lan-
guage. Both automatic and manual methods are 
applied in the process.  
2.2.1 Automatic Synset Alignment  
Following the objective to translate the PWN to 
Thai, we attempted to use the existing lexical 
resources to facilitate the construction. We pro-
posed an automatic method to assign an appro-
priate synset to a lexical entry by considering its 
English equivalent and lexical synonyms which 
are most commonly encoded in a bi-lingual dic-
tionary. (Charoenporn 2008; Sornlertlamvanich, 
2008).    
 
 
 
 
 
 
 WordNet (synset) TE Dict (entry)
 total Assigned total assigned 
Noun 145,103 18,353 
(13%) 
43,072 11,867 
(28%)
Verb 24,884 1,333 
(5%) 
17,669 2,298 
(13%)
Adjective 31,302 4,034 
(13%) 
18,448 3,722 
(20%)
Adverb 5,721 737 
(13%) 
3,008 1,519 
(51%)
Total 207,010 24,457 
(12%) 
82,197 19,406 
(24%)
 
Table 1. Synset assignment to entries in 
Thai-English dictionary 
 
For the result, there is only 12% of the total 
number of the synsets that were able to be as-
signed to Thai lexical entries. And about 24% of 
Thai lexical entries were found with the English 
equivalents that meet one of our criteria. Table 1 
shows the successful rate in assigning synsets to 
the lexical entry in the Thai-English Dictionary.  
Considering the list of unmapped lexical entry, 
the errors can be classified into three groups, as 
the following. 
1. The English equivalent is assigned in a 
compound, especially in case that there 
is no an appropriate translation to repre-
sent exactly the same sense. For exam-
ple, 
L: ??????????? raan3-khaa3-pleek1 
E: retail shop 
2. Some particular words culturally used I 
one language may not be simply trans-
lated into one single word sense in Eng-
lish. In this case, we found it explained 
in a phrase. For example,  
L: ???????? kan0-jeak1 
E: bouquet worn over the ear  
3. Inflected forms i.e. plural, past partici-
ple, are used to express an appropriate 
sense of a lexical entry. This can be 
found in non-inflection languages such 
as Thai and most of Asian languages, 
For example,  
L: ???????? raaw3-ra0-thom0 
E: greived 
By using this method, a little part of PWN has 
been translated into Thai. About 88% of the total 
number of the synsets still cannot be assigned.   
Manual step is therefore applied.   
140
2.2.2 Manual Construction 
Human translation is our next step for synset 
translation. Two important issues were taken into 
discussion, when starting the translation process.  
Those are; 
? How to assign or translate new concepts 
that still do not occur in the Thai lexicon. 
Compound word or phrase is acceptable 
or not. 
? Which equivalent do we need to consider, 
synset-to-synset equivalent or word-to-
word equivalent? 
For the first issue, we actually intend to trans-
late the PWN synsets into single Thai word only. 
But problems occurred when we faced with con-
cept that has not its equivalent word. For exam-
ple, 
  
filly#1 -- (a young female horse under the age 
of four) 
colt2#1 ? (a young male horse under the age 
of four) 
hog2#2, hogget#1, hogg#2 ? (a sheep up to 
the age of one year: one yet to be sheared) 
 
There is not any word that conveys the mean-
ing of the above concepts. That is because of the 
difference of the culture. In this case, phrase or 
compound word will be introduced to use as the 
equivalent word of the concept. This phenome-
non always occurs with cultural dependent con-
cept, technical terms and new concepts.  
As for the second issue, considering between 
(1) synset-to-synset equivalent assignment or (2) 
word-to-word equivalent assignment has to be 
discussed. Let consider the following concept of 
?dog? in the PWN. 
 
dog#1, domestic dog#1, Canis familiaris#1 -- 
(a member of the genus Canis (probably de-
scended from the common wolf) that has been 
domesticated by man since prehistoric times; 
occurs in many breeds; "the dog barked all 
night")  
 
The above synset consists of three words; dog, 
domestic dog, and Canis familiaris. The set of 
Thai synonyms that is equivalent to this English 
synset is the following. 
 
Thai synset of ?dog? 
{T1 ??? maa4 ?dog? (normal word),  
  T2 ????? su1-nak3 ?dog? (polite word),  
  T3 ????????? su1-nak3-baan0 ?domestic dog?,  
  T4 ????? ??????????? kha0-nis3-fae0-mi0-lia0-ris3 
?Canis familiaris?} 
These words have the same concepts but are 
different in usage. How do we choose the right 
Thai word for the right equivalent English word? 
It is a crucial problem. In the paragraph below, 
three English words which represent the concept 
?dog? are used in the different context and can-
not be interchanged. Similarly, T1, T2 and T3 
cannot be used substitutionally. Because it con-
veys different meaning. Therefore, word-to-word 
is our solution.   
 
?...Dog usually means the domestic dog, 
Canis lupus familiaris (or "Canis familiaris" in 
binomial nomenclature)....? 
 
Dog  T1 ??? maa4 ?dog?,  
T2 ????? su1-nak3 ?dog?   
Domestic dog  T3 ????????? 
su1-nak4-baan0  
?domestic dog? 
Canis familiaris T4 ????? ???????????  
kha0-nis3-fae0-mi0-
lia0-ris3 
?Canis familiaris? 
 
    Consequently, word-to-word equivalent is 
very useful for choosing the right synonyms with 
the right context. 
     In conclusion, the main principle for the Eng-
lish to Thai translation includes 
(1) ?Single word? is lexicalized the existence 
of concepts in Thai.  
(2) ?Compound? or ?Phrase? is represented 
some concepts that are not lexicalized in 
Thai. 
(3) Synset-to-synset equivalent is used for 
finding Thai synset that is compatible 
with PWN synset. 
(4) Word-to-word equivalent is used for find-
ing the right Thai word that is compatible 
with PWN word in each synset.  
2.3 Language Issues  
This section describes some significant charac-
teristics of Thai that we have to consider care-
fully during the translation process.    
 
141
2.3.1 Out of concepts in PWN 
There are some Thai words/concepts that do not 
exist in the PWN, especially cultural-related 
words. This is the major problem we have to 
solve during the translation.  
One of our future plans is to add synsets that 
do not exist into the PWN.  
2.3.2 Concept differentiation 
Some concepts in the PWN are not equal to Thai 
concepts. For example, a synset {appear, come 
out} represents one concept ?be issued or pub-
lished? in English, but meanwhile, it represents 
two concepts in Thai, the concept of printed mat-
ter, and the concept of film or movie, respec-
tively.   
2.3.3 Concept Structure differentiation 
In some cases, the level of the concept relation 
between English and Thai is not equal. For ex-
ample, {hair} in the PWN represents a concept 
of ?a covering for the body (or parts of it) con-
sisting of a dense growth of threadlike structures 
(as on the human head); helps to prevent heat 
loss; ?? but in Thai, it is divided into two con-
cepts;  
 
T1 ?? khon4 ?hair? 
= ?hair? that cover the body  
T2 ?? phom4 ?hair? 
= ?hair? that cover on the human head  
 
This shows the nonequivalent of concept. 
Moreover, it also differs in the relation of con-
cept. In PWN ?hair? is a more general concept 
and ?body hair? is more specific concepts. But in 
Thai T1 ?? khon4 ?hair? (hair that covers the 
body) is more general concept and T2 ?? phom5 
?hair? (hair that covers on the human head) is 
more specific one.  
2.3.4 Grammar and usage differentiation  
? Part of speech  
? ?Classifier? is one of Thai POS 
which indicates the semantic 
class to which an item belongs. 
It's widely use in quantitative 
expression. For example, ??? 
knon? used with 'person', ????? 
lang? used with house. 
? Some adjectives in English, such 
as ?beautiful?, 'red' and so on can 
function as the adjective and at-
tribute verb in Thai.  
? Social factors determining language us-
age 
? In Thai, some social factors, 
such as social status, age, or sex 
play an important role to deter-
mine the usage of language. For 
example, these following three 
words ??? kin0, ??? chan4 and ???? 
sa0-waey4, having the same 
meaning ?eat?, are used for dif-
ferent social status of the listener 
or referent. These words cannot 
be grouped in the same synset 
because of their usage. 
3 From Thai to Asian WordNet  
AWN, or Asian WordNet, is the result of the col-
laborative effort in creating an interconnected 
WordNet for Asian languages. Starting with the 
automatic synset assignment as shown in section 
2, we provide KUI (Knowledge Unifying Initia-
tor) (Sornlertlamvanich, 2006), (Sornlertlam-
vanich et al, 2007) to establish an online col-
laborative work in refining the WorNets. KUI is 
community software which allows registered 
members including language experts revise and 
vote for the synset assignment. The system man-
ages the synset assignment according to the pre-
ferred score obtained from the revision process. 
As a result, the community WordNets will be 
accomplished and exported into the original form 
of WordNet database. Via the synset ID assigned 
in the WordNet, the system can generate a cross 
language WordNet result. Through this effort, an 
initial version of Asian WordNet can be fulfilled.  
3.1 Collaboration on Asian WordNet 
Followings are our pilot partners in putting 
things together to make KUI work for AWN. 
? Thai Computational Linguistics Labora-
tory TCL), Thailand  
? National Institute of Information and 
Communications Technology (NICT), 
Japan  
? National Electronics and Computer Tech-
nology Center (NECTEC), Thailand 
? Agency for the Assessment and Applica-
tion of Technology (BPPT), Indonesia 
142
? National University of Mongolia (NUM), 
Mongolia 
? Myanmar Computer Federation (MCF), 
Myanmar 
 
 
 
Figure 1. Collaboration on Asian WordNet 
 
3.2 How words are linked 
In our language WordNet construction, lexical 
entry of each language will be mapped with the 
PWN via its English equivalent. On the process 
of mapping, a unique ID will be generated for 
every lexical entry which contains unique 
sense_key and synset_offset from PWN. Exam-
ples of the generated ID show in Table 2. When 
a word with a unique ID is translated into any 
language, the same unique ID will be attached to 
that word automatically. By this way, the lexicon 
entry in the community can be linked to the each 
other using this unique ID.  
 
 
 
 
 
 
 
 
 
 
 
Table 2. Examples of the unique index with 
sense_key and synset_offset 
3.3 Progress on Thai WordNet and Asian 
WordNet 
This section presents the progress on Asian 
WordNet and Thai WordNet construction. 
3.3.1 Current Asian WordNet 
At present, there are ten Asian languages in the 
community. The amount of the translated synsets 
has been continuously increased. The current 
amount is shown in the table 3. As shown in the 
table, for example, 28,735 senses from 117,659 
senses have been translated into Thai. 
 
Language Synset (s) % of total 
117,659 
senses 
Thai 28,735 24.422 
Korean 23,411 19.897 
Japanese 21,810 18.537 
Myanmar 5,701 4.845 
Vietnamese 3,710 3.153 
Indonesian 3,522 2.993 
Bengali 584 0.496 
Mongolian 424 0.360 
Nepali 13 0.011 
Sudanese 11 0.009 
Assamese 2 0.008 
Khmer 2 0.002 
 
Table 3. Amount of senses translated in  
each language 
3.3.2 Sense Sharing 
Table 4 shows the amount of senses that have 
been conjointly translated in the group of lan-
guage. For example, there are 6 languages that 
found of the same 540 senses.   
 
Language Sense (s) %  
1-Language 27,413 55.598 
2-Language 11,769 23.869 
3-Language 5,903 11.972 
4-Language 2,501 5.072 
5-Language 1,120 2.272 
6-Language 540 1.095 
7-Language 53 0.107 
8-Language 4 0.008 
9-Language 2 0.004 
10-Language 1 0.002 
Total 49,306 100.000 
 
Table 4. Amount of senses translated  
in each language 
3.3.3 Amount of Words in Thai synsets  
From the synset in Thai WordNet, there are the 
minimum of one word (W1) in a synset and the 
maximum of six words (W6) in a synset. The 
percentage shown in Table 5 presents that 
89.78% of Thai synset contain only one word.  
 
 
 
 
143
Amount of word 
in Thai Synset 
Sense (s) %  
W1 19,164 89.78 
W2 1,930 9.04 
W3 211 0.99 
W4 27 0.13  
W5 4 0.02 
W6 8 0.04 
Total 21,344 100.00 
 
Table 5. Amount of Word in Thai synsets 
4 Conclusion  
In this paper we have described the methods of 
Thai WordNet construction. The semi-auto 
alignment method constructed the database by 
using the electronic bilingual dictionary. The 
manual method has constructed by experts and 
the collaborative builders who works on the web 
interface at www.asianwordnet.org.  
References  
Christiane Fellbuam. (ed.). 1998. WordNet: An 
Electronic Lexical Database. MIT Press, 
Cambridge, Mass. 
Xavier Farreres, German Rigau and Horacio 
Rodriguez. 1998. Using WordNet for building 
WordNets. In: Proceedings of the COL-
ING/ACL Workshop on Usage of WordNet in 
Natural Language Processing Systems, Mont-
real. 
Hitoshi Isahara, Francis Bond, Kiyotaka Uchi-
moto, Masao Utiyama and Kyoko Kanzaki. 
2008. Development of the Japanese WordNet. 
In LREC-2008, Marrakech. 
Jordi Atserias, Salvador Climent, Xavier Far-
reres, German Rigau and Horacio Rodriguez. 
1997. Combinding multiple Methods for the 
automatic Construction of Multilingual 
WordNets. In proceedings of International 
Conference ?Recent Advances in Natural 
Language Processing? (RANLP?97). Tzigov 
Chark, Bulgaria. 
Piek Vossen, 1997. EuroWordNet: a multilingual 
database for information retrieval. In proceed-
ings of DELOS workshop on Cross-language 
Information Retrieval, March 5-7, 1997, Zu-
rich. 
Thatsanee Charoenporn, Virach Sornlertlam-
vanich, Chumpol Mokarat, and Hitoshi Isa-
hara. 2008. Semi-automatic Compilation of 
Asian WordNet, In proceedings of the 14th 
NLP2008, University of Tokyo, Komaba 
Campus, Japan, March 18-20, 2008. 
Valenina Balkova, Andrey Suhonogov, Sergey  
Yablonsky. 2004. Rusian WordNet: From 
UML-notation to Internet/Infranet Database 
Implementation. In Porceedings of the Second 
International WordNet Conference (GWC 
2004), pp.31-38. 
Virach Sornlertlamvanich, Thatsanee Charoen-
porn, Chumpol Mokarat, Hitoshi Isahara, 
Hammam Riza, and Purev Jaimai. 2008. 
Synset Assignment for Bi-lingual Dictionary 
with Limited Resource. In proceedings of the 
Third International Joint Conference on Natu-
ral Language Processing (IJCNLP2008), Hy-
derabad, India, January 7-12, 2008. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
144
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
