Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 97?102,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Three reasons to adopt TAG-based surface realisation
Claire Gardent
CNRS / LORIA
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
gardent@loria.fr
Eric Kow
INRIA / LORIA
Universite? Henri Poincare?
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
kow@loria.fr
Abstract
Surface realisation from flat semantic for-
mulae is known to be exponential in the
length of the input. In this paper, we argue
that TAG naturally supports the integration
of three main ways of reducing complex-
ity: polarity filtering, delayed adjunction
and empty semantic items elimination. We
support these claims by presenting some
preliminary results of the TAG-based sur-
face realiser GenI.
1 Introduction
Surface realisation consists in producing all the
sentences associated by a grammar with a given
semantic formula. For lexicalist grammars such
as LTAG (Lexicalised Tree Adjoining Grammar),
surface realisation usually proceeds bottom-up
from a set of flat semantic literals1. However,
surface realisation from flat semantic formulae is
known to be exponential in the length of the input
(Kay96; Bre92; KS02). In this paper, we abstract
from the TAG based surface realiser for French
GenI, (GK05) and argue that TAG naturally sup-
ports the integration of various proposals made to
help reduce either surface realisation or parsing
complexity into a TAG based, lexically driven sur-
face realiser. Specifically, we show:
1. that TAG elementary trees naturally support
the implementation of a technique called po-
larity filtering used to reduce the exponen-
tial factor introduced by lexical ambiguity
(Per03),
1See e.g., (CCFP99) for a discussion summarising the rea-
sons for this choice.
2. that TAG two operations of substitution and
adjunction provides a natural framework for
implementing a delayed adjunction mecha-
nism capable of reducing the complexity due
to the lack of ordering information and
3. that TAG extended domain of locality helps
reduce the potential complexity increment in-
troduced by semantically empty items such as
infinitival ?to? or complementiser ?that?.
2 Surface realisation, flat semantics and
computational complexity
Why is surface realisation exponential in the
length of the input? As shown in (Kay96), one
reason for this is the lack of ordering information.
Contrary to parsing where the input is a string i.e.,
an ordered list of words, the input to surface re-
alisation is a set of literals. Supposing each lit-
eral selects exactly one constituent in the lexicon,
then the number of possible combinations between
these constituents will be 2n (the number of sub-
sets obtainable from a set of size n).
In practice of course, there are possible restric-
tions on constituent combination. In particular,
most existing realisers impose the constraint that
only constituents with non overlapping semantics
and compatible indices can be combined. Be-
cause of this restriction, the core of the complex-
ity stems in practice from intersective modifiers
(Bre92; Kay96). Given a set of n modifiers all
modifying the same structure, all possible inter-
mediate structures will be constructed i.e. 2n+1.
A second reason for the exponential complexity
of surface realisation is lexical ambiguity. As for
bottom-up parsing, in surface realisation from flat
semantics, the input is used to select a set of lexi-
cal entries namely all lexical entries whose seman-
97
tics subsumes one or more of the input literals. In
a realistic grammar, one literal will be associated
with more than one lexical entries. So if Lexi is the
number of lexical entries associated with literal li,
then for an input semantics comprising n literals,
the number of sets of lexical constituents covering
the input semantics is:
?i=n
i=1 Lexi
The two sources of complexity interact by mul-
tiplying out so that the potential number of combi-
nations of constituents is:
2n ?
i=n
?
i=1
Lexi
In what follows, we show that TAG naturally
supports various optimisations that have been pro-
posed to reduce the search space.
3 Polarity filtering
To restrict the impact of lexical ambiguity on pars-
ing efficiency, (Per03) introduces a method called
Polarity filtering. This method is based on the ob-
servation that many of the combinations of lexi-
cal entries which cover the input semantics are in
fact syntactically invalid either because a syntactic
requirement is not fulfilled or because a syntactic
resource is not used. Accordingly, polarity based
filtering eliminates such combinations by:
? assigning each lexical entry with a set of po-
larities reflecting its syntactic requirements
and resources,
? computing for each possible combination of
lexical entries the sum of its polarities and
? only allowing surface realisation on combi-
nations which have a net sum of zero (all re-
quirements are satisfied and all resources are
used).
By filtering the initial search space before the
tree combination phase, polarity filtering in effect
reduces the impact of lexical ambiguity i.e. de-
creases
?i=n
i=1 Lexi.
The definitory properties of TAG elementary
trees provide a natural way to assign polarities to
a TAG lexical entries: each elementary tree can be
associated with a polarity +C , where C is the cat-
egory of its root node and each substitution or foot
node in that tree, a polarity ?C is added, where C
is the category of that node.
We implemented polarity filtering in GenI
based on this way of associating lexical entries
with polarities2 . We then measured the impact of
this filtering on the initial search space (the num-
ber of sets of lexical items actually explored by
the realiser), on space (measured by the number
of chart items created) and on time.
Table 1 summarises the impact of polarity fil-
tering on the initial search space3. possible indi-
cates the number of combinations of lexical entries
which cover the input semantics and thus can po-
tentially lead to a valid syntactic tree realising the
input semantics and explored gives the number of
combinations actually explored by the surface re-
aliser after polarity filtering has ruled out combi-
nations which cannot possibly lead to a valid syn-
tactic tree).
As is to be expected, the impact increases with
the number of input literals so that while polarity
filtering divides the initial search space by 35.6 for
an input ranging between 1 and 6 literals, it divides
it by 441.6 for an input size ranging between 14
and 16 literals
literals possible explored (?)
1-6 199.10 5.60 35.6
7-9 6460.88 40.06 161.3
10-13 43028.25 137.06 313.9
14-16 292747.64 662.91 441.6
Figure 1: Polarity filtering and initial space
(Sets of initial trees covering the input semantics)
Table 2 gives the impact of polarity filtering on
space as measured by the number of created chart
items (or constituents). The first column (w/o pol.)
gives the number of created charted items when
polarity filtering is switched off and the second,
(with pol.) when polarity filtering is on. As can
be seen, the effect is particularly pronounced when
the input exceeds 10 literals.
Finally, Figure 3 shows that the overhead intro-
duced by the construction of the polarity automa-
ton means that formulae under 10 literals are re-
alised in roughly the same time with or without po-
larity filtering. However, for larger sentences, po-
larity filtering is increasingly important in keeping
realisation times reasonable. For instance, given
an input ranging between 14 and 16 literals, polar-
2See (GK05) for more details.
3For each group of input (1-6 literals, 7-9, etc.), measures
are based on an average of 15 cases.
98
literals w/o pol. with pol. (?)
1-6 146.40 83.60 1.8
7-9 3273.50 1281.25 2.6
10-13 7468.06 702.50 10.6
14-16 17502.36 1613.91 10.8
Figure 2: With and without Polarity filtering
(Chart items)
ity filtering divides realisation time by 5, that is,
yields a realisation time of 2.21 seconds instead of
11.61.
literals w/o pol. with pol. (?)
1-6 0.81 0.79 1.0
7-9 1.68 1.35 1.2
10-13 3.56 1.88 1.9
14-16 11.61 2.21 5.3
Figure 3: With and without Polarity filtering (CPU
times)
4 Substitution/adjunction distinction
One important specificity of TAG is that it includes
two combination operations namely, adjunction
and substitution. We now show that this feature
of TAG is particularly useful in improving surface
realisation performance.
4.1 Reducing the impact of intersective
modifiers
To restrict the combinatorics induced by modi-
fiers, (CCFP99; CO05) proposes either to han-
dle modifiers after a complete syntactic tree is
built (i.e., after all syntactic requirements are ful-
filled) or before the modifiee is combined with
other items (e.g., before the head noun has com-
bined with a determiner). Although the number of
intermediate structures generated is still 2n for n
modifiers, both strategies have the effect of block-
ing these 2n structures from multiplying out with
other structures in the chart. More precisely, given
an input semantics of size n where k of its liter-
als are to be realised as modifiers, the number of
intermediate structures possible in the two phase
approach is 2k + 2n?k, which can be considerably
smaller than 2n, depending on the size of k.
In TAG, we can make use of the fact that substi-
tution and adjunction apply independently of each
other to implement a two-phase generation strat-
egy where modifiers are handled only after a com-
plete syntactic tree is built. In the first phase,
only substitutions are performed and in the sec-
ond, only adjunctions. Additionally, before ad-
junction starts, all unsaturated trees (trees with
unfilled substitution sites) are discarded from the
chart thereby ensuring that modifiers do not com-
bine with structures that cannot possibly lead to a
valid result (since no constituent could be found to
fill the unsaturated substitution sites).
Since in TAG, modifiers always involve the use
of adjunction, modifiers will always be handled by
the second phase of the algorithm and thereby ad-
joined into ?saturated trees? i.e., trees devoid of
unfilled substitutions sites. In this way, the prolif-
eration of structures induced by the modifiers can
be restricted.
The substitution-before-adjunction strategy was
integrated in GenI yielding the improvements in-
dicated in Figures 4 and 5.
literals 1 phase 2 phase (?)
? 3 0.73 0.73 1.0
4 0.74 0.75 1.0
5 0.97 0.93 1.0
6 2.91 0.89 3.3
7 4.24 1.30 3.3
? 8 Time out
Figure 4: With and without SBA (CPU times)
literals 1 phase 2 phase (?)
? 3 47.00 44.33 1.1
4 107.00 108.00 1.0
5 310.00 263.00 1.2
6 1387.33 883.00 1.6
7 2293.50 761.33 3.0
Figure 5: With and without SBA (Chart items)
As table 4 shows, when there is more than 7 lit-
erals in the input, the one-phase algorithm times
out. More in general, for the data shown, the two
phase strategy leads to an average decrease in time
ranging between 1 and 3.3% and a decrease in
space varying between 1.1% and 3% respectively.
Although the poor performance of the 1 phase
algorithm is in part due to a very large and strongly
overgenerating grammar4 , the data clearly shows
that SBA is essential in supporting large scale TAG
based surface realisation.
4The grammar used is a grammar for French which con-
tains roughly 3 400 initial trees (CD04).
99
4.2 Substitution-before-adjunction combined
with Polarity Filtering
The substitution-before-adjunction strategy limits
the impact of intersective modifiers by restricting
the number of constituents the modifiers can com-
bine with within one set of lexical items. Because
polarity filtering reduces the number of sets of lex-
ical items to be considered, it trivially also reduces
the number of sets of lexical items involving ad-
junctions.
The space improvement provided by combining
the substitution-before-adjunction (SBA) strategy
with polarity filtering is illustrated in Figures 6
and 7 which show the space reduction associated
with cases ordered either according to their num-
ber of literals or according to their number of foot
nodes (i.e., adjunction cases). As should be ex-
pected, the number of foot nodes is more highly
correlated with a space reduction. Specifically,
a combined SBA/polarity strategy divides by 3.4
the space used for cases involving between 1 and
12 auxiliary trees; and by 18.8 the space used for
cases involving between 14 and 16 auxiliary trees.
literals w/o pol. with pol. (?)
1-6 367.90 109.50 3.4
7-9 6192.69 1550.19 4.0
10-13 11211.06 711.06 15.8
14-16 30660.27 1631.64 18.8
Figure 6: SBA + Polarity (Chart items)
# aux trees w/o pol. with pol. (?)
1-12 2124.27 620.82 3.4
13-120 8751.53 1786.47 4.9
121-190 11528.43 611.50 18.9
191-350 25279.75 1085.75 23.3
Figure 7: SBA + Polarity (Chart items)
4.3 Filtering out unusable trees
Another interesting aspect of TAG?s use of two
combination operations and more specifically of
the substitution-before-adjunction strategy is that
it naturally supports the inclusion of a third phase
to filter out unusable trees that is, trees which can
be determined not to be integrable in any valid
derivation. Specifically, this third phase occurs be-
tween substitution and adjunction and filters out:
? all trees with an unfilled substitution site
? all saturated trees whose root node is not la-
belled with an S category
The first filter (elimination of unsaturated trees)
is required, as indicated above, to restrict the im-
pact of intersective modifiers: by discarding them,
we restrict adjunction to saturated trees. The sec-
ond, makes use of the property of auxiliary trees
which insists that root and foot node be labelled
with the same category. Because of this property,
adjunction cannot affect the category of the tree it
adjoins to. In particular, a tree which after all pos-
sible substitutions have been performed, has root
label C with C 6= S can never lead to the creation
by adjunction of a tree with root label S. Hence it
can be discarded (provided of course, the genera-
tor is seeking to build sentences).
Figures 8 and 9 illustrate the impact of this sec-
ond filter (called the Root Node Filter, RNF) on
the chart size when polarity filtering is switched
off. As for SAB, the figures show a higher correla-
tion between the RNF and the number of adjunc-
tion nodes than with the number of literals. In-
triguingly, the impact of the filter is proportionally
higher on sentences with fewer foot nodes. Al-
though this needs to be checked more thoroughly,
the explanation for this could be the following.
The trees removed by the Root Node Filter are sat-
urated tree not rooted in S hence essentially sat-
urated NP trees. Examination of the data reveals
that the number of these trees removed by the RNF
remains almost constant (though this might be an
ad hoc property of the specific testsuite used).
Hence in proportion, the effect of the RNF dimin-
ishes.
Note however that in absolute terms, the num-
ber of trees whose derivation is avoided by the
RNF remains quite high thus contributing to an
overall better performance.
literals w/o RNF with RNF (?)
1-6 367.90 146.40 2.5
7-9 6192.69 3273.50 1.9
10-13 11211.06 7468.06 1.5
14-16 30660.27 17502.36 1.8
Figure 8: Root node filter w/o Pol (Chart Items).
As Figures 10 and 11 show, combining the Root
Node Filter with polarity filtering simply rein-
forces the biases noted above: Root Node Filtering
is proportionally more effective for short input but
can remain useful in absolute terms. A more thor-
100
# aux trees w/o RNF with RNF (?)
1-12 2124.27 527.36 4.0
13-120 8751.53 5570.33 1.6
121-190 11528.43 6490.14 1.8
191-350 25279.75 15469.17 1.6
Figure 9: Root node filter w/o Pol (Chart Items).
ough investigation of the data and further exper-
iments are needed however to determine whether
such behaviour is not tied to some ad hoc property
of our (still too limited) testsuite.
literals w/o RNF with RNF (?)
1-6 109.50 83.60 1.3
7-9 1550.19 1281.25 1.2
10-13 711.06 702.50 1.0
14-16 1631.64 1613.91 1.0
Figure 10: Root node filter + Pol (Chart Items).
# aux trees w/o RNF with RNF (?)
1-12 422 621 1.5
13-120 1627 1786 1.1
121-190 600 612 1.0
191-350 1073 1086 1.0
Figure 11: Root Node Filter + Pol (Chart Items).
5 TAG extended domain of locality
Arguably there are words such as complementiser
that or infinitival to whose semantics is empty.
These words are to surface realisation what gaps
(or empty categories) are to parsing. In a naive ap-
proach, they require that all trees with an empty
semantics be considered as potential constituent
candidate at each combining step. In terms of ef-
ficiency, this roughly means increasing the size of
the input n (just like postulating gaps at all po-
sition in an input string increases the size of that
string).
To avoid this shortcoming, a common practice
(CCFP99) consists in specifying a set of rules
which selects empty semantic items on the basis
of the input literals. However these rules fail to re-
flect the fact that empty semantic items are usually
functional words and hence governed by syntactic
rather than semantic constraints.
By contrast, in a TAG based surface realiser,
TAG elementary trees provide a natural way to
specify the syntactic environment in which empty
semantic items can occur. For instance, comple-
mentiser that occurs with verbs taking a sentential
argument which is generally captured by includ-
ing the complementiser as a co-anchor in the trees
of these verbs.
More in general, the extended domain of local-
ity provided by TAG elementary trees, together
with the possibility of specifying co-anchors
means that empty semantic items can be avoided
altogether. Hence they do not require specific
treatment and have no impact on efficiency.
6 Discussion
We have argued that TAG presents several fea-
tures that makes it particularly amenable to the
development of an optimised surface realiser. We
now summarise these features and briefly compare
TAG with CCG (Combinatory Categorial Gram-
mar) and HPSG (Head Driven Phrase Structure
Grammar) based surface realisation.
6.1 Using tree node types
The different types of tree nodes identified by TAG
can be used to support polarity filtering whereby
substitution nodes can be associated with negative
polarities (requirements) and root nodes with pos-
itive polarities (resources). As our preliminary ex-
periments show, polarity filtering has a significant
impact on the initial search space, on the space
used and on CPU times.
So far, this particular type of global filtering
on the initial search space has been used neither
in the HPSG (CCFP99; CO05) nor in the CCG
(Whi04) approach. Although it could presumably
be adapted to fit these grammars, such an adapta-
tion is in essence less straightforward than in TAG.
In CCG, the several combination rules mean
that a subcategory can function either as a re-
source or as a requirement depending on the rule
that applies. For instance, in the verbal category
(S\NP )/NP , the subcategory S\NP functions
as a resource when NPs are type raised (it satisfies
the requirement of a type raised NP with category
S/(S\NP )). However it will need to be further
decomposed into a resource and a requirement if
they are not. More in general, polarity specifica-
tion in CCG would need to take into account the
several combination rules in addition to the cate-
gory structure. In HPSG, it is the interaction of
lexical categories with lexical and phrasal rules
that will need to be taken into consideration.
101
6.2 Using rule types
The two types of tree combining operations per-
mitted by TAG can be used to structure the sur-
face realisation algorithm. As we?ve shown, per-
forming all substitutions before allowing for ad-
junction greatly reduces the exponential impact of
intersective modifiers. Moreover, combining such
a substitution-before-adjunction strategy with po-
larity filtering further improves performance.
In comparison, the HPSG and the CCG ap-
proach do not support such a natural structuring
of the algorithm and intersective modifiers induce
either a pre- or a post-processing.
In HPSG, intersective modifiers are discarded
during the chart generation phase and adjoined
into the generated structures at a later stage. This
is inelegant in that (i) intersective modifiers are ar-
tificially treated separately and (ii) structures sub-
ject to adjunction have to be non monotonically
recomputed to reflect the impact of the adjunction
in that part of the tree dominating the adjunction.
In CCG, the input logical form is chunked into
subtrees each corresponding to a separate gen-
eration subproblem to be solved independently.
Again the approach is ad hoc in that it does not
rely on a given grammatical or linguistic property.
As a result, e.g., negation needs special treatment
to avoid incompleteness (if the heuristic applies,
negated sentences cannot be generated). Similarly,
it is unclear how long distance dependencies in-
volving modifiers (e.g., Which office did you say
that Peter work in ?) are handled.
6.3 Using TAG extended domain of locality
TAG extended domain of locality means that
empty semantic items need no special treatment.
In contrast, both the HPSG and the CCG approach
resort to ad hoc filtering rules which, based on
a scan of the input semantics, add semantically
empty items to the chart.
7 Further research
Although the results presented give strong evi-
dence for the claim that TAG naturally supports
the development of an optimised surface based re-
aliser, they are based on a limited testsuite and on
a core grammar for French that heavily overgen-
erates. Hence they do not truly reflect the poten-
tial of the proposed optimisations on the perfor-
mance of a large scale surface realiser. Current
work concentrates on remedying these shortcom-
ings. In particular, we are working on develop-
ing a structured test suite which permits a pre-
cise measure of the impact of different factors both
on complexity and on the optimisations used. In
this testsuite for instance, each item is associated
with a series of indicators concerning its potential
complexity: number of literals in the correspond-
ing input semantics, number of trees, number of
nodes, number of substitutions nodes and number
of foot nodes in the corresponding selection of ini-
tial trees.
Further work also includes restricting overgen-
eration and exploring in how far, polarity filtering
can be used to select one among the many para-
phrases
References
C. Brew. Letting the cat out of the bag: Generation
for shake-and-bake MT. In Proceedings of COLING
?92, Nantes, France, 1992.
J. Carroll, A. Copestake, D. Flickinger, and
V. Paznan?ski. An efficient chart generator for
(semi-)lexicalist grammars. In Proceedings of
EWNLG ?99, 1999.
B. Crabbe? and D. Duchier. Metagrammar redux. In
International Workshop on Constraint Solving and
Language Processing - CSLP 2004, Copenhagen,
2004.
J. Carroll and S. Oepen. High efficiency realization for
a wide-coverage unification grammar. In R. Dale
and K-F. Wong, editors, Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing, volume 3651 of Springer Lec-
ture Notes in Artificial Intelligence, pages 165?176,
2005.
C. Gardent and E. Kow. Generating and select-
ing grammatical paraphrases. In Proceedings of
the 10th European Workshop on Natural Language
Generation, Aberdeen, Scotland, 2005.
M. Kay. Chart Generation. In 34th ACL, pages 200?
204, Santa Cruz, California, 1996.
A. Koller and K. Striegnitz. Generation as dependency
parsing. In Proceedings of the 40th ACL, Philadel-
phia, 2002.
G. Perrier. Les grammaires d?interaction, 2003. Ha-
bilitation a` diriger les recherches en informatique,
universite? Nancy 2.
M. White. Reining in CCG chart realization. In INLG,
pages 182?191, 2004.
102
Proceedings of the 12th European Workshop on Natural Language Generation, pages 16?24,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
System Building Cost vs. Output Quality in Data-To-Text Generation
Anja Belz Eric Kow
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Abstract
Data-to-text generation systems tend to
be knowledge-based and manually built,
which limits their reusability and makes
them time and cost-intensive to create
and maintain. Methods for automating
(part of) the system building process ex-
ist, but do such methods risk a loss in
output quality? In this paper, we inves-
tigate the cost/quality trade-off in gen-
eration system building. We compare
four new data-to-text systems which were
created by predominantly automatic tech-
niques against six existing systems for the
same domain which were created by pre-
dominantly manual techniques. We eval-
uate the ten systems using intrinsic au-
tomatic metrics and human quality rat-
ings. We find that increasing the degree to
which system building is automated does
not necessarily result in a reduction in out-
put quality. We find furthermore that stan-
dard automatic evaluation metrics under-
estimate the quality of handcrafted sys-
tems and over-estimate the quality of au-
tomatically created systems.
1 Introduction
Traditional Natural Language Generation (NLG)
systems tend to be handcrafted knowledge-based
systems. Such systems tend to be brittle, expen-
sive to create and hard to adapt to new domains
or applications. Over the last decade or so, in
particular following Knight and Langkilde?s work
on n-gram-based generate-and-select surface real-
isation (Knight and Langkilde, 1998; Langkilde,
2000), NLG researchers have become increasingly
interested in systems that are automatically train-
able from data. Systems that have a trainable com-
ponent tend to be easier to adapt to new domains
and applications, and increased automation is of-
ten taken as self-evidently a good thing. The ques-
tion is, however, whether reduced system building
cost and increased adaptability are achieved at the
price of a reduction in output quality, and if so,
how great the price is. This in turn raises the ques-
tion of how to evaluate output quality so that a po-
tential decrease can be detected and quantified.
In this paper we set about trying to find answers
to these questions. We start, in the following sec-
tion, we briefly describing the SUMTIME corpus
of weather forecasts which we used in our experi-
ments. In the next section (Section 2), we outline
four different approaches to building data-to-text
generation systems which involve different combi-
nations of manual and automatic techniques. Next
(Section 4) we describe ten systems in the four cat-
egories that generate weather forecast texts in the
SUMTIME domain. In Section 5 we describe the
human-assessed and automatically computed eval-
uation methods we used to comparatively evalu-
ate the quality of the outputs of the ten systems.
We then present the evaluation results and discuss
implications of discrepancies we found between
the results of the human and automatic evaluations
(Section 6).
2 Data
The SUMTIME-METEO corpus was created by the
SUMTIME project team in collaboration with WNI
Oceanroutes (Sripada et al, 2002). The corpus
was collected by WNI Oceanroutes from the com-
mercial output of five different (human) forecast-
ers, and each instance in the corpus consists of nu-
merical data files paired with a weather forecast.
The experiments in this paper focussed on the part
of the forecasts that predicts wind characteristics
for the next 15 hours.
Figure 1 shows an example data file and Fig-
ure 2 shows the corresponding wind forecast writ-
ten by one of the meteorologists. In Figure 1, the
16
Oil1/Oil2/Oil3_FIELDS
05-10-00
05/06 SSW 18 22 27 3.0 4.8 SSW 2.59
05/09 S 16 20 25 2.7 4.3 SSW 2.39
05/12 S 14 17 21 2.5 4.0 SSW 2.29
05/15 S 14 17 21 2.3 3.7 SSW 2.28
05/18 SSE 12 15 18 2.4 3.8 SSW 2.38
05/21 SSE 10 12 15 2.4 3.8 SSW 2.48
06/00 VAR 6 7 8 2.4 3.8 SSW 2.48
...
Figure 1: Meteorological data file for 05-10-2000,
a.m. (names of oil fields anonymised).
FORECAST FOR:-
Oil1/Oil2/Oil3 FIELDS
...
2. FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000
=====WARNINGS: RISK THUNDERSTORM. =======
WIND(KTS) CONFIDENCE: HIGH
10M: SSW 16-20 GRADUALLY BACKING SSE THEN
FALLING VARIABLE 04-08 BY LATE EVENING
50M: SSW 20-26 GRADUALLY BACKING SSE THEN
FALLING VARIABLE 08-12 BY LATE EVENING
...
Figure 2: Wind forecast for 05-10-2000, a.m.
(names of oil fields anonymised).
first column is the day/hour time stamp, the second
the wind direction predicted for the corresponding
time period; the third the wind speed at 10m above
the ground; the fourth the gust speed at 10m; and
the fifth the gust speed at 50m. The remaining
columns contain wave data.
We used a version of the corpus reported pre-
viously (Belz, 2008) which contains pairs of wind
statements and the wind data that is actually in-
cluded in the statement, e.g.:
Data: 1 SSW 16 20 - - 0600 2 SSE - - - -
NOTIME 3 VAR 04 08 - - 2400
Text: SSW 16-20 GRADUALLY BACKING SSE THEN
FALLING VARIABLE 4-8 BY LATE EVENING
The input vector represents a sequence of 7-
tuples ?i, d, smin, smax, gmin, gmax, t? where i is
the tuple?s ID, d is the wind direction, smin
and smax are the minimum and maximum wind
speeds, gmin and gmax are the minimum and max-
imum gust speeds, and t is a time stamp (indicat-
ing for what time of the day the data is valid). The
corpus consists of 2,123 instances, corresponding
to a total of 22,985 words.
3 Four Ways to Build an NLG Systems
In this section, we describe four approaches
to building language generators involving differ-
ent combinations of automatic and manual tech-
niques: traditional handcrafted systems (Sec-
tion 3.1); handcrafted but trainable probabilis-
tic context-free grammar (PCFG) generators (Sec-
tion 3.2); partly automatically constructed and
trainable probabilistic synchronous context-free
grammar (PSCFG) generators; and generators au-
tomatically built with phrase-based statistical ma-
chine translation (PBSMT) methods (Section 3.4).
In Section 4 we explain how we used these tech-
niques to build the ten systems in our evaluation.
3.1 Rule-based NLG
Traditional NLG systems are handcrafted as rule-
based deterministic decision-makers that make de-
cisions locally, at each step in the generation pro-
cess. Decisions are encoded as generation rules
with conditions for rule application (often in the
form of if-then rules or rules with parameters to be
matched), usually on the basis of corpus analysis
and expert consultation. Reiter and Dale?s influen-
tial paper (1997) recommended that NLG systems
be built largely ?by careful analysis of the target
text corpus, and by talking to domain experts? (p.
74, and reiterated on pp. 58, 61, 72 and 73).
Handcrafted generation tools have always
formed the mainstay of NLG research, a situation
virtually unchanged by the statistical revolution
that swept through other NLP fields in the 1990s.
Well-known examples include the surface realis-
ers Penman, FUF/SURGE and RealPro, the re-
ferring expression generation components created
by Dale, Reiter, Horacek and van Deemter, and
content-to-text generators built in the PLANDoc
and M-PIRO projects, to name but a very few.
3.2 PCFG generation
Context-free grammars are non-directional, and
can be used for generation as well as for analy-
sis (parsing). One approach that uses CFGs for
generation is Probabilistic Context-free Represen-
tationally Underspecified (pCRU) language gener-
ation (Belz, 2008). As mentioned above, tradi-
tional NLG systems tend to be composed of gen-
eration rules that apply transformations to rep-
resentations. The basic idea in pCRU is that as
long as the generation rules are all of the form
relation(arg1, ...argn) ? relation1(arg1, ...argp) ...
relationm(arg1, ...argq), m ? 1, n, p, q ? 0, then the
set of all generation rules can be seen as defining
a context-free language and a single probabilistic
model can be estimated from raw or annotated text
to guide generation processes.
In this approach, a CFG is created by hand that
encodes the space of all possible generation pro-
17
Input [[1,SSW,16,20,-,-,0600],[2,SSE,-,-,-,-,NOTIME],[3,VAR,04,08,-,-,2400]]
Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING
SUMTIME-Hybrid SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT
PCFG-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING
PCFG-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8
PCFG-viterbi SSW 16-20 BACKING SSE VARIABLE 4-8 LATER
PCFG-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATER
PCFG-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8
PSCFG-semantic SSW 16-20 BACKING SSE THEN FALLING VARIABLE 04-08 BY LATE EVENING
PSCFG-unstructured SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 04-08 BY LATE EVENING
PBSMT-unstructured LESS SSW 16-20 SOON BACKING SSE BY END OF THEN FALLING VARIABLE 04-08 BY LATE EVENING
PBSMT-structured GUSTS SSW 16-20 BY EVENING STEADILY LESS GUSTS GRADUALLY BACKING SSE BY LATE EVENING
MINONE BY MIDDAY THEN AND FALLING UNKNOWN VARIABLE 04-08 LATER GUSTS
Table 1: Example input with corresponding outputs by all systems and from the corpus (for 5 Oct 2000).
cesses from inputs to outputs, and has no decision-
making ability. A probability distribution over this
base CFG is estimated from a corpus, and this is
what enables decisions between alternative gener-
ation rules to be made. The pCRU package permits
this distribution to be used in one of the follow-
ing three modes to drive generation processes: (i)
greedy ? apply only the most likely rule at each
choice point; (ii) Viterbi ? apply all expansion
rules to each nonterminal to create the generation
forest for the input, then do a Viterbi search of the
generation forest; (iii) greedy roulette-wheel ? se-
lect a rule to expand a nonterminal according to
a non-uniform random distribution proportional to
the likelihoods of expansion rules.
In addition there are two baseline modes: (i)
random ? where generation rules are randomly
selected at each choice point; and (ii) n-gram ?
where all alternatives are generated and the most
likely is selected according to an n-gram language
model (as in HALOGEN).
For the simple SUMTIME domain, pCRU gen-
erators trained on raw corpora have been shown
to perform well (Belz, 2008), but for more com-
plex domains it is likely that manually annotated
corpora will be needed for training the CFG base
generator. As this is in addition to the manually
constructed CFG base generator, the manual com-
ponent in PCFG generator building is potentially
substantial.
3.3 PSCFG generation
Synchronous context-free grammars (SCFGs) are
used in machine translation (Chiang, 2006), but
have also been used for simple concept-to-text
generation (Wong and Mooney, 2007). The sim-
plest form of SCFG can be viewed as a pair of CFGs
G1, G2 with paired production rules such that for
each rule in G1 there is a rule in G2 with the same
left-hand side, and the same non-terminals in the
right-hand side. The order of non-terminals on the
RHSs may differ, and each RHS may additionally
contain any terminals in any order. SCFGs can
be trained from aligned corpora to produce proba-
bilistic (or ?weighted?) SCFGs.
An SCFG can equivalently be seen as a single
grammar G encoding a set of pairs of strings. A
probabilistic SCFG is defined by the 6-tuple G =
?N ,Te,Tf , L, S, ??, where N is a finite set of non-
terminals, Te, Tf are finite sets of terminal sym-
bols, L is a set of paired production rules, S is a
start symbol ? N , and ? is a set of parameters that
define a probability distribution of derivations un-
der G. Each rule in L has the form A ? ??;??,
where A ? N , ? ? N ? Te+, ? ? N ? Tf+, and
N ? N .
In MT the two CFGs that make up an SCFG are
used to encode (the structure of) the two languages
which the MT system translates between. Trans-
lation with an SCFG then consists of (i) parsing
the input string with the source language CFG to
produce a derivation tree, and then (ii) generating
along the same derivation tree, but using the target
language CFG to produce the output string.
When using SCFGs for content-to-text genera-
tion one of the paired CFGs encodes the meaning
representation language, and the other the (natu-
ral) language in which text is supposed to be gen-
erated. A generation process then consists in (i)
?parsing? the meaning representation (MR) into its
constituent structure, and, in the opposite direc-
tion, (ii) assembling strings of words correspond-
ing to constituent parts of the input MR into a sen-
tence or text that realises the entire MR.
We used the WASP?1 method (Wong and
Mooney, 2006; Wong and Mooney, 2007) which
18
provides a way in which a probabilistic SCFG can
be constructed for the most part automatically.
The training process requires two resources as in-
put: a CFG of MRs and a set of sentences paired
with their MRs. As output, it produces a proba-
bilistic SCFG. The training process works in two
phases, producing a (non-probabilistic) SCFG in
the ?lexical acquisition phase?, and associating the
rules with probabilities in the ?parameter estima-
tion phase?.
The lexical acquisition phase uses the GIZA++
word-alignment tool, an implementation (Och and
Ney, 2003) of IBM Model 5 (Brown et al, 1993)
to construct an alignment of MRs with NL strings.
An SCFG is then constructed by using the MR CFG
as a skeleton and inferring the NL grammar from
the alignment.
For the parameter estimation phase, WASP?1
uses a log-linear model from Koehn et al (2003)
which defines a conditional probability distribu-
tion over derivations d given an input MR f as
Pr
?
(d|f) ? Pr(e(d))?1
?
d?d
w?(r(d))
where w?(r(d)) is the weight an individual rule
used in a derivation, defined as
w?(A ? ?e, f?) =
P (f |e)?2P (e|f)?3Pw(f |e)?4Pw(e|f)?5exp(?|?|)?6
where P (?|?) and P (?|?) are the relative fre-
quencies of ? and ?, Pw(?|?) and Pw(?|?) are
lexical weights, and exp(?|?|) is a word penalty
to control output sentence length. The model pa-
rameters ?i are trained using minimum error rate
training.
Compared to probabilistic CFGs, WASP?1-
trained probabilistic SCFGs have a much reduced
manual component in system building. In the lat-
ter, the NL grammar for the output language, the
mapping from MRs to word strings and the rule
probabilities are all created automatically, more-
over from raw corpora, whereas in PCFGs, only
the rule probabilities are created automatically.
3.4 SMT methods
A Statistical Machine Translation (SMT) system is
essentially composed of a translation model and
a language model, where the former translates
source language substrings into target language
substrings, and the language model determines
the most likely linearisation of the translated sub-
strings. The currently most popular phrase-based
SMT (PBSMT) approach translates phrases (an ar-
bitrary sequence of words, rather than the lin-
guistic sense), whereas the original ?IBM models?
translated words. Different PBSMT methods differ
in how they construct the phrase translation table.
We used the phrase-based translation model
proposed by Koehn et al (2003) and implemented
in the MOSES toolkit (Koehn et al, 2007) which is
based on the noisy channel model, where Bayes?s
rule is used to reformulate the task of translat-
ing a source language string f into a target lan-
guage string e as finding the sentence e? such that
e? = argmaxe Pr(e) Pr(f |e).
The translation model (which gives Pr(f |e)) is
obtained from a parallel corpus of source and tar-
get language texts, where the first step is automatic
alignment using the GIZA++ word-level aligner.
Word-level alignments are used to obtain phrase
translation pairs using a set of heuristics.
A 3-gram language model (which gives Pr(e))
for the target language is trained either on the
same or a different corpus. For full details refer
to Koehn et al (2003; 2007).
PBSMT offers a completely automatic method
for constructing generators, where all that is re-
quired as input to the system building process is a
corpus of paired MRs and realisations, on the basis
of which the PBSMT approach constructs a map-
ping from MSRs to realisations.
4 Ten Weather Forecast Text Generators
4.1 SUMTIME-Hybrid
We included the original SUMTIME system (Re-
iter et al, 2005) in our evaluations. This
rule-based system has two modules: a content-
determination module and a microplanning and
realisation module. It can be run without the
content-determination module, taking content rep-
resentations (tuple sequence as described in Sec-
tion 2) as inputs, and is then called SUMTIME-
Hybrid. SUMTIME-Hybrid is a traditional deter-
ministic rule-based generation system, and took
about one year to build.1 Table 1 shows an ex-
ample forecast from the SUMTIME system (and
corresponding outputs from the other systems, de-
scribed below).
1Belz (2008), estimated on the basis of personal commu-
nication with E. Reiter and S. Sripada.
19
4.2 PCFG generators
We also included five pCRU generators for
the SUMTIME domain created previously (Belz,
2008). The pCRU base generator for SUMTIME
is a set of generation rules with atomic arguments
that convert an input into a set of NL forecasts.
To create inputs to the pCRU generators, the in-
put vectors as they appear in the corpus (see Sec-
tion 2) are augmented and converted into sequence
of nonterminals: First, information is added to
each of the 7-tuples in an automatic preprocessing
phase encoding whether the change in wind direc-
tion compared to the preceding 7-tuple was clock-
wise or anti-clockwise; whether change in wind
speed was an increase or a decrease; and whether
a 7-tuple was the last in the vector. Then, the aug-
mented tuples are converted into a representation
of nonterminals with 7 arguments.
A probability distribution over the base genera-
tor was obtained by the multi-treebanking method
(Belz, 2008) from the un-annotated SUMTIME
corpus. This method first parses the corpus with
the base CFG and then obtains rule-application fre-
quency counts from the parsed corpus which are
used to obtain a probability distribution by straigh-
forward maximum likelihood estimation. If there
is more than one parse for a sentence then the fre-
quency count increment is equally split over rules
in alternative parses.
4.3 PSCFG generators
We created two probabilistic synchronous CFG
(PSCFG) generators for the SUMTIME domain us-
ing WASP?1. The main task here was to create
a CFG for wind data representations. We used
two different grammars (resulting in two different
generators). The ?unstructured? grammar encodes
raw corpus input vectors augmented as described
in Section 4.2, whereas the ?semantic? grammar
encodes representations with recursive predicate-
argument structure that more resemble semantic
forms. These were produced automatically from
the raw input vectors.
Both the PSCFG-unstructured and the PSCFG-
semantic generators were built in the same way,
by feeding the CFG for wind data representations
and the corpus of paired wind data representations
and forecasts to WASP?1 which then created prob-
abilistic SCFGs from it.
System BLEU Homogeneous subsets
corpus 1.00 A
PCFG-greedy .65 B
PSCFG-sem .637 B
PSCFG-unstr .617 B C
PCFG-viterbi .57 C D
PCFG-2gram .561 D
PCFG-roule .516 D E
PBSMT-unstr .500 E
SUMTIME .437 F
PBSMT-struc .338 G
PCFG-rand .269 H
Table 2: Mean forecast-level BLEU scores and ho-
mogeneous subsets (Tukey HSD, alpha = .05) for
SUMTIME test sets.
4.4 PBSMT generators
We also created two SUMTIME generators with
the MOSES toolkit. The main question here was
how to represent the ?source language? inputs.
While SMT methods are often applied with no lin-
guistic knowledge at all (and are therefore blind as
to whether paired inputs and outputs are NL strings
or something else), it was not clear how well
they would cope with the task of mapping from
number/symbol vectors to NL strings. We tested
two different input representations, one of which
was simply the augmented corpus input vectors
as described above (PBSMT-unstructured), and an-
other in which the individual 7-tuples of which
the vectors are composed are explicitly marked by
predicate-argument structure (PBSMT-structured).
For comparability with Wong & Mooney (2007)
the structure markers were treated as tokens.
We built two different generators by feeding
the two different versions of the paired corpus to
MOSES. We did not use a factored translation
model (the words used in weather forecasts did not
vary sufficiently), or tuning.
5 Evaluation Methods
5.1 Automatic evaluation methods
The two automatic metrics used in the evaluations,
NIST2 and BLEU3, have been shown to correlate
well with expert judgments (Pearson?s r = 0.82
and 0.79 respectively) in the SUMTIME domain
(Belz and Reiter, 2006).
2http://cio.nist.gov/esd/emaildir/
lists/mt_list/bin00000.bin
3ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v11b.pl
20
BLEU-x is an n-gram based string comparison
measure, originally proposed by Papineni et al
(2001) for evaluation of MT systems. It computes
the proportion of word n-grams of length x and
less that a system output shares with several refer-
ence outputs. Setting x = 4 (i.e. considering all n-
grams of length ? 4) is standard. NIST (Dodding-
ton, 2002) is a version of BLEU, but where BLEU
gives equal weight to all n-grams, NIST gives more
importance to less frequent (hence more informa-
tive) n-grams, and the range of NIST scores de-
pends on the size of the test set. Some research has
shown NIST to correlate with human judgments
more highly than BLEU (Doddington, 2002; Rie-
zler and Maxwell, 2005; Belz and Reiter, 2006).
5.2 Human evaluation
We designed an experiment in which participants
were asked to rate forecast texts for Clarity and
Readability on scales of 1?7. Clarity was ex-
plained as indicating how understandable a fore-
cast was, and Readability as indicating how flu-
ent and readable it was. After an introduction and
detailed explanations, participants carried out the
evaluations over the web. They were able to inter-
rupt and resume the evaluation at any time.
We randomly selected 22 forecast dates and
used outputs from all 10 systems for those dates
(as well as the corresponding forecasts in the cor-
pus) in the evaluation, i.e. a total of 242 forecast
texts. We used a repeated Latin squares design
where each combination of forecast date and sys-
tem is assigned two trials. As there were 2 eval-
uation criteria, there were 968 individual ratings
in this experiment. An evaluation session started
with three training examples; the real trials were
then presented in random order.
We recruited 22 participants from among our
university colleagues whose first language was
English and who had no experience of NLP. We
did not try to recruit master mariners as in earlier
experiments reported by Reiter and Belz (2006),
because these experiments also demonstrated that
the correlation between the ratings by such ex-
pert evaluators and lay-people is very strong in the
SUMTIME domain (Pearson?s r = 0.845).
6 Results
For each evaluation method, we carried out a one-
way ANOVA with ?System? as the fixed factor, and
the evaluation measure as the dependent variable.
System NIST Homogeneous subsets
corpus 4.062 A
PCFG-greedy 3.361 B
PSCFG-sem 3.303 B
PSCFG-unstr 3.191 B C
PCFG-roule 3.033 C D
PBSMT-unstr 2.924 D
PCFG-viterbi 2.854 D E
PCFG-2gram 2.854 D E
SUMTIME 2.707 E F
PCFG-rand 2.540 F
PBSMT-struc 2.331 G
Table 3: Mean forecast-level NIST scores and ho-
mogeneous subsets (Tukey HSD, alpha = .05) for
SUMTIME test sets.
In each case we report the main effect of System
on the measure and (if it is significant) we also
report significant differences between pairs of sys-
tems in the form of homogeneous subsets obtained
with a post-hoc Tukey HSD analysis.
Tables 2 and 3 display the results for the BLEU
and NIST evaluations, where scores were cal-
culated on test data sets, using a 5-fold cross-
validation set-up. System names (in abbrevi-
ated form) are shown in the first column, mean
forecast-level scores in the second, and the re-
maining columns indicate significant differences
between systems. The way to read the homoge-
neous subsets is that two systems which do not
have a letter in common are significantly different
with p < .05.
For the BLEU evaluation, the main effect of Sys-
tem on BLEU score was F = 248.274, at p <
.001. PCFG-greedy, PSCFG-semantic and PSCFG-
unstructured come out top, although only the first
two are significantly better than all other systems.
SUMTIME-Hybrid, PBSMT-structured and PCFG-
random bring up the rear, with the remaining sys-
tems distributed over the middle ground. A strik-
ing result is that the handcrafted SUMTIME sys-
tem comes out near the bottom, being signifi-
cantly worse than all other systems except PCFG-
structured and PBSMT-random.
For the NIST evaluation, the main effect of Sys-
tem on BLEU score was F = 108.086, at p <
.001. The systems were ranked in the same way
as in the BLEU evaluation except for the systems
in the D subset. The correlation between the NIST
and BLEU scores is Pearson?s r = .739, p < .001,
Spearman?s ? = .748, p < .001.
21
Scores on data from human evaluation
Clarity Readability NIST BLEU
SUMTIME 6.06 6.18 5.71 0.52
PSCFG-semantic 5.79 5.70 6.76 0.65
corpus 5.79 5.93 8.45 1
PCFG-greedy 5.79 5.63 6.73 0.67
PSCFG-unstruc 5.72 5.84 6.61 0.64
PCFG-roulette 5.29 5.56 6.07 0.52
PCFG-2gram 5.29 5.29 5.23 0.52
PCFG-viterbi 4.90 5.34 5.15 0.51
PCFG-random 4.43 4.52 4.52 0.25
PBSMT-unstruc 3.70 3.93 5.38 0.49
PBSMT-struc 2.79 2.77 4.21 0.33
Table 4: Mean Clarity and Readability ratings
from human evaluation; NIST and BLEU scores
on same 22 forecasts as used in human evaluation.
The main results from the automatic evalua-
tions are that the two PSCFG systems and the PCFG
system with the greedy generation algorithm are
best overall. However, the human evaluations pro-
duced rather different results.
Figure 3 is a series of bar charts representing
the results of the human evaluation for Clarity. For
each system (indicated by the labels on the x-axis),
there are 7 bars, showing how many ratings of 1,
2, 3, 4, 5, 6 and 7 (7 being the best) a system was
given. So the left-most bar for a system shows
how many ratings of 1 a system was given, the
second bar how many ratings of 2, etc. Systems
are shown in descending order of mode (the value
of the most frequently assigned rating, e.g. 7 for
PSCFG-unstructured on the left, and 1 for PBSMT-
structured on the right). The PSCFG-unstructured
and SUMTIME systems come out top in this eval-
uation, with PSCFG-semantic, PCFG-roulette and
PCFG-greedy close behind. Conversely, PBSMT-
structured clearly came out worst, with no ratings
of 7 and a mode of 1 (=completely unclear).
Figure 4 consists of the same kind of bar charts,
for the Readability ratings. Here the SUMTIME
system is the clear winner, with no ratings of 1
and 2 and 22 ratings of 7 (=excellent, all parts
read well). It is closely followed by PSCFG-
unstructured, the corpus forecasts and PSCFG-
semantic. Again, PBSMT-structured is clearly
worst with no ratings of 7, although this time the
mode is 3 (=fairly bad).
We also looked at the means of the ratings, and
these are shown in the second and third columns
of Table 4. The means have to be treated with
some caution, because ratings are ordinal data
and it is not clear how meaningful it is to com-
pute means. However, it is a simple way of ob-
taining a system ranking for comparison with the
two automatic scores (shown in the remaining two
columns of Table 4, for the 22 dates in the human
evaluation only). In terms of means, SUMTIME
comes out top for both Clarity and Readability.
In Clarity, it is followed by the two PSCFG sys-
tems, the corpus files (the only forecasts actually
written by humans), and PCFG-greedy which have
virtually the same means. For Readability, cor-
pus and PSCFG-unstructured are ahead of PSCFG-
semantic and PCFG-greedy (in this order). Bring-
ing up the rear for both Clarity and Readability, as
in the NIST evaluations, is PBSMT-structured, with
PCFG-random and and PBSMT-unstructured faring
somewhat better.
There are some striking differences between
the automatic and human evaluations. For one,
the human evaluators rank the SUMTIME system
very high, whereas both automatic metrics rank
it very low, just above PCFG-random and PBSMT-
structured. Furthermore, the metrics rank PBSMT-
unstructured more highly than the human evalua-
tors, placing it above the SUMTIME system and
in the case of NIST, also above two of the PCFG
systems (Table 3). The human and the automatic
evaluations agree only that the PSCFG systems and
PCFG-greedy are equally good.
7 Conclusions
Reports of research on automating (part of) system
building often take it as read that such automation
is a good thing. The resulting systems are not of-
ten compared to handcrafted alternatives in terms
of output quality or other quality criteria, and little
is therefore known about the loss of system qual-
ity that results from automation. The existence of
several independently developed systems for the
SUMTIME domain of weather forecasts, to which
we have added four new systems in the research
reported in this paper, provides a unique opportu-
nity to examine the system building cost vs. sys-
tem quality trade-off in data-to-text generation.
We investigated 10 systems which fall into four
categories in terms of the manual work involved in
creating them, ranging from completely manual to
completely automatic system building. We found
that increasing the automatic component in system
building from a handcrafted system to an automat-
22
Figure 3: Clarity ratings: Number of times each system was rated 1, 2, 3, 4, 5, 6, and 7 on Clarity.
Systems in descending order of mode (most frequent rating).
ically trained but manually crafted generator led to
a loss of acceptability to human readers, but an im-
provement in terms of n-gram similarity to corpus
texts. Further increasing the automatic component
to the point where only a CFG for meaning repre-
sentations is created manually did not result in a
further reduction in quality in either acceptability
to humans or corpus similarity. However, com-
pletely removing the manual component resulted
in a reduction in quality in both human acceptabil-
ity and corpus similarity (although this is more ap-
parent in the former).
We found striking differences between the re-
sults from tests of human acceptability and mea-
surements of corpus similarity. Compared to the
human ratings, the automatic metrics severely un-
derestimated the quality of the handcrafted SUM-
TIME system, but overestimated the quality of
the automatically constructed SMT systems. This
will not come as a surprise to those familiar with
the machine translation evaluation literature where
this is a major complaint about BLEU (Callison-
Burch et al, 2006). From our results it seems clear
that when the quality of diverse types of systems is
compared, automatic metrics such as BLEU do not
give a complete and reliable picture, and carrying
out additional evaluations is crucial.
Increased reusability and adaptability of sys-
tems and components have cost and time bene-
fits, and methods for automatically training sys-
tems from data offer advantages in both these re-
spects. However, careful evaluation is needed to
ensure that these advantages are not achieved at
the price of a reduction in system quality that ren-
ders systems unacceptable to human users.
Acknowledgments
The research reported in this paper was supported
under EPSRC grant EP/E029116/1 (the Prodigy
Project). We thank the anonymous reviewers for
their helpful comments.
References
A. Belz and E. Reiter. 2006. Comparing automatic
and human evaluation of NLG systems. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL?06), pages 313?320.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
D. Chiang. 2006. An introduction to synchronous
grammars (part of the course materials for the
ACL?06 tutorial on synchronous grammars).
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
23
Figure 4: Readability ratings: Number of times each system was rated 1, 2, 3, 4, 5, 6, and 7 on Readabil-
ity. Systems in descending order of mode (most frequent rating).
occurrence statistics. In Proceedings of the ARPA
Workshop on Human Language Technology.
K. Knight and I. Langkilde. 1998. Generation that
exploits corpus-based statistical knowledge. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL?98), pages 704?710.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of Hu-
man Language Technologies: The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology (HLT-NAACL?03), pages 48?54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics (ACL?07), pages 177?180.
I. Langkilde. 2000. Forest-based statistical sen-
tence generation. In Proceedings of the 6th Applied
Natural Language Processing Conference and the
1st Meeting of the North American Chapter of the
Association of Computational Linguistics (ANLP-
NAACL ?00), pages 170?177.
F. J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: A method for automatic evaluation of ma-
chine translation. IBM research report, IBM Re-
search Division.
E. Reiter and R. Dale. 1997. Building applied natu-
ral language generation systems. Natural Langauge
Engineering, 3(1):57?87.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005.
Choosing words in computer-generated weather
forecasts. Artificial Intelligence, 167:137?169.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proceedings of the ACL?05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, pages 57?64.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2002.
SUMTIME-METEO: A parallel corpus of naturally
occurring forecast texts and weather data. Technical
Report AUCS/TR0201, Computing Science Depart-
ment, University of Aberdeen.
Y. W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL?06), pages 439?446.
Y. W. Wong and R.J. Mooney. 2007. Generation
by inverting a semantic parser that uses statisti-
cal machine translation. In Proceedings of Human
Language Technologies: The Annual Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (HLT-NAACL?07), pages 172?179.
24
Proceedings of the 12th European Workshop on Natural Language Generation, pages 174?182,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
The TUNA-REG Challenge 2009: Overview and Evaluation Results
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Anja Belz Eric Kow
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Abstract
The TUNA-REG?09 Challenge was one
of the shared-task evaluation competitions
at Generation Challenges 2009. TUNA-
REG?09 used data from the TUNA Cor-
pus of paired representations of enti-
ties and human-authored referring expres-
sions. The shared task was to create sys-
tems that generate referring expressions
for entities given representations of sets
of entities and their properties. Four
teams submitted six systems to TUNA-
REG?09. We evaluated the six systems and
two sets of human-authored referring ex-
pressions using several automatic intrinsic
measures, a human-assessed intrinsic eval-
uation and a human task performance ex-
periment. This report describes the TUNA-
REG task and the evaluation methods used,
and presents the evaluation results.
1 Introduction
This year?s run of the TUNA-REG Shared-Task
Evaluation Competition (STEC) is the third, and
final, competition to involve the TUNA Corpus of
referring expressions. The TUNA Corpus was first
used in the Pilot Attribute Selection for Gener-
ating Referring Expressions (ASGRE) Challenge
(Belz and Gatt, 2007) which took place between
May and September 2007; and again for three of
the shared tasks in Referring Expression Genera-
tion (REG) Challenges 2008, which ran between
September 2007 and May 2008 (Gatt et al, 2008).
This year?s TUNA Task replicates one of the three
tasks from REG?08, the TUNA-REG Task. It uses
the same test data, to enable direct comparison
against the 2008 results. Four participating teams
submitted 6 different systems this year; teams and
their affiliations are shown in Table 1.
Team ID Affiliation
GRAPH Macquarie, Tilburg and Twente Universities
IS ICSI, University of California
NIL-UCM Universidad Complutense de Madrid
USP University of Sa?o Paolo
Table 1: TUNA-REG?09 Participants.
2 Data
Each file in the TUNA corpus1 consists of a sin-
gle pairing of a domain (a representation of 7 en-
tities and their attributes) and a human-authored
description for one of the entities (the target refer-
ent). Some domains represent sets of people, some
represent items of furniture (see also Table 2). The
descriptions were collected in an online elicita-
tion experiment which was advertised mainly on
a website hosted at the University of Zurich Web
Experimentation List2 (a web service for recruit-
ing subjects for experiments), and in which partic-
ipation was not controlled or monitored. In the
experiment, participants were shown pictures of
the entities in the given domain and were asked to
type a description of the target referent (which was
highlighted in the visual display). The main condi-
tion3 manipulated in the experiment was +/?LOC:
in the +LOC condition, participants were told that
they could refer to entities using any of their prop-
erties (including their location on the screen). In
the ?LOC condition, they were discouraged from
doing so, though not prevented.
The XML format we have been using in the
TUNA-REG STECs, shown in Figure 1, is a vari-
ant of the original format of the TUNA corpus.
The root TRIAL node has a unique ID and an
indication of the +/ ? LOC experimental condi-
1http://www.csd.abdn.ac.uk/research/tuna/
2http://genpsylab-wexlist.unizh.ch
3The elicitation experiment had an additional independent
variable, manipulating whether descriptions were elicited in a
?fault-critical? or ?non-fault-critical? condition. For the shared
tasks this was ignored by collapsing all the data in these two
conditions.
174
tion. The DOMAIN node contains 7 ENTITY nodes,
which themselves contain a number of ATTRIBUTE
nodes defining the possible properties of an en-
tity in attribute-value notation. The attributes in-
clude properties such as an object?s colour or
a person?s clothing, and the location of the im-
age in the visual display which the DOMAIN rep-
resents. Each ENTITY node indicates whether it
is the target referent or one of the six distrac-
tors, and also has a pointer to the image that it
represents. The WORD-STRING is the actual de-
scription typed by one of the human authors, the
ANNOTATED-WORD-STRING is the description with
substrings annotated with the attributes they re-
alise, while the ATTRIBUTE-SET contains the set of
attributes only. The ANNOTATED-WORD-STRING and
ATTRIBUTE-SET nodes were provided in the train-
ing and development data only, to show how sub-
strings of a human-authored description mapped
to attributes.
<TRIAL CONDITION="+/-LOC" ID="...">
<DOMAIN>
<ENTITY ID="..." TYPE="target" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
<ENTITY ID="..." TYPE="distractor" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
...
</DOMAIN>
<WORD-STRING>
string describing the target referent
</WORD-STRING>
<ANNOTATED-WORD-STRING>
string in WORD-STRING annotated
with attributes in ATTRIBUTE-SET
</ANNOTATED-WORD-STRING>
<ATTRIBUTE-SET>
set of domain attributes in the description
</ATTRIBUTE-SET>
</TRIAL>
Figure 1: XML format of corpus items.
Apart from differences in the XML format, the
data used in the TUNA-REG Task also differs from
the original TUNA corpus in that it has only the sin-
gular referring expressions from the original cor-
pus, and in that we have added to it the files of
images of entities that the XML mark-up points to.
The test set, which was constructed for the
2008 run of the TUNA-REG Task, consists of 112
items, each with a different domain paired with
two human-authored descriptions. The items are
distributed equally between furniture items and
people, and between both experimental conditions
(+/ ? LOC). In the following sections, the two
sets of human descriptions will be referred to as
HUMAN-1 and HUMAN-2.4 The numbers of files
in the training, development and test sets, as well
as in the people and furniture subdomains, are
shown in Table 2.
Furniture People All
Training 319 274 593
Development 80 68 148
Test 56 56 112
All 455 398 853
Table 2: TUNA-REG data: subset sizes.
3 The TUNA-REG Task
Referring Expression Generation (REG) has been
the subject of intensive research in the NLG com-
munity, giving rise to substantial consensus on the
problem definition, as well as the nature of the in-
puts and outputs of REG algorithms. Typically,
such algorithms take as input a domain, consist-
ing of entities and their attributes, together with an
indication of which is the intended referent, and
output a set of attributes true of the referent which
distinguish it from other entities in the domain.
The TUNA-REG task adds an additional stage (re-
alisation) in which selected attributes are mapped
to a natural language expression (usually a noun
phrase). Realisation has received far less attention
among REG researchers than attribute selection.
The TUNA-REG task is an ?end-to-end? refer-
ring expression generation task, in the sense that
it takes as input a representation of a set of enti-
ties and their properties, and outputs a word string
which describes the target entity. Participating
systems were not constrained to have attribute se-
lection as a separate module from realisation.
In terms of the XML format, the items in
the test set distributed to participants consisted
of a DOMAIN node and ATTRIBUTE-SET, and par-
ticipating systems had to generate appropriate
WORD-STRINGs.
As with previous STECs involving the TUNA
data, we deliberately refrained from including in
the task definition any aim that would imply as-
sumptions about quality (as would be the case if
we had asked participants to aim to produce, say,
minimal or uniquely distinguishing referring ex-
pressions), and instead we simply listed the evalu-
ation criteria that were going to be used (described
in Section 5).
4Descriptions in each set are not all by the same author.
175
Evaluation criterion Type of evaluation Evaluation technique
Humanlikeness Intrinsic/automatic Accuracy, String-edit distance, BLEU-3, NIST
Adequacy/clarity Intrinsic/human Judgment of adequacy as rated by native speakers
Fluency Intrinsic/human Judgment of fluency as rated by native speakers
Referential clarity Extrinsic/human Speed and accuracy in identification experiment
Table 3: Overview of evaluation methods.
4 Participating Teams and Systems
This section briefly describes this year?s submis-
sions. Full descriptions of participating systems
can be found in the participants? reports included
in this volume.
IS: The submission of the IS team, IS-FP-GT, is
based on the idea that different writers use differ-
ent styles of referring expressions, and that, there-
fore, knowing the identity of the writer helps gen-
erate REs similar to those in the corpus. The
attribute-selection algorithm is an extended full-
brevity algorithm which uses a nearest neighbour
technique to select the attribute set (AS) most sim-
ilar to a given writer?s previous ASs, or, in a case
where no ASs by the given writer have previously
been seen, to select the AS that has the highest de-
gree of similarity with all previously seen ASs by
any writer. If multiple ASs remain, the algorithm
first selects the shortest, then the most represen-
tative of the remaining REs, then the AS with the
highest-frequency attributes. Individualised statis-
tical models are used to convert the selected AS
into a surface-syntactic dependency tree which is
then converted to a word stirng with an existing
realiser.
GRAPH: The GRAPH team reused their existing
graph-based attribute selection component, which
represents a domain as a weighted graph, and uses
a cost function for attributes. The team devel-
oped a new realiser which uses a set of templates
derived from the descriptions in the TUNA cor-
pus. In order to build templates, certain subsets
of attributes were grouped together, individual at-
tributes were replaced by their type, and a pre-
ferred order for attributes was determined based
on frequencies of orderings. During realisation,
if a matching template exists, types are replaced
with the most frequent word string for each given
attribute; if no match exists, realisation is done by
a simple rule-based method.
NIL-UCM: The three systems submitted by this
group use a standard evolutionary algorithm for
attribute selection where genotypes consist of
binary-valued genes each representing the pres-
ence or absence of a given attribute. Realisation
is done with a case-based reasoning (CBR) method
which retrieves the most similar previously seen
ASs for an input AS, in order of their similarity
to the input. (Sub)strings are then copied from
the preferred retrieved case to create the output
word string. One system, NIL-UCM-EvoCBR uses
both components as described above. The other
two systems, NIL-UCM-ValuesCBR and NIL-UCM-
EvoTAP, replace one of the components with the
team?s corresponding component from REG?08.
USP: The system submitted by this group, USP-
EACH, is a frequency-based greedy attribute se-
lection strategy which takes into account the +/?
LOC attribute in the TUNA data. Realisation was
done using the surface realiser supplied to partici-
pants in the ASGRE?07 Challenge.
5 Evaluation Methods and Results
We used a range of different evaluation methods,
including intrinsic and extrinsic,5 automatically
computed and human-evaluated, as shown in the
overview in Table 3. Participants computed auto-
matic intrinsic evaluation scores on the develop-
ment set (using the teval program provided by
us). We performed all of the evaluations shown
in Table 3 on the test data set. For all measures,
results were computed both (a) overall, using the
entire test data set, and (b) by entity type, that is,
computing separate values for outputs in the furni-
ture and in the people domain. Evaluation meth-
ods for each evaluation type and corresponding
evaluation results are presented in the following
three sections.
5.1 Automatic intrinsic evaluations
Humanlikeness, by which we mean the similar-
ity of system outputs to sets of human-produced
reference ?outputs?, was assessed using Accuracy,
5Intrinsic evaluations assess properties of peer systems in
their own right, whereas extrinsic evaluations assess the effect
of a peer system on something that is external to it, such as its
effect on human performance at some given task or the added
value it brings to an application.
176
All development data People Furniture
Accuracy SE BLEU Accuracy SE BLEU Accuracy SE BLEU
IS-FP-GT 9.71% 4.313 0.297 4.41% 4.764 0.2263 15% 3.863 0.3684
GRAPH ? 5.03 0.30 ? 5.15 0.33 ? 4.94 0.27
NIL-UCM-EvoTAP 6% 5.41 0.20 3% 6.04 0.15 8% 4.87 0.24
NIL-UCM-ValuesCBR 1% 5.86 0.19 1% 5.80 0.17 1% 5.91 0.20
USP-EACH ? 6.03 0.19 ? 7.50 0.04 ? 4.78 0.31
NIL-UCM-EvoCBR 3% 6.31 0.17 1% 6.94 0.16 4% 5.77 0.18
Table 4: Participating teams? self-reported automatic intrinsic scores on development data set with single
human-authored reference description (listed in order of overall mean SE score).
All test data People Furniture
Acc SE BLEU NIST Acc SE BLEU NIST Acc SE BLEU NIST
GRAPH 12.50 6.41 0.47 2.57 8.93 7.04 0.43 2.16 16.07 5.79 0.51 2.26
IS-FP-GT 3.57 6.74 0.28 0.75 3.57 7.04 0.37 0.94 3.57 6.45 0.13 0.36
NIL-UCM-EvoTAP 6.25 7.28 0.26 0.90 3.57 8.07 0.20 0.45 8.93 6.48 0.34 1.22
USP-EACH 7.14 7.59 0.27 1.33 0.00 9.04 0.11 0.46 14.29 6.14 0.41 2.28
NIL-UCM-ValuesCBR 2.68 7.71 0.27 1.69 3.57 8.07 0.23 0.94 1.79 7.34 0.28 1.99
NIL-UCM-EvoCBR 2.68 8.02 0.26 1.97 0.00 9.07 0.19 1.65 5.36 6.96 0.35 1.69
HUMAN-2 2.68 9.68 0.12 1.78 3.57 10.64 0.12 1.50 1.79 8.71 0.13 1.57
HUMAN-1 2.68 9.68 0.12 1.68 3.57 10.64 0.12 1.41 1.79 8.71 0.12 1.49
Table 5: Automatic intrinsic scores on test data set with two human-authored reference descriptions
(listed in order of overall mean SE score).
string-edit distance, BLEU-3 and NIST-5. Accu-
racy measures the percentage of cases where a
system?s output word string was identical to the
corresponding description in the corpus. String-
edit distance (SE) is the classic Levenshtein dis-
tance measure and computes the minimal number
of insertions, deletions and substitutions required
to transform one string into another. We set the
cost for insertions and deletions to 1, and that for
substitutions to 2. If two strings are identical, then
this metric returns 0 (perfect match). Otherwise
the value depends on the length of the two strings
(the maximum value is the sum of the lengths). As
an aggregate measure, we compute the mean of
pairwise SE scores.
BLEU-x is an n-gram based string comparison
measure, originally proposed by Papineni et al
(2001; 2002) for evaluation of Machine Transla-
tion systems. It computes the proportion of word
n-grams of length x and less that a system out-
put shares with several reference outputs. Setting
x = 4 (i.e. considering all n-grams of length ? 4)
is standard, but because many of the TUNA de-
scriptions are shorter than 4 tokens, we compute
BLEU-3 instead. BLEU ranges from 0 to 1.
NIST is a version of BLEU, but where BLEU
gives equal weight to all n-grams, NIST gives more
importance to less frequent n-grams, which are
taken to be more informative. The maximum NIST
score depends on the size of the test set.
Unlike string-edit distance, BLEU and NIST are
by definition aggregate measures (i.e. a single
score is obtained for a peer system based on the
entire set of items to be compared, and this is not
generally equal to the average of scores for indi-
vidual items).
Because the test data has two human-authored
reference descriptions per domain, the Accuracy
and SE scores had to be computed slightly differ-
ently to obtain test data scores (whereas BLEU and
NIST are designed for multiple reference texts).
For the test data only, therefore, Accuracy ex-
presses the percentage of a system?s outputs that
match at least one of the reference outputs, and SE
is the average of the two pairwise scores against
the reference outputs.
Results: Table 4 is an overview of the self-
reported scores on the development set included in
the participants? reports (not all participants report
Accuracy scores). The corresponding scores for
the test data set as well as NIST scores for the test
data (all computed by us), are shown in Table 5.
The table also includes the result of comparing
the two sets of human descriptions, HUMAN-1 and
HUMAN-2, to each other using the same metrics
(their scores are distinct only for non-commutative
measures, i.e. NIST and BLEU).
We ran6 a one-way ANOVA for the SE scores.
6We used SPSS for all statistical analyses and tests.
177
There was a main effect of SYSTEM on SE (F =
10.938, p < .001). A post-hoc Tukey HSD test
with ? = .05 revealed a number of significant dif-
ferences: all systems were significantly better than
the human-authored descriptions, and GRAPH was
furthermore significantly better than NIL-UCM-
EvoCBR.
We also computed the Kruskal-Wallis H value
for the systems? individual Accuracy scores, using
a chi square test to establish significance. By this
test, the observed aggregate difference among the
seven systems is significant at the .01 level (?27 =
20.169).
5.2 Human intrinsic evaluation
The TUNA?09 Challenge was the first TUNA
shared-task competition to include an intrinsic
evaluation involving human judgments of quality.
Design: The intrinsic human evaluation in-
volved descriptions for all 112 test data items from
all six submitted systems, as well as from the two
sets of human-authored descriptions.7 Thus, each
of the 112 test set items was associated with 8
different descriptions. We used a Repeated Latin
Squares design which ensures that each subject
sees descriptions from each system and for each
domain the same number of times. There were
fourteen 8 ? 8 squares, and a total of 896 indi-
vidual judgments in this evaluation, each system
receiving 112 judgments (14 from each subject).
Procedure: In each of the 112 trials, par-
ticipants were shown a system output (i.e. a
WORD-STRING), together with its corresponding
domain, displayed as the set of corresponding im-
ages on the screen.8 The intended (target) referent
was highlighted by a red frame surrounding it on
the screen. They were asked to give two ratings
in answer to the following questions (the first for
Adequacy, the second for Fluency):
1. How clear is this description? Try to imagine
someone who could see the same grid with
the same pictures, but didn?t know which of
the pictures was the target. How easily would
they be able to find it, based on the phrase
given?
7Note that we refer to all outputs, whether human or
system-generated, as system outputs in what follows.
8The on-screen display of images was very similar, al-
though not identical, to that in the original TUNA elicitation
experiments.
2. How fluent is this description? Here your
task is to judge how well the phrase reads.
Is it good, clear English?
We did not use a rating scale (where integers
correspond to different assessments of quality),
because it is not generally considered appropriate
to apply parametric methods of analysis to ordinal
data. Instead, we asked subjects to give their judg-
ments for Adequacy and Fluency for each item by
manipulating a slider like this:
The slider pointer was placed in the center at the
beginning of each trial, as shown above. The posi-
tion of the slider selected by the subject mapped to
an integer value between 1 and 100. However, the
scale was not visible to participants, whose task
was to move the pointer to the left or right. The
further to the right, the more positive the judgment
(and the higher the value returned); the further to
the left, the more negative.
Following instructions, subjects did two prac-
tice examples, followed by the 112 test items in
random order. Subjects carried out the experi-
ment over the internet, at a time and place of their
choosing, and were allowed to interrupt and re-
sume the experiment. According to self-reported
timings, subjects took between 25 and 60 minutes
to complete the experiment (not counting breaks).
Participants: We recruited eight native speak-
ers of English from among post-graduate students
currently doing a Masters degree in a linguistics-
related subject.9
We recorded subjects? gender, level of educa-
tion, field of study, proficiency in English, vari-
ety of English and colour vision. Since all sub-
jects were native English speakers, had normal
colour vision, and had comparable levels of ed-
ucation and academic backgrounds, as indicated
above, these variables are not included in the anal-
yses reported below.
Results: Table 6 displays the mean Fluency and
Adequacy judgments obtained by each system.
We conducted two separate 8 (SYSTEM) ? 2 (DO-
MAIN) Univariate Analyses of Variance (ANOVAs)
on Adequacy and Fluency, where DOMAIN ranges
9MA Linguistics and MRes Speech, Language and Cog-
nition at UCL; MA Applied Linguistics and MRes Psychol-
ogy at Sussex; and MA Media-assisted Language Teaching
at Brighton.
178
All test data People Furniture
Adequacy Fluency Adequacy Fluency Adequacy Fluency
Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD
GRAPH 84.11 21.07 85.81 17.52 85.30 18.10 87.70 14.42 82.91 23.78 83.93 20.11
USP-EACH 77.72 28.33 84.20 20.27 81.04 26.48 81.82 24.47 74.41 29.93 86.57 14.79
NIL-UCM-EvoTAP 76.16 28.34 61.95 26.13 78.66 27.48 59.13 29.78 73.66 29.22 64.77 21.79
HUMAN-2 74.63 34.77 73.38 27.63 80.93 31.83 73.16 30.88 68.34 36.68 73.59 24.23
NIL-UCM-ValuesCBR 72.34 33.93 59.41 33.94 68.18 37.37 46.23 34.92 76.50 29.86 72.59 27.43
HUMAN-1 70.38 34.92 71.52 30.79 83.39 24.27 72.39 28.55 57.36 39.08 70.64 33.13
NIL-UCM-EvoCBR 63.65 37.19 55.38 35.32 56.61 40.20 41.45 37.38 70.70 32.76 69.30 26.93
IS-FP-GT 59.46 40.94 66.21 30.97 88.79 19.26 65.27 32.22 30.14 35.51 67.16 29.94
Table 6: Human-assessed intrinsic scores on test data set, including the two sets of human-authored
reference descriptions (listed in order of overall mean Adequacy score).
Adequacy Fluency
GRAPH A GRAPH A
USP-EACH A B USP-EACH A B
NIL-UCM-EvoTAP A B HUMAN-2 B C
HUMAN-2 A B C HUMAN-1 C D
NIL-UCM-ValuesCBR A B C IS-FP-GT C D E
HUMAN-1 B C D NIL-UCM-EvoTAP D E
NIL-UCM-EvoCBR C D NIL-UCM-ValuesCBR E
IS-FP-GT D NIL-UCM-EvoCBR E
Table 7: Homogeneous subsets for Adequacy and Fluency. Systems which do not share a letter are
significantly different at ? = .05.
over People and Furniture Items. On Adequacy,
there were main effects of SYSTEM (F (7, 880) =
7.291, p < .001) and DOMAIN (F (1, 880) =
29.133, p < .001), with a significant interac-
tion between the two (F (7, 880) = 15.30, p <
.001). On Fluency, there were main effects of
SYSTEM (F (7, 880) = 18.14) and of DOMAIN
(F (7, 880) = 17.20), again with a significant
SYSTEM ? DOMAIN interaction (F (7, 880) =
5.60), all significant at p < .001. Post-hoc Tukey
comparisons on both dependent measures yielded
the homogeneous subsets displayed in Table 7.
5.3 Extrinsic task-performance evaluation
As for earlier shared tasks involving the TUNA
data, we carried out a task-performance experi-
ment in which subjects have the task of identifying
intended referents.
Design: The extrinsic human evaluation in-
volved descriptions for all 112 test data items from
all six submitted systems, as well as from the two
sets of human-authored descriptions. We used a
Repeated Latin Squares design with fourteen 8?8
squares, so again there were a total of 896 individ-
ual judgments and each system received 112 judg-
ments, however this time it was 7 from each sub-
ject, as there were 16 participants; so half the par-
ticipants did the first 56 items (the first 7 squares),
and the other half the second 56 (the remaining 7
squares).
Procedure: In each of their 5 practice trials and
56 real trials, participants were shown a system
output (i.e. a WORD-STRING), together with its cor-
responding domain, displayed as the set of corre-
sponding images on the screen. In this experiment
the intended referent was not highlighted in the on-
screen display, and the participants? task was to
identify the intended referent among the pictures
by mouse-clicking on it.10
In previous TUNA identification experiments
(Belz and Gatt, 2007; Gatt et al, 2008), sub-
jects had to read the description before identify-
ing the intended referent. In ASGRE?07 both de-
scription and pictures were displayed at the same
time, yielding a single time measure that com-
bined reading and identification times. In REG?08,
subjects first read the description and then called
up the pictures on the screen when they had fin-
ished reading the description, which yielded sepa-
rate reading and identification times.
10Due to limitations related to the stimulus presentation
software, the images in this experiment were displayed in
strict rows and columns, whereas the display grid in the web-
based TUNA elicitation experiment and the intrinsic human
evalution experiment were slightly distorted. This may have
affected timings in those (very rare) cases where a description
explicitly referenced the column a target referent was located
in, as in the chair in column 1.
179
This year we tried out a version of the experi-
ment where subjects listened to descriptions read
out by a synthetic voice11 over headphones while
looking at the pictures displayed on the screen.
Stimulus presentation was carried out using
DMDX, a Win-32 software package for psycholin-
guistic experiments involving time measurements
(Forster and Forster, 2003). Participants initiated
each trial, which consisted of an initial warning
bell and a fixation point flashed on the screen for
1000ms. Following this, the visual domain was
displayed, and the voice reading the description
was initiated after a delay of 500ms. We recorded
time in milliseconds from the start of display to the
mouse-click whereby a participant identified the
target referent. This is hereafter referred to as the
identification speed. The analysis reported below
also uses identification accuracy, the percentage
of correctly identified target referents, as an addi-
tional dependent variable. Trials timed out after
15, 000ms.
Participants: The experiment was carried out
by 16 participants recruited from among the fac-
ulty and administrative staff of the University of
Brighton. All participants carried out the experi-
ment under supervision in the same quiet room on
the same laptop, in the same ambient conditions,
with no interruptions. All participants were native
speakers, and we recorded type of post, whether
they had normal colour vision and hearing, and
whether they were left or right-handed.
Timeouts and outliers: None of the trials
reached time-out stage during the experiment.
Outliers were defined as those identification times
which fell outside the mean ?2SD (standard de-
viation) range. 44 data points (4.9%) out of a to-
tal of 896 were identified as outliers by this defi-
nition; these were replaced with the series mean
(Ratliff, 1993). The results reported for identi-
fication speed below are based on these adjusted
times.
Results: Table 8 displays mean identification
speed and identification accuracy per system. A
univariate ANOVA on identification speed revealed
significant main effects of SYSTEM (F (7, 880) =
4.04, p < .001) and DOMAIN (F (1, 880) =
11We used the University of Edinburgh?s Festival speech
generation system (Black et al, 1999) in combination
with the nitech us slt arctic hts voice, a high-quality female
American voice.
USP-EACH A
GRAPH A
NIL-UCM-EvoTAP A B
IS-FP-GT A B
NIL-UCM-ValuesCBR A B
NIL-UCM-EvoCBR A B
HUMAN-2 B
HUMAN-1 B
Table 9: Homogeneous subsets for Identification
Speed. Systems which do not share a letter are
significantly different at ? = .05.
11.53, p < .001), with a significant interaction
(F (7, 880) = 6.02, p < .001). Table 9 displays
homogeneous subsets obtained following pairwise
comparisons using a post-hoc Tukey HSD analysis.
We treated identification accuracy as an indica-
tor variable (indicating whether a participant cor-
rectly identified a target referent or not in a given
trial). A Kruskal-Wallis test showed a significant
difference between systems (?27 = 44.98; p <
.001).
5.4 Correlations
Table 10 displays the correlations between the
eight evaluation measures we used. The num-
bers are Pearson product-moment correlation co-
efficients, calculated on the means (1 mean per
system on each measure).
As regards the human-assessed intrinsic scores,
there is no significant correlation between Ad-
equacy and Fluency. Among the automatically
computed intrinsic measures, the only significant
correlation is between Accuracy and BLEU. For
the extrinsic identification performance measures,
there is no significant correlation between Identi-
fication Accuracy and Identification Speed.
As for correlations across the two types
(human-assessed and automatically computed) of
intrinsic measures, the only significant correla-
tions are between Fluency and Accuracy, and be-
tween Adequacy and Accuracy. So, a system
with a higher percentage of human-like outputs
(as measured by Accurach) also tends to be scored
more highly in terms of Fluency and Adequacy by
humans.
We also found significant correlations between
intrinsic and extrinsic measures: there was a
strong and significant correlation between Iden-
tification Accuracy and Adequacy, implying that
more adequate system outputs allowed people to
identify target referents more correctly; there was
also a significant (negative) correlation between
180
All test data People Furniture
ID acc. ID. speed ID acc. ID. speed ID acc. ID. speed
% Mean SD % Mean SD % Mean SD
GRAPH 0.96 3069.16 878.89 0.95 3081.01 767.62 0.96 3057.31 984.60
HUMAN-1 0.91 3517.58 1028.83 0.95 3323.76 764.59 0.88 3711.41 1214.55
USP-EACH 0.90 3067.16 821.00 0.86 3262.79 865.61 0.95 2871.53 730.15
NIL-UCM-EvoTAP 0.88 3159.41 910.65 0.88 3375.17 948.46 0.89 2943.65 824.17
NIL-UCM-ValuesCBR 0.87 3262.53 974.55 0.80 3447.50 1003.21 0.93 3077.56 916.87
HUMAN-2 0.83 3463.88 1001.29 0.89 3647.41 1045.95 0.77 3280.35 927.79
NIL-UCM-EvoCBR 0.81 3362.22 892.45 0.75 3779.64 831.91 0.88 2944.80 748.69
IS-FP-GT 0.68 3167.11 964.45 0.89 2980.30 750.78 0.46 3353.91 1114.68
Table 8: Identification speed and accuracy per system. Systems are displayed in descending order of
overall identification accuracy.
Human-assessed, intrinsic Extrinsic Auto-assessed, intrinsic
Fluency Adequacy ID Acc. ID Speed Acc. SE BLEU NIST
Fluency 1 0.68 0.50 -0.89* .85* -0.57 0.66 0.30
Adequacy 0.68 1 0.95** -0.65 .83* -0.29 0.60 0.48
Identification Accuracy 0.50 0.95** 1 -0.39 0.68 -0.01 0.49 0.60
Identification Speed 0.89* -0.65 -0.39 1 -0.79 0.68 -0.51 0.06
Accuracy 0.85* 0.83* 0.68 -0.79 1.00 -0.68 .859* 0.49
SE -0.57 -0.29 -0.01 0.68 -0.68 1 -0.75 -0.07
BLEU 0.66 0.60 0.49 -0.51 .86* -0.75 1 0.71
NIST 0.30 0.48 0.60 0.06 0.49 -0.07 0.71 1
Table 10: Correlations (Pearson?s r) between all evaluation measures. (?significant at p ? .05;
??significant at p ? .01)
Fluency and Identification Speed, implying that
more fluent descriptions led to faster identifica-
tion. While these results differ from previous find-
ings (Belz and Gatt, 2008), in which no significant
correlations were found between extrinsic mea-
sures and automatic intrinsic metrics, it is worth
noting that significance in the results reported here
was only observed between human-assessed in-
trinsic measures and the extrinsic ones.
6 Concluding Remarks
The three editions of the TUNA STEC have at-
tracted a substantial amount of interest. In addi-
tion to a sizeable body of new work on referring
expression generation, as another tangible out-
come of these STECs we now have a wide range
of different sets of system outputs for the same set
of inputs. A particularly valuable resource is the
pairing of these outputs from the submitted sys-
tems in each edition with evaluation data.
As this was the last time we are running a STEC
with the TUNA data, we will now make all data
sets, documentation and evaluation software from
all TUNA STECs available to researchers. We are
planning to add to these as many system outputs
as we can, so that other researchers can perform
evaluations involving these.
We are also planning to complete our evalua-
tions of the evaluation methods we have devel-
oped. Among such experiments will be direct
comparisons between the results of the three vari-
ants of the identification experiment we have tried
out, and a direct comparison between different
designs for human-assessed intrinsic evaluations
(e.g. comparing the slider design reported here to
preference judgments and rating scales).
Apart from the technological progress in REG
which we hope the TUNA STECs have helped
achieve, perhaps the single most important scien-
tific result is strong evidence for the importance
of extrinsic evaluations, as these do not necessar-
ily agree with the results of much more commonly
used intrinsic types of evaluations.
Acknowledgments
We thank our colleagues at the University of
Brighton who participated in the identification ex-
periment, and the Masters students at UCL, Sus-
sex and Brighton who participated in the qual-
ity assessment experiment. The evaluations were
funded by EPSRC (UK) grant EP/G03995X/1.
References
A. Belz and A. Gatt. 2007. The attribute selection for
gre challenge: Overview and evaluation results. In
181
Proceedings of UCNLG+MT: Language Generation
and Machine Translation.
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?08),
pages 197?200.
A. Black, P. Taylor, and R. Caley, 1999. The Festi-
val Speech Synthesis System: System Documenta-
tion. University of Edinburgh, 1.4 edition.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy.
Behavior Research Methods, Instruments, & Com-
puters, 35(1):116?124.
A. Gatt, A. Belz, and Eric Kow. 2008. The tuna chal-
lenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation (INLG?08).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: A method for automatic evaluation of ma-
chine translation. IBM research report, IBM Re-
search Division.
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
Bleu: a. method for automatic evaluation of machine
translation. In Proc. 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-02),
pages 311?318.
R. Ratliff. 1993. Methods for dealing with reaction
time outliers. Psychological Bulletin, 114(3):510?
532.
182
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 79?87,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
The GREC Main Subject Reference Generation Challenge 2009:
Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
The GREC-MSR Task at Generation Chal-
lenges 2009 required participating systems
to select coreference chains to the main
subject of short encyclopaedic texts col-
lected from Wikipedia. Three teams sub-
mitted one system each, and we addition-
ally created four baseline systems. Sys-
tems were tested automatically using ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying corefer-
ence resolution tools to the outputs and
measuring the success of the tools. In ad-
dition, systems were tested in an intrinsic
evaluation involving human judges. This
report describes the GREC-MSR Task and
the evaluation methods applied, gives brief
descriptions of the participating systems,
and presents the evaluation results.
1 Introduction
The GREC-MSR Task is about how to generate ap-
propriate references to an entity in the context of a
piece of discourse longer than a sentence. Rather
than requiring participants to generate referring
expressions from scratch, the GREC-MSR data pro-
vides sets of possible referring expressions for se-
lection. This was the second time we ran a shared
task using the GREC-MSR data (following a first
run in 2008). The task definition was again kept
fairly simple, but in the 2009 round the main aim
for participating systems was to select an appro-
priate word string to serve as a referring expres-
sion, whereas in 2008 it was to select an appropri-
ate type of referring expression (name, common
noun, pronoun, or empty reference).
The immediate motivating application context
for the GREC-MSR Task is the improvement of ref-
erential clarity and coherence in extractive sum-
maries by regenerating referring expressions in
them. There has recently been a small flurry
of work in this area (Steinberger et al, 2007;
Nenkova, 2008). In the longer term, the GREC-
MSR Task is intended to be a step in the direction
of the more general task of generating referential
expressions in discourse context.
The GREC-MSR data is an extension of the
GREC 1.0 Corpus which had about 1,000 texts in
the subdomains of cities, countries, rivers and peo-
ple (Belz and Varges, 2007a). For the purpose of
the GREC-MSR shared task, an additional 1,000
texts in the new subdomain of mountain texts were
obtained and a new XML annotation scheme (Sec-
tion 2.2) was developed.
Team System Name
University of Delaware UDel
ICSI, Berkeley ICSI-CRF
Jadavpur University JUNLG
Table 1: GREC-MSR?09 participating teams.
Nine teams from seven countries registered for
GREC-MSR?09, of which three teams (Table 1)
submitted one system each.1 Participants had to
submit their system reports before downloading
test data inputs, and had to submit test data out-
puts within 48 hours of downloading the test data
inputs. In addition to the participants? systems,
we also used the corpus texts themselves as ?sys-
tem? outputs, and created 4 baseline systems; we
evaluated the resulting 8 systems using a range of
intrinsic and extrinsic evaluation methods (for de-
tails see Sections 5 and 6). This report presents the
results of all evaluations (Section 6), along with
descriptions of GREC-MSR data and task (Sec-
tion 2), test sets (Section 3), evaluation methods
(Section 4), and participating systems (Section 5).
2 Data and Task
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introduc-
1One team submitted by the original deadline (Jan. 2009),
one by the revised deadline (1 June 2009), one slightly later.
79
tory sections in Wikipedia articles, in five different
subdomains (cities, countries, rivers, people and
mountains). In each text, three broad categories
of Main Subject Reference (MSR)2 have been an-
notated, resulting in a total of about 13,000 anno-
tated REs. The GREC-MSR shared task version of
the corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results
on the development data.
2.1 Types of referential expression annotated
Three broad categories of main subject referring
expressions (MSREs) are annotated in the GREC
corpus3 ? subject NPs, object NPs, and geni-
tive NPs and pronouns which function as subject-
determiners within their matrix NP. These cate-
gories of referring expressions (RE) are relatively
straightforward to identify and to achieve high
inter-annotator agreement on (complete agree-
ment among four annotators in 86% of MSRs), and
account for most cases of overt main subject refer-
ence in the GREC texts. The annotators were asked
to identify subject, object and genitive subject-
determiners and decide whether or not they refer
to the main subject of the text. More detail is pro-
vided in Belz and Varges (2007b).
In addition to the above, relative pronouns in
supplementary relative clauses (as opposed to in-
tegrated relative clauses, Huddleston and Pullum,
2002, p. 1058) were annotated, e.g.:
(1) Stoichkov is a football manager and former striker
who was a member of the Bulgaria national team that
finished fourth at the 1994 FIFA World Cup.
We also annotated ?non-realised? subject MSREs
in those cases of VP coordination where an MSRE
is the subject of the coordinated VPs, e.g.:
(2) He stated the first version of the Law of conservation
of mass, introduced the Metric system, and
helped to reform chemical nomenclature.
The motivation for annotating the approximate
place where the subject NP would be if it were
realised (the gap-like underscores above) is that
from a generation perspective there is a choice to
be made about whether to realise the subject NP in
the second and third coordinates or not.
2The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
(and title) of one text is London.
3In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
2.2 XML format
Figure 1 is one of the texts distributed in the
GREC-MSR training/development data set. The
REF element indicates a reference, in the sense of
?an instance of referring? (which could, in princi-
ple, be realised by gesture or graphically, as well
as by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cate-
gory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX ele-
ment (the ?selected? referential expression for the
given reference; in the training/development data
texts it is simply the referential expression found
in the corpus) and one ALT-REFEX element which
in turn is a list of REFEXs which are possible alter-
native referential expressions (see following sec-
tion).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in
the GREC-MSR corpus, the only type of RE that
has EMPHATIC=yes is one which incorporates a re-
flexive pronoun used emphatically (e.g. India it-
self ). The REG08-TYPE attribute indicates basic RE
type. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether
to use an RE that includes a name, such as Mod-
ern India (the corresponding REG08-TYPE value
is name); whether to go for a common-noun RE,
i.e. with a category noun like country as the head
(common); whether to use a pronoun (pronoun); or
whether it can be left unrealised (empty).
2.3 The GREC-MSR Task
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to
the training/development data, except that REF el-
ements contained only an ALT-REFEX list, not the
preceding ?selected? REFEX. ALT-REFEX lists are
generated for each text by an automatic method
80
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TEXT SYSTEM "reg08-grec.dtd">
<TEXT ID="36">
<TITLE>Jean Baudrillard</TITLE>
<PARAGRAPH>
<REF ID="36.1" SEMCAT="person" SYNCAT="np-subj">
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX>
<REFEX REG08-TYPE="empty">_</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX>
</ALT-REFEX>
</REF>
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det">
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX>
</ALT-REFEX>
</REF>
work is frequently associated with postmodernism and post-structuralism.
</PARAGRAPH>
</TEXT>
Figure 1: Example text from the GREC-MSR Training Data.
which collects all the (manually annotated) MSREs
in a text including the title, and adds several de-
faults: pronouns and reflexive pronouns in all sub-
domains; and category nouns (e.g. the river), in
all subdomains except people. The main objec-
tive in the 2009 GREC-MSR Task was to get the
word strings contained in REFEXs right (whereas
in REG?08 it was the REG08-TYPE attributes).
3 Test Data
1. Test Set C-1: a randomly selected 10% sub-
set (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. Test Set C-2: the same subset of texts as in C-
1; however, for C-2 we did not use the MSREs in
the corpus, but replaced them with human-selected
alternatives. These were obtained in an online ex-
periment as described in Belz & Varges (2007a)
where subjects selected MSREs in a setting that du-
plicated the conditions in which the participating
systems in the GREC-MSR Task make selections.4
We obtained three versions of each text, where in
each version all MSREs were selected by the same
person. The motivation for this version of Test Set
C was that having several human-produced chains
of MSREs to compare the outputs of participating
(?peer?) systems against is more reliable than hav-
ing one only; and that Wikipedia texts are edited
4The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
by multiple authors which sometimes adversely
affects MSR chains; we wanted to have additional
reference texts where all references are selected by
a single author.
3. Test Set L: 74 Wikipedia introductory texts
from the subdomain of lakes (there were no lake
texts in the training/development set).
4. Test Set P: 31 short encyclopaedic texts in
the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but of different origin. We
transcribed these texts from printed encyclopae-
dias published in the 1980s which are not avail-
able in electronic form. The texts in this set are
much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs fol-
low very similar patterns. It seems likely that it is
these properties that have resulted in better scores
overall for Test Set P than for the other test sets
in both the 2008 and 2009 runs of the GREC-MSR
task (for the latter, see Section 6).
Each test set was designed to test peer systems
for generalisation to different kinds of unseen data.
Test Set C tests for generalisation to unseen ma-
terial from the same corpus and the same subdo-
mains as the training set; Test Set L tests for gen-
eralisation to unseen material from the same cor-
pus but different subdomain; and Test Set P for
generalisation to a different corpus but the same
subdomains.
81
4 Evaluation methods
4.1 Automatic intrinsic evaluations5
Accuracy of REFEX word strings: when com-
puted against test sets (C-1, L and P), Word String
Accuracy is simply the proportion of REFEX word
strings selected by a participating system that are
identical to the one in the corpus. When computed
against test set C-2, which has three versions of
each text, Word String Accuracy is computed as
follows: first the number of correct REFEX word
strings is computed at the text level for each of the
three versions of a text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number
of REFs in all the texts, which gives the global
Word String Accuracy score. The rationale be-
hind computing the Word String Accuracy scores
in this way for multiple-RE test sets (maximising
scores on RE chains rather than individual REs) is
that an RE is not good or bad in its own right, but
depends on other MSREs in the same text.
Accuracy of REG08-Type: similarly to Word
String Accuracy above, when computed against
test sets C-1, L and P, REG08-Type Accuracy is the
proportion of REFEXs selected by a participating
system that have a REG08-TYPE value identical to
the one in the corpus. When computed against test
set C-2, first the number of correct REG08-Types is
computed at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total num-
ber of REFs in all the texts, which gives the global
REG08-Type Accuracy score.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
cost of 1. We also used a length-normalised ver-
sion of string-edit distance (denoted ?norm. SE? in
results tables below). For test sets C-1, L and P,
the global score is simply the mean of all RE-level
scores. For Test Set C-2, the global score is the
mean of the mean of the three text-level scores.
Other metrics: BLEU is a precision metric
from machine translation that assesses peer trans-
lations in terms of the proportion of word n-grams
5For GREC-MSR?09 we updated the tool that computes all
automatic intrinsic scores and in the course of this eliminated
a character encoding issue; as a result the results for baseline
systems and corpus texts reported here are on the whole very
slightly higher than those reported for GREC-MSR?08.
(n ? 4 is standard) they share with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in
the corpus are less than 4 tokens long. We also
used the NIST version of BLEU which weights in
favour of less frequent n-grams. In both cases,
we assessed just the MSREs selected by peer sys-
tems (leaving out the surrounding text), and com-
puted scores globally (rather than averaging over
RE-level scores), as this is standard for these met-
rics. BLEU, and NIST are designed to work with
one or multiple reference texts, so we did not need
to use a different method for Test Set C-2.
4.2 Automatic extrinsic evaluation
As in GREC-MSR?08, we used an automatic ex-
trinsic evaluation method based on coreference
resolution performance.6 The basic idea is that it
seems likely that badly chosen reference chains af-
fect the ability to resolve REs in automatic coref-
erence resolution tools which will tend to perform
worse with poorly selected MSR reference chains.
To counteract the possibility of results being a
function of a specific coreference resolution algo-
rithm or tool, we used two different resolvers?
those included in LingPipe7 and OpenNLP (Mor-
ton, 2005)?and averaged results.
There does not appear to be a single standard
evaluation metric in the coreference resolution
community, so we opted to use three: MUC-6
(Vilain et al, 1995), CEAF (Luo, 2005), and B-
CUBED (Bagga and Baldwin, 1998), which seem
to be the most widely accepted metrics. All three
metrics compute Recall, Precision and F-Scores
on aligned gold-standard and resolver-tool coref-
erence chains. They differ in how the alignment
is obtained and what components of coreference
chains are counted for calculating scores. Results
for the automatic extrinsic evaluations are reported
below in terms of the F-Scores from these three
metrics, as well as in terms of their mean.
4.3 Human intrinsic evaluation
The intrinsic human evaluation involved 24 ran-
domly selected items from Test Set C and outputs
for these produced by peer and basline systems as
6However, for GREC?09 we overhauled the tool; the cur-
rent version no longer uses JavaRAP, and uses the most recent
versions of the other resolvers; the GREC-MSR?08 and GREC-
MSR?09 results for this method are not entirely comparable
for this reason.
7http://alias-i.com/lingpipe/
82
Figure 2: Example of text presented in human intrinsic evaluation of GREC-MSR systems.
well as those found in the original corpus texts
(8 systems in total). We used a Repeated Latin
Squares design which ensures that each subject
sees the same number of outputs from each sys-
tem and for each test set item. There were three
8x8 squares, and a total of 576 individual judg-
ments in this evaluation (72 per system: 3 criteria
x 3 articles x 8 evaluators).
We recruited 8 native speakers of English from
among post-graduate students currently doing a
linguistics-related degree at University College
London (UCL) and University of Sussex.
Following detailed instructions, subjects did
two practice examples, followed by the 24 texts
to be evaluated, in random order. Subjects carried
out the evaluation over the internet, at a time and
place of their choosing. They were allowed to in-
terrupt and resume the experiment (though discor-
ouged from doing so). According to self-reported
timings, subjects took between 25 and 45 minutes
to complete the evaluation (not counting breaks).
Figure 2 shows what subjects saw during the
evaluation of an individual text. All references to
the MS are highlighted in yellow, and the task is to
evaluate the quality of the REs in terms of three cri-
teria which were explained in the introduction as
follows (the wording of the explanations of Crite-
ria 1 and 3 were taken from the DUC evaluations):
1. Referential Clarity: It should be easy to identify who
or what the referring expressions in the text are refer-
ring to. If a person or other entity is mentioned, it
should be clear what their role in the story is. So, a ref-
erence would be unclear if an entity is referenced, but
their identity or relation to the story remains unclear.
2. Fluency: A referring expression should ?read well?, i.e.
it should be written in good, clear English, and the use
of titles and names etc. should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
3. Structure and Coherence: The text should be well
structured and well organised. The text should not just
be a heap of related information, but should build from
sentence to sentence to a coherent body of information
about a topic. This criterion too is independent of the
others.
Subjects selected evaluation scores by moving
sliders (see Figure 2) along scales ranging from 1
to 5. Slider pointers started out in the middle of
the scale (3). These were continuous scales and
we recorded scores with one decimal place (e.g.
3.2). The meaning of the numbers was explained
in terms of integer scores (1=very poor, 2=poor,
3=neither poor nor good, 4=good, 5=very good).
5 Systems
Base-rand, Base-freq, Base-1st, Base-name:
Baseline system Base-rand selects one of the
REFEXs at random. Base-freq selects the REFEX
that is the overall most frequent given the SYNCAT
and SEMCAT of the reference. Base-1st al-
ways selects the REFEX which appears first in
the ALT-REFEX list; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.8
8Attributes are considered in this order. If for one at-
tribute, the right value is not found, the process ignores that
attribute and moves on the next one.
83
UDel: The UDel system consists of a prepro-
cessing component performing sentence segmen-
tation and identification of non-referring occur-
rences of main subject (MS) names, an RE type
selection component (two C5.0 decision trees, one
optimised for people and mountains, the other for
the other subdomains), and a word string selec-
tion component. The RE type selection decision
trees use the following features: is the MS the sub-
ject of the current, preceding and preceding but
one sentence; was the last MSR in subject position;
are there interfering references to other entities be-
tween the current and the previous MSR; distance
to preceding non-referring occurrences of an MS
name; sentence and reference IDs; other features
indicating whether the reference occurred before
and after certain words and punctuation marks.
Given a selected RE type, the word-string selec-
tion component selects the longest non-emphatic
name for the first named reference in an article,
and the shortest for subsequent named references;
for other types, the first matching word-string is
used, backing off to pronoun or name.
ICSI-CRF: The ICSI-CRF system construes the
GREC-MSR task as a sequence labelling task and
determines the most likely current label given pre-
ceding labels using a Conditional Random Field
model trained using the follow features for the cur-
rent, preceding and preceding but one MSR: pre-
ceding and following word unigram and bigram;
suffix of preceding and following word; preceding
and following punctuation; reference ID; is this is
the beginning of a paragraph. If more than one la-
bel remains, the last in the list of possible REs in
the GREC-MSR data is selected.
JUNLG: The JUNLG system is based on co-
occurrence statistics between REF feature sets and
REFEX feature sets as found in the GREC-MSR data.
REF feature sets were augmented by a paragraph
counter and a within-paragraph REF counter. For
each given set of REF features, the system selects
the most frequent REFEX feature set (as determined
from co-occurrence counts in the training data). If
the current set of possible REFEXs does not include
a REFEX with the selected feature set, then the sec-
ond most likely feature set is selected. Several
hand-coded default rules override the frequency-
based selections, e.g. if the preceding word is a
conjunction, and the current SYNCAT is np-subj,
then the REG08-Type is empty.
6 Results
This section presents the results of all evalua-
tion methods described in Section 4. We start
with Word String Accuracy, the intrinsic auto-
matic metric which participating teams were told
was going to be the chief evaluation method, fol-
lowed by REG08-Type Accuracy and other intrin-
sic automatic metrics (Section 6.2), the intrinsic
human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
System Word String Acc. REG08-Type Acc. Norm. Edit Dist.
ICSI-CRF 0.67 0.75 0.28
UDel 0.6357 0.7027 0.3383
JUNLG 0.532 0.62 0.421
Table 2: Self-reported evaluation scores for devel-
opment set.
6.1 Word String Accuracy
Participants computed Word String Accuracy for
the development set (97 texts) themselves, using
an evaluation tool provided by us. These scores
are shown in column 2 of Table 2, and are also
included in the participants? reports in this vol-
ume. Corresponding results for test set C-1 are
shown in column 2 of Table 3. Surprisingly, Word
String Accuracy results on the test data are better
(than on the development data) for the UDel and
JUNLG systems. Also included in this table are re-
sults for the four baseline systems, and it is clear
that selecting the most frequent word string given
SEMCAT and SYNCAT (as done by the Base-freq sys-
tem) provides a strong baseline.
The other two parts of Table 3 contain results for
test sets L and P. As expected, results for Test Set L
are lower than for Test Set C-1, because in addition
to consisting of unseen texts (like C-1), Test Set L
is also from an unseen subdomain (unlike C-1).
The Word String Accuracy results for Test Set P
are higher than for any other set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 3 we carried out a
univariate ANOVA with System as the fixed factor,
?Number of REFEXs in a text? as a random factor,
and Word String Accuracy as the dependent vari-
able. We found significant main effects of Sys-
tem on Word String Accuracy at p < .001 in the
case of all three test sets (C-1: F(7,1272) = 90.058;
L: F(7,440) = 44.139; P: F(7,168) = 21.991).9
The columns containing capital letters in Table 3
9We included the corpus texts themselves in the analysis,
hence 7 degrees of freedom (8 systems).
84
Test Set C-1 Test Set L Test Set P
UDel 67.68 A UDel 52.89 A UDel 77.16 A
ICSI-CRF 62.98 A JUNLG 50.80 A ICSI-CRF 72.22 A
JUNLG 61.94 A ICSI-CRF 49.20 A JUNLG 71.60 A
Base-freq 47.05 B Base-name 21.06 B Base-freq 53.09 B
Base-name 28.74 C Base-freq 20.74 B Base-name 27.78 C
Base-1st 28.26 C Base-1st 20.74 B Base-1st 27.16 C
Base-rand 18.95 D Base-rand 15.11 B Base-rand 18.52 C
Table 3: Word String Accuracy scores against Test Sets C-1, L and P; homogeneous subsets (Tukey HSD,
alpha = .05) for each test set (systems that do not share a letter are significantly different).
System Word String Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People Mountains
Corpus 71.58 A 65.25 69.11 76.47 80.40 66.87
UDel 70.22 A B 68.09 71.20 76.47 76.63 64.84
JUNLG 64.57 B C 54.61 51.83 73.53 71.86 65.85
ICSI-CRF 63.69 C 58.87 56.54 64.71 72.11 60.98
Base-freq 57.01 D 51.06 57.07 58.82 63.82 53.05
Base-name 40.21 E 51.06 46.07 29.41 29.90 43.90
Base-1st 39.65 E 47.52 41.88 38.24 25.63 47.97
Base-rand 26.99 F 28.37 29.32 23.53 21.61 30.28
Table 4: Word String Accuracy scores against Test Set C-2 for complete set and for subdomains; homo-
geneous subsets (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are
significantly different).
show the homogeneous subsets of systems as de-
termined by post-hoc Tukey HSD comparisons of
means. Systems whose Word String Accuracy
scores are not significantly different (at the .05
level) share a letter.
The results for Word String Accuracy com-
puted against Test Set C-2 are shown in Table 4.
These should be considered the chief results of the
GREC-MSR?09 Task evaluations, as stated in the
participants? guidelines. Here too we performed
a univariate ANOVA with System as the fixed fac-
tor, Number of REFEXs as the random factor and
Word String Accuracy as the dependent variable.
There was a significant main effect of System
(F(7,1272) = 74.892, p < .001). We compared the
mean scores with Tukey?s HSD. As can be seen
from the resulting homogeneous subsets, there is
no significant difference between the corpus texts
(C-1) and the UDel system, but also there is no
significant difference between the latter and the
JUNLG system. In this analysis, all peer systems
outperform all baselines; the Base-freq baseline
outperforms all other baselines; and Base-name
and Base-1st outperform the random baseline.
Overall, there is a marked improvement in Word
String Accuracy compared to GREC-MSR?08
where peer systems? scores ranged from 50.72 to
65.61.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure re-
ported on in the preceding section, we computed
REG08-Type Accuracy and the string similarity
metrics described in Section 4.1. The resulting
scores for Test Set C-2 are shown in Table 5 (re-
call that in Test Set C-2 corpus texts are evalu-
ated against 3 texts with human-selected alterna-
tive REs). The corpus texts again receive the best
scores across the board. Ranks for peer systems
are very similar to those reported in the last sec-
tion.
We performed a univariate ANOVA with Sys-
tem as the fixed factor, Number of REFEXs as the
random factor, and REG08-Type Accuracy as the
dependent variable. The main effect of System
was F(7,1272) = 75.040, p < .001; the homoge-
neous subsets resulting from the Tukey HSD post-
hoc analysis are shown in columns 3?5 of Table 5.
The differences between the scores of the peer sys-
tems and the corpus texts were not found to be sig-
nificant.
6.3 Human-assessed intrinsic measures
Table 6 shows the results of the human intrinsic
evaluation. In each of the three parts of the ta-
ble (showing the results for Fluency, Clarity and
Coherence, respectively) systems are ordered in
terms of their mean scores (shown in the second
column of each part of the table). We first es-
tablished that the main effect of Evaluator was
weak (F between 2.1 and 2.6) on Fluency, Clar-
ity and Coherence, and only of borderline signifi-
cance (just below .05); and that the interaction be-
tween System and Evaluator was very weak and
85
System Other similarity measures for Triple-RE Test Set C-2
REG08-Type BLEU-3 NIST SE norm. SE
Corpus 79.30 A 0.77 5.60 1.04 0.34
UDel 77.71 A 0.74 5.32 1.11 0.37
JUNLG 75.40 A 0.53 4.69 1.34 0.40
ICSI-CRF 75.16 A 0.54 4.68 1.32 0.41
Base-freq 62.50 B 0.54 4.30 1.93 0.50
Base-name 51.04 C 0.46 4.76 1.80 0.63
Base-1st 50.32 C 0.39 4.42 1.93 0.63
Base-rand 48.09 C 0.26 3.02 2.30 0.72
Table 5: REG08-Type Accuracy, BLEU, NIST and string-edit scores, computed on test set C-2 (systems
in order of REG08-Type Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for REG08-Type
Accuracy only (systems that do not share a letter are significantly different).
Fluency Clarity Coherence
Corpus 4.43 A Base-name 4.62 A Corpus 4.40 A
UDel 4.27 A Corpus 4.56 A JUNLG 4.33 A
JUNLG 4.26 A JUNLG 4.50 A UDel 4.27 A B
ICSI-CRF 4.15 A B ICSI-CRF 4.45 A ICSI-CRF 4.02 A B
Base-freq 3.33 B C UDel 4.35 A Base-freq 3.96 A B
Base-name 2.84 C D Base-1st 4.27 A Base-name 3.85 A B
Base-1st 2.76 C D Base-freq 4.10 A Base-1st 3.7 A B
Base-rand 2.15 D Base-rand 3.18 B Base-rand 3.46 B
Table 6: Clarity, Fluency and Coherence scores (with homogeneous subsets) for all systems.
not significant in the case of Clarity and Coher-
ence, and borderline significant in the case of Flu-
ency. We then ran a (non-factorial) multivariate
ANOVA, with Fluency, Coherence and Clarity as
the dependent variables, and (just) System as the
fixed factor. The main effect of System was as
follows: Fluency: F(7,128) = 20.444, p < 0.001;
Clarity: F(7,128) = 5.248, p < 0.001; Coherence:
F(7,128) = 2.680, p < 0.012. The homogeneous
subsets resulting from a post-hoc Tukey analysis
are shown in the letter columns in Table 6.
The effect of System was strongest on Fluency;
here, the system ranks are also the same as for
Word String Accuracy and REG08-Type Accuracy
for Test Set C-2. This, together with the fair
amount of significant differences found, indicates
that the evaluators were able to make sense of the
Fluency criterion and that there were interesting
differences between systems under this criterion.
However, differences between the three peer sys-
tems were not significant.
For Clarity, there were no significant differ-
ences among the peer systems and non-random
baseline systems; all of these were significantly
better than the random baseline. Base-name had
the highest mean Clarity score, possibly because
always chosing the name of an entity when refer-
ring to it ensures high referential clarity.
The Coherence results are perhaps the most dif-
ficult to interpret. Both the main effect of System
on Coherence and its significance were weaker
than for Fluency and Clarity. Only two signifi-
cant pairwise differences were found: Corpus and
JUNLG were better than the random baseline. The
system ranks are roughly the same as for Fluency,
but the mean scores cover a smaller range (from
3.46 to 4.4) than in the case of either of the other
two criteria. Overall, the Coherence results proba-
bly indicate that the evaluators found it somewhat
difficult to make sense of the Coherence criterion.
Computing Pearson?s r for the three criteria
on individual (text-level) scores showed that there
were only moderate correlations between them (all
around r = 0.5) which were all significant at
? = 0.05. This gives some indication that the
evaluators were able to assess the three criteria in-
dependently from each other.
6.4 Automatic extrinsic measures
We fed the outputs of all eight systems through
the two coreference resolvers, and computed mean
MUC, CEAF and B-CUBED F-Scores as described
in Section 4.2. The second column in Table 7
shows the mean of these three F-Scores, to give
a single overall result for this evaluation method.
A univariate ANOVA with mean F-Score as the de-
pendent variable and System as the fixed factor
revealed a significant main effect of System on
mean F-Score (F(7,1456) = 73.061, p < .001).
A post-hoc comparison of the means (Tukey HSD,
alpha = .05) found the significant differences in-
dicated by the homogeneous subsets in columns
3?4 (Table 7). The numbers shown in the last
three columns are the separate MUC, CEAF and B-
CUBED F-Scores for each system, averaged over
the two resolver tools. ANOVAs revealed the fol-
86
lowing effects of System on the separate scoring
methods: on CEAF F(7,1456) = 43.471, p < .001;
on MUC: F(7,1456) =, p < .001; on B-CUBED:
F(7,1456) = 38.574, p < .001. All three scor-
ing methods separately and their mean yielded the
same significant differences (as shown in columns
3?4 of Table 7).
The three F-Score measures (MUC, CEAF and B-
CUBED) are all significantly correlated (p < .001,
2-tailed). However it is not a strong correlation,
with Pearson?s correlation coefficient around 0.5.
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-name 65.19 A 62.35 63.14 70.06
Base-1st 63.77 A 59.95 62.08 69.28
Base-freq 63.14 A 59.08 62.04 68.3
UDel 46.19 B 34.85 46.86 56.86
ICSI-CRF 44.47 B 31.61 45.58 56.21
JUNLG 44.19 B 31.27 45.21 56.10
Base-rand 42.99 B 30.24 43.04 55.7
Corpus 42.52 B 29.53 43.57 54.47
Table 7: MUC, CEAF and B-CUBED F-Scores for
all systems; homogeneous subsets (Tukey HSD),
alpha = .05, for mean of F-Scores.
6.5 Correlations
When assessed on the system-level scores and us-
ing Pearson?s r, all evaluation methods above were
strongly and significantly correlated with each
other (at the 0.01 level, 2-tailed), with the fol-
lowing exceptions. Clarity was not significantly
correlated with any of the other methods except
NIST (r = .902, p < .01); apart from this, NIST
was only correlated with Word String Accuracy on
test set C-2, with non-normalised string-edit dis-
tance, Fluency and Coherence, moreover all at the
weaker 0.05 level. Finally, the extrinsic method
was not correlated with any of the intrinsic meth-
ods (and in fact showed signs of being negatively
correlated with all of them except Clarity).
7 Concluding Remarks
The GREC-MSR Task is still a relatively new task
not only for an NLG shared-task challenge, but also
as a research task in general (post-processing ex-
tractive summaries in order to improve their qual-
ity seems to be just taking off as a research sub-
field). There was substantial interest in the GREC-
MSR Task this year (as indicated by the nine teams
that originally registered). However, only three
teams were ultimately able to participate.
We continued the traditions of previous NLG
shared tasks in that we used a wide range of eval-
uation metrics to obtain a well-rounded view of
the quality of the participating systems. This in-
cluded intrinsic human evaluations for the first
time. However, we decided against an extrinsic
human evaluation this year, given time constraints
as well as the fact that this evaluation type yielded
barely any significant results last year.
Overall, there was an improvement in system
performance compared to last year, to the point
where the performance of the top system was
barely distinguishable from the human topline.
We are not currently planning to run the GREC-
MSR task again next year.
Acknowledgments
Many thanks to the UCL and Sussex students who
participated in the intrinsic evaluation experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC?98, pages
563?566.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz and S. Varges. 2007b. The GREC corpus:
Main subject reference in context. Technical Report
NLTG-07-01, University of Brighton.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of
Pensylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP?08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public refer-
ence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC?04, pages 291?
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management:
Special issue on Summarization, 43(6):1663?1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings of MUC-6, pages
45?52.
87
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 88?98,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
The GREC Named Entity Generation Challenge 2009:
Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Abstract
The GREC-NEG Task at Generation Chal-
lenges 2009 required participating sys-
tems to select coreference chains for all
people entities mentioned in short en-
cyclopaedic texts about people collected
from Wikipedia. Three teams submitted
six systems in total, and we additionally
created four baseline systems. Systems
were tested automatically using a range of
existing intrinsic metrics. We also eval-
uated systems extrinsically by applying
coreference resolution tools to the outputs
and measuring the success of the tools.
In addition, systems were tested in an in-
trinsic evaluation involving human judges.
This report describes the GREC-NEG Task
and the evaluation methods applied, gives
brief descriptions of the participating sys-
tems, and presents the evaluation results.
1 Introduction
The GREC-NEG task is about how to generate ap-
propriate references to people entities in the con-
text of a piece of discourse longer than a sentence.
Rather than requiring participants to generate re-
ferring expressions (REs) from scratch, the GREC-
NEG data provides sets of possible REs for selec-
tion. This was the first time we ran a shared task
using this data. GREC-NEG is a step further from
the related GREC-MSR Task in that it requires sys-
tems to generate plural as well as singular refer-
ences, for all people entities mentioned in a text
(GREC-MSR in contrast only had singular refer-
ences to a single entity). Moreover in GREC-NEG,
possible REs for each entity are provided as one set
for each entity (rather than one set for each con-
text), so the task of selecting an appropriate RE
for a given context is harder than in GREC-MSR.
The main aim for participating systems in GREC-
NEG?09 was to select an appropriate type of RE
(name, common noun, pronoun, or empty refer-
ence).
The immediate motivating application context
for the GREC Tasks is the improvement of referen-
tial clarity and coherence in extractive summaries
and multiply edited texts (such as Wikipedia arti-
cles) by regenerating REs contained in them.
The motivating theoretical interest for the GREC
Tasks is to discover what kind of information is
useful in the input when making decisions about
different properties of referring expressions when
such expressions are being generated in context
(this is in contrast to most traditional referring ex-
pression generation work in NLG which views the
REG task as context-independent).
The GREC-NEG data is derived from the
newly created GREC-People corpus which con-
sists of 1,000 annotated introduction sections from
Wikipedia articles in the category People.
Nine teams from seven countries registered for
the GREC-NEG?09 Task, of which three teams ul-
timately submitted six systems in total (see Ta-
ble 1). We also used the corpus texts themselves
as ?system? outputs, and created four baseline sys-
tems. We evaluated the resulting 11 systems using
a range of intrinsic and extrinsic evaluation meth-
ods. This report presents the results of all evalu-
ations (Section 6), along with descriptions of the
GREC-NEG data (Sections 2) and task (Section 3),
the test sets and evaluation methods (Section 4),
and the participating systems (Section 5).
Team System name(s)
Univ. Delaware UDel-NEG-1, UDel-NEG-2, UDel-NEG-3
ICSI, Berkeley ICSI-CRF
Univ. Wolverhampton WLV-STAND, WLV-BIAS
Table 1: GREC-NEG?09 teams and systems.
2 GREC-NEG Data
The GREC-NEG data is derived from the newly
created GREC-People corpus which consists
88
of 1,000 annotated introduction sections from
Wikipedia articles in the category People. An in-
troduction section was defined as the textual con-
tent of a Wikipedia article from the title up to
(and excluding) the first section heading, the ta-
ble of contents or the end of the text, whichever
comes sooner. Each text belongs to one of three
subcategories: inventors, chefs and early music
composers. For the purposes of the GREC-NEG?09
competition, the GREC-People corpus was divided
into training, development and test data. The num-
ber of texts in the 3 data sets and 3 subdomains are
as follows:
All Inventors Chefs Composers
Total 1,000 307 306 387
Training 809 249 248 312
Development 91 28 28 35
Test 100 31 30 39
In these texts we have annotated mentions of peo-
ple by marking up the word strings that function as
referential expressions (REs) and annotating them
with coreference information as well as syntactic
and semantic features. The subject of each text is a
person, so there is at least one coreference chain in
each text. The numbers of coreference chains (en-
tities) in the 900 texts in the training/development
sets are as shown in Table 2. The texts vary greatly
in length, from 13 words to 935, with an average
of 128.98 words.
2.1 Annotation of REs in GREC-People
This section describes the different types of re-
ferring expression (RE) that we annotated in the
GREC-People corpus. These manual annotations
were then automatically checked and converted to
the XML format described in Section 2.2 (which
encodes slightly less information, as explained be-
low). In terminology and the treatment of syntax
used in the annotation scheme and discussion of it
in this report we rely heavily on The Cambridge
Grammar of the English Language by Huddleston
and Pullum which we will refer to as CGEL for
short below (Huddleston and Pullum, 2002).
In the example sentences below, (unbroken) un-
derlines are used for referential expressions (REs)
that are an example of the specific type of RE they
are intended to illustrate, whereas dashed under-
lines are used for other annotated REs. Corefer-
ence between REs is indicated by subscripts i, j, ...
immediately to the right of an underline (their
scope is one example sentence, i.e. an i in one ex-
ample sentence does not represent the same en-
tity as an i in another example sentence). Square
brackets indicate supplements. The syntactic com-
ponent relativised by a relative pronoun is indi-
cated by vertical bars. Supplements and their an-
chors (in the case of appositive supplements), and
relative clauses and the component they relativise
(in the case of relative-clause supplements) are co-
indexed by superscript x, y, .... Dependents inte-
grated in an RE are indicated by curly brackets.
Supplements and dependents are highlighted in
bold where they specifically are being discussed.
In the XML format of the annotations, the be-
ginning and end of a reference is indicated by
<REF><REFEX>... </REFEX></REF> tags, and
other properties discussed in the following sec-
tions (e.g. syntactic category) are encoded as at-
tributes on these tags (for details see Section 2.2).
For GREC-NEG?09 we decided not to transfer the
annotations of integrated dependents and relative
clauses to the XML format. Such dependents
are included within <REFEX>...</REFEX> annota-
tions where appropriate, but without being marked
up as separate constituents.
2.1.1 Syntactic Category and Function
This section describes the types of REs we annoted
in the GREC-People Corpus.
I Subject NPs: referring subject NPs, including
pronouns and special cases of VP coordination:
1. Hei was born in Ramsay township, near Almonte, On-
tario, Canada, the eldest son of |Scottish immigrants,
{John Naismith and Margaret Young} |xj,k [whoj,k had
arrived in the area in 1851 and j,k worked in the
mining industry]x.
2. The Banu Musa brothersi,j,k were three 9th century
Persian scholars, of Baghdad, active in the House of
Wisdom.
Ia Subjects of gerund-participials:
1. Hisi research on hearing and speech eventually culmi-
nated in Belli being awarded the first U.S. patent for
the invention of the telephone in 1876.
2. Fessendeni used the alternator-transmitter to send out
a short program from Brant Rock, which included hisi
playing the song O Holy Night on the violin and i
reading a passage from the Bible.
II Object NPs: referring NPs including pro-
nouns that function as direct or indirect objects of
VPs and prepositional phrases; e.g.:
1. Many of the alpinists arrested with Vitaly Abalakovi
were executed.
2. Hei entrusted themj,k,l to Ishaq bin Ibrahim
al-Mus?abixm , [a former governor of Baghdad]xm .
89
Entities 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Texts 437 192 80 63 38 31 16 18 4 7 9 1 1 0 0 0 0 0 0 1 1 0 1
Table 2: Numbers of person entities (hence coreference chains) in texts in the training/development data,
e.g. there are 38 texts which mention exactly 5 person entities.
IIa Reflexive pronouns:
1. Smithi called himselfi the ?Komikal Konjurer?.
III Subject-determiner genitives: genitive NPs
(including genitive forms of pronouns) that func-
tion as subject-determiners, i.e. syntactic compo-
nents that ?combine the function of determiner,
marking the NP as definite, with that of comple-
ment (more specifically subject).? (CGEL, p. 56):
1. Theyi,j,k shared the 1956 Nobel Prize in Physics for
theiri,j,k invention.
2. On the eve of hisi death in 1605, the Mughal em-
pire spanned almost 500 million acres (doubling dur-
ing Akbar?si reign).
Note that this category excludes lexicalised cases,
e.g. the so-called ?Newton?s method?.
IIIa REs in composite nominals: this is the
only type of RE we have annotated that is not an
NP, but a nominal. This type functions as inte-
grated attributive complement, e.g.:
1. The Eichengru?ni version was ignored by historians ...
2. The new act was a great success, largely despite the
various things Blacktoni and Smithj were doing be-
tween the Edisonk films.
Note that this category too excludes lexicalised
cases, e.g. the Nobel Prizes; the Gatling gun.
2.1.2 Annotation of supplements
We have annotated two kinds of supplements in
the GREC-People corpus, supplementary relative
clauses (CGEL, p. 1058), and appositive supple-
ments. The former is not transferred to the XML
annotation, for more information see (Belz, 2009).
The following examples illustrate annotation of
appositive supplements (which are in bold):
1. John W. Campbell, Jr.xi
[the editor of Astounding magazinei ]x.
2. was the eldest of the six children of Thomas Aspdinxi ,
[a bricklayer living in the Hunslet district of Leedsi ]x
In the XML version, anchor and supple-
ment are simply annotated as two (or occasion-
ally three) independent, usually adjacent REs
(REFEXs); the syntactic function of the second
(and third) RE is marked as appositive supplement
(SYNFUNC="app-supp").
2.1.3 Further aspects of the annotation
As can be seen from some of the examples above,
we annotated all embedded references. The
maximum depth of embedding that occurs in the
GREC-People corpus is 3.
We annotated all plural REs that refer to groups
of people where the number of group members is
known. For an explanation of our treatment of
REs that are coordinations of NPs, see the GREC-
NEG?09 documentation (Belz, 2009).
We have annotated all mentions of individual
person entities even if they are not actually named
anywhere in the text, and including cases of both
definite and indefinite references, e.g.:
1. The resolution?s sponsori described it as ...
2. ... with the help of Robert Cailliauj and
a {young} student staff {at CERN}k .
2.2 XML Annotation
Figure 1 shows one of the XML-annotated texts
from the GREC-NEG data. Each such text con-
sists of two initial lines of XML declarations fol-
lowed by a GREC-ITEM. A GREC-ITEM consists of a
TEXT element followed by an ALT-REFEX element.
A TEXT has one attribute (an ID unique within the
corpus), and is composed of one TITLE followed
by any number of PARAGRAPHs. A TITLE is just a
string of characters. A PARAGRAPH is any combi-
nation of character strings and REF elements.
The REF element indicates a reference, in the
sense of ?an instance of referring? (which could,
in principle, be realised by gesture or graphically,
as well as by a string of words, or a combination of
these). A REF is composed of one REFEX element
(the ?selected? referential expression for the given
reference; in the corpus texts it is the referential
expression found in the corpus).
The attributes of the REF element are ENTITY
(entity identifier), MENTION (mention identifier),
SEMCAT (semantic category), SYNCAT (syntactic
category), and SYNFUNC (syntactic function). For
full details and ranges of values see (Belz, 2009).
ENTITY and MENTION together constitute a unique
identifier for a reference within a text; together
90
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE GREC-ITEM SYSTEM "genchal09-grec.dtd">
<GREC-ITEM>
<TEXT ID="15">
<TITLE>Alexander Fleming</TITLE>
<PARAGRAPH> <REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX>
</REF> (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist.
<REF ENTITY="0" MENTION="2" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX>
</REF> published many articles on bacteriology, immunology, and chemotherapy.
<REF ENTITY="0" MENTION="3" SEMCAT="person" SYNCAT="np" SYNFUNC="subj-det">
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
</REF> best-known achievements are the discovery of the enzyme lysozyme in 1922 and the discovery
of the antibiotic substance penicillin from the fungus Penicillium notatum in 1928, for which
<REF ENTITY="0" MENTION="4" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
</REF> shared the Nobel Prize in Physiology or Medicine in 1945 with
<REF ENTITY="1" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj">
<REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX>
</REF> and
<REF ENTITY="2" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj">
<REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX>
</REF>.</PARAGRAPH>
</TEXT>
<ALT-REFEX>
<REFEX ENTITY="0" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Fleming?s</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Sir Alexander Fleming?s</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
<REFEX ENTITY="1" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="1" REG08-TYPE="name" CASE="genitive">Florey?s</REFEX>
<REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
<REFEX ENTITY="2" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="2" REG08-TYPE="name" CASE="genitive">Chain?s</REFEX>
<REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
</ALT-REFEX>
</GREC-ITEM>
Figure 1: Example XML-annotated text from the GREC-NEG?09 data.
with the TEXT ID, they constitute a unique iden-
tifier for a reference within the entire corpus.
A REFEX element indicates a referential expres-
sion (a word string that can be used to refer to an
entity). The attributes of the REFEX element are
REG08-TYPE (name, common, pronoun, empty), and
CASE (nominative, accusative, etc.).
We allow arbitrary-depth embedding of refer-
ences. This means that a REFEX element may have
REF element(s) embedded in it. See also next but
one paragraph for embedding in REFEX elements
that are contained in ALT-REFEX lists.
The second (and last) component of a
GREC-ITEM is an ALT-REFEX element which
is a list of REFEX elements. For the GREC-NEG?09
Task, these were obtained by collecting the set of
all REFEXs that are in the text, and adding several
defaults including pronouns and other cases (e.g.
genitive) of REs already in the list.
REF elements that are embedded in REFEX ele-
ments contained in an ALT-REFEX list have an un-
specified MENTION id (the ??? value). Furthermore,
such REF elements have had their enclosed REFEX
removed. For example:
<ALT-REFEX>
...
<REFEX ENTITY="2" REG08-TYPE="common" CASE="plain">
a friend of <REF ENTITY="1" MENTION="?" SEMCAT=
"person" SYNCAT="np" SYNFUNC="obj"></REF></REFEX>
...
</ALT-REFEX>
3 The GREC-NEG Task
The test data inputs were identical to the train-
ing/development data (Figure 1), except that REF
elements in the test data do not contain a REFEX
element, i.e. they are ?empty?. The task for par-
ticipating systems is to select one REFEX from the
ALT-REFEX list for each REF in each TEXT in the
test sets. If the selected REFEX contains an em-
91
bedded REF then participating systems also need
to select a REFEX for this embedded REF and to set
the value of its MENTION attribute. The same ap-
plies to all further embedded REFEXs, at any depth
of embedding.
4 Evaluation Procedures
The GREC-NEG data set was divided into training,
development and test data. We performed eval-
uations on the test data, using a range of different
evaluation methods, including intrinsic and extrin-
sic, automatically assessed and human-evaluated,
as described in the following sections.
Participants computed evaluation scores on the
development set, using the geval-2.0.pl code
provided by us which computes Word String Ac-
curacy, REG?08-Type Recall and Precision, string-
edit distance and BLEU.
4.1 Test sets
We created two versions of the test data for the
GREC-NEG Task:
1. GREC-NEG Test Set 1a: randomly selected 10% subset
(100 texts) of the GREC-People corpus (with the same
proportion of texts in the 3 subdomains as in the train-
ing/development data).
2. GREC-NEG Test Set 1b: the same subset of texts as in
(1a); for this set we did not use the REs in the corpus,
but replaced each of them with human-selected alterna-
tives obtained in an online experiment as described in
(Belz and Varges, 2007); this test set therefore contains
three versions of each text where all the REFEXs in a
given version were selected by one ?author?.
Test Set 1a has a single version of each text, and
the scoring metrics below that are based on count-
ing matches (Word String Accuracy counts match-
ing word strings, REG08-Type Recall/Precision
count matching REG08-Type attribute values)
simply count the number of matches a system
achieves against that single text.
Test Set 1b, however, has three versions of each
text, so the match-based metrics first calculate the
number of matches for each of the three versions
and then use (just) the highest number of matches.
4.2 Automatic intrinsic evaluations
The chief humanlikeness measures we computed
were REG08-Type Recall and Precision. REG08-
Type Precision is defined as the proportion of
REFEXs selected by a participating system which
match the reference REFEXs (where match counts
are obtained as explained in the preceding sec-
tion). REG08-Type Recall is defined as the propor-
tion of reference REFEXs for which a participating
system has produced a match.
The reason why we use REG08-Type Recall and
Precision for GREC-NEG rather than REG08-Type
Accuracy as in GREC-MSR is that in GREC-NEG
(unlike in GREC-MSR) there may be a different
number of REFEXs in system outputs and the ref-
erence texts in the test set (because there are em-
bedded references in GREC-People, and systems
may select REFEXs with or without embedded ref-
erences for any given REF).
We also computed String Accuracy, defined as
the proportion of word strings selected by a par-
ticipating system that match those in the reference
texts. This was computed on complete, ?flattened?
word strings contained in the outermost REFEX i.e.
embedded REFEX word strings were not considered
separately.
We also computed BLEU-3, NIST, string-edit
distance and length-normalised string-edit dis-
tance, all on word strings defined as for String Ac-
curacy. BLEU and NIST are designed for multiple
output versions, and for the string-edit metrics we
computed the mean of means over the three text-
level scores (computed against the three versions
of a text). For details, see GREC-MSR report in
this volume.
4.3 Human-assessed intrinsic evaluations
Given that the motivating application context for
the GREC-NEG Task is improving referential clar-
ity and coherence in multiply edited texts, we
designed the human-assessed intrinsic evaluation
as a preference-judgment test where subjects ex-
pressed their preference, in terms of two criteria,
for either the original Wikipedia text or the version
of it with system-generated referring expressions
in it. The intrinsic human evaluation involved out-
puts for 30 randomly selected items from the test
set from 5 of the 6 participating systems,1 the four
baselines and the original corpus texts (10 systems
in total). We used a Repeated Latin Squares de-
sign which ensures that each subject sees the same
number of outputs from each system and for each
test set item. There were three 10x10 squares, and
a total of 600 individual judgments in this evalu-
ation (60 per system: 2 criteria x 3 articles x 10
1We left out UDel-NEG-1 given our limited resources and
the fact that this is a kind of baseline system.
92
Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.
evaluators). We recruited 10 native speakers of
English from among students currently complet-
ing a linguistics-related degree at Kings College
London and University College London.
Following detailed instructions, subjects did
two practice examples, followed by the 30 texts
to be evaluated, in random order. Subjects car-
ried out the evaluation over the internet, at a time
and place of their choosing. They were allowed to
interrupt and resume the experiment (though dis-
couraged from doing so).
Figure 2 shows what subjects saw during the
evaluation of an individual text pair. The place
(left/right) of the original Wikipedia article was
randomly determined for each individual evalua-
tion of a text pair. People references are high-
lighted in yellow/orange, those that are identical
in both texts are yellow, those that are different
are orange. The evaluator?s task is to express their
preference in terms of each quality criterion by
moving the slider pointers. Moving the slider to
the left means expressing a preference for the text
on the left, moving it to the right means preferring
the text on the right; the further to the left/right the
slider is moved, the stronger the preference. The
two criteria were explained in the introduction as
follows (the wording of the first is from DUC):
1. Referential Clarity: It should be easy to identify who
the referring expressions are referring to. If a person
is mentioned, it should be clear what their role in the
story is. So, a reference would be unclear if a person
is referenced, but their identity or relation to the story
remains unclear.
2. Fluency: A referring expression should ?read well?,
i.e. it should be written in good, clear English, and the
use of titles and names should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
It was not evident to the evaluators that slid-
ers were associated with numerical values. Slider
pointers started out in the middle of the scale (no
preference). The values associated with the points
on the slider ranged from -10.0 to +10.0.
4.4 Extrinsic automatic evaluation
An evaluation we piloted in REG?08 was an auto-
matic approach to extrinsic evaluation (for a more
detailed description, see the GREC-MSR results re-
port elsewhere in this volume). The basic premise
is that poorly chosen reference chains seem likely
to affect the reader?s ability to resolve REs. In our
automatic extrinsic method, the role of the reader
is played by an automatic coreference resolution
tool and the expectation is that the tool performs
worse (is less able to identify coreference chains)
with more poorly chosen referential expressions.
93
To counteract the possibility of results being a
function of a specific coreference resolution algo-
rithm or tool, we used two different resolvers?
those included in LingPipe2 and OpenNLP (Mor-
ton, 2005)?and averaged results. For the same
reason we used three different performance mea-
sures: MUC-6 (Vilain et al, 1995), CEAF (Luo,
2005), and B-CUBED (Bagga and Baldwin, 1998).
5 Systems
Base-rand, Base-freq, Base-1st, Base-name:
We created four baseline systems each with a
different way of selecting a REFEX from those
REFEXs in the ALT-REFEX list that have match-
ing entity IDs. Base-rand selects a REFEX at ran-
dom. Base-1st selects the first REFEX. Base-freq
selects the first REFEX with a REG08-TYPE that
is the overall most frequent (as determined from
the training/development data) given the SYNCAT,
SYNFUNC and SEMCAT of the reference. Base-
name selects the shortest REFEX with attribute
REG08-TYPE=name.
UDel: The UDel-NEG-1 system is identical to
the UDel system that was submitted to the GREC-
MSR Task (for a description of that system see
GREC-MSR?09 results report in this volume), ex-
cept that it was adapted to the different data for-
mat of GREC-NEG. UDel-NEG-2 is identical to
UDel-NEG-1 except that it was retrained on GREC-
NEG data and the feature set was extended by en-
tity and mention IDs. UDel-NEG-3 additionally
utilised improved identification of other entities.
ICSI-CRF: The ICSI-CRF system construes the
GREC-MSR task as a sequence labelling task and
determines the most likely current class label
given preceding labels using a Conditional Ran-
dom Field model trained using the follow features
for the current reference, the most recent preced-
ing reference, and the most recent reference to the
same entity: preceding and following word uni-
gram and bigram; suffix of preceding and follow-
ing word; preceding and following punctuation;
reference ID; and whether this is the beginning of
a paragraph. If more than one class label remains,
the last in the list of possible REs in the GREC-MSR
data is selected.
WLV: The WLV systems start with sentence
splitting and POS tagging. WLV-STAND then em-
2http://alias-i.com/lingpipe/
ploys a J48 decision tree classifier to obtain a prob-
ability for each REF/REFEX pair that it is a good
pair in the current context. The context is repre-
sented by the following set of features. Features
of the REFEX word string: is it the longest of the
possible REFEXs; number of words; all REFEX fea-
tures supplied in GREC-NEG data. Features of the
REF: is it part of the first chain in the text; is it the
first mention of the entity; is it at the beginning of
the sentence; all REF features supplied in GREC-
NEG data. Other features: do the preceding words
match ?, but?, ?and then? and similar phrases; dis-
tance in sentences to last mention; REG08-Type
selected for the two preceding REFs; POS tags of
4 words before and 3 words after; correlation be-
tween SYNFUNC and CASE values; size of the chain.
WLV-BIAS is the same except that it is retrained
on reweighted training instances. The reweighting
scheme assigns a cost of 3 to false negatives and 1
to false positives.
6 Results
This section presents the results of all the evalua-
tion methods described in Section 4. We start with
REG08-Type Precision and Recall, the intrinsic au-
tomatic metrics which participating teams were
told was going to be the chief evaluation method,
followed by Word String Accuracy and other in-
trinsic automatic metrics (Section 6.2), the intrin-
sic human evaluation (Section 6.3) and the extrin-
sic automatic evaluation (Section 6.4).
System REG08-Type WS Acc. Norm. SERecall Precision
ICSI-CRF 83.05 83.05 0.786 0.197
WLV-BIAS 77.61 80.26 0.735 0.239
UDelNEG-3 75.27 75.27 0.333 0.636
UDelNEG-2 74.95 74.95 0.323 0.646
UDelNEG-1 68.87 68.87 0.315 0.658
WLV-STAND 66.20 68.46 0.626 0.351
Table 5: Self-reported evaluation scores for devel-
opment set.
6.1 REG08-Type Precision and Recall
Participants computed scores for the development
set (91 texts) themselves, using the geval evalua-
tion tool provided by us. These scores are shown
in Table 5, and are also included in the partici-
pants? reports elsewhere in this volume.3
REG08-Type Recall and Precision results for
Test Set 1a are shown in column 2 of Table 3.
As would be expected, results on the test data are
3 ICSI-CRF scores obtained directly from ISCI team.
94
System
REG08-Type Precision and Recall Scores against Corpus (Test Set 1a)
All Chefs Composers Inventors
Precision Recall R P R P R P
ICSI-CRF 79.12 A 76.92 A 70.01 73.54 78.11 80.18 80.05 81.86
WLV-BIAS 73.77 B 72.70 A 69.82 71.52 73.53 74.38 73.65 74.56
WLV-STAND 64.49 C 63.55 B 58.28 59.70 65.38 66.14 64.78 65.59
Base-freq 61.52 C 59.6 B 49.41 51.86 63.95 65.74 60.59 62.12
UDel-NEG-2 53.21 D 51.14 C 44.38 47.17 50.50 52.22 57.88 59.80
UDel-NEG-3 52.49 D 50.45 C 43.49 46.23 49.79 51.48 57.39 59.29
UDel-NEG-1 50.47 D 48.51 C 42.90 45.60 47.78 49.41 54.43 56.23
Base-rand 43.32 E 42.00 D 38.76 40.43 41.77 43.00 45.07 46.21
Base-name 40.60 E 39.09 D 44 97 47.80 39.06 40.32 34.24 35.28
Base-1st 10.99 F 10.81 E 12.43 12.73 9.30 9.43 12.07 12.22
Table 3: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set and
for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.
System
REG08-Type Precision and Recall Scores against human topline (Test Set 1b)
All Chefs Composers Inventors
Precision Recall R P R P R P
Corpus 82.67 A 84.01 A 84.24 82.25 84.47 83.26 83.04 82.02
ICSI-CRF 79.33 A B 78.38 B 76.36 77.54 78.81 79.74 79.30 80.10
WLV-BIAS 77.78 B 77.78 B 77.58 77.58 77.86 77.86 77.81 77.81
WLV-STAND 67.51 C 67.51 C 65.76 65.76 68.60 68.60 67.08 67.08
Base-freq 65.38 C 64.37 C 58.48 59.94 68.07 68.97 62.84 63.64
UDel-NEG-2 57.39 D 56.06 D 55.15 57.23 54.86 55.92 58.85 60.05
UDel-NEG-3 57.25 D 55.92 D 55.76 57.86 54.57 55.62 58.35 59.54
Base-name 55.22 D 54.01 D 54.24 56.29 57.04 58.05 48.63 49.49
UDel-NEG-1 53.57 D 52.32 D E 51.21 53.14 50.80 51.78 55.86 57.00
Base-rand 48.46 E 47.75 E 47.88 48.77 46.44 47.13 49.88 50.51
Base-1st 12.54 F 12.54 F 13.94 13.94 10.45 10.45 14.96 14.96
Table 4: REG08-Type Recall and Precision scores against human topline version of Test Set for complete
set and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.
somewhat worse (than on the development data).
Also included in this table are results for the 4
baseline systems, and it is clear that selecting the
most frequent RE type given SEMCAT, SYNFUNC and
SYNCAT (as done by the Base-freq system) pro-
vides a strong baseline for RE type selection.
The last 6 columns in Table 3 contain Recall (R)
and Precision (P) results for the three subdomains.
For most of the systems results are slightly better
for Inventors than for Composers, and better for
Composers than for Chefs. A contributing factor
to this may be the fact that texts in Chefs tend to
be much more colloquial. Base-1st has by far the
worst results; this is because it selects the empty
reference in almost all cases (because ALT-REFEX
lists are sorted and if a list contains an empty ref-
erence it will end up at the beginning).
We carried out univariate ANOVAs with Sys-
tem as the fixed factor, and ?Number of REFEXs
in a text? as a random factor, and REG08-Type Re-
call as the dependent variable in one ANOVA, and
REG08-Type Precision in the other. The result for
Recall was F(10,704) = 81.547, p < 0.001.4 The
result for Precision was F(10,722) = 79.359, p <
0.001. The columns containing capital letters in
Table 3 show the homogeneous subsets of systems
4We included the corpus texts themselves in the analysis,
hence 10 degrees of freedom (11 systems).
as determined by a post-hoc Tukey HSD analysis.
Systems whose scores are not significantly differ-
ent (at the .05 level) share a letter.
Table 4 shows analogous results computed
against Test Set 1b (which has three versions of
each text). These should be considered as the
chief results of the GREC-NEG?09 Task evalua-
tions, as stated in the participants? guidelines. Ta-
ble 4 includes results for the corpus texts, com-
puted (as are results for the system outputs in Ta-
ble 4) against the three versions of each text in Test
Set 1b. We performed univariate ANOVAs with
System as the fixed factor, Number of REFEXs as
a random factor, and Recall as the dependent vari-
able in one, and Precision in the other. The result
for Recall was F(10,724) = 72.528, p < .001),
and for Precision F(10,722) = 75.476, p < .001.
For both cases, we compared the mean scores with
Tukey?s HSD. As can be seen from the resulting
homogeneous subsets (letter columns in Table 4),
system ranks are the same for Precision and for
Recall. In terms of Precision, the difference be-
tween the corpus texts and the ICSI-CRF system
was not significant.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure re-
ported on in the preceding section, we computed
95
System
String similarity against Corpus (Test Set 1a)
Word String Accuracy
BLEU-3 NIST SE norm. SEAll Chefs Composers Inventors
ICSI-CRF 74.84 A 68.24 76.63 77.10 0.75 5.78 0.70 0.23
WLV-BIAS 68.57 B 66.35 69.08 69.47 0.76 5.62 0.82 0.29
WLV-STAND 59.55 C 54.72 61.24 60.56 0.73 5.34 1.01 0.39
Base-name 28.48 D 35.53 27.51 24.43 0.5 4.09 1.80 0.67
UDel-NEG-1 16.58 E 20.13 15.09 16.28 0.43 2.47 2.1 0.82
UDel-NEG-2 16.44 E 19.81 14.79 16.54 0.45 2.37 2.08 0.83
UDel-NEG-3 16.37 E 19.18 15.09 16.28 0.45 2.41 2.08 0.83
Base-rand 8.22 F 8.49 7.10 9.92 0.17 0.9 2.43 0.89
Base-1st 7.28 F 7.23 6.36 8.91 0.16 0.98 2.54 0.90
Base-freq 2.52 G 4.40 2.37 1.27 0.31 1.91 2.34 0.90
Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1a (systems
in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy
only.
System
String similarity against human topline (Test Set 1b)
Word String Accuracy
BLEU-3 NIST SE norm. SEAll Chefs Composers Inventors
Corpus 81.90 A 83.33 82.25 80.15 0.95 7.15 0.71 0.25
ICSI-CRF 74.55 B 71.70 75.15 75.83 0.86 6.35 0.92 0.31
WLV-BIAS 69.07 C 69.50 68.49 69.72 0.88 6.17 1.03 0.36
WLV-STAND 59.70 D 58.18 60.36 59.80 0.84 5.81 1.21 0.45
Base-name 37.27 E 42.14 36.83 34.10 0.65 5.57 1.73 0.63
UDel-NEG-1 19.25 F 22.96 17.60 19.08 0.51 2.62 2.17 0.82
UDel-NEG-2 18.96 F 22.96 17.31 18.58 0.53 2.42 2.15 0.83
UDel-NEG-3 18.89 F 22.64 17.75 17.81 0.53 2.49 2.15 0.82
Base-rand 10.45 G 10.06 9.91 11.70 0.25 1.11 2.49 0.89
Base-1st 8.65 G 8.49 7.54 10.69 0.24 1.29 2.64 0.92
Base-freq 3.24 H 4.40 3.55 1.78 0.39 2.1 2.40 0.90
Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1b (systems
in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy.
Word String Accuracy and the other string simi-
larity metrics described in Section 4.2. The result-
ing scores for Test Set 1a (the corpus texts) are
shown in Table 6. Ranks for peer systems rela-
tive to each other are very similar to the results
reported in the last section. However, the ranks of
the baseline systems have changed substantially,
both in relation to each other and to the peer sys-
tems. In particular, Base-freq has moved all the
way down to the bottom of the table. The rea-
son is that this method is geared towards select-
ing the correct type of RE, but pays no attention
to whether it selects a syntactically appropriate RE
for the given context, instead simply selecting the
first RE from the ALT-REFEX list that has the se-
lected type; in the GREC-NEG?09 Task (unlike the
GRE-MSR task) this just happens to be an RE in
the genitive case most of the time which is over-
all rarer than nominative/plain. It is likely that the
Word String scores for the UDel-NEG systems are
low for a similar reason.
We performed a univariate ANOVA with System
as the fixed factor and Number of REFEXs as a
random factor and Word String Accuracy as the
dependent variable. The result for System was
F(10,726) = 103.339; the homogeneous subsets re-
sulting from the Tukey HSD post-hoc analysis are
shown in columns 3?9 of Table 6.
Table 7 shows analogous results for human
topline Test Set 1b (which has three versions of
each text). We carried out the same kind of ANOVA
as for Test Set 1a; the result for System on Word
String Accuracy was F(10,726) = 106.755, p <
0.001. System rankings and homogeneous sub-
sets are the same as for Test Set 1a; scores across
the board are somewhat higher, because of the way
scores are computed for Test Set 1b: it is the high-
est score a system achieves (at text-level) against
any of the three versions of a test set text that is
taken into account.
Results for BLEU-3, NIST and the two string-
edit distance metrics are shown in the rightmost 4
columns of Tables 6 and 7. Systems whose Word
String Accuracy scores differ significantly are as-
signed the same ranks by NIST and the two string-
edit distance metrics as by Word String Accuracy
(except for Base-1st and Base-freq which swap
ranks in some. BLEU-3 does the same and also
flips ICSI-CRF and WLV-BIAS.
6.3 Human-assessed intrinsic measures
In the human intrinsic evaluation, evaluators rated
system outputs in terms of whether they preferred
them over the original Wikipedia texts. As a re-
96
Clarity Fluency
System Mean + 0 ? System Mean + 0 ?
Corpus 0 A 0 30 0 Corpus 0 A 0 30 0
ICSI-CRF -1.447 A B 3 17 10 ICSI-CRF -0.353 A 9 14 7
WLV-BIAS -2.437 A B C 3 14 13 WLV-BIAS -2.257 A B 2 14 14
Base-name -2.583 B C 7 7 16 WLV-STAND -5.823 B C 1 3 26
WLV-STAND -4.477 C D 1 9 20 Base-name -4.257 C D 2 5 23
UDelNEG-3 -6.427 D E 1 4 26 UDelNEG-3 -6.263 C D E 1 3 26
UDelNEG-2 -6.667 D E 1 3 26 UDelNEG-2 -7.13 D E 0 3 27
Base-rand -8.183 E F 0 1 29 Base-rand -7.513 D E 0 0 30
Base-freq -8.26 E F 0 0 30 Base-freq -7.57 D E 0 0 30
Base-1st -9.357 F 0 0 30 Base-1st -8.477 E 0 0 30
Table 8: Results for Clarity and Fluency preference judgement experiment. Mean = mean of individual
scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred; ? =
number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.
sult of the experiment we had for each system and
each evaluation criterion a set of scores ranging
from -10.0 to +10.0, where 0 meant no prefer-
ence, negative scores meant a preference for the
Wikipedia text, and positive scores a preference
for the system-produced text.
The second column of the left half of Table 8
summarises the Clarity scores for each system in
terms of their mean; if the mean is negative the
evaluators overall preferred the Wikipedia texts,
if it is positive evaluators overall preferred the
system. The more negative the score, the more
strongly evaluators preferred the Wikipedia texts.
Columns 9-11 show corresponding counts of how
many times each system was preferred (+), dis-
preferred (?), and neither (0), when compared to
Wikipedia.
The other half of Table 8 shows corresponding
results for Fluency.
We ran a factorial multivariate ANOVA with Flu-
ency and Clarity as the dependent variables. In the
first version of the ANOVA, the fixed factors were
System, Evaluator and Wikipedia Side (indicating
whether the Wikipedia text was shown on the left
or right during evaluation). This showed no signif-
icant effect of Wikipedia Side on either Fluency or
Clarity, and no significant interaction between any
of the factors. There was however a mild effect of
Evaluator on both Fluency and Clarity. We ran the
ANOVA again, this time with just System and Eval-
uator as fixed factors. The result for System on
Fluency was F(9,200) = 37.925, p < .001, and for
System on Clarity it was F(9,200) = 35.439, p <
.001. Post-hoc Tukey?s HSD tests revealed the sig-
nificant pairwise differences indicated by the letter
columns in Table 8.
Correlation between individual Clarity and Flu-
ency ratings as estimated with Pearson?s coeffi-
cient was r = 696, p < .01, indicating that the
two criteria covary to some extent.
Apart from Base-name and WLV-STAND
switching places, system ranks are the same for
Fluency and Clarity. Moreover, system ranks
are very similar to those produced by the string-
similarity scores above. Perhaps the most striking
result is that the ICSI-CRF system does succeed
in improving Fluency compared to the original
Wikipedia texts: it is preferred 9 times whereas
the Wikipedia texts are preferred only 7 times.
System (MUC+CEAF+B3)/3 M C B3
WLV-BIAS 62.64 A 57 62 69
ICSI-CRF 61.28 A B 53 61 69
Base-name 61.11 A B 55 61 68
Corpus 59.56 A B C 53 59 67
UDel-NEG-3 56.13 B C D 48 56 65
UDel-NEG-2 55.9 B C D 47 55 65
Base-freq 55.85 B C D 47 56 65
UDel-NEG-1 54.79 C D 46 54 64
WLV-STAND 51.69 D 41 53 61
Base-rand 34.86 E 15 38 51
Base-1st 26.36 F 2 31 46
Table 9: MUC, CEAF and B-CUBED F-Scores for
all systems; homogeneous subsets (Tukey HSD),
alpha = .05, for mean of F-Scores.
6.4 Automatic extrinsic measures
We fed the outputs of all 11 systems through the
two coreference resolvers, and computed mean
MUC, CEAF and B-CUBED F-Scores as described
in Section 4.4. The second column in Table 9
shows the mean of means of these three F-Scores,
to give a single overall result for each of for this
evaluation method. A univariate ANOVA with
(text-level) mean F-Score as the dependent vari-
able and System as the single fixed factor revealed
a significant main effect of System on mean F-
Score (F(10,1089) = 91.634, p < .001). A post-
hoc comparison of the means (Tukey HSD, alpha
= .05) found the significant differences indicated
by the homogeneous subsets in columns 3?8 (Ta-
ble 9). The numbers in the last three columns are
the separate MUC, CEAF and B-CUBED F-Scores
97
for each system, averaged over the two resolver
tools (and rounded for reasons of space.
7 Concluding Remarks
This was the first time the GREC-NEG Task was
run. It is a new task not only for an NLG shared-
task challenge, but also as a research task in gen-
eral (post-processing extractive summaries in or-
der to improve their quality seems to be just taking
off as a research subfield). There was substantial
interest in the GREC-NEG Task (as indicated by the
nine teams that originally registered). However,
only 3 teams were ultimately able to submit a sys-
tem.
In particular because of the inclusion of plural
references, multiple entities per text and embed-
ded references, the GREC-NEG Task has a higher
entrance level than the GREC-MSR Task. We are
planning to run it again at Generation Challenges
2010 next year, and are considering the possibility
of providing participants with a baseline system
which would help e.g. with processing embedded
references.
We are also planning to add a named entity
recognition preprocessing task, so that this new
task in combination with GREC-NEG can be used
to perform end-to-end post-processing of extrac-
tive summaries (and other types of multiply edited
texts) to improve the clarity and fluency of the re-
ferring expressions in them.
Acknowledgments
Many thanks to the members of the Corpora and
SIGGEN mailing lists, and Brighton University
colleagues who helped with the online MSRE se-
lection experiments for GREC-NEG test set 1b.
Thanks are also due to the Kings College Lon-
don and University College London students who
helped with the intrinsic evaluation experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC?98, pages
563?566.
A. Belz and S. Varges. 2007. The GREC corpus:
Main subject reference in context. Technical Report
NLTG-07-01, University of Brighton.
A. Belz, 2009. GREC Named Entity Generation Chal-
lenge 2009: Participants? Pack.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of
Pensylvania.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings of MUC-6, pages
45?52.
98
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 328?335,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Symbolic Approach to Near-Deterministic Surface Realisation using Tree
Adjoining Grammar
Claire Gardent
CNRS/LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow
INRIA/LORIA/UHP
Nancy, France
eric.kow@loria.fr
Abstract
Surface realisers divide into those used in
generation (NLG geared realisers) and those
mirroring the parsing process (Reversible re-
alisers). While the first rely on grammars not
easily usable for parsing, it is unclear how
the second type of realisers could be param-
eterised to yield from among the set of pos-
sible paraphrases, the paraphrase appropri-
ate to a given generation context. In this pa-
per, we present a surface realiser which com-
bines a reversible grammar (used for pars-
ing and doing semantic construction) with a
symbolic means of selecting paraphrases.
1 Introduction
In generation, the surface realisation task consists in
mapping a semantic representation into a grammati-
cal sentence.
Depending on their use, on their degree of non-
determinism and on the type of grammar they as-
sume, existing surface realisers can be divided into
two main categories namely, NLG (Natural Lan-
guage Generation) geared realisers and reversible
realisers.
NLG geared realisers are meant as modules in a
full-blown generation system and as such, they are
constrained to be deterministic: a generation system
must output exactly one text, no less, no more. In or-
der to ensure this determinism, NLG geared realisers
generally rely on theories of grammar which sys-
tematically link form to function such as systemic
functional grammar (SFG, (Matthiessen and Bate-
man, 1991)) and, to a lesser extent, Meaning Text
Theory (MTT, (Mel?cuk, 1988)). In these theories, a
sentence is associated not just with a semantic rep-
resentation but with a semantic representation en-
riched with additional syntactic, pragmatic and/or
discourse information. This additional information
is then used to constrain the realiser output.1 One
drawback of these NLG geared realisers however, is
that the grammar used is not usually reversible i.e.,
cannot be used both for parsing and for generation.
Given the time and expertise involved in developing
a grammar, this is a non-trivial drawback.
Reversible realisers on the other hand, are meant
to mirror the parsing process. They are used on a
grammar developed for parsing and equipped with a
compositional semantics. Given a string and such
a grammar, a parser will assign the input string
all the semantic representations associated with that
string by the grammar. Conversely, given a seman-
tic representation and the same grammar, a realiser
will assign the input semantics all the strings as-
sociated with that semantics by the grammar. In
such approaches, non-determinism is usually han-
dled by statistical filtering: treebank induced prob-
abilities are used to select from among the possible
paraphrases, the most probable one. Since the most
probable paraphrase is not necessarily the most ap-
propriate one in a given context, it is unclear how-
ever, how such realisers could be integrated into a
generation system.
In this paper, we present a surface realiser which
1On the other hand, one of our reviewers noted that ?de-
terminism? often comes more from defaults when input con-
straints are not supplied. One might see these realisers as being
less deterministic than advertised; however, the point is that it
is possible to supply the constraints that ensure determinism.
328
combines reversibility with a symbolic approach to
determinism. The grammar used is fully reversible
(it is used for parsing) and the realisation algorithm
can be constrained by the input so as to ensure a
unique output conforming to the requirement of a
given (generation) context. We show both that the
grammar used has a good paraphrastic power (it
is designed in such a way that grammatical para-
phrases are assigned the same semantic representa-
tions) and that the realisation algorithm can be used
either to generate all the grammatical paraphrases of
a given input or just one provided the input is ade-
quately constrained.
The paper is structured as follows. Section 2 in-
troduces the grammar used namely, a Feature Based
Lexicalised Tree Adjoining Grammar enriched with
a compositional semantics. Importantly, this gram-
mar is compiled from a more abstract specification
(a so-called ?meta-grammar?) and as we shall see, it
is this feature which permits a natural and system-
atic coupling of semantic literals with syntactic an-
notations. Section 3 defines the surface realisation
algorithm used to generate sentences from semantic
formulae. This algorithm is non-deterministic and
produces all paraphrases associated by the gram-
mar with the input semantics. We then go on to
show (section 4) how this algorithm can be used
on a semantic input enriched with syntactic or more
abstract control annotations and further, how these
annotations can be used to select from among the
set of admissible paraphrases precisely these which
obey the constraints expressed in the added annota-
tions. Section 5 reports on a quantitative evaluation
based on the use of a core tree adjoining grammar
for French. The evaluation gives an indication of the
paraphrasing power of the grammar used as well as
some evidence of the deterministic nature of the re-
aliser. Section 6 relates the proposed approach to
existing work and section 7 concludes with pointers
for further research.
2 The grammar
We use a unification based version of LTAG namely,
Feature-based TAG. A Feature-based TAG (FTAG,
(Vijay-Shanker and Joshi, 1988)) consists of a set
of (auxiliary or initial) elementary trees and of two
tree composition operations: substitution and ad-
junction. Initial trees are trees whose leaves are la-
belled with substitution nodes (marked with a dow-
narrow) or terminal categories. Auxiliary trees are
distinguished by a foot node (marked with a star)
whose category must be the same as that of the root
node. Substitution inserts a tree onto a substitution
node of some other tree while adjunction inserts an
auxiliary tree into a tree. In an FTAG, the tree nodes
are furthermore decorated with two feature struc-
tures (called top and bottom) which are unified dur-
ing derivation as follows. On substitution, the top
of the substitution node is unified with the top of the
root node of the tree being substituted in. On adjunc-
tion, the top of the root of the auxiliary tree is uni-
fied with the top of the node where adjunction takes
place; and the bottom features of the foot node are
unified with the bottom features of this node. At the
end of a derivation, the top and bottom of all nodes
in the derived tree are unified.
To associate semantic representations with natu-
ral language expressions, the FTAG is modified as
proposed in (Gardent and Kallmeyer, 2003).
NPj
John
name(j,john)
S
NP?s VPr
V
runs
run(r,s)
VPx
often VP*
often(x)
? name(j,john), run(r,j), often(r)
Figure 1: Flat Semantics for ?John often runs?
Each elementary tree is associated with a flat se-
mantic representation. For instance, in Figure 1,2
the trees for John, runs and often are associated with
the semantics name(j,john), run(r,s) and often(x) re-
spectively.
Importantly, the arguments of a semantic functor
are represented by unification variables which occur
both in the semantic representation of this functor
and on some nodes of the associated syntactic tree.
For instance in Figure 1, the semantic index s oc-
curring in the semantic representation of runs also
occurs on the subject substitution node of the asso-
ciated elementary tree.
2Cx/Cx abbreviate a node with category C and a top/bottom
feature structure including the feature-value pair { index : x}.
329
The value of semantic arguments is determined by
the unifications resulting from adjunction and sub-
stitution. For instance, the semantic index s in the
tree for runs is unified during substitution with the
semantic indices labelling the root nodes of the tree
for John. As a result, the semantics of John often
runs is
(1) {name(j,john),run(r,j),often(r)}
The grammar used describes a core fragment of
French and contains around 6 000 elementary trees.
It covers some 35 basic subcategorisation frames
and for each of these frames, the set of argument re-
distributions (active, passive, middle, neuter, reflex-
ivisation, impersonal, passive impersonal) and of ar-
gument realisations (cliticisation, extraction, omis-
sion, permutations, etc.) possible for this frame. As
a result, it captures most grammatical paraphrases
that is, paraphrases due to diverging argument real-
isations or to different meaning preserving alterna-
tion (e.g., active/passive or clefted/non-clefted sen-
tence).
3 The surface realiser, GenI
The basic surface realisation algorithm used is a bot-
tom up, tabular realisation algorithm (Kay, 1996)
optimised for TAGs. It follows a three step strat-
egy which can be summarised as follows. Given an
empty agenda, an empty chart and an input seman-
tics ?:
Lexical selection. Select all elementary trees
whose semantics subsumes (part of) ?. Store
these trees in the agenda. Auxiliary trees
devoid of substitution nodes are stored in a
separate agenda called the auxiliary agenda.
Substitution phase. Retrieve a tree from the
agenda, add it to the chart and try to combine it
by substitution with trees present in the chart.
Add any resulting derived tree to the agenda.
Stop when the agenda is empty.
Adjunction phase. Move the chart trees to the
agenda and the auxiliary agenda trees to the
chart. Retrieve a tree from the agenda, add it
to the chart and try to combine it by adjunction
with trees present in the chart. Add any result-
ing derived tree to the agenda. Stop when the
agenda is empty.
When processing stops, the yield of any syntacti-
cally complete tree whose semantics is ? yields an
output i.e., a sentence.
The workings of this algorithm can be illustrated
by the following example. Suppose that the input se-
mantics is (1). In a first step (lexical selection), the
elementary trees selected are the ones for John, runs,
often. Their semantics subsumes part of the input se-
mantics. The trees for John and runs are placed on
the agenda, the one for often is placed on the auxil-
iary agenda.
The second step (the substitution phase) consists
in systematically exploring the possibility of com-
bining two trees by substitution. Here, the tree for
John is substituted into the one for runs, and the re-
sulting derived tree for John runs is placed on the
agenda. Trees on the agenda are processed one by
one in this fashion. When the agenda is empty, in-
dicating that all combinations have been tried, we
prepare for the next phase.
All items containing an empty substitution node
are erased from the chart (here, the tree anchored by
runs). The agenda is then reinitialised to the content
of the chart and the chart to the content of the aux-
iliary agenda (here often). The adjunction phase
proceeds much like the previous phase, except that
now all possible adjunctions are performed. When
the agenda is empty once more, the items in the chart
whose semantics matches the input semantics are se-
lected, and their strings printed out, yielding in this
case the sentence John often runs.
4 Paraphrase selection
The surface realisation algorithm just sketched is
non-deterministic. Given a semantic formula, it
might produce several outputs. For instance, given
the appropriate grammar for French, the input in (2a)
will generate the set of paraphrases partly given in
(2b-2k).
(2) a. lj :jean(j) la:aime(e,j,m) lm:marie(m)
b. Jean aime Marie
c. Marie est aime?e par Jean
d. C?est Jean qui aime Marie
e. C?est Jean par qui Marie est aime?e
f. C?est par Jean qu?est aime?e Marie
g. C?est Jean dont est aime?e Marie
h. C?est Jean dont Marie est aime?e
i. C?est Marie qui est aime?e par Jean
330
j. C?est Marie qu?aime Jean
k. C?est Marie que Jean aime
To select from among all possible paraphrases of
a given input, exactly one paraphrase, NLG geared
realisers use symbolic information to encode syn-
tactic, stylistic or pragmatic constraints on the out-
put. Thus for instance, both REALPRO (Lavoie and
Rambow, 1997) and SURGE (Elhadad and Robin,
1999) assume that the input associates semantic lit-
erals with low level syntactic and lexical informa-
tion mostly leaving the realiser to just handle in-
flection, word order, insertion of grammatical words
and agreement. Similarly, KPML (Matthiessen and
Bateman, 1991) assumes access to ideational, inter-
personal and textual information which roughly cor-
responds to semantic, mood/voice, theme/rheme and
focus/ground information.
In what follows, we first show that the semantic
input assumed by the realiser sketched in the previ-
ous section can be systematically enriched with syn-
tactic information so as to ensure determinism. We
then indicate how the satisfiability of this enriched
input could be controlled.
4.1 At most one realisation
In the realisation algorithm sketched in Section 3,
non-determinism stems from lexical ambiguity:3 for
each (combination of) literal(s) l in the input there
usually is more than one TAG elementary tree whose
semantics subsumes l. Thus each (combination of)
literal(s) in the input selects a set of elementary
trees and the realiser output is the set of combi-
nations of selected lexical trees which are licensed
by the grammar operations (substitution and adjunc-
tion) and whose semantics is the input.
One way to enforce determinism consists in en-
suring that each literal in the input selects exactly
one elementary tree. For instance, suppose we want
to generate (2b), repeated here as (3a), rather than
3Given two TAG trees, there might also be several ways
of combining them thereby inducing more non-determinism.
However in practice we found that most of this non-
determinism is due either to over-generation (cases where the
grammar is not sufficiently constrained and allows for one tree
to adjoin to another tree in several places) or to spurious deriva-
tion (distinct derivations with identical semantics). The few re-
maining cases that are linguistically correct are due to varying
modifier positions and could be constrained by a sophisticated
feature decorations in the elementary tree.
any of the paraphrases listed in (2c-2k). Intuitively,
the syntactic constraints to be expressed are those
given in (3b).
(3) a. Jean aime Marie
b. Canonical Nominal Subject, Active verb form,
Canonical Nominal Object
c. lj :jean(j) la:aime(e,j,m) lm:marie(m)
The question is how precisely to formulate these
constraints, how to associate them with the seman-
tic input assumed in Section 3 and how to ensure
that the constraints used do enforce uniqueness of
selection (i.e., that for each input literal, exactly one
elementary tree is selected)? To answer this, we rely
on a feature of the grammar used, namely that each
elementary tree is associated with a linguistically
meaningful unique identifier.
The reason for this is that the grammar is com-
piled from a higher level description where tree frag-
ments are first encapsulated into so-called classes
and then explicitly combined (by inheritance, con-
junction and disjunction) to produce the grammar
elementary trees (cf. (Crabbe? and Duchier, 2004)).
More generally, each elementary tree in the gram-
mar is associated with the set of classes used to pro-
duce that tree and importantly, this set of classes
(we will call this the tree identifier) provides a dis-
tinguishing description (a unique identifier) for that
tree: a tree is defined by a specific combination of
classes and conversely, a specific combination of
classes yields a unique tree.4 Thus the set of classes
associated by the compilation process with a given
elementary tree can be used to uniquely identify that
tree.
Given this, surface realisation is constrained as
follows.
1. Each tree identifier Id(tree) is mapped into a
simplified set of tree properties TPt. There
are two reasons for this simplification. First,
some classes are irrelevant. For instance, the
class used to enforce subject-verb agreement
is needed to ensure this agreement but does
not help in selecting among competing trees.
Second, a given class C can be defined to be
4This is not absolutely true as a tree identifier only reflects
part of the compilation process. In practice, they are few ex-
ceptions though so that distinct trees whose tree identifiers are
identical can be manually distinguished.
331
equivalent to the combination of other classes
C1 . . . Cn and consequently a tree identifier
containing C,C1 . . . Cn can be reduced to in-
clude either C or C1 . . . Cn.
2. Each literal li in the input is associated with a
tree property set TPi (i.e., the input we gener-
ate from is enriched with syntactic information)
3. During realisation, for each literal/tree property
pair ?li : TPi? in the enriched input semantics,
lexical selection is constrained to retrieve only
those trees (i) whose semantics subsumes li and
(ii) whose tree properties are TPi
Since each literal is associated with a (simpli-
fied) tree identifier and each tree identifier uniquely
identifies an elementary tree, realisation produces at
most one realisation.
Examples 4a-4c illustrates the kind of constraints
used by the realiser.
(4) a. lj :jean(j)/ProperName
la:aime(e,j,m)/[CanonicalNominalSubject,
ActiveVerbForm, CanonicalNominalObject]
lm:marie(m)/ProperName
Jean aime Marie
* Jean est aime? de Marie
b. lc:le(c)/Det
lc:chien(c)/Noun
ld:dort(e1,c)/RelativeSubject
lr:ronfle(e2,c)/CanonicalSubject
Le chien qui dort ronfle
* Le chien qui ronfle dort
c. lj :jean(j)/ProperName
lp:promise(e1,j,m,e2)/[CanonicalNominalSubject,
ActiveVerbForm, CompletiveObject]
lm:marie(m)/ProperName
le2:partir(e2,j)/InfinitivalVerb
Jean promet a` marie de partir
* Jean promet a` marie qu?il partira
4.2 At least one realisation
For a realiser to be usable by a generation system,
there must be some means to ensure that its input
is satisfiable i.e., that it can be realised. How can
this be done without actually carrying out realisation
i.e., without checking that the input is satisfiable?
Existing realisers indicate two types of answers to
that dilemma.
A first possibility would be to draw on (Yang et
al., 1991)?s proposal and compute the enriched in-
put based on the traversal of a systemic network.
More specifically, one possibility would be to con-
sider a systemic network such as NIGEL, precom-
pile all the functional features associated with each
possible traversal of the network, map them onto the
corresponding tree properties and use the resulting
set of tree properties to ensure the satisfiability of
the enriched input.
Another option would be to check the well
formedness of the input at some level of the linguis-
tic theory on which the realiser is based. Thus for
instance, REALPRO assumes as input a well formed
deep syntactic structure (DSyntS) as defined by
Meaning Text Theory (MTT) and similarly, SURGE
takes as input a functional description (FD) which in
essence is an underspecified grammatical structure
within the SURGE grammar. In both cases, there
is no guarantee that the input be satisfiable since
all the other levels of the linguistic theory must be
verified for this to be true. In MTT, the DSyntS
must first be mapped onto a surface syntactic struc-
ture and then successively onto the other levels of
the theory while in SURGE, the input FD can be re-
alised only if it provides consistent information for
a complete top-down traversal of the grammar right
down to the lexical level. In short, in both cases, the
well formedness of the input can be checked with
respect to some criteria (e.g., well formedness of a
deep syntactic structure in MTT, well formedness of
a FD in SURGE) but this well formedness does not
guarantee satisfiability. Nonetheless this basic well
formedness check is important as it provides some
guidance as to what an acceptable input to the re-
aliser should look like.
We adopt a similar strategy and resort to the no-
tion of polarity neutral input to control the well
formedness of the enriched input. The proposal
draws on ideas from (Koller and Striegnitz, 2002;
Gardent and Kow, 2005) and aims to determine
whether for a given input (a set of TAG elemen-
tary trees whose semantics equate the input seman-
tics), syntactic requirements and resources cancel
out. More specifically, the aim is to determine
whether given the input set of elementary trees, each
substitution and each adjunction requirement is sat-
isfied by exactly one elementary tree of the appro-
priate syntactic category and semantic index.
332
Roughly,5 the technique consists in (automati-
cally) associating with each elementary tree a po-
larity signature reflecting its substitution/adjunction
requirements and resources and in computing the
grand polarity of each possible combination of trees
covering the input semantics. Each such combina-
tion whose total polarity is non-null is then filtered
out (not considered for realisation) as it cannot pos-
sibly lead to a valid derivation (either a requirement
cannot be satisfied or a resource cannot be used).
In the context of a generation system, polarity
checking can be used to check the satisfiability of the
input or more interestingly, to correct an ill formed
input i.e., an input which can be detected as being
unsatisfiable.
To check a given input, it suffices to compute its
polarity count. If it is non-null, the input is unsatis-
fiable and should be revised. This is not very useful
however, as the enriched input ensures determinism
and thereby make realisation very easy, indeed al-
most as easy as polarity checking.
More interestingly, polarity checking can be used
to suggest ways of fixing an ill formed input. In such
a case, the enriched input is stripped of its control
annotations, realisation proceeds on the basis of this
simplified input and polarity checking is used to pre-
select all polarity neutral combinations of elemen-
tary trees. A closest match (i.e. the polarity neutral
combination with the greatest number of control an-
notations in common with the ill formed input) to
the ill formed input is then proposed as a probably
satisfiable alternative.
5 Evaluation
To evaluate both the paraphrastic power of the re-
aliser and the impact of the control annotations on
non-determinism, we used a graduated test-suite
which was built by (i) parsing a set of sentences, (ii)
selecting the correct meaning representations from
the parser output and (iii) generating from these
meaning representations. The gradation in the test
suite complexity was obtained by partitioning the
input into sentences containing one, two or three fi-
nite verbs and by choosing cases allowing for differ-
ent paraphrasing patterns. More specifically, the test
5Lack of space prevents us from giving much details here.
We refer the reader to (Koller and Striegnitz, 2002; Gardent and
Kow, 2005) for more details.
suite includes cases involving the following types of
paraphrases:
? Grammatical variations in the realisations of
the arguments (cleft, cliticisation, question, rel-
ativisation, subject-inversion, etc.) or of the
verb (active/passive, impersonal)
? Variations in the realisation of modifiers (e.g.,
relative clause vs adjective, predicative vs non-
predicative adjective)
? Variations in the position of modifiers (e.g.,
pre- vs post-nominal adjective)
? Variations licensed by a morpho-derivational
link (e.g., to arrive/arrival)
On a test set of 80 cases, the paraphrastic level
varies between 1 and over 50 with an average of
18 paraphrases per input (taking 36 as upper cut
off point in the paraphrases count). Figure 5 gives
a more detailed description of the distribution of
the paraphrastic variation. In essence, 42% of the
sentences with one finite verb accept 1 to 3 para-
phrases (cases of intransitive verbs), 44% accept 4
to 28 paraphrases (verbs of arity 2) and 13% yield
more than 29 paraphrases (ditransitives). For sen-
tences containing two finite verbs, the ratio is 5%
for 1 to 3 paraphrases, 36% for 4 to 14 paraphrases
and 59% for more than 14 paraphrases. Finally, sen-
tences containing 3 finite verbs all accept more than
29 paraphrases.
Two things are worth noting here. First, the para-
phrase figures might seem low wrt to e.g., work by
(Velldal and Oepen, 2006) which mentions several
thousand outputs for one given input and an average
number of realisations per input varying between
85.7 and 102.2. Admittedly, the French grammar
we are using has a much more limited coverage than
the ERG (the grammar used by (Velldal and Oepen,
2006)) and it is possible that its paraphrastic power
is lower. However, the counts we give only take
into account valid paraphrases of the input. In other
words, overgeneration and spurious derivations are
excluded from the toll. This does not seem to be the
case in (Velldal and Oepen, 2006)?s approach where
the count seems to include all sentences associated
by the grammar with the input semantics.
Second, although the test set may seem small it is
important to keep in mind that it represents 80 inputs
333
with distinct grammatical and paraphrastic proper-
ties. In effect, these 80 test cases yields 1 528 dis-
tinct well-formed sentences. This figure compares
favourably with the size of the largest regression test
suite used by a symbolic NLG realiser namely, the
SURGE test suite which contains 500 input each
corresponding to a single sentence. It also compares
reasonably with other more recent evaluations (Call-
away, 2003; Langkilde-Geary, 2002) which derive
their input data from the Penn Treebank by trans-
forming each sentence tree into a format suitable for
the realiser (Callaway, 2003). For these approaches,
the test set size varies between roughly 1 000 and
almost 3 000 sentences. But again, it is worth stress-
ing that these evaluations aim at assessing coverage
and correctness (does the realiser find the sentence
used to derive the input by parsing it?) rather than
the paraphrastic power of the grammar. They fail to
provide a systematic assessment of how many dis-
tinct grammatical paraphrases are associated with
each given input.
To verify the claim that tree properties can be used
to ensure determinism (cf. footnote 4), we started
by eliminating from the output all ill-formed sen-
tences. We then automatically associated each well-
formed output with its set of tree properties. Finally,
for each input semantics, we did a systematic pair-
wise comparison of the tree property sets associated
with the input realisations and we checked whether
for any given input, there were two (or more) dis-
tinct paraphrases whose tree properties were the
same. We found that such cases represented slightly
over 2% of the total number of (input,realisations)
pairs. Closer investigation of the faulty data indi-
cates two main reasons for non-determinism namely,
trees with alternating order of arguments and deriva-
tions with distinct modifier adjunctions. Both cases
can be handled by modifying the grammar in such
a way that those differences are reflected in the tree
properties.
6 Related work
The approach presented here combines a reversible
grammar realiser with a symbolic approach to para-
phrase selection. We now compare it to existing sur-
faces realisers.
NLG geared realisers. Prominent general
purpose NLG geared realisers include REALPRO,
SURGE, KPML, NITROGEN and HALOGEN. Fur-
thermore, HALOGEN has been shown to achieve
broad coverage and high quality output on a set of 2
400 input automatically derived from the Penn tree-
bank.
The main difference between these and the
present approach is that our approach is based on a
reversible grammar whilst NLG geared realisers are
not. This has several important consequences.
First, it means that one and the same grammar and
lexicon can be used both for parsing and for gener-
ation. Given the complexity involved in developing
such resources, this is an important feature.
Second, as demonstrated in the Redwood Lingo
Treebank, reversibility makes it easy to rapidly cre-
ate very large evaluation suites: it suffices to parse a
set of sentences and select from the parser output the
correct semantics. In contrast, NLG geared realis-
ers either work on evaluation sets of restricted size
(500 input for SURGE, 210 for KPML) or require
the time expensive implementation of a preprocessor
transforming e.g., Penn Treebank trees into a format
suitable for the realisers. For instance, (Callaway,
2003) reports that the implementation of such a pro-
cessor for SURGE was the most time consuming part
of the evaluation with the resulting component con-
taining 4000 lines of code and 900 rules.
Third, a reversible grammar can be exploited to
support not only realisation but also its reverse,
namely semantic construction. Indeed, reversibility
is ensured through a compositional semantics that is,
through a tight coupling between syntax and seman-
tics. In contrast, NLG geared realisers often have
to reconstruct this association in rather ad hoc ways.
Thus for instance, (Yang et al, 1991) resorts to ad
334
hoc ?mapping tables? to associate substitution nodes
with semantic indices and ?fr-nodes? to constrain
adjunction to the correct nodes. More generally, the
lack of a clearly defined compositional semantics in
NLG geared realisers makes it difficult to see how
the grammar they use could be exploited to also sup-
port semantic construction.
Fourth, the grammar can be used both to gener-
ate and to detect paraphrases. It could be used for
instance, in combination with the parser and the se-
mantic construction module described in (Gardent
and Parmentier, 2005), to support textual entailment
recognition or answer detection in question answer-
ing.
Reversible realisers. The realiser presented here
differs in mainly two ways from existing reversible
realisers such as (White, 2004)?s CCG system or
the HPSG ERG based realiser (Carroll and Oepen,
2005).
First, it permits a symbolic selection of the out-
put paraphrase. In contrast, existing reversible re-
alisers use statistical information to select from the
produced output the most plausible paraphrase.
Second, particular attention has been paid to the
treatment of paraphrases in the grammar. Recall
that TAG elementary trees are grouped into families
and further, that the specific TAG we use is com-
piled from a highly factorised description. We rely
on these features to associate one and the same se-
mantic to large sets of trees denoting semantically
equivalent but syntactically distinct configurations
(cf. (Gardent, 2006)).
7 Conclusion
The realiser presented here, GENI, exploits a gram-
mar which is produced semi-automatically by com-
piling a high level grammar description into a Tree
Adjoining Grammar. We have argued that a side-
effect of this compilation process ? namely, the as-
sociation with each elementary tree of a set of tree
properties ? can be used to constrain the realiser
output. The resulting system combines the advan-
tages of two orthogonal approaches. From the re-
versible approach, it takes the reusability, the ability
to rapidly create very large test suites and the capac-
ity to both generate and detect paraphrases. From
the NLG geared paradigm, it takes the ability to
symbolically constrain the realiser output to a given
generation context.
GENI is free (GPL) software and is available at
http://trac.loria.fr/?geni.
References
Charles B. Callaway. 2003. Evaluating coverage for large sym-
bolic NLG grammars. In 18th IJCAI, pages 811?817, Aug.
J. Carroll and S. Oepen. 2005. High efficiency realization for a
wide-coverage unification grammar. 2nd IJCNLP.
B. Crabbe? and D. Duchier. 2004. Metagrammar redux. In
CSLP, Copenhagen.
M. Elhadad and J. Robin. 1999. SURGE: a comprehensive
plug-in syntactic realization component for text generation.
Computational Linguistics.
C. Gardent and L. Kallmeyer. 2003. Semantic construction in
FTAG. In 10th EACL, Budapest, Hungary.
C. Gardent and E. Kow. 2005. Generating and selecting gram-
matical paraphrases. ENLG, Aug.
C. Gardent and Y. Parmentier. 2005. Large scale semantic con-
struction for Tree Adjoining Grammars. LACL05.
C. Gardent. 2006. Integration d?une dimension semantique
dans les grammaires d?arbres adjoints. TALN.
M. Kay. 1996. Chart Generation. In 34th ACL, pages 200?204,
Santa Cruz, California.
A. Koller and K. Striegnitz. 2002. Generation as dependency
parsing. In 40th ACL, Philadelphia.
I. Langkilde-Geary. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence genera-
tor. In Proceedings of the INLG.
B. Lavoie and O. Rambow. 1997. RealPro?a fast, portable
sentence realizer. ANLP?97.
C. Matthiessen and J.A. Bateman. 1991. Text generation
and systemic-functional linguistics: experiences from En-
glish and Japanese. Frances Pinter Publishers and St. Mar-
tin?s Press, London and New York.
I.A. Mel?cuk. 1988. Dependency Syntax: Theorie and Prac-
tice. State University Press of New York.
Erik Velldal and Stephan Oepen. 2006. Statistical ranking in
tactical generation. In EMNLP, Sydney, Australia.
K. Vijay-Shanker and AK Joshi. 1988. Feature Structures
Based Tree Adjoining Grammars. Proceedings of the 12th
conference on Computational linguistics, 55:v2.
M. White. 2004. Reining in CCG chart realization. In INLG,
pages 182?191.
G. Yang, K. McKoy, and K. Vijay-Shanker. 1991. From func-
tional specification to syntactic structure. Computational In-
telligence, 7:207?219.
335
Generating and selecting grammatical paraphrases
Claire Gardent
CNRS/LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow
INRIA/LORIA
Nancy, France
eric.kow@loria.fr
Abstract
Natural language has a high paraphrastic power yet
not all paraphrases are appropriate for all contexts.
In this paper, we present a TAG based surface re-
aliser which supports both the generation and the
selection of paraphrases. To deal with the combi-
natorial explosion typical of such an NP-complete
task, we introduce a number of new optimisations
in a tabular, bottom-up surface realisation algo-
rithm. We then show that one of these optimisations
supports paraphrase selection.
1 Introduction
As is well known, natural language has a very high paraphras-
tic power so that the same core meaning can be expressed in
many different ways [Gross, 1975; Mel?c?uk, 1988]. Yet not
all paraphrases are appropriate for all contexts. So for in-
stance, a sentence and its converse (1a) express the same core
meaning and so can be considered paraphrases of each other.
Yet as example (1b) illustrates, they are not interchangeable
in the context of a control verb:
(1) a. John borrowed a book from Mary.
?Mary lent a book to John
b. Peter persuaded John to borrow a book from Mary.
6? Peter persuaded Mary to lend a book to John
Similarly, a canonical and a cleft sentence (2a) communi-
cate the same core meaning yet a contrastive context (2b) only
admits the cleft version.
(2) a. John looks at Mary.
? It is Mary that John looks at
b. * It is not Sarah, John looks at Mary.
It is not Sarah, it is Mary that John looks at
More generally, the anaphoric potential (that is, the dis-
course status of the entities being talked about) of the pre-
ceding discourse, its structure, the presence of an embedding
verb or of a given subordinating or coordinating conjunction
are all factors which may restrict the use of paraphrases. To
preserve completeness, it is therefore important that a gener-
ator be able to produce paraphrases in a systematic fashion.
On the other hand, it is also well known that surface real-
isation (the task of producing the set of sentences associated
by a grammar with a given semantic representation) is NP-
complete [Brew, 1992].
In this paper, we present a TAG based surface realiser
which supports both the generation and the selection of gram-
matical paraphrases (section 2 and 3). To deal with the re-
sulting combinatorics, we introduce a number of new opti-
misations (section 4). We then show how one of these op-
timisations can be used to support the selection of contextu-
ally appropriate paraphrases (section 5). Finally, we relate
our approach to similar proposals and show that it compares
favourably in terms of efficiency (section 6 and 7).
2 The grammar
The grammar used by the surface realiser is Feature-based
TAG, a unification based version of Tree Adjoining Gram-
mar. Briefly1, a Feature-based TAG consists of a set of (aux-
iliary or initial) elementary trees and of two tree composition
operations: substitution and adjunction. Substitution inserts a
tree onto a leaf node of another tree2 while adjunction inserts
an auxiliary tree into a derived tree (i.e., either an elementary
tree or a tree resulting from the combination of two trees). In
an FTAG, each tree node which is not a substitution node is
associated with two feature structures called top and bottom
and during derivation, the following unifications take place.
? The adjunction at some node X with top features tX
and bottom features bX , of an auxiliary tree with root
top features r and foot bottom features f entails the
unification of tX with r and of bX with f .
? The substitution at some node X with top features tX
of a tree with root top features t entails the unification
of tX with t.
? At the end of a derivation, the top and bottom features
of all nodes in the derived tree are unified.
1For more details on FTAG see [Vijay-Shanker and Joshi, 1988].
2Leaf nodes where substitution can take place are graphically
distinguished by a down arrow.
In the FTAG used by the surface realisation algorithm, lin-
guistic expressions are associated with semantic representa-
tions as advocated in [Gardent and Kallmeyer, 2003]. The se-
mantic representations used are flat semantic representations
in the sense of [Copestake et al, 2001] and the semantic pa-
rameters (that is, the semantic indices representing the miss-
ing arguments of the semantic functors) are represented by
unification variables.
Further, each elementary tree is associated with a semantic
representation of the type just described and the appropriate
nodes of the elementary trees are decorated with semantic in-
dices or parameters.
More precisely, the substitution nodes of the tree associated
with a semantic functor will be associated with semantic pa-
rameters whilst root nodes and certain adjunction nodes will
be labelled with semantic indices. As trees are combined,
semantic parameters and semantic indices are unified by the
FTAG unification mechanism thus specifying which semantic
index provides the value for which semantic parameter.
Generally, the idea is that the association between tree
nodes and unification variables encodes the syntax/seman-
tics interface: it specifies which node in the tree provides the
value for which semantic parameter in the semantic represen-
tation of a semantic functor. So for instance, the trees for
John, loves and Mary will be as given in Figure 1. The tree for
loves is associated with a semantic representation including
the two semantic parameters x and y. These parameters also
label the subject and the object substitution nodes of this tree.
Conversely, the root node of the tree for John is labelled with
the semantic index j. If the string parsed is John loves Mary,
this tree will be substituted at the subject substitution node of
the loves tree thus instantiating the semantic parameter x to j.
And similarly, for the Mary tree.
S
NP?x VP
NPj V NP?y NPm
John loves Mary
name(j,john) love(x,y) name(m,mary)
? love(j,m),name(j,john),name(m,mary)
Figure 1: John loves Mary
Coverage. The grammar used describes a core fragment for
French and contains around 4 000 trees. It covers some 35
basic subcategorisation frames and for each of these frames,
the set of argument redistributions (active, passive, middle,
reflexivisation, impersonal, passive impersonal) and of argu-
ment realisations (cliticisation, extraction, omission, permu-
tations, etc.) possible for this frame. As a result, it captures
most grammatical paraphrases that is, paraphrases due to di-
verging argument realisations or to different meaning pre-
serving alternation (e.g., active/passive or clefted/non clefted
sentence).
3 The basic algorithm
The basic surface realisation algorithm used is summarised in
Figure 1 (appendix). It is a bottom up, tabular algorithm [Kay,
1996] optimised for TAGs. Its workings can be illustrated by
the following example. Suppose that the input semantics is
the following :
{camp(s,j),john(j),in(s,l),paris(l)}
Then the algorithm proceeds as follows. In a first step (lex-
ical selection), the elementary trees whose semantics sub-
sumes3 part of the input semantics are retrieved and added
to the agenda. In our simple example, the selected trees are
the trees for Jean, campe, dans and paris.
The second step (the substitution phase) consists in sys-
tematically exploring the possibility of combining two trees
by substitution. It is summarised for our example by the ta-
ble in figure 2 where each line corresponds to a processing
step. The words in each column indicate the trees present at
each step in the chart, the agenda and the agenda for auxiliary
trees (AgendaA). The combination column indicates which
tree combines with which tree by means of which operation
(? indicates a substitution, ? an adjunction). The trees result-
ing from such a combination are represented using the con-
catenation of the names of the combined trees (jeanCampe is
the tree resulting from the combination of the tree anchored
by Jean with that anchored by campe). Thus, the first line in-
dicates that the trees anchored by Jean, campe, dans and Paris
are in the agenda and that the chart is empty. The second line
shows that the next state is a state where the tree anchored
by Jean has been retrieved from the agenda and added to the
chart. The third line indicates that when the trees anchored by
campe and Jean are in the chart, they can be combined using
substitution. The result is added to the agenda etc.
More generally, the items are retrieved one by one from
the agenda to be added either to the chart or to the auxiliary
agenda (in the case of an auxiliary tree devoid of substitution
node). For each item added to the chart, all possible substitu-
tions are carried out and the resulting derived trees are added
to the agenda. The loop ends when the agenda is empty.
At this stage, all the items containing an empty substitution
node are erased from the chart (here, the trees anchored by
campe and dans are erased). The agenda is then reinitialised to
the content of the chart and the chart to the content of the aux-
iliary agenda. The third step (the adjunction phase) occurs
then in which all possible adjunctions are performed (figure
3). Finally (retrieval phase), the strings labelling the items in
the chart whose semantics is the input semantics are printed
3Subsumption is here taken to denote term unification. Hence
lexical selection is done on a very ?syntactic? basis: only these lexi-
cal entries whose semantics representation matches part of the input
semantics are selected. This is partly alleviated by taking lexical
synonymy into account while developing the grammar so that two
(intra- or inter-categorical) synonyms are assigned the same seman-
tic representation. A more complete treatment would require the in-
tegration either of a richer lexical semantics or of a lexical selection
module permitting inference so that for instance ?adult(x) male(x)
human(x)? can be inferred to be denoted by the word ?man?.
Agenda Chart Combination AgendaA
Jean,campe,dans,Paris
campe,dans,Paris Jean
dans,Paris campe,Jean ?(campe,Jean)
Paris,JeanCampe campe,Jean,dans
JeanCampe campe,Jean,dans,Paris ?(dans,Paris)
dansParis campe,Jean,dans,Paris,JeanCampe
campe,Jean,dans,Paris,JeanCampe dansParis
Figure 2: Sample run of the substitution phase
out, which in this case yields the sentence Jean campe dans
Paris.
4 Optimisations
Surface realisation is NP complete [Brew, 1992]. More-
over the paraphrastic power of natural language is enormous
[Gross, 1975; Mel?c?uk, 1988]. Hence optimisation is a key
issue and so is the possibility to select a given paraphrase
on demand. We now present a number of optimisations we
added to the algorithm just described in order to reduce the
combinatorics.
4.1 Tabulation and ordered combinations
Tabulation serves to avoid redundant computations. In analy-
sis, the use of the chart to store intermediate constituents and
avoid multiple computation of the same structure renders an
exponential task polynomial. In generation however, tabula-
tion increases efficiency by avoiding duplicate computations
but the complexity remains exponential because in particular
of multiple modifiers [Brew, 1992]. Suppose for instance
that the input semantic representation is the following:
fierce(x),little(x),cat(x),black(x)
For this input, a naive bottom-up realisation algorithm will
generate all intermediate structures that is, n! intermediate
structures with n the number of modifiers. These n! struc-
tures will furthermore be multiplied by the context so that for
instance given the input for the fierce little black cat runs, the
following structures will all be generated.
(3) a. fierce cat, fierce black cat, little cat,little black cat, fierce
little cat, black cat
b. the fierce cat, the fierce black cat, the little cat, the little
black cat, the fierce little cat, the black cat
c. the fierce cat runs, the fierce black cat runs, the little cat
runs, the little black cat runs, the fierce little cat runs, the black
cat runs
To minimise the impact of multiple modifiers, the algo-
rithm presented here performs all substitutions before con-
sidering adjunctions. In effect, this means that adjunction
only applies to syntactically complete trees and so that the
many intermediate structures induced by the modifiers do not
multiply out with other incomplete structures. In the above
example for instance, (3c) will be computed but neither (3a)
nor (3b).
4.2 Avoiding spurious derivations
Categorical grammars often allow so called spurious deriva-
tions in that one and the same syntactic structure can be
derived in several different ways [Hepple, 1991]. TAGs also
induce spurious derivations due to the fact that substitutions
and adjunctions on different nodes of the same tree can be
carried out in different relative orders all of which result in
one and the same structure. Thus for instance, given the
trees np(Marie), np(Jean), s(np?, v(aime), np?)
and the semantic aime(j,m),jean(j), marie(m), two
derivations are possibles, one where np(Jean) is first
substituted in s(np?, v(aime), np?) before the tree for
np(Marie) is ; and the other where np(Marie) is first
substituted before np(Jean) is added. More generally, for a
tree containing n substitution nodes, there will be n! possible
derivations. For instance given the sentence
(4) Jean persuade Marie de promettre a` Claire de donner un
livre a` Marie.
Jean persuades Mary to promise Claire to give Mary a
book
there will be 3! ? 2! ? 2! = 24 possible derivations all
of them produce the same syntactic tree and hence the same
sentence.
Adjunction suffers from the same shortcoming. Given a
TAG tree and n auxiliary trees that can adjoin to different
nodes of that tree, there are n! possible ways of deriving the
tree resulting from these n adjunctions.
To avoid these spurious derivations, we impose a unique
order (from left to right) on the sequences of substitutions
and adjunctions done within a given tree. Because the al-
gorithm systematically examines all pairs of items, this re-
striction does not affect completeness : the unique derivation
supported by the imposed constraints will be taken into con-
sideration by the algorithm and will therefore be produced.
A third source of spurious derivations come from the pos-
sibility of having multiple adjunctions on the same node of a
given tree for instance in the case of the little black cat. The
auxiliary trees anchored by little and black can adjoin in two
different orders on the tree anchored by cat: either little is ad-
joined to cat and black to the foot node of little in the resulting
tree or black is adjoined to cat and little to the root node of
the resulting derived tree. To eliminate this type of spurious
derivations, adjunction on a foot node is ruled out ? which is
usual in TAG parsers.
Agenda Chart Combination AgendaA
Jean,Paris,JeanCampe dansParis
Paris,JeanCampe dansParis,Jean
JeanCampe dansParis,Jean,Paris
dansParis,Jean,Paris,JeanCampe ?(JeanCampe,dansParis)
JeanCampeDansParis dansParis,Jean,Paris,JeanCampe
dansParis,Jean,Paris,JeanCampe,
JeanCampeDansParis
Figure 3: Sample run of the adjunction phase
4.3 Filtering of valid lexical sequences
The most efficient optimisation takes place between the lex-
ical selection phase and that of combination by substitution
and adjunction. At this stage, the number of combinations
that are a priori possible is
?
1?i?n ai with ai the degree of
lexical ambiguity of the i-th literal and n, the number of lit-
erals in the input semantic. That is, the search space is expo-
nential in the number of literals. To reduce the combinatorics,
we use a technique introduced for parsing by [Perrier, 2003]
called polarity based filtering.
Polarity based filtering is based on the observation that
many of the combinations of lexical items which cover the in-
put semantics are in fact syntactically invalid either because
a syntactic requirement is not fulfilled or because a syntac-
tic resource is not used. Accordingly, polarity based filtering
detects and eliminates such combinations by:
1. assigning each lexical item a polarity signature reflecting
its syntactic requirements and resources
2. computing for each possible combination of lexical
items the net sum of its syntactic requirements and re-
sources and
3. eliminating all combinations of lexical items that do not
have a net sum of zero (because such combinations can-
not possibly lead to a syntactically valid sentence)
As we shall see below, polarity based filtering is imple-
mented using finite state techniques.
Let us see how this works by running through a simple
example. Suppose that the input semantic representation is:
(5) buy(e,t,j), annoying(e), field(t),john(j)
and that the TAG trees selected for this input are the ones
given in Figure 8 (appendix).
In this figure, the literals following the tree name give the
polarities that are automatically assigned to each of these
trees on the basis of their root and substitution nodes (for in-
stance, the v achete has polarity (+p ? 2n) meaning that it
provides a sentence and requires two NPs). Since in a TAG,
substitution nodes indicates syntactic requirements whilst an
initial tree permits fulfilling a syntactic requirement, polarity
signatures can be automatically computed as follows:
? a polarity of the form +C is added to the tree polarity
signature of each initial tree with root node category C.
? a polarity of the form ?S is added to the tree polarity
signature of each initial tree for each substitution node
with category S in that tree.
Now we need to compute the polarity of all possible com-
binations of lexical items. This is done by:
1. building a polarity automaton for each polarity category
occurring in the set of possible combinations (in this
case, n and s),
2. computing the intersection of these automata and
3. minimising the resulting automaton.
In the final automaton, only the combinations that have a
null polarity are represented. These will be the combinations
actually explored by the realiser.
For the above example, the final automaton is that given in
figure 9 where each state is labelled with the cumulated polar-
ity of the path(s) leading to that state and where the transitions
are labelled with the lexical item covered. As can be seen, the
combinations that are syntactically invalid (in grey in the au-
tomaton) have been eliminated. Thus in particular, the com-
bination of the predicative tree n0Vadj with the verb ache`te
and its two complements is ruled out (as the n requirement of
n0Vadj cannot be satisfied) and conversely, the combination
of the predicative tree p0Vadj with the relational noun achat
(because the p requirement of p0Vadj cannot be satisfied)4 .
4.4 Combining polarity based filtering and
tabulation
To preserve the factorisation supported by the use of a chart,
polarity filtering must furthermore be integrated with the re-
alisation algorithm. Indeed, each path through a polarity au-
tomaton represents a combination of lexical items whose total
semantics is the input semantics and which may lead to a syn-
tactically valid expression. But some of these paths may share
some subpath(s). To avoid computing these shared subpaths
several times, each selected elementary tree is annotated with
4For lack of space, we ignore here functional words (determiners,
prepositions). In the full algorithm, their treatment is implemented
either by means of co-anchors (a verb whose comple?ment requires a
given preposition for instance, will be assigned a tree with multiple
anchors, one for the verb, the other for the preposition) or by means
of a richer semantic (contrary to what is shown here, a quantifier will
have a non nul semantics). Note further that lexical items with multi-
literal semantics are also handled as well as items whose semantics
is reduced to an index (pronouns, control verb subject, modifiers,
etc.).
the set of automaton paths it occurs in. During realisation,
two items will be compared only if the intersection of their
path sets is not empty (they appear in the same automaton
path). The result of a combination is labelled with the in-
tersection of the labels of the combined constituents. In this
way, the elementary items appearing in several paths of the
automaton are only introduced once in the chart and the fac-
torisation of both elementary and derived items that are com-
mon to several automaton path is ensured.
5 Paraphrase selection
As pointed out in the introduction, not all paraphrases are ap-
propriate in all contexts. To test the ability to generate contex-
tually appropriate sentences, we augmented the realiser with
a paraphrase selection mechanism based on the polarity fil-
tering system described in section (4.3). For instance, it is
possible to select from among the possible realisations for
regarde(j,m), jean(j), marie(m), the variant where
jean is verbalised as a cleft subject namely, C?est Jean qui re-
garde Marie (It is John who is looking at Mary).
More generally, the selection constraints allowed are
syntactico-semantic constraints of the form Synt:SemIndex
where Synt is a morpho-syntactic feature (declarative, inter-
rogative, cleft, pronoun, etc.) and SemIndex is an index oc-
curring in the input semantics.
Intuitively, a selection constraint supports the selection,
for a given semantic index, of the variant(s) obeying the
syntactico-semantic constraint set by the selection constraint
for that index.
Formally, these constraints are imposed during the polarity
filtering phase as follows. The syntactic properties supported
by the selection constraints are automatically associated dur-
ing grammar compilation to the elementary trees of the gram-
mar by means of so-called hypertags [Kinyon, 2000]. This is
made possible by the fact that the TAG used is derived from
a metagrammar [Crabbe? and Duchier, 2004] that is, from a
highly factorised way of representing the linguistic concepts
encoded in the TAG trees. Roughly, the metagrammar for-
malism is used (i) to define abstractions over these concepts
and (ii) to combine these abstractions so as to produce the el-
ementary trees of a TAG. During the metagrammar compila-
tion process, a so-called hypertag is built for each tree which
records the abstractions used to produce that tree. Thus hy-
pertags contain detailed information about the linguistic con-
tent of the TAG elementary trees. In particular, the hypertag
of the tree with clefted subject of the n0vn1 family (i.e., the
set of verbs taking two nominal arguments) will contain the
property +cleft:X where X is the semantic index associated
with the subject node.
During lexical selection, this index is instantiated by unifi-
cation with the input so that the selected elementary tree for
regarde will have the property +cleft:j.
Conversely, a restrictor is a property that a lexical item in-
tervening in the production of the generated paraphrases must
have. In the above example, the restrictor is -cleft:jmean-
ing that the j index must be realised by a clefted structure.
Paraphrase selection is implemented by parameterising the
realiser with a restrictor (for instance, -cleft:j). This re-
strictor is then used to initialise the polarity automaton and
eliminate (by polarity filtering) all these combinations which
do not contain the +cleft:j charge (since the negative
charge introduced during initialisation must be cancelled). As
a result, the realiser will only produce the variant:
(6) C?est Jean qui regarde Marie.
More generally, the polarity mechanism permits selecting
paraphrases on the basis of the information contained in the
grammar hypertags or in the TAG tree features. This infor-
mation, which is decided upon by the grammar writer, can be
both fine grained and of different natures.
Feature values can be used to control the feature values
associated with the root node of the constructed tree, typically
requiring that it is of interrogative, declarative or imperative
mood.
Hypertags can be used more generally to control the selec-
tion of the lexical items entering in the generated construct.
Importantly, the information they contain can be specified
both at the grammar and at the lexical level so that para-
phrase selection can then operate both on features determined
by syntax and on lexically determined characteristics (level
of speech, modality, type of semantic relation, thematic and
fore/backgrounding structure, etc;).
6 Implementation and Experimentation
The realiser described here has been implemented in Haskell.
It includes a graphical interface as well as a debugger so that
the user can inspect the content of the chart and of the agenda
at each step of the algorithm. It also supports batch process-
ing thus permitting a systematic evaluation of the impact of
various optimisations combinations. In what follows, we dis-
cuss the effect of polarity filtering and of paraphrase selection
in that system.
6.1 The effect of polarity filtering
To get an estimate of how our realiser compares with exist-
ing published results, we revisited the test cases discussed
in [Carroll et al, 1999] and [Koller and Striegnitz, 2002] by
producing similar sentences in French namely (7a) and (7b).
(7) a. Le directeur de ce bureau auditionne un nouveau consul-
tant d?Allemagne (The manager in that office interviews a
new consultant from Germany)
b. Le directeur organise un nouveau seminaire d?equipe heb-
domadaire special (The manager organises an unusual ad-
ditional weekly departmental conference).
The grammar used contains 2063 trees. In this grammar,
the verb organiser is associated with 107 trees and adjectives
with 8 trees. For the purpose of efficiency testing, we fur-
thermore treated the PP d?e?quipe as an adjective. As a result,
there are 107 ? 8 (856) combinations of lexical items cover-
ing the input semantics for example (7a) while for example
(7b), this number is 107? 84. The effect of polarity filtering
for these two examples is summarised in the following table.
That is, polarity filtering reduces the number of lexical
items combinations actually explored from 856 to 55 in the
first case and from 438 272 to 232 in the second.
Example 7a Example 7b
Possible combinations 856 438 272
Combinations explored 55 232
Sentences (w/o selection) 9 216
Figure 4: Filtering out combinations
Note furthermore that despite the overhead introduced by
the construction of the polarity automaton, polarity filtering
reduces overall processing times (cf. Figure 5).
Optimisations Example 7a Example 7b
none 14.8 s 93.8 s
pol 0.8 s 14.7 s
Carroll 1.8 s 4.3 s
Koller 1.4 s 0.8 s
Figure 5: Processing times
Thus, for the examples considered, processing times are
reduced by 95% and 84% respectively. The processing times
for (7a) compares favourably with those published for both
the Carroll et al and the Koller and Striegnitz realisers. This
comparison is not all that meaningful, however, since we are
using different grammars and significantly faster computers,
a 3 Ghz Pentium IV to the 700 Mhz Pentium III in [Koller
and Striegnitz, 2002].
Indeed, the poor performance of our surface realiser in ex-
ample (7b) is directly related to the degree of lexical ambi-
guity in our grammar. As illustrated in section 4.1, input se-
mantics with multiple modifiers pose a problem for surface
realisers. Although performing adjunction separately from
substitution prevents this problem from spilling over into in-
complete structures, the fact remains that n translate to n!
structures. Further aggravating the situation is that our gram-
mar provides 8 trees for every adjective, leading to 85?5!, or
3.9 million possible structures. When we modified our gram-
mar to only have one tree per adjective, our realisation times
dropped to 9s without filtering and 2.7s with. This exam-
ple calls to attention the fact that polarity filtering does not
account for lexical ambiguity in modifiers. In section 7, we
suggest some potential mechanisms for dealing with modi-
fiers, which we expect to be complementary to the filtering
technique.
6.2 Paraphrase selection
Paraphrase selection permits reducing the combinatorics one
step further. Thus introducing a cleft restrictor for examples
(7a) and (7b), causes the generator to produce fewer results,
2 sentences instead of 9 in the first example, and 18 instead
of 54 in the second.
These figures can be explained as follows. The grammar
allows 9 syntactic structures for the input considered namely:
(8) a. C?est par le directeur de ce bureau qu?un nouveau
consultant d?Allemagne est auditionne?
b. C?est le directeur de ce bureau qui auditionne un
nouveau consultant d?Allemagne
c. C?est un nouveau consultant d?Allemagne
qu?auditionne le directeur de ce bureau
d. C?est un nouveau consultant d?Allemagne que le
directeur de ce bureau auditionne
e. C?est un nouveau consultant d?Allemagne qui est
auditionne? par le directeur de ce bureau
f. Le directeur de ce bureau auditionne un nouveau
consultant d?Allemagne
g. Un nouveau consultant d?Allemagne est auditionne?
par le directeur de ce bureau
Since for the moment the grammar places no constraints on
the respective order of modifiers, there are 9 possible realisa-
tions for example (7a) and 9 ? 3! for example (7b). With the
object cleft restrictions on ?consultant?, these numbers drop
to 2 for the first example and to 2? 3! for the second.
Example 7a Example 7b
Sentences (w/o selection) 9 54
Sentences (with selection) 2 18
Figure 6: Selection
Accordingly, the processing time drops by 63% and 88%
with respect to simple polarities (cf. Figure 7).
Optimisations Example 7a Example 7b
none 14.8 s 93.8 s
pol 0.8 s 14.7 s
pol + select 0.3 s 1.8 s
Figure 7: Polarity + Selection
7 Related approaches
Several recent papers focus on improving the efficiency of
surface realisation. In this section, we relate our approach to
the HPSG based approach presented in [Carroll et al, 1999],
to the statistical and semi-statistical strategies used in [Ban-
galore and Rambow, 2000] and in [White, 2004] and to the
constraint based approach described in [Koller and Striegnitz,
2002]. We also briefly relate it to the greedy strategy used in
[Stone et al, 2003].
7.1 Copestake et al?s HPSG approach
As mentioned in section 4.1, multiple modifiers may trig-
ger an exponential number of intermediate structures. The
?adjunction after substitution? idea is inspired from the pro-
posal made in [Carroll et al, 1999] that a complete syntactic
skeleton be built before modifiers be inserted into that tree.
Because the Carroll et al proposal is set within the HPSG
framework however, extracted modifiers as in Which office
did John work in? need specific treatment. In contrast, in
TAG, all modifiers are treated using adjunction so that no spe-
cific treatment is required. All that is needed is that adjunc-
tion only be applied after all possible substitutions have been
carried out. A second, more meaningful difference is that no
such global optimisation as polarity filtering is proposed to
filter out on the basis of global information about the sets of
possible combinations, a priori invalid ones.
7.2 Statistical approaches
Interestingly, [White, 2004] proposes a treatment of modifiers
which is in some sense the converse of the ?adjunction after
substitution? treatment and where complete NPs are first built
before they are combined with the verb. This second option is
also feasible in TAG (adjunction would then apply on specific
sets of lexical entries and the results combined with the verb)
and it would be interesting to experiment and compare the
relative efficiency of both approaches within the TAG frame-
work.
Both approaches isolate the addition of modifiers to a con-
stituent, thereby avoiding spurious combinations with unre-
lated constituents; but neither directly address the fact that are
still an exponential n! ways to combine any n modifiers for
a single constituent. [White, 2004] and [Bangalore and Ram-
bow, 2000] propose statistical solutions to this problem based
on a linear n-gram language model. In White?s approach the
statistical knowledge is used to prune the chart of identical
edges representing different modifier permutations, e.g., to
choose between fierce black cat and black fierce cat. Bangalore
assumes a single derivation tree that encodes a word lattice (a
{fierce black, black fierce} cat), and uses statistical knowledge
to select the best linearilisation. Our framework does not cur-
rently implement either approach, but we hope to adopt an ap-
proach similar to Bangalore?s. Rather than directly perform-
ing adjunction, we associate each node with the set of auxil-
iary trees (modifiers) that are to be adjoined to that node. The
order in which these modifiers are adjoined can be decided
through statistical methods.
There are three other uses for probabilistic techniques: for
lexical selection, optimisation and ranking. Such techniques
are useful for guiding the surface realiser towards a single
best result (or a relatively small number thereof). On the
other hand, we aim to produce all possible paraphrases, that
is explore the entire search space of syntactic variants, and
so with the exception of modifier ordering, we eschew the
use of probabilities in favour of an ?exact method? [G. Bon-
fante, 2004]. While Bangalore uses a tree model to produce
a single most probable lexical selection, we use polarities to
filter out all the definitely impossible ones. While in White?s
system, the best paraphrase is determined on the basis of n-
gram scores that is, on the basis of frequency, in our approach
?best? means ?most contextually appropriate?. Indeed, the
restrictors we use to select a paraphrase, although they are
here given by hand, could equally be set by the context and
so permit modelling the effect of contextual constraints on
paraphrases. We believe that our approach, modulo statis-
tical handling of modifiers, would be roughly equivalent to
White?s with anytime-searching disabled.
7.3 Koller et al?s constraint-based approach
Finally, our approach has interesting connections to the
constraint-based approach proposed by [Koller and Strieg-
nitz, 2002]. In this approach, the subset of the TAG grammar
which is used for a given realisation task is translated into a
set of lexical entries in a dependency grammar defining well
formed TAG derivation trees. This set of entries is then parsed
by an efficient constraint-based dependency parser thus pro-
ducing the derivation trees associated by the grammar with
the set of input lexical entries. A post processing phase pro-
duces the derived trees on the basis of the derivation trees
output by the first step.
The main similarity between this and our approach is that
they both use a global mechanism for filtering out combina-
tions of lexical entries that cannot possibly lead to a syntac-
tically valid sequences. In the Koller et al approach, this
filtering is based on well formed derivation trees (only these
combinations of lexical entries that form a valid derivation
tree are considered) whereas in ours, it is based on polarities
and on the cancelling out of syntactic resources and require-
ments. As a preliminary evaluation shows, such a global op-
timisation is very efficient in pruning the search space.
There are differences though. In particular, while Koller et
al. explicitly ignores feature information, our algorithm han-
dles a TAG with fully specified feature structures. Further
while in our approach, the processing of the valid combina-
tions is done using a tabular algorithm optimised to avoid spu-
rious derivations, the postprocessing step producing derived
trees from derivation trees is left undefined in the Koller et al
approach. Finally, while the Koller et al approach is based
on constraint propagation, ours is based on finite state tech-
niques. These differences open up the door for interesting
comparisons and combinations. It would be interesting for
instance to combine the Koller et alapproach with the tab-
ular surface realisation algorithm presented in this paper, or
to compare run times once feature structures are taken into
account.
7.4 Stone?s greedy approach
[Stone et al, 2003] presents a greedy approach to TAG based
surface realisation. The greedy search applies iteratively to
update a single state in the search space. On each iteration,
all neighbours of the current state are produced but only one
state is chosen at the next current state, based on a heuristic
evaluation.
[Stone et al, 2003]?s search strategy is therefore markedly
different from ours. While we explore the entire search
space and use polarities to control the combinatorics, Stone?s
greedy strategy is a best first strategy which incrementally
trims the search space using heuristics. In terms of efficiency,
the greedy strategy is of course better. The goals behind the
two approaches are distinct however. Thus while Stone?s ap-
proach aims at modelling the interaction of the various mech-
anisms involved in microplanning, the present proposal is di-
rected towards generating and selecting paraphrases. In par-
ticular, we are interested in using the realiser to debug a para-
phrastic grammar that is, a grammar which alleviates the in-
ference task by assigning paraphrases the same semantics ?
this can only be done by adopting an exhaustive search strat-
egy. More generally, ?exhaustive surface realisation? pro-
vides a natural way to debug grammars and reduce their de-
gree of overgeneration. Since the combinatorics is not only
theoretically (worse case analysis) but also practically very
high, it is worth investigating ways of optimising surface re-
alisers which perform an exhaustive search.
8 Conclusion
We have presented a surface realiser for TAG which is opti-
mised to support the generation of grammatical paraphrases
while also permitting the selection, on the basis of syntactico
semantic constraints, of a particular paraphrase. The most
efficient optimisation proposed concerns polarity filtering, a
global technique that permits the elimination of combinations
of lexical items which cannot possibly lead to a syntactically
valid sentence. While used here for generating with TAG, the
technique is fully general and can be used for parsing [Perrier,
2003] but also for generating with other grammatical frame-
works.
Future work will concentrate on extending the grammar
and the lexicon to other types of paraphrases (in particu-
lar, morphoderivational or cross categorical paraphrases), on
providing a systematic evaluation of the paraphrase selection
mechanism and on using the realiser for the debugging of an
existing TAG for French.
References
[Bangalore and Rambow, 2000] S. Bangalore and O. Ram-
bow. Using TAGs, a tree model and a language model
for generation. In Proceedings of TAG+5, Paris, France,
2000.
[Brew, 1992] C. Brew. Letting the cat out of the bag: Gener-
ation for shake-and-bake MT. In Proceedings of COLING
?92, Nantes, France, 1992.
[Carroll et al, 1999] J. Carroll, A. Copestake, D. Flickinger,
and V. Paznan?ski. An efficient chart generator for (semi-
)lexicalist grammars. In Proceedings of EWNLG ?99,
1999.
[Copestake et al, 2001] A. Copestake, A. Lascarides, and
D. Flickinger. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the 39th
ACL, Toulouse, France, 2001.
[Crabbe? and Duchier, 2004] B. Crabbe? and D. Duchier.
Metagrammar redux. In International Workshop on Con-
straint Solving and Language Processing - CSLP 2004,
Copenhagen, 2004.
[G. Bonfante, 2004] G. Perrier G. Bonfante, B. Guillaume.
Polarization and abstraction of grammatical formalisms as
methods for lexical disambiguation. In Proceedings of
CoLing 2004, 2004.
[Gardent and Kallmeyer, 2003] C. Gardent and
L. Kallmeyer. Semantic construction in ftag. In
Proceedings of the 10th EACL, Budapest, Hungary, 2003.
[Gross, 1975] M. Gross. Me?thodes en syntaxe. Masson,
Paris, 1975.
[Hepple, 1991] M. Hepple. Efficient incremental processing
with categorial grammar. In Proceedings of the 29th ACL,
Berkeley, 1991.
[Kay, 1996] M. Kay. Chart Generation. In 34th ACL, pages
200?204, Santa Cruz, California, 1996.
[Kinyon, 2000] A. Kinyon. Hypertags. In Proceedings COL-
ING, Sarrebruck, 2000.
[Koller and Striegnitz, 2002] A. Koller and K. Striegnitz.
Generation as dependency parsing. In Proceedings of the
40th ACL, Philadelphia, 2002.
[Mel?c?uk, 1988] I. Mel?c?uk. Paraphrase et lexique dans la
the?orie linguistique sens-texte. Lexique, 6:13?54, 1988.
[Perrier, 2003] G. Perrier. Les grammaires d?interaction,
2003. Habilitation a` diriger les recherches en informa-
tique, universite? Nancy 2.
[Stone et al, 2003] M. Stone, C. Doran, B. Webber,
T. Bleam, and M. Palmer. Microplanning with commu-
nicative intentions: the SPUD system. Computational In-
telligence, 19(4):311?381, 2003.
[Vijay-Shanker and Joshi, 1988] K. Vijay-Shanker and
A. Joshi. Feature based tags. In Proceedings of the 12th
ACL, pages 573?577, Budapest, 1988.
[White, 2004] M. White. Reining in CCG chart realization.
In INLG, pages 182?191, 2004.
A Appendix
Algorithm 1 The GenI algorithm
1: procedure GENERATE(Gram,Sem)
2: AgendaA? ?; Agenda? ?; Chart? ?
3: for all trees t ? Gram such that t?s semantics subsumes
Sem do
4: Agenda? Agenda + t
5: end for
6: while Agenda 6= ? do
7: t? any tree ? Agenda
8: delete t from Agenda
9: if t has a foot node and no substitution nodes then
10: AgendaA? AgendaA + t
11: else
12: for all trees c ? Chart which can combine with t
via substitution into a new tree ct do
13: Agenda? Agenda + ct
14: end for
15: Chart? Chart + t
16: end if
17: end while
18: delete from Chart any tree with a substitution node
19: Agenda? Chart
20: Chart? AgendaA
21: while Agenda 6= ? do
22: t? any tree ? Agenda
23: delete t from Agenda
24: if t?s semantics is Sem then
25: return the string corresponding to t
26: else
27: for all trees c ? Chart which can combine with t
via adjunction into a new tree ct do
28: Agenda? Agenda + ct
29: end for
30: end if
31: end while
32: end procedure
buy(e,j,f) annoying(e) field(f)
v ache`te (+p -2n) n0Vadj ennuyeux (+p -n) n field (+n)
Pe
N?j Ve N?f
ache`te
P
N?e V Adj
est ennuyeux
Nf
terrain
john(j)
n achat (+n -2n) p0Vadj ennuyeux (+p -p) n jean (+n)
Ne
N GP GP
achat P N?f P N?j
par de
P
P?e V Adj
est ennuyeux
Nj
jean
Figure 8: Grammar for example 5
Figure 9: A minimised polarity automaton
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 115?120,
Sydney, July 2006. c?2006 Association for Computational Linguistics
SemTAG, the LORIA toolbox for TAG-based Parsing and Generation
Eric Kow
INRIA / LORIA
Universite? Henri Poincare?
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
kow@loria.fr
Yannick Parmentier
INRIA / LORIA
Universite? Henri Poincare?
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
parmenti@loria.fr
Claire Gardent
CNRS / LORIA
615, rue du Jardin Botanique
F-54 600 Villers-Le`s-Nancy
gardent@loria.fr
Abstract
In this paper, we introduce SEMTAG, a
toolbox for TAG-based parsing and gen-
eration. This environment supports the
development of wide-coverage grammars
and differs from existing environments
for TAG such as XTAG, (XTAG-Research-
Group, 2001) in that it includes a semantic
dimension. SEMTAG is open-source and
freely available.
1 Introduction
In this paper we introduce a toolbox that allows for
both parsing and generation with TAG. This tool-
box combines existing software and aims at facili-
tating grammar development, More precisely, this
toolbox includes1:
? XMG: a grammar compiler which supports the
generation of a TAG from a factorised TAG
(Crabbe? and Duchier, 2004),
? LLP2 and DyALog: two chart parsers, one
with a friendly user interface (Lopez, 2000)
and the other optimised for efficient parsing
(Villemonte de la Clergerie, 2005)2
? GenI: a chart generator which has been
tested on a middle size grammar for French
(Gardent and Kow, 2005)
1All these tools are freely available, more information and
links at http://trac.loria.fr/?semtag.
2Note that DyALog refers in fact to a logic program-
ming language, and a tabular compiler for this language. The
DyALog system is well-adapted to the compilation of effi-
cient tabular parsers.
2 XMG, a grammar writing environment
for Tree Based Grammars
XMG provides a grammar writing environment for
tree based grammars3 with three distinctive fea-
tures. First, XMG supports a highly factorised and
fully declarative description of tree based gram-
mars. Second, XMG permits the integration in a
TAG of a semantic dimension. Third, XMG is based
on well understood and efficient logic program-
ming techniques. Moreover, it offers a graphical
interface for exploring the resulting grammar (see
Figure 1).
Factorising information. In the XMG frame-
work, a TAG is defined by a set of classes organised
in an inheritance hierarchy where classes define
tree fragments (using a tree logic) and tree frag-
ment combinations (by conjunction or disjunc-
tion). XMG furthermore integrates a sophisticated
treatment of names whereby variables scope can
be local, global or user defined (i.e., local to part
of the hierarchy).
In practice, the resulting framework supports a
very high degree of factorisation. For instance, a
first core grammar (FRAG) for French comprising
4 200 trees was produced from roughly 300 XMG
classes.
Integrating semantic information. In XMG,
classes can be multi-dimensional. That is, they
can be used to describe several levels of linguis-
tic knowledge such as for instance, syntax, seman-
tics or prosody. At present, XMG supports classes
including both a syntactic and a semantic dimen-
sion. As mentioned above, the syntactic dimen-
3Although in this paper we only mention TAG, the XMG
framework is also used to develop so called Interaction Gram-
mars i.e., grammars whose basic units are tree descriptions
rather than trees (Parmentier and Le Roux, 2005).
115
Figure 1: XMG?s graphical interface
sion is based on a tree logic and can be used to
describe (partial) tree fragments. The semantic di-
mension on the other hand, can be used to asso-
ciate with each tree a flat semantic formula. Such a
formula can furthermore include identifiers which
corefer with identifiers occurring in the associated
syntactic tree. In other words, XMG also provides
support for the interface between semantic formu-
lae and tree decorations. Note that the inclusion of
semantic information remains optional. That is, it
is possible to use XMG to define a purely syntactic
TAG.
XMG was used to develop a core grammar for
French (FRAG) which was evaluated to have 75%
coverage4 on the Test Suite for Natural Language
Processing (TSNLP, (Lehmann et al, 1996)). The
FRAG grammar was furthermore enriched with
semantic information using another 50 classes de-
scribing the semantic dimension (Gardent, 2006).
The resulting grammar (SEMFRAG) describes
both the syntax and the semantics of the French
core constructions.
Compiling an XMG specification. By build-
ing on efficient techniques from logic program-
ming and in particular, on the Warren?s Abstract
4This means that for 75 % of the sentences, a TAG parser
can build at least one derivation.
Figure 2: The LLP2 parser.
Machine idea (Ait-Kaci, 1991), the XMG com-
piler allows for very reasonable compilation times
(Duchier et al, 2004). For instance, the compila-
tion of a TAG containing 6 000 trees takes about 15
minutes with a Pentium 4 processor 2.6 GHz and
1 GB of RAM.
3 Two TAG parsers
The toolbox includes two parsing systems: the
LLP2 parser and the DyALog system. Both of
them can be used in conjunction with XMG. First
we will briefly introduce both of them, and then
show that they can be used with a semantic gram-
mar (e.g., SEMFRAG) to perform not only syntac-
tic parsing but also semantic construction.
LLP2 The LLP2 parser is based on a bottom-
up algorithm described in (Lopez, 1999). It has
relatively high parsing times but provides a user
friendly graphical parsing environment with much
statistical information (see Figure 2). It is well
suited for teaching or for small scale projects.
DyALog The DyALog system on the other
hand, is a highly optimised parsing system based
on tabulation and automata techniques (Ville-
monte de la Clergerie, 2005). It is implemented
using the DyALog programming language (i.e.,
it is bootstrapped) and is also used to compile
parsers for other types of grammars such as Tree
Insertion Grammars.
The DyALog system is coupled with a seman-
tic construction module whose aim is to associate
with each parsed string a semantic representation5.
This module assumes a TAG of the type described
in (Gardent and Kallmeyer, 2003; Gardent, 2006)
5The corresponding system is called SemConst (cf section
6).
116
Figure 3: The SemConst system
where initial trees are associated with semantic in-
formation and unification is used to combine se-
mantic representations. In such a grammar, the se-
mantic representation of a derived tree is the union
of the semantic representations of the trees enter-
ing in the derivation of that derived tree modulo
the unifications entailed by analysis. As detailed
in (Gardent and Parmentier, 2005), such grammars
support two strategies for semantic construction.
The first possible strategy is to use the full
grammar and to perform semantic construction
during derivation. In this case the parser must ma-
nipulate both syntactic trees and semantic repre-
sentations. The advantage is that the approach is
simple (the semantic representations can simply
be an added feature on the anchor node of each
tree). The drawback is that the presence of seman-
tic information might reduce chart sharing.
The second possibility involves extracting the
semantic information contained in the grammar
and storing it into a semantic lexicon. Parsing then
proceeds with a purely syntactic grammar and se-
mantic construction is done after parsing on the
basis of the parser output and of the extracted se-
mantic lexicon. This latter technique is more suit-
able for large scale semantic construction as it sup-
ports better sharing in the derivation forests. It
is implemented in the LORIA toolbox where a
module permits both extracting a semantic lexi-
con from a semantic TAG and constructing a se-
mantic representation based on this lexicon and on
the derivation forests output by DyALog (see Fig-
ure 3).
The integration of the DyALog system into the
toolbox is relatively new so that parsing evaluation
Figure 4: The GenI debugger
is still under progress. So far, evaluation has been
restricted to parsing the TSNLP with DyALog
with the following preliminary results. On sen-
tences ranging from 1 to 18 words, with an aver-
age of 7 words per sentence, and with a grammar
containing 5 069 trees, DyALog average parsing
time is of 0.38 sec with a P4 processor 2.6 GHz
and 1 GB of RAM6.
4 A TAG-based surface realiser
The surface realiser GenI takes a TAG and a flat
semantic logical form as input, and produces all
the sentences that are associated with that logi-
cal form by the grammar. It implements two bot-
tom up algorithms, one which manipulates derived
trees as items and one which is based on Earley for
TAG. Both of these algorithms integrate a number
of optimisations such as delayed adjunction and
polarity filtering (Kow, 2005; Gardent and Kow,
2005).
GenI is written in Haskell and includes a
graphical debugger to inspect the state of the gen-
erator at any point in the surface realisation pro-
cess (see Figure 4). It also integrates a test harness
for automated regression testing and benchmark-
ing of the surface realiser and the grammar. The
harness gtester is written in Python. It runs the
surface realiser on a test suite, outputting a single
document with a table of passes and failures and
various performance charts (see Figures 5 and 6).
Test suite and performance The test suite is
built with an emphasis on testing the surface re-
6These features only concern classic syntactic parsing as
the semantic construction module has not been tested on real
grammars yet.
117
test expected simple earley
t1 il le accepter pass pass
t32 il nous accepter pass pass
t83 le ingnieur le lui apprendre pass DIED
t114 le ingnieur nous le prsenter pass pass
t145 le ingnieur vous le apprendre pass pass
t180 vous venir pass pass
Figure 5: Fragment of test harness output - The
Earley algorithm timed out.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 0  20  40  60  80  100  120  140  160  180  200
ch
art
_s
ize
lex_foot_nodes
chart_size for lex_foot_nodes
simpleearley
Figure 6: Automatically generated graph of per-
formance data by the test harness.
aliser?s performance in the face of increasing para-
phrastic power i.e., ambiguity. The suite consists
of semantic inputs that select for and combines
verbs with different valencies. For example, given
a hypothetical English grammar, a valency (2,1)
semantics might be realised in as Martin thinks
Faye drinks (thinks takes 2 arguments and drinks
takes 1), whereas a valency (2,3,2) one would be
Dora says that Martin tells Bob that Faye likes
music. The suite also adds a varying number of
intersective modifiers into the mix, giving us for
instance, The girl likes music, The pretty scary girl
likes indie music.
The sentences in the suite range from 2 to 15
words (8 average). Realisation times for the core
suite range from 0.7 to 2.84 seconds CPU time
(average 1.6 seconds).
We estimate the ambiguity for each test case
in two ways. The first is to count the number of
paraphrases. Given our current grammar, the test
cases in our suite have up to 669 paraphrases (av-
erage 41). The second estimate for ambiguity is
the number of combinations of lexical items cov-
ering the input semantics.
This second measure is based on optimisation
known as polarity filtering (Gardent and Kow,
2005). This optimisation detects and eliminates
combinations of lexical items that cannot be used
to build a result. It associates the syntactic re-
sources (root nodes) and requirements (substitu-
tion nodes) of the lexical items to polarities, which
are then used to build ?polarity automata?. The
automata are minimised to eliminate lexical com-
binations where the polarities do not cancel out,
that is those for which the number of root and sub-
stitution nodes for any given category do not equal
each other.
Once built, the polarity automata can also serve
to estimate ambiguity. The number of paths in the
automaton represent the number of possible com-
binations of lexical items. To determine how ef-
fective polarity filtering with respect to ambiguity,
we compare the combinations before and after po-
larity filtering. Before filtering, we start with an
initial polarity automaton in which all items are
associated with a zero polarity. This gives us the
lexical ambiguity before filtering. The polarity fil-
ter then builds upon this to form a final automaton
where all polarities are taken into account. Count-
ing the paths on this automaton gives us the am-
biguity after filtering, and comparing this number
with the lexical initial ambiguity provides an es-
timate on the usefulness of the polarity filter. In
our suite, the initial automata for each case have
1 to 800 000 paths (76 000 average). The fi-
nal automata have 1 to 6000 paths (192 average).
This can represent quite a large reduction in search
space, 4000 times in the case of the largest au-
tomaton. The effect of this search space reduc-
tion is most pronounced on the larger sentences or
those with the most modifiers. Indeed, realisation
times with and without filtering are comparable for
most of the test suite, but for the most complicated
sentence in the core suite, polarity filtering makes
surface realisation 94% faster, producing a result
in 2.35 seconds instead of 37.38.
5 Benefits of an integrated toolset
As described above, the LORIA toolbox for TAG
based semantic processing includes a lexicon, a
grammar, a parser, a semantic construction mod-
ule and a surface realiser. Integrating these into
a single platform provides some accrued benefits
which we now discuss in more details.
Simplified resource management The first ad-
vantage of an integrated toolkit is that it facilitates
118
the management of the linguistic resources used
namely the grammar and the lexicon. Indeed it is
common that each NLP tool (parser or generator)
has its own representation format. Thus, manag-
ing the resources gets tiresome as one has to deal
with several versions of a single resource. When
one version is updated, the others have to be re-
computed. Using an integrated toolset avoid such
a drawback as the intermediate formats are hidden
and the user can focus on linguistic description.
Better support for grammar development
When developing parsers or surface realisers, it is
useful to test them out by running them on large,
realistic grammars. Such grammars can explore
nooks and crannies in our implementations that
would otherwise have been overlooked by a toy
grammar. For example, it was only when we ran
GenI on our French grammar that we realised our
implementation did not account for auxiliary trees
with substitution nodes (this has been rectified).
In this respect, one could argue that XMG could al-
most be seen as a parser/realiser debugging utility
because it helps us to build and extend the large
grammars that are crucial for testing.
This perspective can also be inverted; parsers
and surface realiser make for excellent grammar-
debugging devices. For example, one possible
regression test is to run the parser on a suite of
known sentences to make sure that the modified
grammar still parses them correctly. The exact
reverse is useful as well; we could also run the
surface realiser over a suite of known semantic
inputs and make sure that sentences are gener-
ated for each one. This is useful for two reasons.
First, reading surface realiser output (sentences)
is arguably easier for human beings than reading
parser output (semantic formulas). Second, the
surface realiser can tell us if the grammar overgen-
erates because it would output nonsense sentences.
Parsers, on the other hand, are much better adapted
for testing for undergeneration because it is easier
to write sentences than semantic formulas, which
makes it easier to test phenomena which might not
already be in the suite.
Towards a reversible grammar Another ad-
vantage of using such a toolset relies on the fact
that we can manage a common resource for both
parsing and generation, and thus avoid inconsis-
tency, redundancy and offer a better flexibility as
advocated in (Neumann, 1994).
On top of these practical questions, having a
unique reversible resource can lead us further.
For instance, (Neumann, 1994) proposes an inter-
leaved parsing/realisation architecture where the
parser is used to choose among a set of para-
phrases proposed by the generator; paraphrases
which are ambiguous (that have multiple parses)
are discarded in favour of those whose meaning is
most explicit. Concretely, we could do this with a
simple pipeline using GenI to produce the para-
phrases, DyALog to parse them, and a small shell
script to pick the best result. This would only be
a simulation, of course. (Neumann, 1994) goes
as far as to interleave the processes, keeping the
shared chart and using the parser to iteratively
prune the search space as it is being explored by
the generator. The version we propose would not
have such niceties as a shared chart, but the point
is that having all the tools at our disposable makes
such experimentation possible in the first place.
Moreover, there are several other interest-
ing applications of the combined toolbox. We
could use the surface realiser to build artifi-
cial corpora. These can in turn be parsed to
semi-automatically create rich treebanks contain-
ing syntactico-semantic analyses a` la Redwoods
(Oepen et al, 2002).
Eventually, another use for the toolbox might be
in components of standard NLP applications such
as machine translation, questioning answering, or
interactive dialogue systems.
6 Availability
The toolbox presented here is open-source and
freely available under the terms of the GPL7. More
information about the requirements and installa-
tion procedure is available at http://trac.
loria.fr/?semtag. Note that this toolbox is
made of two main components: the GenI8 sys-
tem and the SemConst9 system, which respec-
tively performs generation and parsing from com-
mon linguistic resources. The first is written in
Haskell (except the XMG part written in Oz) and is
multi-platform (Linux, Windows, Mac OS). The
latter is written in Oz (except the DyALog part
which is bootstrapped and contains some Intel as-
sembler code) and is available on Unix-like plat-
7Note that XMG is released under the terms of the
CeCILL license (http://www.cecill.info/index.
en.html), which is compatible with the GPL.
8http://trac.loria.fr/?geni
9http://trac.loria.fr/?semconst
119
forms only.
7 Conclusion
The LORIA toolbox provides an integrated envi-
ronment for TAG based semantic processing: ei-
ther to construct the semantic representation of a
given sentence (parsing) or to generate a sentence
verbalising a given semantic content (generation).
Importantly, both the generator and the parsers
use the same grammar (SEMFRAG) so that both
tools can be used jointly to improve grammar pre-
cision. All the sentences outputted by the surface
realiser should be parsed to have at least the se-
mantic representation given by the test suite, and
all parses of a sentence should be realised into at
least the same sentence.
Current and future work concentrates on de-
veloping an automated error mining environment
for both parsing and generation; on extending the
grammar coverage; on integrating further optimi-
sations both in the parser (through parsing with
factorised trees) and in the generator (through
packing and accessibility filtering cf. (Carroll and
Oepen, 2005); and on experimenting with differ-
ent semantic construction strategies (Gardent and
Parmentier, 2005).
References
H. Ait-Kaci. 1991. Warren?s Abstract Machine: A Tu-
torial Reconstruction. In K. Furukawa, editor, Proc.
of the Eighth International Conference of Logic Pro-
gramming. MIT Press, Cambridge, MA.
J. Carroll and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
In R. Dale and K-F. Wong, editors, Proceedings of
the Second International Joint Conference on Natu-
ral Language Processing, volume 3651 of Springer
Lecture Notes in Artificial Intelligence, pages 165?
176.
B. Crabbe? and D. Duchier. 2004. Metagrammar Re-
dux. In Proceedings of CSLP 2004, Copenhagen.
D. Duchier, J. Le Roux, and Y. Parmentier. 2004. The
Metagrammar Compiler: An NLP Application with
a Multi-paradigm Architecture. In 2nd International
Mozart/Oz Conference (MOZ?2004), Charleroi.
C. Gardent and L. Kallmeyer. 2003. Semantic con-
struction in FTAG. In Proceedings of EACL?03, Bu-
dapest.
C. Gardent and E. Kow. 2005. Generating and select-
ing grammatical paraphrases. ENLG, Aberdeen.
C. Gardent and Y. Parmentier. 2005. Large scale
semantic construction for tree adjoining grammars.
In Proceedings of The Fifth International Confer-
ence on Logical Aspects of Computational Linguis-
tics (LACL05).
C. Gardent. 2006. Inte?gration d?une dimension
se?mantique dans les grammaires d?arbres adjoints.
In Actes de la confe?rence TALN?2006 Leuven.
E. Kow. 2005. Adapting polarised disambiguation
to surface realisation. In 17th European Summer
School in Logic, Language and Information - ESS-
LLI?05, Edinburgh, UK, Aug.
S. Lehmann, S. Oepen, S. Regnier-Prost, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,
E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and
D. Arnold. 1996. TSNLP ? Test Suites for Natural
Language Processing. In Proceedings of COLING
1996, Kopenhagen.
P. Lopez. 1999. Analyse d?e?nonce?s oraux pour le dia-
logue homme-machine a` l?aide de grammaires lex-
icalise?es d?arbres. Ph.D. thesis, Universite? Henri
Poincare? ? Nancy 1.
P. Lopez. 2000. Extended Partial Parsing for
Lexicalized Tree Grammars. In Proceedings of
the International Workshop on Parsing Technology
(IWPT2000), Trento, Italy.
G. Neumann. 1994. A Uniform Computational
Model for Natural Language Parsing and Gener-
ation. Ph.D. thesis, University of the Saarland,
Saarbru?cken.
S. Oepen, E. Callahan, C. Manning, and K. Toutanova.
2002. Lingo redwoods?a rich and dynamic tree-
bank for hpsg.
Y. Parmentier and J. Le Roux. 2005. XMG: an Exten-
sible Metagrammatical Framework. In Proceedings
of the Student Session of the 17th European Summer
School in Logic, Language and Information, Edin-
burg, Great Britain, Aug.
E. Villemonte de la Clergerie. 2005. DyALog: a tabu-
lar logic programming based environment for NLP.
In Proceedings of CSLP?05, Barcelona.
XTAG-Research-Group. 2001. A lexical-
ized tree adjoining grammar for english.
Technical Report IRCS-01-03, IRCS, Uni-
versity of Pennsylvania. Available at
http://www.cis.upenn.edu/?xtag/gramrelease.html.
120
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 230?235,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discrete vs. Continuous Rating Scales for Language Evaluation in NLP
Anja Belz Eric Kow
School of Computing, Engineering and Mathematics
University of Brighton
Brighton BN2 4GJ, UK
{A.S.Belz,E.Y.Kow}@brighton.ac.uk
Abstract
Studies assessing rating scales are very com-
mon in psychology and related fields, but
are rare in NLP. In this paper we as-
sess discrete and continuous scales used for
measuring quality assessments of computer-
generated language. We conducted six sep-
arate experiments designed to investigate the
validity, reliability, stability, interchangeabil-
ity and sensitivity of discrete vs. continuous
scales. We show that continuous scales are vi-
able for use in language evaluation, and offer
distinct advantages over discrete scales.
1 Background and Introduction
Rating scales have been used for measuring hu-
man perception of various stimuli for a long time,
at least since the early 20th century (Freyd, 1923).
First used in psychology and psychophysics, they
are now also common in a variety of other disci-
plines, including NLP. Discrete scales are the only
type of scale commonly used for qualitative assess-
ments of computer-generated language in NLP (e.g.
in the DUC/TAC evaluation competitions). Contin-
uous scales are commonly used in psychology and
related fields, but are virtually unknown in NLP.
While studies assessing the quality of individual
scales and comparing different types of rating scales
are common in psychology and related fields, such
studies hardly exist in NLP, and so at present little is
known about whether discrete scales are a suitable
rating tool for NLP evaluation tasks, or whether con-
tinuous scales might provide a better alternative.
A range of studies from sociology, psychophys-
iology, biometrics and other fields have compared
discrete and continuous scales. Results tend to dif-
fer for different types of data. E.g., results from pain
measurement show a continuous scale to outperform
a discrete scale (ten Klooster et al, 2006). Other
results (Svensson, 2000) from measuring students?
ease of following lectures show a discrete scale to
outperform a continuous scale. When measuring
dyspnea, Lansing et al (2003) found a hybrid scale
to perform on a par with a discrete scale.
Another consideration is the types of data pro-
duced by discrete and continuous scales. Parametric
methods of statistical analysis, which are far more
sensitive than non-parametric ones, are commonly
applied to both discrete and continuous data. How-
ever, parametric methods make very strong assump-
tions about data, including that it is numerical and
normally distributed (Siegel, 1957). If these as-
sumptions are violated, then the significance of re-
sults is overestimated. Clearly, the numerical as-
sumption does not hold for the categorial data pro-
duced by discrete scales, and it is unlikely to be nor-
mally distributed. Many researchers are happier to
apply parametric methods to data from continuous
scales, and some simply take it as read that such data
is normally distributed (Lansing et al, 2003).
Our aim in the present study was to system-
atically assess and compare discrete and continu-
ous scales when used for the qualitative assess-
ment of computer-generated language. We start with
an overview of assessment scale types (Section 2).
We describe the experiments we conducted (Sec-
tion 4), the data we used in them (Section 3), and
the properties we examined in our inter-scale com-
parisons (Section 5), before presenting our results
230
Q1: Grammaticality The summary should have no date-
lines, system-internal formatting, capitalization errors or obvi-
ously ungrammatical sentences (e.g., fragments, missing com-
ponents) that make the text difficult to read.
1. Very Poor
2. Poor
3. Barely Acceptable
4. Good
5. Very Good
Figure 1: Evaluation of Readability in DUC?06, compris-
ing 5 evaluation criteria, including Grammaticality. Eval-
uation task for each summary text: evaluator selects one
of the options (1?5) to represent quality of the summary
in terms of the criterion.
(Section 6), and some conclusions (Section 7).
2 Rating Scales
With Verbal Descriptor Scales (VDSs), partici-
pants give responses on ordered lists of verbally de-
scribed and/or numerically labelled response cate-
gories, typically varying in number from 2 to 11
(Svensson, 2000). An example of a VDS used in NLP
is shown in Figure 1. VDSs are used very widely in
contexts where computationally generated language
is evaluated, including in dialogue, summarisation,
MT and data-to-text generation.
Visual analogue scales (VASs) are far less com-
mon outside psychology and related areas than
VDSs. Responses are given by selecting a point on
a typically horizontal line (although vertical lines
have also been used (Scott and Huskisson, 2003)),
on which the two end points represent the extreme
values of the variable to be measured. Such lines
can be mono-polar or bi-polar, and the end points
are labelled with an image (smiling/frowning face),
or a brief verbal descriptor, to indicate which end
of the line corresponds to which extreme of the vari-
able. The labels are commonly chosen to represent a
point beyond any response actually likely to be cho-
sen by raters. There is only one examples of a VAS
in NLP system evaluation that we are aware of (Gatt
et al, 2009).
Hybrid scales, known as a graphic rating scales,
combine the features of VDSs and VASs, and are also
used in psychology. Here, the verbal descriptors are
aligned along the line of a VAS and the endpoints are
typically unmarked (Svensson, 2000). We are aware
of one example in NLP (Williams and Reiter, 2008);
Q1: Grammaticality The summary should have no datelines,
system-internal formatting, capitalization errors or obviously
ungrammatical sentences (e.g., fragments, missing compo-
nents) that make the text difficult to read.
extremely
excellent
bad
Figure 2: Evaluation of Grammaticality with alternative
VAS scale (cf. Figure 1). Evaluation task for each sum-
mary text: evaluator selects a place on the line to repre-
sent quality of the summary in terms of the criterion.
we did not investigate this scale in our study.
We used the following two specific scale designs
in our experiments:
VDS-7: 7 response categories, numbered (7 =
best) and verbally described (e.g. 7 = ?perfectly flu-
ent? for Fluency, and 7 = ?perfectly clear? for Clar-
ity). Response categories were presented in a verti-
cal list, with the best category at the bottom. Each
category had a tick-box placed next to it; the rater?s
task was to tick the box by their chosen rating.
VAS: a horizontal, bi-polar line, with no ticks on
it, mapping to 0?100. In the image description tests,
statements identified the left end as negative, the
right end as positive; in the weather forecast tests,
the positive end had a smiling face and the label
?statement couldn?t be clearer/read better?; the neg-
ative end had a frowning face and the label ?state-
ment couldn?t be more unclear/read worse?. The
raters? task was to move a pointer (initially in the
middle of the line) to the place corresponding to
their rating.
3 Data
Weather forecast texts: In one half of our evalua-
tion experiments we used human-written and auto-
matically generated weather forecasts for the same
weather data. The data in our evaluations was for 22
different forecast dates and included outputs from 10
generator systems and one set of human forecasts.
This data has also been used for comparative sys-
tem evaluation in previous research (Langner, 2010;
Angeli et al, 2010; Belz and Kow, 2009). The fol-
lowing are examples of weather forecast texts from
the data:
1: SSE 28-32 INCREASING 36-40 BY MID AF-
TERNOON
2: S?LY 26-32 BACKING SSE 30-35 BY AFTER-
231
NOON INCREASING 35-40 GUSTS 50 BY MID
EVENING
Image descriptions: In the other half of our eval-
uations, we used human-written and automatically
generated image descriptions for the same images.
The data in our evaluations was for 112 different
image sets and included outputs from 6 generator
systems and 2 sets of human-authored descriptions.
This data was originally created in the TUNA Project
(van Deemter et al, 2006). The following is an ex-
ample of an item from the corpus, consisting of a set
of images and a description for the entity in the red
frame:
the small blue fan
4 Experimental Set-up
4.1 Evaluation criteria
Fluency/Readability: Both the weather forecast and
image description evaluation experiments used a
quality criterion intended to capture ?how well a
piece of text reads?, called Fluency in the latter,
Readability in the former.
Adequacy/Clarity: In the image description ex-
periments, the second quality criterion was Ade-
quacy, explained as ?how clear the description is?,
and ?how easy it would be to identify the image from
the description?. This criterion was called Clarity in
the weather forecast experiments, explained as ?how
easy is it to understand what is being described?.
4.2 Raters
In the image experiments we used 8 raters (native
speakers) in each experiment, from cohorts of 3rd-
year undergraduate and postgraduate students doing
a degree in a linguistics-related subject. They were
paid and spent about 1 hour doing the experiment.
In the weather forecast experiments, we used 22
raters in each experiment, from among academic
staff at our own university. They were not paid and
spent about 15 minutes doing the experiment.
4.3 Summary overview of experiments
Weather VDS-7 (A): VDS-7 scale; weather forecast
data; criteria: Readability and Clarity; 22 raters (uni-
versity staff) each assessing 22 forecasts.
Weather VDS-7 (B): exact repeat of Weather
VDS-7 (A), including same raters.
Weather VAS: VAS scale; 22 raters (university
staff), no overlap with raters in Weather VDS-7 ex-
periments; other details same as in Weather VDS-7.
Image VDS-7: VDS-7 scale; image description
data; 8 raters (linguistics students) each rating 112
descriptions; criteria: Fluency and Adequacy.
Image VAS (A): VAS scale; 8 raters (linguistics
students), no overlap with raters in Image VAS-7;
other details same as in Image VDS-7 experiment.
Image VAS (B): exact repeat of Image VAS (A),
including same raters.
4.4 Design features common to all experiments
In all our experiments we used a Repeated Latin
Squares design to ensure that each rater sees the
same number of outputs from each system and for
each text type (forecast date/image set). Following
detailed instructions, raters first did a small number
of practice examples, followed by the texts to be
rated, in an order randomised for each rater. Eval-
uations were carried out via a web interface. They
were allowed to interrupt the experiment, and in the
case of the 1 hour long image description evaluation
they were encouraged to take breaks.
5 Comparison and Assessment of Scales
Validity is to the extent to which an assessment
method measures what it is intended to measure
(Svensson, 2000). Validity is often impossible to as-
sess objectively, as is the case of all our criteria ex-
cept Adequacy, the validity of which we can directly
test by looking at correlations with the accuracy with
which participants in a separate experiment identify
the intended images given their descriptions.
A standard method for assessing Reliability is
Kendall?s W, a coefficient of concordance, measur-
ing the degree to which different raters agree in their
ratings. We report W for all 6 experiments.
Stability refers to the extent to which the results
of an experiment run on one occasion agree with
the results of the same experiment (with the same
232
raters) run on a different occasion. In the present
study, we assess stability in an intra-rater, test-retest
design, assessing the agreement between the same
participant?s responses in the first and second runs
of the test with Pearson?s product-moment correla-
tion coefficient. We report these measures between
ratings given in Image VAS (A) vs. those given in Im-
age VAS (B), and between ratings given in Weather
VDS-7 (A) vs. those given in Weather VDS-7 (B).
We assess Interchangeability, that is, the extent
to which our VDS and VAS scales agree, by comput-
ing Pearson?s and Spearman?s coefficients between
results. We report these measures for all pairs of
weather forecast/image description evaluations.
We assess the Sensitivity of our scales by de-
termining the number of significant differences be-
tween different systems and human authors detected
by each scale.
We also look at the relative effect of the differ-
ent experimental factors by computing the F-Ratio
for System (the main factor under investigation, so
its relative effect should be high), Rater and Text
Type (their effect should be low). F-ratios were de-
termined by a one-way ANOVA with the evaluation
criterion in question as the dependent variable and
System, Rater or Text Type as grouping factors.
6 Results
6.1 Interchangeability and Reliability for
system/human authored image descriptions
Interchangeability: Pearson?s r between the means
per system/human in the three image description
evaluation experiments were as follows (Spearman?s
? shown in brackets):
A
de
q. VAS (A) VAS (B)
VDS-7 .957**(.958**) .819* (.755*)
VAS (A) ? .874** (.810*)
F
lu
e. VDS-7 .948**(.922**) .864** (.850**)
VAS (A) ? .937** (.929**)
For both Adequacy and Fluency, correlations be-
tween Image VDS-7 and Image VAS (A) (the main
VAS experiment) are extremely high, meaning that
they could substitute for each other here.
Reliability: Inter-rater agreement in terms of
Kendall?s W in each of the experiments:
VDS-7 VAS (A) VAS (B)
K?s W Adequacy .598** .471** .595*
K?s W Fluency .640** .676** .729**
W was higher in the VAS data in the case of Fluency,
whereas for Adequacy, W was the same for the VDS
data and VAS (B), and higher in the VDS data than
in the VAS (A) data.
6.2 Interchangeability and Reliability for
system/human authored weather forecasts
Interchangeability: The correlation coefficients
(Pearson?s r with Spearman?s ? in brackets) between
the means per system/human in the image descrip-
tion experiments were as follows:
C
la
r. VDS-7 (B) VAS
VDS-7 (A) .995** (.989**) .942** (.832**)
VDS-7 (B) ? .939**( .836**)
R
ea
d. VDS-7 (A) .981** (.870**) .947** (.709*)
VDS-7 (B) ? .951** (.656*)
For both Adequacy and Fluency, correlations be-
tween Weather VDS-7 (A) (the main VDS-7 experi-
ment) and Weather VAS (A) are again very high, al-
though rank-correlation is somewhat lower.
Reliability: Inter-rater agreement in terms of
Kendall?s W was as follows:
VDS-7 (A) VDS-7 (B) VAS
W Clarity .497** .453** .485**
W Read. .533** .488** .480**
This time the highest agreement for both Clarity and
Readability was in the VDS-7 data.
6.3 Stability tests for image and weather data
Pearson?s r between ratings given by the same raters
first in Image VAS (A) and then in Image VAS (B)
was .666 for Adequacy, .593 for Fluency. Between
ratings given by the same raters first in Weather
VDS-7 (A) and then in Weather VDS-7 (B), Pearson?s
r was .656 for Clarity, .704 for Readability. (All sig-
nificant at p < .01.) Note that these are computed
on individual scores (rather than means as in the cor-
relation figures given in previous sections).
6.4 F-ratios and post-hoc analysis for image
data
The table below shows F-ratios determined by a one-
way ANOVA with the evaluation criterion in question
(Adequacy/Fluency) as the dependent variable and
System/Rater/Text Type as the grouping factor. Note
233
that for System a high F-ratio is desirable, but a low
F-ratio is desirable for other factors.
Image descriptions
VDS-7 VAS (A)
Adequacy
System 8.822** 6.371**
Rater 12.623** 13.136**
Text Type 1.193 1.519**
Fluency
System 13.312** 17.207**
Rater 27.401** 17.479**
Text Type .894 1.091
Out of a possible 28 significant differences for Sys-
tem, the main factor under investigation, VDS-7
found 8 for Adequacy and 14 for Fluency; VAS (A)
found 7 for Adequacy and 15 for Fluency.
6.5 F-ratios and post-hoc analysis for weather
data
The table below shows F-ratios analogous to the pre-
vious section (for Clarity/Readability).
Weather forecasts
VDS-7 (A) VAS
Clarity
System 23.507** 23.468**
Rater 4.832** 6.857**
Text Type 1.467 1.632*
Read.
System 24.351** 22.538**
Rater 4.824** 5.560**
Text Type 1.961** 2.906**
Out of a possible 55 significant differences for Sys-
tem, VDS-7 (A) found 24 for Clarity, 23 for Read-
ability; VAS found 25 for Adequacy, 26 for Fluency.
6.6 Scale validity test for image data
Our final table of results shows Pearson?s correla-
tion coefficients (calculated on means per system)
between the Adequacy data from the three image
description evaluation experiments on the one hand,
and the data from an extrinsic experiment in which
we measured the accuracy with which participants
identified the intended image described by a descrip-
tion:
ID Acc.
Image VAS (A) Adequacy .870**
Image VAS (B) Adequacy .927**
Image VDS-7 Adequacy .906**
The correlation between Adequacy and ID Accuracy
was strong and highly significant in all three image
description evaluation experiments, but strongest in
VAS (B), and weakest in VAS (A). For comparison,
Pearson?s between Fluency and ID Accuracy ranged
between .3 and .5, whereas Pearson?s between Ade-
quacy and ID Speed (also measured in the same im-
age identfication experiment) ranged between -.35
and -.29.
7 Discussion and Conclusions
Our interchangeability results (Sections 6.1 and 6.2)
indicate that the VAS and VDS-7 scales we have
tested can substitute for each other in our present
evaluation tasks in terms of the mean system scores
they produce. Where we were able to measure va-
lidity (Section 6.6), both scales were shown to be
similarly valid, predicting image identification ac-
curacy figures from a separate experiment equally
well. Stability (Section 6.3) was marginally better
for VDS-7 data, and Reliability (Sections 6.1 and
6.2) was better for VAS data in the image descrip-
tion evaluations, but (mostly) better for VDS-7 data
in the weather forecast evaluations. Finally, the VAS
experiments found greater numbers of statistically
significant differences between systems in 3 out of 4
cases (Section 6.5).
Our own raters strongly prefer working with VAS
scales over VDSs. This has also long been clear from
the psychology literature (Svensson, 2000)), where
raters are typically found to prefer VAS scales over
VDSs which can be a ?constant source of vexation
to the conscientious rater when he finds his judg-
ments falling between the defined points? (Champ-
ney, 1941). Moreover, if a rater?s judgment falls be-
tween two points on a VDS then they must make the
false choice between the two points just above and
just below their actual judgment. In this case we
know that the point they end up selecting is not an
accurate measure of their judgment but rather just
one of two equally accurate ones (one of which goes
unrecorded).
Our results establish (for our evaluation tasks) that
VAS scales, so far unproven for use in NLP, are at
least as good as VDSs, currently virtually the only
scale in use in NLP. Combined with the fact that
raters strongly prefer VASs and that they are regarded
as more amenable to parametric means of statisti-
cal analysis, this indicates that VAS scales should be
used more widely for NLP evaluation tasks.
234
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 15th Conference on
Empirical Methods in Natural Language Processing
(EMNLP?10).
Anja Belz and Eric Kow. 2009. System building cost
vs. output quality in data-to-text generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 16?24.
H. Champney. 1941. The measurement of parent behav-
ior. Child Development, 12(2):131.
M. Freyd. 1923. The graphic rating scale. Biometrical
Journal, 42:83?102.
A. Gatt, A. Belz, and E. Kow. 2009. The TUNA Chal-
lenge 2009: Overview and evaluation results. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG?09), pages 198?206.
Brian Langner. 2010. Data-driven Natural Language
Generation: Making Machines Talk Like Humans
Using Natural Corpora. Ph.D. thesis, Language
Technologies Institute, School of Computer Science,
Carnegie Mellon University.
Robert W. Lansing, Shakeeb H. Moosavi, and Robert B.
Banzett. 2003. Measurement of dyspnea: word la-
beled visual analog scale vs. verbal ordinal scale. Res-
piratory Physiology & Neurobiology, 134(2):77 ?83.
J. Scott and E. C. Huskisson. 2003. Vertical or hori-
zontal visual analogue scales. Annals of the rheumatic
diseases, (38):560.
Sidney Siegel. 1957. Non-parametric statistics. The
American Statistician, 11(3):13?19.
Elisabeth Svensson. 2000. Comparison of the quality
of assessments using continuous and discrete ordinal
rating scales. Biometrical Journal, 42(4):417?434.
P. M. ten Klooster, A. P. Klaar, E. Taal, R. E. Gheith,
J. J. Rasker, A. K. El-Garf, and M. A. van de Laar.
2006. The validity and reliability of the graphic rating
scale and verbal rating scale for measuing pain across
cultures: A study in egyptian and dutch women with
rheumatoid arthritis. The Clinical Journal of Pain,
22(9):827?30.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Conference on Natural
Language Generation, pages 130?132, Sydney, Aus-
tralia, July.
S. Williams and E. Reiter. 2008. Generating basic skills
reports for low-skilled readers. Natural Language En-
gineering, 14(4):495?525.
235
The GREC Challenge: Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
The GREC Task at REG?08 required partici-
pating systems to select coreference chains to
the main subject of short encyclopaedic texts
collected from Wikipedia. Three teams sub-
mitted a total of 6 systems, and we addition-
ally created four baseline systems. Systems
were tested automatically using a range of ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying coreference
resolution tools to the outputs and measuring
the success of the tools. In addition, systems
were tested in a reading/comprehension exper-
iment involving human subjects. This report
describes the GREC Task and the evaluation
methods, gives brief descriptions of the par-
ticipating systems, and presents the evaluation
results.
1 Introduction
The GREC task is about how to generate appropri-
ate references to an entity in the context of a piece
of discourse longer than a sentence. Rather than
requiring participants to generate referring expres-
sions from scratch, the GREC data provides sets of
possible referring expressions for selection. As this
is a new referring expression generation (REG) task,
the shared task definition was kept fairly simple
and the aim for participating systems was to select
the appropriate type of referring expression (more
specifically, its REG08-TYPE, full details below).
The immediate motivating application context for
the GREC Task is the improvement of referential
clarity and coherence in extractive summarisation
by regenerating referring expressions in summaries.
There has recently been a small flurry of work in
this area (Steinberger et al, 2007; Nenkova, 2008).
In the longer term, the GREC Task is intended to be a
step in the direction of the more general task of gen-
erating referential expressions in discourse context.
The GREC Task Corpus is an extension of GREC
1.0 which had about 1,000 texts in the subdomains
of cities, countries, rivers and people (Belz and
Varges, 2007a). for the purpose of the REG?08 GREC
Task, we obtained an additional 1,000 texts in the
new subdomain of mountain texts and developed a
new XML annotation scheme (Section 2.2).
Five teams from four countries registered for the
GREC Task, of which three teams eventually submit-
ted 6 systems. We also used the corpus texts them-
selves as ?system? outputs, and created four base-
line systems. We evaluated the resulting 10 sys-
tems using a range of intrinsic and extrinsic evalu-
ation methods. This report presents the results of all
evaluations (Section 6), along with descriptions of
the GREC data and task (Section 2), test sets (Sec-
tion 3), evaluation methods (Section 4), and partici-
pating systems (Section 5).
2 Data and Task
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introductory
sections in Wikipedia articles, in five different do-
mains (cities, countries, rivers, people and moun-
tains). In each text, three broad categories of Main
Subject Reference (MSR)1 have been annotated, re-
1The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
183
sulting in a total of about 13,000 annotated REs.
The corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results on
the development data. Participants had 48 hours to
submit outputs for the (previously unseen) test data.
2.1 Types of referential expression annotated
Three broad categories of main subject referring ex-
pression (MSREs) are annotated in the GREC corpus2
? subject NPs, object NPs, and genitive NPs and pro-
nouns which function as subject-determiners within
their matrix NP. These categories of referring ex-
pression (RE) are relatively straightforward to iden-
tify and achieve high inter-annotator agreement on
(complete agreement among four annotators in 86%
of MSRs), and account for most cases of overt main
subject reference (MSR) in the GREC texts. The an-
notators were asked to identify subject, object and
genitive subject-determiners and decide whether or
not they refer to the main subject of the text. More
detail is provided in Belz and Varges (2007b).
In addition to the above, relative pronouns in sup-
plementary relative clauses (as opposed to integrated
relative clauses, Huddleston and Pullum, 2002, p.
1058) were annotated, e.g.:
(1) Stoichkov is a football manager and former striker who
was a member of the Bulgaria national team that
finished fourth at the 1994 FIFA World Cup.
We also annotated ?non-realised? subject MSREs
in a restricted set of cases of VP coordination where
an MSRE is the subject of the coordinated VPs, e.g.:
(2) He stated the first version of the Law of conservation of
mass, introduced the Metric system, and helped to
reform chemical nomenclature.
The motivation for annotating the approximate
place where the subject NP would be if it were re-
alised (the gap-like underscores above) is that from
a generation perspective there is a choice to be made
about whether to realise the subject NP in the second
and third coordinates or not.
(and title) of one text is London.
2In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
2.2 XML format
Figure 1 is one of the texts distributed in the GREC
data sample for the REG Challenge. The REF el-
ement indicates a reference, in the sense of ?an
instance of referring? (which could, in principle,
be realised by gesture or graphically, as well as
by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cat-
egory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX el-
ement (the ?selected? referential expression for the
given reference; in the corpus texts it is simply
the referential expression found in the corpus) and
one ALT-REFEX element which in turn is a list of
REFEXs which are alternative referential expressions
obtained by other means (see following section).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in the
present version of the GREC corpus, the only type of
RE that has this attribute is one which incorporates
a reflexive pronoun used emphatically (e.g. India it-
self ). The REG08-TYPE attribute indicates basic RE
type as required for the REG?08 GREC task defini-
tion. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether to
use an RE that includes a name, such as Modern In-
dia (the corresponding REG08-TYPE value is name);
whether to go for a common-noun RE, i.e. with a
category noun like country as the head (common);
whether to pronominalise the RE (pronoun); or
whether it can be left unrealised (empty).
2.3 The REG?08 GREC Task
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to the
184
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TEXT SYSTEM "reg08-grec.dtd">
<TEXT ID="36">
<TITLE>Jean Baudrillard</TITLE>
<PARAGRAPH>
<REF ID="36.1" SEMCAT="person" SYNCAT="np-subj">
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX>
<REFEX REG08-TYPE="empty">_</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX>
</ALT-REFEX>
</REF>
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det">
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX>
</ALT-REFEX>
</REF>
work is frequently associated with postmodernism and post-structuralism.
</PARAGRAPH>
</TEXT>
Figure 1: Example text from REG?08 Training Data.
training/development data, except that REF elements
contained only an ALT-REFEX list, not the preced-
ing ?selected? REFEX. ALT-REFEX lists are gener-
ated for each text by an automatic method which
collects all the (manually annotated) MSREs in a text
including the title and adds several defaults: pro-
nouns and reflexive pronouns in all subdomains; and
category nouns (e.g. the river), in all subdomains
except people. The main objective in the REG?08
GREC Task was to get the REG08-TYPE attribute of
REFEXs right.
3 Test Data
1. GREC Test Set C-1: a randomly selected 10%
subset (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. GREC Test Set C-2: the same subset of texts as
in C-1; however, for C-2 we did not use the MSREs
in the corpus, but replaced each of them with three
human-selected alternatives. These were obtained in
an online experiment as described in Belz & Varges
(2007a) where subjects selected MSREs in a setting
that duplicated the conditions in which the partici-
pating systems in the REG?08 GREC Task made se-
lections.3 We obtained three versions of each text,
where in each version all MSREs were selected by
the same person. The motivation for creating this
version of Test Set C was firstly that having sev-
eral human-produced chains of MSREs to compare
the outputs of participating (?peer?) systems against
is more reliable than having one only; and secondly
that Wikipedia texts are edited by multiple authors
and so MSR chains may sometimes be adversely af-
fected by this; we wanted to have additional refer-
ence texts without this characteristic.
3. GREC Test Set L: 74 Wikipedia introductory
texts from the subdomain of lakes; participants did
not know what this subdomain was until they re-
ceived the test data (there were no lake texts in the
training/development set).
4. GREC Test Set P: 31 short encyclopaedic texts
in the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but from a source other than
3The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
185
Wikipedia. We transcribed these texts from printed
encyclopaedias published in the 1980s which are
not available in electronic form, and this provenance
was not revealed to participants. The texts in this set
are much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs follow
very similar patterns. It seems likely that it is these
properties that have resulted in better scores overall
for Test Set P (see Section 6).
Each test set was designed to test peer systems for
a different aspect of generalisation. Test Set C tests
for generalisation to unseen material from the same
corpus and the same subdomains as the training set;
Test Set L tests for generalisation to unseen material
from the same corpus but different subdomain; and
Test Set P tests generalisation to a different corpus
but same subdomains.
4 Evaluation methods
4.1 Automatic intrinsic evaluations
Accuracy of REG08-Type: when computed against
the single-RE test sets (C-1, L and P), REG08-Type
Accuracy is the proportion of REFEXs selected by a
participating system that have a REG08-TYPE value
identical to the one in the corpus.
When computed against the triple-RE test set (C-
2), first the number of correct REG08-Types is com-
puted at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number of
REFs in all the texts, which gives the global REG08-
Type Accuracy score. The rationale behind com-
puting the REG08-Type Accuracy scores in this way
for multiple-RE test sets (maximising scores on RE
chains rather than individual REs) is that an RE is
not good or bad in its own right, but depends on the
other MSRs in the same text.4
String Accuracy: This is defined just like
REG08-Type Accuracy, except here what is deter-
mined is identity between REFEX word strings (the
MSREs themselves), not between REG08-Types.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
4This definition is also slightly different from the one given
in the Participants? Pack.
cost of 1. We also used the version of string-edit
distance described by Bangalore et al (2000) which
normalises for length. This version is denoted ?SEB?
below. For the single-RE test sets, the global score
is simply the average of all RE-level scores. For Test
Set C-2, we used an approach analogous to that de-
scribed above for REG08-Type Accuracy. We first
computed the best string-edit distance at the text
level (here, just the sum of RE-level distances) and
then obtained the global distance by dividing the
sum of best text-level distances by the number of
REFs in all the texts.
Other metrics: BLEU is a precision metric from
MT that assesses the quality of a peer translation
in terms of the proportion of its word n-grams
(n ? 4 is standard) that it shares with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in the
corpus are less than 4 tokens long. We also used
the NIST version of BLEU which weights in favour
of less frequent n-grams, as well as ROUGE-2 and
ROUGE-SU4 (the two official automatic scores from
the DUC summarisation competitions). In all cases,
we assessed just the MSREs selected by peer systems
(leaving out the surrounding text), and computed
scores globally (rather than averaging over RE-level
scores), as this is standard for these metrics.
BLEU, NIST and ROUGE are designed to work
with either one or multiple reference texts, so we did
not need to use a different method for Test Set C-2.
4.2 Human extrinsic evaluation
We designed a reading/comprehension experiment
in which the task for subjects was to read texts
one sentence at a time and then to answer three
brief multiple-choice comprehension questions after
reading each text. The basic idea was that it seemed
likely that badly chosen MSR reference chains would
adversely affect ease of comprehension, and that this
might in turn affect reading speed and accuracy in
answering comprehension questions.
We used a randomly selected subset of 21 texts
from Test Set C, and recruited 21 subjects from
among the staff, faculty and students of Brighton
and Sussex universities. We used a Repeated Latin
Squares design in which each combination of text
and system was allocated three trials. During the
experiment we recorded SRTime, the time subjects
186
took to read sentences (from the point when the sen-
tence appeared on the screen to the point at which
the subject requested the next sentence).
We also recorded the speed and accuracy with
which subjects answered the questions at the end (Q-
Time and Q-Acc). The role of the comprehension
questions was to encourage subjects to read the texts
properly, rather than skimming through them, and
we did not necessarily expect any significant results
from the associated measures.
The questions were designed to be of varying de-
grees of difficulty and predictability. There was one
set of three questions (each with five possible an-
swers) associated with each text, and questions fol-
lowed the same pattern across the texts: the first
question was always about the subdomain of a text;
the second about the location of the main subject; the
third question was designed not to be predictable.
The order of the answers was randomised for each
question and each subject. The order of texts (with
associated questions) was randomised for each sub-
ject. We used the DMDX package for presentation
of sentences and measuring reading times and ques-
tion answering accuracy (Forster and Forster, 2003).
Subjects did the experiment in a quiet room, under
supervision.
4.3 Automatic extrinsic evaluation
As a new and highly experimental method, we tried
out an automatic approach to extrinsic evaluation.
The basic idea was similar to that in the human-
based experiments described above: badly chosen
reference chains seem likely to affect the reader?s
ability to resolve REs. In the automatic version, the
role of the reader is played by an automatic coref-
erence resolution tool and the expectation is that the
tool performs worse (are less able to identify coref-
erence chains correctly) with worse MSR reference
chains.
To counteract the potential problem of results be-
ing a function of a specific coreference resolution
algorithm or tool, we decided to use three differ-
ent resolvers?those included in LingPipe,5 JavaRap
(Qiu et al, 2004) and OpenNLP (Morton, 2005)?
and to average results.
There does not appear to be a single standard eval-
5http://alias-i.com/lingpipe/
uation metric in the coreference resolution commu-
nity, so we opted to use three: MUC-6 (Vilain et al,
1995), CEAF (Luo, 2005), and B-CUBED (Bagga and
Baldwin, 1998), which seem to be the most widely
accepted metrics.
All three metrics compute Recall, Precision and
F-Scores on aligned gold-standard and resolver-tool
coreference chains. They differ in how the align-
ment is obtained and what components of corefer-
ence chains are counted for calculating scores. Re-
sults for the automatic extrinsic evaluations are re-
ported below in terms of the F-Scores from these
three metrics, as well as in terms of their average.
5 Systems
Base-rand, Base-freq, Base-1st, Base-name: We
created four baseline systems. Base-rand selects
one of the REFEXs at random. Base-freq selects
the REFEX that is the overall most frequent given
the SYNCAT and SEMCAT of the reference. Base-
1st always selects the REFEX which appears first
in the list of REFEXs; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.6
CNTS-Type-g, CNTS-Prop-s: The CNTS sys-
tems are trained using memory-based learning with
automatic parameter optimisation. They use a set of
14 features obtained by various kinds of syntactic
preprocessing and named-entity recognition as well
as from the corpus annotations: SEMCAT, SYNCAT,
position of RE in text, neighbouring words and POS-
tags, distance to previous mention, SYNCATs of
three preceding REFEXs, binary feature indicating
whether the most recent named entity was the main
subject (MS), main verb of the sentence. For Type-
g, a single classifier was trained to predict just the
REG08-TYPE property of REFEXs. For Prop-s, four
classifiers were trained, one for each subdomain, to
predict all four properties of REFEXs (rather than just
REG08-TYPE).
OSU-b-all, OSU-b-nonRE, OSU-n-nonRE: The
OSU-2 systems are maximum-entropy classifiers
trained on a range of features obtained by prepro-
6Attributes are tried in this order. If for one attribute, the
right value is not found, the process ignores that attribute and
moves on the next one.
187
System REG08-Type Accuracy for Development SetAll Cities Coun Riv Peop Moun
CNTS-Type-g 76.52 64.65 75 65 85.37 75.42
CNTS-Prop-s 73.93 65.66 69.57 70 79.51 74.58
IS-G 66 54.5 64 80 66.8 65
OSU-n-nonRE 62.50 53.54 63.04 65 67.32 61.67
OSU-b-all 58.54 53.54 57.61 75 65.85 49.58
OSU-b-nonRE 51.07 51.52 53.26 40 57.07 45.83
Table 1: Self-reported REG08-Type Accuracy scores for
development set.
cessing the text, as well as from the corpus anno-
tations: SEMCAT, SYNCAT, position of RE in text,
presence of contrasting discourse entity, distance be-
tween current and preceding reference to the MS,
string similarity measures between REFEXs and ti-
tle of text. OSU-b-all and OSU-b-nonRE are binary
classifiers which give the likelihood of selecting a
given REFEX vs. not selecting it, whereas OSU-n-
nonRE is a 4-class classifier giving the likelihoods
of selecting each of the four REG08-TYPEs. OSU-
b-all also uses the REFEX attributes as features.
IS-G: The IS-G system is a multi-layer percep-
tron which uses four features obtained by prepro-
cessing texts as well as from the corpus annota-
tions: SYNCAT, distance between current and pre-
ceding reference to the MS, position of RE in text,
REG08-TYPE of preceding reference to the MS, fea-
ture indicating whether the preceding MSR is in the
same sentence.
6 Results
This section presents the results of all the evalua-
tion methods described in Section 4. We start with
REG08-Type Accuracy, an intrinsic automatic met-
ric which participating teams were told was going
to be the chief evaluation method, followed by other
intrinsic automatic metrics (Section 6.2), the extrin-
sic human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
6.1 REG08-Type Accuracy
Participants computed REG08-Type Accuracy for
the development set (97 texts) themselves, using a
tool provided by us. These scores are shown in
Table 1, and are also included in the participants?
reports elsewhere in this volume. Systems are or-
dered in terms of their overall REG08-Type Accu-
racy (column 1), and scores for each subdomain are
also shown. Scores are highly consistent across the
subdomains, except for the river subdomain which
was the smallest set (containing only 4 texts), and
results for it may be idiosyncratic for this reason.
Corresponding results for the (unseen) test set C-1
are shown in column 2 of Table 2. As would be ex-
pected, results are slightly worse than for the (seen)
development set (although some systems managed
to improve over their development set scores). Also
included in this table are results for the four base-
line systems, and it is clear that selecting the most
frequent REG08-Type given SEMCAT and SYNCAT
(as done by the Base-freq system) provides a strong
baseline.
Other columns in Table 2 contain results for test
sets L and P. Again as expected, results for Test Set
L are lower than for Test Set C-1, because in ad-
dition to consisting of unseen texts (like C-1), Test
Set L is also from an unseen subdomain (unlike C-
1). The results for Test Set P are higher and on a par
with those for the development set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 2 we carried out a uni-
variate ANOVA with System as the fixed factor. We
found significant main effects at p < .001 in all
three cases (C-1: F = 95.426; L: F = 63.758;
P: F = 21.188). The columns containing capital
letters in Table 2 show the homogeneous subsets of
systems as determined by post-hoc Tukey HSD com-
parisons of means. Systems whose REG08-Type Ac-
curacy scores are not significantly different (at the
.05 level) share a letter.
The results for REG08-Type Accuracy computed
against the triple-RE Test Set C-2 are shown in Ta-
ble 3. These should be considered as the chief results
of the GREC Task evaluations, as stated in the guide-
lines. Here too we performed a univariate ANOVA
with System as the fixed factor and REG08-Type
as the dependent variable. Having established by
ANOVA that there was a significant main effect of
System (F = 86.946, p < .001), we compared the
mean scores with Tukey?s HSD. As can be seen from
the resulting homogeneous subsets, there is no sig-
nificant difference between the corpus texts (C-1)
and system CNTS-Type-g, but also there is no sig-
188
single-RE Test Set C-1 Test Set L Test Set P
CNTS-Type-g 68.15 A CNTS-Type-g 62.06 A CNTS-Type-g 75.31 A
CNTS-Prop-s 67.04 A CNTS-Prop-s 62.06 A CNTS-Prop-s 72.84 A B
IS-G 66.48 A IS-G 60.93 A IS-G 67.90 A B C
OSU-n-nonRE 63.69 A OSU-n-nonRE 41.80 B OSU-n-nonRE 66.67 A B C
OSU-b-nonRE 53.11 B OSU-b-nonRE 39.23 B OSU-b-all 57.41 B C D
OSU-b-all 52.39 B OSU-b-all 37.62 B C OSU-b-nonRE 56.17 C D
Base-freq 43.47 C Base-freq 35.53 B C Base-freq 44.44 D F
Base-name 39.49 C Base-rand 23.63 C D Base-rand 33.95 F
Base-1st 39.17 C Base-name 23.63 D Base-name 32.10 F
Base-rand 32.72 D Base-1st 29.74 D Base-rand 32.10 F
Table 2: REG08-Type Accuracy scores and homogeneous subsets (Tukey HSD, alpha = .05) for single-RE test sets.
Systems that do not share a letter are significantly different.
System REG08-Type Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People Mountains
Corpus 78.58 A 70.92 77.49 85.29 84.67 75.81
CNTS-Type-g 72.61 A B 65.96 71.73 73.53 77.64 70.73
CNTS-Prop-s 71.34 B 64.54 67.02 70.59 75.38 71.75
IS-G 70.78 B 69.50 65.45 76.47 76.88 67.89
OSU-n-nonRE 69.82 B 65.25 64.92 79.41 78.14 65.65
OSU-b-nonRE 58.76 C 52.48 60.73 50.00 59.80 59.55
OSU-b-all 57.48 C 53.90 58.64 47.06 59.05 57.52
Base-name 50.00 D 53.19 54.45 35.29 43.22 53.86
Base-1st 49.28 D 53.19 49.21 38.24 43.22 53.86
Base-freq 48.17 D 43.97 42.41 55.88 56.78 44.11
Base-rand 41.24 E 41.84 36.13 32.35 44.47 41.06
Table 3: REG08-Type Accuracy scores against Test Set C-2 for complete set and for subdomains; homogeneous subsets
(Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are significantly different).
nificant difference between the latter and systems
CNTS-Prop-s, IS-G and OSU-n-nonRE. In this anal-
ysis, all systems outperform the random baseline;
all peer systems outperform all of the baselines; and
the four best peer systems outperform the remaining
two.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure reported
on in the preceding section, we computed the string
similarity metrics described in Section 4.1 for all
four test sets. Results were very similar to those
for REG08-Type Accuracy, so we are reporting only
scores for Test Set C-2 (Table 4). The corpus texts
again receive the best scores across the board (SE is
the odd one out, because here lower scores are bet-
ter). Ranks for peer systems are very similar to the
results reported in the last section.
We performed an ANOVA (F = 138.159, p <
.001) and Tukey HSD post-hoc analysis for String
Accuracy. The resulting homogeneous subsets (Ta-
ble 4, columns 3?8) reveal significant differences
similar to those for REG08-Type Accuracy. We also
computed Pearson product-moment correlation co-
efficients between all automatic intrinsic evaluation
measures we used. All pairwise correlations were
significant at the .01 level (using a two-tailed test).
One of the strongest correlations (.961) was between
REG08-Type Accuracy and String Accuracy, imply-
ing that getting REG08-Type right gets you some
way towards getting the actual RE right.
6.3 Human-based extrinsic measures
As a result of the experiment described in Sec-
tion 4.2 we had SRTime measures (sentence reading
times) for each sentence in each of the 21 texts that
were included in the experiment. Table 5 shows the
resulting SRTimes in milliseconds averaged per sys-
tem. None of the differences were statistically sig-
nificant. We also analysed SRTimes normalised by
sentence length; SRTimes only from sentences that
contained MRSs; and SRTimes normalised for sub-
ject reading speed. There were no significant differ-
ences under any of these analyses.
Much of the variance in SRTimes was due to sub-
jects? very different average reading speeds: means
of SRTime normalised for sentence length ranged
from 188.45ms to 426.10ms for individual subjects.
189
System Word string similarity for Triple-RE Test Set C-2String Accuracy BLEU-3 NIST ROUGE-2 ROUGE-SU4 SE SEB
Corpus 71.18 A 0.7792 7.5080 0.66102 0.70991 0.7229 0.5136
CNTS-Type-g 65.61 A B 0.7377 6.1288 0.60280 0.64998 0.8838 0.3627
CNTS-Prop-s 65.29 A B 0.6760 5.9338 0.60103 0.64963 0.9068 0.3835
OSU-n-nonRE 63.85 B C 0.6715 5.7745 0.53395 0.57459 0.9666 0.0164
IS-G 58.20 C 0.5107 5.6102 0.50270 0.57052 1.1616 0.1818
OSU-b-nonRE 51.11 D 0.4964 5.5363 0.38255 0.42969 1.2834 0.0247
OSU-b-all 50.72 D 0.5050 5.6058 0.35133 0.39570 1.2994 0.3402
Base-freq 41.32 E 0.2684 3.0155 0.27727 0.33007 1.54299 -0.3250
Base-name 39.41 E 0.4641 5.9372 0.20730 0.25379 1.5175 -0.1912
Base-1st 39.09 E 0.3932 5.1597 0.21443 0.24037 1.6449 -0.0751
Base-rand 17.99 F 0.2182 2.9327 0.36056 0.41847 2.3217 -0.7937
Table 4: String Accuracy, BLEU, NIST, ROUGE and string-edit scores, computed on single-RE and triple-RE test
sets (systems in order of String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy only
(systems that do not share a letter are significantly different).
Mean SRTime (msecs)
CNTS-Prop-s 6305.8551
IS-G 6340.5131
OSU-n-nonRE 6422.5073
CNTS-Type-g 6435.6574
OSU-b-all 6451.7624
OSU-b-nonRE 6454.6749
Corpus 6548.2734
Table 5: Mean SRTimes for each system.
There was also variance from Text, i.e. some of the
texts appear to be harder to read than others.
The other two measures from the task-
performance experiment were Q-Acc (question
answering accuracy) and Q-Time (question answer-
ing speed). ANOVAs revealed no significant main
effect of System on Q-Time. For Q-Acc, we looked
at each of the three question types Q1, Q2, Q3
(see Section 4.2) separately. ANOVAs showed no
significant effect of System on Q-Acc for Q2 and
Q3; there was a slight effect (F = 2.193, p < .05)
of System on Q-Acc for Q1 (the easiest of the
questions which simply asked for the subdomain
of a text). Table 6 shows Q-Acc for Q1 and Q2,
and the results of a post-hoc analysis (Tukey HSD)
which revealed two homogeneous subsets with a lot
of overlap (columns 2 and 3).
Table 6 shows the results of this analysis: there
was
6.4 Automatic extrinsic measures
We used the same 21 texts as in the human extrin-
sic experiments, fed the outputs of each peer sys-
Question 1 Q2 Q3
Corpus 1.00 A .78 .63
CNTS-Type-g 1.00 A .83 .71
CNTS-Prop-s .98 A B .86 .75
OSU-b-nonRE .97 A B .83 .67
OSU-b-all .95 A B .75 .62
IS-G .95 A B .81 .63
OSU-n-nonRE .90 B .76 .76
Table 6: Question types 1?3, proportions correct; homo-
geneous subsets for Q1 (Tukey HSD, alpha = .05).
tem as well as the corpus texts through the three
coreference resolvers, and computed average MUC,
CEAF and B-CUBED F-Scores as described in Sec-
tion 4.3. The second column Table 7 shows the av-
erage of these three F-Scores, to give a single over-
all result for this evaluation method. A univariate
ANOVA with the average F-Score (column 2) as the
dependent variable and System as the single fixed
factor revealed a significant main effect of System
on average F-Score (F = 5.051, p < .001). A
post-hoc comparison of the means (Tukey HSD, al-
pha = .05) found the significant differences indi-
cated by the homogeneous subsets in columns 3?
5 (Table 7). The numbers shown in the last three
columns are the separate MUC, CEAF and B-CUBED
F-Scores for each system, averaged over the three
resolver tools. ANOVAs revealed the following ef-
fects of System: on CEAF F = 9.984, p < .001;
on MUC: F = 10.07, p < .001; on B-CUBED:
F = 8.446, p < .001.
The three F-Score measures (MUC, CEAF and B-
CUBED) are all strongly and highly significantly cor-
190
related: Pearson?s correlation coefficient is .947 for
B-CUBED and CEAF, .917 for B-CUBED and MUC,
and .951 for CEAF and MUC (p < .01, 2-tailed).
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-1st 53.50 A 47.59 52.64 60.28
Base-name 52.84 A 45.99 51.73 60.81
OSU-n-nonRE 51.39 A 46.92 49.8 57.45
OSU-b-nonRE 51.27 A 47.68 48.62 57.50
OSU-b-all 50.87 A 47.06 48.40 57.14
CNTS-Type-g 48.64 A B 43.77 46.32 55.82
IS-G 48.05 A B 43.25 46.24 54.66
CNTS-Prop-s 46.35 A B 42.82 43.36 52.88
Corpus 43.32 A B 37.89 41.6 50.47
Base-freq 41.41 B C 34.48 40.28 49.46
Base-rand 35.13 C 21.24 35.60 48.55
Table 7: MUC, CEAF and B-CUBED F-Scores for all sys-
tems; homogeneous subsets (Tukey HSD), alpha = .05,
for average of F-Scores.
7 Concluding Remarks
The GREC Task is a new task not only for an NLG
shared-task challenge, but also as a research task in
general (improving referential clarity in extractive
summaries seems to be just taking off as a research
subfield). It was therefore not unexpected that only
three teams were able to participate in this task.
We continued the traditions of the ASGRE?07
Challenge in that we used a wide range of evalu-
ation metrics to obtain a well-rounded view of the
quality of the participating systems. It had been our
intention to use evaluation methods in all four possi-
ble extrinsic/intrinsic and automatic/human combi-
nations. However, the combination intrinsic/human
is missing from this report and will have to be left to
future research.
There was no indication in the human task perfor-
mance experiment that the different reference chains
selected by different systems had any impact on sub-
jects? reading speeds, and the evidence that there
is an effect on comprehension was scant. This
means that we will need to investigate alternative
task-performance measures. Because of the lack of
significant results from the human extrinsic experi-
ment, we were also unable to validate the automatic
extrinsic experiment against it, and so at this point
we do not really know how useful it is (despite some
correlation with intrinsic measures), something we
will seek to establish in future research.
Acknowledgments
Many thanks to Jason Baldridge and Pascal De-
nis for help with selecting coreference resolution
tools and metrics, and to the colleagues and students
who helped with the task-performance experiment.
Thanks are also due to the members of the Corpora
and SIGGEN mailing lists, colleagues, friends and
friends of friends who helped with the online MSRE
selection experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Proceedings of the Linguistic
Coreference Workshop at LREC?98, pages 563?566.
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proceedings of
INLG?00, pages 1?8.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz and S. Varges. 2007b. The GREC corpus: Main
subject reference in context. Technical Report NLTG-
07-01, University of Brighton.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy. Be-
havior Research Methods, Instruments, & Computers,
35(1):116?124.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of Pen-
sylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP?08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC?04, pages 291?
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summariza-
tion. Information Processing and Management: Spe-
cial issue on Summarization, 43(6):1663?1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. Proceedings of MUC-6, pages 45?52.
191
The TUNA Challenge 2008: Overview and Evaluation Results
Albert Gatt
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Anja Belz Eric Kow
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
{asb, eykk10}@brighton.ac.uk
Abstract
The TUNA Challenge was a set of three shared
tasks at REG?08, all of which used data from
the TUNA Corpus. The three tasks covered
attribute selection for referring expressions
(TUNA-AS), realisation (TUNA-R) and end-to-
end referring expression generation (TUNA-
REG). 8 teams submitted a total of 33 systems
to the three tasks, with an additional submis-
sion to the Open Track. The evaluation used
a range of automatically computed measures.
In addition, an evaluation experiment was car-
ried out using the peer outputs for the TUNA-
REG task. This report describes each task and
the evaluation methods used, and presents the
evaluation results.
1 Introduction
The TUNA Challenge 2008 built on the foundations
laid in the ASGRE 2007 Challenge (Belz and Gatt,
2007), which consisted of a single shared task, based
on a subset of the TUNA Corpus (Gatt et al, 2007).
The TUNA Corpus is a collection of human-authored
descriptions of a referent, paired with a represen-
tation of the domain in which that description was
elicited.
The 2008 Challenge expanded the scope of the
previous edition in a variety of ways. This year,
there were three shared tasks. TUNA-AS is the At-
tribute Selection task piloted in the 2007 ASGRE
Challenge, which involves the selection of a set of
attributes which are true of a target referent, and
help to distinguish it from its distractors in a do-
main. TUNA-R is a realisation task, involving the
mapping from attribute sets to linguistic descrip-
tions. TUNA-REG is an ?end to end? referring ex-
pression generation task, involving a mapping from
an input domain to a linguistic description of a target
referent. In addition, there was an Open Submission
Track, where participants were invited to submit a
report on any interesting research that involved the
shared task data, and an Evaluation Track, for which
submissions were invited on proposals for evalua-
tion methods. This year?s TUNA Challenge also ex-
panded considerably on the evaluation methods used
in the various tasks. The measures can be divided
into intrinsic, automatically computed methods, and
extrinsic measures obtained through a task-oriented
experiment involving human participants.
The training and development data for the Chal-
lenge included the full dataset used in the ASGRE
Challenge, that is, all of the 2007 training, develop-
ment and test data. For the 2008 edition, two new
test sets were constructed. Test Set 1 was used for
TUNA-R, Test Set 2 was used for both TUNA-AS and
TUNA-REG.
1.1 Overview of submissions
Overall, 8 research groups submitted 33 systems by
the deadline. Table 1 provides a summary of the sub-
missions. The extrinsic evaluation experiment was
carried out on peer outputs in the TUNA-REG task
only, using outputs from at most 4 systems per par-
ticipating group. The 10 systems included are indi-
cated in boldface in the table. An additional submis-
sion was made by the USP team to the Open Track.
No submissions were made to the Evaluation Track.
Given the number of submissions, space restrictions
do not permit us to give an overview of the charac-
teristics of the various systems; these can be found
in the reports authored by each participating group,
which are included in this volume.
198
Group Organisation TUNA-AS TUNA-R TUNA-REG
ATT AT&T Labs Research Inc.
ATT-DR-b ATT-R ATT-TemplateS-ws
ATT-DR-sf ATT-TemplateS-drws
ATT-FB-f ATT-Template-ws
ATT-FB-m ATT-Template-drws
ATT-FB-sf ATT-PermuteRank-ws
ATT-FB-sr ATT-PermuteRank-drws
ATT-Dependency-drws
ATT-Dependency-ws
DIT Dublin Institute of Technology
DIT-FBI DIT-CBSR DIT-FBI-CBSR
DIT-TVAS DIT-RBR DIT-TVAS-RBR
GRAPH University of Tilbug etc GRAPH-FP GRAPH-4+B?
IS University of Stuttgart IS-FP IS-GT IS-FP-GT
JUCSENLP Jadavpur University JU-PTBSGRE
NIL-UCM Universidad Complutense de Madrid NIL-UCM-MFVF NIL-UCM-BSC NIL-UCM-FVBS
OSU Ohio State University OSU-GP OSU-GP?
USP University of Sao Paolo USP-EACH-FREQ
Table 1: Overview of participating teams and systems, by task. TUNA-REG peer systems whose outputs were included
in the extrinsic, task-based evaluation are shown in boldface. Systems marked ? were submissions to TUNA-AS which
made use of the off-the-shelf ASGRE realiser for their entries to TUNA-REG.
Participants in TUNA-AS and TUNA-R were also
given the opportunity to submit peer outputs for
TUNA-REG, and having them included in the ex-
trinsic evaluation, by making the use of off-the-
shelf modules. For systems in TUNA-AS, we
made available a template-based realiser, written by
Irene Langkilde-Geary at the University of Brighton.
Originally used in the 2007 ASGRE Challenge, this
was re-used by some TUNA-AS participants to re-
alise their outputs. Systems which made use of this
facility are marked by a (*) in Table 1.
In the rest of this report, we first give an overview
of the tasks and the data used for the Challenge (Sec-
tion 2), followed by a description of the evaluation
methods (Section 3). Section 4 gives the compar-
ative evaluation results for each task, followed by
a few concluding remarks in Section 5. In what
follows, we will use the following terminology, in
keeping with their usage in Belz and Gatt (2007): a
peer system is a system submitted to the shared-task
challenge, while peer output is an attribute set or a
description (in the form of a word string) produced
by a peer system. We will refer to a description in
the TUNA corpus as a reference output.
2 Data and task overview
2.1 The TUNA Data
The TUNA corpus was constructed via an elicita-
tion experiment as part of the TUNA project1. Each
file in the data consists of a single pairing of a do-
main (representation of entities and their attributes)
and a human-authored description (reference output)
1http://www.csd.abdn.ac.uk/research/tuna/
<TRIAL CONDITION="+/-LOC" ID="...">
<DOMAIN>
<ENTITY ID="..." TYPE="target" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
<ENTITY ID="..." TYPE="distractor" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
...
</DOMAIN>
<WORD-STRING>
the string describing the target referent
</WORD-STRING>
<ANNOTATED-WORD-STRING>
the string in WORD-STRING annotated
with attributes in ATTRIBUTE-SET
</ANNOTATED-WORD-STRING>
<ATTRIBUTE-SET>
the set of domain attributes in the description
</ATTRIBUTE-SET>
</TRIAL>
Figure 1: Format of corpus items
which is intended to describe the target referent in
the domain. Only the singular descriptions in the
corpus were used for the TUNA Challenge.
The descriptions in the corpus are subdivided by
entity type: there are references to people, and refer-
ences to furniture items. In addition, the elicitation
experiment manipulated a single condition, ?LOC.
In the +LOC condition, experimental participants
were told that they could refer to entities using any
of their properties, including their location. In the
?LOC condition, they were discouraged from doing
so, though not prevented.
Figure 1 is an outline of the XML format used in
the Challenge. Each file has a root TRIAL node
with a unique ID and an indication of the experi-
mental condition. The DOMAIN node subsumes 7
199
ENTITY nodes, which themselves subsume a num-
ber of ATTRIBUTE nodes defining the properties of
an entity in attribute-value notation. The attributes
include properties such as an object?s colour or a
person?s clothing, and the location of the image in
the visual display which the DOMAIN represents.
Each ENTITY node indicates whether it is the target
referent or one of the six distractors, and also has a
pointer to the image that it represents. Images were
made available to the TUNA Challenge participants.
The WORD-STRING is the actual de-
scription typed by a human author, and the
ATTRIBUTE-SET is the set of attributes belonging
to the referent that the description includes. The
ANNOTATED-WORD-STRING node was only
provided in the training and development data,
to display how substrings of a human-authored
description were mapped to attributes to determine
the ATTRIBUTE-SET.
Training and development data: For the TUNA
Challenge, the 780 singular corpus instances were
divided into 80% training data and 20% develop-
ment data. This data consists of all the training,
development and test data used in the 2007 ASGRE
Challenge.
Test data: Two new test sets were constructed by
replicating the original TUNA elicitation experi-
ment. The new experiment was designed to ensure
that each DOMAIN in the new test sets had two
reference outputs. Thus, this year?s corpus-based
evaluations are conducted against multiple instances
of each input DOMAIN. Both sets consisted of 112
items, divided equally into furniture and people
descriptions, sampled from both experimental
conditions (?LOC). Test Set 1 was used for the
TUNA-R Task. Participants in this task received a
version of the test set whose items consisted of a
DOMAIN node and an ATTRIBUTE-SET node.
There were 56 unique DOMAINs, each represented
twice in the test set, with two attribute sets from two
different human authors. Because each DOMAIN
and ATTRIBUTE-SET combination in this test
set is unique, the results for this task are reported
below over the whole of Test Set 1. Test Set 2
was used for the TUNA-AS and TUNA-REG Tasks.
For these tasks, the test items given to participants
consisted of a DOMAIN node only. There were
112 unique DOMAINs; the evaluations on these
tasks were conducted by comparing each peer
output to two different reference outputs for each
of these domains. Therefore, in the TUNA-AS and
TUNA-REG tasks, the data presented here averages
over the two outputs per DOMAIN.
2.2 The tasks
Task 1: Attribute Selection (TUNA-AS): The
TUNA-AS task focused on content determination for
referring expressions, and follows the basic prob-
lem definition used in much previous work in the
area: given a domain and a target referent, select a
subset of the attributes of that referent which will
help to distinguish it from its distractors. The inputs
for this task consisted of a TRIAL node enclosing a
DOMAIN node (a representation of entities and prop-
erties). A peer output was a TRIAL node enclos-
ing just an ATTRIBUTE-SET node whose children
were the attributes selected by a peer system for the
target entity.
Task 2: Realisation (TUNA-R): The TUNA-R task
focussed on realisation. The aim was to map an
ATTRIBUTE-SET node to a word string which de-
scribes the ENTITY that is marked as the target such
that the entity can be identified in the domain. The
inputs for this task consisted of a TRIAL node en-
closing a DOMAIN and an ATTRIBUTE-SET node.
A peer output for this task consisted of a TRIAL
node enclosing just a WORD-STRING node.
Task 3: ?End-to-end? Referring Expression Gen-
eration (TUNA-REG): For the TUNA-REG task, the
input consisted of a DOMAIN, and a peer output was
a word string which described the entity marked as
the target such that the entity could be identified in
the domain. The input for this task was identical to
that for TUNA-AS, i.e. a TRIAL node enclosing just
a DOMAIN node. A peer output for this task was
identical in format to that for the TUNA-R task, i.e. a
TRIAL enclosing just a WORD-STRING node.
3 Evaluation methods
The evaluation methods used in each task, and the
quality criteria that they assess, are summarised in
Table 2. Peer outputs from all tasks were evalu-
ated using intrinsic methods. All of these were au-
tomatically computed, and are subdivided into (a)
200
Task Criterion Type Methods
TUNA-AS Humanlikeness Intrinsic Accuracy, Dice, MASI
Minimality Intrinsic Proportion of minimal outputs
Uniqueness Intrinsic Proportion of unique outputs
TUNA-R Humanlikeness Intrinsic Accuracy, BLEU, NIST, string-edit distance
TUNA-REG Humanlikeness Intrinsic Accuracy, BLEU, NIST string-edit distance
Ease of comprehension Extrinsic Self-paced reading in identification experiment
Referential Clarity Extrinsic Speed and accuracy in identification experiment
Table 2: Evaluation methods used per task
those measures that assess humanlikeness, i.e. the
degree of similarity between a peer output and a ref-
erence output; and (b) measures that assess intrin-
sic properties of peer outputs. Peer outputs from
the TUNA-REG task were also included in a human,
task-oriented evaluation, which is extrinsic insofar
as it measures the adequacy of a peer output in terms
of its utility in an externally defined task. In the re-
mainder of this section, we summarise the properties
of the intrinsic methods. Section 3.1 describes the
experiment conducted for the extrinsic evaluation.
Dice coefficient (TUNA-AS): This is a set-
comparison metric, ranging between 0 and 1, where
1 indicates a perfect match between sets. For two
attribute sets A and B, Dice is computed as follows:
Dice(A,B) =
2? |A ?B|
|A|+ |B|
(1)
MASI (TUNA-AS): The MASI score (Passonneau,
2006) is an adaptation of the Jaccard coefficient
which biases it in favour of similarity where one set
is a subset of the other. Like Dice, it ranges between
0 and 1, where 1 indicates a perfect match. It is com-
puted as follows:
MASI(A,B) = ? ?
|A ?B|
|A ?B|
(2)
where ? is a monotonicity coefficient defined as fol-
lows:
? =
?
???
???
0 if A ?B = ?
1 if A = B
2
3 if A ? B or B ? A
1
3 otherwise
(3)
Accuracy (all tasks): This is computed as the pro-
portion of the peer outputs of a system which have
an exact match to a reference output. In TUNA-AS,
Accuracy was computed as the proportion of times a
system returned an ATTRIBUTE-SET identical to
the reference ATTRIBUTE-SET produced by a hu-
man author for the same DOMAIN. In TUNA-R and
TUNA-REG, Accuracy was computed as the propor-
tion of times a peer WORD-STRING was identical
to the reference WORD-STRING produced by an au-
thor for the same DOMAIN.
String-edit distance (TUNA-R, TUNA-REG): This
is the classic Levenshtein distance measure, used to
compare the difference between a peer output and a
reference output in the corpus, as the minimal num-
ber of insertions, deletions and/or substitutions of
words required to transform one string into another.
The cost for insertions and deletions was set to 1,
that for substitutions to 2. Edit distance is an integer
bounded by the length of the longest description in
the pair being compared.
BLEU (TUNA-R, TUNA-REG): This is an n-gram
based string comparison measure, originally pro-
posed by Papineni et al (2002) for evaluation of
Machine Translation systems. It evaluates a system
based on the proportion of word n-grams (consid-
ering all n-grams of length n ? 4 is standard) that
it shares with several reference translations. Unlike
Dice, MASI and String-edit, BLEU is by definition an
aggregate measure (i.e. a single BLEU score is ob-
tained for a system based on the entire set of items
to be compared, and this is generally not equal to the
average of BLEU scores for individual items). BLEU
ranges between 0 and 1.
NIST (TUNA-R, TUNA-REG): This is a version of
BLEU, which gives more importance to less frequent
(hence more informative) n-grams. The range of
NIST scores depends on the size of the test set. Like
BLEU, this is an aggregate measure.
Uniqueness (TUNA-AS): This measure was in-
cluded for backwards comparability with the ASGRE
Challenge 2007. It is defined as the proportion of
peer ATTRIBUTE-SETs which identify the target
referent uniquely, i.e. whose (logical conjunction of)
201
attributes are true of the target, and of no other entity
in the DOMAIN.
Minimality (TUNA-AS): This measure was defined
as the proportion of peer ATTRIBUTE-SETs which
are minimal, where ?minimal? means that there is
no attribute-set which uniquely identifies the target
referent in the domain which is smaller. Note that
this definition includes Uniqueness as a prerequisite,
since the description must identify the target entity
uniquely in order to qualify for Minimality.
All intrinsic evaluation methods except for BLEU
and NIST were computed (a) overall, using the entire
test data set (i.e. Test Set 1 or 2 as appropriate); and
(b) by object type, that is, computing separate values
for outputs referring to targets of type furniture and
people.
3.1 Extrinsic evaluation in TUNA-REG
The experiment for the extrinsic evaluation of
TUNA-REG peer outputs combined a self-paced
reading and identification paradigm, comparing the
peer outputs from 10 of the TUNA-REG systems
shown in Table 1, as well as the two sets of human-
authored reference outputs for Test Set 2. We refer
to the latter as HUMAN-1 and HUMAN-2 in what fol-
lows2.
In the task given to experimental subjects, a trial
consisted of a description paired with a visual do-
main representation corresponding to an item in Test
Set 2. Each trial was split into two phases: (a) in an
initial reading phase, subjects were presented with
the description only. This phase was terminated by
subjects once they had read the description. (b) In
the second, identification phase, subjects saw the vi-
sual domain in which the description had been pro-
duced, consisting of images of the domain entities
in the same spatial configuration as that in the test
set DOMAIN. They clicked on the object that they
thought was the intended referent of the description
they had read.
The experiment yielded three dependent mea-
sures: (a) reading time (RT), measured from the
point at which the description was presented, to the
2Note that HUMAN-1 and HUMAN-2 were both sets of de-
scriptions randomly sampled from the data collected in the ex-
periment. Each set of human descriptions contains output from
different human authors.
point at which a participant called up the next screen
via mouse click; (b) identification time (IT), mea-
sured from the point at which pictures (the visual
domain) were presented on the screen to the point
where a participant identified a referent by clicking
on it; (c) error rate (ER), the proportion of times the
wrong referent was identified.
This design differs from that used in the 2007
ASGRE Challenge, in which descriptions and visual
domains were presented in a single phase (on the
same screen), so that RT and IT were conflated. The
new experiment replicates the methodology reported
in Gatt and Belz (2008), in a follow-up study on
the ASGRE 2007 data. Another difference between
the two experiments is that the current one is based
on peer outputs which are themselves realisations,
whereas the ASGRE experiment involved attribute
sets which had to be realised before they could be
used.
Design: We used a Repeated Latin Squares design,
in which each combination of SYSTEM3 and test set
item is allocated one trial. Since there were 12 lev-
els of SYSTEM, but 112 test set items, 8 randomly
selected items (4 furniture and 4 people) were du-
plicated, yielding 120 items and 10 12 ? 12 latin
squares. The items were divided into two sets of 60.
Half of the participants did the first 60 items (the first
5 latin squares), and the other half the second 60.
Participants and procedure: The experiment was
carried out by 24 participants recruited from among
the faculty and administrative staff of the Univer-
sity of Brighton, as well as from among the au-
thors? acquaintances. Participants carried out the
experiment under supervision in a quiet room on a
laptop. Stimulus presentation was carried out us-
ing DMDX, a Win-32 software package for psy-
cholinguistic experiments involving time measure-
ments (Forster and Forster, 2003). Participants initi-
ated each trial, which consisted of an initial warning
bell and a fixation point flashed on the screen for
1000ms. They then read the description and called
up the visual domain to identify the referent. Trials
timed out after 15000ms.
Treatment of outliers and timeouts: Trials which
3The SYSTEM independent variable in this experiment in-
cludes HUMAN-1 and HUMAN-2.
202
timed out with no response were discounted from
the analysis. Out of a total of (24 ? 60 =) 1440
trials, there were 4 reading timeouts (0.3%) and 7
identification timeouts (0.5%). Outliers for RT and
IT were defined as those exceeding a threshold of
mean ?2SD. There were 64 outliers on RT (4.4%)
and 191 on IT (13.3%). Outliers were replaced by
the overall mean for RT and IT (see Ratliff (1993)
for discussion of this method).
4 Evaluation results
This section presents results for each of the tasks.
For all measures, except BLEU and NIST, we present
separate descriptive statistics by entity type (people
vs. furniture subsets of the relevant test set), and
overall.
4.1 Results for TUNA-AS
Descriptive statistics are displayed for all systems in
Table 3. This includes the Accuracy and Minimal-
ity scores (proportions), and mean MASI and Dice
scores. Values are displayed by entity type and over-
all. The standard deviation for Dice and MASI is
displayed overall. Scores average over both sets of
reference outputs in Test Set 2. All systems scored
100% on Uniqueness, and either 0 or 100% on Min-
imality. These measures are therefore not included
in the significance testing, though Minimality is in-
cluded in the correlations reported below.
Two 15 (SYSTEM) ? 2 (ENTITY TYPE) uni-
variate ANOVAs were conducted on the Dice and
MASI scores. We report significant effects at p ?
.001. There were main effects of SYSTEM (Dice:
F (13, 1540) = 193.08; MASI: F (13, 1540) =
93.45) and ENTITY TYPE (Dice: F (1, 1540) =
91.75; MASI: F (1, 1540) = 168.12), as well
as a significant interaction between the two (Dice:
F (13, 1540) = 7.45, MASI: F (13, 1540) = 7.35).
Post-hoc Tukey?s comparisons on both Dice and
MASI yielded the homogeneous subsets displayed in
Table 4.
Differences among systems on Accuracy were
analysed by coding this as an indicator variable: for
each peer output, the variable indicated whether it
achieved perfect match with at least one of the two
reference outputs on the same DOMAIN. A Kruskall-
Wallis test showed that the difference between sys-
tems was significant (?2 = 275.01, p < .001).
Minimality Accuracy Dice MASI
Minimality -0.877 -0.959 -0.901
Accuracy -0.877 0.973 0.998
Dice -0.959 0.973 0.985
MASI -0.901 0.998 0.985
Table 5: Correlations for TUNA-AS; all values are signif-
icant at p ? .05
Pairwise correlations using Pearson?s r are shown
in Table 5, for all measures except Uniqueness. All
correlations are positive and significant, with the ex-
ception of those involving Minimality, which cor-
relates negatively with all other measures (i.e. the
higher the proportion of minimal descriptions of a
system, the lower its score on humanlikeness, as
measured by Dice, MASI and Accuracy). This re-
sult corroborates a similar finding in the 2007 AS-
GRE Challenge.
4.2 Results for TUNA-R
Table 6 shows descriptives for the 5 participating
systems in TUNA-R. Once again, mean Edit scores
and Accuracy proportions are shown both overall
and by entity type, while BLEU and NIST are overall
aggregate scores.
A 15 (SYSTEM) ? 2 (ENTITY TYPE) univariate
ANOVA was conducted on the Edit Distance scores.
There was no main effect of SYSTEM, and no in-
teraction, but ENTITY TYPE exhibited a main effect
(F (1, 550) = 19.99, p < .001). Given the lack of a
main effect, no post-hoc comparisons between sys-
tems were conducted. A Kruskall-Wallis test also
showed no difference between systems on Accu-
racy. Pairwise correlations between all measures are
shown in Table 7; this time, the only significant cor-
relation is between NIST and BLEU.
Edit Accuracy NIST BLEU
Edit 0.195 -0.095 0.099
Accuracy 0.195 0.837 0.701
NIST -0.095 0.837 0.900?
BLEU 0.099 0.701 0.900?
Table 7: Correlations for the TUNA-R task (? indicates
p ? .05).
4.3 Results for TUNA-REG
4.3.1 Tests on the intrinsic measures
Results for the intrinsic measures on the TUNA-
REG task are shown in Table 8. As in Section 4.1,
203
Dice MASI Accuracy Minimality
furniture people both SD furniture people both SD furniture people both both
GRAPH 0.858 0.729 0.794 0.160 0.705 0.465 0.585 0.272 0.53 0.56 0.40 0.00
JU-PTBSGRE 0.858 0.762 0.810 0.152 0.705 0.501 0.603 0.251 0.55 0.58 0.41 0.00
ATT-DR-b 0.852 0.722 0.787 0.154 0.663 0.441 0.552 0.283 0.52 0.54 0.36 0.00
ATT-DR-sf 0.852 0.722 0.787 0.154 0.663 0.441 0.552 0.283 0.50 0.52 0.36 0.00
DIT-FBI 0.850 0.731 0.791 0.153 0.661 0.451 0.556 0.280 0.50 0.53 0.36 0.00
IS-FP 0.828 0.723 0.776 0.165 0.641 0.475 0.558 0.278 0.52 0.54 0.37 0.00
NIL-UCM-MFVF 0.821 0.684 0.753 0.169 0.601 0.383 0.492 0.290 0.44 0.46 0.31 0.00
USP-EACH-FREQ 0.820 0.663 0.742 0.176 0.616 0.404 0.510 0.291 0.46 0.48 0.33 0.00
DIT-TVAS 0.814 0.684 0.749 0.166 0.580 0.383 0.482 0.285 0.43 0.46 0.29 0.00
OSU-GP 0.640 0.443 0.541 0.226 0.352 0.114 0.233 0.227 0.17 0.20 0.06 0.00
ATT-FB-m 0.357 0.263 0.310 0.245 0.164 0.119 0.141 0.125 0.13 0.14 0.00 1.00
ATT-FB-f 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00
ATT-FB-sf 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00
ATT-FB-sr 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00
Table 3: Descriptives for the TUNA-AS task. All means are shown by entity type; standard deviations are displayed
overall.
Dice MASI
ATT-FB-f A ATT-FB-f A
ATT-FB-sf A ATT-FB-sf A
ATT-FB-sr A ATT-FB-sr A
ATT-FB-m A ATT-FB-m A B
OSU-GP B OSU-GP B
USP-EACH-FREQ C DIT-TVAS C
DIT-TVAS C NIL-UCM-MFVF C D
NIL-UCM-MFVF C USP-EACH-FREQ C D E
IS-FP C ATT-DR-b C D E
ATT-DR-b C ATT-DR-sf C D E
ATT-DR-sf C DIT-FBI C D E
DIT-FBI C IS-FP C D E
GRAPH C GRAPH D E
JU-PTBSGRE C JU-PTBSGRE E
Table 4: Homogeneous subsets for systems in TUNA-AS. Systems which do not share a common letter are significantly
different at p ? .05
Edit Accuracy NIST BLEU
furniture people both SD furniture people both both both
IS-GT 7.750 9.768 8.759 6.319 0.02 0.00 0.01 0.4526 0.0415
NIL-UCM-BSC 7.411 9.143 8.277 6.276 0.05 0.04 0.04 1.7034 0.0784
ATT-1-R 7.143 9.268 8.205 6.140 0.02 0.00 0.01 0.1249 0
DIT-CBSR 7.054 10.286 8.670 6.873 0.09 0.02 0.05 1.1623 0.0686
DIT-RBR 6.929 9.857 8.393 6.668 0.04 0.00 0.02 0.9151 0.0694
Table 6: Descriptives for the TUNA-R task.
Edit Accuracy BLEU NIST
furniture people both SD furniture people both both both
ATT-PermuteRank-ws 8.339 8.304 8.321 3.283 0.00 0 0 0.007 0.0288
ATT-Template-ws 8.304 8.161 8.232 3.030 0.00 0 0 0 0.0059
ATT-Dependency-ws 8.232 8.000 8.116 3.023 0.00 0 0 0.0001 0.0139
ATT-TemplateS-ws 8.214 8.161 8.188 3.063 0.00 0 0 0 0.0057
OSU-GP 7.964 13.232 10.598 4.223 0.00 0 0 1.976 0.0236
ATT-PermuteRank-drws 7.464 8.411 7.938 3.431 0.02 0.04 0.03 0.603 0.0571
DIT-TVAS-RBR 6.893 8.161 7.527 3.358 0.05 0 0.03 1.0233 0.0659
ATT-TemplateS-drws 6.786 7.679 7.232 3.745 0.07 0.02 0.04 0.6786 0.0958
ATT-Template-drws 6.768 7.696 7.232 3.757 0.07 0.02 0.04 0.6083 0.0929
NIL-UCM-FVBS 6.643 8.411 7.527 3.618 0.07 0.04 0.05 1.8277 0.0684
IS-FP-GT 6.607 7.304 6.955 3.225 0.05 0.02 0.04 0.8708 0.1086
DIT-FBI-CBSR 6.536 7.643 7.089 3.889 0.16 0.05 0.11 0.8804 0.1259
ATT-Dependency-drws 6.482 7.446 6.964 3.349 0.07 0 0.04 0.3427 0.0477
GRAPH 5.946 9.018 7.482 3.541 0.18 0 0.09 1.141 0.0696
Table 8: Descriptives for TUNA-REG on the intrinsic measures.
means for the intrinsic measures average over both
sets of reference outputs in Test Set 2.
A 15 (SYSTEM) ?2 (ENTITY TYPE) univariate
ANOVA was conducted on the Edit Distance scores.
There were significant main effects of SYSTEM
(F (13, 1540) = 8.6, p < .001) and ENTITY TYPE
(F (1, 1540) = 47.5, p < .001), as well as a signif-
icant interaction (F (13, 1540) = 5.77, p < .001).
A Kruskall-Wallis test on Accuracy, coded as an in-
dicator variable (see Section 4.2), showed that sys-
tems differed significantly on this measure as well
(?2 = 26.27, p < .05).
Post-hoc Tukey?s comparisons were conducted on
Edit Distance; the homogeneous subsets are shown
in Table 9. The table suggests that the main effect
of Edit Distance may largely have been due to the
204
difference between OSU-GP and all other systems.
Correlations between these measures are shown
in Table 10. Contrary to the results in Section 4.2,
the correlation between BLEU and NIST does not
reach significance. The negative correlations be-
tween Edit distance and Accuracy, and between Edit
and BLEU are as expected, since higher Edit cost im-
plies greater distance from a reference output.
IS-FP-GT A
ATT-Dependency-drws A
DIT-FBI-CBSR A
ATT-Template-drws A
ATT-TemplateS-drws A
GRAPH-4+B A
DIT-TVAS-RBR A
NIL-UCM-FVBS A
ATT-PermuteRank-drws A
ATT-Dependency-ws A
ATT-TemplateS-ws A
ATT-Template-ws A
ATT-PermuteRank-ws A
OSU-GP B
Table 9: Homogeneous subsets for systems in TUNA-
REG, Edit Distance measure. Systems which do not share
a common letter are significantly different at p ? .05
Edit Accuracy NIST BLEU
Edit -0.584? 0.250 -0.636?
Accuracy -0.584? 0.383 0.807??
NIST 0.250 0.383 0.371
BLEU -0.636? 0.807?? 0.371
Table 10: Correlations for TUNA-REG (? indicates p ?
.05; ?? indicates p ? .01).
4.3.2 Tests on the extrinsic measures
Table 11 displays the results for the extrinsic mea-
sures. Reading time (RT), identification time (IT)
and error rate (ER), are displayed only for the sys-
tems that participated in the evaluation experiment,
as well as for the two sets of reference outputs
HUMAN-1 and HUMAN-2.
Separate univariate ANOVAs were conducted test-
ing the effect of SYSTEM and ENTITY TYPE on IT
and RT. For IT, there was a significant main effect
of SYSTEM (F (11, 1409) = 5.66, p < .001) and
ENTITY TYPE (F (1, 1409) = 23.507, p < .001),
as well as a significant interaction (F (11, 1409) =
2.378, p < .05). The same pattern held for RT, with
a main effect of SYSTEM (F (11, 1412) = 9.95, p <
.001) and ENTITY TYPE (F (1, 1412) = 9.74, p <
.05) and a significant interaction (F (11, 1412) =
2.064, p < .05). A Kruskall-Wallis test conducted
on ER showed a significant impact of SYSTEM on the
extent to which experimental participants identified
the wrong referents (?2 = 35.45, p < .001). The
homogeneous subsets yielded by post-hoc Tukey?s
comparisons among systems, on both RT and IT, are
displayed in Table 12.
Finally, pairwise correlations were estimated be-
tween all three extrinsic measures. The only sig-
nificant correlation was between RT and IT (r =
.784, p < .05), suggesting that the longer experi-
mental subjects took to read a description, the longer
they also took to identify the target referent.
5 Conclusion
The first ASGRE Challenge, held in 2007, was re-
garded and presented as a pilot event, for a research
community in which there was growing interest in
comparative evaluation on shared datasets. Refer-
ring Expression Generation was an ideal starting
point, because of its relatively long history within
the NLG community, and the widespread agreement
on inputs, outputs and task definitions.
The tasks described and evaluated in this report
constitute a broadening of scope over the 2007 Chal-
lenge. Like the previous Challenge, the 2008 edition
emphasised diversity in terms of the measures of
quality used. This year, there was also an increased
emphasis on broadening the range of tasks, with the
inclusion of realisation and end-to-end referring ex-
pressions generation. This extends the scope of the
REG problem, which has traditionally been focussed
on content determination (attribute selection) for the
most part. As for evaluation, the diversity of mea-
sures can shed light on different aspects of quality
in these tasks. The fact that the correlation among
measures based on different quality criteria is not
straightforward is in itself an argument for maintain-
ing this diversity, particularly as comparative evalu-
ation exercises such as this one provide the oppor-
tunity for further investigation of the nature of these
relationships.
Another indicator of the growing diversity in this
year?s Challenge is the range of algorithmic solu-
tions in the three tasks, ranging from new models
based on classical algorithms, to data-driven meth-
ods, evolutionary algorithms, and graph- and tree-
based frameworks. The body of work represented
by submissions to the TUNA-R and TUNA-REG tasks
is also interesting for its exploration of how to apply
205
RT IT ER
furniture people both SD furniture people both SD furniture people both
HUMAN-1 2155.376 2187.737 2171.693 2036.462 1973.369 1911.742 1942.297 809.5139 11.864 6.780 9.322
OSU-GP 2080.532 3204.198 2637.644 1555.003 2063.441 2274.690 2167.275 682.8325 6.667 18.966 12.712
HUMAN-2 1823.553 2298.467 2061.010 1475.005 1873.621 1945.880 1909.750 761.3386 16.667 5.000 10.833
ATT-PremuteRank-drws 1664.911 1420.087 1543.528 1392.729 1765.731 1719.456 1742.788 675.3462 10.000 8.475 9.244
DIT-FBI-CBSR 1581.535 1521.799 1551.667 1170.031 1528.119 1932.806 1732.163 694.9878 10.169 10.000 10.084
NIL-UCM-FVBS 1561.291 1933.833 1747.562 1428.490 1531.378 1723.148 1627.263 672.9894 6.667 3.333 5.000
GRAPH 1499.582 1516.804 1508.193 952.158 1706.153 2026.268 1866.211 704.0210 5.000 5.000 5.000
DIT-TVAS-RBR 1485.149 1442.573 1463.861 998.332 1559.953 1734.853 1647.403 588.4615 8.333 13.333 10.833
ATT-Dependency-drws 1460.152 1583.887 1522.019 1177.817 1505.059 2078.336 1791.697 725.9459 1.667 18.333 10.000
ATT-TemplateS-drws 1341.245 1641.539 1490.130 1098.304 1656.401 1720.365 1687.841 650.8357 3.333 10.345 6.780
IS-FT-GT 1292.754 1614.712 1453.733 1374.652 1616.855 1884.557 1750.706 732.4362 6.667 1.667 4.167
ATT-PermuteRank-ws 1218.136 1450.603 1334.369 1203.975 1876.680 1831.485 1854.082 688.3493 31.667 13.333 22.500
Table 11: Descriptives for the extrinsic measures in TUNA-REG.
IT RT
NIL-UCM-FVBS A ATT-PermuteRank-ws A
DIT-TVAS-RBR A IS-FT-GT A
ATT-TemplateS-drws A B DIT-TVAS-RBR A
DIT-FBI-CBSR A B ATT-TemplateS-drws A
ATT-PremuteRank-drws A B GRAPH-4+B A B
IS-FT-GT A B ATT-Dependency-drws A B
ATT-Dependency-drws A B ATT-PremuteRank-drws A B
ATT-PermuteRank-ws A B DIT-FBI-CBSR A B
GRAPH-4+B A B NIL-UCM-FVBS A B C
HUMAN-2 A B C HUMAN-2 B C
HUMAN-1 B C HUMAN-1 C D
OSU-GP C OSU-GP D
Table 12: Homogeneous subsets for systems in TUNA-REG, extrinsic time measures. Systems which do not share a
common letter are significantly different at p ? .05
realisation techniques to the specific problem posed
by referring expressions.
The outcomes of this evaluation exercise are ob-
viously not intended to be a ?final word? on the right
way to carry out evaluation in referring expressions
generation. Rather, comparative results open up the
possibility of improvement and change. Another im-
portant aspect of a shared task of this nature is that
it results in an archive of data that can be further
exploited, either through follow-up studies, or for
the provision of baselines against which to compare
novel approaches. We have already used the data
from ASGRE 2007 for further investigation, particu-
larly in the area of extrinsic evaluation. We plan to
carry out more such studies in the future.
Acknowledgements
Our heartfelt thanks to the participants who helped
to make this event a success. Thanks to Advaith Sid-
dharthan, who proposed MASI for TUNA-AS.
References
A. Belz and A. Gatt. 2007. The attribute selection for
gre challenge: Overview and evaluation results. In
Proc. UCNLG+MT: Language Generation and Ma-
chine Translation.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy. Be-
havior Research Methods, Instruments, & Computers,
35(1):116?124.
A. Gatt and A. Belz. 2008. Attribute selection for refer-
ring expression generation: New algorithms and evalu-
ation methods. In Proceedings of the 5th International
Conference on Natural Language Generation (INLG-
08).
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proc. 11th
European Workshop on Natural Language Generation
(ENLG-07).
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
Bleu: a. method for automatic evaluation of machine
translation. In Proc. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-02), pages
311?318.
R. Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic anno-
tation. In Proc. 5th International Conference on Lan-
guage Resources and Evaluation (LREC-06).
R. Ratliff. 1993. Methods for dealing with reaction time
outliers. Psychological Bulletin, 114(3):510?532.
206
Comparing Rating Scales and Preference Judgements
in Language Evaluation
Anja Belz Eric Kow
Natural Language Technology Group
School of Computing, Mathematical and Information Sciences
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Abstract
Rating-scale evaluations are common in
NLP, but are problematic for a range of
reasons, e.g. they can be unintuitive for
evaluators, inter-evaluator agreement and
self-consistency tend to be low, and the
parametric statistics commonly applied to
the results are not generally considered
appropriate for ordinal data. In this pa-
per, we compare rating scales with an al-
ternative evaluation paradigm, preference-
strength judgement experiments (PJEs),
where evaluators have the simpler task of
deciding which of two texts is better in
terms of a given quality criterion. We
present three pairs of evaluation experi-
ments assessing text fluency and clarity
for different data sets, where one of each
pair of experiments is a rating-scale ex-
periment, and the other is a PJE. We find
the PJE versions of the experiments have
better evaluator self-consistency and inter-
evaluator agreement, and a larger propor-
tion of variation accounted for by system
differences, resulting in a larger number of
significant differences being found.
1 Introduction
Rating-scale evaluations, where human evaluators
assess system outputs by selecting a score on a dis-
crete scale, are the most common form of human-
assessed evaluation in NLP. Results are typically
presented in rank tables of means for each system
accompanied by means-based measures of statisti-
cal significance of the differences between system
scores.
NLP system evaluation tends to involve sets of
systems, rather than single ones (evaluations tend
to at least incorporate a baseline or, more rarely, a
topline system). The aim of system evaluation is
to gain some insight into which systems are bet-
ter than which others, in other words, the aim is
inherently relative. Yet NLP system evaluation ex-
periments have generally preferred rating scale ex-
periments where evaluators assess each system?s
quality in isolation, in absolute terms.
Such rating scales are not very intuitive to use;
deciding whether a text deserves a 5, a 4 or a 3
etc. can be difficult. Furthermore, evaluators may
ascribe different meanings to scores and the dis-
tances between them. Individual evaluators have
different tendencies in using rating scales, e.g.
what is known as ?end-aversion? tendency where
certain individuals tend to stay away from the ex-
treme ends of scales; other examples are positive
skew and acquiescence bias, where individuals
make disproportionately many positive or agree-
ing judgements; see e.g. Choi and Pak, (2005).
It is not surprising then that stable averages of
quality judgements, let alne high levels of agree-
ment, are hard to achieve, as has been observed for
MT (Turian et al, 2003; Lin and Och, 2004), text
summarisation (Trang Dang, 2006), and language
generation (Belz and Reiter, 2006). It has even
been demonstrated that increasing the number of
evaluators and/or data can have no stabilising ef-
fect at all on means (DUC literature).
The result of a rating scale experiment is ordi-
nal data (sets of scores selected from the discrete
rating scale). The means-based ranks and statisti-
cal significance tests that are commonly presented
with the results of RSEs are not generally consid-
ered appropriate for ordinal data in the statistics
literature (Siegel, 1957). At the least, ?a test on the
means imposes the requirement that the measures
must be additive, i.e. numerical? (Siegel, 1957, p.
14). Parametric statistics are more powerful than
non-parametric alternatives, because they make a
number of strong assumptions (including that the
data is numerical). If the assumptions are violated
then the risks is that the significance of results is
overestimated.
In this paper we explore an alternative evalua-
tion paradigm, Preference-strength Judgement Ex-
periments (PJEs). Binary preference judgements
have been used in NLP system evaluation (Reiter et
al., 2005), but to our knowledge this is the first sys-
tematic investigation of preference-strength judge-
ments where evaluators express, in addition to
their preference (which system do you prefer?),
also the strength of their preference (how strongly
do you prefer the system you prefer?). It seems
intuitively convincing that it should be easier to
decide which of two texts is clearer than to de-
cide whether a text?s clarity deserves a 1, 2, 3, 4 or
5. However, it is less clear whether evaluators are
also able to express the strength of their preference
in a consistent fashion, resulting not only in good
self-consistency, but also in good agreement with
other evaluators.
We present three pairs of directly comparable
RSE and PJE evaluations, and investigate how they
compare in terms of (i) the amount of variation ac-
counted for by differences between systems (the
more the better), relative to the amount of varia-
tion accounted for by other factors such as evalu-
ator and arbitrary text properties (the less the bet-
ter); (ii) inter-evaluator agreement, (iii) evaluator
self-consistency, (iv) the number of significant dif-
ferences identified, and (v) experimental cost.
2 Overview of Experiments
In the following three sections we present the de-
sign and results of three pairs of evaluations. Each
pair consists of a rating-scale experiment (RSE)
and a preference-strength judgement experiment
(PJE) that differ only in the rating method they em-
ploy (relative ratings in the PJE and absolute rat-
ings in the RSE).1 In other words, they involve the
same set of system outputs, the same instructions
and method of presentating system outputs. Each
pair is for a different data domain and system task,
the first for generating chains of references to peo-
ple in Wikipedia articles (Section 3); the second
for weather forecast text generation (Section 4);
and the third for generating descriptions of images
of furniture and faces (Section 5).
All experiments use a Repeated Latin Squares
1We are currently preparing an open-source release of the
RSE/PJE toolkit we have developed for implementing the ex-
periments described in this paper which automatically gen-
erates an experiment, including webpages, given some user-
specified parameters and the data to be evaluated.
Figure 1: Standardised 1?5 rating scale represen-
tation for Fluency and Clarity criteria.
design which ensures that each subject sees the
same number of outputs from each system and
for each test set item. Following detailed instruc-
tions, subjects first do 2 or 3 practice examples,
followed by the texts to be evaluated, in an order
randomised for each subject. Subjects carry out
the evaluation over the internet, at a time and place
of their choosing. They are allowed to interrupt
and resume (but are discouraged from doing so).
There are subtle differences between the three
experiment pairs, and for ease of comparison we
provide an overview of the six experiments we in-
vestigate in this paper in Table 1. Each of the as-
pects of experimental design and execution shown
in this table is explained and described in more de-
tail in the relevant subsection below, but some of
the important differences are highlighted here.
In GREC-NEG PJE, each system is compared
with only one other comparisor system (a human-
authored topline), whereas in the other two PJE ex-
periments, each system is compared with all other
systems for each test data set item.
In the two versions of the METEO evaluation,
evaluators were not drawn from the same cohort of
people, whereas in the other two evaluation pairs
they were drawn from the same cohort. GREC-
NEG RSE and METEO RSE used radio buttons (as
shown in Figure 1) as the rating-scale evaluation
mechanism whereas in TUNA RSE it was an un-
marked slider bar. While slightly different names
were used for the evaluation criteria in two of
the evaluation pairs, Fluency/Readability were ex-
plained in very similar terms (does it read well?),
and Adequacy in TUNA was explained in terms of
clarity of reference (is it clear which entity the de-
scription refers to?), so there are in fact just two
evaluation criteria (albeit with different names).
Where we use preference-strength judgements,
Data set GREC-NEG METEO TUNA
Type RSE PJE RSE PJE RSE PJE
Criteria names Fluency, Clarity Readability, Clarity Fluency, Adequacy
Evaluator type linguistics students uni staff ling stud linguistics students
Num evaluators 10 10 22 22 8 28
Comparisor(s) ? human topline ? all systems ? all systems
Test set size 30 22 112
N trials 300 300 484 1210 896 3136
Rating tool radio buttons slider radio buttons slider slider bar slider
Range 1?5 ?10.0.. + 10.0 1?7 ?50.0.. + 50.0 0?100 ?50.0.. + 50.0
Numbers visible? yes no yes no no no
Table 1: Overview of experiments with details of design and execution. (Comparisor(s) = the other
systems against which each system is evaluated.)
the evaluation mechanism is implemented using
slider bars as shown at the bottom of Figure 2
which map to a scale ?X.. + X. The evalua-
tor?s task is to express their preference in terms of
each quality criterion by moving the pointers on
the sliders. Moving the pointer to the left means
expressing a preference for the text on the left,
moving it to the right means preferring the text on
the right; the further to the left/right the slider is
moved, the stronger the preference. It was not ev-
ident to the evaluators that sliders were associated
with numerical values. Slider pointers started out
in the middle of the scale (the position correspond-
ing to no preference). If they wanted to leave the
pointer in the middle (i.e. if they had no prefer-
ence for either of the two texts), evaluators had to
check a box to confirm their rating (to avoid evalu-
ators accidentally not rating a text and leaving the
pointer in the default position).
3 GREC-NEG RSE/PJE: Named entity
reference chains
3.1 Data and generic design
In our first pair of experiments we used system and
human outputs for the GREC-NEG task of selecting
referring expressions for people in discourse con-
text. The GREC-NEG data2 consists of introduction
sections from Wikipedia articles about people in
which all mentions of people have been annotated
by marking up the word strings that function as
referential expressions (REs) and annotating them
with coreference information as well as syntactic
and semantic features. The following is an exam-
ple of an annotated RE from the corpus:
<REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np"
SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="name"
2The GREC-NEG data and documen-
tation is available for download from
http://www.nltg.brighton.ac.uk/home/Anja.Belz
CASE="plain">Sir Alexander Fleming</REFEX> </REF>
(6 August 1881 - 11 March 1955) was a Scottish biol-
ogist and pharmacologist.
This data was used in the GREC-NEG?09
shared-task competition (Belz et al, 2009), where
the task was to create systems which automatically
select suitable REs for all references to all person
entities in a text.
The evaluation experiments use Clarity and Flu-
ency as quality criteria which were explained in
the introduction as follows (the wording of the first
is from DUC):
1. Referential Clarity: It should be easy to identify who
the referring expressions are referring to. If a person
is mentioned, it should be clear what their role in the
story is. So, a reference would be unclear if a person
is referenced, but their identity or relation to the story
remains unclear.
2. Fluency: A referring expression should ?read well?,
i.e. it should be written in good, clear English, and the
use of titles and names should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
The evaluations involved outputs for 30 randomly
selected items from the test set from 5 of the 6
systems which participated in GREC-NEG?10, the
four baseline systems developed by the organisers,
and the original corpus texts (10 systems in total).
3.2 Preference judgement experiment
The human-assessed intrinsic evaluation in
GREC?09 was designed as a preference-judgement
test where subjects expressed their preference, in
terms of the two criteria, for either the original
Wikipedia text (human-authored ?topline?) or
the version of it with system-selected referring
expressions in it. There were three 10x10 Latin
Squares, and a total of 300 trials (with two
judgements in each, one for Fluency and one for
Clarity) in this evaluation. The subjects were 10
Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.
native speakers of English recruited from cohorts
of students currently completing a linguistics-
related degree at Kings College London and
University College London.
Figure 2 shows what subjects saw during the
evaluation of an individual text pair. The place
(left/right) of the original Wikipedia article was
randomly determined for each individual evalua-
tion of a text pair. People references are high-
lighted in yellow/orange, those that are identical
in both texts are yellow, those that are different are
orange.3 The sliders are the standardised design
described in the preceding section.
3.3 Rating scale experiment
Our new experiment used our standardised radio
button design for a 1?5 rating scale as shown in
Figure 1. We used the same Latin Squares design
as for the PJE version, and recruited 10 different
evaluators from the same student cohorts at Kings
College London and University College London.
Evaluators saw just one text in each trial, with the
people references highlighted in yellow.
3.4 Results and comparative analysis
Measures comparing the results from the two ver-
sions of the GREC-NEG evaluation are shown in
Table 2. The first row for each experiment type
3When viewed in black and white, the orange highlights
are the slighly darker ones.
Type Measure Clarity Fluency
RSE F(9,290) 10.975** 35.998**
N sig diffs 19/45 27/45
K?s W (inter) .543** .760**
avg W (intra) .5275 .7192
( Text F(29,270) 2.512** 1.825** )
( Evaluator F(9,290) 3.998** .630 )
PJE F(9,290) 29.539** 26.596**
N sig diffs 26/45 24/45
K?s W (inter) .717** .725**
avg W (intra) .6909 .7125
( Text F(29,270) .910 1.237 )
( Evaluator F(9,290) 1.237 4.145** )
Table 2: GREC-NEG RSE/PJE: Results of analy-
ses looking at effect of System.
shows the F ratio as determined by a one-way
ANOVA with the evaluation criterion in question
as the dependent variable and System as the fac-
tor. F is the ratio of between-groups variability
over within-group (or residual) variability, i.e. the
larger the value of F, the more of the variability ob-
served in the data is accounted for by the grouping
factor, here System, relative to what variability re-
mains within the groups.
The second row shows the number of signifi-
cant differences out of the possible total, as deter-
mined by a Tukey?s HSD analysis. Kendall?s W
(interpretable as a coefficient of concordance) is
a commonly used measure of the agreement be-
tween judges and is based on mean rank. It ranges
from 0 to 1, and the closer to 1 it is the greater the
agreement. The fourth row (K?s W, inter) shows
the standard W measure, estimating the degree to
which the evaluators agreed. The 5th row (K?s W,
intra) shows the average W for repeated ratings
by the same judge, i.e. it is a measure of the av-
erage self-consistency achieved by the evaluators.
Finally, in the last two rows we give F-ratios for
Text (test data set item) and Evaluator, estimating
the effect these two have independently of System.
The F ratios and numbers of significant differ-
ences are very similar in the PJE version, but very
dissimilar in the RSE version of this experiment.
For Fluency, F is greater in the RSE version than
in the PJE version where there appear to be big-
ger differences between scores assigned by evalua-
tors. However, Kendall?s W shows that in terms of
mean score ranks, the evaluators agreed to a simi-
lar extent in both experiment versions.
Clarity in the RSE version has lower values
across the board than the rest of Table 2: it ac-
counts for less of the variation, has fewer signifi-
cant differences and lower levels of inter-evaluator
agreement and self-consistency. If the results from
the PJE version were not also available one might
be inclined to conclude that there was not as much
difference between systems in terms of Clarity as
there was in terms of Fluency. However, because
Fluency and Clarity have a similarly strong effect
in GREC-NEG PJE, it looks instead as though the
evaluators found it harder to apply the Clarity cri-
terion in GREC-NEG RSE than Fluency in GREC-
NEG RSE, and than Clarity in GREC-NEG PJE.
One way of interpreting this is that it is possible
to achieve the same good levels of inter-evaluator
and intra-evaluator variation for the Clarity crite-
rion as for Fluency (both as defined and applied
within the context of this specific experiment), and
that it is therefore worrying that the RSE version
does not achieve it.
4 METEO RSE/PJE: Weather forecasts
4.1 Data
Our second pair of evaluations used the Prodigy-
METEO4 version (Belz, 2009) of the SUMTIME-
METEO corpus (Sripada et al, 2002) which con-
tains system outputs and the pairs of wind forecast
4The Prodigy-METEO corpus is freely available here:
http://www.nltg.brighton.ac.uk/home/Anja.Belz
texts and wind data the systems were trained on,
e.g.:
Data: 1 SSW 16 20 - - 0600 2 SSE - - -
- NOTIME 3 VAR 04 08 - - 2400
Text: SSW 16-20 GRADUALLY BACKING SSE
THEN FALLING VARIABLE 4-8 BY
LATE EVENING
The input vector is a sequence of 7-tuples
?i, d, smin, smax, gmin, gmax, t? where i is the tu-
ple?s ID, d is the wind direction, smin and smax are
the minimum and maximum wind speeds, gmin
and gmax are the minimum and maximum gust
speeds, and t is a time stamp (indicating for what
time of the day the data is valid). The wind fore-
cast texts were taken from comprehensive mar-
itime weather forecasts produced by the profes-
sional meteorologists employed by a commercial
weather forecasting company for clients who run
offshore oilrigs.
There were two evaluation criteria; Clarity was
explained as indicating how understandable a fore-
cast was, and Readability as indicating how fluent
and readable it was. The experiment involved 22
forecast dates and outputs from the 10 systems de-
scribed in (Belz and Kow, 2009) (also included in
the corpus release) for those dates (as well as the
corresponding forecasts in the corpus) in the eval-
uation, i.e. a total of 242 forecast texts.
4.2 Rating scale experiment
We used the results of a previous experiment (Belz
and Kow, 2009) in which participants were asked
to rate forecast texts for Clarity and Readability,
each on a scale of 1?7.
The 22 participants were all University of
Brighton staff whose first language was English
and who had no experience of NLP. While ear-
lier experiments used master mariners as well as
lay-people in a similar evaluation (Belz and Re-
iter, 2006), these experiments also demonstrated
that the correlation between the ratings by expert
evaluators and lay-people is very strong in the ME-
TEO domain (Pearson?s r = 0.845).
We used a single 22 (evaluators) by 22 (test data
items) Latin Square; there were 484 trials in this
experiment.
4.3 Preference judgement experiment
Our new experiment used our standardised pref-
erence strength sliders (bottom of Figure 2). We
recruited 22 different evaluators from among stu-
dents currently completing or recently having
Type Measure Clarity Readability
RSE F(10,473) 23.507** 24.351**
N sig diffs 24/55 23/55
K?s W .497** .533**
( Text F(21,462) 1.467 1.961** )
( Evaluator F(21,462) 4.832** 4.824** )
PJE F(10,1865) 45.081** 41.318**
N sig diffs 34/55 32/55
K?s W .626** .542**
( Text F(21,916) 1.436 1.573 )
( Evaluator F(21,921) .794 1.057 )
Table 3: METEO RSE/PJE: Results of analyses
looking at effect of System.
completed a linguistics-related degree at Oxford,
KCL, UCL, Sussex and Brighton.
We had at our disposal 11 METEO systems, so
there were
(11
2
)
= 55 system combinations to eval-
uate on the 22 test data items. We decided on a
design of ten 11 ? 11 Latin Squares to accommo-
date the 55 system pairings, so there was a total of
1210 trials in this experiment.
4.4 Results and comparative analysis
Table 3 shows the same types of comparative mea-
sures as in the previous section. Note that the
self-consistency measure is missing, because for
METEO-PJE we do not have multiple scores for the
same pair of systems by the same evaluator.
For the METEO task, the relative amount vari-
ation in Clarity and Radability accounted for by
System is similar in the RSE, and again similar in
the PJE. However, F ratios and numbers of signifi-
cant differences found are higher in the latter than
in the RSE. The inter-evaluator agreement measure
also has higher values for both Clarity and Read-
ability in the PJE, although the difference is much
more pronounced in the case of Clarity.
In the RSE version, Evaluator has a small but
significant effect on both Clarity and Readability,
which disappears in the PJE version. Similarly, a
small effect of Text (date of weather forecast in
this data set) on Fluency in the RSE version disap-
pears in the PJE version.
5 RSE/PJE Pair 2: Descriptions of
furniture items and faces
5.1 Data and generic design
In our third pair of evaluations, we used the sys-
tem outputs from the TUNA?09 shared-task com-
petition (Gatt et al, 2009).5 The TUNA data is
a collection of images of domain entities paired
with descriptions of entities. Each pair consists of
seven entity images where one is highlighted (by a
red box surrounding it), paired with a description
of the highlighted entity, e.g.:
the small blue fan
The descriptions were collected in an online ex-
periment with anonymous participants, and then
annotated for semantic content. In TUNA?09, the
task for participating systems was to generate de-
scriptions of the highlighted entities given seman-
tic representations of all seven entities. In the eval-
uation experiments, evaluators were asked to give
two ratings in answer to the following questions
(the first for Adequacy, the second for Fluency):
1. How clear is this description? Try to imagine someone
who could see the same grid with the same pictures, but
didn?t know which of the pictures was the target. How
easily would they be able to find it, based on the phrase
given?
2. How fluent is this description? Here your task is to
judge how well the phrase reads. Is it good, clear En-
glish?
Participants were shown a system output, to-
gether with its corresponding domain, displayed
as the set of corresponding images on the screen.
The intended (target) referent was highlighted by
a red frame surrounding it on the screen.
Following detailed instructions, subjects did
two practice examples, followed by the 112 test
items in random order.
There were 8 ?systems? in the TUNA evalua-
tions: the descriptions produced by the 6 systems
and two sets of humans-authored descriptions.
5.2 Rating scale experiment
The rating scale experiment that was part of the
TUNA?09 evaluations had a design of fourteen 8 ?
8 squares, and a total of 896 trials.
5The TUNA?09 data and documen-
tation is available for download from
http://www.nltg.brighton.ac.uk/home/Anja.Belz
Type Measure Adequacy Fluency
RSE F(7,888) 6.371** 17.207**
N sig diffs 7/28 15/28
K?s W .471** .676**
( Text1 F(111,784) 1.519** 1.091 )
( Text2 F(14,881) 8.992** 4.694** )
( Evaluator F(7,888) 13.136** 17.479** )
PJE F(7,6264) 46.503** 89.236**
N sig diffs 19/28 22/28
K?s W .573** .654**
( Text1 F(111,3024) .746 .921 )
( Text2 F(14,3121) .856 .853 )
( Evaluator F(27,3108) 1.3 1.638* )
Table 4: TUNA RSE/PJE: Results of analyses
looking at effect of System.
Subjects were asked to give their judgments for
Clarity and Fluency for each item by manipulating
a slider. The slider pointer was placed in the center
at the beginning of each trial. The position of the
slider selected by the subject mapped to an integer
value between 1 and 100. However, the scale was
not visible to participants who knew only that one
end of the scale corresponded to the worst possible
score and the opposite end to the best.
Eight native speakers of English were recruited
for this experiment from among post-graduate
students currently doing a Masters degree in a
linguistics-related subject at UCL, Sussex and
Brighton universities.
5.3 Preference judgement experiment
Our new experiment used our standardised pref-
erence strength sliders (bottom of Figure 2). To
accommodate all pairwise comparisons as well as
all test set items, we used a design of four 28
? 28 Latin Squares, and recruited 28 evaluators
from among students currently completing, or re-
cently having completed, a degree in a linguistics-
related subject at Oxford, KCL, UCL, Sussex and
Brighton universities. There were 3,136 trials in
this version of the experiment.
5.4 Results and comparative analysis
Table 4 shows the same measures as we reported
for the other two experiment pairs above. The
picture is somewhat similar in that the measures
have better values for PJE version except for the
inter-evaluator agreement (Kendall?s W) for Flu-
ency which is slightly higher for the RSE version.
For the TUNA dataset, we look at two Text factors.
Text2 refers to different sets of entities used in tri-
als; there are 15 different ones. Text1 refers to sets
of entities and their specific distribution over the
visual display grid in trials (see the figure in Sec-
tion 5.1); there are 112 different combinations of
entity set and grid locations.
The most striking aspect of the results in Table 4
is the effect of Evaluator in the RSE version which
appears to account for more variability in the data
even than System (relative to other factors). In
fact, in the case of Adequacy, even Text2 causes
more variation than System. In contrast, in the PJE
version, by far the biggest cause of variability is
System (for both criteria), and the F ratios for Text
and Evaluators are not significant except for Eval-
uator on Fluency (weakly significant at .05).
On the face of it, the variation between evalua-
tors in the RSE version as evidenced by the F ra-
tio is worrying. However, Kendall?s W shows that
in terms of mean rank, evaluators actually agreed
similarly well on Fluency in both RSE and PJE.
The F measure is based on mean scores whereas W
is based on mean score ranks, so there was more
variation in the absolute scores than in the ranks.
The reason is likely to be connected to the way
ratings were expressed by evaluators in the TUNA-
RSE experiment: recall that evaluators had the task
of moving the pointer to the place on the slider
bar that they felt corresponded to the quality of
text being evaluated. As no numbers were visi-
ble, the only information evaluators had to go on
was which was the ?worse? end and which was the
?better? end of the slider. It seems that different
evaluators used this evaluation tool in very differ-
ent ways (accounting for the variation in absolute
scores), but were able to apply their way of using
the tool reasonably consistently to different texts
(so that they were able to achieve reasonably good
agreement with the other evaluators in terms of
relative scores).
6 Discussion
We have looked at a range of aspects of evalu-
ation experiments: the effect of the factors Sys-
tem, Text and Evaluator on evaluation scores; the
number of significant differences between systems
found; self-consistency; and inter-evaluator agree-
ment (as described by F ratios obtained in one-way
ANOVAs for Evaluator, as well as by Kendall?s W
measuring inter-evaluator agreement).
The results are unambiguous as far as the
Clarity criterion (called Adequacy in TUNA) is
concerned: in all three experiment pairs, the
preference-strength judgement (PSE) version had
a greater effect of System, a smaller effect of
Text and Evaluator, more significant pairwise dif-
ferences, better inter-evaluator agreement, and
(where we were able to measure it) better self-
consistency.
The same is true for Readability in METEO and
Fluency in TUNA, in the latter case except for W
which is slightly lower in TUNA-PJE than TUNA-
RSE. However, Readability in GREC-NEG bucks
the trend: here, all measures are worse in the
PJE version than in the RSE version (although for
the W measures, the differences are small). Part
of the reason for this may be that in GREC-NEG
PJE each system was only compared to one sin-
gle other ?system?, the (human-authored) original
Wikipedia texts.
If we see less effect of Clarity than of Fluency
in an experiment (as in GREC-NEG RSE and TUNA
RSE), then we might want to conclude that sys-
tems differed less in terms of Clarity than in terms
of Fluency. However, the real explanation may
be that evaluators simply found it harder to apply
the Clarity criterion than the Fluency criterion in
a given evaluation set-up. The fact that the differ-
ence in effect between Fluency and Clarity virtu-
ally disappears in GREC-NEG PJE makes this the
more likely explanation at least for the GREC-NEG
evaluations.
Parametric statistics are more powerful than
non-parametric ones because of the strong as-
sumptions they make about the nature of the data.
Roughly speaking, they are more likely to uncover
significant differences. Where the assumptions are
violated, the risk is that significance is overesti-
mated (the likelihood that null hypotheses are in-
correctly rejected increases). One might consider
using a slider mapping to a continuous scale in-
stead of a multiple-choice rating form in order to
overcome this problem, but the evidence from the
TUNA RSE evaluation appears to be that this can
result in unacceptably large variation in how indi-
vidual evaluators apply the scale to assign absolute
scores.
What seems to make the difference in terms of
ease of application of evaluation criteria and re-
duction of undesirable effects is not the use of con-
tinuous scales (as e.g. implemented in slider bars),
but the comparative element, where pairs of sys-
tems are compared and one is selected as better in
terms of a given criterion than the other.
It makes sense intuitively that deciding which
of two texts is clearer should be an easier task than
deciding whether a system is a 5, 4, 3 or 1 in terms
of its clarity. PJEs enabled evaluators to apply the
Clarity criterion to determine ranks more consis-
tently in all three experiment pairs.
However, it was an open question whether eval-
uators would also be able to express the strength
of their preference consistently. From the results
we report here it seems clear that this is indeed the
case: the System F ratios which look at absolute
scores (in the PJEs quantifying the strength of a
preference) are higher, and the Evaluator F ratios
lower, in all but one of the experiments.
While there were the same number of trials
in the two GREC-NEG evaluations, there were
2.5 times as many trials in METEO-PJE than in
METEO-RSE, and 3.5 times as many trials in
TUNA-PJE than in TUNA-RSE. The increase in tri-
als is counter-balanced to some extent by the fact
that evaluators tend to give relative judgements
far more quickly than absolute judgements, but
clearly there is an increase in cost associated with
including all system pairings in a PJE. If this cost
grows unacceptably large, a subset of systems has
to be selected as reference systems.
7 Concluding Remarks
Our aim in the research presented in this paper
was to investigate how rating-scale experiments
compare to preference-strength judgement experi-
ments in the evaluation of automatically generated
language. We find that preference-strength judge-
ment evaluations generally have a greater rela-
tive effect of System (the factor actually under in-
vestigation), a smaller relative effect of Text and
Evaluator (whose effect should be small), a larger
number of significant pairwise differences be-
tween systems, better inter-evaluator agreement,
and (where we were able to measure it) better eval-
uator self-consistency.
References
Anja Belz and Eric Kow. 2009. System building cost
vs. output quality in data-to-text generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 16?24.
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings
of EACL?06, pages 313?320.
Anja Belz, Eric Kow, and Jette Viethen. 2009. The
GREC named entity generation challenge 2009:
Overview and evaluation results. In Proceedings of
the ACL-IJCNLP?09 Workshop on Language Gen-
eration and Summarisation (UCNLG+Sum), pages
88?98.
A. Belz. 2009. Prodigy-METEO: Pre-alpha release
notes (Nov 2009). Technical Report NLTG-09-01,
Natural Language Technology Group, CMIS, Uni-
versity of Brighton.
Bernard Choi and Anita Pak. 2005. A catalog of bi-
ases in questionnaires. Preventing Chronic Disease,
2(1):A13.
A. Gatt, A. Belz, and E. Kow. 2009. The TUNA Chal-
lenge 2009: Overview and evaluation results. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG?09), pages 198?
206.
C.-Y. Lin and F. J. Och. 2004. ORANGE: A method
for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics
(COLING?04), pages 501?507, Geneva.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005.
Choosing words in computer-generated weather
forecasts. Artificial Intelligence, 167:137?169.
Sidney Siegel. 1957. Non-parametric statistics. The
American Statistician, 11(3):13?19.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2002.
SUMTIME-METEO: A parallel corpus of naturally
occurring forecast texts and weather data. Technical
Report AUCS/TR0201, Computing Science Depart-
ment, University of Aberdeen.
H. Trang Dang. 2006. DUC 2005: Evaluation
of question-focused summarization systems. In
Proceedings of the COLING-ACL?06 Workshop on
Task-Focused Summarization and Question Answer-
ing, pages 48?55.
J. Turian, L. Shen, and I. D. Melamed. 2003. Evalu-
ation of machine translation and its evaluation. In
Proceedings of MT Summmit IX, pages 386?393,
New Orleans.
Extracting Parallel Fragments from Comparable Corpora for
Data-to-text Generation
Anja Belz Eric Kow
Natural Language Technology Group
School of Computing, Mathematical and Information Sciences
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Abstract
Building NLG systems, in particular sta-
tistical ones, requires parallel data (paired
inputs and outputs) which do not gener-
ally occur naturally. In this paper, we in-
vestigate the idea of automatically extract-
ing parallel resources for data-to-text gen-
eration from comparable corpora obtained
from the Web. We describe our compa-
rable corpus of data and texts relating to
British hills and the techniques for extract-
ing paired input/output fragments we have
developed so far.
1 Introduction
Starting with Knight, Langkilde and Hatzivas-
siloglou?s work on Nitrogen and its successor
Halogen (Knight and Hatzivassiloglou, 1995;
Knight and Langkilde, 2000), NLG has over the
past 15 years moved towards using statistical tech-
niques, in particular in surface realisation (Langk-
ilde, 2002; White, 2004), referring expression
generation (most of the sytems submitted to the
TUNA and GREC shared task evaluation challenges
are statistical, see Gatt et al (2008), for example),
and data-to-text generation (Belz, 2008).
The impetus for introducing statistical tech-
niques in NLG can be said to have originally come
from machine translation (MT),1 but unlike MT,
where parallel corpora of inputs (source language
texts) and outputs (translated texts) occur naturally
at least in some domains,2 NLG on the whole has
to use manually created input/output pairs.
Data-to-text generation (D2T) is the type of NLG
that perhaps comes closest to having naturally oc-
curing inputs and outputs at its disposal. Work
in D2T has involved different domains including
generating weather forecasts from meteorological
1Nitrogen was conceived as an MT system component.
2Canadian and European parliamentary proceedings, etc.
data (Sripada et al, 2003), nursing reports from in-
tensive care data (Portet et al, 2009), and museum
exhibit descriptions from database records (Isard
et al, 2003; Stock et al, 2007); types of data in-
clude dynamic time-series data (e.g. medical data)
and static database entries (museum exhibits).
While data and texts in the three example do-
mains cited above do occur naturally, two factors
mean they cannot be used directly as example cor-
pora or training data for building D2T systems:
one, most are not freely available to researchers
(e.g. by simply being available on the Web), and
two, more problematically, for the most part, there
is no direct correspondence between inputs and
outputs as there is, say, between a source language
text and its translation. On the whole, naturally
occurring resources of data and related texts are
not strictly parallel, but are merely what has be-
come known as comparable in the MT literature,
with only a subset of data having corresponding
text fragments, and other text fragments having
no obvious corresponding data items. Moreover,
data transformations may be necessary before cor-
responding text fragments can be identified.
In this report, we look at the possibility of au-
tomatically extracting parallel data-text fragments
from comparable corpora in the case of D2T from
static database records. Such a parallel data-text
resource could then be used to train an existing
D2T generation system, or even build a new statis-
tical generator from scratch, e.g. using techniques
from statistical MT (Belz and Kow, 2009). The
steps involved in going from comparable data and
text resources to generators that produce texts sim-
ilar to those in the text resource are then as fol-
lows: (1) identify sources on the Web for com-
parable data and texts; (2) pair up data records
and texts; (3) extract parallel fragments (sets of
data fields paired with word strings); (4) train a
D2T generator using the parallel fragments; and
(5) feed data inputs to the generator which then
Figure 1: Overview of processing steps.
generates new texts describing them. Figure 1 il-
lustrates steps 1?3 which this paper focuses on. In
Section 3 we look at steps 1 and 2; in Section 4 at
step 3. First we briefly survey related work in MT.
2 Related work in MT
In statistical MT, the expense of manually creat-
ing new parallel MT corpora, and the need for very
large amounts of parallel training data, has led
to a sizeable research effort to develop methods
for automatically constructing parallel resources.
This work typically starts by identifying compara-
ble corpora. Much of it has focused on identify-
ing word translations in comparable corpora, e.g.
Rapp?s approach was based on the simple and el-
egant assumption that if words Af and Bf have
a higher than chance co-occurrence frequency in
one language, then two appropriate translations
Ae and Be in another language will also have
a higher than chance co-occurrence frequency
(Rapp, 1995; Rapp, 1999). At the other end of
the spectrum, Resnik & Smith (2003) search the
Web to detect web pages that are translations of
each other. Other approaches aim to identify pairs
of sentences (Munteanu and Marcu, 2005) or sub-
sentential fragments (Munteanu and Marcu, 2006)
that are parallel within comparable corpora.
The latter approach is particularly relevant to
our work. They start by translating each docu-
ment in the source language (SL) word for word
into the target language (TL). The result is given
to an information retrieval (IR) system as a query,
and the top 20 results are retained and paired with
the given SL document. They then obtain all sen-
tence pairs from each pair of SL and TL docu-
ments, and discard those sentence pairs with few
words that are translations of each other. To the re-
maining sentences they then apply a fragment de-
tection method which tries to distinguish between
source fragments that have a translation on the tar-
get side, and fragments that do not.
The biggest difference between the MT situation
and the D2T situation is that in the latter sentence-
aligned parallel resources exist and can be used as
a starting point. E.g. Munteanu & Marcu use an
existing parallel Romanian-English corpus to (au-
tomatically) create a lexicon from which is then
used in various ways in their method.
In D2T we have no analogous resources to help
us get started, and the methods described in this
paper use no such prior knowledge.
3 A Comparable Corpus of British Hills
As a source of data, we use the Database of British
Hills (BHDB) created by Chris Crocker,3 version
11.3, which currently contains measurements and
other information about 5,614 British hills. Ad-
ditionally, we perform reverse geocoding via the
Google Map API4 which allows us to convert
latitude and longitude information from the hills
database into country and region names. We add
the latter to each database entry.
On the text side, we use Wikipedia texts in the
WikiProject British and Irish Hills (retrieved on
2009-11-09). There are currently 899 pages cov-
ered by this WikiProject, 242 of which are of qual-
ity category B or above.5
Matching up data records and documents:
Matching up the data records in the BHDB with
articles in Wikipedia is not trivial: not all BHDB
entries have corresponding Wikipedia articles, dif-
ferent hills often share the same name, and the
same hill can have different names and spellings.
We perform a search of Wikipedia with the hill?s
name as the search term, using the Mediawiki API,
and then retain the top n search results returned
(currently n = 1). The top search result is not
always a correct match for the database record. We
3http://www.biber.fsnet.co.uk
4http://code.google.com/apis/maps/
5B = The article is mostly complete and without major
issues, but requires some further work.
{ "id": 1679, "main-name-info": {"name": "Hill of Stake", "notes": "",
"parent": "", "parent-notes": ""},
"alt-name-info": [], "raw-name": "Hill of Stake", "rhb-section": "27A", "area": "Ayr to River Clyde",
"height-metres": 522, "height-feet": 1713, "map-1to50k": "63", "map-1to25k": "341N", "gridref": "NS273630",
"col-gridref": "NS320527", "col-height": 33, "drop": 489, "gridref10": "NS 27360 62998", "feature": "trig point",
"observations": "", "survey": "", "date-climbed": "", "classification": "Ma,CoH,CoU",
"county-name": "Renfrewshire(CoH); Renfrewshire(CoU)", "revision": "28-Oct-2001", "comments": "",
"streetmap": "http://www.streetmap.co.uk/newmap.srf?x=227356&y=663005&z=3&sv=227356,663005&st=4&tl=?&bi=?&lu=N&ar=n",
"ordanancesurvey-map": "http://getamap.ordnancesurvey.co.uk/getamap/frames.htm?mapAction=gaz&gazName=g&gazString=NS273630",
"x-coord": 227356, "y-coord": 663005, "latitude": 55.82931,
"longitude": -4.75789, "country": "Scotland", "region": "Renfrewshire" }
Hill of Stake is a hill on the boundary between North Ayrshire and Renfrewshire , Scotland . It is 522 metres ( 1712 feet ) high
. It is one of the Marilyns of Lowland Scotland . It is the highest point of the relatively low-lying county of Renfrewshire and
indeed the entire Clyde Muirshiel Regional Park of which it is a part .
Table 1: Output of step 1: data record from British Hills DB and matched Wikipedia text (Hill of Stake).
manually selected the pairs we are confident are a
correct match. This left us with 759 matched pairs
out of a possible 899.
Table 1 shows an example of an automatically
matched database entry and Wikipedia article. It
illustrates the non-parallelism discussed in the pre-
ceding section; e.g. there is no information in the
database corresponding to the last sentence.
4 Towards a Parallelised Corpus
4.1 Aligning data fields and sentences
In the second processing step, we pair up data
fields and sentences. Related methods in MT have
translation lexicons and thesauri that can be used
as bridges between SL and TL texts, but there is
no equivalents in NLG. Our current method asso-
ciates each data field with a hand-written ?match
predicate?. For example, the match predicate for
height-metres returns True if the sentence con-
tains the words ?X metres? (among other patterns),
where X is some number within 5% of the height
of the hill in the database. We retain only the sen-
tences that match at least one data field. Table 2
shows what the data field/sentence alignment pro-
cedure outputs for the Hill of Stake.
4.2 Identifying Parallel Fragments
While it was fine for step 2 to produce some rough
matches, in step 3, parallel fragment detection, the
aim is to retain only those parts of a sentence that
can be said to realise some data field(s) in the set
of data fields with which it has been matched.
Computing data-text associations: Following
some preprocessing of sentences where each oc-
currence of a hill?s name and height is replaced
by lexical class tokens NAME , HEIGHT METRES
or HEIGHT FEET , the first step is to construct a
kind of lexicon of pairs (d,w) of data fields d and
words w, such that w is often seen in the realisa-
tion of d. For this purpose we adapt Munteanu
& Marcu?s (2006) method for (language to lan-
guage) lexicon construction. For this purpose we
compute a measure of the strength of association
between data fields and words; we use the G2 log-
likelihood ratio which has been widely used for
this sort of purpose (especially lexical association)
since it was introduced to NLP (Dunning, 1993).
Following Moore (2004a) rather than Munteanu &
Marcu, our current notion of cooccurrence is that
a data field and word cooccur if they are present
in the same pair of data fields and sentence (as
identified by the method described in Section 4.1
above). We then obtain counts for the number of
times each word cooccurs with each data field, and
the number of times it occurs without the data field
being present (and conversely). This allows us to
compute the G2 score, for which we use the for-
mulation from Moore (2004b) shown in Figure 2.
If the G2 score for a given (d,w) pair is greater
than p(d)p(w), then the association is taken to be
positive, i.e. w is likely to be a realisation of d,
otherwise the association is taken to be negative,
i.e. w is likely not to be part of a realisation of d.
For each d we then convert G2 scores to proba-
bilities by dividing G2 by the appropriate normal-
ising factor (the sum over all negative G2 scores
for d for obtaining the negative association proba-
bilities, and analogously for positive associations).
Table 3 shows the three words with the highest
positive association probabilities for each of our
six data fields. Note that these are not the three
most likely alternative ?translations? of each data
key, but rather the three words which are most
likely to be part of a realisation of a data field, if
seen in conjunction with it.
"main-name-only": "Hill of Stake", NAME is a hill on the boundary between North Ayrshire and Renfrewshire,
"country": "Scotland" Scotland.
"height-metres": 522, It is HEIGHT METERS metres ( HEIGHT FEET feet) high.
"height-feet": 1713
"country": "Scotland", It is one of the Marilyns of Lowland Scotland.
"classification": ["Ma", "CoH", "CoU"]
"main-name-only": "Hill of Stake" It is the highest point of the relatively low-lying county of Renfrewshire and
indeed the entire Clyde Muirshiel Regional Park of which it is a part.
Table 2: Output of step 2: aligned data fields and sentences, for Hill of Stake.
2N
(
p(d,w)log p(d,w)p(d)p(w) + p(d,?w)log
p(d,?w)
p(d)p(?w) + p(?d,w)log
p(?d,w)
p(?d)p(w) + p(?d,?w)log
p(?d,?w)
p(?d)p(?w)
)
Figure 2: Formula for computing G2 from Moore (2004b) (N is the sample size).
Data key d Word w P+(w|d)
main-name-only NAME 0.1355
a 0.0742
in 0.0660
classification as 0.0412
adjoining 0.0193
qualifies 0.0177
region District 0.1855
Lake 0.1661
area 0.1095
country in 0.1640
NAME 0.1122
Scotland 0.0732
height-metres metres 0.1255
m 0.0791
height 0.0679
height-feet feet 0.1511
HEIGHT METERS 0.0974
( 0.0900
Table 3: Data keys with 3 most likely words.
Identifying realisations: The next step is to ap-
ply these probabilities to identify those parts of a
sentence that are likely to be a valid realisation of
the data fields in the input. In Figure 3 we plot
the positive and negative association probabilities
for one of the sentences from our running exam-
ple, Hill of Stake. The light grey graph represents
the association probabilities between each word
in the sentence and height-feet, the dark grey
line those between the words in the sentence and
height-metres. We plot the negative association
probabilities simply by multiplying each by ?1.
The part of the sentence that one would
want to extract as a possible realisation of
{ height-metres, height-feet }, namely
? HEIGHT METRES metres ( HEIGHT FEET feet )
high?, shows up clearly as a sequence of relatively
strong positive association values. Our current
approach identifies such contiguous positive
Figure 3: Positive and negative association prob-
abilities plotted against the words they were com-
puted for.
association scores and extracts the corresponding
sentence fragments. This works well in many
cases, but is too simple as a general approach; we
are currently developing this method further.
5 Concluding Remarks
In this paper we have been interested in the prob-
lem of automatically obtaining parallel corpora for
data-to-text generation. We presented our com-
parable corpus of 759 paired database entries and
human-authored articles about British Hills. We
described the three techniques which we have im-
plemented so far and which we combine to extract
parallel data-text fragments from the corpus: (i)
identification of candidate pairs of data fields and
sentences; (ii) computing scores for the strength
of association between data and words; and (iii)
identifying sequences of words in sentences that
have positive association scores with the given
data fields.
References
Anja Belz and Eric Kow. 2009. System building cost
vs. output quality in data-to-text generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 16?24.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 1:61?74.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
Challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
A. Isard, J. Oberlander, I. Androutsopoulos, and
C. Matheson. 2003. Speaking the users? languages.
18(1):40?45.
K. Knight and V. Hatzivassiloglou. 1995. Two-level,
many-paths generation. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL ?95).
Kevin Knight and Irene Langkilde. 2000. Preserving
ambiguity in generation via automata intersection.
In Proceedings of AAAI/IAAI, pages 697?702.
I. Langkilde. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ?02).
Robert C. Moore. 2004a. Improving ibm word-
alignment model 1. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 519?526.
Robert C. Moore. 2004b. On log-likelihood-ratios and
the significance of rare events. In Proceedings of
the 9th Converence on Empirical Methods in Natu-
ral Language Processing (EMNLP?04), pages 333?
340.
Dragos Munteanu and Daniel Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31:477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics (COLING-ACL?06),
pages 81?88, Morristown, NJ, USA. Association for
Computational Linguistics.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173:789?816.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, pages 320?322, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Philip Resnik and Noah Smith. 2003. The web as a
parallel corpus. Computational Linguistics, 29:349?
380.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2003. Ex-
ploiting a parallel text-data corpus. In Proceedings
of Corpus Linguistics 2003, pages 734?743.
Oliviero Stock, Massimo Zancanaro, Paolo Busetta
adn Charles Callaway, Anbtonio Kru?ger, Michael
Kruppa, Tsvi Kuflik, Elena Not, and Cesare Rocchi.
2007. Adaptive, intelligent presentation of informa-
tion for the museum visitor in PEACH. User Mod-
eling and User-Adapted Interaction, 17(3):257?304.
M. White. 2004. Reining in CCG chart realization. In
A. Belz, R. Evans, and P. Piwek, editors, Proceed-
ings INLG?04, volume 3123 of LNAI, pages 182?
191. Springer.
The GREC Challenges 2010:
Overview and Evaluation Results
Anja Belz Eric Kow
Natural Language Technology Group
School of Computing, Mathematical and Information Sciences
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Abstract
There were three GREC Tasks at Gen-
eration Challenges 2010: GREC-NER re-
quired participating systems to identify
all people references in texts; for GREC-
NEG, systems selected coreference chains
for all people entities in texts; and GREC-
Full combined the NER and NEG tasks, i.e.
systems identified and, if appropriate, re-
placed references to people in texts. Five
teams submitted 10 systems in total, and
we additionally created baseline systems
for each task. Systems were evaluated au-
tomatically using a range of intrinsic met-
rics. In addition, systems were assessed by
human judges using preference strength
judgements. This report presents the eval-
uation results, along with descriptions of
the three GREC tasks, the evaluation meth-
ods, and the participating systems.
1 Introduction
Until recently, referring expression generation
(REG) research focused on the task of selecting the
semantic content of one-off mentions of listener-
familiar discourse entities. In the GREC research
programme we have been interested in REG as (i)
grounded within discourse context, (ii) embedded
within an application context, and (iii) informed
by naturally occurring data.
In general terms, the GREC tasks are about how
to select appropriate references to an entity in the
context of a piece of discourse longer than a sen-
tence. In GREC?10, there were three subtasks:
identification of references to people in free text
(GREC-NER); selection of references to people in
text (GREC-NEG); and regeneration of references
to people in text (GREC-Full) which can be thought
of as combining the NER and NEG tasks.
The immediate motivating application context
for the GREC Tasks is the improvement of referen-
tial clarity and coherence in extractive summaries
and multiply edited texts (such as Wikipedia ar-
ticles) by regenerating referring expressions con-
tained in them. The motivating theoretical inter-
est for the GREC Tasks is to discover what kind of
information is useful for making choices between
different kinds of referring expressions in context.
The GREC?10 tasks used the GREC-People cor-
pus which consists of 1,100 Wikipedia texts about
people within which we have annotated all refer-
ences to people.
Five teams participated in the GREC?10 tasks
(see Table 1), submitting 10 systems in total. Two
of these were created by combining the NER sys-
tem of one of the teams with the NEG systems
of two different teams, producing two ?combined?
systems for the Full Task. We also used the corpus
texts themselves as ?system? outputs, and created
baseline systems for all three tasks. We evaluated
systems using a range of intrinsic automatically
computed and human-assessed evaluation meth-
ods. This report describes the data (Section 2)
and evaluation methods (Section 3) used in the
three GREC?10 tasks, and then presents task defi-
nition, participating systems, evaluation methods,
and evaluation results for each of the three tasks
separately (Sections 4? 6).
2 GREC?10 Data
The GREC?10 data is derived from the GREC-
People corpus which (in its 2010 version) con-
sists of 1,100 annotated introduction sections from
Wikipedia articles in the category People. An in-
troduction section was defined as the textual con-
tent of a Wikipedia article from the title up to (and
excluding) the first section heading, the table of
contents or the end of the text, which ever comes
first. Each text belongs to one of six subcategories:
inventors, chefs, early music composers, explor-
ers, kickboxers and romantic composers. For the
Team Affiliation NEG systems NER systems Full systems
UDelx University of Delaware UDel-NEG UDel-NER UDel-Full
UMUS Universite? du Maine UMUS ? ?
Universita?t Stuttgart
JUx Jadavpur University JU ? ?
Poly-co E?cole Polytechnique de Montre?al ? Poly-co ?
XRCEy Xerox Research Centre Europe XRCE ? ?
UDel/UMUS (see above) ? ? UDel-UMUS-Full
UDel/XRCE (see above) ? ? UDel-XRCE-Full
Table 1: GREC-NEG?09 teams and systems (combined teams in last two rows). x = resubmitted after
fixing character encoding problems and/or software bugs; y = late submission.
All Inventors Chefs Early Explorers Kickboxers Romantic
Composers Composers
Training 809 249 248 312 ? ? ?
Development 91 28 28 35 ? ? ?
Test (NEG) 100 31 30 39 ? ? ?
Test (NER/Full) 100 ? ? ? 33 34 33
Total 1,100 307 306 387 33 34 33
Table 2: Overview of GREC?10 data sets.
purposes of the GREC task, the GREC-People cor-
pus was divided into training, development and
test data. The number of texts in the subsets are
as shown in Table 2.
In the GREC-People annotation scheme, a dis-
tinction is made between reference and referential
expression. A reference is ?an instance of refer-
ring? which is unique, whereas a referential ex-
pression is a word string and each reference can be
realised by many different referential expressions.
In the GREC corpora, each time an entity is re-
ferred to, there is a single reference, but there may
be one or several referring expressions provided
with it: in the training/development data, there is
a single RE for each reference (the one found in
the corpus); in the test set, there are four REs for
each reference (the one from the corpus and three
additional ones selected by subjects in a manual
selection experiment).
We first manually annotated people mentions in
the GREC-People texts by marking up the word
strings that function as referential expressions
(REs) and annotating them with coreference in-
formation as well as semantic category, syntac-
tic category and function, and various supplements
and dependents. Annotations included nested ref-
erences, plurals and coordinated REs, certain un-
named references and indefinites. In terminology
and the treatment of syntax used in the annota-
tion scheme we relied heavily on The Cambridge
Grammar of the English Language by Huddleston
and Pullum (2002). For full details of the manual
annotation please refer to the GREC?10 documen-
tation (Belz, 2010).
The manual annotations were then automat-
ically checked and converted to XML format.
In the XML format of the annotations, the be-
ginning and end of a reference is indicated by
<REF><REFEX>... </REFEX></REF> tags, and
other properties mentioned above (e.g. syntactic
category) are encoded as attributes on these tags.
For the GREC tasks we decided not to transfer
the annotations of integrated dependents and rel-
ative clauses to the XML format. Such dependents
are included within <REFEX>...</REFEX> annota-
tions where appropriate, but without being marked
up as separate constituents.
Figure 1 shows one of the XML-annotated texts
from the GREC data. For full details of the manual
annotations and the XML version, please refer to
the GREC?10 documentation (Belz, 2010). Here
we provide a brief summary.
The REF element indicates a reference, and is
composed of one REFEX element (the ?selected?
referential expression for the given reference; in
the corpus texts it is the referential expression
found in the corpus). The attributes of the REF
element are ENTITY (entity identifier), MENTION
(mention identifier), SEMCAT (semantic category),
SYNCAT (syntactic category), and SYNFUNC (syntac-
tic function). ENTITY and MENTION together con-
stitute a unique identifier for a reference within a
text; together with the TEXT ID, they constitute a
unique identifier for a reference within the entire
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE GREC-ITEM SYSTEM "genchal09-grec.dtd">
<GREC-ITEM>
<TEXT ID="15">
<TITLE>Alexander Fleming</TITLE>
<PARAGRAPH> <REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX>
</REF> (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist.
<REF ENTITY="0" MENTION="2" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX>
</REF> published many articles on bacteriology, immunology, and chemotherapy.
<REF ENTITY="0" MENTION="3" SEMCAT="person" SYNCAT="np" SYNFUNC="subj-det">
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
</REF> best-known achievements are the discovery of the enzyme lysozyme in 1922 and the discovery
of the antibiotic substance penicillin from the fungus Penicillium notatum in 1928, for which
<REF ENTITY="0" MENTION="4" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
</REF> shared the Nobel Prize in Physiology or Medicine in 1945 with
<REF ENTITY="1" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj">
<REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX>
</REF> and
<REF ENTITY="2" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj">
<REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX>
</REF>.</PARAGRAPH>
</TEXT>
<ALT-REFEX>
<REFEX ENTITY="0" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Fleming?s</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Sir Alexander Fleming?s</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
<REFEX ENTITY="1" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="1" REG08-TYPE="name" CASE="genitive">Florey?s</REFEX>
<REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
<REFEX ENTITY="2" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="2" REG08-TYPE="name" CASE="genitive">Chain?s</REFEX>
<REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
</ALT-REFEX>
</GREC-ITEM>
Figure 1: Example XML-annotated text from the GREC-NEG?09 data.
corpus.
A REFEX element indicates a referential expres-
sion (a word string that can be used to refer to an
entity). The attributes of the REFEX element are
REG08-TYPE (name, common, pronoun, empty), and
CASE (nominative, accusative, etc.).
We allow arbitrary-depth embedding of refer-
ences. This means that a REFEX element may have
REF element(s) embedded in it.
The second (and last) component of a
GREC-ITEM is an ALT-REFEX element which
is a list of REFEX elements. For the GREC tasks,
these were obtained by collecting the set of all
REFEXs that are in the text, and adding several
defaults including pronouns and other cases (e.g.
genitive) of REs already in the list.
REF elements that are embedded in REFEX ele-
ments contained in an ALT-REFEX list have an un-
specified MENTION id (the ??? value). Furthermore,
such REF elements have had their enclosed REFEX
removed.
The two test data sets exist in two versions:
1. Version a: each text has a single human-selected refer-
ring expression for each reference (i.e. the one found in
the original Wikipedia article).
2. Version b: the same subset of texts as in (a); for this set
we did not use the REs in the corpus, but replaced each
of them with human-selected alternatives obtained in
an online experiment as described in (Belz and Varges,
2007); this version of the test set therefore contains
three versions of each text where all the REFEXs in a
given version were selected by one ?author?.
The training, development and test data for the
GREC-NEG task is exactly as described above.
The training and development data for the GREC-
NER/Full tasks comes in two versions. The first is
identical to the standard XML-annotated version of
the GREC-People corpus as described above (Sec-
tion 2). The second is in the test data input format.
In this format, texts have no REFEX and REF tags,
and no ALT-REFEX element. A further difference is
that in the test data format, a proportion of REFEX
word strings have been replaced with standardised
named references. All empty references have been
replaced in this way, whereas (non-relative) pro-
nouns, and previously seen named references that
are not identical to the standardised named refer-
ence, are replaced with a likelihood of 0.5.
The reason for this replacement is to make both
tasks easier (as we are running them for the first
time) as well as more realistic (in an extractive
summary, reference chains are unlikely to be as
good as in the Wikipedia texts).
3 Evaluation Procedures
Table 3 is an overview of the evaluation mea-
sures we applied to the three tasks in GREC?10.
Version a of the test sets has a single version
of each text, and the scoring metrics that are
based on counting matches (Word String Ac-
curacy counts matching word strings, REG08-
Type Recall/Precision count matching REG08-
Type attribute values) simply count the number of
matches a system achieves against that single text.
Version b, however, has three versions of each text,
so the match-based metrics first calculate the num-
ber of matches for each of the three versions and
then use (just) the highest number of matches.
3.1 Automatic Evaluations
REG08-Type Precision is defined as the proportion
of REFEXs selected by a participating system which
match the reference REFEXs. REG08-Type Recall
is defined as the proportion of reference REFEXs
for which a participating system has produced a
match.
String Accuracy is defined as the proportion of
word strings selected by a participating system
that match those in the reference texts. This was
computed on complete, ?flattened? word strings
contained in the outermost REFEX i.e. embedded
REFEX word strings were not considered sepa-
rately.
We also computed BLEU-3, NIST, string-edit
distance and length-normalised string-edit dis-
tance, all on word strings defined as for String Ac-
curacy. BLEU and NIST are designed for multiple
output versions, and for the string-edit metrics we
computed the mean of means over the three text-
level scores (computed against the three versions
of a text).
To measure accuracy in the NER task, we ap-
plied three commonly used performance measures
for coreference resolution: MUC-6 (Vilain et al,
1995), CEAF (Luo, 2005), and B-CUBED (Bagga
and Baldwin, 1998).
3.2 Human-assessed evaluations
We designed the human-assessed intrinsic evalua-
tion as a preference-judgement test where subjects
expressed their preference, in terms of two crite-
ria, for either the original Wikipedia text or the
version of it with system-generated referring ex-
pressions in it. For the GREC-NEG systems, the in-
trinsic human evaluation involved system outputs
for 30 randomly selected items from the test set.
We used a Repeated Latin Squares design which
ensures that each subject sees the same number
of outputs from each system and for each test set
item. There were three 10 ? 10 squares, and a
total of 600 individual judgements in this evalua-
tion (60 per system: 2 criteria ? 3 articles ? 10
evaluators). We recruited 10 native speakers of
English from among students currently complet-
ing a linguistics-related degree at Kings College
London and University College London.
For the GREC-Full systems, we used 21 ran-
domly selected test set items, a design analogous
to that for the GREC-NEG experiment, and 7 eval-
uators from the same cohort. This experiment had
three 7 ? 7 squares, and 294 individual judge-
ments.
Following detailed instructions, subjects did
two practice examples, followed by the texts to be
evaluated, in random order. Subjects carried out
the evaluation over the internet, at a time and place
of their choosing. They were allowed to interrupt
and resume the experiment (though discouraged
from doing so).
Figure 2 shows what subjects saw during the
evaluation of an individual text pair. The place
(left/right) of the original Wikipedia article was
randomly determined for each individual evalua-
tion of a text pair. People references are high-
lighted in yellow/orange, those that are identical
in both texts are yellow, those that are different are
orange (in the GREC-Full version, there were only
yellow highlights). The evaluator?s task is to ex-
press their preference in terms of each quality cri-
terion by moving the slider pointers. Moving the
slider to the left means expressing a preference for
Quality criterion: Type of evaluation: Task: Evaluation Method(s):
Humanlikeness Intrinsic/automatic NEG 1. REG?08-Type Recall and Precision
2. String Accuracy
3. String-edit distance
NEG, Full 1. BLEU
2. NIST version of BLEU
NER CEAF, MUC-6, B-CUBED
Fluency Intrinsic/human NEG, Full Human preference-strength judgements
Referential Clarity Intrinsic/human NEG, Full Human preference-strength judgements
Table 3: Overview of GREC?10 evaluation procedures.
Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.
the text on the left, moving it to the right means
preferring the text on the right; the further to the
left/right the slider is moved, the stronger the pref-
erence. The two criteria were explained in the in-
troduction as follows (the wording of the first is
from DUC):
1. Referential Clarity: It should be easy to identify who
the referring expressions are referring to. If a person
is mentioned, it should be clear what their role in the
story is. So, a reference would be unclear if a person
is referenced, but their identity or relation to the story
remains unclear.
2. Fluency: A referring expression should ?read well?,
i.e. it should be written in good, clear English, and the
use of titles and names should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
It was not evident to the evaluators that slid-
ers were associated with numerical values. Slider
pointers started out in the middle of the scale (no
preference). The values associated with the points
on the slider ranged from -10.0 to +10.0.
4 GREC-NEG
4.1 Task
The GREC-NEG test data inputs are identical to
the training/development data (Figure 1), except
that REF elements in the test data do not contain a
REFEX element, i.e. they are ?empty?. The task for
participating systems is to select one REFEX from
the ALT-REFEX list for each REF in each TEXT in
the test sets. If the selected REFEX contains an em-
bedded REF then participating systems also need
to select a REFEX for this embedded REF and to set
the value of its MENTION attribute. The same ap-
plies to all further embedded REFEXs, at any depth
of embedding.
4.2 Systems
NEG-Base-rand, NEG-Base-freq, NEG-Base-
1st, NEG-Base-name: We created four baseline
systems each with a different way of selecting a
REFEX from those REFEXs in the ALT-REFEX list
that have matching entity IDs. Base-rand selects a
REFEX at random. Base-1st selects the first REFEX
(unless the first is the empty reference in which
case it selects the second).1 Base-freq selects the
first REFEX with a REG08-TYPE and CASE combi-
nation that is the overall most frequent (as deter-
mined from the training/development data) given
the SYNCAT, SYNFUNC and SEMCAT of the refer-
ence.1 Base-name selects the shortest REFEX with
attribute REG08-TYPE=name.
UMUS: The UMUS system maps REFEXs to class
labels encoding REG08-TYPE, CASE, pronoun type,
reflexiveness and recursiveness. References are
represented by a set of features encoding the at-
tributes given in the corpus, information about in-
tervening references to other entities, preceding
punctuation, sentence and paragraph boundaries,
surrounding word and POS n-grams, etc. A Condi-
tional Random Fields method is then used to map
features to class labels. The problem is construed
as predicting a sequence of class labels for each
entity, to avoid repetition. If there is more than
one REFEX available with the predicted label then
the longest one is chosen the first time, and selec-
tion iterates through the list subsequently.
UDel: The UDel system is a set of decision-tree
classifiers (separate ones for the main subject and
other person entities) using psycholinguistically
inspired features that predict the REG08-TYPE and
CASE of the REFEX to select. Then the system ap-
plies rules governing the length of first and subse-
quent mentions. There are back-off rules for when
the predicted type/case is not available. An ambi-
guity checker avoids the use of a pronoun if there
has been an intervening reference to a person of
the same gender.
JU: The JU baseline system is similar to our
NEG-Base-freq system described above. The sub-
1Note that this is a change from GREC?09.
mitted JU system adds features to the set of REF
attributes available from the corpus, including in-
dices for paragraph, sentence and word. It also
adds features to the REFEX attributes available from
the corpus, in order to distinguish between several
REFEXs that match the predicted REG08-TYPE and
CASE combination.
XRCE: The XRCE system uses a conditional
random field model in combination with the Sam-
pleRank algorithm for learning model parameters.
The feature functions used include unary ones
(>100 features encoding the attributes provided in
the corpus as well as position within sentence, ad-
jacent POS tags, etc.) and binary ones (distance to
previous mention, distribution of type and case).
Some binary feature functions are activated only
if the previous mention was a name and control
overuse of pronouns.
4.3 Evaluation results
Participants computed evaluation scores on the de-
velopment set, using the geval code provided by us
which computes Word String Accuracy, REG?08-
Type Recall and Precision, string-edit distance and
BLEU. The following is a summary of teams? self-
reported scores:
Recall Precision WSA
UMUS 0.816 0.829 0.813
UMUS?09 0.830 0.830 0.786
XRCE 0.771 0.771 0.702
UDel 0.758 0.758 0.650
JU 0.66 0.63 0.54
REG08-Type Recall and Precision results for Test
Set NEG-a (version a of the test set with just one
REFEX for each REF) are shown in Table 4. As
would be expected, results on the test data are
somewhat worse than on the development data.
Also included in this table are results for the 4
baseline systems, and it is clear that selecting the
most frequent RE type and case combination given
SEMCAT, SYNFUNC and SYNCAT (as done by the
Base-freq system) provides a strong baseline, al-
though it is a much better predictor for Composer
and Inventor texts than Chef texts.
The last 6 columns in Table 4 contain Recall (R)
and Precision (P) results for the three subdomains.
For most of the systems results are slightly better
for Composers than for Chefs. A contributing fac-
tor to this may be the fact that Chef texts tend to
be much more colloquial. A striking detail is the
collapse in scores in the Inventors subdomain for
System
REG08-Type Precision and Recall Scores against Corpus (Test Set NEG-a)
All Chefs Composers Inventors
Precision Recall P R P R P R
UMUS 80.71 A 78.31 A 79.19 75.44 80.88 78.68 81.66 80.05
UMUS?09 80.17 A 77.06 A 75.16 70.71 82.25 79.54 80.66 78.08
XRCE 74.26 A 71.38 A 68.55 64.50 75.44 72.96 76.84 74.38
JU 66.98 A B 64.38 A B 79.56 74.85 84.32 81.55 26.97 26.11
Base-freq 61.52 A B C 59.60 A B C 51.86 49.41 65.74 63.95 62.12 60.59
UDel-NEG 60.92 A B C 58.56 A B C 55.35 52.07 62.43 60.37 62.85 60.84
Base-rand 43.32 B C 42.00 B C 40.43 38.76 43.00 41.77 46.21 45.07
Base-name 40.60 C 39.09 C 47.80 44.97 40.32 39.06 35.28 34.24
Base-1st 40.25 C 39.64 C 47.88 46.75 39.71 39.20 34.91 34.48
Table 4: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set and
for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.
System
REG08-Type Precision and Recall Scores against human topline (Test Set NEG-b)
All Chefs Composers Inventors
Precision Recall P R P R P R
Corpus 82.67 A 84.01 A 82.25 84.24 83.26 84.47 82.02 83.04
UMUS 81.64 A 80.49 A 82.92 80.91 80.59 79.54 82.41 81.80
UMUS?09 80.46 A 78.59 A B 80.50 77.58 80.62 79.10 80.15 78.55
XRCE 73.76 A B 72.04 A B C 73.58 70.91 74.11 72.71 73.28 71.82
UDel-NEG 65.54 A B C 64.01 A B C D 66.04 63.64 66.12 64.88 64.12 62.84
Base-freq 65.38 A B C 64.37 A B C D 59.94 58.48 68.97 68.07 63.64 62.84
JU 63.73 A B C 62.25 A B C D 76.42 73.64 76.04 74.60 32.32 31.67
Base-name 55.22 B C 54.01 B C D 56.29 54.24 58.05 57.04 49.49 48.63
Base-1st 54.68 B C 54.68 C D 55.45 55.45 57.68 57.68 48.88 48.88
Base-rand 48.46 C 47.75 D 48.77 47.88 47.13 46.44 50.51 49.88
Table 5: REG08-Type Recall and Precision scores against human topline version of Test Set for complete
set and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.
the JU system. As a side effect, the resulting vari-
ation led to fewer significant differences between
systems being found in the results than would have
been the case otherwise.
We carried out univariate ANOVAs with System
as the fixed factor, and REG08-Type Recall as the
dependent variable in one ANOVA, and REG08-
Type Precision in the other. The F-ratio for Recall
was F(9,990) = 13.253, p < 0.001.2 The F-ratio
for Precision was F(9,990) = 12.670, p < 0.001.
The columns containing single capital letters in
Table 4 show the homogeneous subsets of systems
as determined by a post-hoc Tukey HSD analysis.
Systems whose scores are not significantly differ-
ent (at the .05 level) share a letter.
Table 5 shows analogous results computed
against Test Set NEG-b (which has three versions
of each text). Table 5 includes results for the cor-
pus texts, also computed against the three ver-
sions of each text in test set GREC-NEG-b. We
performed univariate ANOVAs with System as the
fixed factor, and Recall as the dependent variable
in one, and Precision in the other. The result for
Recall was F(9,990) = 5.248, p < .001), and for
Precision F(9,990) = 5.038, p < .001. We again
compared the mean scores with Tukey?s HSD.
2We included the corpus texts themselves in the analysis,
hence 9 degrees of freedom (10 systems).
One would generally expect results on test set
NEG-b to be better than on NEG-a. This is the case
for all baseline systems and some of the participat-
ing systems, but not all. The JU system in particu-
lar drops in score (and rank).
We also computed Word String Accuracy and
the other string similarity metrics described in
Section 3 for the GREC-NEG Task. The result-
ing scores for Test Set NEG-a are shown in Ta-
ble 6. Ranks for peer systems relative to each other
are very similar to the results for REG08-Type re-
ported above.
We performed a univariate ANOVA with System
as the fixed factor, and Word String Accuracy as
the dependent variable. The F-ratio for System
was F(9,990) = 41.308, p < 0.001; the homoge-
neous subsets resulting from the Tukey HSD post-
hoc analysis are shown in columns 3?7 of Table 6.
Table 7 shows analogous results for human
topline Test Set NEG-b (which has three versions
of each text). We carried out the same kind of
ANOVA as for Test Set NEG-a; the result for Sys-
tem on Word String Accuracy was F(9,990) =
35.123, p < 0.001. System rankings are the same
as for Test Set NEG-a (the differences between JU
and Base-freq, which swap ranks, are not signif-
icant); scores across the board (again, except for
the JU system) are somewhat higher, because of
the way scores are computed for version b test
System
String similarity against Corpus (Test Set NEG-a)
Word String Accuracy
BLEU-3 NIST SE norm. SEAll Chefs Composers Inventors
UMUS 78.51 A 76.42 79.29 78.88 0.7968 7.4986 0.6063 0.2019
UMUS?09 75.05 A 69.18 77.66 75.32 0.7615 6.9865 0.6806 0.2233
XRCE 65.25 A 61.01 66.12 67.18 0.7031 6.0264 0.8969 0.3131
JU 60.71 A 72.96 76.63 23.41 0.5720 5.7264 1.1810 0.3671
Base-freq 57.10 A B 50.31 60.65 56.49 0.5913 4.9860 1.2249 0.4191
UDel-NEG 38.21 B C 37.42 39.20 37.15 0.5498 5.0211 1.6222 0.5869
Base-name 28.48 C D 35.53 27.51 24.43 0.4966 4.9355 1.8017 0.6662
Base-rand 8.22 D E 8.49 7.10 9.92 0.1728 1.2501 2.4290 0.8928
Base-1st 4.69 E 3.46 5.47 4.33 0.1990 2.4018 2.9906 0.8152
Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set NEG-a (sys-
tems in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String
Accuracy only.
System
String similarity against human topline (Test Set NEG-b)
Word String Accuracy
BLEU-3 NIST SE norm. SEAll Chefs Composers Inventors
Corpus 81.90 A 83.33 82.25 80.15 0.9499 9.1087 0.7082 0.2517
UMUS 77.29 A B 79.25 76.48 77.10 0.9296 8.1746 0.8383 0.2906
UMUS?09 74.84 A B C 73.58 75.59 74.55 0.8968 7.5005 0.9096 0.3083
XRCE 63.95 A B C 66.35 63.02 63.61 0.7960 6.0780 1.1577 0.4060
Base-freq 59.84 B C D 55.97 62.72 58.02 0.7393 5.4920 1.3949 0.4717
JU 56.31 C D E 68.87 66.86 27.99 0.5765 5.8764 1.5114 0.4720
UDel-NEG 41.60 D E 44.34 40.38 41.48 0.6503 5.9571 1.7138 0.6057
Base-name 37.27 E 42.14 36.83 34.10 0.6480 6.6551 1.7299 0.6287
Base-rand 10.45 F 10.06 9.91 11.70 0.2468 1.4828 2.4869 0.8884
Base-1st 8.58 F 5.66 10.95 6.87 0.2824 3.5790 2.9226 0.7868
Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set NEG-b (sys-
tems in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String
Accuracy.
sets: a score is the highest score a system achieves
(at text-level) against any of the three versions of
a test set text that is taken into account.
Results for BLEU-3, NIST and the two string-
edit distance metrics are shown in the rightmost 4
columns of Tables 6 and 7. With the exception of
Base-freq/Basename on Test Set NEG-b, systems
whose Word String Accuracy scores differ signif-
icantly are assigned the same relative ranks by all
other string-similarity metrics as by Word String
Accuracy.
In the human intrinsic evaluation, evaluators
rated system outputs in terms of whether they pre-
ferred them over the original Wikipedia texts. As
a result of the experiment we had (for each system
and each evaluation criterion) a set of scores rang-
ing from -10.0 to +10.0, where 0 meant no pref-
erence, negative scores meant a preference for the
Wikipedia text, and positive scores a preference
for the system-produced text.
The second column of the left half of Table 8
summarises the Clarity scores for each system in
terms of their mean; if the mean is negative the
evaluators overall preferred the Wikipedia texts,
if it is positive evaluators overall preferred the
system. The more negative the score, the more
strongly evaluators preferred the Wikipedia texts.
Columns 8?10 show corresponding counts of how
many times each system was preferred (+), dis-
preferred (?), and neither (0).
The other half of Table 8 shows corresponding
results for Fluency.
We ran a factorial multivariate ANOVA with Flu-
ency and Clarity as the dependent variables. In the
first version of the ANOVA, the fixed factors were
System, Evaluator and Wikipedia Side (indicating
whether the Wikipedia text was shown on the left
or right during evaluation). This showed no signif-
icant effect of Wikipedia Side on either Fluency or
Clarity, and no significant interaction between any
of the factors. There was also no significant effect
of Evaluator on Fluency, and only a weakly sig-
nificant effect of Evaluator on Clarity. We ran the
ANOVA again, this time with just System as the
fixed factor. The F-ratio for System on Fluency
was F(9,290) = 22.911, p < .001, and for System
on Clarity it was F(9,290) = 13.051, p < .001.
Post-hoc Tukey?s HSD tests revealed the signif-
icant pairwise differences indicated by the letter
columns in Table 8.
Correlation between individual Clarity and Flu-
ency ratings as estimated with Pearson?s coeffi-
cient was r = 0.66, p < 0.01, indicating that the
two criteria covary to some extent.
Clarity Fluency
System Mean + 0 ? System Mean + 0 ?
Corpus 0.000 A 1 28 1 Corpus 0.133 A 1 29 0
UMUS -2.023 A B 1 13 16 UMUS -1.640 A B 4 12 14
UMUS?09 -2.527 A B C 0 15 15 UMUS?09 -2.130 A B 3 11 16
Base-name -2.900 B C 1 7 22 XRCE -3.587 B C 2 8 20
Base-1st -3.160 B C 4 3 23 JU -4.057 B C D 0 10 20
XRCE -3.500 B C D 1 9 20 Base-freq -4.990 C D 1 3 26
JU -3.577 B C D 0 10 20 Base-name -6.620 D E 0 1 29
UDel-NEG -5.137 C D E 0 1 29 Base-1st -7.823 E 1 0 29
Base-freq -6.190 D E 0 2 28 Base-rand -7.950 E 1 0 29
Base-rand -7.663 E 1 0 29 UDel-NEG -7.970 E 0 1 29
Table 8: GREC-NEG: Results for Clarity and Fluency preference judgement experiment. Mean = mean of
individual scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred;
? = number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.
The relative ranks of the peer systems are the
same in terms of both Fluency and Clarity. How-
ever, there are interesting differences in the ranks
of the baseline systems. For Clarity, Base-name
and Base-1st are scored fairly highly (presumably
because both tend to pick named references which
are clear if not always fluent), but both go back
to not being significantly better than Base-rand in
the Fluency rankings. Base-freq does badly in the
Clarity scores, but is significantly better than the
bottom three systems in terms of Fluency.
5 GREC-NER
5.1 Task
The GREC-NER task is a straightforward combined
named-entity recognition and coreference resolu-
tion task, restricted to people entities. The aim for
participating systems is to identify all those types
of mentions of people that we have annotated in
the GREC-People corpus, and to insert REF and
REFEX tags with coreference IDs into the texts.
5.2 Systems
Baselines: We used the coreference resolvers
included in the LingPipe3 and OpenNLP Tools4
packages as baseline systems.
Poly-co: The Poly-co system starts by applying
a POS tagger to the input text. A Conditional
Random Fields classifier (trained on an automat-
ically annotated Wikipedia corpus) is then used
to detect named mentions, using word and POS
based features. Logical rules then detect pro-
noun mentions, using named-entity, word and POS
features. Coreference of named mentions is de-
termined by clustering with a similarity measure
based on words, POS tags and sentence position,
3http://alias-i.com/lingpipe/
4http://opennlp.sf.net
applied to mentions in order of their appearance.
Coreference of pronouns is determined with the
Hobbs algorithm for anaphora resolution.
UDel-NER: The UDel-NER system starts by (1)
parsing the input text with the Stanford Parser,
from which it extracts syntactic functions of words
and relationships between them; and (2) separately
applying the Stanford Named Entity Recognizer.
Pronoun and common noun mentions are identi-
fied using lists of all English pronouns and of com-
mon nouns which could conceivably be used to
refer to people (occupations like ?painter?, fam-
ily relations like ?grandmother?, etc.). Values for
all REF and REFEX attributes except coreference ID
are obtained. Finally, the system applies a coref-
erence resolution tool which compares each refer-
ence to all previous references in reverse order, on
the basis of case, gender, number, syntactic func-
tion, and REG?08-Type.
5.3 Results
The coreference resolution accuracy scores for the
GREC-NER systems are shown in Table 9. The two
participating systems are both significantly better
than the two baslines in terms of their mean coref-
erence resolution accuracy scores.
6 GREC-Full
6.1 Task
The aim for GREC-Full systems was to improve
the referential clarity and fluency of input texts.
Participants were free to do this in whichever way
they chose. Participants were encouraged, though
not required, to create systems which replace re-
ferring expressions as and where necessary to pro-
duce as clear and fluent a text as possible. This
task could be viewed as composed of three sub-
tasks: (1) named entity recognition (as in GREC-
Test set
Mean B-3 CEAF MUC
UDel-NER 72.71 A 80.51 77.53 60.09
Poly-co 66.99 A 76.92 70.29 53.77
LingPipe 58.23 B 71.19 61.58 41.92
OpenNLP 54.03 B 67.61 59.17 35.32
Table 9: MUC-6, CEAF and B-3 scores for GREC-NER systems. Systems shown in order of average
scores.
NER); (2) a conversion tool to give lists of possi-
ble referring expressions for each entity; and (3)
named entity generation (as in GREC-NEG).
6.2 Systems
All GREC-Full systems in our evaluations are com-
posed of a GREC-NER and a GREC-NEG system.
We created three baseline systems. Two of these
we created by combining the two GREC-NER base-
line systems with the random GREC-NEG base-
line system (Base-rand). For this purpose we cre-
ated a simple conversion utility which adds default
REFEXs. The third baseline system combines the
UDel-NER system with Base-rand.
The only team that submitted both a GREC-NER
and a GREC-NEG system was UDel. All other
GREC-Full systems therefore combine the efforts
of two teams (for overview of system combina-
tions, please refer to Table 1). The two system
combinations involving the UDel-NER system did
not require a conversion utility, because UDel-NER
already outputs full GREC-People format.
6.3 Results
NIST and BLEU scores computed against the
Wikipedia texts for the GREC-Full systems are
shown in Table 10. Note that these have been
computed on the complete texts, not just the refer-
ential expressions (which explains the high BLEU
scores). The scores in the second row (Corpus,
test set vers.) are obtained by comparing the test
set versions of the corpus texts (in which some of
the references have been replaced with standard-
ised named references, as explained in Section 2)
against the Wikipedia texts. The two halves of the
table show scores computed against version a of
the test set (the original Wikipedia texts) on the
left, and against version b of the test set (which has
three versions of each text with human-selected
REs) on the right.
In the human intrinsic evaluation of GREC-Full
systems, evaluators again rated system outputs in
terms of whether they preferred them over the
original Wikipedia texts. Table 11 shows the re-
sults in the same format as in Table 8 for the
GREC-NEG systems.
We ran the same two factorial multivariate
ANOVAs with Fluency and Clarity as the depen-
dent variables. In the first version of the ANOVA,
there were no effects of Evaluator (apart from
a mild one on Clarity) and Wikipedia Side and
no significant interaction between any of the fac-
tors. There was no effect of Evaluator on Fluency
and only a mild effect of Evaluator on Clarity.
The second ANOVA just had System as the fixed
factor. The F-ratio for Fluency was F(6,140) =
13.054, p < .001, and for System on Clarity it
was F(6,140) = 14.07, p < .001. Post-hoc Tukey?s
HSD tests revealed the significant pairwise differ-
ences indicated by the letter columns in Table 11.
Correlation between individual Clarity and Flu-
ency ratings as estimated with Pearson?s coeffi-
cient was r = 0.696, p < .01, indicating that the
two criteria covary to some extent.
Apart from UDel-Full and OpenNLP/Base-rand
switching places, system ranks are the same for
Fluency and Clarity. Moreover, system ranks
are very similar to those produced by the string-
similarity scores above. UDel-Full is a much
harder task than GREC-NEG and it is a very good
result indeed for a system to be preferred over
Wikipedia once or twice and to be rated equally
good as Wikipedia 4?7 times.
7 Concluding Remarks
GREC?10 has, for the first time, produced systems
which can do end-to-end named-entity generation,
moreover most of which can do it well enough for
human judges do rate them as good as Wikipedia
or better around one third of the time.
This was the second time the GREC-NEG Task
was run, and the first time GREC-NER and GREC-
Full were run. As in 2009, many more teams reg-
istered than were able to submit a system by the
deadline, but we hope that the GREC data (which
is now freely available) will lead to many more re-
Test Set NEG-Full-a Test Set NEG-Full-b
System Mean text-level BLEU-4 BLEU-4 NIST System Mean text-level BLEU-4 BLEU-4 NIST
Corpus 1.00 A 1.000 13.71 Corpus .991 A 0.985 13.74
Corpus (test set vers.) .941 B 0.923 12.92 Corpus (test set vers.) .946 B 0.929 13.20
UDel/UMUS .934 B C 0.925 13.13 UDel/UMUS .939 B C 0.928 13.29
UDel/XRCE .921 B C 0.898 12.98 UDel/XRCE .928 B C 0.907 13.15
UDel-Full .905 C 0.870 12.59 UDel-Full .912 C 0.882 12.82
UDel/Base-rand .812 D 0.809 12.17 UDel/Base-rand .823 D 0.821 12.43
OpenNLP/Base-rand .809 D 0.775 11.49 OpenNLP/Base-rand .817 D 0.785 11.72
LingPipe/Base-rand .752 E 0.753 11.48 LingPipe/Base-rand .763 E 0.764 11.70
Table 10: GREC-FULL: Mean text-level BLEU-4 scores, system-level BLEU-4 and NIST scores.
Clarity Fluency
System Mean + 0 ? System Mean + 0 ?
Corpus -0.033 A 1 20 0 Corpus 0 A 0 30 0
UDel/XRCE -2.209 A B 0 6 15 UDel/XRCE -3.424 B 1 4 16
UDel/UMUS -2.638 A B 1 6 14 UDel/UMUS -4.057 B C 2 5 14
UDel-Full -2.833 B 0 7 14 OpenNLP/Base-rand -4.671 B C 2 4 15
OpenNLP/Base-rand -3.486 B 1 7 13 UDel-Full -4.967 B C 0 4 16
UDel/Base-rand -4.667 B 0 5 16 UDel/Base-rand -6.800 C D 0 2 19
LingPipe/Base-rand -7.829 C 0 0 21 LingPipe/Base-rand -8.405 D 0 0 21
Table 11: GREC-FULL: Results for Clarity and Fluency preference judgement experiment. Mean =
mean of individual scores (where scores ranged from -10.0 to + 10.0); + = number of times system was
preferred; ? = number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was
preferred.
sults being produced and reported over time.
Acknowledgments
Many thanks to the members of the Corpora and
SIGGEN mailing lists, and Brighton University
colleagues who helped with the online RE selec-
tion experiments for the b-versions of the test sets.
Thanks are also due to the Oxford, Kings College
London and University College London students
who helped with the intrinsic evaluation experi-
ments.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC?98, pages
563?566.
A. Belz and S. Varges. 2007. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz. 2010. GREC named entity recognition and
GREC named entity regeneration challenges 2010:
Participants? Pack. Technical Report NLTG-10-01,
Natural Language Technology Group, University of
Brighton.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. CambridgeUni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings of MUC-6, pages
45?52.
Unsupervised Alignment of Comparable Data and Text Resources
Anja Belz Eric Kow
School of Computing, Engineering and Mathematics
University of Brighton
Brighton BN2 4GJ, UK
{A.S.Belz,E.Y.Kow}@brighton.ac.uk
Abstract
In this paper we investigate automatic data-
text alignment, i.e. the task of automatically
aligning data records with textual descrip-
tions, such that data tokens are aligned with
the word strings that describe them. Our meth-
ods make use of log likelihood ratios to esti-
mate the strength of association between data
tokens and text tokens. We investigate data-
text alignment at the document level and at
the sentence level, reporting results for sev-
eral methodological variants as well as base-
lines. We find that log likelihood ratios pro-
vide a strong basis for predicting data-text
alignment.
1 Introduction
Much of NLP system building currently uses aligned
parallel resources that provide examples of the in-
puts to a system and the outputs it is intended to pro-
duce. In Machine Translation (MT), such resources
take the form of sentence-aligned parallel corpora of
source-language and target-language texts; in pars-
ing and surface realisation, parse-annotated corpora
of naturally occurring texts are used, where in pars-
ing, the inputs are the sentences in the texts and the
outputs are the parses represented by the annotations
on the sentences, and in surface realisation, the roles
of inputs and outputs are reversed.
In MT parallel resources exist, and in fact are pro-
duced in large quantities daily, and in some cases
(e.g. multilingual parliamentary proceedings) are
publicly available. Moreover, even if resources are
created specifically for system building (e.g. NIST?s
OpenMT evaluations) the cost is offset by the fact
that the resulting translation system can be expected
to generalise to new domains to some extent.
While parse-annotated corpora are in the first in-
stance created by hand, here too, parsers and surface
realisers built on the basis of such corpora are ex-
pected to generalise beyond the immediate corpus
domain.
In data-to-text generation, as in parsing, parallel
resources do not occur naturally and have to be cre-
ated manually. The associated cost is, however, in-
curred for every new task, as systems trained on a
given parallel data-text resource cannot be expected
to generalise beyond task and domain. Automatic
data-text alignment methods, i.e. automatic methods
for creating parallel data-text resources, would be
extremely useful for system building in this situa-
tion, but no such methods currently exist.
In MT there have been recent efforts (reviewed
in the following section) to automatically produce
aligned parallel corpora from comparable resources
where texts in two different languages are about sim-
ilar topics, but are not translations of each other).
Taking our inspiration from this work in MT, in this
paper we investigate the feasibility of automatically
creating aligned parallel data-text resources from
comparable data and text resources available on the
web. This task of automatic data-text alignment,
previously unexplored as far as we are aware, is the
task of automatically aligning data records with tex-
tual descriptions, such that data tokens are aligned
with the word strings that describe them. For exam-
ple, the data tokens height metres=250 might be
aligned with the word string with an altitude of 250
102
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 102?109,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
metres above sea level.
We start in Section 2 with an overview of data-
to-text generation and of related work in MT. In
Section 3 we describe our comparable data and text
resources and the pre-processing methods we apply
to them. In Section 4 we provide an overview of
our unsupervised learning task and of the method-
ology we have developed for it. We then describe
our methods and results for sentence selection (Sec-
tion 5) and sentence-level data selection (Section 6)
in more detail. We finish with a discussion of our
results and some conclusions (Section 7).
2 Background and Related Research
Work in data-to-text generation has involved a va-
riety of different domains, including generating
weather forecasts from meteorological data (Sripada
et al, 2003), nursing reports from intensive care data
(Portet et al, 2009), and museum exhibit descrip-
tions from database records (Isard et al, 2003; Stock
et al, 2007); types of data have included dynamic
time-series data (such as meteorological or medical
data) and static database entries (as in museum ex-
hibits).
The following is an example of an input/output
pair from the M-PIRO project (Androutsopoulos et
al., 2005), where the input is a database record for a
museum artifact, and the output is a description of
the artifact:
creation-period=archaic-period,
current-location=Un-museum-Pennsylvania,
painting-techinique-used=red-figure-technique,
painted-by=Eucharides, creation-time=between
(500 year BC)(480 year BC)
Classical kylix
This exhibit is a kylix; it was created during the archaic period
and was painted with the red figure technique by Eucharides.
It dates from between 500 and 480 B.C. and currently it is in
the University Museum of Pennsylvania.
While data and texts in the three example domains
cited above do occur naturally, two factors mean
they cannot be used directly as target corpora or
training data for building data-to-text generation
systems: one, most are not freely available to re-
searchers (e.g. by simply being available on the
Web), and two, more problematically, the correspon-
dence between inputs and outputs is not as direct
as it is, say, between a source language text and its
translation. In general, naturally occurring resources
of data and related texts are not parallel, but are
merely what has become known as comparable in
the MT literature, with only a subset of data having
corresponding text fragments, and other text frag-
ments having no obvious corresponding data items.
Moreover, data transformations may be necessary
before corresponding text fragments can be identi-
fied.
In this paper we look at the possibility of automat-
ically identifying parallel data-text fragments from
comparable corpora in the case of data-to-text gen-
eration from static database records. Such a paral-
lel data-text resource could then be used to train an
existing data-to-text generation system, or even to
build a new statistical generator from scratch, e.g.
using techniques from statistical MT (Belz and Kow,
2009).
In statistical MT, the expense of manually creat-
ing new parallel MT corpora, and the need for very
large amounts of parallel training data, has led to a
sizeable research effort to develop methods for auto-
matically constructing parallel resources. This work
typically starts by identifying comparable corpora.
Much of it has focused on identifying word trans-
lations in comparable corpora, e.g. Rapp?s approach
was based on the simple and elegant assumption that
if words Af and Bf have a higher than chance co-
occurrence frequency in one language, then two ap-
propriate translations Ae and Be in another language
will also have a higher than chance co-occurrence
frequency (Rapp, 1995; Rapp, 1999). At the other
end of the spectrum, Resnik and Smith (2003) search
the Web to detect web pages that are translations of
each other. Other approaches aim to identify pairs
of sentences (Munteanu and Marcu, 2005) or sub-
sentential fragments (Munteanu and Marcu, 2006)
that are parallel within comparable corpora.
The latter approach is particularly relevant to our
work. Munteanu and Marcu start by translating each
document in the source language (SL) word for word
into the target language (TL). The result is given to
an information retrieval (IR) system as a query, and
the top 20 results are retained and paired with the
given SL document. They then obtain all sentence
pairs from each pair of SL and TL documents, and
discard those sentence pairs that have only a small
103
number of words that are translations of each other.
To the remaining sentences they then apply a frag-
ment detection method which tries to distinguish be-
tween source fragments that have a translation on the
target side, and fragments that do not.
The biggest difference between the MT situation
and the data-to-text generation situation is that in the
former, sentence-aligned parallel resources exist and
can be used as a starting point. E.g. Munteanu and
Marcu use an existing parallel Romanian-English
corpus to (automatically) create a lexicon which is
then used in various ways in their method. In data-
to-text generation we have no analogous resources to
help us get started. The approach to data-text align-
ment described in this paper therefore uses no prior
knowledge, and all our learning methods are unsu-
pervised.
3 Data and Texts about British Hills
As a source of data, we use the Database of British
Hills (BHDB) created by Chris Crocker,1 version
11.3, which contains measurements and other in-
formation about 5,614 British hills. We add some
information to the BHDB records by performing re-
verse geocoding via the Google Map API2 which al-
lows us to convert latitude and longitude informa-
tion from the hills database into country and region
names. We add the latter to each database record.
On the text side, we use Wikipedia articles in
the WikiProject British and Irish Hills (retrieved on
2009-11-09). At the time of retrieval there were 899
pages covered by this WikiProject, 242 of which
were of quality category B or above.3
3.1 Aligning database entries with documents
Given that different hills can share the same name,
and that the same hill can have several different
names and spellings, matching up the data records in
the BHDB with articles in Wikipedia is not entirely
trivial. The method we use is to take a given hill?s
name from the BHDB record and to perform a search
of Wikipedia with the hill?s name as a search term,
using the Mediawiki API. We then pair up the BHDB
1http://www.biber.fsnet.co.uk
2http://code.google.com/apis/maps/
3B = The article is mostly complete and without major is-
sues, but requires some further work.
k-name v-name-Beacon_Fell
k-area v-area-Lakes:_S_Fells
k-height-metres v-height-metres-255
k-height-feet v-height-feet-837
k-feature v-feature-cairn
k-classification v-classification-WO
k-classification v-classification-Hu
k-locality v-locality-Skelwith
k-admin-area-level1 v-admin-area-level1-England
k-admin-area-level2 v-admin-area-level2-Cumbria
k-country v-country-United_Kingdom
Figure 1: Result of preprocessing BHDB record for Bea-
con Fell.
record with the Wikipedia article returned as the top
search result.
We manually evaluated the data-text pairs
matched by this method, scoring each pair
good/unsure/bad. We found that 759 pairs out of 899
(the number of Wikipedia articles in the WikiPro-
ject British and Irish Hills at the time of retrieval),
or 84.4%, were categorised ?good? (i.e. they had
been matched correctly), a further 89 pairs (9.8%)
were categorised ?unsure?, and the remainder was a
wrong match. This gave us a corpus of 759 correctly
matched data record/text pairs to work with.
We randomly selected 20 of the data record/text
pairs for use as a development set to optimise mod-
ules on, and another 20 pairs for use as a test set, for
which we did not compute scores until the methods
were finalised. We manually annotated the 40 texts
in the development and test sets to mark up which
subsets of the data and which text substrings cor-
respond to each other for each sentence (indicating
parallel fragments as shown at the bottom of Fig-
ure 2).
3.2 Pre-processing of data records and texts
Database records: We perform three kinds of pre-
processing on the data fields of the BHDB database
records: (1) deletion; (2) structure flattening, and
(3) data conversion including the reverse geocoding
mentioned above (the result of these preprocessing
steps for the English hill Beacon Fell can be seen in
Figure 1).
Furthermore, for each data field key = value
we separate out key and value, prefixing the key
with k- and the value with v-key (e.g. v-area and
k-area-Berkshire). Each data field is thus con-
104
verted into two ?data tokens?.
Texts: For the texts, we first strip out Wikipedia
mark-up to yield text-only versions. We then per-
form sentence splitting and tokenisation (with our
own simple tools). Each text thus becomes a se-
quence of strings of ?text tokens?.
4 Task and Methodology Overview
Our aim is to automatically create aligned data-text
resources where database records are paired with
documents, and in each document, strings of word
tokens are aligned with subsets of data tokens from
the corresponding database record. The first two
items shown in Figure 2 are the text of the Wikipedia
article and the BHDB record about Black Chew Head
(the latter cut down to the fields we actually use
and supplemented by the administrative area infor-
mation from reverse geocoding). The remainder of
the figure shows fragments of text paired with sub-
sets of data fields that could be extracted from the
two comparable inputs.
How to get from a collection of texts and a sepa-
rate but related collection of database records, to the
parallel fragments shown at the bottom of Figure 2
is in essence the task we address. In order to do this
automatically, we identify the following steps (the
list includes, for the sake of completeness, the data
record/document pairing and pre-processing meth-
ods from the previous section):
1. Identify comparable data and text resources and
pair up individual data records and documents
(Section 3).
2. Preprocess data and text, including e.g. tokeni-
sation and sentence splitting (Section 3.2).
3. Select sentences that are likely to contain word
strings that correspond to (?realise?) any data
fields (Section 5).
4. For each sentence selected in the previous step,
select the subset of data tokens that are likely to
be realised by the word strings in the sentence
(Section 6).
5. Extract parallel fragments (future work).
5 Sentence Selection
The Wikipedia articles about British Hills in our cor-
pus tend to have a lot of text in them for which the
corresponding entry in BHDB contains no matching
data. This is particularly true of longer articles about
more well-known hills such as Ben Nevis. The ar-
ticle about the latter, for example, contains sections
about the name?s etymology, the geography, geol-
ogy, climate and history, and even a section about the
Ben Nevis Distillery and another about ships named
after the hill, none of which the BHDB entry for Ben
Nevis contains any data about. The task of sentence
selection is to rule out such sections, and pick out
those sentences that are likely to contain text that can
be aligned with data. Using the example in Figure 2,
the aim would be to select the first two sentences
only.
Our sentence selection method consists of (i) esti-
mating the strength of association between data and
text tokens (Section 5.1); and (ii) selecting those
sentences for further consideration that have suf-
ficiently strong and/or numerous associations with
data tokens (Section 5.2).
5.1 Computing positive and negative
associations between data and text
We measure the strength of association between data
tokens and text tokens using log-likelihood ratios
which have been widely used for this sort of pur-
pose (especially lexical association) since they were
introduced to NLP (Dunning, 1993). They were e.g.
used by Munteanu & Marcu (2006) to obtain a trans-
lation lexicon from word-aligned parallel texts.
We start by obtaining counts for the number of
times each text token w co-occurs with each data
token d, the number of times w occurs without d be-
ing present, the number of times d occurs without w,
and finally, the number of times neither occurs. Co-
occurrence here is at the document/data record level,
i.e. a data token and a text token co-occur if they are
present in the same document/data record pair (pairs
as produced by the method described in Section 3).
This allows us to compute log likelihood ratios for
all data-token/text-token pairs, using one of the G2
formulations from Moore (2004) which is shown in
slightly different representation in Figure 3. The re-
sulting G2 scores tell us whether the frequency with
which a data token d and a text token w co-occur
deviates from that expected by chance.
If the G2 score for a given (d,w) pair is greater
than their joint probability p(d)p(w), then the asso-
105
Wikipedia text:
Black Chew Head is the highest point ( or county top ) of Greater Manchester , and forms part of the Peak District , in northern England . Lying
within the Saddleworth parish of the Metropolitan Borough of Oldham , close to Crowden , Derbyshire , it stands at a height of 542 metres above
sea level . Black Chew Head is an outlying part of the Black Hill and overlooks the Chew Valley , which leads to the Dovestones Reservoir .
Entry from Database of British Hills:
name area height m height ft feature classification top locality adm area1 adm area2 country
Black Peak 542 1778 fence Dewey Greater Glossop England Derbyshire UKChew Head District Manchester
Parallel fragments:
name area top adm area1 adm area2
Black Chew Head Peak District Greater Manchester England Derbyshire
Black Chew Head is the highest point ( or county top
) of Greater Manchester , and forms part of the Peak
District , in northern England .
height (m)
542 it stands at a height of 542 metres above sea level .
Figure 2: Black Chew Head: Wikipedia article, entry in British Hills database (the part of it we use), and parallel
fragments that could be extracted.
ciation is taken to be positive, i.e. w is likely to be
part of a realisation of d, otherwise the association
is taken to be negative, i.e. w is likely not to be part
of a realisation of d.
Note that we use the notation G2+ below to denote
a G2 score which reflects a positive association.
5.2 Selecting sentences on the basis of
association strength
In this step, we consider each sentence s in turn. We
ignore those text tokens that have only negative asso-
ciations with data tokens. For each of the remaining
text tokens ws in s we obtain maxg2score(ws), its
highest G2+ score with any data token d in the set D
of data tokens in the database record:
maxg2score(ws) = argmax
d?D
G2+(d,ws)
We then use these scores in two different ways to
select sentences for further processing:
1. Thresholding: Select all sentences that have at
least one text token w with maxg2score(w) >
t, where t is a given threshold.
2. Greater-than-the-mean selection: Select all
sentences whose mean maxg2score (com-
puted over all text tokens with positive associ-
ation in the sentence) is greater than the mean
of mean maxg2scores (computed over all sen-
tences in the corpus).
The reason why we are not interested in negative as-
sociations in sentence selection is that we want to
identify those sentences that are likely to contain a
text fragment of interest (characterised by high pos-
itive association scores), and such sentences may
well also contain material unlikely to be of interest
(characterised by negative association scores).
5.3 Results
Table 1 shows the results for sentence selection, in
terms of Precision, Recall and F1 Scores. In addi-
tion to the two methods described in the preceding
section, we computed two baselines. Baseline 1 se-
lects just the first sentence, which yields a Precision
of 1 and a Recall of 0.141 for the test set (0.241 for
the development set), indicating that in the manually
aligned data, the first sentence is always selected and
that less than a quarter of sentences selected are first
sentences. Baseline 2 selects all sentences which
yields a Recall of 1 and a Precision of 0.318 for the
test set (0.377 for the development set), indicating
that around one third of all sentences were selected
in the manually aligned data.
Greater-than-the-mean selection roughly evens
out Recall and Precision scores, with an F1 Score
above both baselines. As for thresholded selection,
applying thresholds t < 10 results in all sentences
being selected (hence the same R/P/F1 scores as for
Baseline 2).4 Very high thresholds (500+) result in
4This ties in with Moore?s result confirming previous anec-
106
G2(d,w) = 2N
(
p(d,w)log p(d,w)p(d)p(w) + p(d,?w)log
p(d,?w)
p(d)p(?w) + p(?d,w)log
p(?d,w)
p(?d)p(w) + p(?d,?w)log
p(?d,?w)
p(?d)p(?w)
)
Figure 3: Formula for computing G2 from Moore (2004) (N is the sample size).
Development Set Test Set
Selection Method P R F1 P R F1
1st sentence only (Baseline 1) 1.000 0.241 0.388 1.000 0.141 0.247
All sentences (Baseline 2) 0.377 1.000 0.548 0.318 1.000 0.483
Greater-than-the-mean selection 0.516 0.590 0.551 0.474 0.634 0.542
Thresholded selection t = 60 0.487 0.928 0.639 0.423 0.965 0.588
Table 1: Sentence selection results in terms of Precision, Recall and F1 Score.
very high Precision (> .90) with Recall dropping
below 0.15. In the table, we show just the threshold
that achieved the highest F1 Score on the develop-
ment set (t = 60).
Selecting a threshold on the basis of highest F1
Score (rather than, say, F0.5) in our case means we
are favouring Recall over Precision, the intuition be-
ing that at this stage it is more important not to lose
sentences that are likely to have useful realisations in
them (than it is to get rid of sentences that are not).
6 Data Selection
For data selection, the aim is to select, for each sen-
tence remaining after sentence selection, the subset
of data tokens that are realised by (some part of) the
sentence. In terms of Figure 2, the aim would be to
select for each of sentence 1 and 2 the data tokens
which are shown next to the fragment(s) extracted
from it at the bottom of Figure 2. Looked at another
way, we want to get rid of any data tokens that are
not likely to be realised by any part of the sentence
they are paired with.
We preform sentence selection separately for each
sentence s, obtaining the subset Ds of data tokens
likely to be realised by s, in one of the following
two ways:
1. Individual selection: Retain all and only those data
tokens that have a sufficiently strong positive asso-
ciation with at least one text token ws:
Ds = {d
???ws(G2+(d,ws) > t)
}
dotal evidence that G2 scores above 10 are a reliable indication
of significant association (Moore, 2004, p. 239).
2. Pairwise selection: Consider each pair of key and
value data tokens dki , dvi that were originally de-
rived from the same data field fi. Retain all and
only those pairs dki , dvi where either dki or dvi has a
sufficiently strong association with at least one text
token:
Ds =
{
dki , dvi
????wsj
(
G2+(dki , wsj ) > t
)
?
?wsm
(
G2+(dvi , wsm) > t
)}
Note that while previously each sentence in a text
was associated with the same set of data tokens (the
original complete set), after data selection each sen-
tence is associated with its own set of data tokens
which may be smaller than the original set.
If data selection produces an empty data token set
Ds for a given sentence s, then s, along with its data
token set Ds, are removed from the set of pairs of
data token set and sentence.
We evaluate data selection for the baseline of se-
lecting all sentences, and the above two methods in
combination with different thresholds t. As the eval-
uation measure we use the Dice coefficient (a mea-
sure of set similarity), computed at the document
level between (i) the union D of all sentence-level
sets of data tokens selected by a given method and
(ii) the corresponding reference data token set DR,
i.e. the set of data tokens in the manual annotations
of the same text in the development/test data. Dice
is defined as follows:
Dice(D,DR) = 2|D ?D
R|
|D|+ |DR|
Table 6 shows results for the baseline and individual
and pairwise data selection, on the development set
107
Sentence selection method
Greater-than-the-mean Thresholded, t = 60 All-sentences 1st-sentence
De
vS
et All data tokens 0.666 0.666 0.666 0.666
Individual selection t = 0: 0.666 t = 0: 0.666 t = 0: 0.666 t = 0: 0.666
Pairwise selection t = 19: 0.706 t = 18: 0.709 t = 18: 0.717 t = 1: 0.697
Te
stS
et All data tokens 0.716 0.748 0.748 0.748
Individual selection t = 0: 0.716 t = 0: 0.748 t = 0: 0.748 t = 0: 0.748
Pairwise selection t = 19: 0.751 t = 18: 0.777 t = 18: 0.775 t = 1: 0.767
Table 2: Data selection results in terms of Dice coefficient. Results shown for data selection methods preceded by
different sentence selection methods.
(top half of the table), and on the test set (bottom
half). In each case we show results for the given
data selection method applied after each of the four
different sentence selection methods described in
Section 5: greater-than-the-mean, thresholded with
t = 60, and the first-sentence-only and all-sentences
baselines (these index the columns).
Again, we optimised the two non-baseline meth-
ods on the development set, finding the best thresh-
old t separately for each combination of a given
data selection method with a given sentence selec-
tion method. This yielded the t values shown in the
cells in the table.
Looking at the results, selecting data tokens indi-
vidually (second row in each half of Table 6) cannot
improve Dice scores compared to leaving the origi-
nal data token set in place (first row); this is the case
across all four sentence selection methods. The pair-
wise data selection method (third row) achieves the
best results, although it does not appear to make a
real difference whether or not sentence selection is
applied prior to data selection.
7 Conclusion
In this paper we have reported our work to date
on data-text alignment, a previously unexplored
problem as far as we are aware. We looked at
alignment of two comparable resources (one a col-
lection of data records about British Hills, the
other a collection of texts about British Hills) at
the data record/document level, where our simple
search-based method achieved an accuracy rate of
84%. Next we looked at alignment at the data
record/sentence level. Here we obtained a best F1
score of 0.588 for sentence selection and a best mean
Dice score of 0.777 for data selection.
The best performing methods described here pro-
vide a good basis for further development of our
parallel fragment extraction methods, in particular
considering that the methods start from nothing and
obtain all knowledge about data-text relations in a
completely unsupervised way. Our results show
that log likelihood ratios, which have been widely
used for measuring lexical association, but were so
far unproven for the data-text situation, can provide
a strong basis for identifying associations between
data and text.
References
I. Androutsopoulos, S. Kallonis, and V. Karkaletsis.
2005. Exploiting owl ontologies in the multilingual
generation of object descriptions. In Proceedings of
the 10th European Workshop on Natural Language
Generationf (ENLG?05), pages 150?155.
Anja Belz and Eric Kow. 2009. System building cost
vs. output quality in data-to-text generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 16?24.
E. Briscoe, J. Carroll, and J. Graham. 2006. The sec-
ond release of the rasp system. In Proceedings of the
COLING/ACL 2006 Interactive Presentation Sessions.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 1:61?74.
A. Isard, J. Oberlander, I. Androutsopoulos, and C. Math-
eson. 2003. Speaking the users? languages. IEEE In-
telligent Systems Magazine: Special Issue ?Advances
in Natural Language Processing, 18(1):40?45.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
9th Converence on Empirical Methods in Natural Lan-
guage Processing (EMNLP?04), pages 333?340.
Dragos Munteanu and Daniel Marcu. 2005. Improving
machine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 31:477?
504.
108
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), pages 81?88,
Morristown, NJ, USA. Association for Computational
Linguistics.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic generation of
textual summaries from neonatal intensive care data.
Artificial Intelligence, 173:789?816.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 320?322, Morristown, NJ, USA. Association for
Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, pages 519?526, Morristown, NJ,
USA. Association for Computational Linguistics.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29:349?380.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2003. Ex-
ploiting a parallel text-data corpus. In Proceedings of
Corpus Linguistics 2003, pages 734?743.
Oliviero Stock, Massimo Zancanaro, Paolo Busetta adn
Charles Callaway, Anbtonio Kru?ger, Michael Kruppa,
Tsvi Kuflik, Elena Not, and Cesare Rocchi. 2007.
Adaptive, intelligent presentation of information for
the museum visitor in PEACH. User Modeling and
User-Adapted Interaction, 17(3):257?304.
109
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 125?127,
Utica, May 2012. c?2012 Association for Computational Linguistics
Natural Language Generation for a Smart Biology Textbook
Eva Banik1, Eric Kow1, Vinay Chaudhri2, Nikhil Dinesh2, and Umangi Oza3
1{ebanik,kowey}@comp-ling.co.uk, Computational Linguistics Ltd, London, UK
2 {chaudhri,dinesh}@ai.sri.com, SRI International, Menlo Park, CA
3umangi.oza@evalueserve.com, Evaluserve, New Delhi, India
1 Application Context
In this demo paper we describe the natural lan-
guage generation component of an electronic
textbook application, called Inquire1. Inquire
interacts with a knowledge base which encodes
information from a biology textbook. The
application includes a question-understanding
module which allows students to ask questions
about the contents of the book, and a question-
answering module which retrieves the corre-
sponding answer from the knowledge base. The
task of the natural language generation mod-
ule is to present specific parts of the answer in
English. Our current generation pipeline han-
dles inputs that describe the biological func-
tions of entities, the steps of biological processes,
and the spatial relations between parts of enti-
ties. Our ultimate goal is to generate paragraph-
length texts from arbitrary paths in the knowl-
edge base. We describe here the natural lan-
guage generation pipeline and demonstrate the
inputs and generated texts. In the demo pre-
sentation we will show the textbook application
and the knowledge base authoring environment,
and provide an opportunity to interact with the
system.
2 The Knowledge Base
The knowledge base contains information from
a college-level biology textbook2, encoded by bi-
1The work described in this paper and presented in
the demo is funded by Vulcan Inc.
2 Reece et al 2010. Campbell biology. Pearson
Publishing.
ologists as part of project HALO at SRI3. The
core of the knowledge base is the CLIB ontol-
ogy4, which is extended with biology-specific in-
formation. The knowledge base encodes entity-
to-event relations (similar to thematic roles in
linguistics), event-to-event relations (discourse
relations), various property values and relations
between properties, spatial relations, cardinality
constraints, and roles that participants play in
events. The input to the generation pipeline is a
set of triples extracted from the biology knowl-
edge base. Currently our content selection in-
cludes either an event and the entities that par-
ticipate in the event, or a set of entities and
spatial relations between them.
3 Generation Grammar and Lexicon
Our generation grammar consists of a set of Tree
Adjoining Grammar (TAG) elementary trees.
Each tree is associated with either a single rela-
tion, or a set of relations in the knowledge base.
As an example, Fig 1 illustrates the mapping
between elementary trees and event participant
relations in the KB for the above input. We
currently associate up to three different elemen-
tary trees with each event and the connected
set of participant relations: an active senten-
tial tree, a passive sentential tree and a complex
noun phrase.
The knowledge base provides concept-to-word
3 Gunning Et al, 2010. Project halo update
progress toward digital aristotle. AI Magazine Fall:33-
58. See also http://www.projecthalo.com/
4http://www.cs.utexas.edu/users/mfkb/RKF/clib.html
125
Figure 1: The grammar of the surface realizer
mappings (a list of synonyms) for every concept,
and the words are used in the generation lexi-
con to anchor elementary TAG trees. Our gen-
eration grammar consists of a set of TAG tree
templates, which are defined as combinations of
tree fragments and are compiled using the XMG
metgrammar toolkit5.
These underspecified elementary trees are fur-
ther specified in the generation lexicon, which
maps concepts onto elementary tree templates,
and associates a word (an anchor) with the
tree, along with other idiosynchratic information
(e.g., preposition choice). We create a genera-
tion lexicon dynamically at run-time, by map-
ping tree templates onto concepts based on the
number and types of participants, and the lexi-
cal information associated with the event (e.g.,
the preposition requirements of the verb).
Concept names for entities are included in
the elementary trees as features on the corre-
sponding NP nodes. These features form part
of the input to the referring expression genera-
tion module, which looks up the concept name
5https://sourcesup.renater.fr/xmg/
in the concept-to-word mapping to obtain a list
of possible noun phrases.
4 Realization
Our natural language generation pipeline is cen-
tered around the GenI surface realizer6,7. The
set of triples yielded by content selection are first
aggregated and converted to GenI?s input for-
mat, a set of flat semantic literals. We then feed
this input to GenI to produce an underspecified
surface form in which referring expressions are
still underspecified:
NP is detach from NP resulting in NP at NP
NP detach from NP resulting in NP at NP
Detachment of NP from NP resulting in NP at NP
A post-processing module carries out refer-
ring expression generation and morphological re-
alization to produce the fully specified output.
6 Kow, Eric. 2007. Surface realisation: ambiguity
and determinism. Doctoral Dissertation, Universite de
Henri Poincare - Nancy 1.
7 Banik, Eva 2010. A minimalist approach to gen-
erating coherent texts. Phd thesis, Department of Com-
puting, The Open University
126
Question Answering & Reasoning Algorithms
Event Instance 
Content Selection
Set of triples
Input aggregation and conversion +Stylistic control
Knowledge Base
Realization with GenI
Morphology &referring expression generation
Semantic literals +input parameters
Ranking
Underspecified realizations
Linguistic Resources
Generation Lexicon
Grammar: Description of TAG tree templates
Concept-to-Wordmappings
Mapping of KB relationsto TAG tree templates
Morphological lexicon
Verb frames (preposition choice)
NLG Pipeline
Figure 2: Linguistic resources and the generation pipeline
Our referring expression realization algorithm
performs further semantic aggregation where
necessary to produce cardinals (?two chromo-
somes?), and decides on a suitable determiner
based on previous mentions of instance names
and subclasses in the discourse context (def-
inite/indefinite determiner, ?another? or ?the
same?). For the input shown in Fig 1, our sys-
tem will produce the following three realizations:
1. A sister chromatid detaches from another sister chro-
matid resulting in two chromosomes at a kinetochore.
2. A sister chromatid is detached from another sister
chromatid resulting in two chromosomes at a kinetochore.
3. Detachment of a sister chromatid from another sister
chromatid resulting in two chromosomes at a kinetochore
We rank the generated outputs based on their
linguistic properties using optimality theoretic
constraints (e.g., active sentences are ranked
above passive sentences), where each constraint
corresponds to a (set of) tree fragments that
contributed to building the tree that appears in
the output. Our system also allows for extra in-
put parameters to be sent to GenI to restrict the
set of generated outputs to fit a specific context
(e.g., syntactic type or focused discourse entity).
Our full natural language generation pipeline is
illustrated in Fig 2.
5 Future Work
We are currently working on extending the sys-
tem to handle more relations and other data
types in the knowledge base. This involves ex-
tending the grammar to new sentence types and
other linguistic constructions, and extending the
content selection module to return more triples
from the knowledge base. Our ultimate goal is
to be able to generate arbitrary ? but in some
sense well-formed ? paths from the knowledge
base as coherent paragraphs of text.
127
Proceedings of the 14th European Workshop on Natural Language Generation, pages 20?29,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
User-Controlled, Robust Natural Language Generation from an Evolving
Knowledge Base
Eva Banik
Computational
Linguistics Ltd
London, UK
ebanik@comp-ling.com
Eric Kow
Computational
Linguistics Ltd
London, UK
kowey@comp-ling.com
Vinay Chaudhri?
SRI International
Menlo Park, CA
chaudhri@ai.sri.com
Abstract
In this paper we describe a natural lan-
guage generation system which produces
complex sentences from a biology knowl-
edge base. The NLG system allows do-
main experts to discover errors in the
knowledge base and generates certain
parts of answers in response to users?
questions in an e-textbook application.
The system allows domain experts to cus-
tomise its lexical resources and to set pa-
rameters which influence syntactic con-
structions in generated sentences. The
system is capable of dealing with certain
types of incomplete inputs arising from a
knowledge base which is constantly edited
and includes a referring expression gen-
eration module which keeps track of dis-
course history. Our referring expression
module is available for download as the
open source Antfarm tool1.
1 Introduction
In this paper we describe a natural language gen-
eration system we have developed to interface
with a biology knowledge base. The knowledge
base (KB) encodes sentences from a biology text-
book, and the ultimate goal of our project is to
develop an intelligent textbook application which
can eventually answer students? questions about
biology2 (Spaulding et al, 2011).
?The work reported in this paper was supported by fund-
ing from Vulcan, Inc. We would also like to thank the mem-
bers of the Inquire Biology development team: Roger Cor-
man, Nikhil Dinesh, Debbie Frazier, Stijn Heymans, Sue Hi-
nojoza, David Margolies, Adam Overholtzer, Aaron Spauld-
ing, Ethan Stone, William Webb, Michael Wessel and Neil
Yorke-Smith.
1https://github.com/kowey/antfarm
2http://www.aaaivideos.org/2012/
inquire_intelligent_textbook/
The natural language generation module is part
of a larger system, which includes a question un-
derstanding module, question answering and rea-
soning algorithms, as well as an answer presenta-
tion module which produces pages with informa-
tion from the KB. We measure the progress and
consistency of encoding by asking ?what is an X??
type questions of the application and evaluate the
quality of answers. In response to these questions,
the system generates ?glossary pages? of concepts,
which display all information about concept X in
the KB that are deemed relevant. The NLG mod-
ule is used for two purposes in our system: to
check the completeness and consistency of the KB
(instead of looking at complex graphs of the en-
coded knowledge, it is easier to detect errors in
natural language sentences), and to present parts
of answers in response to questions.
One goal of our project was to develop a tool
which empowers biology teachers to encode do-
main knowledge with little training in formal
knowledge representation. In the same spirit, we
aimed to develop an NLG system which allowed
domain experts to easily and intuitively customize
the generated sentences as much as possible, with-
out any training on the grammar or internal work-
ings of the system. This was necessary because
many domain-specific concepts in the KB are best
expressed by biology terminology and linguistic
constructions specific to the domain. We devel-
oped a utility which allows encoders to not only
associate lexical items with concepts in the KB but
also customise certain lexical parameters which
influence the structure of sentences generated to
describe events.
Another requirement was robustness: since the
knowledge base is constantly edited, the NLG sys-
tem had to be able to deal with missing lexical in-
formation, incomplete inputs, changing encoding
guidelines, and bugs in the KB as much as possi-
ble. The system also had to be flexible in the sense
20
Figure 1: Architecture of the AURA NLG system
that it had to be able to generate different versions
of the same output to suit specific contexts or types
of concepts in its input. Our system therefore gen-
erates all possible realizations for a given input,
and allows the answer presentation module to send
parameters to determine which output is returned
in a specific context.
After describing the architecture of the NLG
module in detail we explain how the system is able
to deal with unseen combination of event-to-entity
relations when describing events. We illustrate the
utility we developed to allow domain experts to
customize the system?s output by adding parame-
ters to lexical entries associated with concepts.
2 Related Work
Work on natural language generation from ontolo-
gies and knowledge bases tends to fall into two
groups. On the one hand, there are tools for ontol-
ogy verbalization which tend to handle a limited
number of relations, and where the goal of the sys-
tem is to help the work of knowledge engineers.
These systems produce template based outputs,
and the texts closely follow the structure of the
ontology (Wilcock, 2003; Galanis and Androut-
sopoulos, 2007). Some of these systems attempt
to minimize reliance on domain-specific linguistic
resources and attempt to detect words in the labels
of the ontology to use as lexical items (Mellish and
Sun, 2005). On the other hand there are NLG sys-
tems which take their input from complex knowl-
edge bases (Reiter et al, 2003; Paris, 1988) and
produce fluent texts geared towards users other
than knowledge engineers. These systems pro-
duce outputs tailored to the user or the context
and they are difficult for non-NLG-experts to cus-
tomize or port to a different domain. Our system
falls halfway between these two groups: like on-
tology verbalizers, we wanted to produce output
for all inputs, using ontology labels if necessary in
the absence of lexical entries. However, like so-
phisticated NLG systems, we also wanted to gen-
erate good quality output for inputs for which the
system had lexical resources, and we also wanted
to be able to tailor the generated output to the con-
text in which it is displayed. Our input was also
more expressive than the input of ontology verbal-
izers, because of the presence of cardinality con-
straints and co-references in our KB. Our work is
perhaps most closely related to the MIAKT sys-
tem which also allows domain experts to edit lex-
ical knowledge and schemas (Bontcheva, 2004;
Bontcheva and Wilks, 2004). Like MIAKT, we
also aimed to develop an NLG system which can
be easily maintained as the KB changes.
3 Architecture of the AURA NLG system
Our NLG system generates complex sentences
from the AURA knowledge base (Gunning et al,
2010), which contains information from a college-
level biology textbook. AURA is a frame-based
KB which encodes events, the entities that partici-
pate in events, properties, and roles that the entities
play in an event (e.g., catalyst, reactant, messen-
ger, parent). The KB specifies relations between
these types, including event-to-entity, event-to-
event, event-to-property, entity-to-property. The
AURA KB is built on top of the CLIB ontology of
general concepts (Barker et al, 2001), which was
extended with biology-specific information. The
KB consists of a set of concept maps, which de-
scribe all the statements that are true about a con-
cept in our KB. The input to our NLG system is
a set of relations extracted from the KB either in
response to users? questions or when generating
glossary pages that describe specific concepts in
21
detail. The generation pipeline consists of four
main stages: content selection, input conversion,
realisation and referring expression generation, as
illustrated in Fig1.
3.1 Content Selection
Question answering and reasoning algorithms that
return answers or other content in AURA are not
engineered to satisfy the purposes of natural lan-
guage generation. The output of these algorithms
can be best thought of as pointers to concepts in
the KB, which need to be described to provide an
answer to the user. In order for the answer to be
complete in a given context, the output of reason-
ing algorithms have to be extended with additional
relations, depending on the specific question that
was asked, and the context in which the answer
was found in the KB. The relations selected from
the KB also vary depending on the type of con-
cept that is being described (event, entity, role,
property). For example, a user might ask ?What
is a catalyst??. To answer this question, AURA
will retrieve entities from the KB (?role players?)
which play the role of catalyst in various events.
For example, it will find ?adenylyl cyclase?, which
is defined in the KB as a universal catalyst, i.e.,
this information is encoded on the concept map of
Adenylyl cyclase and is regarded as a ?universal
truth?. In this case, our content selection algorithm
will return a single plays triple, and the NLG sys-
tem will produce ?Adenylyl cyclase is a catalyst?.?
Another entity that will be returned in response to
the question is ?ribosomal RNA?. However, ribo-
somal RNA is a catalyst only in specific situations,
and therefore we need to give more detail on the
contexts in which it can play the role of a catalyst.
This includes the event in which ribosomal RNA
is a catalyst, and perhaps the larger process dur-
ing which this event occurs. Accordingly, content
selection here will return a number of relations (in-
cluding agent, object, subevent), and our NLG
system will produce:
?In translation elongation, ribosomal RNA is a
catalyst in the formation of a peptide bond by the
ribosomal RNA and a ribosome.?
Similarly, for ?triose phosphate dehydrogenase?
we will produce
?In energy payoff phase of glycolysis, NAD plus is
converted by a triose phosphate dehydrogenase to
a hydrogen ion, an NADH and a PGAP. Here, the
triose phosphate dehydrogenase is a catalyst.?
For ?cellulose synthase? the situation is slightly
different, because the event in which this entity
plays the role of catalyst is not part of a larger pro-
cess but the function of the entity. So we need
slightly different information to produce the cor-
rect sentence: ?The function of cellulose synthase
is conversion of a chemical in a cell to cellulose.
Here, a cellulose synthase is a catalyst.?
The task of the AURA content selection module is
to determine what information to include for each
entity or event that was returned as the answer to
the question. We do this by retrieving sets of rela-
tions from the KB that match contextual patterns.
We also filter out relations which contain overly
generic classes (e.g., Tangible-Entity), and any du-
plication arising from the presence of inverse rela-
tions or inferences in the KB. The output of con-
tent selection is a structured bundle (Fig. 2), which
contains
(1) the relations that form the input to NLG
(2) information about concepts in the input: what
class(es) they belong to, cardinality constraints
(3) parameters influencing the style of output texts.
3.2 Input Conversion
The realisation phase in our system is carried out
by the GenI surface realizer (Kow, 2007), using
a Tree-Adjoining Grammar (Joshi and Schabes,
1997). The task of the input conversion module
is to interpret the structured bundles returned by
content selection, and to convert the information
to GenI?s input format. We parse the structured
bundles, perform semantic aggregation, interpret
parameters in bundles which influence the style of
the generated text, and convert triples to semantic
literals as required by GenI.
4 Handling Unseen Combinations of
Relations
As Fig 3 shows, a combination of event-to-entity
relations are associated with elementary trees in
the grammar to produce a full sentence. The do-
main of the relations associated with the same
tree is the event which specifies the main pred-
icate of the sentence and the range of the rela-
tions are entities that fill in the individual argu-
ment and modifier positions. Depending on the
event, different relations can be used to fill in the
subject and object positions, and verbs might de-
termine the prepositions needed to realize some of
the arguments. Ideally the mapping between sets
22
(TRIPLES-DATA
:TRIPLES
((|_Cell56531| |has-part| |_Ribosome56523|)
(|_Ribosome56523| |has-part| |_Active-Site56548|)
(|Enzyme-Synthesis17634| |base| |_Cell56531|)
(|Enzyme-Synthesis17634| |raw-material| |_Free-Energy56632|)
(|Enzyme-Synthesis17634| |raw-material| |_Monomer56578|)
(|Enzyme-Synthesis17634| |raw-material| |_Activation-Energy56580|)
(|Enzyme-Synthesis17634| |raw-material| |_Monomer56581|)
(|Enzyme-Synthesis17634| |raw-material| |_Amino-Acid56516|)
(|Enzyme-Synthesis17634| |result| |_Free-Energy56575|)
(|Enzyme-Synthesis17634| |result| |Protein-Enzyme17635|))
:CONSTRAINTS
((|Enzyme-Synthesis17634| |raw-material| (|at-least| 3 |Amino-Acid|)))
:INSTANCE-TYPES
((|_Ribosome56523| |instance-of| |Ribosome|)
(|_Active-Site56548| |instance-of| |Active-Site|)
(|_Cell56531| |instance-of| |Cell|)
(|_Free-Energy56632| |instance-of| |Free-Energy|)
(|_Monomer56578| |instance-of| |Monomer|)
(|_Activation-Energy56580| |instance-of| |Activation-Energy|)
(|_Monomer56581| |instance-of| |Monomer|)
(|_Amino-Acid56516| |instance-of| |Amino-Acid|)
(|_Free-Energy56575| |instance-of| |Free-Energy|)
(|Enzyme-Synthesis17634| |instance-of| |Enzyme-Synthesis|)
(|Protein-Enzyme17635| |instance-of| |Protein-Enzyme|)
(|Free-Energy| |subclasses| |Energy|)
(|Activation-Energy| |subclasses| |Energy|)
(|Free-Energy| |subclasses| |Energy|))
:CONTEXT NIL
:OUTPUT-PARAMETERS NIL)
A protein enzyme is synthesized in an active site of a ribosome of a cell using at least 3 amino acids and 2 monomers.
This process transforms activation energy and free-energy to another free-energy.
Enzyme synthesis ? a protein enzyme is synthesized in an active site of a ribosome of a cell using at least 3 amino acids
and 2 monomers. This process transforms activation energy and free-energy to another free-energy.
Synthesis of a protein enzyme in an active site of a ribosome of a cell using at least 3 amino acids and 2 monomers. This
process transforms activation energy and free-energy to another free-energy.
Figure 2: An example input bundle and the three outputs generated by our system for this input
S
NP ? S
RX[1]
agent
VP
V RX[2]
object
PP PP PP PP
P RX[3]
destination
P RX[4]
instrument
P RX[5]
origin
P RX[6]
path
S
S? S Punct
Punct N VP
this
process
V RX[7]
require
raw-material
RX
RX? P RX[8]
of
has-part
Figure 3: Tree selection
of event-to-entity relations and sentences would be
given based on encoding guidelines used to cre-
ate the knowledge base. However, the goal of
our project is to continuously expand the knowl-
edge base with more information, encoding new
types of events, and enriching existing events with
more detail as we go along (e.g., by specifying en-
ergy consumption and regulation mechanisms for
processes), therefore our encoding guidelines are
continuously revised. In order to produce output,
our realizer requires a generation lexicon, which
maps sets of relations onto elementary trees in the
grammar. Determining this mapping would re-
quire knowing the number of entities that can be
23
associated with each event type, and the relations
that can be used to express them. However, be-
cause our knowledge base is continuously chang-
ing, neither the number of entities linked to spe-
cific events, nor the types of relations used are
stable and therefore it was impossible to build
such a generation lexicon from the KB. Instead,
we adopted an approach where we detect ?event
frames? in the input of the system, and automat-
ically create entries for them in the generation
lexicon, guessing sentence structure and ordering
based on the event participants. An event frame
is a set of event-to-entity relations which have the
same event in the domain of the relations, and par-
ticipating entities in the range. We currently dis-
tinguish between two types of event frames, de-
pending on the type of the entities in the range of
relations: participant frames (ranges are of type
Tangible-Entity) and energy frames (ranges are
type Energy). An example of a participant frame
and an energy frame extracted from the input il-
lustrated in section 4.2 is illustrated below:
Participant frame:
(Uptake07 path Plasma-membrane78)
(Uptake07 origin Extracellular-Side52)
(Uptake07 destination Cytoplasm39)
(Uptake07 agent Cell-Surface-Receptor79)
(Uptake07 instrument Coated-Vesicle49)
(Uptake07 object Cholesterol08)
Energy frame:
(Uptake07 raw-material Chemical-Energy70)
(Uptake07 raw-material Free-Energy89)
Our input conversion module detects event
frames and automatically creates an entry in
GenI?s generation lexicon for each frame, an-
chored on a noun or verb associated with the event
in our concept-to-word mapping lexicon. The en-
tries link the sets of relations in the frame to a tree
with the same number of arguments, attempting
to place entities that play agent and object par-
ticipants into subject/object positions in the tree if
they exist. Our algorithm also attempts to deter-
mine the best syntactic construction for the spe-
cific combination of participant relations, and de-
cides between selecting an active sentential tree,
a passive sentential tree, a complex noun phrase,
or a combination of these. This process also in-
volves deciding based on the event participants
whether the tree will be anchored on a transitive
verb, an intransitive verb, or a verb with a prepo-
sitional object, and assigning default prepositions
to event participants (unless we have more detail
specified in the lexicon, as described in the next
section). The elementary trees in the grammar
are named after the number of referring expres-
sions and prepositional phrases in the tree, and we
use this naming convention to automatically gen-
erate tree names (or tree family names) for lexi-
cal entries, thereby linking trees in the grammar to
GenI?s generation lexicon. The two S-rooted trees
in Fig 3 were selected based on automatically gen-
erated lexical entries for the two frames above.
4.1 Realisation
The GenI surface realizer selects elementary TAG
trees for (sets of) relations in its input and com-
bines them using the standard operations of sub-
stitution and adjunction to produce a single de-
rived tree. We have developed a feature-based lex-
icalized Tree Adjoining Grammar to generate sen-
tences from relations in the KB. Our grammar has
two important properties, following the approach
in (Banik, 2010):
(1) our grammar includes discourse-level elemen-
tary trees for relations that are generated in sepa-
rate sentences, and
(2) instead of the standard treatment of entities as
nouns or NPs substituted into elementary trees,
our grammar treats entities as underspecified re-
ferring expressions, leaving the generation of noun
phrases to the next stage. The underspecified re-
ferring expressions replace elementary trees in the
grammar, which the generator would otherwise
have to combine with substitution. This under-
specification saves us computational complexity
in surface realisation, and at the same time allows
us to make decisions on word choice at a later
stage when we have more information on the syn-
tax of the sentence and discourse history.
The output of the realizer is an underspecified
text in the form of a sequence of lemma - feature
structure pairs. Lemmas here can be underspeci-
fied ? instead of an actual word, they can be an in-
dex or a sequence of indices pointing to concepts
in the KB. The syntax and sentence boundaries
are fully specified, and the output can be one or
more sentences long. The feature structures asso-
ciated with lemmas include all information neces-
sary for referring expression generation and mor-
phological realisation, which is performed in the
next phase. To give an example, the set of rela-
tions below would produce an output with 8 un-
derspecified referring expressions (shown as RX),
distributed over two sentences:
(Uptake07 path Plasma-membrane78)
(Uptake07 origin Extracellular-Side52)
24
(Uptake07 destination Cytoplasm39)
(Uptake07 agent Cell-Surface-Receptor79)
(Uptake07 instrument Coated-Vesicle49)
(Uptake07 object Cholesterol08)
(Uptake07 raw-material Chemical-Energy70)
(Uptake07 raw-material Free-Energy89)
NP(Uptake07) ? RX[1] absorb RX[2] to RX[3] of RX[8]
with RX[4] from RX[5] through RX[6]. This process re-
quires RX[7].
The elementary trees selected by the realizer for
this output, and the correspondences between re-
lations and referring expressions are illustrated in
Fig.3.
4.2 Referring Expression Generation
The final stage in the NLG pipeline is performing
morphological realisation and spelling out the re-
ferring expressions left underspecified by the real-
isation module. The input to referring expression
generation is a list of lemma - feature structure
pairs, where lemmas are words on leaf nodes in
the derived tree produced by syntactic realisation.
In our system, some of the lemmas can be unspec-
ified, i.e., there is no word associated with the leaf
node, only a feature structure. For these cases, we
perform lexicon lookup and referring expression
generation based on the feature structure, as well
as morphological realisation. To give an example,
the input illustrated in the previous section will be
generated as
?Uptake of cholesterol by human cell? a cell
surface receptor absorbs cholesterol to the cyto-
plasm of a human cell with a coated vesicle from
an extracellular side through a plasma membrane.
This process requires chemical energy and free-
energy.?
Many concept labels in our ontology are very
complex, often giving a description of the concept
or the corresponding biology terminology, and
therefore these labels can only be used for NLG
under specific circumstances. To overcome this
problem, we have created a lexicon that maps con-
cept names to words, and the grammar has control
over which form is used in a particular construc-
tion. Accordingly, we distinguish between two
types of underspecified nodes:
? NP nodes where the lexical item for the
node is derived by normalizing the concept
class associated with the node (Uptake-Of-
Cholesterol-By-Human-Cell ? ?uptake of
cholesterol by human cell?)
? RX (referring expression) nodes where lex-
ical items are obtained by looking up class
names in the concept-to-word mapping lexi-
con (Uptake-Of-Cholesterol-By-Human-Cell
? ?absorb?)
The feature structures on RX nodes in the out-
put of GenI describe properties of entities in the in-
put, which were associated with that specific node
during realisation. The feature structures specify
three kinds of information:
? the identifier (or a list of identifiers) for the
specific instances of entities the RX node
refers to
? the KB class for each entity
? any cardinality constraints that were asso-
ciated with each entity for the relation ex-
pressed by the tree in which the RX node ap-
pears
We define cardinality constraints as a triple (Do-
main, Slot, Constraint) where the Constraint itself
is another triple of the form (ConstraintExpres-
sion, Number, ConstraintClass). ConstraintEx-
pression is one of at least, at most, or exactly
and ConstraintClass is a KB class over which the
constraint holds. There is usually (but not neces-
sarily) one or more relations associated with ev-
ery cardinality constraint. We say a triple (Do-
main Slot Range) is associated with a cardinality
constraint (Domain, Slot, (ConstraintExpression,
Number, ConstraintClass)) if
? the Domain and Slot of the associated triple
is equal to the Domain and Slot of the cardi-
nality constraint and
? one of the following holds:
? either (Range instance-of Constraint-
Class) holds for the range of the triple
? or Range is taxonomically related to
ConstraintClass (via a chain of subclass
relations)
We define a referring expression language
(Fig. 4) which describes groups of instance names
(variables) that belong to the same KB class, and
the associated cardinality constraints. Groups
themselves can be embedded within a larger group
(an umbrella), resulting in a complex expression
which gives examples of a concept (e.g., ?three
atoms (a carbon and two oxygens)?). Expressions
25
<refex> = <umbrella> SPACE <refex> | <umbrella>
<umbrella> = <group> ( <refex> )| <group>
<group> = <class> <instances> <constraints>
<instances> = :: <instance> <instances> | <instance>
<constraints> = : <constraint> <constraints> | <constraint>
<constraint> = <op> : <num> | unk : <dash-delimited-string>
<op> = ge | le | eq
Figure 4: Syntax of the referring expression language
in this language are constructed from triples dur-
ing the input conversion stage, when we perform
semantic aggregation. The groups are then passed
through elementary trees by the realisation module
(GenI) to appear in the output as complex feature
structures on leaf nodes of the derived tree. The re-
ferring expression generation module parses these
complex feature values, and constructs (possibly
complex) noun phrases as appropriate.
To illustrate some examples, the following fea-
ture value shows a simple referring expression
group which encodes two entities (Monomer14
and Monomer7) and two cardinality constraints (at
least 2 and at most 5). This expression will be gen-
erated as ?between 2 and 5 monomers?:
Monomer::Monomer14::Monomer7:ge:2:le:5
We also allow more complex cardinality con-
straints which give the general type of an entity
and specify examples of the general type, as in ?at
least 3 organic molecules (2 ATPs and an ethyl al-
cohol)?:
Organic-Molecule:ge:3
(ATP:: ATP80938:eq:2
Ethyl-Alcohol:: Ethyl-Alcohol80922)
The referring expression generation module
makes three main decisions based on the refer-
ring expression, additional feature structures on
the node, and discourse history: it chooses lem-
mas, constructs discriminators, and decides be-
tween singular/plural form. The algorithm for dis-
criminator choice in the referring expression gen-
eration module is illustrated in Fig 5. Our refer-
ring expression generation module, including dis-
course history tracking and determiner choice, is
made available in the Antfarm3 open source tool.
5 Giving Domain Experts Control over
Sentence Structure
By automatically associating event frames with el-
ementary trees we are able to generate a sentence
for all combinations of event-to-entity relations
3https://github.com/kowey/antfarm
Figure 6: Parameters in the concept-to-word map-
ping lexicon
without having to maintain the grammar and gen-
eration lexicon of the realizer as the knowledge
base evolves. However sentences generated this
way are not always well-formed. Events in the
KB can be realized with a wide range of verbs and
nouns, which require different prepositions or syn-
tactic constructions, and different types of events
may require different participants to be their gram-
matical subject or object. To give an example, for
events that have an agent, in the majority of the
cases we get a grammatical sentence if we place
the agent in subject position. If the frame lacks
an agent but has an object, we can usually gener-
ate a grammatical passive sentence, with the ob-
ject participant as the subject. However, it is often
the case that events do not have an agent, and we
get a grammatical (active) sentence by placing an-
other relation in the subject position e.g., base for
the event Store or instrument for Block. Which
26
for each group in the referring expression do
if all members of the group are first mentions and there are no distractors in the history: then
if the group has cardinality constraints: then
upper bound M ? at most M
lower bound N ? at least N (multiple group members in this case are also interpreted as lower bound)
both bounds? between N and M or exactly N
else
one group member? generate an indefinite determiner (a/an)
more than one member? generate a cardinal
end if
end if
if the group is a first mention but there are distractors in the discourse history then
if the group has only one member then
if the group exactly matches one previous mention? another
if the group exactly matches N > 1 previous mentions? the Nth
if there is a 2-member group in the history, and one of the members was mentioned by itself? the other
if the discourse history has more than one distractor? a(n) Nth
end if
if there are multiple group members then
if the group is a subset of a previously mentioned group which has no distractors? N of the
end if
end if
if the group is not a first mention then
if the group has upper and/or lower bounds? the same
if the group has one member only? the
if the group has multiple members? the N
end if
end for
Figure 5: Algorithm for discriminator choice in our referring expression module
event participant can appear in subject and ob-
ject positions depends not only on the type of the
event, but also on the encoding guidelines which
are continuously evolving.
In order to improve the quality of the gener-
ated output, and to give domain experts control
over customizing the system without having to un-
derstand details of the grammar, we extended the
concept-to-word mapping lexicon with parameters
which control preposition choice, and allow cus-
tomization of the position of participating entities.
We developed a graphical user interface which al-
lows encoders (biology domain experts) to add
and edit these lexical parameters as they encode
concepts in the KB.
To give an example, in the absence of a lexical
item and any parameters for the event Glycogen-
Storage, our system would produce the following
default output, attempting to use the concept label
as the main verb of the sentence in an automati-
cally produced generation lexicon entry:
?Glycogen storage ? glycogen is glycogened
storage in a vertebrate in a liver cell and a muscle
cell.?
In order to improve the quality of the output, one
of our biology teachers has customized the param-
eters in the lexicon to yield:
?Glycogen storage ? glycogen is stored by a
Figure 7: Concept map for the event ?Reduction?
vertebrate within a liver cell and a muscle cell.?
This was achieved through a graphical user inter-
face which is part of the tool used for knowledge
encoding, and is illustrated in Fig 6. Our sys-
tem allows encoders to re-generate sentences af-
ter editing the parameters to see the effect of the
changes on the output. The top half of the win-
dow in in Fig 6 allows encoders to associate words
or phrases with concepts, where they can add as
many synonyms as they see fit. One of the syn-
onyms has to be marked as the primary form, to
be used for generation by default.4 For events,
4The concept-to-word mapping lexicon is shared between
the question interpretation and the NLG module, and the ad-
ditional synonyms are currently only used for mapping ques-
27
(a) ?Plastocyanin reduces P700+? (b) ?P700+ receives an electron from plastocyanin.?
Figure 8: Concept-to-word mapping parameters for the two synonyms of Reduction
the primary form is a verb and its nominalization,
and for entities it is a noun. The bottom half of
the window shows the parameter settings for each
synonym associated with the concept. Here the
encoders can specify relations which link the sub-
ject and object of a verb to the event (grammatical
subject/object), and assign prepositions to other
event-to-entity relations for the verb, when it is
used to realize the specified event. There is also
an option to tell the NLG system to ignore some
of the event participants when using a specific verb
for the event. This functionality is used for verbs
that already imply one of the participants. For ex-
ample, the word polymerization already implies
that the result of the event is a polymer. In these
cases there is no need for the NLG system to gen-
erate the implied participant (here, result). An-
other example is the verb reduce, which implies
that the object of the event is an electron. The ed-
itor allows the users to enter different parameter
values for the synonyms of the same event. For
example, the graph in Fig 7 could be described in
at least three different ways:
1. P700+ is reduced by plastocyanin
2. Plastocyanin reduces P700+
3. P700+ receives an electron from plastocyanin.
Here sentences 1 and 2 make no mention of the
electron involved in the process, but sentence 3 ex-
plicitly includes it. In order for the system to cor-
rectly generate sentences 1 and 2, the concept-to-
word mapping parameters for ?reduce? (as a syn-
onym for Reduction) have to include an implied
participant. Otherwise the system will assume that
all participants should be mentioned in the sen-
tence, and it will generate ?P700+ is reduced by
a plastocyanin of an electron?. Fig 8. illustrates
the different concept-to-word mapping parameters
needed for the two synonyms for Reduction in or-
der to generate the above sentences correctly.
tions onto concepts in the KB.
6 Conclusions
We have presented an NLG system which gen-
erates complex sentences from a biology KB.
Our system includes a content selection module,
which tailors the selected relations to the context
in which the output is displayed, and allows the
presentation module to send parameters to influ-
ence properties of generated outputs. We have de-
veloped a referring expression generation module
which generates complex noun phrases from ag-
gregated cardinality constraints and entities in the
input, and keeps track of discourse history to dis-
tinguish mentions of different groups of concepts.
Our system allows biology teachers to detect in-
consistencies and incompleteness in the KB, such
as missing cardinality constraints, errors where
two instances of the concept were added unnec-
essarily (unification errors on entities), and miss-
ing or incorrect relations. To make the system
robust, we have developed an algorithm to pro-
duce sentences and complex noun phrases for un-
seen combinations of event-to-entity relations in
the KB by automatically generating entries in the
lexicon of the GenI surface realizer. Our algorithm
makes default decisions on sentence structure and
ordering based on relations sent to the NLG sys-
tem, expressing the event?s participants. To allow
domain experts to easily improve the default out-
puts generated by our algorithm, we have defined
a framework for adding lexical parameters to con-
cepts, which allow non-NLG-experts to customize
the structure of generated sentences for events in
the KB as they are encoded. Although our system
currently only produces one or two possibly com-
plex sentences, it was designed to ultimately gen-
erate paragraph-length texts. This can be achieved
simply by adding more discourse-level elementary
trees to the grammar of the realizer, since our sys-
tem is already able to handle referring expressions
across sentence boundaries.
28
References
E. Banik. 2010. A Minimalist Architecture for Gener-
ating Coherent Text. Ph.D. thesis, The Open Univer-
sity, UK.
K. Barker, B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases. In
Proceedings K-CAP 2001, pages 14?21.
K. Bontcheva and Y. Wilks. 2004. Automatic report
generation from ontologies: the MIAKT approach.
In 9th Int. Conf. on Applications of Natural Lan-
guage to Information Systems, page 324335, Manch-
ester, UK.
K. Bontcheva. 2004. Open-source tools for creation,
maintenance, and storage of lexical resources for
language generation from ontologies. In 4th Conf.
on Language Resources and Evaluation, Lisbon,
Portugal.
D. Galanis and I. Androutsopoulos. 2007. Generat-
ing multilingual descriptions from linguistically an-
notated owl ontologies: the NaturalOWL system. In
INLG07, Schloss Dagstuhl, Germany, page 143146.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date - progress toward digital aristotle. AI Magazine,
Fall:33?58.
A. K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. In Grzegorz Rosenberg and Arto Sa-
lomaa, editors, Handbook of Formal Languages
and Automata, volume 3, pages 69?124. Springer-
Verlag, Heidelberg.
E. Kow. 2007. Surface realisation: ambiguity and
determinism. Ph.D. thesis, Universite de Henri
Poincare, Nancy.
C. Mellish and X. Sun. 2005. The semantic web as
a linguistic resource: Opportunities for natural lan-
guage generation. In Knowledge-Based Systems.
C.L. Paris. 1988. Tailoring object descriptions to the
users level of expertise. Computational Linguistics,
14(3):6478. Special Issue on User Modelling.
E. Reiter, R. Robertson, and L. M. Osman. 2003.
Lessons from a failure: generating tailored smok-
ing cessation letters. Artificial Intelligence, 144(1-
2):41?58.
A. Spaulding, A. Overholtzer, J. Pacheco, J. Tien, V. K.
Chaudhri, D. Gunning, and P. Clark. 2011. Inquire
for ipad: Bringing question-answering ai into the
classroom. In International Conference on AI in Ed-
ucation (AIED).
G. Wilcock. 2003. Talking owls: Towards an ontology
verbalizer. In Human Lan- guage Technology for
the Semantic Web and Web Services, ISWC03, page
109112, Sanibel Island, Florida.
29
Proceedings of the 14th European Workshop on Natural Language Generation, pages 94?97,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The KBGen Challenge
Eva Banik
Computational
Linguistics Ltd
London, UK
ebanik@comp-ling.com
Claire Gardent
CNRS, LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow?
Computational
Linguistics Ltd
London, UK
kowey@comp-ling.com
1 Introduction
The KBGen 2013 natural language generation
challenge1 was intended to survey and compare
the performance of various systems which perform
tasks in the content realization stage of generation
(Banik et al, 2012). Given a set of relations which
form a coherent unit, the task is to generate com-
plex sentences which are grammatical and fluent
in English. The relations for this year?s challenge
were selected from the AURA knowledge base
(KB) (Gunning et al, 2010). In this paper we give
an overview of the KB, describe our methodology
for selecting sets of relations from the KB to pro-
vide input-output pairs for the challenge, and give
details of the development and test data set that
was provided to participating teams. Three teams
have submitted system outputs for this year?s chal-
lenge. In this paper we show BLEU and NIST
scores for outputs generated by the teams. The full
results of our evaluation, including human judge-
ments, as well as the development and test data set
are available at http://www.kbgen.org.
2 The AURA Knowledge Base
The AURA knowledge base (Gunning et al,
2010) encodes information from a biology text-
book (Reece et al, 2010). It was developed to
support a question answering system, to help stu-
dents understand biological concepts by allowing
them to ask questions about the material while
reading the textbook. AURA is a frame-based
KB which encodes events, the entities that partic-
ipate in events, properties, and roles that the en-
tities play in an event. The relations in the KB
include relations between these types, including
event-to-entity, event-to-event, event-to-property,
entity-to-property. The KB is built on top of the
?The work reported in this paper was supported by fund-
ing from Vulcan, Inc.
1http://www.kbgen.org
CLIB generic library of concepts (Barker et al,
2001). As part of the encoding process, concepts
in CLIB are specialized and/or combined to en-
code biology-specific information. AURA is or-
ganized into a set of concept maps, where each
concept map corresponds to a biological entity or
process. The KB was encoded by biology teach-
ers and contains around 5,000 concept maps. It is
available for download for academic purposes in
various formats including OWL2.
3 The Content Selection Process for
KBGen 2012
The input provided to the participants consisted
of a set of content units extracted from the KB,
and a sentence corresponding to each content unit.
The content units were semi-automatically se-
lected from AURA such that:
? the set of relations in each content unit
formed a connected graph
? each content unit can be verbalised by a
single, possibly complex sentence which is
grammatical and meaningful
? the set of content units contain as many dif-
ferent relations and concepts of different se-
mantic types (events, entities, properties, etc)
as possible.
To produce these inputs we first asked biology
teachers to provide coherent content units using
the AURA graphical interface. The basic assump-
tion behind this approach was that, since every
content unit can be expressed by a coherent sen-
tence, each set of relations will exhibit a ?coher-
ence pattern?. We then created a search space of
candidate content units by extracting patterns from
the KB which were similar to the patterns given
by the biologists. Finally, we manually selected
coherent content units.
2http://www.ai.sri.com/halo/
halobook2010/exported-kb/biokb.html
94
Figure 1: ? A hydrophobic compound attaches to a
carrier protein at a region called the binding site.?
3.1 Manual Selection of Content Units
In the first step of our process, biology teachers
manually selected parts of concept maps which
represented educationally useful information for
biology students by searching for specific con-
cepts in AURA?s graph-based user interface. For
each content unit they wrote a sentence verbalis-
ing the selected relations (Fig. 1). The biology
teachers who identified these coherent, sentence-
sized chunks of information were familiar with the
encoding practices in AURA, the underlying biol-
ogy textbook, and had experience with the Inquire
e-book application (Spaulding et al, 2011) which
displays educationally useful content from the KB.
3.2 From Graphs to Queries
In the second step, the graphical representations
produced by the biologists were manually trans-
lated to specific knowledge base queries which
were run in AURA to retrieve the instances sat-
isfying the queries. Queries consist of two parts:
a set of triples whose domain and range are vari-
ables, and a set of instance-of triples stating type
constraints on the variables. The graph shown in
Figure 1 was translated to the following query:
Type constraints:
(?CP instance-of Carrier-Protein)
(?A instance-of Attach)
(?BS instance-of Binding-Site)
(?HP instance-of Hydrophilic-Compound)
Relation constraints:
(?A object ?HP)
(?A base ?CP)
(?A site ?BS)
(?CP has-region ?BS)
3.3 From Queries to Generalized Query
Patterns
After checking that it returns an answer, each
query was generalized to a query pattern in or-
der to find other queries which involved different
concepts and relations, but still exhibited the same
general coherence pattern. To derive generalized
query patterns, specific queries were modified in
two ways: 1) by removing type constraints on con-
cepts, and 2) by replacing specific relations with
generalized relation types.
Removing type constraints
Manually specified queries were extended by re-
moving type constraints on variables. In the above
example, types were generalised to Event or En-
tity:
(?CP instance-of Entity)
(?A instance-of Event)
(?BS instance-of Entity)
(?HP instance-of Entity)
Other generalized types we used from the ontol-
ogy were Property-Values and Roles.
Generalizing relations
Each query was generalized by defining equiva-
lence classes over semantically similar relations
and replacing the specific relation in the query
with its equivalence class. The basic assumption
behind this was that if a set of relations is coherent,
we should be able to replace a relation with an-
other, semantically similar relation in the set, and
still have a coherent content unit. For example,
whether two entities are connected by has-part
or has-region is unlikely to make a difference
to the coherence of a content unit.
Following this approach we identified groups of
semantically similar relations within each relation
type (Event-to-Event, Event-to-Entity, etc). The
equivalence classes over relations were straight-
forwardly derived from distinctions made in CLIB
(Barker et al, 2001), the upper ontology and li-
brary of general concepts that AURA is built on,
although there was some manual fine-tuning re-
quired to exclude relations which were not re-
liably encoded in the KB. For example, we di-
vided Entity-to-Entity relations into three cate-
gories, based on whether they had a spatial or
meronymic sense, or expressed a specific relation
between two chemicals:
en2en-spatial: abuts is-above is-along is-at is-
inside is-opposite is-outside is-over location
95
is-across is-on is-parallel-to is-perpendicular-
to is-under is-between is-facing is-below is-
beside is-near
en2en-part: possesses has-part has-region
encloses has-basic-structural-unit has-
structural-part has-functional-part
en2en-chemical: has-solute has-solvent has-
atom has-ion has-oxidized-form has-
reduced-form has-isomer
Here the distinction between spatial relations
and meronymic relations was given by CLIB. Re-
lations in the third group were specific to our do-
main and added during the process of encoding.
Event-to-entity relations were divided into
?aux-participant? relations, which express the spa-
tial orientation of an event, and ?core-participant?
relations which describe ways in which entities
participate in the event. Here we used the cat-
egories of spatial relations and ?participant? re-
lations from CLIB. Our terminology reflects the
fact that entities connected to an event by a
core-participant relation are typically expressed as
obligatory arguments of the verb in a sentence,
whereas aux-participants would be expressed as
optional modifiers:
core-participants: agent object donor base in-
strument raw-material recipient result
aux-participants: away-from destination origin
path site toward
With these definitions, the specific query illus-
trated above in section 3.2 was translated to the
following query pattern:
(?A core-participant ?X)
(?A core-participant ?CP)
(?A aux-participant ?BS)
(?CP en2en-part ?BS)
3.4 From Query Results to Content Units
Query patterns were expanded by producing all
valid instantiations of the pattern in order to cre-
ate a search space of candidate content units, and
we ran each expanded query in AURA. The last
step was filtering the results returned by satisfi-
able queries to obtain content units which can be
verbalised in a single sentence. We used the fol-
lowing selection criteria to do this:
? A meaningful and grammatical sentence
could be formed by verbalising all concepts,
relations and properties present in the query
result.
(KBGEN-INPUT :ID "ex03c.99-1"
:TRIPLES (
(|Secretion21994| |object| |Mucus21965|)
(|Secretion21994| |base| |Earthworm21974|)
(|Secretion21994| |site| |Alimentary-Canal21978|)
(|Earthworm21974| |has-region|
|Alimentary-Canal21978|))
:INSTANCE-TYPES (
(|Mucus21965| |instance-of| |Mucus|)
(|Secretion21994| |instance-of| |Secretion|)
(|Earthworm21974| |instance-of| |Earthworm|)
(|Alimentary-Canal21978| |instance-of|
|Alimentary-Canal|))
:ROOT-TYPES (
(|Secretion21994| |instance-of| |Event|)
(|Mucus21965| |instance-of| |Entity|)
(|Earthworm21974| |instance-of| |Entity|)
(|Alimentary-Canal21978| |instance-of| |Entity|)
))
Figure 2: Input for the sentence ?Mucus is se-
creted in the alimentary canal of earthworms.?
? The set of content units should be as varied
as possible. In particular, we did not keep
a content unit if another very similar content
unit was present in the selected units. For in-
stance, if two content units contain identical
relations (modulo concept labels), only one
of these two units would be kept.
Given the pattern shown in Fig. 1 for instance,
we obtained 27 coherent content units. Each con-
tent unit was verbalized as a sentence to provide
development data for the content realization chal-
lenge. The following sentences illustrate the vari-
ation in the resulting content units:
- Polymers are digested in the lysosomes of eu-
karyotic cells.
- Mucus is secreted in the alimentary canal of
earthworms.
- Lysosomal enzymes digest proteins and poly-
mers at the lysosome of a eukaryotic cell.
- A chemical is attached to the active site of a
protein enzyme with an ionic bond.
- An enzyme substrate complex is formed
when a chemical attaches to the active site of
a protein enzyme with a hydrogen bond.
- Starch is stored in the lateral root of carrots.
4 Development Data Set
The development data set provided to participants
contained 207 input-output pairs. These inputs
96
were based on 19 different coherence patterns.
Fig. 2 shows an input-output pair based on the
pattern illustrated above. We also provided two
lexicons: a lexicon for events which gave a map-
ping from events to verbs, their inflected forms and
nominalizations and a lexicon for entities, which
provided a noun and its plural form. The rele-
vant entries in these lexicons for the input in Fig. 2
were:
Secretion,secretes,secrete,secreted,secretion
Mucus, mucus, mucus
Earthworm,earthworm,earthworms
Alimentary-Canal,alimentary canal,alimentary canals
5 Test Set
Our test data set contained 72 inputs in the same
format (and corresponding lexical resources as
above), which were divided into three categories:
(1) seen patterns, seen relations: inputs that have
exactly the same relations as some of the inputs in
the development data set, but different concepts
(2) seen patterns, unseen relations: these in-
puts are derived from patterns in the development
data set. They have similar structure, but contain
slightly different combinations of relations.
(3) unseen patterns: inputs extracted from a pre-
viously unused pattern, containing combinations
of relations not seen in the development data set.
6 Evaluation
Participants submitted two sets of outputs:
(1) outputs generated by their system as is (mod-
ulo including the lexicon provided in the test data
set) (2) outputs generated 6 days later, during
which time teams had a chance to make improve-
ments.
Each team was allowed to submit a set of 5 ranked
outputs for each input. We have evaluated all
of the submitted outputs using BLEU and NIST
scores and we are currently in the process of col-
lecting human judgements for the final system out-
puts that were ranked first. Table 1 shows the
overall results of automatic evaluation on both the
initial and final data sets for our three teams3, as
well as the coverage of the individual systems over
the 72 test inputs. More detail including the full
results of our evaluation can be found at http:
//www.kbgen.org, along with a link to download
3IMS: Stuttgart University Institute for Computational
Language Processing, LOR: LORIA, University of Nancy,
UDEL: University of Delaware, Computer and Information
Science Department
NIST BLEU coverage
HUMAN-1 10.0098 1.0000 100%
UDEL-final-1 5.9749 0.3577 97%
UDEL-initial-1 5.6030 0.3165 100%
LOR-final-1 4.8569 0.3053 84%
LOR-final-3 4.7238 0.2993 100%
LOR-final-2 4.6711 0.2945 100%
LOR-final-5 4.5720 0.2812 100%
LOR-final-4 4.4889 0.2781 100%
IMS-final-2 3.9649 0.1107 100%
IMS-final-4 3.8813 0.1140 100%
IMS-final-1 3.8670 0.1111 100%
IMS-final-3 3.7765 0.1023 100%
IMS-initial-2 3.6726 0.1117 100%
IMS-initial-3 3.6608 0.1181 100%
IMS-initial-1 3.6384 0.1173 100%
IMS-initial-4 3.5817 0.1075 100%
LOR-initial-1 0.1206 0.0822 30%
LOR-initial-3 0.1091 0.0751 100%
LOR-initial-4 0.0971 0.0732 100%
LOR-initial-2 0.0948 0.0757 100%
LOR-initial-5 0.0881 0.0714 100%
Table 1: BLEU and NIST scores of initial and final
system outputs. The digit behind the team names
refer to the output rank
the development and test data set used in the chal-
lenge, and more information about AURA and re-
lated resources.
References
E. Banik, C. Gardent, D. Scott, N. Dinesh, and
F. Liang. 2012. Kbgen text generation from knowl-
edge bases as a new shared task. In INLG 2012,
Starved Rock State Park, Illinois,USA.
K. Barker, B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases. In
Proceedings K-CAP 2001, pages 14?21.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date - progress toward digital aristotle. AIMagazine,
Fall:33?58.
Jane B. Reece, Lisa A. Urry, Michael L. Cain,
Steven A. Wasserman, Peter V. Minorsky, and
Robert B. Jackson. 2010. Campbell Biology. Pear-
son Publishing.
A. Spaulding, A. Overholtzer, J. Pacheco, J. Tien, V. K.
Chaudhri, D. Gunning, and P. Clark. 2011. Inquire
for iPad: Bringing question-answering AI into the
classroom. In International Conference on AI in Ed-
ucation (AIED).
97
