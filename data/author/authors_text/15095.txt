Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602?1612,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Multi-Relational Latent Semantic Analysis
Kai-Wei Chang?
University of Illinois
Urbana, IL 61801, USA
kchang10@illinois.edu
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Abstract
We present Multi-Relational Latent Seman-
tic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA). MRLSA pro-
vides an elegant approach to combining mul-
tiple relations between words by construct-
ing a 3-way tensor. Similar to LSA, a low-
rank approximation of the tensor is derived
using a tensor decomposition. Each word in
the vocabulary is thus represented by a vec-
tor in the latent semantic space and each re-
lation is captured by a latent square matrix.
The degree of two words having a specific
relation can then be measured through sim-
ple linear algebraic operations. We demon-
strate that by integrating multiple relations
from both homogeneous and heterogeneous
information sources, MRLSA achieves state-
of-the-art performance on existing benchmark
datasets for two relations, antonymy and is-a.
1 Introduction
Continuous semantic space representations have
proven successful in a wide variety of NLP and IR
applications, such as document clustering (Xu et al,
2003) and cross-lingual document retrieval (Dumais
et al, 1997; Platt et al, 2010) at the document level
and sentential semantics (Guo and Diab, 2012; Guo
and Diab, 2013) and syntactic parsing (Socher et
al., 2013) at the sentence level. Such representa-
tions also play an important role in applications for
lexical semantics, such as word sense disambigua-
tion (Boyd-Graber et al, 2007), measuring word
?Work conducted while interning at Microsoft Research.
similarity (Deerwester et al, 1990) and relational
similarity (Turney, 2006; Zhila et al, 2013; Mikolov
et al, 2013). In many of these applications, La-
tent Semantic Analysis (LSA) (Deerwester et al,
1990) has been widely used, serving as a fundamen-
tal component or as a strong baseline.
LSA operates by mapping text objects, typically
documents and words, to a latent semantic space.
The proximity of the vectors in this space implies
that the original text objects are semantically re-
lated. However, one well-known limitation of LSA
is that it is unable to differentiate fine-grained re-
lations. For instance, when applied to lexical se-
mantics, synonyms and antonyms may both be as-
signed high similarity scores (Landauer and Laham,
1998; Landauer, 2002). Asymmetric relations like
hyponyms and hypernyms also cannot be differenti-
ated. Although there exists some recent work, such
as PILSA which tries to overcome this weakness
of LSA by introducing the notion of polarity (Yih
et al, 2012). This extension, however, can only
handle two opposing relations (e.g., synonyms and
antonyms), leaving open the challenge of encoding
multiple relations.
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA), which strictly gener-
alizes LSA to incorporate information of multiple
relations concurrently. Similar to LSA or PILSA
when applied to lexical semantics, each word is still
mapped to a vector in the latent space. However,
when measuring whether two words have a specific
relation (e.g., antonymy or is-a), the word vectors
will be mapped to a new space according to the rela-
tion where the degree of having this relation will be
1602
judged by cosine similarity. The raw data construc-
tion in MRLSA is straightforward and similar to the
document-term matrix in LSA. However, instead of
using one matrix to capture all relations, we extend
the representation to a 3-way tensor. Each slice cor-
responds to the document-term matrix in the original
LSA design but for a specific relation. Analogous to
LSA, the whole linear transformation mapping is de-
rived through tensor decomposition, which provides
a low-rank approximation of the original tensor. As
a result, previously unseen relations between two
words can be discovered, and the information en-
coded in other relations can influence the construc-
tion of the latent representations, and thus poten-
tially improves the overall quality. In addition, the
information in different slices can come from het-
erogeneous sources (conceptually similar to (Riedel
et al, 2013)), which not only improves the model,
but also extends the word coverage in a reliable way.
We provide empirical evidence that MRLSA is ef-
fective using two different word relations: antonymy
and is-a. We use the benchmark GRE test of closest-
opposites (Mohammad et al, 2008) to show that
MRLSA performs comparably to PILSA, which was
the pervious state-of-the-art approach on this prob-
lem, when given the same amount of information. In
addition, when other words and relations are avail-
able, potentially from additional resources, MRLSA
is able to outperform previous methods significantly.
We use the is-a relation to demonstrate that MRLSA
is capable of handling asymmetric relations. We
take the list of word pairs from the Class-Inclusion
(i.e., is-a) relations in SemEval-2012 Task 2 (Jur-
gens et al, 2012), and use our model to measure the
degree of two words have this relation. The mea-
sures derived from our model correlate with human
judgement better than the best system that partici-
pated in the task.
The rest of this paper is organized as follows. We
first survey some related work in Section 2, followed
by a more detailed description of LSA and PILSA
in Section 3. Our proposed model, MRLSA, is pre-
sented in Section 4. Section 5 presents our experi-
mental results. Finally, Section 6 concludes the pa-
per.
2 Related Work
MRLSA can be viewed as a model that derives gen-
eral continuous space representations for capturing
lexical semantics, with the help of tensor decompo-
sition techniques. We highlight some recent work
related to our approach.
The most commonly used continuous space rep-
resentation of text is arguably the vector space
model (VSM) (Turney and Pantel, 2010). In this
representation, each text object can be represented
by a high-dimensional sparse vector, such as a
term-vector or a document-vector that denotes the
statistics of term occurrences (Salton et al, 1975)
in a large corpus. The text can also be repre-
sented by a low-dimensional dense vector derived
by linear projection models like latent semantic
analysis (LSA) (Deerwester et al, 1990), by dis-
criminative learning methods like Siamese neural
networks (Yih et al, 2011), recurrent neural net-
works (Mikolov et al, 2013) and recursive neu-
ral networks (Socher et al, 2011), or by graphical
models such as probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) and latent Dirichlet
allocation (LDA) (Blei et al, 2003). As a general-
ization of LSA, MRLSA is also a linear projection
model. However, while the words are represented
by vectors as well, multiple relations between words
are captured separately by matrices.
In the context of lexical semantics, VSMs provide
a natural way of measuring semantic word related-
ness by computing the distance between the cor-
responding vectors, which has been a standard ap-
proach (Agirre et al, 2009; Reisinger and Mooney,
2010; Yih and Qazvinian, 2012). These approaches
do not apply directly to the problem of modeling
other types of relations. Existing methods that do
handle multiple relations often use a model com-
bination scheme to integrate signals from various
types of information sources. For instance, mor-
phological variations discovered from the Google
n-gram corpus have been combined with informa-
tion from thesauri and vector-based word related-
ness models for detecting antonyms (Mohammad et
al., 2008). An alternative approach proposed by Tur-
ney (2008) that handles synonyms, antonyms and
associations is to use a uniform approach by first
reducing the problem to determining whether two
1603
pairs of words can be analogous, and then predicting
it using a supervised model with features based on
the frequencies of patterns in the corpus. Similarly,
to measure whether two word pairs have the same
relation, Zhila et al (2013) proposed to combine het-
erogeneous models, which achieved state-of-the-art
performance. In comparison, MRLSA models mul-
tiple lexical relations holistically. The degree that
two words having a particular relation is estimated
using the same linear function of the corresponding
vectors and matrix.
Tensor decomposition generalizes matrix factor-
ization and has been applied to several NLP applica-
tions recently. For example, Cohen et al (2013) pro-
posed an approximation algorithm for PCFG pars-
ing that relies on Kruskal decomposition. Van de
Cruys et al (2013) modeled the composition of
subject-verb-object triples using Tucker decompo-
sition, which results in a better similarity measure
for transitive phrases. Similar to this construction
but used in the community-based question answer-
ing (CQA) scenario, Qiu et al (2013) represented
triples of question title, question content and answer
as a tensor and applied 3-mode SVD to derive latent
semantic representations for question matching. The
construction of MRLSA bears some resemblance to
the work that use tensors to capture triples. How-
ever, our goal of modeling different relations for lex-
ical semantics is very different from the intended us-
age of tensor decomposition in the existing work.
3 Latent Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used continuous vector space
model that maps words and documents into a low
dimensional space. LSA consists of two main steps.
First, taking a collection of d documents that con-
tains words from a vocabulary list of size n, it first
constructs a d ? n document-term matrix W to en-
code the occurrence information of a word in a docu-
ment. For instance, in its simplest form, the element
Wi,j can be the term frequency of the j-th word in
the i-th document. In practice, a weighting scheme
that better captures the importance of a word in the
document, such as TF?IDF (Salton et al, 1975),
is often used instead. Notice that ?document? here
simply means a group of words and has been applied
W V X = U T
Figure 1: SVD applied to a d?n document-term ma-
trix W. The rank-k approximation, X, is the mul-
tiplication of U, ? and VT , where U and V are
d ? k and n ? k orthonormal matrices and ? is a
k ? k diagonal matrix. The column vectors of VT
multiplied by the singular values ? represent words
in the latent semantic space.
to various texts including news articles, sentences
and bags of words. Once the matrix is constructed,
the second step is to apply singular value decom-
position (SVD) to W in order to derive a low-rank
approximation. To have a rank-k approximation, X
is the reconstruction matrix of W, defined as
W ? X = U?VT (1)
where the dimensions of U and V are d? k and
n? k, respectively, and ? is a k ? k diagonal ma-
trix. In addition, the columns in U and V are or-
thonormal and the elements in ? are the singular
values and are conventionally reverse-ordered. Fig-
ure 1 illustrates this decomposition.
LSA can be used to compute the similarity be-
tween two documents or two words in the latent
space. For instance, to compare the u-th and v-th
words in the vocabulary, one can compute the co-
sine similarity of the u-th and v-th column vectors
of X, the reconstruction matrix of W. In contrast to
a direct lexical matching via the columns of W, the
similarity measure computed as a result of the SVD
may have a nonzero similarity score even if these
two words do not co-occur in any documents. This
is due to the fact that those words can share some
latent components.
An alternative view of using LSA is to treat the
column vectors of ?VT as a representation of the
words in a new k-dimensional latent space. This
comes from the observation that the inner product
of every two column vectors in X is the inner prod-
uct of the corresponding column vectors of ?VT ,
1604
joyfulness
gladden
sad
1
anger
1
-1
0
1
1
0
0
-1
0
1
0
0
-1
1
0
0
0
0
1
0
0
0
0
0
0
0
0
Figure 2: The matrix construction of PILSA. The
vocabulary is {joy, gladden, sorrow, sadden, anger,
emotion, feeling} and target words are {joyfulness,
gladden, sad, anger}. For ease of presentation,
we show the numbers with 0-1 values instead of
TF?IDF scores. The polarity (i.e., sign) indicates
whether the term in the vocabulary is a synonym or
antonym of the target word.
which can be derived from the equations below.
XTX = (U?VT )T (U?VT )
= V?UTU?VT (? is diagonal)
= V?2VT (Columns of U are orthonormal)
= (?VT )T (?VT ) (2)
Thus, the semantic relatedness between the i-th and
j-th words can be computed by cosine similarity1:
cos(X:,i,X:,j) (3)
When used to compare words, one well-known
limitation of LSA is that the score captures the gen-
eral notion of semantic similarity, and is unable
to distinguish fine-grained word relations, such as
antonyms (Landauer and Laham, 1998; Landauer,
2002). This is due to the fact that the raw matrix rep-
resentation only records the occurrences of words in
documents without knowing the specific relation be-
tween the word and document. To address this issue,
Yih et al (2012) proposed a polarity inducing latent
semantic analysis model recently, which we intro-
duce next.
1Cosine similarity is equivalent to the inner product of the
normalized vectors.
3.1 Polarity Inducing Latent Semantic
Analysis
In order to distinguish antonyms from synonyms,
the polarity inducing LSA (PILSA) model (Yih et
al., 2012) takes a thesaurus as input. Synonyms and
antonyms of the same target word are grouped to-
gether as a ?document? and a document-term matrix
is constructed accordingly as done in LSA. Because
each word in a group belongs to either one of the two
opposite relations, synonymy and antonymy, the po-
larity information is induced by flipping the signs of
antonyms. While the absolute value of each element
in the matrix is still the same TF?IDF score, the
elements that correspond to the antonyms become
negative.
This design has an intriguing effect. When com-
paring two words using the cosine similarity (or sim-
ply inner product) of their corresponding column
vectors in the matrix, the score of a synonym pair
remains positive, but the score of an antonym pair
becomes negative. Figure 2 illustrates this design
using a simplified matrix as example.
Once the matrix is constructed, PILSA applies
SVD as done in LSA, which generalizes the model
to go beyond lexical matching. The sign of the co-
sine score of the column vectors of any two words
indicates whether they are close to synonyms or to
antonyms and the absolute value reflects the degree
of the relation. When all the column vectors are nor-
malized to unit vectors, it can also be viewed as syn-
onyms are clustered together and antonyms lie on
the opposite sides of a unit sphere. Although PILSA
successfully extends LSA to handle not just one sin-
gle occurrence relation, the extension is limited to
encoding two opposing relations
4 Multi-Relational Latent Semantic
Analysis
The fundamental reason why it is difficult to handle
multiple relations is due to the 2-dimensional ma-
trix representation. In order to overcome this, we
encode the raw data in a 3-way tensor. Each slice
captures a particular relation and is in the format of
the document-term matrix in LSA. Just as in LSA,
where the low-rank approximation by SVD helps
generalize the representation and discover unseen
relations, we apply a tensor decomposition method,
1605
joyfulness
gladden
sad
1
anger
1
0
0
1
1
0
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
(a) Synonym layer
joyfulness
gladden
sad
0
anger
0
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(b) Antonym layer
joyfulness
gladden
sad
0
anger
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
1
0
1
1
(c) Hypernym layer
Figure 3: The three slices of MRLSA raw tensorW for an example with vocabulary {joy, gladden, sorrow,
sadden, anger, emotion, feeling} and target words {joyfulness, gladden, sad, anger}. Figures 3(a), 3(b), 3(c)
show the matrices W:,:,syn, W:,:,ant, W:,:,hyper, respectively. Rows represent documents (see definition in
text), and columns represent words. For ease of presentation, we show numbers with 0-1 values instead of
TF?IDF scores.
the Tucker decomposition, to the tensor.
4.1 Representing Multi-Relational Data in
Tensors
A tensor is simply a multi-dimensional array. In this
work, we use a 3-way tensor W to encode multi-
ple word relations. An element of W is denoted
by Wi,j,k using its indices, and W:,:,k represents
the k-th slice of W (a slice of a 3-way tensor is
a matrix, obtained by fixing the third index). Fol-
lowing (Kolda and Bader, 2009), a fiber of a ten-
sor W:,j,k is a vector, which is a high order analog
of a matrix row or column.
When constructing the raw tensorW in MRLSA,
each slice is analogous to the document-term ma-
trix in LSA, but created based on the data of a par-
ticular relation, such as synonyms. With a slight
abuse of notation, we sometimes use the value rather
than index when there is no confusion. For in-
stance, W:,?word?,k represents the fiber correspond-
ing to the ?word? in slice k, and W:,:,syn refers to
the slice that encodes the synonymy relation. Below
we use an example to compare this construction to
the raw matrix in PILSA, and discuss how it extends
LSA.
Suppose we are interested in representing two re-
lations, synonymy and antonymy. The raw tensor in
MRLSA would then consist of two slices, W:,:,syn
and W:,:,ant, to encode synonyms and antonyms of
target words from a knowledge source (e.g., a the-
saurus). Each row in W:,:,syn represents the syn-
onyms of a target word, and the corresponding
row in W:,:,ant encodes its antonyms. Figures 3(a)
and 3(b) illustrate an example, where ?joy?, ?glad-
den? are synonyms of the target word ?joyfulness?
and ?sorrow? is its antonym. Therefore, the values
of the corresponding entries are 1. Notice that the
matrix W? = W:,:,syn ? W:,:,ant is identical to the
PILSA raw matrix. We can extend the construction
above to enable MRLSA to utilize other semantic
relations (e.g., hypernymy) by adding a slice cor-
responding to each relation of interest. Fig. 3(c)
demonstrates how to add another slice W:,:,hyper to
the tensor for encoding hypernyms.
4.2 Tensor Decomposition
The MRLSA raw tensor encodes relations in one or
more data resources, such as thesauri. However, the
knowledge from a thesaurus is usually noisy and in-
complete. In this section, we derive a low-rank ap-
proximation of the tensor to generalize the knowl-
edge. This step is analogous to the rank-k approxi-
mation in LSA.
Various tensor decomposition methods have been
proposed in literature. Among them, Tucker decom-
position (Tucker, 1966) is recognized as a multi-
dimensional extension of SVD and has been widely
used in many applications. An illustration of this
method is in Fig. 4(a). In Tucker decomposition,
a d? n?m tensor W is decomposed into four
components G,U,V,T. A low-rank approximation
1606
X U VG
T
T=W  
(a) Tucker Tensor Decomposition
X U VS: , : , 1 T=
S
(b) Our Reformulation
Figure 4: Fig. 4(a) illustrates the Tucker tensor decomposition method which factors a 3-way tensorW to
three orthogonal matrices, U,V,T, and a core tensor G. We further apply a n-mode matrix product on the
core tensor G with T. Consequently, each slice of the resulted core tensor S (a square matrix) captures a
semantic relation type, and each column of VT is a vector representing a word.
X ofW is defined by
Wi,j,k ? Xi,j,k
=
R1?
r1=1
R2?
r2=1
R3?
r3=1
Gr1,r2,r3Ui,r1Vj,r2Tk,r3 ,
where G is a core tensor with dimensionsR1?R2?
R3 and U,V,T are orthogonal matrices with di-
mensions d ? R1, n ? R2,m ? R3, respectively.
The rank parameters R1 ? d,R2 ? n,R3 ? m are
given as input to the algorithm. In MRLSA, m (the
number of relations) is usually small, while d and n
are typically large (often in the scale of hundreds
of thousands). Therefore, we choose R1 = R2 = ? ,
?  d, n andR3 = m, where ? is typically less than
1000.
To make the analogy to SVD clear, we rewrite the
results of Tucker decomposition by performing a n-
mode matrix product over the core tensor G with the
matrix T. This produces a tensor S where each slice
is a linear combination of the slices of G with coeffi-
cients given by T (see (Kolda and Bader, 2009) for
detail). That is, we have
S:,:,k =
m?
t=1
Tt,kG:,:,t, ?k.
An illustration is shown in Fig. 4(b), Then, a
straightforward calculation shows that k-th slice of
tensorW is approximated by
W:,:,k ? X:,:,k = US:,:,kV
T . (4)
Comparing Eq. (4) to Eq. (1), one can observe
that matrices U and V play similar roles here, and
each slice of the core tensor S is analogous to ?.
However, the square matrix G:,:,k is not necessary
to be diagonal. As in SVD, the column vectors
of G:,:,kVT (capture both word and relation infor-
mation) behave similarly to the column vectors of
the original tensor sliceW:,:,k.
4.3 Measuring the Degrees of Word Relations
In principle, the raw information in the input ten-
sor W can be used for computing lexical similarity
using the cosine score between the column vectors
for two words from the same slice of the tensor. To
measure the degree of other relations, however, our
approach requires one to specify a pivot slice. The
key role of the pivot slice is to expand the lexical
coverage of the relation of interest to additional lexi-
cal entries and, for this reason, the pivot slice should
be chosen to capture the equivalence of the lexical
entries. In this paper, we use the synonymy relation
as our pivot slice. First we consider measuring the
degree of a relation rel holding between the i-th and
j-th words using the raw tensor W , which can be
computed as
cos
(
W:,i,syn,W:,j,rel
)
. (5)
This measurement can be motivated from the logical
rule: syn(wordi, target) ? rel(target,wordj) ?
rel(wordi,wordj), where the pivot relation syn ex-
pands the coverage of the relation of interest rel.
Turning to the use of the tensor decomposition,
we use a similar derivation to Eq. (3), and measure
the degree of relation rel between two words by
cos
(
S:,:,synVTi,:,S:,:,relV
T
j,:
)
. (6)
1607
For instance, the degree of antonymy between
?joy? and ?sorrow? is measured by the co-
sine similarity between the respective fibers
cos(X:,?joy?,syn,X:,?sorrow?,ant). We can encode both
symmetric relations (e.g., antonymy and synonymy)
and asymmetric relations (e.g., hypernymy and
hyponymy) in the same tensor representation. For a
symmetric relation, we use both cos(X:,i,syn,X:,j,rel)
and cos(X:,j,syn,X:,i,rel) and measure the degree of
a symmetric relation by the average of these two
cosine similarity scores. However, for asymmetric
relations, we use only cos(X:,i,syn,X:,j,rel).
5 Experiments
We evaluate MRLSA on two tasks: answering the
closest-opposite GRE questions and measuring de-
grees of various class-inclusion (i.e., is-a) relations.
In both tasks, we design the experiments to empir-
ically validate the following claims. When encod-
ing two opposite relations from the same source,
MRLSA performs comparably to PILSA. However,
MRLSA generalizes LSA to model multiple rela-
tions, which could be obtained from both homoge-
neous and heterogeneous data sources. As a result,
the performance of a target task can be further im-
proved.
5.1 Experimental Setup
We construct the raw tensors to encode a particular
relation in each slice based on two data sources.
Encarta The Encarta thesaurus is developed by
Bloomsbury Publishing Plc2. For each target word,
it provides a list of synonyms and antonyms. We
use the same version of the thesaurus as in (Yih et
al., 2012), which contains about 47k words and a
vocabulary list of approximately 50k words.
WordNet We use four types of relations from
WordNet: synonymy, antonymy, hypernymy and
hyponymy. The number of target words and the
size of the vocabulary in our version are 117,791
and 149,400, respectively. WordNet has better vo-
cabulary coverage, but fewer antonym pairs. For
instance, the WordNet antonym slice contains only
46,945 nonzero entries, while the Encarta antonym
slice has 129,733.
2http://www.bloomsbury.com
We apply a memory-efficient Tucker decomposi-
tion algorithm (Kolda and Sun, 2008) implemented
in tensor toolbox v2.5 (Bader et al, 2012)3 to factor
the tensor. The largest tensor considered in this pa-
per can be decomposed in about 3 hours using less
than 4GB of memory with a commodity PC.
5.2 Answering GRE Antonym Questions
The first task is to answer the closest-opposite ques-
tions from the GRE test provided by Mohammad et
al. (2008)4. Each question in this test consists of
a target word and five candidate words, where the
goal is to pick the candidate word that has the most
opposite meaning to the target word. In order to
have a fair comparison, we use the same data split
as in (Mohammad et al, 2008), with 162 questions
used for the development set and 950 for test. Fol-
lowing (Mohammad et al, 2008; Yih et al, 2012),
we report the results in precision (accuracy of the
questions that the system attempts to answer), re-
call (percentage of the questions answered correctly
over all questions) and F1 (the harmonic mean of
precision and recall).
We tune two sets of parameters using the devel-
opment set: (1) the rank parameter ? in the tensor
decomposition and (2) the scaling factors of differ-
ent slices of the tensor. The rank parameter spec-
ifies the number of dimensions of the latent space.
In the experiments, We pick the best value of ? from
{100, 200, 300, 500, 750, 1000}. The scaling factors
adjust the values of each slice of the tensor. The el-
ements of each slice are multiplied by the scaling
factor before factorization. This is important be-
cause Tucker decomposition minimizes the recon-
struction error (the Frobenius norm of the residual
tensor). As a result, the slice with a larger range of
values becomes more influential to U and V. In this
work, we fixW:,:,ant, and search for the scaling fac-
tor of W:,:,syn in {0.25, 0.5, 1, 2, 4} and the factors
ofW:,:,hyper andW:,:,hypo in {0.0625, 0.125, 0.25}.
Table 1 summarizes the results of training
3http://www.sandia.gov/?tgkolda/
TensorToolbox. The Tucker decomposition involves
performing SVD on a large matrix. We modify the MATLAB
code of tensor toolbox to use the built-in svd function instead
of svds. This modification reduces both the running time and
memory usage.
4http://www.saifmohammad.com
1608
Dev. Set Test Set
Prec. Rec. F1 Prec. Rec. F1
WordNet Lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet RawTensor 0.42 0.41 0.42 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA:Syn+Ant 0.63 0.62 0.62 0.59 0.58 0.59
WordNet MRLSA:4-layers 0.66 0.65 0.65 0.61 0.59 0.60
Encarta Lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta RawTensor 0.67 0.64 0.65 0.62 0.57 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA:Syn+Ant 0.87 0.82 0.84 0.82 0.74 0.78
MRLSA:WordNet+Encarta 0.88 0.85 0.87 0.81 0.77 0.79
Table 1: GRE antonym test results of models based on Encarta and WordNet data in precision, recall and F1.
RawTensor evaluates the performance of the tensor with 2 slices encoding synonyms and antonyms be-
fore decomposition (see Eq. (5)), which is comparable to checking the original data directly (Lookup).
MRLSA:Syn+Ant applies Tucker decomposition to the raw tensor and measures the degree of antonymy
using Eq. (6). The result is similar to that of PILSA (see Sec. 3.1). MRLSA:4-layers adds hypernyms and
hyponyms from WordNet; MRLSA:WordNet+Encarta consists of synonyms/antonyms from Encarta and hy-
pernyms/hyponyms from WordNet, where the target words are aligned using the synonymy relations. Both
models demonstrate the advantage of encoding more relations, from either the same or different resources.
MRLSA using two different corpora, Encarta and
WordNet. The performance of the MRLSA raw
tensor is close to that of looking up the thesaurus.
This indicates the tensor representation is able to
capture the word relations explicitly described in
the thesaurus. After conducting tensor decomposi-
tion, MRLSA:Syn+Ant achieves similar results to
PILSA. This confirms our claim that when giv-
ing the same among of information, MRLSA per-
forms at least comparably to PILSA. However, the
true power of MRLSA is its ability to incorpo-
rate other semantic relations to boost the perfor-
mance of the target task. For example, when
we add the hypernymy and hyponymy relations to
the tensor, these class-inclusion relations provide a
weak signal to help resolve antonymy. We sus-
pect that this is due to the fact that antonyms typ-
ically share the same properties but only have the
opposite meaning on one particular semantic di-
mension. For instance, the antonyms ?sadness?
and ?happiness? are different forms of emotion.
When two words are hyponyms of a target word,
the likelihood that they are antonyms should thus
be increased. We show that the target relations
and these auxiliary semantic relations can be col-
lected from the same data source (e.g., WordNet
MRLSA:4-layers) or from multiple, heterogeneous
sources (e.g., MRLSA:WordNET+Encarta). In both
cases, the performance of the model improves as
more relations are incorporated. Moreover, our ex-
periments show that adding the hypernym and hy-
ponym layers from WordNet improves modeling
antonym relations based on the Encarta thesaurus.
This suggests that the weak signal from a resource
with a large vocabulary (e.g., WordNet) can help
predict relations between out-of-vocabulary words
and thus improve the recall.
To better understand the model, we examine the
top antonyms for three question words from the
GRE test. The lists below show antonyms and their
MRLSA scores for each of the GRE question words
as determined by the MRLSA:WordNET+Encarta
model. Antonyms that can be found directly in the
Encarta thesaurus are in italics.
inanimate alive (0.91), living (0.90), bodily (0.90), in-
the-flesh (0.89), incarnate (0.89)
alleviate exacerbate (0.68), make-worse (0.67), in-
flame (0.66), amplify (0.65), stir-up (0.64)
relish detest (0.33), abhor (0.33), abominate (0.33), de-
spise (0.33), loathe (0.31)
We can see that from these examples, MRLSA not
1609
Dev. Test
1a (Taxonomic) 1b (Functional) 1c (Singular) 1d (Plural) Avg.
WordNet Lookup 52.9 34.5 41.4 34.3 36.7
WordNet RawTensor 51.0 38.3 50.0 42.1 43.5
WordNet MRLSA:Syn+Hypony 55.8 41.7 (43.2) 51.0 (51.4) 37.5 (44.4) 43.4 (46.3)
WordNet MRLSA:4-layers 52.9 51.5 (53.9) 51.9 (60.0) 43.5 (50.5) 49.0 (54.8)
MRLSA:WordNet+Encarta 62.1 55.3 (58.7) 57.1 (65.7) 48.6 (53.7) 55.8 (60.1)
UTDNB (Rink and Harabagiu, 2012) - 38.3 36.7 28.2 34.4
Table 2: Results of measuring the class-inclusion (is-a) relations in MaxDiff accuracy (see text for de-
tail). RawTensor has synonym and hyponym slices and measures the degree of is-a relation using Eq. (5).
MRLSA:Syn+Hypo factors the raw tensor and judges the relation by Eq. (6). The constructions of
MRLSA:4-layers and MRLSA:WordNet+Encarta are the same as in Sec. 5.2 (see the caption of Table 1
for detail). For MRLSA models, numbers shown in the parentheses are the results when parameters are
tuned using the test sets. UTDNB is the results of the best performing system in SemEval-2012 Task 2.
only preserves the antonyms in the thesaurus, but
also discovers additional ones, such as exacerbate
and inflame for ?alleviate?. Another interesting find-
ing is that while the scores are useful in ranking
the candidate words, they might not be comparable
across different question words. This could be an
issue for some applications, which need to make a
binary decision on whether two words are antonyms.
5.3 Measuring degrees of Is-A relations
We evaluate MRLSA using the class-inclusion por-
tion of SemEval-2012 Task 2 data (Jurgens et al,
2012). Here the goal is to measure the degree
of two words having the is-a relation. Five an-
notated datasets are provided for different subcate-
gories of this relation: 1a-taxonomic, 1b-functional,
1c-singular, 1d-plural, 1e-class individual. We omit
1e because it focuses on real world entities (e.g.,
queen:Elizabeth, river:Nile), which are not included
in WordNet.
Each dataset contains about 100 questions based
on approximately 40 word pairs. The question con-
sists of 4 randomly chosen word pairs and asks the
best and worst pairs that exemplify the specific is-a
relation. The performance is measured by the av-
erage prediction accuracy, also called the MaxDiff
accuracy (Louviere and Woodworth, 1991).
Because the questions are generated from the
same set of word pairs, these questions are not mutu-
ally independent. Therefore, it is not proper to split
the data of each subcategory into the development
and test sets. Alternatively, we follow the setting
of SemEval-2012 Task 2 and use the first subcat-
egory (1a-taxonomy) to tune the model and eval-
uate its performance based on the results on other
datasets. Since the models are tuned and tested on
different types of subcategories, they might not be
the optimal ones when evaluated on the test sets.
Therefore, we show results using the best parame-
ters tuned on the development set and those tuned on
the test set, where the latter suggests a performance
upper-bound. Besides the rank parameter, we tune
the scaling factors of the synonym, hypernym and
hyponym slices from {4, 16, 64}. The scaling factor
of the antonym slice is fixed to 1.
Table 2 shows the performance in MaxDiff accu-
racy. Results show that even the raw tensor repre-
sentation (RawTensor) performs better than Word-
Net lookup. We suspect that this is because the
tensor representation can capture the fact that the
hyponyms of a word are usually synonymous to
each other. By performing Tucker decomposition
on the raw Tensor, MRLSA achieves better per-
formance. MRLSA:4-layers further leverages the
information from antonyms and hypernyms and
thus improves the model. As we notice in the
GRE antonym test, models based on the Encarta
thesaurus perform better in predicting antonyms.
Therefore, it is interesting to check if combining
synonyms and antonyms from Encarta helps. As
a result, MRLSA:WordNet+Encarta improves over
MRLSA:4-layers significantly. This demonstrates
again that MRLSA can leverage knowledge stored in
heterogeneous resources. Notably, MRLSA outper-
1610
forms the best system participated in the SemEval-
2012 task with a large margin, with a difference of
21.4 in MaxDiff accuracy.
Next we examine the top words that have the is-
a relation relative to three question words from the
task. The lists below show the hyponyms and their
respective MRLSA scores for each of the question
words as determined by MRLSA:4-layers.
bird ostrich (0.75), gamecock (0.75), nighthawk (0.75),
amazon (0.74), parrot (0.74)
automobile minivan (0.48), wagon (0.48), taxi (0.46),
minicab (0.45), gypsy cab (0.45)
vegetable buttercrunch (0.61), yellow turnip (0.61), ro-
maine (0.61), chipotle (0.61), chilli (0.61)
Although the model in general does a good job
finding hyponyms, we observe that some suggested
words, such as buttercrunch (a mild lettuce) vs.
?vegetable?, do not seem intuitive (e.g., compared to
carrot). Having one additional slice to capture the
general term co-occurrence relation may help im-
prove the model in this respect.
6 Conclusions
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA) for lexical seman-
tics. MRLSA models multiple word relations by
leveraging a 3-way tensor, where each slice cap-
tures one particular relation. A low-rank approx-
imation of the tensor is then derived using a ten-
sor decomposition. Consequently, words in the vo-
cabulary are represented by vectors in the latent se-
mantic space, and each relation is captured by a
latent square matrix. Given two words, MRLSA
not only can measure their degree of having a spe-
cific relation, but also can discover unknown rela-
tions between them. These advantages have been
demonstrated in our experiments. By encoding re-
lations from both homogeneous or heterogeneous
data sources, MRLSA achieves state-of-the-art per-
formance on existing benchmark datasets for two re-
lations, antonymy and is-a.
For future work, we plan to explore directions that
aim for improving both the quality and word cover-
age of the model. For instance, the knowledge en-
coded by MRLSA can be enriched by adding more
relations from a variety of linguistic resources, in-
cluding the co-occurrence relations from large cor-
pora. On model refinement, we notice that MRLSA
can be viewed as a 3-layer neural network without
applying the sigmoid function. Following the strat-
egy of using Siamese neural networks to enhance
PILSA (Yih et al, 2012), training MRLSA with a
multi-task discriminative learning setting can be a
promising approach as well.
Acknowledgments
We thank Geoff Zweig for valuable discussions and
the anonymous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Brett W. Bader, Tamara G. Kolda, et al 2012. Matlab
tensor toolbox version 2.5. Available online, January.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP-CoNLL, pages 1024?1033.
Shay B. Cohen, Giorgio Satta, and Michael Collins.
2013. Approximate PCFG parsing using tensor de-
composition. In NAACL-HLT 2013, pages 487?496.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science, 41(96).
S. Dumais, T. Letsche, M. Littman, and T. Landauer.
1997. Automatic cross-language retrieval using latent
semantic indexing. In AAAI-97 Spring Symposium Se-
ries: Cross-Language Text and Speech Retrieval.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In ACL 2012, pages 864?872.
Weiwei Guo and Mona Diab. 2013. Improving lexical
semantics for sentential semantics: Modeling selec-
tional preference and similar words in a latent variable
model. In NAACL-HLT 2013, pages 739?745.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289?296.
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 356?364.
1611
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable ten-
sor decompositions for multi-aspect data mining. In
ICDM 2008, pages 363?372.
T. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In NIPS 1998.
T. Landauer. 2002. On the computational basis of learn-
ing and cognition: Arguments from lsa. Psychology of
Learning and Motivation, 41:43?84.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In NAACL-HLT 2013.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251?261.
Xipeng Qiu, Le Tian, and Xuanjing Huang. 2013. Latent
semantic tensor indexing for community-based ques-
tion answering. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 434?439, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109?117.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT 2013, pages 74?84.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In ICML ?11.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ledyard R Tucker. 1966. Some mathematical
notes on three-mode factor analysis. Psychometrika,
31(3):279?311.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37(1):141?
188.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of se-
mantic compositionality. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1142?1151, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267?273, New York, NY,
USA. ACM.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247?256, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining het-
erogeneous models for measuring relational similar-
ity. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1000?1009, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
1612
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1568?1579,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Typed Tensor Decomposition of Knowledge Bases for Relation Extraction
Kai-Wei Chang
??
Wen-tau Yih
\
Bishan Yang
??
Christopher Meek
\
?
University of Illinois, Urbana, IL 61801, USA
?
Cornell University, Ithaca, NY 14850, USA
\
Microsoft Research, Redmond, WA 98052, USA
Abstract
While relation extraction has traditionally
been viewed as a task relying solely on
textual data, recent work has shown that
by taking as input existing facts in the form
of entity-relation triples from both knowl-
edge bases and textual data, the perfor-
mance of relation extraction can be im-
proved significantly. Following this new
paradigm, we propose a tensor decompo-
sition approach for knowledge base em-
bedding that is highly scalable, and is es-
pecially suitable for relation extraction.
By leveraging relational domain knowl-
edge about entity type information, our
learning algorithm is significantly faster
than previous approaches and is better
able to discover new relations missing
from the database. In addition, when ap-
plied to a relation extraction task, our ap-
proach alone is comparable to several ex-
isting systems, and improves the weighted
mean average precision of a state-of-the-
art method by 10 points when used as a
subcomponent.
1 Introduction
Identifying the relationship between entities from
free text, relation extraction is a key task for ac-
quiring new facts to increase the coverage of a
structured knowledge base. Given a pre-defined
database schema, traditional relation extraction
approaches focus on learning a classifier using tex-
tual data alone, such as patterns between the oc-
currences of two entities in documents, to deter-
mine whether the entities have a particular rela-
tion. Other than using the existing known facts
to label the text corpora in a distant supervision
setting (Bunescu and Mooney, 2007; Mintz et al.,
?
Work conducted while interning at Microsoft Research.
2009; Riedel et al., 2010; Ritter et al., 2013), an
existing knowledge base is typically not involved
in the process of relation extraction.
However, this paradigm has started to shift re-
cently, as researchers showed that by taking exist-
ing facts of a knowledge base as an integral part of
relation extraction, the model can leverage richer
information and thus yields better performance.
For instance, Riedel et al. (2013) borrowed the
idea of collective filtering and constructed a ma-
trix where each row is a pair of entities and each
column is a particular relation. For a true entity-
relation triple (e
1
, r, e
2
), either from the text cor-
pus or from the knowledge base, the correspond-
ing entry in the matrix is 1. A previously unknown
fact (i.e., triple) can be discovered through ma-
trix decomposition. This approach can be viewed
as creating vector representations of each relation
and candidate pair of entities. Because each entity
does not have its own representation, relationships
of any unpaired entities cannot be discovered. Al-
ternatively, Weston et al. (2013) created two types
of embedding ? one based on textual similarity and
the other based on knowledge base, where the lat-
ter maps each entity and relation to the same d-
dimensional vector space using a model proposed
by Bordes et al. (2013a). They also showed that
combining these two models results in a signif-
icant improvement over the model trained using
only textual data.
To make such an integrated strategy work, it is
important to capture all existing entities and rela-
tions, as well as the known facts, from both tex-
tual data and large databases. In this paper, we
propose a new knowledge base embedding model,
TRESCAL, that is highly efficient and scalable,
with relation extraction as our target application.
Our work is built on top of RESCAL (Nickel
et al., 2011), which is a tensor decomposition
method that has proven its scalability by factoring
YAGO (Biega et al., 2013) with 3 million entities
1568
and 41 million triples (Nickel et al., 2012). We
improve the tensor decomposition model with two
technical innovations. First, we exclude the triples
that do not satisfy the relational constraints (e.g.,
both arguments of the relation spouse-of need to
be person entities) from the loss, which is done
by selecting sub-matrices of each slice of the ten-
sor during training. Second, we introduce a math-
ematical technique that significantly reduces the
computational complexity in both time and space
when the loss function contains a regularization
term. As a consequence, our method is more than
four times faster than RESCAL, and is also more
accurate in discovering unseen triples.
Our contributions are twofold. First, compared
to other knowledge base embedding methods de-
veloped more recently, it is much more efficient
to train our model. As will be seen in Sec. 5,
when applied to a large knowledge base created
using NELL (Carlson et al., 2010) that has 1.8M
entity-relation triples, our method finishes training
in 4 to 5 hours, while an alternative method (Bor-
des et al., 2013a) needs almost 3 days. Moreover,
the prediction accuracy of our model is competi-
tive to others, if not higher. Second, to validate its
value to relation extraction, we apply TRESCAL to
extracting relations from a free text corpus along
with a knowledge base, using the data provided
in (Riedel et al., 2013). We show that TRESCAL
is complementary to existing systems and signif-
icantly improves their performance when using it
as a subcomponent. For instance, this strategy im-
proves the weighted mean average precision of the
best approach in (Riedel et al., 2013) by 10 points
(47% to 57%).
The remainder of this paper is organized as fol-
lows. We survey most related work in Sec. 2 and
provide the technical background of our approach
in Sec. 3. Our approach is detailed in Sec. 4, fol-
lowed by the experimental validation in Sec. 5. Fi-
nally, Sec. 6 concludes the paper.
2 Related Work
Our approach of creating knowledge base em-
bedding is based on tensor decomposition, which
is a well-developed mathematical tool for data
analysis. Existing tensor decomposition models
can be categorized into two main families: the
CP and Tucker decompositions. The CP (CAN-
DECOMP/PARAFAC) decomposition (Kruskal,
1977; Kiers, 2000) approximates a tensor by a sum
of rank-one tensors, while the Tucker decompo-
sition (Tucker, 1966), also known as high-order
SVD (De Lathauwer et al., 2000), factorizes a ten-
sor into a core tensor multiplied by a matrix along
each dimension. A highly scalable distributional
algorithm using the Map-Reduce architecture has
been proposed recently for computing CP (Kang et
al., 2012), but not for the Tucker decomposition,
probably due to its inherently more complicated
model form.
Matrix and tensor decomposition methods have
been applied to modeling multi-relational data.
For instance, Speer et al. (2008) aimed to cre-
ate vectors of latent components for representing
concepts in a common sense knowledge base us-
ing SVD. Franz et al. (2009) proposed TripleRank
to model the subject-predicate-object
RDF triples in a tensor, and then applied the CP
decomposition to identify hidden triples. Fol-
lowing the same tensor encoding, Nickel et al.
(2011) proposed RESCAL, a restricted form of
Tucker decomposition for discovering previously
unknown triples in a knowledge base, and later
demonstrated its scalability by applying it to
YAGO, which was encoded in a 3M ? 3M ? 38
tensor with 41M triples (Nickel et al., 2012).
Methods that revise the objective function
based on additional domain information have been
proposed, such as MrWTD, a multi-relational
weighted tensor decomposition method (London
et al., 2013), coupled matrix and tensor fac-
torization (Papalexakis et al., 2014), and col-
lective matrix factorization (Singh and Gordon,
2008). Alternatively, instead of optimizing for the
least-squares reconduction loss, a non-parametric
Bayesian approach for 3-way tensor decomposi-
tion for modeling relational data has also been pro-
posed (Sutskever et al., 2009). Despite the exis-
tence of a wide variety of tensor decomposition
models, most methods do not scale well and have
only been tested on datasets that are much smaller
than the size of real-world knowledge bases.
Multi-relational data can be modeled by neural-
network methods as well. For instance, Bordes et
al. (2013b) proposed the Semantic Matching En-
ergy model (SME), which aims to have the same
d-dimensional vector representations for both en-
tities and relations. Given the vectors of entities
e
1
, e
2
and relation r. They first learn the latent
representations of (e
1
, r) and (e
2
, r). The score
of (e
1
, r, e
2
) is determined by the inner product
1569
of the vectors of (e
1
, r) and (e
2
, r). Later, they
proposed a more scalable method called translat-
ing embeddings (TransE) (Bordes et al., 2013a).
While both entities and relations are still repre-
sented by vectors, the score of (e
1
, r, e
2
) becomes
the negative dissimilarity measure of the corre-
sponding vectors ??e
i
+ r
k
? e
j
?, motivated by
the work in (Mikolov et al., 2013b; Mikolov et al.,
2013a). Alternatively, Socher et al. (2013) pro-
posed a Neural Tensor Network (NTN) that repre-
sents entities in d-dimensional vectors created sep-
arately by averaging pre-trained word vectors, and
then learns a d?d?m tensor describing the inter-
actions between these latent components in each
of the m relations. All these methods optimize
for loss functions that are more directly related to
the true objective ? the prediction accuracy of cor-
rect entity-relation triples, compared to the mean-
squared reconstruction error in our method. Nev-
ertheless, they typically require much longer train-
ing time.
3 Background
In this section, we first describe how entity-
relation triples are encoded in a tensor. We then
introduce the recently proposed tensor decompo-
sition method, RESCAL (Nickel et al., 2011) and
explain how it adopts an alternating least-squares
method, ASALSAN (Bader et al., 2007), to com-
pute the factorization.
3.1 Encoding Binary Relations in a Tensor
Suppose we are given a knowledge base with
n entities and m relation types, and the facts
in the knowledge base are denoted as a set of
entity-relation triples T = {(e
i
, r
k
, e
j
)}, where
i, j ? {1, 2, ? ? ?n} and k ? {1, 2, ? ? ?m}. A
triple (e
i
, r
k
, e
j
) simply means that the i-th en-
tity and the j-th entity have the k-th relation.
Following (Franz et al., 2009), these triples can
naturally be encoded in a 3-way tensor X ?
{0, 1}
n?n?m
, such that X
i,j,k
= 1 if and only if
the triple (e
i
, r
k
, e
j
) ? T
1
. The tensor can be
viewed as consisting of m slices, where each slice
is an n?n square matrix, denoting the interactions
of the entities of a particular relation type. In the
remainder of this paper, we will use X
k
to refer to
the k-th slice of the tensor X . Fig. 1 illustrates this
representation.
1
This representation can easily be extended for a proba-
bilistic knowledge base by allowing nonnegative real values.
e1  en
e 1  
 e n
?? k
Figure 1: A tensor encoding of m binary relation
types and n entities. A sliceX
k
denotes the entities
having the k-th relation.
3.2 RESCAL
In order to identify latent components in a ten-
sor for collective learning, Nickel et al. (2011)
proposed RESCAL, which is a tensor decomposi-
tion approach specifically designed for the multi-
relational data described in Sec. 3.1. Given a ten-
sor X
n?n?m
, RESCAL aims to have a rank-r ap-
proximation, where each slice X
k
is factorized as
X
k
? AR
k
A
T
. (1)
A is an n ? r matrix, where the i-th row denotes
the r latent components of the i-th entity. R
k
is an
asymmetric r ? r matrix that describes the inter-
actions of the latent components according to the
k-th relation. Notice that while R
k
differs in each
slice, A remains the same.
A and R
k
are derived by minimizing the loss
function below.
min
A,R
k
f(A,R
k
) + ? ? g(A,R
k
), (2)
where f(A,R
k
) =
1
2
(
?
k
?X
k
?AR
k
A
T
?
2
F
)
is the mean-squared reconstruction error and
g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
is the regu-
larization term.
RESCAL is a special form of Tucker decom-
position (Tucker, 1966) operating on a 3-way ten-
sor. Its model form (Eq. (1)) can also be regarded
as a relaxed form of DEDICOM (Bader et al.,
2007), which derives the low-rank approximation
as: X
k
? AD
k
RD
k
A
T
. To compare RESCAL
to other tensor decomposition methods, interested
readers can refer to (Kolda and Bader, 2009).
1570
The optimization problem in Eq. (2) can be
solved using the efficient alternating least-squares
(ALS) method. This approach alternatively fixes
R
k
to solve for A and then fixes A to solve
R
k
. The whole procedure stops until
f(A,R
k
)
?X?
2
F
con-
verges to some small threshold  or the maximum
number of iterations has been reached.
By finding the solutions where the gradients are
0, we can derive the update rules of A and R
k
as
below.
A?
[
?
k
X
k
AR
T
k
+X
T
k
AR
k
][
?
k
B
k
+C
k
+?I
]
?1
,
where B
k
= R
k
A
T
AR
T
k
and C
k
= R
T
k
A
T
AR
k
.
vec(R
k
)?
(
Z
T
Z + ?I
)
?1
Z
T
vec(X
k
), (3)
where vec(R
k
) is the vectorization of R
k
, Z =
A?A and the operator ? is the Kronecker prod-
uct.
Complexity Analysis Following the analysis in
(Nickel et al., 2012), we assume that each X
k
is a
sparse matrix, and let p be the number of non-zero
entries
2
. The complexity of computing X
k
AR
T
k
and X
T
k
AR
k
is O(pr + nr
2
). Evaluating B
k
and
C
k
requires O(nr
2
) and the matrix inversion re-
quires O(r
3
). Therefore, the complexity of updat-
ing A isO(pr+nr
2
) assuming n r. The updat-
ing rule of R
k
involves inverting an r
2
? r
2
ma-
trix. Therefore, directly computing the inversion
requires time complexity O(r
6
) and space com-
plexity O(r
4
). Although Nickel et al. (2012) con-
sidered using QR decomposition to simplify the
updates, it is still time consuming with the time
complexity O(r
6
+ pr
2
). Therefore, the total time
complexity isO(r
6
+pr
2
) and the step of updating
R
k
is the bottleneck in the optimization process.
We will describe how to reduce the time complex-
ity of this step to O(nr
2
+ pr) in Section 4.2.
4 Approach
We describe how we leverage the relational do-
main knowledge in this section. By removing the
incompatible entity-relation triples from the loss
2
Notice that we use a slightly different definition of p
from the one in (Nickel et al., 2012). The time complexity
of multiplying an n ? n sparse matrix X
k
with p non-zero
entries by an n? r dense matrix is O(pr) assuming n r.
function, training can be done much more effi-
ciently and results in a model with higher pre-
diction accuracy. In addition, we also introduce
a mathematical technique to reduce the compu-
tational complexity of the tensor decomposition
methods when taking into account the regulariza-
tion term.
4.1 Applying Relational Domain Knowledge
In the domain of knowledge bases, the notion of
entity types is the side information that commonly
exists and dictates whether some entities can be
legitimate arguments of a given predicate. For
instance, suppose the relation of interest is born-
in, which denotes the birth location of a person.
When asked whether an incompatible pair of en-
tities, such as two person entities like Abraham
Lincoln and John Henry, having this rela-
tion, we can immediately reject the possibility. Al-
though the type information and the constraints
are readily available, it is overlooked in the pre-
vious work on matrix and tensor decomposition
models for knowledge bases (Riedel et al., 2013;
Nickel et al., 2012). Ignoring the type information
has two implications. Incompatible entity-relation
triples still participate in the loss function of the
optimization problem, which incurs unnecessary
computation. Moreover, by choosing values for
these incompatible entries we introduce errors in
training the model that can reduce the quality of
the model.
Based on this observation, we propose Typed-
RESCAL, or TRESCAL, which leverages the en-
tity type information to improve both the effi-
ciency of model training and the quality of the
model in term of prediction accuracy. We em-
ploy a direct and simple approach by excluding
the triples of the incompatible entity types from
the loss in Eq. (2). For each relation, let L
k
and
R
k
be the set of entities with a compatible type to
the k-th relation. That is, (e
i
, r
k
, e
j
) is a feasible
triple if and only if e
i
? L
k
and e
j
? R
k
. For no-
tational convenience, we use A
k
l
,A
k
r
to denote
the sub-matrices of A that consists of rows asso-
ciated with L
k
and R
k
, respectively. Analogously,
let X
k
lr
be the sub-matrix of X
k
that consists of
only the entity pairs compatible to the k-th rela-
tion. The rows and columns of X
k
lr
map to the en-
tities in A
k
l
and A
k
r
, respectively. In other words,
entries of X
k
but not in X
k
lr
do not satisfy the type
constraint and are ignored from the computation.
1571
~ ~ ? ? 
?k A 
A TRk
A kl A krT?klr
e ??Lk
e ??Rk
Figure 2: The construction of TRESCAL. Suppose
the k-th relation is born-in. L
k
is then a set of
person entities and R
k
is a set of location entities.
Only the sub-matrix corresponds to the compati-
ble entity pairs (i.e., X
k
lr
) and the sub-matrices of
the associated entities (i.e., A
k
l
and A
T
k
r
) will be
included in the loss.
Fig. 2 illustrates this construction.
TRESCAL solves the following optimization
problem:
min
A,R
k
f
?
(A,R
k
) + ? ? g(A,R
k
), (4)
where f
?
(A,R
k
) =
1
2
?
k
?X
k
lr
?A
k
l
R
k
A
T
k
r
?
2
F
and g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
.
Similarly, A and R
k
can be solved using the
alternating least-squares method. The update rule
of A is
A?
[
?
k
(
X
k
lr
A
k
r
R
T
k
+ X
T
k
lr
A
k
l
R
k
)
]
?
[
?
k
B
k
r
+ C
k
l
+ ?I
]
?1
,
where B
k
r
= R
k
A
T
k
r
A
k
r
R
T
k
and C
k
l
=
R
T
k
A
T
k
l
A
k
l
R
k
.
The update ofR
k
becomes:
vec(R
k
)?
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
?
vec(A
k
l
T
X
k
lr
A
k
r
),
(5)
Complexity Analysis Let n? be the average
number of entities with a compatible type to a
relation. Follow a similar derivation in Sec. 3.2,
the time complexity of updating A isO(pr+ n?r
2
)
and the time complexity of updating R
k
remains
to be O(r
6
+ pr
2
).
4.2 Handling Regularization Efficiently
Examining the update rules of both RESCAL
and TRESCAL, we can see that the most time-
consuming part is the matrix inversions. For
RESCAL, this is the term (Z
T
Z+?I)
?1
in Eq. (3),
where Z = A?A. Nickel et al. (2011) made the
observation that if ? = 0, the matrix inversion can
be calculated by
(Z
T
Z)
?1
= (A
T
A)
?1
A? (A
T
A)
?1
A.
Then, it only involves an inversion of an r? r ma-
trix, namely A
T
A. However, if ? > 0, directly
calculating Eq. (3) requires to invert an r
2
? r
2
matrix and thus becomes a bottleneck in solving
Eq. (2).
To reduce the computational complexity of
the update rules of R
k
, we compute the inver-
sion
(
Z
T
Z + ?I
)
?1
by applying singular value
decomposition (SVD) to A, such that A =
U?V
T
, where U and V are orthogonal matrices
and ? is a diagonal matrix. Then by using proper-
ties of the Kronecker product we have:
(
Z
T
Z + ?I
)
?1
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
.
The last equality holds because V ? V is
also an orthogonal matrix. We leave the de-
tailed derivations in Appendix A. Notice that
(
?I + ?
2
??
2
)
?1
is a diagonal matrix. There-
fore, the inversion calculation is trivial.
This technique can be applied to TRESCAL
as well. By applying SVD to both A
k
l
and A
k
r
, we have A
k
l
= U
k
l
?
k
l
V
T
k
l
and
A
k
r
= U
k
r
?
k
r
V
T
k
r
, respectively. The computa-
tion of
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
of Eq. (5)
thus becomes:
(V
k
l
?V
k
r
)
(
?I + ?
2
k
l
??
2
k
r
)
?1
(V
k
l
?V
k
r
)
T
.
The procedure of updating R is depicted in Al-
gorithm 1.
Complexity Analysis For RESCAL, V and ?
can be computed by finding eigenvectors of A
T
A.
Therefore, computing SVD of A costs O(nr
2
+
r
3
) = O(nr
2
). Computing Step 4 in Algorithm 1
takes O(nr
2
+ pr). Step 5 and Step 6 require
1572
Algorithm 1 UpdatingR in TRESCAL
Require: X , A, and entity sets R
k
,L
k
,?k
Ensure: R
k
,?k.
1: for k = 1 . . .m do
2: [U
k
l
,?
2
k
l
,V
k
l
]? SVD(A
T
k
l
A
k
l
).
3: [U
k
r
,?
2
k
r
,V
k
r
]? SVD(A
T
k
r
A
k
r
).
4: M
1
? V
T
k
l
A
T
k
l
X
k
lr
A
k
r
V
k
r
.
5: M
2
? diag(?
2
k
l
) diag(?
2
k
r
)
T
+ ?1.
(1 is a matrix of all ones. Function diag
converts the diagonal entries of a matrix to
a vector. )
6: R
k
? V
k
l
(M
1
./M
2
)V
T
k
r
.
(The operator ?./? is element-wise divi-
sion.)
7: end for
O(r
2
) and O(r
3
), respectively. The overall time
complexity of updatingR
k
becomesO(nr
2
+pr).
Using a similar derivation, the time complex-
ity of updating R
k
in TRESCAL is O(n?r
2
+ pr).
Therefore, the total complexity of each iteration is
O(n?r
2
+ pr).
5 Experiments
We conduct two sets of experiments. The first
evaluates the proposed TRESCAL algorithm on
inferring unknown facts using existing relation?
entity triples, while the second demonstrates its
application to relation extraction when a text cor-
pus is available.
5.1 Knowledge Base Completion
We evaluate our approach on a knowledge base
generated by the CMU Never Ending Language
Learning (NELL) project (Carlson et al., 2010).
NELL collects human knowledge from the web
and has generated millions of entity-relation
triples. We use the data generated from version
165 for training
3
, and collect the new triples gen-
erated between NELL versions 166 and 533 as the
development set and those generated between ver-
sion 534 and 745 as the test set
4
. The data statistics
of the training set are summarized in Table 1. The
numbers of triples in the development and test sets
are 19,665 and 117,889, respectively. Notice that
this dataset is substantially larger than the datasets
used in recent work. For example, the Freebase
data used in (Socher et al., 2013) and (Bordes et
3
http://www.cs.cmu.edu/
?
nlao/
4
http://bit.ly/trescal
NELL
# entities 753k
# relation types 229
# entity types 300
# entity-relation triples 1.8M
Table 1: Data statistics of the training set from
NELL in our experiments.
al., 2013a) have 316k and 483k
5
triples, respec-
tively, compared to 1.8M in this dataset.
In the NELL dataset, the entity type informa-
tion is encoded in a specific relation, called Gen-
eralization. Each entity in the knowledge base is
assigned to at least one category presented by the
Generalization relationship. Based on this infor-
mation, the compatible entity type constraint of
each relation can be easily identified. Specifically,
we examined the entities and relations that occur
in the triples of the training data, and counted all
the types appearing in these instances of a given
relation legitimate.
We implement RESCAL and TRESCAL in
MATLAB with the Matlab tensor Toolbox (Bader
et al., 2012). With the efficient implementation
described in Section 4.2, all experiments can be
conducted on a commodity PC with 16 GB mem-
ory. We set the maximal number of iterations of
both RESCAL and TRESCAL to be 10, which we
found empirically to be enough to generate a sta-
ble model. Note that Eq. (4) is non-convex, and the
optimization process does not guarantee to con-
verge to a global minimum. Therefore, initial-
izing the model properly might be important for
the performance. Following the implementation of
RESCAL, we initialize A by performing singular
value decomposition over
?
X =
?
k
(X
k
+ X
T
k
),
such that
?
X = U?V
T
and set A = U. Then,
we apply the update rule ofR
k
to initialize {R
k
}.
RESCAL and TRESCAL have two types of param-
eters: (1) the rank r of the decomposed tensor and
(2) the regularization parameter ?. We tune the
rank parameter on development set in a range of
{100, 200, 300, 400} and the regularization pa-
rameter in a range of {0.01, 0.05, 0.1, 0.5, 1}.
For comparison, we also use the code released
by Bordes et al. (2013a), which is implemented
using Python and the Theano library (Bergstra
et al., 2010), to train a TransE model using the
5
In (Bordes et al., 2013a), there is a much larger dataset,
FB1M, that has 17.5M triples used for evaluation. However,
this dataset has not been released.
1573
Entity Retrieval Relation Retrieval
TransE RESCAL TRESCAL TransE RESCAL TRESCAL
w/o type checking 51.41%
?
51.59% 54.79% 75.88% 73.15%
?
76.12%
w/ type checking 67.56% 62.91%
?
69.26% 70.71%
?
73.08%
?
75.70%
Table 2: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval.
? and ? indicate the comparison to TRESCAL in the same setting is statistically significant using a paired-
t test on average precision of each query, with p < 0.01 and p < 0.05, respectively. Enforcing type
constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.
same NELL dataset. We reserved randomly 1%
of the training triples for the code to evaluate the
model performance in each iteration. As sug-
gested in their paper, we experiment with sev-
eral hyper-parameters, including learning rate of
{0.01, 0.001}, the latent dimension of {50, 100}
and the similarity measure of {L1, L2}. In addi-
tion, we also adjust the number of batches of {50,
100, 1000}. Of all the configurations, we keep the
models picked by the method, as well as the fi-
nal model after 500 training iterations. The final
model is chosen by the performance on our devel-
opment set.
5.1.1 Training Time Reduction
We first present experimental results demonstrat-
ing that TRESCAL indeed reduces the time re-
quired to factorize a knowledge database, com-
pared to RESCAL. The experiment is conducted
on NELL with r = 300 and ? = 0.1. When
? 6= 0, the original RESCAL algorithm described
in (Nickel et al., 2011; Nickel et al., 2012) cannot
handle a large r, because updating matrices {R
k
}
requires O(r
4
) memory. Later in this section, we
will show that in some situation a large rank r is
necessary for achieving good testing performance.
Comparing TRESCAL with RESCAL, each it-
eration of TRESCAL takes 1,608 seconds, while
that of RESCAL takes 7,415 seconds. In other
words, by inducing the entity type information
and constraints, TRESCAL enjoys around 4.6 times
speed-up, compared to an improved regularized
version of RESCAL. When updating A and {R
k
}
TRESCAL only requires operating on sub-matrices
of A, {R
k
} and {X
k
}, which reduces the compu-
tation substantially. In average, TRESCAL filters
96% of entity triples that have incompatible types.
In contrast, it takes TransE at least 2 days and 19
hours to finish training the model (the default 500
iterations)
6
, while TRESCAL finishes the training
6
It took almost 4 days to train the best TransE model that
in roughly 4 to 5 hours
7
.
5.1.2 Test Performance Improvement
We consider two different types of tasks to evalu-
ate the prediction accuracy of different models ?
entity retrieval and relation retrieval.
Entity Retrieval In the first task, we collect a
set of entity-relation pairs {(e
i
, r
k
)} and aim at
predicting e
j
such that the tuple (e
i
, r
k
, e
j
) is a
recorded triple in the NELL knowledge base. For
each pair (e
i
, r
k
), we collect triples {(e
i
, r
k
, e
?
j
)}
from the NELL test corpus as positive samples
and randomly pick 100 entries e
?
j
to form negative
samples {e
i
, r
k
, e
?
j
}. Given A and R
k
from the
factorization generated by RESCAL or TRESCAL,
the score assigned to a triple {e
i
, r
k
, e
?
j
} is com-
puted by a
T
i
R
k
a
j
where a
i
and a
j
are the i-th
and j-th rows of A. In TransE, the score is de-
termined by the negative dissimilarity measures of
the learned embeddings: ?d(e
i
, r
k
, e
?
j
) = ??e
i
+
r
k
? e
?
j
?
2
2
.
We evaluate the performance using mean aver-
age precision (MAP), which is a robust and sta-
ble metric (Manning et al., 2008). As can be
observed in Table 2 (left), TRESCAL achieves
54.79%, which outperforms 51.59% of RESCAL
and 51.41% of TransE. Adding constraints during
test time by assigning the lowest score to the en-
tity triples with incompatible types improves re-
sults of all models ? TRESCAL still performs the
best (69.26%), compared to TransE (67.56%) and
RESCAL (62.91%).
Relation Retrieval In the second task, given a
relation type r
k
, we are looking for the entity pairs
(e
i
, e
j
) that have this specific relationship. To gen-
erate test data, for each relation type, we collect
is included in Table 2.
7
We also tested the released code from (Socher et al.,
2013) for training a neural tensor network model. However,
we are not able to finish the experiments as each iteration of
this method takes almost 5 hours.
1574
gold entity pairs from the NELL knowledge base
as positive samples and randomly pick a set of en-
tity pairs as negative samples such that the number
of positive samples are the same as negative ones.
Results presented in Table 2 (right) show that
TRESCAL achieves 76.12%, while RESCAL and
TransE are 73.15% and 75.88%, respectively.
Therefore, incorporating the type information in
training seems to help in this task as well. Enforc-
ing the type constraints during test time does not
help as in entity retrieval. By removing incom-
patible entity pairs, the performance of TRESCAL,
RESCAL and TransE drop slightly to 75.70%,
73.08% and 70.71% respectively. One possible
explanation is that the task of relation retrieval is
easier than entity retrieval. The incorrect type in-
formation of some entities ends up filtering out a
small number of entity pairs that were retrieved
correctly by the model.
Notice that TRESCAL achieves different levels
of performance on various relations. For example,
it performs well on predicting AthletePlaysSport
(81%) and CoachesInLeague (88%), but achieves
suboptimal performance on predicting Works-
For (49%) and BuildingLocatedInCity (35%).
We hypothesize that it is easier to gener-
alize entity-relation triples when the relation
has several related relations. For examples,
AthletePlaysForTeam and TeamPlaysSport may
help discover entity-relation triples of Ath-
letePlaysSport.
5.1.3 Sensitivity to Parameters
We also study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?,
where the detailed results can be found in Ap-
pendix B. In short, we found that increasing the
rank r generally leads to better models. Also,
while the model is not very sensitive to the value
of the regularization parameter ?, tuning ? is still
necessary for achieving the best performance.
5.2 Relation Extraction
Next, we apply TRESCAL to the task of extract-
ing relations between entities, jointly from a text
corpus and a structured knowledge base. We use
a corpus from (Riedel et al., 2013) that is cre-
ated by aligning the entities in NYTimes and Free-
base. The corpus consists of a training set and a
test set. In the training set, a list of entity pairs
are provided, along with surface patterns extracted
from NYTimes and known relations obtained from
Freebase. In the test set, only the surface patterns
are given. By jointly factoring a matrix consist-
ing of the surface patterns and relations, Riedel et
al. (2013) show that their model is able to capture
the mapping between the surface patterns and the
structured relations and hence is able to extract the
entity relations from free text. In the following, we
show that TRESCAL can be applied to this task.
We focus on the 19 relations listed in Table 1
of (Riedel et al., 2013) and only consider the
surface patterns that co-occur with these 19 re-
lations. We prune the surface patterns that oc-
cur less than 5 times and remove the entities that
are not involved in any relation and surface pat-
tern. Based on the training and test sets, we
build a 80,698?80,698?1,652 tensor, where each
slice captures a particular structured relation or a
surface pattern between two entities. There are
72 fine types extracted from Freebase assigned
to 53,836 entities that are recorded in Freebase.
In addition, special types, PER, LOC, ORG and
MISC, are assigned to the remaining 26,862 enti-
ties based on the predicted NER tags provided by
the corpus. A type is considered incompatible to a
relation or a surface pattern if in the training data,
none of the argument entities of the relation be-
longs to the type. We use r = 400 and ? = 0.1 in
TRESCAL to factorize the tensor.
We compare the proposed TRESCAL model to
RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011),
MI09 (Mintz et al., 2009) and SU12 (Surdeanu et
al., 2012)
8
. We follow the protocol used in (Riedel
et al., 2013) to evaluate the results. Given a re-
lation as query, the top 1,000 entity pairs output
by each system are collected and the top 100 ones
are judged manually. Besides comparing individ-
ual models, we also report the results of combined
models. To combine the scores from two models,
we simply normalize the scores of entity-relation
tuples to zero mean and unit variance and take the
average. The results are summarized in Table 3.
As can been seen in the table, using TRESCAL
alone is not very effective and its performance is
only compatible to MI09 and YA11, and is sig-
nificantly inferior to RI13. This is understandable
because the problem setting favors RI13 as only
entity pairs that have occurred in the text or the
database will be considered in RI13, both during
model training and testing. In contrast, TRESCAL
8
The corpus and the system outputs are from http://
www.riedelcastro.org/uschema
1575
Relation # MI09 YA11 SU12 RI13 TR TR+SU12 TR+RI13
person/company 171 0.41 0.40 0.43 0.49 0.43 0.53 0.64
location/containedby 90 0.39 0.43 0.44 0.56 0.23 0.46 0.58
parent/child 47 0.05 0.10 0.25 0.31 0.19 0.24 0.35
person/place of birth 43 0.32 0.31 0.34 0.37 0.50 0.61 0.66
person/nationality 38 0.10 0.30 0.09 0.16 0.13 0.16 0.22
author/works written 28 0.52 0.53 0.54 0.71 0.00 0.39 0.62
person/place of death 26 0.58 0.58 0.63 0.63 0.54 0.72 0.89
neighborhood/neighborhood of 13 0.00 0.00 0.08 0.67 0.08 0.13 0.73
person/parents 8 0.21 0.24 0.51 0.34 0.01 0.16 0.38
company/founders 7 0.14 0.14 0.30 0.39 0.06 0.17 0.44
film/directed by 4 0.06 0.15 0.25 0.30 0.03 0.13 0.35
sports team/league 4 0.00 0.43 0.18 0.63 0.50 0.29 0.63
team/arena stadium 3 0.00 0.06 0.06 0.08 0.00 0.04 0.09
team owner/teams owned 2 0.00 0.50 0.70 0.75 0.00 0.00 0.75
roadcast/area served 2 1.00 0.50 1.00 1.00 0.50 0.83 1.00
structure/architect 2 0.00 0.00 1.00 1.00 0.00 0.02 1.00
composer/compositions 2 0.00 0.00 0.00 0.12 0.00 0.00 0.12
person/religion 1 0.00 1.00 1.00 1.00 0.00 1.00 1.00
film/produced by 1 1.00 1.00 1.00 0.33 0.00 1.00 0.25
Weighted MAP 0.33 0.36 0.39 0.47 0.30 0.44 0.57
Table 3: Weighted Mean Average Precisions. The # column shows the number of true facts in the pool.
Bold faced are winners per relation, italics indicate ties based on a sign test.
predicts all the possible combinations between en-
tities and relations, which makes the model less fit
to the task. However, when combining TRESCAL
with a pure text-based method, such as SU12,
we can clearly see TRESCAL is complementary
to SU12 (0.39 to 0.44 in weighted MAP score),
which makes the results competitive to RI13.
Interestingly, although both TRESCAL and RI13
leverage information from the knowledge base, we
find that by combining them, the performance is
improved quite substantially (0.47 to 0.57). We
suspect that the reason is that in our construc-
tion, each entity has its own vector representa-
tion, which is lacked in RI13. As a result, the
new triples that TRESCAL finds are very different
from those found by RI13. Nevertheless, com-
bining more methods do not always yield an im-
provement. For example, combining TR, RI13 and
SU12 together (not included in Table 3) achieves
almost the same performance as TR+RI13.
6 Conclusions
In this paper we developed TRESCAL, a tensor
decomposition method that leverages relational
domain knowledge. We use relational domain
knowledge to capture which triples are potentially
valid and found that, by excluding the triples that
are incompatible when performing tensor decom-
position, we can significantly reduce the train-
ing time and improve the prediction performance
as compared with RESCAL and TransE. More-
over, we demonstrated its effectiveness in the ap-
plication of relation extraction. Evaluated on the
dataset provided in (Riedel et al., 2013), the per-
formance of TRESCAL alone is comparable to sev-
eral existing systems that leverage the idea of dis-
tant supervision. When combined with the state-
of-the-art systems, we found that the results can
be further improved. For instance, the weighted
mean average precision of the previous best ap-
proach in (Riedel et al., 2013) has been increased
by 10 points (47% to 57%).
There are a number of interesting potential ex-
tensions of our work. First, while the experiments
in this paper are on traditional knowledge bases
and textual data, the idea of leveraging relational
domain knowledge is likely to be of value to other
linguistic databases as well. For instance, part-of-
speech tags can be viewed as the ?types? of words.
Incorporating such information in other tensor de-
composition methods (e.g., (Chang et al., 2013))
may help lexical semantic representations. Sec-
ond, relational domain knowledge goes beyond
entity types and their compatibility with specific
relations. For instance, the entity-relation triple
(e
1
, child-of, e
2
) can be valid only if e
1
.type =
person ? e
2
.type = person ? e
1
.age < e
2
.age.
It would be interesting to explore the possibility
of developing efficient methods to leverage other
types of relational domain knowledge. Finally, we
would like to create more sophisticated models of
knowledge base embedding, targeting complex in-
1576
ference tasks to better support semantic parsing
and question answering.
Acknowledgments
We thank Sebastian Riedel for providing the data
for experiments. We are also grateful to the anony-
mous reviewers for their valuable comments.
References
Brett W Bader, Richard A Harshman, and Tamara G
Kolda. 2007. Temporal analysis of semantic graphs
using ASALSAN. In ICDM, pages 33?42. IEEE.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan-
uary.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Joanna Biega, Erdal Kuzey, and Fabian M Suchanek.
2013. Inside YOGO2s: a transparent information
extraction architecture. In WWW, pages 325?328.
A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,
and O. Yakhnenko. 2013a. Translating Embeddings
for Modeling Multi-relational Data. In Advances in
Neural Information Processing Systems 26.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2013b. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, pages 1?27.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 576?583, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM journal on Matrix Analysis and Appli-
cations, 21(4):1253?1278.
Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. 2009. Triplerank: Ranking semantic web
data by tensor decomposition. In The Semantic Web-
ISWC 2009, pages 213?228. Springer.
U Kang, Evangelos Papalexakis, Abhay Harpale, and
Christos Faloutsos. 2012. Gigatensor: scaling ten-
sor analysis up by 100 times-algorithms and discov-
eries. In KDD, pages 316?324. ACM.
Henk AL Kiers. 2000. Towards a standardized nota-
tion and terminology in multiway analysis. Journal
of chemometrics, 14(3):105?122.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Joseph B Kruskal. 1977. Three-way arrays: rank and
uniqueness of trilinear decompositions, with appli-
cation to arithmetic complexity and statistics. Lin-
ear algebra and its applications, 18(2):95?138.
Alan J Laub, 2005. Matrix analysis for scientists and
engineers, chapter 13, pages 139?150. SIAM.
Ben London, Theodoros Rekatsinas, Bert Huang, and
Lise Getoor. 2013. Multi-relational learning using
weighted tensor decomposition with modular loss.
Technical report, University of Maryland College
Park. http://arxiv.org/abs/1303.1733.
C. Manning, P. Raghavan, and H. Schutze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013a. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Systems
26.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML, pages
809?816.
1577
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: scalable ma-
chine learning for linked data. In WWW, pages 271?
280.
Evangelos E Papalexakis, Tom M Mitchell, Nicholas D
Sidiropoulos, Christos Faloutsos, Partha Pratim
Talukdar, and Brian Murphy. 2014. Turbo-smt:
Accelerating coupled sparse matrix-tensor factoriza-
tions by 200x. In SDM.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML/PKDD
2010. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL, pages 74?84.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics,
1:367?378, October.
Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 650?658. ACM.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In Advances in Neural Information Processing Sys-
tems 26.
Robert Speer, Catherine Havasi, and Henry Lieberman.
2008. Analogyspace: Reducing the dimensionality
of common sense knowledge. In AAAI, pages 548?
553.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan
Salakhutdinov. 2009. Modelling relational data us-
ing Bayesian clustered tensor factorization. In NIPS,
pages 1821?1828.
Ledyard R Tucker. 1966. Some mathematical notes
on three-mode factor analysis. Psychometrika,
31(3):279?311.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366?1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456?1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Appendix A Detailed Derivation
We first introduce some lemmas that will be useful
for our derivation. Lemmas 2, 3 and 4 are the basic
properties of the Kronecker product. Their proofs
can be found at (Laub, 2005).
Lemma 1. Let V be an orthogonal matrix and
? a diagonal matrix. Then (I + V?V
T
)
?1
=
V(I + ?)
?1
V
T
.
Proof.
(I + V?V
T
)
?1
= (VIV
T
+ V?V
T
)
?1
= V(I + ?)
?1
V
T
Lemma 2. (A?B)(C?D) = AC?BD.
Lemma 3. (A?B)
T
= A
T
?B
T
.
Lemma 4. If A and B are orthogonal matrices,
then A?B will also be an orthogonal matrix.
Let Z = A ? A and apply singular value
decomposition to A = U?V
T
. The term
(
Z
T
Z + ?I
)
?1
can be rewritten as:
(
Z
T
Z + ?I
)
?1
=
(
?I + (A
T
?A
T
)(A?A)
)
?1
(6)
=
(
?I + A
T
A?A
T
A
)
?1
(7)
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
(8)
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
(9)
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
(10)
Eq. (6) is from replacing Z with A ? A and
Lemma 3. Eq. (7) is from Lemma 2. Eq. (8) is
from the properties of SVD, where U and V are
orthonormal matrices. Eq. (9) is from Lemma 2
and Lemma 3. Finally, Eq. (10) comes from
Lemma 1.
1578
Figure 3: Prediction performance of TRESCAL
and RESCAL with different rank (r).
Figure 4: Prediction performance of TRESCAL
with different regularization parameter (?).
Appendix B Hyper-parameter Sensitivity
We study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?.
We use the task of relation retrieval and present
the model performance on the development set.
Fig. 3 shows the performance of TRESCAL and
RESCAL with different rank (r) values while fix-
ing ? = 0.01. Results show that both TRESCAL
and RESCAL achieve better performance when r
is reasonably large. TRESCAL obtains a bet-
ter model with smaller r than RESCAL, because
TRESCAL only needs to fit the triples of the com-
patible entity types. Therefore, it allows to use
smaller number of latent variables to fit the train-
ing data.
Fixing r = 400, Fig. 4 shows the performance
of TRESCAL at different values of the regulariza-
tion parameter ?, including no regularization at
all (? = 0). While the results suggest that the
method is not very sensitive to ?, tuning ? is still
necessary for achieving the best performance.
1579
Proceedings of NAACL-HLT 2013, pages 1000?1009,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Combining Heterogeneous Models for Measuring Relational Similarity
Alisa Zhila?
Instituto Politecnico Nacional
Mexico City, Mexico
alisa.zhila@gmail.com
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Geoffrey Zweig
Microsoft Research
Redmond, WA 98052, USA
gzweig@microsoft.com
Tomas Mikolov?
BRNO University of Technology
BRNO, Czech Republic
tmikolov@gmail.com
Abstract
In this work, we study the problem of mea-
suring relational similarity between two word
pairs (e.g., silverware:fork and clothing:shirt).
Due to the large number of possible relations,
we argue that it is important to combine mul-
tiple models based on heterogeneous informa-
tion sources. Our overall system consists of
two novel general-purpose relational similar-
ity models and three specific word relation
models. When evaluated in the setting of a
recently proposed SemEval-2012 task, our ap-
proach outperforms the previous best system
substantially, achieving a 54.1% relative in-
crease in Spearman?s rank correlation.
1 Introduction
The problem of measuring relational similarity is
to determine the degree of correspondence between
two word pairs. For instance, the analogous word
pairs silverware:fork and clothing:shirt both exem-
plify well a Class-Inclusion:Singular Collective re-
lation and thus have high relational similarity. Un-
like the problem of attributional similarity, which
measures whether two words share similar attributes
and is addressed in extensive research work (Bu-
danitsky and Hirst, 2006; Reisinger and Mooney,
2010; Radinsky et al, 2011; Agirre et al, 2009; Yih
and Qazvinian, 2012), measuring relational similar-
ity is a relatively new research direction pioneered
by Turney (2006), but with many potential appli-
cations. For instance, problems of identifying spe-
cific relations between words, such as synonyms,
?Work conducted while interning at Microsoft Research.
antonyms or associations, can be reduced to mea-
suring relational similarity compared to prototypical
word pairs with the desired relation (Turney, 2008).
In scenarios like information extraction or question
answering, where identifying the existence of cer-
tain relations is often the core problem, measuring
relational similarity provides a more flexible solu-
tion rather than creating relational classifiers for pre-
defined or task-specific categories of relations (Tur-
ney, 2006; Jurgens et al, 2012).
In order to promote this research direction, Ju-
rgens et al (2012) proposed a new shared task of
measuring relational similarity in SemEval-2012 re-
cently. In this task, each submitted system is re-
quired to judge the degree of a target word pair
having a particular relation, measured by its re-
lational similarity compared to a few prototypical
example word pairs. The system performance is
evaluated by its correlation with the human judg-
ments using two evaluation metrics, Spearman?s
rank correlation and MaxDiff accuracy (more de-
tails of the task and evaluation metrics will be given
in Sec. 3). Although participating systems incorpo-
rated substantial amounts of information from lex-
ical resources (e.g., WordNet) and contextual pat-
terns from large corpora, only one system (Rink and
Harabagiu, 2012) is able to outperform a simple
baseline that uses PMI (pointwise mutual informa-
tion) scoring, which demonstrates the difficulty of
this task.
In this paper, we explore the problem of mea-
suring relational similarity in the same task setting.
We argue that due to the large number of possible
relations, building an ensemble of relational simi-
1000
larity models based on heterogeneous information
sources is the key to advance the state-of-the-art on
this problem. By combining two general-purpose re-
lational similarity models with three specific word-
relation models covering relations like IsA and syn-
onymy/antonymy, we improve the previous state-
of-the-art substantially ? having a relative gain of
54.1% in Spearman?s rank correlation and 14.7% in
the MaxDiff accuracy!
Our main contributions are threefold. First, we
propose a novel directional similarity method based
on the vector representation of words learned from
a recurrent neural network language model. The re-
lation of two words is captured by their vector off-
set in the latent semantic space. Similarity of rela-
tions can then be naturally measured by a distance
function in the vector space. This method alone
already performs better than all existing systems.
Second, unlike the previous finding, where SVMs
learn a much poorer model than naive Bayes (Rink
and Harabagiu, 2012), we show that using a highly-
regularized log-linear model on simple contextual
pattern features collected from a document collec-
tion of 20GB, a discriminative approach can learn a
strong model as well. Third, we demonstrate that by
augmenting existing word-relation models, which
cover only a small number of relations, the overall
system can be further improved.
The rest of this paper is organized as follows. We
first survey the related work in Sec. 2 and formally
define the problem in Sec. 3. We describe the indi-
vidual models in detail in Sec. 4. The combination
approach is depicted in Sec. 5, along with experi-
mental comparisons to individual models and exist-
ing systems. Finally, Sec. 6 concludes the paper.
2 Related Work
Building a classifier to determine whether a relation-
ship holds between a pair of words is a natural ap-
proach to the task of measuring relational similarity.
While early work was mostly based on hand-crafted
rules (Finin, 1980; Vanderwende, 1994), Rosario
and Hearst (2001) introduced a machine learning ap-
proach to classify word pairs. They targeted clas-
sifying noun modifier pairs from the medical do-
main into 13 classes of semantic relations. Fea-
tures for each noun modifier pair were constructed
using large medical lexical resources and a multi-
class classifier was trained using a feed-forward neu-
ral network with one hidden layer. This work was
later extended by Nastase and Szpakowicz (2003)
to classify general domain noun-modifier pairs into
30 semantic relations. In addition to extracting fea-
tures using WordNet and Roget?s Thesaurus, they
also experimented with several different learners in-
cluding decision trees, memory-based learning and
inductive logic programming methods like RIPPER
and FOIL. Using the same dataset as in (Nastase
and Szpakowicz, 2003), Turney and Littman (2005)
created a 128-dimentional feature vector for each
word pair based on statistics of their co-occurrence
patterns in Web documents and applied the k-NN
method (k = 1 in their work).
Measuring relational similarity, which determines
whether two word pairs share the same relation, can
be viewed as an extension of classifying relations
between two words. Treating a relational similar-
ity measure as a distance metric, a testing pair of
words can be judged by whether they have a rela-
tion that is similar to some prototypical word pairs
having a particular relation. A multi-relation clas-
sifier can thus be built easily in this framework as
demonstrated in (Turney, 2008), where the prob-
lems of identifying synonyms, antonyms and asso-
ciated words are all reduced to finding good anal-
ogous word pairs. Measuring relational similarity
has been advocated and pioneered by Turney (2006),
who proposed a latent vector space model for an-
swering SAT analogy questions (e.g., mason:stone
vs. carpenter:wood). In contrast, we take a slightly
different view when building a relational similarity
measure. Existing classifiers for specific word re-
lations (e.g., synonyms or Is-A) are combined with
general relational similarity measures. Empirically,
mixing heterogeneous models tends to make the fi-
nal relational similarity measure more robust.
Although datasets for semantic relation classifica-
tion or SAT analogous questions can be used to eval-
uate a relational similarity model, their labels are ei-
ther binary or categorical, which makes the datasets
suboptimal for determining the quality of a model
when evaluated on instances of the same relation
class. As a result, Jurgens et al (2012) proposed a
new task of ?Measuring Degrees of Relational Simi-
larity? at SemEval-2012, which includes 79 relation
1001
categories exemplified by three or four prototypical
word pairs and a schematic description. For exam-
ple, for the Class-Inclusion:Taxonomic relation, the
schematic description is ?Y is a kind/type/instance
of X?. Using Amazon Mechanical Turk1, they col-
lected word pairs for each relation, as well as their
degrees of being a good representative of a partic-
ular relation when compared with defining exam-
ples. Participants of this shared task proposed var-
ious kinds of approaches that leverage both lexical
resources and general corpora. For instance, the
Duluth systems (Pedersen, 2012) created word vec-
tors based on WordNet and estimated the degree of
a relation using cosine similarity. The BUAP sys-
tem (Tovar et al, 2012) represented each word pair
as a whole by a vector of 4 different types of fea-
tures: context, WordNet, POS tags and the aver-
age number of words separating the two words in
text. The degree of relation was then determined
by the cosine distance of the target pair from the
prototypical examples of each relation. Although
their models incorporated a significant amount of
information of words or word pairs, unfortunately,
the performance were not much better than a ran-
dom baseline, which indicates the difficulty of this
task. In comparison, a supervised learning approach
seems more promising. The UTD system (Rink and
Harabagiu, 2012), which mined lexical patterns be-
tween co-occurring words in the corpus and then
used them as features to train a Naive Bayes classi-
fier, achieved the best results. However, potentially
due to the large feature space, this strategy did not
work as well when switching the learning algorithm
to SVMs.
3 Problem Definition & Task Description
Following the setting of SemEval-2012 Task 2 (Ju-
rgens et al, 2012), the problem of measur-
ing the degree of relational similarity is to rate
word pairs by the degree to which they are
prototypical members of a given relation class.
For instance, comparing to the prototypical word
pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of
the Class-Inclusion:Singular Collective relation, we
would like to know among the input word pairs
{dish:bowl, book:novel, furniture:desk}, which one
1http://www.mturk.com
best demonstrates the relation.
Because our approaches are evaluated using the
data provided in this SemEval-2012 task, we de-
scribe briefly below how the data was collected, as
well as the metrics used to evaluate system perfor-
mance. The dataset consists of 79 relation classes
that are chosen according to (Bejar et al, 1991)
and broadly fall into 10 main categories, includ-
ing Class-Inclusion, Part-Whole, Similar and more.
With the help of Amazon Mechanical Turk, Jurgens
et al (2012) used a two-phase approach to collect
word pairs and their degrees. In the first phase,
a lexical schema, such as ?a Y is one item in a
collection/group of X? for the aforementioned rela-
tion Class-Inclusion:Singular Collective, and a few
prototypical pairs for each class were given to the
workers, who were asked to provide approximately
a list of 40 word pairs representing the same rela-
tion class. Naturally, some of these pairs were bet-
ter examples than the others. Therefore, in the sec-
ond phase, the goal was to measure the degree of
their similarity to the corresponding relation. This
was done using the MaxDiff technique (Louviere
and Woodworth, 1991). For each relation, about one
hundred questions were first created. Each question
consists of four different word pairs randomly sam-
pled from the list. The worker was then asked to
choose the most and least representative word pairs
for the specific relation in each question.
The set of 79 word relations were randomly split
into training and testing sets. The former contains
10 relations and the latter has 69. Word pairs in all
79 relations were given to the task participants in ad-
vance, but only the human judgments of the training
set were available for system development. In this
work, we treat the training set as the validation set
? all the model exploration and refinement is done
using this set of data, as well as the hyper-parameter
tuning when learning the final model combination.
The quality of a relational similarity measure is
estimated by its correlation to human judgments.
This is evaluated using two metrics in the task: the
MaxDiff accuracy and Spearman?s rank correlation
coefficient (?). A system is first asked to pick the
most and least representative word pairs of each
question in the MaxDiff setting. The average accu-
racy of the predictions compared to the human an-
swers is then reported. In contrast, Spearman?s ?
1002
measures the correlation between the total orderings
of all word pairs of a relation, where the total order-
ing is derived from the MaxDiff answers (see (Jur-
gens et al, 2012) for the exact procedure).
4 Models for Relational Similarity
We investigate three types of models for relational
similarity. Operating in a word vector space, the di-
rectional similarity model compares the vector dif-
ferences of target and prototypical word pairs to es-
timate their relational similarity. The lexical pat-
tern method collects contextual information of pairs
of words when they co-occur in large corpora, and
learns a highly regularized log-linear model. Finally,
the word relation models incorporate existing, spe-
cific word relation measures for general relational
similarity.
4.1 Directional Similarity Model
Our first model for relational similarity extends pre-
vious work on semantic word vector representa-
tions to a directional similarity model for pairs of
words. There are many different methods for cre-
ating real-valued semantic word vectors, such as
the distributed representation derived from a word
co-occurrence matrix and a low-rank approxima-
tion (Landauer et al, 1998), word clustering (Brown
et al, 1992) and neural-network language model-
ing (Bengio et al, 2003; Mikolov et al, 2010). Each
element in the vectors conceptually represents some
latent topicality information of the word. The goal
of these methods is that words with similar mean-
ings will tend to be close to each other in the vector
space.
Although the vector representation of single
words has been successfully applied to problems
like semantic word similarity and text classifica-
tion (Turian et al, 2010), the issue of how to repre-
sent and compare pairs of words in a vector space
remains unclear (Turney, 2012). In a companion
paper (Mikolov et al, 2013), we present a vector
offset method which performs consistently well in
identifying both syntactic and semantic regularities.
This method measures the degree of the analogy
?a is to b as c is to d? using the cosine score of
(~vb?~va +~vc, ~vd), where a, b, c, d are the four given
words and ~va, ~vb, ~vc, ~vd are the corresponding vec-
q 
shirt
clothing
furniture
desk
v1
v2'
v2'
Figure 1: Directional vectors ?1 and ?2 capture the rela-
tions of clothing:shirt and furniture:desk respectively in
this semantic vector space. The relational similarity of
these two word pairs is estimated by the cosine of ?.
tors. In this paper, we propose a variant called the
directional similarity model, which performs bet-
ter for semantic relations. Let ?i = (wi1 , wi2) and
?j = (wj1 , wj2) be the two word pairs being com-
pared. Suppose (~vi1 , ~vi2) and (~vj1 , ~vj2) are the cor-
responding vectors of these words. The directional
vectors of ?i and ?j are defined as ~?i ? ~vi2 ? ~vi1
and ~?j ? ~vj2 ? ~vj1 , respectively. Relational simi-
larity of these two word pairs can be measured by
some distance function of ?i and ?j , such as the co-
sine function:
~?i ? ~?j
?~?i??~?j?
The rationale behind this variant is as follows. Be-
cause the difference of two word vectors reveals the
change from one word to the other in terms of mul-
tiple topicality dimensions in the vector space, two
word pairs having similar offsets (i.e., being rela-
tively parallel) can be interpreted as they have simi-
lar relations. Fig. 1 further illustrates this method.
Compared to the original method, this variant
places less emphasis on the similarity between
words wj1 and wj2 . That similarity is necessary
for syntactic relations where the words are often re-
lated by morphology, but not for semantic relations.
On semantic relations studied in this paper, the di-
rectional similarity model performs about 18% rela-
tively better in Spearman?s ? than the original one.
The quality of the directional similarity method
depends heavily on the underlying word vector
space model. We compared two choices with dif-
1003
Word Embedding Spearman?s ? MaxDiff Acc. (%)
LSA-80 0.055 34.6
LSA-320 0.066 34.4
LSA-640 0.102 35.7
RNNLM-80 0.168 37.5
RNNLM-320 0.214 39.1
RNNLM-640 0.221 39.2
RNNLM-1600 0.234 41.2
Table 1: Results of measuring relational similarity using
the directional similarity method, evaluated on the train-
ing set. The 1600-dimensional RNNLM vector space
achieves the highest Spearman?s ? and MaxDiff accuracy.
ferent dimensionality settings: the word embedding
learned from the recurrent neural network language
model (RNNLM)2 and the LSA vectors, both were
trained using the same Broadcast News corpus of
320M words as described in (Mikolov et al, 2011).
All the word vectors were first normalized to unit
vectors before applying the directional similarity
method. Given a target word pair, we computed
its relational similarity compared with the prototyp-
ical word pairs of the same relation. The average
of these measurements was taken as the final model
score. Table 1 summarizes the results when evalu-
ated on the training set. As shown in the table, the
RNNLM vectors consistently outperform their LSA
counterparts with the same dimensionality. In addi-
tion, more dimensions seem to preserve more infor-
mation and lead to better performance. Therefore,
we take the 1600-dimensional RNNLM vectors to
construct our final directional similarity model.
4.2 Lexical Pattern Model
Our second model for measuring relational similar-
ity is built based on lexical patterns. It is well-known
that contexts in which two words co-occur often pro-
vide useful cues for identifying the word relation.
For example, having observed frequent text frag-
ments like ?X such as Y?, it is likely that there is a
Class-Inclusion:Taxonomic relation between X and
Y; namely, Y is a type of X. Indeed, by mining lexical
patterns from a large corpus, the UTD system (Rink
and Harabagiu, 2012) managed to outperform other
participants in the SemEval-2012 task of measuring
relational similarity.
2http://www.fit.vutbr.cz/?imikolov/rnnlm
In order to find more co-occurrences of each pair
of words, we used a large document set that con-
sists of the Gigaword corpus (Parker et al, 2009),
Wikipedia and LA Times articles3, summing up to
more than 20 Gigabytes of texts. For each word
pair (w1, w2) that co-occur in a sentence, we col-
lected the words in between as its context (or so-
called ?raw pattern?). For instance, ?such as? would
be the context extracted from ?X such as Y? for
the word pair (X, Y). To reduce noise, contexts with
more than 9 words were dropped and 914,295 pat-
terns were collected in total.
Treating each raw pattern as a feature where the
value is the logarithm of the occurrence count, we
then built a probabilistic classifier to determine the
association of the context and relation. For each re-
lation, we treated all its word pairs as positive ex-
amples and all the word pairs in other relations as
negative examples4. 79 classifiers were trained in
total, where each one was trained using 3,218 ex-
amples. The degree of relational similarity of each
word pair can then be judged by the output of the
corresponding classifier5. Although this seems like a
standard supervised learning setting, the large num-
ber of features poses a challenge here. Using almost
1M features and 3,218 examples, the model could
easily overfit if not regularized properly, which may
explain why learning SVMs on pattern features per-
formed poorly (Rink and Harabagiu, 2012). In-
stead of employing explicit feature selection meth-
ods, we used an efficient L1 regularized log-linear
model learner (Andrew and Gao, 2007) and chose
the hyper-parameters based on model performance
on the training data. The final models we chose
were trained with L1 = 3, where 28,065 features
in average were selected automatically by the algo-
3We used a Nov-2010 dump of English Wikipedia, which
contains approximately 917M words after pre-processing. The
LA Times corpus consists of articles from 1985 to 2002 and has
about 1.1B words.
4Given that not all word pairs belonging to the same relation
category are equally good, removing those with low judgment
scores may help improve the quality of the labeled data. We
leave this study to future work.
5Training a separate classifier for each MaxDiff question us-
ing all words pairs except the four target pairs appears to be a
better setting, as it would avoid including the target pairs in the
training process. We did not use this setting because it is more
complicated and performed roughly the same empirically.
1004
rithm. The performance on the training data is 0.322
in Spearman?s ? and 41.8% in MaxDiff accuracy.
4.3 Word Relation Models
The directional similarity and lexical pattern mod-
els can be viewed as general purpose methods for
relational similarity as they do not differentiate the
specific relation categories. In contrast, for specific
word relations, there exist several high-quality meth-
ods. Although they are designed for detecting spe-
cific relations between words, incorporating them
could still improve the overall results. Next, we ex-
plore the use of some of these word relation mod-
els, including information encoded in the knowledge
base and a lexical semantic model for synonymy and
antonymy.
4.3.1 Knowledge Bases
Predetermined types of relations can often be
found in existing lexical and knowledge databases,
such as WordNet?s Is-A taxonomy and the exten-
sive relations stored in the NELL (Carlson et al,
2010) knowledge base. Although in theory, these
resources can be directly used to solve the problem
of relational similarity, such direct approaches often
suffer from two practical issues. First, the word cov-
erage of these databases is usually very limited and
it is common that the relation of a given word pair
is absent. Second, the degree of relation is often not
included, which makes the task of measuring the de-
gree of relational similarity difficult.
One counter example, however, is Probase (Wu
et al, 2012), which is a knowledge base that es-
tablishes connections between more than 2.5 mil-
lion concepts discovered automatically from the
Web. For the Is-A and Attribute relations it en-
codes, Probase also returns the probability that two
input words share the relation, based on the co-
occurrence frequency. We used some relations in
the training set to evaluate the quality of Probase.
For instance, its Is-A model performs exception-
ally well on the relation Class-Inclusion:Taxonomic,
reaching a high Spearman?s ? = 0.642 and MaxD-
iff accuracy 55.8%. Similarly, its Attribute model
performs better than our lexical pattern model
on Attribute:Agent Attribute-State with Spearman?s
? = 0.290 and MaxDiff accuracy 32.7%.
4.3.2 Lexical Semantics Measures
Most lexical semantics measures focus on the se-
mantic similarity or relatedness of two words. Since
our task focuses on distinguishing the difference be-
tween word pairs in the same relation category. The
crude relatedness model does not seem to help in our
preliminary experimental study. Instead, we lever-
age the recently proposed polarity-inducing latent
semantic analysis (PILSA) model (Yih et al, 2012),
which specifically estimates the degree of synonymy
and antonymy. This method first forms a signed co-
occurrence matrix using synonyms and antonyms in
a thesaurus and then generalizes it using a low-rank
approximation derived by SVD. Given two words,
the cosine score of their PILSA vectors tend to be
negative if they are antonymous and positive if syn-
onymous. When tested on the Similar:Synonymity
relation, it has a Spearman?s ? = 0.242 and MaxD-
iff accuracy 42.1%, both are better than those of our
directional similarity and lexical pattern models.
5 Model Combination
In order to fully leverage the diverse models pro-
posed in Sec. 4, we experiment with a model combi-
nation approach and conduct a model ablation study.
Performance of the combined and individual models
is evaluated using the test set and compared with ex-
isting systems.
We seek an optimal linear combination of all the
individual models by treating their output as fea-
tures and use a logistic regression learner to learn
the weights6. The training setting is essentially the
same as the one used to learn the lexical pattern
model (Sec. 4.2). For each relation, we treat all the
word pairs in this relation group as positive exam-
ples and all other word pairs as negative ones. Con-
sequently, 79 sets of weights for model combination
are learned in total. The average Spearman?s ? of the
10 training relations is used for selecting the values
of the L1 and L2 regularizers7. Evaluated on the re-
maining 69 relations (i.e., the test set), the average
results of each main relation group and the overall
6Nonlinear methods, such as MART (Friedman, 2001), do
not perform better in our experiments (not reported here).
7We tested 15 combinations, where L1 ? {0, 0.01, 0.1} and
L2 ? {0, 0.001, 0.01, 1, 10}. The parameter setting that gave
the highest Spearman rank correlation coefficient score on the
training set was selected.
1005
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519
Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329
Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303
Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268
Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406
Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296
Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473
Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296
Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443
Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208
Average 0.018 0.014 0.050 0.229 0.324? 0.235 0.058? -0.010? -0.009? 0.353?
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2
Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9
Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3
Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8
Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3
Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6
Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6
Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7
Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7
Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5
Average 31.2 31.7 32.4 39.4 44.5? 39.2 33.3? 29.8? 30.7? 45.2?
Table 2: Average Spearman?s ? (Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69
testing relations. The best result in each row is highlighted in boldface font. Statistical significance tests are conducted
by comparing each of our systems with the previous best performing system, UTDNB . ? and ? indicate the difference
in the average results is statistically significant with 95% or 99% confidence level, respectively.
results are presented in Table 2. For comparison, we
also show the performance of a random baseline and
the best performing system of each participant in the
SemEval-2012 task.
We draw two conclusions from this table. First,
both of our general relational similarity models, the
directional similarity (DS) and lexical pattern (Pat)
models are fairly strong. The former outperforms
the previous best system UTDNB in both Spear-
man?s ? and MaxDiff accuracy, where the differ-
ences are statistically significant8; the latter has
comparable performance, where the differences are
not statistically significant. In contrast, while the
IsA relation from Probase is exceptionally good
in identifying Class-Inclusion relations, with high
Spearman?s ? = 0.619 and MaxDiff accuracy
8We conducted a paired-t test on the results of each of the
69 relation. The difference is considered statistically significant
if the p-value is less than 0.05.
59.6%, it does not have high correlations with hu-
man judgments in other relations. Like in the case of
Probase Attribute and PILSA, specific word-relation
models individually are not good measures for gen-
eral relational similarity. Second, as expected, com-
bining multiple diverse models (Com) is a robust
strategy, which provides the best overall perfor-
mance. It achieves superior results in both evalua-
tion metrics compared to UTDNB and only a lower
Spearman?s ? value in one of the ten relation groups
(namely, Reference). The differences are statisti-
cally significant with p-value less than 10?3.
In order to understand the interaction among dif-
ferent component models, we conducted an ablation
study by iteratively removing one model from the fi-
nal combination. The weights are re-trained using
the same procedure that finds the best regularization
parameters with the help of training data. Table 3
summarizes the results and compares them with the
1006
Spearman?s ? MaxDiff Accuracy (%)
Relation Group Com. -Attr -IsA -PILSA -DS -Pat Com. -Attr -IsA -PILSA -DS -Pat
Class-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2
Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9
Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9
Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1
Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0
Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0
Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2
Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1
Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5
Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4
Average 0.353 0.348 0.350 0.354 0.238? 0.329 45.2 45.0 44.9? 44.7 39.6? 45.4
Table 3: Average Spearman?s ? and MaxDiff accuracy results of different model combinations. Com indicates combin-
ing all models, where other columns show the results when the specified model is removed. The best result in each row
is highlighted in boldface font. Statistical significance tests are conducted by comparing each ablation configuration
with Com. ? indicates the difference in the average results is statistically significant with 99% confidence level.
original combination model.
Overall, it is clear that the directional similarity
method based on RNNLM vectors is the most crit-
ical component model. Removing it from the fi-
nal combination decreases both the Spearman?s ?
and MaxDiff accuracy by a large margin; both dif-
ferences (Com vs. -DS) are statistically significant.
The Probase IsA model also has an important im-
pact on the performance on the Class-Inclusion re-
lation group. Eliminating the IsA model makes
the overall MaxDiff accuracy statistically signifi-
cantly lower (Com vs. -IsA). Again, the benefits
of incorporating Probase Attribute and PILSA mod-
els are not clear. Removing them from the final
combination lowers the MaxDiff accuracy, but nei-
ther the difference in Spearman?s ? nor MaxDiff
accuracy is statistically significant. Compared to
the RNNLM directional similarity model, the lex-
ical pattern model seems less critical. Removing
it lowers the Similar and Contrast relation groups,
but improves some other relation groups like Class-
Inclusion and Case Relations. The final MaxDiff ac-
curacy becomes slightly higher but the Spearman?s
? drops a little (Com vs. -Pat); neither is statistically
significant.
Notice that the main purpose of the ablation study
is to verify the importance of an individual compo-
nent model when a significant performance drop is
observed after removing it. However, occasionally
the overall performance may go up slightly. Typi-
cally this is due to the fact that some models do not
provide useful signals to a particular relation, but in-
stead introduce more noise. Such effects can often
be alleviated when there are enough quality training
data, which is unfortunately not the case here.
6 Conclusions
In this paper, we presented a system that combines
heterogeneous models based on different informa-
tion sources for measuring relational similarity. Our
two individual general-purpose relational similarity
models, directional similarity and lexical pattern
methods, perform strongly when compared to ex-
isting systems. After incorporating specific word-
relation models, the final system sets a new state-of-
the-art on the SemEval-2012 task 2 test set, achiev-
ing Spearman?s ? = 0.353 and MaxDiff accuracy
45.4% ? resulting in 54.1% and 14.7% relative im-
provement in these two metrics, respectively.
Despite its simplicity, our directional similarity
approach provides a robust model for relational sim-
ilarity and is a critical component in the final sys-
tem. When the lexical pattern model is included, our
overall model combination method can be viewed
as a two-stage learning system. As demonstrated in
our work, with an appropriate regularization strat-
egy, high-quality models can be learned in both
stages. Finally, as we observe from the positive ef-
fect of adding the Probase IsA model, specific word-
relation models can further help improve the system
1007
although they tend to cover only a small number of
relations. Incorporating more such models could be
a steady path to enhance the final system.
In the future, we plan to pursue several research
directions. First, as shown in our experimental re-
sults, the model combination approach does not al-
ways outperform individual models. Investigating
how to select models to combine for each specific re-
lation or relation group individually will be our next
step for improving this work. Second, because the
labeling process of relational similarity comparisons
is inherently noisy, it is unrealistic to request a sys-
tem to correlate human judgments perfectly. Con-
ducting some user study to estimate the performance
ceiling in each relation category may help us focus
on the weaknesses of the final system to enhance
it. Third, it is intriguing to see that the directional
similarity model based on the RNNLM vectors per-
forms strongly, even though the RNNLM training
process is not related to the task of relational sim-
ilarity. Investigating the effects of different vector
space models and proposing some theoretical jus-
tifications are certainly interesting research topics.
Finally, we would like to evaluate the utility our ap-
proach in other applications, such as the SAT anal-
ogy problems proposed by Turney (2006) and ques-
tion answering.
Acknowledgments
We thank Richard Socher for valuable discussions,
Misha Bilenko for his technical advice and anony-
mous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML ?07.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cog-
nitive and psychometric analysis of analogical prob-
lem solving. Recent research in psychology. Springer-
Verlag.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
J.H. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist, 29(5):1189?
1232.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
degrees of relational similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 356?364, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045?1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
ASRU.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth edi-
tion. Technical report, Linguistic Data Consortium,
Philadelphia.
Ted Pedersen. 2012. Duluth: Measuring degrees of re-
lational similarity with the gloss vector measure of se-
mantic relatedness. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
1008
2012), pages 497?501, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-01, pages 82?90.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilarin?o, David Pinto, and Saul Leo?n. 2012.
BUAP: A first approximation to relational similarity
measuring. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 502?505, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
(JAIR), 44:533?585.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782?788.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.
Zhu. 2012. Probase: a probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481?492, May.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
1009
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601?610,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Computational Approaches to Sentence Completion
Geoffrey Zweig, John C. Platt
Christopher Meek
Christopher J.C. Burges
Microsoft Research
Redmond, WA 98052
Ainur Yessenalina
Cornell University
Computer Science Dept.
Ithaca, NY 14853
Qiang Liu
Univ. of California, Irvine
Info. & Comp. Sci.
Irvine, California 92697
Abstract
This paper studies the problem of sentence-
level semantic coherence by answering SAT-
style sentence completion questions. These
questions test the ability of algorithms to dis-
tinguish sense from nonsense based on a vari-
ety of sentence-level phenomena. We tackle
the problem with two approaches: methods
that use local lexical information, such as the
n-grams of a classical language model; and
methods that evaluate global coherence, such
as latent semantic analysis. We evaluate these
methods on a suite of practice SAT questions,
and on a recently released sentence comple-
tion task based on data taken from five Conan
Doyle novels. We find that by fusing local
and global information, we can exceed 50%
on this task (chance baseline is 20%), and we
suggest some avenues for further research.
1 Introduction
In recent years, standardized examinations have
proved a fertile source of evaluation data for lan-
guage processing tasks. They are valuable for many
reasons: they represent facets of language under-
standing recognized as important by educational ex-
perts; they are organized in various formats designed
to evaluate specific capabilities; they are yardsticks
by which society measures educational progress;
and they affect a large number of people.
Previous researchers have taken advantage of this
material to test both narrow and general language
processing capabilities. Among the narrower tasks,
the identification of synonyms and antonyms has
been studied by (Landauer and Dumais, 1997; Mo-
hammed et al, 2008; Mohammed et al, 2011; Tur-
ney et al, 2003; Turney, 2008), who used ques-
tions from the Test of English as a Foreign Lan-
guage (TOEFL), Graduate Record Exams (GRE)
and English as a Second Language (ESL) exams.
Tasks requiring broader competencies include logic
puzzles and reading comprehension. Logic puzzles
drawn from the Law School Administration Test
(LSAT) and the GRE were studied in (Lev et al,
2004), which combined an extensive array of tech-
niques to solve the problems. The DeepRead sys-
tem (Hirschman et al, 1999) initiated a long line of
research into reading comprehension based on test
prep material (Charniak et al, 2000; Riloff and The-
len, 2000; Wang et al, 2000; Ng et al, 2000).
In this paper, we study a new class of problems
intermediate in difficulty between the extremes of
synonym detection and general question answer-
ing - the sentence completion questions found on
the Scholastic Aptitude Test (SAT). These questions
present a sentence with one or two blanks that need
to be filled in. Five possible words (or short phrases)
are given as options for each blank. All possible an-
swers except one result in a nonsense sentence. Two
examples are shown in Figure 1.
The questions are highly constrained in the sense
that all the information necessary is present in the
sentence itself without any other context. Neverthe-
less, they vary widely in difficulty. The first of these
examples is relatively simple: the second half of the
sentence is a clear description of the type of behavior
characterized by the desired adjective. The second
example is more sophisticated; one must infer from
601
1. One of the characters in Milton Murayama?s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless .. effective
(B) mild .. benign
(C) curative .. toxic
(D) harmful .. fatal
(E) beneficial .. miraculous
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
the contrast between medicine and poison that the
correct answer involves a contrast, either useless vs.
effective or curative vs. toxic. Moreover, the first, in-
correct, possibility is perfectly acceptable in the con-
text of the second clause alone; only irrelevance to
the contrast between medicine and poison eliminates
it. In general, the questions require a combination of
semantic and world knowledge as well as occasional
logical reasoning. We study the sentence comple-
tion task because we believe it is complex enough to
pose a significant challenge, yet structured enough
that progress may be possible.
As a first step, we have approached the prob-
lem from two points-of-view: first by exploiting lo-
cal sentence structure, and secondly by measuring
a novel form of global sentence coherence based
on latent semantic analysis. To investigate the use-
fulness of local information, we evaluated n-gram
language model scores, from both a conventional
model with Good-Turing smoothing, and with a re-
cently proposed maximum-entropy class-based n-
gram model (Chen, 2009a; Chen, 2009b). Also
in the language modeling vein, but with potentially
global context, we evaluate the use of a recurrent
neural network language model. In all the language
modeling approaches, a model is used to compute a
sentence probability with each of the potential com-
pletions. To measure global coherence, we propose
a novel method based on latent semantic analysis
(LSA). We find that the LSA based method performs
best, and that both local and global information can
be combined to exceed 50% accuracy. We report re-
sults on a set of questions taken from a collection
of SAT practice exams (Princeton-Review, 2010),
and further validate the methods with the recently
proposed MSR Sentence Completion Challenge set
(Zweig and Burges, 2011).
Our paper thus makes the following contributions:
First, we present the first published results on the
SAT sentence completion task. Secondly, we eval-
uate the effectiveness of both local n-gram informa-
tion, and global coherence in the form of a novel
LSA-based metric. Finally, we illustrate that the lo-
cal and global information can be effectively fused.
The remainder of this paper is organized as fol-
lows. In Section 2 we discuss related work. Section
3 describes the language modeling methods we have
evaluated. Section 4 outlines the LSA-based meth-
ods. Section 5 presents our experimental results. We
conclude with a discussion in Section 6.
2 Related Work
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find alternates
for match in ?After the match, replace any remain-
ing fluid deficit to prevent problems of chronic de-
hydration throughout the tournament.? Two consis-
tently high-performing systems for this task are the
KU (Yuret, 2007) and UNT (Hassan et al, 2007)
systems. These operate in two phases: first they find
a set of potential replacement words, and then they
rank them. The KU system uses just an N-gram lan-
guage model to do this ranking. The UNT system
uses a large variety of information sources, and a
language model score receives the highest weight.
N-gram statistics were also very effective in (Giu-
liano et al, 2007). That paper also explores the use
of Latent Semantic Analysis to measure the degree
of similarity between a potential replacement and its
context, but the results are poorer than others. Since
the original word provides a strong hint as to the pos-
602
sible meanings of the replacements, we hypothesize
that N-gram statistics are largely able to resolve the
remaining ambiguities. The SAT sentence comple-
tion sentences do not have this property and thus are
more challenging.
Related to, but predating the Semeval lexical sub-
stitution task are the ESL synonym questions pro-
posed by Turney (2001), and subsequently consid-
ered by numerous research groups including Terra
and Clarke (2003) and Pado and Lapata (2007).
These questions are similar to the SemEval task, but
in addition to the original word and the sentence
context, the list of options is provided. Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance, which is 82%.
Other work on standardized tests includes the syn-
onym and antonym tasks mentioned in Section 1,
and more recent work on a SAT analogy task in-
troduced by (Turney et al, 2003) and extensively
used by other researchers (Veale, 2004; Turney and
Littman, 2005; D. et al, 2009).
3 Sentence Completion via Language
Modeling
Perhaps the most straightforward approach to solv-
ing the sentence completion task is to form the com-
plete sentence with each option in turn, and to eval-
uate its likelihood under a language model. As
discussed in Section 2, this was found be be very
effective in the ranking phase of several SemEval
systems. In this section, we describe the suite of
state-of-the-art language modeling techniques for
which we will present results. We begin with n-
gram models; first a classical n-gram backoff model
(Chen and Goodman, 1999), and then a recently pro-
posed class-based maximum-entropy n-gram model
(Chen, 2009a; Chen, 2009b). N-gram models have
the obvious disadvantage of using a very limited
context in predicting word probabilities. There-
fore we evaluate the recurrent neural net model of
(Mikolov et al, 2010; Mikolov et al, 2011b). This
model has produced record-breaking perplexity re-
sults in several tasks (Mikolov et al, 2011a), and has
the potential to encode sentence-span information in
the network hidden-layer activations. We have also
evaluated the use of parse scores, using an off-the-
shelf stochastic context free grammar parser. How-
ever, the grammatical structure of the alternatives is
often identical. With scores differing only in the fi-
nal non-terminal/terminal rewrites, this did little bet-
ter than chance. The use of other syntactically de-
rived features, for example based on a dependency
parse, are likely to be more effective, but we leave
this for future work.
3.1 Backoff N-gram Language Model
Our baseline model is a Good-Turing smoothed
model trained with the CMU language modeling
toolkit (Clarkson and Rosenfeld, 1997). For the SAT
task, we used a trigram language model trained on
1.1B words of newspaper data, described in Section
5.1. All bigrams occurring at least twice were re-
tained in the model, along with all trigrams occur-
ring at least three times. The vocabulary consisted
of all words occurring at least 100 times in the data,
along with every word in the development or test
sets. This resulted in a 124k word vocabulary and
59M n-grams. For the Conan Doyle data, which we
henceforth refer to as the Holmes data (see Section
5.1), the smaller amount of training data allowed us
to use 4-grams and a vocabulary cutoff of 3. This re-
sulted in 26M n-grams and a 126k word vocabulary.
3.2 Maximum Entropy Class-Based N-gram
Language Model
Word-class information provides a level of abstrac-
tion which is not available in a word-level lan-
guage model; therefore we evaluated a state-of-the-
art class based language model. Model M (Chen,
2009a; Chen, 2009b) is a recently proposed class
based exponential n-gram language model which
has shown improvements across a variety of tasks
(Chen, 2009b; Chen et al, 2009; Emami et al,
2010). The key ideas are the modeling of word n-
gram probabilities with a maximum entropy model,
and the use of word-class information in the defini-
tion of the features. In particular, each word w is
assigned deterministically to a class c, allowing the
n-gram probabilities to be estimated as the product
of class and word parts
P (wi|wi?n+1 . . . wi?2wi?1) =
P (ci|ci?n+1 . . . ci?2ci?1, wi?n+1 . . . wi?2wi?1)
P (wi|wi?n+1 . . . wi?2wi?1, ci).
603
Both components are themselves maximum entropy
n-gram models in which the probability of a word
or class label l given history h is determined by
1
Z exp(
?
k fk(h, l)). The features fk(h, l) used are
the presence of various patterns in the concatena-
tion of hl, for example whether a particular suffix
is present in hl.
3.3 Recurrent Neural Net Language Model
Many of the questions involve long-range depen-
dencies between words. While n-gram models have
no ability to explicitly maintain long-span context,
the recently proposed recurrent neural-net model of
(Mikolov et al, 2010) does. Related approaches
have been proposed by (Sutskever et al, 2011;
Socher et al, 2011). In this model, a set of neu-
ral net activations s(t) is maintained and updated at
each sentence position t. These activations encapsu-
late the sentence history up to the tth word in a real-
valued vector which typically has several hundred
dimensions. The word at position t is represented as
a binary vector w(t) whose length is the vocabulary
size, and with a ?1? in a position uniquely associated
with the word, and ?0? elsewhere. w(t) and s(t) are
concatenated to predict an output distribution over
words, y(t). Updating is done with two weight ma-
trices u and v and nonlinear functions f() and g()
(Mikolov et al, 2011b):
x(t) = [w(t)T s(t ? 1)T ]T
sj(t) = f(
?
i
xi(t)uji)
yk(t) = g(
?
j
sj(t)vkj)
with f() being a sigmoid and g() a softmax:
f(x) =
1
1 + exp(?z)
, g(zm) =
exp(zm)
?
k exp(zk)
The output y(t) is a probability distribution over
words, and the parameters u and v are trained with
back-propagation to minimize the Kullback-Leibler
(KL) divergence between the predicted and observed
distributions. Because of the recurrent connections,
this model is similar to a nonlinear infinite impulse
response (IIR) filter, and has the potential to model
long span dependencies. Theoretical considerations
(Bengio et al, 1994) indicate that for many prob-
lems, this may not be possible, but in practice it is
an empirical question.
4 Sentence Completion via Latent
Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used method for representing
words and documents in a low dimensional vector
space. The method is based on applying singular
value decomposition (SVD) to a matrix W repre-
senting the occurrence of words in documents. SVD
results in an approximation of W by the product
of three matrices, one in which each word is rep-
resented as a low-dimensional vector, one in which
each document is represented as a low dimensional
vector, and a diagonal scaling matrix. The simi-
larity between two words can then be quantified as
the cosine-similarity between their respective scaled
vectors, and document similarity can be measured
likewise. It has been used in numerous tasks, rang-
ing from information retrieval (Deerwester et al,
1990) to speech recognition (Bellegarda, 2000; Coc-
caro and Jurafsky, 1998).
To perform LSA, one proceeds as follows. The
input is a collection of n documents which are ex-
pressed in terms of words from a vocabulary of size
m. These documents may be actual documents such
as newspaper articles, or simply as in our case no-
tional documents such as sentences. Next, a m x n
matrix W is formed. At its simplest, the ijth entry
contains the number of times word i has occurred in
document j - its term frequency or TF value. More
conventionally, the entry is weighted by some no-
tion of the importance of word i, for example the
negative logarithm of the fraction of documents that
contain it, resulting in a TF-IDF weighting (Salton
et al, 1975). Finally, to obtain a subspace represen-
tation of dimension d, W is decomposed as
W ? USV T
where U is m x d, V T is d x n, and S is a d x d diag-
onal matrix. In applications, d << n and d << m;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the rows
of US - which represents the words - behave sim-
ilarly to the original rows of W , in the sense that
the cosine similarity between two rows in US ap-
proximates the cosine similarity between the corre-
604
sponding rows in W . Cosine similarity is defined as
sim(x,y) = x?y?x??y? .
4.1 Total Word Similarity
Perhaps the simplest way of doing sentence comple-
tion with LSA is to compute the total similarity of a
potential answer a with the rest of the words in the
sentence S, and to choose the most related option.
We define the total similarity as:
totsim(a,S) =
?
w?S
sim(a,w)
When the completion requires two words, total sim-
ilarity is the sum of the contributions for both words.
This is our baseline method for using LSA, and one
of the best methods we have found.
4.2 Sentence Reconstruction
Recall that LSA approximates a weighted word-
document matrix W as the product of low rank
matrices U and V along with a scaling matrix S:
W ? USV T . Using singular value decomposition,
this is done so as to minimize the mean square re-
construction error
?
ij Q
2
ij whereQ = W?USV
T .
From the basic definition of LSA, each column ofW
(representing a document) is represented as
Wj = USV Tj , (1)
that is, as a linear combination of the set of basis
functions formed by the columns of US, with the
combination weights specified in V Tj . When a new
document is presented, it is also possible to repre-
sent it in terms of the same basis vectors. Moreover,
we may take the reconstruction error induced by this
representation to be a measure of how consistent the
new document is with the original set of documents
used to determine U S and V (Bellegarda, 2000).
It remains to represent a new document in terms
of the LSA bases. This is done as follows (Deer-
wester et al, 1990; Bellegarda, 2000), again with
the objective of minimizing the reconstruction error.
First, note that since U is column-orthonormal, (1)
implies that
Vj = W Tj US
?1 (2)
Thus, if we notionally index a new document by p,
we proceed by forming a new column (document)
vector Wp using the standard term-weighting, and
then find its LSA-space representation Vp using (2).
We can evaluate the reconstruction quality by insert-
ing the result in (1). The reconstruction error is then
||(UUT ? I)Wp||2
Note that if all the dimensions are retained, the re-
construction error is zero; in the case that only the
highest singular vectors are used, however, it is not.
Due to the fact that the sentences vary in length we
choose the number of retained singular vectors as a
fraction f of the sentence length. If the answer has
n words we use the top nf components. In practice,
a f of 1.2 was selected on the basis of development
set results.
4.3 A LSA N-gram Language Model
In the context of speech recognition, LSA has been
combined with classical n-gram language models
in (Coccaro and Jurafsky, 1998; Bellegarda, 2000).
The crux of this idea is to interpolate an n-gram lan-
guage model probability with one based on LSA,
with the intuition that the standard n-gram model
will do a good job predicting function words, and
the LSA model will do a good job on words pre-
dicted by their long-span context. This logic makes
sense for the sentence completion task as well, mo-
tivating us to evaluate it.
To do this, we adopt the procedure of (Coccaro
and Jurafsky, 1998), using linear interpolation be-
tween the n-gram and LSA probabilities:
p(w|history) =
?png(w|history) + (1 ? ?)plsa(w|history)
The probability of a word given its history is com-
puted by the LSA model in the following way. Let h
be the sum of all the LSA word vectors in the his-
tory. Let m be the smallest cosine similarity be-
tween h and any word in the vocabulary V : m =
minw?V sim(h,w). The probability of a word w in
the context of history h is given by
Plsa(w|h) =
sim(h,w) ? m
?
q?V (sim(h, q) ? m)
Since similarity can be negative, subtracting the
minimum (m) ensures that all the estimated prob-
abilities are between 0 and 1.
605
4.4 Improving Efficiency and Expressiveness
Given the basic framework described above, a num-
ber of enhancements are possible. In terms of ef-
ficiency, recall that it is necessary to perform SVD
on a term-document matrix. The data we used was
grouped into paragraph ?documents,? of which there
were over 27 million, with 2.6 million unique words.
While the resulting matrix is highly sparse, it is nev-
ertheless impractical to perform SVD. We overcome
this difficulty in two ways. First, we restrict the set
of documents used to those which are ?relevant? to
a given test set. This is done by requiring that a doc-
ument contain at least one of the potential answer-
words. Secondly, we restrict the vocabulary to the
set of words present in the test set. For the sentence-
reconstruction method of Section 4.2, we have found
it convenient to do data selection per-sentence.
To enhance the expressive power of LSA, the term
vocabulary can be expanded from unigrams to bi-
grams or trigrams of words, thus adding information
about word ordering. This was also used in the re-
construction technique.
5 Experimental Results
5.1 Data Resources
We present results with two datasets. The first is
taken from 11 Practice Tests for the SAT & PSAT
2011 Edition (Princeton-Review, 2010). This book
contains eleven practice tests, and we used all the
sentence completion questions in the first five tests
as a development set, and all the questions in the last
six tests as the test set. This resulted in sets with 95
and 108 questions respectively. Additionally, we re-
port results on the recently released MSR Sentence
Completion Challenge (Zweig and Burges, 2011).
This consists of a set of 1, 040 sentence completion
questions based on sentences occurring in five Co-
nan Doyle Sherlock Holmes novels, and is identical
in format to the SAT questions. Due to the source of
this data, we refer to it as the Holmes data.
To train models, we have experimented with a
variety of data sources. Since there is no publi-
cally available collection of SAT questions suitable
to training, our methods have all relied on unsu-
pervised data. Early on, we ran a set of experi-
ments to determine the relevance of different types
of data. Thinking that data from an encyclopedia
Data Dev % Correct Test % Correct
Encarta 26 33
Wikipedia 32 31
LA Times 39 42
Table 1: Effectiveness of different types of training data.
might be useful, we evaluated an electronic version
of the 2003 Encarta encyclopedia, which has ap-
proximately 29M words. Along similar lines, we
used a collection of Wikipedia articles consisting of
709M words. This data is the entire Wikipedia as of
January 2011, broken down into sentences, with fil-
tering to remove sentences consisting of URLs and
Wiki author comments. Finally, we used a com-
mercial newspaper dataset consisting of all the Los
Angeles Times data from 1985 to 2002, containing
about 1.1B words. These data sources were evalu-
ated using the baseline n-gram LM approach of Sec-
tion 3.1. Initial experiments indicated that that the
Los Angeles Times data is best suited to this task
(see Table 1), and our SAT experiments use this
source. For the MSR Sentence Completion data,
we obtained the training data specified in (Zweig
and Burges, 2011), consisting of approximately 500
19th-century novels available from Project Guten-
berg, and comprising 48M words.
5.2 Human Performance
To provide human benchmark performance, we
asked six native speaking high school students and
five graduate students to answer the questions on the
development set. The high-schoolers attained 87%
accuracy and the graduate students 95%. Zweig and
Burges (2011) cite a human performance of 91%
on the Holmes data. Statistics from a large cross-
section of the population are not available. As a fur-
ther point of comparison, we note that chance per-
formance is 20%.
5.3 Language Modeling Results
Table 2 summarizes our language modeling results
on the SAT data. With the exception of the base-
line backoff n-gram model, these techniques were
too computationally expensive to utilize the full Los
Angeles Times corpus. Instead, as with LSA, a ?rel-
evant? corpus was selected of the sentences which
contain at least one answer option from either the
606
Method Data (Dev / Test) Dev Test
3-gram GT 1.1B / 1.1B 39% 42%
Model M 193M / 236M 35 41
RNN 36M / 44M 37 42
LSA-LM 293M / 358 M 48 44
Table 2: Performance of language modeling methods on
SAT questions.
Method Dev ppl Dev Test ppl Test
3-gram GT 195 36% 190 44%
Model M 178 36 175 42
RNN 147 37 144 42
Table 3: Performance of language modeling methods us-
ing identical training data and vocabularies.
development or test set. Separate subsets were made
for development and test data. This data was further
sub-sampled to obtain the training set sizes indicated
in the second column. For the LSA-LM, an interpo-
lation weight of 0.1 was used for the LSA score, de-
termined through optimization on the development
set. We see from this table that the language models
perform similarly and achieve just above 40% on the
test set.
To make a more controlled comparison that nor-
malizes for the amount of training data, we have
trained Model M, and the Good-Turing model on
the same data subset as the RNN, and with the same
vocabulary. In Table 3, we present perplexity re-
sults on a held-out set of dev/test-relevant Los Ange-
les Times data, and performance on the actual SAT
questions. Two things are notable. First, the re-
current neural net has dramatically lower perplexity
than the other methods. This is consistent with re-
sults in (Mikolov et al, 2011a). Secondly, despite
the differences in perplexity, the methods show little
difference on SAT performance. Because Model M
was not better, only uses n-gram context, and was
used in the construction of the Holmes data (Zweig
and Burges, 2011), we do not consider it further.
5.4 LSA Results
Table 4 presents results for the methods of Sections
4.1 and 4.2. Of all the methods in isolation, the sim-
ple approach of Section 4.1 - to use the total cosine
similarity between a potential answer and the other
words in the sentence - has performed best. The ap-
Method Dev Test
Total Word Similarity 46% 46%
Reconstruction Error 53 41
Table 4: SAT performance of LSA based methods.
Method Test
3-input LSA 46%
LSA + Good-Turing LM 53
LSA + Good-Turing LM + RNN 52
Table 5: SAT test set accuracy with combined methods.
proach of using reconstruction error performed very
well on the development set, but unremarkably on
the test set.
5.5 Combination Results
A well-known trick for obtaining best results from
a machine learning system is to combine a set of
diverse methods into a single ensemble (Dietterich,
2000). We use ensembles to get the highest accuracy
on both of our data sets.
We use a simple linear combination of the out-
puts of the other models discussed in this paper. For
the LSA model, the linear combination has three in-
puts: the total word similarity, the cosine similarity
between the sum of the answer word vectors and the
sum of the rest of sentence?s word vectors, and the
number of out-of-vocabulary terms in the answer.
Each additional language model beyond LSA con-
tributes an additional input: the probability of the
sentence under that language model.
We train the parameters of the linear combination
on the SAT development set. The training minimizes
a loss function of pairs of answers: one correct and
one incorrect fill-in from the same question. We use
the RankNet loss function (Burges et al, 2005):
min
~w
f(~w ? (~x ? ~y)) + ?||~w||2
where ~x are the input features for the incorrect an-
swer, ~y are the features for the correct answer, ~w
are the weights for the combination, and f(z) =
log(1 + exp(z)). We tune the regularizer via 5-
fold cross validation, and minimize the loss using
L-BFGS (Nocedal and Wright, 2006). The results
on the SAT test set for combining various models
are shown in Table 5.
607
5.6 Holmes Data Results
To measure the robustness of our approaches, we
have applied them to the MSR Sentence Completion
set (Zweig and Burges, 2011), termed the Holmes
data. In Table 6, we present the results on this set,
along with the comparable SAT results. Note that
the latter are derived from models trained with the
Los Angeles Times data, while the Holmes results
are derived from models trained with 19th-century
novels. We see from this table that the results are
similar across the two tasks. The best performing
single model is LSA total word similarity.
For the Holmes data, combining the models out-
performs any single model. We train the linear com-
bination function via 5-fold cross-validation: the
model is trained five times, each time on 3/5 of the
data, the regularization tuned on 1/5 of the data, and
tested on 1/5. The test results are pooled across all
5 folds and are shown in Table 6. In this case, the
best combination is to blend LSA, the Good-Turing
language model, and the recurrent neural network.
6 Discussion
To verify that the differences in accuracy between
the different algorithms are not statistical flukes, we
perform a statistical significance test on the out-
puts of each algorithm. We use McNemar?s test,
which is a matched test between two classifiers (Di-
etterich, 1998). We use the False Discovery Rate
method (Benjamini and Hochberg, 1995) to control
the false positive rate caused by multiple tests. If
we allow 2% of our tests to yield incorrectly false
results, then for the SAT data, the combination of
the Good-Turing smoothed language model with an
LSA-based global similarity model (52% accuracy)
is better that the baseline alone (42% accuracy).
Secondly, for the Holmes data, we can state that
LSA total similarity beats the recurrent neural net-
work, which in turn is better than the baseline n-
gram model. The combination of all three is sig-
nificantly better than any of the individual models.
To better understand the system performance and
gain insight into ways of improving it, we have ex-
amined the system?s errors. Encouragingly, one-
third of the errors involve single-word questions
which test the dictionary definition of a word. This
is done either by stating the definition, or provid-
Method SAT Holmes
Chance 20% 20%
GT N-gram LM 42 39
RNN 42 45
LSA Total Similarity 46 49
Reconstruction Error 41 41
LSA-LM 44 42
Combination 53 52
Human 87 to 95 91
Table 6: Performance of methods on the MSR Sentence
Completion Challenge, contrasted with SAT test set.
ing a stereotypical use of the word. An example of
the first case is: ?Great artists are often prophetic
(visual): they perceive what we cannot and antici-
pate the future long before we do.? (The system?s
incorrect answer is in parentheses.) An example
of the second is: ?One cannot help but be moved
by Theresa?s heartrending (therapeutic) struggle to
overcome a devastating and debilitating accident.?
At the other end of the difficulty spectrum are
questions involving world knowledge and/or logical
implications. An example requiring both is, ?Many
fear that the ratification (withdrawal) of more le-
nient tobacco advertising could be detrimental to
public health.? About 40% of the errors require this
sort of general knowledge to resolve. Based on our
analysis, we believe that future research could prof-
itably exploit the structured information present in
a dictionary. However, the ability to identify and
manipulate logical relationships and embed world
knowledge in a manner amenable to logical manip-
ulation may be necessary for a full solution. It is
an interesting research question if this could be done
implicitly with a machine learning technique, for ex-
ample recurrent or recursive neural networks.
7 Conclusion
In this paper we have investigated methods for
answering sentence-completion questions. These
questions are intriguing because they probe the abil-
ity to distinguish semantically coherent sentences
from incoherent ones, and yet involve no more con-
text than the single sentence. We find that both local
n-gram information and an LSA-based global coher-
ence model do significantly better than chance, and
that they can be effectively combined.
608
References
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 ?166.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
fase discovery rate: a practical and powerful approach
to multiple testing. J. Royal Statistical Society B,
53(1):289?300.
C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proc. ICML, pages 89?
96.
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo
Braz, Benjamin Garrett, Margaret Kosmala, Tomer
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun,
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa
Zorn. 2000. Reading comprehension programs in
a statistical-language-processing class. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 1?5. Asso-
ciation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4):359?393.
S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and
A. Sethy. 2009. Scaling shrinkage-based language
models. In ASRU.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ESCA Eurospeech,
http://www.speech.cs.cmu.edu/SLM/toolkit.html.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, ICSLP.
Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measur-
ing the similarity between implicit semantic relations
from the web. InWorldWideWeb Conference (WWW).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
T.G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
T.G. Dietterich. 2000. Ensemble methods in machine
learning. In International Workshop on Multiple Clas-
sifier Systems, pages 1?15. Springer-Verlag.
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/digital assets/
assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and
B. Zhao. 2010. Decoding with shrinkage-based lan-
guage models. In Interspeech.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 145?148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ?07,
pages 410?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lynette Hirschman, Mark Light, Eric Breck, and John D.
Burger. 1999. Deep read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and
Roger Levy. 2004. Solving logic puzzles: from ro-
bust processing to precise semantics. In Proceedings
of the 2nd Workshop on Text Meaning and Interpreta-
tion, pages 9?16. Association for Computational Lin-
guistics.
Jarmasz M. and Szpakowicz S. 2003. Roget?s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
609
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng
Kwan. 2000. A machine learning approach to answer-
ing questions for reading comprehension tests. In Pro-
ceedings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ?00, pages 124?132.
J. Nocedal and S. Wright. 2006. Numerical Optimiza-
tion. Springer-Verlag.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161?199.
Princeton-Review. 2010. 11 Practice Tests for the SAT
& PSAT, 2011 Edition. The Princeton Review.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In Proceedings of the 2000 ANLP/NAACL Work-
shop on Reading comprehension tests as evaluation for
computer-based language understanding sytems - Vol-
ume 6, ANLP/NAACL-ReadingComp ?00, pages 13?
19.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural networks.
In Proceedings of the 2011 International Conference
on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
T. Veale. 2004. Wordnet sits the sat: A knowledge-based
approach to lexical analogy. In European Conference
on Artificial Intelligence (ECAI).
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 28?35.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 207?213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Geoffrey Zweig and Christopher J.C. Burges. 2011. The
Microsoft Research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft.
610
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744?1753,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Question Answering Using Enhanced Lexical Semantic Models
Wen-tau Yih Ming-Wei Chang Christopher Meek Andrzej Pastusiak
Microsoft Research
Redmond, WA 98052, USA
{scottyih,minchang,meek,andrzejp}@microsoft.com
Abstract
In this paper, we study the answer
sentence selection problem for ques-
tion answering. Unlike previous work,
which primarily leverages syntactic analy-
sis through dependency tree matching, we
focus on improving the performance us-
ing models of lexical semantic resources.
Experiments show that our systems can
be consistently and significantly improved
with rich lexical semantic information, re-
gardless of the choice of learning algo-
rithms. When evaluated on a bench-
mark dataset, the MAP and MRR scores
are increased by 8 to 10 points, com-
pared to one of our baseline systems using
only surface-form matching. Moreover,
our best system also outperforms pervious
work that makes use of the dependency
tree structure by a wide margin.
1 Introduction
Open-domain question answering (QA), which
fulfills a user?s information need by outputting di-
rect answers to natural language queries, is a chal-
lenging but important problem (Etzioni, 2011).
State-of-the-art QA systems often implement a
complicated pipeline architecture, consisting of
question analysis, document or passage retrieval,
answer selection and verification (Ferrucci, 2012;
Moldovan et al, 2003). In this paper, we focus
on one of the key subtasks ? answer sentence se-
lection. Given a question and a set of candidate
sentences, the task is to choose the correct sen-
tence that contains the exact answer and can suf-
ficiently support the answer choice. For instance,
although both of the following sentences contain
the answer ?Jack Lemmon? to the question ?Who
won the best actor Oscar in 1973?? only the first
sentence is correct.
A1: Jack Lemmon won the Academy Award for
Best Actor for Save the Tiger (1973).
A2: Oscar winner Kevin Spacey said that Jack
Lemmon is remembered as always making
time for other people.
One of the benefits of answer sentence selec-
tion is that the output can be provided directly to
the user. Instead of outputting only the answer, re-
turning the whole sentence often adds more value
as the user can easily verify the correctness with-
out reading a lengthy document.
Answer sentence selection can be naturally re-
duced to a semantic text matching problem. Con-
ceptually, we would like to measure how close
the question and sentence can be matched seman-
tically. Due to the variety of word choices and
inherent ambiguities in natural languages, bag-of-
words approaches with simple surface-form word
matching tend to produce brittle results with poor
prediction accuracy (Bilotti et al, 2007). As a
result, researchers put more emphasis on exploit-
ing both the syntactic and semantic structure in
questions/sentences. Representative examples in-
clude methods based on deeper semantic anal-
ysis (Shen and Lapata, 2007; Moldovan et al,
2007) and on tree edit-distance (Punyakanok et
al., 2004; Heilman and Smith, 2010) and quasi-
synchronous grammar (Wang et al, 2007) that
match the dependency parse trees of questions and
sentences. However, such approaches often re-
quire more computational resources. In addition
to applying a syntactic or semantic parser during
run-time, finding the best matching between struc-
tured representations of sentences is not trivial.
For example, the computational complexity of tree
matching is O(V 2L4), where V is the number of
nodes and L is the maximum depth (Tai, 1979).
Instead of focusing on the high-level seman-
tic representation, we turn our attention in this
work to improving the shallow semantic compo-
1744
nent, lexical semantics. We formulate answer se-
lection as a semantic matching problem with a la-
tent word-alignment structure as in (Chang et al,
2010) and conduct a series of experimental stud-
ies on leveraging recently proposed lexical seman-
tic models. Our main contributions in this work
are two key findings. First, by incorporating the
abundant information from a variety of lexical se-
mantic models, the answer selection system can
be enhanced substantially, regardless of the choice
of learning algorithms and settings. Compared to
the previous work, our latent alignment model im-
proves the result on a benchmark dataset by a wide
margin ? the mean average precision (MAP) and
mean reciprocal rank (MRR) scores are increased
by 25.6% and 18.8%, respectively. Second, while
the latent alignment model performs better than
unstructured models, the difference diminishes af-
ter adding the enhanced lexical semantics infor-
mation. This may suggest that compared to in-
troducing complex structured constraints, incorpo-
rating shallow semantic information is both more
effective and computationally inexpensive in im-
proving the performance, at least for the specific
word alignment model tested in this work.
The rest of the paper is structured as follows.
We first survey the related work in Sec. 2. Sec. 3
defines the problem of answer sentence selection,
along with the high-level description of our solu-
tion. The enhanced lexical semantic models and
the learning frameworks we explore are presented
in Sec. 4 and Sec. 5, respectively. Our experimen-
tal results on a benchmark QA dataset is shown in
Sec. 6. Finally, Sec. 7 concludes the paper.
2 Related Work
While the task of question answering has a long
history dated back to the dawn of artificial in-
telligence, early systems like STUDENT (Wino-
grad, 1977) and LUNAR (Woods, 1973) are typ-
ically designed to demonstrate natural language
understanding for a small and specific domain.
The Text REtrieval Conference (TREC) Question
Answering Track was arguably the first large-
scale evaluation of open-domain question answer-
ing (Voorhees and Tice, 2000). The task is de-
signed in an information retrieval oriented setting.
Given a factoid question along with a collection
of documents, a system is required to return the
exact answer, along with the document that sup-
ports the answer. In contrast, the Jeopardy! TV
quiz show provides another open-domain question
answering setting, in which IBM?s Watson system
famously beat the two highest ranked players (Fer-
rucci, 2012). Questions in this game are presented
in a statement form and the system needs to iden-
tify the true question and to give the exact answer.
A short sentence or paragraph to justify the answer
is not required in either TREC-QA or Jeopardy!
As any QA system can virtually be decomposed
into two major high-level components, retrieval
and selection (Echihabi and Marcu, 2003), the an-
swer selection problem is clearly critical. Limiting
the scope of an answer to a sentence is first high-
lighted by Wang et al (2007), who argued that it
was more informative to present the whole sen-
tence instead of a short answer to users.
Observing the limitations of the bag-of-words
models, Wang et al (2007) proposed a syntax-
driven approach, where each pair of question and
sentence are matched by their dependency trees.
The mapping is learned by a generative probabilis-
tic model based on a Quasi-synchronous Gram-
mar formulation (Smith and Eisner, 2006). This
approach was later improved by Wang and Man-
ning (2010) with a tree-edit CRF model that learns
the latent alignment structure. In contrast, gen-
eral tree matching methods based on tree-edit dis-
tance have been first proposed by Punyakanok et
al. (2004) for a similar answer selection task. Heil-
man and Smith (2010) proposed a discriminative
approach that first computes a tree kernel func-
tion between the dependency trees of the question
and candidate sentence, and then learns a classifier
based on the tree-edit features extracted.
Although lexical semantic information derived
from WordNet has been used in some of these
approaches, the research has mainly focused
on modeling the mapping between the syntac-
tic structures of questions and sentences, pro-
duced from syntactic analysis. The potential im-
provement from enhanced lexical semantic mod-
els seems to have been deliberately overlooked.1
3 Problem Definition
We consider the answer selection problem in a
supervised learning setting. For a set of ques-
tions {q1, ? ? ? , qm}, each question qi is associated
with a list of labeled candidate answer sentences
1For example, Heilman and Smith (2010) emphasized that
?The tree edit model, which does not use lexical semantics
knowledge, produced the best result reported to date.?
1745
What is the fastest car in the world?
The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet. 
Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan,
2001). Words connected by solid lines are clear synonyms or hyponym/hypernym; words with weaker
semantic association are linked by dashed lines.
{(yi1 , si1), (yi1 , si2), ? ? ? , (yin , sin)}, where yij =
1 indicates that sentence sij is a correct answer to
question qi, and 0 otherwise. Using this labeled
data, our goal is to learn a probabilistic classifier
to predict the label of a new, unseen pair of ques-
tion and sentence.
Fundamentally, what the classifier predicts is
whether the sentence ?matches? the question se-
mantically. In other words, does s have the an-
swer that satisfies the semantic constraints pro-
vided in the question? Without representing the
question and sentence in logic or syntactic trees,
we take a word-alignment view for solving this
problem. We assume that there is an underly-
ing structure h that describes how q and s can
be associated through the relations of the words
in them. Figure 1 illustrates this setting using a
revised example from (Harabagiu and Moldovan,
2001). In this figure, words connected by solid
lines are clear synonyms or hyponym/hypernym;
words connected by dashed lines indicate that they
are weakly related. With this alignment structure,
features like the degree of mapping or whether all
the content words in the question can be mapped
to some words in the sentence can be extracted and
help improve the classifier. Notice that the struc-
ture representation in terms of word-alignment is
fairly general. For instance, if we assume a naive
complete bipartite matching, then effectively it re-
duces to the simple bag-of-words model.
Typically, the ?ideal? alignment structure is not
available in the data, and previous work exploited
mostly syntactic analysis (e.g., dependency trees)
to reveal the latent mapping structure. In this
work, we focus our study on leveraging the low-
level semantic cues from recently proposed lexical
semantic models. As will be shown in our experi-
ments, such information not only improves a latent
structure learning method, but also makes a simple
bipartite matching approach extremely strong.2
4 Lexical Semantic Models
In this section, we introduce the lexical seman-
tic models we adopt for solving the semantic
matching problem in answer selection. To go be-
yond the simple, limited surface-form matching,
we aim to pair words that are semantically re-
lated, specifically measured by models of word
relations including synonymy/antonymy, hyper-
nymy/hyponymy (the Is-A relation) and general se-
mantic word similarity.
4.1 Synonymy and Antonymy
Among all the word relations, synonymy is per-
haps the most basic one and needs to be handled
reliably. Although sets of synonyms can be eas-
ily found in thesauri or WordNet synsets, such
resources typically cover only strict synonyms.
When comparing two words, it is more useful to
estimate the degree of synonymy as well. For in-
stance, ship and boat are not strict synonyms be-
cause a ship is usually viewed as a large boat.
Knowing that two words are somewhat synony-
mous could be valuable in determining whether
they should be mapped.
In order to estimate the degree of synonymy, we
leverage a recently proposed polarity-inducing la-
tent semantic analysis (PILSA) model (Yih et al,
2012). Given a thesaurus, the model first con-
structs a signed d-by-n co-occurrence matrix W ,
where d is the number of word groups and n is
the size of the vocabulary. Each row consists of a
2Proposed by an anonymous reviewer, one justification of
this word-alignment approach, where syntactic analysis plays
a less important role, is that there are often few sensible com-
binations of words. For instance, knowing only the set of
words {?car?, ?fastest?, ?world?}, one may still guess cor-
rectly the question ?What is the fastest car in the world??
1746
group of synonyms and antonyms of a particular
sense and each column represents a unique word.
Values of the elements in each row vector are the
TFIDF values of the corresponding words in this
group. The notion of polarity is then induced by
making the values of words in the antonym groups
negative, and the matrix is generalized by a low-
rank approximation derived by singular-value de-
composition (SVD) in the end. This design has an
intriguing property ? if the cosine score of two col-
umn vectors are positive, then the two correspond-
ing words tend to be synonymous; if it?s negative,
then the two words are antonymous. The degree is
measured by the absolute value.
Following the setting described in (Yih et al,
2012), we construct a PILSA model based on the
Encarta thesaurus and enhance it with a discrimi-
native projection matrix training method. The es-
timated degrees of both synonymy and antonymy
are used our experiments.3
4.2 Hypernymy and Hyponymy
The Class-Inclusion or Is-A relation is commonly
observed between words in questions and answer
sentences. For example, to correctly answer the
question ?What color is Saturn??, it is crucial that
the selected sentence mentions a specific kind of
color, as in ?Saturn is a giant gas planet with
brown and beige clouds.? Another example is
?Who wrote Moonlight Sonata??, where compose
in ?Ludwig van Beethoven composed the Moon-
light Sonata in 1801.? is one kind of write.
Traditionally, WordNet taxonomy is the linguis-
tic resource for identifying hypernyms and hy-
ponyms, applied broadly to many NLP problems.
However, WordNet has a number of well-known
limitations including its rather limited or skewed
concept distribution and the lack of the coverage
of the Is-A relation (Song et al, 2011). For in-
stance, when a word refers to a named entity, the
particular sense and meaning is often not encoded.
As a result, relations such as ?Apple? is-a ?com-
pany? and ?Jaguar? is-a ?car? cannot be found in
WordNet. Similar to the case in synonymy, the
Is-A relation defined in WordNet does not provide
a native, real-valued degree of the relation, which
can only be roughly approximated using the num-
ber of links on the taxonomy path connecting two
3Mapping two antonyms may be desired if one of them is
in the scope of negation (Morante and Blanco, 2012; Blanco
and Moldovan, 2011). However, we do not attempt to resolve
the negation scope in this work.
concepts (Resnik, 1995).
In order to remedy these issues, we aug-
ment WordNet with the Is-A relations found in
Probase (Wu et al, 2012). Probase is a knowledge
base that establishes connections between 2.7 mil-
lion concepts, discovered automatically by apply-
ing Hearst patterns (Hearst, 1992) to 1.68 billion
Web pages. Its abundant concept coverage dis-
tinguishes it from other knowledge bases, such as
Freebase (Bollacker et al, 2008) and WikiTaxon-
omy (Ponzetto and Strube, 2007). Based on the
frequency of term co-occurrences, each Is-A rela-
tion from Probase is associated with a probability
value, indicating the degree of the relation.
We verified the quality of Probase Is-A relations
using a recently proposed SemEval task of rela-
tional similarity (Jurgens et al, 2012) in a com-
panion paper (Zhila et al, 2013), where a subset
of the data is to measure the degree of two words
having a class-inclusion relation. Probase?s pre-
diction correlates well with the human annotations
and achieves a high Spearman?s rank correlation
coefficient score, ? = 0.619. In comparison, the
previous best system (Rink and Harabagiu, 2012)
in the task only reaches ? = 0.233. These appeal-
ing qualities make Probase a robust lexical seman-
tic model for hypernymy/hyponymy.
4.3 Semantic Word Similarity
The third lexical semantic model we introduce tar-
gets a general notion of word similarity. Unlike
synonymy and hyponymy, word similarity is only
loosely defined when two words can be associated
by some implicit relation.4 The general word sim-
ilarity model can be viewed as a ?back-off? so-
lution when the exact lexical relation (e.g., part-
whole and attribute) is not available or cannot be
accurately detected.
Among various word similarity models (Agirre
et al, 2009; Reisinger and Mooney, 2010;
Gabrilovich and Markovitch, 2007; Radinsky et
al., 2011), the vector space models (VSMs) based
on the idea of distributional similarity (Turney
and Pantel, 2010) are often used as the core com-
ponent. Inspired by (Yih and Qazvinian, 2012),
which argues the importance of incorporating het-
erogeneous vector space models for measuring
word similarity, we leverage three different VSMs
in this work: Wiki term-vectors, recurrent neural
4Instead of making the distinction, word similarity here
refers to the larger set of relations commonly covered by word
relatedness (Budanitsky and Hirst, 2006).
1747
network language model (RNNLM) and a concept
vector space model learned from click-through
data. Semantic word similarity is estimated using
the cosine score of the corresponding word vectors
in these VSMs.
Contextual term-vectors created using the
Wikipedia corpus have shown to perform well
on measuring word similarity (Reisinger and
Mooney, 2010). Following the setting suggested
by Yih and Qazvinian (2012), we create term-
vectors representing about 1 million words by ag-
gregating terms within a window of [?10, 10] of
each occurrence of the target word. The vectors
are further refined by applying the same vocabu-
lary and feature pruning techniques.
A recurrent neural network language
model (Mikolov et al, 2010) aims to esti-
mate the probability of observing a word given its
preceding context. However, one by-product of
this model is the word embedding learned in its
hidden-layer, which can be viewed as capturing
the word meaning in some latent, conceptual
space. As a result, vectors of related words tend
to be close to each other. For this word similarity
model, we take a 640-dimensional version of
RNNLM vectors, which is trained using the
Broadcast News corpus of 320M words.5
The final word relatedness model is a projec-
tion model learned from the click-through data of
a commercial search engine (Gao et al, 2011).
Unlike the previous two models, which are cre-
ated or trained using a text corpus, the input for
this model is pairs of aggregated queries and ti-
tles of pages users click. This parallel data is
used to train a projection matrix for creating the
mapping between words in queries and documents
based on user feedback, using a Siamese neural
network (Yih et al, 2011). Each row vector of
this matrix is the dense vector representation of
the corresponding word in the vocabulary. Perhaps
due to its unique information source, we found this
particular word embedding seems to complement
the other two VSMs and tends to improve the word
similarity measure in general.
5 Learning QA Matching Models
In this section, we investigate the effectiveness of
various learning models for matching questions
and sentences, including the bag-of-words setting
5http://www.fit.vutbr.cz/?imikolov/
rnnlm/
and the framework of learning latent structures.
5.1 Bag-of-Words Model
The bag-of-words model treats each question and
sentence as an unstructured bag of words. When
comparing a question with a sentence, the model
first matches each word in the question to each
word in the sentence. It then aggregates features
extracted from each of these word pairs to rep-
resent the whole question/sentence pair. A bi-
nary classifier can be trained easily using any ma-
chine learning algorithm in this standard super-
vised learning setting.
Formally, let x = (q, s) be a pair of question q
and sentence s. Let Vq = {wq1 , wq2 , ? ? ? , wqm}
and Vs = {ws1 , ws2 , ? ? ? , wsn} be the sets of
words in q and s, respectively. Given a word pair
(wq, ws), where wq ? Vq and ws ? Vs, feature
functions ?1, ? ? ? , ?d map it to a d-dimensional
real-valued feature vector.
We consider two aggregate functions for defin-
ing the feature vectors of the whole ques-
tion/answer pair: average and max.
?avgj (q, s) =
1
mn
?
wq?Vq
ws?Vs
?j(wq, ws) (1)
?maxj (q, s) = maxwq?Vq
ws?Vs
?j(wq, ws) (2)
Together, each question/sentence pair is repre-
sented by a 2d-dimensional feature vector.
We tested two learning algorithms in this set-
ting: logistic regression and boosted decision
trees (Friedman, 2001). The former is the log-
linear model widely used in the NLP community
and the latter is a robust non-linear learning algo-
rithm that has shown great empirical performance.
The bag-of-words model does not require an ad-
ditional inference stage as in structured learning,
which may be computationally expensive. Nev-
ertheless, its lack of structure information could
limit the expressiveness of the model and make it
difficult to capture more sophisticated semantics
in the sentences. To address this concern, we in-
vestigate models of learning latent structures next.
5.2 Learning Latent Structures
One obvious issue of the bag-of-words model is
that words in the unrelated part of the sentence
may still be paired with words in the question,
which introduces noise to the final feature vector.
1748
This is observed in many question/sentence pairs,
such as the one below.
Q: Which was the first movie that James Dean
was in?
A: James Dean, who began as an actor on TV
dramas, didn?t make his screen debut until
1951?s ?Fixed Bayonet.?
While this sentence correctly answers the ques-
tion, the fact that James Dean began as a TV
actor is unrelated to the question. As a result,
an ?ideal? word alignment structure should not
link words in this clause to those in the ques-
tion. In order to leverage the latent structured in-
formation, we adapt a recently proposed frame-
work of learning constrained latent representa-
tions (LCLR) (Chang et al, 2010). LCLR can be
viewed as a variant of Latent-SVM (Felzenszwalb
et al, 2009) with different learning formulations
and a general inference framework. The idea of
LCLR is to replace the decision function of a stan-
dard linear model ?T?(x) with
arg max
h
?T?(x, h), (3)
where ? represents the weight vector and h repre-
sents the latent variables.
In this answer selection task, x = (q, s) rep-
resents a pair of question q and candidate sen-
tence s. As described in Sec. 3, h refers to the
latent alignment between q and s. The intuition
behinds Eq. (3) is: candidate sentence s correctly
answers question q if and only if the decision can
be supported by the best alignment h.
The objective function of LCLR is defined as:
min?
1
2 ||?||
2 + C
?
i
?2i
s.t. ?i ? 1? yi maxh ?
T?(x, h)
Note that the alignment is latent, so LCLR uses
the binary labels in the training data as feedback
to find the alignment for each example.
The computational difficulty of the inference
problem (Eq. (3)) largely depends on the con-
straints we enforce in the alignment. Complicated
constraints may result in a difficult inference prob-
lem, which can be solved by integer linear pro-
gramming (Roth and Yih, 2007). In this work,
we considered several sets of constraints for the
alignment task, including a two-layer phrase/word
alignment structure, but found that they generally
performed the same. Therefore, we chose the
many-to-one alignment6, where inference can be
solved exactly using a simple greedy algorithm.
6 Experiments
We present our experimental results in this sec-
tion by first introducing the data and evaluation
metrics, followed by the results of existing sys-
tems and some baseline methods. We then show
the positive impact of adding information of word
relations from various lexical semantics models,
with some discussion on the limitation of the
word-matching approach.
6.1 Data & Evaluation Metrics
The answer selection dataset we used was orig-
inally created by Wang et al (2007) based on
the QA track of past Text REtrieval Confer-
ences (TREC-QA). Questions in this dataset are
short factoid questions, such as ?What is Crips?
gang color?? In average, each question is associ-
ated with approximately 33 answer candidate sen-
tences. A pair of question and sentence is judged
positive if the sentence contains the exact answer
key and can provide sufficient context as support-
ing evidence.
The training set of the data contains manu-
ally labeled 5,919 question/sentence pairs from
TREC 8-12. The development and testing sets
are both from TREC 13, which contain 1,374
and 1,866 pairs, respectively. The task is treated as
a sentence ranking problem for each question and
thus evaluated in Mean Average Precision (MAP)
and Mean Reciprocal Rank (MRR), using the offi-
cial TREC evaluation program. Following (Wang
et al, 2007), candidate sentences with more than
40 words are removed from evaluation, as well as
questions with only positive or negative candidate
sentences.
6.2 Baseline Methods
Several systems have been proposed and tested
using this dataset. Wang et al (2007) pre-
sented a generative probabilistic model based on
a Quasi-synchronous Grammar formulation and
was later improved by Wang and Manning (2010)
with a tree-edit CRF model that learns the la-
tent alignment structure. In contrast, Heilman and
6Each word in the question needs to be linked to a word
in the sentence. Each word in the sentence can be linked to
zero or multiple words in the question.
1749
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
Table 1: Test set results of existing methods, taken
from Table 3 of (Wang and Manning, 2010).
Dev Test
Baseline MAP MRR MAP MRR
Random 0.5243 0.5816 0.4708 0.5286
Word Cnt 0.6516 0.7216 0.6263 0.6822
Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071
Table 2: Results of three baseline methods.
Smith (2010) proposed a discriminative approach
that first computes a tree kernel function between
the dependency trees of the question and candidate
sentence, and then learns a classifier based on the
tree-edit features extracted. Table 1 summarizes
their results on the test set. All these systems in-
corporated lexical semantics features derived from
WordNet and named entity features.
In order to further estimate the difficulty of
this task and dataset, we tested three simple base-
lines. The first is random scoring, which sim-
ply assigns a random score to each candidate sen-
tence. The second one, word count, is to count
how many words in the question that also occur in
the answer sentence, after removing stopwords7,
and lowering the case. Finally, the last base-
line method, weighted word count, is basically the
same as identical word matching, but the count is
re-weighted using the IDF value of the question
word. This is similar to the BM25 ranking func-
tion (Robertson et al, 1995). The results of these
three methods are shown in Table 1.
Somewhat surprisingly, we find that word count
is fairly strong and performs comparably to previ-
ous systems.8 In addition, weighting the question
words with their IDF values further improves the
results.
6.3 Incorporating Rich Lexical Semantics
We test the effectiveness of adding rich lexical
semantics information by creating examples of
different feature sets. As described in Sec. 5,
7We used a list of 101 stopwords, including articles, pro-
nouns and punctuation.
8The finding has been confirmed by the lead author
of (Wang et al, 2007).
all the features are based on the properties of
the pair of a word from the question and a
word from the candidate sentence. Stopwords
are first removed from both questions and sen-
tences and all words are lower-cased. Features
used in the experiments can be categorized into
six types: identical word matching (I), lemma
matching (L), WordNet (WN), enhanced Lexi-
cal Semantics (LS), Named Entity matching (NE)
and Answer type checking (Ans). Inspired by
the weighted word count baseline, all features ex-
cept (Ans) are weighted by the IDF value of the
question word. In other words, the IDF values help
decide the importance of word pairs to the model.
Staring from the our baseline model, weighted
word count, the identical word matching (I) fea-
ture checks whether the pair of words are the
same. Instead of checking the surface form of
the word, lemma matching (L) verifies whether
the two words have the same lemma form. Ar-
guably the most common source of word rela-
tions, WordNet (WN) provides the primitive fea-
tures of whether two words could belong to the
same synset in WordNet, could be antonyms and
whether one is a hypernym of the other. Alter-
natively, the enhanced lexical semantics (LS) fea-
tures apply the models described in Sec. 4 to the
word pair and use their estimated degree of syn-
onymy, antonymy, hyponymy and semantic relat-
edness as features. Named entity matching (NE)
checks whether two words are individually part
of some named entities with the same type. Fi-
nally, when the question word is the WH-word, we
check if the paired word belongs to some phrase
that has the correct answer type using simple rules,
such as ?Who should link to a word that is part of
a named entity of type Person.? We created exam-
ples in each round of experiments by augmenting
these features in the same order, and observed how
adding different information helped improve the
model performance.
Three models are included in our study. For
the unstructured, bag-of-words setting, we tested
logistic regression (LR) and boosted decision
trees (BDT). As mentioned in Sec. 5, the features
for the whole question/sentence pair are the aver-
age and max of features of all the word pairs. For
the structured-output setting, we used the frame-
work of learning constrained latent representa-
tion (LCLR) and required that each question word
needed to be mapped to a word in the sentence.
1750
LR BDT LCLR
Feature set MAP MRR MAP MRR MAP MRR
1: I 0.6531 0.7071 0.6323 0.6898 0.6629 0.7279
2: I+L 0.6744 0.7223 0.6496 0.6923 0.6815 0.7270
3: I+L+WN 0.7039 0.7705 0.6798 0.7450 0.7316 0.7921
4: I+L+WN+LS 0.7339 0.8107 0.7523 0.8455 0.7626 0.8231
5: All 0.7374 0.8171 0.7495 0.8450 0.7648 0.8255
Table 3: Test results of various models and feature groups. Logistic regression (LR) and boosted decision
trees (BDT) are the two unstructured models. LCLR is the algorithm for learning latent structures.
Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhanced
Lexical Semantics (LS). All includes these four plus Named Entity matching (NE) and Answer type
checking (Ans).
Hyper-parameters are selected using the ones that
achieve the best MAP score on the development
set. Results of these models and feature sets are
presented in Table 3.
We make two observations from the results.
First, while incorporating more information of the
word pairs in general helps, it is clear that map-
ping words beyond surface-form matching with
the help of WordNet (Line #3 vs. #2) is impor-
tant. Moreover, when richer information from
other lexical semantic models is available, the per-
formance can be further improved (Line #4 vs.
#3). Overall, by simply incorporating more in-
formation on word relations, we gain approxi-
mately 10 points in both MAP and MRR com-
pared to surface-form matching (Line #4 vs. #2),
consistently across all three models. However,
adding more information like named entity match-
ing and answer type verification does not seem to
help much (Line #5 vs. #4). Second, while the
structured-output model usually performs better
than both unstructured models (LCLR vs. LR &
BDT), the performance gain diminishes after more
information of word pairs is available (e.g., Lines
#4 and #5).
6.4 Limitation of Word Matching Models
Although we have demonstrated the benefits of
leveraging various lexical semantic models to help
find the association between words, the problem of
question answering is nevertheless far from solved
using the word-based approach. Examining the
output of the LCLR model with all features on the
development set, we found that there were three
main sources of errors, including uncovered or in-
accurate entity relations, the lack of robust ques-
tion analysis and the need of high-level semantic
representation and inference. While the first two
can be improved by, say, using a better named en-
tity tagger, incorporating other knowledge bases
and building a question classifier, how to solve the
third problem is tricky. Below is an example:
Q: In what film is Gordon Gekko the main char-
acter?
A: He received a best actor Oscar in 1987 for his
role as Gordon Gekko in ?Wall Street?.
This is a correct answer sentence because ?win-
ning a best actor Oscar? implies that the role Gor-
don Gekko is the main character. It is hard to be-
lieve that a pure word-matching model would be
able to solve this type of ?inferential question an-
swering? problem.
7 Conclusions
In this paper, we present an experimental study
on solving the answer selection problem using en-
hanced lexical semantic models. Following the
word-alignment paradigm, we find that the rich
lexical semantic information improves the models
consistently in the unstructured bag-of-words set-
ting and also in the framework of learning latent
structures. Another interesting finding we have
is that while the latent structured model, LCLR,
performs better than the other two unstructured
models, the difference diminishes after more in-
formation, including the enhanced lexical seman-
tic knowledge and answer type verification, has
been incorporated. This may suggest that adding
shallow semantic information is more effective
than introducing complex structured constraints,
at least for the specific word alignment model we
experimented with in this work.
1751
In the future, we plan to explore several di-
rections. First, although we focus on improv-
ing TREC-style open-domain question answering
in this work, we would like to apply the pro-
posed technology to other QA scenarios, such
as community-based QA (CQA). For instance,
the sentence matching technique can help map a
given question to some questions in an existing
CQA database (e.g., Yahoo! Answers). More-
over, the answer sentence selection scheme could
also be useful in extracting the most related sen-
tences from the answer text to form a summary
answer. Second, because the task of answer sen-
tence selection is very similar to paraphrase de-
tection (Dolan et al, 2004) and recognizing tex-
tual entailment (Dagan et al, 2006), we would like
to investigate whether systems for these tasks can
be improved by incorporating enhanced lexical se-
mantic knowledge as well. Finally, we would like
to improve our system for the answer sentence se-
lection task and for question answering in general.
In addition to following the directions suggested
by the error analysis presented in Sec. 6.4, we plan
to use logic-like semantic representations of ques-
tions and sentences, and explore the role of lexical
semantics for handling questions that require in-
ference.
Acknowledgments
We are grateful to Mengqiu Wang for providing
the dataset and helping clarify some issues in the
experiments. We also thank Chris Burges and Hoi-
fung Poon for valuable discussion and the anony-
mous reviewers for their useful comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas?ca and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of NAACL, pages
19?27.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of SIGIR, pages 351?358.
E. Blanco and D. Moldovan. 2011. Semantic repre-
sentation of negation using focus detection. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011).
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In ACM Conference on Management of Data
(SIGMOD), pages 1247?1250.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13?47,
March.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In Proceedings of NAACL.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge, volume 3944. Springer-Verlag, Berlin.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 16?23.
Oren Etzioni. 2011. Search needs a shake-up. Nature,
476(7358):25?26.
P. Felzenszwalb, R. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
99(1).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1?1.
J. Friedman. 2001. Greedy function approximation:
a gradient boosting machine. Annals of Statistics,
29(5):1189?1232.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In AAAI Conference on Artificial
Intelligence (AAAI).
J. Gao, K. Toutanova, and W. Yih. 2011.
Clickthrough-based latent semantic models for web
search. In Proceedings of SIGIR, pages 675?684.
S. Harabagiu and D. Moldovan. 2001. Open-domain
textual question answering. Tutorial of NAACL-
2001.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
M. Heilman and N. Smith. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and
answers to questions. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 1011?1019.
1752
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 356?364.
T. Mikolov, M. Karafia?t, L. Burget, J. Cernocky?, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 1045?1048.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analy-
sis in an open-domain question answering system.
ACM Transactions on Information Systems (TOIS),
21(2):133?154.
D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges.
2007. COGEX: A semantically and contextually en-
riched logic prover for question answering. Journal
of Applied Logic, 5(1):49?69.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: Resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 265?274.
S. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In AAAI Confer-
ence on Artificial Intelligence (AAAI).
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping
dependencies trees: An application to question an-
swering. In International Symposium on Artificial
Intelligence and Mathematics (AI & Math).
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis.
In WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In Proceed-
ings of NAACL.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International
Joint Conference on Artificial Intelligence (IJCAI).
B. Rink and S. Harabagiu. 2012. UTD: Determining
relational similarity using lexical patterns. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 413?418.
S. Robertson, S. Walker, S. Jones, M. Hancock-
Beaulieu, and M. Gatford. 1995. Okapi at TREC-3.
In Text REtrieval Conference (TREC), pages 109?
109.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12?21.
D. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of the HLT-NAACL
Workshop on Statistical Machine Translation, pages
23?30.
Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. 2011.
Short text conceptualization using a probabilistic
knowledgebase. In International Joint Conference
on Artificial Intelligence (IJCAI), pages 2330?2336.
K. Tai. 1979. The tree-to-tree correction problem. J.
ACM, 26(3):422?433, July.
P. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141?
188.
E. Voorhees and D. Tice. 2000. Building a question
answering test collection. In Proceedings of SIGIR,
pages 200?207.
M. Wang and C. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In Proceed-
ings of COLING.
M. Wang, N. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
T. Winograd. 1977. Five lectures on artificial intelli-
gence. In A. Zampolli, editor, Linguistic Structures
Processing, pages 399?520. North Holland.
W. Woods. 1973. Progress in natural language under-
standing: An application to lunar geology. In Pro-
ceedings of the National Computer Conference and
Exposition (AFIPS), pages 441?450.
W. Wu, H. Li, H. Wang, and K. Zhu. 2012. Probase:
a probabilistic taxonomy for text understanding. In
ACM Conference on Management of Data (SIG-
MOD), pages 481?492.
W. Yih and V. Qazvinian. 2012. Measuring word relat-
edness using heterogeneous vector space models. In
Proceedings of NAACL-HLT 2012, pages 616?620.
W. Yih, K. Toutanova, J. Platt, and C. Meek. 2011.
Learning discriminative projections for text similar-
ity measures. In ACL Conference on Natural Lan-
guage Learning (CoNLL), pages 247?256.
W. Yih, G. Zweig, and J. Platt. 2012. Polarity inducing
latent semantic analysis. In Proceedings of EMNLP-
CoNLL, pages 1212?1222.
A. Zhila, W. Yih, C. Meek, G. Zweig, and T. Mikolov.
2013. Combining heterogeneous models for mea-
suring relational similarity. In Proceedings of HLT-
NAACL.
1753
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643?648,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Semantic Parsing for Single-Relation Question Answering
Wen-tau Yih Xiaodong He Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,xiaohe,meek}@microsoft.com
Abstract
We develop a semantic parsing framework
based on semantic similarity for open do-
main question answering (QA). We focus
on single-relation questions and decom-
pose each question into an entity men-
tion and a relation pattern. Using convo-
lutional neural network models, we mea-
sure the similarity of entity mentions with
entities in the knowledge base (KB) and
the similarity of relation patterns and re-
lations in the KB. We score relational
triples in the KB using these measures
and select the top scoring relational triple
to answer the question. When evaluated
on an open-domain QA task, our method
achieves higher precision across different
recall points compared to the previous ap-
proach, and can improve F
1
by 7 points.
1 Introduction
Open-domain question answering (QA) is an im-
portant and yet challenging problem that remains
largely unsolved. In this paper, we focus on an-
swering single-relation factual questions, which
are the most common type of question observed in
various community QA sites (Fader et al, 2013),
as well as in search query logs. We assumed
such questions are answerable by issuing a single-
relation query that consists of the relation and an
argument entity, against a knowledge base (KB).
Example questions of this type include: ?Who is
the CEO of Tesla?? and ?Who founded Paypal??
While single-relation questions are easier to
handle than questions with more complex and
multiple relations, such as ?When was the child of
the former Secretary of State in Obama?s admin-
istration born??, single-relation questions are still
far from completely solved. Even in this restricted
domain there are a large number of paraphrases of
the same question. That is to say that the problem
of mapping from a question to a particular relation
and entity in the KB is non-trivial.
In this paper, we propose a semantic parsing
framework tailored to single-relation questions.
At the core of our approach is a novel semantic
similarity model using convolutional neural net-
works. Leveraging the question paraphrase data
mined from the WikiAnswers corpus by Fader et
al. (2013), we train two semantic similarity mod-
els: one links a mention from the question to an
entity in the KB and the other maps a relation pat-
tern to a relation. The answer to the question can
thus be derived by finding the relation?entity triple
r(e
1
, e
2
) in the KB and returning the entity not
mentioned in the question. By using a general se-
mantic similarity model to match patterns and re-
lations, as well as mentions and entities, our sys-
tem outperforms the existing rule learning system,
PARALEX (Fader et al, 2013), with higher pre-
cision at all the recall points when answering the
questions in the same test set. The highest achiev-
able F
1
score of our system is 0.61, versus 0.54 of
PARALEX.
The rest of the paper is structured as follows.
We first survey related work in Sec. 2, followed by
the problem definition and the high-level descrip-
tion of our approach in Sec. 3. Sec. 4 details our
semantic models and Sec. 5 shows the experimen-
tal results. Finally, Sec. 6 concludes the paper.
2 Related Work
Semantic parsing of questions, which maps nat-
ural language questions to database queries, is
a critical component for KB-supported QA. An
early example of this research is the semantic
parser for answering geography-related questions,
learned using inductive logic programming (Zelle
and Mooney, 1996). Research in this line origi-
nally used small, domain-specific databases, such
as GeoQuery (Tang and Mooney, 2001; Liang et
643
al., 2013). Very recently, researchers have started
developing semantic parsers for large, general-
domain knowledge bases like Freebase and DB-
pedia (Cai and Yates, 2013; Berant et al, 2013;
Kwiatkowski et al, 2013). Despite significant
progress, the problem remains challenging. Most
methods have not yet been scaled to large KBs
that can support general open-domain QA. In con-
trast, Fader et al (2013) proposed the PARALEX
system, which targets answering single-relation
questions using an automatically created knowl-
edge base, ReVerb (Fader et al, 2011). By
applying simple seed templates to the KB and
by leveraging community-authored paraphrases of
questions from WikiAnswers, they successfully
demonstrated a high-quality lexicon of pattern-
matching rules can be learned for this restricted
form of semantic parsing.
The other line of work related to our approach
is continuous representations for semantic simi-
larity, which has a long history and is still an
active research topic. In information retrieval,
TF-IDF vectors (Salton and McGill, 1983), latent
semantic analysis (Deerwester et al, 1990) and
topic models (Blei et al, 2003) take the bag-of-
words approach, which captures well the contex-
tual information for documents, but is often too
coarse-grained to be effective for sentences. In
a separate line of research, deep learning based
techniques have been proposed for semantic un-
derstanding (Mesnil et al, 2013; Huang et al,
2013; Shen et al, 2014b; Salakhutdinov and Hin-
ton, 2009; Tur et al, 2012). We adapt the work
of (Huang et al, 2013; Shen et al, 2014b) for mea-
suring the semantic distance between a question
and relational triples in the KB as the core compo-
nent of our semantic parsing approach.
3 Problem Definition & Approach
In this paper, we focus on using a knowledge
base to answer single-relation questions. A single-
relation question is defined as a question com-
posed of an entity mention and a binary rela-
tion description, where the answer to this ques-
tion would be an entity that has the relation with
the given entity. An example of a single-relation
question is ?When were DVD players invented??
The entity is dvd-player and the relation is
be-invent-in. The answer can thus be de-
scribed as the following lambda expression:
?x. be-invent-in(dvd-player, x)
Q? RP ?M (1)
RP ? when were X invented (2)
M ? dvd players (3)
when were X invented
? be-invent-in (4)
dvd players
? dvd-player (5)
Figure 1: A potential semantic parse of the ques-
tion ?When were DVD players invented??
A knowledge base in this work can be simply
viewed as a collection of binary relation instances
in the form of r(e
1
, e
2
), where r is the relation and
e
1
and e
2
are the first and second entity arguments.
Single-relation questions are perhaps the easiest
form of questions that can directly be answered
by a knowledge base. If the mapping of the re-
lation and entity in the question can be correctly
resolved, then the answer can be derived by a sim-
ple table lookup, assuming that the fact exists in
the KB. However, due to the large number of para-
phrases of the same question, identifying the map-
ping accurately remains a difficult problem.
Our approach in this work can be viewed as a
simple semantic parser tailored to single-relation
questions, powered by advanced semantic similar-
ity models to handle the paraphrase issue. Given a
question, we first separate it into two disjoint parts:
the entity mention and the relation pattern. The
entity mention is a subsequence of consecutive
words in the question, where the relation pattern
is the question where the mention is substituted
by a special symbol. The mapping between the
pattern and the relation in the KB, as well as the
mapping between the mention and the entity are
determined by corresponding semantic similarity
models. The high-level approach can be viewed
as a very simple context-free grammar, which is
shown in Figure 1.
The probability of the rule in (1) is 1 since
we assume the input is a single-relation ques-
tion. For the exact decomposition of the ques-
tion (e.g., (2), (3)), we simply enumerate all com-
binations and assign equal probabilities to them.
The performance of this approach depends mainly
on whether the relation pattern and entity mention
can be resolved correctly (e.g., (4), (5)). To deter-
644
15K 15K 15K 15K 15K
500 500 500
max max
...
...
... max
500
...
...
Word hashing layer: ft
Convolutional layer: ht
Max pooling layer: v
Semantic layer: y
     <s>             w1              w2           wT             <s>Word sequence: xt
Word hashing matrix: Wf
Convolution matrix: Wc
Max pooling operation
Semantic projection matrix: Ws
... ...
500
Figure 2: The CNNSM maps a variable-length
word sequence to a low-dimensional vector in a
latent semantic space. A word contextual window
size (i.e., the receptive field) of three is used in the
illustration. Convolution over word sequence via
learned matrix W
c
is performed implicitly via the
earlier word hashing layer?s mapping with a local
receptive field. The max operation across the se-
quence is applied for each of 500 feature dimen-
sions separately.
mine the probabilities of such mappings, we pro-
pose using a semantic similarity model based on
convolutional neural networks, which is the tech-
nical focus in this paper.
4 Convolutional Neural Network based
Semantic Model
Following (Collobert et al, 2011; Shen et al,
2014b), we develop a new convolutional neural
network (CNN) based semantic model (CNNSM)
for semantic parsing. The CNNSM first uses a
convolutional layer to project each word within a
context window to a local contextual feature vec-
tor, so that semantically similar word-n-grams are
projected to vectors that are close to each other
in the contextual feature space. Further, since the
overall meaning of a sentence is often determined
by a few key words in the sentence, CNNSM uses
a max pooling layer to extract the most salient lo-
cal features to form a fixed-length global feature
vector. The global feature vector can be then fed
to feed-forward neural network layers to extract
non-linear semantic features. The architecture of
the CNNSM is illustrated in Figure 2. In what fol-
lows, we describe each layer of the CNNSM in
detail, using the annotation illustrated in Figure 2.
In our model, we leverage the word hash-
ing technique proposed in (Huang et al, 2013)
where we first represent a word by a letter-
trigram count vector. For example, given a
word (e.g., cat), after adding word boundary sym-
bols (e.g., #cat#), the word is segmented into a se-
quence of letter-n-grams (e.g., letter-trigrams: #-
c-a, c-a-t, a-t-#). Then, the word is represented
as a count vector of letter-trigrams. For exam-
ple, the letter-trigram representation of ?cat? is:
In Figure 2, the word hashing matrix W
f
de-
notes the transformation from a word to its letter-
trigram count vector, which requires no learning.
Word hashing not only makes the learning more
scalable by controlling the size of the vocabulary,
but also can effectively handle the OOV issues,
sometimes due to spelling mistakes. Given the
letter-trigram based word representation, we rep-
resent a word-n-gram by concatenating the letter-
trigram vectors of each word, e.g., for the t-th
word-n-gram at the word-n-gram layer, we have:
l
t
=
[
f
T
t?d
, ? ? ? , f
T
t
, ? ? ? , f
T
t+d
]
T
, t = 1, ? ? ? , T
where f
t
is the letter-trigram representation of the
t-th word, and n = 2d + 1 is the size of the con-
textual window. The convolution operation can
be viewed as sliding window based feature extrac-
tion. It captures the word-n-gram contextual fea-
tures. Consider the t-th word-n-gram, the convo-
lution matrix projects its letter-trigram representa-
tion vector l
t
to a contextual feature vector h
t
. As
shown in Figure 2, h
t
is computed by
h
t
= tanh(W
c
? l
t
), t = 1, ? ? ? , T
where W
c
is the feature transformation matrix, as
known as the convolution matrix, which are shared
among all word n-grams. The output of the con-
volutional layer is a sequence of local contextual
feature vectors, one for each word (within a con-
textual window). Since many words do not have
significant influence on the semantics of the sen-
tence, we want to retain in the global feature vector
only the salient features from a few key words. For
this purpose, we use a max operation, also known
as max pooling, to force the network to retain only
645
the most useful local features produced by the con-
volutional layers. Referring to the max-pooling
layer of Figure 2, we have
v(i) = max
t=1,??? ,T
{f
t
(i)}, i = 1, ? ? ? ,K
where v(i) is the i-th element of the max pool-
ing layer v, h
t
(i) is the i-th element of the t-th
local feature vector h
t
. K is the dimensionality
of the max pooling layer, which is the same as
the dimensionality of the local contextual feature
vectors {h
t
}. One more non-linear transformation
layer is further applied on top of the global feature
vector v to extract the high-level semantic repre-
sentation, denoted by y. As shown in Figure 2, we
have y = tanh(W
s
? v), where v is the global fea-
ture vector after max pooling, W
s
is the semantic
projection matrix, and y is the vector representa-
tion of the input query (or document) in latent se-
mantic space. Given a pattern and a relation, we
compute their relevance score by measuring the
cosine similarity between their semantic vectors.
The semantic relevance score between a pattern Q
and a relation R is defined as the cosine score of
their semantic vectors y
Q
and y
R
.
We train two CNN semantic models from sets of
pattern?relation and mention?entity pairs, respec-
tively. Following (Huang et al, 2013), for every
pattern, the corresponding relation is treated as a
positive example and 100 randomly selected other
relations are used as negative examples. The set-
ting for the mention?entity model is similar.
The posterior probability of the positive relation
given the pattern is computed based on the cosine
scores using softmax:
P (R
+
|Q) =
exp(? ? cos(y
R
+ , y
Q
))
?
R
?
exp(? ? cos(y
R
?
, y
Q
))
where ? is a scaling factor set to 5. Model train-
ing is done by maximizing the log-posteriori us-
ing stochastic gradient descent. More detail can
be found in (Shen et al, 2014a).
5 Experiments
In order to provide a fair comparison to previ-
ous work, we experimented with our approach
using the PARALAX dataset (Fader et al, 2013),
which consists of paraphrases of questions mined
from WikiAnswers and answer triples from Re-
Verb. In this section, we briefly introduce the
dataset, describe the system training and evalua-
tion processes and, finally, present our experimen-
tal results.
5.1 Data & Model Training
The PARALEX training data consists of ap-
proximately 1.8 million pairs of questions and
single-relation database queries, such as ?When
were DVD players invented??, paired with
be-invent-in(dvd-player,?). For eval-
uation, the authors further sampled 698 questions
that belong to 37 clusters and hand labeled the an-
swer triples returned by their systems.
To train our two CNN semantic models, we
derived two parallel corpora based on the PAR-
ALEX training data. For relation patterns, we first
scanned the original training corpus to see if there
was an exact surface form match of the entity (e.g.,
dvd-player would map to ?DVD player? in the
question). If an exact match was found, then the
pattern would be derived by replacing the mention
in the question with the special symbol. The corre-
sponding relation of this pattern was thus the rela-
tion used in the original database query, along with
the variable argument position (i.e., 1 or 2, indicat-
ing whether the answer entity was the first or sec-
ond argument of the relation). In the end, we de-
rived about 1.2 million pairs of patterns and rela-
tions. We then applied these patterns to all the 1.8
million training questions, which helped discover
160 thousand new mentions that did not have the
exact surface form matches to the entities.
When training the CNNSM for the pattern?
relation similarity measure, we randomly split the
1.2 million pairs of patterns and relations into two
sets: the training set of 1.19 million pairs, and
the validation set of 12 thousand pairs for hyper-
parameter tuning. Data were tokenized by re-
placing hyphens with blank spaces. In the ex-
periment, we used a context window (i.e., the re-
ceptive field) of three words in the convolutional
neural networks. There were 15 thousand unique
letter-trigrams observed in the training set (used
for word hashing). Five hundred neurons were
used in the convolutional layer, the max-pooling
layer and the final semantic layer, respectively.
We used a learning rate of 0.002 and the train-
ing converged after 150 iterations. A similar set-
ting was used for the CNNSM for the mention?
entity model, which was trained on 160 thousand
mention-entity pairs.
5.2 Results
We used the same test questions in the PARALEX
dataset to evaluate whether our system could find
646
F1
Precision Recall MAP
CNNSM
pm
0.57 0.58 0.57 0.28
CNNSM
p
0.54 0.61 0.49 0.20
PARALEX 0.54 0.77 0.42 0.22
Table 1: Performance of two variations of our sys-
tems, compared with the PARALEX system.
the answers from the ReVerb database. Because
our systems might find triples that were not re-
turned by the PARALEX systems, we labeled these
new question?triple pairs ourselves.
Given a question, the system first enumerated
all possible decompositions of the mentions and
patterns, as described earlier. We then computed
the similarity scores between the pattern and all
relations in the KB and retained 150 top-scoring
relation candidates. For each selected relation, the
system then checked all triples in the KB that had
this relation and computed the similarity score be-
tween the mention and corresponding argument
entity. The product of the probabilities of these
two models, which are derived from the cosine
similarity scores using softmax as described in
Sec. 4, was used as the final score of the triple for
ranking the answers. The top answer triple was
used to compute the precision and recall of the sys-
tem when reporting the system performance. By
limiting the systems to output only answer triples
with scores higher than a predefined threshold, we
could control the trade-off between recall and pre-
cision and thus plot the precision?recall curve.
Table 1 shows the performance in F
1
, preci-
sion, recall and mean average precision of our sys-
tems and PARALEX. We provide two variations
here. CNNSM
pm
is the full system and consists
of two semantic similarity models for pattern?
relation and mention?entity. The other model,
CNNSM
p
, only measures the similarity between
the patterns and relations, and maps a mention to
an entity when they have the same surface form.
Since the trade-off between precision and re-
call can be adjusted by varying the threshold, it
is more informative to compare systems on the
precision?recall curves, which are shown in Fig-
ure 3. As we can observe from the figure, the
precision of our CNNSM
pm
system is consistently
higher than PARALEX across all recall regions.
The CNNSM
m
system also performs similarly to
CNNSM
pm
in the high precision regime, but is in-
ferior when recall is higher. This is understandable
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6
Pre
cis
ion
Recall
  CNNSMpm  CNNSMp  Paralex
Figure 3: The precision?recall curves of the two
variations of our systems and PARALEX.
since the system does not match mentions with
entities of different surface forms (e.g., ?Robert
Hooke? to ?Hooke?). Notice that the highest F
1
values of them are 0.61 and 0.56, compared to
0.54 of PARALEX. Tuning the thresholds using a
validation set would be needed if there is a metric
(e.g., F
1
) that specifically needs to be optimized.
6 Conclusions
In this work, we propose a semantic parsing
framework for single-relation questions. Com-
pared to the existing work, our key insight is to
match relation patterns and entity mentions using
a semantic similarity function rather than lexical
rules. Our similarity model is trained using convo-
lutional neural networks with letter-trigrams vec-
tors. This design helps the model go beyond bag-
of-words representations and handles the OOV is-
sue. Our method achieves higher precision on the
QA task than the previous work, PARALEX, con-
sistently at different recall points.
Despite the strong empirical performance, our
system has room for improvement. For in-
stance, due to the variety of entity mentions in
the real world, the parallel corpus derived from
the WikiAnswers data and ReVerb KB may not
contain enough data to train a robust entity link-
ing model. Replacing this component with a
dedicated entity linking system could improve
the performance and also reduce the number of
pattern/mention candidates when processing each
question. In the future, we would like to extend
our method to other more structured KBs, such as
Freebase, and to explore approaches to extend our
system to handle multi-relation questions.
647
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Qingqing Cai and Alexander Yates. 2013. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 423?433,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference of Em-
pirical Methods in Natural Language Processing
(EMNLP ?11), Edinburgh, Scotland, UK, July 27-
31.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1608?1618,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management, pages 2333?2338.
ACM.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969?978.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw
Hill.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014a. A convolutional latent
semantic model for web search. Technical Report
MSR-TR-2014-55, Microsoft Research.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014b. Learning semantic
representations using convolutional neural networks
for web search. In Proceedings of the Companion
Publication of the 23rd International Conference on
World Wide Web Companion, pages 373?374.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Machine Learn-
ing: ECML 2001, pages 466?477. Springer.
Gokhan Tur, Li Deng, Dilek Hakkani-Tur, and Xi-
aodong He. 2012. Towards deeper understanding:
deep convex networks for semantic utterance classi-
fication. In Acoustics, Speech and Signal Processing
(ICASSP), 2012 IEEE International Conference on,
pages 5045?5048. IEEE.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Confer-
ence on Artificial Intelligence, pages 1050?1055.
648
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247?256,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning Discriminative Projections for Text Similarity Measures
Wen-tau Yih Kristina Toutanova John C. Platt Christopher Meek
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,kristout,jplatt,meek}@microsoft.com
Abstract
Traditional text similarity measures consider
each term similar only to itself and do not
model semantic relatedness of terms. We pro-
pose a novel discriminative training method
that projects the raw term vectors into a com-
mon, low-dimensional vector space. Our ap-
proach operates by finding the optimal matrix
to minimize the loss of the pre-selected sim-
ilarity function (e.g., cosine) of the projected
vectors, and is able to efficiently handle a
large number of training examples in the high-
dimensional space. Evaluated on two very dif-
ferent tasks, cross-lingual document retrieval
and ad relevance measure, our method not
only outperforms existing state-of-the-art ap-
proaches, but also achieves high accuracy at
low dimensions and is thus more efficient.
1 Introduction
Measures of text similarity have many applications
and have been studied extensively in both the NLP
and IR communities. For example, a combination
of corpus and knowledge based methods have been
invented for judging word similarity (Lin, 1998;
Agirre et al, 2009). Similarity derived from a large-
scale Web corpus has been used for automatically
extending lists of typed entities (Vyas and Pantel,
2009). Judging the degree of similarity between
documents is also fundamental to classical IR prob-
lems such as document retrieval (Manning et al,
2008). In all these applications, the vector-based
similarity method is the most widely used. Term
vectors are first constructed to represent the origi-
nal text objects, where each term is associated with
a weight indicating its importance. A pre-selected
function operating on these vectors, such as cosine,
is used to output the final similarity score. This ap-
proach has not only proved to be effective, but is also
efficient. For instance, only the term vectors rather
than the raw data need to be stored. A pruned inverse
index can be built to support fast similarity search.
However, the main weakness of this term-vector
representation is that different but semantically re-
lated terms are not matched and cannot influence
the final similarity score. As an illustrative ex-
ample, suppose the two compared term-vectors
are: {purchase:0.4, used:0.3, automobile:0.2} and
{buy:0.3, pre-owned: 0.5, car: 0.4}. Even though
the two vectors represent very similar concepts, their
similarity score will be 0, for functions like cosine,
overlap or Jaccard. Such an issue is more severe
in cross-lingual settings. Because language vocab-
ularies typically have little overlap, term-vector rep-
resentations are completely inapplicable to measur-
ing similarity between documents in different lan-
guages. The general strategy to handle this prob-
lem is to map the raw representation to a common
concept space, where extensive approaches have
been proposed. Existing methods roughly fall into
three categories. Generative topic models like La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
assume that the terms are sampled by probabil-
ity distributions governed by hidden topics. Lin-
ear projection methods like Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) learn a projec-
tion matrix and map the original term-vectors to the
dense low-dimensional space. Finally, metric learn-
ing approaches for high-dimensional spaces have
247
also been proposed (Davis and Dhillon, 2008).
In this paper, we propose a new projection learn-
ing framework, Similarity Learning via Siamese
Neural Network (S2Net), to discriminatively learn
the concept vector representations of input text ob-
jects. Following the general Siamese neural network
architecture (Bromley et al, 1993), our approach
trains two identical networks concurrently. The in-
put layer corresponds to the original term vector
and the output layer is the projected concept vector.
Model parameters (i.e., the weights on the edges)
are equivalently the projection matrix. Given pairs
of raw term vectors and their labels (e.g., similar or
not), the model is trained by minimizing the loss of
the similarity scores of the output vectors. S2Net
is closely related to the linear projection and met-
ric learning approaches, but enjoys additional ad-
vantages over existing methods. While its model
form is identical to that of LSA, CCA and OPCA, its
objective function can be easily designed to match
the true evaluation metric of interest for the target
task, which leads to better performance. Compared
to existing high-dimensional metric learning meth-
ods, S2Net can learn from a much larger number
of labeled examples. These two properties are cru-
cial in helping S2Net outperform existing methods.
For retrieving comparable cross-lingual documents,
S2Net achieves higher accuracy than the best ap-
proach (OPCA) at a much lower dimension of the
concept space (500 vs. 2,000). In a monolingual
setting, where the task is to judge the relevance of
an ad landing page to a query, S2Net alo has the
best performance when compared to a number of ap-
proaches, including the raw TFIDF cosine baseline.
In the rest of the paper, we first survey some
existing work in Sec. 2, with an emphasis on ap-
proaches included in our experimental comparison.
We present our method in Sec. 3 and report on an
extensive experimental study in Sec. 4. Other re-
lated work is discussed in Sec. 5 and finally Sec. 6
concludes the paper.
2 Previous Work
In this section, we briefly review existing ap-
proaches for mapping high-dimensional term-
vectors to a low-dimensional concept space.
2.1 Generative Topic Models
Probabilistic Latent Semantic Analysis
(PLSA) (Hofmann, 1999) assumes that each
document has a document-specific distribution ?
over some finite number K of topics, where each
token in a document is independently generated
by first selecting a topic z from a multinomial
distribution MULTI(?), and then sampling a word
token from the topic-specific word distribution
for the chosen topic MULTI(?z). Latent Dirichlet
Allocation (LDA) (Blei et al, 2003) generalizes
PLSA to a proper generative model for documents
and places Dirichlet priors over the parameters
? and ?. In the experiments in this paper, our
implementation of PLSA is LDA with maximum a
posteriori (MAP) inference, which was shown to be
comparable to the current best Bayesian inference
methods for LDA (Asuncion et al, 2009).
Recently, these topic models have been general-
ized to handle pairs or tuples of corresponding doc-
uments, which could be translations in multiple lan-
guages, or documents in the same language that are
considered similar. For instance, the Poly-lingual
Topic Model (PLTM) (Mimno et al, 2009) is an
extension to LDA that views documents in a tu-
ple as having a shared topic vector ?. Each of the
documents in the tuple uses ? to select the topics
z of tokens, but could use a different (language-
specific) word-topic-distribution MULTI(?Lz ). Two
additional models, Joint PLSA (JPLSA) and Cou-
pled PLSA (CPLSA) were introduced in (Platt et al,
2010). JPLSA is a close variant of PLTM when doc-
uments of all languages share the same word-topic
distribution parameters, and MAP inference is per-
formed instead of Bayesian. CPLSA extends JPLSA
by constraining paired documents to not only share
the same prior topic distribution ?, but to also have
similar fractions of tokens assigned to each topic.
This constraint is enforced on expectation using pos-
terior regularization (Ganchev et al, 2009).
2.2 Linear Projection Methods
The earliest method for projecting term vectors into
a low-dimensional concept space is Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990). LSA
models all documents in a corpus using a n ?
d document-term matrix D and performs singular
248
value decomposition (SVD) on D. The k biggest
singular values are then used to find the d ? k pro-
jection matrix. Instead of SVD, LSA can be done
by applying eigen-decomposition on the correlation
matrix between terms C = DTD. This is very sim-
ilar to principal component analysis (PCA), where a
covariance matrix between terms is used. In prac-
tice, term vectors are very sparse and their means
are close to 0. Therefore, the correlation matrix is in
fact close to the covariance matrix.
To model pairs of comparable documents,
LSA/PCA has been extended in different ways. For
instance, Cross-language Latent Semantic Indexing
(CL-LSI) (Dumais et al, 1997) applies LSA to con-
catenated comparable documents from different lan-
guages. Oriented Principal Component Analysis
(OPCA) (Diamantaras and Kung, 1996; Platt et al,
2010) solves a generalized eigen problem by intro-
ducing a noise covariance matrix to ensure that com-
parable documents can be projected closely. Canon-
ical Correlation Analysis (CCA) (Vinokourov et al,
2003) finds projections that maximize the cross-
covariance between the projected vectors.
2.3 Distance Metric Learning
Measuring the similarity between two vectors can be
viewed as equivalent to measuring their distance, as
the cosine score has a bijection mapping to the Eu-
clidean distance of unit vectors. Most work on met-
ric learning learns a Mahalanobis distance, which
generalizes the standard squared Euclidean distance
by modeling the similarity of elements in different
dimensions using a positive semi-definite matrix A.
Given two vectors x and y, their squared Maha-
lanobis distance is: dA = (x ? y)TA(x ? y).
However, the computational complexity of learn-
ing a general Mahalanobis matrix is at least O(n2),
where n is the dimensionality of the input vectors.
Therefore, such methods are not practical for high
dimensional problems in the text domain.
In order to tackle this issue, special metric
learning approaches for high-dimensional spaces
have been proposed. For example, high dimen-
sion low-rank (HDLR) metric learning (Davis and
Dhillon, 2008) constrains the form of A = UUT ,
where U is similar to the regular projection ma-
trix, and adapts information-theoretic metric learn-
ing (ITML) (Davis et al, 2007) to learn U.
sim(vp,vq) 
1t
dtvp vq 
it
1c
kcjc
'tw
tw
Figure 1: Learning concept vectors. The output layer
consists of a small number of concept nodes, where the
weight of each node is a linear combination of all the
original term weights.
3 Similarity Learning via Siamese Neural
Network (S2Net)
Given pairs of documents with their labels, such as
binary or real-valued similarity scores, our goal is
to construct a projection matrix that maps the corre-
sponding term-vectors into a low-dimensional con-
cept space such that similar documents are close
when projected into this space. We propose a sim-
ilarity learning framework via Siamese neural net-
work (S2Net) to learn the projection matrix directly
from labeled data. In this section, we introduce its
model design and describe the training process.
3.1 Model Design
The network structure of S2Net consists of two lay-
ers. The input layer corresponds to the raw term vec-
tor, where each node represents a term in the original
vocabulary and its associated value is determined by
a term-weighting function such as TFIDF. The out-
put layer is the learned low-dimensional vector rep-
resentation that captures relationships among terms.
Similarly, each node of the output layer is an ele-
ment in the new concept vector. In this work, the
final similarity score is calculated using the cosine
function, which is the standard choice for document
similarity (Manning et al, 2008). Our framework
can be easily extended to other similarity functions
as long as they are differentiable.
The output of each concept node is a linear com-
249
bination of the weights of all the terms in the orig-
inal term vector. In other words, these two layers
of nodes form a complete bipartite graph as shown
in Fig. 1. The output of a concept node cj is thus
defined as:
tw?(cj) =
?
ti?V
?ij ? tw(ti) (1)
Notice that it is straightforward to add a non-linear
activation function (e.g., sigmoid) in Eq. (1), which
can potentially lead to better results. However, in
the current design, the model form is exactly the
same as the low-rank projection matrix derived by
PCA, OPCA or CCA, which facilitates comparison
to alternative projection methods. Using concise
matrix notation, let f be a raw d-by-1 term vector,
A = [?ij ]d?k the projection matrix. g = AT f is
thus the k-by-1 projected concept vector.
3.2 Loss Function and Training Procedure
For a pair of term vectors fp and fq, their similar-
ity score is defined by the cosine value of the corre-
sponding concept vectors gp and gq according to the
projection matrix A.
simA(fp, fq) =
gTp gq
||gp||||gq||
,
where gp = AT fp and gq = AT fq. Let ypq be
the true label of this pair. The loss function can
be as simple as the mean-squared error 12(ypq ?
simA(fp, fq))2. However, in many applications, the
similarity scores are used to select the closest text
objects given the query. For example, given a query
document, we only need to have the comparable
document in the target language ranked higher than
any other documents. In this scenario, it is more
important for the similarity measure to yield a good
ordering than to match the target similarity scores.
Therefore, we use a pairwise learning setting by con-
sidering a pair of similarity scores (i.e., from two
vector pairs) in our learning objective.
Consider two pairs of term vectors (fp1 , fq1) and
(fp2 , fq2), where the first pair has higher similarity.
Let ? be the difference of their similarity scores.
Namely, ? = simA(fp1 , fq1)? simA(fp2 , fq2). We
use the following logistic loss over ?, which upper-
bounds the pairwise accuracy (i.e., 0-1 loss):
L(?;A) = log(1 + exp(???)) (2)
Because of the cosine function, we add a scaling
factor ? that magnifies ? from [?2, 2] to a larger
range, which helps penalize more on the prediction
errors. Empirically, the value of ? makes no dif-
ference as long as it is large enough1. In the ex-
periments, we set the value of ? to 10. Optimizing
the model parameters A can be done using gradi-
ent based methods. We derive the gradient of the
whole batch and apply the quasi-Newton optimiza-
tion method L-BFGS (Nocedal and Wright, 2006)
directly. For a cleaner presentation, we detail the
gradient derivation in Appendix A. Given that the
optimization problem is not convex, initializing the
model from a good projection matrix often helps re-
duce training time and may lead to convergence to
a better local minimum. Regularization can be done
by adding a term ?2 ||A ? A0||
2 in Eq. (2), which
forces the learned model not to deviate too much
from the starting point (A0), or simply by early stop-
ping. Empirically we found that the latter is more
effective and it is used in the experiments.
4 Experiments
We compare S2Net experimentally with existing ap-
proaches on two very different tasks: cross-lingual
document retrieval and ad relevance measures.
4.1 Comparable Document Retrieval
With the growth of multiple languages on the Web,
there is an increasing demand of processing cross-
lingual documents. For instance, machine trans-
lation (MT) systems can benefit from training on
sentences extracted from parallel or comparable
documents retrieved from the Web (Munteanu and
Marcu, 2005). Word-level translation lexicons can
also be learned from comparable documents (Fung
and Yee, 1998; Rapp, 1999). In this cross-lingual
document retrieval task, given a query document in
one language, the goal is to find the most similar
document from the corpus in another language.
4.1.1 Data & Setting
We followed the comparable document retrieval
setting described in (Platt et al, 2010) and evalu-
ated S2Net on the Wikipedia dataset used in that pa-
per. This data set consists of Wikipedia documents
1Without the ? parameter, the model still outperforms other
baselines in our experiments, but with a much smaller gain.
250
in two languages, English and Spanish. An article
in English is paired with a Spanish article if they
are identified as comparable across languages by the
Wikipedia community. To conduct a fair compari-
son, we use the same term vectors and data split as in
the previous study. The numbers of document pairs
in the training/development/testing sets are 43,380,
8,675 and 8,675, respectively. The dimensionality
of the raw term vectors is 20,000.
The models are evaluated by using each English
document as query against all documents in Span-
ish and vice versa; the results from the two direc-
tions are averaged. Performance is evaluated by two
metrics: the Top-1 accuracy, which tests whether
the document with the highest similarity score is the
true comparable document, and the Mean Recipro-
cal Rank (MRR) of the true comparable.
When training the S2Net model, all the compara-
ble document pairs are treated as positive examples
and all other pairs are used as negative examples.
Naively treating these 1.8 billion pairs (i.e., 433802)
as independent examples would make the training
very inefficient. Fortunately, most computation in
deriving the batch gradient can be reused via com-
pact matrix operations and training can still be done
efficiently. We initialized the S2Net model using the
matrix learned by OPCA, which gave us the best per-
formance on the development set2.
Our approach is compared with most methods
studied in (Platt et al, 2010), including the best per-
forming one. For CL-LSI, OPCA, and CCA, we in-
clude results from that work directly. In addition, we
re-implemented and improved JPLSA and CPLSA
by changing three settings: we used separate vocab-
ularies for the two languages as in the Poly-lingual
topic model (Mimno et al, 2009), we performed 10
EM iterations for folding-in instead of only one, and
we used the Jensen-Shannon distance instead of the
L1 distance. We also attempted to apply the HDLR
algorithm. Because this algorithm does not scale
well as the number of training examples increases,
we used 2,500 positive and 2,500 negative docu-
ment pairs for training. Unfortunately, among all the
2S2Net outperforms OPCA when initialized from a random
or CL-LSI matrix, but with a smaller gain. For example, when
the number of dimensions is 1000, the MRR score of OPCA
is 0.7660. Starting from the CL-LSI and OPCA matrices, the
MRR scores of S2Net are 0.7745 and 0.7855, respectively.
Figure 2: Mean reciprocal rank versus dimension for
Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
hyper-parameter settings we tested, HDLR could not
outperform its initial model, which was the OPCA
matrix. Therefore we omit these results.
4.1.2 Results
Fig. 2 shows the MRR performance of all meth-
ods on the development set, across different dimen-
sionality settings of the concept space. As can be
observed from the figure, higher dimensions usually
lead to better results. In addition, S2Net consistently
performs better than all other methods across differ-
ent dimensions. The gap is especially large when
projecting input vectors to a low-dimensional space,
which is preferable for efficiency. For instance, us-
ing 500 dimensions, S2Net aleady performs as well
as OPCA with 2000 dimensions.
Table 1 shows the averaged Top-1 accuracy and
MRR scores of all methods on the test set, where
the dimensionality for each method is optimized on
the development set (Fig. 2). S2Net clearly outper-
forms all other methods and the difference in terms
of accuracy is statistically significant3.
4.2 Ad Relevance
Paid search advertising is the main revenue source
that supports modern commercial search engines.
To ensure satisfactory user experience, it is impor-
tant to provide both relevant ads and regular search
3We use the unpaired t-test with Bonferroni correction and
the difference is considered statistically significant when the p-
value is less than 0.01.
251
Algorithm Dimension Accuracy MRR
S2Net 2000 0.7447 0.7973
OPCA 2000 0.7255 0.7734
CCA 1500 0.6894 0.7378
CPLSA 1000 0.6329 0.6842
JPLSA 1000 0.6079 0.6604
CL-LSI 5000 0.5302 0.6130
Table 1: Test results for comparable document retrieval
in Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
results. Previous work on ad relevance focuses on
constructing appropriate term-vectors to represent
queries and ad-text (Broder et al, 2008; Choi et al,
2010). In this section, we extend the work in (Yih
and Jiang, 2010) and show how S2Net can exploit
annotated query?ad pairs to improve the vector rep-
resentation in this monolingual setting.
4.2.1 Data & Tasks
The ad relevance dataset we used consists of
12,481 unique queries randomly sampled from the
logs of the Bing search engine. For each query, a
number of top ranked ads are selected, which results
in a total number of 567,744 query-ad pairs in the
dataset. Each query-ad pair is manually labeled as
same, subset, superset or disjoint. In our experi-
ment, when the task is a binary classification prob-
lem, pairs labeled as same, subset, or superset are
considered relevant, and pairs labeled as disjoint are
considered irrelevant. When pairwise comparisons
are needed in either training or evaluation, the rele-
vance order is same > subset = superset > disjoint.
The dataset is split into training (40%), validation
(30%) and test (30%) sets by queries.
Because a query string usually contains only a few
words and thus provides very little content, we ap-
plied the same web relevance feedback technique
used in (Broder et al, 2008) to create ?pseudo-
documents? to represent queries. Each query in our
data set was first issued to the search engine. The
result page with up to 100 snippets was used as the
pseudo-document to create the raw term vectors. On
the ad side, we used the ad landing pages instead
of the short ad-text. Our vocabulary set contains
29,854 words and is determined using a document
frequency table derived from a large collection of
Web documents. Only words with counts larger than
a pre-selected threshold are retained.
How the data is used in training depends on the
model. For S2Net, we constructed preference pairs
in the following way. For the same query, each rel-
evant ad is paired with a less relevant ad. The loss
function from Eq. (2) encourages achieving a higher
similarity score for the more relevant ad. For HDLR,
we used a sample of 5,000 training pairs of queries
and ads, as it was not able to scale to more train-
ing examples. For OPCA, CCA, PLSA and JPLSA,
we constructed a parallel corpus using only rele-
vant pairs of queries and ads, as the negative exam-
ples (irrelevant pairs of queries and ads) cannot be
used by these models. Finally, PCA and PLSA learn
the models from all training queries and documents
without using any relevance information.
We tested S2Net and other methods in two differ-
ent application scenarios. The first is to use the ad
relevance measure as an ad filter. When the similar-
ity score between a query and an ad is below a pre-
selected decision threshold, this ad is considered ir-
relevant to the query and will be filtered. Evaluation
metrics used for this scenario are the ROC analysis
and the area under the curve (AUC). The second one
is the ranking scenario, where the ads are selected
and ranked by their relevance scores. In this sce-
nario, the performance is evaluated by the standard
ranking metric, Normalized Discounted Cumulative
Gain (NDCG) (Jarvelin and Kekalainen, 2000).
4.2.2 Results
We first compare different methods in their AUC
and NDCG scores. TFIDF is the basic term vec-
tor representation with the TFIDF weighting (tf ?
log(N/df)). It is used as our baseline and also as
the raw input for S2Net, HDLR and other linear pro-
jection methods. Based on the results on the devel-
opment set, we found that PCA performs better than
OPCA and CCA. Therefore, we initialized the mod-
els of S2Net and HDLR using the PCA matrix. Ta-
ble 2 summarizes results on the test set. All models,
except TFIDF, use 1000 dimensions and their best
configuration settings selected on the validation set.
TFIDF is a very strong baseline on this monolin-
gual ad relevance dataset. Among all the methods
we tested, at dimension 1000, only S2Net outper-
forms the raw TFIDF cosine measure in every eval-
uation metric, and the difference is statistically sig-
252
AUC NDCG@1 NDCG@3 NDCG@5
S2Net 0.892 0.855 0.883 0.901
TFIDF 0.861 0.825 0.854 0.876
HDLR 0.855 0.826 0.856 0.877
CPLSA 0.853 0.845 0.872 0.890
PCA 0.848 0.815 0.847 0.870
OPCA 0.844 0.817 0.850 0.872
JPLSA 0.840 0.838 0.864 0.883
CCA 0.836 0.820 0.852 0.874
PLSA 0.835 0.831 0.860 0.879
Table 2: The AUC and NDCG scores of the cosine sim-
ilarity scores on different vector representations. The di-
mension for all models except TFIDF is 1000.
 0.3 0.4 0.5 0.6 0.7 0.8 0.9
 0.05  0.1  0.15  0.2  0.25True-Positive Rate False-Positive RateThe ROC Curves S2NetTFIDFHDLRCPLSA
Figure 3: The ROC curves of S2Net, TFIDF, HDLR and
CPLSA when the similarity scores are used as ad filters.
nificant4. In contrast, both CPLSA and HDLR have
higher NDCG scores but lower AUC values, and
OPCA/CCA perform roughly the same as PCA.
When the cosine scores of these vector represen-
tations are used as ad filters, their ROC curves (fo-
cusing on the low false-positive region) are shown
in Fig. 3. It can be clearly observed that the similar-
ity score computed based on vectors derived from
S2Net indeed has better quality, compared to the
raw TFIDF representation. Unfortunately, other ap-
proaches perform worse than TFIDF and their per-
formance in the low false-positive region is consis-
tent with the AUC scores.
Although ideally we would like the dimensional-
ity of the projected concept vectors to be as small
4For AUC, we randomly split the data into 50 subsets and
ran a paired-t test between the corresponding AUC scores. For
NDCG, we compared the DCG scores per query of the com-
pared models using the paired-t test. The difference is consid-
ered statistically significant when the p-value is less than 0.01.
as possible for efficient processing, the quality of
the concept vector representation usually degrades
as well. It is thus interesting to know the best trade-
off point between these two variables. Table 3 shows
the AUC and NDCG scores of S2Net at different di-
mensions, as well as the results achieved by TFIDF
and PCA, HDLR and CPLSA at 1000 dimensions.
As can be seen, S2Net surpasses TFIDF in AUC at
dimension 300 and keeps improving as the dimen-
sionality increases. Its NDCG scores are also con-
sistently higher across all dimensions.
4.3 Discussion
It is encouraging to find that S2Net achieves strong
performance in two very different tasks, given that
it is a conceptually simple model. Its empirical suc-
cess can be attributed to two factors. First, it is flex-
ible in choosing the loss function and constructing
training examples and is thus able to optimize the
model directly for the target task. Second, it can
be trained on a large number of examples. For ex-
ample, HDLR can only use a few thousand exam-
ples and is not able to learn a matrix better than its
initial model for the task of cross-lingual document
retrieval. The fact that linear projection methods
like OPCA/CCA and generative topic models like
JPLSA/CPLSA cannot use negative examples more
effectively also limits their potential.
In terms of scalability, we found that methods
based on eigen decomposition, such as PCA, OPCA
and CCA, take the least training time. The complex-
ity is decided by the size of the covariance matrix,
which is quadratic in the number of dimensions. On
a regular eight-core server, it takes roughly 2 to 3
hours to train the projection matrix in both experi-
ments. The training time of S2Net scales roughly
linearly to the number of dimensions and training
examples. In each iteration, performing the projec-
tion takes the most time in gradient derivation, and
the complexity is O(mnk), where m is the num-
ber of distinct term-vectors, n is the largest number
of non-zero elements in the sparse term-vectors and
k is the dimensionality of the concept space. For
cross-lingual document retrieval, when k = 1000,
each iteration takes roughly 48 minutes and about 80
iterations are required to convergence. Fortunately,
the gradient computation is easily parallelizable and
further speed-up can be achieved using a cluster.
253
TFIDF HDLR CPLSA PCA S2Net100 S2Net300 S2Net500 S2Net750 S2Net1000
AUC 0.861 0.855 0.853 0.848 0.855 0.879 0.880 0.888 0.892
NDCG@1 0.825 0.826 0.845 0.815 0.843 0.852 0.856 0.860 0.855
NDCG@3 0.854 0.856 0.872 0.847 0.871 0.879 0.881 0.884 0.883
NDCG@5 0.876 0.877 0.890 0.870 0.890 0.897 0.899 0.902 0.901
Table 3: The AUC and NDCG scores of S2Net at different dimensions. PCA, HDLR & CPLSA (at dimension 1000)
along with the raw TFIDF representation are used for reference.
5 Related Work
Although the high-level design of S2Net follows the
Siamese architecture (Bromley et al, 1993; Chopra
et al, 2005), the network construction, loss func-
tion and training process of S2Net are all differ-
ent compared to previous work. For example, tar-
geting the application of face verification, Chopra
et al (2005) used a convolutional network and de-
signed a contrastive loss function for optimizing a
Eucliden distance metric. In contrast, the network
of S2Net is equivalent to a linear projection ma-
trix and has a pairwise loss function. In terms of
the learning framework, S2Net is closely related to
several neural network based approaches, including
autoencoders (Hinton and Salakhutdinov, 2006) and
finding low-dimensional word representations (Col-
lobert and Weston, 2008; Turian et al, 2010). Archi-
tecturally, S2Net is also similar to RankNet (Burges
et al, 2005), which can be viewed as a Siamese neu-
ral network that learns a ranking function.
The strategy that S2Net takes to learn from la-
beled pairs of documents can be analogous to the
work of distance metric learning. Although high
dimensionality is not a problem to algorithms like
HDLR, it suffers from a different scalability issue.
As we have observed in our experiments, the al-
gorithm can only handle a small number of simi-
larity/dissimilarity constraints (i.e., the labeled ex-
amples), and is not able to use a large number of
examples to learn a better model. Empirically, we
also found that HDLR is very sensitive to the hyper-
parameter settings and its performance can vary sub-
stantially from iteration to iteration.
Other than the applications presented in this pa-
per, concept vectors have shown useful in traditional
IR tasks. For instance, Egozi et al (2008) use ex-
plicit semantic analysis to improve the retrieval re-
call by leveraging Wikipedia. In a companion pa-
per, we also demonstrated that various topic mod-
els including S2Net can enhance the ranking func-
tion (Gao et al, 2011). For text categorization, simi-
larity between terms is often encoded as kernel func-
tions embedded in the learning algorithms, and thus
increase the classification accuracy. Representative
approaches include latent semantic kernels (Cris-
tianini et al, 2002), which learns an LSA-based ker-
nel function from a document collection, and work
that computes term-similarity based on the linguis-
tic knowledge provided by WordNet (Basili et al,
2005; Bloehdorn and Moschitti, 2007).
6 Conclusions
In this paper, we presented S2Net, a discrimina-
tive approach for learning a projection matrix that
maps raw term-vectors to a low-dimensional space.
Our learning method directly optimizes the model
so that the cosine score of the projected vectors can
become a reliable similarity measure. The strength
of this model design has been shown empirically in
two very different tasks. For cross-lingual document
retrieval, S2Net significantly outperforms OPCA,
which is the best prior approach. For ad selection
and filtering, S2Net alo outperforms all methods we
compared it with and is the only technique that beats
the raw TFIDF vectors in both AUC and NDCG.
The success of S2Net is truly encouraging, and
we would like to explore different directions to fur-
ther enhance the model in the future. For instance, it
will be interesting to extend the model to learn non-
linear transformations. In addition, since the pairs of
text objects being compared often come from differ-
ent distributions (e.g., English documents vs. Span-
ish documents or queries vs. pages), learning two
different matrices instead of one could increase the
model expressivity. Finally, we would like to apply
S2Net to more text similarity tasks, such as word
similarity and entity recognition and discovery.
254
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, June.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In UAI.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics via
kernel-based learning. In CoNLL.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text clas-
sification. In ECIR, pages 307?318.
Andrei Z. Broder, Peter Ciccolo, Marcus Fontoura,
Evgeniy Gabrilovich, Vanja Josifovski, and Lance
Riedel. 2008. Search advertising using web relevance
feedback. In CIKM, pages 1013?1022.
Jane Bromley, James W. Bentz, Le?on Bottou, Isabelle
Guyon, Yann LeCun, Cliff Moore, Eduard Sa?ckinger,
and Roopak Shah. 1993. Signature verification us-
ing a ?Siamese? time delay neural network. Interna-
tional Journal Pattern Recognition and Artificial Intel-
ligence, 7(4):669?688.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
ICML.
Y. Choi, M. Fontoura, E. Gabrilovich, V. Josifovski,
M. Mediano, and B. Pang. 2010. Using landing pages
for sponsored search ad selection. In WWW.
Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with ap-
plication to face verification. In Proceedings of CVPR-
2005, pages 539?546.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In ICML.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002. Latent semantic kernels. Journal of Intelligent
Information Systems, 18(2?3):127?152.
Jason V. Davis and Inderjit S. Dhillon. 2008. Struc-
tured metric learning for high dimensional problems.
In KDD, pages 195?203.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S. Dhillon. 2007. Information-theoretic met-
ric learning. In ICML.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
linguistic information retrieval using latent seman-
tic indexing. In AAAI-97 Spring Symposium Series:
Cross-Language Text and Speech Retrieval.
Ofer Egozi, Evgeniy Gabrilovich, and Shaul Markovitch.
2008. Concept-based feature generation and selection
for information retrieval. In AAAI.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING-ACL.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In SIGIR.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507, July.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In SIGIR ?99, pages 50?57.
K. Jarvelin and J. Kekalainen. 2000. Ir evaluation meth-
ods for retrieving highly relevant documents. In SI-
GIR, pages 41?48.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL 98.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Pres.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
255
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation of
text via cross-language correlation analysis. In NIPS-
15.
Vishnu Vyas and Patrick Pantel. 2009. Semi-automatic
entity set refinement. In NAACL ?09, pages 290?298.
Wen-tau Yih and Ning Jiang. 2010. Similarity models
for ad relevance measures. In MLOAD - NIPS 2010
Workshop on online advertising.
Appendix A. Gradient Derivation
The gradient of the loss function in Eq. (2) can be
derived as follows.
?L(?,A)
?A
=
??
1 + exp(???)
??
?A
??
?A
=
?
?A
simA(fp1 , fq1)?
?
?A
simA(fp2 , fq2)
?
?A
simA(fp, fq) =
?
?A
cos(gp,gq),
where gp = AT fp and gq = AT fq are the projected
concept vectors of fq and fq. The gradient of the
cosine score can be further derived in the following
steps.
cos(gp,gq) =
gTp gq
?gp??gq?
?Ag
T
p gq = (?AA
T fp)gq + (?AA
T fq)gp
= fpgTq + fqg
T
p
?A
1
?gp?
= ?A(g
T
p gp)
? 12
= ?
1
2
(gTp gp)
? 32?A(g
T
p gp)
= ?(gTp gp)
? 32 fpgTp
?A
1
?gq?
= ?(gTq gq)
? 32 fqgTq
Let a, b, c be gTp gq, 1/?gp? and 1/?gq?, respec-
tively.
?A
gTp gq
?gp??gq?
= ? abc3fqgTq ? acb
3fpgTp
+ bc(fpgTq + fqg
T
p )
256
