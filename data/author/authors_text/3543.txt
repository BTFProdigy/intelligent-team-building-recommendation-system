Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 8?16,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Thai Sentence-Breaking for Large-Scale SMT 
 
 
Glenn Slayden 
thai-language.com  
glenn@thai-language.com 
Mei-Yuh Hwang 
Microsoft Research 
mehwang@microsoft.com 
Lee Schwartz 
Microsoft Research 
leesc@microsoft.com 
 
 
Abstract 
Thai language text presents challenges 
for integration into large-scale multi-
language statistical machine translation 
(SMT) systems, largely stemming from 
the nominal lack of punctuation and in-
ter-word space. For Thai sentence break-
ing, we describe a monolingual maxi-
mum entropy classifier with features that 
may be applicable to other languages 
such as Arabic, Khmer and Lao. We ap-
ply this sentence breaker to our large-
vocabulary, general-purpose, bidirec-
tional Thai-English SMT system, and 
achieve BLEU scores of around 0.20, 
reaching our threshold of releasing it as a 
free online service. 
1 Introduction 
NLP research has consolidated around the notion 
of the sentence as the fundamental unit of trans-
lation, a consensus which has fostered the devel-
opment powerful statistical and analytical ap-
proaches which incorporate an assumption of 
deterministic sentence delineation. As such sys-
tems become more sophisticated, languages for 
which this assumption is challenged receive in-
creased attention. Thai is one such language, 
since it uses space neither to distinguish syl-
lables from words or affixes, nor to unambi-
guously signal sentence boundaries. 
Written Thai has no sentence-end punctuation, 
but a space character is always present between 
sentences. There is generally no space between 
words, but a space character may appear within a 
sentence according to linguistic or prescriptive 
orthographic motivation (Wathabunditkul 2003), 
and these characteristics disqualify sentence-
breaking (SB) methods used for other languages, 
such as Palmer and Hearst (1997). Thai SB has 
therefore been regarded as the task of classifying 
each space that appears in a Thai source text as 
either sentence-breaking (sb) or non-sentence-
breaking (nsb). 
Several researchers have investigated Thai 
SB. Along with a discussion of Thai word break-
ing (WB), Aroonmanakun (2007) examines the 
issue. With a human study, he establishes that 
sentence breaks elicited from Thai informants 
exhibit varying degrees of consensus. Mittra-
piyanuruk and Sornlertlamvanich (2000) define 
part-of-speech (POS) tags for sb and nsb and 
train a trigram model over a POS-annotated cor-
pus. At runtime, they use the Viterbi algorithm 
to select the POS sequence with the highest 
probability, from which the corresponding space 
type is read back. Charoenpornsawat and Sornler-
tlamvanich (2001) apply Winnow, a multiplica-
tive trigger threshold classifier, to the problem. 
Their model has ten features: the number of 
words to the left and right, and the left-two and 
right-two POS tags and words. 
We present a monolingual Thai SB based on a 
maximum entropy (ME) classifier (Ratnaparkhi 
1996; Reynar and Ratnaparkhi, 1997) which is 
suitable for sentence-breaking SMT training data 
and runtime inputs. Our model uses a four token 
window of Thai lemmas, plus categorical fea-
tures, to describe the proximal environment of 
the space token under consideration, allowing 
runtime classification of space tokens with pos-
sibly unseen contexts. 
As our SB model relies on Thai WB, we re-
view our approach to this problem, plus related 
preprocessing, in the next section. Section 2 also 
discusses the complementary operation to WB, 
namely, the re-spacing of Thai text generated by 
SMT output. Section 3 details our SB model and 
evaluates its performance. We describe the inte-
gration of this work with our large-scale SMT 
system in Section 4. We draw conclusions in 
Section 5. 
8
2 Pre- and Post-processing 
As will be shown in Section 3, our sentence 
breaker relies on Thai WB. In turn, with the aim 
of minimizing WB errors, we perform Unicode 
character sequence normalization prior to WB. 
As output byproducts, our WB analysis readily 
identifies certain types of named entities which 
we propagate into our THA-ENG SMT; in this 
section, we briefly summarize these preliminary 
processing steps, and we conclude the section 
with a discussion of Thai text re-spacing.  
2.1 Character Sequence Normalization 
Thai orthography uses an alphabet of 44 conso-
nants and a number of vowel glyphs and tone 
marks. The four Thai tone marks and some Thai 
vowel characters are super- and/or sub-scripted 
with respect to a base character. For example, 
the ?? ? sequence consists of three code points: 
?  ? ? ? ?. When two or more of these combining 
marks are present on the same base character, the 
ordering of these code points in memory should 
be consistent so that orthographically identical 
entities are recognized as equivalent by comput-
er systems. However, some computer word pro-
cessors do not enforce the correct sequence or do 
not properly indicate incorrect sequences to the 
user visually. This often results in documents 
with invalid byte sequences. 
Correcting these errors is desirable for SMT 
inputs. In order to normalize Thai input character 
sequences to a canonical Unicode form, we de-
veloped a finite state transducer (FST) which 
detects and repairs a number of sequencing er-
rors which render Thai text either orthographi-
cally invalid, or not in a correct Unicode se-
quence. 
For example, a superscripted Thai tone mark 
should follow a super- or sub-scripted Thai vo-
wel when they both apply to the same consonant. 
When the input has the tone mark and the vowel 
glyph swapped, the input can be fully repaired: 
?  ?  ? ? ? ?  ?  ? ? ?  ?  ?  ???? 
?  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ???? 
Figure 1. Two unambiguous repairs 
Other cases are ambiguous. The occurrence of 
multiple adjacent vowel glyphs is an error where 
the intention may not be clear. We retain the 
first-appearing glyph, unless it is a pre-posed 
vowel, in which case we retain the last-appearing 
instance. These two treatments are contrasted in 
Figure 2. Miscoding (Figure 3) is another variety 
of input error that is readily repaired. 
????  ?  ?? 
???? ?  ?? 
Figure 2. Two ambiguous repairs 
Within the Infoquest Thai newswire corpus, a 
low-noise corpus, about 0.05% of the lines exhi-
bit at least one of the problems mentioned here. 
For some chunks of broad-range web scraped 
data, we observe rates as high as 4.1%. This 
measure is expected to under-represent the utility 
of the filter to WB, since Thai text streams, lack-
ing intra-word spacing and permitting two un-
written vowels, have few re-alignment check-
points, allowing tokenization state machines to 
linger in misaligned states. 
?   ? ? ?  ?  ?   ???   ?  ??? 
?   ?   ?  ?  ?  ?  ?  ?? 
Figure 3. Two common mis-codings 
2.2 Uniscribe Thai Tokenization 
Thai text does not normally use the space cha-
racter to separate words, except in certain specif-
ic contexts. Although Unicode offers the Zero-
Width Space (ZWSP) as one solution for indicat-
ing word breaks in Thai, it is infrequently used. 
Programmatic tokenization has become a staple 
of Thai computational linguistics. The problem 
has been well studied, with precision and recall 
near 95% (Haruechaiyasak et al 2008).  
In our SMT application, both the sentence 
breaker and the SMT system itself require Thai 
WB, and we use the same word breaker for these 
tasks (although the system design currently pro-
hibits directly passing tokens between these two 
components). Our method is to apply post-
processing heuristics to the output of Uniscribe 
(Bishop et al 2003), which is provided as part of 
the Microsoft? WindowsTM operating system 
interface. Our heuristics fall into two categories: 
?re-gluing? words that Uniscribe broke too ag-
gressively, and a smaller class of cases of further 
breaking of words that Uniscribe did not break. 
Re-gluing is achieved by comparing Uniscribe 
output against a Thai lexicon in which desired 
breaks within a word are tagged. Underbreaking 
by Uniscribe is less common and is restricted to 
a number of common patterns which are repaired 
explicitly. 
9
2.3 Person Name Entities 
In written Thai, certain types of entities employ 
prescriptive whitespace patterns. By removing 
these recognized patterns from consideration, SB 
precision can be improved. Furthermore, be-
cause our re-gluing procedure requires a lookup 
of every syllable proposed by Uniscribe, it is 
efficient to consider, during WB, additional 
processing that can be informed by the same 
lookup. Accordingly, we briefly mention some 
of the entity types that our WB identifies, focus-
ing on those that incorporate distinctive spacing 
patterns. 
Person names in Thai adhere to a convention 
for the use of space characters. This helps Thai 
readers to identify the boundaries of multi-
syllable surnames that they may not have seen 
before. The following grammar summarizes the 
prescriptive conventions for names appearing in 
Thai text:  
<name-entity> ::= <honorific>  <full-name> 
<full-name> ::= <first-name> [<last-name>] 
<first-name> ::= <name-text> space 
<last-name> ::= <name-text> space 
<name-text> ::= <thai-alphabetic-char>+ 
<thai-alphabetic-char> ::= ? | ? | ? | ? | ... 
Figure 4. Name entity recognition grammar 
The re-glue lookup also determines if a sylla-
ble matches one of the following predefined spe-
cial categories: name-introducing honorific (h), 
Thai or foreign given name (g), token which is 
likely to form part of a surname (s), or token 
which aborts the gathering of a name (i.e. is un-
likely to form part of a name).  
.../???/???/?/???/???/ /?/????/??/??/ /???/... 
??? ??? ? ? ?? ???  ? ? ??? ?? ? ?  ??? 
 h g0 g1 g2 sp0 s0 s1 s2 s3 sp1  
th
at
 
M
r. 
<o
ov
> 
hi
t 
be
lo
ve
d 
 <o
ov
> 
st
ab
le
 
<o
ov
> 
<o
ov
> 
 sa
i d
 
...that Mr. Chiranut Winichotkun said...
Figure 5. Thai person-name entity recognition 
Figure 5 shows a Thai name appearing within 
a text fragment, with Uniscribe detected token 
boundaries indicated by slashes. In the third row 
we have identified the special category, if any, 
for each token. The fourth line shows the Eng-
lish translation gloss, or <oov> if none. The bot-
tom row is the desired translation output. 
Our name identifier first notes the presence of 
an honorific {h} ??? followed by a pattern of 
tokens {g0-gn}, {s0-sn} and spaces {sp0, sp1} 
that is compatible with a person name and sur-
name of sensible length. 
Next, we determine which of those tokens in 
the ranges {g} and {s} following the honorific 
do not have a gloss translation (i.e., are not 
found in the lexicon). These tokens are indicated 
by <oov> in the gloss above. When the number 
of unknown tokens exceeds a threshold, we hy-
pothesize that these tokens form a name. The 
lack of lexical morphology in Thai facilitates 
this method because token (or syllable) lookup 
generally equates with the lookup of a stemmed 
lemma. 
2.4 Calendar Date Entities 
Our WB also identifies Thai calendar dates, as 
these also exhibit a pattern which incorporates 
spaces. As a prerequisite to identifying dates, we 
map Thai orthographic digits {? ? ? ? ? ? ? 
? ? ?} to Arabic digits 0 through 9, respec-
tively. For example, our system would interpret 
the input text ???? as equivalent to ?2540.? 
.../??/???/??/? /14/ ??????/ /????/ /???/...
?? ??? ??? sp 14 ?????? sp ???? sp ??? 
on day which  14 March  2540  and 
...on March 14th, 1997 and... 
Figure 6. Date entity recognition 
Figure 6 shows a fragment of Thai text which 
contains a calendar date for which our system 
will emit a single token. As shown in the exam-
ple, our system detects and adjusts for the use of 
Thai Buddhist year dates when necessary. Ga-
thering of disparate and optional parts of the 
Thai date is summarized by the grammar in Fig-
ure 7. 
<date-entity> ::= [<cardinal-words>] [space] <date> 
<cardinal-words> ::= ????? ?| ?? ?
<date> ::= month-date [space] year 
<year> ::= <tha-digit> <tha-digit> <tha-digit> <tha-digit> 
<year> ::= <ara-digit> <ara-digit> <ara-digit> <ara-digit> 
<month-date> ::= <day> [space] <month> 
<day> ::= <thai-digit>+ 
<day> ::= <ara-digit>+ 
<month> ::= <month-full> | <month-abbr> 
<month-full> ::= ?????? | ????????? ?| ?????? | ... 
<month-abbr> ::= ?.?. | ?.?. | ?.??. | ... 
<tha-digit> ::= ? | ? | ? | ? | ? | ? | ? | ? | ? | ? 
<ara-digit> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 
Figure 7. Date recognition grammar 
10
2.5 Thai Text Re-spacing 
To conclude this section, we mention an opera-
tion complementary to Thai WB, whereby Thai 
words output by an SMT system must be re-
spaced in accordance with Thai prescriptive 
convention. As will be mentioned in Section 4.2, 
for each input sentence, our English-Thai system 
has access to an English dependency parse tree, 
as well as links between this tree and a Thai 
transfer dependency tree. After using these links 
to transfer syntactic information to the Thai tree, 
we are able to apply prescriptive spacing rules 
(Wathabunditkul 2003) as closely as possible. 
Human evaluation showed satisfactory results 
for this process. 
3 Maximum Entropy Sentence-Breaking 
We now turn to a description of our statistical 
sentence-breaking model. We train an ME clas-
sifier on features which describe the proximal 
environment of the space token under considera-
tion and use this model at runtime to classify 
space tokens with possibly unseen contexts. 
3.1 Modeling 
Under the ME framework, let B={sb, nsb} 
represent the set of possible classes we are inter-
ested in predicting for each space token in the 
input stream. Let C={linguistic contexts} 
represent the set of possible contexts that we can 
observe, which must be encoded by binary fea-
tures, ??(?, ?), 1 ? ? ? ?, such as: 
??(?, ?) = ? 1 if the previous word is English ???  ? = ???.0 otherwise.  
This feature helps us learn that the space after an 
English word is usually not a sentence boundary. 
??(?, ?) = ?
 1 if the distance to the previous honorific 
is less than 15 tokens ??? ? = ???
0 otherwise.
 
This feature enables us to learn that spaces 
which follow an honorific are less likely to mark 
sentence boundaries. Assume the joint probabili-
ty p(b,c) is modeled by 
?(?, ?) = ?? ??
??(?,?)?
???
 
where we have k free parameters {??}  to esti-
mate and Z is a normalization factor to make 
? ?(?, ?) = 1.?,?  The ME learning algorithm 
finds a solution {??} representing the most un-
certain commitment 
max  ?(?) = ???(?, ?) log ?(?, ?)  
that satisfies the observed distribution ???(?, ?) of 
the training data 
   ??(?, ?)??(?, ?) = ? ???(?, ?)??(?, ?), 1 ? ? ? ? . 
This is solved via the Generalized Iterative Scal-
ing algorithm (Darroch and Ratcliff 1972). At 
run-time, a space token is considered an sb, if 
and only if p(sb|c) > 0.5, where 
?(??|?) = ?(??, ?)?(??, ?) + ?(???, ?) . 
3.2 Feature Selection 
The core context of our model, {w, x, y, z}, is a 
window spanning two tokens to the left (posi-
tions w and x) and two tokens to the right (posi-
tions y and z) of a classification candidate space 
token. 
c token characteristic 
yk Yamok (syllable reduplication) symbol ? 
sp space 
?? Thai numeric digits 
num Arabic numeric digits 
ABC Sequence of all capital ASCII characters 
cnn single character (derived from hex) 
ckkmmnn single character (derived from UTF8 hex) 
ascii any amount of non-Thai text 
(Thai text) Thai word (derived from lemma) 
Table 1. Categorical and derived feature names 
The possible values of each of the window 
positions {w, x, y, z} are shown in Table 1, 
where the first match to the token at the desig-
nated position is assigned as the feature value for 
that position. Foreign-text tokens plus any inter-
vening space are merged, so a single ?ascii? fea-
ture may represent an arbitrary amount of non-
Thai script with interior space. 
Figure 8 shows an example sentence that has 
been tokenized. Token boundaries are indicated 
by slashes. Although there are three space tokens 
in the original input, we extract four contexts. 
The shaded boxes in the source text?and the 
shaded line in the figure?indicate the single sb 
context that is synthesized by wrapping, to be 
described in Section 3.4. 
For each context, in addition to the {w, x, y, z} 
features, we extract two more features indicated 
by {l ,r} in Figure 8. They are the number of 
11
tokens between the previous space token (wrap-
ping as necessary) and the current one, and the 
number of tokens between the current space to-
ken and the next space token (wrapping as ne-
cessary). These features do not distinguish 
whether the bounding space token is sb or nsb. 
This is because, processing left-to-right, it is 
permissible to use a feature such as ?number of 
tokens since last sb,? but not ?number of tokens 
until next sb,? which would be available during 
training but not at runtime. 
??????????????????? R1C1  ??????????????????????
??????????  A1 
?R1C1 reference style was converted to A1 reference style.? 
__/??????/???/????/???/???/  /R1C1/   /???/????/??/
????/??????/???/????/???/???/  /A1/__ 
 b c=w c=x c=y c=z c=l c=r 
nsb ??? ??? ABC sp 5 1 
nsb sp ABC ??? ???? 1 9 
nsb ??? ??? ABC sp 9 1 
sb sp ABC ?????? ??? 1 5 
Figure 8. A Thai sentence and the training contexts extracted. Hig-
hlighting shows the context for sb.  
In addition to the above core features, our 
model emits certain extra features only if they 
appear: 
? An individual feature for each English punc-
tuation mark, since these are sometimes used 
in Thai. For example, there is one feature for 
the sentence end period (i.e. full-stop); 
? The current nest depth for paired glyphs with 
directional variation, such as brackets, braces, 
and parentheses; 
? The current parity value for paired glyphs 
without directional distinction such as 
?straight? quotation marks. 
The following example illustrates paired direc-
tional glyphs (in this case, parentheses): 
.../??????????/?  /(/??????/__/???/)/  /??????/  /???????/???/... 
...Unilever (Thailand) Ltd. disclosed that... 
 b c=w c=x c=y c=z c=pn 
nsb ( ?????? ??? ) 1 
Figure 9. Text fragment illustrating paired directional glyphs and 
the context for the highlighted space 
     In Figure 9, the space between ?????? 
?country? and ??? ?Thai,? generates an nsb 
context which includes the features shown, 
where ?pn? is an extra feature which indicates 
the parenthesis nesting level. This feature helps 
the model learn that spaces which occur within 
parentheses are likely to be nsb. 
Parity features for the non-directional paired 
glyphs, which do nest, are true binary features. 
Since these features have only two possible val-
ues (inside or outside), they are only emitted 
when their value is ?inside,? that is, when the 
space under consideration occurs between such a 
pair. 
3.3 Sentence Breaker Training Corpus 
Thai corpora which are marked with sentence 
breaks are required for training. We assembled a 
corpus of 361,802 probable sentences. This cor-
pus includes purchased, publicly available, and 
web-crawled content. In total it contains 911,075 
spaces, a figure which includes one inter-
sentence space per sentence, generated as de-
scribed below. 
3.4 Out-of-context Sentences 
For SB training, paragraphs are first tokenized 
into words as described in Section 2.2. This 
process does not introduce new spaces between 
tokens; only original spaces in the text are classi-
fied as sb/nsb and used for the context features 
described below. To keep this distinction clear, 
token boundaries are indicated by a slash rather 
than space in the examples shown in this paper. 
For 91% of our training sentences, the para-
graphs from which they originate are inaccessi-
ble. In feature extraction for each of these sen-
tences, we wrap the sentence?s head around to its 
tail to obtain its sb context. In other words, for a 
sentence of tokens t0-tn-1, the context of sb (the 
last space) is given by 
{ w=tn-2, x=tn-1, y=t0, z=t1 }. 
     This process was illustrated in Figure 8. Al-
though not an ideal substitute for sentences in 
context, this ensures that we extract at least one 
sb context per sentence. The number of nsb con-
texts extracted per sentence is equal to the num-
ber of interior space tokens in the original sen-
tence. Sentence wrapping is not needed when 
training with sentence-delimited paragraph 
sources. Contexts sb and nsb are extracted from 
the token stream of the entire paragraph and 
wrapping is used only to generate one additional 
sb for the entire paragraph. 
12
3.5 Sentence Breaker Evaluation 
Although evaluation against a single-domain 
corpus does not measure important design re-
quirements of our system, namely resilience to 
broad-domain input texts, we evaluated against 
the ORCHID corpus (Charoenporn et al 1997) 
for the purpose of comparison with the existing 
literature. Following the methodology of the stu-
dies cited below, we use 10-fold ?10% averaged 
testing against the ORCHID corpus. 
Our results are consistent with recent work us-
ing the Winnow algorithm, which itself com-
pares favorably with the probabilistic POS tri-
gram approach. Both of these studies use evalua-
tion metrics, attributed to Black and Taylor 
(1997), which aim to more usefully measure sen-
tence-breaker utility. Accordingly, the following 
definitions are used in Table 2: 
space-correct =  (#correct sb+#correct nsb)total # of space tokens  
false break= #sb false positivestotal # of space tokens 
     It was generally possible to reconstruct preci-
sion and recall figures from these published re-
sults1 and we present a comprehensive table of 
results. Reconstructed values are marked with a 
dagger and the optimal result in each category is 
marked in boldface. 
 Mittrapiyanuruk
et al 
Charoenpornsawat 
et al 
Our result
method POS Trigram Winnow MaxEnt 
#sb in reference 10528 1086? 2133 
#space tokens 33141 3801 7227 
nsb-precision 90.27? 91.48? 93.18 
nsb-recall 87.18? 97.56? 94.41 
sb-precision 74.35? 92.69? 86.21 
sb-recall 79.82 77.27 83.50 
?space-correct? 85.26 89.13 91.19 
?false-break? 8.75 1.74 3.94 
Table 2. Evaluation of Thai Sentence Breakers against 
ORCHID 
Finally, we would be remiss in not acknowl-
edging the general hazard of assigning sentence 
breaks in a language such as Thai, where source 
                                                 
1 Full results for Charoenpornsawat et al are reconstructed based 
on remarks in their text, including that ?the ratio of the number of 
[nsb to sb] is about 5:2.? 
text authors may intentionally include or omit 
spaces in order to create syntactic or semantic 
ambiguity. We defer to Mittrapiyanuruk and 
Sornlertlamvanich (2000) and Aroonmanakun 
(2007) for informed commentary on this topic. 
4 SMT System and Integration 
The primary application for which we developed 
the Thai sentence breaker described in this work 
is the Microsoft? BING? general-domain ma-
chine translation service. In this section, we pro-
vide a brief overview of this large-scale SMT 
system, focusing on Thai-specific integration 
issues. 
4.1 Overview 
Like many multilingual SMT systems, our sys-
tem is based on hybrid generative/discriminative 
models. Given a sequence of foreign words, f, its 
best translation is the sequence of target words, 
e, that maximizes  
?? = argmax? ?(?|?) =  argmax? ?(?|?)?(?) 
= argmaxe  { log ?(?|?) + log ?(?)} 
where the translation model ?(?|?) is computed 
on dozens to hundreds of features. The target 
language model (LM), ?(?), is represented by a 
smoothed n-grams (Chen 1996) and sometimes 
more than one LM is adopted in practice. To 
achieve the best performance, the log likelihoods 
evaluated by these features/models are linearly 
combined. After ?(?|?) and ?(?) are trained, the 
combination weights ??  are tuned on a held-out 
dataset to optimize an objective function, which 
we set to be the BLEU score (Papineni et al 
2002): 
{???} = max{??}  BLEU({??}, {?}) 
?? =  argmaxe  {???log
?
 ??(?|?) +???log
?
??(?)} 
where {r} is the set of gold translations for the 
given input source sentences. To learn ?? we use 
the algorithm described by Och (2003), where 
the decoder output at any point is approximated 
using n-best lists, allowing an optimal line 
search to be employed. 
4.2 Phrasal and Treelet Translation 
Since we have a high-quality real-time rule-
based English parser available, we base our Eng-
13
lish-to-Thai translation (ENG-THA) on the 
?treelet? concept suggested in Menezes and 
Quirk (2008). This approach parses the source 
language into a dependency tree which includes 
part-of-speech labels.  
   Lacking a Thai parser, we use a purely statis-
tical phrasal translator after Pharaoh (Koehn 
2004) for THA-ENG translation, where we 
adopt the name and date translation described in 
Sections 2.3 and 2.4.  
     We also experimented with phrasal ENG-
THA translation. Though we actually achieved a 
slightly better BLEU score than treelet for this 
translation direction, qualitative human evalua-
tion by native speaker informants was mixed. 
We adopted the treelet ENG-THA in the final 
system, for its better re-spacing (Section 2.5). 
4.3 Training, Development and Test Data 
Naturally, our system relies on parallel text cor-
pora to learn the mapping between two languag-
es. The parallel corpus contains sentence pairs, 
corresponding to translations of each other. For 
Thai, quality corpora are generally not available 
in sufficient quality for training a general-
domain SMT system. For the ENG-THA pair, 
we resort to Internet crawls as a source of text. 
We first identify paired documents, break each 
document into sentences, and align sentences in 
one document against those in its parallel docu-
ment. Bad alignments are discarded. Only sen-
tence pairs with high alignment confidence are 
kept in our parallel corpus. Our sentence align-
ment algorithm is based on Moore (2002). 
For our ENG-THA translation system, we as-
sembled three resources: a parallel training cor-
pus, a development bitext (also called the lamb-
da set) for training the feature combination 
weights {??}, and a test corpus for BLEU and 
human evaluation. Both the lambda and the test 
sets have single reference translations per sen-
tence. 
Data Set #Sentences 
(ENG||THA) training 725K 
(ENG,THA) lambda 2K 
(ENG,THA) test 5K 
THA LM text 10.3M 
ENG LM text 45.6M 
Table 3. Corpus size of parallel and monolingual data 
 
Although it is well known that language trans-
lation pairs are not symmetric, we use these 
same resources to build our THA-ENG transla-
tion system due to the lack of additional corpora.  
Our parallel MT corpus consists of approx-
imately 725,000 English-Thai sentence pairs 
from various sources. Additionally we have 9.6 
million Thai sentences, which are used to train a 
Thai 4-gram LM for ENG-THA translation, to-
gether with the Thai sentences in the parallel 
corpus. Trigrams and 4-grams that occur only 
once are pruned, and n-gram backoff weights are 
re-normalized after pruning, with the surviving 
KN smoothed probabilities intact (Kneser and 
Ney 1995). Similarly, a 4-gram ENG LM is 
trained for THA-ENG translation, on a total of 
45.6M English sentences. 
For both the lambda and test sets, THA LM 
incurs higher out-of-vocabulary (OOV) rates 
(1.6%) than ENG LM (0.7%), due to its smaller 
training set and thus smaller lexicon. Both trans-
lation directions define the maximum 
phrase/treelet length to be 4 and the maximum 
re-ordering jump to be 4 as well. 
4.4 BLEU Scores 
To evaluate our end-to-end performance, we 
compute case insensitive 4-gram BLEU scores. 
Translation outputs are WB first according to the 
Thai/English tokenizer, before BLEU scores are 
computed. The BLEU scores on the test sets are 
shown in Table 4. We are not aware of any pre-
viously published BLEU results for either direc-
tion of this language pair. 
  BLEU 
THA-ENG 0.233 
ENG-THA 0.194 
Table 4. Four-gram case-insensitive BLEU scores. 
Figures 10 and 11 illustrate sample outputs for 
the each translation direction, with reference 
translations. 
INPUT: ??????????????????????????? ??? ???? ???
??????????????????????? ??????????????????????????
OUTPUT: In Thailand a Orchid approximately 175 type if 
extinct from Thailand. It means extinct from the world. 
REF: In Thailand, there are about 175 species of Orchid. If 
they disappear from Thailand, they will be gone from the 
world. 
Figure 10.  THA-ENG Sample Translation Output 
14
INPUT: In our nation the problems and barriers we face are 
just problems and barriers of law not selection or develop-
ment. 
OUTPUT: ?????????????????? ?????????????????????
??????????????????????????????????????????????????
????? 
REF: ????????????????????????????? ???????????
???????????????????? ???????????????????????????
????????????????????? ?
Figure 11. ENG-THA Sample Translation Output 
Although the translation quality is far from being 
perfect, SMT is making good process on build-
ing useful applications. 
5 Conclusion and Future Work 
Our maximum entropy model for Thai sentence-
breaking achieves results which are consistent 
with contemporary work in this task, allowing us 
to overcome this obstacle to Thai SMT integra-
tion. This general approach can be applied to 
other South-East Asian languages in which space 
does not deterministically delimit sentence 
boundaries. 
In Arabic writing, commas are often used to 
separate sentences until the end of a paragraph 
when a period is finally used. In this case, the 
comma character is similar to the space token in 
Thai where its usage is ambiguous. We can use 
the same approach (perhaps with different lin-
guistic features) to identify which commas are 
sentence-breaking and which are not. 
Our overall system incorporates a range of in-
dependent solutions to problems in Thai text 
processing, including character sequence norma-
lization, tokenization, name and date identifica-
tion, sentence-breaking, and Thai text re-
spacing. We successfully integrated each solu-
tion into an existing large-scale SMT frame-
work, obtaining sufficient quality to release the 
Thai-English language pair in a high-volume, 
general-domain, free public online service. 
There remains much room for improvement. 
We need to find or create true Thai-English di-
rectional corpora to train the lambdas and to test 
our models. The size of our parallel corpus for 
Thai should increase by at least an order of mag-
nitude, without loss of bitext quality. With a 
larger corpus, we can consider longer phrase 
length, higher-order n-grams, and longer re-
ordering distance. 
References 
W. Aroonmanakun. 2007. Thoughts on Word 
and Sentence Segmentation in Thai. In Pro-
ceedings of the Seventh International Sympo-
sium on Natural Language Processing, Pat-
taya, Thailand, 85-90. 
F. Avery Bishop, David C. Brown and David M. 
Meltzer. 2003. Supporting Multilanguage 
Text Layout and Complex Scripts with Win-
dows 2000. http://www.microsoft.com/typo-
graphy/developers/uniscribe/intro.htm 
A. W. Black and P. Taylor. 1997. Assigning 
Phrase Breaks from Part-of-Speech Se-
quences. Computer Speech and Language, 
12:99-117. 
Thatsanee Charoenporn, Virach Sornlertlamva-
nich, and Hitoshi Isahara. 1997. Building A 
Thai Part-Of-Speech Tagged Corpus (ORC-
HID). 
Paisarn Charoenpornsawat and Virach Sornler-
tlamvanich. 2001. Automatic sentence break 
disambiguation for Thai. In International 
Conference on Computer Processing of 
Oriental Languages (ICCPOL), 231-235. 
S. F. Chen and J. Goodman. 1996. An empirical 
study of smoothing techniques for language 
modeling. In Proceedings of the 34th Annual 
Meeting on Association for Computational 
Linguistics, 310-318. Morristown, NJ: ACL. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models. The 
Annals of Mathematical Statistics, 43(5): 
1470-1480. 
Choochart Haruechaiyasak, Sarawoot Kon-
gyoung, and Matthew N. Dailey. 2008. A 
Comparative Study on Thai Word Segmenta-
tion Approaches. In Proceedings of ECTI-
CON 2008. Pathumthani, Thailand: ECTI. 
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-Gram Language 
Modeling. In Proceedings of International 
Conference on Acoustics, Speech and Signal 
Procesing (ICASSP), 1:181-184. 
Philipp Koehn. 2004. Pharaoh: a Beam Search 
Decoder for Phrase-Based Statistical Machine 
Translation Models. In Proceedings of the As-
15
sociation of Machine Translation in the Amer-
icas (AMTA-2004). 
Arul Menezes, and Chris Quirk. 2008. Syntactic 
Models for Structural Word Insertion and De-
letion during Translation. In Proceedings of 
the 2008 Conference on Empirical Methods in 
Natural Language Processing. 
P. Mittrapiyanuruk and V. Sornlertlamvanich. 
2000. The Automatic Thai Sentence Extrac-
tion. In Proceedings of the Fourth Symposium 
on Natural Language Processing, 23-28. 
Robert C. Moore. 2002. Fast and Accurate Sen-
tence Alignment of Bilingual Corpora. In Ma-
chine Translation: From Research to Real 
Users (Proceedings, 5th Conference of the As-
sociation for Machine Translation in the 
Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, 135-244 
Franz Josef Och. 2003. Minimum error rate 
training in statistical machine translation. In 
Proceedings of the 41th Annual Meeting of the 
Association for Computational Linguistics. 
Stroudsburg, PA: ACL. 
David D. Palmer and Marti A. Hearst. 1997. 
Adaptive Multilingual Sentence Boundary 
Disambiguation. Computational Linguistics, 
23:241-267. 
Kishore Papineni, Salim Roukos, Todd Ward, 
and Wei-jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. 
In Proceedings of the 40th Annual meeting of 
the Association for Computational Linguistics, 
311?318. Stroudsburg, PA: ACL. 
Adwait Ratnaparkhi, 1996. A Maximum Entropy 
Model for Part-of-Speech Tagging. In Pro-
ceedings of the Conference on Empirical Me-
thods in Natural Language Processing, 133-
142. 
Jeffrey C. Reynar and Adwait Ratnaparkhi. 
1997. A Maximum Entropy Approach to Iden-
tifying Sentence Boundaries, In Proceedings 
of the Fifth Conference on Applied Natural 
Language Processing, 16-19. 
Suphawut Wathabunditkul. 2003. Spacing in the 
Thai Language. http://www.thailanguage.com/ 
ref/spacing 
16
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 460?467,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Leave-One-Out Phrase Model Training for Large-Scale Deployment
Joern Wuebker
Human Language Technology
and Pattern Recognition Group
RWTH Aachen University, Germany
wuebker@cs.rwth-aachen.de
Mei-Yuh Hwang, Chris Quirk
Microsoft Corporation
Redmond, WA, USA
{mehwang,chrisq}@microsoft.com
Abstract
Training the phrase table by force-aligning
(FA) the training data with the reference trans-
lation has been shown to improve the phrasal
translation quality while significantly reduc-
ing the phrase table size on medium sized
tasks. We apply this procedure to several
large-scale tasks, with the primary goal of re-
ducing model sizes without sacrificing transla-
tion quality. To deal with the noise in the auto-
matically crawled parallel training data, we in-
troduce on-demand word deletions, insertions,
and backoffs to achieve over 99% successful
alignment rate. We also add heuristics to avoid
any increase in OOV rates. We are able to re-
duce already heavily pruned baseline phrase
tables by more than 50% with little to no
degradation in quality and occasionally slight
improvement, without any increase in OOVs.
We further introduce two global scaling fac-
tors for re-estimation of the phrase table via
posterior phrase alignment probabilities and
a modified absolute discounting method that
can be applied to fractional counts.
Index Terms: phrasal machine translation, phrase
training, phrase table pruning
1 Introduction
Extracting phrases from large amounts of noisy
word-aligned training data for statistical machine
translation (SMT) generally has the disadvantage of
producing many unnecessary phrases (Johnson et
al., 2007). These can include poor quality phrases,
composite phrases that are concatenations of shorter
ones, or phrases that are assigned very low proba-
bilities, so that they have no realistic chance when
competing against higher scoring phrase pairs. The
goal of this work is two-fold: (i) investigating forced
alignment training as a phrase table pruning method
for large-scale commercial SMT systems and (ii)
proposing several extensions to the training proce-
dure to deal with practical issues and stimulate fur-
ther research.
Generative phrase translation models have the in-
herent problem of over-fitting to the training data
(Koehn et al, 2003; DeNero et al, 2006). (Wue-
bker et al, 2010) introduce a leave-one-out proce-
dure which is shown to counteract over-fitting ef-
fects. The authors report significant improvements
on the German-English Europarl data with the ad-
ditional benefit of a severely reduced phrase table
size. This paper investigates its impact on a num-
ber of commercial large-scale systems and presents
several extensions.
The first extension is to deal with the highly noisy
training data, which is automatically crawled and
sentence aligned. The noise and the baseline prun-
ing of the phrase table lead to low success rates
when aligning the source sentence with the target
sentence. We introduce on-demand word deletions,
insertions, and backoff phrases to increase the suc-
cess rate so that we can cover essentially the en-
tire training data. Secondly, phrase table pruning
makes out-of-vocabulary (OOV) issues even more
pronounced. To avoid an increased OOV rate, we
retrieve single-word translations from the baseline
phrase table. Lastly, we propose two global scaling
460
factors to allow fine-tuning of the phrase counts in
an attempt to re-estimate the translation probabili-
ties and a modification of absolute discounting that
can be applied to fractional counts.
Our main contribution is applying forced-
alignment on the training data to prune the phrase
table. The rationale behind this is that by decoding
the training data, we can identify the phrases that are
actually used by the decoder. Further, we present
preliminary experiments on re-estimating the chan-
nel models in the phrase table based on counts ex-
tracted from the force-aligned data.
This work is organized as follows. We discuss re-
lated work in Section 2, describe our decoder and
training procedure in Section 3 and the experiments
in Section 4. A conclusion and discussion of future
work is given in Section 5.
2 Related Work
Force-aligning bilingual data has been explored as
a means of model training in previous work. Liang
et al (2006) use it for their bold updating strategy
to update discriminative feature weights. Utilizing
force-aligned data to train a unigram phrase segmen-
tation model is proposed by Shen et al (2008). Wue-
bker et al (2010) apply forced alignment to train the
phrase table in an EM-like fashion. They report a
significant reduction in phrase table size.
In this work we apply forced alignment training
as a pure phrase table pruning technique. Johnson
et al (2007) successfully investigate a number of
pruning methods for the phrase inventory based on
significance testing. While their approach is more
straightforward and less elaborate, we argue that our
method is directly tailored to the decoding process
and works on top of an already heavily pruned base-
line phrase table.
We further experiment with applying the (scaled)
phrase alignment posteriors to train the phrase ta-
ble. A similar idea has been addressed in previous
work, e.g. (Venugopal et al, 2003; de Gispert et al,
2010), where word alignment posterior probabilities
are leveraged for grammar extraction.
Finally, a number of papers describe extending
real phrase training to the hierarchical machine
translation paradigm (Blunsom et al, 2008; Cme-
jrek et al, 2009; Mylonakis and Sima?an, 2010).
3 Phrase Training
3.1 Decoder
Our translation decoder is similar to the open-source
toolkit Moses (Koehn et al, 2007). It models trans-
lation as a log-linear combination of two phrasal
and two lexical channel models, an n-gram language
model (LM), phrase, word and distortion penalties
and a lexicalized reordering model. The decoding
can be summarized as finding the best scoring target
sentence T ? given a source sentence S:
T ? = argmax
T
?
i
?i log gi(S,T ) (1)
where each gi represents one feature (the channel
models, n-gram, phrase count, etc.). The model
weights ?i are usually discriminatively learned on a
development data set via minimum error rate train-
ing (MERT) (Och, 2003).
Constraining the decoder to a fixed target sentence
is straightforward. Each partial hypothesis is com-
pared to the reference and discarded if it does not
match. The language model feature can be dropped
since all hypotheses lead to the same target sentence.
The training data is divided into subsets for parallel
alignment. A bilingual phrase matching is applied to
the phrase table to extract only the subset of entries
that are pertinent to each subset of training data, for
memory efficiency. For forced alignment training,
we set the distortion limit ? to be larger than in reg-
ular translation decoding. As unlimited distortion
leads to very long training times, we compromise on
the following heuristic. The distortion limit is set
to be the maximum of 10, twice that of the baseline
setting, and 1.5 times the maximum phrase length:
? = max{10,
2? (baseline distortion),
1.5? (max phrase length)} (2)
To avoid over-fitting, we employ the same leave-
one-out procedure as (Wuebker et al, 2010) for
training. Here, it is applied on top of the Good-
Turing (GT) smoothed phrase table (Foster et al,
461
2006). Our phrase table stores the channel proba-
bilites and marginal counts for each phrase pair, but
not the discounts applied. Therefore, for each sen-
tence, if the phrase pair (s, t) has a joint count c(s, t)
computed from the entire training data, and occurs
c1(s, t) times in the current sentence, the leave-one-
out probability p?(t|s) for the current sentence will
be:
p?(t|s) =
c?(s, t)?d
c?(s)
=
c(s, t)? c1(s, t)?d
c(s)? c1(s)
=
p(t|s)c(s)? c1(s, t)
c(s)? c1(s)
(3)
since p(t|s)c(s) = c(s, t)?d, where d is the GT dis-
count value. In the case where c(s, t) = c1(s, t) (i.e.
(s, t) occurs exclusively in one sentence pair), we
use a very low probability as the floor value. We
apply leave-one-out discounting to the forward and
backward translation models only, not to the lexical
channel models.
Our baseline phrase extraction applies some
heuristic-based pruning strategies. For example,
it prunes offensive translations and many-words to
many-words singletons (i.e. a joint count of 1 and
both source phrase and target phrase contain mul-
tiple words)?. Finally the forward and backward
translation probabilities are smoothed with Good-
Turing discounting.
3.2 Weak Lambda Training with High
Distortion
Our leave-one-out training flowchart can be illus-
trated in Figure 1. To force-align the training data
with good quality, we need a set of trained lambda
weights, as shown in Equation 1. We can use the
lambda weights learned from the baseline system for
that purpose. However, ideally we want the lambda
values to be learned under a similar configuration as
the forced alignment. Therefore, for this purpose we
run MERT with the larger distortion limit given in
Equation 2.
?The pruned entries are nevertheless used in computing joint
counts and marginal counts.
 Parallel training data 
with word-level alignments 
Phrase extraction with 
heuristic pruning 
Weak lambda training 
Phrase table 
Leave-one-out 
forced alignment 
? 1 = {?} 
Normal lambda training 
Intersection + 
OOV Recovery 
Selected phrases 
Selected phrases+ 
Large ? 
2-grams 
5-grams 
Small ? 
? 2 = {?} 
 {uniform ?} 
 {baseline ?} 
Figure 1: Flowchart of forced-alignment phrase training.
Additionally, since forced alignment does not use
the language model, we propose to use a weaker lan-
guage model for training the lambdas (?1) to be used
in the forced alignment decoding.
Using a weaker language model also speeds up the
lambda training process, especially when we are us-
ing a distortion limit ? at least twice as high as in
the baseline system. In our experiments, the base-
line system uses an English 5-gram language model
trained on a large amount of monolingual data. The
lambda values used for forced alignment are learned
using the bigram LM trained on the target side of the
462
parallel corpus for each system.
We compared a number of systems using differ-
ent degrees of weak models and found out the im-
pact on the final system was minimal. However, us-
ing a small bigram LM with large distortion yielded
a stable performance in terms of BLEU, and was
25% faster than using a large 5-gram with the base-
line distortion. Because of the speed improvement
and its stability, this paper adopts the weak bigram
lambda training.
3.3 On-demand Word Insertions and Deletions
For many training sentences the translation decoder
is not able to find a phrasal alignment. We identified
the following main reasons for failed alignments:
? Incorrect sentence alignment or sentence seg-
mentation by the data crawler,
? OOVs due to initial pruning in the phrase ex-
traction phase,
? Faulty word alignments,
? Strongly reordered sentence structure. That is,
the distortion limit during forced alignment is
too restrictive.
For some of these cases, discarding the sentence
pairs can be seen as implicit data cleaning. For
others, there do exist valid sub-sentences that are
aligned properly. We would like to be able to lever-
age those sub-sentences, effectively allowing us to
do partial sentence removal. Therefore, we in-
troduce on-demand word insertions and deletions.
Whenever a partial hypothesis can not be expanded
to the next target word t j, with the given phrase ta-
ble, we allow the decoder to artificially introduce a
phrase pair (null, t j) to insert the target word into
the hypothesis without consuming any source word.
These artificial phrase pairs are introduced with a
high penalty and are ignored when creating the out-
put phrase table. We can also introduce backoff
phrase pairs (si, t j) for all source words si that are
not covered so far, also with a fixed penalty.
After we reach the end of the target sentence, if
there are any uncovered source words si, we arti-
ficially add the deletion phrase pairs (si,null) with
a high penalty. Introducing on-demand word inser-
tions and deletions increases the data coverage to
at least 99% of the training sentences on all tasks
we have worked on. Due to the success of inser-
tion/deletion phrases, we have not conducted exper-
iments using backoff phrases within the scope of this
work, but leave this to future work.
3.4 Phrase Training as Pruning
This work concentrates on practical issues with large
and noisy training data. Our main goal is to ap-
ply phrase training to reduce phrase table size with-
out sacrificing quality. We do this by dumping n-
best alignments of the training data, where n ranges
from 100-200. We prune the baseline phrase table to
only contain phrases that appear in any of the n-best
phrase alignments, leaving the channel probabilities
unchanged. That is, the model scores are still esti-
mated from the original counts. We can control the
size of the final phrase table by adjusting the size
of the n-best list. Based on the amount of memory
we can afford, we can thus keep the most important
entries in the phrase table.
3.5 OOV retrieval
When performing phrase table pruning as de-
scribed in Section 3.4, OOV rates tend to increase.
This effect is even more pronounced when dele-
tion/insertion phrases are not used, due to the low
alignment success rate. For commercial applica-
tions, untranslated words are a major concern for
end users, although it rarely has any impact on BLEU
scores. Therefore, for the final phrase table after
forced alignment training, we check the translations
for single words in the baseline phrase table. If any
single word has no translation in the new table, we
recover the top x translations from the baseline table.
In practice, we set x = 3.
3.6 Fractional Counts and Model
Re-estimation
As mentioned in Section 3.4, for each training sen-
tence pair we produce the n-best phrasal alignments.
If we interpret the model score of an alignment as
its log likelihood, we can weight the count for each
phrase by its posterior probability. However, as the
463
log-linear model weights are trained in a discrim-
inative fashion, they do not directly correspond to
probabilities. In order to leverage the model scores,
we introduce two scaling factors ? and ? that al-
low us to shape the count distribution according to
our needs. For one sentence pair, the count for the
phrase pair (s, t) is defined as
c(s, t)=
?
?
?
?
?
n
?
i=1
c(s, t|hi) ?
exp(? ??(hi))
n
?
j=1
exp(? ??(h j))
?
?
?
?
?
?
, (4)
where hi is the i-th hypothesis of the n-best list,
?(hi) the log-linear model score of the alignment
hypothesis hi and c(s, t|hi) the count of (s, t) within
hi. If ? = 0, all alignments within the n-best list
are weighted equally. Setting ? = 0 means that all
phrases that are used anywhere in the n-best list re-
ceive a count of 1.
Absolute discounting is a popular smoothing
method for relative frequencies (Foster et al, 2006).
Its application, however, is somewhat difficult, if
counts are not required to be integer numbers and
can in fact reach arbitrarily small values. We pro-
pose a minor modification, where the discount pa-
rameter d is added to the denominator, rather than
subtracting it from the numerator. The discounted
relative frequency for a phrase pair (s, t) is computed
as
p(s|t) =
c(s, t)
d+?
s?
c(s?, t)
(5)
3.7 Round-Two Lambda Training
After the phrase table is pruned with forced align-
ment (either re-estimating the channel probabilities
or not), we recommend a few more iterations of
lambda training to ensure our lambda values are ro-
bust with respect to the new phrase table. In our
experiments, we start from the baseline lambdas and
train at most 5 more iterations using the baseline dis-
tortion and the 5-gram English language model. The
settings have to be consistent with the final decod-
ing; therefore we are not using weak lambda training
here.
system parallel corpus Dev Test1 WMT
(sent. pairs)
it-en 13.0M 2000 5000 3027
pt-en 16.9M 2448 5000 1000
nl-en 15.0M 499 4996 1000
et-en 3.5M 1317 1500 995
Table 1: Data sizes of the four systems Italian, Por-
tuguese, Dutch and Estonian to English. All numbers
refer to sentence pairs.
Empirically we found the final lambdas (?2) made
a very small improvement over the baseline lamb-
das. However, we decided to keep this second round
of lambda training to guarantee its stability across
all language pairs.
4 Experiments
In this section, we describe our experiments on
large-scale training data. First, we prune the orig-
inal phrase table without re-estimation of the mod-
els. We conducted experiments on many language
pairs. But due to the limited space here, we chose to
present two high traffic systems and the two worst
systems so that readers can set the correct expecta-
tion with the worst-case scenario. The four systems
are: Italian (it), Portuguese (pt), Dutch (nl) and Es-
tonian (et), all translating to English (en).
4.1 Corpora
The amount of data for the four systems is shown in
Table 1. There are two test sets: Test1 and WMT.
Test1 is our internal data set, containing web page
translations among others. WMT is sampled from
the English side of the benchmark test sets of the
Workshop on Statistical Machine Translation?. The
sampled English sentences are then manually trans-
lated into other languages, as the input to test X-to-
English translation. WMT tends to contain news-
like and longer sentences. The development set (for
learning lambdas) is from our internal data set. We
make sure that there is no overlap among the devel-
opment set, test sets, and the training set.
?www.statmt.org/wmt09
464
baseline FA w/ del. FA w/o del.
it-en
suc.rate ? 99.5% 61.2%
Test1 42.27 42.05 42.31
WMT 30.16 30.19 30.19
pt-en
suc.rate ? 99.5% 66.9%
Test1 47.55 47.47 47.24
WMT 40.74 41.36 41.01
nl-en
suc.rate ? 99.6% 79.9%
Test1 32.39 31.87 31.18
WMT 43.37 43.06 43.38
et-en
suc.rate ? 99.1% 73.1%
Test1 46.14 46.35 45.77
WMT 20.08 19.60 19.83
Table 2: BLEU scores of forced-alignment-based phrase-
table pruning using weak lambda training. n-best size is
100 except for nl-en, where it is 160. We contrast forced
alignment with and without on-demand insertion/deletion
phrases. With the on-demand artificial phrases, FA suc-
cess rate is over 99%.
4.2 Insertion/Deletion Phrases
Unless explicitly stated, all experiments here used
the weak bigram LMs to obtain the lambdas used for
forced alignment, and on-demand insertion/deletion
phrases are applied. For the size of n-best, we use
n = 100. The only exception is the nl-en language
pair, for which we set n = 160 because its phrase
distortion setting is higher than the others and for its
higher number of morphological variations. Table 2
shows the BLEU performance of the four systems, in
the baseline setting and in the forced-alignment set-
ting with insertion/deletion phrases and without in-
sertion/deletion phrases. Whether partial sentences
should be kept or not (via insertion/deletion phrases)
depends on the quality of the training data. One
would have to run both settings to decide which is
better for each system. In all cases, there is little
or no degradation in quality after the table is suffi-
ciently pruned.
Table 3 shows that our main goal of reducing the
phrase table size is achieved. On all four language
pairs, we are able to prune over 50% of the phrase
PT size reduction
w/o del. w/ del.
it-en 65.4% 54.0%
pt-en 68.5% 61.3%
nl-en 64.1% 56.9%
et-en 63.6% 58.5%
Table 3: % Phrase table size reduction compared with the
baseline phrase table
table. Without on-demand insertions/deletions, the
size reduction is even stronger. Notice the size re-
duction here is relative to the already heavily pruned
baseline phrase table.
With such a successful size cut, we expected a
significant increase in decoding speed in the final
system. In practice we experienced 3% to 12% of
speedup across all the systems we tested. Both our
baseline and the reduced systems use a tight beam
width of 20 hypotheses per stack. We assume that
with a wider beam, the speed improvement would
be more pronounced.
We also did human evaluation on all 8 system out-
puts (four language pairs, with two test sets per lan-
guage pair) and all came back positive (more im-
provements than regressions), even on those that had
minor BLEU degradation. We conclude that the size
cut in the phrase table is indeed harmless, and there-
fore we declare our initial goal of phrase table prun-
ing without sacrificing quality is achieved.
In (Wuebker et al, 2010) it was observed, that
phrase training reduces the average phrase length.
The longer phrases, which are unlikely to gener-
alize, are dropped. We can confirm this obersva-
tion for the it-en and pt-en language pairs in Ta-
ble 4. However, for nl-en and et-en the aver-
age source phrase length is not significantly af-
fected by phrase training, especially with the inser-
tion/deletion phrases. When these artificial phrases
are added during forced alignment, they tend to en-
courage long target phrases as uncovered single tar-
get words can be consumed by the insertion phrases.
However, these insertion phrases are not dumped
into the final phrase table and hence cannot help
in reducing the average phrase length of the final
phrase table.
465
avg. src phrase length
baseline w/o del. w/ del.
it-en 3.1 2.4 2.4
pt-en 3.7 3.0 3.0
nl-en 3.1 3.0 3.0
et-en 2.9 2.8 3.0
Table 4: Comparison of average source phrase length in
the phrase table.
nl-en Test1 WMT PT size reduction
baseline 32.29 43.37 ?
n=100 31.45 42.90 66.0%
n=160 31.87 43.06 64.1%
et-en Test1 WMT PT size reduction
baseline 46.14 20.08 ?
n=100 46.35 19.60 63.6%
n=200 46.34 19.88 58.4%
Table 5: BLEU scores of different n-best sizes for the
highly inflected Dutch system and the noisy Estonian sys-
tem.
Table 5 illustrates how the n-best size affects
BLEU scores and model sizes for the nl-en and et-
en systems.
4.3 Phrase Model Re-estimation
This section conducts a preliminary evaluation of
the techniques introduced in Section 3.6. For fast
turnaround, these experiments were conducted on
approximately 1/3 of the Italian-English training
data. Training is performed with and without inser-
tion/deletion phrases and both with (FaTrain) and
without (FaPrune) re-training of the forward and
backward phrase translation probabilities. Table 6
shows the BLEU scores with different settings of the
global scaling factor ? and the inverse discount d.
The second global scaling factor is fixed to ? = 0.
The preliminary results seem to be invariant of the
settings. We conclude that using forced alignment
posteriors as a feature training method seems to be
less effective than using competing hypotheses from
free decoding as in (He and Deng, 2012).
BLEU
ins/del ? d Test1 WMT
baseline - - - 40.6 28.9
FaPrune no - - 40.7 29.1
FaTrain no 0 0 40.4 28.9
0.5 0 40.2 28.9
FaPrune yes - - 40.6 28.9
FaTrain yes 0 0 40.1 28.6
0.5 0 40.5 29.1
0.5 0.2 40.5 29.0
0.5 0.4 40.5 29.0
Table 6: Phrase pruning (FaPrune) vs. further model
re-estimation after pruning (FaTrain) on 1/3 it-en train-
ing data, both with and without on-demand inser-
tions/deletions.
5 Conclusion and Outlook
We applied forced alignment on parallel training
data with leave-one-out on four large-scale commer-
cial systems. In this way, we were able to reduce the
size of our already heavily pruned phrase tables by
at least 54%, with almost no loss in translation qual-
ity, and with a small improvement in speed perfor-
mance. We show that for language pairs with strong
reordering, the n-best list size needs to be increased
to account for the larger search space.
We introduced several extensions to the training
procedure. On-demand word insertions and dele-
tions can increase the data coverage to nearly 100%.
We plan to extend our work to use backoff transla-
tions (the target word that can not be extended given
the input phrase table will be aligned to any uncov-
ered single source word) to provide more alignment
varieties, and hence hopefully to be able to keep
more good phrase pairs. To avoid higher OOV rates
after pruning, we retrieved single-word translations
from the baseline phrase table.
We would like to emphasize that this leave-one-
out pruning technique is not restricted to phrasal
translators, even though all experiments presented
in this paper are on phrasal translators. It is possible
to extend the principle of forced alignment guided
pruning to hierarchical decoders, treelet decoders, or
syntax-based decoders, to prune redundant or use-
less phrase mappings or translation rules.
466
Re-estimating phrase translation probabilities us-
ing forced alignment posterior scores did not yield
any noticable BLEU improvement so far. Instead, we
propose to apply discriminative training similar to
(He and Deng, 2012) after forced-alignment-based
pruning as future work.
References
[Blunsom et al2008] Phil Blunsom, Trevor Cohn, and
Miles Osborne. 2008. A discriminative latent vari-
able model for statistical machine translation. In Pro-
ceedings of the 46th Annual Conference of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-08:HLT), pages 200?208, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
[Cmejrek et al2009] Martin Cmejrek, Bowen Zhou, and
Bing Xiang. 2009. Enriching SCFG Rules Directly
From Efficient Bilingual Chart Parsing. In Proc. of the
International Workshop on Spoken Language Transla-
tion, pages 136?143, Tokyo, Japan.
[de Gispert et al2010] Adria? de Gispert, Juan Pino, and
William Byrne. 2010. Hierarchical Phrase-based
Translation Grammars Extracted from Alignment Pos-
terior Probabilities. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 545?554, MIT, Massachusetts,
U.S.A., October.
[DeNero et al2006] John DeNero, Dan Gillick, James
Zhang, and Dan Klein. 2006. Why Generative Phrase
Models Underperform Surface Heuristics. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 31?38, New York City, June.
[Foster et al2006] George Foster, Roland Kuhn, and
Howard Johnson. 2006. Phrasetable Smoothing for
Statistical Machine Translation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 53?61, Sydney, Australia, July.
[He and Deng2012] Xiaodong He and Li Deng. 2012.
Maximum Expected BLEU Training of Phrase and
Lexicon Translation Models. In Proceedings of the
50th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page to appear, Jeju, Republic
of Korea, Jul.
[Johnson et al2007] J Howard Johnson, Joel Martin,
George Foster, and Roland Kuhn. 2007. Improv-
ing Translation Quality by Discarding Most of the
Phrasetable. In Proceedings of 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 967?975, Prague, June.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-
jar, Alexandra Constantine, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), demonstration ses-
sion, pages 177?180, Prague, Czech Republic, June.
[Liang et al2006] Percy Liang, Alexandre Buchard-Co?te?,
Dan Klein, and Ben Taskar. 2006. An End-to-End
Discriminative Approach to Machine Translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 761?768, Sydney, Australia.
[Mylonakis and Sima?an2010] Markos Mylonakis and
Khalil Sima?an. 2010. Learning Probabilistic Syn-
chronous CFGs for Phrase-based Translation. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 117?, Up-
psala,Sweden, July.
[Och2003] Franz Josef Och. 2003. Minimum Error Rate
Training in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
[Shen et al2008] Wade Shen, Brian Delaney, Tim Ander-
son, and Ray Slyh. 2008. The MIT-LL/AFRL IWSLT-
2008 MT System. In Proceedings of IWSLT 2008,
pages 69?76, Hawaii, U.S.A., October.
[Venugopal et al2003] Ashish Venugopal, Stephan Vo-
gel, and Alex Waibel. 2003. Effective Phrase Transla-
tion Extraction from Alignment Models. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics, pages 319?326, Sapporo,
Japan, July.
[Wuebker et al2010] Joern Wuebker, Arne Mauser, and
Hermann Ney. 2010. Training phrase translation mod-
els with leaving-one-out. In Proceedings of the 48th
Annual Meeting of the Assoc. for Computational Lin-
guistics, pages 475?484, Uppsala, Sweden, July.
467
