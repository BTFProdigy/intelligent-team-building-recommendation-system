Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89?94,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
That?s What She Said: Double Entendre Identification
Chloe? Kiddon and Yuriy Brun
Computer Science & Engineering
University of Washington
Seattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-
guage understanding problem. We identify
a subproblem ? the ?that?s what she said?
problem ? with two distinguishing character-
istics: (1) use of nouns that are euphemisms
for sexually explicit nouns and (2) structure
common in the erotic domain. We address
this problem in a classification approach that
includes features that model those two char-
acteristics. Experiments on web data demon-
strate that our approach improves precision by
12% over baseline techniques that use only
word-based features.
1 Introduction
?That?s what she said? is a well-known family of
jokes, recently repopularized by the television show
?The Office? (Daniels et al, 2005). The jokes con-
sist of saying ?that?s what she said? after someone
else utters a statement in a non-sexual context that
could also have been used in a sexual context. For
example, if Aaron refers to his late-evening basket-
ball practice, saying ?I was trying all night, but I just
could not get it in!?, Betty could utter ?that?s what
she said?, completing the joke. While somewhat ju-
venile, this joke presents an interesting natural lan-
guage understanding problem.
A ?that?s what she said? (TWSS) joke is a type of
double entendre. A double entendre, or adianoeta,
is an expression that can be understood in two differ-
ent ways: an innocuous, straightforward way, given
the context, and a risque? way that indirectly alludes
to a different, indecent context. To our knowledge,
related research has not studied the task of identify-
ing double entendres in text or speech. The task is
complex and would require both deep semantic and
cultural understanding to recognize the vast array of
double entendres. We focus on a subtask of double
entendre identification: TWSS recognition. We say
a sentence is a TWSS if it is funny to follow that
sentence with ?that?s what she said?.
We frame the problem of TWSS recognition as
a type of metaphor identification. A metaphor is
a figure of speech that creates an analogical map-
ping between two conceptual domains so that the
terminology of one (source) domain can be used to
describe situations and objects in the other (target)
domain. Usage of the source domain?s terminol-
ogy in the source domain is literal and is nonliteral
in the target domain. Metaphor identification sys-
tems seek to differentiate between literal and nonlit-
eral expressions. Some computational approaches to
metaphor identification learn selectional preferences
of words in multiple domains to help identify nonlit-
eral usage (Mason, 2004; Shutova, 2010). Other ap-
proaches train support vector machine (SVM) mod-
els on labeled training data to distinguish metaphoric
language from literal language (Pasanek and Scul-
ley, 2008).
TWSSs also represent mappings between two do-
mains: the innocuous source domain and an erotic
target domain. Therefore, we can apply methods
from metaphor identification to TWSS identifica-
tion. In particular, we (1) compare the adjectival
selectional preferences of sexually explicit nouns to
those of other nouns to determine which nouns may
be euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in the
erotic domain and nonerotic contexts. We present
a novel approach ? Double Entendre via Noun
Transfer (DEviaNT) ? that applies metaphor iden-
tification techniques to solving the double entendre
problem and evaluate it on the TWSS problem. DE-
viaNT classifies individual sentences as either funny
if followed by ?that?s what she said? or not, which
is a type of automatic humor recognition (Mihal-
cea and Strapparava, 2005; Mihalcea and Pulman,
2007).
We argue that in the TWSS domain, high preci-
sion is important, while low recall may be tolerated.
In experiments on nearly 21K sentences, we find
that DEviaNT has 12% higher precision than that of
baseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:
Section 2 will outline the characteristics of the
TWSS problem that we leverage in our approach.
Section 3 will describe the DEviaNT approach. Sec-
tion 4 will evaluate DEviaNT on the TWSS problem.
Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.
First, sentences with nouns that are euphemisms for
sexually explicit nouns are more likely to be TWSSs.
For example, containing the noun ?banana? makes
a sentence more likely to be a TWSS than contain-
ing the noun ?door?. Second, TWSSs share com-
mon structure with sentences in the erotic domain.
For example, a sentence of the form ?[subject] stuck
[object] in? or ?[subject] could eat [object] all day?
is more likely to be a TWSS than not. Thus, we
hypothesize that machine learning with euphemism-
and structure-based features is a promising approach
to solving the TWSS problem. Accordingly, apart
from a few basic features that define a TWSS joke
(e.g., short sentence), all of our approach?s lexical
features model a metaphorical mapping to objects
and structures in the erotic domain.
Part of TWSS identification is recognizing that
the source context in which the potential TWSS is
uttered is not in an erotic one. If it is, then the map-
ping to the erotic domain is the identity and the state-
ment is not a TWSS. In this paper, we assume all test
instances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-
ture work.
There are two interesting and important aspects
of the TWSS problem that make solving it difficult.
First, many domains in which a TWSS classifier
could be applied value high precision significantly
more than high recall. For example, in a social set-
ting, the cost of saying ?that?s what she said? inap-
propriately is high, whereas the cost of not saying
it when it might have been appropriate is negligible.
For another example, in automated public tagging of
twitter and facebook data, false positives are consid-
ered spam and violate usage policies, whereas false
negatives go unnoticed. Second, the overwhelm-
ing majority of everyday sentences are not TWSSs,
making achieving high precision even more difficult.
In this paper, we strive specifically to achieve high
precision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-
istics: (1) TWSSs are likely to contain nouns that
are euphemisms for sexually explicit nouns and (2)
TWSSs share common structure with sentences in
the erotic domain. Our approach to solving the
TWSS problem is centered around an SVM model
that uses features designed to model those charac-
teristics. We call our approach Double Entendre via
Noun Transfer, or the DEviaNT approach.
We will use features that build on corpus statistics
computed for known erotic words, and their lexical
contexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. We
manually approximated SN with a set of 76 nouns
that are predominantly used in sexual contexts. We
clustered the nouns into 9 categories based on which
sexual object, body part, or participant they identify.
Let SN? ? SN be the set of sexually explicit nouns
that are likely targets for euphemism. We did not
consider euphemisms for people since they rarely, if
ever, are used in TWSS jokes. In our approximation,?
?SN?
?
? = 61. Let BP be an open set of body-part
nouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpus
consists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removed
headers, footers, URLs, and unparseable text. The
Brown corpus (Francis and Kucera, 1979) is 57K
sentences that represent standard (nonerotic) litera-
ture. We tagged the erotica corpus with the Stanford
Parser (Toutanova and Manning, 2000; Toutanova
et al, 2003); the Brown corpus is already tagged.
To make the corpora more generic, we replaced all
numbers with the CD tag, all proper nouns with the
NNP tag, all nouns ? SN with an SN tag, and all
nouns 6? BP with the NN tag. We ignored determin-
ers and punctuation.
3.2 Word- and phrase-level analysis
We define three functions to measure how closely
related a noun, an adjective, and a verb phrase are to
the erotica domain.
1. The noun sexiness function NS(n) is a real-
valued measure of the maximum similarity a noun
n /? SN has to each of the nouns ? SN?. For each
noun, let the adjective count vector be the vector of
the absolute frequencies of each adjective that mod-
ifies the noun in the union of the erotica and the
Brown corpora. We define NS(n) to be the maxi-
mum cosine similarity, over each noun ? SN?, using
term frequency-inverse document frequency (tf-idf)
weights of the nouns? adjective count vectors. For
nouns that occurred fewer that 200 times, occurred
fewer than 50 times with adjectives, or were asso-
ciated with 3 times as many adjectives that never
occurred with nouns in SN than adjectives that did,
NS(n) = 10?7 (smaller than all recorded similari-
ties). Example nouns with high NS are ?rod? and
?meat?.
2. The adjective sexiness function AS(a) is a
real-valued measure of how likely an adjective a is
to modify a noun ? SN. We define AS(a) to be the
relative frequency of a in sentences in the erotica
corpus that contain at least one noun ? SN. Exam-
ple adjectives with high AS are ?hot? and ?wet?.
3. The verb sexiness function VS(v) is a real-
valued measure of how much more likely a verb
phrase v is to appear in an erotic context than a
nonerotic one. Let SE be the set of sentences in the
erotica corpus that contain nouns ? SN. Let SB be
the set of all sentences in the Brown corpus. Given
a sentence s containing a verb v, the verb phrase v
is the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closest
noun or one of the set of pronouns {I, you, it, me}.
(If neither a noun nor none of the pronouns occur on
a side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilities
of v appearing in an erotic and a nonerotic context
with counts in SE and SB, respectively. We normal-
ize the counts in SB such that P(s? SE) = P(s? SB).
Let VS(v) be the probability that (v ? s) =? (s is
in an erotic context). Then,
VS(v) = P(s ? SE |v ? s)
=
P(v ? s|s ? SE)P(s ? SE)
P(v ? s)
.
Intuitively, the verb sexiness is a measure of how
likely the action described in a sentence could be an
action (via some metaphoric mapping) to an action
in an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-
tential mappings of a sentence s into the erotic do-
main, organized into two categories: NOUN EU-
PHEMISMS and STRUCTURAL ELEMENTS.
NOUN EUPHEMISMS:
? (boolean) does s contain a noun ? SN?,
? (boolean) does s contain a noun ? BP?,
? (boolean) does s contain a noun n such that
NS(n) = 10?7,
? (real) average NS(n), for all nouns n ? s such
that n /? SN?BP,
STRUCTURAL ELEMENTS:
? (boolean) does s contain a verb that never oc-
curs in SE?,
? (boolean) does s contain a verb phrase that
never occurs in SE?,
? (real) average VS(v) over all verb phrases v? s,
? (real) average AS(a) over all adjectives a ? s,
? (boolean) does s contain an adjective a such
that a never occurs in a sentence s ? SE ? SB
with a noun ? SN.
DEviaNT also uses the following features to iden-
tify the BASIC STRUCTURE of a TWSS:
? (int) number of non-punctuation tokens,
? (int) number of punctuation tokens,
91
? ({0, 1, 2+}) for each pronoun and each part-of-
speech tag, number of times it occurs in s,
? ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,
?she?, ?it?), other pronoun}) the subject of s.
(We approximate the subject with the first noun
or pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKA
machine learning package (Hall et al, 2009) with
the features from Section 3.3. In our prototype im-
plementation, DEviaNT uses the default parameter
settings and has the option to fit logistic regression
curves to the outputs to allow for precision-recall
analysis. To minimize false positives, while toler-
ating false negatives, DEviaNT employs the Meta-
Cost metaclassifier (Domingos, 1999), which uses
bagging to reclassify the training data to produce
a single cost-sensitive classifier. DEviaNT sets the
cost of a false positive to be 100 times that of a false
negative.
4 Evaluation
The goal of our evaluation is somewhat unusual.
DEviaNT explores a particular approach to solving
the TWSS problem: recognizing euphemistic and
structural relationships between the source domain
and an erotic domain. As such, DEviaNT is at a dis-
advantage to many potential solutions because DE-
viaNT does not aggressively explore features spe-
cific to TWSSs (e.g., DEviaNT does not use a lexical
n-gram model of the TWSS training data). Thus, the
goal of our evaluation is not to outperform the base-
lines in all aspects, but rather to show that by using
only euphemism-based and structure-based features,
DEviaNT can compete with the baselines, particu-
larly where it matters most, delivering high precision
and few false positives.
4.1 Datasets
Our goals for DEviaNT?s training data were to
(1) include a wide range of negative samples to
distinguish TWSSs from arbitrary sentences while
(2) keeping negative and positive samples similar
enough in language to tackle difficult cases. DE-
viaNT?s positive training data are 2001 quoted sen-
tences from twssstories.com (TS), a website of
user-submitted TWSS jokes. DEviaNT?s negative
training data are 2001 sentences from three sources
(667 each): textsfromlastnight.com (TFLN), a
set of user-submitted, typically-racy text messages;
fmylife.com/intimacy (FML), a set of short (1?
2 sentence) user-submitted stories about their love
lives; and wikiquote.org (WQ), a set of quotations
from famous American speakers and films. We did
not carefully examine these sources for noise, but
given that TWSSs are rare, we assumed these data
are sufficiently negative. For testing, we used 262
other TS and 20,700 other TFLN, FML, and WQ
sentences (all the data from these sources that were
available at the time of the experiments). We cleaned
the data by splitting it into individual sentences, cap-
italizing the first letter of each sentence, tagging it
with the Stanford Parser (Toutanova and Manning,
2000; Toutanova et al, 2003), and fixing several tag-
ger errors (e.g., changing the tag of ?i? from the for-
eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven other
classifiers: (1) a Na??ve Bayes classifier on unigram
features, (2) an SVM model trained on unigram fea-
tures, (3) an SVM model trained on unigram and
bigram features, (4?6) MetaCost (Domingos, 1999)
(see Section 3.4) versions of (1?3), and (7) a version
of DEviaNT that uses just the BASIC STRUCTURE
features (as a feature ablation study). The SVM
models use the same parameters and kernel function
as DEviaNT.
The state-of-the-practice approach to TWSS iden-
tification is a na??ve Bayes model trained on a un-
igram model of instances of twitter tweets, some
tagged with #twss (VandenBos, 2011). While this
was the only existing classifier we were able to find,
this was not a rigorously approached solution to the
problem. In particular, its training data were noisy,
partially untaggable, and multilingual. Thus, we
reimplemented this approach more rigorously as one
of our baselines.
For completeness, we tested whether adding un-
igram features to DEviaNT improved its perfor-
mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P
r
e
c
i
s
i
o
n
Recall
DEviaNTBasic StructureUnigram SVM w/ MetaCostUnigram SVM w/o MetaCostBigram SVM w/ MetaCostBigram SVM w/o MetaCostNaive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT and
baseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-
viaNT and the other seven classifiers. DEviaNT and
Basic Structure achieve the highest precisions. The
best competitor ? Unigram SVM w/o MetaCost ?
has the maximum precision of 59.2%. In contrast,
DEviaNT?s precision is over 71.4%. Note that the
addition of bigram features yields no improvement
in (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we compared
those sentences that DEviaNT, Basic Structure, and
Unigram SVM w/o MetaCost are most sure are
TWSSs. DEviaNT returned 28 such sentences (all
tied for most likely to be a TWSS), 20 of which
are true positives. However, 2 of the 8 false pos-
itives are in fact TWSSs (despite coming from the
negative testing data): ?Yes give me all the cream
and he?s gone.? and ?Yeah but his hole really smells
sometimes.? Basic Structure was most sure about 16
sentences, 11 of which are true positives. Of these,
7 were also in DEviaNT?s most-sure set. However,
DEviaNT was also able to identify TWSSs that deal
with noun euphemisms (e.g., ?Don?t you think these
buns are a little too big for this meat??), whereas Ba-
sic Structure could not. In contrast, Unigram SVM
w/o MetaCost is most sure about 130 sentences, 77
of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVM
w/o MetaCost, it accomplishes our goal of deliver-
ing high-precision, while tolerating low recall.
Note that the DEviaNT?s precision appears low in
large because the testing data is predominantly neg-
ative. If DEviaNT classified a randomly selected,
balanced subset of the test data, DEviaNT?s preci-
sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-
problem of the double entendre problem. We then
identified two characteristics of the TWSS prob-
lem ? (1) TWSSs are likely to contain nouns that
are euphemisms for sexually explicit nouns and (2)
TWSSs share common structure with sentences in
the erotic domain ? that we used to construct
DEviaNT, an approach for TWSS classification.
DEviaNT identifies euphemism and erotic-domain
structure without relying heavily on structural fea-
tures specific to TWSSs. DEviaNT delivers sig-
nificantly higher precision than classifiers that use
n-gram TWSS models. Our experiments indicate
that euphemism- and erotic-domain-structure fea-
tures contribute to improving the precision of TWSS
identification.
While significant future work in improving DE-
viaNT remains, we have identified two character-
istics important to the TWSS problem and demon-
strated that an approach based on these character-
istics has promise. The technique of metaphorical
mapping may be generalized to identify other types
of double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and Mark
Yatskar for their insights and help with data, Bran-
don Lucia for his part in coming up with the name
DEviaNT, and Luke Zettlemoyer for helpful com-
ments. This material is based upon work supported
by the National Science Foundation Graduate Re-
search Fellowship under Grant #DGE-0718124 and
under Grant #0937060 to the Computing Research
Association for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-
chant. 2005. The Office. Television series, the
National Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A general
method for making classifiers cost-sensitive. In
Proceedings of the 5th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 155?164. San Diego, CA,
USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-
dard Corpus of Present-Day Edited American En-
glish. Department of Linguistics, Brown Univer-
sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction
system. Computational Linguistics, 30(1):23?44.
Rada Mihalcea and Stephen Pulman. 2007. Char-
acterizing humour: An exploration of features in
humorous texts. In Proceedings of the 8th Con-
ference on Intelligent Text Processing and Com-
putational Linguistics (CICLing07). Mexico City,
Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-
ing computers laugh: Investigations in auto-
matic humor recognition. In Human Language
Technology Conference / Conference on Empir-
ical Methods in Natural Language Processing
(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Mining
millions of metaphors. Literary and Linguistic
Computing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings
of Human Language Technologies: The 11th An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics
(HLT10), pages 1029?1037. Los Angeles, CA,
USA.
Kristina Toutanova, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of Human Language Tech-
nologies: The Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT03), pages 252?259. Ed-
monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Joint SIG-
DAT Conference on Empirical Methods in NLP
and Very Large Corpora (EMNLP/VLC00), pages
63?71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained ?that?s what she
said? bayes classifier. http://rubygems.org/
gems/twss.
94
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	