Identifying Real or Fake Articles: Towards better Language Modeling
Sameer Badaskar
School of Computer Science
Carnegie Mellon University
Pittsburgh PA, United States
sbadaska@cs.cmu.edu
Sachin Agarwal
School of Computer Science
Carnegie Mellon University
Pittsburgh PA, United States
sachina@cs.cmu.edu
Shilpa Arora
School of Computer Science
Carnegie Mellon University
Pittsburgh PA, United States
shilpaa@cs.cmu.edu
Abstract
The problem of identifying good features
for improving conventional language mod-
els like trigrams is presented as a classifica-
tion task in this paper. The idea is to use
various syntactic and semantic features ex-
tracted from a language for classifying be-
tween real-world articles and articles gener-
ated by sampling a trigram language model.
In doing so, a good accuracy obtained on the
classification task implies that the extracted
features capture those aspects of the lan-
guage that a trigram model may not. Such
features can be used to improve the exist-
ing trigram language models. We describe
the results of our experiments on the classi-
fication task performed on a Broadcast News
Corpus and discuss their effects on language
modeling in general.
1 Introduction
Statistical Language Modeling techniques attempt
to model language as a probability distribution
of its components like words, phrases and topics.
Language models find applications in classification
tasks like Speech Recognition, Handwriting Recog-
nition and Text Categorization among others. Con-
ventional language models based on n-grams ap-
proximate the probability distribution of a language
by computing probabilities of words conditioned on
previous n words as follows
P (s) ?
m?
i=1
p(wi|wi?n+1, . . . , wi?1) (1)
In most applications, lower order n-grams (such as
bigram or trigram) are used but they are an unre-
alistic approximation of the underlying language.
Higher order n-grams are desirable but they present
problems concerning data sparsity. On the other
hand, low order n-grams are incapable of represent-
ing other aspects of the language like the underlying
topics, topical redundancy etc. In order to build a
better language model, additional features have to
be augmented to the existing language model (e.g.
a trigram model) which capture those aspects of the
language that the trigram model does not. Now, one
way to test the goodness of a feature under consider-
ation is to use it in a framework like an exponential
model (Rosenfeld, 1997; Cai et al, 2000) and note
the improvement in perplexity. An alternative way
(Eneva et al, 2001) is as follows: Let L be the lan-
guage and L? be an approximation of the language
obtained by sampling the trigram language model.
Also, let X be a piece of text obtained from either L
or L?. Let y = h(f(X)) such that y = 1 if X ? L
and y = 0 if X ? L? where f(.) is the computed fea-
ture and h(.) is the hypothesis function (a classifier
like AdaBoost, SVM etc). If Pr[y = h(f(x))] is
found to be sufficiently high, it means that the fea-
ture f(x) is able to distinguish effectively between
the actual language L and the approximate language
L?. In other words, f(x) captures those features of
the language that are complementary to the ones
captured by the trigram model and therefore f(x)
is a good feature to augment the trigram language
model with.
The formalism explained previously can be inter-
preted as a classification task in-order to distinguish
817
between Real articles and Fake articles. Articles of
different lengths drawn at random from the Broad-
cast News Corpus (BNC)1 are termed as Real arti-
cles (from language L). Articles generated by sam-
pling the trigram model trained on the same corpus
are termed as Fake articles (language L?). These arti-
cles together form the training data for the classifier
to associate the features with the classification labels
(real or fake) where the features are computed from
the text. The features that give high classification ac-
curacy on the test set of articles are considered good
candidates for adding to the trigram model. Further-
more, the confidence that the classifier attaches to
a classification decision can be used to compute the
perplexity.
In this paper, a classification-task based formal-
ism is used to investigate the goodness of some new
features for language modeling. At the same time
features proposed in the previous literature on lan-
guage modeling are also revisited (Cai et al, 2000)
Section 2 discusses various syntactic and semantic
features used for the classification task, Section 3
gives details about the experiments conducted and
the classification results obtained and finally, Sec-
tion 4 concludes the paper by discussing the implica-
tions of the classification results on language model-
ing with pointers to improvements and future work.
2 Feature Engineering
To differentiate a real article from a fake one, the
empirical, syntactic and semantic characteristics of
a given article are used to compute the features for
the classification task. The various types of features
that were experimented are as follows:
2.1 Empirical Features
Empirical features are based on the statistical anal-
ysis of both the real and fake articles. They include
the count of uncommon pairs of words within an ar-
ticle, the ratio of perplexity of trigram and quadgram
models for a given article and the nature of the POS
tags that occur at the start and end of sentences in an
article.
1http://www.cs.cmu.edu/ roni/11761-s07/project/LM-train-
100MW.txt.gz
Ratio of Perplexity of trigram and quad-gram
models
Given an article, the ratio of its perplexity for a tri-
gram model to a quad-gram model is computed. The
trigram and quad-gram models are both trained on
the same BNC corpus. Both real and fake articles
would give a low perplexity score for the tri-gram
model but for the quad-gram model, real articles
would have significantly lower perplexity than the
fake articles. This implies that the ratio of trigram
to quad-gram perplexities would be lower for a fake
article than for a real article. In other words, this ra-
tio is similar to computing the likelihood ratio of an
article w.r.t the trigram and quad-gram models. The
histogram in Figure 1 shows a good separation in
the distribution of values of this feature for the real
and fake articles which indicates the effectiveness of
this feature. A quadgram language model is a better
approximation of real text than a trigram model and
by using this as a feature, we are able to demonstrate
the usefulness of the classification task as a method
for identifying good features for language modeling.
In the subsequent sections, we investigate other fea-
tures using this classification framework.
Figure 1: Histogram for the ratio of perplexities with
respect to Trigram and Quadgram Language models
over the training set
Count of uncommon pairs of words
Content words are the frequently occurring words in
the corpus excluding the stop-words. All the words
in corpus are ranked according to frequency of their
occurrence and content words are defined to be the
words with rank between 150 and 6500. A list of
common content word pairs (pairs of content words
818
atleast 5 words apart) is prepared from the real cor-
pus by sorting the list of content word pairs by their
frequency of occurrence and retaining those above a
certain threshold. For a given article, a list of content
word pairs is compared against this list and word
pairs not in this list form the set of uncommon word
pairs.
A real article is expected to have lesser num-
ber of uncommon content-word pairs than fake arti-
cles. When normalized by the total number of word
pairs, we get the probability of finding an uncom-
mon content-word pair in an article. This probabil-
ity is greater for fake articles than the real articles
and we use this probability as a feature for the clas-
sification task.
Start and End POS Tags
Certain POS tags are more probable than others to
appear at the beginning or end of a real sentence.
This characteristic of real text could be used as a
feature to distinguish real articles from fake. The
distribution of POS tags of the first and last words of
the sentences in an article is used as a feature. Our
experiments show that this feature had very little ef-
fect in the overall contribution to the classification
accuracy over the development set.
2.2 Syntactic Features
These features are derived from the parse struc-
ture of the sentence. It is hypothesized that real
sentences tend to be grammatical while the same
may not be the case for fake sentences. An objec-
tive measure of the grammaticality of a sentence
can be obtained by running it through a statisti-
cal parser. The log-likelihood score returned by
the parser can be used to judge the grammatical-
ity of a sentence and thus determine whether it
is fake or real. The Charniak Parser (Charniak,
2001; Charniak, 2005) was used for assessing the
grammaticality of the articles under test. Given
an article containing sentences S1, S2, . . . , SN with
lengths L1, L2, . . . , LN , we compute the parser log-
likelihood scores P (S1), P (S2), . . . , P (SN ). The
overall grammaticality score for an article is given
by
PGram =
?N
i=1 LiP (Si)?N
i=1 Li
(2)
The grammaticality score was normalized using the
average and standard deviation over the entire train-
ing set. This feature gave small improvement in
terms of classification accuracy. There may be sev-
eral reasons for this: (1) Our training data consisted
of spoken transcripts from a broadcast news corpus
whereas the Charniak Parser was trained on a differ-
ent domain (Wall Street Journal) and (2) The parser
was trained on mixed case text where as the data we
used was all upper case.
2.3 Semantic Features
Real articles contain sentences with correlated pairs
of content-words and sentences that are correlated
with each other. An article with such sentence/word
correlations is said to be semantically coherent. Ow-
ing to the use of only the short term word history for
computing the probability distribution of a language,
a trigram model fails to model semantic coherence
and we exploit this fact for the classification task.
Specifically, we intend to model both intra-sentence
and inter-sentence semantic coherence and use them
as features for classification.
Intra-sentence Coherence
To model the intra-sentence word correlations, we
use Yule?s Q-statistic (Eneva et al, 2001). The word
correlations are learned from the BNC corpus as
well as the fake corpus. The coherence score for
an article is defined as the sum of the correlations
between pairs of content words present in the arti-
cle. The coherence score for an article is normalized
by the total number of content-word pairs found in
the article. Since the trigram and quad-gram lan-
guage model can capture short distance coherences
well, coherences between distant words can be used
to differentiate between real and fake articles. The
Yule Q-statistic is calculated for every pair of con-
tent words, which are atleast 5 words apart within a
sentence, both in the real and fake corpus.
The articles are scored according to content word-
pair correlations learned from the real as well as
fake corpus. Each article is given two scores, one
for the word-pair correlations from real articles and
other for the word-pair correlations from fake arti-
cles. For a real article, the real word-pair correla-
tion score would be relatively higher compared to
the fake word-pair correlation score (and vice-versa
819
for a fake article).
Modeling Topical Redundancy (Inter-sentence
Coherence)
A characteristic of real articles is that they tend to
be cohesive in terms of the topic under discussion.
For example, a news-article about a particular event
(topic) would have several direct or indirect refer-
ences to the event. We interpret this as some sort
of a redundancy in terms of the information con-
tent which we term as Topical Redundancy. The
fake articles would not exhibit such a redundancy.
If a real article is transformed to another represen-
tation space where some form of truncation is ap-
plied, on transformation back to the original space,
the amount of information-loss may not be signif-
icant due to information redundancy. However, if
the same process is applied on a fake article, the
information-loss would be significant when trans-
formed back to the original space. We intend to ex-
ploit this fact for our classification task.
Let DW?N be an article represented in the form
of a matrix, where W is the article vocabulary and N
is the number of sentences in that article. Every term
of this matrix represents the frequency of occurrence
of a vocabulary word in a particular sentence. We
construct a sentence-sentence matrix as follows:
A = DTD (3)
We now transform A into the Eigen-space using Sin-
gular Value Decomposition (SVD) which gives
A = USUT (4)
Here, UN?N is the eigen-vector matrix and SN?N
is the diagonal eigen-value matrix. If we retain only
the top K eigen-values from S , we get the truncated
(lossy) form S?K?K . Thus the truncated form of A
i.e. A? is
A? = US?UT (5)
We believe that the information loss ? A?A? ?2
will not be significant in the case of real articles
since the topical redundancy is captured in a very
compact manner by the eigen-representation. How-
ever, in the case of a fake article, the loss is con-
siderable. For a real article, the matrix would be
less sparse than a fake article and so is the case for
the reconstructed matrix. Therefore, the statistics -
mean, median, minimum and maximum computed
from the reconstructed matrix have higher values for
real articles than a fake articles. We use these statis-
tics as features for classifying the article. Figure 2
show the histograms of the statistics computed from
the reconstructed matrix for the training set. As can
be seen, there is a good separation between the two
classes fake and real in all the cases. Using these
features increased the classification accuracy by a
significant amount as shown later. From another per-
spective, these features model the inter-sentence se-
mantic coherence (Deerwester et al, 1990) within an
article and this is consistent with our notion of topi-
cal redundancy as explained previously. The matrix
package developed by NIST (Hicklin et al, 2005)
was used for SVD.
3 Experimental Results
3.1 Data Distribution
The training data consisted of 1000 articles (500 real
and 500 fake) obtained from Broadcast News Cor-
pus (BNC) and the test set consisted of 200 articles
(100 real and 100 fake). Additionally, a develop-
ment dataset consisting of 200 articles and having
the same distribution as that of the test dataset was
used for tuning the parameters of the classifiers. To
ensure that the training and test data come from the
same article length distribution, the training data was
resampled to have the same percentage of articles of
a given length as in the test set. The article length
distribution for both the training(resampled) and test
datasets is shown in Tables 1 and 2.
3.2 Classifier
Classifiers like AdaBoost (Freund et al, 1999) and
Max-Entropy (Rosenfeld, 1997) models were used
for the classification task.
The number of iterations for AdaBoost was esti-
mated using 5-fold cross-validation. Given a sub-
set of features, Maxent classified 74.5% of the doc-
uments correctly compared to 82% for AdaBoost.
Therefore, Adaboost was chosen as the classifier for
further experiments.
820
(a) Mean (b) Median
(c) Minimum (d) Maximum
Figure 2: Histograms of topical redundancy features computed over the training set. In (b) , the median
values for the fake articles are close to zero and hence cannot be seen clearly.
3.3 Results and Discussion
We used two performance measures to evaluate our
model. First is the accuracy which measures the
number of articles correctly classified as real or fake
and the second measure is the log-probability that
the model assigns to the classification decision i.e. it
measures the confidence the model has in its classi-
fication. Table 3 shows our experimental results on
the syntactic, semantic and empirical features.
The combination of syntactic, semantic and em-
pirical features gave an accuracy of 91.5% with an
average log-likelihood of -0.22 on development data
set. The accuracy on the test dataset was 87% with
an average log-likelihood of -0.328.
4 Conclusions and Future Work
In this work, we have used a classification-task
based formalism for evaluating various syntactic,
semantic and empirical features with the objective
of improving conventional language models. Fea-
tures that perform well in the task of classifying
real and trigram-generated fake articles are useful
for augmenting the trigram model. Semantic fea-
tures, such as topical redundancy, model long-range
dependencies which are not captured by a trigram
language model. Therefore, the semantic features
contribute significantly to the classification task ac-
curacy. Additionally, linguistic resources such as
WordNet (WordNet, 1998) can be used to model
821
# Sentences
per article
# Real
Art.
# Fake
Art.
% Total
(Real &
Fake)
1 938 940 19.76
2 440 471 9.58
3 502 474 10.26
4 507 533 10.94
5 497 525 10.75
7 431 524 10.05
10 475 479 10.04
15 482 421 9.50
20 421 446 9.12
Table 1: Distribution of article lengths for training
dataset.
# Sentences
per article
# Real
Art.
# Fake
Art.
% Total
(Real &
Fake)
1 20 20 20
2 10 10 10
3 10 10 10
4 10 10 10
5 10 10 10
7 10 10 10
10 10 10 10
15 10 10 10
20 10 10 10
Table 2: Distribution of article lengths for test
dataset.
topical redundancy using synonyms and other inter-
word dependencies. The semantic features we ex-
plored assume a single underlying topic for an arti-
cle which may not be always true. An article can
be a representation of different topics and we aim to
explore this direction in future.
References
Can Cai, Larry Wasserman and Roni Rosenfeld.
2000. Exponential language models, logistic regres-
sion, and semantic coherence. Proceedings of the
NIST/DARPA Speech Transcription Workshop.
Eugene Charniak. 2001. Immediate-Head Parsing for
Language Models. Proceedings of 39th Annual Meet-
ing of the ACL, 124-131.
Feature Combination Classification
Accuracy
Avg. Log
Likelihood
Syntactic 60.5% -0.663
Semantic 79.5% -0.510
Empirical 83.0% -0.446
Semantic + Syntactic 80.0% -0.553
Semantic + Empirical 86.0% -0.410
Semantic + Syntactic +
Empirical
91.5% -0.220
Table 3: Performance of different features on the
development-set.
Eugene Charniak. 2005. ftp://ftp.cs.brown.edu/pub/ nl-
parser/parser05Aug16.tar.gz
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley & Sons, New York.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of
Japanese Society for Artificial Intelligence, 41(6).
Elena Eneva, Rose Hoberman and Lucian Lita. 2001.
Learning within-sentence semantic coherence. Pro-
ceedings of the EMNLP 2001.
Yoav Freund and Robert E. Schapire. 1999. A short in-
troduction to boosting Journal of Japanese Society for
Artificial Intelligence, 14(5):771-780.
Joe Hicklin, Cleve Moler, Peter Webb. 2005.
http://math.nist.gov/javanumerics/jama/
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Roni Rosenfeld. 1997. A whole sentence maximum en-
tropy language model. In Proc. of the IEEE Workshop
on Automatic Speech Recognition and Understanding,
1997.
WordNet: An Electronic Lexical Database, ISBN-13:
978-0-262-06197-1.
822
Proceedings of NAACL HLT 2009: Short Papers, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Identifying Types of Claims in Online Customer Reviews
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?
Language Technologies Institute
School of Computer Science
Carnegie Mellon University, Pittsburgh PA 15213
{shilpaa,maheshj,cprose}@cs.cmu.edu
Abstract
In this paper we present a novel approach to
categorizing comments in online reviews as
either a qualified claim or a bald claim. We ar-
gue that this distinction is important based on
a study of customer behavior in making pur-
chasing decisions using online reviews. We
present results of a supervised algorithm for
learning this distinction. The two types of
claims are expressed differently in language
and we show that syntactic features capture
this difference, yielding improvement over a
bag-of-words baseline.
1 Introduction
There has been tremendous recent interest in opin-
ion mining from online product reviews and it?s ef-
fect on customer purchasing behavior. In this work,
we present a novel alternative categorization of com-
ments in online reviews as either being qualified
claims or bald claims.
Comments in a review are claims that reviewers
make about the products they purchase. A customer
reads the reviews to help him/her make a purchas-
ing decision. However, comments are often open
to interpretation. For example, a simple comment
like this camera is small is open to interpretation
until qualified by more information about whether
it is small in general (for example, based on a poll
from a collection of people), or whether it is small
compared to some other object. We call such claims
bald claims. Customers hesitate to rely on such bald
claims unless they identify (from the context or oth-
erwise) themselves to be in a situation similar to the
customer who posted the comment. The other cate-
gory of claims that are not bald are qualified claims.
Qualified claims such as it is small enough to fit
easily in a coat pocket or purse are more precise
claims as they give the reader more details, and are
less open to interpretation. Our notion of qualified
claims is similar to that proposed in the argumenta-
tion literature by Toulmin (1958). This distinction
of qualified vs. bald claims can be used to filter
out bald claims that can?t be verified. For the quali-
fied claims, the qualifier can be used in personalizing
what is presented to the reader.
The main contributions of this work are: (i) an an-
notation scheme that distinguishes qualified claims
from bald claims in online reviews, and (ii) a super-
vised machine learning approach that uses syntactic
features to learn this distinction. In the remainder
of the paper, we first motivate our work based on
a customer behavior study. We then describe the
proposed annotation scheme, followed by our su-
pervised learning approach. We conclude the paper
with a discussion of our results.
2 Customer Behavior Study
In order to study how online product reviews are
used to make purchasing decisions, we conducted
a user study. The study involved 16 pair of gradu-
ate students. In each pair there was a customer and
an observer. The goal of the customer was to de-
cide which camera he/she would purchase using a
camera review blog1 to inform his/her decision. As
the customer read through the reviews, he/she was
1http://www.retrevo.com/s/camera
37
asked to think aloud and the observer recorded their
observations.
The website used for this study had two types of
reviews: expert and user reviews. There were mixed
opinions about which type of reviews people wanted
to read. About six customers could relate more with
user reviews as they felt expert reviews were more
like a ?sales pitch?. On the other hand, about five
people were interested in only expert reviews as they
believed them to be more practical and well rea-
soned.
From this study, it was clear that the customers
were sensitive to whether a claim was qualified or
not. About 50% of the customers were concerned
about the reliability of the comments and whether
it applied to them. Half of them felt it was hard
to comprehend whether the user criticizing a feature
was doing so out of personal bias or if it represented
a real concern applicable to everyone. The other half
liked to see comments backed up with facts or ex-
planations, to judge if the claim could be qualified.
Two customers expressed interest in comments from
users similar to themselves as they felt they could
base their decision on such comments more reli-
ably. Also, exaggerations in reviews were deemed
untrustworthy by at least three customers.
3 Annotation Scheme
We now present the guidelines we used to distin-
guish bald claims from qualified claims. A claim
is called qualified if its validity or scope is limited
by making the conditions of its applicability more
explicit. It could be either a fact or a statement that
is well-defined and attributed to some source. For
example, the following comments from our data are
qualified claims according to our definition,
1. The camera comes with a lexar 16mb starter
card, which stores about 10 images in fine mode
at the highest resolution.
2. I sent my camera to nikon for servicing, took
them a whole 6 weeks to diagnose the problem.
3. I find this to be a great feature.
The first example is a fact about the camera. The
second example is a report of an event. The third
example is a self-attributed opinion of the reviewer.
Bald claims on the other hand are non-factual
claims that are open to interpretation and thus cannot
be verified. A straightforward example of the dis-
tinction between a bald claim and a qualified claim
is a comment like the new flavor of peanut butter is
being well appreciated vs. from a survey conducted
among 20 people, 80% of the people liked the new
flavor of peanut butter. We now present some exam-
ples of bald claims. A more detailed explanation is
provided in the annotation manual2:
? Not quantifiable gradable3 words such as
good, better, best etc. usually make a claim
bald, as there is no qualified definition of being
good or better.
? Quantifiable gradable words such as small,
hot etc. make a claim bald when used without
any frame of reference. For example, a com-
ment this desk is small is a bald claim whereas
this desk is smaller than what I had earlier is a
qualified claim, since the comparative smaller
can be verified by observation or actual mea-
surement, but whether something is small in
general is open to interpretation.
? Unattributed opinion or belief: A comment
that implicitly expresses an opinion or belief
without qualifying it with an explicit attribu-
tion is a bald claim. For example, Expectation
is that camera automatically figures out when
to use the flash.
? Exaggerations: Exaggerations such as on ev-
ery visit, the food has blown us away do not
have a well defined scope and hence are not
well qualified.
The two categories for gradable words defined above
are similar to what Chen (2008) describes as vague-
ness, non-objective measurability and imprecision.
4 Related work
Initial work by Hu and Liu (2004) on the product
review data that we have used in this paper focuses
on the task of opinion mining. They propose an ap-
proach to summarize product reviews by identifying
opinionated statements about the features of a prod-
uct. In our annotation scheme however, we classify
2www.cs.cmu.edu/?shilpaa/datasets/
opinion-claims/qbclaims-manual-v1.0.pdf
3http://en.wikipedia.org/wiki/English_
grammar#Semantic_gradability
38
all claims in a review, not restricting to comments
with feature mentions alone.
Our task is related to opinion mining, but with a
specific focus on categorizing statements as either
bald claims that are open to interpretation and may
not apply to a wide customer base, versus qualified
claims that limit their scope by making some as-
sumptions explicit. Research in analyzing subjec-
tivity of text by Wiebe et al (2005) involves identi-
fying expression of private states that cannot be ob-
jectively verified (and are therefore open to interpre-
tation). However, our task differs from subjectivity
analysis, since both bald as well as qualified claims
can involve subjective language. Specifically, objec-
tive statements are always categorized as qualified
claims, but subjective statements can be either bald
or qualified claims. Work by Kim and Hovy (2006)
involves extracting pros and cons from customer re-
views and as in the case of our task, these pros and
cons can be either subjective or objective.
In supervised machine learning approaches to
opinion mining, the results using longer n-grams and
syntactic knowledge as features have been both pos-
itive as well as negative (Gamon, 2004; Dave et al,
2003). In our work, we show that the qualified vs.
bald claims distinction can benefit from using syn-
tactic features.
5 Data and Annotation Procedure
We applied our annotation scheme to the product re-
view dataset4 released by Hu and Liu (2004). We
annotated the data for 3 out of 5 products. Each
comment in the review is evaluated as being quali-
fied or bald claim. The data has been made available
for research purposes5.
The data was completely double coded such that
each review comment received a code from the two
annotators. For a total of 1, 252 review comments,
the Cohen?s kappa (Cohen, 1960) agreement was
0.465. On a separate dataset (365 review com-
ments)6, we evaluated our agreement after remov-
ing the borderline cases (only about 14%) and there
4http://www.cs.uic.edu/?liub/FBS/
CustomerReviewData.zip
5www.cs.cmu.edu/?shilpaa/datasets/
opinion-claims/qbclaims-v1.0.tar.gz
6These are also from the Hu and Liu (2004) dataset, but not
included in our dataset yet.
was a statistically significant improvement in kappa
to 0.532. Since the agreement was low, we resolved
our conflict by consensus coding on the data that was
used for supervised learning experiments.
6 Experiments and Results
For our supervised machine learning experiments on
automatic classification of comments as qualified or
bald, we used the Support Vector Machine classifier
in the MinorThird toolkit (Cohen, 2004) with the de-
fault linear kernel. We report average classification
accuracy and average Cohen?s Kappa using 10-fold
cross-validation.
6.1 Features
We experimented with several different features in-
cluding standard lexical features such as word uni-
grams and bigrams; pseudo-syntactic features such
as Part-of-Speech bigrams and syntactic features
such as dependency triples7. Finally, we also used
syntactic scope relationships computed using the de-
pendency triples. Use of features based on syntactic
scope is motivated by the difference in how quali-
fied and bald claims are expressed in language. We
expect these features to capture the presence or ab-
sence of qualifiers for a stated claim. For example,
?I didn?t like this camera, but I suspect it will be a
great camera for first timers.? is a qualified claim,
whereas a comment like ?It will be a great camera
for first timers.? is not a qualified claim. Analysis of
the syntactic parse of the two comments shows that
in the first comment the word ?great? is in the scope
of ?suspect?, whereas this is not the case for the sec-
ond comment. We believe such distinctions can be
helpful for our task.
We compute an approximation to the syntactic
scope using dependency parse relations. Given
the set of dependency relations of the form
relation, headWord, dependentWord, such as
AMOD,camera,great, an in-scope feature is de-
fined as INSCOPE headWord dependentWord (IN-
SCOPE camera great). We then compute a tran-
sitive closure of such in-scope features, similar to
Bikel and Castelli (2008). For each in-scope feature
in the entire training fold, we also create a corre-
7We use the Stanford Part-of-Speech tagger and parser re-
spectively.
39
Features QBCLAIM HL-OP
Majority .694(.000) .531(.000)
Unigrams .706(.310) .683(.359)
+Bigrams .709(.321) .693(.378)
+POS-Bigrams .726*(.353*) .683(.361)
+Dep-Triples .711(.337) .692(.376)
+In-scope .706(.340) .688(.367)
+Not-in-scope .726(.360*) .687(.370)
+All-scope .721(.348) .699(.396)
Table 1: The table shows accuracy (& Cohen?s kappa in paren-
theses) averaged across ten folds. Each feature set is individ-
ually added to the baseline set of unigram features. * - Re-
sult is marginally significantly better than unigrams-only (p <
0.10, using a two-sided pairwise T-test). HL-OP - Opinion an-
notations from Hu and Liu (2004). QBCLAIM - Qualified/Bald
Claim.
sponding not-in-scope feature which triggers when
either (i) the dependent word appears in a comment,
but not in the transitive-closured scope of the head
word, or (ii) the head word is not contained in the
comment but the dependent word is present.
We evaluate the benefit of each type of feature
by adding them individually to the baseline set of
unigram features. Table 1 presents the results. We
use the majority classifier and unigrams-only perfor-
mance as our baselines. We also experimented with
using the same feature combinations to learn the
opinion category as defined by Hu and Liu (2004)
[HL-OP] on the same subset of data.
It can be seen from Table 1 that using purely
unigram features, the accuracy obtained is not
any better than the majority classifier for quali-
fied vs. bald distinction. However, the Part-of-
Speech bigram features and the not-in-scope fea-
tures achieve a marginally significant improvement
over the unigrams-only baseline.
For the opinion dimension from Hu and Liu
(2004), there was no significant improvement from
the type of syntactic features we experimented with.
Hu and Liu (2004)?s opinion category covers several
different types of opinions and hence finer linguis-
tic distinctions that help in distinguishing qualified
claims from bald claims may not apply in that case.
7 Conclusions
In this work, we presented a novel approach to re-
view mining by treating comments in reviews as
claims that are either qualified or bald. We argued
with examples and results from a user study as to
why this distinction is important. We also proposed
and motivated the use of syntactic scope as an ad-
ditional type of syntactic feature, apart from those
already used in opinion mining literature. Our eval-
uation demonstrates a marginally significant posi-
tive effect of a feature space that includes these and
other syntactic features over the purely unigram-
based feature space.
Acknowledgments
We would like to thank Dr. Eric Nyberg for the
helpful discussions and the user interface for doing
the annotations. We would also like to thank all the
anonymous reviewers for their helpful comments.
References
Daniel Bikel and Vittorio Castelli. Event Matching Using
the Transitive Closure of Dependency Relations. Pro-
ceedings of ACL-08: HLT, Short Papers, pp. 145?148.
Wei Chen. 2008. Dimensions of Subjectivity in Natural
Language. In Proceedings of ACL-HLT?08. Colum-
bus Ohio.
Jacob Cohen. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Measure-
ment, Vol. 20, No. 1., pp. 37-46.
William Cohen. 2004. Minorthird: Methods for Iden-
tifying Names and Ontological Relations in Text us-
ing Heuristics for Inducing Regularities from Data.
http://minorthird.sourceforge.net/
Kushal Dave, Steve Lawrence and David M. Pennock
2006. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
In Proc of WWW?03.
Michael Gamon. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Minqing Hu and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proc. of the ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining.
Soo-Min Kim and Eduard Hovy. 2006. Automatic Iden-
tification of Pro and Con Reasons in Online Reviews.
In Proc. of the COLING/ACL Main Conference Poster
Sessions.
Stephen Toulmin 1958 The Uses of Argument. Cam-
bridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
40
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 55?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Interactive Annotation Learning with Indirect Feature Voting
Shilpa Arora and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{shilpaa,ehn}@cs.cmu.edu
Abstract
We demonstrate that a supervised annotation
learning approach using structured features
derived from tokens and prior annotations per-
forms better than a bag of words approach.
We present a general graph representation for
automatically deriving these features from la-
beled data. Automatic feature selection based
on class association scores requires a large
amount of labeled data and direct voting can
be difficult and error-prone for structured fea-
tures, even for language specialists. We show
that highlighted rationales from the user can
be used for indirect feature voting and same
performance can be achieved with less labeled
data.We present our results on two annotation
learning tasks for opinion mining from prod-
uct and movie reviews.
1 Introduction
Interactive Annotation Learning is a supervised ap-
proach to learning annotations with the goal of min-
imizing the total annotation cost. In this work, we
demonstrate that with additional supervision per ex-
ample, such as distinguishing discriminant features,
same performance can be achieved with less anno-
tated data. Supervision for simple features has been
explored in the literature (Raghavan et al, 2006;
Druck et al, 2008; Haghighi and Klein, 2006). In
this work, we propose an approach that seeks super-
vision from the user on structured features.
Features that capture the linguistic structure in
text such as n-grams and syntactic patterns, referred
to as structured features in this work, have been
found to be useful for supervised learning of annota-
tions. For example, Pradhan et al (2004) show that
using features like syntactic path from constituent
to predicate improves performance of a semantic
parser. However, often such features are ?hand-
crafted? by domain experts and do not generalize to
other tasks and domains. In this work, we propose
a general graph representation for automatically ex-
tracting structured features from tokens and prior an-
notations such as part of speech, dependency triples,
etc. Gamon (2004) shows that an approach using
a large set of structured features and a feature selec-
tion procedure performs better than an approach that
uses a few ?handcrafted? features. Our hypothesis
is that structured features are important for super-
vised annotation learning and can be automatically
derived from tokens and prior annotations. We test
our hypothesis and present our results for opinion
mining from product reviews.
Deriving features from the annotation graph gives
us a large number of very sparse features. Fea-
ture selection based on class association scores such
as mutual information and chi-square have often
been used to identify the most discriminant features
(Manning et al, 2008). However, these scores are
calculated from labeled data and they are not very
meaningful when the dataset is small. Supervised
feature selection, i.e. asking the user to vote for the
most discriminant features, has been used as an al-
ternative when the training dataset is small. Ragha-
van et al (2006) and Druck et al (2008) seek feed-
back on unigram features from the user for docu-
ment classification tasks. Haghighi and Klein (2006)
ask the user to suggest a few prototypes (examples)
for each class and use those as features. These ap-
proaches ask the annotators to identify globally rel-
55
evant features, but certain features are difficult to
vote on without the context and may take on very
different meanings in different contexts. Also, all
these approaches have been demonstrated for uni-
gram features and it is not clear how they can be
extended straightforwardly to structured features.
We propose an indirect approach to interactive
feature selection that makes use of highlighted ra-
tionales from the user. A rationale (Zaidan et al,
2007) is the span of text a user highlights in support
of his/her annotation. Rationales also allow us to
seek feedback on features in context. Our hypothe-
sis is that with rationales, we can achieve same per-
formance with lower annotation cost and we demon-
strate this for opinion mining from movie reviews.
In Section 2, we describe the annotation graph
representation and motivate the use of structured
features with results on learning opinions from prod-
uct reviews. In Section 3, we show how rationales
can be used for identifying the most discriminant
features for opinion classification with less training
data. We then list the conclusions we can draw from
this work, followed by suggestions for future work.
2 Learning with Structured Features
In this section, we demonstrate that structured fea-
tures help in improving performance and propose a
formal graph representation for deriving these fea-
tures automatically.
2.1 Opinions and Structured Features
Unigram features such as tokens are not sufficient
for recognizing all kinds of opinions. For example,
a unigram feature good may seem useful for identi-
fying opinions, however, consider the following two
comments in a review: 1) This camera has good fea-
tures and 2) I did a good month?s worth of research
before buying this camera. In the first example,
the unigram good is a useful feature. However, in
the second example, good is not complementing the
camera and hence will mislead the classifier. Struc-
tured features such as part-of-speech, dependency
relations etc. are needed to capture the language
structure that unigram features fail to capture.
2.2 Annotation Graph and Features
We define the annotation graph as a quadruple: G =
(N,E,?, ?), where N is the set of nodes, E is the
set of edges E ? N ? N , ? = ?N ? ?E is a
set of labels for nodes and edges. ? is the label-
ing function ? : N ? E ? ?, that assigns labels to
nodes and edges. In this work, we define the set of
labels for nodes, ?N as tokens, part of speech and
dependency annotations and set of labels for edges,
?E as relations, ?E = {leftOf, parentOf, restricts}.
The leftOf relation is defined between two adjacent
nodes. The parentOf relation is defined between the
dependency type and its attributes. For example, for
the dependency triple ?nsubj perfect camera?, there
is a parentOf relation between the dependency type
?nsubj? and tokens ?perfect? and ?camera?. The re-
stricts relation exists between two nodes a and b if
their textual spans overlap completely and a restricts
how b is interpreted. For a word with multiple senses
the restricts relation between the word and its part of
speech, restricts the way the word is interpreted, by
capturing the sense of the word in the given context.
The Stanford POS tagger (Toutanova and Manning,
2000) and the Stanford parser (Klein and Manning,
2003) were used to produce the part of speech and
dependency annotations.
Features are defined as subgraphs, G? =
(N ?, E?,??, ??) in the annotation graph G, such that
N ? ? N ,E? ? N ??N ? andE? ? E, ?? = ??N???E
where ??N ? ?N and ??E ? ?E and ?? : N ??E? ???. For a bag of words approach that only uses to-
kens as features, ??N = T , where T is the token
vocabulary and E = ? and ?E = ? (where ? is the
null set). We define the degree of a feature subgraph
as the number of edges it contains. For example, the
unigram features are the feature subgraphs with no
edges i.e. degree = 0. Degree? 1 features are the
feature subgraphs with two nodes and an edge. In
this paper, we present results for feature subgraphs
with degree = 0 and degree = 1.
Figure 1 shows the partial annotation graph for
two comments discussed above. The feature sub-
graph that captures the opinion expressed in 1(a),
can be described in simple words as ?camera has
features that are good?. This kind of subject-object
relationship with the same verb, between the ?cam-
era? and what?s being modified by ?good?, is not
present in the second example (1(b)). A slight modi-
fication of 1(b), I did a month?s worth of research be-
fore buying this good camera does express an opin-
ion about the camera. A bag of words approach that
uses only unigram features will not be able to differ-
56
entiate between these two examples; structured fea-
tures like dependency relation subgraphs can capture
this linguistic distinction between the two examples.
P24:amod
[16,29]
P23:JJ
[16,20]
P22:dobj
[12,29]
P21:nsubj
[5,15]
restricts
parentOf
parentOf
parentOf
(a)
(b)
Figure 1: The figure shows partial annotation graphs for two examples.
Only some of the nodes and edges are shown for clarity. Spans of nodes
in brackets are the character spans.
2.3 Experiments and Results
The dataset we used is a collection of 244 Amazon?s
customer reviews (2962 comments) for five products
(Hu and Liu, 2004). A review comment is annotated
as an opinion if it expresses an opinion about an as-
pect of the product and the aspect is explicitly men-
tioned in the sentence. We performed 10-fold cross
validation (CV) using the Support Vector Machine
(SVM) classifier in MinorThird (Cohen, 2004) with
the default linear kernel and chi-square feature se-
lection to select the top 5000 features. As can be
seen in Table 1, an approach using degree ? 0 fea-
tures, i.e. unigrams, part of speech and dependency
triples together, outperforms using any of those fea-
tures alone and this difference is significant. Us-
ing degree ? 1 features with two nodes and an
edge improves performance further. However, using
degree?0 features in addition to degree?1 features
does not improve performance. This suggests that
when using higher degree features, we may leave out
the features with lower degree that they subsume.
Features Avg F1 Outperforms
unigram [uni] 65.74 pos,dep
pos-unigram [pos] 64 dep
dependency [dep] 63.18 -
degree-0 [deg-0] 67.77 uni,pos,dep
degree-1 [deg-1] 70.56 uni,pos,dep,deg-0, deg-*
(deg-0 + deg-1) [deg-*] 70.12 uni,pos,dep,deg-0
Table 1: The table reports the F-measure scores averaged over ten cross
validation folds. The value in bold in the Avg F1 column is the best
performing feature combination. For each feature combination in the
row, outperforms column lists the feature combinations it outperforms,
with significant differences highlighted in bold (paired t-test with p <
0.05 considered significant).
3 Rationales & Indirect Feature voting
We propose an indirect feature voting approach that
uses user-highlighted rationales to identify the most
discriminant features. We present our results on
Movie Review data annotated with rationales.
3.1 Data and Experimental Setup
The data set by Pang and Lee (2004) consists of
2000 movie reviews (1000-pos, 1000-neg) from the
IMDb review archive. Zaidan et al (2007) provide
rationales for 1800 reviews (900-pos, 900-neg). The
annotation guidelines for marking rationales are de-
scribed in (Zaidan et al, 2007). An example of a
rationale is: ?the movie is so badly put together
that even the most casual viewer may notice the mis-
erable pacing and stray plot threads?. For a test
dataset of 200 reviews, randomly selected from 1800
reviews, we varied the training data size from 50 to
500 reviews, adding 50 reviews at a time. Training
examples were randomly selected from the remain-
ing 1600 reviews. During testing, information about
rationales is not used.
We used tokens1, part of speech and dependency
triples as features. We used the KStem stemmer
(Krovetz, 1993) to stem the token features. In or-
der to compare the approaches at their best perform-
ing feature configuration, we varied the total num-
ber of features used, choosing from the set: {1000,
2000, 5000, 10000, 50000}. We used chi-square
feature selection (Manning et al, 2008) and the
SVM learner with default settings from the Minor-
third package (Cohen, 2004) for these experiments.
We compare the following approaches:
Base Training Dataset (BTD): We train a model
from the labeled data with no feature voting.
1filtering the stop words using the stop word list: http:
//www.cs.cmu.edu/?shilpaa/stop-words-ial-movie.
txt
57
Rationale annotated Training Dataset (RTD):
We experimented with two different settings for in-
direct feature voting: 1) only using features that
overlap with rationales (RTD(1, 0)); 2) features
from rationales weighted twice as much as features
from other parts of the text (RTD(2, 1)). In general,
R(i, j) describes an experimental condition where
features from rationales are weighted i times and
other features are weighted j times. In Minorthird,
weighing a feature two times more than other fea-
tures is equivalent to that feature occurring twice as
much.
Oracle voted Training Data (OTD): In order to
compare indirect feature voting to direct voting on
features, we simulate the user?s vote on the features
with class association scores from a large dataset
(all 1600 documents used for selecting training doc-
uments). This is based on the assumption that the
class association scores, such as chi-square, from a
large dataset can be used as a reliable discriminator
of the most relevant features. This approach of sim-
ulating the oracle with large amount of labeled data
has been used previously in feature voting (Ragha-
van et al, 2006).
3.2 Results and Discussion
In Table 2, we present the accuracy results for the
four approaches described in the previous section.
We compare the best performing feature configura-
tions for three approaches - BTD, RTD(1, 0) and
RTD (2,0). As can be seen, RTD(1, 0) always per-
forms better than BTD. As expected, improvement
with rationales is greater and it is significant when
the training dataset is small. The performance of
all approaches converge as the training data size in-
creases and hence we only present results up to train-
ing dataset size of 500 examples in this paper.
Since our goal is to evaluate the use of rationales
independently of how many features the model uses,
we also compared the four approaches in terms of
the accuracy averaged over five feature configura-
tions. Due to space constraints, we do not include
the table of results. On average RTD(1, 0) signif-
icantly outperforms BTD when the total training
dataset is less than 350 examples. When the train-
ing data has fewer than 400 examples, RTD(1, 0)
also significantly outperforms RTD(2, 1).
OTD with simulated user is an approximate up-
#Ex Approach Number of Features1000 2000 5000 10000 50000
50
OTD 67.63 66.30 62.90 52.17 55.03
BTD 58.10 57.47 52.67 51.80 55.03
RTD(1,0)* 55.43 55.93 61.63 61.63 61.63
RTD(2,1) 57.77 57.53 52.73 52.30 56.33
100
OTD 71.97 71.07 70.27 69.37 64.33
BTD 64.17 64.43 62.70 56.63 64.37
RTD(1,0)* 65.43 63.27 65.13 67.23 67.23
RTD(2,1) 64.27 63.93 62.47 56.10 63.77
150
OTD 73.83 74.83 74.20 74.00 63.83
BTD 66.17 67.77 68.60 64.33 60.47
RTD(1,0)* 69.30 68.30 67.27 71.30 71.87
RTD(2,1) 68.00 67.07 68.43 63.57 58.90
200
OTD 74.83 75.87 75.70 75.10 56.97
BTD 71.63 71.37 72.57 71.53 58.90
RTD(1,0) 72.23 72.63 71.63 73.80 73.93
RTD(2,1) 71.20 71.10 73.03 70.77 57.87
250
OTD 75.63 76.90 77.70 77.67 62.20
BTD 72.60 73.57 74.73 75.20 58.93
RTD(1,0) 73.00 73.57 73.57 74.70 76.70
RTD(2,1) 72.87 73.90 74.63 75.40 57.43
300
OTD 76.57 77.67 78.93 78.43 68.17
BTD 72.97 74.13 74.93 76.57 63.83
RTD(1,0) 74.43 74.83 74.67 74.73 77.67
RTD(2,1) 72.67 74.53 74.37 76.53 61.30
350
OTD 76.47 78.20 80.20 79.80 71.73
BTD 74.43 74.30 74.73 77.27 66.80
RTD(1,0) 75.07 76.20 75.80 75.20 78.53
RTD(2,1) 74.63 75.70 74.80 78.23 64.93
400
OTD 77.97 78.93 80.53 80.60 75.27
BTD 75.83 76.77 76.47 78.93 70.63
RTD(1,0) 75.17 76.40 75.83 76.00 79.23
RTD(2,1) 75.73 76.07 76.80 78.50 68.20
450
OTD 77.67 79.20 80.57 80.73 77.13
BTD 75.73 76.80 77.80 78.80 74.37
RTD(1,0)* 74.83 76.50 76.23 76.47 80.40
RTD(2,1) 75.87 76.87 77.87 78.87 71.80
500
OTD 78.03 80.10 81.27 81.67 79.87
BTD 75.27 77.33 79.37 80.30 75.73
RTD(1,0) 75.77 77.63 77.47 77.27 81.10
RTD(2,1) 75.83 77.47 79.50 79.70 74.50
Table 2: Accuracy performance for four approaches, five feature con-
figurations and increasing training dataset size. Accuracy reported is
averaged over five random selection of training documents for three ran-
domly selected test datasets. The numbers in bold in a row represents
the best performing feature configuration for a given approach and train-
ing dataset size. The approach in bold represents the best performing
approach among BTD, RTD(1, 0) and RTD(2, 1) for a given train-
ing dataset size. ?*? indicates significant improvement in performance
over BTD (paired t-test with p < 0.05 considered significant).
per bound for rationale based approaches. It tells
us how far we are from direct supervision on struc-
tured features. On average, OTD significantly out-
performed RTD(1, 0) for training data size of 100,
150, 400, 450 and 500 examples but not always.
As can be seen from Table 2, difference between
OTD and RTD(1, 0) reduces with more training
data, since with more data and hence more rationales
we get better feature coverage.
Results presented here show that for a given train-
ing dataset, we can boost the performance by ask-
58
ing the user to label rationales. However, there is
an additional cost associated with the rationales. It
is important to evaluate how much total annotation
cost rationales can save us while achieving the de-
sired performance. In Figure 2, we compare the
number of training examples an approach needs to
achieve a given level of performance. As can be
seen, RTD(1, 0) needs fewer training examples to
achieve the same performance as BTD. The differ-
ence is large initially when the total number of train-
ing examples is small (50 forRTD(1, 0) and 150 for
BTD to achieve a performance between 66? 67).
Figure 2: The Figure shows the number of examples needed by the
two approaches, RTD(1, 0) and BTD, to achieve an accuracy in the
given range.
Comparison with Zaidan et al (2007): Zaidan
et al (2007) conclude that using only features from
rationales performs worse than both: 1) using all the
features in the documents, and 2) using features that
do not overlap with the rationales. The results pre-
sented in this paper seem to contradict their results.
However, they only experimented with unigram fea-
tures and only one approach to using features from
rationales, RTD(1, 0) and not RTD(2, 1). In order
to compare our work directly with theirs, we exper-
imented with an equivalent set of unigram features.
In Table 3, we present the results using same num-
ber of total features (17744) as Zaidan et al (2007).
As can be seen from the table, when only unigram
features are used,RTD(2, 1) outperformsBTD but
RTD(1, 0) performs worse than BTD. Thus, our
results are consistent with (Zaidan et al, 2007) i.e.
using unigram features only from the rationales does
not boost performance.
From Table 3, we also analyze the improvement
in performance when part of speech and depen-
dency features are used in addition to the unigram
features i.e. using all degree ? 0 subgraph fea-
#Ex Approach uni uni-pos uni-pos-dep
100
OTD 68.6 68.8 61.6
BTD 68.6 68.8 52.2
RTD(1,0) 68.2 68.1 69.0*
RTD(2,0) 70.0 67.0 51.7
200
OTD 73.6 73.8 75.3
BTD 73.6 73.8 67.1
RTD(1,0) 73.9 73.2 73.9*
RTD(2,0) 75.3* 70.3 65.2
300
OTD 76.2 76.1 79.1
BTD 76.2 76.1 73.7
RTD(1,0) 75.0 74.9 77.1*
RTD(2,0) 77.5* 73.3 74.8
400
OTD 77.4 76.8 79.9
BTD 77.4 76.8 76.2
RTD(1,0) 75.9 75.9 77.0
RTD(2,0) 78.0 74.7 77.7*
500
OTD 78.1 78.1 80.0
BTD 78.1 78.1 78.4
RTD(1,0) 76.3 76.2 77.6
RTD(2,0) 78.2 75.4 79.0
Table 3: The Table reports accuracy for four approaches in a setting
similar to (Zaidan et al, 2007). Accuracy reported is averaged over ten
random selection of training documents for two randomly selected test
datasets.The numbers in bold are the best among BTD, RTD(1, 0),
RTD(2, 1) for a given feature combination. ?*? highlights the signif-
icant improvement in performance over BTD (using paired t-test, with
p < 0.05 considered significant).
tures. For RTD(1, 0), adding these features im-
proves performance for all data sizes with signifi-
cant improvement for dataset size of 300 and 500 ex-
amples. RTD(1, 0) also significantly outperforms
BTD when all three features are used. For direct
voting on features (OTD), a significant improve-
ment with these structured features is seen when the
training dataset size is greater than 200 examples.
For BTD and RTD(2, 1) approaches, there is no
significant improvement with these additional fea-
tures. In the future, we plan to investigate further
the benefit of using higher degree subgraph features
for opinion mining from the movie review data.
Comparing ranking of features:We also com-
pared the features that the rationales capture to what
the oracle will vote for as the most relevant features.
Features are ranked based on chi-square scores used
in feature selection. We compare the ranked list of
features from RTD(1, 0), BTD and OTD and use
a weighted F-measure score for evaluating the top
100 ranked features by each approach. This measure
is inspired by the Pyramid measure used in Summa-
rization (Nenkova and Passonneau, 2004). Instead
of using counts in calculating F-measure, we used
the chi-square score assigned to the features by the
oracle dataset, in order to give more weight to the
more discriminant features. As can be seen from
59
Table 4, RTD(1, 0) outperforms BTD in captur-
ing the important features when the datasize set is
small (< 300) and this difference is significant. Be-
yond 300 examples, as the data size increases,BTD
outperforms RTD(1, 0). This implies that the ra-
tionales alone are able to capture the most relevant
features when the dataset is small.
100 200 300 400 500 600 700
RO 47.70 53.80 57.68 59.54 62.13 60.86 61.56
TO 31.22 44.43 52.98 60.57 64.61 67.10 70.39
Table 4: Weighted F-measure performance comparison of ranked list
of features from RTD(1, 0) & OTD(RO) and BTD & OTD(TO).
Results are averaged over ten random selections of the training data for
a randomly selected test dataset. Significant differences are highlighted
in bold (paired t-test with p < 0.05 considered significant).
4 Conclusion and Future Work
In this work, we demonstrated that using structured
features boosts performance of supervised annota-
tion learning. We proposed a formal annotation
graph representation that can be used to derive these
features automatically. However, the space of pos-
sible feature subgraphs can grow very large with
more prior annotations. Standard feature selection
techniques based on class association scores are less
effective when the dataset is small. Feature voting
from the user for identifying the relevant features
is limited to simple features. Supplementary input
from the user in terms of highlighted rationales can
be used instead to prune the feature space. The pro-
posed approach is general and can be applied to a
variety of problems and features.
In this work, we presented our results with
degree ? 0 and degree ? 1 feature subgraphs.
We will extend our algorithm to automatically ex-
tract higher degree features from the annotation
graph. For the rationale annotated training data
(RTD(i, j)), we experimented with two possible
values for i and j. We aim to learn these weights
empirically using a held out dataset. Rationales are
associated with an additional cost per example and
hence two approaches, with and without the ratio-
nales, are not directly comparable in terms of the
number of examples. In the future, we will conduct
an annotation experiment with real users to evaluate
the usefulness of rationales in terms of clock time.
Acknowledgments
We would like to thank Dr. Carolyn P. Rose for
her help with statistical analysis of the results. We
would also like to thank all the anonymous review-
ers for their helpful comments.
References
Cohen W. Minorthird: Methods for Identifying Names
and Ontological Relations in Text using Heuristics for
Inducing Regularities from Data. 2004. (http://
minorthird.sourceforge.net/).
Druck G., Mann G. and McCallum A. Learning from la-
beled features using generalized expectation criteria.
In Proceedings of the ACM SIGIR, 2008.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. In Proceedings of
COLING, 2005.
Haghighi A. and Klein D. Prototype-driven learning for
sequence models. In Proceedings of the NAACL HLT
2006.
Minqing Hu and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proc. of the ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining.
Klein D. and Manning C. Accurate Unlexicalized Pars-
ing. In Proceedings of ACL 2003.
Krovetz R. Viewing Morphology as an Infer-
ence Process. http://ciir.cs.umass.edu/
pubfiles/ir-35.pdf
Manning C., Raghavan P. and Schu?tze H. Introduction to
Information Retrieval. Cambridge University Press.
2008.
Nenkova A. and Passonneau R. Evaluating Content Se-
lection In Summarization: The Pyramid Method. In
Proceedings of HLT-NAACL 2004.
Pang B. and Lee L. ?A Sentimental Education: Sen-
timent Analysis Using Subjectivity Summarization
Based on Minimum Cuts? In Proceedings of the ACL,
2004.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Martin, Daniel Jurafsky. 2004. Shallow Seman-
tic Parsing using Support Vector Machines. In Pro-
ceedings of HLT/NAACL-2004,Boston, MA, May 2-
7, 2004
Raghavan H., Madani O. and Jones R. Active Learning
with Feedback on Both Features and Instances. Jour-
nal of Machine Learning Research, 2006.
Toutanova K. and Manning C. Enriching the Knowledge
Sources Used in a Maximum Entropy Part-of-Speech
Tagger. In Proceedings of EMNLP/VLC-2000.
Zaidan O., Eisner J. and Piatko C. Using ?annotator ra-
tionales? to improve machine learning for text catego-
rization. In Proceedings of NAACL-HLT 2007.
Zaidan O. and Eisner J. Modeling Annotators: A Genera-
tive Approach to Learning from Annotator Rationales.
In Proceedings of EMNLP 2008.
60
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 18?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating Annotation Cost for Active Learning
in a Multi-Annotator Environment
Shilpa Arora, Eric Nyberg and Carolyn P. Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{shilpaa,ehn,cprose}@cs.cmu.edu
Abstract
We present an empirical investigation of the
annotation cost estimation task for active
learning in a multi-annotator environment. We
present our analysis from two perspectives:
selecting examples to be presented to the user
for annotation; and evaluating selective sam-
pling strategies when actual annotation cost
is not available. We present our results on a
movie review classification task with rationale
annotations. We demonstrate that a combina-
tion of instance, annotator and annotation task
characteristics are important for developing an
accurate estimator, and argue that both corre-
lation coefficient and root mean square error
should be used for evaluating annotation cost
estimators.
1 Introduction
Active Learning is the process of selectively query-
ing the user to annotate examples with the goal
of minimizing the total annotation cost. Annota-
tion cost has been traditionally measured in terms
of the number of examples annotated, but it has
been widely acknowledged that different examples
may require different annotation effort (Settles et al,
2008; Ringger et al, 2008).
Ideally, we would use actual human annotation
cost for evaluating selective sampling strategies, but
this will require conducting several user studies, one
per strategy on the same dataset. Alternatively, we
may be able to simulate the real user by an annota-
tion cost estimator that can then be used to evaluate
several selective sampling strategies without having
to run a new user study each time. An annotation
cost estimator models the characteristics that can
differentiate the examples in terms of their annota-
tion time. The characteristics that strongly correlate
with the annotation time can be used as a criterion
in selective sampling strategies to minimize the total
annotation cost.
In some domains, the annotation cost of an ex-
ample is known or can be calculated exactly before
querying the user. For example, in biological ex-
periments it might be calculable from the cost of
the equipment and the material used (King et al,
2004). In NLP, sometimes a simplifying assumption
is made that the annotation cost for an example can
be measured in terms of its length (e.g. seconds of
voicemail annotated (Kapoor et al, 2007); number
of tokens annotated (Tomanek et al, 2007)). An-
other assumption is that the number of user anno-
tation actions can be used as a proxy for annota-
tion cost of an example (e.g. number of brackets
added for parsing a sentence (Hwa, 2000); number
of clicks for correcting named entities (Kristjannson
et al, 2004)). While these are important factors in
determining the annotation cost, none of them alone
can fully substitute for the actual annotation cost.
For example, a short sentence with a lot of embed-
ded clauses may be more costly to annotate than a
longer sentence with simpler grammatical structure.
Similarly, a short sentence with multiple verbs and
discontinuous arguments may take more time to an-
notate with semantic roles than a longer sentence
with a single verb and simple subject-verb-object
structure (Carreras and Ma?rquez, 2004).
What further complicates the estimation of anno-
tation cost is that even for the same example, anno-
tation cost may vary across annotators (Settles et al,
2008). For example, non-native speakers of English
were found to take longer time to annotate part of
18
speech tags (Ringger et al, 2008). Often multiple
annotators are used for creating an annotated cor-
pus to avoid annotator bias, and we may not know
all our annotators beforehand. Annotation cost also
depends on the user interface used for annotation
(Gweon et al, 2005), and the user interface may
change during an annotation task. Thus, we need
a general annotation cost estimator that can predict
annotation cost for a given annotator and user inter-
face. A general estimator can be built by using an-
notator and user interface characteristics in addition
to the instance characteristics for learning an anno-
tation cost model, and training on data from mul-
tiple annotators and multiple user interfaces. Such
a general estimator is important for active learning
research where the goal is to compare selective sam-
pling strategies independent of the annotator and the
user interface.
In this work, we investigate the annotation cost es-
timation problem for a movie review classification
task in a multi-annotator environment with a fixed
user interface. We demonstrate that a combination
of instance, annotation task and annotator charac-
teristics is important for accurately estimating the
annotation cost. In the remainder of the paper, we
first present a survey of related work and an analysis
of the data collected. We then describe the features
used for our supervised learning approach to anno-
tation cost estimation, followed by the experimental
setup and results. Finally, we conclude with some
future directions we would like to explore.
2 Related work
There has been some recent research effort in using
supervised learning for estimating annotation cost.
The most closely related work is that by Settles et al
(2008) and Ringger et al (2008). Settles et al (2008)
present a detailed analysis of annotation cost for four
NLP applications: named entity recognition, image
retrieval, speculative vs. definite distinction, and in-
formation extraction. They study the effect of do-
main, annotator, jitter, order of examples, etc., on
the annotation cost.
Results from Settles et al (2008) are promising
but leave much room for improvement. They used
only instance level features such as number of en-
tities, length, number of characters, percentage of
non-alpha numeric characters, etc. for annotation
cost estimation. For three of their tasks, the corre-
lation between the estimated and actual annotation
times was in the range (R = 0.587 to 0.852). Note
that the percentage of variance accounted for by a
model is obtained by squaring the R value from the
correlation coefficient. Thus, an R value of 0.587
indicates that only about 34% (R2) of the variance
is accounted for, so the model will make incorrect
predictions about ranking in the majority of cases.
Nevertheless, we acknowledge that our results are
not substantially better, although we argue that this
work contributes to the pool of knowledge that will
hopefully lead to better performance in the future.
Settles et al (2008) train and test their estimator
on data from the same annotator. Thus, in order
to use their model for a new annotator, we would
need to first collect data for that annotator and train
a model. In our work, a group of annotators anno-
tate the same text, and we train and test on different
annotators. We also show that using characteristics
of the annotators and annotation task in addition to
the instance characteristics improves performance.
Ringger et al (2008) use linear regression for an-
notation cost estimation for Part-Of-Speech (POS)
tagging. About 30 annotators annotated 36 different
instances each. The authors present about 13 de-
scriptive statistics of the data, annotator and annota-
tion task, but in their model they only used number
of tokens in the sentence and the number of correc-
tions needed as features. They report that the other
variables didn?t have a significant effect when eval-
uated using a Bayesian Information Criterion (from
the R package).
Ringger et al (2008) noticed that nativeness of
the annotator did have an effect on the annotation
time, but they chose not to include that feature
in their model as they expected to have a similar
mix of skills and background in their target anno-
tators. However, if annotation times differ substan-
tially across annotators, then not accounting for this
difference will reduce the performance of the model.
Also, the low adjusted correlation value for their
model (R = 0.181) indicates that there is only a
weak correlation between the annotation time and a
linear combination of the length of the example and
the number of corrections.
19
3 Analysis and Experiments
In this section, we present our annotation methodol-
ogy and analysis of the data we collected, followed
by a description of the features we used.We then
present our experimental setup followed by a dis-
cussion of our results.
3.1 Annotation Methodology and Data
Analysis
In this work, we estimate the annotation cost for a
movie review classification task. The data we used
were collected as part of a graduate course. Twenty
annotators (students and instructors) were grouped
into five groups of four each. The groups were cre-
ated such that each group had similar variance in
annotator characteristics such as department, educa-
tional experience, programming experience, etc. We
used the first 200 movie reviews from the dataset
provided by Zaidan et al (2007), with an equal dis-
tribution of positive and negative examples. Each
group annotated 25 movie reviews randomly se-
lected from the 200 reviews and all annotators in
each group annotated all 25 reviews. In addition
to voting positive or negative for a review, annota-
tors also annotated rationales (Zaidan et al, 2007),
spans of text in the review that support their vote.
Rationales can be used to guide the model by identi-
fying the most discriminant features. In related work
(Arora and Nyberg, 2009), we ascertain that with ra-
tionales the same performance can be achieved with
less annotated data. The annotation task with ra-
tionales involved a variety of user actions: voting
positive or negative, highlighting spans of text and
adding rationale annotations. We used the same an-
notation guidelines as Zaidan et al (2007). The data
has been made available for research purposes1. Fig-
ure 1 shows a screenshot of the GUI used. We per-
formed an analysis of our data similar to that con-
ducted by Settles et al (2008). We address the fol-
lowing main questions.
Are the annotation times variable enough? If all
examples take a similar time to annotate, then the
number of examples can be used as an approxima-
tion for the annotation cost. Figure 2 shows the his-
togram of averaged annotation times (averaged over
1www.cs.cmu.edu/?shilpaa/datasets/ial/
ial-uee-mr-v0.1.zip
Figure 1: The GUI used for the annotation task. The user
selects the review (segment) to annotate from the list in the
right panel. The review text is displayed in the left panel. The
user votes positive or negative using the radio buttons. Ratio-
nales are added by selecting a span of text and right clicking
to select the rationale tag. The start/stop button can be used to
pause the current task.
Figure 2: Distribution of averaged annotation times
4 annotators in a group). As can be seen from the
mean (? = 165 sec.) and the standard deviation
(? = 68.85), there is a meaningful variance in the
annotation times.
How do the annotation times vary across annota-
tors? A strong correlation between annotation times
from different annotators on a set of instances sug-
gests that there are certain characteristics of these in-
stances, independent of the annotator characteristics,
that can determine their ranking based on the time it
takes to annotate them. We evaluated the pairwise
correlation for all pairs of annotators in each group
(Table 1). As can be seen, there is significant pair-
wise correlation in more than half of the pairs of an-
notators that differ in nativeness (10/16). However,
not all such pairs of annotators are associated with
significant correlation. This suggests that it is im-
portant to consider both instance and annotator char-
acteristics for estimating annotation time.
20
group Avg-Na(Std) Avg-CR(Std) #sign-pairs
0 2.25(0.96) 0.54(0.27) 4/6 (4/5)
1 1.75(0.5) 0.45(0.08) 5/6 (2/3)
2 1(0) 0.13(0.17) 0/6 (0/0)
3 1.75(0.96) 0.36(0.12) 2/6 (1/5)
4 2.75(0.5) 0.47(0.04) 6/6 (3/3)
Avg. 1.9(0.58) 0.39(0.21) 17/30 (10/16)
Table 1: The Table shows the average nativeness and average
pairwise correlation between annotation times for the mem-
bers of each group (and their standard deviation). #sign-pairs
shows the fraction of pairwise correlations within the groups
that were significant (p < 0.05). In brackets, is the fraction
of correlations between annotators with different nativeness
within the groups that were significant.
The box plot in Figure 3 shows the distribution
of annotation times across annotators. As can be
seen, some annotators take in general much longer
than others, and the distribution of times is very dif-
ferent across annotators. For some, the annotation
times vary a lot, but not so much for others. This
suggests that using annotator characteristics as fea-
tures in addition to the instance characteristics may
be important for learning a better estimator.
Figure 3: Box plot shows the annotation time (in sec) dis-
tribution (y-axis) for an annotator (x-axis) for a set of 25 doc-
uments. g0-a1 represents annotator 1 of group 0 and g0-avg
represents the average annotation time. A box represents the
middle 50% of annotation times, with the line representing the
median. Whiskers on either side span the 1st and 4th quartiles
and asterisks indicate the outliers.
3.2 Feature Design
We group the features in the following three cat-
egories: Instance, Annotation Task and Annotator
characteristics.
3.2.1 Instance characteristics
Instance characteristics capture the properties of
the example the user annotates. Table 2 describes
the instance based features we used and the intu-
ition supporting their use for annotation cost esti-
mation. Table 3 shows the mean and standard de-
viation of each of these characteristics, and as can
be seen, these characteristics do vary across exam-
ples and hence these features can be beneficial for
distinguishing examples.
3.2.2 Annotation Task characteristics
Annotation task characteristics are those that can
be captured only during or after the annotation task.
We used the number of rationales as a feature from
this category. In addition to voting for movie re-
views as positive or negative, the user also adds ra-
tionales that support their vote. More rationales im-
ply more work since the user must look for the rele-
vant span of text and perform the physical action of
selecting the span and adding an annotation for each
rationale. Table 3 shows the distribution of the aver-
age Number of Rationales (NR) per example (aver-
aged over the four annotators for a given set).
3.2.3 Annotator characteristics
The annotation cost of an example may vary
across annotators. As reported in Table 1, the aver-
age correlation for annotators on the same document
is low (R = 0.39) with 17 out of 30 pairwise correla-
tions being significant. Thus, it is important to con-
sider annotator characteristics, such as whether the
annotator is a native speaker of English, their educa-
tion level, reading ability, etc. In this work, we only
use nativeness of the annotator as a feature and plan
to explore other characteristics in the future. We as-
signed each annotator a nativeness value. A value
of 3 was given to an annotator whose first language
is English. A value of 2 was given to an annotator
who has a different first language but has either been
educated in English or has been in the United States
for a long time. A value of 1 was assigned to the re-
maining annotators. Among the 20 annotators in the
study, there were 8 annotators with nativeness value
of 1, and 6 each for nativeness values of 2 and 3.
Table 1 shows the average and standard deviation of
the nativeness score in each group.
21
Feature Definition Intuition
Character
Length (CL)
Length of review in
terms of number of
characters
Longer documents
take longer to anno-
tate
Polar word
Count (PC)
Number of words
that are polar (strong
subjective words
from the lexicon
(Wilson et al, 2005))
More subjectivity
implies user would
need more time to
judge positive vs.
negative
Stop word
Percent (SC)
Percentage of words
that are stop words
A high percentage
of stop words im-
plies that the text is
not very complex and
hence easier to read.
Avg. Sen-
tence Length
(SL)
Average of the char-
acter length of sen-
tences in the review
Long sentences in a
review may make it
harder to read.
Table 2: Instance characteristics
Feature Mean Standard Deviation
CL 2.25 0.92
PC 41.50 20.39
SP 0.45 0.03
SL 121.90 28.72
NR 4.80 2.30
Table 3: Mean and the standard deviation for the feature oc-
currences in the data.
3.3 Evaluation Metric
We use both Root Mean Square (RMS) error and
Correlation Coefficient (CRCoef) to evaluate our
model, since the two metrics evaluate different as-
pects of an estimate. RMS is a way to quantify the
amount by which an estimator differs from the true
value of the quantity being estimated. It tells us how
?off? our estimate is from the truth. CRCoef on the
other hand measures the strength and direction of a
linear relationship between two random variables. It
tells us how well correlated our estimate is with the
actual annotation time. Thus, for evaluating how ac-
curate our model is in predicting annotation times,
RMS is a more appropriate metric. For evaluating
the utility of the estimated annotation cost as a cri-
terion for ranking and selecting examples for user?s
annotation, CRCoef is a better metric.
3.4 Experiments & Results
We learn an annotation cost estimator using the Lin-
ear Regression and SMO Regression (Smola and
Scholkopf, 1998) learners from the Weka machine
learning toolkit (Witten and Frank, 2005). As men-
tioned earlier, we have 5 sets of 25 documents each,
and each set was annotated by four annotators. The
results reported are averaged over five folds, where
each set is one fold, and two algorithms (Linear Re-
gression and SMO Regression). Varying the algo-
rithm helps us find the most predictive feature com-
binations across different algorithms. Since each set
was annotated by different annotators, we never train
and test on the data from same annotators. We used
the JMP2 and Minitab3 statistical tools for our analy-
sis. We used an ANOVA model with Standard Least
Squares fitting to compare the different experimen-
tal conditions. We make all comparisons in terms
of both the CRCoef and the RMS metrics. For sig-
nificance results reported, we used 2-tailed paired
T-test, considering (p < 0.05) as significant.
We present our results and analysis in three parts.
We first compare the four instance characteristics,
annotator and annotation task characteristics; and
their combination. We then present an analysis
of the interaction between features and annotation
time. Finally, we compare the ranking of features
based on the two evaluation metrics we used.
3.4.1 Comparing characteristics for annotation
cost estimation
Instance Characteristics: We compare the four
instance characteristics described in Section 3.2.1
and select the most predictive characteristic for fur-
ther analysis with annotator and annotation task
characteristics. As can be seen in Table 4, character
length performs the best, and it is significantly better
than stop word percent and average sentence length.
Character length also outperforms polar word count,
but this difference is not significant. Because of the
large significant difference between the performance
of stop word percent and average sentence length,
compared to character length, we do not consider
them for further analysis.
Feature Combinations: In Table 5, we compare
the feature combinations of instance, annotator and
annotation task characteristics. The table also shows
the weights for the features used and the constant for
the linear regression model trained on all the data. A
missing weight for a feature indicates that it wasn?t
used in that feature combination.
2http://www.jmp.com/software/
3http://www.minitab.com/
22
Feature CR-Coef RMS
CL 0.358 104.51
PC 0.337 105.92
SP -0.041* 114.34*
SL 0.042* 114.50*
Table 4: CR-Coef and RMS results for Character Length
(CL), Polar word Count (PC), Stop word Percent (SP) and av-
erage Sentence Length (SL). Best performance is highlighted
in bold. ? marks the results significantly worse than the best.
We use only the best performing instance charac-
teristic, the character length. The length of an ex-
ample has often been substituted for the annotation
cost (Kapoor et al, 2007; Tomanek et al, 2007).
We show in Table 5 that certain feature combina-
tions significantly outperform character length. The
combination of all three features (last row) performs
the best for both CRCoef and RMS, and this result
is significantly better than the character length (third
row). The combination of number of rationales and
nativeness (fourth row) also outperforms character
length significantly in CRCoef. This suggests that
the number of rationales we expect or require in a re-
view and the annotator characteristics are important
factors for annotation cost estimation and should be
considered in addition to the character length.
CL NR AN Const. CR-Coef RMS
-29.33 220.77 0.135? 123.93?
17.59 82.81 0.486 95.29
0.027 61.53 0.357? 104.51?
19.11 -40.78 153.21 0.55+ 96.04
0.028 32.79 120.18 0.397? 109.85?
0.02 15.15 17.57 0.553+ 90.27+
0.021 16.64 -41.84 88.09 0.626+ 88.44+
Table 5: CR-Coef and RMS results for seven feature com-
binations of Character Length (CL), Number of Rationales
(NR) and Annotator Nativeness (AN). The values in feature
and ?Const.? columns are weights and constant for the linear
regression model trained on all the data. The numbers in bold
are the results for the best feature combination. ? marks the
results significantly worse than the best. + marks the results
significantly better than CL.
The impact of the nativeness feature is somewhat
mixed. Adding the nativeness feature always im-
proves the correlation and for RMS, it helps when
added to the combined feature (CL+NR) but not oth-
erwise. Although this improvement with addition
of the nativeness feature is not significant, it does
suggest that annotator characteristics might be im-
portant to consider. To investigate this further, we
evaluated our assumption that native speakers take
less time to annotate. For each set, we compared the
average annotation times (averaged over examples)
against the nativeness values. For all sets, annotators
with nativeness value of 3 always took less time on
average than those with nativeness value of 2 or 1.
Between 2 and 1, there were no reliable differences.
Sometimes annotators with value of 1 took less time
than annotators with value of 2. Also, for group 2
which had all annotators with nativeness value of 1,
we observed a poor correlation between annotators
(Table 1). This suggest two things: 1) our assign-
ment of nativeness value may not be accurate and
we need other ways of quantifying nativeness, 2)
there are other annotator characteristics we should
take into consideration.
PC CL NR AN Const. CR RMS
0.027 61.53 0.358ab 104.5x
2.2 74.20 0.337a 105.9x
0.7 0.019 60.89 0.355b 104.9x
0.028 -32.8 120.2 0.397ab 109.8x
2.3 -35.5 135.1 0.382a 111.1x
1.1 0.016 -34.3 121.8 0.395b 109.9x
0.02 15.1 17.57 0.553a 90.27x
1.5 15.1 32.02 0.542a 91.65x
0.0 0.02 15.1 17.57 0.554a 90.40x
0.021 16.6 -41.8 88.09 0.626a 88.44x
1.6 16.5 -43.5 102.8 0.614a 90.42y
0.0 0.021 16.6 -41.8 88.09 0.626a 88.78x
Table 6: Each block of 3 rows in this table compares the
performance of Character Length (CL) and Polar word Count
(PC) in combination with Number of Rationales (NR) and An-
notator Nativeness (AN) features. The values in feature and
?Const.? columns are weights and constant for the linear re-
gression model trained on all the data. Best performance is
highlighted in bold. Results in a block not connected by same
letter are significantly different.
Polar word Count and Character Length: As we
saw in Table 4, the difference between character
length and polar word count is not significant. We
further compare these two instance characteristics
in the presence of the annotator and annotation task
characteristics. Our goal is to ascertain whether
character length performs better than polar word
count, or vice versa, and whether this difference is
significant. We also evaluate whether using both
performs better than using any one of them alone.
The results presented in Table 6 help us answer these
questions. For all feature combinations character
length, with and without polar word count, performs
23
better than polar word count, but this difference is
not significant except in three cases. These results
suggests that polar word count can be used as an al-
ternative to character length in annotation cost esti-
mation.
3.4.2 Interaction between Features and
Annotation Time
As a post-experiment analysis, we studied the
interaction between the features we used and an-
notation time, and the interaction among features
themselves. Table 7 reports the pairwise correlation
(Pearson, 1895) for these variables, calculated over
all 125 reviews. As can be seen, all features have
significant correlation with annotation time except
stop words percentage and average sentence length.
Note that number of rationales has higher correla-
tion with annotation time (R = 0.529) than charac-
ter length (R = 0.417). This suggests that number
of rationales may have more influence than charac-
ter length on annotation time, and a low correlation
between number of rationales and character length
(R = 0.238) indicates that it might not be the case
that longer documents necessarily contain more ra-
tionales. Annotating rationales requires cognitive
effort of identifying the right span and manual ef-
fort to highlight and add an annotation, and hence
more rationales implies more annotation time. We
also found some examples in our data where docu-
ments with substantially different lengths but same
number of rationales took a similar time to anno-
tate. One possible explanation for this observation is
user?s annotation strategy. If the annotator chooses
to skim through the remaining text when enough ra-
tionales are found, two examples with same number
of rationales but different lengths might take similar
time. We plan to investigate the effect of annotator?s
strategy on annotation time in the future.
A negative correlation of nativeness with annota-
tion time (R = ?0.219) is expected, since native
speakers (AN = 3) are expected to take less anno-
tation time than non-native speakers (AN = {2, 1}),
although this correlation is low. A low correla-
tion between number of rationales and nativeness
(R = 0.149) suggests that number of rationales
a user adds may not be influenced much by their
nativeness value. A not significant low correlation
(R = ?0.06) between character length and native-
AT CL NR AN PC SP SL
AT 1
CL 0.42 1
NR 0.53 0.24 1
AN -0.22 0.06 0.15 1
PC 0.4 0.89 0.28 0.11 1
SP 0.03 0.06 0.14 0.03 0.04 1
SL 0.08 0.15 0.01 -0.01 0.14 -0.13 1
Table 7: Correlation between Character Length (CL), Num-
ber of Rationales (NR), Annotator Nativeness (AN), Polar
word Count (PC), Stop word Percent (SP), average Sentence
Length (SL) and Annotation Time (AT), calculated over all
documents (125) and all annotators (20). Significant corre-
lations are highlighted in bold.
ness provides no evidence that reviews with different
lengths were distributed non-uniformly across anno-
tators with different nativeness.
The number of polar words in a document has a
similar correlation with annotation time as character
length (R = 0.4). There is also a strong correla-
tion between character length and polar word count
(R = 0.89). Since reviews are essentially people?s
opinions, we can expect longer documents to have
more polar words. This also explains why there is no
significant difference in performance for polar word
count and character length (Table 4). A more useful
feature may be the information about the number of
positive and negative polar words in a review, since a
review with both positive and negative opinions can
be difficult to classify as positive or negative. We
plan to explore these variations of the polar word
feature in the future. We also plan to investigate how
we can exploit this dependence between characteris-
tics for annotation cost estimation.
3.4.3 CRCoef Vs. RMS
We presented our results using correlation coef-
ficient and root mean squared error metrics. Ta-
ble 8 shows the ranking of the feature combinations
from better to worse for both these metrics and as
we can see, there is a difference in the order of fea-
ture combinations for the two metrics. Also, signif-
icance results differ in some cases for the two met-
rics. These differences suggest that features which
correlate well with the annotation times (higher CR-
Coef rank) can give an accurate ranking of examples
based on their annotation cost, but they may not be
as accurate in their absolute estimate for simulating
annotators and thus might have a lower RMS rank.
Thus, it is important to evaluate the user effort esti-
24
mator in terms of both these metrics so that the right
estimator can be chosen for a given objective.
Rank CR-Coef RMS
1 (CL+NR+AN) (CL+NR+AN)
2 (CL+NR) (CL+NR)
3 (NR+AN) (NR)
4 (NR) (NR+AN)
5 (CL+AN) (CL)
6 (CL) (CL+AN)
7 (AN) (AN)
Table 8: Ranking of feature combinations.
4 Towards a General Annotation Cost
Estimator
Our multi-annotator environment allows us to train
and test on data from different annotators by using
annotator characteristics as features in the annota-
tion cost estimation. A model trained on data from a
variety of annotators can be used for recommend-
ing examples to annotators not represented in our
training data but with similar characteristics. This
is important since we may not always know all our
annotators before building the model, and training
an estimator for each new annotator is costly. Also,
in active learning research, the goal is to evaluate
selective sampling approaches independently of the
annotator. Choosing annotators for supervised an-
notation cost estimation such that the within group
variance in annotator characteristics is high will give
us a more generic estimator and a stricter evaluation
criterion. Thus, we have a framework that has the
potential to be used to build a user-independent an-
notation cost estimator for a given task.
However, this framework is specific to the User
Interface (UI) used. A change in the user interface
might require recollecting the data from all the an-
notators and training a model on the new data. For
example, if annotating rationales was made signif-
icantly faster in a new UI design, it would have
a major impact on annotation cost. An alternative
would be to incorporate UI features in our model and
train it on several different UIs or modifications of
the same UI, which will allow us to use our trained
model with a new user interface or modifications of
the existing UIs, without having to recollect the data
and retrain the model. A few UI features that can be
used in our context are: adding a rationale annota-
tion, voting positive or negative, etc. The units for
expressing these features will be the low-level user
interface actions such as number of clicks, mouse
drags, etc. For example, in our task, adding a ra-
tionale annotation requires one mouse drag and two
clicks, and adding a vote requires one click. In a dif-
ferent user interface, adding a rationale annotation
might require just one mouse drag.
Using UI features raises a question of whether
they can replace the annotation task features; e.g.,
whether the UI feature for adding rationale anno-
tation can replace the number of rationales feature.
Our hypothesis is that number of rationales has more
influence on annotation time than just the manual ef-
fort of annotating them. It also requires the cognitive
effort of finding the rationale, deciding its span, etc.
We aim to explore incorporating UI features in our
annotation cost estimation model in the future.
5 Conclusion and Future Work
In this work we presented a detailed investigation of
annotation cost estimation for active learning with
multiple annotators. We motivated the task from two
perspectives: selecting examples to minimize anno-
tation cost and simulating annotators for evaluating
active learning approaches. We defined three cate-
gories of features based on instance, annotation task
and annotator characteristics. Our results show that
using a combination of features from all three cate-
gories performs better than any one of them alone.
Our analysis was limited to a small dataset. In the
future, we plan to collect a larger dataset for this task
and explore more features from each feature group.
With the multi-annotator annotation cost estima-
tor proposed, we also motivated the need for a gen-
eral estimator that can be used with new annotators
or user interfaces without having to retrain. We aim
to explore this direction in the future by extending
our model to incorporate user interface features. We
also plan to use the annotation cost model we devel-
oped in an active learning experiment.
Acknowledgments
We would like to thank Hideki Shima for his help
with the task setup and Jing Yang for helpful discus-
sions. We would also like to thank all the anony-
mous reviewers for their helpful comments.
25
References
Shilpa Arora and Eric Nyberg. 2009. Interactive An-
notation Learning with Indirect Feature Voting. In
Proceedings of NAACL-HLT 2009 (Student Research
Workshop).
Xavier Carreras and Llu?`s Ma?rquez. 2004. Intro-
duction to the CoNLL-2004 Shared Task: Seman-
tic Role Labeling. http://www.lsi.upc.edu/
?srlconll/st04/st04.html.
Gahgene Gweon, Carolyn Penstein Ros?e, Joerg Wittwer
and Matthias Nueckles. 2005. Supporting Efficient
and Reliable Content Analysis Using Automatic Text
Processing Technology. In proceedings of INTER-
ACT 2005: 1112-1115.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger and
Janes L. Cattoll. 2008. Return on Investment for Ac-
tive Learning. In proceedings of NIPS Workshop on
Cost Sensitive Learning.
Rebecca Hwa. 2000. Sample Selection for Statistical
Grammar Induction. In proceedings of joint SIGDAT
conference on Empirical Methods in NLP and Very
Large Corpora.
Ashish Kapoor, Eric Horvitz and Sumit Basu. 2007. Se-
lective supervision:Guiding supervised learning with
decision-theoretic active learning. In proceedings of
IJCAI, pages 877-882.
Ross D. King, Kenneth E. Whelan, Ffion M. Jones, Philip
G. K. Reiser, Christopher H. Bryant, Stephen H. Mug-
gleton, Douglas B. Kell and Stephen G. Oliver. 2004.
Functional Genomics hypothesis generation and ex-
perimentation by a robot scientist. In proceedings of
Nature, 427(6971):247-52.
Trausti Kristjansson, Aron Culotta, Paul Viola and An-
drew Mccallum. 2004. Interactive Information Ex-
traction with Constrained Conditional Random Fields.
In proceedings of AAAI.
Karl Pearson. 1895. Correlation Coefficient. Royal So-
ciety Proceedings, 58, 214.
Eric Ringger, Marc Carmen, Robbie Haertel, Kevin
Seppi, Deryle Lonsdale, Peter McClanahan, Janes L.
Cattoll and Noel Ellison. 2008. Assessing the Costs of
Machine-Assisted Corpus Annotation through a User
Study. In proceedings of LREC.
Burr Settles, Mark Craven and Lewis Friedland. 2008.
Active Learning with Real Annotation Costs. In pro-
ceedings of NIPS Workshop on Cost Sensitive Learn-
ing.
Alex J. Smola and Bernhard Scholkopf 1998. A Tutorial
on Support Vector Regression. NeuroCOLT2 Techni-
cal Report Series - NC2-TR-1998-030.
Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007.
An approach to text corpus construction which cuts an-
notation costs and maintains reusability of annotated
data. In proceedings of EMNLP-CoNLL, pp. 486-
495.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In proceedings of
HLT/EMNLP, Vancouver, Canada.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. 2nd Edi-
tion, Morgan Kaufmann, San Francisco.
Omar Zaidan, Jason Eisner and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Proceedings of
NAACL-HLT, pp. 260-267, Rochester, NY.
26
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 131?139,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sentiment Classification using Automatically Extracted Subgraph Features
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose? and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{shilpaa, emayfiel, cprose, ehn}@cs.cmu.edu
Abstract
In this work, we propose a novel representa-
tion of text based on patterns derived from lin-
guistic annotation graphs. We use a subgraph
mining algorithm to automatically derive fea-
tures as frequent subgraphs from the annota-
tion graph. This process generates a very large
number of features, many of which are highly
correlated. We propose a genetic program-
ming based approach to feature construction
which creates a fixed number of strong classi-
fication predictors from these subgraphs. We
evaluate the benefit gained from evolved struc-
tured features, when used in addition to the
bag-of-words features, for a sentiment classi-
fication task.
1 Introduction
In recent years, the topic of sentiment analysis has
been one of the more popular directions in the field
of language technologies. Recent work in super-
vised sentiment analysis has focused on innovative
approaches to feature creation, with the greatest im-
provements in performance with features that in-
sightfully capture the essence of the linguistic con-
structions used to express sentiment, e.g. (Wilson et
al., 2004), (Joshi and Rose?, 2009)
In this spirit, we present a novel approach that
leverages subgraphs automatically extracted from
linguistic annotation graphs using efficient subgraph
mining algorithms (Yan and Han, 2002). The diffi-
culty with automatically deriving complex features
comes with the increased feature space size. Many
of these features are highly correlated and do not
provide any new information to the model. For ex-
ample, a feature of type unigram POS (e.g. ?cam-
era NN?) doesn?t provide any additional informa-
tion beyond the unigram feature (e.g. ?camera?),
for words that are often used with the same part of
speech. However, alongside several redundant fea-
tures, there are also features that provide new infor-
mation. It is these features that we aim to capture.
In this work, we propose an evolutionary ap-
proach that constructs complex features from sub-
graphs extracted from an annotation graph. A con-
stant number of these features are added to the un-
igram feature space, adding much of the represen-
tational benefits without the computational cost of a
drastic increase in feature space size.
In the remainder of the paper, we review prior
work on features commonly used for sentiment anal-
ysis. We then describe the annotation graph rep-
resentation proposed by Arora and Nyberg (2009).
Following this, we describe the frequent subgraph
mining algorithm proposed in Yan and Han (2002),
and used in this work to extract frequent subgraphs
from the annotation graphs. We then introduce our
novel feature evolution approach, and discuss our
experimental setup and results. Subgraph features
combined with the feature evolution approach gives
promising results, with an improvement in perfor-
mance over the baseline.
2 Related Work
Some of the recent work in sentiment analysis has
shown that structured features (features that capture
syntactic patterns in text), such as n-grams, depen-
dency relations, etc., improve performance beyond
131
the bag of words approach. Arora et al (2009) show
that deep syntactic scope features constructed from
transitive closure of dependency relations give sig-
nificant improvement for identifying types of claims
in product reviews. Gamon (2004) found that using
deep linguistic features derived from phrase struc-
ture trees and part of speech annotations yields sig-
nificant improvements on the task of predicting sat-
isfaction ratings in customer feedback data. Wilson
et al (2004) use syntactic clues derived from depen-
dency parse tree as features for predicting the inten-
sity of opinion phrases1.
Structured features that capture linguistic patterns
are often hand crafted by domain experts (Wilson
et al, 2005) after careful examination of the data.
Thus, they do not always generalize well across
datasets and domains. This also requires a signif-
icant amount of time and resources. By automati-
cally deriving structured features, we might be able
to learn new annotations faster.
Matsumoto et al (2005) propose an approach that
uses frequent sub-sequence and sub-tree mining ap-
proaches (Asai et al, 2002; Pei et al, 2004) to derive
structured features such as word sub-sequences and
dependency sub-trees. They show that these features
outperform bag-of-words features for a sentiment
classification task and achieve the best performance
to date on a commonly-used movie review dataset.
Their approach presents an automatic procedure for
deriving features that capture long distance depen-
dencies without much expert intervention.
However, their approach is limited to sequences
or tree annotations. Often, features that combine
several annotations capture interesting characteris-
tics of text. For example, Wilson et al (2004), Ga-
mon (2004) and Joshi and Rose? (2009) show that
a combination of dependency relations and part of
speech annotations boosts performance. The anno-
tation graph representation proposed by Arora and
Nyberg (2009) is a formalism for representing sev-
eral linguistic annotations together on text. With an
annotation graph representation, instances are rep-
resented as graphs from which frequent subgraph
patterns may be extracted and used as features for
learning new annotations.
1Although, in this work we are classifying sentences and not
phrases, similar clues may be used for sentiment classification
in sentences as well
In this work, we use an efficient frequent sub-
graph mining algorithm (gSpan) (Yan and Han,
2002) to extract frequent subgraphs from a linguis-
tic annotation graph (Arora and Nyberg, 2009). An
annotation graph is a general representation for ar-
bitrary linguistic annotations. The annotation graph
and subgraph mining algorithm provide us a quick
way to test several alternative linguistic representa-
tions of text. In the next section, we present a formal
definition of the annotation graph and a motivating
example for subgraph features.
3 Annotation Graph Representation and
Feature Subgraphs
Arora and Nyberg (2009) define the annotation
graph as a quadruple: G = (N,E,?, ?), where
N is the set of nodes, E is the set of edges, s.t.
E ? N ? N , and ? = ?N ? ?E is the set of la-
bels for nodes and edges. ? : N ? E ? ? is the
labeling function for nodes and edges. Examples of
node labels (?N ) are tokens (unigrams) and annota-
tions such as part of speech, polarity etc. Examples
of edge labels (?E) are leftOf, dependency type etc.
The leftOf relation is defined between two adjacent
nodes. The dependency type relation is defined be-
tween a head word and its modifier.
Annotations may be represented in an annotation
graph in several ways. For example, a dependency
triple annotation ?good amod movie?, may be repre-
sented as a d amod relation between the head word
?movie? and its modifier ?good?, or as a node d amod
with edges ParentOfGov and ParentOfDep to the
head and the modifier words. An example of an an-
notation graph is shown in Figure 1.
The instance in Figure 1 describes a movie review
comment, ?interesting, but not compelling.?. The
words ?interesting? and ?compelling? both have pos-
itive prior polarity, however, the phrase expresses
negative sentiment towards the movie. Heuristics for
special handling of negation have been proposed in
the literature. For example, Pang et al (2002) ap-
pend every word following a negation, until a punc-
tuation, with a ?NOT? . Applying a similar technique
to our example gives us two sentiment bearing fea-
tures, one positive (?interesting?) and one negative
(?NOT-compelling?), and the model may not be as
sure about the predicted label, since there is both
132
positive and negative sentiment present.
In Figure 2, we show three discriminating sub-
graph features derived from the annotation graph in
Figure 1. These subgraph features capture the nega-
tive sentiment in our example phrase. The first fea-
ture in 2(a) captures the pattern using dependency
relations between words. A different review com-
ment may use the same linguistic construction but
with a different pair of words, for example ?a pretty
good, but not excellent story.? This is the same lin-
guistic pattern but with different words the model
may not have seen before, and hence may not clas-
sify this instance correctly. This suggests that the
feature in 2(a) may be too specific.
In order to mine general features that capture the
rhetorical structure of language, we may add prior
polarity annotations to the annotation graph, us-
ing a lexicon such as Wilson et al (2005). Fig-
ure 2(b) shows the subgraph in 2(a) with polar-
ity annotations. If we want to generalize the pat-
tern in 2(a) to any positive words, we may use the
feature subgraph in Figure 2(c) with X wild cards
on words that are polar or negating. This feature
subgraph captures the negative sentiment in both
phrases ?interesting, but not compelling.? and ?a
pretty good, but not excellent story.?. Similar gener-
alization using wild cards on words may be applied
with other annotations such as part of speech anno-
tations as well. By choosing where to put the wild
card, we can get features similar to, but more pow-
erful than, the dependency back-off features in Joshi
and Rose? (2009).
 
U_interesting U_, U_but U_not U_compelling U_. 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
posQ 
P_VBN 
posQ 
P_, 
posQ 
P_CC 
posQ 
P_RB 
posQ 
P_JJ 
posQ 
P_. 
Figure 1: Annotation graph for sentence ?interesting, but not
compelling.? . Prefixes: ?U? for unigrams (tokens), ?L? for po-
larity, ?D? for dependency relation and ?P? for part of speech.
Edges with no label encode the ?leftOf? relation between words.
4 Subgraph Mining Algorithms
In the previous section, we demonstrated that sub-
graphs from an annotation graph can be used to iden-
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
(a)
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(b)
 
X X X 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(c)
Figure 2: Subgraph features from the annotation graph in
Figure 1
tify the rhetorical structure used to express senti-
ment. The subgraph patterns that represent general
linguistic structure will be more frequent than sur-
face level patterns. Hence, we use a frequent sub-
graph mining algorithm to find frequent subgraph
patterns, from which we construct features to use in
the supervised learning algorithm.
The goal in frequent subgraph mining is to find
frequent subgraphs in a collection of graphs. A
graph G? is a subgraph of another graph G if there
exists a subgraph isomorphism2 from G? to G, de-
noted by G? ? G.
Earlier approaches in frequent subgraph mining
(Inokuchi et al, 2000; Kuramochi and Karypis,
2001) used a two-step approach of first generating
the candidate subgraphs and then testing their fre-
quency in the graph database. The second step in-
volves a subgraph isomorphism test, which is NP-
complete. Although efficient isomorphism testing
algorithms have been developed making it practical
to use, with lots of candidate subgraphs to test, it can
2http://en.wikipedia.org/wiki/Subgraph_
isomorphism_problem
133
still be very expensive for real applications.
gSpan (Yan and Han, 2002) uses an alternative
pattern growth based approach to frequent subgraph
mining, which extends graphs from a single sub-
graph directly, without candidate generation. For
each discovered subgraph G, new edges are added
recursively until all frequent supergraphs of G have
been discovered. gSpan uses a depth first search tree
(DFS) and restricts edge extension to only vertices
on the rightmost path. However, there can be multi-
ple DFS trees for a graph. gSpan introduces a set of
rules to select one of them as representative. Each
graph is represented by its unique canonical DFS
code, and the codes for two graphs are equivalent if
the graphs are isomorphic. This reduces the compu-
tational cost of the subgraph mining algorithm sub-
stantially, making gSpan orders of magnitude faster
than other subgraph mining algorithms. With sev-
eral implementations available 3, gSpan has been
commonly used for mining frequent subgraph pat-
terns (Kudo et al, 2004; Deshpande et al, 2005). In
this work, we use gSpan to mine frequent subgraphs
from the annotation graph.
5 Feature Construction using Genetic
Programming
A challenge to overcome when adding expressive-
ness to the feature space for any text classification
problem is the rapid increase in the feature space
size. Among this large set of new features, most
are not predictive or are very weak predictors, and
only a few carry novel information that improves
classification performance. Because of this, adding
more complex features often gives no improvement
or even worsens performance as the feature space?s
signal is drowned out by noise.
Riloff et al (2006) propose a feature subsump-
tion approach to address this issue. They define a
hierarchy for features based on the information they
represent. A complex feature is only added if its
discriminative power is a delta above the discrimi-
native power of all its simpler forms. In this work,
we use a Genetic Programming (Koza, 1992) based
approach which evaluates interactions between fea-
3http://www.cs.ucsb.edu/?xyan/software/
gSpan.htm, http://www.kyb.mpg.de/bs/people/
nowozin/gboost/
tures and evolves complex features from them. The
advantage of the genetic programing based approach
over feature subsumption is that it allows us to eval-
uate a feature using multiple criteria. We show that
this approach performs better than feature subsump-
tion.
A lot of work has considered this genetic pro-
gramming problem (Smith and Bull, 2005). The
most similar approaches to ours are taken by Kraw-
iec (2002) and Otero et al (2002), both of which use
genetic programming to build tree feature represen-
tations. None of this work was applied to a language
processing task, though there has been some sim-
ilar work to ours in that community, most notably
(Hirsch et al, 2007), which built search queries for
topic classification of documents. Our prior work
(Mayfield and Rose?, 2010) introduced a new feature
construction method and was effective when using
unigram features; here we extend our approach to
feature spaces which are even larger and thus more
problematic.
The Genetic Programming (GP) paradigm is most
advantageous when applied to problems where there
is not a correct answer to a problem, but instead
there is a gradient of partial solutions which incre-
mentally improve in quality. Potential solutions are
represented as trees consisting of functions (non-leaf
nodes in the tree, which perform an action given
their child nodes as input) and terminals (leaf nodes
in the tree, often variables or constants in an equa-
tion). The tree (an individual) can then be inter-
preted as a program to be executed, and the output
of that program can be measured for fitness (a mea-
surement of the program?s quality). High-fitness in-
dividuals are selected for reproduction into a new
generation of candidate individuals through a breed-
ing process, where parts of each parent are combined
to form a new individual.
We apply this design to a language processing
task at the stage of feature construction - given many
weakly predictive features, we would like to com-
bine them in a way which produces a better feature.
For our functions we use boolean statements AND
and XOR, while our terminals are selected randomly
from the set of all unigrams and our new, extracted
subgraph features. Each leaf?s value, when applied
to a single sentence, is equal to 1 if that subgraph is
present in the sentence, and 0 if the subgraph is not
134
present.
The tree in Figure 3 is a simplified example of our
evolved features. It combines three features, a uni-
gram feature ?too? (centre node) and two subgraph
features: 1) the subgraph in the leftmost node oc-
curs in collocations containing ?more than? (e.g.,
?nothing more than? or ?little more than?), 2) the
subgraph in the rightmost node occurs in negative
phrases such as ?opportunism at its most glaring?
(JJS is a superlative adjective and PRP$ is a pos-
sessive pronoun). A single feature combining these
weak indicators can be more predictive than any part
alone.
!"#$
!"#$
%&'(($
%&)(*+$
,&-*+-&'./0$
%&1'2$
3"4&3#35$
3"4&664$
,&-(22$
Figure 3: A tree constructed using subgraph features and GP
(Simplified for illustrative purposes)
In the rest of this section, we first describe the
feature construction process using genetic program-
ming. We then discuss how fitness of an individual
is measured for our classification task.
5.1 Feature Construction Process
We divide our data into two sets, training and test.
We again divide our training data in half, and train
our GP features on only one half of this data4 This is
to avoid overfitting the final SVM model to the GP
features. In a single GP run, we produce one feature
to match each class value. For a sentiment classifica-
tion task, a feature is evolved to be predictive of the
positive instances, and another feature is evolved to
be predictive of the negative documents. We repeat
this procedure a total of 15 times (using different
seeds for random selection of features), producing
a total of 30 new features to be added to the feature
space.
4For genetic programming we used the ECJ toolkit
(http://cs.gmu.edu/?eclab/projects/ecj/).
5.2 Defining Fitness
Our definition of fitness is based on the concepts
of precision and recall, borrowed from informa-
tion retrieval. We define our set of documents
as being comprised of a set of positive documents
P0, P1, P2, ...Pu and a set of negative documents
N0, N1, N2, ...Nv. For a given individual I and doc-
ument D, we define hit(I,D) to equal 1 if the state-
ment I is true of that document and 0 otherwise. Pre-
cision and recall of an individual feature for predict-
ing positive documents5 is then defined as follows:
Prec(I) =
u
?
i=0
hit(I, Pi)
u
?
i=0
hit(I, Pi) +
v
?
i=0
hit(I,Ni)
(1)
Rec(I) =
u
?
i=0
hit(I, Pi)
u
(2)
We then weight these values to give significantly
more importance to precision, using the F? measure,
which gives the harmonic mean between precision
and recall:
F?(I) =
(1 + ?2)? (Prec(I)?Rec(I))
(?2 ? Prec(I)) +Rec(I) (3)
In addition to this fitness function, we add two
penalties to the equation. The first penalty applies to
prevent trees from becoming overly complex. One
option to ensure that features remain moderately
simple is to simply have a maximum depth beyond
which trees cannot grow. Following the work of
Otero et al (2002), we penalize trees based on the
number of nodes they contain. This discourages
bloat, i.e. sections of trees which do not contribute to
overall accuracy. This penalty, known as parsimony
pressure, is labeled PP in our fitness function.
The second penalty is based on the correlation be-
tween the feature being constructed, and the sub-
graphs and unigrams which appear as nodes within
that individual. Without this penalty, a feature may
5Negative precision and recall are defined identically, with
obvious adjustments to test for negative documents instead of
positive.
135
often be redundant, taking much more complexity
to represent the same information that is captured
with a simple unigram. We measure correlation us-
ing Pearson?s product moment, defined for two vec-
tors X , Y as:
?x,y =
E[(X ? ?X)(Y ? ?Y )]
?X?Y
(4)
This results in a value from 1 (for perfect align-
ment) to -1 (for inverse alignment). We assign a
penalty for any correlation past a cutoff. This func-
tion is labeled CC (correlation constraint) in our fit-
ness function.
Our fitness function therefore is:
Fitness = F 1
8
+ PP + CC (5)
6 Experiments and Results
We evaluate our approach on a sentiment classifi-
cation task, where the goal is to classify a movie
review sentence as expressing positive or negative
sentiment towards the movie.
6.1 Data and Experimental Setup
Data: The dataset consists of snippets from Rot-
ten Tomatoes (Pang and Lee, 2005) 6. It consists
of 10662 snippets/sentences total with equal num-
ber positive and negative sentences (5331 each).
This dataset was created and used by Pang and Lee
(2005) to train a classifier for identifying positive
sentences in a full length review. We use the first
8000 (4000 positive, 4000 negative) sentences as
training data and evaluate on remaining 2662 (1331
positive, 1331 negative) sentences. We added part
of speech and dependency triple annotations to this
data using the Stanford parser (Klein and Manning,
2003).
Annotation Graph: For the annotation graph rep-
resentation, we used Unigrams (U), Part of Speech
(P) and Dependency Relation Type (D) as labels for
the nodes, and ParentOfGov and ParentOfDep as la-
bels for the edges. For a dependency triple such as
?amod good movie?, five nodes are added to the an-
notation graph as shown in Figure 4(a). ParentOf-
Gov and ParentOfDep edges are added from the
6http://www.cs.cornell.edu/people/pabo/
movie-review-data/rt-polaritydata.tar.gz
D_amod
U_good
P_JJ
P_NN
U_movie
ParentofGov
ParentofGovParentofDep
ParentofDep
(a)
D_amod
U_good
P_NN
ParentofGov
ParentofDep
(b)
D_amod
X
P_JJ
P_NN
X
posQ
ParentofGov
ParentofDep
posQ
(c)
Figure 4: Annotation graph and a feature subgraph for
dependency triple annotation ?amod good camera?. (c)
shows an alternative representation with wild cards
dependency relation node D amod to the unigram
nodes U good and U movie. These edges are also
added for the part of speech nodes that correspond
to the two unigrams in the dependency relation, as
shown in Figure 4(a). This allows the algorithm to
find general patterns, based on a dependency rela-
tion between two part of speech nodes, two unigram
nodes or a combination of the two. For example,
a subgraph in Figure 4(b) captures a general pat-
tern where good modifies a noun. This feature ex-
ists in ?amod good movie?, ?amod good camera?
and other similar dependency triples. This feature is
similar to the the dependency back-off features pro-
posed in Joshi and Rose? (2009).
The extra edges are an alternative to putting wild
cards on words, as proposed in section 3. On the
other hand, putting a wild card on every word in
the annotation graph for our example (Figure 4(c)),
will only give features based on dependency rela-
tions between part of speech annotations. Thus, the
wild card based approach is more restrictive than
136
adding more edges. However, with lots of edges, the
complexity of the subgraph mining algorithm and
the number of subgraph features increases tremen-
dously.
Classifier: For our experiments we use Support
Vector Machines (SVM) with a linear kernel. We
use the SVM-light7 implementation of SVM with
default settings.
Parameters: The gSpan algorithm requires setting
the minimum support threshold (minsup) for the
subgraph patterns to extract. Support for a subgraph
is the number of graphs in the dataset that contain
the subgraph. We experimented with several values
for minimum support and minsup = 2 gave us the
best performance.
For Genetic Programming, we used the same pa-
rameter settings as described in Mayfield and Rose?
(2010), which were tuned on a different dataset8
than one used in this work, but it is from the same
movie review domain. We also consider one alter-
ation to these settings. As we are introducing many
new and highly correlated features to our feature
space through subgraphs, we believe that a stricter
constraint must be placed on correlation between
features. To accomplish this, we can set our correla-
tion penalty cutoff to 0.3, lower than the 0.5 cutoff
used in prior work. Results for both settings are re-
ported.
Baselines: To the best of our knowledge, there is
no supervised machine learning result published on
this dataset. We compare our results with the fol-
lowing baselines:
? Unigram-only Baseline: In sentiment analysis,
unigram-only features have been a strong base-
line (Pang et al, 2002; Pang and Lee, 2004).
We only use unigrams that occur in at least
two sentences of the training data same as Mat-
sumoto et al (2005). We also filter out stop
words using a small stop word list9.
? ?2 Baseline: For our training data, after filter-
ing infrequent unigrams and stop words, we get
7http://svmlight.joachims.org/
8Full movie review data by Pang et al (2002)
9http://nlp.stanford.edu/
IR-book/html/htmledition/
dropping-common-terms-stop-words-1.html
(with one modification: removed ?will?, added ?this?)
8424 features. Adding subgraph features in-
creases the total number of features to 44, 161,
a factor of 5 increase in size. Feature selec-
tion can be used to reduce this size by select-
ing the most discriminative features. ?2 feature
selection (Manning et al, 2008) is commonly
used in the literature. We compare two methods
of feature selection with ?2, one which rejects
features if their ?2 score is not significant at the
0.05 level, and one that reduces the number of
features to match the size of our feature space
with GP.
? Feature Subsumption (FS): Following the idea
in Riloff et al (2006), a complex feature
C is discarded if IG(S) ? IG(C) ? ?,
where IG is Information Gain and S is
a simple feature that representationally sub-
sumes C, i.e. the text spans that match S
are a superset of the text spans that match
C. In our work, complex features are sub-
graph features and simple features are uni-
gram features contained in them. For example,
(D amod) Edge ParentOfDep (U bad) is
a complex feature for which U bad is a sim-
ple feature. We tried same values for ? ?
{0.002, 0.001, 0.0005}, as suggested in Riloff
et al (2006). Since all values gave us same
number of features, we only report a single re-
sult for feature subsumption.
? Correlation (Corr): As mentioned earlier,
some of the subgraph features are highly corre-
lated with unigram features and do not provide
new knowledge. A correlation based filter for
subgraph features can be used to discard a com-
plex feature C if its absolute correlation with its
simpler feature (unigram feature) is more than
a certain threshold. We use the same threshold
as used in the GP criterion, but as a hard filter
instead of a penalty.
6.2 Results and Discussion
In Table 1, we present our results. As can be
seen, subgraph features when added to the unigrams,
without any feature selection, decrease the perfor-
mance. ?2 feature selection with fixed feature space
size provides a very small gain over unigrams. All
other feature selection approaches perform worse
137
Settings #Features Acc. ?
Uni 8424 75.66 -
Uni + Sub 44161 75.28 -0.38
Uni + Sub, ?2 sig. 3407 74.68 -0.98
Uni + Sub, ?2 size 8454 75.77 +0.11
Uni + Sub, (FS) 18234 75.47 -0.19
Uni + Sub, (Corr) 18980 75.24 -0.42
Uni + GP (U) ? 8454 76.18 +0.52
Uni + GP (U+S) ? 8454 76.48 +0.82
Uni + GP (U+S) ? 8454 76.93 +1.27
Table 1: Experimental results for feature spaces with un-
igrams, with and without subgraph features. Feature se-
lection with 1) fixed significance level (?2 sig.), 2) fixed
feature space size (?2 size), 3) Feature Subsumption (FS)
and 4) Correlation based feature filtering (Corr)). GP fea-
tures for unigrams only {GP(U)}, or both unigrams and
subgraph features {GP(U+S)}. Both the settings from
Mayfield and Rose? (2010) (?) and more stringent correla-
tion constraint (?) are reported. #Features is the num-
ber of features in the training data. Acc is the accuracy
and ? is the difference from unigram only baseline. Best
performing feature configuration is highlighted in bold.
than the unigram-only approach. With GP, we ob-
serve a marginally significant gain (p < 0.1) in per-
formance over unigrams, calculated using one-way
ANOVA. Benefit from GP is more when subgraph
features are used in addition to the unigram features,
for constructing more complex pattern features. Ad-
ditionally, our performance is improved when we
constrain the correlation more severely than in previ-
ously published research, supporting our hypothesis
that this is a helpful way to respond to the problem
of redundancy in subgraph features.
A problem that we see with ?2 feature selection is
that several top ranked features may be highly cor-
related. For example, the top 5 features based on ?2
score are shown in Table 2; it is immediately obvi-
ous that the features are highly redundant.
With GP based feature construction, we can con-
sider this relationship between features, and con-
struct new features as a combination of selected un-
igram and subgraph features. With the correlation
criterion in the evolution process, we are able to
build combined features that provide new informa-
tion compared to unigrams.
The results we present are for the best perform-
(D advmod) Edge ParentOfDep (U too)
U too
U bad
U movie
(D amod) Edge ParentOfDep (U bad)
Table 2: Top features based on ?2 score
ing parameter configuration that we tested, after a
series of experiments. We realize that this places us
in danger of overfitting to the particulars of this data
set, however, the data set is large enough to partially
mitigate this concern.
7 Conclusion and Future Work
We have shown that there is additional information
to be gained from text beyond words, and demon-
strated two methods for increasing this information -
a subgraph mining approach that finds common syn-
tactic patterns that capture sentiment-bearing rhetor-
ical structure in text, and a feature construction
technique that uses genetic programming to com-
bine these more complex features without the redun-
dancy, increasing the size of the feature space only
by a fixed amount. The increase in performance that
we see is small but consistent.
In the future, we would like to extend this work to
other datasets and other problems within the field of
sentiment analysis. With the availability of several
off-the-shelf linguistic annotators, we may add more
linguistic annotations to the annotation graph and
richer subgraph features may be discovered. There
is also additional refinement that can be performed
on our genetic programming fitness function, which
is expected to improve the quality of our features.
Acknowledgments
This work was funded in part by the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and in part by NSF grant DRL-0835426.
We would like to thank Dr. Xifeng Yan and Marisa
Thoma for the gSpan code.
References
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?. 2009.
Identifying Types of Claims in Online Customer Re-
138
views. Proceedings of the HLT/NAACL.
Shilpa Arora and Eric Nyberg. 2009. Interactive Anno-
tation Learning with Indirect Feature Voting. Proceed-
ings of the HLT/NAACL (Student Research Work-
shop).
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroshi
Sakamoto and Setsuo Arikawa. 2002. Efficient sub-
structure discovery from large semi-structured data.
Proceedings of SIAM Int. Conf. on Data Mining
(SDM).
Mukund Deshpande , Michihiro Kuramochi , Nikil Wale
and George Karypis. 2005. Frequent Substructure-
Based Approaches for Classifying Chemical Com-
pounds. IEEE Transactions on Knowledge and Data
Engineering.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vec-
tors, and the role of linguistic analysis, Proceedings
of COLING.
Laurence Hirsch, Robin Hirsch and Masoud Saeedi.
2007. Evolving Lucene Search Queries for Text Clas-
sification. Proceedings of the Genetic and Evolution-
ary Computation Conference.
Mahesh Joshi and Carolyn P. Rose?. 2009. Generalizing
Dependency Features for Opinion Mining. Proceed-
ings of the ACL-IJCNLP Conference (Short Papers).
Akihiro Inokuchi, Takashi Washio and Hiroshi Motoda.
2000. An Apriori-based Algorithm for Mining Fre-
quent Substructures from Graph Data. Proceedings
of PKDD.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the main con-
ference of the ACL.
John Koza. 1992. Genetic Programming: On the Pro-
gramming of Computers by Means of Natural Selec-
tion. MIT Press.
Krzysztof Krawiec. 2002. Genetic programming-based
construction of features for machine learning and
knowledge discovery tasks. Genetic Programming and
Evolvable Machines.
Taku Kudo, Eisaku Maeda and Yuji Matsumoto. 2004.
An Application of Boosting to Graph Classification.
Proceedings of NIPS.
Michihiro Kuramochi and George Karypis. 2002. Fre-
quent Subgraph Discovery. Proceedings of ICDM.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Proceedings of PAKDD.
Shotaro Matsumoto, Hiroya Takamura and Manabu Oku-
mura. 2005. Sentiment Classification Using Word
Sub-sequences and Dependency Sub-trees. Proceed-
ings of PAKDD.
Elijah Mayfield and Carolyn Penstein-Rose?. 2010.
Using Feature Construction to Avoid Large Feature
Spaces in Text Classification. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Fernando Otero, Monique Silva, Alex Freitas and Julio
Nievola. 2002. Genetic Programming for Attribute
Construction in Data Mining. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classication using Ma-
chine Learning Techniques. Proceedings of EMNLP.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of the
main conference of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. Proceedings of the main con-
ference of ACL.
Jian Pei, Jiawei Han, Behzad Mortazavi-asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal
and Mei-chun Hsu. 2004. Mining Sequential Pat-
terns by Pattern-Growth: The PrefixSpan Approach.
Proceedings of IEEE Transactions on Knowledge and
Data Engineering.
Ellen Riloff, Siddharth Patwardhan and Janyce Wiebe.
2006. Feature Subsumption for Opinion Analysis.
Proceedings of the EMNLP.
Matthew Smith and Larry Bull. 2005. Genetic Program-
ming with a Genetic Algorithm for Feature Construc-
tion and Selection. Genetic Programming and Evolv-
able Machines.
Theresa Wilson, Janyce Wiebe and Rebecca Hwa. 2004.
Just How Mad Are You? Finding Strong and Weak
Opinion Clauses. Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity
in Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP.
Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-
Based Substructure Pattern Mining. UIUC Techni-
cal Report, UIUCDCS-R-2002-2296 (shorter version
in ICDM?02).
139
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 106?114,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Assessing Benefit from Feature Feedback in Active Learning
for Text Classification
Shilpa Arora
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
shilpaa@cs.cmu.edu
Eric Nyberg
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
ehn@cs.cmu.edu
Abstract
Feature feedback is an alternative to instance
labeling when seeking supervision from hu-
man experts. Combination of instance and
feature feedback has been shown to reduce the
total annotation cost for supervised learning.
However, learning problems may not benefit
equally from feature feedback. It is well un-
derstood that the benefit from feature feed-
back reduces as the amount of training data
increases. We show that other characteristics
such as domain, instance granularity, feature
space, instance selection strategy and propor-
tion of relevant text, have a significant effect
on benefit from feature feedback. We estimate
the maximum benefit feature feedback may
provide; our estimate does not depend on how
the feedback is solicited and incorporated into
the model. We extend the complexity mea-
sures proposed in the literature and propose
some new ones to categorize learning prob-
lems, and find that they are strong indicators
of the benefit from feature feedback.
1 Introduction
Linear classifiers model the response as a weighted
linear combination of the features in input instances.
A supervised approach to learning a linear classifier
involves learning the weights for the features from
labeled data. A large number of labeled instances
may be needed to determine the class association of
the features and learn accurate weights for them. Al-
ternatively, the user may directly label the features.
For example, for a sentiment classification task, the
user may label features, such as words or phrases,
as expressing positive or negative sentiment. Prior
work (Raghavan et al, 2006; Zaidan et al, 2007)
has demonstrated that users are able to reliably pro-
vide useful feedback on features.
Direct feedback on a list of features (Raghavan et
al., 2006; Druck et al, 2008) is limited to simple fea-
tures like unigrams. However, unigrams are limited
in the linguistic phenomena they can capture. Struc-
tured features such as dependency relations, paths in
syntactic parse trees, etc., are often needed for learn-
ing the target concept (Pradhan et al, 2004; Joshi
and Rose?, 2009). It is not clear how direct feature
feedback can be extended straightforwardly to struc-
tured features, as they are difficult to present visu-
ally for feedback and may require special expertise
to comprehend. An alternative approach is to seek
indirect feedback on structured features (Arora and
Nyberg, 2009) by asking the user to highlight spans
of text, called rationales, that support the instance
label (Zaidan et al, 2007). For example, when clas-
sifying the sentiment of a movie review, rationales
are spans of text in the review that support the senti-
ment label for the review.
Assuming a fixed cost per unit of work, it might
be cheaper to ask the user to label a few features, i.e.
identify relevant features and their class association,
than to label several instances. Prior work (Ragha-
van et al, 2006; Druck et al, 2008; Druck et al,
2009; Zaidan et al, 2007) has shown that a combi-
nation of instance and feature labeling can be used
to reduce the total annotation cost required to learn
the target concept. However, the benefit from feature
feedback may vary across learning problems. If we
can estimate the benefit from feature feedback for a
106
given problem, we can minimize the total annotation
cost for achieving the desired performance by select-
ing the optimal annotation strategy (feature feedback
or not) at every stage in learning. In this paper, we
present the ground work for this research problem by
analyzing how benefit from feature feedback varies
across different learning problems and what charac-
teristics of a learning problem have a significant ef-
fect on benefit from feature feedback.
We define a learning problem (P = {D, G, F , L,
I , S}) as a tuple of the domain (D), instance gran-
ularity (G), feature representation (F ), labeled data
units (L), amount of irrelevant text (I) and instance
selection strategy (S).
With enough labeled data, we may not benefit
from feature feedback. Benefit from feature feed-
back also depends on the features used to represent
the instances. If the feature space is large, we may
need several labeled instances to identify the rele-
vant features, while relatively fewer labeled features
may help us quickly find these relevant features.
Apart from the feature space size, it also matters
what types of features are used. When hand crafted
features from a domain expert are used (Pradhan et
al., 2004) we expect to gain less from feature feed-
back as most of the features will be relevant. On
the other hand, when features are extracted automat-
ically as patterns in annotation graphs (Arora et al,
2010) feature feedback can help to identify relevant
features from the large feature space.
In active learning, instances to be labeled are se-
lectively sampled in each iteration. Benefit from fea-
ture feedback will depend on the instances that were
used to train the model in each iteration. In the case
of indirect feature feedback through rationales or di-
rect feature feedback in context, instances selected
will also determine what features receive feedback.
Hence, instance selection strategy should affect the
benefit from feature feedback.
In text classification, an instance may contain a
large amount of text, and even a simple unigram
representation will generate a lot of features. Often
only a part of the text is relevant for the classifica-
tion task. For example, in movie reviews, often the
reviewers talk about the plot and characters in addi-
tion to providing their opinion about the movie. Of-
ten this extra information is not relevant to the clas-
sification task and bloats the feature space without
adding many useful features. With feature feedback,
we hope to filter out some of this noise and improve
the model. Thus, the amount of irrelevant informa-
tion in the instance should play an important role in
determining the benefit from feature feedback. We
expect to see less of such noise when the text in-
stance is more concise. For example, a movie review
snippet (about a sentence length) tends to have less
irrelevant text than a full movie review (several sen-
tences). In addition to analyzing document instances
with varying amount of noise, we also compare the
benefit from feature feedback for problems with dif-
ferent granularity. Granularity for a learning prob-
lem is defined based on the average amount of text
in its instances.
Benefit from feature feedback will also depend on
how feedback is solicited from the user and how it
is incorporated back into the model. Independently
from these factors, we estimate the maximum pos-
sible benefit and analyze how it varies across prob-
lems. Next we describe measures proposed in the
literature and propose some new ones for categoriz-
ing learning problems. We then discuss our experi-
mental setup and analysis.
2 Related Work
There has been little work on categorizing learn-
ing problems and how benefit from feature feedback
varies with them. To the best of our knowledge
there is only one work in this area by Raghavan et
al. (2007). They categorize problems in terms of
their feature complexity. Feature complexity is de-
fined in terms of the minimum number of features
required to learn a good classifier (close to maxi-
mum performance). If the concept can be described
by a weighted combination of a few well-selected
features, it is considered to be of low complexity.
In this estimate of complexity, an assumption is
made that the best performance is achieved when
the learner has access to all available features and
not for any subset of the features. This is a reason-
able assumption for text classification problems with
robust learners like SVMs together with appropriate
regularization and sufficient training data.
Instead of evaluating all possible combinations of
features to determine the minimum number of fea-
tures required to achieve close to the best perfor-
107
mance, feature complexity is estimated using an in-
telligent ranking of the features. This ranking is
based on their discriminative ability determined us-
ing a large amount of labeled data (referred to as
oracle) and a feature selection criterion such as In-
formation Gain (Rijsbergen, 1979). It is intuitive
that the rate of learning, i.e., the rate at which per-
formance improves as we add more features to the
model, is also associated with problem complexity.
Raghavan et al (2007) define the feature learning
convergence profile (pfl) as the area under the fea-
ture learning curve (performance vs. number of fea-
tures used in training), given by:
pfl =
?log2N
t=1 F1(M, 2
t)
log2N ? F1(M,N)
(1)
where F1(M, 2t) is the F1 score on the test data
when using all M instances for training with top
ranked 2t features. The features are added at an ex-
ponentially increasing interval to emphasize the rel-
ative increase in feature space size. The three feature
complexity measures proposed by Raghavan et al
(2007) are the following: 1) Feature size complex-
ity (Nf ): Logarithm (base 2) of the number of fea-
tures needed to achieve 95% of the best performance
(when all instances are available), 2) Feature profile
complexity (Fpc), given by Fpc = 1 ? pfl, and 3)
Combined feature complexity (Cf ) , Cf = Fpc ? nf ,
incorporates both the learning profile and the num-
ber of features required.
In order to evaluate the benefit from feature feed-
back, Raghavan et al (2007) use their tandem learn-
ing approach of interleaving instance and feature
feedback (Raghavan et al, 2006), referred to as
interactive feature selection (ifs). The features
are labeled as ?relevant? (feature discriminates well
among the classes), or ?non-relevant/don?t know?.
The labeled features are incorporated into learning
by scaling the value of the relevant features by a con-
stant factor in all instances.
Raghavan et al (2007) measure the benefit from
feature feedback as the gain in the learning speed
with feature feedback. The learning speed measures
the rate of performance improvement with increas-
ing amount of supervision. It is defined in terms of
the convergence profile similar to feature learning
convergence profile in Equation 1, except in terms
of the number of labeled units instead of the num-
ber of features. A labeled unit is either a labeled
instance or an equivalent set of labeled features with
the same annotation time. The benefit from feature
feedback is then measured as the difference in the
convergence profile with interactive feature selec-
tion (pifs) and with labeled instances only (pal).
Raghavan et al (2007) analysed 9 corpora and
358 binary classification tasks. Most of these cor-
pora, such as Reuters (Lewis, 1995), 20-newsgroup
(Lang, 1995), etc., have topic-based category la-
bels. For all classification tasks, they used simple
and fixed feature space containing only unigram fea-
tures (n-gram features were added where it seemed
to improve performance). They observed a negative
correlation (r = ?0.65) between the benefit from
feature feedback and combined feature complexity
(Cf ), i.e., feature feedback accelerates active learn-
ing by an amount that is inversely proportional to
the feature complexity of the problem. If a concept
can be expressed using a few well-selected features
from a large feature space, we stand to benefit from
feature feedback as few labeled features can provide
this information. On the other hand, if learning a
concept requires all or most of the features in the
feature space, there is little knowledge that feature
feedback can provide.
3 Estimating Maximum Benefit &
Additional Measures
In this section, we highlight some limitations of the
prior work that we address in this work.
Raghavan et al (2007) only varied the domain
among different problems they analyzed, i.e, only
the variable D in our problem definition (P =
{D,G,F, L, I, S}). However, as motivated in the
introduction, other characteristics are also important
when categorizing learning problems and it is not
clear if we will observe similar results on problems
that differ in these additional characteristics. In this
work, we apply their measures to problems that dif-
fer in these characteristics in addition to the domain.
Analysis in Raghavan et al (2007) is specific to
their approach for incorporating feature feedback
into the model, which may not work well for all do-
mains and datasets as also mentioned in their work
(Section 6.1). It is not clear how their results can be
108
extended to alternate approaches for seeking and in-
corporating feature feedback. Thus, in this work we
analyze the maximum benefit a given problem can
get from feature feedback independent of the feed-
back solicitation and incorporation approach.
Raghavan et al (2007) analyze benefit from fea-
ture feedback at a fixed training data size of 42 la-
beled units. However, the difference between learn-
ing problems may vary with the amount of labeled
data. Some problems may benefit significantly from
feature feedback even at relatively larger amount of
labeled data. On the other hand, with very large
training set, the benefit from feature feedback can
be expected to be small and not significant for all
problems and all problems will look similar. Thus,
we evaluate the benefit from feature feedback at dif-
ferent amount of labeled data.
Raghavan et al (2007) evaluate benefit from fea-
ture feedback in terms of the gain in learning speed.
However, the learning rate does not tell us how much
improvement we get in performance at a given stage
in learning. In fact, even if at every point in the
learning curve performance with feature feedback
was lower than performance without feature feed-
back, the rate of convergence to the corresponding
maximum performance may still be higher when us-
ing feature feedback. Thus, in this work, in addi-
tion to evaluating the improvement in the learning
speed, we also evaluate the improvement in the ab-
solute performance at a given stage in learning.
3.1 Determining the Maximum Benefit
Annotating instances with or without feature feed-
back may require different annotation time. It is
only fair to compare different annotation strategies
at same annotation cost. Raghavan et al (2006)
found that on average labeling an instance takes the
same amount of time as direct feedback on 5 fea-
tures. Zaidan et al (2007) found that on average
it takes twice as much time to annotate an instance
with rationales than to annotate one without ratio-
nales. In our analysis, we focus on feedback on fea-
tures in context of the instance they occur in, i.e., in-
direct feature feedback through rationales or direct
feedback on features that occur in the instance be-
ing labeled. Thus, based on the findings in Zaidan et
al. (2007), we assume that on average annotating an
instance with feature feedback takes twice as much
time as annotating an instance without feature feed-
back. We define a currency for annotation cost as
Annotation cost Units (AUs). For an annotation bud-
get of a AUs, we compare two annotation strategies
of annotating a instances without feature feedback
or a2 instances with feature feedback.
In this work, we only focus on using feature feed-
back as an alternative to labeled data, i.e., to pro-
vide evidence about features in terms of their rele-
vance and class association. Thus, the best feature
feedback can do is provide as much evidence about
features as evidence from a large amount of labeled
data (oracle). Let F1(k,Nm) be the F1 score of a
model trained with features that occur in m train-
ing instances (Nm) and evidence for these features
from k instances (k ? m). For an annotation budget
of a AUs, we define the maximum improvement in
performance with feature feedback (IPa) as the dif-
ference in performance with feature feedback from
oracle on a2 training instances and performance with
a training instances without feature feedback.
IPa = F1(o,Na
2
)? F1(a,Na) (2)
where o is the number of instances in the oracle
dataset (o >> a). We also compare annotation
strategies in terms of the learning rate similar to
Raghavan et al (2007), except that we estimate and
compare the maximum improvement in the learning
rate. For an annotation budget of a AUs, we define
the maximum improvement in learning rate from 0
to a AUs (ILR0?a) as follows.
ILR0?a = pcp
wFF ? pcp
woFF (3)
where pcpwFF and pcpwoFF are the convergence
profiles with and without feature feedback at same
annotation cost, calculated as follows.
pcp
wFF =
?log2 a2
t=1 F1(o,N2t)
log2 a2 ? F1(o,Na2 )
(4)
pcp
woFF =
?log2a
t=2 F1(2
t, N2t)
(log2a? 1)? F1(a,Na)
(5)
where 2t denotes the training data size in iteration
t. Like Raghavan et al (2007), we use exponen-
tially increasing intervals to emphasize the relative
increase in the training data size, since adding a few
109
labeled instances earlier in learning will give us sig-
nificantly more improvement in performance than
adding the same number of instances later on.
3.2 Additional Metrics
The feature complexity measures require an ?ora-
cle?, simulated using a large amount of labeled data,
which is often not available. Thus, we need mea-
sures that do not require an oracle.
Benefit from feature feedback will depend on the
uncertainty of the model on its predictions, since it
suggests uncertainty on the features and hence scope
for benefit from feature feedback. We use the proba-
bility of the predicted label from the model as an es-
timate of the model?s uncertainty. We evaluate how
benefit from feature feedback varies with summary
statistics such as mean, median and maximum prob-
ability from the model on labels for instances in a
held out dataset.
4 Experiments, Results and Observations
In this section, we describe the details of our exper-
imental setup followed by the results.
4.1 Data
We analyzed three datasets: 1) Movie reviews
with rationale annotations by Zaidan et al (2007),
where the task is to classify the sentiment (posi-
tive/negative) of a review, 2) Movie review snippets
from Rotten Tomatoes (Pang and Lee., 2005), and 3)
WebKB dataset with the task of classifying whether
or not a webpage is a faculty member?s homepage.
Raghavan et al (2007) found that the webpage clas-
sification task has low feature complexity and ben-
efited the most from feature feedback. We compare
our results on this task and the sentiment classifica-
tion task on the movie review datasets.
4.2 Experimental Setup
Table 1 describes the different variables and their
possible values in our experiments. We make a log-
ical distinction for granularity based on whether an
instance in the problem is a document (several sen-
tences) or a sentence. Labeled data is composed of
instances and their class labels with or without fea-
ture feedback. As discussed in Section 3.1, instances
with feature feedback take on average twice as much
time to annotate as instances without feature feed-
back. Thus, we measure the labeled data in terms of
the number of annotation cost units which may mean
different number of labeled instances based on the
annotation strategy. We used two feature configura-
tions of ?unigram only? and ?unigram+dependency
triples?. The unigram and dependency annotations
are derived from the Stanford Dependency Parser
(Klein and Manning, 2003).
Rationales by definition are spans of text in a re-
view that convey the sentiment of the reviewer and
hence are the part of the document most relevant for
the classification task. In order to vary the amount
of irrelevant text, we vary the amount of text (mea-
sured in terms of the number of characters) around
the rationales that is included in the instance repre-
sentation. We call this the slack around rationales.
When using the rationales with or without the slack,
only features that overlap with the rationales (and
the slack, if used) are used to represent the instance.
Since we only have rationales for the movie review
documents, we only studied the effect of varying the
amount of irrelevant text on this dataset.
Variable Possible Values
Domain (D) {Movie Review classifica-
tion (MR), Webpage classi-
fication (WebKB)}
Instance Granu-
larity (G)
{document (doc), sentence
(sent)}
Feature Space (F ) {unigram only (u), uni-
gram+dependency (u+d)}
Labeled Data
(#AUs) (L)
{64, 128, 256, 512, 1024}
Irrelevant Text (I) {0, 200, 400, 600,? }
Instance Selection
Strategy (S))
{deterministic (deter), un-
certainty (uncert)}
Table 1: Experiment space for analysis of learning prob-
lems (P = {D,G,F, L, I, S})
For all our experiments, we used Support Vec-
tor Machines (SVMs) with linear kernel for learn-
ing (libSVM (Chang and Lin, 2001) in Minorthird
(Cohen, 2004)). For identifying the discrimina-
tive features we used the information gain score.
For all datasets we used 1800 total examples with
equal number of positive and negative examples. We
110
held out 10% of the data for estimating model?s un-
certainty as explained in Section 3.2. The results
we present are averaged over 10 cross validation
folds on the remaining 90% of the data (1620 in-
stances). In a cross validation fold, 10% data is used
for testing (162 instances) and all of the remaining
1458 instances are used as the ?oracle? for calculat-
ing the feature complexity measures and estimating
the maximum benefit from feature feedback as dis-
cussed in Sections 2 and 3.1 respectively. The train-
ing data size is varied from 64 to 1024 instances
(from the total of 1458 instances for training in a
fold), based on the annotation cost budget. Instances
with their label are added to the training set either in
the original order they existed in the dataset, i.e. no
selective sampling (deterministic), or in the decreas-
ing order of current model?s uncertainty on them.
Uncertainty sampling in SVMs (Tong and Koller,
2000) selects the instances closest to the decision
boundary since the model is expected to be most un-
certain about these instances. In each slice of the
data, we ensured that there is equal distribution of
the positive and negative class. SVMs do not yield
probabilistic output but a decision boundary, a com-
mon practice is to fit the decision values from SVMs
to a sigmoid curve to estimate the probability of the
predicted class (Platt, 1999).
4.3 Results and Analysis
To determine the effect of various factors on benefit
from feature feedback, we did an ANOVA analysis
with Generalized Linear Model using a 95% confi-
dence interval. The top part of Table 2 shows the
average F1 score for the two annotation strategies
at same annotation cost. As can be seen, with fea-
ture feedback, we get a significant improvement in
performance.
Next we analyze the significance of the effect of
various problem characteristics discussed above on
benefit from feature feedback in terms of improve-
ment in performance (IP ) at given annotation cost
and improvement in learning rate (ILR). Improve-
ment in learning rate is calculated by comparing
the learning profile for the two annotation strategies
with increasing amount of labeled data, up to the
maximum annotation cost of 1024 AUs.
As can be seen from the second part of Table 2,
most of the factors have a significant effect on bene-
fit from feature feedback. The benefit is significantly
higher for the webpage classification task than the
sentiment classification task in the movie review do-
main. We found that average feature complexity for
the webpage classification task (Nf = 3.07) to be
lower than average feature complexity for the senti-
ment classification task (Nf = 5.18) for 1024 train-
ing examples. Lower feature complexity suggests
that the webpage classification concept can be ex-
pressed with few keywords such as professor, fac-
ulty, etc., and with feature feedback we can quickly
identify these features. Sentiment on the other hand
can be expressed in a variety of ways which explains
the high feature complexity.
The benefit is more for document granularity than
sentence granularity, which is intuitive as feature
space is substantially larger for documents and we
expect to gain more from the user?s feedback on
which features are important. This difference is sig-
nificant for improvement in the learning rate and
marginally significant for improvement in perfor-
mance. Note that here we are comparing docu-
ments (with or without rationale slack) and sen-
tences. However, documents with low rationale
slack should have similar amount of noise as a sen-
tence. Also, a significant difference between do-
mains suggests that documents in WebKB domain
might be quite different from those in Movie Review
domain. This may explain the marginal significant
difference between benefit for documents and sen-
tences. To understand the effect of granularity alone,
we compared the benefit from feature feedback for
documents (without removing any noise) and sen-
tences in movie review domain only and we found
that this difference in also not significant. Thus, con-
trary to our intuition, sentences and documents seem
to benefit equally from feature feedback.
The benefit is more when the feature space is
larger and more diverse, i.e., when dependency fea-
tures are used in addition to unigram features. We
found that on average adding dependency features
to unigram features increases the feature space by
a factor of 10. With larger feature space, feature
feedback can help to identify a few relevant features.
As can also be seen, feature feedback is more help-
ful when there is more irrelevant text, i.e., there is
noise that feature feedback can help to filter out.
Unlike improvement in performance, the improve-
111
ment in learning rate does not decrease monoton-
ically as the amount of rationale slack decreases.
This supports our belief that improvement in perfor-
mance does not necessarily imply improvement in
the learning rate. We saw similar result when com-
paring benefit from feature feedback at different in-
stance granularity. Improvement in learning rate for
problems with different granularity was statistically
significant but improvement in performance was not
significant. Thus, both metrics should be used when
evaluating the benefit from feature feedback.
We also observe that when training examples are
selectively sampled as the most uncertain instances,
we gain more from feature feedback than without
selective sampling. This is intuitive as instances
the model is uncertain about are likely to contain
features it is uncertain about and hence the model
should benefit from feedback on features in these in-
stances. Next we evaluate how well the complexity
measures proposed in Raghavan et al (2007) corre-
late with improvement in performance and improve-
ment in learning rate.
V ar. V alues AvgF1 Group
Strat.
wFF 78.2 A
woFF 68.2 B
V ar. V alues AvgIP GrpIP AvgILR GrpILR
D
WebKB 11.9 A 0.32 A
MR 8.0 B 0.20 B
G
Doc 10.9 A 0.30 A
Sent 9.0 A 0.22 B
F
u+d 12.1 A 0.30 A
u 7.8 B 0.22 B
I
? 12.8 A 0.34 A
600 11.2 A B 0.23 B
400 11.1 A B 0.26 A B
200 9.8 B 0.26 A B
0 4.8 C 0.21 B
S
Uncer. 12.7 A 0.32 A
Deter. 7.1 B 0.20 B
Table 2: Effect of variables defined in Table 1 on benefit
from feature feedback. AvgIP is the average increase in
performance (F1) and AvgILR is the average increase in
the learning rate. Different letters in GrpIP and GrpILR
indicate significantly different results.
For a given problem with an annotation cost bud-
get of a AUs, we calculate the benefit from feature
feedback by comparing the performance with fea-
ture feedback on a2 instances and the performance
without feature feedback on a instances as described
in Section 3.1. The feature complexity measures are
calculated using a2 instances, since it should be the
characteristics of these a2 training instances that de-
termine whether we would benefit from feature feed-
back on these a2 instances or from labeling new
a
2
instances. As can be seen from Table 3, the correla-
tion of feature complexity measures with both mea-
sures of benefit from feature feedback is strong, neg-
ative and significant. This suggests that problems
with low feature complexity, i.e. concepts that can
be expressed with few well-selected features, benefit
more from feature feedback.
It is intuitive that the benefit from feature feed-
back decreases as amount of labeled data increases.
We found a significant negative correlation (?0.574)
between annotation budget (number of AUs) and
improvement in performance with feature feedback.
However, note that this correlation is not very
strong, which supports our belief that factors other
than the amount of labeled data affect benefit from
feature feedback.
Measure R(IP ) R(ILR)
Nf -0.625 -0.615
Fpc -0.575 -0.735
Cf -0.603 -0.629
Table 3: Correlation coefficient (R) for feature size com-
plexity (Nf ), feature profile complexity (Fpc) and com-
bined feature complexity (Cf ) with improvement in per-
formance (IP ) and improvement in learning rate (ILR).
All results are statistically significant (p < 0.05)
Feature complexity measures require an ?oracle?
simulated using a large amount of labeled data
which is not available for real annotation tasks.
In Section 3.2, we proposed measures based on
model?s uncertainty that do not require an oracle.
We calculate the mean, maximum and median of
the probability scores from the learned model on in-
stances in the held out dateset. We found a signifi-
cant but low negative correlation of these measures
with improvement in performance with feature feed-
back (maxProb = ?0.384, meanProb = ?0.256,
medianProb = ?0.242). This may seem counter-
intuitive. However, note that when the training data
is very small, the model might be quite certain about
112
its prediction even when it is wrong and feature feed-
back may help by correcting the model?s beliefs. We
observed that these probability measures have only
medium and significant positive correlation (around
0.5) with training datasize. Also, the held out dataset
we used may not be representative of the whole set
and using a larger dataset may give us more accurate
estimate of the model?s uncertainty. There are also
other ways to measure the model?s uncertainty, for
example, in SVMs the distance of an instance from
the decision boundary gives us an estimate of the
model?s uncertainty about that instance. We plan to
explore additional measures for model?s uncertainty
in the future.
5 Conclusion and Future Work
In this work, we analyze how the benefit from fea-
ture feedback varies with different problem charac-
teristics and how measures for categorizing learning
problems correlate with benefit from feature feed-
back. We define a problem instance as a tuple of
domain, instance granularity, feature representation,
labeled data, amount of irrelevant text and selective
sampling strategy.
We compare the two annotation strategies, with
and without feature feedback, in terms of both im-
provement in performance at a given stage in learn-
ing and improvement in learning rate. Instead of
evaluating the benefit from feature feedback us-
ing a specific feedback incorporation approach, we
estimate and compare how the maximum benefit
from feature feedback varies across different learn-
ing problems. This tells us what is the best feature
feedback can do for a given learning problem.
We find a strong and significant correlation be-
tween feature complexity measures and the two
measures of maximum benefit from feature feed-
back. However, these measures require an ?ora-
cle?, simulated using a large amount of labeled data
which is not available in real world annotation tasks.
We present measures based on the uncertainty of the
model on its prediction that do not require an oracle.
The proposed measures have a low but significant
correlation with benefit from feature feedback. In
our current work, we are exploring other measures
of uncertainty of the model. It is intuitive that a met-
ric that measures the uncertainty of the model on
parameter estimates should correlate strongly with
benefit from feature feedback. Variance in param-
eter estimates is one measure of uncertainty. The
Bootstrap or Jacknife method (Efron and Tibshirani,
1994) of resampling from the training data is one
way of estimating variance in parameter estimates
that we are exploring.
So far only a linear relationship of various mea-
sures with benefit from feature feedback has been
considered. However, some of these relationships
may not be linear or a combination of several mea-
sures together may be stronger indicators of the ben-
efit from feature feedback. We plan to do further
analysis in this direction in the future.
We only considered one selective sampling strat-
egy based on model?s uncertainty which we found
to provide more benefit from feature feedback. In
the future, we plan to explore other selective sam-
pling strategies. For example, density-based sam-
pling (Donmez and Carbonell, 2008) selects the in-
stances that are representative of clusters of simi-
lar instances, and may facilitate more effective feed-
back on a diverse set of features.
In this work, feature feedback was simulated us-
ing an oracle. Feedback from the users, however,
might be less accurate. Our next step will be to ana-
lyze how the benefit from feature feedback varies as
the quality of feature feedback varies.
Our eventual goal is to estimate the benefit from
feature feedback for a given problem so that the right
annotation strategy can be selected for a given learn-
ing problem at a given stage in learning and the total
annotation cost for learning the target concept can
be minimized. Note that in addition to the charac-
teristics of the labeled data analyzed so far, expected
benefit from feature feedback will also depend on
the properties of the data to be labeled next for the
two annotation strategies - with or without feature
feedback.
Acknowledgments
We thank Carolyn P. Rose?, Omid Madani, Hema
Raghavan, Jaime Carbonell, Pinar Donmez and
Chih-Jen Lin for helpful discussions, and the re-
viewers for their feedback. This work is supported
by DARPA?s Machine Reading program under con-
tract FA8750-09-C-0172.
113
References
Shilpa Arora and Eric Nyberg. 2009. Interactive annota-
tion learning with indirect feature voting. In Proceed-
ings of NAACL-HLT 2009 (Student Research Work-
shop).
Shilpa Arora, Elijah Mayfield, Carolyn Penstein Rose?,
and Eric Nyberg. 2010. Sentiment classification
using automatically extracted subgraph features. In
Proceedings of the Workshop on Emotion in Text at
NAACL.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
William W. Cohen. 2004. Minorthird: Methods for iden-
tifying names and ontological relations in text using
heuristics for inducing regularities from data.
Pinar Donmez and Jaime G. Carbonell. 2008. Paired
Sampling in Density-Sensitive Active Learning. In
Proceedings of the International Symposium on Arti-
ficial Intelligence and Mathematics.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In SIGIR ?08: Proceedings
of the 31st annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 595?602, New York, NY, USA. ACM.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active learning by labeling features. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP). Association
for Computational Linguistics.
B. Efron and R.J. Tibshirani. 1994. An introduction to
the bootstrap. Monographs on Statistics and Applied
Probability. Chapman and Hall/CRC, New York.
Mahesh Joshi and Carolyn Penstein Rose?. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL-IJCNLP ?09: Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 313?316, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423?430, Morristown, NJ,
USA. Association for Computational Linguistics.
K. Lang. 1995. NewsWeeder: Learning to filter net-
news. In 12th International Conference on Machine
Learning (ICML95), pages 331?339.
D. Lewis. 1995. The reuters-21578 text categorization
test collection.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized like-
lihood methods. In ADVANCES IN LARGE MARGIN
CLASSIFIERS, pages 61?74. MIT Press.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. 2004. Shallow seman-
tic parsing using support vector machines. In Pro-
ceedings of the Human Language Technology Con-
ference/North American chapter of the Association of
Computational Linguistics (HLT/NAACL).
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning with feedback on features and in-
stances. Journal of Machine Learning Research,
7:1655?1686.
Hema Raghavan, Omid Madani, and Rosie Jones. 2007.
When will feature feedback help? quantifying the
complexity of classification problems. In IJCAI Work-
shop on Human in the Loop Computing.
C. J. Van Rijsbergen. 1979. Information Retrieval. But-
terworths, London, 2 edition.
Simon Tong and Daphne Koller. 2000. Support vector
machine active learning with applications to text clas-
sification. In JOURNAL OF MACHINE LEARNING
RESEARCH, pages 999?1006.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Human Language
Technologies: Proceedings of the Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT), pages 260?
267, Rochester, NY, April.
114
