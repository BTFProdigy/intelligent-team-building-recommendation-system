Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 37?47, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Lexical Differences in Autobiographical Narratives from Schizophrenic
Patients and Healthy Controls
Kai Hong1, Christian G. Kohler2, Mary E. March2, Amber A. Parker3, Ani Nenkova1
University of Pennsylvania
Philadelphia, PA, 19104, USA
1{hongkai1,nenkova}@seas.upenn.edu
2{kohler,memarch}@mail.med.upenn.edu
3{parker}@sas.upenn.edu
Abstract
We present a system for automatic
identification of schizophrenic patients
and healthy controls based on narratives
the subjects recounted about emotional
experiences in their own life. The focus of the
study is to identify the lexical features that
distinguish the two populations. We report the
results of feature selection experiments that
demonstrate that the classifier can achieve
accuracy on patient level prediction as high as
76.9% with only a small set of features. We
provide an in-depth discussion of the lexical
features that distinguish the two groups and
the unexpected relationship between emotion
types of the narratives and the accuracy of
patient status prediction.
1 Introduction
Recent studies have shown that automatic language
analysis can be successfully applied to detect
cognitive impairment and language disorders. Our
work further extends this line of investigation with
analysis of the lexical differences between patients
suffering from schizophrenia and healthy controls.
Prior work has reported on characteristic
language peculiarities exhibited by schizophrenia
patients. There are more repetitions in speech
of patients compared to controls (Manschreck et
al., 1985). Patients also tend to repeatedly refer
back to themselves (Andreasen., 1986). Deviations
from normal language use in patients on different
levels, including phonetics and syntax, have been
documented (Covington et al2005), however
lexical differences have not been investigated in
detail.
In this paper we introduce a dataset of
autobiographical narratives told by schizophrenic
patients and by healthy controls. The narratives
are related to emotional personal experiences of the
subjects for five basic emotions: ANGER, SAD,
HAPPY, DISGUST, FEAR. We train an SVM
classifier to predict subject status. Our good results
on the relatively small dataset indicate the potential
of the approach. An automatic system for predicting
patient status from autobiographical narratives can
aid psychiatrists in tracking patients over time and
can serve as an easy way to administer large
scale screening. The detailed feature analysis we
performed also pinpoints key differences between
the two populations.
We study a range of lexical features including
individual words, repetitions as well as classes
of words defined in specialized dictionaries
compiled by psychologists (Section 4). We use
several approaches for feature analysis to identify
statistically significant differences in the two
populations. There are 169 significant features
among all of the 6057 features we examined.
Through feature selection we are able to obtain a
small set of 25 highly predictive features which
lead to status classification accuracy significantly
better than chance (Section 6.3). We also show
that differences between patients and controls are
revealed best in stories related to SAD and ANGRY
narratives, they are decent in HAPPY stories, and
that distinctions are poor for DISGUST and FEAR
(Section 6.5).
37
2 Related Work
Research in psychometrics has studied patterns
of lexical usage in a large variety of scenarios.
A popular tool used for psychometric analysis
is Linguistic Inquiry and Word Count (LIWC)
(Pennebaker et al2007). One of the most
interesting discoveries in that line of research is that
people with physical or emotional pain are likely to
use first-person singular pronouns more often than
the general population (Rude et al2004). In the
view of therapy, Pennebaker discovered that writing
emotional experiences can be helpful in therapeutic
process (Pennebaker, 1997). It has also been shown
that the usage of pronouns and function words can
be indicators of writing styles, physical health and
other distinctions (Tausczik and Pennebaker, 2010).
The combination of natural language processing
(NLP) and machine learning (ML) has been
explored in many psychology related projects,
and is gaining popularity. It has been shown
that features from language models (LMs) can
be used to detect impairment in monolingual
and bilingual children (Gabani et al2009).
Even better results are achieved when features
derived from LMs are combined with other surface
features to predict language impairment. Similarly,
studies on child language development and autism
have shown that n-gram cross-entropy from LMs
representative of healthy and impaired subjects is
a highly significant feature predictive of language
impairment (Prud?hommeaux et al2011). The
feasibility of making use of lexical features
to analyze language dominance among bilingual
children has also been confirmed (Solorio et al
2011).
In non-medically related research, LIWC and
lexical features have been used to recognize
different personalities such as introvert vs extrovert,
openness vs experience, conscientiousness vs
unconscientiousness, etc. (Mairesse et al2007).
Similar features have been applied to differentiate
author personality of e-mails (Gill et al2006),
blogs (Gill et al2009) and other documents.
Speech-related features and interactional aspects
of dialog behavior such as pauses, fillers, etc,
have also been found helpful in identifying autistic
patients (Heeman et al2010).
Variables Schizophrenia Control
(# Subjects) (n=23) (n=16)
Mean age (SD) 33.81 (9.65) 32.29 (6.59)
Mean number of
words per story (SD) 192.22 (122.4) 180.79 (95.87)
Table 1: Basic demographic information
Syntax features have been used in approaches
of automatic detection of neurological problems.
Parsing texts produced by subjects and using
bag of rules as features have been applied in
analyzing language dominance (Solorio et al
2011). Methods that quantify syntactic complexity
like Yngve score and Fraizer score have been used
to analyze autism (Prud?hommeaux et al2011).
Moreover, there has been research on detecting mild
cognitive impairment, which could be an earlier
state of Alzheimer?s disease: five different ways
of evaluating syntactic complexity measures were
introduced in their paper (Roark et al2011).
In our own work, we focus our analysis
exclusively on lexical features. Similarly to prior
work, we present the most significant features
related to differences between schizophrenic
patients and healthy controls. Unlike prior work,
instead of doing class ablation studies we perform
feature selection from the full set of available
features and identify a small set of highly predictive
features which are sufficient to achieve the top
performance we report. Such targeted analysis
is more helpful for medical professionals as they
search to develop new therapies and ways to track
patient status between visits.
3 Data
For our experiments we collected autobiographical
narratives from 39 speakers. The speakers are
asked to tell their experience involving the following
emotions: HAPPY, ANGER, SAD, FEAR and
DISGUST, which comprise the set of the five basic
emotions (Cowie, 2000). Most subjects told a single
story for each of the emotions, some told two. The
total number of stories in the dataset is 201.
The stories were narrated in the doctor?s office.
The recordings of the narratives were manually
transcribed in plain text format. We show age and
length in words of the told stories for the two groups
38
in Table 1. There are 23 patients with schizophrenia
and 16 healthy controls, telling 120 and 81 stories
respectively.
4 Features
Here we introduce the large set of lexical features
that we group in three classes: a large class of
features computed for individual lexical items, basic
features, features derived on the basis of pre-existing
dictionaries and language model features. We also
detail the way we performed feature normalization
and feature selection.
4.1 Surface Features
4.1.1 Basic Features
Basic features include token to type ratio to
capture vocabulary diversity, letters per word, words
per sentence, sentences per document and words
per document. These features describe the general
properties of the language used by the subject,
without focus on specific words.
Repetitions, revisions, large amount of fillers
or disfluencies can be indicators for language
impairment. In our basic features we detect the
number of repetitions in words, punctuations and
sentences for each transcript. Then these three
measures are normalized by total number of words
or sentences.
We define repetitions as the occurrence of the
same token in a sliding window of five items
within the same sentence. We count repetitions of
words and punctuation separately. The repetition
of punctuation, mostly commas and full-stops, are
indicative of phrasing in speech which has been
indirectly captured in the transcript. Repetition of
any word is counted, regardless of which specific
word was repeated. For example, for the sentence I
am, am, afraid, that something bad would happen.
am is counted as repeated once, and comma is
counted as repeated twice. Finally, sentence
repetition captures the amount of overlapping at the
beginning of two adjacent sentences, defined as the
number of tokens from the beginning of the sentence
until the first token where the two sentences differ.
4.1.2 Lexical Features
For words in the vocabulary: we use a real
value feature equal to the word frequency for each
document. Of particular interest we track the use
of pronouns because early research has reported that
people with cognitive impairment have a tendency
to use subjective words or referring to themselves
(Rude et al2004).
In addition, for each word in the vocabulary,
we apply the presence of the repetition about one
particular word.
4.1.3 Perplexity from Language Models
Inspired by the predictive power of language
model reported in prior work, we also include
several language model features. We build language
models on words as well as part-of-speech (POS)
tags from Stanford POS-tagger (Toutanova et al
2003). We tried unigram, bigram and trigram
language models by word and POS tag. Experiments
showed that bigram performed better than random,
and the other two performed below random. Thus
in the experiments we report later we train one
model for patients and one for controls and use the
perplexity of a given text according to the bigram
language models on word and POS as features in
prediction.
4.2 Dictionaries: LIWC and Diction
Text analysis packages have been widely used in
research related to personality analysis, sentimental
analysis and psychometric studies. We use two
dictionary-based systems, LIWC (Pennebaker et al
2007)1 and Diction2, which both give scores to
transcripts based on broad categories.
4.2.1 Linguistic Inquiry&Word Count(LIWC)
LIWC calculates the degree to which people use
different categories of words. Several manually
compiled dictionaries are at the heart of the
application. Each word or word stem could be in
one or more word categories or sub-dictionaries.
For instance, the word ?cried? is part of the
following categories: sadness, negative emotion,
overall affect, verb, and past tense verb. When
a narrative contains the word ?cried?, the scale
scores corresponding to these five subcategories are
incremented. The final output for each narrative is a
real value score for each of the 69 categories.
1See http://www.liwc.net
2See http://www.dictionsoftware.com
39
Because of the elaborate development of
dictionaries and categories, LIWC has been used
for predicting emotional and cognitive problems
from subject?s spoken and written samples.
Representative applications include studying
attention focus through personal pronouns, studying
honesty and deception by emotion words and
exclusive words and identifying thinking styles
(Tausczik and Pennebaker, 2010). Thus it is
reasonable to expect that LIWC derived features
would be helpful in identifying schizophrenia
patients. In Section 6.4 we discuss in more detail
the features which turned out to be significantly
different between patients and controls within
LIWC.
4.2.2 Diction
We also use Diction to analyze the lexical
characteristics of the transcripts. Similar to
LIWC, Diction scores are computed with reference
to manually compiled dictionaries. The master
variable scores in Diction include activity, certainty,
commonality, optimism and realism. These five
main scores are computed with 33 dictionaries that
define pertinent subcategories. The master variable
scores are constructed as follows: Sm =
?n
i=1 ai ?
?m
j=1 sj , where ai are additive traits, sj are
subtractive traits (giving positive/negative evidence
for the presence of the feature, respectively).
For example, Certainty and Realism scores are
calculated as follows:
Realism = [Familiarity + Spatial Awareness +
Temporal Awareness + Present Concern + Human
Interest + Concreteness] - [Past Concern +
Complexity]
Certainty = [Tenacity + Leveling + Collectives +
Insistence] - [Numerical Terms + Ambivalence +
Self Reference + Variety]
We also give definitions for some important
categories. The complete description of categories
is available in the Diction manual (Hart, 2000).
Cognition: Words referring to cerebral processes,
both functional and imaginative.
Satisfaction: Terms associated with positive
affective states.
Insistence: A measure of code-restriction and
contentedness, with the assumption that the
repetition of key terms indicates a preference for a
limited, ordered world.
Diversity: Words describing individuals or groups
of individuals differing from the norm.
Familiarity: Consisted of the most common words
in English.
Certainty: Language indicating resoluteness,
inflexibility, and completeness and a tendency to
speak ex cathedra.
Realism: Language describing tangible, immediate,
recognizable matters that affect people?s everyday
lives.
4.3 Feature normalization
We use two feature normalization approaches:
projection normalization and binary normalization.
Both of the two approaches are applied to basic
features, dictionary features and word features. As
for repetition, we don?t use normalization, because
it is in itself binary. For transcript i, we denote
the value of the jth feature as vij . We denote
minj , maxj , averagej as the minimum, maximum
and average value for each feature in the training
corpus, respectively. Thus for each feature j,
we have: averagej = 1n
?n
i=1 vij minj =
mini{vij},maxj = maxi{vij}.
4.3.1 Projection Normalization
Here we simply normalize all feature values to a
range of [0, 1], where 0 corresponds to the smallest
observed value and 1 to the largest observed value
across all transcripts. Then we could have pij =
vij?minj
maxj?minj , where pij is the feature value after
normalization.
4.3.2 Binary normalization
Here all features are converted to binary values,
reflecting whether the value falls below or above the
average value for that feature observed in training.
The value pij of j-th feature for the i-th instance is
as below:
pij =
{
0 vij < 1n
?n
i=1 vij
1 otherwise
4.3.3 Prediction on the Test Set
All of the previous values, averagej , maxj and
minj are derived from the training set. While
doing classification, for a new testing instance, we
denote the feature vector as f = (f1, f2, . . . fn).
40
fj is then compared with averagej to do binary
normalization. We also use pj = fj?minjmaxj?minj to do
projection normalization. If pj < 0, we change pj
into 0; if pj > 1, we change pj into 1. For the
words or features that are not seen in training, we
just ignore this dimension.
4.4 Feature selection
All lexically based analysis is plagued by data
sparsity problems. In the medical domain this
problem is even more acute because collecting
patient data is difficult. The number of features
we defined outnumbers our samples by orders
of magnitude. Therefore, in our classification
procedure, we perform feature selection by doing
two-sided T-test to compare the values of features
in the patient and control groups. The features with
p-value ? 0.05 are considered as indicative and are
selected for later machine learning experiments, in
which 169 out of 6057 features have been selected.
We discuss the significant features in the full set in
Section 6.4 .
Note however that we don?t use the features
selected on the full dataset for machine learning
experiments because when T-tests are applied
on the full dataset feature selection decisions
would include information about the test set as
well. Therefore, we adopt a leave-one-subject-out
(LOSO) evaluation approach instead. In each
iteration, we set aside one subject as test set. The
data from the remaining subjects form the training
set. Feature selection is done on the training set only
and a model is trained. The predictions are tested on
the held out subject. The procedure is repeated for
every subject as test set.
The choice of p-value cut-off allows us to relax
and tighten the requirement on significance of the
features and thus the size of the feature set. We
report results with different p-values in Table 3.
We also explore alternative feature ranking and
feature selection procedures in Section 6.3. In
each fold different features may be selected. For
ease of discussing feature differences we present
a discussion of the 169 significant features on the
entire dataset.
5 Our approach
The goal of our system is to classify the person who
told a story in one of two categories: Schizophrenia
group (SC) and Control group (CO). In order to
do this, we give labels to the stories told by each
subject. Therefore we could use our model to
identify the status of the person who told each
individual story, the task is to answer the question
?Was the subject who told this story a patient or
control??. Then we combine the predictions for
stories to predict status of each subject, and the
task becomes answering the question ?Is this subject
a patient or control given that they told these five
stories??. Thus in story level prediction we use no
information about the fact that subjects told more
than one story, while in subject-level prediction we
do use this information.
First we present an experiment that relies only
on language models for the prediction. Then we
present the complete learning-based system that
uses the full set of features. Finally, we describe
the decision making approach to combine the story
level predictions to derive a subject-level prediction.
5.1 Language Model
Language models have been used previously for
language impairment on children (Gabani et al
2009) and language dominance prediction (Solorio
et al2011). Patients with speaking disorder
or cognitive impairment express themselves in
atypical ways. Language models (LMs) give a
straightforward way of estimating the probability
of the productions of a given subject. We expect
that the approach would be useful for the study of
schizophrenia as well and so start with a description
of the LM experiments.
We use LMs on words to recognize the difference
between patients and controls in vocabulary use.
We also trained a LM on POS tags because
it could reduce sparsity and focus more on
grammatical patterns. Two separate LMs are
trained on transcripts of schizophrenia and controls
respectively, using leave-one-subject-out protocol.
Story-level decisions are made by assigning the
class whose language model yields lower perplexity:
s(t) =
{
SC PERSC(t) ? PERCO(t)
CO otherwise
41
by Story (%) SC-F CO-F Accuracy Macro-F
Random 54.4 44.6 50.0 49.5
Majority 74.8 0.0 59.7 37.4
2-gram 62.5 44.4 55.2 53.5
2-gram-Pos 62.2 53.3 58.2 57.8
by Subject (%) SC-F CO-F Accuracy Macro-F
Random 54.1 45.1 50.0 49.6
Majority 74.2 0.0 59.1 37.1
2-gram 65.2 50.0 58.9 57.6
2-gram-Pos 66.7 54.5 61.5 60.6
Table 2: Language model performance
Here t means a transcript from a subject, while
PERSC and PERCO are perplexities for patients
and controls, respectively. We experimented with
unigram, bigram and trigram LMs on words and
POS tags. Laplace smoothing is used when
generating word probabilities.
5.2 Classification Phase
Language models are convenient because they
summarize information from patterns in lexical and
POS use into a single number. However, most of the
successful applications of LMs require large amount
of training data while our dataset is relatively small.
Moreover, we would like to analyze more specific
differences between the patient and control group
and this would be more appropriately done using a
larger set of features.
We have described our features and feature
selection process in Section 4. We use SVM-light
(Joachims, 1999) for our machine learning
algorithm, as its effectiveness has been proved in
various learning-based clinical tasks compared to
other classifiers (Gabani et al2009) .
5.3 Status Decision
Story level predictions are made for each transcript
either based on LM perplexity or SVM prediction.
The most intuitive way to obtain a subject-level
prediction is by voting from story-level predictions
between the stories told by the particular subject.
The subject-level prediction is simply set to equal
the majority prediction from individual stories. On
the few occasions where there are equal votes for
schizophrenia and control, the system makes a
preference towards schizophrenia, because it is more
P-value cut-off by Story by Subject # Features
0.15 59.0 58.9 450
0.10 61.7 64.1 341
0.05 62.7 64.1 169
0.01 57.7 65.4 44
0.005 64.2 71.6 32
0.001 65.7 75.6 18
0.0005 61.7 66.7 14
Table 3: Performance by subject after T-test feature
selection in different confidence levels.
dangerous to omit a potential patient.
6 Experiments and Results
We perform our experiments on the 201 transcripts
of the 39 speakers. The two baselines we
compare with are doing random assignments and
majority class, which for our datasets correspond to
predicting all subjects into the Schizophrenia group.
We report precision, recall and F-measure for
both patient and control groups, as well as overall
accuracy and Macro-F value. We get predictions
in leave-one-subject-out fashion and compute the
results over the complete set of predictions.
6.1 Language Model Performance
Our first experiment relies only on the perplexity
from language models to make the prediction.
We use the 1,2,3-gram models on word and POS
sequences. From the result in Table 2 we can
see bigram LM performed better than random
baseline for both story and subject level prediction.
3-gram and 1-gram LM did not give a credible
performance, with results worse than that of the
baselines. Because of space constraints we do not
report the specific numbers.
6.2 Classification Result after Feature Selection
Next we evaluate the performance of classification
with different number of features from the classes
we define in Section 4. As discussed above, we
performed feature selection by choosing different
levels of significance for the p-value cut-off. Feature
selection is performed 39 times for each LOSO
training fold. On the standard cut-off p-value ?
0.05, our system could achieve 62.7% accuracy on
story and 64.1% on patient level prediction. The best
performance is achieved when the cut-off p-value is
42
Schizophrenia Control General
Measurement P (%) R (%) F (%) P (%) R (%) F (%) Accuracy (%) Macro-F (%)
Story Random 59.7 50.0 54.4 40.5 50.0 44.6 50.0 49.5
Majority 59.7 100.0 74.8 NA 0.0 0.0 (NA) 59.7 37.4
25-Features 68.7 75.0 71.7 57.1 49.4 52.9 64.7 62.3
Subject Random 59.0 50.0 54.1 41.0 50.0 45.0 50.0 49.6
Majority 59.0 100.0 74.2 NA 0.0 0.0 (NA) 59.0 37.1
25-Features 75.0 91.3 82.4 81.8 56.3 66.7 76.9 74.6
Table 4: Performance on best feature-set by feature ranking using signal to noise
stricter, 0.001, where an accuracy of 75.6% can be
reached. In this case only about 18 features are used
for the classification. Detailed results are shown in
Table 3.
6.3 Performance with Different Feature Size
Next we investigate the relationship between feature
set size and accuracy of prediction. We are
interested in identifying the smallest possible set
of features which gives performance close to the
one reported on the full set of significant features.
Narrowing the feature set as much as possible will
be most useful for clinicians as they understand
the differences between the groups and look for
indicators of the illness they need to track during
regular patient visits. Physicians and psychologists
are also interested to know the most significant
lexical differences revealed by the stories.
As an alternative to ranking features by p-value,
we use the Challenge Learning Object Package
(CLOP) 3 (Guyon et al2006) . It is a toolkit
with a combination of preprocessing and feature
selection. We experiment with signal-to-noise (s2n),
Gram-Schmidt orthogonalization and Recursive
Feature Elimination for finding a subset of indicative
features (Guyon and Elisseeff, 2003). The
signal-to-noise method gives better results than the
other two by at least 6% for the top performance
feature set. Thus we pick the best k features
according to the s2n result and use only those k
features for classification.
Figure 1 shows how prediction accuracy changes
with feature sets of different sizes. From the plot
we clearly see that our top performance is achieved
with 25 to 40 features, after which performance
drops. The peak performance is achieved when
3See http://clopinet.com/CLOP/
Figure 1: Story and Subject prediction accuracy
there are 25 features, where we could reach 75.0%
precision, 91.3% recall, 82.4% F-measure for
patient, and 76.9% accuracy for overall, as shown
in Table 4. Detailed information about the top
30 features can be found in Table 5. ?+? and ?-?
means more prevalent for patient and control, while
?prj? and ?01? correspond to the two normalization
approaches in Section 4.3, projection and binary
respectively.
6.4 Analysis of Significant Features
In this section we discuss the specific features that
were revealed as most predictive by the feature
selection methods that we employed. We have seen
that it only requires about 25-40 features to obtain
peak performance.
First we briefly review the features that turned
out to be statistically significant (for 0.05 p-value
cut-off). Table 7 provides a list of the features
with higher values for Schizophrenia and Control
respectively. 4 We group the significant features
according to the feature classes we introduced in
4LM1 is defined as the ratio of CO perplexity and
SC perplexity from LMs, LM7 comes from projection
normalization of LM1. If LM perplexity for CO is smaller than
that of SC, then we set LM3 as 1; otherwise we set LM4 as 1.
43
Rank Feature Category P-value
1 Prj-Self + Diction 5.33E-06
2 01-Self + Diction 7.34E-06
3 Prj-punctuation - Basic 1.33E-05
4 01-I + LIWC 2.73E-05
5 01-sorry - Lexical 0.007
6 01-money + Lexical 6.95E-05
7 01-punctuation - Basic 4.88E-05
8 prj-I + LIWC 5.12E-05
9 01-extremely + Lexical 5.10E-05
10 prj-mildly + Lexical 0.0006
11 prj-sorry - Lexical 0.011
12 prj-I + Lexical 0.0002
13 LM1 + LM 0.0002
14 LM7 + LM 0.0002
15 I + Repeat 0.0003
Rank Feature Category P-value
16 and + Repeat 0.0002
17 01-mildly + Lexical 0.0004
18 prj-adverb - LIWC 0.0006
19 01-relationship - Lexical 0.024
20 01-late - Lexical 0.024
21 prj-comma - Lexical 0.001
22 Repeat word - Basic 0.001
23 prj-late - Lexical 0.034
24 prj-very - Lexical 0.007
25 prj-extremely + Lexical 0.001
26 01-couldn?t + Lexical 0.001
27 prj-relationship - Lexical 0.037
28 very - Repeat 0.007
29 prj-? + Lexical 0.002
30 prj-moderately + Lexical 0.006
Table 5: Table of the top 30 features by signal-to-noise ranking
Section 4. Of the 169 significant features, 111 are
more prevalent in patients, 58 are more prevalent
among the controls. If a feature was significant with
both normalizations we use, we list it only once in
Table 7.
Among the words indicative of schizophrenia,
subjective words such as I and LIWC category
self are among the most significant. This finding
conforms with prior research that patients with
mental disorders refer to themselves more often than
regular people. Patients produce more questions (as
indicated by the significance of the question mark
as a feature). It is possible that this indicates a
disruption in their thought process and they forget
what they are talking about. Further work will be
needed to understand this difference better.
In terms of words, patients talked more about
money, trouble, and used adverbs like moderately
and basically. Repetition in language is also a
revealing characteristic of the patient narratives.
There is a substantial difference in the appearance
of repetitions between the two groups, as well as
repetition of specific words: I, and, and repetition
of filled pauses um. As patients focus more on their
own feelings, they talked a lot about their family,
using words such as son, grandfather and even dogs.
Diction features revealed some unexpected
differences. The schizophrenia group scores
higher in the Self, Cognition, Past, Insistence and
Satisfaction categories. This indicates that they are
more likely to talk about past experience, using
cognitive terms and having a repetition of key
terms. We were particularly curious to understand
why patients score higher on Satisfaction ratings.
On closer inspection we discovered that patients?
stories were rated higher in Satisfaction when
they were telling SAD stories. This finding has
important clinical implications because one of the
diagnostic elements for the disease is inappropriate
emotion expression. Our study is the first to apply
an automatic measure to detect such anomaly in
patients? emotional narratives. Prompted by this
discovery, we take a closer look at the interaction
between the emotion expressed in a story and the
accuracy of status prediction in the next section.
The control group exhibited more word
complexity, sentence complexity and thoughtfulness
in their stories. They use more adverbs and exclusive
words (e.g. but, without, exclude) on general trend.
They use the word sorry significantly more often
than patients.
6.5 Status Prediction by Emotion
We also investigate if classification accuracy differs
depending on the type of conveyed emotion.
Accuracy per emotion with three feature selection
methods is shown in Table 6. When using
signal-to-noise, we can see that on SAD stories the
two groups can be distinguished better. Story-level
accuracies on HAPPY stories reach 72.5%, and
that the accuracy on HAPPY stories is the next
highest one. When applying the 0.05 p-value
cut-off to select significant features, ANGER stories
become the ones for which the status of a subject
44
Accuracy (%) s2n (25) T-test (0.05) T-test (0.001)
Happy 66.7 59.0 71.8
Disgust 63.4 61.0 51.2
Anger 61.0 70.7 70.7
Fear 60.0 55.0 67.5
Sad 72.5 60.0 67.5
Story 64.7 62.9 65.7
Patient 76.9 64.1 74.4
Majority 59.0 59.0 59.0
Table 6: Accuracy per emotion by different feature-sets
can be predicted most accurately. Using the
threshold of 0.001 for selection gives the best overall
prediction. In that case, HAPPY and ANGER are
the emotions for which recognition is best. The
changes in the recognition accuracy depending on
feature selection suggests that in future studies it
may be more beneficial to perform feature selection
only on stories from a given type because obviously
indicative features exist at least for the SAD, ANGER
and HAPPY stories.
Regardless of the feature selection approach, it
is more difficult to tell the two groups apart when
they tell DISGUST and FEAR stories. These results
seem to indicate that when talking about certain
emotions patients and controls look much more alike
than when other emotions are concerned. Future
data acquisition efforts can focus only on collecting
autobiographical narratives relevant to the emotions
for which patients and controls differ most.
Figure 2: Number of significant features by P-value
selection on different thresholds (per emotion)
In future work we would like to use only stories
from a given emotion to classify between patients
Types Significant features more common in SCH
Basic repeat-word, sentence/document
LIWC I, insight, personal-pronoun
Diction self, cognition, past, insistence, satisfaction
Lexical ?, ain?t, alone, at, aw, become, before, behind
care, chance, confused, couldn?t, December, dog
dogs, extreme, extremely, feeling, forty, friends
god, got, grandfather, guess, guy, hand, hanging
hearing, hundred, increased, looking, loved
mental, met, mild, mildly, moderate, moderately
money, my, myself, outside, paper, passed, piece
remember, sister, son, stand, step, story, take
taken, throwing, took, trouble, use, wake
wanna, way
Repeat a, and, I, um, was
LM LM1, LM4, LM7
Types Significant features more common in CO
Basic length/word, words/sentence
LIWC ?6-letters, adverb, exclusive words, inhibitive
Diction certainty, cooperation, diversity
familiarity, realism
Lexical ?,?, able, actually, are, basically, be, being, get?s
in, late, not, really, relationship, result, she?s
sleep, sorry, tell, their, there?s, very, weeks
Repeat very, ?,?
LM LM3
Table 7: Significant features (p-value ? 0.05)
and controls. Doing this with our current dataset
is not feasible because there are only about 40
transcripts per emotion. Therefore, we use our
data to identify significant features that distinguish
patients from controls only on narratives from a
particular emotion. For example, we compare the
differences of SAD stories told by patients and
controls. We count the number of significant
features between patients and controls with 11
different p-value cut-offs, and provide a plot that
visualizes the results in Figure 2. From the graph,
it is clear that there are many more differences
between the two groups in ANGER and SAD
narratives. HAPPY comes next, then DISGUST and
FEAR. However, at lower confidence levels, HAPPY
has equal number of significant features as ANGER
and SAD, which is in line with the result in Table 6.
The feature analysis performed by emotion
reveals more differences between patients and
controls, beyond common features such as self,
I, etc. For HAPPY stories, patients talk more
about their friends and relatives; they also have a
45
higher tendency of being ambivalent. For DISGUST
stories, patients are more disgusted with dogs, and
they talk more about health. The control group
shows a higher communication score, referring to
a better social interaction. ANGER is one of the
emotions that best reveals the differences between
groups, and schizophrenia patients show more
aggression and cognition while talking, according
to features derived from Diction. The control
group sometimes talks more about praise. In FEAR
stories patients talk about money more often than
controls. Meanwhile, the control group uses more
inhibition words, for instance: block, constrain and
stop. An interesting phenomenon happens in SAD
narratives. When talking about sad experiences,
patients sometimes show satisfaction and insistence,
while the controls talked more about working
experiences.
7 Conclusion
In this paper, we analyzed the predictive power
of different kinds of features for distinguishing
schizophrenia patients from healthy controls. We
provided an in-depth analysis of features that
distinguish patients from controls and showed that
the type of emotion conveyed by the personal
narratives is important for the distinction and that
stories for different emotions give different sets
indicators for subject status. We report classification
results as high as 76.9% on the subject level,
with 75.0% precision and 91.3% on recall for
schizophrenia patients.
We consider the results presented here to be
a pilot study. We are currently collecting and
transcribing additional stories from the two groups
which we would like to use as a definitive test
set to verify the stability of our findings. We
plan to explore syntactic and coherence models to
analyze the stories, as well as emotion analysis of
the narratives.
References
Nancy C. Andreasen. 1986. Scale for the assessment
of thought, language, and communication (TLC).
Schizophrenia Bulletin, 12:473 ? 482.
Michael A. Covington, Congzhou He, Cati Brown,
Lorina Naci, Jonathan T. McClain, Bess Sirmon
Fjordbak, James Semple, and John Brown. 2005.
Schizophrenia and the structure of language: The
linguist?s view. Schizophrenia Research, 77(1):85 ?
98.
Roddy Cowie. 2000. Describing the emotional states
expressed in speech. In Proceedings of the ISCA
Workshop on Speech and Emotion.
Keyur Gabani, Melissa Sherman, Thamar Solorio, Yang
Liu, Lisa Bedore, and Elizabeth Pen?a. 2009.
A corpus-based approach for the prediction of
language impairment in monolingual english and
spanish-english bilingual children. In Proceedings of
HLT-NAACL, pages 46?55.
Alastair J. Gill, Jon Oberlander, and Elizabeth Austin.
2006. Rating e-mail personality at zero acquaintance.
Personality and Individual Differences, 40(3):497 ?
507.
Alastair J. Gill, Scott Nowson, and Jon Oberlander. 2009.
What are they blogging about? personality, topic and
motivation in blogs. In Proceedings of the AAAI
ICWSM?09.
Isabelle Guyon and Andre? Elisseeff. 2003. An
introduction to variable and feature selection. J. Mach.
Learn. Res., 3:1157?1182, March.
Isabelle Guyon, Jiwen Li, Theodor Mader, Patrick A.
Pletscher, Georg Schneider, and Markus Uhr. 2006.
Feature selection with the CLOP package. Technical
report, http://clopinet.com/isabelle/Projects/ETH/
TM-fextract-class.pdf.
Rodrick Hart. 2000. Diction 5.0, the text-analysis
program user?s manual, Scolari Software, Sage Press.
http://www.dictionsoftware.com/.
Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois M. Black, and Jan P. H. van Santen. 2010.
Autism and interactional aspects of dialogue. In
Proceedings of the SIGDIAL 2010 Conference, pages
249?252.
T. Joachims. 1999. Making large?scale SVM learning
practical. In B. Scho?lkopf, C. J. C. Burges, and
A. J. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning, pages 169?184, Cambridge,
MA. MIT Press.
F. Mairesse, M. A. Walker, M. R. Mehl, and R. K.
Moore. 2007. Using Linguistic Cues for the
Automatic Recognition of Personality in Conversation
and Text. Journal of Artificial Intelligence Research,
30:457?500.
Theo C. Manschreck, Brendan A. Maher, Toni M.
Hoover, and Donna Ames. 1985. Repetition in
schizophrenic speech. Language & Speech, 28(3):255
? 268.
J.W. Pennebaker, R.J. Booth, and Francis. 2007.
Linguistic inquiry and word count (LIWC
46
2007): A text analysis program. Austin, Texas.
http://www.liwc.net/.
James W. Pennebaker. 1997. Writing about Emotional
Experiences as a Therapeutic Process. Psychological
Science, 8(3):162?166.
Emily T. Prud?hommeaux, Brian Roark, Lois M. Black,
and Jan van Santen. 2011. Classification of atypical
language in autism. In Proceedings of the 2nd
Workshop on Cognitive Modeling and Computational
Linguistics, CMCL?11, pages 88?96.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011.
Spoken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech & Language Processing, 19(7):2081?2090.
Stephanie Rude, Eva-Maria Gortner, and James
Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition &
Emotion, 18(8):1121?1133.
Thamar Solorio, Melissa Sherman, Y. Liu, Lisa Bedore,
Elizabeth Pen?a, and A. Iglesias. 2011. Analyzing
language samples of spanish-english bilingual children
for the automated prediction of language dominance.
Natural Language Engineering, 17(3):367?395.
Yla R. Tausczik and James W. Pennebaker. 2010.
The Psychological Meaning of Words: LIWC and
Computerized Text Analysis Methods. Journal
of Language and Social Psychology, 29(1):24?54,
March.
Kristina Toutanova, Dan Klein, and Christopher D.
Manning. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of
HLT-NAACL 03.
47
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712?721,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Improving the Estimation of Word Importance for News Multi-Document
Summarization
Kai Hong
University of Pennsylvania
Philadelphia, PA, 19104
hongkai1@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA, 19104
nenkova@seas.upenn.edu
Abstract
We introduce a supervised model
for predicting word importance that
incorporates a rich set of features. Our
model is superior to prior approaches
for identifying words used in human
summaries. Moreover we show
that an extractive summarizer using
these estimates of word importance is
comparable in automatic evaluation with
the state-of-the-art.
1 Introduction
In automatic extractive summarization, sentence
importance is calculated by taking into account,
among possibly other features, the importance
of words that appear in the sentence. In this
paper, we describe experiments on identifying
words from the input that are also included in
human summaries; we call such words summary
keywords. We review several unsupervised
approaches for summary keyword identification
and further combine these, along with features
including position, part-of-speech, subjectivity,
topic categories, context and intrinsic importance,
in a superior supervised model for predicting word
importance.
One of the novel features we develop aims
to determine the intrinsic importance of words.
To this end, we analyze abstract-article pairs in
the New York Times corpus (Sandhaus, 2008)
to identify words that tend to be preserved in
the abstracts. We demonstrate that judging word
importance just based on this criterion leads to
significantly higher performance than selecting
sentences at random. Identifying intrinsically
important words allows us to generate summaries
without doing any feature computation on the
input, equivalent in quality to the standard baseline
of extracting the first 100 words from the latest
article in the input. Finally, we integrate the
schemes for assignment of word importance into
a summarizer which greedily optimizes for the
presence of important words. We show that our
better estimation of word importance leads to
better extractive summaries.
2 Prior work
The idea of identifying words that are descriptive
of the input can be dated back to Luhn?s earliest
work in automatic summarization (Luhn, 1958).
There keywords were identified based on the
number of times they appeared in the input,
and words that appeared most and least often
were excluded. Then the sentences in which
keywords appeared near each other, presumably
better conveying the relationship between the
keywords, were selected to form a summary.
Many successful recent systems also estimate
word importance. The simplest but competitive
way to do this task is to estimate the word
probability from the input (Nenkova and
Vanderwende, 2005). Another powerful method
is log-likelihood ratio test (Lin and Hovy, 2000),
which identifies the set of words that appear in
the input more often than in a background corpus
(Conroy et al., 2006; Harabagiu and Lacatusu,
2005).
In contrast to selecting a set of keywords,
weights are assigned to all words in the input
in the majority of summarization methods.
Approaches based on (approximately) optimizing
the coverage of these words have become widely
popular. Earliest such work relied on TF*IDF
weights (Filatova and Hatzivassiloglou, 2004),
later approaches included heuristics to identify
summary-worthy bigrams (Riedhammer et al.,
2010). Most optimization approaches, however,
use TF*IDF or word probability in the input as
word weights (McDonald, 2007; Shen and Li,
2010; Berg-Kirkpatrick et al., 2011).
712
Word weights have also been estimated by
supervised approaches, with word probability and
location of occurrence as typical features (Yih et
al., 2007; Takamura and Okumura, 2009; Sipos et
al., 2012).
A handful of investigations have productively
explored the mutually reinforcing relationship
between word and sentence importance, iteratively
re-estimating each in either supervised or
unsupervised framework (Zha, 2002; Wan et
al., 2007; Wei et al., 2008; Liu et al., 2011).
Most existing work directly focuses on predicting
sentence importance, with emphasis on the
formalization of the problem (Kupiec et al., 1995;
Celikyilmaz and Hakkani-Tur, 2010; Litvak et al.,
2010). There has been little work directly focused
on predicting keywords from the input that will
appear in human summaries. Also there has been
only a few investigations of suitable features
for estimating word importance and identifying
keywords in summaries; we address this issue by
exploring a range of possible indicators of word
importance in our model.
3 Data and Planned Experiments
We carry out our experiments on two datasets from
the Document Understanding Conference (DUC)
(Over et al., 2007). DUC 2003 is used for training
and development, DUC 2004 is used for testing.
These are the last two years in which generic
summarization was evaluated at DUC workshops.
There are 30 multi-document clusters in DUC
2003 and 50 in DUC 2004, each with about 10
news articles on a related topic. The task is
to produce a 100-word generic summary. Four
human abstractive summaries are available for
each cluster.
We compare different keyword extraction
methods by the F-measure
1
they achieve against
the gold-standard summary keywords. We do not
use stemming when calculating these scores.
In our work, keywords for an input are defined
as those words that appear in at least i of the
human abstracts, yielding four gold-standard sets
of keywords, denoted by G
i
. |G
i
| is thus the
cardinality of the set for the input. We only
consider the words in the summary that also
appear in the original input
2
, with stopwords
1
2*precision*recall/(precision+recall)
2
On average 26.3% (15.0% with stemming) of the words
in the four abstracts never appear in the input.
excluded
3
. Table 1 shows the average number of
unique content words for the respective keyword
gold-standard.
i 1 2 3 4
Mean |G
i
| 102 32 15 6
Table 1: Average number of words in G
i
For the summarization task, we compare results
using ROUGE (Lin, 2004). We report ROUGE-1,
-2, -4 recall, with stemming and without removing
stopwords. We consider ROUGE-2 recall as
the main metric for this comparison due to its
effectiveness in comparing machine summaries
(Owczarzak et al., 2012). All of the summaries
were truncated to the first 100 words by ROUGE
4
.
We use Wilcoxon signed-rank test to examine
the statistical significance as advocated by Rankel
et al. (2011) for both tasks, and consider
differences to be significant if the p-value is less
than 0.05.
4 Unsupervised Word Weighting
In this section we describe three unsupervised
approaches of assigning importance weights to
words. The first two are probability and
log-likelihood ratio, which have been extensively
used in prior work. We also apply a markov
random walk model for keyword ranking, similar
to Mihalcea and Tarau (2004). In the next
section we describe a summarizer that uses these
weights to form a summary and then describe
our regression approach to combine these and
other predictors in order to achieve more accurate
predictions for the word importance in Section 7.
The task is to assign a score to each word in the
input. The keywords extracted are thus the content
words with highest scores.
4.1 Word Probability (Prob)
The frequency with which a word occurs in the
input is often considered as an indicator of its
importance. The weight for a word is computed
as p(w) =
c(w)
N
, where c(w) is the number of
times word w appears in the input and N is the
total number of word tokens in the input.
3
We use the stopword list from the SMART system
(Salton, 1971), augmented with punctuation and symbols.
4
ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n
4 -m -a -l 100 -x
713
4.2 Log-likelihood Ratio (LLR)
The log-likelihood ratio test (Lin and Hovy, 2000)
compares the distribution of a word in the input
with that in a large background corpus to identify
topic words. We use the Gigaword corpus (Graff et
al., 2007) for background counts. The test statistic
has a ?
2
distribution, so a desired confidence level
can be chosen to find a small set of topic words.
4.3 Markov Random Walk Model (MRW)
Graph methods have been successfully applied to
weighting sentences for generic (Wan and Yang,
2008; Mihalcea and Tarau, 2004; Erkan and
Radev, 2004) and query-focused summarization
(Otterbacher et al., 2009).
Here instead of constructing a graph with
sentences as nodes and edges weighted by
sentence similarity, we treat the words as vertices,
similar to Mihalcea and Tarau (2004). The
difference in our approach is that the edges
between the words are defined by syntactic
dependencies rather than depending on the
co-occurrence of words within a window of k. We
use the Stanford dependency parser (Marneffe et
al., 2006). In our approach, we consider a word
w more likely to be included in a human summary
when it is syntactically related to other (important)
words, even if w itself is not mentioned often.
The edge weight between two vertices is equal to
the number of syntactic dependencies of any type
between two words within the same sentence in
the input. The weights are then normalized by
summing up the weights of edges linked to one
node.
We apply the Pagerank algorithm (Lawrence
et al., 1998) on the resulting graph. We set the
probability of performing random jump between
nodes ?=0.15. The algorithm terminates when
the change of node weight between iterations is
smaller than 10
?4
for all nodes. Word importance
is equal to the final weight of its corresponding
node in the graph.
5 Summary Generation Process
In this section, we outline how summaries
are generated by a greedy optimization system
which selects the sentence with highest weight
iteratively. This is the main process we use in all
our summarization systems. For comparison we
also use a summarization algorithm based on KL
divergence.
5.1 Greedy Optimization Approach
Our algorithm extracts sentences by weighting
them based on word importance. The approach is
similar to the standard word probability baseline
(Nenkova et al., 2006) but we explore a range
of possibilities for assigning weights to individual
words. For each sentence, we calculate the
sentence weight by summing up the weights of
all words, normalized by the number of words in
the sentence. We sort the sentences in descending
order of their scores into a queue. To create a
summary, we iteratively dequeue one sentence,
check if the sentence is more than 8 words (as
in Erkan and Radev (2004)), then append it to
the current summary if it is non-redundant. A
sentence is considered non-redundant if it is not
similar to any sentences already in the summary,
measured by cosine similarity on binary vector
representations with stopwords excluded. We use
the cut-off of 0.5 for cosine similarity. This value
was tuned on the DUC 2003 dataset, by testing the
impact of the cut-off value on the ROUGE scores
for the final summary. Possible values ranged
from 0.1 to 0.9 with step of 0.1.
5.2 KL Divergence Summarizer
The KLSUM summarizer (Haghighi and
Vanderwende, 2009) aims at minimizing the KL
divergence between the probability distribution
over words estimated from the summary and
the input respectively. This summarizer is a
component of the popular topic model approaches
(Daum?e and Marcu, 2006; Celikyilmaz and
Hakkani-T?ur, 2011; Mason and Charniak, 2011)
and achieves competitive performance with
minimal differences compared to a full-blown
topic model system.
6 Global Indicators from NYT
Some words evoke topics that are of intrinsic
interest to people. Here we search for global
indicators of word importance regardless of
particular input.
6.1 Global Indicators of Word Importance
We analyze a large corpus of original documents
and corresponding summaries in order to identify
words that consistently get included in or excluded
from the summary. In the 2004-2007 NYT corpus,
many news articles have abstracts along with the
original article, which makes it an appropriate
714
Metric Top-30 words
KL(A ? G)(w) photo(s), pres, article, column, reviews, letter, York, Sen, NY, discusses, drawing, op-ed, holds, Bush
correction, editorial, dept, city, NJ, map, corp, graph, contends, Iraq, John, dies, sec, state, comments
KL(G ? A)(w) Mr, Ms, p.m., lot, Tuesday, CA, Wednesday, Friday, told, Monday, time, a.m., added, thing, Sunday
things, asked, good, night, Saturday, nyt, back, senator, wanted, kind, Jr., Mrs, bit, looked, wrote
Pr
A
(w) photo, photos, article, York, column, letter, Bush, state, reviews, million, American
pres, percent, Iraq, year, people, government, John, years, company, correction
national, federal, officials, city, drawing, billion, public, world, administration
Table 2: Top 30 words by three metrics from NYT corpus
resource to do such analysis. We identified
160, 001 abstract-original pairs in the corpus.
From these, we generate two language models,
one estimated from the text of all abstracts (LM
A
),
the other estimated from the corpus of original
articles (LM
G
). We use SRILM (Stolcke, 2002)
with Ney smoothing.
We denote the probability of wordw in LM
A
as
Pr
A
(w), the probability in LM
G
as Pr
G
(w), and
calculate the difference Pr
A
(w)?Pr
G
(w) and the
ratio Pr
A
(w)/Pr
G
(w) to capture the change of
probability. In addition, we calculate KL-like
weighted scores for words which reflect both the
change of probabilities between the two samples
and the overall frequency of the word. Here
we calculate both KL(A ? G) and KL(G ?
A). Words with high values for the former score
are favored in the summaries because they have
higher probability in the abstracts than in the
originals and have relatively high probability in
the abstracts. The later score is high for words that
are often not included in summaries.
KL(A ? G)(w) = Pr
A
(w) ? ln
Pr
A
(w)
Pr
G
(w)
KL(G ? A)(w) = Pr
G
(w) ? ln
Pr
G
(w)
Pr
A
(w)
Table 2 shows examples of the global
information captured from the three types
of scores?KL(A ? G), KL(G ? A) and
Pr
A
(w)?listing the 30 content words with
highest scores for each type. Words that tend to
be used in the summaries, characterized by high
KL(A ? G) scores, include locations (York, NJ,
Iraq), people?s names and titles (Bush, Sen, John),
some abbreviations (pres, corp, dept) and verbs of
conflict (contends, dies). On the other hand, from
KL(G ? A), we can see that it is unlikely for
writers to include courtesy titles (Mr, Ms, Jr.) and
relative time reference in summaries. The words
with high Pr
A
(w) scores overlaps with those
ranked highly by KL(A ? G) to some extent,
but also includes a number of generally frequent
words which appeared often both in the abstracts
and original texts, such as million and percent.
6.2 Blind Sentence Extraction
In later sections we include the measures of
global word importance as a feature of our
regression model for predicting word weights for
summarization. Before turning to that, however,
we report the results of an experiment aimed to
confirm the usefulness of these features. We
present a system, BLIND, which uses only weights
assigned to words by KL(A ? G) from NYT,
without doing any analysis of the original input.
We rank all non-stopword words from the input
according to this score. The top k words are given
weight 1, while the others are given weight 0.
The summaries are produced following the greedy
procedure described in Section 5.1.
Systems R-1 R-2 R-4
RANDOM 30.32 4.42 0.36
BLIND (80 keywords) 30.77 5.18 0.53
BLIND (300 keywords) 32.91 5.94 0.61
LASTESTLEAD 31.39 6.11 0.63
FIRST-SENTENCE 34.26 7.22 1.21
Table 3: Blind sentence extraction system,
compared with three baseline systems (%)
Table 3 shows that the BLIND system has R-2
recall of 0.0594 using the top 300 keywords,
significantly better than picking sentences from
the input randomly. It also achieves comparable
performance with the baseline in DUC 2004,
formed by selecting the first 100 words from
the latest article in the input (LASTESTLEAD).
However it is significantly worse than another
baseline of selecting the first sentences from the
input. Table 4 gives sample summaries generated
by these three approaches. These results confirm
that the information gleaned from the analysis
715
Random Summary
It was sunny and about 14 degrees C(57 degrees F) in Tashkent on Sunday. The president is a strong person, and he has been
through far more difficult political situations, Mityukov said, according to Interfax. But Yeltsin?s aides say his first term,
from 1991 to 1996, does not count because it began six months before the Soviet Union collapsed and before the current
constitution took effect. He must stay in bed like any other person, Yakushkin said. The issue was controversial earlier this
year when Yeltsin refused to spell out his intentions and his aides insisted he had the legal right to seek re-election.
NYT Summary from global keyword selection, KL(A ? G), k = 300
Russia?s constitutional court opened hearings Thursday on whether Boris Yeltsin can seek a third term. Yeltsin?s growing
health problems would also seem to rule out another election campaign. The Russian constitution has a two-term limit for
presidents. Russian president Boris Yeltsin cut short a trip to Central Asia on Monday due to a respiratory infection that
revived questions about his overall health and ability to lead Russia through a sustained economic crisis. The upper house of
parliament was busy voting on a motion saying he should resign. The start of the meeting was shown on Russian television.
First Sentence Generated Summary
President Boris Yeltsin has suffered minor burns on his right hand, his press office said Thursday. President Boris Yeltsin?s
doctors have pronounced his health more or less normal, his wife Naina said in an interview published Wednesday. President
Boris Yeltsin, on his first trip out of Russia since this spring, canceled a welcoming ceremony in Uzbekistan on Sunday
because he wasn?t feeling well, his spokesman said. Doctors ordered Russian President Boris Yeltsin to cut short his Central
Asian trip because of a respiratory infection and he agreed to return home Monday, a day earlier than planned, officials said.
Table 4: Summary comparison by Random, Blind Extraction and First Sentence systems
of NYT abstract-original pairs encodes highly
relevant information about important content
independent of the actual text of the input.
7 Regression-Based Keyword Extraction
Here we introduce a logistic regression model
for assigning importance weights to words in the
input. Crucially, this model combines evidence
from multiple indicators of importance. We have
at our disposal abundant data for learning because
each content word in the input can be treated as
a labeled instance. There are in total 32, 052
samples from the 30 inputs of DUC 2003 for
training, 54, 591 samples from the 50 inputs of
DUC 2004 for testing. For a word in the input,
we assign label 1 if the word appears in at least
one of the four human summaries for this input.
Otherwise we assign label 0.
In the rest of this section, we describe the rich
variety of features included in our system. We also
analyze and discuss the predictive power of those
features by performing Wilcoxon signed-rank test
on the DUC 2003 dataset. There are in total 9, 261
features used, among them 1, 625 are significant
(p-value < 0.05). We rank these features in
increasing p-values derived from Wilcoxon test.
Apart from the widely used features of word
frequency and positions, some other less explored
features are highly significant.
7.1 Frequency Features
We use the Probability, LLR chi-square statistic
value and MRW scores as features. Since prior
work has demonstrated that for LLR weights in
particular, it is useful to identify a small set of
important words and ignore all other words in
summary selection (Gupta et al., 2007), we use
a number of keyword indicators as features. For
these indicators, the value of feature is 1 if the
word is ranked within top k
i
, 0 otherwise. Here k
i
are preset cutoffs
5
. These cutoffs capture different
possibilities for defining the keywords in the input.
We also add the number of input documents that
contain the word as a feature. There are a total of
100 features in this group, all of which are highly
significant, ranked among the top 200.
7.2 Standard features
We now describe some standard features which
have been applied in prior work on summarization.
Word Locations: Especially in news articles,
sentences that occur at the beginning are often the
most important ones. In line with this observation,
we calculate several features related to the position
in which a word appears. We first compute
the relative positions for word tokens, where
the tokens are numbered sequentially in order of
appearance in each document in the input. The
relative position for one word token is therefore
its corresponding number, divided by total number
of tokens minus one in the document, e.g., 0
for the first token, 1 for the last token. For
each word, we calculate its earliest first location,
latest last location, average location and average
first location for tokens of this word across all
documents in the input. In addition we have a
binary feature indicating if the word appears in the
5
10, 15, 20, 30, 40, ? ? ? , 190, 200, 220, 240, 260, 280,
300, 350, 400, 450, 500, 600, 700 (in total 33 values)
716
first sentence and the number of times it appears
in a first sentence among documents in one input.
There are 6 features in this group. All of them are
very significant, ranked within the top 100.
Word type: These features include Part of
Speech (POS) tags, Name Entity (NE) labels and
capitalization information. We use the Stanford
POS-Tagger (Toutanova et al., 2003) and Name
Entity Recognizer (Finkel et al., 2005). We have
one feature corresponding to each possible POS
and NE tag. The value of this feature is the
proportion of occurrences of the word with this
tag; in most cases only one feature gets a non-zero
value. We have two features which indicate if
one word has been capitalized and the ratio of its
capitalized occurrences.
Most of the NE features (6 out of 8) are
significant: there are more Organizations and
Locations but fewer Time and Date words in the
human summaries. Of the POS tags, 11 out of 41
are significant: there are more nouns (NN, NNS,
NNPS); fewer verbs (VBG, VBP, VB) and fewer
cardinal numbers in the abstracts compared to the
input. Capitalized words also tend to be included
in human summaries.
KL: Prior work has shown that having estimates
of sentence importance can also help in estimating
word importance (Wan et al., 2007; Liu et al.,
2011; Wei et al., 2008). The summarizer based
on KL-divergence assigns importance to sentences
directly, in a complex function according to the
word distribution in the sentence. Therefore,
we use these summaries as potential indicators
of word importance. We include two features
here, the first one indicates if the word appears
in a KLSUM summary of the input, as well as
a feature corresponding to the number of times
the word appeared in that summary. Both of the
features are highly significant, ranked within the
top 200.
7.3 NYT-weights as Features
We include features from the relative rank of
a word according to KL(A ? G), KL(G ?
A), Pr
A
(w)?Pr
G
(w), Pr
A
(w)/Pr
G
(w) and
Pr
A
(w), derived from the NYT as described in
Section 6. If the rank of a word is within top-k
or bottom-k by one metric, we would label it as
1, where k is selected from a set of pre-defined
values
6
. We have in total 70 features in this
6
100, 200, 500, 1000, 2000, 5000, 10000 in this case.
category, of which 56 are significant, 47 having
a p-value less than 10
?7
. The predictive power of
those global indicators are only behind the features
which indicates frequency and word positions.
7.4 Unigrams
This is a binary feature corresponding to each
of the words that appeared at least twice in the
training data. The idea is to learn which words
from the input tend to be mentioned in the human
summaries. There are in total 8, 691 unigrams,
among which 1, 290 are significant. Despite the
high number of significant unigram features, most
of them are not as significant as the more general
ones we described so far. It is interesting to
compare the significant unigrams identified in the
DUC abstract/input data with those derived from
the NYT corpus. Unigrams that tend to appear in
DUC summaries include president, government,
political. We also find the same unigrams among
the top words from NYT corpus according to
KL(A ? G) . As for words unlikely to appear in
summaries, we see Wednesday, added, thing, etc,
which again rank high according to KL(G ? A).
7.5 Dictionary Features: MPQA and LIWC
Unigram features are notoriously sparse. To
mitigate the sparsity problem, we resort to
more general groupings to words according to
salient semantic and functional categories. We
employ two hand-crafted dictionaries, MPQA for
subjectivity analysis and LIWC for topic analysis.
The MPQA dictionary (Wiebe and Cardie,
2005) contains words with different polarities
(positive, neutral, negative) and intensities (strong,
weak). The combinations correspond to six
features. It turns out that words with strong
polarity, either positive or negative, are seldomly
included in the summaries. Most strikingly,
the p-value from significance test for the strong
negative words is less than 10
?4
?these words
are rarely included in summaries. There is no
significant difference on weak polarity categories.
Another dictionary we use is LIWC (Tausczik
and Pennebaker, 2007), which contains manually
constructed dictionaries for multiple categories
of words. The value of the feature is 1 for
one word if the word appears in the particular
dictionary for the category. 34 out of 64 LIWC
features are significant. Interesting categories
which appear at higher rate in summaries include
events about death, anger, achievements, money
717
and negative emotions. Those that appear at lower
rate in the summaries include auxiliary verbs, hear,
pronouns, negation, function words, social words,
swear, adverbs, words related to families, etc.
7.6 Context Features
We use context features here, based on the
assumption that context importance around a word
affects the importance of this word. For context
we consider the words before and after the target
word. We extend our feature space by calculating
the weighted average of the feature values of the
context words. For word w, we denote L
w
as the
set of words before w, R
w
as the set of words
after w. We denote the feature for one word as
w.f
i
, the way of calculating the newly extended
word-before feature w.l
f
i
could be written as:
w.l
f
i
=
?
i
p(w
l
) ? w
l
.f
i
, ?w
l
? L
w
Here p(w
l
) is the probability word w
l
appears
before w among all words in L
w
.
For context features, we calculate the weighted
average of the most widely used basic features,
including frequency, location and capitalization
for surrounding contexts. There are in total
220 features of this kind, among which 117 are
significant, 74 having a p-value less than 10
?4
.
8 Experiments
The performance of our logistic regression model
is evaluated on two tasks: keyword identification
and extractive summarization. We name our
system REGSUM.
8.1 Regression for Keyword Identification
For each input, we define the set of keywords
as the top k words according to the scores
generated from different models. We compare
our regression system with three unsupervised
systems: PROB, LLR, MRW. To show the
effectiveness of new features, we compare our
results with a regression system trained only
on word frequency and location related features
described in Section 7. Those features are the
ones standardly used for ranking the importance
of words in recent summarization works (Yih et
al., 2007; Takamura and Okumura, 2009; Sipos et
al., 2012), and we name this system REGBASIC.
Figure 1 shows the performance of systems
when selecting the 100 words with highest weights
Figure 1: Precision, Recall and F-score of
keyword identification, 100 words selected, G
1
as
gold-standard
as keywords. Each word from the input that
appeared in any of the four human summaries is
considered as a gold-standard keyword. Among
the unsupervised approaches, word probability
identifies keywords better than LLR and MRW
by at least 4% on F-score. REGBASIC does not
give better performance at keyword identification
compared with PROB, even though it includes
location information. Our system gets 2.2%
F-score improvement over PROB, 5.2% over
REGBASIC, and more improvement over the
other approaches. All of these improvements are
statistically significant by Wilcoxon test.
Table 5 shows the performance of keyword
identification for different G
i
and different
number of keywords selected. The regression
system has no advantage over PROB when
identifying keywords that appeared in all of the
four human summaries. However our system
achieves significant improvement for predicting
words that appeared in more than one or two
human summaries.
7
8.2 Regression for Summarization
We now show that the performance of extractive
summarization can be improved by better
estimation of word weights. We compare our
regression system with the four models introduced
in Section 8.1. We also include PEER-65, the best
system in DUC-2004, as well as KLSUM for
comparison. Apart from these, we compare our
model with two state-of-the-art systems, including
the submodular approach (SUBMOD) (Lin and
7
We also apply a weighted keyword evaluation approach,
similar to the pyramid method for summarization. Still
our system shows significant improvement over the others.
See https://www.seas.upenn.edu/~hongkai1/regsum.html for
details.
718
Gi
#words PROB LLR MRW REGBASIC REGSUM
G
1
80 43.6 37.9 38.9 39.9 45.7
G
1
100 44.3 38.7 39.2 41.0 46.5
G
1
120 44.6 38.5 39.2 40.9 46.4
G
2
30 47.8 44.0 42.4 47.4 50.2
G
2
35 47.1 43.3 42.1 47.0 49.5
G
2
40 46.5 42.4 41.8 46.4 49.2
G
3
10 51.2 46.2 43.8 46.9 50.2
G
3
15 51.4 47.5 43.7 49.8 52.9
G
3
20 49.7 47.6 42.5 49.3 51.5
G
4
5 50.0 48.8 44.9 43.6 45.1
G
4
6 51.4 46.9 43.7 45.2 47.6
G
4
7 50.9 48.2 43.7 45.8 47.8
Table 5: Keyword identification F-score (%) for different G
i
and different number of words selected.
Bilmes, 2012) and the determinantal point process
(DPP) summarizer (Kulesza and Taskar, 2012).
The summaries were kindly provided by the
authors of these systems (Hong et al., 2014).
As can been seen in Table 6, our system
outperforms PROB, LLR, MRW, PEER-65,
KLSUM and REGBASIC. These improvements
are significant on ROUGE-2 recall. Interestingly,
although the supervised system REGBASIC which
uses only frequency and positions achieve
low performance in keyword identification, the
summaries it generates are of high quality. The
inclusion of position features negatively affects the
performance in summary keyword identification
but boosts the weights for the words which appear
close to the beginning of the documents, which is
helpful for identifying informative sentences. By
including other features we greatly improve over
REGBASIC in keyword identification. Similarly
here the richer set of features results in better
quality summaries.
We also examined the ROUGE-1, -2, -4
recall compared with the SUBMOD and DPP
summarizers
8
. There is no significant difference
on R-2 and R-4 recall compared with these
two state-of-the-art systems. DPP performed
significantly better than our system on R-1 recall,
but that system is optimizing on R-1 F-score in
training. Overall, our conceptually simple system
is on par with the state of the art summarizers and
points to the need for better models for estimating
word importance.
8
The results are slightly different from the ones reported
in the original papers due to the fact that we truncated to 100
words, while they truncated to 665 bytes.
System R-1 R-2 R-4
PROB 35.14 8.17 1.06
LLR 34.60 7.56 0.83
MRW 35.78 8.15 0.99
REGBASIC 37.56 9.28 1.49
KL 37.97 8.53 1.26
PEER-65 37.62 8.96 1.51
SUBMOD 39.18 9.35 1.39
DPP 39.79 9.62 1.57
REGSUM 38.57 9.75 1.60
Table 6: System performance comparison (%)
9 Conclusion
We presented a series of experiments which
show that keyword identification can be improved
in a supervised framework which incorporates
a rich set of indicators of importance. We
also show that the better estimation of word
importance leads to better extractive summaries.
Our analysis of features related to global
importance, sentiment and topical categories
reveals rather unexpected results and confirms that
word importance estimation is a worthy research
direction. Success in the task is likely to improve
sophisticated summarization approaches too, as
well as sentence compression systems which use
only crude frequency related measures to decide
which words should be deleted from a sentence.
9
9
The work is partially funded by NSF CAREER award
IIS 0953445.
719
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010.
A hybrid hierarchical model for multi-document
summarization. In Proceedings of ACL, pages
815?824.
Asli Celikyilmaz and Dilek Hakkani-T?ur. 2011.
Discovery of topically coherent sentences for
extractive summarization. In Proceedings of
ACL-HLT, pages 491?499.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of COLING/ACL, pages 152?159.
Hal Daum?e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
ACL, pages 305?312.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457?479.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in
multi-sentence text extraction. In Proceedings of
COLING.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In Proceedings of ACL, pages
363?370.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2007.
English gigaword third edition. Linguistic Data
Consortium, Philadelphia, PA.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.
2007. Measuring importance and query relevance
in topic-focused multi-document summarization. In
Proceedings of ACL, pages 193?196.
Aria Haghighi and Lucy Vanderwende. 2009.
Exploring content models for multi-document
summarization. In Proceedings of HLT-NAACL,
pages 362?370.
Sanda Harabagiu and Finley Lacatusu. 2005. Topic
themes for multi-document summarization. In
Proceedings of SIGIR 2005, pages 202?209.
Kai Hong, John M. Conroy, Benoit Favre, Alex
Kulesza, Hui Lin, and Ani Nenkova. 2014. A
repositary of state of the art and competitive baseline
summaries for generic news summarization. In
Proceedings of LREC, May.
Alex Kulesza and Ben Taskar. 2012. Determinantal
point processes for machine learning. Foundations
and Trends in Machine Learning, 5(2?3).
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR, pages 68?73.
Page Lawrence, Brin Sergey, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation
ranking: Bringing order to the web. Technical
report, Stanford University.
Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In UAI, pages 479?490.
Chin-Yew Lin and Eduard Hovy. 2000. The
automated acquisition of topic signatures for text
summarization. In Proceedgins of COLING, pages
495?501.
Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Text
Summarization Branches Out: Proceedings of the
ACL-04 Workshop, pages 74?81.
Marina Litvak, Mark Last, and Menahem Friedman.
2010. A new approach to improving multilingual
summarization using a genetic algorithm. In
Proceedings of ACL, pages 927?936.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised
framework for keyword extraction from meeting
transcripts. Transactions on Audio Speech and
Language Processing, 19(3):538?548.
H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Research and
Development, 2(2):159?165, April.
M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating Typed Dependency Parses from Phrase
Structure Parses. In Proceedings of LREC-06, pages
449?454.
Rebecca Mason and Eugene Charniak. 2011.
Extractive multi-document summaries should
explicitly not contain document-specific content.
In Proceedings of the Workshop on Automatic
Summarization for Different Genres, Media, and
Languages, pages 49?54.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of ECIR, pages 557?564.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of EMNLP,
pages 404?411.
Ani Nenkova and Lucy Vanderwende. 2005. The
impact of frequency on summarization. Technical
report, Microsoft Research.
720
Ani Nenkova, Lucy Vanderwende, and Kathleen
McKeown. 2006. A compositional context sensitive
multi-document summarizer: exploring the factors
that influence summarization. In Proceedings of
SIGIR, pages 573?580.
Jahna Otterbacher, G?unes Erkan, and Dragomir R.
Radev. 2009. Biased lexrank: Passage
retrieval using random walks with question-based
priors. Information Processing and Management,
45(1):42?54.
Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Inf. Process. Manage., 43(6):1506?1520.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in
summarization. In NAACL-HLT 2012: Workshop
on Evaluation Metrics and System Comparison for
Automatic Summarization, pages 1?9.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine
summarization systems. In Proceedings of EMNLP,
pages 467?473.
Korbinian Riedhammer, Beno??t Favre, and Dilek
Hakkani-T?ur. 2010. Long story short -
global unsupervised models for keyphrase based
meeting summarization. Speech Communication,
52(10):801?815.
G. Salton. 1971. The SMART Retrieval System:
Experiments in Automatic Document Processing.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
PA.
Chao Shen and Tao Li. 2010. Multi-document
summarization via the minimum dominating set. In
Proceedings of Coling, pages 984?992.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin learning of
submodular summarization models. In Proceedings
of EACL, pages 224?233.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of
ICSLP, volume 2, pages 901?904.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL,
pages 781?789.
Yla R Tausczik and James W Pennebaker. 2007.
The Psychological Meaning of Words: LIWC and
Computerized Text Analysis Methods. Journal of
Language and Social Psychology, 29:24?54.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the NAACL-HLT, pages
173?180.
XiaojunWan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR, pages 299?306.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007. Towards an iterative reinforcement approach
for simultaneous document summarization and
keyword extraction. In Proceedings of ACL, pages
552?559.
Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He. 2008.
Query-sensitive mutual reinforcement chain and
its application in query-oriented multi-document
summarization. In Proceedings of SIGIR, pages
283?290.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities), page 1(2).
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document
summarization by maximizing informative
content-words. In Proceedings of IJCAI, pages
1776?1782.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings
of SIGIR, pages 113?120.
721
