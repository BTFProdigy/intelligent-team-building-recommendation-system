Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474?1480,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Leveraging Effective Query Modeling Techniques  
for Speech Recognition and Summarization 
 
 Kuan-Yu Chen*?,  Shih-Hung Liu*, Berlin Chen#, Ea-Ee Jan+,  
Hsin-Min Wang*, Wen-Lian Hsu*, and Hsin-Hsi Chen? 
*Institute of Information Science, Academia Sinica, Taiwan 
?National Taiwan University, Taiwan 
#National Taiwan Normal University, Taiwan 
+IBM Thomas J. Watson Research Center, USA 
{kychen, journey, whm, hsu}@iis.sinica.edu.tw, 
berlin@ntnu.edu.tw, hhchen@csie.ntu.edu.tw, ejan@us.ibm.com 
 
Abstract 
Statistical language modeling (LM) that 
purports to quantify the acceptability of a 
given piece of text has long been an in-
teresting yet challenging research area. In 
particular, language modeling for infor-
mation retrieval (IR) has enjoyed re-
markable empirical success; one emerg-
ing stream of the LM approach for IR is 
to employ the pseudo-relevance feedback 
process to enhance the representation of 
an input query so as to improve retrieval 
effectiveness. This paper presents a con-
tinuation of such a general line of re-
search and the main contribution is three-
fold. First, we propose a principled 
framework which can unify the relation-
ships among several widely-used query 
modeling formulations. Second, on top of 
the successfully developed framework, 
we propose an extended query modeling 
formulation by incorporating critical que-
ry-specific information cues to guide the 
model estimation. Third, we further adopt 
and formalize such a framework to the 
speech recognition and summarization 
tasks. A series of empirical experiments 
reveal the feasibility of such an LM 
framework and the performance merits of 
the deduced models on these two tasks. 
1 Introduction 
Along with the rapidly growing popularity of the 
Internet and the ubiquity of social web commu-
nications, tremendous volumes of multimedia 
contents, such as broadcast radio and television 
programs, digital libraries and so on, are made 
available to the public. Research on multimedia 
content understanding and organization has wit-
nessed a booming interest over the past decade. 
By virtue of the developed techniques, a variety 
of functionalities were created to help distill im-
portant content from multimedia collections, or 
provide locations of important speech segments 
in a video accompanied with their corresponding 
transcripts, for users to listen to or to digest. Sta-
tistical language modeling (LM) (Jelinek, 1999; 
Jurafsky and Martin, 2008; Zhai, 2008), which 
manages to quantify the acceptability of a given 
word sequence in a natural language or capture 
the statistical characteristics of a given piece of 
text, has been proved to offer both efficient and 
effective modeling abilities in many practical 
applications of natural language processing and 
speech recognition (Ponte and Croft, 1998; Jelin-
ek, 1999; Huang, et al., 2001; Zhai and Lafferty, 
2001a; Jurafsky and Martin, 2008; Furui et al., 
2012; Liu and Hakkani-Tur, 2011). 
The LM approach was first introduced for the 
information retrieval (IR) problems in the late 
1990s, indicating very good potential, and was 
subsequently extended in a wide array of follow-
up studies. One typical realization of the LM ap-
proach for IR is to access the degree of relevance 
between a query and a document by computing 
the likelihood of the query generated by the doc-
ument (usually referred to as the query-
likelihood approach) (Zhai, 2008; Baeza-Yates 
and Ribeiro-Neto, 2011). A document is deemed 
to be relevant to a given query if the correspond-
ing document model is more likely to generate 
the query. On the other hand, the Kullback-
Leibler divergence measure (denoted by KLM 
for short hereafter), which quantifies the degree 
of relevance between a document and a query 
from a more rigorous information-theoretic per-
spective, has been proposed (Lafferty and Zhai, 
2001; Zhai and Lafferty, 2001b; Baeza-Yates and 
Ribeiro-Neto, 2011). KLM not only can be 
thought as a natural generalization of the query-
likelihood approach, but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the performance of doc-
ument ranking. For example, a main challenge 
facing such a measure is that since a given query 
usually consists of few words, the true infor-
mation need is hard to be inferred from the sur-
face statistics of a query. As such, one emerging 
stream of thought for KLM is to employ the 
1474
pseudo-relevance feedback process to construct 
an enhanced query model (or representation) so 
as to achieve better retrieval effectiveness (Hi-
emstra et al., 2004; Lv and Zhai, 2009; Carpineto 
and Romano, 2012; Lee and Croft, 2013). 
Following this line of research, the major con-
tribution of this paper is three-fold: 1) we ana-
lyze several widely-used query models and then 
propose a principled framework to unify the rela-
tionships among them; 2) on top of the success-
fully developed query models, we propose an 
extended modeling formulation by incorporating 
additional query-specific information cues to 
guide the model estimation; 3) we explore a nov-
el use of these query models by adapting them to 
the speech recognition and summarization tasks. 
As we will see, a series of experiments indeed 
demonstrate the effectiveness of the proposed 
models on these two tasks. 
2 Language Modeling Framework 
2.1 Kullback-Leibler Divergence Measure 
A promising realization of the LM approach to 
IR is the Kullback-Leibler divergence measure 
(KLM), which determines the degree of rele-
vance between a document and a query from a 
rigorous information-theoretic perspective. Two 
different language models are involved in KLM: 
one for the document and the other for the query. 
The divergence of the document model with re-
spect to the query model is defined by  
.)|( )|(log)|()||(KL ? ?? Vw DwP QPQwPDQ
  (1)  
KLM not only can be thought as a natural gener-
alization of the traditional query-likelihood ap-
proach (Yi and Allan, 2009; Baeza-Yates and 
Ribeiro-Neto, 2011), but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the estimation of its 
component models in a systematic way for better 
document ranking (Zhai, 2008).  
Due to that a query usually consists of only a 
few words, the true query model P(w|Q)
 
might 
not be accurately estimated by the simple ML 
estimator (Jelinek, 1991). There are several stud-
ies devoted to estimating a more accurate query 
modeling, saying that it can be approached with 
the pseudo-relevance feedback process (Lavren-
ko and Croft, 2001; Zhai and Lafferty, 2001b). 
However, the success depends largely on the as-
sumption that the set of top-ranked documents, 
DTop={D1,D2,...,Dr,...}, obtained from an initial 
round of retrieval, are relevant and can be used to 
estimate a more accurate query language model. 
2.2 Relevance Modeling  
Under the notion of relevance modeling (RM, 
often referred to as RM-1), each query Q is as-
sumed to be associated with an unknown rele-
vance class RQ, and documents that are relevant 
to the semantic content expressed in query are 
samples drawn from the relevance class RQ. 
Since there is no prior knowledge about RQ, we 
may use the top-ranked documents DTop to ap-
proximate the relevance class RQ. The corre-
sponding relevance model can be estimated using 
the following equation (Lavrenko and Croft, 
2001; Lavrenko, 2004): 
.)|()(
)|()|()(  )|(RM ? ? ??
? ? ??
???
?
?????
??
ToprD
ToprD
Qw rr
Qw rrr
DwPDP
DwPDwPDPQwP
D
D
(2) 
2.3 Simple Mixture Model 
Another perspective of estimating an accurate 
query model with the top-ranked documents is 
the simple mixture model (SMM), which as-
sumes that words in DTop are drawn from a two-
component mixture model: 1) One component is 
the query-specific topic model PSMM(w|Q), and 2) 
the other is a generic background model 
P(w|BG). By doing so, the SMM model 
PSMM(w|Q) can be estimated by maximizing the 
likelihood over all the top-ranked documents 
(Zhai and Lafferty, 2001b; Tao and Zhai, 2006): 
? ? ,)|()1()|( ),(SMM?? ?? ????? Topr rD Vw DwcBGwPQwPL D ??
(3) 
where ?  is a pre-defined weighting parameter 
used to control the degree of reliance between 
PSMM(w|Q) and P(w|BG). This estimation will 
enable more specific words to receive more 
probability mass, thereby leading to a more dis-
criminative query model PSMM(w|Q). 
Although the SMM modeling aims to extract 
extra word usage cues for enhanced query mod-
eling, it may confront two intrinsic problems. 
One is the extraction of word usage cues from 
DTop is not guided by the original query. The oth-
er is that the mixing coefficient ?  is fixed across 
all top-ranked documents albeit that different 
documents would potentially contribute different 
amounts of word usage cues to the enhanced 
query model. To mitigate these two problems, 
the regularized simple mixture model has been 
proposed and can be estimated by maximizing 
the likelihood function (Tao and Zhai, 2006; Dil-
lon and Collins-Thompson, 2010) 
? ? ,)|()1()|(    
)|(
),(
RSMM
)|(
RSMM
?
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
Dwc
DD
Vw
QwP
BGwPQwP
QwPL
D
??
?
(4) 
where   is a weighting factor indicating the con-
fidence on the prior information. 
3 The Proposed Modeling Framework 
3.1 Fundamentals 
It is obvious that the major difference among the 
1475
representative query models mentioned above is 
how to capitalize on the set of top-ranked docu-
ments and the original query. Several subtle rela-
tionships can be deduced through the following 
in-depth analysis. First, a direct inspiration of the 
LM-based query reformulation framework can 
be drawn from the celebrated Rocchio?s formula-
tion, while the former can be viewed as a proba-
bilistic counterpart of the latter (Robertson, 1990; 
Ponte and Croft, 1998; Baeza-Yates and Ribeiro-
Neto, 2011). Second, after some mathematical 
manipulation, the formulation of the RM model 
(c.f. Eq. (2)) can be rewritten as 
.)()|(
)()|()|(  )|(RM ? ? ???? ????? ToprD ToprD rr
rrr DPDQP
DPDQPDwPQwP D D
(5) 
It becomes evident that the RM model is com-
posed by mixing a set of document models 
P(w|Dr). As such, the RM model bears a close 
resemblance to the Rocchio?s formulation. Fur-
thermore, based on Eq. (5), we can recast the 
estimation of the RM model as an optimization 
problem, and the likelihood (or objective) func-
tion is formulated as 
1)|( ..
,)|()|(
),(
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Topr
Topr
D
r
Vw
Qwc
D
rr
QDPts
QDPDwPL
D
D
  (6) 
where the document models P(w|Dr) are known 
in advance; the conditional probability P(Dr|Q) 
of each document Dr is unknown and leave to be 
estimated. Finally, a principled framework can 
be obtained to unify all of these query models, 
including RM (c.f. Eq. (6)), SMM (c.f. Eq. (3)) 
and RSMM (c.f. Eq. (4))), by using a generalized 
objective likelihood function: 
1)( ..
,)()|(
),(
?
???
?
???
?
?
?
?
?
?
?
?
?
?
M
E M
r
i
i
r
M
r
Vw E
Ewc
M
rr
MPts
MPMwPL  (7) 
where E represents a set of observations which 
we want to maximize their likelihood, and M 
denotes a set of mixture components.  
3.2 Query-specific Mixture Modeling 
The SMM model and the RSMM model are in-
tended to extract useful word usage cues from 
DTop, which are not only relevant to the original 
query Q but also external to those already cap-
tured by the generic background model. Howev-
er, we argue in this paper that the ?generic in-
formation? should be carefully crafted for each 
query due mainly to the fact that users? infor-
mation needs may be very diverse from one an-
other. To crystallize the idea, a query-specific 
background model PQ(w|BG) for each query Q 
can be derived from DTop directly. Another con-
sideration is that since the original query model 
P(w|Q) cannot be accurately estimated, it thus 
may not necessarily be the best choice for use in 
defining a conjugate Dirichlet prior for the en-
hanced query model to be estimated. We propose 
to use the RM model as a prior to guide the esti-
mation of the enhanced query model. The en-
hanced query model is termed query-specific 
mixture model (QMM), and its corresponding 
training objective function can be expressed as 
? ??
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
DwcQDD
Vw
QwP
BGwPQwP
QwPL
D
.)|()1()|(    
)|(
),(QMM
)|(QMM RM
??
?
 (8) 
4 Applications 
4.1 Speech Recognition 
Language modeling is a critical and integral 
component in any large vocabulary continuous 
speech recognition (LVCSR) system (Huang et 
al., 2001; Jurafsky and Martin, 2008; Furui et al., 
2012). More concretely, the role of language 
modeling in LVCSR can be interpreted as calcu-
lating the conditional probability P(w|H), in 
which H is a search history, usually expressed as 
a sequence of words H=h1, h2,?, hL, and w is 
one of its possible immediately succeeding 
words. Once the various aforementioned query 
modeling methods are applied to speech recogni-
tion, for a search history H, we can conceptually 
regard it as a query and each of its immediately 
succeeding words w as a (single-word) document. 
Then, we may leverage an IR procedure that 
takes H as a query and poses it to a retrieval sys-
tem to obtain a set of top-ranked documents from 
a contemporaneous (or in-domain) corpus. Final-
ly, the enhanced query model (that is P(w|H) in 
speech recognition) can be estimated by RM, 
SMM, RSMM or QM , and further combined 
with the background n-gram (e.g., trigram) lan-
guage model to form an adaptive language model 
to guide the speech recognition process. 
4.2 Speech Summarization 
On the other hand, extractive speech summariza-
tion aims at producing a concise summary by 
selecting salient sentences or paragraphs from 
the original spoken document according to a pre-
defined target summarization ratio (Carbonell 
and Goldstein, 1998; Mani and Maybury, 1999; 
Nenkova and McKeown, 2011; Liu and 
Hakkani-Tur, 2011). Intuitively, this task could 
be framed as an ad-hoc IR problem, where the 
spoken document is treated as an information 
need and each sentence of the document is re-
garded as a candidate information unit to be re-
trieved according to its relevance to the infor-
mation need. Therefore, KLM can be used to 
quantify how close the document D and one of 
its sentences S are: the closer the sentence model 
P(w|S) to the document model P(w|D), the more 
1476
likely the sentence would be part of the summary. 
Due to that each sentence S of a spoken docu-
ment D to be summarized usually consists of 
only a few words, the corresponding sentence 
model P(w|S) might not be appropriately esti-
mated by the ML estimation. To alleviate the 
deficiency, we can leverage the merit of the 
above query modeling techniques to estimate an 
accurate sentence model for each sentence to 
enhance the summarization performance. 
5 Experimental Setup 
The speech corpus consists of about 196 hours of 
Mandarin broadcast news collected by the Aca-
demia Sinica and the Public Television Service 
Foundation of Taiwan between November 2001 
and April 2003 (Wang et al., 2005), which is 
publicly available and has been segmented into 
separate stories and transcribed manually. Each 
story contains the speech of one studio anchor, as 
well as several field reporters and interviewees. 
A subset of 25-hour speech data compiled during 
November 2001 to December 2002 was used to 
bootstrap the acoustic model training. The vo-
cabulary size is about 72 thousand words. The 
background language model was estimated from 
a background text corpus consisting of 170 mil-
lion Chinese characters collected from the Chi-
nese Gigaword Corpus released by LDC. 
The dataset for use in the speech recognition 
experiments is compiled by a subset of 3-hour 
speech data from the corpus within 2003 (1.5 
hours for development and 1.5 hours for test). 
The contemporaneous (in-domain) text corpus 
used for training the various LM adaptation 
methods was collected between 2001 and 2003 
from the corpus (excluding the test set), which 
consists of one million Chinese characters of the 
orthographic broadcast news transcripts. In this 
paper, all the LM adaptation experiments were 
performed in word graph rescoring. The associ-
ated word graphs of the speech data were built 
beforehand with a typical LVCSR system (Ort-
manns et al., 1997; Young et al., 2006). 
In addition, the summarization task also em-
ploys the same broadcast news corpus as well. A 
subset of 205 broadcast news documents com-
piled between November 2001 and August 2002 
was reserved for the summarization experiments 
(185 for development and 20 for test). A subset 
of about 100,000 text news documents, compiled 
during the same period as the documents to be 
summarized, was employed to estimate the relat-
ed summarization models compared in this paper. 
We adopted three variants of the widely-used 
ROUGE metric (i.e., ROUGE-1, ROGUE-2 and 
ROUGE-L) for the assessment of summarization 
performance (Lin, 2003). The summarization 
ratio, defined as the ratio of the number of words 
in the automatic (or manual) summary to that in 
the reference transcript of a spoken document, 
was set to 10% in this research. 
6 Experimental Results 
In the first part of experiments, we evaluate the 
effectiveness of the various query models applied 
to the speech recognition task. The correspond-
ing results with respect to different numbers of 
top-ranked documents being used for estimating 
their component models are shown in Table 1. 
Also worth mentioning is that the baseline sys-
tem with the background trigram language model, 
which was trained with the SRILM toolkit 
(Stolcke, 2005) and Good-Turing smoothing 
(Jelinek, 1999), results in a Chinese character 
error rate (CER) of 20.08% on the test set. Con-
sulting Table 1 we notice two particularities. One 
is that there is more fluctuation in the CER re-
sults of SMM than in those of RM. The reason 
might be that, for SMM, the extraction of rele-
vance information from the top-ranked docu-
ments is conducted with no involvement of the 
test utterance (i.e., the query; or its correspond-
ing search histories), as elaborated earlier in Sec-
tion 2. When too many feedback documents are 
being used, there would be a concern for SMM 
to be distracted from being able to appropriate 
model the test utterance, which is probably 
caused by some dominant distracting (or irrele-
vant) feedback documents. The other interesting 
observation is that RSMM only achieves a com-
parable (even worse) result when compared to 
SMM. A possible reason is that the prior con-
straint of the RSMM may contain too much 
noisy information so as to bias the model estima-
tion. Furthermore, it is evident that the proposed 
QMM is the best-performing method among all 
the query models compared in the paper. Alt-
hough the improvements made by QMM are not 
as pronounced as expected, we believe that 
QMM has demonstrated its potential to be ap-
plied to other related applications. On the other 
hand, we compare the various query models with 
two well-practiced language models, namely the 
cache model (Cache) (Kuhn and Mori, 1990; 
Jelinek et al., 1991) and the latent Dirichlet allo-
cation (LDA) (Liu and Liu, 2007; Tam and 
Schultz, 2005). The CER results of these two 
models are also shown in Table 1, respectively. 
For the cache model, bigram cache was used 
since it can yield better results than the unigram 
and trigram cache models in our experiments. It 
is worthy to notice that the LDA model was 
trained with the entire set of contemporaneous 
text document collection (c.f. Section 4), while 
all of the query models explored in the paper 
were estimated based on a subset of the corpus 
selected by an initial round of retrieval. The re-
sults reveal that most of these query models can 
achieve superior performance over the two con-
ventional language models. 
1477
In the second part of experiments, we evaluate 
the utilities of the various query models as ap-
plied to the speech summarization task. At the 
outset, we assess the performance level of the 
baseline KLM method by comparison with two 
well-practiced unsupervised methods, viz. the 
vector space model (VSM) (Gong and Liu, 2001), 
and its extension, maximal marginal relevance 
(MMR) (Carbonell and Goldstein, 1998). The 
corresponding results are shown in Table 2 and 
can be aligned with several related literature re-
views. By looking at the results, we find that 
KLM outperforms VSM by a large margin, con-
firming the applicability of the language model-
ing framework for speech summarization. Fur-
thermore, MMR that presents an extension of 
VSM performs on par with KLM for the text 
summarization task (TD) and exhibits superior 
performance over KLM for the speech summari-
zation task (SD). We now turn to evaluate the 
effectiveness of the various query models (viz. 
RM, SMM, RSMM and QMM) in conjunction 
with the pseudo-relevance feedback process for 
enhancing the sentence model involved in the 
KLM method. The corresponding results are also 
shown in Table 2. Two noteworthy observations 
can be drawn from Table 2. One is that all these 
query models can considerably improve the 
summarization performance of the KLM method, 
which corroborates the advantage of using them 
for enhanced sentence representations. The other 
is that QMM is the best-performing one among 
all the formulations studied in this paper for both 
the TD and SD cases.  
Going one step further, we explore to use extra 
prosodic features that are deemed complemen-
tary to the LM cue provided by QMM for speech 
summarization. To this end, a support vector ma-
chine (SVM) based summarization model is 
trained to integrate a set of 28 commonly-used 
prosodic features (Liu and Hakkani-Tur, 2011) 
for representing each spoken sentence, since 
SVM is arguably one of the state-of-the-art su-
pervised methods that can make use of a diversi-
ty of indicative features for text or speech sum-
marization (Xie and Liu, 2010; Chen et al., 
2013). The sentence ranking scores derived by 
QMM and SVM are in turn integrated through a 
simple log-linear combination. The correspond-
ing results are shown in Table 2, demonstrating 
consistent improvements with respect to all the 
three variants of the ROUGE metric as compared 
to that using either QMM or SVM in isolation. 
We also investigate using SVM to additionally 
integrate a richer set of lexical and relevance fea-
tures to complement QMM and further enhance 
the summarization effectiveness. However, due 
to space limitation, we omit the details here. As a 
side note, there is a sizable gap between the TD 
and SD cases, indicating room for further im-
provements. We may seek remedies, such as ro-
bust indexing schemes, to compensate for imper-
fect speech recognition. 
7 Conclusion and Outlook 
In this paper, we have presented a systematic and 
thorough analysis of a few well-practiced query 
models for IR and extended their novel applica-
bility to speech recognition and summarization in 
a principled way. Furthermore, we have pro-
posed an extension of this research line by intro-
ducing query-specific mixture modeling; the util-
ities of the deduced model have been extensively 
compared with several existing query models. As 
to future work, we would like to investigate 
jointly integrating proximity and other different 
kinds of relevance and lexical/semantic infor-
mation cues into the process of feedback docu-
ment selection so as to improve the empirical 
effectiveness of such query modeling.  
Acknowledgements 
This research is supported in part by the ?Aim 
for the Top University Project? of National Tai-
wan Normal University (NTNU), sponsored by 
the Ministry of Education, Taiwan, and by the 
Ministry of Science and Technology, Taiwan, 
under Grants MOST 103-2221-E-003-016-MY2, 
NSC 101-2221-E-003-024-MY3, NSC 102-
2221-E-003-014-, NSC 101-2511-S-003-057-
MY3, NSC 101-2511-S-003-047-MY3 and NSC 
103-2911-I-003-301. 
  
Table 1. The speech recognition results (in CER 
(%)) achieved by various language models along 
with different numbers of latent topics/pseudo-
relevance feedback documents. 
 16 32 64 128 
Baseline 20.08 
Cache 19.86 
LDA 19.29 19.30 19.28 19.15 
RM 19.26 19.26 19.26 19.26 
SMM 19.19 19.00 19.14 19.10 
RSMM 19.18 19.14 19.15 19.19 
QMM 19.05 18.97 19.00 18.99 
Table 2. The summarization results (in F-scores) 
achieved by various language models along with 
text and spoken documents. 
 
Text Documents (TD) Spoken Documents (SD) 
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L 
VSM 0.347 0.228 0.290 0.342 0.189 0.287 
MMR 0.407 0.294 0.358 0.381 0.226 0.331 
KLM 0.411 0.298 0.361 0.364 0.210 0.307 
RM 0.453 0.335 0.403 0.382 0.239 0.331 
SMM 0.439 0.320 0.388 0.383 0.229 0.327 
RSMM 0.472 0.365 0.423 0.381 0.235 0.329 
QMM 0.486 0.382 0.435 0.395 0.256 0.349 
SVM 0.441 0.334 0.396 0.370 0.222 0.326 
QMM+
SVM 
0.492 0.395 0.448 0.398 0.261 0.358 
 
 
 
 
1478
References 
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 
2011. Modern information retrieval: the con-
cepts and technology behind search, ACM 
Press. 
David M. Blei, Andrew Y. Ng, and Michael I. 
Jordan. 2003. Latent dirichlet allocation. 
Journal of Machine Learning Research, 
pp.993?1022. 
David M. Blei and John Lafferty. 2009. Topic 
models. In A. Srivastava and M. Sahami, 
(eds.), Text Mining: Theory and Applications. 
Taylor and Francis.  
Jaime Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversitybased reranking for 
reordering documents and producing sum-
maries. In Proc. SIGIR, pp. 335?336. 
Claudio Carpineto and Giovanni Romano. 2012. 
A survey of automatic query expansion in in-
formation retrieval. ACM Computing Surveys, 
vol. 44, pp.1?56. 
Stephane Clinchant and Eric Gaussier. 2013. A 
theoretical analysis of pseudo-relevance 
feedback models. In Proc. ICTIR. 
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and 
Stephen Robertson. 2008. Selecting good 
expansion terms for pseudo-relevance feed-
back. In Proc. SIGIR, pp. 243?250. 
Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang, 
and Jia-Wen Liu. 2013. Extractive speech 
summarization using evaluation metric-
related training criteria. Information Pro-
cessing & Management, 49(1), pp. 1cess 
Arthur P. Dempster, Nan M. Laird, and Donald 
B. Rubin. 1977. Maximum likelihood from 
incomplete data via the EM algorithm. Jour-
nal of Royal Statistical Society B, 39(1), pp. 
1?38. 
Joshua V. Dillon and Kevyn Collins-Thompson. 
2010. A unified optimization framework for 
robust pseudo-relevance feedback algorithms. 
In Proc. CIKM, pp. 1069?1078. 
Sadaoki Furui, Li Deng, Mark Gales, Hermann 
Ney, and Keiichi Tokuda. 2012. Fundamen-
tal technologies in modern speech recogni-
tion. IEEE Signal Processing Magazine, 
29(6), pp. 16?17. 
Yihong Gong and Xin Liu. 2001. Generic text 
summarization using relevance measure and 
latent semantic analysis. In Proc. SIGIR, pp. 
19?25. 
Djoerd Hiemstra, Stephen Robertson, and Hugo 
Zaragoza. 2004. Parsimonious language 
models for information retrieval. In Proc. 
SIGIR, pp. 178?185. 
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proc. SIGIR, pp. 50?57.  
Thomas Hofmann. 2001. Unsupervised learning 
by probabilistic latent semantic analysis. 
Machine Learning, 42, pp. 177?196.  
Xuedong Huang, Alex Acero, and Hsiao-Wuen 
Hon. 2001. Spoken language processing: a 
guide to theory, algorithm, and system de-
velopment. Prentice Hall PTR, Upper Saddle 
River, NJ, USA. 
Frederick Jelinek, Bernard Merialdo, Salim Rou-
kos, and M. Strauss. 1991. A dynamic lan-
guage model for speech recognition. In Proc. 
the DARPA workshop on speech and natural 
language, pp. 293?295. 
Frederick Jelinek. 1999. Statistical methods for 
speech recognition. MIT Press. 
Daniel Jurafsky and James H. Martin. 2008. 
Speech and language processing. Prentice 
Hall PTR, Upper Saddle River, NJ, USA. 
Roland Kuhn and Renato D. Mori. 1990. A 
cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6), pp. 570?583. 
Solomon Kullback and Richard A. Leibler. 1951. 
On information and sufficiency. The Annals 
of Mathematical Statistics, 22(1), pp. 79?86. 
Chin-Yew Lin. 2003. ROUGE: Recall-oriented 
Understudy for Gisting Evaluation. Availa-
ble: http://haydn.isi.edu/ROUGE/. 
Feifan Liu and Yang Liu. 2007. Unsupervised 
language model adaptation incorporating 
named entity information. In Proc. ACL, pp. 
672?769. 
Yang Liu and Dilek Hakkani-Tur. 2011. Speech 
summarization. Chapter 13 in Spoken Lan-
guage Understanding: Systems for Extract-
ing Semantic Information from Speech, G. 
Tur and R. D. Mori (Eds), New York: Wiley. 
John Lafferty and Chengxiang Zhai. 2001. Doc-
ument language models, query models, and 
risk minimization for information retrieval. 
In Proc. SIGIR, pp. 111?119. 
Victor Lavrenko and W. Bruce Croft. 2001. Rel-
evance-based language models. In Proc. 
SIGIR, pp. 120?127. 
Victor Lavrenko. 2004. A Generative Theory of 
Relevance. PhD thesis, University of Massa-
chusetts, Amherst. 
1479
Shasha Xie and Yang Liu. 2010. Improving su-
pervised learning for meeting summarization 
using sampling and regression. Computer 
Speech & Language, 24(3), pp. 495?514. 
Yuanhua Lv and Chengxiang Zhai. 2009. A 
comparative study of methods for estimating 
query language models with pseudo feed-
back. In Proc. CIKM, pp. 1895?1898. 
Yuanhua Lv and Chengxiang Zhai. 2010. Posi-
tional relevance model for pseudo-relevance 
feedback. In Proc. SIGIR, pp. 579?586. 
Kyung Soon Lee, W. Bruce Croft, and James 
Allan. 2008. A cluster-based resampling 
method for pseudo-relevance feedback. In 
Proc. SIGIR, pp. 235?242. 
Kyung Soon Lee and W. Bruce Croft. 2013. A 
deterministic resampling method using over-
lapping document clusters for pseudo-
relevance feedback. Inf. Process. Manage. 
49(4), pp. 792?806. 
Inderjeet Mani and Mark T. Maybury (Eds.). 
1999. Advances in automatic text summari-
zation. Cambridge, MA: MIT Press. 
Ani Nenkova and Kathleen McKeown. 2011. 
Automatic summarization. Foundations and 
Trends in Information Retrieval, 5(2?3), pp. 
103?233. 
Stefan Ortmanns, Hermann Ney, and Xavier Au-
bert. 1997. A word graph algorithm for large 
vocabulary continuous speech recognition. 
Computer Speech and Language, pp. 43?72. 
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information re-
trieval. In Proc. SIGIR, pp. 275?281. 
Stephen E. Robertson. 1990. On term selection 
for query expansion. Journal of Documenta-
tion, 46(4), pp. 359?364. 
Andreas Stolcke. 2005. SRILM - An extensible 
language modeling toolkit. In Proc. INTER-
SPEECH, pp.901?904. 
Tao Tao and Chengxiang Zhai. 2006. Regular-
ized estimation of mixture models for robust 
pseudo-relevance feedback. In Proc. SIGIR, 
pp. 162?169. 
Yik-Cheung Tam and Tanja Schultz. 2005. Dy-
namic language model adaptation using vari-
ational Bayes inference. In Proc. INTER-
SPEECH, pp. 5?8. 
Xuanhui Wang, Hui Fang, and Chengxiang Zhai. 
2008. A study of methods for negative rele-
vance feedback. In Proc. SIGIR, pp. 219?226. 
Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, and 
Shih-Sian Cheng. 2005. MATBN: A Manda-
rin Chinese broadcast news corpus. Interna-
tional Journal of Computational Linguistics 
& Chinese Language Processing, 10(2), pp. 
219?236. 
Xing Yi and James Allan. 2009. A comparative 
study of utilizing topic models for infor-
mation retrieval. In Proc. ECIR, pp. 29?41. 
Steve Young, Dan Kershaw, Julian Odell, Dave 
Ollason, Valtcho Valtchev, and Phil Wood-
land. 2006. The HTK book version 3.4. 
Cambridge University Press. 
Chengxiang Zhai and John Lafferty. 2001a. A 
study of smoothing methods for language 
models applied to ad hoc information re-
trieval. In Proc. SIGIR, pp. 334?342.  
Chengxiang Zhai and John Lafferty. 2001b. 
Model-based feedback in the language mod-
eling approach to information retrieval. In 
Proc. CIKM, pp. 403?410. 
Chengxiang Zhai. 2008. Statistical language 
models for information retrieval: a critical 
review. Foundations and Trends in Infor-
mation Retrieval, 2 (3), pp. 137?213. 
Yi Zhang, Jamie Callan, and Thomas Minka. 
2002. Novelty and redundancy detection in 
adaptive filtering. In Proc. SIGIR, pp. 81?88.  
1480
Term Contributed Boundary Tagging by Conditional Random 
Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff 
Tian-Jian Jiang?? Shih-Hung Liu*? Cheng-Lung Sung*? Wen-Lian Hsu?? 
?Department of 
Computer Science 
National Tsing-Hua University 
*Department of 
Electrical Engineering 
National Taiwan University 
?Institute of 
Information Science 
Academia Sinica 
{tmjiang,journey,clsung,hsu}@iis.sinica.edu.tw 
 
Abstract 
This paper presents a Chinese word 
segmentation system submitted to the 
closed training evaluations of CIPS-
SIGHAN-2010 bakeoff. The system uses 
a conditional random field model with 
one simple feature called term contri-
buted boundaries (TCB) in addition to 
the ?BI? character-based tagging ap-
proach. TCB can be extracted from unla-
beled corpora automatically, and seg-
mentation variations of different do-
mains are expected to be reflected impli-
citly. The experiment result shows that 
TCB does improve ?BI? tagging domain-
independently about 1% of the F1 meas-
ure score. 
1 Introduction 
The CIPS-SIGHAN-2010 bakeoff task of Chi-
nese word segmentation is focused on cross-
domain texts. The design of data set is challeng-
ing particularly. The domain-specific training 
corpora remain unlabeled, and two of the test 
corpora keep domains unknown before releasing, 
therefore it is not easy to apply ordinary machine 
learning approaches, especially for the closed 
training evaluations. 
2 Methodology 
2.1 The ?BI? Character-Based Tagging of 
Conditional Random Field as Baseline 
The character-based ?OBI? tagging of 
Conditional Random Field (Lafferty et al, 2001) 
has been widely used in Chinese word 
segmentation recently (Xue and Shen, 2003; 
Peng and McCallum, 2004; Tseng et al, 2005). 
Under the scheme, each character of a word is 
labeled as ?B? if it is the first character of a 
multiple-character word, or ?I? otherwise. If the 
character is a single-character word itself, ?O? 
will be its label. As Table 1 shows, the lost of 
performance is about 1% by replacing ?O? with 
?B? for character-based CRF tagging on the 
dataset of CIPS-SIGHAN-2010 bakeoff task of 
Chinese word segmentation, thus we choose 
?BI? as our baseline for simplicity, with this 1% 
lost bearing in mind. In tables of this paper, SC 
stands for Simplified Chinese and TC represents 
for Traditional Chinese. Test corpora of SC and 
TC are divided into four domains, where suffix 
A, B, C and D attached, for texts of literature, 
computer, medicine and finance, respectively. 
  R P F OOV 
SC-A OBI 0.906 0.916 0.911 0.539 
 BI 0.896 0.907 0.901 0.508 
SC-B OBI 0.868 0.797 0.831 0.410 
 BI 0.850 0.763 0.805 0.327 
SC-C OBI 0.897 0.897 0.897 0.590 
 BI 0.888 0.886 0.887 0.551 
SC-D OBI 0.900 0.903 0.901 0.472 
 BI 0.888 0.891 0.890 0.419 
TC-A OBI 0.873 0.898 0.886 0.727 
 BI 0.856 0.884 0.870 0.674 
TC-B OBI 0.906 0.932 0.919 0.578 
 BI 0.894 0.920 0.907 0.551 
TC-C OBI 0.902 0.923 0.913 0.722 
 BI 0.891 0.914 0.902 0.674 
TC-D OBI 0.924 0.934 0.929 0.765 
 BI 0.908 0.922 0.915 0.722 
Table 1. OBI vs. BI; where the lost of F > 1%, 
such as SC-B, is caused by incorrect English 
segments that will be discussed in the section 4. 
2.2 Term Contributed Boundary 
The word boundary and the word frequency are 
the standard notions of frequency in corpus-
based natural language processing, but they lack 
the correct information about the actual boun-
dary and frequency of a phrase?s occurrence. 
The distortion of phrase boundaries and frequen-
cies was first observed in the Vodis Corpus 
when the bigram ?RAIL ENQUIRIES? and tri-
gram ?BRITISH RAIL ENQUIRIES? were ex-
amined and reported by O'Boyle (1993). Both of 
them occur 73 times, which is a large number for 
such a small corpus. ?ENQUIRIES? follows 
?RAIL? with a very high probability when it is 
preceded by ?BRITISH.? However, when 
?RAIL? is preceded by words other than ?BRIT-
ISH,? ?ENQUIRIES? does not occur, but words 
like ?TICKET? or ?JOURNEY? may. Thus, the 
bigram ?RAIL ENQUIRIES? gives a misleading 
probability that ?RAIL? is followed by ?EN-
QUIRIES? irrespective of what precedes it. This 
problem happens not only with word-token cor-
pora but also with corpora in which all the com-
pounds are tagged as units since overlapping N-
grams still appear, therefore corresponding solu-
tions such as those of Zhang et al (2006) were 
proposed. 
We uses suffix array algorithm to calculate ex-
act boundaries of phrase and their frequencies 
(Sung et al, 2008), called term contributed 
boundaries (TCB) and term contributed fre-
quencies (TCF), respectively, to analogize simi-
larities and differences with the term frequencies 
(TF). For example, in Vodis Corpus, the original 
TF of the term ?RAIL ENQUIRIES? is 73. 
However, the actual TCF of ?RAIL ENQUI-
RIES? is 0, since all of the frequency values are 
contributed by the term ?BRITISH RAIL EN 
QUIRIES?. In this case, we can see that ?BRIT-
ISH RAIL ENQUIRIES? is really a more fre-
quent term in the corpus, where ?RAIL EN-
QUIRIES? is not. Hence the TCB of ?BRITISH 
RAIL ENQUIRIES? is ready for CRF tagging as 
?BRITISH/TB RAIL/TB ENQUIRIES/TI,? for 
example. 
3 Experiments 
Besides submitted results, there are several 
different experiments that we have done. The 
configuration is about the trade-off between data 
sparseness and domain fitness. For the sake of 
OOV issue, TCBs from all the training and test 
corpora are included in the configuration of 
submitted results. For potentially better consis-
tency to different types of text, TCBs from the 
training corpora and/or test corpora are grouped 
by corresponding domains of test corpora. Table 
2 and Table 3 provide the details, where the 
baseline is the character-based ?BI? tagging, and 
others are ?BI? with additional different TCB 
configurations: TCBall stands for the submitted 
results; TCBa, TCBb, TCBta, TCBtb, TCBtc, 
TCBtd represents TCB extracted from the train-
ing corpus A, B, and the test corpus A, B, C, D, 
respectively. Table 2 indicates that F1 measure 
scores can be improved by TCB about 1%, do-
main-independently. Table 3 gives a hint of the 
major contribution of performance is from TCB 
of each test corpus. 
Table 2. Baseline vs. Submitted Results 
 
 
 
 
 
 
  R P F OOV 
SC-A BI 0.896 0.907 0.901 0.508 
 TCBall 0.917 0.921 0.919 0.699 
SC-B BI 0.850 0.763 0.805 0.327 
 TCBall 0.876 0.799 0.836 0.456 
SC-C BI 0.888 0.886 0.887 0.551 
 TCBall 0.900 0.896 0.898 0.699 
SC-D BI 0.888 0.891 0.890 0.419 
 TCBall 0.910 0.906 0.908 0.562 
TC-A BI 0.856 0.884 0.870 0.674 
 TCBall 0.871 0.891 0.881 0.670 
TC-B BI 0.894 0.920 0.907 0.551 
 TCBall 0.913 0.917 0.915 0.663 
TC-C BI 0.891 0.914 0.902 0.674 
 TCBall 0.900 0.915 0.908 0.668 
TC-D BI 0.908 0.922 0.915 0.722 
 TCBall 0.929 0.922 0.925 0.732 
  F OOV 
SC-A TCBta 0.918 0.690 
 TCBa 0.917 0.679 
 TCBta + TCBa 0.917 0.690 
 TCBall 0.919 0.699 
SC-B TCBtb 0.832 0.465 
 TCBb 0.828 0.453 
 TCBtb + TCBb 0.830 0.459 
 TCBall 0.836 0.456 
SC-C TCBtc 0.897 0.618 
 TCBall 0.898 0.699 
SC-D  TCBtd 0.905 0.557 
 TCBall 0.910 0.562 
Table 3a. Simplified Chinese Domain-specific 
TCB vs. TCBall 
  F OOV 
TC-A TCBta 0.889 0.706 
 TCBa 0.888 0.690 
 TCBta + TCBa 0.889 0.710 
 TCBall 0.881 0.670 
TC-B TCBtb 0.911 0.636 
 TCBb 0.921 0.696 
 TCBtb + TCBb 0.912 0.641 
 TCBall 0.915 0.663 
TC-C TCBtc 0.918 0.705 
 TCBall 0.908 0.668 
TC-D TCBtd 0.927 0.717 
 TCBall 0.925 0.732 
Table 3b. Traditional Chinese Domain-specific 
TCB vs. TCBall 
 
4 Error Analysis 
The most significant type of error in our results 
is unintentionally segmented English words. Ra-
ther than developing another set of tag for Eng-
lish alphabets, we applies post-processing to fix 
this problem under the restriction of closed train-
ing by using only alphanumeric character infor-
mation. Table 4 compares F1 measure score of 
the Simplified Chinese experiment results before 
and after the post-processing. 
 
 
 F1 measure score 
before after 
SC-A OBI 0.911 0.918 
 BI 0.901 0.908 
 TCBta 0.918 0.920 
 TCBta + TCBa 0.917 0.920 
 TCBall 0.919 0.921 
SC-B OBI 0.831 0.920 
 BI 0.805 0.910 
 TCBtb 0.832 0.917 
 TCBtb + TCBb 0.830 0.916 
 TCBall 0.836 0.916 
SC-C OBI 0.897 0.904 
 BI 0.887 0.896 
 TCBtc 0.897 0.901 
 TCBall 0.898 0.902 
SC-D OBI 0.901 0.919 
 BI 0.890 0.908 
 TCBtd 0.905 0.915 
 TCBall 0.908 0.918 
Table 4. F1 measure scores before and after 
English Problem Fixed 
The major difference between gold standards 
of the Simplified Chinese corpora and the Tradi-
tional Chinese corpora is about non-Chinese 
characters. All of the alphanumeric and the 
punctuation sequences are separated from Chi-
nese sequences in the Simplified Chinese corpo-
ra, but can be part of the Chinese word segments 
in the Traditional Chinese corpora. For example, 
a phrase ??? / simvastatin / ? / statins? / ? / ? /
? / ?? (?/? represents the word boundary) from 
the domain C of the test data cannot be either 
recognized by ?BI? and/or TCB tagging ap-
proaches, or post-processed. This is the reason 
why Table 4 does not come along with Tradi-
tional Chinese experiment results. 
Some errors are due to inconsistencies in the 
gold standard of non-Chinese character, For ex-
ample, in the Traditional Chinese corpora, some 
percentage digits are separated from their per-
centage signs, meanwhile those percentage signs 
are connected to parentheses right next to them. 
5 Conclusion 
This paper introduces a simple CRF feature 
called term contributed boundaries (TCB) for 
Chinese word segmentation. The experiment 
result shows that it can improve the basic ?BI? 
tagging scheme about 1% of the F1 measure 
score, domain-independently. 
Further tagging scheme for non-Chinese cha-
racters are desired for recognizing some sophis-
ticated gold standard of Chinese word segmenta-
tion that concatenates alphanumeric characters 
to Chinese characters. 
Acknowledgement 
The CRF model used in this paper is developed based 
on CRF++, http://crfpp.sourceforge.net/ 
Term Contributed Boundaries used in this paper are 
extracted by YASA, http://yasa.newzilla.org/ 
References 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference of Machine Learning, 591?598. 
Peter O'Boyle. 1993. A Study of an N-Gram Lan-
guage Model for Speech Recognition. PhD thesis. 
Queen's University Belfast. 
Fuchun Peng and Andrew McCallum. 2004. Chinese 
segmentation and new word detection using condi-
tional random fields. In Proceedings of Interna-
tional Conference of Computational linguistics, 
562?568, Geneva, Switzerland. 
Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian 
Hsu. 2008. Compute the Term Contributed Fre-
quency. In Proceedings of the 2008 Eighth Inter-
national Conference on Intelligent Systems Design 
and Applications, 325-328, Washington, D.C., 
USA. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky, and Christopher Manning. 2005. A 
conditional random field word segmenter for Sig-
han bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea. 
Nianwen Xue and Libin Shen. 2003. Chinese word-
segmentation as LMR tagging. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by conditional 
random fields for Chinese word segmentation. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 193-
196, New York, USA. 
 
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 76?80,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cost-benefit Analysis of Two-Stage Conditional Random Fields based 
English-to-Chinese Machine Transliteration 
 
Chan-Hung Kuoa  Shih-Hung Liuab  MikeTian-Jian Jiangac  
Cheng-Wei Leea  Wen-Lian Hsua 
aInstitute of Information Science, Academia Sinica 
bDepartment of Electrical Engineering, National Taiwan University 
cDepartment of Computer Science, National Tsing Hua University 
{laybow, journey, tmjiang, aska, hsu}@iis.sinica.edu.tw 
 
 
Abstract 
This work presents an English-to-Chinese 
(E2C) machine transliteration system based 
on two-stage conditional random fields 
(CRF) models with accessor variety (AV) 
as an additional feature to approximate 
local context of the source language. 
Experiment results show that two-stage 
CRF method outperforms the one-stage 
opponent since the former costs less to 
encode more features and finer grained 
labels than the latter. 
1 Introduction 
Machine transliteration is the phonetic 
transcription of names across languages and is 
essential in numerous natural language processing 
applications, such as machine translation, cross-
language information retrieval/extraction, and 
automatic lexicon acquisition (Li et al, 2009). It 
can be either phoneme-based, grapheme-based, or 
a hybrid of the above. The phoneme-based 
approach transforms source and target names into 
comparable phonemes for an intuitive phonetic 
similarity measurement between two names 
(Knight and Graehl, 1998; Virga and Khudanpur, 
2003). The grapheme-based approach, which treats 
transliteration as statistical machine translation 
problem under monotonic constraint, aims to 
obtain a direct orthographical mapping (DOM) to 
reduce possible errors introduced in multiple 
conversions (Li et al, 2004). The hybrid approach 
attempts to utilize both phoneme and grapheme 
information (Oh and Choi, 2006). Phoneme-based 
approaches are usually not good enough, because 
name entities have various etymological origins 
and transliterations are not always decided by 
pronunciations (Li et al, 2004). The state-of-the-
art of transliteration approach is bilingual DOMs 
without intermediate phonetic projections (Yang et 
al., 2010). 
Due to the success of CRF on sequential 
labeling problem (Lafferty et al, 2001), numerous 
machine transliteration systems applied it. Some of 
them treat transliteration as a two-stage sequential 
labeling problem: the first stage predicts syllable 
boundaries of source names, and the second stage 
uses those boundaries to get corresponding 
characters of target names (Yang et al, 2010; Qin 
and Chen, 2011). Dramatically de-creasing the cost 
of training with complex features is the major 
advantage of two-stage methods, but their 
downside is, compared to one-stage methods, 
features of target language are not directly applied 
in the first stage. 
Richer context generally gains better results of 
sequential labeling, but squeezed performance 
always comes with a price of computational 
complexity. To balance cost and benefit for 
English-to-Chinese (E2C) transliteration, this work 
compares the one-stage method with the two-stage 
one, using additional features of AV (Feng et al, 
2004) and M2M-aligner as an initial alignment  
(Jiampojamarn et al, 2007), to explore where the 
best investment reward is. 
The remainder of this paper is organized as 
follows. Section 2 briefly introduces related works, 
including two-stage methods and AV. The 
machine transliteration system using M2M-aligner, 
CRF models, and AV features in this work is 
explained in Section 3. Section 4 describes 
76
experiment results along with a discussion in 
Section 5. Finally, Section 6 draws a conclusion. 
2 Related Works 
Reddy and Waxmonsky (2009) presented a phrase-
based transliteration system that groups characters 
into substrings mapping onto target names, to 
demonstrate how a substring representation can be 
incorporated into CRF models with local context 
and phonemic information. Shishtla et al (2009) 
adopted a statistical transliteration technique that 
consists of alignment models of GIZA++ (Och and 
Ney, 2003) and CRF models. Jiang et al (2011) 
used M2M-aligner instead of GIZA++ and applied 
source grapheme?s AV in a CRF-based 
transliteration. 
A two-stage CRF-based transliteration was first 
designed to pipeline two independent processes 
(Yang et al, 2009). To recover from error 
propagations of the pipeline, a joint optimization of 
two-stage CRF method is then proposed to utilize 
n-best candidates of source name segmentations 
(Yang et al 2010). Another approach to resist 
errors from the first stage is split training data into 
pools to lessen computation cost of sophisticated 
CRF models for the second stage (Qin and Chen, 
2011). 
3 System Description  
3.1 EM for Initial Alignments 
M2M-aligner first maximizes the probability of 
observed source-target pairs using EM algorithm 
and subsequently sets alignments via maximum a 
posteriori estimation. To obtain initial alignments 
as good as possible, this work empirically sets the 
parameter ?maxX? of M2M-aligner for the 
maximum size of sub-alignments in the source side 
to 8, and sets the parameter ?maxY? for the 
maximum size of sub-alignments in the target side 
to 1 (denoted as X8Y1 in short), since one of the 
well-known a priori of Chinese is that almost all 
Chinese characters are monosyllabic. 
3.2 Format of Electronic Manuscript 
The two-stage CRF method consists of syllable 
segmentation and Chinese character conversion 
CRF models, namely Stage-1 and Stage-2, 
respectively. Stage-1 CRF model is trained with 
source name segmentations initially aligned by 
M2M-aligner to predict syllable boundaries as 
accurate as possible. According to the 
discriminative power of CRF, some syllable 
boundary errors from preliminary alignments could 
be counterbalanced. Stage-2 CRF model then sees 
predicted syllable boundaries as input to produce 
optimal target names. For CRF modeling, this 
work uses Wapiti (Lavergne et al, 2010). 
Using ?BULLOUGH? as an example, labeling 
schemes below are for Stage-1 training. 
? B/B U/B L/I L/I O/I U/I G/I H/E 
? B/S U/B L/1 L/2 O/3 U/4 G/5 H/E 
The first one is the common three-tag set ?BIE?. 
The last one is the eight-tag set ?B8?, including B, 
1-5, E and S: tag B indicates the beginning 
character of a syllable segment, tag E means the 
ending character, tag I or 1-5 stand for characters 
in-between, and tag S represents a single character 
segment. The expectation of the eight-tag set is the 
finer grained tags we used, the better segmentation 
accuracy we would gain. 
For Stage-2, two labeling schemes are listed in 
the following. 
? B/? ULLOUGH/? 
? B/? U/? L/I L/I O/I U/I G/I H/I 
The former as substring-based labeling scheme are 
commonly used in two-stage CRF-based 
transliteration. Syllable segments in a source word 
are composed from Stage-1 results and then are 
associated with corresponding Chinese characters 
(Yang et al 2009; Yang et al 2010; Qin and Chen, 
2011). The latter is a character-based labeling 
scheme where tags B or S from Stage-1 will be 
labeled with a Chinese character and others will be 
labeled as I. The merit of character-based method 
is to retrench the duration of the training, while 
substring-based method takes too much time to be 
included in this work for NEWS shared task. 
Section 5 will discuss more about pros and cons 
between substring and character based labeling 
schemes. 
This work tests numerous CRF feature 
combinations, for example: 
? C-3, C-2, C-1, C0, C1 , C2, C3 and 
? C-3C-2, C-2C-1, C-1C0, C0C1, C1C2, C2C3, where local context is ranging from -3 to 3, and Ci 
denotes the characters bound individually to the 
prediction label at its current position i. 
77
3.3 CRF with AV  
AV was for unsupervised Chinese word 
segmentation (Feng et al, 2004). Jiang et al, 
(2011) showed that using AV of source grapheme 
as CRF features could improve transliteration. In 
our two-stage system, Source AV is used in Stage-
1 in hope for better syllable segmentations, but not 
in Stage-2 since it may be redundant and surely 
increase training cost of Stage-2. 
4 Experiment Results 
4.1 Results of Standard Runs 
Four standard runs are submitted to NEWS12 E2C 
shared task. Their configurations are listed in Table 
1, where ?U? and ?B? denote observation 
combinations of unigram and bigram, respectively. 
A digit in front of a ?UB?, for example, ?2?, 
indicates local context ranging from -2 to 2. PBIE 
stands for ?BIE? tag set and PB8 is for ?B8? tag set. 
To summarize, the 4th (i.e. the primary) standard 
run exceeds 0.3 in terms of top-1 accuracy (ACC), 
and other ACCs of standard runs are approximate 
to 0.3. The 3rd standard run uses the one-stage CRF 
method to compare with the two-stage CRF 
method. Experiment results show that the two-
stage CRF method can excel the one-stage 
opponent, while AV and richer context also 
improve performance.  
4.2 Results of Inside Tests 
Numerous pilot tests have been conducted by 
training with both the training and development 
sets, and then testing on the development set, as 
?inside? tests. Three of them are shown in Table 2, 
where configurations I and II use the two-stage 
method, and configuration III is in one-stage. 
Table 2 suggests a trend that the one-stage CRF 
method performs better than the two-stage one on 
inside tests, but Table 1 votes the opposite. Since 
the development set includes semi-semantic 
transliterations that are unseen in both the training 
and the test sets (Jiang et al, 2011), models of 
inside tests are probably over-fitted to these noises. 
Table 3 further indicates that the number of 
features in the one-stage CRF method is doubled 
than that in the two-stage one. By putting these 
observations together, the two-stage CRF method 
is believed to be more effective and efficient than 
the one-stage CRF method. 
5 Discussions  
There are at least two major differences of two-
stage CRF-based transliteration between our 
approach and others. One is that we enrich the 
local context as much as possible, such as using 
eight-tag set in Stage-1. The other is using a 
character-based labeling method instead of a 
substring-based one in Stage-2. 
Reasonable alignments can cause CRF models 
troubles when a single source grapheme is mapped 
onto multiple phones. For instance, the alignment 
between ?HAX? and ????? generating by 
M2M-aligner. 
 HA ? ? 
 X ? ?? 
In this case, a single grapheme <X> pronounced as 
/ks/ in English therefore is associated with two 
Chinese characters ????, and won?t be an easy 
case to common character-based linear-chain CRF. 
Although for the sake of efficiency, this work 
adopts character-based CRF models, only a few of 
such single grapheme for consonant blends or 
diphthongs appeared in training and test data, and 
then the decline of accuracy would be moderate. 
One may want to know how high the price is for 
using a substring-based method to solve this 
problem. We explore the number of features 
between substring-based and character-based 
ID Configuration ACC Mean 
F-score
1 Two-stage, 2UB, PBIE 0.295 0.652 2 Two-stage, 2UB, PBIE, AV 0.299 0.659 3 One-stage, 3UB, PBIE, AV 0.291 0.654 4 Two-stage, 3UB, PB8, AV 0.311  0.662 
Table 1. Selected E2C standard runs 
 
ID Configuration ACC Mean F-score
I Two-stage, 2UB, PBIE, AV 0.363 0.707 II Two-stage, 3UB, PB8, AV 0.397 0.727III One-stage, 3UB, PBIE, AV 0.558 0.834 
Table 2. Selected E2C inside tests 
 
ID Number of Features  Numbers of Label 
II Stage-1: 60,496 Stage-1: 8 Stage-2: 2,567,618 Stage-2: 547 
III 4,439,896 548 
Table 3. Cost of selected E2C inside tests 
78
methods in Stage-2 with the same configuration II, 
as shown in Table 4. Features of substring-based 
method are tremendously more than character-
based one. Qin (2011) also reported similar 
observations. 
However, there is another issue in our character-
based method: only the starting position of a 
source syllable segment will be labeled as Chinese 
character, others are labeled as I. Base on this 
labeling strategy, the local context of the target 
graphemes is missing. 
6 Conclusions and Future Works  
This work analyzes cost-benefit trade-offs between 
two-stage and one-stage CRF-based methods for 
E2C transliteration. Experiment results indicate 
that the two-stage method can outperform its one-
stage opponent since the former costs less to 
encode more features and finer grained labels than 
the latter. Recommended future investigations 
would be encoding more features of target 
graphemes and utilizing n-best lattices from the 
outcome of Stage-1. 
Acknowledgments 
This research was supported in part by the National 
Science Council under grant NSC 100-2631-S-
001-001, and the research center for Humanities 
and Social Sciences under grant IIS-50-23. The 
authors would like to thank anonymous reviewers 
for their constructive criticisms. 
References  
Haodi Feng, Kang Chen, Xiaotie Deng, and Wiemin 
Zheng. 2004. Accessor Variety Criteria for Chinese 
Word Extraction. Computational Linguistics, 
30(1):75-93. 
Zellig Sabbetai Harris. 1970. Morpheme boundaries 
within words. Papers in Structural and 
Transformational Linguistics, 68-77. 
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek 
Sherif. 2007. Applying Many-to-Many Alignments 
and Hidden Markov Models to Letter-to-Phoneme 
Conversion. Proceedings of NAACL 2007, 372-379. 
Mike Tian-Jian Jiang, Chan-Hung Kuo and Wen-Lian 
Hsu. 2011. English-to-Chinese Machine 
Transliteration using Accessor Variety Features of 
Source Graphemes. Proceedings of the 2011 Named 
Entities Workshop. 86-90. 
K. Knight and J. Graehl. 1998. Machine Transliteration. 
Computational Linguistics, 24(4):599-612. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of ICML, 591-598. 
Thomas Lavergne, Oliver Capp? and Fran?ois Yvon. 
2010. Practical Very Large Scale CRF. Proceedings 
the 48th ACL, 504-513. 
Haizhou Li, Min Zhang and Jian Su. 2004. A Joint 
Source Channel Model for Machine Transliteration. 
Proceedings of the 42nd ACL, 159-166. 
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2009. Report of NEWS 2009 
Transliteration Generation Shared Task. Proceedings 
of the 2009 Named Entities Workshop. 1-18. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19-51. 
J. H. Oh and K. S. Choi. 2006. An Ensemble of 
Transliteration Models for Information Retrieval. 
Information Processing and Management, 42:980-
1002. 
Ying Qin. 2011. Phoneme strings based machine 
transliteration. Proceedings of the 7th IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering. 304-309. 
Ying Qin and Guohua Chen. 2011. Forward-backward 
Machine Transliteration between English and 
Chinese Base on Combined CRF. Proceedings of the 
2011 Named Entities Workshop. 82-85. 
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning 
String Edit Distance. IEEE Transactions on Pattern 
Recognition and Machine Intelligence, 20(5):522-
532. 
Sravana Reddy and Sonjia Waxmonsky. 2009. 
Substring-based transliteration with conditional 
random fields. Proceedings of the 2009 Named 
Entities Workshop, 92-95. 
Praneeth Shishtla, V. Surya Ganesh, Sethuramalingam 
Subramaniam and Vasudeva Varma. 2009. A 
language-independent transliteration schema using 
character aligned models at NEWS 2009. 
Proceedings of the 2009 Named Entities Workshop, 
40-43. 
ID Substring-based Character-Based 
II 106,070,874 2,567,618 
Table 4. Number of features between substring 
and character based method in Stage-2 
79
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-lingual Information Retrieval. 
In the Proceedings of the ACL Workshop on Multi-
lingual Named Entity Recognition. 
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku 
Oonishi, Masanobu Nakamura, Sadaoki Furui. 2009. 
Combining a two-step conditional random field 
model and a joint source channel model for machine 
transliteration. Proceedings of the 2009 Named 
Entities Workshop, 72-75. 
Dong Yang, Paul Dixon and Sadaoki Furui. 2010. 
Jointly optimizing a two-step conditional random 
field model for machine transliteration and its fast 
decoding algorithm. Proceedings of the ACL 2010. 
Conference Short Papers, 275-280 
Hai Zhao and Chunyu Kit. 2008. Unsupervised 
Segmentation Helps Supervised Learning of 
Character Tagging for Word Segmentation and 
Named Entity Recognition. Proceedings of the Sixth 
SIGHAN Workshop on Chinese Language 
Processing. 
 
80
