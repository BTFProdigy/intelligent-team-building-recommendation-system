Phrase-based Evaluation of Word-to-Word Alignments  
Michael Carl and Sisay Fissaha 
Institut f?r Angewandte Informationsforschung 
66111 Saarbr?cken, Germany 
{carl;sisay}@iai.uni-sb.de 
Abstract 
We evaluate the English?French word align-
ment data of the shared tasks from a phrase 
alignment perspective. We discuss pe-
culiarities of the submitted data and the test 
data. We show that phrase-based evaluation is 
closely related to word-based evaluation. We 
show examples of phrases which are easy to 
align and also phrases which are difficult to 
align. 
1 
2 
Introduction 
We describe a phrase-based evaluation of the 16 Eng-
lish-French alignment submissions for the shared task 
on Parallel Texts. The task was to indicate which word 
token in an English alignment sample corresponds to 
which word token in the French alignment sample. Two 
types of submission were permitted: for restricted sub-
missions were allowed a ?sentence? aligned segment of 
the Canadian Hansards to train the systems while unre-
stricted submission would be allowed to use additional 
resources. The performance of the systems was com-
pared for a set of 447 English?French hand-aligned 
test samples which were also taken from the Canadian 
Hansards.  
Five institutes participated in the English?French 
alignment task, submitting a total of 16 sets of align-
ment data. To evaluate the submitted data, we extracted 
bilingual phrase dictionaries from the word-alignment 
data. The extracted dictionaries of the submitted data 
were compared with the extracted dictionary of the test 
data. 
We first discuss word-to-word and phrase-to-phrase 
alignment format. We present two different methods for 
extracting bilingual dictionaries from the word align-
ment data: a minimal dictionary contains the least num-
ber of unambiguous phrase-to-phrase translations while 
an exhaustive dictionary contains all possible unambi-
guous translations. We examine the test data (i.e. the 
?golden standard?) and the submitted alignment data. 
We discuss their peculiarities and give examples of 
phrases easy and difficult to align. 
 
Types of Alignment  
The test set consists of 447 alignment samples from the 
Canadian Hansards which were pre-tokenized. A three-
tuple containing the alignment number, an English word 
offset and a French word offset would indicate an exact 
word-to-word translation1. The submitted data was sup-
posed to comply with this word-to-word alignment for-
mat. In example 1 the English sentence has 15 tokens 
while the French sentence has 16 tokens. Example 1 
shows the word-to-word alignment data of sample 91 
for submission 12 and a plot of the data. 
 
Example 1:   Alignment sample 91: 
 
English (vertical): 
i was not asking for a detailed explana-
tion as to what he was doing . 
 
French (horizontal):  
je ne lui ai pas demand? de me fournir de 
telles explications sur ces activit?s . 
 
Plot and word alignment data for submission 12: 
    Sample En Fr 
15                 x 91 15 16 
14               x   91 14 14 
13         x         91 13  8 
12         x         91 12  8 
11             x     91 11 12 
10          x        91 10  9 
09            x      91  9 11 
08             x     91  8 12 
07          x        91  7  9 
06          x        91  6  9 
05             x     91  5 12 
04    x              91  4  3 
03   x               91  3  2 
02         x         91  2  8 
01  x                91  1  1 
00     xxxx  x  x x  
   01234567890123456 
 
                                                           
1 There was also an optional slot to indicate whether this 
alignment would be [S]ure or [P]robable. We ignore this in-
formation in our evaluation.  
Figure1:   Number of word alignment points and size of extracted dictionaries
0
5000
10000
15000
20000
25000
30000
35000
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
null-alignment
submitted
text-dic1
text-dic2
align-dic1
 
2.1 Word-to-word alignment 
There are two underlying assumptions in word-to-word 
alignment: 
(i) each word token on the English side can have 
any number of word correspondences -- includ-
ing zero -- on the French side and vice versa. 
Word alignments may have crossing and am-
biguous branches. For instance in example 1, 
the French word ?me? on position 8 has the 
translations ?was?, and ?he?, while ?ai? has no 
connection to the English side. 
(ii) words (English or French) for which no align-
ments are given in the submitted data are as-
signed a null-alignment.  
Example 1 has 22 word alignment points, where the 
evaluators inserted 7 null-alignments. In some cases (i.e. 
submission 11) this insertion accounts for almost 50% 
of the alignment data. In figure 1, ?null-alignment? plots 
the union of the submitted alignment data and the in-
serted null-alignments. Null-alignments were not added 
to submission 16 as it provides alignment information 
for every word. The last data point on the x-axis (i.e. 17) 
epresents the test data. 
s outlined in Melamed (1998), a sequence of words 
(ii) an English phrase may only be unambiguously 
linked to exactly one French phrase and vice 
versa. 
Phrase-to-phrase alignments can be nested. For instance, 
the shorter English?French phrase translation 9-9 <-
> 11-11 is included in the longer phrase translation 
5-11 <-> 9-12:  
5-11 9-12: for a detailed explanation 
 as to what  
 <-> fournir de telles explications 
9-9 11-11: as <-> telles 
In this way structural information can be stored. On the 
other hand, we do not allow ambiguous phrase align-
ments as e.g.: 
 8-8 12-12 explanation <-> explications 
11-11 12-12  what <-> explications 
When extracting phrase-to-phrase translations from the 
word-to-word alignment data we include a sufficient 
context which disambiguates the phrases. Given the 
word alignment data in example 1, the minimum con-
text required to disambiguate the French word ?explica-
tions? is the phrase 5-11 <-> 9-12. 
From the word alignment data we generate bilingual 
dictionaries in two different ways: a minimal dictionary 
contains only the shortest unambiguous phrase-to-r
A2.2 
                                                          
which translates in a non-compositional fashion into a 
target sequence is exhaustively linked (see example 2). 
Phrase-to-phrase alignment 
Phrase-to-phrase alignment is represented by intervals 
indicating the starting and ending words of the phrases. 
In phrase-to-phrase alignment:  
(i) a sequence of English word tokens (i.e. a 
phrase) are mutually linked with sequences of 
French word tokens (i.e. a French phrase)2.  
                                                           
2 We do not use the term ?phrase? here in its linguistic sense: a 
phrase in this paper can be any sequence of words, even if 
they are not a linguistic constituent. 
phrase translations. For instance, from the alignment 
data in example 1, the following 8 entries are generated 
as a minimal dictionary:3 
 En  Fr 
 1-1   1-1  
 2-13  2-12  
 3-3   2-2  
 4-4   3-3   
 5-11   9-12  
 9-9 11-11  
14-14 14-14  
15-15 16-16  
 
3 As shorthand notation we use here the offset numbers. In the 
generated dictionary, we have extracted the sequences of 
words instead of the offset numbers. 
In an exhaustive dictionary all possible unambiguous 
phrase translations are extracted. An exhaustive diction-
ary is a superset of the minimal dictionary. For example 
1, seven additional entries are generated: 
 En  Fr 
 1-13  1-12 
 1-14  1-14 
 1-15  1-16 
 2-14  2-14 
 2-15  2-16 
 3-4   2-3  
14-15 14-16 
Note that these additional phrase translations can be 
compositionally generated with the minimal dictionary. 
To evaluate the word alignment data through phrasal 
alignments, we generated three types of dictionaries for 
all 16 submissions and the test data:  
(i) an alignment-based minimal dictionary, 
align-dic1; actually 447 small dictionar-
ies for each sample alignment. 
(ii) a text-based minimal dictionary (text-
dic1)which is the union of the align-dic1. 
(iii) an exhaustive text-based dictionary (text-
dic2) which is the union of exhaustive 
alignment dictionaries. 
As can be seen from figure 1, the size of the ex-
haustive dictionary (text-dic2) is in most cases 
much bigger than those of the minimal dictionar-
ies align-dic1 and text-dic1. The reason is due to 
the way the data has been aligned.  
3 The word alignment data 
In this section we show that the test alignment 
data is structurally different from the submitted 
data. The hand aligned test data reflects the 
phrasal nature of the alignments, while the sub-
missions are to a greater extent compositional.  
The test data (see set 17 in figure 1) has about twice
three times as many word-alignment points than 
submissions. While this often leads to high precis
and lower recall for word alignment, the reverse is t
for the extracted phrasal dictionaries (also figure 3). T
test alignment data of sample 91 contains 68 word
word alignment points shown in example 2; about f
times the average number of word alignment points 
this sample. Comparing example 2 with the submit
data of submission 16 (example 3) brings to light 
phrasal nature of the test set. 
Extracting a minimal phrase dictionary from test 
word alignment data in example 2 produces the follo
ing three entries 
 1-14  1-15 
 5-8   7-12 
15-15 16-16 
The following additional entry is generated in the 
haustive dictionary:1-15 <-> 1-16. Note that m
word alignments could have been possible here, for in-
stance: 
i <-> je 
asking <-> demand? 
explanation <-> explications  
not <-> ne , pas 
Despite the existence of some fine-grained word-to-
word correspondences in the test data, human aligners 
tend to mark phrasal translations. In contrast to the 
phrasal nature of the test alignments, most submissions 
show a more compositional alignment structure. For 
instance, alignment data of sample 91 for submission 16 
has 18 word-to-word alignment points (example 3). 
When the alignments are more compositional, more 
coherent phrasal translations can be extracted. Thus, the 
minimal phrase dictionary extracted from example 3 
hile t -
efore e 
 the lt 
 prec -
Example 2: sample 91  
of test set: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Example 3: sample 91  
of submission 16 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  to 
the 
contains 13 entries, w
tains 54 entries. Ther
phrase dictionaries on
in high recall and low
15                 x
14              xxx 
13              xxx 
12    x         xxx 
11              xxx 
10              xxx 
09              xxx 
08        xxxxxx    
07        xxxxxx    
06        xxxxxx    
05        xxxxxx    
04  xxxxxx          
03  xxxxxx          
02  xxxxxx          
01  xxxxxx          
00                  
   012345678901234564 
ion 
rue 
he 
-to-
our 
for 
ted 
the 
set 
w-
ex-
ore 
mitted word alignment dat
in high precision and low r
Phrase-based Eva
Figure 2 shows the correla
extracted dictionaries and t
sure and probable). For eac
calculated as 2*precision
curves for the alignment-b
show the mean f-score com
Roughly all submissions 
word-to-word alignment an
We wanted to see what fa
recall. A correlation betw
alignment and its average he exhaustive dictionary con
, we expect that mapping th
test dictionaries would resu
ision while mapping the sub
15                 x
14                x 
13              xx  
12         x        
11           x      
10           x      
09            x     
08             x    
07          x       
06        x         
05       x          
04       x          
03      x           
02   xxx            
01  x               
00                  
   01234567890123456a on the test data would result 
ecall.  
luation 
tion of the f-score of the three 
he word alignment data (both 
h submission the f-score was 
*recall/precision+recall. The 
ased dictionaries (align-dic1) 
puted over all 447 samples. 
show a similar pattern for 
d phrase dictionaries. 
ctors influence precision and 
een the length of the sample 
recall and precision is shown 
Figure 2:  f-score of word alignments and dictionaries
0
10
20
30
40
50
60
70
80
90
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
sure
prob
align-dic1
text-dic1
text dic2
in figure 3. As one would have expected, the graph 
shows a tendency that shorter samples are easier to align 
(higher precision and recall) than longer samples. How-
ever, there is higher variation among shorter alignments 
than among longer sample alignments which indicates 
unpredictability of shorter samples. As an example con-
sider alignment sample 7 (length 3):  
hear, hear ! <-> bravo ! 
The extracted minimal test dictionary contains the two 
entries:  
hear,hear <-> bravo 
! <->! 
While most of the minimal dictionaries extracted from 
the submitted data contain the entries: 
hear <-> bravo 
! <-> ! 
This leads to a value of 50 for recall and precision for 
both word and phrase alignments. The average recall 
and precision of sample 7 (length 3) is 53,1 and 57,8. 
Figure 3 also shows that PD-recall (phrasal dictionary) 
is higher than PD-precision as the samples become 
longer. For example, sample 91 (length 15,5) has PD-
recall and PD-precision values of 65, and 50,2 respec-
tively. For word-to-word evaluation, however, WD-
precision is higher than WD-recall.  For sample 91 
(length 15,5)  the WD-recall and WD-precision values 
are 18,03 and 53,86 respectively.  
Next we wanted to see which parts in the sample align-
ments would be easy and which parts would be difficult 
to align. We assume that correct translations which ap-
pear in all submissions would be easy to find while 
translations which occur only in the test set but in none 
of the submissions would be difficult to find. Finally, 
the same noisy translations produced by all submissions 
would indicate mistakes in the test data. The cardinality 
of these sets is shown in the table below. 
Intersection of text-dic1 text-dic2
correct 150 434
missing 837 1949
noise 11 22
There were 150 one-word entries in 
the intersection of the correct transla-
tions contained in all 16 dictionaries 
text-dic1. These translations include 
transfer rules which are easy to dis-
cover such as numbers, function 
words, pronouns, frequent content 
words and also domain specific trans-
lations: 
Figure3:  length of alignments vs. Recall and Precision
0
10
20
30
40
50
60
70
80
90
100
Le
ng
th
Length
PD_Recall
PD_Precision
WD_Recall
WD_Precision
1) pronouns 
he <-> il 
it <-> il 
there <-> il 
2) frequent content words 
women <-> femmes 
work <-> travaillent 
compulsory <--> obligatoire 
say <-> dire 
says <-> dit 
3) function words 4,
5 7
9,
5 12
14
,5 17
19
,5 22
24
,5 27
such <-> tel 
to <-> de 
to <-> pour 
5) Text typical translations: 
House <-> Chambre 
The set of translation equivalences missing in all sub-
missions was much larger. There were only the follow-
ing five one-word equivalences: 
1) on-word translations: 
and <-> puisque 
balance <-> niveau 
do <-> fait 
per <-> le 
very <-> fondamentalement 
Most of the missing entries were multi-word transla-
tions, such as idioms, compound words etc.  
1) idiomatic expressions 
A buck is a buck is a buck <-> une 
piastre est toujours une piastre 
thank you very much <-> je vous re-
mercie 
2) compound: 
Canadian Wheat Board <->  
  Commission canadienne de le bl? 
3) complex prepositions 
as for <-> en ce qui concerne 
4) complex verbs and negation 
does not like <-> ne aime pas 
will be <-> feront 
5) adverbs and adjective phrases 
previous <-> qui me a pr?c?d? 
a good thing <-> int?ressant 
6) unresolved pronouns 
the government <-> il 
There were also 11 noisy entries which occurred in all 
generated submissions dictionaries but not in the test 
data dictionary. The obvious explanation for this is, 
again, the phrasal nature of the test data: single word 
translations would be hidden in phrase translations and 
not extracted as separated word translations:  
before <-> avant 
believe <-> crois 
days <-> jours 
every <-> chacune 
facilities <-> installations 
jobs <-> emploi 
positive <-> positifs 
public <-> public 
representations <-> instances 
why <-> comment 
will <-> servira 
5 
6 
Submissions 
This section lists the origin of the submitted data. A 
more detailed description can be found in the system 
description contained in these proceedings. 
1 BiBr.EF.7 
Limited Resources  7. intersection of 1 & 3 
2 BiBr.EF.1  
Limited Resources 1.  Baseline of Bi-lingual Bracketing 
3 BiBr.EF.2  
Unlimited Resources 2. Baseline of Bi-lingual Bracket-
ing + POS (Brill's POS tagger for English only) 
4 BiBr.EF.8  
Unlimited Resources 8.  intersection of 3 & 6 
5 BiBr.EF.3  
Unlimited Resources 3.  Baseline of Bi-lingual Bracket-
ing + POS (Brill's POS tagger for English only) + Eng-
lish_Chunker. 
6 BiBr.EF.4  
Limited Resources 4.  reverse direction of (1) 
7 BiBr.EF.5  
Unlimited Resources 4.  reverse direction of (2) 
8 BiBr.EF.6  
Unlimited Resources 4.  reverse direction of (3) 
9 data withdrawn 
10 UMD.EF. 
Limited Resources Trained on House and Senate Data 
11 ProAlign.EF.1  
Unlimited Resources ProAlign uses the cohesion be-
tween the source and target languages to constrain the 
search for the most probable alignment (based on a 
novel probability model). The extra resources include: 
An English parser A distributional similarity database 
for English words. 
12 data withdrawn 
13 XRCE.Base.EF.1  
Limited Resources GIZA++ with English and French 
lemmatizer (no trinity lexicon) 
14 XRCE.Nolem.EF.2  
Limited Resources GIZA++ only (no lemmatizer, no 
trinity lexicon), Corpus used: Quarter 
15 XRCE.Nolem.EF.3  
Limited Resources GIZA++ only (no lemmatizer, no 
trinity lexicon), Corpus used: Half 
16 ralign.EF.1  
Limited Resources Recursive parallel segmentation of 
texts; scoring based on IBM-2 
17 test data (golden standard) 
References 
Ulrich Germann, editor (2001). Aligned Hansards of the 
36th Parliamentof Canada. http://www.isi.edu/natural-
language/download/hansard/index.html 
I. Dan Melamed (1998). Annotation Style Guide for the 
Blinker Project, IRCS Technical Report #98-06, 
http://www.cs.nyu.edu/~melamed/ftp/papers/styleguide.
ps.gz 
Franz Josef Och, Hermann Ney (2000). A Comparison 
of Alignment Models for Statistical Machine Transla-
tion.. COLING 2000..http://www-i6.informatik.rwth-
aachen.de/Colleagues/och/COLING00.ps 
The University of Amsterdam at Senseval-3:
Semantic Roles and Logic Forms
David Ahn Sisay Fissaha Valentin Jijkoun Maarten de Rijke
Informatics Institute, University of Amsterdam
Kruislaan 403
1098 SJ Amsterdam
The Netherlands
{ahn,sfissaha,jijkoun,mdr}@science.uva.nl
Abstract
We describe our participation in two of the tasks or-
ganized within Senseval-3: Automatic Labeling of
Semantic Roles and Identification of Logic Forms
in English.
1 Introduction
This year (2004), Senseval, a well-established fo-
rum for the evaluation and comparison of word
sense disambiguation (WSD) systems, introduced
two tasks aimed at building semantic representa-
tions of natural language sentences. One task, Auto-
matic Labeling of Semantic Roles (SR), takes as its
theoretical foundation Frame Semantics (Fillmore,
1977) and uses FrameNet (Johnson et al, 2003) as
a data resource for evaluation and system develop-
ment. The definition of the task is simple: given
a natural language sentence and a target word in
the sentence, find other fragments (continuous word
sequences) of the sentence that correspond to ele-
ments of the semantic frame, that is, that serve as
arguments of the predicate introduced by the target
word.
For this task, the systems receive a sentence, a
target word, and a semantic frame (one target word
may belong to multiple frames; hence, for real-
world applications, a preliminary WSD step might
be needed to select an appropriate frame). The out-
put of a system is a list of frame elements, with their
names and character positions in the sentence. The
evaluation of the SR task is based on precision and
recall. For this year?s task, the organizers chose 40
frames from FrameNet 1.1, with 32,560 annnotated
sentences, 8,002 of which formed the test set.
The second task, Identification of Logic Forms
in English (LF), is based on the LF formalism de-
scribed in (Rus, 2002). The LF formalism is a sim-
ple logical form language for natural language se-
mantics with only predicates and variables; there
is no quantification or negation, and atomic predi-
cations are implicitly conjoined. Predicates corre-
spond directly to words and are composed of the
base form of the word, the part of speech tag, and a
sense number (corresponding to the WordNet sense
of the word as used). For the task, the system is
given sentences and must produce LFs. Word sense
disambiguation is not part of the task, so the pred-
icates need not specify WordNet senses. System
evaluation is based on precision and recall of pred-
icates and predicates together with all their argu-
ments as compared to a gold standard.
2 Syntactic Processing
For both tasks, SR and LF, the core of our systems
was the syntactic analysis module described in de-
tail in (Jijkoun and de Rijke, 2004). We only have
space here to give a short overview of the module.
Every sentence was part-of-speech tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
parsed using a state-of-the-art wide coverage phrase
structure parser (Collins, 1999). Both the tagger and
the parser are trained on the Penn Treebank Wall
Street Journal Corpus (WSJ in the rest of this paper)
and thus produce structures similar to those in the
Penn Treebank. Unfortunately, the parser does not
deliver some of the information available in WSJ
that is potentially useful for our two applications:
Penn functional tags (e.g., subject, temporal, closely
related, logical subject in passive) and non-local de-
pendencies (e.g., subject and object control, argu-
ment extraction in relative clauses). Our syntactic
module tries to compensate for this and make this
information explicit in the resulting syntactic analy-
ses.
As a first step, we converted phrase trees pro-
duced by the parser to dependency structures, by
detecting heads of constituents and then propagat-
ing the lexical head information up the syntactic
tree, similarly to (Collins, 1999). The resulting de-
pendency structures were labeled with dependency
labels derived from corresponding Penn phrase la-
bels: e.g., a verb phrase (VP) modified by a prepo-
sitional phrase (PP) resulted in a dependency with
label ?VP|PP?.
Then, the information available in the WSJ (func-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
VP
to seek NP
seats
VP
planned
S
directors
this month
      NP
     NP  S
planned
directors
VP|S
S|NP
S|NP
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
planned
directors
VP|SS|NP?SBJ
S|NP?TMP
S|NP?SBJ
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
(a) (b) (c)
Figure 1: Stages of the syntactic processing: (a) the parser?s output, (b) the result of conversion to a depen-
dency structure, (c) final output of our syntactic module
tional tags, non-local dependencies) was added to
dependency structures using Memory-Based Learn-
ing (Daelemans et al, 2003): we trained the learner
to change dependency labels, or add new nodes or
arcs to dependency structures. Trained and tested
on WSJ, our system achieves state-of-the-art perfor-
mance for recovery of Penn functional tags and non-
local dependencies (Jijkoun and de Rijke, 2004).
Figure 1 shows three stages of the syntactic anal-
ysis of the sentence Directors this month planned to
seek seats (a simplified actual sentence from WSJ):
(a) the phrase structure tree produced by Collins?
parser, (b) the phrase structure tree converted to a
dependency structure and (c) the transformed de-
pendency structure with added functional tags and a
non-local dependency?the final output of our syn-
tactic module. Dependencies are shown as arcs
from heads to dependents.
3 Automatic Labeling of Semantic Roles
For the SR task, we applied a method very similar to
the one used in (Jijkoun and de Rijke, 2004) for re-
covering syntactic structures and somewhat similar
to the first method for automatic semantic role iden-
tification described in (Gildea and Jurafsky, 2002).
Essentially, our method consists of extracting possi-
ble syntactic patterns (paths in syntactic dependency
structures), introducing semantic relations from a
training corpus, and then using a machine learn-
ing classifier to predict which syntactic paths cor-
respond to which frame elements.
Our main assumption was that frame elements,
as annotated in FrameNet, correspond directly to
constituents (constituents being complete subtrees
of dependency structures). Similarly to (Gildea and
Jurafsky, 2002), our own evaluation showed that
about 15% of frame elements in FrameNet 1.1 do
not correspond to constituents, even when applying
some straighforward heuristics (see below) to com-
pensate for this mismatch. This observation puts an
upper bound of around 85% on the accuracy of our
system (with strict evaluation, i.e., if frame element
boundaries must match the gold standard exactly).
Note, though, that these 15% of ?erroneous? con-
stituents also include parsing errors.
Since the core of our SR system operates on
words, constituents, and dependencies, two im-
portant steps are the conversion of FrameNet el-
ements (continuous sequences of characters) into
head words of constituents, and vice versa. The con-
version of FrameNet elements is straightforward:
we take the head of a frame element to be the word
that dominates the most words of this element in
the dependency graph of the sentence. In the other
direction, when converting a subgraph of a depen-
dency graph dominated by a word w into a contin-
uous sequence of words, we take all (i.e., not only
immediate) dependents of w, ignoring non-local de-
pendencies, unless w is the target word of the sen-
tence, in which case we take the word w alone. This
latter heuristic helps us to handle cases when a noun
target word is a semantic argument of itself. Sev-
eral other simple heristics were also found helpful:
e.g., if the result of the conversion of a constituent to
a word sequence contains the target word, we take
only the words to the right of the target word.
With this conversion between frame elements and
constituents, the rest of our system only needs to
operate on words and labeled dependencies.
3.1 Training: the major steps
First, we extract from the training corpus
(dependency-parsed FrameNet sentences, with
words marked as targets and frame elements) all
shortest undirected paths in dependency graphs that
connect target words with their semantic arguments.
In this way, we collect all ?interesting? syntactic
paths from the training corpus.
In the second step, for all extracted syntactic
paths and again for all training sentences, we extract
all occurences of the paths (i.e., paths, starting from
a target word, that actually exist in the dependency
graph), recording for each such occurrence whether
it connects a target word to one of its semantic ar-
guments. For performance reasons, we consider for
each target word only syntactic paths extracted from
sentences annotated with respect to the same frame,
and we ignore all paths of length more than 3.
For every extracted occurrence, we record the
features describing the occurrence of a path in more
detail: the frame name, the path itself, the words
along the path (including the target word and the
possible head of a frame element?first and last
node of the path, respectively), their POS tags and
semantic classes. For nouns, the semantic class
of a word is defined as the hypernym of the first
sense of the noun in WordNet, one of 19 manu-
ally selected terms (animal, person, social group,
clothes, feeling, property, phenomenon, etc.) For
lexical adverbs and prepositions, the semantic class
is one of the 6 clusters obtained automatically using
the K-mean clustering algorithm on data extracted
from FrameNet. Examples of the clusters are:
(abruptly, ironically, slowly, . . . ), (above, beneath,
inside, . . . ), (entirely, enough, better, . . . ). The list
of WordNet hypernyms and the number of clusters
were chosen experimentally. We also added features
describing the subcategorization frame of the tar-
get word; this information is straightforwardly ex-
tracted from the dependency graph. In total, the sys-
tem used 22 features.
The set of path occurrences obtained in the sec-
ond step, with all the extracted features, is a pool of
positive and negative examples of whether certain
syntactic patterns correspond to any semantic argu-
ments. The pool is used as an instance base to train
TiMBL, a memory-based learner (Daelemans et al,
2003), to predict whether the endpoint of a syntac-
tic path starting at a target word corresponds to a
semantic argument, and if so, what its name is.
We chose TiMBL for this task because we had
previously found that it deals successfully with
complex feature spaces and data sparseness (in our
case, in the presence of many lexical features) (Ji-
jkoun and de Rijke, 2004). Moreover, TiMBL is
very flexible and implements many variants of the
basic k-nearest neighbor algorithm. We found that
tuning various parameters (the number of neigh-
bors, weighting and voting schemes) made substan-
tial differences in the performance of our system.
3.2 Applying the system
Once the training is complete, the system can be
applied to new sentences (with the indicated target
word and its frame) as follows. A sentence is parsed
and its dependency structure is built, as described in
Section 2. All occurences of ?interesting? syntac-
tic paths are extracted, along with their features as
described above. The resulting feature vectors are
fed to TiMBL to determine whether the endpoints
of the syntactic paths correspond to semantic argu-
ments of the target word. For the path occurences
classified positively, the constituents of their end-
points are converted to continuous word sequences,
as described earlier; in this case the system has de-
tected a frame element.
3.3 Results
During the development of our system, we used
only the 24,558 sentences from FrameNet set aside
for training by the SR task organizers. To tune the
system, this corpus was randomly split into training
and development sets (70% and 30%, resp.), evenly
for all target words. The official test set (8002 sen-
tences) was used only once to produce the submitted
run, with the whole training set (24,558 sentences)
used for training.
We submitted one run of the system (with iden-
tification of both element boundaries and element
names). Our official scores are: precision 86.9%,
recall 75.2% and overlap 84.7%. Our own evalua-
tion of the submitted run with the strict measures,
i.e., an element is considered correct only if both its
name and boundaries match the gold standard, gave
precision 73.5% and recall 63.6%.
4 Logic Forms
4.1 Method
For the LF task, it was straightforward to turn de-
pendency structures into LFs. Since the LF for-
malism does not attempt to represent the more sub-
tle aspects of semantics, such as quantification, in-
tensionality, modality, or temporality (Rus, 2002),
the primary information encoded in a LF is based
on argument structure, which is already well cap-
tured by the dependency parses. Our LF genera-
tor traverses the dependency structure, turning POS-
tagged lexical items into LF predicates, creating ref-
erential variables for nouns and verbs, and using
dependency labels to order the arguments for each
predicate. We make one change to the dependency
graphs originally produced by the parser. Instead of
taking coordinators, such as and, to modify the con-
stituents they coordinate, we take the coordinated
constituents to be arguments of the coordinator.
Our LF generator builds a labeled directed graph
from a dependency structure and traverses this
graph depth-first. In general, a well-formed depen-
dency graph has exactly one root node, which cor-
responds to the main verb of the sentence. Sen-
tences with multiple independent clauses may have
one root per clause. The generator begins traversing
the graph at one of these root nodes; if there is more
than one, it completes traversal of the subgraph con-
nected to the first node before going on to the next
node.
The first step in processing a node?producing an
LF predicate from the node?s lexical item?is taken
care of in the graph-building stage. We use a base
form dictionary to get the base form of the lexical
item and a simple mapping of Penn Treebank tags
into ?n?, ?v?, ?a?, and ?r? to get the suffix. For words
that are not tagged as nouns, verbs, adjectives, or
adverbs, the LF predicate is simply the word itself.
As the graph is traversed, the processing of a node
depends on its type. The greatest amount of pro-
cessing is required for a node corresponding to a
verb. First, a fresh referential variable is generated
as the event argument of the verbal predication. The
out-edges are then searched for nodes to process.
Since the order of arguments in an LF predication
is important and some sentence constitutents are ig-
nored for the purposes of LF, the out-edges are cho-
sen in order by label: first particles (?VP|PRT?),
then arguments (?S|NP-SBJ?, ?VP|NP?, etc.), and
finally adjuncts. We attempt to follow the argu-
ment order implicit in the description of LF given
in (Rus, 2002), and as the formalism requires, we
ignore auxiliary verbs and negation. The processing
of each of these arguments or adjuncts is handled re-
cursively and returns a set of predications. For mod-
ifiers, the event variable also has to be passed down.
For referential arguments and adjuncts, a referen-
tial variable also is returned to serve as an argument
for the verb?s LF predicate. Once all the arguments
and adjuncts have been processed, a new predica-
tion is generated, in which the verb?s LF predicate
is applied to the event variable and the recursively
generated referential variables. This new predica-
tion, along with the recursively generated ones, is
returned.
The processing of a nominal node proceeds sim-
ilarly. A fresh referential variable is generated?
since determiners are ignored in the LF formalism,
it is simply assumed that all noun phrases corre-
spond to a (possibly composite) individual. Out-
edges are examined for modifiers and recursively
processed. Both the referential variable and the set
of new predications are returned. Noun compounds
introduce some additional complexity; each modi-
fying noun introduces two additional variables, one
for the modifying noun and one for composite indi-
vidual realizing the compound. This latter variable
then replaces the referential variable for the head
noun.
Processing of other types of nodes proceeds in a
similar fashion. For modifiers such as adjectives,
adverbs, and prepositional phrases, a variable (cor-
responding to the individual or event being modi-
fied) is passed in, and the LF predicate of the node
is applied to this variable, rather than to a fresh
variable. In the case of prepositional phrases, the
predicate is applied to this variable and to the vari-
able corresponding to the object of the preposition,
which must be processed, as well. The latter vari-
able is then returned along with the new predica-
tions. For other modifiers, just the predications are
returned.
4.2 Development and results
The rules for handling dependency labels were writ-
ten by hand. Of the roughly 1100 dependency la-
bels that the parser assigns (see Section 2), our sys-
tem handles 45 labels, all of which fall within the
most frequent 135 labels. About 50 of these 135
labels are dependencies that can be ignored in the
generation of LFs (labels involving punctuation, de-
terminers, auxiliary verbs, etc.); of the remaining
85 labels, the 45 labels handled were chosen to pro-
vide reasonable coverage over the sample corpus
provided by the task organizers. Extending the sys-
tem is straightforward; to handle a dependency label
linking two node types, a rule matching the label
and invoking the dependent node handler is added
to the head node handler.
On the sample corpus of 50 sentences to which
our system was tuned, predicate identification, com-
pared to the provided LFs, including POS-tags, was
performed with 89.1% precision and 87.1% recall.
Argument identification was performed with 78.9%
precision and 77.4% recall. On the test corpus of
300 sentences, our official results, which exclude
POS-tags, were 82.0% precision and 78.4% recall
for predicate identification and 73.0% precision and
69.1% recall for argument identification.
We did not get the gold standard for the test cor-
pus in time to perform error analysis for our official
submission, but we did examine the errors in the
LFs we generated for the trial corpus. Most could
be traced to errors in the dependency parses, which
is unsurprising, since the generation of LFs from de-
pendency parses is relatively straightforward. A few
errors resulted from the fact that our system does not
try to identify multi-word compounds.
Some discrepancies between our LFs and the LFs
provided for the trial corpus arose from apparent
inconsistencies in the provided LFs. Verbs with
particles were a particular problem. Sometimes,
as in sentences 12 and 13 of the trial corpus, a
verb-particle combination such as look forward to
is treated as a single predicate (look forward to); in
other cases, such as in sentence 35, the verb and its
particle (go out) are treated as separate predicates.
Other inconsistencies in the provided LFs include
missing arguments (direct object in sentence 24),
and verbs not reduced to base form (felt, saw, and
found in sentences 34, 48, 50).
5 Conclusions
Our main finding during the development of the sys-
tems for the two Senseval tasks was that semantic
relations are indeed very close to syntactic depen-
dencies. Using deep dependency structures helped
to keep the manual rules for the LF task simple
and made the learning for the SR task easier. Also
we found that memory-based learning can be effi-
ciently applied to complex, highly structured prob-
lems such as the identification of semantic roles.
Our future work includes more accurate fine-
tuning of the learner for the SR task, extending the
coverage of the LF generator, and experimenting
with the generated LFs for question answering.
6 Acknowledgments
Ahn and De Rijke were supported by a grant from
the Netherlands Organization for Scientific Re-
search (NWO) under project number 612.066.302.
Fissaha, Jijkoun, and De Rijke were supported by a
grant from NWO under project number 220-80-001.
De Rijke was also supported by grants from NWO,
under project numbers 365-20-005, 612.069.006,
612.000.106, and 612.000.207.
References
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch, 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10. Available from
http://ilk.kub.nl/downloads/pub/papers/ilk0310.pdf.
C. J. Fillmore. 1977. The need for a frame semantics in
linguistics. Statistical Methods in Linguistics, 12:5?
29.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Proceed-
ings of ACL 2004.
C. Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-
penhofer, and C. Fillmore. 2003. Framenet: Theory
and practice. http://www.icsi.berkeley.edu/ framenet.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference.
V. Rus. 2002. Logic Form for WordNet Glosses. Ph.D.
thesis, Southern Methodist University.
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 9?16,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Feature Engineering and Post-Processing for Temporal Expression
Recognition Using Conditional Random Fields
Sisay Fissaha Adafre Maarten de Rijke
Informatics Institute, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
sfissaha,mdr@science.uva.nl
Abstract
We present the results of feature engineer-
ing and post-processing experiments con-
ducted on a temporal expression recogni-
tion task. The former explores the use of
different kinds of tagging schemes and of
exploiting a list of core temporal expres-
sions during training. The latter is con-
cerned with the use of this list for post-
processing the output of a system based on
conditional random fields.
We find that the incorporation of knowl-
edge sources both for training and post-
processing improves recall, while the use
of extended tagging schemes may help
to offset the (mildly) negative impact on
precision. Each of these approaches ad-
dresses a different aspect of the over-
all recognition performance. Taken sep-
arately, the impact on the overall perfor-
mance is low, but by combining the ap-
proaches we achieve both high precision
and high recall scores.
1 Introduction
Temporal expressions (timexes) are natural language
phrases that refer directly to time points or intervals.
They not only convey temporal information on their
own but also serve as anchors for locating events re-
ferred to in a text. Timex recognition is a named
entity recognition (NER) task to which a variety of
natural language processing and machine learning
techniques have been applied. As with other NER
tasks, timex recognition is naturally viewed as a se-
quence labeling task, easily lending itself to ma-
chine learning techniques such as conditional ran-
dom fields (CRFs) (Lafferty et al, 2001).
A preliminary experiment showed that, using
CRFs, a respectable recognition performance can
easily be achieved with a straightforward baseline
system that is based on a simple tagging scheme and
requires very little tuning, yielding F-scores around
0.78 (exact match) or even 0.90 (partial match).
Interestingly, these high scores are mainly due to
high or even very high precision scores, while recall
leaves much to be improved.
The main focus of this paper is on boosting re-
call while maintaining precision at an acceptable
(i.e., high) level. We report on two types of ex-
periments aimed at achieving this goal. One type
concerns feature engineering and the other concerns
post-processing the output of a machine learner.
While we do exploit the special nature of timexes,
for portability reasons we avoid using task-specific
and richer linguistic features (POS, chunks, etc.). In-
stead, we focus on features and techniques that can
readily be applied to other NER tasks.
Specifically, our feature engineering experiments
have two facets. The first concerns identification of
a set of simple features that results in high general-
ization ability (accuracy). Here, particular emphasis
will be placed on the use of a list of core timexes as
a feature. The assumption is that the performance of
data-driven approaches for timex recognition can be
improved by taking into account the peculiar prop-
erties of timexes. Timexes exhibit various patterns,
ranging from regular patterns that can easily be cap-
tured using simple regular expressions to complex
linguistic forms (phrases). While timexes are real-
9
ized in different phrase types, the core lexical items
of timexes are restricted. This suggests that a list
of core timexes can easily be compiled and used in
machine learning-based timex recognition. One ap-
proach of integrating such a list is using them to gen-
erate features, but the availability of such a list also
opens up other possibilities in feature design that we
present in later sections.
The second aspect concerns the tagging scheme.
As in most NER experiments, the task of recogniz-
ing timexes is reduced to tagging. Commonly used
tagging schemes are Inside-Outside (IO) and Begin-
Continue-End-Unique-Negative (BCEUN) (Borth-
wick et al, 1998). The IO tagging scheme, which we
use as a baseline, assigns the tag I to a token if it is
part of a timex and O otherwise. The richer BCEUN
scheme assigns the five tags B, C, E, U, and N to to-
kens depending on whether the token is single-token
timex (U), a non-timex (N), appears at the beginning
(B), at the end (E) or inside a timex boundary (C). In
this paper, we compare the IO, BCEUN and an ex-
tended form of the BCEUN tagging scheme. The
extended scheme adds two tags, PRE and POST, to
the BCEUN scheme, which correspond to tokens ap-
pearing to the left and to the right of a timex.
In contrast, our post-processing experiments in-
vestigate the application of the list of core timexes
for filtering the output of a machine learner. The in-
corporation into the recognition process of explicit
knowledge in the form of a list for post-processing
requires a carefully designed strategy to ensure that
the important properties of the trained model are
kept intact as much as possible while at the same-
time improving overall results. We present an ap-
proach for using a list for post-processing that ex-
ploits the knowledge embodied in the trained model.
The paper is organized as follows. In Section 2
we provide background material, both on the timex
extraction task (?2.1) and on the machine learning
techniques on which we build in this paper, condi-
tional random fields (?2.2). Our ideas on engineer-
ing feature sets and tagging schemes are presented
in Section 3, while we describe our method for ex-
ploiting the explicit knowledge contained in a list in
Section 4. In Section 5, we describe the experimen-
tal setup and present the results of our experiments.
Related work is briefly reviewed in Section 6, and
we conclude in Section 7.
2 Background
2.1 Task Description
In recent years, temporal aspects of information ac-
cess have received increasing amounts of attention,
especially as it relates to news documents. In addi-
tion to factual content, news documents have a tem-
poral context, reporting events that happened, are
happening, or will happen in relation to the publi-
cation date. Temporal document retrieval concerns
the inclusion of both the document publication date
and the in-text temporal expressions in the retrieval
model (Kalczynski and Chou, 2005). The task in
which we are interested in this paper is identifying
the latter type of expressions, i.e., extraction of tem-
poral expressions. TERN, the Temporal Expression
Recognition and Normalization Evaluation, is orga-
nized under the auspices of the Automatic Content
Extraction program (ACE, http://www.nist.
gov/speech/tests/ace/). The TERN evalu-
ation provides specific guidelines for the identifica-
tion and normalization of timexes, as well as tagged
corpora for training and testing and evaluation soft-
ware. These guidelines and resources were used for
the experiments described below.
The TERN evaluation consisted of two distinct
tasks: recognition and normalization. Timex recog-
nition involves correctly detecting and delimiting
timexes in text. Normalization involves assigning
recognized timexes a fully qualified temporal value.
Our focus in this paper is on the recognition task;
it is defined, for human annotators, in the TIDES
TIMEX2 annotation guidelines (Ferro et al, 2004).
The recognition task is performed with respect to
corpora of transcribed broadcast news speech and
news wire texts from ACE 2002, ACE 2003, and
ACE 2004, marked up in SGML format and, for
the training set, hand-annotated for TIMEX2s. An
official scorer that evaluates the recognition perfor-
mance is provided as part of the TERN evaluation. It
computes precision, recall, and F-measure both for
TIMEX2 tags (i.e., for overlap with a gold standard
TIMEX2 element) and for extent of TIMEX2 ele-
ments (i.e., exact match of entire timexes).
2.2 Conditional Random Fields
We view the recognition of timexes task as a se-
quence labeling task in which each token in the text
10
is classified as being either a timex or not. One ma-
chine learning technique that has recently been in-
troduced to tackle the problem of labeling and seg-
menting sequence data is conditional random fields
(CRFs, (Lafferty et al, 2001)). CRFs are conditional
probability distributions that take the form of ex-
ponential models. The special case of linear chain
CRFs, which takes the following form, has been
widely used for sequence labeling tasks:
P (y | x) =
1
Z (x)
exp
(
?
t=1
?
k
?kfk (t, yt?1, yt, x)
)
,
where Z (x) is the normalization factor, X =
{x1, . . . , xn} is the observation sequence, Y =
{y1, . . . , yT } is the label sequences, fk and ?k are
the feature functions and their weights respectively.
An important property of these models is that proba-
bilities are computed based on a set of feature func-
tions, i.e., fk (usually binary valued), which are de-
fined on both the observation X and label sequences
Y . These feature functions describe different aspect
of the data and may overlap, providing a flexible way
of describing the task.
CRFs have been shown to perform well in a
number of natural language processing applications,
such as POS tagging (Lafferty et al, 2001), shallow
parsing or NP chunking (Sha and Pereira, 2003), and
named entity recognition (McCallum and Li, 2003).
In this paper, CRFs are applied to the recognition of
timexes; in our experiments we used the minorThird
implementation of CRFs (Cohen, 2004).
3 Feature Engineering
The success of applying CRFs depends on the qual-
ity of the set of features used and the tagging scheme
chosen. Below, we discuss these two aspects in
greater detail.
3.1 Feature sets
Our baseline feature set consists of simple lexical
and character features. These features are derived
from a context window of two words (left and right).
Specifically, the features are the lowercase form of
all the tokens in the span, with each token contribut-
ing a separate feature, and the tokens in the left and
right context window constitute another set of fea-
tures. These feature sets capture the lexical con-
tent and context of timexes. Additionally, charac-
ter type pattern features (such as capitalization, digit
sequence) of tokens in the timexes are used to cap-
ture the character patterns exhibited by some of the
tokens in temporal expressions. These features con-
stitute the basic feature set.
Another important feature is the list of core
timexes. The list is obtained by first extracting the
phrases with -TMP function tags from the PennTree
bank, and taking the words in these phrases (Marcus
et al, 1993). The resulting list is filtered for stop-
words. Among others, the list of core timexes con-
sists of the names of days of the week and months,
temporal units ?day,? ?month,? ?year,? etc. This list
is used to generate binary features. In addition, the
list is used to guide the design of other complex fea-
tures that may involve one or more of token-tag pairs
in the context of the current token. One way of using
the list for this purpose is to generate a feature that
involves bi-grams tokens. In certain cases, informa-
tion extracted from bi-grams, e.g. +Xx 99 (May 20),
can be more informative than information generated
from individual tokens. We refer to these features as
the list feature set.
3.2 Tagging schemes
A second aspect of feature engineering that we
consider in this paper concerns different tagging
schemes. As mentioned previously, the task of rec-
ognizing timexes is reduced to a sequence-labeling
task. We compare three tagging schemes, IO
(our baseline), BCEUN, and BCEUN+PRE&POST.
While the first two are relatively standard, the last
one is an extension of the BCEUN scheme. The
intuition underlying this tagging scheme is that the
most relevant features for timex recognition are ex-
tracted from the immediate context of the timex,
e.g., the word ?During? in (1) below.
(1) During <TIMEX2>the past week</TIMEX2>,
the storm has pounded the city.
During-PRE the-B past-C week-E ,-POST the
storm has pounded the city.
Therefore, instead of treating these elements uni-
formly as outside (N), which ignores their relative
importance, we conjecture that it is worthwhile to
11
assign them a special category, like PRE and POST
corresponding to the tokens immediately preceding
and following a timex, and that this leads to im-
proved results.
4 Post-processing Using a List
In this section, we describe the proposed method
for incorporating a list of core lexical timexes for
post-processing the output of a machine learner. As
we will see below, although the baseline system
(with the IO tagging scheme and the basic feature
set) achieves a high accuracy, the recall scores leave
much to be desired. One important problem that we
have identified is that timexes headed by core lexical
items on the list may be missed. This is either due
to the fact that some of these lexical items are se-
mantically ambiguous and appear in a non-temporal
sense, or the training material does not cover the par-
ticular context. In such cases, a reliable list of core
timexes can be used to identify the missing timexes.
For the purposes of this paper, we have created a
list containing mainly headwords of timexes. These
words are called trigger words since they are good
indicators of the presence of temporal expressions.
How can we use trigger words? Before describ-
ing our method in some detail, we briefly describe
a more naive (and problematic) approach. Observe
that trigger words usually appear in a text along with
their complements or adjuncts. As a result, pick-
ing only these words will usually contribute to token
recall but span precision is likely to drop. Further-
more, there is no principled way of deciding which
one to pick (semantically ambiguous elements will
also be picked). Let?s make this more precise. The
aim is to take into account the knowledge acquired
by the trained model and to search for the next op-
timal sequence of tags, which assigns the missed
timex a non-negative tag. However, searching for
this sequence by taking the whole word sequence
is impractical since the number of possible tag se-
quences (number of all possible paths in a viterbi
search) is very large. But if one limits the search to
a window of size n (n < 6), sequential search will
be feasible. The method, then, works on the output
of the system. We illustrate the method by using the
example given in (2) below.
(2) The chairman arrived in the city yesterday, and
will leave next week. The press conference will
be held tomorrow afternoon.
Now, assume that (2) is a test instance (a two-
sentence document), and that the system returns the
following best sequence (3). For readability, the tag
N is not shown on the words that are assigned nega-
tive tags in all the examples below.
(3) The chairman arrived in the city yesterday-U ,
and will leave next week . The press conference
will be held tomorrow-B afternoon-E .
According to (3), the system recognizes only ?yes-
terday? and ?tomorrow afternoon? but misses ?next
week?. Assuming our list of timexes contains the
word ?week?, it tells us that there is a missing tem-
poral expression, headed by ?week.? The naive
method is to go through the above output sequence
and change the token-tag pair ?week-N? to ?week-
U?. This procedure recognizes the token ?week? as a
valid temporal expression, but this is not correct: the
valid temporal expression is ?next week?.
We now describe a second approach to incorpo-
rating the knowledge contained in a list of core lexi-
cal timexes as a post-processing device. To illustrate
our ideas, take the complete sequence in (3) and ex-
tract the following segment, which is a window of 7
tokens centered at ?week?.
(4) . . . [will leave next week . The press] . . .
We reclassify the tokens in (4) assuming the history
contains the token ?and? (the token which appears to
the left of this segment in the original sequence) and
the associated parameters. Of course, the best se-
quence will still assign both ?next? and ?week? the N
tag since the underlying parameters (feature sets and
the associated weights) are the same as the ones in
the system. However, since the word sequence in (4)
is now short (contains only 7 words) we can main-
tain a list of all possible tag sequences for it and per-
form a sequential search for the next best sequence,
which assigns the ?week? token a non-negative tag.
Assume the new tag sequence looks as follows:
(5) . . . [will leave next-B week-E . The press] . . .
This tag sequence will then be placed back into the
original sequence resulting in (6):
12
(6) The chairman arrived in the city yesterday-U ,
and will leave next-B week-E . The press con-
ference will be held tomorrow-B afternoon-E .
In this case, all the temporal expressions will be ex-
tracted since the token sequence ?next week? is prop-
erly tagged. Of course, the above procedure can also
return other, invalid sequences as in (7):
(7) a. . . . will leave next-B week-C . The press . . .
b. . . . will leave next week-C . The press . . .
c. . . . will leave next week-C .-E The press . . .
The final extraction step will not return any timex
since none of the candidate sequences in (7) contains
a valid tag sequence. The assumption here is that of
all the tag sequences, which assign the token ?week?
a non-negative tag, those tag sequences which con-
tain the segment ?next-B week-E? are likely to re-
ceive a higher weight since the underlying system
is trained to recognize temporal expressions and the
phrase ?next week? is a likely temporal expression.
This way, we hypothesize, it is possible to ex-
ploit the knowledge embodied in the trained model.
As pointed out previously, simply going through
the list and picking only head words like ?week?
will not guarantee that the extracted tokens form a
valid temporal expression. On the other hand, the
above heuristics, which relies on the trained model,
is likely to pick the adjunct ?next?.
The post-processing method we have just out-
lined boils down to reclassifying a small segment
of a complete sequence using the same parameters
(feature sets and associated weights) as the original
model, and keeping all possible candidate sequences
and searching through them to find a valid sequence.
5 Experimental Evaluation
In this section we provide an experimental assess-
ment of the feature engineering and post-processing
methods introduced in Sections 3 and 4. Specifi-
cally, we want to determine what their impact is on
the precision and recall scores of the baseline sys-
tem, and how they can be combined to boost recall
while keeping precision at an acceptable level.
5.1 Experimental data
The training data consists of 511 files, and the test
data consists of 192 files; these files were made
available in the 2004 Temporal Expression Recog-
nition and Normalization Evaluation. The tempo-
ral expressions in the training files are marked with
XML tags. The minorThird system takes care of
automatically converting from XML format to the
corresponding tagging schemes. A temporal expres-
sion enclosed by <TIMEX2> tags constitutes a span.
The features in the training instances are generated
by looking at the surface forms of the tokens in the
spans and their surrounding contexts.
5.2 Experimental results
Richer feature sets Table 1 lists the results of the
first part of our experiments. Specifically, for every
tagging scheme, there are two sets of features, basic
and list. The results are based on both exact-match
and partial match between the spans in the gold stan-
dard and the spans in the output of the systems, as
explained in Subsection 2.1. In both the exact and
partial match criteria, the addition of the list features
leads to an improvement in recall, and no change or
a decrease in precision.
In sum, the feature addition helps recall more than
it hurts precision, as the F score goes up nearly ev-
erywhere, except for the exact-match/baseline pair.
Tagging schemes In Table 1 we also list the ex-
traction scores for the tagging schemes we con-
sider, IO, BCEUN, and BCEUN+PRE&POST, as
described in Section 3.2.
Let us first look at the impact of the different tag-
ging schemes in combination with the basic feature
set (rows 3, 5, 7). As we go from the baseline
tagging scheme IO to the more complex BCEUN
and BCEUN+PRE&POS, precision increases on
the exact-match criterion but remains almost the
same on the partial match criterion. Recall, on
the other hand, does not show the same trend.
BCEUN has the highest recall values followed by
BCEUN+PRE&POST and finally IO. In general,
IO based tagging seems to perform worse whereas
BCEUN based tagging scores slightly above its ex-
tended tagging scheme BCEUN+PRE&POST.
Next, considering the combination of extend-
ing the feature set and moving to a richer tagging
scheme (rows 4, 6, 8), we have very much the same
pattern. In both the exact match and the partial
match setting, BCEUN tops (or almost tops) the two
13
Exact Match Partial Match
Tagging scheme Features Prec. Rec. F Prec. Rec. F
IO (baseline) basic 0.846 0.723 0.780 0.973 0.832 0.897
basic + list 0.822 0.736 0.776 0.963 0.862 0.910
BCEUN basic 0.874 0.768 0.817 0.974 0.856 0.911
basic + list 0.872 0.794 0.831 0.974 0.887 0.928
BCEUN+PRE&POS basic 0.882 0.749 0.810 0.979 0.831 0.899
basic + list 0.869 0.785 0.825 0.975 0.881 0.925
Table 1: Timex: Results of training on basic and list features, and different tagging schemes. Highest scores
(Precision, Recall, F-measure) are in bold face.
other schemes in both precision and recall.
In sum, the richer tagging schemes function as
precision enhancing devices. The effect is clearly
visible for the exact-match setting, but less so for
partial matching. It is not the case that the learner
trained on the richest tagging scheme outperforms
all learners trained with poorer schemes.
Post-processing Table 2 shows the results of ap-
plying the post-processing method described in
Section 4. One general pattern we observe in
Table 2 is that the addition of the list features
improves precision for IO and BCEUN tagging
scheme and shows a minor reduction in precision
for BCEUN+PRE&POS tagging scheme in both
matching criteria. Similarly, in the presence of
post-processing, the use of a more complex tagging
scheme results in a better precision. On the other
hand, recall shows a different pattern. The addi-
tion of list features improves recall both for BCEUN
and BCEUN+PRE&POS, but hurts recall for the IO
scheme for both matching criteria.
Comparing the results in Table 1 and Table 2,
we see that post-processing is a recall enhancing
device since all the recall values in Table 2 are
higher than the recall values in Table 1. Pre-
cision values in Table 2, on the other hand, are
lower than those of Table 1. Importantly, the
use of a more complex tagging scheme such as
BCEUN+PRE&POS, allows us to minimize the
drop in precision. In general, the best result (on
partial match) in Table 1 is achieved through the
combination of BCEUN and basic&list features
whereas the best result in Table 2 is achieved by
the combination of BCEUN+PRE&POS and basic
&list features. Although both have the same over-
all scores on the exact match criteria, the latter per-
forms better on partial match criteria. This, in turn,
shows that the combination of post-processing, and
BCEUN+PRE&POS achieves better results.
Stepping back We have seen that the extended
tagging scheme and the post-processing methods
improve on different aspects of the overall per-
formance. As mentioned previously, the ex-
tended tagging scheme is both recall and precision-
oriented, while the post-processing method is pri-
marily recall-oriented. Combining these two meth-
ods results in a system which maintains both these
properties and achieves a better overall result. In or-
der to see how these two methods complement each
other it is sufficient to look at the highest scores
for both precision and recall. The extended tagging
scheme with basic features achieves the highest pre-
cision but has relatively low recall. On the other
hand, the simplest form, the IO tagging scheme
and basic features with post-processing, achieves
the highest recall and the lowest precision in par-
tial match. This shows that the IO tagging scheme
with basic features imposes a minimal amount of
constraints, which allows for most of the timexes in
the list to be extracted. Put differently, it does not
discriminate well between the valid vs invalid oc-
currences of timexes from the list in the text. At the
other extreme, the extended tagging scheme with 7
tags imposes strict criteria on the type of words that
constitute a timex, thereby restricting which occur-
rences of the timex in the list count as valid timexes.
In general, although the overall gain in score is
limited, our feature engineering and post-processing
efforts reveal some interesting facts. First, they show
one possible way of using a list for post-processing.
14
Exact Match Partial Match
Tagging scheme Features Prec. Rec. F Prec. Rec. F
IO basic (baseline) 0.846 0.723 0.780 0.973 0.832 0.897
basic 0.756 0.780 0.768 0.902 0.931 0.916
basic + list 0.772 0.752 0.762 0.930 0.906 0.918
BCEUN basic 0.827 0.789 0.808 0.945 0.901 0.922
basic + list 0.847 0.801 0.823 0.958 0.906 0.931
BCEUN+PRE&POS basic 0.863 0.765 0.811 0.973 0.863 0.915
basic + list 0.861 0.804 0.831 0.970 0.906 0.937
Table 2: Timex: Results of applying post-processing on the systems in Table 1. The baseline (from Table 1)
is repeated for ease of reference; it does not use post-processing. Highest scores (Precision, Recall, F-
measure) are in bold face.
This method is especially appropriate for situations
where better recall is important. It offers a means of
controlling the loss in precision (gain in recall) by
allowing a systematic process of recovering missing
expressions that exploits the knowledge already em-
bodied in a probabilistically trained model, thereby
reducing the extent to which we have to make ran-
dom decisions. The method is particularly sensitive
to the criterion (the quality of the list in the current
experiment) used for post-processing.
6 Related Work
A large number of publications deals with extraction
of temporal expressions; the task is often treated as
part of a more involved task combining recognition
and normalization of timexes. As a result, many
timex interpretation systems are a mixture of both
rule-based and machine learning approaches (Mani
and Wilson, 2000). This is partly due to the fact that
timex recognition is more amenable to data-driven
methods whereas normalization is best handled us-
ing primarily rule-based methods. We focused on
machine learning methods for the timex recognition
task only. See (Katz et al, 2005) for an overview of
methods used for addressing the TERN 2004 task.
In many machine learning-based named-entity
recognition tasks dictionaries are used for improving
results. They are commonly used to generate binary
features. Sarawagi and Cohen (2004) showed that
semi-CRFs models for NE recognition perform bet-
ter than conventional CRFs. One advantage of semi-
CRFs models is that the units that will be tagged are
segments which may contain one or more tokens,
rather than single tokens as is done in conventional
CRFs. This in turn allows one to incorporate seg-
ment based-features, e.g., segment length, and also
facilitates integration of external dictionaries since
segments are more likely to match the entries of an
external dictionary than tokens. In this paper, we
stuck to conventional CRFs, which are computation-
ally less expensive, and introduced post-processing
techniques, which takes into account knowledge em-
bodied in the trained model.
Kristjannson et al (2004) introduced constrained
CRFs (CCRFs), a model which returns an optimal
label sequence that fulfills a set of constraints im-
posed by the user. The model is meant to be used in
an interactive information extraction environment,
in which the system extracts structured information
(fields) from a text and presents it to the user, and
the user makes the necessary correction and submits
it back to the system. These corrections constitute
an additional set of constraints for CCRFs. CCRFs
re-computes the optimal sequence by taking these
constraints into account. The method is shown to
reduce the number of user interactions required in
validating the extracted information. In a very lim-
ited sense our approach is similar to this work. The
list of core lexical timexes that we use represents
the set of constraints on the output of the underly-
ing system. However, our method differs in the way
in which the constraints are implemented. In our
case, we take a segment of the whole sequence that
contains a missing timex, and reclassify the words
in this segment while keeping all possible tag se-
quences sorted based on their weights. We then
15
search for the next optimal sequence that assigns the
missing timex a non-negative tag sequentially. On
the other hand, Kristjannson et al (2004) take the
whole sequence and recompute an optimal sequence
that satisfies the given constraints. The constraints
are a set of states which the resulting optimal se-
quence should include.
7 Conclusion
In this paper we presented different feature engi-
neering and post-processing approaches for improv-
ing the results of timex recognition task. The first
approach explores the different set of features that
can be used for training a CRF-based timex recog-
nition system. The second investigates the effect of
the different tagging scheme for timex recognition
task. The final approach we considered applies a list
of core timexes for post-processing the output of a
CRF system. Each of these approaches addresses
different aspects of the overall performance. The
use of a list of timexes both during training and for
post-processing resulted in improved recall whereas
the use of a more complex tagging scheme results
in better precision. Their individual overall contri-
bution to the recognition performances is limited or
even negative whereas their combination resulted in
substantial improvements over the baseline.
While we exploited the special nature of timexes,
we did avoid using linguistic features (POS, chunks,
etc.), and we did so for portability reasons. We fo-
cused exclusively on features and techniques that
can readily be applied to other named entity recog-
nition tasks. For instance, the basic and list features
can also be used in NER tasks such as PERSON,
LOCATION, etc. Moreover, the way that we have
used a list of core expressions for post-processing is
also task-independent, and it can easily be applied
for other NER tasks.
Acknowledgments
Sisay Fissaha Adafre was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project number 220-80-001. Maarten de
Rijke was supported by grants from NWO, under
project numbers 365-20-005, 612.069.006, 220-80-
001, 612.000.106, 612.000.207, 612.066.302, 264-
70-050, and 017.001.190.
References
[Borthwick et al1998] A. Borthwick, J. Sterling,
E. Agichtein, and R. Grishman. 1998. Exploiting
diverse knowledge sources via maximum entropy in
named entity recognition. In Workshop on Very Large
Corpora, ACL.
[Cohen2004] W. Cohen. 2004. Methods for identifying
names and ontological relations in text using heuris-
tics for inducing regularities from data. http://
minorthird.sourceforge.net.
[Ferro et al2004] L. Ferro, L. Gerber, I. Mani, and
G. Wilson, 2004. TIDES 2003 Standard for the An-
notation of Temporal Expressions. MITRE, April.
[Kalczynski and Chou2005] P.J. Kalczynski and A. Chou.
2005. Temporal document retrieval model for business
news archives. Information Processing and Manage-
ment, 41:635?650.
[Katz et al2005] G. Katz, J. Pustejovsky, and F. Schilder,
editors. 2005. Proceedings Dagstuhl Workshop on
Annotating, Extracting, and Reasoning about Time
and Events.
[Kristjannson et al2004] T. Kristjannson, A. Culotta,
P. Viola, and A. McCallum. 2004. Interactive infor-
mation extraction with constrained conditional random
fields. In Nineteenth National Conference on Artificial
Intelligence, AAAI.
[Lafferty et al2001] J. Lafferty, F. Pereira, and A. McCal-
lum. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the International Conference on Ma-
chine Learning.
[Mani and Wilson2000] I. Mani and G. Wilson. 2000.
Robust temporal processing of news. In Proceedings
of the 38th ACL.
[Marcus et al1993] M.P. Marcus, B. Santorini, and M.A.
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19:313?330.
[McCallum and Li2003] A. McCallum and W. Li. 2003.
Early results for Named Entity Recognition with con-
ditional random fields, feature induction and web-
enhanced lexicons. In Proceedings of the 7th CoNLL.
[Sarawagi and Cohen2004] S. Sarawagi and W.W. Cohen.
2004. Semi-markov conditional random fields for in-
formation extraction. In NIPs (to appear).
[Sha and Pereira2003] F. Sha and F. Pereira. 2003. Shal-
low parsing with conditional random fields. In Pro-
ceedings of Human Language Technology-NAACL.
16
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 47?54,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Part of Speech tagging for Amharic using Conditional Random Fields
Sisay Fissaha Adafre
Informatics Institute, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
sfissaha@science.uva.nl
Abstract
We applied Conditional Random Fields
(CRFs) to the tasks of Amharic word seg-
mentation and POS tagging using a small
annotated corpus of 1000 words. Given
the size of the data and the large number of
unknown words in the test corpus (80%),
an accuracy of 84% for Amharic word
segmentation and 74% for POS tagging
is encouraging, indicating the applicabil-
ity of CRFs for a morphologically com-
plex language like Amharic.
1 Introduction
Part-of-speech (POS) tagging is often considered
as the first phase of a more complex natural lan-
guage processing application. The task is partic-
ularly amenable to automatic processing. Specifi-
cally, POS taggers that are trained on pre-annotated
corpora achieve human-like performance, which is
adequate for most applications. The road to such
high performance levels is, however, filled with a
hierarchy of sub-problems. Most techniques gener-
ally assume the availability of large POS annotated
corpora. The development of annotated corpora in
turn requires a standard POS tagset. None of these
resources are available for Amharic. This is due
mainly to the fact that data preparation, i.e., devel-
oping a comprehensive POS tagset and annotating a
reasonably sized text, is an arduous task. Although
the POS tagging task, taken as a whole, seems chal-
lenging, a lot can be gained by analyzing it into sub-
problems and dealing with each one step-by-step,
and also bringing in the experience from other lan-
guages in solving these problems, since POS taggers
have been developed for several languages resulting
in a rich body of knowledge.
Several attempts have been made in the past
to develop algorithms for analyzing Amharic
words. Among these is the stemming algorithm
of Nega (1999), which reduces Amharic words
into their common stem forms by removing affixes.
Nega?s work focuses on investigating the effective-
ness of the stemming algorithm in information re-
trieval for Amharic. Abyot (2000) developed a word
parser for Amharic verbs that analyses verbs into
their constituting morphemes and determines their
morphosyntactic categories. Abyot?s work only cov-
ers verbs and their derivations. Mesfin (2001) devel-
oped a Hidden Markov Model (HMM) based part of
speech tagger for Amharic. Building on the work of
Mesfin, Atelach (2002) developed a stochastic syn-
tactic parser for Amharic. Sisay and Haller (2003a;
2003b) applied finite-state tools, and corpus-based
methods for the Amharic morphological analysis.
This work provided important insights into the is-
sues surrounding the development of Amharic nat-
ural language processing applications, especially, in
compiling a preliminary POS tagset for Amharic.
In this paper, our aim is to explore recent develop-
ments in the morphological analysis of related lan-
guages, such as Arabic and Hebrew, and machine
learning approaches, and apply them to the Amharic
language. Amharic belongs to the Semitic family of
languages, and hence shares a number of common
morphological properties with Arabic and Hebrew
for which active research is being carried out. Stud-
47
ies on these languages propose two alternative POS
tagging approaches which differ on the unit of anal-
ysis chosen; morpheme-based and word-based (Bar-
Haim et al, 2004). The former presupposes a seg-
mentation phase in which words are analysed into
constituting morphemes which are then passed to
the POS tagging step, whereas the latter applies POS
tagging directly on fully-inflected word forms. Due
to scarce resources, it is impossible for us to fully
carry out these tasks for Amharic. Therefore, the
segmentation and POS tagging tasks are carried out
independently. Furthermore, POS tagging is applied
only on fully-inflected word forms. The motivation
for doing the segmentation task comes from the need
to provide some measure of the complexity of the
task in the context of the Amharic language. As
regards implementation, new models have been in-
troduced recently for segmentation and sequence-
labeling tasks. One such model is Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001). In this
paper, we describe important morphosyntactic char-
acteristics of Amharic, and apply CRFs to Amharic
word segmentation and POS tagging.
The paper is organized as follows. Section 2 pro-
vides a brief description of Amharic morphology.
Section 3 presents some of the work done in the
area of Amharic morphological analysis, and exam-
ines one POS tagset proposed by previous studies.
This tagset has been revised and applied on a sample
Amharic newspaper text, which is discussed in Sec-
tion 4. Section 5 describes the tasks in greater de-
tail. Section 6 provides a brief description of CRFs,
the machine learning algorithm that will be applied
in this paper. Section 7 describes the experimental
setup and Section 8 presents the result of the exper-
iment. Finally, Section 9 makes some concluding
remarks.
2 Amharic Morphology
Amharic is one of the most widely spoken lan-
guages in Ethiopia. It has its own script that is bor-
rowed from Ge?ez, another Ethiopian Semitic lan-
guage (Leslau, 1995). The script is believed to have
originated from the South Sabean script. It is a syl-
labary writing system where each character repre-
sents an open CV syllable, i.e., a combination of a
consonant followed by a vowel (Daniels, 1997).
Amharic has a complex morphology. Word
formation involves prefixation, suffixation, infixa-
tion, reduplication, and Semitic stem interdigitation,
among others. Like other Semitic languages, e.g.,
Arabic, Amharic verbs and their derivations con-
stitute a significant part of the lexicon. In Semitic
languages, words, especially verbs, are best viewed
as consisting of discontinuous morphemes that are
combined in a non-concatenative manner. Put dif-
ferently, verbs are commonly analyzed as consist-
ing of root consonants, template patterns, and vowel
patterns. With the exception of very few verb forms
(such as the imperative), all derived verb forms take
affixes in order to appear as independent words.
Most function words in Amharic, such as Con-
junction, Preposition, Article, Relative marker,
Pronominal affixes, Negation markers, are bound
morphemes, which are attached to content words,
resulting in complex Amharic words composed of
several morphemes. Nouns inflect for the mor-
phosyntactic features number, gender, definiteness,
and case. Amharic adjectives share some morpho-
logical properties with nouns, such as definiteness,
case, and number. As compared to nouns and verbs,
there are fewer primary adjectives. Most adjec-
tives are derived from nouns or verbs. Amharic
has very few lexical adverbs. Adverbial meaning
is usually expressed morphologically on the verb or
through prepositional phrases. While prepositions
are mostly bound morphemes, postpositions are typ-
ically independent words.
The segmentation task (cf. Section 7.1) consid-
ers the following bound morphemes as segments:
Prepositions, Conjunctions, Relative Makers, Aux-
iliary verbs, Negation Marker and Coordinate Con-
junction. Other bound morphemes such as definite
article, agreement features (i.e., number, gender),
case markers, etc are not considered as segments and
will be treated as part of the word. These are chosen
since they are commonly treated as separate units in
most syntactic descriptions.
Although the above description of Amharic is far
from complete, it highlights some of the major char-
acteristics of Amharic, which it shares with other
Semitic languages such as Arabic. It is, therefore,
worthwhile to take into consideration the work done
for other Semitic languages in proposing a method
for Amharic natural language processing.
48
3 Amharic POS Tagset
Mesfin (2001) compiled a total of 25 POS tags: N,
NV, NB, NP, NC, V, AUX, VCO, VP, VC, J, JC,
JNU, JPN, JP, PREP, ADV, ADVC, C, REL, ITJ,
ORD, CRD, PUNC, and UNC. These tags capture
important properties of the language at a higher level
of description. For example, the fact that there
is no category for Articles indicates that Amharic
does not have independent lexical forms for arti-
cles. However, a close examination of the de-
scription of some of the tags reveals some miss-
classification that we think will lead to tagging in-
consistency. For example, the tag JPN is assigned
to nouns with the ?ye? prefix morpheme that func-
tion as an adjective, e.g. yetaywan sahn - A
Taiwan made plate (Mesfin, 2001). This ex-
ample shows that grammatical function takes prece-
dence over morphological form in deciding the POS
category of a word. In Amharic, the ye+NOUN con-
struction can also be used to represent other kinds
of relation such as Possession relation. On the
other hand, the ye+NOUN construction is a simple
morphological variant of the NOUN that can easily
be recognized. Therefore, treating ye+NOUN con-
struction as a subclass of a major noun class will re-
sult in a better tagging consistency than treating it as
an adjective. Furthermore, a hierarchical tagset, or-
ganized into major classes and subclasses, seems to
be a preferred design strategy (Wilson, 1996; Khoja
et al, 2001). Although it is possible to guess (from
the tagset description) some abstract classes such as,
N* (nouns), V* (verbs), J* (adjectives), etc., such a
hierarchical relation is not clearly indicated. One ad-
vantage of such a hierarchical organization is that it
allows one to work at different levels of abstraction.
The POS tags that are used in this paper are ob-
tained by collapsing some of the categories proposed
by Mesfin (2001). The POS tags are Noun (N), Verb
(V), Auxiliary verbs (AUX), Numerals (NU), Ad-
jective (AJ), Adverb (AV), Adposition (AP), Inter-
jection (I), Residual (R), and Punctuation (PU). The
main reason for working with a set of abstract POS
tags is resource limitation, i.e., the absence of a large
annotated corpus. Since we are working on a small
annotated corpus, 25 POS tags make the data sparse
and the results unreliable. Therefore, we have found
it necessary to revise the tagset.
4 Application of the Revised Tagset
The above abstract POS tags are chosen by tak-
ing into account the proposals made in Amharic
grammar literature and the guidelines of other lan-
guages (Baye, 1986; Wilson, 1996; Khoja et al,
2001). It is, however, necessary to apply the revised
tagset to a real Amharic text and see if it leads to any
unforeseeable problems. It is also useful to see the
distribution of POS tags in a typical Amahric news-
paper text. Therefore, we selected 5 Amharic news
articles and applied the above tagset.
All the tokens in the corpus are assigned one
of the tags in the proposed tagset relatively easily.
There do not seem to be any gaps in the tagset.
Unlike Mesfin (2001), who assigns collocations a
single POS tag, we have assumed that each token
should be treated separately. This means that words
that are part of a collocation are assigned tags indi-
vidually. This in turn contributes towards a better
tagging consistency by minimizing context depen-
dent decision-making steps.
Table 1 shows the distribution of POS tags in the
corpus. Nouns constitute the largest POS category
in the corpus based on the above tagging scheme.
This seems to be characteristic of other languages
too. However, Amharic makes extensive use of noun
clauses for representing different kinds of subordi-
nate clauses. Noun clauses are headed by a verbal
noun, which is assigned a noun POS tag. This adds
to the skewedness of POS tag distributions which
in turn biases the POS tagger that relies heavily on
morphological features as we will show in Section 7.
Interjections, on the other hand, do not occur in the
sample corpus, as these words usually do not appear
often in newspaper text.
Once the POS tagset has been compiled and
tested, the next logical step is to explore automatic
methods of analyzing Amharic words, which we ex-
plore in the next section.
5 POS Tagging of Amharic
Semitic languages like Arabic, Hebrew and Amharic
have a much more complex morphology than En-
glish. In these languages, words usually consist
of several bound morphemes that would normally
have independent lexical entries in languages like
English. Furthermore, in Arabic and Hebrew, the
49
Description POS tag Frequency
Noun N 586
Verb V 203
Auxiliary AUX 20
Numeral NU 65
Adjective AJ 31
Adverb AV 8
Adposition AP 30
Interjection I 0
Punctuation PU 36
Residual R 15
Table 1: Distribution of POS tags
diacritics that represent most vowels and gemina-
tion patterns are missing in written texts. Although
Amharic does not have a special marker for gem-
ination, the Amharic script fully encodes both the
vowels and the consonants, hence it does not suffer
from the ambiguity problem that may arise due to
the missing vowels.
As mentioned briefly in Section 1, the morpho-
logical complexity of these languages opens up dif-
ferent alternative approaches in developing POS
taggers for them (Bar-Haim et al, 2004; Diab
et al, 2004). Bar-Haim et al (2004) showed
that morpheme-based tagging performs better than
word-based tagging; they used Hidden Markov
Models (HMMs) for developing the tagger.
On the basis of the idea introduced by Bar-Haim
et al (2004), we formulate the following two related
tasks for the analysis of Amharic words: segmen-
tation and POS tagging (sequence labeling). Seg-
mentation refers to the analysis of a word into con-
stituting morphemes. The POS tagging task, on the
other hand, deals with the assignment of POS tags
to words. The revised POS tags that are introduced
in Section 3 will be used for this task. The main
reason for choosing words as a unit of analysis and
adopting the abstract POS tags is that the limited re-
source that we have prohibits us from carrying out
fine-grained classification experiments. As a result
of this, we choose to aim at a less ambitious goal of
investigating to what extent the strategies used for
unknown word recognitions can help fill the gap left
by scarce resources. Therefore, we mainly focus on
word-based tagging and explore different kinds of
features that contribute to tagging accuracy.
Although the segmentation and POS tagging tasks
look different, both can be reduced to sequence la-
beling tasks. Since the size of the annotated cor-
pora is very small, a method needs to be chosen
that allows an optimal utilization of the limited re-
sources that are available for Amharic. In this re-
spect, CRFs are more appropriate than HMMs since
they allow us to integrate information from different
sources (Lafferty et al, 2001). In the next section,
we provide a brief description of CRFs.
6 Conditional Random Fields
Conditional Random Fields are conditional proba-
bility distributions that take the form of exponential
models. A special case of CRFs, linear chain CRF,
which takes the following form, has been widely
used for sequence labeling tasks.
P (y | x) =
1
Z (x)
exp
(
?
t=1
?
k
?kfk (t, yt?1, yt, x)
)
,
where Z (x) is the normalization factor, X =
{x1, . . . , xn} is the observation sequence, Y =
{y1, . . . , yT } is the label sequences, fk and ?k
are the feature functions and their corresponding
weights respectively (Lafferty et al, 2001).
An important property of these models is that
probabilities are computed based on a set of feature
functions, i.e. fk, (usually binary valued), which
are defined on both the observation X and label se-
quences Y . These feature functions describe differ-
ent aspect of the data and may overlap, providing
a flexible way of describing the task. CRFs have
been shown to perform well in a number of natural
language processing applications, such as POS tag-
ging (Lafferty et al, 2001), shallow parsing or NP
chunking (Sha and Pereira, 2003), and named entity
recognition (McCallum and Li, 2003).
In POS tagging, context information such as sur-
rounding words and their morphological features,
i.e., suffixes and prefixes, significantly improves per-
formance. CRFs allow us to integrate large set of
such features easily. Therefore, it would be interest-
ing to see to what extent the morphological features
help in predicting Amharic POS tags. We used the
minorThird implementation of CRF (Cohen, 2004).
50
7 Experiments
There are limited resources for the Amharic lan-
guage, which can be used for developing POS tag-
ger. One resource that may be relevant for the cur-
rent task is a dictionary consisting of some 15,000
entries (Amsalu, 1987). Each entry is assigned one
of the five POS tags; Noun, Verb, Adjectives, Ad-
verb, and Adposition. Due to the morphological
complexity of the language, a fully inflected dic-
tionary consisting only of 15,000 entries is bound
to have limited coverage. Furthermore, the dictio-
nary contains entries for phrases, which do not fall
into any of the POS categories. Therefore the actual
number of useful entries is a lot less than 15,000.
The data for the experiment that will be described
below consists of 5 annotated news articles (1000
words). The Amharic text has been transliterated us-
ing the SERA transliteration scheme, which encodes
Amharic scripts using Latin alphabets (Daniel,
1996). This data is very small compared to the data
used in other segmentation and POS tagging experi-
ments. However, it is worthwhile to investigate how
such a limited resource can meaningfully be used for
tackling the aforementioned tasks.
7.1 Segmentation
The training data for segmentation task consists of 5
news articles in which the words are annotated with
segment boundaries as shown in the following ex-
ample.
. . .<seg>Ind</seg><seg>
astawequt</seg>#
<seg>le</seg><seg>arso
</seg>#<seg> aderu
</seg># <seg>be</seg>
<seg>temeTaTaN</seg> . . .
In this example, the morphemes are enclosed in
<seg> and </seg> XML tags. Word-boundaries
are indicated using the special symbol #. The reduc-
tion of the segmentation task to a sequence labeling
task is achieved by converting the XML-annotated
text into a sequence of character-tag pairs. Each
character constitutes a training (test) instance. The
following five tags are used for tagging the char-
acters; B(egin), C(ontinue), E(nd), U(nique) and
N(egative). Each character in the segment is as-
signed one of these tags depending on where it ap-
pears in the segment; at the beginning (B), at the end
(E), inside (C), or alone (U). While the tags BCE are
used to capture multi-character morphemes, the U
tag is used to represent single-character morphemes.
The negative tag (N) is assigned to the special sym-
bol # used to indicate the word boundary. Though
experiments have been carried out with less elab-
orate tagging schemes such as BIO (Begin-Inside-
Outside), no significant performance improvement
has been observed. Therefore, results are reported
only for the BCEUN tagging scheme.
The set of features that are used for training are
composed of character features, morphological fea-
tures, dictionary features, the previous tag, and char-
acter bi-grams. We used a window of eleven charac-
ters centered at the current character. The charac-
ter features consist of the current character, the five
characters to the left and to the right of the current
characters. Morphological features are generated by
first merging the set of characters that appear be-
tween the word boundaries (both left and right) and
the current character. Then a binary feature will be
generated in which its value depends on whether the
resulting segment appears in a precompiled list of
valid prefix and suffix morphemes or not. The same
segment is also used to generate another dictionary-
based feature, i.e., it is checked whether it exists in
the dictionary. Character bi-grams that appear to the
left and the right of the current character are also
used as features. Finally, the previous tag is also
used as a feature.
7.2 POS Tagging
The experimental setup for POS tagging is similar to
that of the segmentation task. However, in our cur-
rent experiments, words, instead of characters, are
annotated with their POS tags and hence we have
more labels now. The following example shows the
annotation used in the training data.
. . .<V>yemikahEdut</V>
<N>yemrmr</N>
<N>tegbarat</N>
<V>yatekorut</V>
<N>bemgb</N> <N>sebl</N>
. . .
51
Each word is enclosed in an XML tag that denotes its
POS tag. These tags are directly used for the training
of the sequence-labeling task. No additional reduc-
tion process is carried out.
The set of features that are used for training are
composed of lexical features, morphological fea-
tures, dictionary features, the previous two POS
tags, and character bi-grams. We used a window of
five words centered at the current word. The lex-
ical features consist of the current word, the two
words to the left and to the right of the current word.
Morphological features are generated by extracting
a segment of length one to four characters long from
the beginning and end of the word. These segments
are first checked against a precompiled list of valid
prefix and suffix morphemes of the language. If the
segment is a valid morpheme then an appropriate
feature will be generated. Otherwise the null pre-
fix or suffix feature will be generated to indicate the
absence of an affix. The dictionary is used to gen-
erate a binary feature for a word based on the POS
tag found in the dictionary. In other words, if the
word is found in the dictionary, its POS tag will be
used as a feature. For each word, a set of character
bi-grams has been generated and each character bi-
gram is used as a feature. Finally, the last two POS
tags are also used as a feature.
8 Results
We conducted a 5-fold cross-validation experiment.
In each run, one article is used as a test dataset and
the remaining four articles are used for training. The
results reported in the sections below are the average
of these five runs. On average 80% of the words in
the test files are unknown words. Most of the un-
known words (on average 60%) are nouns.
8.1 Segmentation Result
As mentioned in Section 7.1, four sets of features,
i.e., character features, morphological features, dic-
tionary features, and previous label, are used for the
segmentation task. Table 2 shows results for some
combinations of these features. The results without
the previous label feature are also shown (Without
Prev. Label).
The simple character features are highly informa-
tive features, as can be seen in Table 2 (Row 1).
Using only these features, the system with previous
label feature already achieved an accuracy of 0.819.
The dictionary feature improved the result by 2%
whereas the morphological features brought minor
improvements. As more features are added the vari-
ation between the different runs increases slightly.
Performace significantly decreases when we omit
the previous label feature as it is shown in Without
Prev. Label column.
8.2 POS Tagging Results
Table 3 shows the word-based evaluation results of
the POS tagging experiment. The baseline (Row 1)
means assigning all the words the most frequently
occurring POS tag, i.e., N (noun). The result ob-
tained using only lexical features (Row 2) is bet-
ter than the baseline. Adding morphological fea-
tures improves the result almost by the same amount
(Row 3). Incorporation of the dictionary feature,
however, has brought only slight improvement. The
addition of bi-gram features improved the result by
3%.
As mentioned before, it is not possible to com-
pare the results, i.e. 74% accuracy (With Prev. La-
bel), with other state of the art POS taggers since our
data is very small compared to the data used by other
POS taggers. It is also difficult to claim with abso-
lute certainty as to the applicability of the technique
we have applied. However, given the fact that 80%
of the test instances are unseen instances, an accu-
racy of 74% is an acceptable result. This claim re-
ceives further support when we look at the results re-
ported for unknown word guessing methods in other
POS tagging experiments (Nakagawa et al, 2001).
As we add more features, the system shows less vari-
ation among the different folds. As with segmenta-
tion task, the omission of the previous label feature
decreases performace. The system with only lexical
features and without previous label feature has the
same performace as the baseline system.
8.3 Error Analysis
The results of both the segmentation and POS tag-
ging tasks show that they are not perfect. An ex-
amination of the output of these systems shows cer-
tain patterns of errors. In case of the segmenta-
tion task, most of the words that are incorrectly seg-
mented have the same beginning or ending charac-
52
With Prev. Label Without Prev. Label
Features accuracy stddev accuracy stddev
Char. 0.819 0.7 0.661 4.7
Char.+Dict. 0.837 1.6 0.671 4.1
Char.+Dict.+Morph. 0.841 1.7 0.701 3.9
Table 2: Segmentation Results
With Prev. Label Without Prev. Label
Features accuracy stddev accuracy stddev
Baseline 0.513 6.4 ? ?
Lex. 0.613 5.3 0.513 6.4
Lex.+Morph. 0.700 5.0 0.688 5.2
Lex.+Morph.+Dict. 0.713 4.3 0.674 5.6
Lex.+Morph.+Dict.+Bigram 0.748 4.3 0.720 2.9
Table 3: Word-based evaluation results of POS tagging
ters as words with affix morphemes. Increasing the
size of the lexical resources, such as the dictionary,
can help the system in distinguishing between words
that have affixes from those that do not.
The POS tagging system, on the other hand,
has difficulties in distinguishing between nouns and
other POS tags. This in turn shows how similar
nouns are to words in other POS tags morpholog-
ically, since our experiment relies heavily on mor-
phological features. This is not particularly sur-
prising given that most Amharic affixes are shared
among nouns and words in other POS tags. In
Amharic, if a noun phrase contains only the head
noun, most noun affixes, such as prepositions, def-
inite article, and case marker appear on the head
noun. If, on the other hand, a noun phrase contains
prenominal constituents such as adjectives, numer-
als, and other nouns, then the above noun affixes
appear on prenominal constituents, thereby blurring
the morphological distinction between the nouns
and other constituents. Furthermore, similar sets
of morphemes are used for prepositions and subor-
dinate conjunctions, which again obscures the dis-
tinction among the nouns and verbs. This, together
with the fact that nouns are the dominant POS cate-
gory in the data, resulted in most words being miss-
classified as nouns.
In general, we believe that the above problems can
be alleviated by making more training data available
to the system, which will enable us to determine im-
proved parameters for both segmentation and POS
tagging models.
9 Concluding Remarks
In this paper, we provided preliminary results of the
application of CRFs for Amharic word segmentation
and POS tagging tasks. Several features were exam-
ined for these tasks. Character features were found
to be useful for the segmentation task whereas mor-
phological and lexical features significantly improve
the results of the POS tagging task. Dictionary-
based features contribute more to the segmentation
task than to the POS tagging task. In both experi-
ments, omition of previous label feature hurts per-
formance.
Although the size of the data limits the scope of
the claims that can be made on the basis of the re-
sults, the results are good especially when we look
at them from the perspective of the results achieved
in unknown word recognition methods of POS tag-
ging experiments. These results could be achieved
since CRFs allow us to integrate several overlapping
features thereby enabling optimum utilization of the
available information.
In general, the paper dealt with a restricted as-
pect of the morphological analysis of Amharic, i.e.,
Amharic word segmentation and POS tagging. Fur-
thermore, these tasks were carried out relatively in-
dependently. Future work should explore how these
tasks could be integrated into a single system that
53
allows for fine-grained POS tagging of Amharic
words. Parallel to this, resource development needs
to be given due attention. As mentioned, the lack
of adequate resources such as a large POS annotated
corpus imposes restrictions on the kind of methods
that can be applied. Therefore, the development of
a standard Amharic POS tagset and annotation of a
reasonably sized corpus should be given priority.
Acknowledgements
This research was supported by the Netherlands
Organization for Scientific Research (NWO) under
project number 220-80-001.
References
Nega Alemayehu. 1999. Development of stemming al-
gorithm for Amharic text retrieval. PhD Thesis, Uni-
versity of Sheffield.
Atelach Alemu. 2002. Automatic Sentence Parsing
for Amharic Text: An Experiment using Probabilistic
Context Free Grammars. Master Thesis, Addis Ababa
University.
Amsalu Aklilu. 1987. Amharic-English Dictionary. Ku-
raz Publishing Agency.
Roy Bar-Haim, Khalil Simaan and Yoad Winter. 2004.
Part-of-Speech Tagging for Hebrew and Other Semitic
Languages. Technical Report.
Abiyot Bayou. 2000. Developing automatic word parser
for Amharic verbs and their derivation. Master The-
sis, Addis Ababa University.
W. Cohen. 2004. Methods for Identifying Names
and Ontological Relations in Text using Heuristics
for Inducing Regularities from Data. http://
minorthird.sourceforge.net
Peter T. Daniels, 1997. Script of Semitic Languages in:
Robert Hetzron, editor,Proceedings of the Corpus Lin-
guistics. 16?45.
Mona Diab, Kadri Hacioglu and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From row text to
base phrase chunks. In Daniel Marku, Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
papers, pages 149?152, Boston, Massachusetts, USA,
May 2?May 7. Association for Computational Lin-
guistics
Sisay Fissaha Adafre and Johann Haller. 2003a.
Amharic verb lexicon in the context of machine trans-
lation. Traitement Automatique des Langues Na-
turelles 2:183?192
Sisay Fissaha Adafre and Johann Haller. 2003b. Ap-
plication of corpus-based techniques to Amharic texts.
Machine Translation for Semitic languages MT Sum-
mit IX Workshop, New Orleans
Mesfin Getachew. 2001. Automatic part of speech
tagging for Amharic language: An experiment using
stochastic HMM. Master Thesis, Addis Ababa Uni-
versity.
Wolf Leslau. 1995. Reference Grammar of Amharic.
Otto Harrassowitz, Wiesbaden.
A. McCallum and W. Li. 2003. Early results for Named
Entity Recognition with conditional random fields,
feature induction and web-enhanced lexicons. Pro-
ceedings of the 7th CoNLL.
Tetsuji Nakagawa, Taku Kudo and Yuji Matsumoto.
2001. Unknown Word Guessing and Part-of-
Speech Tagging Using Support Vector Machines.
NLPRS pages 325-331, Boston, Massachusetts,
USA, May 2?May 7. http://www.afnlp.org/
nlprs2001/pdf/0053-01.pdf
J. Lafferty, F. Pereira and A. McCallum. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings of
the International Conference on Machine Learning.
F. Sha and F. Pereira. 2004. Shallow parsing with con-
ditional random fields. Proceedings of Human Lan-
guage Technology-NAACL.
Khoja S., Garside R., and Knowles G. 2001. A Tagset for
the Morphosyntactic Tagging of Arabic Proceedings
of the Corpus Linguistics. Lancaster University (UK),
Volume 13 - Special issue, 341.
Leech G. Wilson. 1996. Recommendations
for the Morphosyntactic Annotation of Cor-
pora. EAGLES Report EAG-TCWG-MAC/R,
http://www.ilc.pi.cnr.it/EAGLES96/
annotate/annotate.html
Daniel Yacob. 1996. System for Ethiopic Representation
in ASCII. http://www.abyssiniagateway.
net/fidel
Baye Yimam. 1999. Root. Ethiopian Journal of Lan-
guage Studies, 9:56?88.
Baye Yimam. 1986. Yamara Swasw E.M.P.D.A, Addis
Ababa.
54
Finding Similar Sentences across Multiple Languages in Wikipedia
Sisay Fissaha Adafre Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam
sfissaha,mdr@science.uva.nl
Abstract
We investigate whether the Wikipedia cor-
pus is amenable to multilingual analysis
that aims at generating parallel corpora.
We present the results of the application of
two simple heuristics for the identification
of similar text across multiple languages
in Wikipedia. Despite the simplicity of the
methods, evaluation carried out on a sam-
ple of Wikipedia pages shows encouraging
results.
1 Introduction
Parallel corpora form the basis of much multilin-
gual research in natural language processing, rang-
ing from developing multilingual lexicons to sta-
tistical machine translation systems. As a conse-
quence, collecting and aligning text corpora writ-
ten in different languages constitutes an important
prerequisite for these research activities.
Wikipedia is a multilingual free online encyclo-
pedia. Currently, it has entries for more than 200
languages, the English Wikipedia being the largest
one with 895,674 articles, and no fewer than eight
language versions having upwards of 100,000 ar-
ticles as of January 2006. As can be seen in Fig-
ure 1, Wikipedia pages for major European lan-
guages have reached a level where they can sup-
port multilingual research. Despite these devel-
opments in its content, research on Wikipedia has
largely focused on monolingual aspects so far; see
e.g., (Voss, 2005) for an overview.
In this paper, we focus on multilingual aspects
of Wikipedia. Particularly, we investigate to what
extent we can use properties of Wikipedia itself
to generate similar sentences acrose different lan-
guages. As usual, we consider two sentences sim-
ilar if they contain (some or a large amount of)
overlapping information. This includes cases in
which sentences may be exact translations of each
other, one sentence may be contained within an-
other, or both share some bits of information.
en de fr ja pl it sv nl pt es zh ru no fi da
0
100000
200000
300000
400000
500000
600000
700000
800000
900000
1000000
Figure 1: Wikipedia pages for the top 15 lan-
guages
The conceptually simple but fundamental task
of identifying similar sentences across multiple
languages has a number of motivations. For a
start, and as mentioned earlier, sentence aligned
corpora play an important role in corpus based lan-
guage processing methods in general. Second, in
the context of Wikipedia, being able to align sim-
ilar sentences across multiple languages provides
insight into Wikipedia as a knowledge source: to
which extent does a given topic get different kinds
of attention in different languages? And thirdly,
the ability to find similar content in other lan-
guages while creating a page for a topic in one lan-
guage constitutes a useful type of editing support.
Furthermore, finding similar content acrose differ-
ent languages can form the basis for multilingual
summarization and question answering support for
62
Wikipedia; at present the latter task is being devel-
oped into a pilot for CLEF 2006 (WiQA, 2006).
There are different approaches for finding sim-
ilar sentences across multiple languages in non-
parallel but comparable corpora. Most methods
for finding similar sentences assume the availabil-
ity of a clean parallel corpus. In Wikipedia, two
versions of a Wikipedia topic in two different lan-
guages are a good starting point for searching sim-
ilar sentences. However, these pages may not al-
ways conform to the typical definitions of a bitext
which current techniques assume. Bitext gener-
ally refers to two versions of a text in two differ-
ent languages (Melamed, 1996). Though it is not
known how information is shared among the dif-
ferent languages in Wikipedia, some pages tend to
be translations of each other whereas the majority
of the pages tend to be written independently of
each other. Therefore, two versions of the same
topic in two different languages can not simply be
taken as parallel corpora. This in turn limits the
application of some of the currently available tech-
niques.
In this paper, we present two approaches for
finding similar sentences across multiple lan-
guages in Wikipedia. The first approach uses
freely available online machine translation re-
sources for translating pages and then carries out
monolingual sentence similarity. The approach
needs a translation system, and these are not avail-
able for every pair of languages in Wikipedia.
This motivates a second approach to finding
similar sentences across multiple languages, one
which uses a bilingual title translation lexicon in-
duced automatically using the link structure of
Wikipedia. Briefly, two sentences are similar if
they link to the same entities (or rather: to pages
about the same entities), and we use Wikipedia it-
self to relate pages about a given entity across mul-
tiple languages. In Wikipedia, pages on the same
topic in different languages are topically closely
related. This means that even if one page is not
a translation of another, they tend to share some
common information. Our underlying assumption
here is that there is a general agreement on the
kind of information that needs to be included in the
pages of different types of topics such as a biogra-
phy of a person, and the definition and description
of a concept etc., and that this agreement is to a
consderable extent ?materialized? in the hypertext
links (and their anchor texts) in Wikipedia.
Our main research question in this paper is this:
how do the two methods just outlined differ? A
priori it seems that the translation based approach
to finding similar sentences across multiple lan-
guages will have a higher recall than the link-
based method, while the latter outperforms the for-
mer in terms of precision. Is this correct?
The remainder of the paper is organized as fol-
lows. In Section 2, we briefly discuss related
work. Section 3 provides a detailed description
of Wikipedia as a corpus. The two approaches to
identifying similar sentences across multiple lan-
guages are presented in Section 4. An experimen-
tal evaluation is presented in Section 5. We con-
clude in Section 6.
2 Related Work
The main focus of this paper lies with multilin-
gual text similarity and its application to infor-
mation access in the context of Wikipedia. Cur-
rent research work related to Wikipedia mostly
describes its monolingual properties (Ciffolilli,
2003; Vie?gas et al, 2004; Lih, 2004; Miller,
2005; Bellomi and Bonato, 2005; Voss, 2005; Fis-
saha Adafre and de Rijke, 2005). This is proba-
bly due to the fact that different language versions
of Wikipedia have different growth rates. Others
describe its application in question answering and
other types of IR systems (Ahn et al, 2005). We
believe that currently, Wikipedia pages for major
European languages have reached a level where
they can support multilingual research.
On the other hand, there is a rich body of knowl-
edge relating to multilingual text similarity. These
include example-based machine translation, cross-
lingual information retrieval, statistical machine
translation, sentence alignment cost functions, and
bilingual phrase translation (Kirk Evans, 2005).
Each approach uses relatively different features
(content and structural features) in identifying
similar text from bilingual corpora. Furthermore,
most methods assume that the bilingual corpora
can be sentence aligned. This assumption does
not hold for our case since our corpus is not par-
allel. In this paper, we use content based fea-
tures for identifying similar text across multilin-
gual corpora. Particularly, we compare bilingual
lexicon and MT system based methods for identi-
fying similar text in Wikipedia.
63
3 Wikipedia as a Multilingual Corpus
Wikipedia is a free online encyclopedia which is
administered by the non-profit Wikimedia Foun-
dation. The aim of the project is to develop free
encyclopedias for different languages. It is a col-
laborative effort of a community of volunteers, and
its content can be edited by anyone. It is attracting
increasing attention amongst web users and has
joined the top 50 most popular sites.
As of January 1, 2006, there are versions of
Wikipedia in more than 200 languages, with sizes
ranging from 1 to over 800,000 articles. We used
the ascii text version of the English and Dutch
Wikipedia, which are available as database dumps.
Each entry of the encyclopedia (a page in the on-
line version) corresponds to a single line in the text
file. Each line consists of an ID (usually the name
of the entity) followed by its description. The de-
scription part contains the body of the text that de-
scribes the entity. It contains a mixture of plain
text and text with html tags. References to other
Wikipedia pages in the text are marked using ?[[?
?]]? which corresponds to a hyperlink on the on-
line version of Wikipedia. Most of the formatting
information which is not relevant for the current
task has been removed.
3.1 Links within a single language
Wikipedia is a hypertext document with a rich link
structure. A description of an entity usually con-
tains hypertext links to other pages within or out-
side Wikipedia. The majority of these links cor-
respond to entities, which are related to the en-
tity being described, and have a separate entry
in Wikipedia. These links are used to guide the
reader to a more detailed description of the con-
cept denoted by the anchor text. In other words,
the links in Wikipedia typically indicate a topical
association between the pages, or rather the enti-
ties being described by the pages. E.g., in describ-
ing a particular person, reference will be made to
such entities as country, organization and other im-
portant entities which are related to it and which
themselves have entries in Wikipedia. In general,
due to the peculiar characteristics of an encyclope-
dia corpus, the hyperlinks found in encyclopedia
text are used to exemplify those instances of hy-
perlinks that exist among topically related entities
(Ghani et al, 2001; Rao and Turoff, 1990).
Each Wikipedia page is identified with a unique
ID. These IDs are formed by concatenating the
words of the titles of the Wikipedia pages which
are unique for each page, e.g., the page on Vin-
cent van Gogh has ?Vincent van Gogh? as its ti-
tle and ?Vincent van Gogh? as its ID. Each page
may, however, be represented by different anchor
texts in a hyperlink. The anchor texts may be sim-
ple morphological variants of the title such as plu-
ral form or may represent closely related seman-
tic concept. For example, the anchor text ?Dutch?
may point to the page for the Netherlands. In a
sense, the IDs function as the canonical form for
several related concepts.
3.2 Links across different languages
Different versions of a page in different languages
are also hyperlinked. For a given page, transla-
tions of its title in other languages for which pages
exist are given as hyperlinks. This property is par-
ticularly useful for the current task as it helps us to
align the corpus at the page level. Furthermore, it
also allows us to induce bilingual lexicon consist-
ing of the Wikipedia titles. Conceptual mismatch
between the pages (e.g. Roof vs Dakconstructie)
is rare, and the lexicon is generally of high qual-
ity. Unlike the general lexicon, this lexicon con-
tains a relatively large number of names of indi-
viduals and other entities which are highly infor-
mative and hence are useful in identifying similar
text. This lexicon will form the backbone of one
of the methods for identifying similar text across
different languages, as will be shown in Section 4.
4 Approaches
We describe two approaches for identifying simi-
lar sentences across different languages. The first
uses an MT system to obtain a rough translation of
a given page in one language into another and then
uses word overlap between sentences as a similar-
ity measure. One advantage of this method is that
it relies on a large lexical resource which is bigger
than what can be extracted from Wikipedia. How-
ever, the translation can be less accurate especially
for the Wikipedia titles which form part of the con-
tent of a page and are very informative.
The second approach relies on a bilingual lexi-
con which is generated from Wikipedia using the
link structure: pages on the same topic in differ-
ent languages are hyperlinked; see Figure 2. We
use the titles of the pages that are linked in this
manner to create a bilingual lexicon. Thus, our
bilingual lexicon consists of terms that represent
64
concepts or entities that have entries in Wikipedia,
and we will represent sentences by entries from
this lexicon: an entry is used to represent the con-
tent of a sentence if the sentence contains a hy-
pertext link to the Wikipedia page for that entry.
Sentence similarity is then captured in terms of the
shared lexicon entries they share. In other words,
the similarity measure that we use in this approach
is based on ?concept? or ?page title? overlap. In-
tuitively, this approach has the advantage of pro-
ducing a brief but highly accurate representation
of sentences, more accurate, we assume than the
MT approach as the titles carry important seman-
tic information; it will also be more accurate than
the MT approach because the translations of the
titles are done manually.
Figure 2: Links to pages devoted to the same topic
in other languages.
Both approaches assume that the Wikipedia cor-
pus is aligned at the page level. This is eas-
ily achieved using the link structure since, again,
pages on the same topic in different languages are
hyperlinked. This, in turns, narrows down the
search for similar text to a page level. Hence, for
a given text of a page (sentence or chunk) in one
language, we search for its equivalent text (sen-
tence or chunk) only in the corresponding page in
the other language, not in the entire corpus.
We now describe the two approaches in more
detail. To remain focused and avoid getting lost
in technical details, we consider only two lan-
guages in our technical descriptions and evalua-
tions below: Dutch and English; it will be clear
from our presentation, however, that our second
approach can be used for any pair of languages in
Wikipedia.
4.1 An MT based approach
In this approach, we translate the Dutch Wikipedia
page into English using an online MT system. We
refer to the English page as source and the trans-
lated (Dutch page) version as target. We used the
Babelfish MT system of Altavista. It supports a
number of language pairs among which are Dutch-
English pairs. After both pages have been made
available in English, we split the pages into sen-
tences or text chucks. We then link each text chunk
or sentence in the source to each chuck or sentence
in the target. Following this we compute a simple
word overlap score for each pair. We used the Jac-
card similarity measure for this purpose. Content
words are our main features for the computation
of similarity, hence, we remove stopwords. Gram-
matically correct translations may not be neces-
sary since we are using simple word overlap as our
similarity measure.
The above procedure will generate a large set
of pairs, not all of which will actually be similar.
Therefore, we filter the list assuming a one-to-one
correspondence, where for each source sentence
we identify at most one target sentence. This is
a rather strict criterion (another possibility being
one-to-many), given the fact that the corpus is gen-
erally assumed to be not parallel. But it gives some
idea on how much of the text corpus can be aligned
at smaller units (i.e., sentence or text chunks).
Filtering works as follows. First we sort the
pairs in decreasing order of their similarity scores.
This results in a ranked list of text pairs in which
the most similar pairs are ranked top whereas the
least similar pairs are ranked bottom. Next we take
the top most ranking pair. Since we are assuming
a one-to-one correspondence, we remove all other
pairs ranked lower in the list containing either of
the the sentences or text chunks in the top ranking
pair. We then repeat this process taking the second
top ranking pair. Each step results in a smaller list.
The process continues until there is no more pair
to remove.
4.2 Using a link-based bilingual lexicon
As mentioned previously, this approach makes
use of a bilingual lexicon that is generated from
Wikipedia using the link structure. A high level
description of the algorithm is given in Figure 3.
Below, we first describe how the bilingual lexicon
is acquired and how it is used for enriching the link
structure of Wikipedia. Finally, we detail how the
65
? Generating bilingual lexicon
? Given a topic, get the corresponding pages
from English and Dutch Wikipedia
? Split pages into sentences and enrich the
hyperlinks in the sentence or identify
named-entities in the pages.
? Represent the sentences in these pages us-
ing the bilingual lexicon.
? Compute term overlap between the sen-
tences thus represented.
Figure 3: The Pseudo-algorithm for identifying
similar sentences using a link-based bilingual lex-
icon.
bilingual lexicon is used for the identification of
similar sentences.
Generating the bilingual lexicon
Unlike the MT based approach, which uses con-
tent words from the general vocabulary as fea-
tures, in this approach, we use page titles and their
translations (as obtained through hyperlinks as ex-
plained above) as our primitives for the compu-
tation of multilingual similarity. The first step of
this approach, then, is acquiring the bilingual lexi-
con, but this is relatively straightforward. For each
Wikipedia page in one language, translations of
the title in other languages, for which there are
separate entries, are given as hyperlinks. This in-
formation is used to generate a bilingual transla-
tion lexicon. Most of these titles are content bear-
ing noun phrases and are very useful in multilin-
gual similarity computation (Kirk Evans, 2005).
Most of these noun phrases are already disam-
buiguated, and may consist of either a single word
or multiword units.
Wikipedia uses a redirection facility to map
several titles into a canonical form. These titles
are mostly synonymous expressions. We used
Wikipedia?s redirect feature to identify synony-
mous expression.
Canonical representation of a sentence
Once we have the bilingual lexicon, the next step
is to represent the sentences in both language pairs
using this lexicon. Each sentence is represented by
the set of hyperlinks it contains. We search each
hyperlink in the bilingual lexicon. If it is found,
we replace the hyperlink with the corresponding
unique identification of the bilingual lexicon entry.
If it is not found, the hyperlink will be included as
is as part of the representation. This is done since
Dutch and English are closely related languages
and may share many cognate pairs.
Enriching the Wikipedia link structure
As described in the previous section, the method
uses hyperlinks in a sentence as a highly focused
entity-based representation of the aboutness of the
sentence. In Wikipedia, not all occurrences of
named-entities or concepts that have entries in
Wikipedia are actually used as anchor text of a
hypertext link; because of this, a number of sen-
tences may needlessly be left out from the simi-
larity computation process. In order to avoid this
problem, we automatically identify other relevant
hyperlinks using the bilingual lexicon generated in
the previous section.
Identification of additional hyperlinks in
Wikipedia sentences works as follows. First
we split the sentences into constituent words.
We then generate N gram words keeping the
relative order of words in the sentences. Since the
anchor texts of hypertext links may be multiword
expressions, we start with higher order N gram
words (N=4). We search these N grams in the
bilingual lexicon. If the N gram is found in the
lexicon, it is taken as a new hyperlink and will
form part of the representation of a sentence. The
process is repeated for lower order N grams.
Identifying similar sentences
Once we are done representing the sentences as
described previously, the final step involves com-
putation of the term overlap between the sentence
pairs and filtering the resulting list. The remain-
ing steps are similar to those described in the MT
based approach. For completeness, we briefly re-
peat the steps here. First, all sentences from a
Dutch Wikipedia page are linked to all sentences
of the corresponding English Wikipedia page. We
then compute the similarity between the sentence
representations, using the Jaccard similarity coef-
ficient.
A sentence in Dutch page may be similar to
several sentences in English page which may re-
sult in a large number of spurious pairs. There-
fore, we filter the list using the following recursive
procedure. First, the sentence pairs are sorted by
their similarity scores. We take the pairs with the
highest similarity scores. We then eliminate all
66
other sentence pairs from the list that contain ei-
ther of sentences in this pair. We continue this pro-
cess taking the second highest ranking pair. Note
that this procedure assumes a one-to-one matching
rule; a sentences in Dutch can be linked to at most
one sentence in English.
5 Experimental Evaluation
Now that we have described the two algorithms
for identifying similar sentences, we return to our
research questions. In order to answer them we
run the experiment described below.
5.1 Set-up
We took a random sample of 30 English-Dutch
Wikipedia page pairs. Each page is split into sen-
tences. We generated candidate Dutch-English
sentence pairs and passed them on to the two
methods. Both methods return a ranked list of sen-
tence pairs that are similar. As explained above,
we assumed a one-to-one correspondence, i.e., one
English sentence can be linked to at most to one
Dutch sentence.
The outputs of the systems are manually evalu-
ated. We apply a relatively lenient criteria in as-
sessing the results. If two sentences overlap in-
terms of their information content then we con-
sider them to be similar. This includes cases in
which sentences may be exact translation of each
other, one sentence may be contained within an-
other, or both share some bits of information.
5.2 Results
Table 1 shows the results of the two methods de-
scribed in Section 4. In the table, we give two
types of numbers for each of the two methods
MT and Bilingual lexicon: Total (the total number
of sentence pairs) and Match (the number of cor-
rectly identified sentence pairs) generated by the
two approaches.
Overall, the two approaches tend to produce
similar numbers of correctly identified similar sen-
tence pairs. The systems seem to perform well
on pages which tend to be alignable at sentence
level, i.e., parallel. This is clearly seen on the
following pages: Pierluigi Collina, Marcus Cor-
nelius Fronto, George F. Kennan, which show a
high similarity at sentence level. Some pages con-
tain very small description and hence the figures
for correct similar sentences are also small. Other
topics such as Classicism (Dutch: Classicisme),
Tennis, and Tank, though they are described in suf-
ficient details in both languages, there tends to be
less overlap among the text. The methods tend to
retrieve more accurate similar pairs from person
pages than other pages especially those pages de-
scribing a more abstract concepts. However, this
needs to be tested more thoroughly.
When we look at the total number of sentence
pairs returned, we notice that the bilingual lexi-
con based method consistently returns a smaller
amount of similar sentence pairs which makes
the method more accurate than the MT based ap-
proach. On average, the MT based approach re-
turns 4.5 (26%) correct sentences and the bilingual
lexicon based approach returns 2.9 correct sen-
tences (45%). But, on average, the MT approach
returns three times as many sentence pairs as bilin-
gual lexicon approach. This may be due to the fact
that the former makes use of restricted set of im-
portant terms or concepts whereas the later uses a
large general lexicon. Though we remove some
of the most frequently occuring stopwords in the
MT based approach, it still generates a large num-
ber of incorrect similar sentence pairs due to some
common words.
In general, the number of correctly identified
similar pages extracted seems small. However,
most of the Dutch pages are relatively small,
which sets the upper bound on the number of
correctly identified sentence pairs that can be ex-
tracted. On average, each Dutch Wikipedia page
in the sample contains 18 sentences whereas En-
glish Wikipedia pages contain 65 sentences. Ex-
cluding the pages for Tennis, Tank (Dutch: vo-
ertuig), and Tricolor, which are relatively large,
each Dutch page contains on average 8 sentences,
which is even smaller. Given the fact that the
pages are in general not parallel, the methods,
using simple heuristics, identified high quality
translation equivalent sentence pairs from most
Wikipedia pages. Furthermore, a close examina-
tion of the output of the two approaches show that
both tend to identify the same set of similar sen-
tence pairs.
We ran our bilingual lexicon based approach on
the whole Dutch-English Wikipedia corpus. The
method returned about 80M of candidate similar
sentences. Though we do not have the resources
to evaluate this output, the results we got from
sample data (cf. Table 1) suggest that it contains
a significant amount of correctly identified similar
67
Title MT Bilingual Lexicon
English Dutch Total Match Total Match
Hersfeld Rotenburg Hersfeld Rotenburg 2 3 2
Manganese nodule Mangaanknol 5 2 1 1
Kettle Ketel 1 1
Treason Landverraad 2 1
Pierluigi Collina Pierluigi Collina 14 13 13 11
Province of Ferrara Ferrara (provincie) 7 1 1 1
Classicism Classicisme 8 1
Tennis Tennis 93 4 15 3
Hysteria Hysterie 14 6 9 5
George F. Kennan George Kennan 27 12 29 11
Marcus Cornelius Fronto Marcus Cornelius Fronto 11 9 5 5
Delphi Delphi (Griekenland) 34 2 8 1
De Beers De Beers 11 5 10 5
Pavel Popovich Pavel Popovytsj 7 4 4 4
Rice pudding Rijstebrij 11 1 4
Manta ray Reuzenmanta 15 3 7 2
Michelstadt Michelstadt 1 1 1 1
Tank Tank (voertuig) 84 3 27 2
Cheyenne(Wyoming) Cheyenne(Wyoming) 5 2 2 2
Goa Goa(deelstaat) 13 4 6 1
Tricolour Driekleur 57 36 13 12
Oral cancer Mondkanker 25 2 7 2
Pallium Pallium 12 2 5 4
Ajanta Ajanta 3 3 2 2
Captain Jack (band) Captain Jack 16 3 2 2
Proboscis Monkey Neusaap 15 6 4 1
Patti Smith Patti Smith 6 2 4 2
Flores Island, Portugal Flores (Azoren) 3 2 1 1
Mercury 8 Mercury MA 8 11 3 4 1
Mutation Mutatie 16 4 6 3
Average 17.6 4.5 6.5 2.9
Table 1: Test topics (column 1 and 2). The total number of sentence pairs (column 3) and the number
of correctly identified similar sentence pairs (column 4) returned by the MT based approach. The to-
tal number of sentence pairs (column 5) and the number of correctly identified similar sentence pairs
(column 6) returned by the method using a bilingual lexicon.
sentences.
6 Conclusion
In this paper we focused on multilingual aspects of
Wikipedia. Particularly, we investigated the poten-
tial of Wikipedia for generating parallel corpora by
applying different methods for identifying similar
text across multiple languages. We presented two
methods and carried out an evaluation on a sam-
ple of Dutch-English Wikipedia pages. The results
show that both methods, using simple heuristics,
were able to identify similar text between the pair
of Wikipedia pages though they differ in accuracy.
The bilingual lexicon approach returns fewer in-
correct pairs than the MT based approach. We
interpret this as saying that our bilingual lexicon
based method provides a more accurate represen-
tation of the aboutness of sentences in Wikipedia
than the MT based approach. Furthermore, the re-
sult we obtained on a sample of Wikipedia pages
and the output of running the bilingual based ap-
proach on the whole Dutch-English gives some in-
dication of the potential of Wikipedia for generat-
ing parallel corpora.
68
As to future work, the sentence similarity de-
tection methods that we considered are not perfect.
E.g., the MT based approach relies on rough trans-
lations; it is important to investigate the contri-
bution of high quality translations. The bilingual
lexicon approach uses only lexical features; other
language specific sentence features might help im-
prove results.
Acknowledgments
This research was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project numbers 017.001.190, 220-80-
001, 264-70-050, 612-13-001, 612.000.106,
612.000.207, 612.066.302, 612.069.006, 640.-
001.501, and 640.002.501.
References
D. Ahn, V. Jijkoun, G. Mishne, K. Mu?ller, M. de Rijke,
and S. Schlobach. 2005. Using Wikipedia at the
TREC QA Track. In E.M. Voorhees and L.P. Buck-
land, editors, The Thirteenth Text Retrieval Confer-
ence (TREC 2004).
F. Bellomi and R. Bonato. 2005. Lex-
ical authorities in an encyclopedic cor-
pus: a case study with wikipedia. URL:
http://www.fran.it/blog/2005/01/
lexical-authorities-in-encyclopedic.
htm%l. Site accessed on June 9, 2005.
A. Ciffolilli. 2003. Phantom authority, selfselective re-
cruitment and retention of members in virtual com-
munities: The case of Wikipedia. First Monday,
8(12).
S. Fissaha Adafre and M. de Rijke. 2005. Discovering
missing links in Wikipedia. In Proceedings of the
Workshop on Link Discovery: Issues, Approaches
and Applications (LinkKDD-2005).
R. Ghani, S. Slattery, and Y. Yang. 2001. Hypertext
categorization using hyperlink patterns and meta
data. In Carla Brodley and Andrea Danyluk, ed-
itors, Proceedings of ICML-01, 18th International
Conference on Machine Learning, pages 178?185.
D. Kirk Evans. 2005. Identifying similarity
in text: Multi-lingual analysis for summariza-
tion. URL: http://www1.cs.columbia.
edu/nlp/theses/dave_evans.pdf. Site
accessed on January 5, 2006.
A. Lih. 2004. Wikipedia as participatory journalism:
Reliable sources? Metrics for evaluating collabora-
tive media as a news resource. In Proceedings of the
5th International Symposium on Online Journalism.
D. Melamed. 1996. A geometric approach to mapping
bitext correspondence. In Eric Brill and Kenneth
Church, editors, Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1?12, Somerset, New Jersey. Association
for Computational Linguistics.
N. Miller. 2005. Wikipedia and the disappearing
?Author?. ETC: A Review of General Semantics,
62(1):37?40.
U. Rao and M. Turoff. 1990. Hypertext functionality:
A theoretical framework. International Journal of
Human-Computer Interaction.
F. Vie?gas, M. Wattenberg, and D. Kushal. 2004.
Studying cooperation and conflict between authors
with history flow visualization. In Proceedings of
the 2004 conference on Human factors in comput-
ing systems.
J. Voss. 2005. Measuring Wikipedia. In Proceedings
10th International Conference of the International
Society for Scientometrics and Informetrics.
WiQA. 2006. Question answering using Wikipedia.
URL: http://ilps.science.uva.nl/
WiQA/. Site accessed on January 5, 2006.
69
