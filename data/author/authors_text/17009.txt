Proceedings of the ACL Student Research Workshop, pages 165?171,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A corpus-based evaluation method for Distributional Semantic Models
Abdellah Fourtassi1,2 Emmanuel Dupoux2,3
abdellah.fourtassi@gmail.com emmanuel.dupoux@gmail.com
1Institut d?Etudes Cognitives, Ecole Normale Superieure, Paris
2Laboratoire de Sciences Cognitives et Psycholinguistique, CNRS, Paris
3Ecole des Hautes Etudes en Sciences Sociales, Paris
Abstract
Evaluation methods for Distributional Se-
mantic Models typically rely on behav-
iorally derived gold standards. These
methods are difficult to deploy in lan-
guages with scarce linguistic/behavioral
resources. We introduce a corpus-based
measure that evaluates the stability of the
lexical semantic similarity space using a
pseudo-synonym same-different detection
task and no external resources. We show
that it enables to predict two behavior-
based measures across a range of parame-
ters in a Latent Semantic Analysis model.
1 Introduction
Distributional Semantic Models (DSM) can be
traced back to the hypothesis proposed by Harris
(1954) whereby the meaning of a word can be in-
ferred from its context. Several implementations
of Harris?s hypothesis have been proposed in the
last two decades (see Turney and Pantel (2010) for
a review), but comparatively little has been done
to develop reliable evaluation tools for these im-
plementations. Models evaluation is however an
issue of crucial importance for practical applica-
tions, i.g., when trying to optimally set the model?s
parameters for a given task, and for theoretical rea-
sons, i.g., when using such models to approximate
semantic knowledge.
Some evaluation techniques involve assigning
probabilities to different models given the ob-
served corpus and applying maximum likelihood
estimation (Lewandowsky and Farrell, 2011).
However, computational complexity may prevent
the application of such techniques, besides these
probabilities may not be the best predictor for the
model performance on a specific task (Blei, 2012).
Other commonly used methods evaluate DSMs by
comparing their semantic representation to a be-
haviorally derived gold standard. Some standards
are derived from the TOEFL synonym test (Lan-
dauer and Dumais, 1997), or the Nelson word
associations norms (Nelson et al, 1998). Oth-
ers use results from semantic priming experiments
(Hutchison et al, 2008) or lexical substitutions er-
rors (Andrews et al, 2009). Baroni and Lenci
(2011) set up a more refined gold standard for En-
glish specifying different kinds of semantic rela-
tionship based on dictionary resources (like Word-
Net and ConceptNet).
These behavior-based evaluation methods are
all resource intensive, requiring either linguistic
expertise or human-generated data. Such meth-
ods might not always be available, especially in
languages with fewer resources than English. In
this situation, researchers usually select a small set
of high-frequency target words and examine their
nearest neighbors (the most similar to the target)
using their own intuition. This is used in partic-
ular to set the model parameters. However, this
rather informal method represents a ?cherry pick-
ing? risk (Kievit-Kylar and Jones, 2012), besides
it is only possible for languages that the researcher
speaks.
Here we introduce a method that aims at pro-
viding a rapid and quantitative evaluation for
DSMs using an internal gold standard and re-
quiring no external resources. It is based on a
simple same-different task which detects pseudo-
synonyms randomly introduced in the corpus. We
claim that this measure evaluates the intrinsic
ability of the model to capture lexical semantic
similarity. We validate it against two behavior-
based evaluations (Free association norms and the
TOEFL synonym test) on semantic representa-
tions extracted from a Wikipedia corpus using one
of the most commonly used distributional seman-
tic models : the Latent Semantic Analysis (LSA,
Landauer and Dumais (1997)).
In this model, we construct a word-document
matrix. Each word is represented by a row, and
165
each document is represented by a column. Each
matrix cell indicates the occurrence frequency of
a given word in a given context. Singular value
decomposition (a kind of matrix factorization) is
used to extract a reduced representation by trun-
cating the matrix to a certain size (which we call
the semantic dimension of the model). The cosine
of the angle between vectors of the resulting space
is used to measure the semantic similarity between
words. Two words end up with similar vectors if
they co-occur multiple times in similar contexts.
2 Experiment
We constructed three successively larger corpora
of 1, 2 and 4 million words by randomly select-
ing articles from the original ?Wikicorpus? made
freely available on the internet by Reese et al
(2010). Wikicorpus is itself based on articles from
the collaborative encyclopedia Wikipedia. We se-
lected the upper bound of 4 M words to be com-
parable with the typical corpus size used in theo-
retical studies on LSA (see for instance Landauer
and Dumais (1997) and Griffiths et al (2007)). For
each corpus, we kept only words that occurred at
least 10 times and we excluded a stop list of high
frequency words with no conceptual content such
as: the, of, to, and ... This left us with a vocab-
ulary of 8 643, 14 147 and 23 130 words respec-
tively. For the simulations, we used the free soft-
ware Gensim (R?ehu?r?ek and Sojka, 2010) that pro-
vides an online Python implementation of LSA.
We first reproduced the results of Griffiths et al
(2007), from which we derived the behavior-based
measure. Then, we computed our corpus-based
measure with the same models.
2.1 The behavior-based measure
Following Griffiths et al (2007), we used the
free association norms collected by Nelson et al
(1998) as a gold standard to study the psychologi-
cal relevance of the LSA semantic representation.
The norms were constructed by asking more than
6000 participants to produce the first word that
came to mind in response to a cue word. The
participants were presented with 5,019 stimulus
words and the responses (word associates) were
ordered by the frequency with which they were
named. The overlap between the words used in
the norms and the vocabulary of our smallest cor-
pus was 1093 words. We used only this restricted
overlap in our experiment.
In order to evaluate the performance of LSA
models in reproducing these human generated
data, we used the same measure as in Griffiths
et al (2007): the median rank of the first associates
of a word in the semantic space. This was done in
three steps : 1) for each word cue Wc, we sorted
the list of the remaining words Wi in the overlap
set, based on their LSA cosine similarity with that
cue: cos(LSA(Wc), LSA(Wi)), with highest co-
sine ranked first. 2) We found the ranks of the first
three associates for that cue in that list. 3) We ap-
plied 1) and 2) to all words in the overlap set and
we computed the median rank for each of the first
three associates.
Griffiths et al (2007) tested a set of seman-
tic dimensions going from 100 to 700. We ex-
tended the range of dimensions by testing the
following set : [2,5,10,20,30,40,50,100, 200,
300,400,500,600,700,800,1000]. We also manip-
ulated the number of successive sentences to be
taken as defining the context of a given word (doc-
ument size), which we varied from 1 to 100.
In Figure 1 we show the results for the 4 M size
corpus with 10 sentences long documents.
Figure 1 : The median rank of the three associates as a
function of the semantic dimensions (lower is better)
For the smaller corpora we found similar results
as we can see from Table 1 where the scores rep-
resent the median rank averaged over the set of
dimensions ranging from 10 to 1000. As found
in Griffiths et al (2007), the median rank measure
predicts the order of the first three associates in the
norms.
In the rest of the article, we will need to char-
acterize the semantic model by a single value. In-
stead of taking the median rank of only one of the
166
Size associate 1 associate 2 associate 3
1 M 78.21 152.18 169.07
2 M 57.38 114.57 131
4 M 54.57 96.5 121.57
Table 1 : The median rank of the first three associates for
different sizes
associates, we will consider a more reliable mea-
sure by averaging over the median ranks of the
three associates across the overlap set. We will
call this measure the Median Rank.
2.2 The Pseudo-synonym detection task
The measure we introduce in this part is based
on a Same-Different Task (SDT). It is described
schematically in Figure 2, and is computed as
follows: for each corpus, we generate a Pseudo-
Synonym-corpus (PS-corpus) where each word in
the overlap set is randomly replaced by one of two
lexical variants. For example, the word ?Art? is
replaced in the PS-corpus by ?Art1? or ?Art2?. In
the derived corpus, therefore, the overlap lexicon
is twice as big, because each word is duplicated
and each variant appears roughly with half of the
frequency of the original word.
The Same-Different Task is set up as follows: a
pair of words is selected at random in the derived
corpus, and the task is to decide whether they are
variants of one another or not, only based on their
cosine distances. Using standard signal detection
techniques, it is possible to use the distribution
of cosine distances across the entire list of word
pairs in the overlap set to compute a Receiver
Operating Characteristic Curve (Fawcett, 2006),
from which one derives the area under the curve.
We will call this measure : SDT-?. It can be
interpreted as the probability that given two pairs
of words, of which only one is a pseudo-synonym
pair, the pairs are correctly identified based on
cosine distance only. A value of 0.5 represents
pure chance and a value of 1 represents perfect
performance.
It is worth mentioning that the idea of gen-
erating pseudo-synonyms could be seen as the
opposite of the ?pseudo-word? task used in
evaluating word sense disambiguation models
(see for instance Gale et al (1992) and Dagan
et al (1997)). In this task, two different words
w1 and w2 are combined to form one ambiguous
pseudo-word W12 = {w1, w2} which replaces
both w1 and w2 in the test set.
We now have two measures evaluating the
quality of a given semantic representation: The
Median Rank (behavior-based) and the SDT-?
(corpus-based). Can we use the latter to predict
the former? To answer this question, we compared
the performance of both measures across differ-
ent semantic models, document lengths and cor-
pus sizes.
3 Results
In Figure 3 (left), we show the results of the
behavior-based Median Rank measure, obtained
from the three corpora across a number of seman-
tic dimensions. The best results are obtained with
a few hundred dimensions. It is important to high-
light the fact that small differences between high
dimensional models do not necessarily reflect a
difference in the quality of the semantic repre-
sentation. In this regard, Landauer and Dumais
(1997) argued that very small changes in com-
puted cosines can in some cases alter the LSA or-
dering of the words and hence affect the perfor-
mance score. Therefore only big differences in the
Median Ranks could be explained as a real dif-
ference in the overall quality of the models. The
global trend we obtained is consistent with the re-
sults in Griffiths et al (2007) and with the findings
in Landauer and Dumais (1997) where maximum
performance for a different task (TOEFL synonym
test) was obtained over a broad region around 300
dimensions.
Besides the effect of dimensionality, Figure 3 (left)
indicates that performance gets better as we in-
crease the corpus size.
In Figure 3 (right) we show the corresponding re-
sults for the corpus-based SDT-? measure. We can
see that SDT-? shows a parallel set of results and
correctly predicts both the effect of dimensionality
and the effect of corpus size. Indeed, the general
trend is quite similar to the one described with the
Median Rank in that the best performance is ob-
tained for a few hundred dimensions and the three
curves show a better score for large corpora.
Figure 4 shows the effect of document length on
the Median Rank and SDT-?. For both measures,
we computed these scores and averaged them over
the three corpora and the range of dimensions go-
ing from 100 to 1000. As we can see, SDT-? pre-
dicts the psychological optimal document length,
167
Figure 2 : Schematic description of the Same-Different Task used.
which is about 10 sentences per document. In the
corpus we used, this gives on average of about 170
words/document. This value confirms the intuition
of Landauer and Dumais (1997) who used a para-
graph of about 150 word/document in their model.
Finally, Figure 5 (left) summarizes the entire
set of results. It shows the overall correlation
between SDT-? and the Median Rank. One
point in the graph corresponds to a particular
choice of semantic dimension, document length
and corpus size. To measure the correlation, we
use the Maximal Information Coefficient (MIC)
recently introduced by Reshef et al (2011). This
measure captures a wide range of dependencies
between two variables both functional and not.
For functional and non-linear associations it gives
a score that roughly equals the coefficient of
determination (R2) of the data relative to the
regression function. For our data this correlation
measure yields a score of MIC = 0.677 with
(p < 10?6).
In order to see how the SDT-? measure would
correlate with another human-generated bench-
mark, we ran an additional experiment using the
TOEFL synonym test (Landauer and Dumais,
1997) as gold standard. It contains a list of
80 questions consisting of a probe word and
four answers (only one of which is defined as
the correct synonym). We tested the effect of
semantic dimensionality on a 6 M word sized
Wikipedia corpus where documents contained
respectively 2, 10 and 100 sentences for each
series of runs. We kept only the questions for
which the probes and the 4 answers all appeared
in the corpus vocabulary. This left us with a
set of 43 questions. We computed the response
of the model on a probe word by selecting the
answer word with which it had the smallest cosine
angle. The best performance (65.1% correct) was
obtained with 600 dimensions. This is similar
to the result reported in Landauer and Dumais
(1997) where the best performance obtained was
64.4% (compared to 64.5% produced by non-
native English speakers applying to US colleges).
The correlation with SDT-? is shown in Figure
5 (right). Here again, our corpus-based measure
predicts the general trend of the behavior-based
measure: higher values of SDT-? correspond
to higher percentage of correct answers. The
correlation yields a score of MIC = 0.675 with
(p < 10?6).
In both experiments, we used the overlap set of
the gold standard with the Wikicorpus to compute
the SDT-? measure. However, as the main idea
is to apply this evaluation method to corpora for
which there is no available human-generated gold
standards, we computed new correlations using a
SDT-? measure computed, this time, over a set
of randomly selected words. For this purpose we
used the 4M corpus with 10 sentences long docu-
ments and we varied the semantic dimensions. We
used the Median Rank computed with the Free as-
sociation norms as a behavior-based measure.
We tested both the effect of frequency and size:
we varied the set size from 100 to 1000 words
which we randomly selected from three frequency
ranges : higher than 400, between 40 and 400 and
between 40 and 1. We chose the limit of 400 so
that we can have at least 1000 words in the first
range. On the other hand, we did not consider
words which occur only once because the SDT-?
requires at least two instances of a word to gener-
ate a pseudo-synonym.
The correlation scores are shown in Table 2.
Based on the MIC correlation measure, mid-
168
Figure 3 : The Median rank (left) and SDT-? (right) as a function of a number of dimensions and corpus sizes. Document size
is 10 sentences.
Figure 4 : The Median rank (left) and SDT-? (right) as a function of document length (number of sentences). Both measures
are averaged over the three corpora and over the range of dimensions going from 100 to 1000.
Figure 5 : Overall correlation between Median Rank and SDT-? (left) and between Correct answers in TOEFL synonym test
and SDT-? (right) for all the runs. .
169
Freq. x 1 < x < 40 40 < x < 400 x > 400 All Overlap
Size 100 500 1000 100 500 1000 100 500 1000 ? 4 M 1093
MIC 0.311 0.219 0.549? 0.549? 0.717? 0.717? 0.311 0.205 0.419 0.549? 0.717?
* : p < 0.05
Table 2 : Correlation scores of the Median Rank with the SDT-? measure computed over randomly selected words from the
corpus, the whole lexicon and the overlap with the free association norms. We test the effect of frequency and set size.
frequency words yield better scores. The corre-
lations are as high as the one computed with the
overlap even with a half size set (500 words).
The overlap is itself mostly composed of mid-
frequency words, but we made sure that the ran-
dom test sets have no more than 10% of their
words in the overlap. Mid-frequency words are
known to be the best predictors of the conceptual
content of a corpus, very common and very rare
terms have a weaker discriminating or ?resolving?
power (Luhn, 1958).
4 Discussion
We found that SDT-? enables to predict the out-
come of behavior-based evaluation methods with
reasonable accuracy across a range of parameters
of a LSA model. It could therefore be used as a
proxy when human-generated data are not avail-
able. When faced with a new corpus and a task
involving similarity between words, one could im-
plement this rather straightforward method in or-
der, for instance, to set the semantic model param-
eters.
The method could also be used to compare the
performance of different distributional semantic
models, because it does not depend on a partic-
ular format for semantic representation. All that is
required is the existence of a semantic similarity
measure between pairs of words. However, fur-
ther work is needed to evaluate the robustness of
this measure in models other than LSA.
It is important to keep in mind that the correla-
tion of our measure with the behavior-based meth-
ods only indicates that SDT-? can be trusted, to
some extent, in evaluating these semantic tasks.
It does not necessarily validate its ability to as-
sess the entire semantic structure of a distribu-
tional model. Indeed, the behavior-based methods
are dependent on particular tasks (i.g., generating
associates, or responding to a multiple choice syn-
onym test) hence they represent only an indirect
evaluation of a model, viewed through these spe-
cific tasks.
It is worth mentioning that Baroni and Lenci
(2011) introduced a comprehensive technique that
tries to assess simultaneously a variety of seman-
tic relations like meronymy, hypernymy and coor-
dination. Our measure does not enable us to as-
sess these relations, but it could provide a valu-
able tool to explore other fine-grained features of
the semantic structure. Indeed, while we intro-
duced SDT-? as a global measure over a set of test
words, it can also be computed word by word. In-
deed, we can compute how well a given seman-
tic model can detect that ?Art1? and ?Art2? are
the same word, by comparing their semantic dis-
tance to that of random pairs of words. Such a
word-specific measure could assess the semantic
stability of different parts of the lexicon such as
concrete vs. abstract word categories, or the distri-
bution properties of different linguistic categories
(verb, adjectives, ..). Future work is needed to as-
sess the extent to which the SDT-? measure and
its word-level variant provide a general framework
for DSMs evaluation without external resources.
Finally, one concern that could be raised by our
method is the fact that splitting words may affect
the semantic structure of the model we want to as-
sess because it may alter the lexical distribution in
the corpus, resulting in unnaturally sparse statis-
tics. There is in fact evidence that corpus attributes
can have a big effect on the extracted model (Srid-
haran and Murphy, 2012; Lindsey et al, 2007).
However, as shown by the high correlation scores,
the introduced pseudo-synonyms do not seem to
have a dramatic effect on the model, at least as far
as the derived SDT-? measure and its predictive
power is concerned. Moreover, we showed that in
order to apply the method, we do not need to use
the whole lexicon, on the contrary, a small test set
of about 500 random mid-frequency words (which
represents less than 2.5 % of the total vocabulary)
was shown to lead to better results. However, even
if the results are not directly affected in our case,
future work needs to investigate the exact effect
word splitting may have on the semantic model.
170
References
Andrews, M., G. Vigliocco, and D. Vinson (2009).
Integrating experiential and distributional data
to learn semantic representations. Psychologi-
cal Review 116, 463?498.
Baroni, M. and A. Lenci (2011). How we
BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP 2011 Geometri-
cal Models for Natural Language Semantics
(GEMS 2011) Workshop, East Stroudsburg PA:
ACL, pp. 1?10.
Blei, D. (2012). Probabilistic topic models. Com-
munications of the ACM 55(4), 77?84.
Dagan, I., L. Lee, and F. Pereira (1997).
Similarity-based methods for word sense dis-
ambiguation. In Proceedings of the 35th
ACL/8th EACL, pp. 56?63.
Fawcett, T. (2006). An introduction to ROC anal-
ysis. Pattern Recognition Letters 27(8), 861?
874.
Gale, W., K. Church, and D. Yarowsky (1992).
Work on statistical methods for word sense dis-
ambiguation. Workings notes, AAAI Fall Sym-
posium Series, Probabilistic Approaches to Nat-
ural Language, 54?60.
Griffiths, T., M. Steyvers, and J. Tenenbaum
(2007). Topics in semantic representation. Psy-
chological Review 114, 114?244.
Harris, Z. (1954). Distributional structure.
Word 10(23), 146?162.
Hutchison, K., D. Balota, M. Cortese, and J. Wat-
son (2008). Predicting semantic priming at the
item level. Quarterly Journal of Experimental
Psychology 61(7), 1036?1066.
Kievit-Kylar, B. and M. N. Jones (2012). Visualiz-
ing multiple word similarity measures. Behav-
ior Research Methods 44(3), 656?674.
Landauer, T. and S. Dumais (1997). A solution
to plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Re-
view 104(2), 211?240.
Lewandowsky, S. and S. Farrell (2011). Compu-
tational modeling in cognition : principles and
practice. Thousand Oaks, Calif. : Sage Publi-
cations.
Lindsey, R., V. Veksler, and A. G. andWayne Gray
(2007). Be wary of what your computer reads:
The effects of corpus selection on measuring
semantic relatedness. In Proceedings of the
Eighth International Conference on Cognitive
Modeling, pp. 279?284.
Luhn, H. P. (1958). The automatic creation of lit-
erature abstracts. IBM Journal of Research and
Development 2(2), 157?165.
Nelson, D., C. McEvoy, and T. Schreiber (1998).
The university of south florida word association,
rhyme, and word fragment norms.
Reese, S., G. Boleda, M. Cuadros, L. Padro, and
G. Rigau (2010). Wikicorpus: A word-sense
disambiguated multilingual wikipedia corpus.
In Proceedings of 7th Language Resources and
Evaluation Conference (LREC?10), La Valleta,
Malta.
R?ehu?r?ek, R. and P. Sojka (2010). Software frame-
work for topic modelling with large corpora. In
Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks, Valletta,
Malta, pp. 45?50.
Reshef, D., Y. Reshef, H. Finucane, S. Gross-
man, G. McVean, P. Turnbaugh, E. Lander,
M. Mitzenmacher, and P. Sabeti (2011). De-
tecting novel associations in large datasets. Sci-
ence 334(6062), 1518?1524.
Sridharan, S. and B. Murphy (2012). Modeling
word meaning: distributional semantics and the
sorpus quality-quantity trade-off. In Proceed-
ings of the 3rd Workshop on Cognitive Aspects
of the Lexicon, COLING 2012, Mumbai, pp.
53?68.
Turney, P. D. and P. Pantel (2010). From frequency
to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research 37,
141?188.
171
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1?6,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exploring the Relative Role of Bottom-up and Top-down Information in
Phoneme Learning
Abdellah Fourtassi
1
, Thomas Schatz
1,2
, Balakrishnan Varadarajan
3
, Emmanuel Dupoux
1
1
Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris, France
2
SIERRA Project-Team, INRIA/ENS/CNRS, Paris, France
3
Center for Language and Speech Processing, JHU, Baltimore, USA
{abdellah.fourtassi; emmanuel.dupoux; balaji.iitm1}@gmail
thomas.schatz@laposte.net
Abstract
We test both bottom-up and top-down ap-
proaches in learning the phonemic status
of the sounds of English and Japanese. We
used large corpora of spontaneous speech
to provide the learner with an input that
models both the linguistic properties and
statistical regularities of each language.
We found both approaches to help dis-
criminate between allophonic and phone-
mic contrasts with a high degree of accu-
racy, although top-down cues proved to be
effective only on an interesting subset of
the data.
1 Introduction
Developmental studies have shown that, during
their first year, infants tune in on the phonemic cat-
egories (consonants and vowels) of their language,
i.e., they lose the ability to distinguish some
within-category contrasts (Werker and Tees, 1984)
and enhance their ability to distinguish between-
category contrasts (Kuhl et al, 2006). Current
work in early language acquisition has proposed
two competing hypotheses that purport to account
for the acquisition of phonemes. The bottom-up
hypothesis holds that infants converge on the lin-
guistic units of their language through a similarity-
based distributional analysis of their input (Maye
et al, 2002; Vallabha et al, 2007). In contrast,
the top-down hypothesis emphasizes the role of
higher level linguistic structures in order to learn
the lower level units (Feldman et al, 2013; Mar-
tin et al, 2013). The aim of the present work is
to explore how much information can ideally be
derived from both hypotheses.
The paper is organized as follows. First we de-
scribe how we modeled phonetic variation from
audio recordings, second we introduce a bottom-
up cue based on acoustic similarity and top-
down cues based of the properties of the lexicon.
We test their performance in a task that consists
in discriminating within-category contrasts from
between-category contrasts. Finally we discuss
the role and scope of each cue for the acquisition
of phonemes.
2 Modeling phonetic variation
In this section, we describe how we modeled the
representation of speech sounds putatively pro-
cessed by infants, before they learn the relevant
phonemic categories of their language. Following
Peperkamp et al (2006), we make the assumption
that this input is quantized into context-dependent
phone-sized unit we call allophones. Consider the
example of the allophonic rule that applies to the
French /r/:
/r/?
{
[X] / before a voiceless obstruent
[K] elsewhere
Figure 1: Allophonic variation of French /r/
The phoneme /r/ surfaces as voiced ([K]) before
a voiced obstruent like in [kanaK Zon] (?canard
jaune?, yellow duck) and as voiceless ([X]) before
a voiceless obstruent as in [kanaX puXpK] (?ca-
nard pourpre?, purple duck). Assuming speech
sounds are coded as allophones, the challenge fac-
ing the learner is to distinguish the allophonic vari-
ation ([K], [X]) from the phonemic variation (re-
lated to a difference in the meaning) like the con-
trast ([K],[l]).
Previous work has generated allophonic varia-
tion using random contexts (Martin et al, 2013).
This procedure does not take into account the fact
that contexts belong to natural classes. In addition,
it does not enable to compute an acoustic distance.
Here, we generate linguistically and acoustically
controlled allophones using Hidden Markov Mod-
els (HMMs) trained on audio recordings.
1
2.1 Corpora
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al, 2007), which consists of 40
hours of spontaneous conversations with 40 speak-
ers of American English, and the core of the Cor-
pus of Spontaneous Japanese (Maekawa et al,
2000) which also consists of about 40 hours of
recorded spontaneous conversations and public
speeches in different fields. Both corpora are time-
aligned with phonetic labels. Following Boruta
(2012), we relabeled the japanese corpus using 25
phonemes. For English, we used the phonemic
version which consists of 45 phonemes.
2.2 Input generation
2.2.1 HMM-based allophones
In order to generate linguistically and acoustically
plausible allophones, we apply a standard Hidden
Markov Model (HMM) phoneme recognizer with
a three-state per phone architecture to the signal,
as follows.
First, we convert the raw speech waveform of
the corpora into successive vectors of Mel Fre-
quency Cepstrum Coefficients (MFCC), computed
over 25 ms windows, using a period of 10 ms
(the windows overlap). We use 12 MFCC coeffi-
cients, plus the energy, plus the first and second or-
der derivatives, yielding 39 dimensions per frame.
Second, we start HMM training using one three-
state model per phoneme. Third, each phoneme
model is cloned into context-dependent triphone
models, for each context in which the phoneme
actually occurs (for example, the phoneme /A/ oc-
curs in the context [d?A?g] as in the word /dAg/
(?dog?). The triphone models are then retrained on
only the relevant subset of the data, corresponding
to the given triphone context. These detailed mod-
els are clustered back into inventories of various
sizes (from 2 to 20 times the size of the phone-
mic inventory) using a linguistic feature-based de-
cision tree, and the HMM states of linguistically
similar triphones are tied together so as to max-
imize the likelihood of the data. Finally, the tri-
phone models are trained again while the initial
gaussian emission models are replaced by mix-
ture of gaussians with a progressively increasing
number of components, until each HMM state is
modeled by a mixture of 17 diagonal-covariance
gaussians. The HMM were built using the HMM
Toolkit (HTK: Young et al, 2006).
2.2.2 Random allophones
As a control, we also reproduce the random al-
lophones of Martin et al (2013), in which allo-
phonic contexts are determined randomly: for a
given phoneme /p/, the set of all possible con-
texts is randomly partitioned into a fixed number
n of subsets. In the transcription, the phoneme /p/
is converted into one of its allophones (p
1
,p
2
,..,p
n
)
depending on the subset to which the current con-
text belongs.
3 Bottom-up and top-down hypotheses
3.1 Acoustic cue
The bottom-up cue is based on the hypothesis that
instances of the same phoneme are likely to be
acoustically more similar than instances of two
different phonemes (see Cristia and Seidl, in press)
for a similar proposition). In order to provide
a proxy for the perceptual distance between al-
lophones, we measure the information theoretic
distance between the acoustic HMMs of these al-
lophones. The 3-state HMMs of the two allo-
phones were aligned with Dynamic Time Warping
(DTW), using as a distance between pairs of emit-
ting states, a symmetrized version of the Kullback-
Leibler (KL) divergence measure (each state was
approximated by a single non-diagonal Gaussian):
A(x, y) =
?
(i,j)?DTW (x,y)
KL(N
x
i
||N
y
j
) +KL(N
y
j
||N
x
i
)
Where {(i, j) ? DTW (x, y)} is the set of in-
dex pairs over the HMM states that correspond to
the optimal DTW path in the comparison between
phone model x and y, and N
x
i
the full covariance
Gaussian distribution for state i of phone x. For
obvious reasons, the acoustic distance cue cannot
be computed for Random allophones.
3.2 Lexical cues
The top-down information we use in this study, is
based on the insight of Martin et al (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes.
In fact, the latter creates variants (alternants) of the
same lexical item since adjacent sounds condition
the realization of the first and final phoneme. For
example, as shown in figure 1, the phoneme /r/ sur-
faces as [X] or [K] depending on whether or not the
2
next sound is a voiceless obstruent. Therefore, the
lexical item /kanar/ surfaces as [kanaX] or [kanaK].
The lexical cue assumes that a pair of words dif-
fering in the first or last segment (like [kanaX] and
[kanaK]) is more likely to be the result of a phono-
logical process triggered by adjacent sounds, than
a true semantic minimal pair.
However, this strategy clearly gives rise to false
alarms in the (albeit relatively rare) case of true
minimal pairs like [kanaX] (?duck?) and [kanal]
(?canal?), where ([X], [l]) will be mistakenly la-
beled as allophonic.
In order to mitigate the problem of false alarms,
we also use Boruta (2011)?s continuous version,
where each pair of phones is characterized by the
number of lexical minimal pairs it forms.
B(x, y) = |(Ax,Ay) ? L
2
|+ |(xA, yA) ? L
2
|
where {Ax ? L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax,Ay) ?
L
2
} is the set of phonological minimal pairs in
L? L that vary on the final segment.
In addition, we introduce another cue that could
be seen as a normalization of Boruta?s cue:
N (x, y) =
|(Ax,Ay)?L
2
|+|(xA,yA)?L
2
|
|{Ax?L}|+|{Ay?L}|+|{xA?L}|+|{yA?L}|
4 Experiment
4.1 Task
For each corpus we list all the possible pairs of
attested allophones. Some of these pairs are allo-
phones of the same phoneme (allophonic pair) and
others are allophones of different phonemes (non-
allophonic pairs). The task is a same-different
classification, whereby each of these pairs is given
a score from the cue that is being tested. A good
cue gives higher scores to allophonic pairs.
4.2 Evaluation
We use the same evaluation procedure as in Mar-
tin et al (2013). It is carried out by computing
the area under the curve of the Receiver Operat-
ing Characteristic (ROC). A value of 0.5 repre-
sents chance and a value of 1 represents perfect
performance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks (of 20 utterances each). In each run, we
draw randomly with replacement from this set of
blocks a sample of the same size as the original
corpus. This sample is then used to retrain the
acoustic models and generate a phonetic inven-
tory that we use to re-transcribe the corpus and
re-compute the cues. We report scores averaged
over 5 such runs.
4.3 Results
Table 1 shows the classification scores for the lex-
ical cues when we vary the inventory size from
2 allophones per phoneme in average, to 20 al-
lophones per phoneme, using the Random allo-
phones. The top-down scores are very high, repli-
cating Martin et al?s results, and even improving
the performance using Boruta?s cue and our new
Normalized cue.
? English Japanese
Allo./phon. M B N M B N
2 0.784 0.935 0.951 0.580 0.989 1.00
5 0.845 0.974 0.982 0.653 0.978 0.991
10 0.886 0.974 0.981 0.733 0.944 0.971
20 0.918 0.961 0.966 0.785 0.869 0.886
Table 1 : Same-different scores for top-down cues on
Random allophones, as a function of the average number of
allophones per phoneme. M=Martin et al, B=Boruta, N=
Normalized
Table 2 shows the results for HMM-based allo-
phones. The acoustic score is very accurate for
both languages and is quite robust to variation.
Top-down cues, on the other hand, perform, sur-
prisingly, almost at chance level in distinguish-
ing between allophonic and non-allophonic pairs.
A similar discrepancy for the case of Japanese
was actually noted, but not explained, in Boruta
(2012).
? English Japanese
Allo./phon. A M B N A M B N
2 0.916 0.592 0.632 0.643 0.885 0.422 0.524 0.537
5 0.918 0.592 0.607 0.611 0.908 0.507 0.542 0.551
10 0.893 0.569 0.571 0.571 0.827 0.533 0.546 0.548
20 0.879 0.560 0.560 0.559 0.876 0.541 0.543 0.543
Table 2 : Same-different scores for bottom-up and top-down
cues on HMM-based allophones, as a function of the
average number of allophones per phoneme. A=Acoustic,
M=Martin et al, B=Boruta, N= Normalized
5 Analysis
5.1 Why does the performance drop for
realistic allophones?
When we list all possible pairs of allophones in
the inventory, some of them correspond to lexi-
3
cal alternants ([X], [K]) ? ([kanaX] and [kanaK]),
others to true minimal pairs ([K], [l]) ? ([kanaK]
and [kanal]), and yet others will simply not gen-
erate lexical variation at all, we will call those:
invisible pairs. For instance, in English, /h/ and
/N/ occur in different syllable positions and thus
cannot appear in any minimal pair. As defined
above, top-down cues are set to 0 in such pairs
(which means that they are systematically classi-
fied as non-allophonic). This is a correct decision
for /h/ vs. /N/, but not for invisible pairs that also
happen to be allophonic, resulting in false nega-
tives. In tables 3, we show that, indeed, invisible
pairs is a major issue, and could explain to a large
extent the pattern of results found above. In fact,
the proportion of visible allophonic pairs (?allo?
column) is way lower for HMM-based allophones.
This means that the majority of allophonic pairs in
the HMM case are invisible, and therefore, will be
mistakenly classified as non-allophonic.
? Random HMM
? English Japanese English Japanese
Allo./phon. allo ? allo allo ? allo allo ? allo allo ? allo
2 92.9 36.3 100 83.9 48.9 25.3 37.1 53.2
5 97.2 28.4 99.6 69.0 31.1 14.3 25.0 25.9
10 96.8 19.9 96.7 50.1 19.8 4.23 21.0 14.4
20 94.3 10.8 83.4 26.4 14.0 1.89 12.4 4.04
Table 3 : Proportion (in %) of allophonic pairs (allo), and
non-allophonic pairs (? allo) associated with at least one
lexical minimal pair, in Random and HMM allophones.
There are basically two reasons why an allo-
phonic pair would be invisible ( will not generate
lexical alternants). The first one is the absence of
evidence, e.g., if the edges of the word with the
underlying phoneme do not appear in enough con-
texts to generate the corresponding variants. This
happens when the corpus is so small that no word
ending with, say, /r/ appears in both voiced and
voiceless contexts. The second, is when the allo-
phones are triggered on maximally different con-
texts (on the right and the left) as illustrated below:
/p/?
{
[p
1
] / A B
[p
2
] / C D
When A doesn?t overlap with C and B does not
overlap with D, it becomes impossible for the pair
([p
1
], [p
2
]) to generate a lexical minimal pair. This
is simply because a pair of allophones needs to
share at least one context to be able to form vari-
ants of a word (the second or penultimate segment
of this word).
When asked to split the set of contexts in two
distinct categories that trigger [p
1
] and [p
2
] (i.e.,
A B and C D), the random procedure will of-
ten make A overlap with B and C overlap with D
because it is completely oblivious to any acous-
tic or linguistic similarity, thus making it always
possible for the pair of allophones to generate lex-
ical alternants. A more realistic categorization
(like the HMM-based one), will naturally tend to
minimize within-category distance, and maximize
between-category distance. Therefore, we will
have less overlap, making the chances of the pair
to generate a lexical pair smaller. The more al-
lophones we have, the bigger is the chance to end
up with non-overlapping categories (invisible allo-
phonic pairs), and the more mistakes will be made,
as shown in Table 3.
5.2 Restricting the role of top-down cues
The analysis above shows that top-down cues can-
not be used to classify all contrasts. The approxi-
mation that consists in considering all pairs that do
not generate lexical pairs as non-allophonic, does
not scale up to realistic input. A more intuitive,
but less ambitious, assumption is to restrict the
scope of top-down cues to contrasts that do gen-
erate lexical variation (lexical alternants or true
minimal pairs). Thus, they remain completely ag-
nostic to the status of invisible pairs. This restric-
tion makes sense since top-down information boils
down to knowing whether two word forms belong
to the same lexical category (reducing variation to
allophony), or to two different categories (varia-
tion is then considered non-allophonic). Phonetic
variation that does not cause lexical variation is, in
this particular sense, orthogonal to our knowledge
about the lexicon.
We test this hypothesis by applying the cues
only to the subset of pairs that are associated with
at least one lexical minimal pair. We vary the num-
ber of allophones per phoneme on the one hand
(Table 4) and the size of the input on the other
hand (Table 5). We refer to this subset by an aster-
isk (*), by which we also mark the cues that apply
to it. Notice that, in this new framing, the M cue is
completely uninformative since it assigns the same
value to all pairs.
As predicted, the cues perform very well on this
subset, especially the N cue. The combination of
top-down and bottom-up cues shows that the for-
mer is always useful, and that these two sources of
4
? English Japanese
? ? Individual cues Combination ? Individual cues Combination
Allo./phon. * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
2 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
4 14.3 0.918 0.964 0.858 0.951 0.975 0.991 30.88 0.908 0.917 0.850 0.936 0.934 0.976
10 4.24 0.893 0.937 0.813 0.939 0.960 0.968 16.06 0.827 0.839 0.899 0.957 0.904 0.936
20 1.67 0.879 0.907 0.802 0.907 0.942 0.940 5.02 0.876 0.856 0.882 0.959 0.913 0.950
Table 4 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of average
number of allophones per phonemes.
? English Japanese
? ? Individual cues Combination ? Individual cues Combination
Size (hours) * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
1 9.87 0.885 0.907 0.741 0.915 0.927 0.969 34.78 0.890 0.883 0.835 0.915 0.889 0.934
4 18.3 0.918 0.958 0.798 0.917 0.967 0.989 48.00 0.917 0.939 0.860 0.937 0.938 0.973
8 21.3 0.916 0.964 0.837 0.942 0.971 0.992 51.71 0.915 0.940 0.889 0.937 0.954 0.977
20 24.4 0.911 0.960 0.827 0.936 0.969 0.994 58.12 0.921 0.954 0.865 0.912 0.945 0.971
40 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
? 34.82 ? ? ? ? ? ? 72.16 ? ? ? ? ? ?
Table 5 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of corpus size.
* (%) refers to the proportion of the subset of contrasts associated with at least one minimal pair. The cues applied to this
subset are marked with an asterisk (*)
information are not completely redundant. How-
ever, the scope of top-down cues (the proportion of
the subset * ) shrinks as we increase the number of
allophones. Table 5 shows that this problem can,
in principle, be mitigated by increasing the amount
of data available to the learner. As we were limited
to only 40 hours of speech, we generated an artifi-
cial corpus that uses the same lexicon but with all
possible word orders so as to maximize the num-
ber of contexts in which words appear. This artifi-
cial corpus increases the proportion of the subset,
but we are still not at 100 % coverage, which ac-
cording the analysis above, is due (at least in part)
to the irreducible set of non-overlapping pairs.
6 Conclusion
In this study we explored the role of both bottom-
up and top-down hypotheses in learning the
phonemic status of the sounds of two typologically
different languages. We introduced a bottom-up
cue based on acoustic similarity, and we used al-
ready existing top-down cues to which we pro-
vided a new extension. We tested these hypothe-
ses on English and Japanese, providing the learner
with an input that mirrors closely the linguistic
and acoustic properties of each language. We
showed, on the one hand, that the bottom-up cue is
a very reliable source of information, across differ-
ent levels of variation and even with small amount
of data. Top-down cues, on the other hand, were
found to be effective only on a subset of the data,
which corresponds to the interesting contrasts that
cause lexical variation. Their role becomes more
relevant as the learner gets more linguistic experi-
ence, and their combination with bottom-up cues
shows that they can provide non-redundant infor-
mation. Note, finally, that even if this work is
based on a more realistic input compared to previ-
ous studies, it still uses simplifying assumptions,
like ideal word segmentation, and no low-level
acoustic variability. Those assumptions are, how-
ever, useful in quantifying the information that can
ideally be extracted from the input, which is a nec-
essary preliminary step before modeling how this
input is used in a cognitively plausible way. Inter-
ested readers may refer to (Fourtassi and Dupoux,
2014; Fourtassi et al, 2014) for a more learning-
oriented approach, where some of the assumptions
made here about high level representations are re-
laxed.
Acknowledgments
This project is funded in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R?egion Ile de France (DIM cerveau et pens?ee). We
thank Luc Boruta, Sanjeev Khudanpur, Isabelle
Dautriche, Sharon Peperkamp and Benoit Crabb?e
for highly useful discussions and contributions.
5
References
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88?93.
Luc Boruta. 2012. Indicateurs d?allophonie et
de phon?emicit?e. Doctoral dissertation, Universit
?e
Paris-Diderot - Paris VII.
A. Cristia and A. Seidl. In press. The hyperarticula-
tion hypothesis of infant-directed speech. Journal
of Child Language.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. 2013. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review, 120(4):751?778.
Abdellah Fourtassi and Emmanuel Dupoux. 2014. A
rudimentary lexicon and semantics help bootstrap
phoneme acquisition. In Proceedings of the 18th
Conference on Computational Natural Language
Learning (CoNLL).
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th Annual Meeting of the Cognitive Science
Society.
Patricia K. Kuhl, Erica Stevens, Akiko Hayashi,
Toshisada Deguchi, Shigeru Kiritani, and Paul Iver-
son. 2006. Infants show a facilitation effect for na-
tive language phonetic perception between 6 and 12
months. Developmental Science, 9(2):F13?F21.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947?952, Athens, Greece.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103?124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101?B111.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre
Nadal, and Emmanuel Dupoux. 2006. The acqui-
sition of allophonic rules: Statistical learning with
linguistic constraints. Cognition, 101(3):B31?B41.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker,
and S. Amano. 2007. Unsupervised learning
of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 ? 63.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
6
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Whyisenglishsoeasytosegment?
Abdellah Fourtassi1, Benjamin Bo?rschinger2,3
Mark Johnson3 and Emmanuel Dupoux1
(1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
(2) Department of Computing, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au
?
Abstract
Cross-linguistic studies on unsupervised
word segmentation have consistently
shown that English is easier to segment
than other languages. In this paper, we
propose an explanation of this finding
based on the notion of segmentation
ambiguity. We show that English has a
very low segmentation ambiguity com-
pared to Japanese and that this difference
correlates with the segmentation perfor-
mance in a unigram model. We suggest
that segmentation ambiguity is linked
to a trade-off between syllable structure
complexity and word length distribution.
1 Introduction
During the course of language acquisition, in-
fants must learn to segment words from continu-
ous speech. Experimental studies show that they
start doing so from around 7.5 months of age
(Jusczyk and Aslin, 1995). Further studies indi-
cate that infants are sensitive to a number of word
boundary cues, like prosody (Jusczyk et al, 1999;
Mattys et al, 1999), transition probabilities (Saf-
fran et al, 1996; Pelucchi et al, 2009), phonotac-
tics (Mattys et al, 2001), coarticulation (Johnson
and Jusczyk, 2001) and combine these cues with
different weights (Weiss et al, 2010).
Computational models of word segmentation
have played a major role in assessing the relevance
and reliability of different statistical cues present
in the speech input. Some of these models focus
mainly on boundary detection, and assess differ-
ent strategies to identify them (Christiansen et al,
1998; Xanthos, 2004; Swingley, 2005; Daland and
Pierrehumbert, 2011). Other models, sometimes
called lexicon-building algorithms, learn the lexi-
con and the segmentation at the same time and use
knowledge about the extracted lexicon to segment
novel utterances. State-of-the-art lexicon-building
segmentation algorithms are typically reported to
yield better performance than word boundary de-
tection algorithms (Brent, 1999; Venkataraman,
2001; Batchelder, 2002; Goldwater, 2007; John-
son, 2008b; Fleck, 2008; Blanchard et al, 2010).
As seen in Table 1, however, the performance
varies considerably across languages with English
winning by a high margin. This raises a general-
izability issue for NLP applications, but also for
the modeling of language acquisition since, obvi-
ously, it is not the case that in some languages,
infants fail to acquire an adult lexicon. Are these
performance differences only due to the fact that
the algorithms might be optimized for English? Or
do they also reflect some intrinsic linguistic differ-
ences between languages?
Lang. F-score Model Reference
English 0.89 AG Johnson (2009)
Chinese 0.77 AG Johnson (2010)
Spanish 0.58 DP Bigram Fleck (2008)
Arabic 0.56 WordEnds Fleck (2008)
Sesotho 0.55 AG Johnson (2008)
Japanese 0.55 BootLex Batchelder (2002)
French 0.54 NGS-u Boruta (2011)
Table 1: State-of-the-art unsupervised segmentation scores
for eight languages.
The aim of the present work is to understand
why English usually scores better than other lan-
guages, as far as unsupervised segmentation is
concerned. As a comparison point, we chose
Japanese because it is among the languages that
have given the poorest word segmentation scores.
In fact, Boruta et al (2011) found an F-score
around 0.41 using both Brent (1999)?s MBDP-1
and Venkataraman (2001)?s NGS-u models, and
Batchelder (2002) found an F-score that goes
from 0.40 to 0.55 depending on the corpus used.
Japanese also differs typologically from English
along several phonological dimensions such as
1
number of syllabic types, phonotactic constraints
and rhythmic structure. Although most lexicon-
building segmentation algorithms do not attempt
to model these dimensions, they still might be rel-
evant to speech segmentation and help explain the
performance difference.
The structure of the paper is as follows. First,
we present the class of lexical-building segmen-
tation algorithm that we use in this paper (Adap-
tor Grammar), and our English and Japanese cor-
pora. We then present data replicating the basic
finding that segmentation performance is better for
English than for Japanese. We then explore the hy-
pothesis that this finding is due to an intrinsic dif-
ference in segmentation ambiguity in the two lan-
guages, and suggest that the source of this differ-
ence rests in the structure of the phonological lexi-
con in the two languages. Finally, we use these in-
sights to try and reduce the gap between Japanese
and English segmentation through a modification
of the Unigram model where multiple linguistic
levels are learned jointly.
2 Computational Framework and
Corpora
2.1 Adaptor Grammar
In this study, we use the Adaptor Grammar frame-
work (Johnson et al, 2007) to test different mod-
els of word segmentation on English and Japanese
Corpora. This framework makes it possible to
express a class of hierarchical non-parametric
Bayesian models using an extension of probabilis-
tic context-free grammars called Adaptor Gram-
mar (AG). It allows one to easily define models
that incorporate different assumptions about lin-
guistic structure and is therefore a useful practical
tool for exploring different hypotheses about word
segmentation (Johnson, 2008b; Johnson, 2008a;
Johnson et al, 2010; Bo?rschinger et al, 2012).
For mathematical details and a description of
the inference procedure for AGs, we refer the
reader to Johnson et al (2007). Briefly, AG uses
the non-parametric Pitman-Yor-Process (Pitman
and Yor, 1997) which, as in Minimum Descrip-
tion lengths models, finds a compact representa-
tion of the input by re-using frequent structures
(here, words).
2.2 Corpora
In the present study, we used both Child Di-
rected Speech (CDS) and Adult Directed Speech
(ADS) corpora. English CDS was derived from
the Bernstein-Ratner corpus (Bernstein-Ratner,
1987), which consists in transcribed verbal inter-
action of parents with nine children between 1
and 2 years of age. We used the 9,790 utter-
ances that were phonemically transcribed by Brent
and Cartwright (1996). Japanese CDS consists in
the first 10, 000 utterances of the Hamasaki cor-
pus (Hamasaki, 2002). It provides a phonemic
transcript of spontaneous speech to a single child
collected from when the child was 2 up to when
it was 3.5 years old. Both CDS corpora are avail-
able from the CHILDES database (MacWhinney,
2000).
As for English ADS, we used the first 10,000
utterances of the Buckeye Speech Corpus (Pitt et
al., 2007) which consists in spontaneous conver-
sations with 40 speakers in American English. To
make it comparable to the other corpora in this
paper, we only used the idealized phonemic tran-
scription. Finally, for Japanese ADS, we used
the first 10,000 utterances of a phonemic tran-
scription of the Corpus of Spontaneous Japanese
(Maekawa et al, 2000). It consists of recorded
spontaneous conversations, or public speeches in
different fields ranging from engineering to hu-
manities. For each corpus, we present elementary
statistics in Table 2.
3 Unsupervised segmentation with the
Unigram Model
3.1 Setup
In this experiment we used the Adaptor Gram-
mar framework to implement a Unigram model of
word segmentation (Johnson et al, 2007). This
model has been shown to be equivalent to the orig-
inal MBDP-1 segmentation model (see Goldwater
(2007)). The model is defined as:
?
Utterance?Word+
Word? Phoneme+
?
In the AG framework, an underlined non-
terminal indicates that this non-terminal is
adapted, i.e. that the AG will cache (and learn
probabilities for) entire sub-trees rooted in this
non-terminal. Here, Word is the only unit that the
model effectively learns, and there are no depen-
dencies between the words to be learned. This
grammar states that an utterance must be analyzed
in terms of one or more Words, where a Word is a
2
Corpus Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
Tokens
? Utterances 9, 790 10, 000 10, 000 10, 000
? Words 33, 399 27, 362 57, 185 87, 156
? Phonemes 95, 809 108, 427 183, 196 289, 264
Types
? Words 1, 321 2, 389 3, 708 4, 206
? Phonemes 50 30 44 25
Average Lengths
? Words per utterance 3.41 2.74 5.72 8.72
? Phonemes per utterance 9.79 10.84 18.32 28.93
? Phonemes per word 2.87 3.96 3.20 3.32
Table 2 : Characteristics of phonemically transcribed corpora
sequence of Phonemes.
We ran the model twice on each corpus for
2,000 iterations with hyper-parameter sampling
and we collected samples throughout the process,
following the methodology of Johnson and Gold-
water (2009)1. For evaluation, we performed their
Minimum Bayes Risk decoding using the col-
lected samples to get a single score.
3.2 Evaluation
For the evaluation, we used the same measures as
Brent (1999), Venkataraman (2001) and Goldwa-
ter (2007), namely token Precision (P), Recall (R)
and F-score (F). Precision is defined as the num-
ber of correct word tokens found out of all tokens
posited. Recall is the number of correct word to-
kens found out of all tokens in the gold standard.
The F-score is defined as the harmonic mean of
Precision and Recall , F = 2?P?RP+R .
We will refer to these scores as the segmentation
scores. In addition, we define similar measures for
word boundaries and word types in the lexicon.
3.3 Results and discussion
The results are shown in Table 3. As expected,
the model yields substantially better scores in En-
glish than Japanese, for both CDS and ADS. In
addition, we found that in both languages, ADS
yields slightly worse results than CDS. This is to
be expected because ADS uses between 60% and
300% longer utterances than CDS, and as a result
presents the learner with a more difficult segmen-
tation problem. Moreover, ADS includes between
1We used incremental initialization
70% and 280% more word types than CDS, mak-
ing it a more difficult lexical learning problem.
Note, however, that despite these large differences
in corpus statistics, the difference in segmentation
performance between ADS and CDS are small
compared to the differences between Japanese and
English.
An error analysis on English data shows that
most errors come from the Unigrammodel mistak-
ing high frequency collocations for single words
(see also Goldwater (2007)). This leads to an
under-segmentation of chunks like ?a boy? or ?is
it? 2. Yet, the model also tends to break off fre-
quent morphological affixes, especially ?-ing? and
?-s? , leading to an over-segmentation of words
like ?talk ing? or ?black s?.
Similarly, Japanese data shows both over-
and under-segmentation errors. However, over-
segmentation is more severe than for English, as
it does not only affect affixes, but surfaces as
breaking apart multi-syllabic words. In addition,
Japanese segmentation faces another kind of er-
ror which acts across word boundaries. For exam-
ple, ?ni kashite? is segmented as ?nika shite? and
?nurete inakatta? as ?nure tei na katta?. This leads
to an output lexicon that, on the one hand, allows
for a more compact analysis of the corpus than
the true lexicon: the number of word types drops
from 2,389 to 1,463 in CDS and from 4,206 to
2,372 in ADS although the average token length ?
and consequently, overall number of tokens ? does
not change as dramatically, dropping from 3.96 to
2For ease of presentation, we use orthography to present
examples although all experiments are run on phonemic tran-
scripts.
3
? Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
? F P R F P R F P R F P R
Segmentation 0.77 0.76 0.77 0.55 0.51 0.61 0.69 0.66 0.73 0.50 0.48 0.52
Boundaries 0.87 0.87 0.88 0.72 0.63 0.83 0.86 0.81 0.91 0.76 0.74 0.79
Lexicon 0.62 0.65 0.59 0.33 0.43 0.26 0.41 0.48 0.36 0.30 0.42 0.23
Table 3 : Word segmentation scores of the Unigram model
3.31 for CDS and from 3.32 to 3.12 in ADS. On
the other hand, however, most of the output lex-
icon items are not valid Japanese words and this
leads to the bad lexicon F-scores. This, in turn,
leads to the bad overall segmentation performance.
In brief, we have shown that, across two dif-
ferent corpora, English yields consistently better
segmentation results than Japanese for the Uni-
gram model. This confirms and extends the results
of Boruta et al (2011) and Batchelder (2002). It
strongly suggests that the difference is neither due
to a specific choice of model nor to particularities
of the corpora, but reflects a fundamental property
of these two languages.
In the following section, we introduce the no-
tion of segmentation ambiguity, it to English and
Japanese data, and show that it correlates with seg-
mentation performance.
4 Intrinsic Segmentation Ambiguity
Lexicon-based segmentation algorithms like
MBDP-1, NGS-u and the AG Unigram model
learn the lexicon and the segmentation at the
same time. This makes it difficult, in case of
poor performance, to see whether the problem
comes from the intrinsic segmentability of the
language or from the quality of the extracted
lexicon. Our claim is that Japanese is intrinsically
more difficult to segment than English, even when
a good lexicon is already assumed. We explore
this hypothesis by studying segmentation alone,
assuming a perfect (Gold) lexicon.
4.1 Segmentation ambiguity
Without any information, a string of N phonemes
could be segmented in 2N?1 ways. When a lexi-
con is provided, the set of possible segmentations
is reduced to a smaller number. To illustrate this,
suppose we have to segment the input utterance:
/ay s k r iy m/ 3, and that the lexicon contains the
following words : /ay/ (I), /s k r iy m/ (scream),
/ay s/ (ice), /k r iy m/ (cream). Only two segmen-
tations are possible : /ay skriym/ (I scream) and
/ays kriym/ (ice cream).
We are interested in the ambiguity generated by
the different possible parses that result from such a
supervised segmentation. In order to quantify this
idea in general, we define a Normalized Segmenta-
tion Entropy. To do this, we need to assign a prob-
ability to every possible segmentation. To this end,
we use a unigram model where the probability of a
lexical item is its normalized frequency in the cor-
pus and the probability of a parse is the product
of the probabilities of its terms. In order to obtain
a measure that does not depend on the utterance
length, we normalize by the number of possible
boundaries in the utterance. So for an utterance of
length N , the Normalized Segmentation Entropy
(NSE) is computed using Shannon formula (Shan-
non, 1948) as follows:
?
NSE = ?
?
i Pilog2(Pi)/(N ? 1)
?
where Pi is the probability of the parse i .
For CDS data we found Normalized Segmen-
tation Entropies of 0.0021 bits for English and
0.0156 bits for Japanese. In ADS data we
found similar results with 0.0032 bits for English
and 0.0275 bits for Japanese. This means that
Japanese needs between 7 and 8 times more bits
than English to encode segmentation information.
This is a very large difference, which is of the
same magnitude in CDS and ADS. These differ-
ences clearly show that intrinsically, Japanese is
more ambiguous than English with regards to seg-
mentation.
One can refine this analysis by distinguishing
two sources of ambiguity: ambiguity across word
boundaries, as in ?ice cream / [ay s] [k r iy m]?
3We use ARPABET notation to represent phonemic input.
4
Figure 1 : Correlation between Normalized Segmentation Entropy (in bits) and the segmentation F-score for CDS (left) and
ADS (Right)
vs ?I scream / [ay] [s k r iy m]?. And ambigu-
ity within the lexicon, that occurs when a lexical
item is composed of two or more sub-words (like
in ?Butterfly?).
Since we are mainly investigating lexicon-
building models, it is important to measure the am-
biguity within the lexicon itself, in the ideal case
where this lexicon is perfect. To this end, we com-
puted the average number of segmentations for a
lexicon item. For example, the word ?butterfly?
has two possible segmentations : the original word
?butterfly? and a segmentation comprising the two
sub-words : ?butter? and ?fly?. For English to-
kens, we found an average of 1.039 in CDS and
1.057 in ADS. For Japanese tokens, we found an
average of 1.811 in CDS and 1.978 in ADS. En-
glish?s averages are close to 1, indicating that it
doesn?t exhibit lexicon ambiguity. Japanese, how-
ever, has averages close to 2 which means that lex-
ical ambiguity is quite systematic in both CDS and
ADS.
4.2 Segmentation ambiguity and supervised
segmentation
The intrinsic ambiguity in Japanese only shows
that a given sentence has multiple possible seg-
mentations. What remains to be demonstrated is
that these multiple segmentations result in system-
atic segmentation errors. To do this we propose
a supervised segmentation algorithm that enumer-
ates all possible segmentations of an utterance
based on the gold lexicon, and selects the segmen-
tation with the highest probability. In CDS data,
this algorithm yields a segmentation F-score equal
to 0.99 for English and 0.95 for Japanese. In ADS
we find an F-score of 0.96 for English and 0.93 for
Japanese. These results show that lexical informa-
tion alone plus word frequency eliminates almost
all segmentation errors in English, especially for
CDS. As for Japanese, even if the scores remain
impressively high, the lexicon alone is not suffi-
cient to eliminate all the errors. In other words,
even with a gold lexicon, English remains easier
to segment than Japanese.
To quantify the link between segmentation en-
tropy and segmentation errors, we binned the sen-
tences of our corpus in 10 bins according to the
Normalized Segmentation Entropy, and correlate
this with the average segmentation F-score for
each bin. As shown Figure 1, we found significant
correlations: (R = ?0.86, p < 0.001) for CDS
and (R = ?0.93, p < 0.001) for ADS, showing
that segmentation ambiguity has a strong effect
even on supervised segmentation scores. The cor-
relation within language was also significant but
only in the Japanese data : R = ?0.70 for CDS
and R = ?0.62 for ADS.
?
Next, we explore one possible reason for this
structural difference between Japanese and En-
glish, especially at the level of the lexicon.
4.3 Syllable structure and lexical
composition of Japanese and English
One of the most salient differences between En-
glish and Japanese phonology concerns their syl-
lable structure. This is illustrated in Figure 2
(above), where we plotted the frequency of the dif-
ferent syllabic structures of monosyllabic tokens
in English and Japanese CDS. The statistics show
that English has a very rich syllabic composition
where a diversity of consonant clusters is allowed,
whereas Japanese syllable structure is quite simple
and mostly composed of the default CV type. This
difference is bound to have an effect on the struc-
ture of the lexicon. Indeed, Japanese has to use
5
Figure 2 : Trade-off between the complexity of syllable structure (above) and the word token length in terms of syllables
(below) for English and Japanese CDS.
multisyllabic words in order to achieve a large size
lexicon, whereas, in principle, English could use
mostly monosyllables. In Figure 2 (below) we dis-
play the distribution of word length as measured
in syllables in the two languages for the CDS cor-
pora. The English data is indeed mostly composed
of mono-syllabic words whereas the Japanese one
is made of words of more varied lengths. Overall,
we have documented a trade-off between the di-
versity of syllable structure on the one hand, and
the diversity of word lengths on the other (see Ta-
ble 4 for a summary of this tradeoff expressed in
terms of entropy).
? CDS ADS
? Eng. Jap. Eng. Jap.
Syllable types 2.40 1.38 2.58 1.03
Token lengths 0.62 2.04 0.99 1.69
Table 4 : Entropies of syllable types and token lengths in
terms of syllables (in bits)
We suggest that this trade-off is responsible for
the difference in the lexicon ambiguity across the
two languages. Specifically, the combination of
a small number of syllable types and, as a conse-
quence, the tendency for multi-syllabic word types
in Japanese makes it likely that a long word will
be composed of smaller ones. This cannot happen
very often in English, since most words are mono-
syllabic, and words smaller than a syllable are not
allowed.
5 Improving Japanese unsupervised
segmentation
We showed in the previous section that ambigu-
ity impacts segmentation even with a gold lexicon,
mainly because the lexicon itself could be ambigu-
ous. In an unsupervised segmentation setting, the
problem is worse because ambiguity within and
across word boundaries leads to a bad lexicon,
which in turn results in more segmentation errors.
In this section, we explore the possibility of miti-
gating some of these negative consequences.
In section 3, we saw that when the Unigram
model tries to learn Japanese words, it produces an
output lexicon composed of both over- and under-
segmented words in addition to words that re-
sult from a segmentation across word boundaries.
One way to address this is by learning multiple
kinds of units jointly, rather than just words; in-
deed, previous work has shown that richer mod-
els with multiple levels improve segmentation for
English (Johnson, 2008a; Johnson and Goldwater,
2009).
5.1 Two dependency levels
As a first step, we will allow the model to not
just learn words but to also memorize sequences of
words. Johnson (2008a) introduced these units as
?collocations? but we choose to use the more neu-
tral notion of level for reasons that become clear
shortly. Concretely, the grammar is:
6
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.81 0.77 0.86 0.42 0.33 0.55 0.70 0.63 0.78 0.42 0.35 0.50
? Boundaries 0.91 0.84 0.98 0.63 0.47 0.96 0.86 0.76 0.98 0.73 0.61 0.90
? Lexicon 0.64 0.79 0.54 0.18 0.55 0.10 0.36 0.56 0.26 0.15 0.68 0.08
Level 2
? Segmentation 0.33 0.45 0.26 0.59 0.65 0.53 0.50 0.60 0.43 0.45 0.54 0.38
? Boundaries 0.56 0.98 0.40 0.71 0.87 0.60 0.76 0.95 0.64 0.73 0.92 0.60
? Lexicon 0.36 0.25 0.59 0.47 0.44 0.49 0.46 0.38 0.56 0.43 0.37 0.50
Table 5 : Word segmentation scores of the two levels model
?
Utterance? level2+
level2? level1+
level1? Phoneme+
?
We run this model under the same conditions
as the Unigram model but evaluate two different
situations. The model has no inductive bias that
would force it to equate level1 with words, rather
than level2. Consequently, we evaluate the seg-
mentation that is the result of taking there to be a
boundary between every level1 constituent (Level
1 in Table 5) and between every level2 constituent
(Level 2 in Table 5 ). From these results , we see
that English data has better scores when the lower
level represents the Word unit and when the higher
level captures regularities above the word. How-
ever, Japanese data is best segmented when the
higher level is the Word unit and the lower level
captures sub-word regularities.
Level 1 generally tends to over-segment utter-
ances as can be seen by comparing the Boundary
Recall and Precision scores (Goldwater, 2007). In
fact when the Recall is much higher than the Pre-
cision, we can say that the model has a tendency
to over-segment. Conversely, we see that Level 2
tends to under-segment utterances as the Bound-
ary Precision is higher than the Recall.
Over-segmentation at Level 1 seems to benefit
English since it counteracts the tendency of the
Unigram model to cluster high frequency colloca-
tions. As far as segmentation is concerned, this
effect seems to outweigh the negative effect of
breaking words apart (especially in CDS), as En-
glish words are mostly monosyllabic.
For Japanese, under-segmentation at Level 2
seems to be slightly less harmful than over-
segmentation at Level 1, as it prevents, to some
extent, multi-syllabic words to be split. However,
the scores are not very different from the ones we
had with the Unigram model and slightly worse
for the ADS. What seems to be missing is an inter-
mediate level where over- and under-segmentation
would counteract one another.
5.2 Three dependency levels
We add a third dependency level to our model as
follows :
?
Utterance? level3+
level3? level2+
level2? level1+
level1? Phoneme+
?
As with the previous model, we test each of the
three levels as the word unit, the results are shown
in Table 6.
Except for English CDS, all the corpora
have their best scores with this intermediate
level. Level 1 tends to over-segment Japanese
utterances into syllables and English utterances
into morphemes. Level 3, however, tends to
highly under-segment both languages. English
CDS seems to be already under-segmented at
Level 2, very likely caused by the large number
of word collocations like ?is-it? and ?what-is?,
an observation also made by Bo?rschinger et al
(2012) using different English CDS corpora.
English ADS is quantitatively more sensitive to
over-segmentation than CDS mainly because it
has a richer morphological structure and relatively
longer words in terms of syllables (Table 4).
7
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.79 0.74 0.85 0.27 0.20 0.41 0.35 0.28 0.48 0.37 0.30 0.47
? Boundaries 0.89 0.81 0.99 0.56 0.39 0.99 0.68 0.52 0.99 0.70 0.57 0.93
? Lexicon 0.58 0.76 0.46 0.10 0.47 0.05 0.13 0.39 0.07 0.10 0.70 0.05
Level 2
? Segmentation 0.49 0.60 0.42 0.70 0.70 0.70 0.77 0.76 0.79 0.60 0.65 0.55
? Boundaries 0.71 0.97 0.56 0.81 0.82 0.81 0.90 0.88 0.92 0.81 0.90 0.74
? Lexicon 0.51 0.41 0.64 0.53 0.59 0.47 0.58 0.69 0.50 0.51 0.57 0.46
Level 3
? Segmentation 0.18 0.31 0.12 0.39 0.53 0.30 0.43 0.55 0.36 0.28 0.42 0.21
? Boundaries 0.26 0.99 0.15 0.46 0.93 0.31 0.71 0.98 0.55 0.59 0.96 0.43
? Lexicon 0.17 0.10 0.38 0.32 0.25 0.41 0.37 0.28 0.51 0.27 0.20 0.42
Table 6 : Word segmentation scores of the three levels model
6 Conclusion
In this paper we identified a property of lan-
guage, segmentation ambiguity, which we quan-
tified through Normalized Segmentation Entropy.
We showed that this quantity predicts performance
in a supervised segmentation task.
With this tool we found that English was in-
trinsically less ambiguous than Japanese, account-
ing for the systematic difference found in this pa-
per. More generally, we suspect that Segmentation
Ambiguity would, to some extent, explain much
of the difference observed across languages (Ta-
ble 1). Further work needs to be carried out to test
the robustness of this hypothesis on a larger scale.
We showed that allowing the system to learn
at multiple levels of structure generally improves
performance, and compensates partially for the
negative effect of segmentation ambiguity on un-
supervised segmentation (where a bad lexicon am-
plifies the effect of segmentation ambiguity). Yet,
we end up with a situation where the best level of
structure may not be the same across corpora or
languages, which raises the question as to how to
determine which level is the correct lexical level,
i.e., the level that can sustain successful grammat-
ical and semantic learning. Further research is
needed to answer this question.
Generally speaking, ambiguity is a challenge in
many speech and language processing tasks: for
example part-of-speech tagging and word sense
disambiguation tackle lexical ambiguity, proba-
bilistic parsing deals with syntactic ambiguity and
speech act interpretation deals with pragmatic am-
biguities. However, to our knowledge, ambiguity
has rarely been considered as a serious problem in
word segmentation tasks.
As we have shown, the lexicon-based approach
does not completely solve the segmentation am-
biguity problem since the lexicon itself could be
more or less ambiguous depending on the lan-
guage. Evidently, however, infants in all lan-
guages manage to overcome this ambiguity. It has
to be the case, therefore, that they solve this prob-
lem through the use of alternative strategies, for
instance by relying on sub-lexical cues (see Jarosz
and Johnson (2013)) or by incorporating semantic
or syntactic constraints (Johnson et al, 2010). It
remains a major challenge to integrate these strate-
gies within a common model that can learn with
comparable performance across typologically dis-
tinct languages.
Acknowledgements
The research leading to these results has received funding
from the European Research Council (FP/2007-2013) / ERC
Grant Agreement n. ERC-2011-AdG-295810 BOOTPHON,
from the Agence Nationale pour la Recherche (ANR-2010-
BLAN-1901-1 BOOTLANG, ANR-11-0001-02 PSL* and
ANR-10-LABX-0087) and the Fondation de France. This
research was also supported under the Australian Research
Council?s Discovery Projects funding scheme (project num-
bers DP110102506 and DP110102593).
8
References
Eleanor Olds Batchelder. 2002. Bootstrapping the lex-
icon: A computational model of infant speech seg-
mentation. Cognition, 83(2):167?206.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
Daniel Blanchard, Jeffrey Heinz, and Roberta
Golinkoff. 2010. Modeling the contribution of
phonotactic cues to the problem of word segmenta-
tion. Journal of Child Language, 37(3):487?511.
Benjamin Bo?rschinger, Katherine Demuth, and Mark
Johnson. 2012. Studying the effect of input size
for Bayesian word segmentation on the Providence
corpus. In Proceedings of the 24th International
Conference on Computational Linguistics (Coling
2012), pages 325?340, Mumbai, India. Coling 2012
Organizing Committee.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
the 2nd Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 1?9, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
M. Brent and T. Cartwright. 1996. Distributional regu-
larity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93?125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Morten H Christiansen, Joseph Allen, and Mark S Sei-
denberg. 1998. Learning to segment speech using
multiple cues: A connectionist model. Language
and cognitive processes, 13(2-3):221?268.
Robert Daland and Janet B Pierrehumbert. 2011.
Learning diphone-based segmentation. Cognitive
Science, 35(1):119?155.
Margaret M. Fleck. 2008. Lexicalized phonotac-
tic word segmentation. In Proceedings of ACL-08:
HLT, pages 130?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Sharon Goldwater. 2007. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Naomi Hamasaki. 2002. The timing shift of two-year-
olds responses to caretakers yes/no questions. In
Studies in language sciences (2)Papers from the 2nd
Annual Conference of the Japanese Society for Lan-
guage Sciences, pages 193?206.
Gaja Jarosz and J Alex Johnson. 2013. The richness
of distributional cues to word boundaries in speech
to young children. Language Learning and Devel-
opment, (ahead-of-print):1?36.
Mark Johnson and Katherine Demuth. 2010. Unsuper-
vised phonemic Chinese word segmentation using
Adaptor Grammars. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 528?536, Beijing, China,
August. Coling 2010 Organizing Committee.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Elizabeth K. Johnson and Peter W. Jusczyk. 2001.
Word segmentation by 8-month-olds: When speech
cues count more than statistics. Journal of Memory
and Language, 44:1?20.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York. Associ-
ation for Computational Linguistics.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, ed-
itors, Advances in Neural Information Processing
Systems 23, pages 1018?1026.
Mark Johnson. 2008a. Unsupervised word segmen-
tation for Sesotho using Adaptor Grammars. In
Proceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology and
Phonology, pages 20?27, Columbus, Ohio, June.
Association for Computational Linguistics.
Mark Johnson. 2008b. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Peter W Jusczyk and Richard N Aslin. 1995. Infants
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1?23.
Peter W. Jusczyk, E. A. Hohne, and A. Bauman.
1999. Infants? sensitivity to allophonic cues for
word segmentation. Perception and Psychophysics,
61:1465?1476.
9
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Transcription, format and
programs, volume 1. Lawrence Erlbaum.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In proc. LREC, volume 2, pages 947?952.
Sven L Mattys, Peter W Jusczyk, Paul A Luce, James L
Morgan, et al 1999. Phonotactic and prosodic ef-
fects on word segmentation in infants. Cognitive
psychology, 38(4):465?494.
Sven L Mattys, Peter W Jusczyk, et al 2001. Do
infants segment words or recurring contiguous pat-
terns? Journal of experimental psychology, human
perception and performance, 27(3):644?655.
Bruna Pelucchi, Jessica F Hay, and Jenny R Saffran.
2009. Learning in reverse: Eight-month-old infants
track backward transitional probabilities. Cognition,
113(2):244?247.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
J. Saffran, R. Aslin, and E. Newport. 1996. Sta-
tistical learning by 8-month-old infants. Science,
274:1926?1928.
Claude Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal,
27(3):379?423.
Daniel Swingley. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50:86?132.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational
Linguistics, 27(3):351?372.
Daniel J Weiss, Chip Gerfen, and Aaron D Mitchel.
2010. Colliding cues in word segmentation: the
role of cue strength and general cognitive processes.
Language and Cognitive Processes, 25(3):402?422.
Aris Xanthos. 2004. Combining utterance-boundary
and predictability approaches to speech segmenta-
tion. In First Workshop on Psycho-computational
Models of Human Language Acquisition, page 93.
10
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191?200,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
A Rudimentary Lexicon and Semantics Help Bootstrap Phoneme
Acquisition
Abdellah Fourtassi ???????????? Emmanuel Dupoux
Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com
Abstract
Infants spontaneously discover the rele-
vant phonemes of their language without
any direct supervision. This acquisition
is puzzling because it seems to require
the availability of high levels of linguistic
structures (lexicon, semantics), that logi-
cally suppose the infants having a set of
phonemes already. We show how this cir-
cularity can be broken by testing, in real-
size language corpora, a scenario whereby
infants would learn approximate represen-
tations at all levels, and then refine them in
a mutually constraining way. We start with
corpora of spontaneous speech that have
been encoded in a varying number of de-
tailed context-dependent allophones. We
derive, in an unsupervised way, an approx-
imate lexicon and a rudimentary seman-
tic representation. Despite the fact that
all these representations are poor approxi-
mations of the ground truth, they help re-
organize the fine grained categories into
phoneme-like categories with a high de-
gree of accuracy.
One of the most fascinating facts about human
infants is the speed at which they acquire their
native language. During the first year alone, i.e.,
before they are able to speak, infants achieve im-
pressive landmarks regarding three key language
components. First, they tune in on the phone-
mic categories of their language (Werker and Tees,
1984). Second, they learn to segment the continu-
ous speech stream into discrete units (Jusczyk and
Aslin, 1995). Third, they start to recognize fre-
quent words (Ngon et al., 2013), as well as the
semantics of many of them (Bergelson and Swing-
ley, 2012).
Even though these landmarks have been doc-
umented in detail over the past 40 years of re-
search, little is still known about the mechanisms
that are operative in infant?s brain to achieve such
a result. Current work in early language acquisi-
tion has proposed two competing but incomplete
hypotheses that purports to account for this stun-
ning development path. The bottom-up hypothesis
holds that infants converge onto the linguistic units
of their language through a statistical analysis over
of their input. In contrast, the top-down hypothesis
emphasizes the role of higher levels of linguistic
structure in learning the lower level units.
1 A chicken-and-egg problem
1.1 Bottom-up is not enough
Several studies have documented the fact that in-
fants become attuned to the native sounds of their
language, starting at 6 months of age (see Ger-
vain & Mehler, 2010 for a review). Some re-
searchers have claimed that such an early attune-
ment is due to a statistical learning mechanism that
only takes into account the distributional prop-
erties of the sounds present in the native input
(Maye et al., 2002). Unsupervised clustering al-
gorithms running on simplified input have, indeed,
provided a proof of principle for bottom-up learn-
ing of phonemic categories from speech (see for
instance Vallabha et al., 2007).
It is clear, however, that distributional learning
cannot account for the entire developmental pat-
tern. In fact, phoneme tokens in real speech ex-
hibit high acoustic variability and result in phone-
mic categories with a high degree of overlap (Hil-
lenbrand et al., 1995). When purely bottom up
clustering algorithms are tested on realistic input,
they ended up in either a too large number of sub-
phonemic units (Varadarajan et al., 2008) or a too
small number of coarse grained categories (Feld-
man et al., 2013a).
191
1.2 The top-down hypothesis
Inspection of the developmental data shows that
infants do not wait to have completed the acqui-
sition of their native phonemes to start to learn
words. In fact, lexical and phonological acquisi-
tion largely overlap. Infant can recognize highly
frequent word forms like their own names, by as
early as 4 months of age (Mandel et al., 1995).
Vice versa, the refinement of phonemic categories
does not stop at 12 months. The sensitivity to pho-
netic contrasts has been reported to continue at 3
years of age (Nittrouer, 1996) and beyond (Hazan
and Barrett, 2000), on par with the development of
the lexicon.
Some researchers have therefore suggested that
there might be a learning synergy which allows in-
fants to base some of their acquisition not only on
bottom up information, but also on statistics over
lexical items or even on the basis of word mean-
ing (Feldman et al., 2013a; Feldman et al., 2013b;
Yeung and Werker, 2009)
These experiments and computational models,
however, have focused on simplified input or/and
used already segmented words. It remains to be
shown whether the said top-down strategies scale
up when real size corpora and more realistic repre-
sentations are used. There are indeed indications
that, in the absence of a proper phonological repre-
sentation, lexical learning becomes very difficult.
For example, word segmentation algorithms that
work on the basis of phoneme-like units tend to
degrade quickly if phonemes are replaced by con-
textual allophones (Boruta et al., 2011) or with the
output of phone recognizers (Jansen et al., 2013;
Ludusan et al., 2014).
In brief, we are facing a chicken-and-egg prob-
lem: lexical and semantic information could help
to learn the phonemes, but phonemes are needed
to acquire lexical information.
1.3 Breaking the circularity: An incremental
discovery procedure
Here, we explore the idea that instead of learning
adult-like hierarchically organized representations
in a sequential fashion (phonemes, words, seman-
tics), infants learn approximate, provisional lin-
guistic representations in parallel. These approxi-
mate representations are subsequently used to im-
prove each other.
More precisely, we make four assumptions.
First, we assume that infants start by paying atten-
tion to fine grained variation in the acoustic input,
thus constructing perceptual phonetic categories
that are not phonemes, but segments encoding fine
grained phonetic details (Werker and Curtin, 2005;
Pierrehumbert, 2003). Second, we assume that
these units enable infants to segment proto-words
from continuous speech and store them in this de-
tailed format. Importantly, this proto-lexicon will
not be adult-like: it will contain badly segmented
word forms, and store several alternant forms for
the same word. Ngon et al. (2013) have shown
that 11 month old infants recognize frequent sound
sequences that do not necessarily map to adult
words. Third, we assume that infants can use this
imperfect lexicon to acquire some semantic repre-
sentation. As shown in Shukla et al. (2011), in-
fants can simultaneously segment words and asso-
ciate them with a visual referent. Fourth, we as-
sume that as their exposure to language develops,
infants reorganize these initial categories along the
relevant dimensions of their native language based
on cues from all these representations.
The aim of this work is to provide a proof of
principle for this general scenario, using real size
corpora in two typologically different languages,
and state-of-the-art learning algorithms.
The paper is organized as follows. We begin
by describing how we generated the input and
how we modeled different levels of representation.
Then, we explain how information from the higher
levels (word forms and semantics) can be used to
refine the learning of the lower level (phonetic cat-
egories). Next, we present the results of our sim-
ulations and discuss the potential implications for
the language learning process.
2 Modeling the representations
Here, we describe how we model different levels
of representation (phonetic categories, lexicon and
semantics) starting from raw speech in English
and Japanese.
2.1 Corpus
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al., 2007), which contains 40 hours
of spontaneous conversations in American En-
glish, and the 40 hours core of the Corpus of Spon-
taneous Japanese (Maekawa et al., 2000), which
contains spontaneous conversations and public
speeches in different fields, ranging from engi-
neering to humanities. Following Boruta (2012),
192
we use an inventory of 25 phonemes for transcrib-
ing Japanese, and for English, we use the set of 45
phonemes in the phonemic transcription of Pitt et
al. (2007).
2.2 Phonetic categories
Here, we describe how we model the percep-
tual phonetic categories infants learn in a first
step before converging on the functional cate-
gories (phonemes). We make the assumption that
these initial categories correspond to fine grained
allophones, i.e., different systematic realizations
of phonemes, depending on context. Allophonic
variation can range from categorical effects due to
phonological rules to gradient effects due to coar-
ticulation, i.e, the phenomenon whereby adjacent
sounds affect the physical realization of a given
phoneme. An example of a rather categorical allo-
phonic rule is given by /r/ devoicing in French:
/r/?
{
[X] / before a voiceless obstruent
[K] elsewhere
Figure 1: Allophonic variation of French /r/
The phoneme /r/ surfaces as voiced ([K]) be-
fore a voiced obstruent like in [kanaK Zon] (?ca-
nard jaune?, yellow duck) and as voiceless ([X])
before a voiceless obstruent as in [kanaX puXpK]
(?canard pourpre?, purple duck). The challenge
facing the leaner is, therefore, to distinguish pairs
of segments that are in an allophonic relationship
([K], [X]) from pairs that are two distinct phonemes
and can carry a meaning difference ([K],[l]).
Previous work has generated allophonic varia-
tion artificially (Martin et al., 2013). Here, we fol-
low Fourtassi et al. (2014b) in using a linguisti-
cally and statistically controlled method, starting
from audio recordings and using a standard Hid-
den Markov Models (HMM) phone recognizer to
generate them, as follows.
We convert the raw speech waveform into suc-
cessive 10ms frames containing a vector of Mel
Frequency Cepstrum Coefficients (MFCC). We
use 12 MFC coefficients (plus the energy) com-
puted over a 25ms window, to which we add the
first and second order derivatives, yielding 39 di-
mensions per frame.
The HMM training starts with one three-state
model per phoneme. Each state is modeled by
a mixture of 17 diagonal Gaussians. After train-
ing, each phoneme model is cloned into context-
dependent triphone models, for each context in
which the phoneme actually occurs (for example,
the phoneme /A/ occurs in the context [d?A?g] as
in the word /dAg/ (?dog?). The triphone models
cloned from the phonemes are then retrained, but,
this time, only on the relevant subset of the data,
corresponding to the given triphone context. Fi-
nally, these detailed models are clustered back into
inventories of various sizes (from 2 to 20 times
the size of the phonemic inventory) and retrained.
Clustering is done state by state using a phonetic
feature-based decision tree, and results in tying
together the HMM states of linguistically simi-
lar triphones so as to maximize the likelihood of
the data. The HMM were built using the HMM
Toolkit (HTK: Young et al., 2006).
2.3 The proto-lexicon
Finding word boundaries in the continuous se-
quence of phones is part of the problem infants
have to solve without direct supervision. We
model this segmentation using a state-of-the-art
unsupervised word segmentation model based on
the Adaptor Grammar framework (Johnson et al.,
2007). The input consists of a phonetic transcrip-
tion of the corpus, with boundaries between words
eliminated (we vary this transcription to corre-
spond to different inventories with different granu-
larity in the allophonic representation as explained
above). The model tries to reconstruct the bound-
aries based on a Pitman-Yor process (Pitman and
Yor, 1997), which uses a language-general sta-
tistical learning process to find a compact rep-
resentation of the input. The algorithm stores
high frequency chunks and re-uses them to parse
novel utterances. We use a grammar which learns
a hierarchy of three levels of chunking and use
the intermediate level to correspond to the lexi-
cal level. This grammar was shown by Fourtassi
et al. (2013) to avoid both over-segmentation and
under-segmentation.
2.4 The proto-semantics
It has been shown that infants can keep track of co-
occurrence statistics (see Lany and Saffran (2013)
for a review). This ability can be used to develop a
sense of semantic similarity as suggested by Har-
ris (1954). The intuition behind the distributional
hypothesis is that words that are similar in mean-
ing occur in similar contexts. In order to model
the acquisition of this semantic similarity from a
193
transcribed and segmented corpus, we use one of
the simplest and most commonly used distribu-
tional semantic models, Latent Semantic Analysis
(LSA: Landauer & Dumais, 1997). The LSA al-
gorithm takes as input a matrix consisting of rows
representing word types and columns represent-
ing contexts in which tokens of the word type oc-
cur. A context is defined as a fixed number of
utterances. Singular value decomposition (a kind
of matrix factorization) is used to extract a more
compact representation. The cosine of the angle
between vectors in the resulting space is used to
measure the semantic similarity between words.
Two words have a high semantic similarity if they
have similar distributions, i.e., if they co-occur in
most contexts. The model parameters, namely the
dimension of the semantic space and the number
of utterances to be taken as defining the context
of a given word form, are set in an unsupervised
way to optimize the latent structure of the seman-
tic model (Fourtassi and Dupoux, 2013). Thus, we
use 20 utterances as a semantic window and set the
semantic space to 100 dimensions.
3 Method
Here we explore whether the approximate high
level representations, built bottom-up and with-
out supervision, still contain useful information
one can use to refine the phonetic categories into
phoneme-like units. To this end, we extract po-
tential cues from the lexical and the semantic in-
formation, and test their performance in discrim-
inating allophonic contrasts from non-allophonic
(phonemic) contrasts.
3.1 Top down cues
3.1.1 Lexical cue
The top down information from the lexicon is
based on the insight of Martin et al. (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes
(figure 1). The latter creates alternants of the same
lexical item since adjacent sounds condition the
realization of the first and final phoneme. There-
fore, finding a minimal pair of words differing in
the first or last segment (as in [kanaX] and [kanaK])
is good evidence that these two phones ([K], [X])
are allophones of one another. Conversely, if a
pair of phones is not forming any minimal pair,
it is classified as non-allophonic (phonemic).
However, this binary strategy clearly gives rise
to false alarms in the (albeit relatively rare) case
of true minimal pairs like [kanaX] (?duck?) and
[kanal] (?canal?), where ([X], [l]) will be mis-
takenly labeled as allophonic. In order to miti-
gate the problem of false alarms, we use Boruta?s
continuous version (Boruta, 2011) and we define
the lexical cue of a pair of phones Lex(x, y) as
the number of lexical minimal pairs that vary on
the first segment (xA, yA) or the last segment
(Ax,Ay). The higher this number, the more the
pair of phones is likely to be considered as allo-
phonic.
The lexical cue is consistent with experimen-
tal findings. For example Feldman et al. (2013b)
showed that 8 month-old infants pay attention
to word level information, and demonstrated that
they do not discriminate between sound contrasts
that occur in minimal pairs (as suggested by our
cue), and, conversely, discriminate contrasts that
occur in non-minimal pairs.
3.1.2 Semantic cue
The semantic cue is based on the intuition that
true minimal pairs ([kanaX] and [kanal]) are asso-
ciated with different events, whereas alternants of
the same word ([kanaX] and [kanal]) are expected
to co-occur with similar events.
We operationalize the semantic cue associated
with a pair of phones Sem(x, y) as the average
semantic similarity between all the lexical mini-
mal pairs generated by this pair of phones. The
higher the average semantic similarity, the more
the learner is prone to classify them as allophonic.
We take as a measure of the semantic similar-
ity, the cosine of the angle between word vec-
tors of the pairs that vary on the final segment
cos(
?
Ax,Ay) or the first segment cos(
?
xA, yA).
This strategy is similar in principle to the phe-
nomenon of acquired distinctiveness, according
to which, pairing two target stimuli with distinct
events enhances their perceptual differentiation,
and acquired equivalence, whereby pairing two
target stimuli with the same event, impairs their
subsequent differentiation (Lawrence, 1949). In
the same vein, Yeung and Werker (2009) tested 9
month-olds english learning infants in a task that
consists in discriminating two non-native phonetic
categories. They found that infants succeeded only
when the categories co-occurred with two distinct
visual cues.
194
? Segmentation Lexicon
? English Japanese English Japanese
Allo./phon. F P R F P R F P R F P R
2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.15
4 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.10
10 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.10
20 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0.16 0.32 0.10
Table 1 : Scores of the segmentation and the resulting lexicon, as a function of the average number of
allophones per phoneme. P=Precison, R=Recall and F=F-score.
3.1.3 Combined cue
Finally, we consider the combination of both cues
in one single cue where the contextual information
(semantics) is used as a weighing scheme of the
lexical information, as follows:
Comb(x, y) =
?
(Ax,Ay)?L
2
cos(
?
Ax,Ay) +
?
(xA,yA)?L
2
cos(
?
xA, yA)
(1)
where {Ax ? L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax,Ay) ?
L
2
} is the set of phonological minimal pairs in
L? L that vary on the final segment.
The lexical cue is incremented by one, for ev-
ery minimal pair. The combined cue is, instead,
incremented by one, times the cosine of the angle
between the word vectors of this pair. When the
words have similar distributions, the angle goes to
zero and the cosine goes to 1, and when the words
have orthogonal distributions, the angle goes to
90
?
and the cosine goes to 0.
The semantic information here would basically
enable us to avoid false alarms generated by poten-
tial true minimal pairs like the above-mentioned
example of ( [kanaX] and [kanal]). Such a pair will
probably score high as far as the lexical cue is con-
cerned, but it will score low on the semantic level.
Thus, by taking the combination, the model will
be less prone to mistakenly classify ([X], [l]) as al-
lophones.
3.2 Task
For each corpus we list all possible pairs of al-
lophones. Some of these pairs are allophones of
the same phoneme (allophonic pair) and others are
allophones of different phonemes (non-allophonic
pairs). The task is a same-different classification,
whereby each of these pairs is given a score from
the cue that is being tested. A good cue gives
higher scores to allophonic pairs.
Only pairs of phones that generate at least one
lexical minimal pair are considered. Phonetic vari-
ation that does not cause lexical variation is ?in-
visible? to top down strategies, and is, therefore,
more probably clustered through purely bottom up
strategies (Fourtassi et al., 2014b)
3.3 Evaluation
We use the same evaluation procedure as Martin et
al. (2013). This is carried out by computing the as-
sociated ROC curve (varying the z-score threshold
and computing the resulting proportions of misses
and false alarms). We then derive the Area Under
the Curve (AUC), which also corresponds to the
probability that given two pairs of phones, one al-
lophonic, one not, they are correctly classified on
the basis of the score. A value of 0.5 represents
chance and a value of 1 represents perfect perfor-
mance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks of 20 utterances each (the semantic win-
dow). In each run, we draw randomly with re-
placement from this set of blocks a sample of
the same size as the original corpus. This sam-
ple is then used to retrain the acoustic models and
generate a phonetic inventory that we used to re-
transcribe the corpus and re-compute the cues. We
report scores averaged over 5 such runs.
4 Results and discussion
4.1 Segmentation
We first explore how phonetic variation influences
the quality of the segmentation and the resulting
lexicon. For the evaluation, we use the same mea-
sures as Brent (1999) and Goldwater et al. (2009),
namely Segmentation Precision (P), Recall (R)
and F-score (F). Segmentation precision is defined
195
as the number of correct word tokens found, out of
all tokens posited. Recall is the number of correct
word tokens found, out of all tokens in the ideal
segmentation. The F-score is defined as the har-
monic mean of Precision and Recall:
F =
2 ? P ?R
P + R
We define similar measures for word types (lex-
icon). Table 1 shows the scores as a function of
the number of allophones per phonemes. For both
corpora, the segmentation performance decreases
as we increase the number of allophones. As for
the lexicon, the recall scores show that only 15
to 22% of the ?words? found by the algorithm in
the English corpus are real words; in Japanese,
this number is even lower (between 10 and 15%).
This pattern can be attributed in part to the fact
that increasing the number of allophones increases
the number of word forms, which occur therefore
with less frequency, making the statistical learn-
ing harder. Table 2 shows the average number of
word forms per word as a function of the average
number of allophones per phoneme, in the case of
ideal segmentation.
Allo./Phon. W. forms/Word
? English Japanese
2 1.56 1.20
4 2.03 1.64
10 2.69 2.11
20 3.47 2.83
Table 2 : Average number of word-forms per
word as a function of the average number of
allophones per phoneme.
Another effect seen in Table 1 is the lower
overall performance of Japanese compared to En-
glish. This difference was shown by Fourtassi et
al. (2013) to be linked to the intrinsic segmenta-
tion ambiguity of Japanese, caused by the fact that
Japanese words contain more syllables compared
to English.
4.2 Allophonic vs phonemic status of sound
contrasts
Here we test the performance of the cues described
above, in discriminating between allophonic con-
trasts from phonemic ones. We vary the number
of allophones per phoneme, on the one hand (Fig-
ure 2a), and the amount of data available to the
learner, on the other hand, in the case of two allo-
phones per phonemes (Figure 2b). In both situa-
tions, we compare the case wherein the lexical and
semantic cues are computed on the output of the
unsupervised segmentation (right), to the control
case where these cues are computed on the ideally
segmented speech (left).
We see that the overall accuracy of the cues is
quite high, even in the case of bad word segmen-
tation and very small amount of data.
The lexical cue is robust to extreme variation
and to the scarcity of data. Indeed, it does not seem
to vary monotonically neither with the number of
allophones, nor with the size of the corpus. The as-
sociated f-score generally remains above the value
of 0.7 (chance level is 0.5). The semantics, on
the other hand, gets better as the variability de-
creases and as the amount of data increases. This
is a natural consequence of the fact that the se-
mantic structure is more accurate with more data
and with word forms consistent enough to sustain
a reasonable co-occurrence statistics.
The comparison with the ideal segmentation,
shows, interestingly, that the semantics is more ro-
bust to segmentation errors than the lexical cue. In
fact, while the lexical strategy performs, overall,
better than the semantics under the ideal segmen-
tation, the patterns reverses as we move to a a more
realistic (unsupervised) segmentation.
These results suggest that both lexical and se-
mantic strategies can be crucial to learning the
phonemic status of phonetic categories since they
provide non-redundant information. This finding
is summarized by the combined cue which resists
to both variation and segmentation errors, overall,
better than each of the cues taken alone.
From a developmental point of view, this shows
that infants can, in principle, benefit from higher
level linguistic structures to refine their phonetic
categories, even if these structures are rudimen-
tary. Previous studies about top down strategies
have mainly emphasized the role of word forms;
the results of this work show that the semantics
can be at least as useful. Note that the notion
of semantics used here is weaker than the clas-
sic notion of referential semantics as in a word-
concept matching. The latter might, indeed, not
be fully operative at the early stages of the child
development, since it requires some advanced con-
ceptual abilities (like forming symbolic represen-
tations and understanding a speaker?s referential
196
a)
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
2 5 10 20 2 5 10 20
Allophones/Phoneme
AU
C
English
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
2 5 10 20 2 5 10 20
Allophones/Phoneme
AU
C
Japanese
Cues
Lexical
Semantic
Combined
b)
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
1 2 4 8 20 40 1 2 4 8 20 40
Size (in hours)
AU
C
English
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
1 2 4 8 20 40 1 2 4 8 20 40
Size (in hours)
AU
C
Japanese
Cues
Lexical
Semantic
Combined
Figure 2: Same-different scores (AUC) for different cues as a function of the average number of allo-
phones per phoneme (a), and as a function of the size of the corpus, in the case of two allophones per
phonemes (b). The scores are shown for both ideal and unsupervised word segmentation in English and
Japanese. The points show the mean scores over 5 runs. The lines are smoothed interpolations (local
regressions) through the means. The grey band shows a 95% confidence interval.
intentions) (Waxman and Gelman, 2009). What
we call the ?semantics? of a word in this study, is
the general context provided by the co-occurrence
with other words. Infants have been shown to have
a powerful mechanism for tracking co-occurrence
relationships both in the speech and the visual do-
main (Lany and Saffran, 2013) . Our experiments
demonstrate that a similar mechanism could be
enough to develop a sense of semantic similarity
that can successfully be used to refine phonetic
categories.
5 General discussion and future work
Phonemes are abstract categories that form the ba-
sis for words in the lexicon. There is a traditional
view that they should be defined by their ability to
contrast word meanings (Trubetzkoy, 1939). Their
full acquisition, therefore, requires lexical and se-
mantic top-down information. However, since the
quality of the semantic representations depends on
the quality of the phonemic representations that
are used to build the lexicon, we face a chicken-
and-egg problem. In this paper, we proposed a
way to break the circularity by building approxi-
mate representation at all the levels.
The infants? initial attunement to language-
specific categories was represented in a way that
mirrors the linguistic and statistical properties of
the speech closely. We showed that this de-
tailed (proto-phonemic) inventory enabled word
segmentation from continuous transcribed speech,
but, as expected, resulted in a low quality lexicon.
The poorly segmented corpus was then used to de-
rive a semantic similarity matrix between pairs of
words, based on their co-occurrence statistics. The
results showed that information from the derived
lexicon and semantics, albeit very rudimentary,
help discriminate between allophonic and phone-
mic contrasts, with a high degree of accuracy.
Thus, this works strongly support the claim that
the lexicon and semantics play a role in the re-
finement of the phonemic inventory (Feldman et
197
al., 2013a; Frank et al., 2014), and, interestingly,
that this role remains functional under more realis-
tic assumptions (unsupervised word segmentation,
and bottom-up inferred semantics). We also found
that lexical and semantic information were not re-
dundant and could be usefully combined, the for-
mer being more resistant to the scarcity of data
and variation, and the latter being more resistant
to segmentation errors.
That being said, this work relies on the assump-
tion that infants start with initial perceptual cate-
gories (allophones), but we did not show how such
categories could be constructed from raw speech.
More work is needed to explore the robustness of
the model when these units are learned in an unsu-
pervised fashion (Lee and Glass, 2012; Huijbregts
et al., 2011; Jansen and Church, 2011; Varadarajan
et al., 2008).
This work could be seen as a proof of princi-
ple for an iterative learning algorithm, whereby
phonemes emerge from the interaction of low level
perceptual categories, word forms, and the seman-
tics (see Werker and Curtin (2005) for a similar
theoretical proposition). The algorithm has yet to
be implemented, but it has to address at least two
major issues: First, the fact that some sound pairs
are not captured by top down cues because they
do not surface as minimal word forms. For in-
stance, in English, /h/ and /N/ occur in different
syllable positions and therefore, cannot appear in
any minimal pair. Second, even if we have enough
information about how phonetic categories are or-
ganized in the perceptual space, we still need to
know how many categories are relevant in a par-
ticular language (i.e., where to stop the categoriza-
tion process).
For the first problem, Fourtassi et al. (2014b)
showed that the gap could, in principle, be filled by
bottom-up information (like acoustic similarity).
As for the second problem, a possible direction
could be found in the notion of Self-Consistency.
In fact, (Fourtassi et al., 2014a) proposed that an
optimal level of clustering is also a level that glob-
ally optimizes the predictive power of the lexicon.
Too detailed allophones result in too many syn-
onyms. Too broad classes result in too many ho-
mophones. Somewhere in the middle, the optimal
number of phonemes optimizes how lexical items
predict each other. Future work will address these
issues in more detail in order to propose a com-
plete phoneme learning algorithm.
Acknowledgments
This work was supported in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R?egion Ile de France (DIM cerveau et pens?ee).
References
Elika Bergelson and Daniel Swingley. 2012. At 6
to 9 months, human infants know the meanings of
many common nouns. Proceedings of the National
Academy of Sciences, 109(9).
Luc Boruta, Sharon Peperkamp, Beno??t Crabb?e, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
CMCL, pages 1?9. Association for Computational
Linguistics.
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88?93.
Luc Boruta. 2012. Indicateurs d?allophonie et
de phon?emicit?e. Doctoral dissertation, Universit
?e
Paris-Diderot - Paris VII.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
N. Feldman, T. Griffiths, S. Goldwater, and J. Morgan.
2013a. A role for the developing lexicon in pho-
netic category acquisition. Psychological Review,
120(4):751?778.
N. Feldman, B. Myers, K. White, T. Griffiths, and
J. Morgan. 2013b. Word-level information influ-
ences phonetic learning in adults and infants. Cog-
nition, 127:427?438.
Abdellah Fourtassi and Emmanuel Dupoux. 2013. A
corpus-based evaluation method for distributional
semantic models. In 51st Annual Meeting of the
Association for Computational Linguistics Proceed-
ings of the Student Research Workshop, pages 165?
171, Sofia, Bulgaria. Association for Computational
Linguistics.
Abdellah Fourtassi, Benjamin B?orschinger, Mark
Johnson, and Emmanuel Dupoux. 2013. WhyisEn-
glishsoeasytosegment? In Proceedings of CMCL,
pages 1?10. Association for Computational Linguis-
tics.
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014a. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th annual meeting of the Cognitive Science
Society.
198
Abdellah Fourtassi, Thomas Schatz, Balakrishnan
Varadarajan, and Emmanuel Dupoux. 2014b. Ex-
ploring the Relative Role of Bottom-up and Top-
down Information in Phoneme Learning. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics.
Stella Frank, Naomi Feldman, and Sharon Goldwater.
2014. Weak semantic context helps phonetic learn-
ing in a model of infant language acquisition. In
Proceedings of the 52nd Annual Meeting of the As-
sociation of Computational Linguistics.
Judit Gervain and Jacques Mehler. 2010. Speech per-
ception and language acquisition in the first year of
life. Annual Review of Psychology, 61:191?218.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Valerie Hazan and Sarah Barrett. 2000. The develop-
ment of phonemic categorization in children aged 6
to12. Journal of Phonetics, 28:377?396.
James Hillenbrand, Laura A. Getty, Michael J. Clark,
and Kimberlee Wheeler. 1995. Acoustic charac-
teristics of american english vowels. Journal of the
Acoustical Society of America, 97:3099?3109.
M. Huijbregts, M. McLaren, and D. van Leeuwen.
2011. Unsupervised acoustic sub-word unit detec-
tion for query-by-example spoken term detection. In
Proceedings of ICASSP, pages 4436?4439.
A. Jansen and K. Church. 2011. Towards unsupervised
training of speaker independent acoustic models. In
Proceedings of INTERSPEECH, pages 1693?1696.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwa-
ter, Mark Johnson, Sanjeev Khudanpur, Kenneth
Church, Naomi Feldman, Hynek Hermansky, Flo-
rian Metze, Richard Rose, Mike Seltzer, Pascal
Clark, Ian McGraw, Balakrishnan Varadarajan, Erin
Bennett, Benjamin Borschinger, Justin Chiu, Ewan
Dunbar, Abdallah Fourtassi, David Harwath, Chia
ying Lee, Keith Levin, Atta Norouzian, Vijay
Peddinti, Rachel Richardson, Thomas Schatz, and
Samuel Thomas. 2013. A summary of the 2012 jhu
clsp workshop on zero resource speech technologies
and models of early language acquisition. In Pro-
ceedings of ICASSP.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Peter W Jusczyk and Richard N Aslin. 1995. Infants?
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1?23.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
J. Lany and J. Saffran. 2013. Statistical learning mech-
anisms in infancy. In J. Rubenstein and P. Rakic, ed-
itors, Comprehensive Developmental Neuroscience:
Neural Circuit Development and Function in the
Brain, volume 3, pages 231?248. Elsevier, Amster-
dam.
D.H. Lawrence. 1949. Acquired distinctiveness of
cues: I. transfer between discriminations on the ba-
sis of familiarity with the stimulus. Journal of Ex-
perimental Psychology, 39(6):770?784.
C. Lee and J. Glass. 2012. A nonparametric bayesian
approach to acoustic model discovery. In Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1, pages 40?49.
Bogdan Ludusan, Maarten Versteegh, Aren Jansen,
Guillaume Gravier, Xuan-Nga Cao, Mark Johnson,
and Emmanuel Dupoux. 2014. Bridging the gap be-
tween speech technology and natural language pro-
cessing: an evaluation toolbox for term discovery
systems. In Proceedings of LREC.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947?952, Athens, Greece.
D.R. Mandel, P.W. Jusczyk, and D.B. Pisoni. 1995. In-
fants? recognition of the sound patterns of their own
names. Psychological Science, 6(5):314?317.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103?124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101?B111.
C. Ngon, A. Martin, E. Dupoux, D. Cabrol, M. Duthat,
and S. Peperkamp. 2013. (non)words, (non)words,
(non)words: evidence for a protolexicon during the
first year of life. Developmental Science, 16(1):24?
34.
S. Nittrouer. 1996. Discriminability and perceptual
weighting of some acoustic cues to speech percep-
tion by 3-year-olds. Journal of Speech and Hearing
Research, 39:278?297.
J. B. Pierrehumbert. 2003. Phonetic diversity, statis-
tical learning, and acquisition of phonology. Lan-
guage and Speech, 46(2-3):115?154.
199
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
M Shukla, K White, and R Aslin. 2011. Prosody
guides the rapid mapping of auditory word forms
onto visual objects in 6-mo-old infants. Proceedings
of the National Academy of Sciences, 108(15):6038?
6043.
N. S. Trubetzkoy. 1939. Grundz?uge der Phonolo-
gie (Principles of phonology). Vandenhoeck &
Ruprecht, G?ottingen, Germany.
G. K. Vallabha, J. L. McClelland, F. Pons, J. F.
Werker, and S. Amano. 2007. Unsupervised learn-
ing of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and
Emmanuel Dupoux. 2008. Unsupervised learning
of acoustic sub-word units. In Proceedings of ACL-
08: HLT, Short Papers, pages 165?168. Association
for Computational Linguistics.
Sandra R. Waxman and Susan A. Gelman. 2009. Early
word-learning entails reference, not merely associa-
tions. Trends in Cognitive Sciences, 13(6):258?263.
J. F. Werker and S. Curtin. 2005. PRIMIR: A develop-
mental framework of infant speech processing. Lan-
guage Learning and Development, 1(2):197?234.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 ? 63.
H Yeung and J Werker. 2009. Learning words? sounds
before learning how words sound: 9-month-olds use
distinct objects as cues to categorize speech infor-
mation. Cognition, 113:234?243.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
200
