Automated Induction of Sense in Context
James PUSTEJOVSKY, Patrick HANKS, Anna RUMSHISKY
Brandeis University
Waltham, MA 02454, USA
{jamesp,patrick,arum}@cs.brandeis.edu
Abstract
In this paper, we introduce a model for sense as-
signment which relies on assigning senses to the
contexts within which words appear, rather than to
the words themselves. We argue that word senses
as such are not directly encoded in the lexicon
of the language. Rather, each word is associated
with one or more stereotypical syntagmatic pat-
terns, which we call selection contexts. Each selec-
tion context is associated with a meaning, which
can be expressed in any of various formal or com-
putational manifestations. We present a formalism
for encoding contexts that help to determine the
semantic contribution of a word in an utterance.
Further, we develop a methodology through which
such stereotypical contexts for words and phrases
can be identified from very large corpora, and sub-
sequently structured in a selection context dictio-
nary, encoding both stereotypical syntactic and se-
mantic information. We present some preliminary
results.
1 Introduction
This paper describes a new model for the acquisi-
tion and exploitation of selectional preferences for
predicates from natural language corpora. Our goal
is to apply this model in order to construct a dic-
tionary of normal selection contexts for natural lan-
guage; that is, a computational lexical database of
rich selectional contexts, associated with procedures
for assigning interpretations on a probabilistic basis
to less normal contexts. Such a semi-automatically
developed resource promises to have applications for
a number of NLP tasks, including word-sense disam-
biguation, selectional preference acquisition, as well
as anaphora resolution and inference in specialized
domains. We apply this methodology to a selected
set of verbs, including a subset of the verbs in the
Senseval 3 word sense discrimination task and report
our initial results.
1.1 Selectional Preference Acquisition:
Current State of the Art
Predicate subcategorization information constitutes
an essential part of the computational lexicon entry.
In recent years, a number of approaches have been
proposed for dealing computationally with selec-
tional preference acquisition (Resnik (1996); Briscoe
and Carroll (1997); McCarthy (1997); Rooth et al
(1999); Abney and Light (1999); Ciaramita and
Johnson (2000); Korhonen (2002)).
The currently available best algorithms developed
for the acquisition of selectional preferences for pred-
icates are induction algorithms modeling selectional
behavior as a distribution over words (cf. Abney and
Light (1999)). Semantic classes assigned to predi-
cate arguments in subcategorization frames are ei-
ther derived automatically through statistical clus-
tering techniques (Rooth et al (1999), Light and
Greiff (2002)) or assigned using hand-constructed
lexical taxonomies such as the WordNet hierarchy or
LDOCE semantic classes. Overwhelmingly, Word-
Net is chosen as the default resource for dealing with
the sparse data problem (Resnik (1996); Abney and
Light (1999); Ciaramita and Johnson (2000); Agirre
and Martinez (2001); Clark and Weir (2001); Carroll
and McCarthy (2000); Korhonen and Preiss (2003)).
Much of the work on inducing selectional prefer-
ences for verbs from corpora deals with predicates in-
discriminately, assuming no differentiation between
predicate senses (Resnik (1996); Abney and Light
(1999); Ciaramita and Johnson (2000); Rooth et al
(1999)). Those approaches that do distinguish be-
tween predicate senses or complementation patterns
in acquisition of selectional constraints (Korhonen
(2002); Korhonen and Preiss (2003)) do not use cor-
pus analysis for verb sense classification.
1.2 Word Sense Disambiguation: Current
State of the Art
Previous computational concerns for economy of
grammatical representation have given way to mod-
els of language that not only exploit generative
grammatical resources but also have access to large
lists of contexts of linguistic items (words), to which
new structures can be compared in new usages.
However, following the work of Yarowsky (1992),
Yarowsky (1995), many supervised WSD systems
use minimal information about syntactic structures,
for the most part restricting the notion of con-
text to topical and local features. Topical features
track open-class words that appear within a cer-
tain window around a target word, and local fea-
tures track small N-grams associated with the tar-
get word. Disambiguation therefore relies on word
co-occurrence statistics, rather than on structural
similarities. That remains the case for most systems
that participated in Senseval-2 (Preiss and Yarowsky
(2001)). Some recent work (Stetina et al (1998);
Agirre et al (2002); Yamashita et al (2003)) at-
tempts to change this situation and presents a di-
rected effort to investigate the impact of using syn-
tactic features for WSD learning algorithms. Agirre
et al(2002) and Yamashita et al (2003) report re-
sulting improvement in precision.
Stevenson and Wilks (2001) propose a somewhat
related technique to handle WSD, based on inte-
grating LDOCE classes with simulated annealing.
Although space does not permit discussion here, ini-
tial comparisons suggest that our selection contexts
could incorporate similar knowledge resources; it is
not clear what role model bias plays in associating
patterns with senses, however.
In this paper we modify the notion of word sense,
and at the same time revise the manner in which
senses are encoded. The notion of word sense that
has been generally adopted in the literature is an
artifact of several factors in the status quo, notably
the availability of lexical resources such as machine-
readable dictionaries, in which fine sense distinctions
are not supported by criteria for selecting one sense
rather than another, and WordNet, where synset
groupings are taken as defining word sense distinc-
tions. Thus, for instance, Senseval-2 WSD tasks re-
quired disambiguation using WordNet senses (see,
e.g., discussion in Palmer et al (2004)). The feature
sets used in the supervised WSD algorithms at best
use only minimal information about the typing of ar-
guments. The approach we adopt, Corpus Pattern
Analysis (CPA) (Pustejovsky and Hanks (2001)),
incorporates semantic features of the arguments of
the target word. Semantic features are expressed in
terms of a restricted set of shallow types, chosen for
their prevalence in selection context patterns. This
type system is extended with predicate-based noun
clustering, in the bootstrapping process described
below.
1.3 Related Resources: FrameNet
It is necessary to say a few words about the dif-
ferences between CPA and FrameNet. The CPA
approach has its origins in the analysis of large
corpora for lexicographic purposes (e.g. Cobuild
(Sinclair et al, 1987)) and in systemic-functional
grammar, in particular in Halliday?s notion of ?lexis
as a linguistic level? (Halliday, 1966) and Sin-
clair?s empirical approach to collocational anal-
ysis (Sinclair, 1991). FrameNet (freely avail-
able online in a beautifully designed data base at
http://www.icsi.berkeley.edu/?framenet/), is an attempt to
implement Fillmore?s 1975 proposal that, instead of
seeking to satisfy a set of necessary and sufficient
conditions, the meanings of words in text should be
analyzed by calculating resemblance to a prototype
(Fillmore, 1975).
CPA (Hanks, 2004) is concerned with establishing
prototypical norms of usage for individual words. It
is possible (and certainly desirable) that CPA norms
will be mappable onto FrameNet?s semantic frames
(for which see the whole issue of the International
Journal of Lexicography for September 2003 (in par-
ticular Atkins et al (2003a), Atkins et al (2003b),
Fillmore et al (2003a), Baker et al (2003), Fillmore
et al (2003b)). In frame semantics, the relationship
between semantics and syntactic realization is often
at a comparatively deep level, i.e. in many sentences
there are elements that are potentially present but
not actually expressed. For example, in the sentence
?he risked his life?, two semantic roles are expressed
(the risker and the valued object ?his life? that is put
at risk). But at least three other roles are sublim-
inally present although not expressed: the possible
bad outcome (?he risked his death?), the beneficiary
or goal (?he risked his life for her/for a few dollars?),
and the means (?he risked a backward glance?).
CPA, on the other hand, is shallower and more
practical: the objective is to identify, in relation to
a given target word, the overt textual clues that
activate one or more components of its meaning
potential. There is also a methodological differ-
ence: whereas FrameNet research proceeds frame by
frame, CPA proceeds word by word. This means
that when a word has been analysed in CPA the
patterns are immediately available for disambigua-
tion. FrameNet will be usable for disambiguation
only when all frames have been completely analysed.
Even then, FrameNet?s methodology, which requires
the researchers to think up all possible members of
a Frame a priori, means that important senses of
words that have been partly analysed are missing
and may continue to be missing for years to come.
There is no attempt in FrameNet to identify the
senses of each word systematically and contrastively.
In its present form, at least, FrameNet has at least
as many gaps as senses. For example, at the time
of writing toast is shown as part of the Apply Heat
frame but not the Celebrate frame. It is not clear
how or whether the gaps are to be filled systemat-
ically. We do not even know whether there is (or
is going to be) a Celebrate frame and if so what it
will be called. What is needed is a principled fix ? a
decision to proceed from evidence, not frames. This
is ruled out by FrameNet for principled reasons: the
unit of analysis for FrameNet is the frame, not the
word.
2 CPA Methodology
The Corpus Pattern Analysis (CPA) technique uses
a semi-automatic bootstrapping process to produce
a dictionary of selection contexts for predicates
in a language. Word senses for verbs are distin-
guished through corpus-derived syntagmatic pat-
terns mapped to Generative Lexicon Theory (Puste-
jovsky (1995)) as a linguistic model of interpreta-
tion, which guides and constrains the induction of
senses from word distributional information. Each
pattern is specified in terms of lexical sets for each
argument, shallow semantic typing of these sets, and
other syntagmatically relevant criteria (e.g., adver-
bials of manner, phrasal particles, genitives, nega-
tives).
The procedure consists of three subtasks: (1) the
manual discovery of selection context patterns for
specific verbs; (2) the automatic recognition of in-
stances of the identified patterns; and (3) automatic
acquisition of patterns for unanalyzed cases. Ini-
tially, a number of patterns are manually formulated
by a lexicographer through corpus pattern analysis
of about 500 occurrences of each verb lemma. Next,
for higher frequency verbs, the remaining corpus oc-
currences are scrutinized to see if any low-frequency
patterns have been missed. The patterns are then
translated into a feature matrix used for identifying
the sense of unseen instances for a particular verb.
In the remainder of this section, we describe these
subtasks in more detail. The following sections ex-
plain the current status of the implementation of
these tasks.
2.1 Lexical Discovery
Norms of usage are captured in what we call selec-
tion context patterns. For each lemma, contexts
of usage are sorted into groups, and a stereotypi-
cal CPA pattern that captures the relevant seman-
tic and syntactic features of the group is recorded.
Many patterns have alternations, recorded in satel-
lite CPA patterns. Alternations are linked to
the main CPA pattern through the same sense-
modifying mechanisms as those that allow for ex-
ploitations (coercions) of the norms of usage to be
understood. For example, here is the set of pat-
terns for the verb treat. Note that these patterns
do not capture all possible uses, and other patterns
may be added, e.g. if additional evidence is found
in domain-specific corpora.
(1) CPA Pattern set for treat:
I. [[Person 1]] treat [[Person 2]] ({at | in} [[Location]])
(for [[Event = Injury | Ailment]]); NO [Adv[Manner]]
II. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
IIIa. [[Person]] treat [[TopType 1]] {{as | like} [[TopType 2]]}
IIIb. [[Person]] treat [[TopType]] {{as if | as though | like}
[CLAUSE]}
IV. [[Person 1]] treat [[Person 2]] {to [[Event]]}
V. [[Person]] treat [[PhysObj | Stuff 1]] (with [[Stuff 2]])
There may be several patterns realizing a single
sense of a verb, as in (IIIa/IIIb) above. Also, there
may be several equivalent alternations or there may
be a stereotype. Note that alternations are different
realizations of the same norm, not exploitations (i.e.,
not coercions).
(2) Alternations for treat Pattern 1 :
[[Person 1]] treat [[Person 2]] ({at | in} [[Hospital]])
(for [[Injury | Ailment]]); NO [Adv[Manner]]
Alternation 1:
[[Person 1 <--> Medicament | Med-Procedure | Institution]]
Alternation 2:
[[Person 2 <--> Injury | Ailment | Bodypart]]
CPA Patterns
A CPA pattern extends the traditional notion of se-
lectional context to include a number of other con-
textual features, such as minor category parsing and
subphrasal cues. Accurate identification of the se-
mantically relevant aspects of a pattern is not an
obvious and straightforward procedure, as has some-
times been assumed in the literature. For example,
the presence or absence of an adverbial of manner in
the third valency slot around a verb can dramatically
alter the verb?s meaning. Simple syntactic encoding
of argument structure, for instance, is insufficient to
discriminate between the two major senses of the
verb treat, as illustrated below.
(3) a. They say their bosses treat them with respect.
b. Such patients are treated with antibiotics.
The ability to recognize the shallow semantic type
of a phrase in the context of a predicate is of course
crucial ?for example, in (3a) recognizing the PP as
(a) an adverbial, and (b) an adverbial of manner,
rather than an instrumental co-agent (as in (3b)),
is crucial for assigning the correct sense to the verb
treat above.
In the CPA model, automatic identification of
selection contexts not only captures the argument
structure of a predicate, but also more delicate fea-
tures, which may have a profound effect on the
semantic interpretation of a predicate in context.
There are four constraint sets that contribute to the
patterns for encoding selection contexts. These are:
(4) a. Shallow Syntactic Parsing: Phrase-level recogni-
tion of major categories.
b. Shallow Semantic Typing: 50-100 primitive shal-
low types, such as Person, Institution, Event, Abstract, Ar-
tifact, Location, and so forth. These are the top types se-
lected from the Brandeis Shallow Ontology (BSO), and are
similar to entities (and some relations) employed in Named
Entity Recognition tasks, such as TREC and ACE.
c. Minor Syntactic Category Parsing: e.g., loca-
tives, purpose clauses, rationale clauses, temporal adjuncts.
d. Subphrasal Syntactic Cue Recognition: e.g.,
genitives, partitives, bare plural/determiner distinctions,
infinitivals, negatives.
The notion of a selection context pattern, as pro-
duced by a human annotator, is expressed as a BNF
specification in Table 1.1 This specification relies
on word order to specify argument position, and is
easily translated to a template with slots allocated
for each argument. Within this grammar, a seman-
tic roles can be specified for each argument, but this
information currently is not used in the automated
processing.
English contains only about 8,000 verbs, of which
we estimate that about 30% have only one basic pat-
tern. The rest are evenly split between verbs hav-
ing 2-3 patterns and verbs having more than 4 or
1Round brackets indicate optional elements of the pattern,
and curly brackets indicate syntactic constituents.
CPA-Pattern ? Segment verb-lit Segment | verb-lit Segment | Segment verb-lit | CPA-Pattern ?;? Element
Segment ? Element | Segment Segment | ?? Segment ?? | ?(? Segment ?)? | Segment ?|? Segment
Element ? literal | ?[? Rstr ArgType ?]? | ?[? Rstr literal ?]? | ?[? Rstr ?]? | ?[? NO Cue ?]? | ?[? Cue ?]?
Rstr ? POS | Phrasal | Rstr ?|? Rstr | epsilon
Cue ? POS | Phrasal | AdvCue
AdvCue ? ADV ?[? AdvType ?]?
AdvType ? Manner | Dir | Location
Phrasal ? OBJ | CLAUSE | VP | QUOTE
POS ? ADJ | ADV | DET | POSDET | COREF POSDET | REFL-PRON | NEG |
MASS | PLURAL | V | INF | PREP | V-ING | CARD | QUANT | CONJ
ArgType ? ?[? SType ?]? | ?[? SType ?=? SubtypeSpec ?]? | ArgType ?|? ArgType | ?[? SType ArgIdx ?]? |
?[? SType ArgIdx ?=? SubtypeSpec ?]?
SType ? AdvType | TopType | Entity | Abstract | PhysObj | Institution | Asset | Location | Human | Animate |
Human Group | Substance | Unit of Measurement | Quality | Event | State of Affairs | Process
SubtypeSpec ? SubtypeSpec ?|? SubtypeSpec | SubtypeSpec ?&? SubtypeSpec | Role | Polarity | LSet
Role ? Role | Role ?|? Role | Benficiary | Meronym | Agent | Payer
Polarity ? Negative | Positive
LSet ? Worker | Pilot | Musician | Competitor | Hospital | Injury | Ailment | Medicament | Medical Procedure |
Hour-Measure | Bargain | Clothing | BodyPart | Text | Sewage | Part | Computer | Animal
ArgIdx ? <number> verb-lit ? <verb-word-form>
literal ? word word ? <word>
CARD ? <number> NEG ? not
POSDET ? my | your | ... INF ? to
QUANT ? CARD | a lot | longer | more | many | ...
Table 1: Pattern grammar
more patterns. About 20 light verbs have between
100 and 200 patterns each. This is less alarming
than it sounds, because the majority of light verb
patterns involve selection of just one specific nom-
inal head, e.g., take account, take plunge, take
photograph, with few if any alternations. The pat-
tern sets for verbs of different frequency groups differ
in terms of the number and type of features each pat-
tern requires, the number of patterns in a set for a
given verbs, the number of alternations for each pat-
tern, and the type of selectional preferences affecting
the verb?s arguments.
Brandeis Shallow Ontology
The Brandeis Shallow Ontology (BSO) is a shallow
hierarchy of types selected for their prevalence in
manually identified selection context patterns. At
the time of writing, there are just 65 types, in terms
of which patterns for the first one hundred verbs
have been analyzed. New types are added occasion-
ally, but only when all possibilities of using existing
types prove inadequate. Once the set of manually
extracted patterns is sufficient, the type system will
be re-populated and become pattern-driven.
The BSO type system allows multiple inheri-
tance (e.g. Document v PhysObj and Document v
Information. The types currently comprising the
ontology are listed above. The BSO contains type
assignments for 20,000 noun entries and 10,000 nom-
inal collocation entries.
Corpus-driven Type System
The acquisition strategy for selectional preferences
for predicates proceeds as follows:
(5) a. Partition the corpus occurrences of a
predicate according to the selection contexts
pattern grammar, distinguished by the four
levels of constraints mentioned in (4). These
are uninterpreted patterns for the predicate.
b. Within a given pattern, promote the statisti-
cally significant literal types from the corpus for
each argument to the predicate. This induces
an interpretation of the pattern, treating the
promoted literal type as the specific binding of
a shallow type from step (a) above.
c. Within a given pattern, coerce all lexical
heads in the same shallow type for an argu-
ment, into the promoted literal type, assigned
in (b) above. This is a coercion of a lexical
head to the interpretation of the promoted
literal type induced from step (b) above.
In a sense, (5a) can be seen as a broad multi-
level partitioning of the selectional behavior for a
predicate according to a richer set of syntactic and
semantic discriminants. Step (5b) can be seen as
capturing the norms of usage in the corpus, while
step (5c) is a way of modeling the exploitation
of these norms in the language (through coercion,
metonymy, and other generative operations). To
illustrate the way in which CPA discriminates un-
interpreted patterns from the corpus, we return to
the verb treat as it is used in the BNC. Although
there are three basic senses for this verb, the two
major senses, as illustrated in (1) above, emerge as
correlated with two distinct context patterns, us-
ing the discriminant constraints mentioned in (4)
above. For the full specification for this verb, see
www.cs.brandeis.edu/~arum/cpa/treat.html.
(6) a. [[Person 1]] treat [[Person 2]]; NO [Adv[Manner]]
b. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
Given a distinct (contextual) basis on which to an-
alyze the actual statistical distribution of the words
in each argument position, we can promote statisti-
cally relevant and significant literal types for these
positions. For example, for pattern (a) above, this
induces Doctor as Person 1, and Patient as bound
to Person 2. This produces the interpreted context
pattern for this sense as shown below.
(7) [[doctor]] treat [[patient]]
Promoted literal types are corpus-derived and
predicate-dependent, and are syntactic heads of
phrases that occur with the greatest frequency in ar-
gument positions for a given sense pattern; they are
subsequently assumed to be subtypes of the particu-
lar shallow type in the pattern. Step (5c) above then
enables us to bind the other lexical heads in these po-
sitions as coerced forms of the promoted literal type.
This can be seen below in the concordance sample,
where therapies is interpreted as Doctor, and people
and girl are interpreted as Patient.
(8) a. returned with a doctor who treated the girl till an am-
bulance arrived.
b. more than 90,000 people have been treated for cholera
since the epidemic began
c. nonsurgical therapies to treat the breast cancer, which
may involve
Model Bias
The assumption within GL is that semantic types
in the grammar map systematically to default syn-
tactic templates (cf. Pustejovsky (1995)). These
are termed canonical syntactic forms (CSFs). For
example, the CSF for the type proposition is a
tensed S. There are, however, many possible real-
izations (such as infinitival S and NP) for this type
due to the different possibilities available from gen-
erative devices in a grammar, such as coercion and
co-composition. The resulting set of syntactic forms
associated with a particular semantic type is called
a phrasal paradigm for that type. The model bias
provided by GL acts to guide the interpretation of
purely statistically based measures (cf. Pustejovsky
(2000)).
2.2 Automatic Recognition of Pattern Use
Essentially, this subtask is similar to the traditional
supervised WSD problem. Its purpose is (1) to test
the discriminatory power of CPA-derived feature-
set, (2) to extend and refine the inventory of features
captured by the CPA patterns, and (3) to allow for
predicate-based argument groupings by classifying
unseen instances. Extension and refinement of the
inventory of features should involve feature induc-
tion, but at the moment this part has not been im-
plemented. During the lexical discovery stage, lex-
ical sets that fill some of the argument slots in the
patterns are instantiated from the training exam-
ples. As more predicate-based lexical sets within
shallow types are explored, the data will permit
identification of the types of features that unite ele-
ments in lexical sets.
2.3 Automatic Pattern Acquisition
The algorithm for automatic pattern acquisition in-
volves the following steps:
(9) a. Collect all constituents in a particular argu-
ment position;
b. Identify syntactic alternations;
c. Perform clustering on all nouns that occur in
a particular argument position of a given pred-
icate;
d. For each cluster, measure its relatedness
to the known lexical sets, obtained previously
during the lexical discovery stage and extended
through WSD of unseen instances. If none of
the existing lexical sets pass the distance thresh-
old, establish the cluster as a new lexical set, to
be used in future pattern specification.
Step (9d) must include extensive filtering proce-
dures to check for shared semantic features, look-
ing for commonality between the members. That
is, there must be some threshold overlap between
subgroups of the candidate lexical set and and the
existing semantic classes. For instance, checking if,
for a certain percentage of pairs in the candidate set,
there already exists a set of which both elements are
members.
3 Current Implementation
The CPA patterns are developed using the British
National Corpus (BNC). The sorted instances are
used as a training set for the supervised disambigua-
tion. For the disambiguation task, each pattern is
translated into into a set of preprocessing-specific
features.
The BNC is preprocessed using the Robust Accu-
rate Statistical Parsing system (RASP) and seman-
tically tagged with BSO types. The RASP system
(Briscoe and Carroll (2002)) tokenizes, POS-tags,
and lemmatizes text, generating a forest of full parse
trees for each sentence and associating a probability
with each parse. For each parse, RASP produces a
set of grammatical relations, specifying the relation
type, the headword, and the dependent element. All
our computations are performed over the single top-
ranked tree for the sentences where a full parse was
successfully obtained. Some of the grammatical re-
lations identified by RASP are shown in (10).
(10) subjects: ncsubj, clausal (csubj, xsubj)
objects: dobj, iobj, clausal complement
modifiers: adverbs, modifiers of event nominals
We use endocentric semantic typing, i.e., the head-
word of each constituent is used to establish its se-
mantic type. The semantic tagging strategy is simi-
lar to the one described in Pustejovsky et al (2002).
Currently, a subset of 24 BSO types is used for se-
mantic tagging.
A CPA pattern is translated into a feature set,
which in the current implementation uses binary fea-
tures. It is further complemented with other dis-
criminant context features which, rather than dis-
tinguishing a particular pattern, are merely likely to
occur with a given subset of patterns; that is, the fea-
tures that only partially determine or co-determine
a sense. In the future, these should be learned from
the training set through feature induction from the
training sample, but at the moment, they are added
manually. The resulting feature matrix for each pat-
tern contains features such as those in (11) below.
Each pattern is translated into a template of 15-25
features.
(11) Selected context features:
a. obj institution: object belongs to the BSO type ?Insti-
tution?
b. subj human group: subject belongs to the BSO type ?Hu-
manGroup?
c. mod adv ly: target verb has an adverbial modifier, with a
-ly adverb
d. clausal like: target verb has a clausal argument intro-
duced by ?like?
e. iobj with: target verb has an indirect object by ?with?
f. obj PRP: direct object is a personal pronoun
g. stem VVG: the target verb stem is an -ing form
Each feature may be realized by a number of RASP
relations. For instance, a feature dealing with
objects would take into account RASP relations
?dobj?, ?obj2?, and ?ncsubj? (for passives). The
features such as (11a)-(11e) are typically taken di-
rectly from the pattern specification, while features
such as in (11f) and (11g) would typically be added
as co-determining the pattern.
4 Results and Discussion
The experimental trials performed to date are too
preliminary to validate the methodology outlined
above in general terms for the WSD task. Our re-
sults are encouraging however, and comparable to
the best performing systems reported from Senseval
2. For our experiments, we implemented two ma-
chine learning algorithms, instance-based k-Nearest
Neighbor, and a decision tree algorithm (a version
of ID3). For these experiments, kNN was run with
the full training set. Table 2 shows the results on a
subset of verbs that have been processed, also listing
the number of patterns in the pattern set for each of
the verbs.2
verb number of training accuracy
patterns set ID3 kNN
edit 2 100 87% 86%
treat 4 200 45% 52%
submit 4 100 59% 64%
Table 2: Accuracy of pattern identification
Further experimentation is obviously needed to
adequately gauge the effectiveness of the selection
context approach for WSD and other NLP tasks.
It is already clear, however, that the traditional
sense enumeration approach, where senses are asso-
ciated with individual lexical items, must give way
to a model where senses are assigned to the contexts
within which words appear. Furthermore, because
the variability of the stereotypical syntagmatic pat-
terns that are associated with words appears to be
relatively small, such information can be encoded as
2Test set size for each lemma is 100 instances, selected out
of several randomly chosen segments of BNC, non-overlapping
with the training set
lexically-indexed contexts. A comprehensive dictio-
nary of such contexts could prove to be a powerful
tool for a variety of NLP tasks.
References
S. Abney and M. Light. 1999. Hiding a semantic hierarchy in a
markov model.
E. Agirre and D. Martinez. 2001. Learning class-to-class se-
lectional preferences. In Walter Daelemans and Re?mi Zajac,
editors, Proceedings of CoNLL-2001, pages 15?22. Toulouse,
France.
E. Agirre, D. Martinez, and L. Marquez. 2002. Syntactic features
for high precision word sense disambiguation. COLING 2002.
S. Atkins, C. Fillmore, and C. Johnson. 2003a. Lexicographic
relevance: Selecting information from corpus evidence. Inter-
national Journal of Lexicography, 16(3):251?280, September.
S. Atkins, M. Rundell, and H. Sato. 2003b. The contribution of
Framenet to practical lexicography. International Journal of
Lexicography, 16(3):333?357, September.
C. Baker, C. Fillmore, and B. Cronin. 2003. The structure of the
Framenet database. International Journal of Lexicography,
16(3):281?296, September.
T. Briscoe and J. Carroll. 1997. Automatic extraction of sub-
categorization from corpora. Proceedings of the 5th ANLP
Conference, Washington DC, pages 356?363.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical anno-
tation of general text. Proceedings of the Third International
Conference on Language Resources and Evaluation (LREC
2002), Las Palmas, Canary Islands, May 2002, pages 1499?
1504.
J. Carroll and D. McCarthy. 2000. Word sense disambiguation
using automatically acquired verbal preferences.
M. Ciaramita and M. Johnson. 2000. Explaining away ambiguity:
Learning verb selectional preference with Bayesian networks.
COLING 2000.
S. Clark and D. Weir. 2001. Class-based probability estimation
using a semantic hierarchy. Proceedings of the 2nd Conference
of the North American Chapter of the ACL. Pittsburgh, PA.
C. Fillmore, C. Johnson, and M. Petruck. 2003a. Background to
Framenet. International Journal of Lexicography, 16(3):235?
250, September.
C. Fillmore, M. Petruck, J. Ruppenhofer, and A. Wright. 2003b.
Framenet in action: The case of attaching. International
Journal of Lexicography, 16(3):297?332, September.
C. Fillmore. 1975. Santa Cruz Lectures on Deixis. Indiana Uni-
versity Linguistics Club. Bloomington, IN.
M. A. K. Halliday. 1966. Lexis as a linguistic level. In C. E.
Bazell, J. C. Catford, M. A. K. Halliday, and R. H. Robins,
editors, In Memory of J. R. Firth. Longman.
P. Hanks. 2004. The syntagmatics of metaphor (forthcoming).
International Journal of Lexicography, 17(3), September.
forthcoming.
A. Korhonen and J. Preiss. 2003. Improving subcategorization
acquisition using word sense disambiguation. Proceedings of
the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo, Japan.
A. Korhonen. 2002. Subcategorization Acquisition. PhD thesis
published as Techical Report UCAM-CL-TR-530. Computer
Laboratory, University of Cambridge.
M. Light and W. Greiff. 2002. Statistical models for the induction
and use of selectional preferences. Cognitive Science, Volume
26(3), pp. 269- 281.
D. McCarthy. 1997. Word sense disambiguation for acquisition of
selectional preferences. In Piek Vossen, Geert Adriaens, Nico-
letta Calzolari, Antonio Sanfilippo, and Yorick Wilks, editors,
Automatic Information Extraction and Building of Lexical
Semantic Resources for NLP Applications, pages 52?60. As-
sociation for Computational Linguistics, New Brunswick, New
Jersey.
M. Palmer, H. T. Dang, and C. Fellbaum. 2004. Making fine-
grained and coarse-grained sense distinctions, both manually
and automatically. Natural Language Engineering. Preprint.
J. Preiss and D. Yarowsky, editors. 2001. Proceedings of the Sec-
ond Int. Workshop on Evaluating WSD Systems (Senseval
2). ACL2002/EACL2001.
J. Pustejovsky and P. Hanks. 2001. Very Large Lexical
Databases: A tutorial. ACL Workshop, Toulouse, France.
J. Pustejovsky, A. Rumshisky, and J. Castano. 2002. Rerendering
Semantic Ontologies: Automatic Extensions to UMLS through
Corpus Analytics. In LREC 2002 Workshop on Ontologies
and Lexical Knowledge Bases. Las Palmas, Canary Islands,
Spain.
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.):
MIT Press.
J. Pustejovsky. 2000. Lexical shadowing and argument closure.
In Y. Ravin and C. Leacock, editors, Lexical Semantics. Ox-
ford University Press.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cognition,
61:127?159.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999.
Inducing a semantically annotated lexicon via EM?based clus-
tering. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL?99), Mary-
land.
J. Sinclair, P. Hanks, and et al 1987. The Collins Cobuild En-
glish Language Dictionary. HarperCollins, 4th (2003) edition.
Published as Collins Cobuild Advanced Learner?s English Dic-
tionary.
J. M. Sinclair. 1991. Corpus, Concordance, Collocation. Oxford
University Press.
J. Stetina, S. Kurohashi, and M. Nagao. 1998. General word
sense disambiguation method based on A full sentential con-
text. In Sanda Harabagiu, editor, Use of WordNet in Natural
Language Processing Systems: Proceedings of the Confer-
ence, pages 1?8. Association for Computational Linguistics,
Somerset, New Jersey.
M. Stevenson and Y. Wilks. 2001. The interaction of knowledge
sources in word sense disambiguation. Computational Lin-
guistics, 27(3), September.
K. Yamashita, K. Yoshida, and Y. Itoh. 2003. Word sense dis-
ambiguation using pairwise alignment. ACL2003.
D. Yarowsky. 1992. Word-sense disambiguation using statistical
models of Roget?s categories trained on large corpora. Proc.
COLING92, Nantes, France.
D. Yarowsky. 1995. Unsupervised word sense disambiguation ri-
valing supervised methods. In Meeting of the Association for
Computational Linguistics, pages 189?196.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 81?84, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automating Temporal Annotation with TARSQI
Marc Verhagen?, Inderjeet Mani?, Roser Sauri?,
Robert Knippen?, Seok Bae Jang?, Jessica Littman?,
Anna Rumshisky?, John Phillips?, James Pustejovsky?
? Department of Computer Science, Brandeis University, Waltham, MA 02254, USA
{marc,roser,knippen,jlittman,arum,jamesp}@cs.brandeis.edu
? Computational Linguistics, Georgetown University, Washington DC, USA
{im5,sbj3,jbp24}@georgetown.edu
Abstract
We present an overview of TARSQI, a
modular system for automatic temporal
annotation that adds time expressions,
events and temporal relations to news
texts.
1 Introduction
The TARSQI Project (Temporal Awareness and
Reasoning Systems for Question Interpretation)
aims to enhance natural language question an-
swering systems so that temporally-based questions
about the events and entities in news articles can be
addressed appropriately. In order to answer those
questions we need to know the temporal ordering of
events in a text. Ideally, we would have a total order-
ing of all events in a text. That is, we want an event
like marched in ethnic Albanians marched Sunday
in downtown Istanbul to be not only temporally re-
lated to the nearby time expression Sunday but also
ordered with respect to all other events in the text.
We use TimeML (Pustejovsky et al, 2003; Saur?? et
al., 2004) as an annotation language for temporal
markup. TimeML marks time expressions with the
TIMEX3 tag, events with the EVENT tag, and tempo-
ral links with the TLINK tag. In addition, syntactic
subordination of events, which often has temporal
implications, can be annotated with the SLINK tag.
A complete manual TimeML annotation is not
feasible due to the complexity of the task and the
sheer amount of news text that awaits processing.
The TARSQI system can be used stand-alone
or as a means to alleviate the tasks of human
annotators. Parts of it have been intergrated in
Tango, a graphical annotation environment for event
ordering (Verhagen and Knippen, Forthcoming).
The system is set up as a cascade of modules
that successively add more and more TimeML
annotation to a document. The input is assumed to
be part-of-speech tagged and chunked. The overall
system architecture is laid out in the diagram below.
Input Documents
GUTime
Evita
SlinketGUTenLINK
SputLink
TimeML Documents
In the following sections we describe the five
TARSQI modules that add TimeML markup to news
texts.
2 GUTime
The GUTime tagger, developed at Georgetown Uni-
versity, extends the capabilities of the TempEx tag-
ger (Mani and Wilson, 2000). TempEx, developed
81
at MITRE, is aimed at the ACE TIMEX2 standard
(timex2.mitre.org) for recognizing the extents and
normalized values of time expressions. TempEx
handles both absolute times (e.g., June 2, 2003) and
relative times (e.g., Thursday) by means of a num-
ber of tests on the local context. Lexical triggers like
today, yesterday, and tomorrow, when used in a spe-
cific sense, as well as words which indicate a posi-
tional offset, like next month, last year, this coming
Thursday are resolved based on computing direc-
tion and magnitude with respect to a reference time,
which is usually the document publication time.
GUTime extends TempEx to handle time ex-
pressions based on the TimeML TIMEX3 standard
(timeml.org), which allows a functional style of en-
coding offsets in time expressions. For example, last
week could be represented not only by the time value
but also by an expression that could be evaluated to
compute the value, namely, that it is the week pre-
ceding the week of the document date. GUTime also
handles a variety of ACE TIMEX2 expressions not
covered by TempEx, including durations, a variety
of temporal modifiers, and European date formats.
GUTime has been benchmarked on training data
from the Time Expression Recognition and Normal-
ization task (timex2.mitre.org/tern.html) at .85, .78,
and .82 F-measure for timex2, text, and val fields
respectively.
3 EVITA
Evita (Events in Text Analyzer) is an event recogni-
tion tool that performs two main tasks: robust event
identification and analysis of grammatical features,
such as tense and aspect. Event identification is
based on the notion of event as defined in TimeML.
Different strategies are used for identifying events
within the categories of verb, noun, and adjective.
Event identification of verbs is based on a lexi-
cal look-up, accompanied by a minimal contextual
parsing, in order to exclude weak stative predicates
such as be or have. Identifying events expressed by
nouns, on the other hand, involves a disambigua-
tion phase in addition to lexical lookup. Machine
learning techniques are used to determine when an
ambiguous noun is used with an event sense. Fi-
nally, identifying adjectival events takes the conser-
vative approach of tagging as events only those ad-
jectives that have been lexically pre-selected from
TimeBank1, whenever they appear as the head of a
predicative complement. For each element identi-
fied as denoting an event, a set of linguistic rules
is applied in order to obtain its temporally relevant
grammatical features, like tense and aspect. Evita
relies on preprocessed input with part-of-speech tags
and chunks. Current performance of Evita against
TimeBank is .75 precision, .87 recall, and .80 F-
measure. The low precision is mostly due to Evita?s
over-generation of generic events, which were not
annotated in TimeBank.
4 GUTenLINK
Georgetown?s GUTenLINK TLINK tagger uses
hand-developed syntactic and lexical rules. It han-
dles three different cases at present: (i) the event
is anchored without a signal to a time expression
within the same clause, (ii) the event is anchored
without a signal to the document date speech time
frame (as in the case of reporting verbs in news,
which are often at or offset slightly from the speech
time), and (iii) the event in a main clause is anchored
with a signal or tense/aspect cue to the event in the
main clause of the previous sentence. In case (iii), a
finite state transducer is used to infer the likely tem-
poral relation between the events based on TimeML
tense and aspect features of each event. For ex-
ample, a past tense non-stative verb followed by a
past perfect non-stative verb, with grammatical as-
pect maintained, suggests that the second event pre-
cedes the first.
GUTenLINK uses default rules for ordering
events; its handling of successive past tense non-
stative verbs in case (iii) will not correctly or-
der sequences like Max fell. John pushed him.
GUTenLINK is intended as one component in a
larger machine-learning based framework for order-
ing events. Another component which will be de-
veloped will leverage document-level inference, as
in the machine learning approach of (Mani et al,
2003), which required annotation of a reference time
(Reichenbach, 1947; Kamp and Reyle, 1993) for the
event in each finite clause.
1TimeBank is a 200-document news corpus manually anno-
tated with TimeML tags. It contains about 8000 events, 2100
time expressions, 5700 TLINKs and 2600 SLINKs. See (Day
et al, 2003) and www.timeml.org for more details.
82
An early version of GUTenLINK was scored at
.75 precision on 10 documents. More formal Pre-
cision and Recall scoring is underway, but it com-
pares favorably with an earlier approach developed
at Georgetown. That approach converted event-
event TLINKs from TimeBank 1.0 into feature vec-
tors where the TLINK relation type was used as the
class label (some classes were collapsed). A C5.0
decision rule learner trained on that data obtained an
accuracy of .54 F-measure, with the low score being
due mainly to data sparseness.
5 Slinket
Slinket (SLINK Events in Text) is an application
currently being developed. Its purpose is to automat-
ically introduce SLINKs, which in TimeML specify
subordinating relations between pairs of events, and
classify them into factive, counterfactive, evidential,
negative evidential, and modal, based on the modal
force of the subordinating event. Slinket requires
chunked input with events.
SLINKs are introduced by a well-delimited sub-
group of verbal and nominal predicates (such as re-
gret, say, promise and attempt), and in most cases
clearly signaled by the context of subordination.
Slinket thus relies on a combination of lexical and
syntactic knowledge. Lexical information is used to
pre-select events that may introduce SLINKs. Pred-
icate classes are taken from (Kiparsky and Kiparsky,
1970; Karttunen, 1971; Hooper, 1975) and subse-
quent elaborations of that work, as well as induced
from the TimeBank corpus. A syntactic module
is applied in order to properly identify the subor-
dinated event, if any. This module is built as a
cascade of shallow syntactic tasks such as clause
boundary recognition and subject and object tag-
ging. Such tasks are informed from both linguistic-
based knowledge (Papageorgiou, 1997; Leffa, 1998)
and corpora-induced rules (Sang and De?je?an, 2001);
they are currently being implemented as sequences
of finite-state transducers along the lines of (A??t-
Mokhtar and Chanod, 1997). Evaluation results are
not yet available.
6 SputLink
SputLink is a temporal closure component that takes
known temporal relations in a text and derives new
implied relations from them, in effect making ex-
plicit what was implicit. A temporal closure compo-
nent helps to find those global links that are not nec-
essarily derived by other means. SputLink is based
on James Allen?s interval algebra (1983) and was in-
spired by (Setzer, 2001) and (Katz and Arosio, 2001)
who both added a closure component to an annota-
tion environment.
Allen reduces all events and time expressions to
intervals and identifies 13 basic relations between
the intervals. The temporal information in a doc-
ument is represented as a graph where events and
time expressions form the nodes and temporal re-
lations label the edges. The SputLink algorithm,
like Allen?s, is basically a constraint propagation al-
gorithm that uses a transitivity table to model the
compositional behavior of all pairs of relations. For
example, if A precedes B and B precedes C, then
we can compose the two relations and infer that A
precedes C. Allen allowed unlimited disjunctions of
temporal relations on the edges and he acknowl-
edged that inconsistency detection is not tractable
in his algebra. One of SputLink?s aims is to ensure
consistency, therefore it uses a restricted version of
Allen?s algebra proposed by (Vilain et al, 1990). In-
consistency detection is tractable in this restricted al-
gebra.
A SputLink evaluation on TimeBank showed that
SputLink more than quadrupled the amount of tem-
poral links in TimeBank, from 4200 to 17500.
Moreover, closure adds non-local links that were
systematically missed by the human annotators. Ex-
perimentation also showed that temporal closure al-
lows one to structure the annotation task in such
a way that it becomes possible to create a com-
plete annotation from local temporal links only. See
(Verhagen, 2004) for more details.
7 Conclusion and Future Work
The TARSQI system generates temporal informa-
tion in news texts. The five modules presented here
are held together by the TimeML annotation lan-
guage and add time expressions (GUTime), events
(Evita), subordination relations between events
(Slinket), local temporal relations between times and
events (GUTenLINK), and global temporal relations
between times and events (SputLink).
83
In the nearby future, we will experiment with
more strategies to extract temporal relations from
texts. One avenue is to exploit temporal regularities
in SLINKs, in effect using the output of Slinket as
a means to derive even more TLINKs. We are also
compiling more annotated data in order to provide
more training data for machine learning approaches
to TLINK extraction. SputLink currently uses only
qualitative temporal infomation, it will be extended
to use quantitative information, allowing it to reason
over durations.
References
Salah A??t-Mokhtar and Jean-Pierre Chanod. 1997. Sub-
ject and Object Dependency Extraction Using Finite-
State Transducers. In Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications. ACL/EACL-97 Workshop Proceed-
ings, pages 71?77, Madrid, Spain. Association for
Computational Linguistics.
James Allen. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
David Day, Lisa Ferro, Robert Gaizauskas, Patrick
Hanks, Marcia Lazo, James Pustejovsky, Roser Saur??,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics.
Joan Hooper. 1975. On Assertive Predicates. In John
Kimball, editor, Syntax and Semantics, volume IV,
pages 91?124. Academic Press, New York.
Hans Kamp and Uwe Reyle, 1993. From Discourse to
Logic, chapter 5, Tense and Aspect, pages 483?546.
Kluwer Academic Publishers, Dordrecht, Netherlands.
Lauri Karttunen. 1971. Some Observations on Factivity.
In Papers in Linguistics, volume 4, pages 55?69.
Graham Katz and Fabrizio Arosio. 2001. The Anno-
tation of Temporal Information in Natural Language
Sentences. In Proceedings of ACL-EACL 2001, Work-
shop for Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France. Association for
Computational Linguistics.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
Manfred Bierwisch and Karl Erich Heidolph, editors,
Progress in Linguistics. A collection of Papers, pages
143?173. Mouton, Paris.
Vilson Leffa. 1998. Clause Processing in Complex Sen-
tences. In Proceedings of the First International Con-
ference on Language Resources and Evaluation, vol-
ume 1, pages 937?943, Granada, Spain. ELRA.
Inderjeet Mani and George Wilson. 2000. Processing
of News. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL2000), pages 69?76.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
Short Paper. In Proceedings of the Human Language
Technology Conference (HLT-NAACL?03).
Harris Papageorgiou. 1997. Clause Recognition in the
Framework of Allignment. In Ruslan Mitkov and
Nicolas Nicolov, editors, Recent Advances in Natural
Language Recognition. John Benjamins, Amsterdam,
The Netherlands.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In IWCS-5 Fifth
International Workshop on Computational Semantics.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
MacMillan, London.
Tjong Kim Sang and Erik Herve De?je?an. 2001. Introduc-
tion to the CoNLL-2001 Shared Task: Clause Identifi-
cation. In Proceedings of the Fifth Workshop on Com-
putational Language Learning (CoNLL-2001), pages
53?57, Toulouse, France. ACL.
Roser Saur??, Jessica Littman, Robert Knippen, Robert
Gaizauskas, Andrea Setzer, and James Puste-
jovsky. 2004. TimeML Annotation Guidelines.
http://www.timeml.org.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Marc Verhagen and Robert Knippen. Forthcoming.
TANGO: A Graphical Annotation Environment for
Ordering Relations. In James Pustejovsky and Robert
Gaizauskas, editors, Time and Event Recognition in
Natural Language. John Benjamin Publications.
Marc Verhagen. 2004. Times Between The Lines. Ph.D.
thesis, Brandeis University, Waltham, Massachusetts,
USA.
Marc Vilain, Henry Kautz, and Peter van Beek. 1990.
Constraint propagation algorithms: A revised report.
In D. S. Weld and J. de Kleer, editors, Qualitative Rea-
soning about Physical Systems, pages 373?381. Mor-
gan Kaufman, San Mateo, California.
84
Automated Induction of Sense in Context
James PUSTEJOVSKY, Patrick HANKS, Anna RUMSHISKY
Brandeis University
Waltham, MA 02454, USA
{jamesp,patrick,arum}@cs.brandeis.edu
1 Introduction
In this work, we introduce a model for sense assign-
ment which relies on assigning senses to the con-
texts within which words appear, rather than to the
words themselves. We argue that word senses as
such are not directly encoded in the lexicon of the
language. Rather, each word is associated with one
or more stereotypical syntagmatic patterns, which
we call selection contexts. Each selection context is
associated with a meaning, which can be expressed
in any of various formal or computational manifesta-
tions. We present a formalism for encoding contexts
that help to determine the semantic contribution of a
word in an utterance. Further, we develop a method-
ology through which such stereotypical contexts for
words and phrases can be identified from very large
corpora, and subsequently structured in a selection
context dictionary, encoding both stereotypical syn-
tactic and semantic information. We present some
preliminary results.
2 CPA Methodology
The Corpus Pattern Analysis (CPA) technique uses
a semi-automatic bootstrapping process to produce
a dictionary of selection contexts for predicates
in a language. Word senses for verbs are distin-
guished through corpus-derived syntagmatic pat-
terns mapped to Generative Lexicon Theory (Puste-
jovsky (1995)) as a linguistic model of interpreta-
tion, which guides and constrains the induction of
senses from word distributional information. Each
pattern is specified in terms of lexical sets for each
argument, shallow semantic typing of these sets, and
other syntagmatically relevant criteria (e.g., adver-
bials of manner, phrasal particles, genitives, nega-
tives).
The procedure consists of three subtasks: (1) the
manual discovery of selection context patterns for
specific verbs; (2) the automatic recognition of in-
stances of the identified patterns; and (3) automatic
acquisition of patterns for unanalyzed cases. Ini-
tially, a number of patterns are manually formulated
by a lexicographer through corpus pattern analysis
of about 500 occurrences of each verb lemma. Next,
for higher frequency verbs, the remaining corpus oc-
currences are scrutinized to see if any low-frequency
patterns have been missed. The patterns are then
translated into a feature matrix used for identifying
the sense of unseen instances for a particular verb.
In the remainder of this section, we describe these
subtasks in more detail. The following sections ex-
plain the current status of the implementation of
these tasks.
2.1 Lexical Discovery
Norms of usage are captured in what we call selec-
tion context patterns. For each lemma, contexts
of usage are sorted into groups, and a stereotypi-
cal CPA pattern that captures the relevant semantic
and syntactic features of the group is recorded. For
example, here is the set of common patterns for the
verb treat.
(1) CPA pattern set for treat:
I. [[Person 1]] treat [[Person 2]] ({at | in} [[Location]])
(for [[Event = Injury | Ailment]]); NO [Adv[Manner]]
II. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
IIIa. [[Person]] treat [[TopType 1]] {{as | like} [[TopType 2]]}
IIIb. [[Person]] treat [[TopType]] {{as if | as though | like}
[CLAUSE]}
IV. [[Person 1]] treat [[Person 2]] {to [[Event]]}
V. [[Person]] treat [[PhysObj | Stuff 1]] (with [[Stuff 2]])
There may be several patterns realizing a single
sense of a verb, as in (IIIa/IIIb) above. Addi-
tionally, many patterns have alternations, recorded
in satellite CPA patterns. Alternations are linked
to the main CPA pattern through the same sense-
modifying mechanisms as those that allow for coer-
cions to be understood. However, alternations are
different realizations of the same norm. For exam-
ple, the following are alternations for treat, pattern
(I):
[[Person 1 <--> Medicament | Med-Procedure | Institution]]
[[Person 2 <--> Injury | Ailment | Bodypart]]
CPA Patterns
A CPA pattern extends the traditional notion of se-
lectional context to include a number of other con-
textual features, such as minor category parsing and
subphrasal cues. Accurate identification of the se-
mantically relevant aspects of a pattern is not an
obvious and straightforward procedure, as has some-
times been assumed in the literature. For example,
the presence or absence of an adverbial of manner in
the third valency slot around a verb can dramatically
alter the verb?s meaning. Simple syntactic encoding
of argument structure, for instance, is insufficient to
discriminate between the two major senses of the
verb treat, as illustrated below.
(3) a. They say their bosses treat them with respect.
b. Such patients are treated with antibiotics.
The ability to recognize the shallow semantic type
of a phrase in the context of a predicate is of course
crucial ?for example, in (3a) recognizing the PP as
(a) an adverbial, and (b) an adverbial of manner,
rather than an instrumental co-agent (as in (3b)),
is crucial for assigning the correct sense to the verb
treat above.
There are four constraint sets that contribute to
the patterns for encoding selection contexts. These
are:
(4) a. Shallow Syntactic Parsing: Phrase-level recogni-
tion of major categories.
b. Shallow Semantic Typing: 50-100 primitive shal-
low types, such as Person, Institution, Event, Abstract, Ar-
tifact, Location, and so forth. These are the top types se-
lected from the Brandeis Shallow Ontology (BSO), and are
similar to entities (and some relations) employed in Named
Entity Recognition tasks, such as TREC and ACE.
c. Minor Syntactic Category Parsing: e.g., loca-
tives, purpose clauses, rationale clauses, temporal adjuncts.
d. Subphrasal Syntactic Cue Recognition: e.g.,
genitives, partitives, bare plural/determiner distinctions,
infinitivals, negatives.
The notion of a selection context pattern, as pro-
duced by a human annotator, is expressed as a BNF
specification in Table 1.1 This specification relies
on word order to specify argument position, and is
easily translated to a template with slots allocated
for each argument. Within this grammar, a seman-
tic roles can be specified for each argument, but this
information currently is not used in the automated
processing.
Brandeis Shallow Ontology
The Brandeis Shallow Ontology (BSO) is a shallow
hierarchy of types selected for their prevalence in
manually identified selection context patterns. At
the time of writing, there are just 65 types, in terms
of which patterns for the first one hundred verbs
have been analyzed. New types are added occasion-
ally, but only when all possibilities of using existing
types prove inadequate. Once the set of manually
extracted patterns is sufficient, the type system will
be re-populated and become pattern-driven.
The BSO type system allows multiple inheri-
tance (e.g. Document v PhysObj and Document v
Information. The types currently comprising the
ontology are listed below. The BSO contains type
assignments for 20,000 noun entries and 10,000 nom-
inal collocation entries.
1Round brackets indicate optional elements of the pattern,
and curly brackets indicate syntactic constituents.
Corpus-driven Type System
The acquisition strategy for selectional preferences
for predicates proceeds as follows:
(5) a. Partition the corpus occurrences of a predicate according
to the selection contexts pattern grammar, distinguished
by the four levels of constraints mentioned in (4). These
are uninterpreted patterns for the predicate.
b. Within a given pattern, promote the statistically
significant literal types from the corpus for each argument
to the predicate. This induces an interpretation of the
pattern, treating the promoted literal type as the specific
binding of a shallow type from step (a) above.
c. Within a given pattern, coerce all lexical heads in the
same shallow type for an argument, into the promoted
literal type, assigned in (b) above. This is a coercion of a
lexical head to the interpretation of the promoted literal
type induced from step (b) above.
In a sense, (5a) can be seen as a broad multi-level
partitioning of the selectional behavior for a pred-
icate according to a richer set of syntactic and se-
mantic discriminants. Step (5b) can be seen as cap-
turing the norms of usage in the corpus, while step
(5c) is a way of modeling the exploitation of these
norms in the language (through coercion, metonymy,
and other generative operations). To illustrate the
way in which CPA discriminates uninterpreted pat-
terns from the corpus, we return to the verb treat as
it is used in the BNC. Two of its major senses, as
listed in (1), emerge as correlated with two distinct
context patterns, using the discriminant constraints
mentioned in (4) above.
CPA-Pattern ? Segment verb-lit Segment | verb-lit Segment | Segment verb-lit | CPA-Pattern ?;? Element
Segment ? Element | Segment Segment | ?? Segment ?? | ?(? Segment ?)? | Segment ?|? Segment
Element ? literal | ?[? Rstr ArgType ?]? | ?[? Rstr literal ?]? | ?[? Rstr ?]? | ?[? NO Cue ?]? | ?[? Cue ?]?
Rstr ? POS | Phrasal | Rstr ?|? Rstr | epsilon
Cue ? POS | Phrasal | AdvCue
AdvCue ? ADV ?[? AdvType ?]?
AdvType ? Manner | Dir | Location
Phrasal ? OBJ | CLAUSE | VP | QUOTE
POS ? ADJ | ADV | DET | POSDET | COREF POSDET | REFL-PRON | NEG |
MASS | PLURAL | V | INF | PREP | V-ING | CARD | QUANT | CONJ
ArgType ? ?[? SType ?]? | ?[? SType ?=? SubtypeSpec ?]? | ArgType ?|? ArgType | ?[? SType ArgIdx ?]? |
?[? SType ArgIdx ?=? SubtypeSpec ?]?
SType ? AdvType | TopType | Entity | Abstract | PhysObj | Institution | Asset | Location | Human | Animate |
Human Group | Substance | Unit of Measurement | Quality | Event | State of Affairs | Process
SubtypeSpec ? SubtypeSpec ?|? SubtypeSpec | SubtypeSpec ?&? SubtypeSpec | Role | Polarity | LSet
Role ? Role | Role ?|? Role | Benficiary | Meronym | Agent | Payer
Polarity ? Negative | Positive
LSet ? Worker | Pilot | Musician | Competitor | Hospital | Injury | Ailment | Medicament | Medical Procedure |
Hour-Measure | Bargain | Clothing | BodyPart | Text | Sewage | Part | Computer | Animal
ArgIdx ? <number> verb-lit ? <verb-word-form>
literal ? word word ? <word>
CARD ? <number> NEG ? not
POSDET ? my | your | ... INF ? to
QUANT ? CARD | a lot | longer | more | many | ...
Table 1: Pattern grammar
(6) a. [[Person 1]] treat [[Person 2]]; NO [Adv[Manner]]
b. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
Given a distinct (contextual) basis on which to an-
alyze the actual statistical distribution of the words
in each argument position, we can promote statisti-
cally relevant and significant literal types for these
positions. For example, for pattern (a) above, this
induces Doctor as Person 1, and Patient as bound
to Person 2. This produces the interpreted context
pattern for this sense as shown below.
(7) [[doctor]] treat [[patient]]
Promoted literal types are corpus-derived and
predicate-dependent, and are syntactic heads of
phrases that occur with the greatest frequency in ar-
gument positions for a given sense pattern; they are
subsequently assumed to be subtypes of the particu-
lar shallow type in the pattern. Step (5c) above then
enables us to bind the other lexical heads in these po-
sitions as coerced forms of the promoted literal type.
This can be seen below in the concordance sample,
where therapies is interpreted as Doctor, and people
and girl are interpreted as Patient.
(8) a. a doctor who treated the girl till an ambulance arrived.
b. over 90,000 people have been treated for cholera
c. nonsurgical therapies to treat the breast cancer, which
Model Bias
The assumption within GL is that semantic types
in the grammar map systematically to default syn-
tactic templates (cf. Pustejovsky (1995)). These
are termed canonical syntactic forms (CSFs). For
example, the CSF for the type proposition is a
tensed S. There are, however, many possible real-
izations (such as infinitival S and NP) for this type
due to the different possibilities available from gen-
erative devices in a grammar, such as coercion and
co-composition. The resulting set of syntactic forms
associated with a particular semantic type is called
a phrasal paradigm for that type. The model bias
provided by GL acts to guide the interpretation of
purely statistically based measures.
2.2 Automatic Recognition of Pattern Use
Essentially, this subtask is similar to the traditional
supervised WSD problem. Its purpose is (1) to test
the discriminatory power of CPA-derived feature-
set, (2) to extend and refine the inventory of features
captured by the CPA patterns, and (3) to allow for
predicate-based argument groupings by classifying
unseen instances. Extension and refinement of the
inventory of features should involve feature induc-
tion, but at the moment this part has not been im-
plemented. During the lexical discovery stage, lex-
ical sets that fill some of the argument slots in the
patterns are instantiated from the training exam-
ples. As more predicate-based lexical sets within
shallow types are explored, the data will permit
identification of the types of features that unite ele-
ments in lexical sets.
2.3 Automatic Pattern Acquisition
The algorithm for automatic pattern acquisition in-
volves the following steps:
(9) a. Collect all constituents in a particular argument position;
b. Identify syntactic alternations;
c. Perform clustering on all nouns that occur in a particular
argument position of a given predicate;
d. For each cluster, measure its relatedness to the known
lexical sets, obtained previously during the lexical discovery
stage and extended through WSD of unseen instances. If
none of the existing lexical sets pass the distance threshold,
establish the cluster as a new lexical set, to be used in future
pattern specification.
Step (9d) must include extensive filtering procedures
to check for shared semantic features, looking for
commonality between the members. That is, there
must be some threshold overlap between subgroups
of the candidate lexical set and and the existing se-
mantic classes. For instance, checking if, for a cer-
tain percentage of pairs in the candidate set, there
already exists a set of which both elements are mem-
bers.
3 Current Implementation
The CPA patterns are developed using the British
National Corpus (BNC). The sorted instances are
used as a training set for the supervised disambigua-
tion. For the disambiguation task, each pattern is
translated into into a set of preprocessing-specific
features.
The BNC is preprocessed with the RASP parser
and semantically tagged with BSO types. The
RASP system (Briscoe and Carroll (2002)) gener-
ates full parse trees for each sentence, assigning a
probability to each parse. It also produces a set of
grammatical relations for each parse, specifying the
relation type, the headword, and the dependent ele-
ment. All our computations are performed over the
single top-ranked tree for the sentences where a full
parse was successfully obtained. Some of the RASP
grammatical relations are shown in (10).
(10) subjects: ncsubj, clausal (csubj, xsubj)
objects: dobj, iobj, clausal complement
modifiers: adverbs, modifiers of event nominals
We use endocentric semantic typing, i.e., the head-
word of each constituent is used to establish its se-
mantic type. The semantic tagging strategy is simi-
lar to the one described in Pustejovsky et al (2002),
and currently uses a subset of 24 BSO types.
A CPA pattern is translated into a feature set,
currently using binary features. It is further com-
plemented with other discriminant context features
which, rather than distinguishing a particular pat-
tern, are merely likely to occur with a given subset
of patterns; that is, the features that only partially
determine or co-determine a sense. In the future,
these should be learned from the training set through
feature induction from the training sample, but at
the moment, they are added manually. The result-
ing feature matrix for each pattern contains features
such as those in (11) below. Each pattern is trans-
lated into a template of 15-25 features.
(11) Selected context features:
a. obj institution: object belongs to the BSO type ?Insti-
tution?
b. subj human group: subject belongs to the BSO type ?Hu-
manGroup?
c. mod adv ly: target verb has an adverbial modifier, with a
-ly adverb
d. clausal like: target verb has a clausal argument intro-
duced by ?like?
e. iobj with: target verb has an indirect object introduced
by ?with?
f. obj PRP: direct object is a personal pronoun
g. stem VVG: the target verb stem is an -ing form
Each feature may be realized by a number of RASP
relations. For instance, a feature dealing with
objects would take into account RASP relations
?dobj?, ?obj2?, and ?ncsubj? (for passives).
4 Results and Discussion
The experimental trials performed to date are too
preliminary to validate the methodology outlined
above in general terms for the WSD task. Our re-
sults are encouraging however, and comparable to
the best performing systems reported from Senseval
2. For our experiments, we implemented two ma-
chine learning algorithms, instance-based k-Nearest
Neighbor, and a decision tree algorithm (a version of
ID3). Table 2 shows the results on a subset of verbs
that have been processed, also listing the number of
patterns in the pattern set for each of the verbs.2
verb number of training accuracy
patterns set ID3 kNN
edit 2 100 87% 86%
treat 4 200 45% 52%
submit 4 100 59% 64%
Table 2: Accuracy of pattern identification
Further experimentation is obviously needed to
adequately gauge the effectiveness of the selection
context approach for WSD and other NLP tasks.
It is already clear, however, that the traditional
sense enumeration approach, where senses are asso-
ciated with individual lexical items, must give way
to a model where senses are assigned to the contexts
within which words appear. Furthermore, because
the variability of the stereotypical syntagmatic pat-
terns that are associated with words appears to be
relatively small, such information can be encoded as
lexically-indexed contexts. A comprehensive dictio-
nary of such contexts could prove to be a powerful
tool for a variety of NLP tasks.
References
T. Briscoe and J. Carroll. 2002. Robust accurate statistical anno-
tation of general text. Proceedings of the Third International
Conference on Language Resources and Evaluation (LREC
2002).
J. Pustejovsky, A. Rumshisky, and J. Castano. 2002. Rerendering
Semantic Ontologies: Automatic Extensions to UMLS through
Corpus Analytics. In LREC 2002 Workshop on Ontologies
and Lexical Knowledge Bases..
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.):
MIT Press.
2Test set size for each lemma is 100 instances, selected out
of several randomly chosen segments of BNC, non-overlapping
with the training set
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 117?125,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Classification of Discourse Coherence Relations: An Exploratory Study
using Multiple Knowledge Sources
Ben Wellner
 
, James Pustejovsky   , Catherine Havasi   ,
Anna Rumshisky
 
and Roser Saur??
 
 
Department of Computer Science
Brandeis University
Waltham, MA USA

The MITRE Corporation
202 Burlington Road
Bedford, MA USA

wellner,jamesp,havasi,arum,roser  @cs.brandeis.edu
Abstract
In this paper we consider the problem of
identifying and classifying discourse co-
herence relations. We report initial re-
sults over the recently released Discourse
GraphBank (Wolf and Gibson, 2005). Our
approach considers, and determines the
contributions of, a variety of syntactic and
lexico-semantic features. We achieve 81%
accuracy on the task of discourse relation
type classification and 70% accuracy on
relation identification.
1 Introduction
The area of modeling discourse has arguably seen
less success than other areas in NLP. Contribut-
ing to this is the fact that no consensus has been
reached on the inventory of discourse relations
nor on the types of formal restrictions placed on
discourse structure. Furthermore, modeling dis-
course structure requires access to considerable
prior linguistic analysis including syntax, lexical
and compositional semantics, as well as the res-
olution of entity and event-level anaphora, all of
which are non-trivial problems themselves.
Discourse processing has been used in many
text processing applications, most notably text
summarization and compression, text generation,
and dialogue understanding. However, it is also
important for general text understanding, includ-
ing applications such as information extraction
and question answering.
Recently, Wolf and Gibson (2005) have pro-
posed a graph-based approach to representing in-
formational discourse relations.1 They demon-
strate that tree representations are inadequate for
1The relations they define roughly follow Hobbs (1985).
modeling coherence relations, and show that many
discourse segments have multiple parents (incom-
ing directed relations) and many of the relations
introduce crossing dependencies ? both of which
preclude tree representations. Their annotation of
135 articles has been released as the GraphBank
corpus.
In this paper, we provide initial results for the
following tasks: (1) automatically classifying the
type of discourse coherence relation; and (2) iden-
tifying whether any discourse relation exists on
two text segments. The experiments we report
are based on the annotated data in the Discourse
GraphBank, where we assume that the discourse
units have already been identified.
In contrast to a highly structured, compositional
approach to discourse parsing, we explore a sim-
ple, flat, feature-based methodology. Such an ap-
proach has the advantage of easily accommodat-
ing many knowledge sources. This type of de-
tailed feature analysis can serve to inform or aug-
ment more structured, compositional approaches
to discourse such as those based on Segmented
Discourse Representation Theory (SDRT) (Asher
and Lascarides, 2003) or the approach taken with
the D-LTAG system (Forbes et al, 2001).
Using a comprehensive set of linguistic fea-
tures as input to a Maximum Entropy classifier,
we achieve 81% accuracy on classifying the cor-
rect type of discourse coherence relation between
two segments.
2 Previous Work
In the past few years, the tasks of discourse seg-
mentation and parsing have been tackled from
different perspectives and within different frame-
works. Within Rhetorical Structure Theory (RST),
Soricut and Marcu (2003) have developed two
117
probabilistic models for identifying clausal ele-
mentary discourse units and generating discourse
trees at the sentence level. These are built using
lexical and syntactic information obtained from
mapping the discourse-annotated sentences in the
RST Corpus (Carlson et al, 2003) to their corre-
sponding syntactic trees in the Penn Treebank.
Within SDRT, Baldridge and Lascarides
(2005b) also take a data-driven approach to
the tasks of segmentation and identification of
discourse relations. They create a probabilistic
discourse parser based on dialogues from the Red-
woods Treebank, annotated with SDRT rhetorical
relations (Baldridge and Lascarides, 2005a). The
parser is grounded on headed tree representations
and dialogue-based features, such as turn-taking
and domain specific goals.
In the Penn Discourse TreeBank (PDTB) (Web-
ber et al, 2005), the identification of discourse
structure is approached independently of any lin-
guistic theory by using discourse connectives
rather than abstract rhetorical relations. PDTB
assumes that connectives are binary discourse-
level predicates conveying a semantic relationship
between two abstract object-denoting arguments.
The set of semantic relationships can be estab-
lished at different levels of granularity, depend-
ing on the application. Miltsakaki, et al (2005)
propose a first step at disambiguating the sense of
a small subset of connectives (since, while, and
when) at the paragraph level. They aim at distin-
guishing between the temporal, causal, and con-
trastive use of the connective, by means of syntac-
tic features derived from the Penn Treebank and a
MaxEnt model.
3 GraphBank
3.1 Coherence Relations
For annotating the discourse relations in text, Wolf
and Gibson (2005) assume a clause-unit-based
definition of a discourse segment. They define
four broad classes of coherence relations:
(1) 1. Resemblance: similarity (par), con-
trast (contr), example (examp), generaliza-
tion (gen), elaboration (elab);
2. Cause-effect: explanation (ce), violated
expectation (expv), condition (cond);
3. Temporal (temp): essentially narration;
4. Attribution (attr): reporting and evidential
contexts.
The textual evidence contributing to identifying
the various resemblance relations is heterogeneous
at best, where, for example, similarity and contrast
are associated with specific syntactic constructions
and devices. For each relation type, there are well-
known lexical and phrasal cues:
(2) a. similarity: and;
b. contrast: by contrast, but;
c. example: for example;
d. elaboration: also, furthermore, in addi-
tion, note that;
e. generalization: in general.
However, just as often, the relation is encoded
through lexical coherence, via semantic associa-
tion, sub/supertyping, and accommodation strate-
gies (Asher and Lascarides, 2003).
The cause-effect relations include conventional
causation and explanation relations (captured as
the label ce), such as (3) below:
(3) cause: SEG1: crash-landed in New Hope,
Ga.,
effect: SEG2: and injuring 23 others.
It also includes conditionals and violated expecta-
tions, such as (4).
(4) cause: SEG1: an Eastern Airlines Lockheed
L-1011 en route from Miami to the Bahamas
lost all three of its engines,
effect: SEG2: and land safely back in Miami.
The two last coherence relations annotated in
GraphBank are temporal (temp) and attribution
(attr) relations. The first corresponds generally to
the occasion (Hobbs, 1985) or narration (Asher
and Lascarides, 2003) relation, while the latter is
a general annotation over attribution of source.2
3.2 Discussion
The difficulty of annotating coherence relations
consistently has been previously discussed in the
literature. In GraphBank, as in any corpus, there
are inconsistencies that must be accommodated
for learning purposes. As perhaps expected, an-
notation of attribution and temporal sequence rela-
tions was consistent if not entirely complete. The
most serious concern we had from working with
2There is one non-rhetorical relation, same, which identi-
fies discontiguous segments.
118
the corpus derives from the conflation of diverse
and semantically contradictory relations among
the cause-effect annotations. For canonical cau-
sation pairs (and their violations) such as those
above, (3) and (4), the annotation was expectedly
consistent and semantically appropriate. Problems
arise, however when examining the treatment of
purpose clauses and rationale clauses. These are
annotated, according to the guidelines, as cause-
effect pairings. Consider (5) below.
(5) cause: SEG1: to upgrade lab equipment in
1987.
effect: SEG2: The university spent $ 30,000
This is both counter-intuitive and temporally false.
The rationale clause is annotated as the cause, and
the matrix sentence as the effect. Things are even
worse with purpose clause annotation. Consider
the following example discourse:3
(6) John pushed the door to open it, but it was
locked.
This would have the following annotation in
GraphBank:
(7) cause: to open it
effect: John pushed the door.
The guideline reflects the appropriate intuition
that the intention expressed in the purpose or ra-
tionale clause must precede the implementation of
the action carried out in the matrix sentence. In
effect, this would be something like
(8) [INTENTION TO SEG1] CAUSES SEG2
The problem here is that the cause-effect re-
lation conflates real event-causation with telos-
directed explanations, that is, action directed to-
wards a goal by virtue of an intention. Given that
these are semantically disjoint relations, which
are furthermore triggered by distinct grammatical
constructions, we believe this conflation should be
undone and characterized as two separate coher-
ence relations. If the relations just discussed were
annotated as telic-causation, the features encoded
for subsequent training of a machine learning al-
gorithm could benefit from distinct syntactic envi-
ronments. We would like to automatically gen-
erate temporal orderings from cause-effect rela-
tions from the events directly annotated in the text.
3This specific example was brought to our attention by
Alex Lascarides (p.c).
Splitting these classes would preserve the sound-
ness of such a procedure, while keeping them
lumped generates inconsistencies.
4 Data Preparation and Knowledge
Sources
In this section we describe the various linguistic
processing components used for classification and
identification of GraphBank discourse relations.
4.1 Pre-Processing
We performed tokenization, sentence tagging,
part-of-speech tagging, and shallow syntactic
parsing (chunking) over the 135 GraphBank docu-
ments. Part-of-speech tagging and shallow parsing
were carried out using the Carafe implementation
of Conditional Random Fields for NLP (Wellner
and Vilain, 2006) trained on various standard cor-
pora. In addition, full sentence parses were ob-
tained using the RASP parser (Briscoe and Car-
roll, 2002). Grammatical relations derived from
a single top-ranked tree for each sentence (head-
word, modifier, and relation type) were used for
feature construction.
4.2 Modal Parsing and Temporal Ordering
of Events
We performed both modal parsing and tempo-
ral parsing over events. Identification of events
was performed using EvITA (Saur?? et al, 2006),
an open-domain event tagger developed under the
TARSQI research framework (Verhagen et al,
2005). EvITA locates and tags all event-referring
expressions in the input text that can be tempo-
rally ordered. In addition, it identifies those gram-
matical features implicated in temporal and modal
information of events; namely, tense, aspect, po-
larity, modality, as well as the event class. Event
annotation follows version 1.2.1 of the TimeML
specifications.4
Modal parsing in the form of identifying sub-
ordinating verb relations and their type was per-
formed using SlinkET (Saur?? et al, 2006), an-
other component of the TARSQI framework. Slin-
kET identifies subordination constructions intro-
ducing modality information in text; essentially,
infinitival and that-clauses embedded by factive
predicates (regret), reporting predicates (say), and
predicates referring to events of attempting (try),
volition (want), command (order), among others.
4See http://www.timeml.org.
119
SlinkET annotates these subordination contexts
and classifies them according to the modality in-
formation introduced by the relation between the
embedding and embedded predicates, which can
be of any of the following types:
 factive: The embedded event is presupposed
or entailed as true (e.g., John managed to
leave the party).
 counter-factive: The embedded event is pre-
supposed as entailed as false (e.g., John was
unable to leave the party).
 evidential: The subordination is introduced
by a reporting or perception event (e.g., Mary
saw/told that John left the party).
 negative evidential: The subordination is a
reporting event conveying negative polarity
(e.g., Mary denied that John left the party).
 modal: The subordination creates an inten-
sional context (e.g., John wanted to leave the
party).
Temporal orderings between events were iden-
tified using a Maximum Entropy classifier trained
on the TimeBank 1.2 and Opinion 1.0a corpora.
These corpora provide annotated events along
with temporal links between events. The link
types included: before ( 
	 occurs before  ) , in-
cludes ( 

occurs sometime during 
	
), simultane-
ous ( 	 occurs over the same interval as  ), begins
(  	 begins at the same time as   ), ends (  	 ends at
the same time as 

).
4.3 Lexical Semantic Typing and Coherence
Lexical semantic types as well as a measure of
lexical similarity or coherence between words in
two discourse segments would appear to be use-
ful for assigning an appropriate discourse rela-
tionship. Resemblance relations, in particular, re-
quire similar entities to be involved and lexical
similarity here serves as an approximation to defi-
nite nominal coreference. Identification of lexical
relationships between words across segments ap-
pears especially useful for cause-effect relations.
In example (3) above, determining a (potential)
cause-effect relationship between crash and injury
is necessary to identify the discourse relation.
4.3.1 Corpus-based Lexical Similarity
Lexical similarity was computed using the
Word Sketch Engine (WSE) (Killgarrif et al,
2004) similarity metric applied over British Na-
tional Corpus. The WSE similarity metric imple-
ments the word similarity measure based on gram-
matical relations as defined in (Lin, 1998) with mi-
nor modifications.
4.3.2 The Brandeis Semantic Ontology
As a second source of lexical coherence, we
used the Brandeis Semantic Ontology or BSO
(Pustejovsky et al, 2006). The BSO is a lexically-
based ontology in the Generative Lexicon tradi-
tion (Pustejovsky, 2001; Pustejovsky, 1995). It fo-
cuses on contextualizing the meanings of words
and does this by a rich system of types and qualia
structures. For example, if one were to look up the
phrase RED WINE in the BSO, one would find its
type is WINE and its type?s type is ALCOHOLIC
BEVERAGE. The BSO contains ontological qualia
information (shown below). Using the BSO, one






wine
CONSTITUTIVE  Alcohol
HAS ELEMENT  Alcohol
MADE OF  Grapes
INDIRECT TELIC  drink activity
INDIRECT AGENTIVE  make alcoholic beverage






is able to find out where in the ontological type
system WINE is located, what RED WINE?s lexi-
cal neighbors are, and its full set of part of speech
and grammatical attributes. Other words have a
different configuration of annotated attributes de-
pending on the type of the word.
We used the BSO typing information to seman-
tically tag individual words in order to compute
lexical paths between word pairs. Such lexical as-
sociations are invoked when constructing cause-
effect relations and other implicatures (e.g. be-
tween crash and injure in Example 3).
The type system paths provide a measure of the
connectedness between words. For every pair of
head words in a GraphBank document, the short-
est path between the two words within the BSO
is computed. Currently, this metric only uses the
type system relations (i.e., inheritance) but prelim-
inary tests show that including qualia relations as
connections is promising. We also computed the
earliest common ancestor of the two words. These
metrics are calculated for every possible sense of
the word within the BSO.
120
The use of the BSO is advantageous compared
to other frameworks such as Wordnet because it
focuses on the connection between words and their
semantic relationship to other items. These con-
nections are captured in the qualia information and
the type system. In Wordnet, qualia-like informa-
tion is only present in the glosses, and they do
not provide a definite semantic path between any
two lexical items. Although synonymous in some
ways, synset members often behave differently in
many situations, grammatical or otherwise.
5 Classification Methodology
This section describes in detail how we con-
structed features from the various knowledge
sources described above and how they were en-
coded in a Maximum Entropy model.
5.1 Maximum Entropy Classification
For our experiments of classifying relation types,
we used a Maximum Entropy classifier5 in order
to assign labels to each pair of discourse segments
connected by some relation. For each instance (i.e.
pair of segments) the classifier makes its decision
based on a set of features. Each feature can query
some arbitrary property of the two segments, pos-
sibly taking into account external information or
knowledge sources. For example, a feature could
query whether the two segments are adjacent to
each other, whether one segment contains a dis-
course connective, whether they both share a par-
ticular word, whether a particular syntactic con-
struction or lexical association is present, etc. We
make strong use of this ability to include very
many, highly interdependent features6 in our ex-
periments. Besides binary-valued features, fea-
ture values can be real-valued and thus capture fre-
quencies, similarity values, or other scalar quanti-
ties.
5.2 Feature Classes
We grouped the features together into various
feature classes based roughly on the knowledge
source from which they were derived. Table 1
describes the various feature classes in detail and
provides some actual example features from each
class for the segment pair described in Example 5
in Section 3.2.
5We use the Maximum Entropy classifier included with
Carafe available at http://sourceforge.net/projects/carafe
6The total maximum number of features occurring in our
experiments is roughly 120,000.
6 Experiments and Results
In this section we provide the results of a set of
experiments focused on the task of discourse rela-
tion classification. We also report initial results on
relation identification with the same set of features
as used for classification.
6.1 Discourse Relation Classification
The task of discourse relation classification in-
volves assigning the correct label to a pair of dis-
course segments.7 The pair of segments to assign
a relation to is provided (from the annotated data).
In addition, we assume, for asymmetric links, that
the nucleus and satellite are provided (i.e., the di-
rection of the relation). For the elaboration rela-
tions, we ignored the annotated subtypes (person,
time, location, etc.). Experiments were carried out
on the full set of relation types as well as the sim-
pler set of coarse-grained relation categories de-
scribed in Section 3.1.
The GraphBank contains a total of 8755 an-
notated coherence relations. 8 For all the ex-
periments in this paper, we used 8-fold cross-
validation with 12.5% of the data used for test-
ing and the remainder used for training for each
fold. Accuracy numbers reported are the average
accuracies over the 8 folds. Variance was gener-
ally low with a standard deviation typically in the
range of 1.5 to 2.0. We note here also that the
inter-annotator agreement between the two Graph-
Bank annotators was 94.6% for relations when
they agreed on the presence of a relation. The
majority class baseline (i.e., the accuracy achieved
by calling all relations elaboration) is 45.7% (and
66.57% with the collapsed categories). These are
the upper and lower bounds against which these
results should be based.
To ascertain the utility of each of the various
feature classes, we considered each feature class
independently by using only features from a sin-
gle class in addition to the Proximity feature class
which serve as a baseline. Table 2 illustrates the
result of this experiment.
We performed a second set of experiments
shown in Table 3 that is essentially the converse
of the previous batch. We take the union of all the
7Each segment may in fact consist of a sequence of seg-
ments. We will, however, use the term segment loosely to
refer to segments or segment sequences.
8All documents are doubly annotated; we used the anno-
tator1 annotations.
121
Feature Description Example
Class
C Words appearing at beginning and end of the two discourse seg-
ments - these are often important discourse cue words.
first1-is-to; first2-is-The
P Proximity and direction between the two segments (in terms of
segments) - binary features such as distance less than 3, distance
greater than 10 were used in addition to the distance value itself;
the distance from beginning of the document using a similar bin-
ning approach
adjacent; dist-less-than-3; dist-less-
than-5; direction-reverse; samesentence
BSO Paths in the BSO up to length 10 between non-function words in the
two segments.
ResearchLab  EducationalActivity
 University
WSE WSE word-pair similarities between words in the two segments
were binned as (  0.05,  0.1,  0.2). We also computed sen-
tence similarity as the sum of the word similarities divided by the
sum of their sentence lengths.
WSE-greater-than-0.05; WSE-
sentence-sim = 0.005417
E Event head words and event head word pairs between segments as
identified by EvITA.
event1-is-upgrade; event2-is-spent;
event-pair-upgrade-spent
SlinkET Event attributes, subordinating links and their types between event
pairs in the two segments
seg1-class-is-occurrence; seg2-class-
is-occurrence; seg1-tense-is-infinitive;
seg2-tense-is-past; seg2-modal-seg1
C-E Cuewords of one segment paired with events in the other. first1-is-to-event2-is-spent; first2-is-
The-event1-is-upgrade
Syntax Grammatical dependency relations between two segments as iden-
tified by the RASP parser. We also conjoined the relation with one
or both of the headwords associated with the grammatical relation.
gr-ncmod; gr-ncmod-head1-equipment;
gr-ncmod-head-2-spent; etc.
Tlink Temporal links between events in the two segments. We included
both the link types and the number of occurrences of those types
between the segments
seg2-before-seg1
Table 1: Feature classes, their descriptions and example feature instances for Example 5 in Section 3.2.
Feature Class Accuracy Coarse-grained Acc.
Proximity 60.08% 69.43%
P+C 76.77% 83.50%
P+BSO 62.92% 74.40%
P+WSE 62.20% 70.10%
P+E 63.84% 78.16%
P+SlinkET 69.00% 75.91%
P+CE 67.18% 78.63%
P+Syntax 70.30% 80.84%
P+Tlink 64.19% 72.30%
Table 2: Classification accuracy over standard and
coarse-grained relation types with each feature
class added to Proximity feature class.
feature classes and perform ablation experiments
by removing one feature class at a time.
Feature Class Accuracy Coarse-grain Acc.
All Features 81.06% 87.51%
All-P 71.52% 84.88%
All-C 75.71% 84.69%
All-BSO 80.65% 87.04%
All-WSE 80.26% 87.14%
All-E 80.90% 86.92%
All-SlinkET 79.68% 86.89%
All-CE 80.41% 87.14%
All-Syntax 80.20% 86.89%
All-Tlink 80.30% 87.36%
Table 3: Classification accuracy with each fea-
ture class removed from the union of all feature
classes.
6.2 Analysis
From the ablation results, it is clear that overall
performance is most impacted by the cue-word
features (C) and proximity (P). Syntax and Slin-
kET also have high impact improving accuracy by
roughly 10 and 9 percent respectively as shown
in Table 2. From the ablation results in Table 3,
it is clear that the utility of most of the individ-
ual features classes is lessened when all the other
feature classes are taken into account. This indi-
cates that multiple feature classes are responsible
for providing evidence any given discourse rela-
tions. Removing a single feature class degrades
performance, but only slightly, as the others can
compensate.
Overall precision, recall and F-measure results
for each of the different link types using the set
of all feature classes are shown in Table 4 with the
corresponding confusion matrix in Table A.1. Per-
formance correlates roughly with the frequency of
the various relation types. We might therefore ex-
pect some improvement in performance with more
annotated data for those relations with low fre-
quency in the GraphBank.
122
Relation Precision Recall F-measure Count
elab 88.72 95.31 91.90 512
attr 91.14 95.10 93.09 184
par 71.89 83.33 77.19 132
same 87.09 75.00 80.60 72
ce 78.78 41.26 54.16 63
contr 65.51 66.67 66.08 57
examp 78.94 48.39 60.00 31
temp 50.00 20.83 29.41 24
expv 33.33 16.67 22.22 12
cond 45.45 62.50 52.63 8
gen 0.0 0.0 0.0 0
Table 4: Precision, Recall and F-measure results.
6.3 Coherence Relation Identification
The task of identifying the presence of a rela-
tion is complicated by the fact that we must con-
sider all ffProceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 88?93,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 7: Argument Selection and Coercion
James Pustejovsky
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
jamesp@cs.brandeis.edu
Anna Rumshisky
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
arum@cs.brandeis.edu
Abstract
In this paper, we describe the Argument Se-
lection and Coercion task, currently in devel-
opment for the SemEval-2 evaluation exercise
scheduled for 2010. This task involves char-
acterizing the type of compositional operation
that exists between a predicate and the argu-
ments it selects. Specifically, the goal is to
identify whether the type that a verb selects is
satisfied directly by the argument, or whether
the argument must change type to satisfy the
verb typing. We discuss the problem in detail
and describe the data preparation for the task.
1 Introduction
In recent years, a number of annotation schemes that
encode semantic information have been developed
and used to produce data sets for training machine
learning algorithms. Semantic markup schemes that
have focused on annotating entity types and, more
generally, word senses, have been extended to in-
clude semantic relationships between sentence ele-
ments, such as the semantic role (or label) assigned
to the argument by the predicate (Palmer et al, 2005;
Ruppenhofer et al, 2006; Kipper, 2005; Burchardt
et al, 2006; Ohara, 2008; Subirats, 2004).
In this task, we take this one step further, in that
this task attempts to capture the ?compositional his-
tory? of the argument selection relative to the pred-
icate. In particular, this task attempts to identify the
operations of type adjustment induced by a predicate
over its arguments when they do not match its selec-
tional properties. The task is defined as follows: for
each argument of a predicate, identify whether the
entity in that argument position satisfies the type ex-
pected by the predicate. If not, then one needs to
identify how the entity in that position satisfies the
typing expected by the predicate; that is, to identify
the source and target types in a type-shifting (or co-
ercion) operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition as in (1). Notice, however, that through a
metonymic interpretation, this constraint can be vi-
olated as demonstrated in (1).
(1) a. John reported in late from Washington.
b. Washington reported in late.
Neither the surface annotation of entity extents and
types, nor assigning semantic roles associated with
the predicate would reflect in this case a crucial
point: namely, that in order for the typing require-
ments of the predicate to be satisfied, what has been
referred to a type coercion or a metonymy (Hobbs et
al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg,
2005) has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This task
involved two types with their metonymic variants:
(2) i. Categories for Locations: literal, place-for-people,
place-for-event, place-for-product;
ii. Categories for Organizations: literal, organization-
for-members, organization-for-event, organization-for-
product, organization-for-facility.
One of the limitations of this approach, how-
ever, is that, while appropriate for these special-
ized metonymy relations, the annotation specifica-
tion and resulting corpus are not an informative
88
guide for extending the annotation of argument se-
lection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (3) below, the sense annotation for
the verb enjoy should arguably assign similar values
to both (3a) and (3b).
(3) a. Mary enjoyed drinking her beer .
b. Mary enjoyed her beer.
The consequence of this, however, is that, under cur-
rent sense and role annotation strategies, the map-
ping to a syntactic realization for a given sense is
made more complex, and is in fact, perplexing for a
clustering or learning algorithm operating over sub-
categorization types for the verb.
2 Methodology of Annotation
Before introducing the specifics of the argument se-
lection and coercion task, let us review briefly our
assumptions regarding the role of annotation within
the development and deployment of computational
linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough to
capture the desired behavior. These linguistic de-
scriptions are typically distilled from extensive the-
oretical modeling of the phenomenon. The descrip-
tions in turn form the basis for the annotation values
of the specification language, which are themselves
the features used in a development cycle for training
and testing an identification or labeling algorithm
over text. Finally, based on an analysis and evalu-
ation of the performance of a system, the model of
the phenomenon may be revised, for retraining and
testing.
We call this particular cycle of development the
MATTER methodology:
(4) a. Model: Structural descriptions provide
theoretically-informed attributes derived from
empirical observations over the data;
b. Annotate: Annotation scheme assumes a feature
set that encodes specific structural descriptions and
properties of the input data;
c. Train: Algorithm is trained over a corpus annotated
with the target feature set;
Figure 1: The MATTER Methodology
d. Test: Algorithm is tested against held-out data;
e. Evaluate: Standardized evaluation of results;
f. Revise: Revisit the model, annotation specification,
or algorithm, in order to make the annotation more
robust and reliable.
Some of the current and completed annotation ef-
forts that have undergone such a development cycle
include:
? PropBank (Palmer et al, 2005)
? NomBank (Meyers et al, 2004)
? TimeBank (Pustejovsky et al, 2005)
? Opinion Corpus (Wiebe et al, 2005)
? Penn Discourse TreeBank (Miltsakaki et al, 2004)
3 Task Description
This task involves identifying the selectional mech-
anism used by the predicate over a particular argu-
ment.1 For the purposes of this task, the possible re-
lations between the predicate and a given argument
are restricted to selection and coercion. In selection,
the argument NP satisfies the typing requirements of
the predicate, as in (5).
(5) a. The spokesman denied the statement (PROPOSITION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn?t believe the rumor (PROPOSI-
TION).
Coercion encompasses all cases when a type-
shifting operation must be performed on the com-
plement NP in order to satisfy selectional require-
ments of the predicate, as in (6). Note that coercion
operations may apply to any argument position in a
sentence, including the subject, as seen in (6b). Co-
ercion can also be seen as an object of a proposition
as in (6c).
(6) a. The president denied the attack (EVENT ? PROPOSI-
TION).
b. The White House (LOCATION ? HUMAN) denied this
statement.
c. The Boston office called with an update (EVENT ?
INFO).
1This task is part of a larger effort to annotate text with com-
positional operations (Pustejovsky et al, 2009).
89
The definition of coercion will be extended to in-
clude instances of type-shifting due to what we term
the qua-relation.
(7) a. You can crush the pill (PHYSICAL OBJECT) between
two spoons. (Selection)
b. It is always possible to crush imagination (ABSTRACT
ENTITY qua PHYSICAL OBJECT) under the weight of
numbers. (Coercion/qua-relation)
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve the following (1) identifying the verb sense
and the associated syntactic frame, (2) identifying
selectional requirements imposed by that verb sense
on the target argument, and (3) identifying semantic
type of the target argument. Sense inventories for
the verbs and the type templates associated with dif-
ferent syntactic frames will be provided to the par-
ticipants.
3.1 Semantic Types
In the present task, we use a subset of semantic types
from the Brandeis Shallow Ontology (BSO), which
is a shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky et al,
2004; Rumshisky et al, 2006). The BSO types were
selected for their prevalence in manually identified
selection context patterns developed for several hun-
dreds English verbs. That is, they capture common
semantic distinctions associated with the selectional
properties of many verbs.
The following list of types is currently being used
for annotation:
(8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT,
ORGANIZATION, EVENT, PROPOSITION, INFORMA-
TION, SENSATION, LOCATION, TIME PERIOD, AB-
STRACT ENTITY, ATTITUDE, EMOTION, PROPERTY,
PRIVILEGE, OBLIGATION, RULE
The subset of types chosen for annotation is pur-
posefully shallow, and is not structured in a hierar-
chy. For example, we include both HUMAN and AN-
IMATE in the type system along with PHYSICAL OB-
JECT. While HUMAN is a subtype of both ANIMATE
and PHYSICAL OBJECT, the system should simply
choose the most relevant type (i.e. HUMAN) and not
be concerned with type inheritance. The present set
of types may be revised if necessary as the annota-
tion proceeds.
Figure 2: Corpus Development Architecture
4 Resources and Corpus Development
Preparing the data for this task will be done in two
phases: the data set construction phase and the an-
notation phase. The first phase consists of (1) select-
ing the target verbs to be annotated and compiling a
sense inventory for each target, and (2) data extrac-
tion and preprocessing. The prepared data is then
loaded into the annotation interface. During the an-
notation phase, the annotation judgments are entered
into the database, and the adjudicator resolves dis-
agreements. The resulting database representation is
used by the exporting module to generate the corre-
sponding XML markup or stand-off annotation. The
corpus development architecture is shown in Fig. 2.
4.1 Data Set Construction Phase
In the set of target verbs selected for the task, pref-
erence will be given to the verbs that are strongly
coercive in at least one of their senses, i.e. tend to
impose semantic typing on one of their arguments.
The verbs will be selected by examining the data
from several sources, using the Sketch Engine (Kil-
garriff et al, 2004) as described in (Rumshisky and
Batiukova, 2008).
An inventory of senses will be compiled for each
verb. Whenever possible, the senses will be mapped
to OntoNotes (Pradhan et al, 2007) and to the CPA
patterns (Hanks, 2009). For each sense, a set of type
90
templates will be compiled, associating each sense
with one or more syntactic patterns which will in-
clude type specification for all arguments. For ex-
ample, one of the senses of the verb deny is refuse
to grant. This sense is associated with the following
type templates:
(9) HUMAN deny ENTITY to HUMAN
HUMAN deny HUMAN ENTITY
The set of type templates for each verb will be built
using a modification of the CPA technique (Hanks
and Pustejovsky, 2005; Pustejovsky et al, 2004)).
A set of sentences will be randomly extracted for
each target verb from the BNC (BNC, 2000) and
the American National Corpus (Ide and Suderman,
2004). This choice of corpora should ensure a more
balanced representation of language than is available
in commonly annotated WSJ and other newswire
text. Each extracted sentence will be automatically
parsed, and the sentences organized according to the
grammatical relation involving the target verb. Sen-
tences will be excluded from the set if the target ar-
gument is expressed as anaphor, or is not present in
the sentence. Semantic head for the target grammat-
ical relation will be identified in each case.
4.2 Annotation Phase
Word sense disambiguation will need to be per-
formed as a preliminary stage for the annotation of
compositional operations. The annotation task is
thus divided into two subtasks, presented succes-
sively to the annotator:
(1) Word sense disambiguation of the target predi-
cate
(2) Identification of the compositional relationship
between target predicate and its arguments
In the first subtask, the annotator is presented with
a set of sentences containing the target verb and the
chosen grammatical relation. The annotator is asked
to select the most fitting sense of the target verb, or
to throw out the example (pick the ?N/A? option) if
no sense can be chosen either due to insufficient con-
text, because the appropriate sense does not appear
in the inventory, or simply no disambiguation can be
made in good faith. The interface is shown in Fig.
3. After this step is complete, the appropriate sense
is saved into the database, along with the associated
type template.
In the second subtask, the annotator is presented
with a list of sentences in which the target verb
is used in the same sense. The data is annotated
one grammatical relation at a time. The annotator
is asked to determine whether the argument in the
specified grammatical relation to the target belongs
to the type associated with that sense in the corre-
sponding template. The illustration of this can be
seen in Fig. 4. We will perform double annotation
and subsequent adjudication at each of the above an-
notation stages.
5 Data Format
The test and training data will be provided in XML
format. The relation between the predicate (viewed
as function) and its argument will be represented by
a composition link (CompLink) as shown below.
In case of coercion, there is a mismatch between the
source and the target types, and both types need to
be identified:
The State Department repeatedly denied the attack.
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
the
<NOUN nid="n1">attack</NOUN> .
<CompLink cid="cid1" sID="s1"
relatedToNoun="n1" gramRel="dobj"
compType="COERCION"
sourceType="EVENT"
targetType="PROPOSITION"/>
When the compositional operation is selection, the
source and the target types must match:
The State Department repeatedly denied this statement.
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
this
<NOUN nid="n1">statement</NOUN> .
<CompLink cid="cid1" sID="s1"
relatedToNoun="n1" gramRel="dobj"
compType="selection"
sourceType="PROPOSITION"
targetType="PROPOSITION"/>
6 Evaluation Methodology
Precision and recall will be used as evaluation met-
rics. A scoring program will be supplied for partic-
ipants. Two subtasks will be evaluated separately:
91
Figure 3: Predicate Sense Disambiguation for deny.
(1) identifying the compositional operation (i.e. se-
lection vs. coercion) and (2) identifying the source
and target argument type, for each relevant argu-
ment. Both subtasks require sense disambiguation
which will not be evaluated separately.
Since type-shifting is by its nature a relatively
rare event, the distribution between different types
of compositional operations in the data set will be
necessarily skewed. One of the standard sampling
methods for handling class imbalance is downsiz-
ing (Japkowicz, 2000; Monard and Batista, 2002),
where the number of instances of the major class in
the training set is artificially reduced. Another possi-
ble alternative is to assign higher error costs to mis-
classification of minor class instances (Chawla et al,
2004; Domingos, 1999).
7 Conclusion
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2, to be held
in 2010. This task involves the identifying the rela-
tion between a predicate and its argument as one that
encodes the compositional history of the selection
process. This allows us to distinguish surface forms
that directly satisfy the selectional (type) require-
ments of a predicate from those that are coerced in
context. We described some details of a specifica-
tion language for selection and the annotation task
using this specification to identify argument selec-
tion behavior. Finally, we discussed data preparation
for the task and evaluation techniques for analyzing
the results.
References
BNC. 2000. The British National Corpus.
The BNC Consortium, University of Oxford,
http://www.natcorp.ox.ac.uk/.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The salsa corpus: a german corpus resource for lexical
semantics. In Proceedings of LREC, Genoa, Italy.
N. Chawla, N. Japkowicz, and A. Kotcz. 2004. Editorial:
special issue on learning from imbalanced data sets.
ACM SIGKDD Explorations Newsletter, 6(1):1?6.
P. Domingos. 1999. Metacost: A general method for
making classifiers cost-sensitive. In Proceedings of
the fifth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 155?
164. ACM New York, NY, USA.
Marcus Egg. 2005. Flexible semantics for reinterpreta-
tion phenomena. CSLI, Stanford.
P. Hanks and J. Pustejovsky. 2005. A pattern dictionary
for natural language processing. Revue Franc?aise de
Linguistique Applique?e.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpreta-
tion as abduction. Artificial Intelligence, 63:69?142.
N. Ide and K. Suderman. 2004. The American National
Corpus first release. In Proceedings of LREC 2004,
pages 1681?1684.
92
Figure 4: Identifying Compositional Relationship for deny.
N. Japkowicz. 2000. Learning from imbalanced data
sets: a comparison of various strategies. In AAAI
workshop on learning from imbalanced data sets,
pages 00?05.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell. 2004.
The Sketch Engine. Proceedings of Euralex, Lorient,
France, pages 105?116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, University
of Pennsylvania, PA.
K. Markert and M. Nissim. 2007. Metonymy resolution
at SemEval I: Guidelines for participants. In Proceed-
ings of the ACL 2007 Conference.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24?
31.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The Penn Discourse Treebank. In Proceedings of the
4th International Conference on Language Resources
and Evaluation.
M.C. Monard and G.E. Batista. 2002. Learning with
skewed class distributions. Advances in logic, artifi-
cial intelligence and robotics (LAPTEC?02).
Geoffrey Nunberg. 1979. The non-uniqueness of seman-
tic solutions: Polysemy. Linguistics and Philosophy,
3:143?184.
Kyoko Hirose Ohara. 2008. Lexicon, grammar, and mul-
tilinguality in the japanese framenet. In Proceedings
of LREC, Marrakech, Marocco.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007.
Semeval-2007 task-17: English lexical sample, srl and
all words. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 87?92, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004. Au-
tomated Induction of Sense in Context. In COLING
2004, Geneva, Switzerland, pages 924?931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123?164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth International
Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy in
verbs: systematic relations between senses and their
effect on annotation. In COLING Workshop on Hu-
man Judgement in Computational Linguistics (HJCL-
2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Confer-
ence, FLAIRS 2006, Melbourne Beach, Florida, USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice.
Carlos Subirats. 2004. FrameNet Espan?ol. Una red
sema?ntica de marcos conceptuales. In VI International
Congress of Hispanic Linguistics, Leipzig.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2):165?210.
93
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 27?32,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 7: Argument Selection and Coercion
James Pustejovsky and Anna Rumshisky and Alex Plotnick
Dept. of Computer Science
Brandeis University
Waltham, MA, USA
Elisabetta Jezek
Dept. of Linguistics
University of Pavia
Pavia, Italy
Olga Batiukova
Dept. of Humanities
Carlos III University of Madrid
Madrid, Spain
Valeria Quochi
ILC-CNR
Pisa, Italy
Abstract
We describe the Argument Selection and
Coercion task for the SemEval-2010 eval-
uation exercise. This task involves char-
acterizing the type of compositional oper-
ation that exists between a predicate and
the arguments it selects. Specifically, the
goal is to identify whether the type that
a verb selects is satisfied directly by the
argument, or whether the argument must
change type to satisfy the verb typing. We
discuss the problem in detail, describe the
data preparation for the task, and analyze
the results of the submissions.
1 Introduction
In recent years, a number of annotation schemes
that encode semantic information have been de-
veloped and used to produce data sets for training
machine learning algorithms. Semantic markup
schemes that have focused on annotating entity
types and, more generally, word senses, have
been extended to include semantic relationships
between sentence elements, such as the seman-
tic role (or label) assigned to the argument by the
predicate (Palmer et al, 2005; Ruppenhofer et al,
2006; Kipper, 2005; Burchardt et al, 2006; Subi-
rats, 2004).
In this task, we take this one step further and
attempt to capture the ?compositional history? of
the argument selection relative to the predicate. In
particular, this task attempts to identify the oper-
ations of type adjustment induced by a predicate
over its arguments when they do not match its se-
lectional properties. The task is defined as fol-
lows: for each argument of a predicate, identify
whether the entity in that argument position satis-
fies the type expected by the predicate. If not, then
identify how the entity in that position satisfies the
typing expected by the predicate; that is, identify
the source and target types in a type-shifting or co-
ercion operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition, as in (1a). Notice, however, that through
a metonymic interpretation, this constraint can be
violated, as demonstrated in (1b).
(1) a. John reported in late from Washington.
b. Washington reported in late.
Neither the surface annotation of entity extents
and types nor assigning semantic roles associated
with the predicate would reflect in this case a cru-
cial point: namely, that in order for the typing
requirements of the predicate to be satisfied, a
type coercion or a metonymy (Hobbs et al, 1993;
Pustejovsky, 1991; Nunberg, 1979; Egg, 2005)
has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This
task involved two types with their metonymic
variants: categories-for-locations (e.g., place-
for-people) and categories-for-organizations (e.g.,
organization-for-members). One of the limitations
of this approach, however, is that while appropri-
ate for these specialized metonymy relations, the
annotation specification and resulting corpus are
not an informative guide for extending the annota-
tion of argument selection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (2) below, the sense annotation
for the verb enjoy should arguably assign similar
values to both (2a) and (2b).
27
Figure 1: The MATTER Methodology
(2) a. Mary enjoyed drinking her beer.
b. Mary enjoyed her beer.
The consequence of this is that under current sense
and role annotation strategies, the mapping to a
syntactic realization for a given sense is made
more complex, and is in fact perplexing for a clus-
tering or learning algorithm operating over subcat-
egorization types for the verb.
2 Methodology of Annotation
Before introducing the specifics of the argument
selection and coercion task, we will briefly review
our assumptions regarding the role of annotation
in computational linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough
to capture the desired behavior. These linguistic
descriptions are typically distilled from extensive
theoretical modeling of the phenomenon. The de-
scriptions in turn form the basis for the annota-
tion values of the specification language, which
are themselves the features used in a development
cycle for training and testing a labeling algorithm
over a text. Finally, based on an analysis and eval-
uation of the performance of a system, the model
of the phenomenon may be revised.
We call this cycle of development the MATTER
methodology (Fig. 1):
Model: Structural descriptions provide theoretically in-
formed attributes derived from empirical observations
over the data;
Annotate: Annotation scheme assumes a feature set that en-
codes specific structural descriptions and properties of
the input data;
Train: Algorithm is trained over a corpus annotated with the
target feature set;
Test: Algorithm is tested against held-out data;
Evaluate: Standardized evaluation of results;
Revise: Revisit the model, annotation specification, or algo-
rithm, in order to make the annotation more robust and
reliable.
Some of the current and completed annotation ef-
forts that have undergone such a development cy-
cle include PropBank (Palmer et al, 2005), Nom-
Bank (Meyers et al, 2004), and TimeBank (Puste-
jovsky et al, 2005).
3 Task Description
The argument selection and coercion (ASC) task
involves identifying the selectional mechanism
used by the predicate over a particular argument.
1
For the purposes of this task, the possible relations
between the predicate and a given argument are re-
stricted to selection and coercion. In selection, the
argument NP satisfies the typing requirements of
the predicate, as in (3):
(3) a. The spokesman denied the statement (PROPOSI-
TION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn?t believe the rumor (PROPOSI-
TION).
Coercion occurs when a type-shifting operation
must be performed on the complement NP in order
to satisfy selectional requirements of the predicate,
as in (4). Note that coercion operations may apply
to any argument position in a sentence, including
the subject, as seen in (4b). Coercion can also be
seen as an object of a proposition, as in (4c).
(4) a. The president denied the attack (EVENT? PROPO-
SITION).
b. The White House (LOCATION ? HUMAN) denied
this statement.
c. The Boston office called with an update (EVENT?
INFO).
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve (1) identifying the verb sense and the asso-
ciated syntactic frame, (2) identifying selectional
requirements imposed by that verb sense on the
target argument, and (3) identifying the semantic
type of the target argument.
4 Resources and Corpus Development
We prepared the data for this task in two phases:
the data set construction phase and the annotation
phase (see Fig. 2). The first phase consisted of
(1) selecting the target verbs to be annotated and
compiling a sense inventory for each target, and
(2) data extraction and preprocessing. The pre-
pared data was then loaded into the annotation in-
terface. During the annotation phase, the annota-
tion judgments were entered into the database, and
an adjudicator resolved disagreements. The result-
ing database was then exported in an XML format.
1
This task is part of a larger effort to annotate text with
compositional operations (Pustejovsky et al, 2009).
28
Figure 2: Corpus Development Architecture
4.1 Data Set Construction Phase: English
For the English data set, the data construction
phase was combined with the annotation phase.
The data for the task was created using the fol-
lowing steps:
1. The verbs were selected by examining the data
from the BNC, using the Sketch Engine (Kilgar-
riff et al, 2004) as described in (Rumshisky and
Batiukova, 2008). Verbs that consistently im-
pose semantic typing on one of their arguments
in at least one of their senses (strongly coercive
verbs) were included into the final data set: ar-
rive (at), cancel, deny, finish, and hear.
2. Sense inventories were compiled for each verb,
with the senses mapped to OntoNotes (Pradhan
et al, 2007) whenever possible. For each sense,
a set of type templates was compiled using a
modification of the CPA technique (Hanks and
Pustejovsky, 2005; Pustejovsky et al, 2004):
every argument in the syntactic pattern asso-
ciated with a given sense was assigned a type
specification. Although a particular sense is
often compatible with more than one semantic
type for a given argument, this was never the
case in our data set, where no disjoint types
were tested. The coercive senses of the chosen
verbs were associated with the following type
templates:
a. Arrive (at), sense reach a destination or goal : HU-
MAN arrive at LOCATION
b. Cancel, sense call off : HUMAN cancel EVENT
c. Deny, sense state or maintain that something is un-
true: HUMAN deny PROPOSITION
d. Finish, sense complete an activity: HUMAN finish
EVENT
e. Hear, sense perceive physical sound : HUMAN hear
SOUND
We used a subset of semantic types from the
Brandeis Shallow Ontology (BSO), which is a
shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky
et al, 2004; Rumshisky et al, 2006). Types
were selected for their prevalence in manually
identified selection context patterns developed
for several hundred English verbs. That is,
they capture common semantic distinctions as-
sociated with the selectional properties of many
verbs. The types used for annotation were:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
DOCUMENT,DRINK,EMOTION,ENTITY,EVENT, FOOD,
HUMAN,HUMAN GROUP, IDEA, INFORMATION, LOCA-
TION,OBLIGATION,ORGANIZATION, PATH, PHYSICAL
OBJECT, PROPERTY, PROPOSITION,RULE, SENSATION,
SOUND, SUBSTANCE, TIME PERIOD, VEHICLE
This set of types is purposefully shallow and
non-hierarchical. For example, HUMAN is a
subtype of both ANIMATE and PHYSICAL OB-
JECT, but annotators and system developers
were instructed to choose the most relevant type
(e.g., HUMAN) and to ignore inheritance.
3. A set of sentences was randomly extracted for
each target verb from the BNC (Burnard, 1995).
The extracted sentences were parsed automati-
cally, and the sentences organized according to
the grammatical relation the target verb was in-
volved in. Sentences were excluded from the set
if the target argument was expressed as anaphor,
or was not present in the sentence. The seman-
tic head for the target grammatical relation was
identified in each case.
4. Word sense disambiguation of the target predi-
cate was performed manually on each extracted
sentence, matching the target against the sense
inventory and the corresponding type templates
as described above. The appropriate senses
were then saved into the database along with the
associated type template.
5. The sentences containing coercive senses of the
target verbs were loaded into the Brandeis An-
notation Tool (Verhagen, 2010). Annotators
were presented with a list of sentences and
asked to determine whether the argument in
the specified grammatical relation to the target
belongs to the type associated with that sense
in the corresponding template. Disagreements
were resolved by adjudication.
29
Coerion Type Verb Train Test
EVENT?LOCATION arrive at 38 37
ARTIFACT?EVENT cancel 35 35
finish 91 92
EVENT?PROPOSITION deny 56 54
ARTIFACT?SOUND hear 28 30
EVENT?SOUND hear 24 26
DOCUMENT?EVENT finish 39 40
Table 1: Coercions in the English data set
6. To guarantee robustness of the data, two addi-
tional steps were taken. First, only the six most
recurrent coercion types were selected; these
are given in table 1. Preference was given to
cross-domain coercions, where the source and
the target types are not related ontologically.
Second, the distribution of selection and co-
ercion instances were skewed to increase the
number of coercions. The final English data set
contains about 30% coercions.
7. Finally, the data set was randomly split in half
into a training set and a test set. The training
data has 1032 instances, 311 of which are co-
ercions, and the test data has 1039 instances,
314 of which are coercions.
4.2 Data Set Construction Phase: Italian
In constructing the Italian data set, we adopted the
same methodology used for the English data set,
with the following differences:
1. The list of coercive verbs was selected by exam-
ining data from the ItWaC (Baroni and Kilgar-
riff, 2006) using the Sketch Engine (Kilgarriff
et al, 2004):
accusare ?accuse?, annunciare ?announce?, arrivare ?ar-
rive?, ascoltare ?listen?, avvisare ?inform?, chiamare
?call?, cominciare ?begin?, completare ?complete?, con-
cludere ?conclude?, contattare ?contact?, divorare ?de-
vour?, echeggiare ?echo?, finire ?finish?, informare ?in-
form?, interrompere ?interrupt?, leggere ?read?, raggiun-
gere ?reach?, recar(si) ?go to?, rimbombare ?resound?,
sentire ?hear?, udire ?hear?, visitare ?visit?.
2. The coercive senses of the chosen verbs were
associated with type templates, some of which
are listed listed below. Whenever possible,
senses and type templates were adapted from
the Italian Pattern Dictionary (Hanks and Jezek,
2007) and mapped to their SIMPLE equiva-
lents (Lenci et al, 2000).
a. arrivare, sense reach a location: HUMAN arriva
[prep] LOCATION
b. cominciare, sense initiate an undertaking: HUMAN
comincia EVENT
c. completare, sense finish an activity: HUMAN com-
pleta EVENT
d. udire, sense perceive a sound : HUMAN ode SOUND
e. visitare, sense visit a place: HUMAN visita LOCA-
TION
The following types were used to annotate
the Italian dataset:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY,
EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, IN-
FORMATION, LIQUID, LOCATION, ORGANIZATION,
PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND,
TIME PERIOD, VEHICLE
The annotators were provided with a set of def-
initions and examples of each type.
3. A set of sentences for each target verb was ex-
tracted and parsed from the PAROLE sottoin-
sieme corpus (Bindi et al, 2000). They were
skimmed to ensure that the final data set con-
tained a sufficient number of coercions, with
proportionally more selections than coercions.
Sentences were preselected to include instances
representing one of the chosen senses.
4. In order to exclude instances that may have been
wrongly selected, a judge performed word sense
disambiguation of the target predicate in the ex-
tracted sentences.
5. Annotators were presented with a list of sen-
tences and asked to determine the usual seman-
tic type associated with the argument in the
specified grammatical relation. Every sentence
was annotated by two annotators and one judge,
who resolved disagreements.
6. Some of the coercion types selected for Italian
were:
a. LOCATION? HUMAN (accusare, annunciare)
b. ARTIFACT? HUMAN (annunciare, avvisare)
c. EVENT? LOCATION (arrivare, raggiungere)
d. ARTIFACT? EVENT (cominciare, completare)
e. EVENT? DOCUMENT (leggere, divorare)
f. HUMAN? DOCUMENT (leggere, divorare)
g. EVENT? SOUND (ascoltare, echeggiare)
h. ARTIFACT? SOUND (ascoltare, echeggiare)
7. The Italian training data contained 1466 in-
stances, 381 of which are coercions; the test
data had 1463 instances, with 384 coercions.
5 Data Format
The test and training data were provided in XML.
The relation between the predicate (viewed as
a function) and its argument were represented
by composition link elements (CompLink), as
30
shown below. The test data differed from the train-
ing data in the omission of CompLink elements.
In case of coercion, there is a mismatch between
the source and the target types, and both types
need to be identified; e.g., The State Department
repeatedly denied the attack:
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
the <TARGET id="t1">attack</TARGET>.
<CompLink cid="cid1"
compType="COERCION"
selector_id="s1"
relatedToTarget="t1"
sourceType="EVENT"
targetType="PROPOSITION"/>
When the compositional operation is selection,
the source and target types must match; e.g., The
State Department repeatedly denied the statement:
The State Department repeatedly
<SELECTOR sid="s2">denied</SELECTOR>
the <TARGET id="t2">statement</TARGET>.
<CompLink cid="cid2"
compType="SELECTION"
selector_id="s2"
relatedToTarget="t2"
sourceType="PROPOSITION"
targetType="PROPOSITION"/>
6 Results & Analysis
We received only a single submission for the
ASC task. The UTDMet system was an SVM-
based system with features derived from two main
sources: a PageRank-style algorithm over Word-
Net hypernyms used to define semantic classes,
and statistics from a PropBank-style parse of some
8 million documents from the English Gigaword
corpus. The results, shown in Table 2, were
computed from confusion matrices constructed for
each of four classification tasks for the 1039 link
instances in the English test data: determination
of argument selection or coercion, identification of
the argument source type, identification of the ar-
gument target type, and the joint identification of
the source/target type pair.
Clearly, the UTDMet system did quite well at
this task. The one immediately noticeable outlier
is the macro-averaged precision for the joint type,
which reflects a small number of miscategoriza-
tions of rare types. For example, eliminating the
single miscategorized ARTIFACT-LOCATION link
in the submitted test data bumps this score up to
a respectable 94%. This large discrepancy can ex-
plained by the lack of any coercions with those
types in the gold-standard data.
Prec. Recall Averaging
Selection vs. 95 96 (macro)
Coercion: 96 96 (micro)
Source Type: 96 96 (macro)
96 96 (micro)
Target Type: 100 100 (both)
Joint Type: 86 95 (macro)
96 96 (micro)
Table 2: Results for the UTDMet submission.
In the absence of any other submissions, it is
difficult to provide a point of comparison for this
performance. However, we can provide a base-
line by taking each link to be a selection whose
source and target types are the most common type
(EVENT for the gold-standard English data). This
yields micro-averaged precision scores of 69% for
selection vs. coercion, 33% for source type iden-
tification, 37% for the target type identification,
and 22% for the joint type.
The performance of the UTDMet system sug-
gests that most of the type coercions were identifi-
able based largely on examination of lexical clues
associated with selection contexts. This is in fact
to be expected for the type coercions that were the
focus of the English data set. It will be interesting
to see how systems perform on the Italian data set
and an expanded corpus for English and Italian,
where more subtle and complex type exploitations
and manipulations are at play. These will hope-
fully be explored in future competitions.
7 Conclusion
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2010. This
task involves identifying the relation between a
predicate and its argument as one that encodes
the compositional history of the selection process.
This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of
a predicate from those that are coerced in context.
We described some details of a specification lan-
guage for selection, the annotation task using this
specification to identify argument selection behav-
ior, and the preparation of the data for the task.
Finally, we analyzed the results of the task sub-
missions.
31
References
M. Baroni and A. Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In Proceedings of European ACL.
R. Bindi, P. Baroni, M. Monachini, and E. Gola. 2000.
PAROLE-Sottoinsieme. ILC-CNR Internal Report.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC,
Genoa, Italy.
L. Burnard, 1995. Users? Reference Guide, British Na-
tional Corpus. British National Corpus Consortium,
Oxford, England.
Marcus Egg. 2005. Flexible semantics for reinterpre-
tation phenomena. CSLI, Stanford.
P. Hanks and E. Jezek. 2007. Building Pattern Dictio-
naries with Corpus Analysis. In International Col-
loquium on Possible Dictionaries, Rome, June, 6-7.
Oral Presentation.
P. Hanks and J. Pustejovsky. 2005. A pattern dic-
tionary for natural language processing. Revue
Franc?aise de Linguistique Appliqu?ee.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpre-
tation as abduction. Artificial Intelligence, 63:69?
142.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105?116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, Univer-
sity of Pennsylvania, PA.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowski, I. Peters, W. Peters,
N. Ruimy, et al 2000. SIMPLE: A general frame-
work for the development of multilingual lexicons.
International Journal of Lexicography, 13(4):249.
K. Markert and M. Nissim. 2007. SemEval-2007
task 8: Metonymy resolution. In Eneko Agirre,
Llu??s M`arquez, and Richard Wicentowski, editors,
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31.
Geoffrey Nunberg. 1979. The non-uniqueness of se-
mantic solutions: Polysemy. Linguistics and Phi-
losophy, 3:143?184.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, E. Hovy, MS Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In
International Conference on Semantic Computing,
2007, pages 517?526.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924?931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123?164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth Interna-
tional Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Compu-
tational Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy
in verbs: systematic relations between senses and
their effect on annotation. In COLING Workshop
on Human Judgement in Computational Linguistics
(HJCL-2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Carlos Subirats. 2004. FrameNet Espa?nol. Una red
sem?antica de marcos conceptuales. In VI Interna-
tional Congress of Hispanic Linguistics, Leipzig.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
32
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 33?41
Manchester, August 2008
Polysemy in verbs: systematic relations between senses
and their effect on annotation
Anna Rumshisky
?Dept. of Computer Science
Brandeis University
Waltham, MA USA
arum@cs.brandeis.edu
Olga Batiukova??
?Dept. of Spanish Philology
Madrid Autonomous University
Madrid, Spain
volha.batsiukova@uam.es
Abstract
Sense inventories for polysemous predicates
are often comprised by a number of related
senses. In this paper, we examine different
types of relations within sense inventories and
give a qualitative analysis of the effects they
have on decisions made by the annotators and
annotator error. We also discuss some common
traps and pitfalls in design of sense inventories.
We use the data set developed specifically for
the task of annotating sense distinctions depen-
dent predominantly on semantics of the argu-
ments and only to a lesser extent on syntactic
frame.
1 Introduction
Lexical ambiguity is pervasive in natural language, and
its resolution has been used to improve performance of
a number of natural language processing (NLP) appli-
cations, such as statistical machine translation (Chan
et al, 2007; Carpuat and Wu, 2007), cross-language
information retrieval and question answering (Resnik,
2006). Sense differentiation for the predicates depends
on a number of factors, including syntactic frame, se-
mantics of the arguments and adjuncts, contextual clues
from the wider context, text domain identification, etc.
Preparing sense-tagged data for training and evalua-
tion of word sense disambiguation (WSD) systems in-
volves two stages: (1) creating a sense inventory and
(2) applying it in annotation. Creating sense invento-
ries for polysemous words is a task that is notoriously
difficult to formalize. For polysemous verbs especially,
constellations of related meanings make this task even
more difficult. In lexicography, ?lumping and splitting?
senses during dictionary construction ? i.e. deciding
when to describe a set of usages as a separate sense
? is a well-known problem (Hanks and Pustejovsky,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2005; Kilgarriff, 1997). It is often resolved on an ad-
hoc basis, resulting in numerous cases of ?overlapping
senses?, i.e. instances when the same occurrence may
fall under more than one sense category simultaneously.
This problem has also been the subject of extensive
study in lexical semantics, addressing questions such
as when the context selects a distinct sense and when
it merely modulates the meaning, what is the regular
relationship between related senses, and what composi-
tional processes are involved in sense selection (Puste-
jovsky, 1995; Cruse, 1995; Apresjan, 1973). A num-
ber of syntactic and semantic tests are traditionally ap-
plied for sense identification, such as examining syn-
onym series, compatible syntactic environments, coor-
dination tests such as cross-understanding or zeugma
test (Cruse, 2000). None of these tests are conclu-
sive and normally a combination of factors is used.
At the recent Senseval competitions (Mihalcea et al,
2004; Snyder and Palmer, 2004; Preiss and Yarowsky,
2001), the choice of sense inventories frequently pre-
sented problems, spurring the efforts to create coarser-
grained sense inventories (Hovy et al, 2006; Palmer et
al., 2007; Navigli, 2006).
Part of the reason for such difficulties in establish-
ing a set of senses available to a lexical item is that
the meaning of a polysemous verb is often determined
in composition and depends to the same extent on se-
mantics of the particular arguments as it does on the
base meaning of the verb itself. A number of system-
atic relations often holds between different senses of a
polysemous verb. Depending on the kind of ambiguity
involved in each case, some senses are easier to dis-
tinguish than others. Sense-tagged data (e.g. SemCor
(Landes et al, 1998), PropBank (Palmer et al, 2005),
OntoNotes (Hovy et al, 2006)) typically provides no
way to differentiate between sense distinctions moti-
vated by different factors. Treating different disam-
biguation factors separately would allow one to exam-
ine the contribution of each factor, as well as the success
of a given algorithm in identifying the corresponding
senses.
Within the scope of a sentence, syntactic frame and
semantics of the arguments are most prominent in sense
33
disambiguation. The latter is often more subtle and
hence complex. Our goal in the present study was to tar-
get sense distinctions motivated strongly or exclusively
by differences in argument semantics. We base the
present discussion on the sense-tagged data set we de-
veloped for 20 polysemous verbs. We argue below that
cases which can not be reliably disambiguated by hu-
mans introduce noise into the data and therefore should
be kept out, a principle adhered to in the design of this
data set.
The choice of argument semantics as the target dis-
ambiguation factor was motivated by several consider-
ations. In automatic sense detection systems, argument
semantics is often represented using external resources
such as thesauri or shallow ontologies. Sense induction
systems using distributional information often do not
take into account the possible implications of induced
word clusters for sense disambiguation. Our goal was
to analyze differences in argument semantics that con-
tribute to disambiguation.
In this paper, we discuss different kinds of systematic
relations observed between senses of polysemous pred-
icates and examine the effects they have on decisions
made by the annotators. We also examine sense in-
ventories for other factors that influence inter-annotator
agreement rates and lead to annotation error. In Section
2, we discuss some of the factors that influence com-
pilation of sense inventories and the methodology in-
volved. In Section 3, we describe briefly the data set
and the annotation task. In Sections 4 and 5, we discuss
the relations observed between different senses within
sense inventories in our data set, their effect on deci-
sions made by the annotators, and the related annotation
errors.
2 Defining A Sense Inventory
Several current resource-oriented projects undertake to
formalize the procedure of identifying a word sense.
FrameNet (Ruppenhofer et al, 2006) attempts to orga-
nize lexical information in terms of script-like semantic
frames, with semantic and syntactic combinatorial pos-
sibilities specified for each frame-evoking lexical unit
(word/sense pairing). Semantics of the arguments is
represented by Fillmore?s case roles (frame elements)
which are derived on ad-hoc basis for each frame.
In OntoNotes project, annotators use small-scale cor-
pus analysis to create sense inventories derived by
grouping together WordNet senses. The procedure is
restricted to maintain 90% inter-annotator agreement
(Hovy et al, 2006).
Corpus Pattern Analysis (CPA) (Hanks and Puste-
jovsky, 2005; Pustejovsky et al, 2004) attempts to cat-
alog prototypical norms of usage for individual words,
specifying them in terms of context patterns. As a cor-
pus analysis technique, CPA has its origins in the anal-
ysis of large corpora for lexicographic purposes, of the
kind that was used for compiling the Cobuild dictionary
(Sinclair and Hanks, 1987). Each pattern gives a com-
bination of surface textual clues and argument specifi-
cations. A lexicographer creates a set of patterns by
sorting a concordance for the target predicate according
to the context features. In the present study, we use a
modification of the CPA technique in the way explained
in Section 3.
In CPA, syntactic and textual clues include argu-
ment structure and minor syntactic categories such as
locatives and adjuncts; collocates from wider context;
subphrasal cues such as genitives, partitives, bare plu-
ral/determiner, infinitivals, negatives, etc. Semantics
of the arguments is represented either through a set of
shallow semantic types corresponding to basic seman-
tic features (e.g. Person, Location, PhysObj, Abstract,
Event, etc.) or extensionally through lexical sets, which
are effectively collections of lexical items.1
Several CPA patterns may correspond to a single
sense. The patterns vary in syntactic structure or the en-
coding of semantic roles relative to the described event.
For example, for the verb treat, DOCTOR treating PA-
TIENT and DOCTOR treating DISEASE both correspond
to the medical sense of treat. Knowing which seman-
tic role is expressed by a particular argument is often
useful for performing inference. For instance, treating
a disease eliminates the disease, but not the patient. In
the present annotation task, each pattern is viewed as
sense in construction and labeled as a separate sense.
In the rest of the paper, we will use the term ?sense? to
refer also to such microsenses.
For the cases where sense differentiation depends
strongly on differences in semantics of the arguments,
several factors further complicate creating a sense in-
ventory. Prototypicality as a general principle of cat-
egory organization seems to play an important role in
defining both the boundaries of senses and the corre-
sponding argument groupings. The same sense of the
predicate is often activated by a number of semantically
diverse arguments. Such argument sets are frequently
organized around a core of typical members that are
a ?good fit? with respect to semantic requirements of
the corresponding sense of the target. The relevant se-
mantic feature is prominent for them, while other, more
peripheral members of the argument set, merely allow
the relevant interpretation (see Rumshisky (2008) for
discussion). For example, the verb absorb has a sense
involving absorbing a substance, and the typical mem-
bers of the corresponding argument set would be actual
substances, such as oil, oxygen, water, air, salt, etc. But
goodness, dirt, flavor, moisture would also activate the
same sense.
Each decision to split a sense and make another cat-
egory is to a certain extent an arbitrary decision. For
example, for the verb absorb, one can separate absorb-
ing a substance (oil, oxygen, water, air, salt) from ab-
sorbing energy (radiation, heat, sound, energy). The
latter sense may or may not be separated from absorb-
1See Rumshisky et al (2006) and Pustejovsky et al (2004)
for more detail.
34
ing impact (blow, shock, stress). But it is a marked con-
tinuum, i.e. certain points in the continuum are more
prominent, with necessity of a given concept reflected
in the frequency of use.
When several senses are postulated based on argu-
ment distinctions, there are almost always boundary
cases that can be seen to belong to both categories.
Consider, for example, two senses defined for the verb
launch and the corresponding direct objects in (1):
(1) a. Physically propel an object into the air or water
missile, rocket, torpedo, satellite, shuttle, craft
b. Begin or initiate an endeavor
campaign, initiative, investigation, expedition, drive,
competition, crusade, attack, assault, inquiry
The senses seem to be very clearly separated, yet ex-
amples like launch a ship clearly fall on the bound-
ary: while ships are physical objects propelled into wa-
ter, launching a ship can be virtually synonymous with
launching an expedition.
Similarly, for the verb conclude, two senses below
which are linked to nominal complements are clearly
separated:
(2) a. finish
meeting, debate, investigation, visit, tour, discussion;
letter, chapter, novel
b. reach an agreement
treaty, agreement, deal, contract, truce, alliance,
ceasefire, sale
However, conclude negotiations is clearly a boundary
case where both interpretations are equally possible
(negotiations may be concluded without reaching an
agreement). In fact, the two annotators chose different
senses for this example:2
(3) We were able to operate under a lease agreement until
purchase negotiations were concluded.
annoA: finish
annoB: reach an agreement
In many cases, postulating a separate sense for a co-
herent set of nominal complements is not justified, as
there are regular semantic processes that allow the com-
plements to satisfy selectional requirements of the verb.
For example, the verb conclude, in the finish sense ac-
cepts EVENT complements. Therefore, nouns such as
letter, chapter, novel in (2) must be coerced into events
corresponding to the activity that typically brings them
about, that is, re-interpreted as events of writing (their
Agentive quale, cf. Pustejovsky (1995)). Similarly, the
verb deny in the first sense (state or maintain that some-
thing is untrue) accepts PROPOSITION complements:
(4) a. state or maintain that something is untrue
allegations, reports, rumour; significance, impor-
tance, difference; attack, assault, involvement
b. refuse to grant something
access, visa, approval, funding, license
2All examples are taken from the annotated data set.
In some cases, sentence structure was slightly modified for
brevity.
Event nouns such as attack and assault are coerced into
a propositional reading, as are relational nouns such as
significance and importance.
Interestingly, as we have noted before (Rumshisky
et al, 2006), each predicate imposes its own gradation
with respect to prototypicality of elements of the ar-
gument set. As a result, even though basic semantic
types such as PHYSOBJ, ANIMATE, EVENT, are used
uniformly by many predicates, argument sets, while se-
mantically similar, typically differ between predicates.
For example, fall in the subject position and cut in the
direct object position select for things that can be de-
creased:
(5) a. cut (dobj): reduce or lessen
price, inflation, profits, cost, emission, spending,
deficit, wages overhead, production, consumption,
fees, staff
b. fall (subj): decrease
price, inflation, profits, attendance, turnover, temper-
ature, membership, import, demand, level
While there is a clear commonality between these argu-
ment sets, the overlap is only partial. To give another
example, consider INFORMATION-selecting predicates
explain (subj), grasp (dobj) and know (dobj). The nouns
book and note occur in the subject position of explain;
answer occurs both as the subject of explain and direct
object of know; however, grasp accepts neither of these
nouns as direct object. Thus, the actual selectional be-
havior of the predicates does not seem to be well de-
scribed in terms of a fixed set of types, which is what
is typically assumed by many ontologies used in auto-
matic WSD.
3 Task Description
We were interested specifically in those cases where
disambiguation needs to be made without relying on
syntactic frame, and the main source of disambiguation
is semantics of the arguments. Such cases are harder
to identify formally in the development of sense inven-
tories and harder for the annotators to determine. For
example, phrasal verbs or idiomatic constructions that
help identify a particular sense were intentionally ex-
cluded from our data set. Thus, for the verb cut, one of
the senses involves cutting out a shape or a form (e.g.
cut a suit), but the sentences with the corresponding
phrasal form cut out were thrown out.
Even so, syntactic clues that contribute to disam-
biguation in some cases overrule the interpretation sug-
gested by the argument. For example, for the verb deny,
in deny the attack, the direct object strongly suggests
a propositional interpretation for deny (that the attack
didn?t happen). However, the use of ditransitive con-
struction (indicated in the example below by the past
participle) overrules this interpretation, and we get the
refuse to grant sense:
(6) Astorre, denied his attack, had stayed in camp, uneasily
brooding.
35
In fact, during the actual annotation, one of the anno-
tators did not recognize the use of past participle, and
erroneously assigned the state or maintain something to
be untrue sense to this sentence.
3.1 Data set
The data set was developed using the British National
Corpus (BNC), which is more balanced than the more
commonly annotated Wall Street Journal data. We se-
lected 20 polysemous verbs with sense distinctions that
were judged to depend for disambiguation on seman-
tics of the argument in several argument positions, in-
cluding direct object (dobj), subject (subj), or indirect
object within a prepositional phrase governed by with
(iobj with):
dobj: absorb, acquire, admit, assume, claim, conclude,
cut, deny, dictate, drive, edit, enjoy, fire, grasp, know,
launch
subj: explain, fall, lead
iobj with: meet
We used the Sketch Engine (Kilgarriff et al, 2004)
both to select the verbs and to aid the creation of the
sense inventories. The Sketch Engine is a lexicographic
tool that lists collocates that co-occur with a given target
word in the specified grammatical relation. The collo-
cates are sorted by their association score with the tar-
get.
A set of senses was created for each verb using a
modification of the CPA technique (Pustejovsky et al,
2004). A set of complements was examined in the
Sketch Engine. If a clear division was observed be-
tween semantically different groups of collocates in a
certain argument position, the verb was selected. For
semantically distinct groups of collocates, a separate
sense was added to the sense inventory for the target.
For example, for the verb acquire, a separate sense was
added for each of the following sets of direct objects:
(7) a. Take on certain characteristics
shape, meaning, color, form, dimension, reality, sig-
nificance, identity, appearance, characteristic, flavor
b. Purchase or become the owner of property
land, stock, business, property, wealth, subsidiary, es-
tate, stake
The sense inventory for each verb was cross-checked
against several resources, including WordNet, Prop-
Bank, Merriam-Webster and Oxford English dictionar-
ies, and existing correspondences in FrameNet (Rup-
penhofer et al, 2006; Hiroaki, 2003), OntoNotes (Hovy
et al, 2006),3 and CPA patterns (Hanks and Puste-
jovsky, 2005; Rumshisky and Pustejovsky, 2006; Puste-
jovsky et al, 2004).
We performed test annotation on 100 instances, with
the sense inventory additionally modified upon exam-
ining the results of the annotation. This sense inven-
tory was provided to two annotators, along with 200
3Sense inventories released for the 65 verbs made avail-
able for SemEval-2007.
sentences for each verb. Each sentence was pre-parsed
with RASP (Briscoe and Carroll, 2002), and the head
of the target argument phrase was identified. Misparses
were manually corrected in post-processing.
3.2 Defining the task for the annotators
Data set creation for a WSD task is notoriously hard (cf.
Palmer et al (2007)), as the annotators are frequently
forced to perform disambiguation on sentences where
no disambiguation can really be performed. This is the
case, for example, for overlapping senses, where more
than one sense is activated simultaneously (Rumshisky,
2008; Pustejovsky and Boguraev, 1993). The goal was
to create, for each target word, a set of instances where
humans had no trouble disambiguating between differ-
ent senses.
Two undergraduate linguistics majors served as an-
notators. The annotators were instructed to mark each
sentence with the most fitting sense. The annotators
were allowed to mark the sentence as ?N/A? and were
instructed to do so if (i) the sense inventory was missing
the relevant sense, (ii) more than one sense seemed to
fit, or (iii) the sense was impossible to determine from
the context.
With respect to metaphoric senses, instructions were
to throw out cases of creative use where the interpreta-
tion was difficult or not immediately clear. The cases
where the target grammatical relation was actually ab-
sent from the sentence also had to be marked as ?N/A?
(e.g. for fire, sentences without direct object, e.g. a
stolen car was fired upon). The annotators were also
instructed to mark idiomatic expressions and phrasal
verbs as ?N/A?, e.g. for the verb fall: fall from favor,
fall through, fall in, fall back, fall silent, fall short, fall
in love.
Disagreements between the annotators were resolved
in adjudication by the co-authors. The average inter-
annotator agreement (ITA) for our data set was com-
puted as a macro-average of the percentage of instances
that were annotated with the same sense by both anno-
tators to the total number of instances retained in the
data set for each verb. The instances that were marked
as ?N/A? by one of the annotators (or thrown out during
the adjudication) were not included in the computation.
The ITA value for our data set was 95%. However, as
we will see below, the ITA values do not always reflect
the actual accuracy of annotation, due to some common
problems with sense inventories.
3.3 Glossing a sense
A very common problem with glossing a sense in-
volves the situation where a sense inventory includes
two senses one of which is an extension of the other.
The derived sense may be related to the primary sense
through metaphor, and this often results in the for-
mer taking on a semantically less specific interpreta-
tion. The problem with creating glosses in this situa-
tion is that the words used may have sense distinctions
36
parallel to the ones in the target verb being described.
This leaves the annotators free to choose either sense.
This seems to be the case, for example, with OntoNotes
sense inventory for fire, where ignite or become ignited
is the gloss under which very divergent examples are
grouped: oil fired the furnace (literal, primary sense)
and curiosity fired my imagination (metaphoric exten-
sion). Clearly, annotators were having a problem with
this sense due to the fact that the verb ignite has sense
distinctions which are based on the same metaphor (fire
= inspire) and therefore are very similar to those of the
verb fire.
In case of semantic underspecification, annotators
may be left free to choose the more generic sense,
which contaminates the data set while not being re-
flected in the inter-annotator agreement values. For ex-
ample, in our sense inventory for acquire, the gloss for
acquire a new customer has to be very generic. We
used the gloss ?become associated with something, of-
ten newly brought into being?. However, that led the
annotators to overuse this gloss and select this sense in
cases where a more specific gloss was more appropri-
ate:4
(8) By this treaty, Russia acquired a Black Sea coastline.
annoA: become associated with something, often newly
brought into being
annoB: become associated with something, ...
correct: purchase or become the owner of property
For a more detailed analysis of this phenomenon, see
Section 5.
4 Relations Between Senses
In this section, we discuss linguistic processes underly-
ing relations between senses within a single sense in-
ventory. We believe that a detailed analysis of these
processes should help to account for the annotator?s
ability to perform disambiguation. Some sense distinc-
tions appear more striking to the annotators, depending
on the type of relation involved.
In line with existing approaches to sense relations,
we will look at both the linguistic structures involved
in sense modification and the productive processes act-
ing on linguistic structures. For the purposes of our
present discussion, we interpret the literal (physical, di-
rect) senses to be primary, with respect to more abstract
or metaphorical senses.
4.1 Argument structure alternations
Some of the most striking differences between the
senses are related to the argument structure alternations:
1. Different case roles (frame elements) may be ex-
pressed in the same argument position (in this case, di-
rect object), corresponding to different perspectives on
the same event. For example, direct object position of
the verb drive may be filled by VEHICLE, DISTANCE,
4We will refer to annotators A and B as annoA and annoB.
or PHYSOBJ giving rise to three distinct senses: (i) op-
erate a vehicle controlling its motion, (ii) travel in a ve-
hicle a certain distance, and (iii) transport something or
someone. Similarly, for the verb fire, PROJECTILE or
WEAPON in direct object position give rise to two re-
lated senses: (i) shoot, discharge a weapon, (ii) shoot,
propel a projectile.
2. The distinction between propositional and non-
propositional complements, as for the verbs admit and
deny in (9) and (10):
(9) a. admit defeat, inconsistency, offense
(acknowledge the truth or reality of )
b. admit patients, students
(grant entry or allow into a community)
(10) a. deny reports, importance, allegations
(state or maintain to be untrue)
b. deny visa, access
(refuse to grant)
3. There is a mutual dependency between subcate-
gorization features of the complements in different ar-
gument positions. For example, the [+animate] subject
may combine with specific complements not available
for [?animate], as for the two senses of acquire: (i)
learn and (ii) take on certain characteristics. Compare
NP
subj
[-animate] acquire NP
dobj
(language, man-
ners, knowledge, skill) vs. NP
subj
[?animate] acquire
NP
dobj
(importance, significance). Similarly, for ab-
sorb, compare NP
subj
[?animate] absorb NP
dobj
(sub-
stance) and NP
subj
[+animate] absorb NP
dobj
(skill,
information). Note that, as one would expect, such de-
pendencies are inevitable even despite the fact that our
data set was developed specifically to target sense dis-
tinctions dependent on a single argument position.
4.2 Event structure modification
Event structure modifications (i.e. operations affecting
aspectual properties of the predicate) are another source
of sense differentiation. Two cases appear most promi-
nent:
1. The event structure is modified along with the
characteristics of the arguments. For example, for en-
joy, compare enjoy skiing, vacation (DYNAMIC EVENT)
with enjoying a status (STATE). Similarly, for lead,
compare a person leads smb somewhere (PROCESS) vs.
a road (PATH) leads somewhere (STATE); for explain,
compare something or somebody explains smth (= clar-
ifies, describes, makes comprehensible, PROCESS) vs.
something [?inanimate, +abstract] explains something
(= is a reason for something, STATE); for fall, compare
PHYSOBJ falls (TRANSITION or ACCOMPLISHMENT)
vs. a case falls into a certain category (STATE).
2. The aspectual nature of the predicate is the only
semantically relevant feature that remains unchanged
after consecutive sense modifications. For example, the
ingressive meaning of ?beginning something? is pre-
served in shifting from the physical sense of the verb
launch in launch a missile to launch a campaign and
launch a product.
37
4.3 Lexical semantic features
Sense distinctions often involve deeper semantic char-
acteristics of the verbs which could be accounted for by
means of lexical semantic features such as qualia struc-
ture roles in Generative Lexicon (Pustejovsky, 1995):5
1. Consider how the meaning component ?manner
of motion? (typically associated with the agentive role)
gets transformed in the different senses of drive. It is
obviously present in the physical uses of drive (such
as operate a vehicle, transport something or somebody,
etc.), but is completely lost in motivate the progress
of (as in drive the economy, drive the market forward,
etc.). The value of the agentive role of drive becomes
underspecified or semantically weak, so that the overall
meaning of drive is transformed to cause something to
move.
2. Information about semantic type contained in
qualia structure allows apparently diverse elements to
activate the same sense of the verb. For instance, the
verb absorb in the sense learn or incorporate skill or
information occurs with direct objects such as values,
atmosphere, information, idea, words, lesson, attitudes,
culture. The requisite semantic component is realized
differently for each of these words. Some of them are
complex types6 with INFORMATION as one of the con-
stituent types: words (ACOUSTIC/VISUAL ENTITY ?
INFO), lesson (EVENT ? INFO). Others, such as idea,
are polysemous, with one of the senses being INFOR-
MATION. Cases like culture and values are more diffi-
cult, but since they refer to knowledge, the INFORMA-
TION component is clearly present. Consequently, the
annotators are able to identify the corresponding sense
of absorb with a high degree of agreement.
4.4 Metaphor and metonymy
The processes causing the mentioned meaning trans-
formations in our corpus often involve metaphor and
metonymy. Below are some of the conventionalized ex-
tensions with metaphorical flavor:
(11) a. grasp object vs. grasp meaning
b. launch object vs. launch an event (campaign, as-
sault) or launch a product (newspaper, collection)
c. meet with a person vs. meet with success, resistance
d. lead somebody somewhere vs. lead to a consequence
Note that these metaphorical extensions involve ab-
stract or continuous objects (meaning, assault, success,
consequence), which in turn cause event structure mod-
ifications (lead as a process vs. lead as a state). Thus,
the processes and structures we are dealing with are
clearly interrelated.
The metonymical process can be exemplified by edit
as make changes to the text and as supervise publica-
5We will use the terminology from Generative Lexicon
(Pustejovsky, 1995; Pustejovsky, 2007) to discuss lexical se-
mantic properties, such as qualia roles, complex and func-
tional types, and so on.
6Complex type is a term used for concepts that inherently
refer to more than one semantic type.
tion, which are in a clear contiguity relationship.
One of the effects of the metaphorization and pro-
gressive emptying of the primary (physical, concrete)
senses is the distinction between generic and specific
senses. For example, compare acquire land, business
(specific sense) to acquire an infection, a boyfriend, a
following, which refers to some extremely light generic
association. Similar process is observed for the seman-
tically weak sense of fall, be associated with or get as-
signed to a person or location or for event to fall onto a
time:
(12) Birthdays, lunches, celebrations fall on a certain date or
time
Stress or emphasis fall on a given topic or a syllable
Responsibility, luck, suspicion fall on or to a person
The specificity often involves specialization within a
certain domain:
(13) a. conclude as finish vs. conclude as reach an agree-
ment (Law, Politics)
b. fire as shoot a weapon or a projectile vs. fire as kick
or pass an object of play in sports (Sport)
Thus, when concluding a pact or an agreement, a cer-
tain EVENT is also being finished (negotiation of that
agreement), necessarily with a positive outcome.
In the following section, we will try to show how dif-
ferent kinds of relations between senses influence dis-
ambiguation carried out by the annotators. In particular,
we look at different sources of disagreement and anno-
tator error as determined in adjudication.
5 Analysis of Annotation Decisions
As we have seen above, in many cases disambigua-
tion is impossible due to the nature of compositional-
ity. Also, as there are no clear answers to a number of
questions concerning sense identification, the annota-
tors deal with sense inventories that are imperfect. Re-
sults of the disambiguation task carried out by the an-
notators reflect all these defects.
In cases when a specific meaning from the data set
is not included into the sense inventory (e.g. due to its
low frequency or extreme fine-grainedness) the annota-
tors may use a more general meaning or pick the clos-
est meaning available. For example, within the sense
inventory for fire, there was no separate gloss for fire an
engine. Annotator A in our experiment chose the clos-
est specific meaning available, and Annotator B marked
it with a more generic sense:
(14) Engineers successfully fired thrusters to boost the re-
search satellite to an altitude of 507 km.
annoA: shoot, propel a projectile
annoB: apply fire to
As mentioned in Section 3.3, even when the appropriate
specific sense is available, annotators frequently chose
the more generic sense in its place, as in (15), (16) and
(17), and also in (8).
38
(15) Several referrals fell into this category.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: be categorized as or fall into a range
(16) The terrible silence had fallen.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: for a state (such as darkness or silence) to come,
to commence
(17) He acquired a taste for performing in public.
annoA: become associated with something, often
newly brought into being
annoB: become associated with something, ...
correct: learn
Note that in (8) this decision was probably motivated by
the annotators? uncertainty about the semantic ascrip-
tion of the relevant argument (coastline is not a proto-
typical owned property). The generic sense seems to be
the safest option to take for the annotators, as compared
to taking a chance with a specific meaning. Due to its
low degree of semantic specification, the generic sense
is potentially able to embrace almost every possible use.
This is not a desirable outcome because the generic
senses are introduced in the inventory to account only
for semantically underspecified cases. For instance, be-
come associated with something, often newly brought
into being is appropriate for acquire a grandchild, but
not for acquire a taste or acquire a proficiency.
Remarkable variation is also observed with respect to
non-literal uses as discussed in Section 4.4. For exam-
ple, in (18) and (19) abstract NPs panic and imbalance
of forces are equated with energy or impact by one an-
notator and with substance by the other.
(18) Her panic was absorbed by his warmth.
annoA: absorb energy or impact
annoB: absorb substance
(19) Alternatively, imbalance of forces can be absorbed into
the body.
annoA: absorb energy or impact
annoB: absorb substance
In some cases, the literal and the metaphoric senses
are activated simultaneously resulting in ambiguity (cf.
Cruse (2000)):
(20) For over 300 years this waterfall has provided the en-
ergy to drive the wheels of industry.
annoA: motivate the progress of
annoB: provide power for or physically move a mech-
anism
(21) But fashion changed and the short skirt fell ? literally ?
from favour and started skimming the ankles.
annoA: lose power or suffer a defeat
annoB: N/A
(22) She was delighted when the story of Hank fell into her
lap.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: physically drop; move or extend downward
Impact of subcategorization features on disam-
biguation (cf. Section 4.1 para 3) is illustrated in (23).
(23) The reggae tourist can easily absorb the current reggae
vibe.
annoA: absorb energy or impact
annoB: learn or incorporate skill or information
Both interpretations chosen here (absorb energy or im-
pact and learn or incorporate skill or information) were
possible due to the animacy of the subject, which acti-
vates two different subcategorization frames and subse-
quently two different senses.
Typically, cases where semantic type of the relevant
arguments (cf. Section 4.3 para 2) is not clear result in
annotator disagreement:
(24) The AAA launched education programs.
annoA: begin or initiate an endeavor (EVENT)
annoB: begin to produce or distribute; start a company
(PRODUCT)
(25) France plans to launch a remote-sensing vehicle called
Spot.
annoA: physically propel into the air, water or space
(PHYSOBJ)
annoB: begin to produce or distribute; start a company
(PRODUCT)
The two cases above are interesting in that both pro-
gram and vehicle are ambiguous and can be analyzed
semantically as members of different semantic classes.
This is what the annotators in fact do, and as a result,
ascribe them to different senses. Program can be cate-
gorized as EVENT (?series of steps?) or as INTELLEC-
TUAL ACTIVITY PRODUCT (?document or system of
projects?). It is a complex type, i.e. it is an inherently
polysemous word that represents at least two different
semantic types. Vehicle, in turn, is a functional type:
on the one hand, it represents an entity with certain for-
mal properties (PHYSOBJ interpretation), on the other
hand, it is an artifact, with a prominent practical pur-
pose (PRODUCT interpretation).
In fact, most problems the annotators had with the
task are due to the inherent semantic complexity of
words such as vehicle and program in (24) and (25) and
to the existence of boundary cases, where the relevant
noun does not properly belong to one or another seman-
tic category. This is the case with panic, imbalance or
reggae vibe in (18), (19), and (23), and also with taste
and coastline in (17) and (7).
In some of these cases, other contextual clues may
come into play and tip the balance in favor of one or an-
other sense. Note that disambiguation was influenced
by a wider context even despite the intentionally re-
strictive task design (targeting a particular syntactic re-
lation for each verb). For instance, in (26), domain-
specific clues referring to war or military conflict (such
as rebel control) could have motivated Annotator B?s
decision to ascribe it to the sense lose power or suffer
a defeat (even though a road is not typically an entity
that can lose power), while the other annotator chose a
more generic meaning:
39
(26) The road fell into rebel control.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: lose power or suffer a defeat
Other pragmatic and discourse-oriented clues played
a role, in particular, positive and negative connotation
of the senses and the relevant arguments, as well as
the temporal organization of discourse. For example, in
(27) and (28), positive or neutral interpretation of wave
of immigrants and change could have led to the choice
of take in or assimilate and learn or incorporate skill or
information senses, while the negatively-colored inter-
pretation might explain the choice of the bear the cost
of sense.
(27) ..help absorb the latest wave of immigrants.
annoA: bear the cost of; take on an expense
annoB: take in or assimilate, making part of a whole or
a group
(28) For senior management an important lesson was the
trade unions? capacity to absorb change and to become
its agents.
annoA: learn or incorporate skill or information
annoB: bear the cost of; take on an expense
Temporal organization of a broader discourse is an-
other important factor. For example, for the verb claim,
the senses claim the truth of and claim property you are
entitled to have different presuppositions with respect
to preexistence of the thing claimed. In (28), due to the
absence of a broader context, the annotators chose two
different temporal reference interpretations. For Anno-
tator B, success was something that has happened al-
ready, while for A this was not clear (success might
have been achieved or not):
(29) One area where the government can claim some success
involves debt repayment.
annoA: come in possession of or claim property you are
entitled to
annoB: claim the truth of
6 Conclusion
We have given a brief overview of different types of
sense relations commonly found in polysemous predi-
cates and analyzed their effect on different aspects of
the annotation task, including sense inventory design
and execution of the WSD annotation.
The present analysis suggests that theoretical tools
must be refined and further developed in order to give
an adequate account to the sense modifications found in
real corpus data. To this end, broader contextual clues
and discourse-oriented clues need to be included in the
analysis.
Semantically annotated corpora are routinely devel-
oped for the training and testing of automatic sense
detection and induction algorithms. But they do not
typically provide a way to distinguish between differ-
ent kinds of ambiguities. Consequently, it is difficult
to perform adequate error analysis for different sense
detection systems. Appropriate semantic annotation
that would allow one to determine which sense dis-
tinctions can be detected better by automatic systems
does not need to be highly specific and unnecessarily
complex, but requires development of robust general-
izations about sense relations.
One obvious conclusion is that data sets need to be
explicitly restricted to the instances where humans have
no trouble disambiguating between different senses.
Thus, prototypical cases can be accounted for reliably,
ensuring the clarity of annotated sense distinctions. At
face value, imposing such restrictions may appear to
negatively influence usability of the resulting data set
in particular applications requiring WSD, such as ma-
chine translation or information retrieval. However, this
decision impacts most strongly those boundary cases
which are not reliably disambiguated by human anno-
tators, and which rather introduce noise into the data
set.
Acknowledgments
This work was supported in part by NSF CRI grant to
Brandeis University. The work of O. Batiukova is sup-
ported by postdoctoral grant of the Ministry of Educa-
tion of Spain and Madrid Autonomous University.
References
Apresjan, Ju. 1973. Regular polysemy. Linguistics,
142(5):5?32.
Briscoe, T. and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), Las Palmas,
Canary Islands, May 2002, pages 1499?1504.
Carpuat, M. and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Proc. of EMNLP-CoNLL, pages 61?72.
Chan, Y. S., H. T. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical machine
translation. In Proc. of ACL, pages 33?40, Prague,
Czech Republic, June.
Cruse, D. A. 1995. Polysemy and related phenom-
ena from a cognitive linguistic viewpoint. In Dizier,
Patrick St. and Evelyne Viegas, editors, Computa-
tional Lexical Semantics, pages 33?49. Cambridge
University Press, Cambridge, England.
Cruse, D. A. 2000. Meaning in Language, an Intro-
duction to Semantics and Pragmatics. Oxford Uni-
versity Press, Oxford, United Kingdom.
Hanks, P. and J. Pustejovsky. 2005. A pattern
dictionary for natural language processing. Revue
Franc?aise de Linguistique Applique?e.
Hiroaki, S. 2003. FrameSQL: A software tool for
FrameNet. In Proceedigns of ASIALEX ?03, pages
251?258, Tokyo, Japan. Asian Association of Lexi-
cography.
40
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 57?60, New York City,
USA, June. Association for Computational Linguis-
tics.
Kilgarriff, A., P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105?116.
Kilgarriff, A. 1997. I don?t believe in word senses.
Computers and the Humanities, 31:91?113.
Landes, S., C. Leacock, and R.I. Tengi. 1998. Build-
ing semantic concordances. In Fellbaum, C., editor,
Wordnet: an electronic lexical database. MIT Press,
Cambridge (Mass.).
Mihalcea, R., T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Mi-
halcea, Rada and Phil Edmonds, editors, Senseval-3:
Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 25?
28, Barcelona, Spain, July. Association for Compu-
tational Linguistics.
Navigli, R. 2006. Meaningful clustering of senses
helps boost word sense disambiguation performance.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 105?112, Sydney, Australia, July. Association
for Computational Linguistics.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Palmer, M., H. Dang, and C. Fellbaum. 2007. Making
fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natu-
ral Language Engineering.
Preiss, J and D. Yarowsky, editors. 2001. Proceedings
of the Second Int. Workshop on Evaluating WSD Sys-
tems (Senseval 2). ACL2002/EACL2001.
Pustejovsky, J. and B. Boguraev. 1993. Lexical knowl-
edge representation and natural language processing.
Artif. Intell., 63(1-2):193?223.
Pustejovsky, J., P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924?931.
Pustejovsky, J. 1995. Generative Lexicon. Cambridge
(Mass.): MIT Press.
Pustejovsky, J. 2007. Type Theory and Lexical De-
composition. In Bouillon, P. and C. Lee, editors,
Trends in Generative Lexicon Theory. Kluwer Pub-
lishers (in press).
Resnik, P. 2006. Word sense disambiguation in NLP
applications. In Agirre, E. and P. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Rumshisky, A. and J. Pustejovsky. 2006. Induc-
ing sense-discriminating context patterns from sense-
tagged corpora. In LREC 2006, Genoa, Italy.
Rumshisky, A., P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
Rumshisky, A. 2008. Resolving polysemy in verbs:
Contextualized distributional approach to argument
semantics. Distributional Models of the Lexicon in
Linguistics and Cognitive Science, special issue of
Italian Journal of Linguistics / Rivista di Linguistica.
forthcoming.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Sinclair, J. and P. Hanks. 1987. The Collins Cobuild
English Language Dictionary. HarperCollins, 4th
edition (2003) edition. Published as Collins Cobuild
Advanced Learner?s English Dictionary.
Snyder, B. and M. Palmer. 2004. The english all-words
task. In Mihalcea, Rada and Phil Edmonds, edi-
tors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text, pages 41?43, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
41
Proceedings of the 8th International Conference on Computational Semantics, pages 169?180,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
GLML: Annotating Argument
Selection and Coercion
James Pustejovsky, Anna Rumshisky,
Jessica L. Moszkowicz, Olga Batiukova
Abstract
In this paper we introduce a methodology for annotating compo-
sitional operations in natural language text, and describe a mark-up
language, GLML, based on Generative Lexicon, for identifying such
relations. While most annotation systems capture surface relation-
ships, GLML captures the ?compositional history? of the argument
selection relative to the predicate. We provide a brief overview of GL
before moving on to our proposed methodology for annotating with
GLML. There are three main tasks described in the paper: (i) Com-
positional mechanisms of argument selection; (ii) Qualia in modifica-
tion constructions; (iii) Type selection in modification of dot objects.
We explain what each task includes and provide a description of the
annotation interface. We also include the XML format for GLML in-
cluding examples of annotated sentences.
1 Introduction
1.1 Motivation
In this paper, we introduce a methodology for annotating compositional
operations in natural language text. Most annotation schemes encoding
?propositional? or predicative content have focused on the identification
of the predicate type, the argument extent, and the semantic role (or label)
assigned to that argument by the predicate (see Palmer et al, 2005, Ruppen-
hofer et al, 2006, Kipper, 2005, Burchardt et al, 2006, Ohara, 2008, Subirats,
2004).
The emphasis here will be on identifying the nature of the composi-
tional operation rather than merely annotating the surface types of the en-
tities involved in argument selection.
169
Consider the well-known example below. The distinction in semantic
types appearing as subject in (1) is captured by entity typing, but not by any
sense tagging from, e.g., FrameNet (Ruppenhofer et al, 2006) or PropBank
(Palmer et al, 2005).
(1) a. Mary called yesterday.
b. The Boston office called yesterday.
While this has been treated as type coercion or metonymy in the literature (cf.
Hobbs et al, 1993 , Pustejovsky, 1991, Nunberg, 1979, Egg, 2005), the point
here is that an annotation using frames associated with verb senses should
treat the sentences on par with one another. Yet this is not possible if the
entity typing given to the subject in (1a) is HUMAN and that given for (1b)
is ORGANIZATION.
The SemEval Metonymy task (Markert and Nissim, 2007) was a good
attempt to annotate such metonymic relations over a larger data set. This
task involved two types with their metonymic variants:
(2) i. Categories for Locations: literal, place-for-people, place-for-event,
place-for-product;
ii. Categories for Organizations: literal, organization-for-members,
organization-for-event, organization-for-product, organization-for-fa-
cility.
One of the limitations with this approach, however, is that, while appropri-
ate for these specialized metonymy relations, the annotation specification
and resulting corpus are not an informative guide for extending the anno-
tation of argument selection more broadly.
In fact, the metonymy example in (1) is an instance of a much more
pervasive phenomenon of type shifting and coercion in argument selection.
For example, in (3) below, the sense annotation for the verb enjoy should
arguably assign similar values to both (3a) and (3b).
(3) a. Mary enjoyed drinking her beer .
b. Mary enjoyed her beer.
The consequence of this, however, is that, under current sense and role an-
notation strategies, the mapping to a syntactic realization for a given sense
is made more complex, and is, in fact, perplexing for a clustering or learn-
ing algorithm operating over subcategorization types for the verb.
170
1.2 Theoretical Preliminaries
The theoretical foundations for compositional operations within the sen-
tence have long been developed in considerable detail. Furthermore, type
shifting and type coercion operations have been recognized as playing an
important role in many formal descriptions of language, in order to main-
tain compositionality (cf. Partee and Rooth, 1983; Chierchia, 1998; Groe-
nendijk and Stokhof, 1989; Egg, 2005; Pinkal, 1999; Pustejovsky, 1995, and
many others). The goal of the present work is to: (a) create a broadly appli-
cable specification of the compositional operations involved in argument
selection; (b) apply this specification over a corpus of natural language
texts, in order to encode the selection mechanisms implicated in the com-
positional structure of the language.
The creation of a corpus that explicitly identifies the ?compositional his-
tory? associated with argument selection will be useful to computational
semantics in several respects: (a) the actual contexts within which type
coercions are allowed can be more correctly identified and perhaps gen-
eralized; (b) machine learning algorithms can take advantage of the map-
ping as an additional feature in the training phase; and (c) some consensus
might emerge on the general list of type-changing operations involved in
argument selection, as the tasks are revised and enriched.
For the purpose of this annotation task, we will adopt the general ap-
proach to argument selection within Generative Lexicon, as recently out-
lined in Pustejovsky (2006) and Asher and Pustejovsky (2006). We can dis-
tinguish the following modes of composition in natural language:
(4) a. PURE SELECTION (Type Matching): the type a function requires is
directly satisfied by the argument;
b. ACCOMMODATION: the type a function requires is inherited by the
argument;
c. TYPE COERCION: the type a function requires is imposed on the
argument type. This is accomplished by either:
i. Exploitation: taking a part of the argument?s type;
ii. Introduction: wrapping the argument with the required type.
Each of these will be identified as a unique relation between the predicate
and a given argument. In this annotation effort, we restrict the possible
relations between the predicate and a given argument to selection and coer-
cion. A more fine-grained typology of relations may be applied at a later
171
point. Furthermore, qualia structure values1 are identified in both argu-
ment selection and modification contexts.
The rest of this document proceeds as follows. In Section 2, we describe
our general methodology and architecture for GL annotation. Section 3
gives an overview of each of the annotation tasks as well as some details
on the resulting GLMLmarkup. Amore thorough treatment of thematerial
we present, including the complete GLML specification and updates on the
annotation effort can be found at www.glml.org.
2 General Methodology and Architecture
In this section, we describe the set of tasks for annotating compositional
mechanisms within the GL framework. The current GL markup will in-
clude the following tasks, each of which is described below in Section 3.
(5) a. Mechanisms of Argument Selection: Verb-based Annotation
b. Qualia in Modification Constructions
c. Type Selection in Modification of Dot Objects
2.1 System Architecture
Each GLML annotation task involves two phases: the data set construction
phase and the annotation phase. The first phase consists of (1) selecting the
target words to be annotated and compiling a sense inventory for each tar-
get, and (2) data extraction and preprocessing. The prepared data is then
loaded into the annotation interface. During the annotation phase, the an-
notation judgments are entered into the database, and the adjudicator re-
solves disagreements. The resulting database representation is used by the
exporting module to generate the corresponding XML markup, stand-off
annotation, or GL logical form.
These steps will differ slightly for each of the major GLML annotation
tasks. For example, Task 1 focuses on annotating compositional processes
between the verbs and their arguments. The first step for this task involves
(1) selecting the set of target verbs, (2) compiling a sense inventory for each
1The qualia structure, inspired by Moravcsik (1975)?s interpretation of the aitia of Aris-
totle, is defined as the modes of explanation of a word or phrase, and defined below (Puste-
jovsky, 1991): (a) FORMAL: the category distinguishing the meaning of a word within a
larger domain; (b) CONSTITUTIVE: the relation between an object and its constituent parts;
(c) TELIC: the purpose or function of the object, if there is one; (d) AGENTIVE: the factors
involved in the object?s origins or ?coming into being?.
172
target, and (3) associating a type template or a set of templates with each
sense. Since the objective of the task is to annotate coercion, our choices
must include the verbs that exhibit the coercive behavior at least in some of
their senses.
At the next step, the data containing the selected target words is ex-
tracted from a corpus and preprocessed. Since the GLML annotation is
intra-sentential, each extracted instance is a sentence. Sentences are parsed
to identify the relevant arguments, adjuncts or modifiers for each target.
The data is presented to the annotatator with the target word and the head-
word of the relevant phrase highlighted.
Due to the complexity of the GLML annotation, we chose to use the
task-based annotation architecture. The annotation environment is designed
so that the annotator can focus on one facet of the annotation at a time.
Thus, in Task 1, the verbs are disambiguated by the annotator in one sub-
task, and the annotation of the actual compositional relationship is done in
another subtask. Figure 1 shows an example of the interface for the verb-
based annotation task .
Figure 1: Example of Annotation Interface for GLML Annotation
173
2.2 The Type System for Annotation
The type system we have chosen for annotation is purposefully shallow,
but we also aimed to include types that would ease the complexity of the
annotation task. The type system is not structured in a hierarchy, but rather
it is presented as a set of types. For example, we include both HUMAN and
ANIMATE in the type system along with PHYSICAL OBJECT. While HUMAN
is a subtype of both ANIMATE and PHYSICAL OBJECT, the annotator does
not need to be concerned with this. This allows the annotator to simply
choose the HUMAN type when necessary rather than having to deal with
type inheritance.
While the set of types for GLML annotation can easily be modified, the
following list is currently being used:
(6) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT, ORGANIZATION, EVENT, PROPOSITION, IN-
FORMATION, SENSATION, LOCATION, TIME PERIOD, ABSTRACT ENTITY, ATTITUDE, EMOTION,
PROPERTY, OBLIGATION, AND RULE
3 Annotation Tasks
In this section, we describe the annotation process: the steps involved in
each task and the way they are presented to the annotators. In this paper,
we focus on the task descriptions rather than an in depth review of the
annotation interface and the resulting GLML markup.
The general methodology for each task is as follows: 1) Select a target
set of words and compile a sense inventory for each one, 2) Select a set of
sentences for each target, 3) Disambiguate the sense of the target in a given
sentence, and 4) Answer questions specific to the annotation task in order
to create the appropriate GLML link.
3.1 Mechanisms of Argument Selection: Verb-based Annotation
This annotation task involves choosing which selectional mechanism is
used by the predicate over a particular argument. The possible relations
between the predicate and a given argument will, for now, be restricted
to selection and coercion. In selection, the argument NP satisfies the typ-
ing requirements of the predicate, as in The child threw the stone (PHYS-
ICAL OBJECT). Coercion encompasses all cases when a type-shifting op-
eration (exploitation or introduction) must be performed on the comple-
ment NP in order to satisfy selectional requirements of the predicate, as in
The White House (LOCATION ? HUMAN) denied this statement.
174
An initial set of verbs and sentences containing them has been selected
for annotation. For each sentence, the compositional relationship of the
verb with every argument and adjunct will be annotated. The target types
for each argument are provided in a type template that is associated with
the sense of the verb in the given sentence. For example, one of the senses
of the verb deny (glossed as ?State or maintain that something is untrue?)
would have the following type template: HUMAN deny PROPOSITION.
In the first subtask, the annotator is presented with a set of sentences
containing the target verb and the chosen grammatical relation. The anno-
tator is asked to select the most fitting sense of the target verb, or to throw
out the example (pick the ?N/A? option) if no sense can be chosen either
due to insufficient context, because the appropriate sense does not appear
in the inventory, or simply no disambiguation can be made in good faith.
Next, the annotator is presented with a list of sentences in which the
target verb is used in the same sense and is asked to determine whether the
argument in the specified grammatical relation belongs to the type speci-
fied in the corresponding template. If the argument belongs to the appro-
priate type, the ?yes? box is clicked, generating a CompLink with comp-
Type=?SELECTION?. If ?no? is selected, a type selection menu pops up
below the first question, and the annotator is asked to pick a type from a
list of shallow types which is usually associated with the argument. Con-
sequently, a CompLink with compType=?COERCION? is created with the
corresponding source and target type.
The following example of GLMLmarkup is generated from the database2:
Sir Nicholas Lyell, Attorney General, denies a cover-up.
<SELECTOR sid="s1">denies</SELECTOR>
a <NOUN nid="n1">cover-up</NOUN> .
<CompLink cid="cid1" sID="s1" relatedToNoun="n1" gramRel="dobj"
compType="COERCION" sourceType="EVENT" targetType="PROPOSITION"/>
3.2 Qualia Selection in Modification Constructions
For this task, the relevant semantic relations are defined in terms of the
qualia structure. We examine two kinds of constructions in this task: adjec-
tival modification of nouns and nominal compounds3.
2While we present these examples as an inline annotation, a LAF (Ide and Romary, 2003)
compliant offset annotation is fully compatible with GLML.
3Since target nouns have already been selected for these two tasks, it is also possible
to annotate qualia selection in verb-noun contexts such as Can you shine the lamp over here?
(TELIC). However, here we focus solely on the modification contexts mentioned here.
175
3.2.1 Adjectival Modification of Nouns
This task involves annotating how particular noun qualia values are bound
by the adjectives. Following Pustejovsky (2000), we assume that the prop-
erties grammatically realized as adjectives ?bind into the qualia structure
of nouns, to select a narrow facet of the noun?s meaning.? For example, in
the NP ?a sharp metal hunting knife?, sharp refers to the knife as a physi-
cal object, its FORMAL type, metal is associated with a material part of the
knife (CONSTITUTIVE), and hunting is associatedwith how the knife is used
(TELIC). Similarly, forged in ?a forged knife? is associated with the creation
of the knife (AGENTIVE).
The task begins with sense disambiguation of the target nouns. Ques-
tions are then used to help the annotator identify which qualia relations are
selected. For example, the TELIC question for the noun table would be ?Is
this adjective associated with the inherent purpose of table?? These ques-
tions will change according to the type associated with the noun. Thus,
for natural types such as woman, the TELIC question would be ?Is this ad-
jective associated with a specific role of woman?? Similarly, for the AGEN-
TIVE role, the question corresponding to the PHYSICAL OBJECT-denoting
nouns refers to the ?making or destroying? the object, while for the EVENT-
denoting nouns, the same question involves ?beginning or ending? of the
event. QLinks are then created based on the annotator?s answers, as in the
following example:
The walls and the wooden table had all been lustily scrubbed.
<SELECTOR sid="s1">wooden</SELECTOR>
<NOUN nid="n1">table</NOUN>
<QLink qid="qid1" sID="s1" relatedToNoun="n1" qType="CONST"/>
3.2.2 Nominal Compounds
This task explores the semantic relationship between elements in nominal
compounds. The general relations presented in Levi (1978) are a useful
guide for beginning a classification of compound types, but the relations
between compound elements quickly prove to be too coarse-grained. War-
ren?s comprehensive work (Warren, 1978) is a valuable resource for differ-
entiating relation types between compound elements.
The class distinction in compound types in language can be broken
down into three forms (Spencer, 1991): endocentric compounds, exocen-
tric compounds, and dvandva compounds. Following Bisetto and Scalise
176
(2005), however, it is possible to distinguish three slightly differently con-
structed classes of compounds, each exhibiting endocentric and exocentric
behavior: subordinating, attributtive, and coordinate.
We will focus on the two classes of subordinating and attributive com-
pounds. Within each of these, we will distinguish between synthetic and
non-synthetic compounds. The former are deverbal nouns, and when act-
ing functionally (subordinating), take the sister noun as an argument, as
in bus driver and window cleaner. The non-synthetic counterparts of these
include pastry chef and bread knife, where the head is not deverbal in any
obvious way. While Bisetto and Scalise?s distinction is a useful one, it does
little to explain how non-relational sortal nouns such as chef and knife act
functionally over the accompanying noun in the compound, as above.
This construction has been examined within GL by Johnston and Busa
(1999). We will assume much of that analysis in our definition of the task
described here. Our basic assumption regarding the nature of the seman-
tic link between both parts of compounds is that it is generally similar to
the one present in adjectival modification. The only difference is that in
nominal compounds, for instance, the qualia of a head noun are activated
or exploited by a different kind of modifier, a noun. Given this similar-
ity, the annotation for this task is performed just as it is for the adjectival
modification task. A QLink is created as in the following example:
Our guest house stands some 100 yards away.
<SELECTOR sid="s1">guest</SELECTOR>
<NOUN nid="n1">house</NOUN>
<QLink qid="qid1" sID="s1" relatedToNoun="n1" qType="TELIC"/>
3.3 Type Selection in Modification of Dot Objects
This task involves annotating how particular types within dot objects are
exploited in adjectival and nominal modification constructions. Dot objects
or complex types (Pustejovsky, 1995) are defined as the product of a type
constructor ? (?dot?), which creates dot objects from any two types a and
b , creating a ? b. Complex types are unique because they are made up of
seemingly incompatible types such as FOOD and EVENT.
Given a complex type c = a ? b, there are three possible options: 1) the
modifier applies to both a and b, 2) the modifier applies to a only, or 3) the
modifier applies to b only. Option 1 would be illustrated by examples such
as good book [+info, +physobj] and long test [+info, +event]. Examples such as
177
delicious lunch [+food, -event] and long lunch [-food, +event] illustrate options
2 and 3. A listing of dot objects can be found in Pustejovsky (2005).
The sense inventory for the collection of dot objects chosen for this task
will include only homonyms. That is, only contrastive senses such as the
river bank versus financial institution for bank will need to be disambiguated.
Complementary senses such as the financial institution itself versus the
building where it is located are not included.
In order to create the appropriate CompLink, the annotator will select
which type from a list of component types for a given dot object is exploited
in the sentence. The resulting GLML is:
After a while more champagne and a delicious lunch was served.
<SELECTOR sid="s1">delicious</SELECTOR>
<NOUN nid="n1">lunch</NOUN>
<CompLink cid="cid1" sID="s1" relatedToNoun="n1" gramRel="mod"
compType="SELECTION" sourceType="[PHYS_OBJ,EVENT]"
targetType="PHYS_OBJ" />
4 Conclusion
In this paper, we approach the problem of annotating the relation between
a predicate and its argument as one that encodes the compositional history
of the selection process. This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of a predicate from those
that are accommodated or coerced in context. We described a specification
language for selection, GLML, based largely on the type selective opera-
tions in GL, and three annotation tasks using this specification to identify
argument selection behavior.
There are clearly many compositional operations in language that have
not been addressed in this paper. The framework is general enough, how-
ever, to describe a broad range of type selective behavior. As the tasks be-
come more refined, the extensions will also become clearer. Furthermore,
as other languages are examined for annotation, new tasks will emerge re-
flecting perhaps language-specific constructions.
Acknowledgements
The idea for annotating a corpus according to principles of argument selec-
tion within GL arose during a discussion at GL2007 in Paris, between one
178
of the authors (J. Pustejovsky) and Nicoletta Calzolari and Pierrette Bouil-
lon. Recently, the authors met with other members of the GLML Working
Group in Pisa at the ILC (September 23-25, 2008). We would like to thank
the members of that meeting for their fruitful feedback and discussion on
an earlier version of this document. In particular, we would like to thank
Nicoletta Calzolari, Elisabetta Jezek, Alessandro Lenci, Valeria Quochi, Jan
Odijk, Tommaso Caselli, Claudia Soria, Chu-Ren Huang, Marc Verhagen,
and Kiyong Lee.
References
N. Asher and J. Pustejovsky. 2006. A type composition logic for generative
lexicon. Journal of Cognitive Science, 6:1?38.
A. Bisetto and S. Scalise. 2005. The classification of compounds. Lingue e
Linguaggio, 2:319?332.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian
Pado, and Manfred Pinkal. 2006. The salsa corpus: a german corpus
resource for lexical semantics. In Proceedings of LREC, Genoa, Italy.
Gennaro Chierchia. 1998. Reference to kinds across language. Natural Lan-
guage Semantics, 6(4).
Marcus Egg. 2005. Flexible semantics for reinterpretation phenomena. CSLI,
Stanford.
Jeroen Groenendijk and Martin Stokhof, 1989. Type-shifting rules and the
semantics of interrogatives, volume 2, pages 21?68. Kluwer, Dordrecht.
Jerry R. Hobbs, Mark Stickel, and Paul Martin. 1993. Interpretation as ab-
duction. Artificial Intelligence, 63:69?142.
Nancy Ide and L. Romary. 2003. Outline of the international standard lin-
guistic annotation framework. In Proceedings of ACL?03Workshop on Lin-
guistic Annotation: Getting the Model Right.
M. Johnston and F. Busa. 1999. The compositional interpretation of com-
pounds. In E. Viegas, editor, Breadth and Depth of Semantics Lexicons,
pages 167?167. Dordrecht: Kluwer Academic.
Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon.
Phd dissertation, University of Pennsylvania, PA.
J. N. Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic
Press, New York.
K. Markert and M. Nissim. 2007. Metonymy resolution at semeval i:
Guidelines for participants. In Proceedings of the ACL 2007 Conference.
179
J. M. Moravcsik. 1975. Aitia as generative factor in aristotle?s philosophy.
Dialogue, 14:622?636.
Geoffrey Nunberg. 1979. The non-uniqueness of semantic solutions: Poly-
semy. Linguistics and Philosophy, 3:143?184.
Kyoko Hirose Ohara. 2008. Lexicon, grammar, and multilinguality in the
japanese framenet. In Proceedings of LREC, Marrakech, Marocco.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles. Computational Linguistics, 31(1):71?
106.
Barbara Partee and Mats Rooth, 1983. Generalized conjunction and type ambi-
guity, pages 361?383. de Gruyter, Berlin.
Manfred Pinkal. 1999. On semantic underspecification. In Harry Bunt and
Reinhard Muskens, editors, Proceedings of the 2nd International Workshop
on Computational Semantics (IWCS 2), January 13-15, Tilburg University,
The Netherlands.
J. Pustejovsky. 1991. The generative lexicon. Computational Linguistics,
17(4).
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.): MIT Press.
J. Pustejovsky. 2000. Events and the semantics of opposition. In C. Tenny
and J. Pustejovsky, editors, Events as Grammatical Objects, pages 445?
482. Center for the Study of Language and Information (CSLI), Stan-
ford, CA.
J. Pustejovsky. 2005. A survey of dot objects. Technical report, Brandeis
University.
J. Pustejovsky. 2006. Type theory and lexical decomposition. Journal of Cog-
nitive Science, 6:39?76.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk.
2006. FrameNet II: Extended Theory and Practice.
A. Spencer. 1991. Morphological Theory: An Introduction to Word Structure
in Generative Grammar. Blackwell Textbooks in Linguistics, Oxford, UK
and Cambridge, USA.
Carlos Subirats. 2004. FrameNet Espan?ol. Una red sema?ntica de mar-
cos conceptuales. In VI International Congress of Hispanic Linguistics,
Leipzig.
B. Warren. 1978. Semantic Patterns of Noun-Noun Compounds. Acta Univer-
sitatis Gothoburgensis, Go?teborg.
180
Proceedings of the Fifth Law Workshop (LAW V), pages 74?81,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Crowdsourcing Word Sense Definition
Anna Rumshisky??
? Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA
arum@csail.mit.edu
?Department of Computer Science
Brandeis University
Waltham, MA
Abstract
In this paper, we propose a crowdsourcing
methodology for a single-step construction of
both an empirically-derived sense inventory
and the corresponding sense-annotated cor-
pus. The methodology taps the intuitions of
non-expert native speakers to create an expert-
quality resource, and natively lends itself to
supplementing such a resource with additional
information about the structure and reliabil-
ity of the produced sense inventories. The re-
sulting resource will provide several ways to
empirically measure distances between related
word senses, and will explicitly address the
question of fuzzy boundaries between them.
1 Introduction
A number of recent initiatives has focused on cre-
ating sense-annotated gold standards for word sense
disambiguation and induction algorithms. However,
such work has frequently come under criticism over
the lack of a satisfactory set of standards for creat-
ing consistent, task-independent sense inventories.
More systematic efforts to replace ad hoc lexico-
graphic procedures for sense inventory construction
have often focused on working with existing sense
inventories, attempting to resolve the specific asso-
ciated problems (e.g. sense granularity, overlapping
senses, etc.) Methodologically, defining a robust
procedure for sense definition has remained an elu-
sive task.
In this paper, we propose a method for creating
a sense inventory from scratch for any polysemous
word, simultaneously with the corresponding sense-
annotated lexical sample. The methodology we
propose explicitly addresses the question of related
word senses and fuzzy boundaries between them,
without trying to establish hard divisions where em-
pirically there are none.
The proposed method uses Amazon?s Mechani-
cal Turk for sense annotation. Over the last several
of years, Mechanical Turk, introduced by Amazon
as ?artificial artificial intelligence?, has been used
successfully for a number of NLP tasks, including
robust evaluation of machine translation systems by
reading comprehension (Callison-Burch, 2009), and
other tasks explored in the recent NAACL workshop
(Callison-Burch and Dredze, 2010b). Mechanical
Turk has also been used to create labeled data sets
for word sense disambiguation (Snow et al, 2008)
and even to modify sense inventories. But the origi-
nal sense inventory construction has always been left
to the experts. In contrast, in the annotation method
we describe, the expert is eliminated from the an-
notation process. As has been the case with using
Mechanical Turk for other NLP tasks, the proposed
annotation is quite inexpensive and can be done very
quickly, while maintaining expert-level annotation
quality.
The resulting resource will produce several ways
to empirically measure distances between senses,
and should help to address some open research ques-
tions regarding word sense perceptions by native
speakers. We describe a set of pilot annotation stud-
ies needed to ensure reliability of this methodology
and test the proposed quality control mechanisms.
The outcome will be a lexicon where sense inven-
tories are represented as clusters of instances, and
an explicit quantitative representation of sense con-
74
sistency, distance between senses, and sense overlap
is associated with the senses for each word. The goal
is to provide a more accurate representation the way
speakers of a language conceptualize senses, which
can be used for training and testing of the automated
WSD systems, as well as to automatically induce se-
mantic and syntactic context patterns that represent
usage norms and permit native speakers to perform
sense disambiguation.
2 The Problem of Sense Definition
The quality of the annotated corpora depends di-
rectly on the selected sense inventory, so, for ex-
ample, SemCor (Landes et al, 1998), which used
WordNet synsets, inherited all the associated prob-
lems, including using senses that are too fine-
grained and in many cases poorly distinguished. At
the Senseval competitions (Mihalcea et al, 2004;
Snyder and Palmer, 2004; Preiss and Yarowsky,
2001), the choice of a sense inventory also fre-
quently presented problems, spurring the efforts to
create coarser-grained sense inventories (Navigli,
2006; Hovy et al, 2006; Palmer et al, 2007). Inven-
tories derived from WordNet by using small-scale
corpus analysis and by automatic mapping to top
entries in Oxford Dictionary of English were used
in the recent workshops on semantic evaluation, in-
cluding Semeval-2007 and Semeval-2010 (Agirre et
al., 2007; Erk and Strapparava, 2010).
Several current resource-oriented projects attempt
to formalize the procedure of creating a sense inven-
tory. FrameNet (Ruppenhofer et al, 2006) attempts
to organize lexical information in terms of script-
like semantic frames, with semantic and syntactic
combinatorial possibilities specified for each frame-
evoking lexical unit (word/sense pairing). Corpus
Pattern Analysis (CPA) (Hanks and Pustejovsky,
2005) attempts to catalog norms of usage for in-
dividual words, specifying them in terms of con-
text patterns. Other large-scale resource-building
projects also use corpus analysis techniques. In
PropBank (Palmer et al, 2005), verb senses were
defined based on their use in Wall Street Journal cor-
pus and specified in terms of framesets which con-
sist of a set of semantic roles for the arguments of a
particular sense. In the OntoNotes project (Hovy et
al., 2006), annotators use small-scale corpus anal-
ysis to create sense inventories derived by group-
ing together WordNet senses, with the procedure re-
stricted to maintain 90% inter-annotator agreement.
Importantly, most standard WSD resources con-
tain no information about the clarity of distinctions
between different senses in the sense inventory. For
example, OntoNotes, which was used for evaluation
in the word sense disambiguation and sense induc-
tion tasks in the latest SemEval competitions con-
tains no information about sense hierarchy, related
senses, or difficulty and consistency of a given set of
senses.
3 Characteristics of the Proposed Lexical
Resource
The lexical resource we propose to build is a sense-
disambiguated lexicon which will consist of an
empirically-derived sense inventory for each word in
the language, and a sense-tagged corpus annotated
with the derived inventories. The resource will be
assembled from ?the ground up? using the intuitions
of non-expert native speakers about the similarity
between different uses of the same word. Each sense
will be represented as a cluster of instances grouped
together in annotation. The following information
will be associated with each sense cluster:
1. Consistency rating for each sense cluster, in-
cluding several of the following measures:
? Annotator agreement, using the inter-
annotator agreement measures for the
sense cluster (e.g. Fleiss? Kappa);
? Cluster tightness, determined from the
distributional contextual features associ-
ated with instance comprising the cluster;
2. Distances to other sense clusters derived for the
same word, using several distance measures,
including:
? Cluster overlap, determined from the per-
centage of instances associated with both
clusters;
? Translation similarity, determined as the
number existing different lexicalizations
in an aligned multilingual parallel corpus,
using a measurement methodology similar
to Resnik and Yarowsky (1999).
75
The resource would also include a Membership
rating for each instance within a given sense clus-
ter, which would represent how typical this exam-
ple is for the associated sense cluster. The instances
whose membership in the cluster was established
with minimal disagreement between the annotators,
and which do not have multiple sense cluster mem-
bership will be designated as the core of the sense
cluster. The membership ratings would be based on
(1) inter-annotator agreement for that instance (2)
distance from the core elements of the cluster.
Presently, the evaluation of automated WSD and
WSI systems does not take into account the rela-
tive difficulty of sense distinctions made within a
given sense inventories. In the proposed resource,
for every lexical item, annotator agreement values
will be associated with each sense separately, as well
as with the full sense inventory for that word, provid-
ing an innate measure of disambiguation difficulty
for every lexical item.
Given that the fluidity of senses is such a perva-
sive problem for lexical resources and that it cre-
ates severe problems for the usability of the systems
trained using these resources, establishing the relia-
bility and consistency of each sense cluster and the
?prototypicality? of each example associated with
that sense is crucial for any lexical resource. Simi-
larly crucial is the information about the overlap be-
tween senses in a sense inventory as well as the sim-
ilarity between senses. And yet, none of the exist-
ing resources contain this information.1 As a result,
the systems trained on sense-tagged corpora using
the existing sense inventories attempt to make sense
distinctions where empirically no hard division be-
tween senses exist. And since the information about
consistency and instance typicality is not available,
the standard evaluation paradigm currently used in
the field for the automated WSD/WSI systems does
not take it into account. In contrast, the methodology
we propose here lends itself naturally to quantitative
analysis needed to explicitly address the question of
related word senses and fuzzy boundaries between
them.
1One notable exception is the sense-based inter-annotator
agreement available in OntoNotes.
4 Annotation Methodology
In traditional annotation settings, the quality of an-
notation directly depends on how well the annota-
tion task is defined. The effects of felicitous or poor
task design are greatly amplified when one is target-
ing untrained non-expert annotators.
Typically for the tasks performed using Mechan-
ical Turk, complex annotation is split into simpler
steps. Each step is farmed out to the non-expert an-
notators employed via Mechanical Turk (henceforth,
MTurkers) in a form of a HIT (Human Intelligence
Task), a term used to refer to the tasks that are hard
to perform automatically, yet very easy to do for hu-
mans.
4.1 Prototype-Based Clustering
We propose a simple HIT design intended to imi-
tate the work done by a lexicographer in corpus-
based dictionary construction, of the kind used in
Corpus Pattern Analysis (CPA, 2009). The task is
designed as a sequence of annotation rounds, with
each round creating a cluster corresponding to one
sense. MTurkers are first given a set of sentences
containing the target word, and one sentence that is
randomly selected from this set as a target sentence.
They are then asked to identify, for each sentence,
whether the target word is used in the same way as
in the target sentence. If the sense is unclear or it
is impossible to tell, they are instructed to pick the
?unclear? option. After the first round of annota-
tion is completed, the sentences that are judged as
similar to the target sentence by the majority vote
are set apart into a separate cluster corresponding to
one sense, and excluded from the set used in further
rounds. The procedure is repeated with the remain-
ing set, i.e. a new target sentence is selected, and the
remaining examples are presented to the annotators.
This cycle is repeated until all the remaining exam-
ples are classified as ?unclear? by the majority vote,
or no examples remain.
4.2 Proof-of-Concept Study
A preliminary proof-of-concept study for this task
design has been reported on previously (Rumshisky
et al, 2009). In that study, the proposed task design
was tested on a chosen polysemous verb of medium
difficulty. The results were then evaluated against
76
the groupings created by a professional lexicogra-
pher, giving the set-matching F-score of 93.0 and the
entropy of the two clustering solutions of 0.3. The
example sentences were taken from the CPA verb
lexicon for crush. Figure 1 shows the first screen
displayed to MTurkers for the HIT, with ten exam-
ples presented on each screen. Each example was
annotated by 5 MTurkers.
The prototype sentences associated with each
cluster obtained for the verb crush are shown below:
C1 By appointing Majid as Interior Minister, Pres-
ident Saddam placed him in charge of crushing
the southern rebellion.
C2 The lighter woods such as balsa can be crushed
with the finger.
C3 This time the defeat of his hopes didn?t crush
him for more than a few days.
Each round took approximately 30 minutes to an
hour to complete, depending on the number of sen-
tences in that round. Each set of 10 sentences took
on the average 1 minute, and the annotator received
$0.03 USD as compensation. The experiment was
conducted using 5-way annotation, and the total sum
spent was less than $10 USD. It should be noted
that in a large-scale annotation effort, the cost of the
annotation for a single word will certainly vary de-
pending on the number of senses it has. However,
time is less of an issue, since the annotators can work
in parallel on many words at the same time.
4.3 Removing Prototype Impact
Prototype-based clustering produces hard clus-
ters, without explicit information about the origin
of boundary cases or the potentially overlapping
senses. One of the possible alternatives to having in-
stances judged against a single prototype, with mul-
tiple iterations, is to have pairs of concordance lines
evaluated against each other. This is in effect more
realistic, since (1) each sentence is effectively a pro-
totype, and (2) there is no limitation on the types of
similarity judgments allowed; ?cross-cluster? con-
nections can be retained.
Whether obtained in a prototype-based setup, or
in pairs, the obtained data lends itself well to a
graph representation. The pairwise judgments in-
duce an undirected graph, in which judgments can
be thought of as edges connecting the instance
nodes, and interconnected clusters of nodes corre-
spond to the derived sense inventory (cf. Figure 2).
In the pairwise setup, results do not depend on the
selection of a prototype sentence, so it provides a
natural protection against a single unclear sentence
having undue impact on cluster results, and does so
without having to introduce an additional step into
the annotation process. It also protects against di-
rectional similarity evaluation bias. However, one
of the disadvantages is the number of judgments re-
quired to collect. The prototype-based clustering
of N instances requires between N(N ? 1)/2 and
N ? 1 judgments (depending on the way instances
split between senses), which gives O(N2) for 1 clus-
ter 1 instance case vs. O(N) for 1 cluster 1 word
case. A typical sense inventory has < 10 senses, so
that gives us an estimate of about 10N judgments
to cluster N concordance lines, to be multiplied by
the number of annotators for each pair. In order to
bypass prototyping, we must allow same/different
judgments for every pair of examples. For N ex-
amples, this gives O(N2) judgments, which makes
collecting all pair judgments, from multiple annota-
tors, too expensive.
One of the alternatives for reducing the number
of judgments is to use a partial graph approxima-
tion. The idea behind it is that rather than collecting
repeat judgments (multiple annotations) of the same
instance, one would collect a random subset of edges
from the full graph, and then perform clustering on
the obtained sparse graph. Full pairwise annotation
will need to be performed on a small cross-section
of English vocabulary in order to get an idea of how
sparse the judgment graph can be to obtain results
comparable to those we obtained with prototype-
based clustering using good prototypes.
Some preliminary experiments using Markov
Clustering (MCL) on a sparse judgment graph sug-
gest that the number of judgments collected in the
proof-of-concept experiment above by Rumshisky et
al. (2009) in order to cluster 350 concordance lines
would only be sufficient to reliably cluster about 140
concordance lines.
77
Figure 1: Task interface and instructions for the HIT presented to the non-expert annotators in proof-of-concept
experiment.
5 Pilot Annotations
In this section, we outline the pilot studies that
need to be conducted prior to applying the described
methodology in a large-scale annotation effort. The
goal of the pilot studies we propose is to establish
the best MTurk annotation practice that would en-
sure the reliability of obtained results while mini-
mizing the required time and cost of the annotation.
The anticipated outcome of these studies is a robust
methodology which can be applied to unseen data
during the construction of the proposed lexical re-
source.
5.1 Testing the validity of obtained results
The goal of the first set of studies is to establish
the validity of sense groupings obtained using non-
expert annotators. We propose to use the procedure
outlined in Sec 4 on the data from existing sense-
tagged corpora, in particular, OntoNotes, PropBank,
NomBank, and CPA.
This group of pilot studies would involve per-
forming prototype-based annotation for a selected
set of words representing a cross-section of English
vocabulary. A concordance for each selected word
will be extracted from the gold standard provided by
an expert-tagged sense-annotated corpus. The initial
set of selected content words would be evenly split
between verbs and nouns. Each group will consist
of a set of words with different degrees of polysemy.
The lexical items would need to be prioritized ac-
cording to corpus frequencies, with more frequent
words from each group being given preference.
For example, for verbs, a preliminary study done
within the framework of the CPA project suggested
that out of roughly 6,000 verbs in a language, 30%
have one sense, with the rest evenly split between
verbs having 2-3 senses and verbs having more than
4 senses. About 20 light verbs have roughly 100
senses each. The chosen lexical sample will there-
fore need to include low-polysemy verbs, mid-range
verbs with 3-10 senses, lighter verbs with 10-20
senses, and several light verbs. Degree of polysemy
would need to be obtained from the existing lexi-
cal resource used as a gold standard. The annota-
tion procedure could also be tested additionally on a
small number of adjectives and adverbs.
78
Figure 2: Similarity judgment graph
A smaller subset of the re-annotated data would
then need to be annotated using full pairwise
annotation. The results of this annotation would
need to be used to investigate the quality of the
clusters obtained using a partial judgment graph, in-
duced by a subset of collected judgments. The re-
sults of both types of annotation could then be used
to evaluate different measures of sense consistency
and as well as for evaluation of distance between dif-
ferent senses of a lexical item.
5.2 Testing quality control mechanisms
The goal of this set of studies is to establish reliable
quality control mechanisms for the annotation. A
number of mechanisms for quality control have been
proposed for use with Mechanical Turk annotation
(Callison-Burch and Dredze, 2010a). We propose to
investigate the following mechanisms:
? Multiple annotation. A subset of the data
from existing resources would need to be an-
notated by a larger number of annotators, (e.g.
10 MTurkers. The obtained clustering results
would need to be compared to the gold standard
data from the existing resource, while varying
the number of annotators producing the clus-
tering through majority voting. Results from
different subsets of annotators for each subset
size would need to be aggregated to evaluate
the consistency of annotation for each value.
For example, for 3-way annotation, the cluster-
ings obtained from by the majority vote within
all possible triads of annotators would be eval-
uated and the results averaged.
? Checking annotator work against gold
standard. Using the same annotated data set,
we could investigate the effects of eliminating
the annotators performing poorly on the judg-
ments of similarity for the first 50 examples
from the gold standard. The judgments of the
remaining annotators would need to be aggre-
gated to produce results through a majority
vote.
? Checking annotator work against the majority
vote. Using a similar approach, we can inves-
tigate the effects of eliminating the annotators
performing poorly against the majority vote.
The data set obtained above would allow us to
experiment with different thresholds for elim-
inating annotators, in each case evaluating the
resulting improvement in cluster quality.
? Using prototype-quality control step. We
would need to re-annotate a subset of words us-
ing an additional step, during which poor qual-
ity prototype sentences will be eliminated. This
step would be integrated with the main annota-
tion as follows. For each candidate prototype
sentence, we would collect the first few similar-
ity judgments from the selected number of an-
notators. If a certain percentage of judgments
are logged as unclear, the sentence is elimi-
79
nated from the set, and another prototype sen-
tence is selected. We would evaluate the results
of this modification, using different thresholds
for the number of judgments collected and the
percentage of ?unclear? ratings.
5.3 Using translation equivalents to compute
distances between senses
The goal of this set of studies is to investigate the
viability of computing distances between the sense
clusters obtained for a given word by using its trans-
lation equivalents in other languages. If this method-
ology proves viable, then the proposed lexical re-
source can be designed to include some data from
multilingual parallel corpora. This would provide
both a methodology for measuring relatedness of de-
rived senses and a ready set of translation equiva-
lents for every sense.
Resnik and Yarowsky (1999) used human anno-
tators to produce cross-lingual data in order to mea-
sure distances between different senses in a mono-
lingual sense inventory and derive a hierarchy of
senses, at different levels of sense granularity. Two
methods were tested, where the first one involved
asking human translators for the ?best? translation
for a given polysemous word in a monolingual
sense-annotated lexical sample data set. The sec-
ond method involved asking the human translators,
for each pair of examples in the lexical sample, to
provide different lexicalizations for the target word,
if they existed in their language. The distances be-
tween different senses were then determined from
the number of languages in which different lexi-
calizations were preferred (or existed) for different
senses of the target word.
In the present project, we propose to obtain simi-
lar information by using the English part of a word-
aligned multilingual parallel corpus for sense anno-
tation. The degree of cross-lingual lexicalization of
the target word in instances associated with differ-
ent sense classes could then be used to evaluate the
distance between these senses. We propose the fol-
lowing to be done as a part of this pilot study. For a
selected sample of polysemous words:
? Extract several hundred instances for each
word from the English part of a multilingual
corpus, such as the Europarl (Koehn, 2005); 2
? Use the best MTurk annotation procedure as es-
tablished in Sec 5.2 to cluster the extracted in-
stances;
? Obtain translation equivalents for each instance
of the target word using word-alignment pro-
duced with Giza++ (Och and Ney, 2000);
? Compute the distances between the obtained
clusters by estimating the probability of differ-
ent lexicalization of the two senses from the
word-aligned parallel corpus.
The distances would then be computed using a mul-
tilingual cost function similar to the one used by
Resnik and Yarowsky (1999), shown in Figure 5.3.
The Europarl corpus contains Indo-European lan-
guages (except for Finnish), predominantly of the
Romanic and Germanic family. These languages of-
ten have parallel sense distinctions. If that proves to
be the case, a small additional parallel corpus with
the data from other non-European languages would
need to be used to supplement the data from Eu-
roparl.
6 Conclusion
In this paper, we have presented a proposal for a new
annotation strategy for obtaining sense-annotated
data WSD/WSI applications, together with the cor-
responding sense inventories, using non-expert an-
notators. We have described a set of pilot studies that
would need to be conducted prior to applying this
strategy in a large-scale annotation effort. We out-
lined the provisional design of the lexical resource
that can be constructed using this strategy, including
the native measures for sense consistency and diffi-
culty, distance between related senses, sense over-
lap, and other parameters necessary for the hierar-
chical organization of sense inventories.
Acknowledgments
I would like to thank James Pustejovsky and David
Tresner-Kirsch for their contributions to this project.
2If necessary, the instance set for selected words may be sup-
plemented with the data from other corpora, such as the JRC-
Acquis corpus (Steinberger et al, 2006).
80
Cost(sensei, sensej) =
1
|Languages|
?
L?Languages
PL(diff-lexicalization|sensei, sensej)
Figure 3: Multilingual cost function for distances between senses.
References
E. Agirre, L. Ma`rquez, and R. Wicentowski, editors.
2007. Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007). ACL,
Prague, Czech Republic, June.
Chris Callison-Burch and Mark Dredze. 2010a. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. ACL.
Chris Callison-Burch and Mark Dredze, editors. 2010b.
Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk. ACL, Los Angeles, June.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009).
CPA. 2009. Corpus Pattern Analysis.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation. ACL, Uppsala, Sweden, July.
P. Hanks and J. Pustejovsky. 2005. A pattern dictionary
for natural language processing. Revue Franc?aise de
Linguistique Applique?e.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solu-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 57?60, New York City, USA,
June. ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Citeseer.
S. Landes, C. Leacock, and R.I. Tengi. 1998. Building
semantic concordances. In C. Fellbaum, editor, Word-
net: an electronic lexical database. MIT Press, Cam-
bridge (Mass.).
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Rada Mi-
halcea and Phil Edmonds, editors, Senseval-3: Third
International Workshop on the Evaluation of Sys-
tems for the Semantic Analysis of Text, pages 25?28,
Barcelona, Spain, July. ACL.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation performance.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 105?112, Sydney, Australia, July. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, Hongkong, China, Oc-
tober.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
M. Palmer, H. Dang, and C. Fellbaum. 2007. Mak-
ing fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natural
Language Engineering.
J Preiss and D. Yarowsky, editors. 2001. Proceedings of
the Second Int. Workshop on Evaluating WSD Systems
(Senseval 2). ACL2002/EACL2001.
P. Resnik and D. Yarowsky. 1999. Distinguishing sys-
tems and distinguishing senses: new evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5(2):113?134.
A. Rumshisky, J. Moszkowicz, and M. Verhagen. 2009.
The holy grail of sense definition: Creating a sense-
disambiguated corpus from scratch. In Proceedings
of 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009), Pisa, Italy.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice.
R. Snow, B. OConnor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fastbut is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP 2008).
B. Snyder and M. Palmer. 2004. The english all-
words task. In Rada Mihalcea and Phil Edmonds,
editors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 41?43, Barcelona, Spain, July. ACL.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, D. Tufis, and D. Varga. 2006. The JRC-Acquis:
A multilingual aligned parallel corpus with 20+ lan-
guages. Arxiv preprint cs/0609058.
81
