Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332?339,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Assessing the effectiveness of conversational features for dialogue
segmentation in medical team meetings and in the AMI corpus
Saturnino Luz
Department of Computer Science
Trinity College Dublinm Ireland
luzs@cs.tcd.ie
Jing Su
School of Computer Science and Statistics
Trinity College Dublin, Ireland
sujing@scss.tcd.ie
Abstract
This paper presents a comparison of two
similar dialogue analysis tasks: segment-
ing real-life medical team meetings into
patient case discussions, and segment-
ing scenario-based meetings into topics.
In contrast to other methods which use
transcribed content and prosodic features
(such as pitch, loudness etc), the method
used in this comparison employs only the
duration of the prosodic units themselves
as the basis for dialogue representation. A
concept of Vocalisation Horizon (VH) al-
lows us to treat segmentation as a clas-
sification task where each instance to be
classified is represented by the duration
of a talk spurt, pause or speech overlap
event in the dialogue. We report on the re-
sults this method yielded in segmentation
of medical meetings, and on the implica-
tions of the results of further experiments
on a larger corpus, the Augmented Multi-
party Meeting corpus, to our ongoing ef-
forts to support data collection and infor-
mation retrieval in medical team meetings.
1 Introduction
As computer mediated communication becomes
more widespread, and data gathering devices start
to make their way into the meeting rooms and the
workplace in general, the need arises for mod-
elling and analysis of dialogue and human com-
municative behaviour in general (Banerjee et al,
2005). The focus of our interest in this area is
the study of multi-party interaction at Multidis-
ciplinary Medical Team Meeting (MDTMs), and
the eventual recording of such meetings followed
by indexing and structuring for integration into
electronic health records. MDTMs share a num-
ber of characteristics with more conventional busi-
ness meetings, and with the meeting scenarios tar-
geted in recent research projects (Renals et al,
2007). However, MDTMs are better structured
than these meetings, and more strongly influenced
by the time pressures placed upon the medical pro-
fessionals who take part in them (Kane and Luz,
2006).
In order for meeting support and review systems
to be truly effective, they must allow users to effi-
ciently browse and retrieve information of interest
from the recorded data. Browsing in these media
can be tedious and time consuming because con-
tinuous media such as audio and video are difficult
to access since they lack natural reference points.
A good deal of research has been conducted on in-
dexing recorded meetings. From a user?s point of
view, an important aspect of indexing continuous
media, and audio in particular, is the task of struc-
turing the recorded content. Banerjee et al (2005),
for instance, showed that users took significantly
less time to retrieve answers when they had access
to discourse structure annotation than in a control
condition in which they had access only to unan-
notated recordings.
The most salient discourse structure in a meet-
ing is the topic of conversation. The content within
a given topic is cohesive and should therefore be
viewed as a whole. In MDTMs, the meeting con-
sists basically of successive patient case discus-
sions (PCDs) in which the patient?s condition is
discussed among different medical specialists with
the objective of agreeing diagnoses, making pa-
tient management decisions etc. Thus, the individ-
ual PCDs can be regarded as the different ?topics?
which make up an MDTM (Luz, 2009).
In this paper we explore the use of a content-
free approach to the representation of vocalisation
events for segmentation of MDTM dialogues. We
start by extending the work of Luz (2009) on a
small corpus of MDTM recordings, and then test
our approach on a larger dataset, the AMI (Aug-
332
mented Multi-Party Interaction) corpus (Carletta,
2007). Our ultimate goal is to analyse and apply
the insights gained on the AMI corpus to our work
on data gathering and representation in MDTMs.
2 Related work
Topic segmentation and detection, as an aid to
meeting information retrieval and meeting index-
ing, has attracted the interest of many researchers
in recent years. The objective of topic segmenta-
tion is to locate the beginning and end time of a
cohesive segment of dialogue which can be sin-
gled out as a ?topic?. Meeting topic segmentation
has been strongly influenced by techniques devel-
oped for topic segmentation in text (Hearst, 1997),
and more recently in broadcast news audio, even
though it is generally acknowledged that dialogue
segmentation differs from text and scripted speech
in important respects (Gruenstein et al, 2005).
In early work (Galley et al, 2003), meeting
annotation focused on changes that produce high
inter-annotator agreement, with no further specifi-
cation of topic label or discourse structure. Cur-
rent work has paid greater attention to discourse
structure, as reflected in two major meeting cor-
pus gathering and analysis projects: the AMI
project (Renals et al, 2007) and the ICSI meet-
ing project (Morgan et al, 2001). The AMI cor-
pus distinguishes top-level and functional topics
such as ?presentation?, ?discussion?, ?opening?,
?closing?, ?agenda? which are further specified
into sub-topics (Hsueh et al, 2006). Gruenstein
et al (2005) sought to annotated the ICSI cor-
pus hierarchically according to topic, identifying,
in addition, action items and decision points. In
contrast to these more general types of meetings,
MDTMs are segmented into better defined units
(i.e. PCDs) so that inter-annotator agreement on
topic (patient case discussion) boundaries is less of
an issue, since PCDs are collectively agreed parts
of the formal structure of the meetings.
Meeting transcripts (either done manually or
automatically) have formed the basis for a num-
ber of approaches to topic segmentation (Galley
et al, 2003; Hsueh et al, 2006; Sherman and Liu,
2008). The transcript-based meeting segmentation
described in (Galley et al, 2003) adapted the un-
supervised lexical cohesion method developed for
written text segmentation (Hearst, 1997). Other
approaches have employed supervised machine
learning methods with textual features (Hsueh et
al., 2006). Prosodic and conversational features
have also been integrated into text-based represen-
tations, often improving segmentation accuracy
(Galley et al, 2003; Hsueh and Moore, 2007).
However, approaches that rely on transcription,
and sometimes higher-level annotation on tran-
scripts, as is the case of (Sherman and Liu, 2008),
have two shortcomings which limit their applica-
bility to MDTM indexing. First, manual transcrip-
tion is unfeasible in a busy hospital setting, and
automatic speech recognition of unconstrained,
noisy dialogues falls short of the levels of accu-
racy required for good segmentation. Secondly,
the contents of MDTMs are subject to stringent
privacy and confidentiality constraints which limit
access to training data. Regardless of such appli-
cation constraints, some authors (Malioutov et al,
2007; Shriberg et al, 2000) argue for the use of
prosodic features and other acoustic patterns di-
rectly from the audio signal for segmentation. The
approach investigated in this paper goes a step fur-
ther by representing the data solely through what
is, arguably, the simplest form of content-free rep-
resentation, namely: duration of talk spurts, si-
lences and speech overlaps, optionally comple-
mented with speaker role information (e.g. medi-
cal speciality).
3 Content-free representations
There is more to the structure (and even the
semantics) of a dialogue than the textual con-
tent of the words exchanged by its participants.
The role of prosody in shaping the illocution-
ary force of vocalisations, for instance, is well
documented (Holmes, 1984), and prosodic fea-
tures have been used for automatic segmentation
of broadcast news data into sentences and topics
(Shriberg et al, 2000). Similarly, recurring audio
patterns have been employed in segmentation of
recorded lectures (Malioutov et al, 2007). Works
in the area of social psychology have used the sim-
ple conversational features of duration of vocalisa-
tions, pauses and overlaps to study the dynamics
of group interaction. Jaffe and Feldstein (1970)
characterise dialogues as Markov processes, and
Dabbs and Ruback (1987) suggest that a ?content-
free? method based on the amount and structure
of vocal interactions could complement group in-
teraction frameworks such as the one proposed
by Bales (1950). Pauses and overlap statistics
alone can be used, for instance, to characterise
333
Speakers
Vocalisationevents ...
Eventdurations ... ... ...
Gaps &Overlaps
Figure 1: Vocalisation Horizon for event v.
the differences between face-to-face and telephone
dialogue (ten Bosch et al, 2005), and a corre-
lation between the duration of pauses and topic
boundaries has been demonstrated for recordings
of spontaneous narratives (Oliveira, 2002).
These works provided the initial motivation for
our content-free representation scheme and the
topic segmentation method proposed in this paper.
It is easy to verify by inspection of both the corpus
of medical team meetings described in Section 4
and the AMI corpus that pauses and vocalisations
vary significantly in duration and position on and
around topic boundaries. Table 1 shows the mean
durations of vocalisations that initiate new topics
or PCDs in MDTMs and the scenario-based AMI
meetings, as well as the durations of pauses and
overlaps that surround it (within one vocalisation
event to the left and right). In all cases the dif-
ferences were statistically significant. These re-
sults agree with those obtained by Oliveira (2002)
for discourse topics, and suggest that an approach
based on representing the duration of vocalisa-
tions, pauses and overlaps in the immediate con-
text of a vocalisation might be effective for auto-
matic segmentation of meeting dialogues into top-
ics or PCDs.
Table 1: Mean durations in seconds (and standard
deviations) of vocalisation and pauses on and near
topic boundaries in MDTM and AMI meetings.
Boundary Non-boundary t-test
AMI vocal. 5.3 (8.2) 1.6 (3.5) p < .01
AMI pauses 2.6 (4.9) 1.2 (2.8) p < .01
AMI overlaps 0.4 (0.4) 0.3 (0.6) p < .01
MDTM vocal. 12.0 (15.5) 8.1 (14.7) p < .05
MDTM pauses 9.7 (12.7) 8.2 (14.8) p < .05
We thus conceptualise meeting topic segmenta-
tion as a classification task approachable through
supervised machine learning. A meeting can
be pre-segmented into separate vocalisations (i.e.
talk spurts uttered by meeting participants) and si-
lences, and such basic units (henceforth referred
to as vocalisation events) can then be classified
as to whether they signal a topic transition. The
basic defining features of a vocalisation event are
the identity of the speaker who uttered the vocali-
sation (or speakers, for events containing speech
overlap) and its duration, or the duration of a
pause, for silence events. However, identity la-
bels and interval durations by themselves are not
enough to enable segmentation. As we have seen
above, some approaches to meeting segmentation
complement these basic data with text (keywords
or full transcription) uttered during vocalisation
events, and with prosodic features. Our proposal
is to retain the content-free character of the ba-
sic representation by complementing the speaker
and duration information for an event with data de-
scribing its preceding and succeeding events. We
thus aim to capture an aspect of the dynamics of
the dialogue by representing snapshots of vocali-
sation sequences. We call this general representa-
tion strategy Vocalisation Horizon (VH).
Figure 1 illustrates the basic idea. Vocalisa-
tion events are placed on a time line and com-
bine utterances produced by the speakers who took
part in the meeting. These events can be labelled
with nominal attributes (s1, s2, . . .) denoting the
speaker (or some other symbolic attribute, such as
the speaker?s role in the meeting). Silences (gaps)
and group talk (overlap) can either be assigned re-
served descriptors (such as ?Floor? and ?Group?)
or regarded as separate annotation layers. The
general data representation scheme for, say, seg-
ment v would involve a data from its left context
(v1?, v2?, v3?, . . .) and its right context (v1, v2, v3, . . .)
in addition to the data for v itself. These can be a
combination of symbolic labels (in Figure 1, for
instance, s1 for the current speaker, s3, s2, s1, . . .
for the preceding events and s3, s2, s3, . . . for the
following events), durations (d, d1?, d2?, d3?, . . . etc)
334
SpeakersVoarocaloit
nnnir
virV.EedeElratcuropln G rtsce&Onscpeaker lipou.EedeEattilaloit 