Experimenting with the Interaction between Aggregation and 
Text Structuring 
Hua Cheng 
Division of Informatics 
University of Edinburgh 
80 South Bridge, Edinburgh EH1 1HN, UK 
Email: huacOdai ,  ed. ac.  uk 
Abst rac t  
In natural anguage generation, different gener- 
ation tasks often interact with each other in a 
complex way, which is hard to capture in the 
pipeline architecture described by Reiter (Re- 
iter, 1994). This paper focuses on the interac- 
tion between a specific type of aggregation and 
text planning, in particular, maintaining local 
coherence, and tries to explore what preferences 
exist among the factors related to the two tasks. 
The evaluation result shows that it is these pref- 
erences that decide the quality of the generated 
text and capturing them properly in a genera- 
tion system could lead to coherent text. 
1 In t roduct ion  
In automatic natural language generation 
(NLG), various versions of the pipeline archi- 
tecture specified by Reiter and Dale ((Reiter, 
1994) and (Reiter and Dale, 1997)) are usually 
adopted. They successfully modularise the gen- 
eration problem, but fail to capture the complex 
interactions between different modules. Take 
aggregation as an example. It combines imple 
representations to form a complex one, which 
in the mean time leads to a shorter text as a 
whole. There is no consensus as to where aggre- 
gation should happen and how it is related to 
other generation processes ((Wilkinson, 1995) 
and (Reape and Mellish, 1999)). 
We think that the effect of aggregation 
spreads from text planning to sentence reali- 
sation. The task of text planning is to se- 
lect the relevant information to be expressed 
in the text and organise it into a hierarchi- 
cal structure which captures certain discourse 
preferences such as preferences for global co- 
herence (e.g. the use of RST relations (Mann 
and Thompson, 1987)) and local coherence (e.g. 
center transitions as defined in Centering The- 
ory (Grosz et al, 1995)). Aggregation affects 
text planning by taking away facts from a se- 
quence featuring preferred center movements for 
subordination. As a result, the preferred cen- 
ter transitions in the sequence are cut off. For 
example, comparing the two descriptions of a 
necklace in Figure 1, 2 is less coherent han 1 
because of the shifting from the description of 
the necklace to that of the designer. To avoid 
this side effect, aggregation should be consid- 
ered in text planning, which might produce a 
different planning sequence. 
Aggregation is also closely related to the task 
of referring expression generation. A referring 
expression is used not only for identifying a ref- 
erent, but also for providing additional infor- 
mation about the referent and expressing the 
speaker's emotional attitude toward the refer- 
ent (Appelt, 1985). The syntactic form of a re- 
ferring expression affects how much additional 
information can be expressed, but it can only be 
determined after sentence planning, when the 
ordering between sentences and sentence com- 
ponents has been decided. This demands that 
the factors relevant o referring expression gen- 
eration and aggregation be considered at the 
same time rather than sequentially to generate 
referring expressions capable of serving multiple 
goals. 
In this paper, we are concerned with a specific 
type of aggregation called embedding, which 
shifts one clause to become a component within 
the structure of an NP in another clause. We 
focus on the interaction between maintaining 
local coherence and embedding, and describe 
how to capture this interaction as preferences 
among related factors. We believe that if these 
preferences are used properly, we would be able 
to generate more flexible texts without sacri- 
ficing quality. We implemented the preferences 
I. This necklace is in the Arts and Crafts style. Arts and Crafts style jewels usually have 
an elaborate design. They tend to have floral motifs. For instance, this necklace has floral 
motifs. It was designed by Jessie King. King once lived in Scotland. 
2. This necklace, which was designed by Jessie King, is in the Arts and Crafts style. Arts 
and Crafts style jewels usually have an elaborate design. They tend to have floral motifs. 
For instance, this necklace has floral motifs. King once lived in Scotland. 
Figure 1: An aggregation example 
in an experimental generation system based on 
a Genetic Algorithm to produce museum de- 
scriptions, which describe museum objects on 
display. The result shows that the system can 
generate a number texts of similar qualities to 
human written texts. 
2 Embedd ing  in a GA Text  P lanner  
To experiment with the interaction between 
maintaining local coherence and embedding, we 
adopt the text planner based on a Genetic Al- 
gorithm (GA) as described in (Mellish et al, 
1998). The task is, given a set of facts and a 
set of relations between facts, to produce a le- 
gal rhetorical structure tree using all the facts 
and some relations. A fragment of the possible 
input is given in Figure 2. 
A genetic algorithm is suitable for such a 
problem because the number of possible com- 
binations is huge, the search space is not per- 
fectly smooth and unimodal, and the genera- 
tion task does not require a global optimum 
to be found. The algorithm of (Mellish et al, 
1998) is basically a repeated two step process -
first sequences of facts are generated by apply- 
ing GA operators (crossover and mutation) and 
then the RS trees built from these sequences are 
evaluated. This provides a mechanism to inte- 
grate various planning factors in the evaluation 
function and search for the best combinations 
of them. 
To explore the whole space of embedding, we 
did not perform embedding on structured facts 
or on adjacent facts in a linear sequence be- 
cause these might restrict the possibilities and 
even miss out good candidates. Instead, we de- 
fined an operator called embedding mutation. 
It randomly selects two units (say Ui and Uk) 
mentioning a common entity from a sequence 
\[U1,U2,...,Ui,...,Uk,...,Uu\] to form a list \[Ui,Uk\] 
representing an embedding. The list substitutes 
the original unit Ui to produce a new sequence 
\[U1,U2,...,\[Ui,Uk\],...,Un\], which is then evalu- 
ated and ordered in the population. 
3 Captur ing  the  In teract ions  as 
Pre ferences  
A key requirement of the GA approach is the 
ability to evaluate the quality of a possible so- 
lution. We claim that it is the relative prefer- 
ences among factors rather than each individ- 
ual factor that play the crucial role in deciding 
the quality. Therefore, if we can capture these 
preferences in a generation system properly, we 
would be able to produce coherent text. In this 
section, we first discuss the preferences among 
factors related to text planning, based on which 
those for embedding can be introduced. 
3.1 P re ferences  for global coherence 
Following the assumption of RST, a text is glob- 
ally coherent if a hierarchical structure like an 
RST tree can be constructed from the text. In 
addition to the semantic relations and the Joint 
relation 1 used in (Mellish et al, 1998), we as- 
sume a Conjunct or Disjunct relation between 
two facts with at least two identical compo- 
nents, so that semantic parataxis can be treated 
as a combining operation on two subtrees con- 
nected by the relation. 
Embedding a Conjunct relation inside an- 
other semantic relation is not preferred because 
this could convey wrong information, for exam- 
ple, in Figure 3, 2 cannot be used to substitute 
1. Also a semantic relation is preferred to be 
used whenever possible. Here is the preferences 
concerning the use of relations, where "A>B" 
means that A is preferred over B: 
1In (Mellish et al, 1998), a Joint relat ion is used to 
connect every two text spans that  do not have a normal  
semant ic  relat ion in between. 
2 
fact (choker, is, broad, fact_no de- 1 ). 
fact('Queen Alexandra',wore,choker,fact_node-2). 
fact (choker,'can cover',scar,fact_node-3). 
fact(band,'might be made of',plaques,fact_node-4). 
fact(band/might be made of',panels,fact_node-5). 
fact(scar,is/on her neck',fact_node-6). 
rel(in_that_reln,fact_node-2,fact_node-3, ~). 
rel(conjunct,fact_node-4,fact_node-5,\[\]). 
Figure 2: A fragment of the input to the GA text planner 
1. The necklace is set with jewels in that it features cabuchon stones. Indeed, an Arts and 
Crafts style jewel usually uses cabuchon stones. An Arts and Crafts style jewel usually uses 
oval stones. 
2. The necklace is set with jewels in that it features cabuchon stones. Indeed, an Arts and 
Crafts style jewel usually uses cabuchon stones and oval stones. 
Figure 3: Conjunct and semantic relations 
Heur i s t i c  1 Preferences among features for 
global coherence: 
a semantic relation > Conjunct > Joint > 
parataxis in a semantic relation 
3.2 Preferences for local coherence 
In Centering Theory, Rule 2 specifies prefer- 
ences among center transitions in a locally co- 
herent discourse segment: sequences of continu- 
ation are preferred over sequences of retaining, 
which are then preferred over sequences of shift- 
ing. Instead of claiming that this is the best 
model, we use it simply as an example of a lin- 
guistic model being used for evaluating factors 
for text planning. 
Another type of center transition that ap- 
pears frequently in museum descriptions i asso- 
ciate shifting, where the description starts with 
an object and then moves to a closely associated 
object or perspectives of that object. Our ob- 
servation from museum descriptions shows that 
associate shifting is preferred by human writ- 
ers to all other types of movements except for 
center continuation. 
Oberlander et al (1999) define yet another 
type of transition called resuming, where an ut- 
terance mentions an entity not in the immedi- 
ate previous utterance, but in the previous dis- 
course. The following is the preferences among 
features for local coherence: 
Heur i s t i c  2 Preferences among center transi- 
tions and semantic relations: 
Continuation > Associate shifting > Retain- 
ing > Shifting > Resuming 
a semantic relation > Joint + Continuation 
3.3 Preferences for embedding 
For a randomly produced embedding, we must 
be able to judge its quality. We distinguish be- 
tween a good, normal and bad embedding based 
on the features it bears 2. A good embedding is 
one satisfying all following conditions: 
1. The referring expression is an indefinite, 
a demonstrative or a bridging description (as 
defined in (Poesio et al, 1997)). 
2. The embedded part can be realised as an 
adjective or a prepositional phrase (Scott and 
de Souza, 1990) 3. 
3. The embedded part does not lie between 
text spans connected by semantic parataxis or 
hypotaxis (Cheng, 1998). 
4. There is an available syntactic slot to hold 
the embedded part. 
2We do not claim that the set of features is complete. 
In a different context, more criteria might have to be 
considered. 
3We assume that syntactic onstraints have been in- 
serted before in text planning, using Meteer's Text Struc- 
ture (Meteer, 1992) for example. 
3 
A good embedding is highly preferred and 
should be performed whenever possible. A nor- 
mal embedding is one satisfying condition 1, 
3 and 4 and the embedded part is a relative 
clause. A bad embedding consists of all those 
left. 
To decide the preferences among embeddings 
and center transitions, let's look at the para- 
graphs in Figure 1 again. The only difference 
between them is the position of the sentence 
"This necklace was designed by Jessie King", 
which can be represented in terms of features of 
local coherence and embedding as follows: 
the last three sentences in 1: Joint + 
Continuation + Joint + Shifting 
the last two sentences plus embedding 
in 2: Joint + Resuming + Normal 
embedding 
Since 1 is preferred over 2, we have the fol- 
lowing heuristics: 
Heur i s t i c  3 Preferences among features for 
embedding and center transition: 
Continuation + Shifting + Joint > Resuming 
+ Normal embedding 
Good embedding > Normal embedding > 
Joint > Bad embedding 
Good embedding > Continuation + Joint 
4 Jus t i fy ing  the  Eva luat ion  Funct ion  
We have illustrated the linguistic theories that 
can be used to evaluate a text. However, they 
only give evidence in qualitative terms. For a 
GA-based planner to work, we have to come 
up with actual numbers that can be used to 
evaluate an RS tree. 
We extended the existing scoring scheme of 
(Mellish et al, 1998) to account for features 
for local coherence, embedding and semantic 
parataxis. This resulted in the rater 1 in Ta- 
ble 14 , which satisfied all the heuristics intro- 
duced in Section 3. 
We manually broke down four human written 
museum descriptions into individual facts and 
relations and reconstructed sequences of facts 
with the same orderings and aggregations as in 
4The table only shows the features we are concerned 
with in this paper. 
the original texts. We then used the evaluation 
function of the GA planner to score the RS trees 
built from these sequences. In the meantime, 
we ran the GA algorithm for 5000 iterations on 
the facts and relations for 10 times. All human 
texts were scored among the highest and ma- 
chine generated texts can get scores very close 
to human ones sometimes (see Table 2 for the 
actual scores of the four texts). Since the four 
human texts were written and revised by mu- 
seum experts, they can be treated as "nearly 
best texts". The result shows that the evalu- 
ation function based on our heuristics can find 
good combinations. 
To justify our claim that it is the preferences 
among generation factors that decide the co- 
herence of a text, we fed the heuristics into a 
constraint-based program, which produced a lot 
of raters satisfying the heuristics. One of them 
is given in Table 1 as the rater 2. We then gen- 
erated all possible combinations, including em- 
bedding, of seven facts from a human text and 
used the two raters to score each of them. The 
two distributions are shown in Figure 4. 
The qualities of the generated texts are nor- 
mally distributed according to both raters. The 
two raters assign different scores to a text as 
the means of the two distributions are quite dif- 
ferent. There is also slight difference in stan- 
dard deviations, where the deviation of Rater 
2 is bigger and therefore it has more distin- 
guishing power. Despite these differences, the 
behaviours of the two raters are indeed very 
similar as the two histograms are of roughly 
the same shape, including the two right halves 
which tell us how many good texts there are and 
if they can be distinguished from the rest. The 
difference in standard deviations is not signifi- 
cant at all. So the distributions of the scores 
from the two raters show that they behave very 
similarly in distinguishing the qualities of texts 
from the same population. 
As to what extent the two raters agree with 
each other, we drew the scatterplot of the 
scores, which showed a strong positive linear 
correlation between the variables representing 
the two scores. That is, the higher the score 
from rater 1 for a given text of the population, 
the higher the score from rater 2 tends to be. 
We also calculated the Pearson correlation co- 
efficient between the two raters and the corre- 
4 
Features/Factors 
I Values 
11 2 
Semantic relations 
a Joint -20 -46 
a Conjunct or Disjunct 10 11 
a relation other than Joint, Conjunct or Disjunct 21 69 
a Conjunct inside another semantic relation -50 -63 
a precondition ot satisfied -30 -61 
Center transitions 
a Continuation 20 7 
an Associate shifting 16 1 
a Shifting 14 -3 
resuming a previous center 6 -43 
Embedding 
a Good embedding 
a Normal embedding 
a Bad embedding 
Others 
topic not mentioned in the first sentence 
6 3 
3 0 
-30 -64 
-10 -12 
Table 1: Two different raters satisfying the same constraints 
scores of the human texts 
l~ighest scores of the generated texts 
average scores of the generated texts 
text 1 text2 text3 text4 
170 22 33 24 
167 24 31 25 
125.7 18.9 26.1 9.3 
Table 2: The scores of 
lation was .9567. So we can claim that for this 
data, the scores from rater 1 and rater 2 corre- 
late, and we have fairly good chance to believe 
our hypothesis that the two raters, randomly 
produced in a sense, agree with each other on 
evaluating the text and they measure basically 
the same thing. 
Since the two raters are derived from the 
heuristics in Section 3, the above result partially 
validates our claim that it is the relevant pref- 
erences among factors that decide the quality of 
the generated text. 
5 Summary  and Future  work  
This paper focuses on the complex interac- 
tions between embedding and planning local co- 
herence, and tries to capture the interactions 
as preferences among related features. These 
interactions cannot be easily modelled in a 
pipeline architecture, but the GA-based archi- 
tecture offers a mechanism to coordinate them 
in the planning of a coherent ext. The result 
shows to some extent that capturing the inter- 
four human written texts 
actions properly in an NLG system is important 
to the generation of coherence text. 
Our experiment could be extended in many 
aspects, for example, validating the evaluation 
function through empirical analysis of human 
assessments of the generated texts, and experi- 
menting with the interaction between aggrega- 
tion and referring expression generation. The 
architecture based on the Genetic Algorithm 
can also be used for testing interactions between 
or within other text generation modules. To 
generalise our claim, a larger scale experiment 
is needed. 
Acknowledgement  This research is sup- 
ported by a University of Edinburgh Stu- 
dentship. 
Re ferences  
Douglas Appelt. 1985. Planning english refer- 
ring expressions. Artificial Intelligence, 26:1- 
33. 
Hua Cheng. 1998. Embedding new informa- 
tion into referring expressions. In Proceedings 
5 
Figure 4: Histogram ofthe scores from rater 1 (top) 
and rater 2 (bottom) 
of COLING-ACL'98, pages 1478-1480, Mon- 
treal, Canada. 
Barbara Grosz, Aravind Joshi, and Scott We- 
instein. 1995. Centering: A framework for 
modelling the local coherence of discourse. 
Computational Linguistics, 21 (2):203-226. 
William Mann and Sandra Thompson. 1987. 
Rhetorical Structure Theory: A Theory 
of Text Organization. Technical Report 
ISI/RR-87-190, Information Sciences Insti- 
tute, University of Southern California. 
Chris Mellish, Alistair Knott, Jon Oberlander, 
and Mick O'Donnell. 1998. Experiments us- 
ing stochastic search for text planning. In 
Proceedings o\] the 9th International Work- 
shop on Natural Language Generation, On- 
tario, Canada. 
Marie Meteer. 1992. Expressibility and The 
Problem of Efficient Text Planning. Commu- 
nication in Artificial Intelligence. Pinter Pub- 
lishers Limited, London. 
Jon Oberlander, Alistair Knott, Mick O'Don- 
nell, and Chris Mellish. 1999. Beyond elabo- 
ration: Generating descriptive t xts contain- 
ing it-clefts. In T Sanders, J Schilperoord, 
and W Spooren, editors, Text Representation: 
Linguistic and Psycholinguistic Aspects. Ben- 
jamins, Amsterdam. 
Massimo Poesio, Renata Vieira, and Simone 
Teufel. 1997. Resolving bridging references in 
unrestricted text. Research paper hcrc-rp87, 
Centre for Cognitive Science, University of 
Edinburgh. 
Michael Reape and Chris Mellish. 1999. Just 
what is aggregation a yway? In Proceedings 
of the 7th European Workshop on Natural 
Language Generation, pages 20-29, Toulouse, 
France. 
Ehud Reiter and Robert Dale. 1997. Building 
applied natural language generation systems. 
Natural Language Engineering, 3(1):57-87. 
Ehud Reiter. 1994. Has a consensus nl gen- 
eration architecture appeared, and is it psy- 
cholinguistically plausible? In Proceedings of 
the 7th International Workshop on Natural 
Language Generation. 
Donia Scott and Clarisse Sieckenius de Souza. 
1990. Getting the Message Across in RST- 
based Text Generation. In R. Dale, C. Mel- 
lish, and M. Zock, editors, Current Research 
in Natural Language Generation, pages 47- 
73. Academic Press. 
John Wilkinson. 1995. Aggregation i Natural 
Language Generation: Another Look. Tech- 
nical report, Computer Science Department, 
University of Waterloo. 
G 
Pronomina l i za t ion  rev is i ted*  
Renate  Hensche l  and Hua Cheng ~md Mass imo Poes io  
HCRC,  University of Edinburgh, UK 
{henschel,huac,poesio}@cogsci .ed.ac.uk 
Abst rac t  
Pronolninalization has been related to tile idea 
of a local focus - a set of discourse entities in 
the speaker's centre of attention, for exmnple ill 
Gundel et al (1993)'s givenness hierarchy or 
in centering theory. Both accounts ay that the 
determination of tile tbcus depends on syntac- 
tic as well as pragmatic factors, but have not 
been able to pin those factors down. In this 
paper, we uncover the major factors which de- 
termine the focus set in descriptive texts. This 
new tbcus definition has been ew, luated with re- 
spect to two corporm museum exhibit labels, 
mid newspaper mtieles. It provides an opera- 
tionalizable basis for pronoun production, and 
has been implemented as the reusable module 
gnome-np. The algorithm l)ehind gnome-np is 
conlpared with the most recent pronoun gener- 
ation algorithm of McCoy and Strube (1999). 
1 In t roduct ion  
Besides the well established problem of ln'onoun 
resolution, pronoun generation is now attract- 
ing renewed attention. In the past, generation 
systelns generated pronouns without attaching 
much importance to the problem, one notable 
exception being the classical algorithm of Dale 
(1990), loosely based on centering theory. With 
the emergence of corpus based studies in comtm- 
rational linguistics, the question arises whether 
it is possible to refine known standard algo- 
rithms, or whether an improvement is only to 
be achieved with the hell) of world knowledge 
reasoning - a matter too complex to be dealt 
with reliably at this time. Tile Ibrlner direction 
is represented by tile pioneering work of McCoy 
and Strube (1999). They propose a refined algo- 
rithm for tile choice between definite description 
on the one hand and pronoun on the other for 
* The work reported in this paper has been calmed out 
with the tinancial support of UK ESPllC grant L51126. 
animate referents I , which is based on distancG 
time structure mid ambiguity constraints. 
Here we introduce a more general algoritlnn 
for the pronominalization decision that is valid 
not only for animate but for inanimate referents 
as well. In conformity with McCoy and Strube, 
we group noun phrases with definite determiner 
and proper nalnes together under tile term "det'- 
inite description". The algorithm proposes a 
new pronominalization strategy, which beyond 
McCoy and Strube (1999)'s criteria makes use 
of the discom'se status of the antecedent and 
parallelism effects. 
The algorithm has been implemented as the 
reusable module gnome-np. It has been re-used 
in tile web hypertext generation system ILEX 
(see Oberlander el; al. (1998)). It shows ml 
accuracy over 87% with respect o two corpora 
(each 5000 words) of difl'erent genres. 
2 Accounts  of  p ronomina l i za t ion  
In previous ace(rants pronominalization has 
been related to the idea of a local focus of ~d;- 
tention: a set of discourse referents who/wlfich 
is in the center of attention of the speaker (e.g. 
Sidner (1979), givenness hierarchy (Gun(lel et 
al., 1993), centering theory (Grosz et al, 1995), 
RAFT/RAPId. (Suri, 1993)). Whereas (Gundel 
el; al., 1993) do not atteml)t to make their fo- 
cus notion operationalizable, this has been at- 
tempted by fllrther develolmlents of centering. 
However these have mostly been applied to the 
pronoun resolution problem. In the following 
we discuss three versions of centering and show 
that their application to the pronoun generation 
problem is nevertheless linfited. 
Center ing .  Centering was developed to ex- 
plain local discourse coherence; the extent to 
which it benefits pronoun generation is how- 
ever not immediately clear, hi centering, 
1We llSO the terms "discourse ntity" and "refc'rent" 
synonymously in this paper. 
306 
the discour.~e ntiti(:s (:vok(:d in a (',(n'l;ain ui:- 
terall(;e 'tt i ga'e c~dle(t fi)rward-looldng centers 
(Cfs). It is assumed i;lmt they are 1)ari;ially or- 
der(:d. As a major dei;erminant of the ordering, 
the gramma.tical fun(:t;ion hierarchy (roughly: 
SUI/.I~>OIM>OTIII~;IIS) has 1)een 1)r,')t)o,~ed. 13e- 
CallSe other fa.cl;ors afl'e(:ting th(: ord(:r have no(; 
1)een (',lld)or;~ted in de(;ail, this ranking (as tit(' 
only Ol)erai, ionaliza/)le ha.n(lle) has 1)(:(x)m(: the 
si;:m(lard ra.nking in several comt)utational i)- 
1)\]ic~(,ions of (:entering. The 1)ackward-tooking 
center (hencefl)rth C1)) is a distinguished 1nero- 
bet of (;lie CI:~, which is defined as the most 
highly ranked member of the Ct5 of th(' previous 
lltterallee 'u , i_  \] which is realized in v, i .  The Cb is 
consid(:red as (;h(: \]o('al focus of ai;i;(:ntion. Cen- 
tering sta,t('s two rules. Only the first; rule makes 
~t (:brim at)()u(; l)ronominaliz~tion: ll!any (;lemen(; 
of the uti;eran(:e ui--1 is realiz(,,d in v,i as l)ro- 
nomh th(m l;he C\]) must l)e l)r()l~()minalized in 'it i 
as well. As lloted tw McCoy mid Struhe (\]999), 
this rule apl)lies only in (,h(; ease that (:we sul)se- 
qllent lltterallCeS share Hl()r('~ (;}l~tll o11(~ ref('A'ellt~ 
m:td (;hal; l;h(: non-el) ref(.'r(,.nt is 1)ronominalized 
in (,he se(xmd ui:termme. \]}ut why (;\]fis non-el) 
referent is realized as a, l)ronomt is not given t)y 
(~h(' (:heory. 
Itowev(:r, f()lk)wing mot(: (;he sl>irit of (xuli;er- 
ing tha.n the actual definition, ()he (:au under- 
stand (;he el) as (;he refin'ent which is prefer?d)lv 
l)ronominalize(t. General t)r(mominalizai;ion ()f 
the backward-looking center was in fact a claim 
of (mrly c(mtering, lmi; h~d 1;o l)e al)andone(1 be- 
cause of (:olmter-evidenee from r(,'al discourse. 
Bnt the id(::r (,ha.t t)l'on()nfinaliztti;ion of the Ct) 
could 1)e a, m(:ans of establishing lo(:td discourse 
(x)herence is still 1)revalenl;. \]t has accordingly 
})een use(t l)y seine generation systems to (:on- 
trol 1)ronominalization e.g. in the IIA,;X sys- 
l~em (O1)erlm~dcr et al, 1998), the el) is always 
realized as a pronomL 
Semant ic  center ing .  Centering is a.lso found 
in Dale (1990) as the method of t)ronominaliza- 
tion control. However, Dale's center detinii;ion 
differs from standard centering theory in i;hat it 
is defined semantically and not on the basis of 
a syni;aetie ranking. 2 This apl)roaeh has some 
appeal, espc.cially for generation, ix;cause it sup- 
l)ori, s the natural mo(hfla.rity bel;ween strate- 
2In 1)ari;icular, \])al(: adol)l;s (;he l'estll(; o1' I;he aci;ion 
(Ienol:ed 1)3' ihe previous claus(; of a recil)(: as (;he center. 
~i(: generation -wlfi(:h would determine t;he se- 
mantk: c(:nter for each uttera.llce and tactical 
g(:neration which dec.ides about granmmt:.ical 
fimetions. 
Funct iona l  center ing.  Finally, the cent;ering 
version suggest;ed \])y Stl'ul)e and Hahn (\]!)99) 
al>l)ears to r(:veal an underlying discourse mech- 
anism resl)onsil)le for centering: the information 
si;rH(:ture of  an u t terance  (roughly the given- 
new i)ai;t(:rn) is (;he de('l)er reason for the rank- 
ing ()\[ the %rward-h)oking (:eni:(:rs. This l)er- 
mii;s a generalization of sl;andar(l centering into 
a language-indel)ell(leld; l;heory eovering 1)oth 
trex: and fixed word-order languages. It is how- 
ever then surprising that this result is not made 
maximld use of in the Sltb.sc,,(lttent generaISon- 
orient, cA work of McCoy an(1 Strube (1999). 
Beyond center ing.  The questions wlfi(:h re- 
main ol)en with all (;hree al)l)roat:hes - stan- 
dard (;(;nterillg, S(:lll;~ll(;ic entering and fun(:-- 
tional eentering- are: 
\ ] .~ .~ \~?\]ly are ill real texts  a. lat%e nunl\])(:r of 
C1)'s not t)rononfinalized? 
_P21 \~q~y are non-C1)r(:ferents 1)ronominalized? 
or (:x1)ressc(l indel)(,.1Mently of centering: 
IP~I  \~qty are in real texl;s a large nunfl)(,r of 
(tis(:our,s(: entities with an ani;ecedent in i;h(', 
previous utteran(:e not l)ronomina\]ized? 
FI?2~ \NqW can more tlmn one entity 1)e t)ronom- 
inalize(l in one ui;t(,rance? 
\].h'on~ a corlms-driven view, question \ [~  is the 
larger prol)lem. 
~/\[(;C()y all(t Stl'lt\])e (19,()9) were the first to 
sng;~est all al~ol;il~llln for ~ellel'~ttioll which solves 
t;h(,se problems. It was motivated by the ol)- 
servation that ~t la.rge percentage of NPs which 
would have been realized by 1)ronouns using 
known algorithms, are in fact not, realized as 
pronomls in real text. They suggest that such 
NPs serve to mark ~time changes' in the dis- 
course. Their algorithm aecordingly makes use 
of distance, context ambiguity mid telnl)ora\] 
discourse structure to decide about 1)ronolni- 
nalization. In our work, we have considered a 
corpus of a ditl'erent genre in which I, emt)oral 
cha.nge does nol, 1)bW a determining role: de- 
script, ive (;exts. \?e 1)repose a new algorithm 
307 
that significantly simplifies the problem of pro- 
noun choice. It is based on a new definition of 
the local focus, which views the discourse status 
of the antecedent as the major motivation be- 
hind focusing. The algorithm performs equally 
well when applied to McCoy and Strube's cor- 
pus of newspaper articles. 
3 Corpus  ana lys is  
The algorithm we will present below has been 
developed in close relation to the MUSE corpus -
a corpus of museum exhibit labels a. The corpus 
is a collection of web pages of the Paul Getty 
Museum, pages from an exhibition catalogue, 
and pages froln a jewellery book. Typical char- 
acteristics are tile central role of inanimate ref- 
erents in these texts, and the lack of temporal 
change thus providing an interesting counter- 
t)art to the newst)aper genre investigated by Mc- 
Coy and Strube. 
With an overall set of around 5000 words, tile 
cortms contains 1450 NPs. Each NP has been 
annotated with respect o, among others, gram- 
matical function, discourse status, gender, num- 
ber, countability, and antecedent relationships. 
23% of the NPs form reference chains (i.e. at 
least two mentions of one and the same referent 
in one text), the other 77% are only mentioned 
once. We have 101 different reference chains; 
the chain-fbrming NPs fall into 10\] discourse- 
new and 213 anaphoric NPs. In the following, 
we will only discuss the anaphoric NPs. 50% of 
the anaphoric NPs are realized as definite de- 
scriptions, 50% as pronouns. We distinguish be- 
tween locally bound pronouns, which are deter- 
mined syntactically (Binding Theory, (Chore- 
sky, 1981)), and which we expect the tacti- 
cal generator to handle correctly, and pronouns 
which are not locally bound so-called dis- 
course pronouns. We investigated possible co l  
relations between the discourse 1)ronouns and 
semantic/pragmatic features of their context. 
The basic notions that we found were dis- 
tance, discourse status of the antecedent, and 
grammatical function of the antecedent. All 
three notions need a precise definition. 
D is tance .  ~lb be able to determine the dis- 
tance between a discourse entity and its an- 
tecedent, a precise determination f what counts 
3UI{L: h t tp  ://www. hcrc.  ed. ac. uk/ 'gnome/corpora 
as utterance unit is necessary. Following 
Kameyama (1998), we take as u t te rance  unit 
the finite clause, l{elative clauses and con> 
plenmnt clauses are not counted as utterances 
on their own. This means that we count 
clauses containing complement clauses or rel- 
ative clauses as single utterances. 4,a The pre- 
v ious u t te rance  is the preceding utterance at 
the same level of embedding. 
Note that we allow the treatment of clauses 
with VP coordination (subject ellipsis) as com- 
plex coordinated clauses as done in Kameyalna 
(1998), thus handling subject ellipsis as a dis- 
course pronoun; our algorithm does not; insist 
on this view however. 
The following correlation between pronoun 
use and distance was tbund in our corpus: 97% 
of the pronouns have an antecedent in the same 
or the previous utterance. 
D iscourse s tatus .  The information status of 
a discourse ntity in an utterance is either given 
or new. We use these terms with an identi- 
cal Lneaning as g~vn'nd and focus in Vallduvi 
(1993). Discourse status, as introduced by 
Prince (1992), is a similar but different notion: 
A discourse ntity is d iscourse-o ld,  if it has 
been mentioned before in the same discourse; 
it is d i scourse -new otherwise. All cases of 
givenness by indirect means like part-whole, 
set-member relationships, other bridging rela- 
tions, inferences (Prince's inferrables, anchored 
and situationally evoked entities) are judged as 
discourse-new, thus taking into account only 
tile identity antecedent relationship. We share 
Prince's opinion that pronominalization has to 
do with discourse status, whereas definiteness 
has to do with information status. 
66% of all short-distance discourse pronouns 
in the MUSE corpus refer to an antecedent which 
is in itself discourse-old. 
Sub jec thood .  The third strong correlation is
the relation between pronoun use and the grmn- 
matical function of the antecedent. 63% of dis- 
course pronouns have a subject as antecedent. 
The following table shows the overall distribu- 
tion of antecedent properties for short-distance 
4This deviates from Kameyama, who analyzes re- 
ported speech as separate utterance. 
'SComplement and relative clauses consisting of inore 
than one tinite clause create their own internal evel of 
focusing. 
308 
discourse 1}ronouns and (shown ill 1}rackets) h),' 
short-distance definite descriptions. 
old new 
,~,t)io{:{; a8% (22%) 25% (12%) 
~ot s,,1,.i 28% (18%) .(}% (48%) 
4 Algor i thm 
\] ~ased on these corpus  s{,ndy resnl|;s, we {lefhle a 
new notion of the local focus -- the set of refer- 
ents which arc awfilal)le for prononfilmlization. 
The local focus is ut}dated at each utterance 
boundary, and is defined as the set of referents 
of the 1)revious utterance which are: 
(a) d iscourse-o ld ,  or 
(b) rea l ized as sub jec t .  
This set {:all theoretically {:ontain \]11{}re thnu one 
\ ] . l t  ill \]HOSt cas,, s, O0 a.(1 (\])) are o,,o 
and the same singlel;on sol;, which coul{l be seen 
as the well-known Cb. Thus sta\]ldar(l cent{~r- 
ing app{,,a\]'s as ~t spe{'ial case of {}m a\])l}roa{:h. 
This account means that newly introduced r(;t L 
{;l'C\]l|,S arc ll(}l; immediately l)ron{}mina\]ized i\]1 
the following utterance, Ulfless they have bc{,.n 
introduced as subject- -ml ol)servation made l}y 
Brenlmn (19!}8) and now confirlile(t with respect 
to our data also. 
The l}rOl}osed efinition of the lo{:al focus ~,,ou-.. 
eralizes the t2){:using mechanisms assm~led i\]1 
{x211i,el'\]llg all(t intro(hlces the discourse status ()f 
the antecedent as (}he lnain {:rii;eriol~ })ehi\]\]d th{~ 
l}\]onomi\]laliza.ti{}n decision. It is i\]\]t{;\]'{>ting iT() 
lIOl,e I;ll:tl; \]X/i{:Coy all{1 Sl;rul}{; (1.9991 also nmke 
use of the discourse status of the ant(x:{,Aent; 
without mentioning it exl)lh:itly. For a. certah\] 
sul)sel; of' intrasentealtial nat)horic relal, ions ill 
amMguous cont(,xts they I)rOl}OSe \])ro\]\]on\]ina.1- 
ization in case the antec(;dent would 1)c the 1)re- 
ferred one in Stl'ut)e (1998)'s pr{}lloun resohl- 
lion algoritlnn. Because the set {}i' a.ntecedents 
is l'mlkcd there with resl)eCt to infbrm~tion sta- 
tus, this is identical with {mr proposal. Why 
tlmy do not use the discourse status as a gen- 
eral criBerion is not clear. We believe that the 
discourse status of the antecedent as pr{momi- 
nalization trigger is a general rule, in discourse 
Sell-l&ii/;,ies. 
The central role of discourse sLat;us and sub- 
jecthoo{1 are in our opinion 1101; accidental. The 
Bwo nol;ions retlecl; tw{\] tyt)ical stra{,egies 1;o 
introduce a new referenl; inBo l;he (liscourse. 
Wc will assume here the mnm~rked inf(}rnmt;ion 
structure of an utterance: g iven  - new. The 
subject usually is part of (or identical to) the 
9i'uen. Let X i)e a certain referent; which is newly 
intl'oduced in utterance (ul), and referred to 
again in t;lle following ui~tera.nce 0121. In the 
first strategy, X is introduced in the new non- 
subject )arl; of (u\].). And ill this l)ati;el'n the sec- 
ond lnention of X in (u2) is not pronominalized. 
In exalnple (1) given in Figure 1 tile local focus 
for ,,t ;el'a,,ce one el m0,1  : {t,.4; 
'm..,in 'morns" is new in (\]1\].) and \]1ol; pronominal -  
ized in (112). The other typical strategy is where 
the referent is tirst mentioned ill a subject posi- 
tion. This is typical for a segment onset, or the 
beginning of ~ text,. Ofl;en this referent is given 
})y other lneans -- for example, l)y refhrence to a 
1)icture., or to a r(;lated object. In example (2) 
of Figure \]., the second mention is i)rononfinal - 
ized. ~\]'\]ms 1;11(} sul)jecI; position seems to time- 
lion as creating a givemless allocation for the 
denotexl rcfercnl;. These two strategies roughly 
correspond with two types of thcnlatic develop- 
nlent identified in l)mm,q (197d). 
Para l le l i sm.  Our definition of l;he local tS- 
cus licenses 91% (62 of 68 pronouns) of all 
short-distance discourse pronouns in ore" corpus. 
Looking at tile pronomls violating the prol)osed 
accounl;, we nm.de ,,11 interesting observal;ion: 
n}osl; el l;heln occtlr ill conl;exts of strong t)ar- 
allelism. \'Vc call an anphoric NP ~/~,1~2 paral le l  
if it has ml a.ntccedenl; ' ~q)l in the previous utter- 
~Ill(;{;~ alld 'll,l) I alld '**,i12 \]rove Lhe 5alllC graummt i -  
cal funct ion.  1,k)l" work  wi th  real I;ex{;, il; is useful  
to inchlde cases whel'(~' 7L\]) 2 is a 1)osscssive or gen- 
itive NP inside a certain 'npa, and  'np\] and  'np:~ 
have the same gralnmatical flmction. Depend- 
ing on the concrel;e function, we distinguish sub- 
.ic{:t and object 1)arallelisln. Strong parallelism 
is a simulta.neous subject mid object para.lMism 
in two consecutive clauses. Strong i)arallelism 
always overrides the local focus criterion, mid 
allows tbr pronominalization of referents with 
discourse-new antecedents in nonsut)ject posi- 
tion. 
The local focus definition refilled by the par- 
allelism eff'ect is ml explanation for question P~ 
and a small portion of \ [~\ ] ,  but most cases of 
problem \ [~ r(nnain open.  q_'wo reasons for not  
prononfinalizing a reh~rent which is a nwanber 
of the local focus need to be considered: 
~ alnbiguous context, 
309 
(1) (\[11.) Shortly after irzh, eriti~,.q the building in 1752, he commissioned th, e areh, iteet Pierre Conta, nt 
d'Ivry to renovate the main  rooms. 
(u2) The engravings for these rooms , showing the wall lights in place, were reproduced in Dide~vt's 
Encyclopaedic, one of the principal works of the Age of E'nlightenment. 
(2) (ul) Scottish born, Canadian based jeweller, A l i son  Ba i ley -Smi th ,  constructs elaborate and cer- 
emoniaI jewellers} from irtd,ustrial wire. 
(u2) Her  materials are often gathered from so'arces such as abandoned television .sets ... 
(3) ~ i~ With attachments s'ach as an omtlav micvometer~ the microscope irtcovporates the latest sei- 
entitle technology of the mid-17OOs. 
(u2) The design of its era'ring gilt b~vnze stand was the heigh, t of the Rococo stifle ... 
(4) (u0) the table probably came from, the Tr ianon de Porcela ine , a small house built for th, e King's 
mistress, Madame de }l/\[ontespar~,, on the 9ro'unds of the Palace of Vet.sallies. 
(ul) This table's marquetry of ivory and horn, painted blue underneath, would have followed the 
house's  blue-and-white color sehcrne, imitating blue-and-white Chinese porcelain, a fashionable aTI.d 
highly prized material. 
(u2) Blue-arM-white cevarnie tiles decorated the house, ... 
Figure t: Corlms examples 
discourse structure signalling. 
Ambigu i ty .  Along with McCoy and Strube 
we argue that ambiguity with respect to gen- 
der/number influences the pronominalization 
decision: members of the local fbcus which have 
a competing referent (refbrent with similar gen- 
der/number) in some span to the left of the ref- 
erent to be generated should not be realized as 
t)ronouns o as to minimize the inference load 
for the reader. However, not to allow pronom- 
inalization in all ambiguous context situations 
does not ~I)t)ear to be consistent with real texts 
(McCoy and Strube, 1999). In the MUSE cortms 
one third of all focal NPs occur in ambiguous 
contexts, one half of them is pronominalized, 
the other half is not. Two questions require a 
precise answer to use the ambiguity constraint 
in a generation algorithm: 
? Which set of 1)reviously mentioned refer- 
ents or text st)an is taken into account br 
referents to be in competition? 
? Which referents are pronominalizcd despite 
an alnbiguous context? 
The answer is surprisingly simple: I/.eferents 
of the previous utterance which are not in the lo- 
cal fbcus do not disturb pronominalization, even 
if they have the stone gender/nmnber. Only if 
the actual referent has a competitor in the local 
fbcus, is pronominalization blocked. This is il- 
lustrated in Figure 1 with exmnples (3) and (4), 
respectively. In (3) the microscope is discourse- 
old and the only member in the local focus for 
(u2); the competing referents ocular micrvmc- 
tcr" mid technology are new and hence not local 
fbr utterance (u2). In (4), the local focus for 
(u2) is {the  at, th,  tl, e l,.o'asc} 
A slight improvement of the performance of 
the algorithm can be achieved by regarding 
the role of "heavy" nonrestrictive modification. 
hm\]uding the referents of discom'se-new NPs 
which are amplified by appositions or nonre- 
strictive relative clauses into the set; of' possible 
competitors improves accuracy slightly. 
D iscourse  s t ructure  signall ing. It is now 
known that detinite descriptions (or more gen- 
eral overspecified NPs) signal the start of a new 
discourse segment (Passommau, 1996; Vonk et 
al., 1992). For most generation systems gener- 
ate from an I/ST-like text; plan, discourse seg- 
ments are naturally given. The only question 
fl'om the generation perspective is the degree of 
detail provided by the segmentation. 
Our algorithm gnome-up assumes that the 
discourse segmentation has already been speci- 
fied. At each segment boundary, the local focus 
is set to n i l ,  thereby disallowing pronominal- 
ization for all discourse ntities of the first ut- 
terance in the segment onset. 
It is also well known that plmmed discourse 
with repeated phrases at the begimfing of a 
clause are seen as 'bad style'. Identical repeated 
pronouns at the clause onset are rarely found 
in expository and descriptive texts (2.6% of all 
discourse pronouns in our corpus). Hmnan writ- 
ers usually avoid possibly dull lack of variation 
by employing various aggregation techniques. 
310 
Let X 1)e a refl,'renl; I o 1)e generated in Ill;l;erailCO (112), and focu,.s 1)e the' scl; of rc'h;reni;s of the 1)rcvious 
ul;l;eran(:(; ul) which are 
(a) discoursc-okl, or 
(b) realized as subject. 
(1) X has an antecedent beyond a segment boundary def description 
(2) X has an antecedent two or more ul;i;cranccs distant def (lescril)tion 
(:~) X hits ~Ul alll;(X'(xl(,ll(; i l l  ( l l \ ] ) :  ~lll(l 
(3a) X occurs in strong 1)aralM contc'xt 1)ronoun 
(31)) X ? focu,.s (lcf dcscril)tion 
(3(:) X C .foc~z.s and 
? X has a coral)cling relbrcnt Y c focus dc'f description 
? X has a comp(~ting retba'cnt Y in (ul) amplitic(1 with appo- (lef dcscril)tion 
sition or nonrestrictive r lative clause 
? o lso  pro l lOl l l l  
The repeti|;ion 1)locking rul(; overri(lcs the 1)ronominalization suggesl;ed in (3c) to a definite description. 
Figme 2: The algoritlnn 
rl'hlls pronoun rel)etition 1)hwking seems 1.o Jw~ 
an aggregation trigger rather than ~ motivation 
for definite description generation. We hyl)ot\]> 
(;size l;hat t;he at)l)ar(mt Kcquen(:y of (lelinite (le- 
scriptions ill t)lmnm(l discourse has much to do 
with repetit ion blocking, but is used with re- 
specl; to a very line-grained, 1)tel)ably genre- 
specific discourse, si;rtlCl;lll'e. Olle candidate for 
this is the, t(,mt)oral structure in newst)at)er ar- 
i;ieles proposed by McCoy and Sta'ul)e. 
When evahutting Ollr algorM m l, w(' only used 
tile pa.l'agr~I)h seglnenl;~ti;ion given in the corpus. 
\]{lit for g;etlel'al;ioll systel l ls,  which usually sir(' 
not equil)l)Cd wit.h develol)ed a.ggrcgal.i(m eal- 
tries, we have also made avai\]ablc a t)ronoun rci> 
et,itioli blocking rule: If a discourse ntity in the 
local focus has a nont)ossessiv(~ l)ronomilml an- 
|;ecedelit, in'onolninalizal;ioli will 1)e J)loel?cd at 
this l/line. Figure 2 SUll:lnlarizcs the algorithm. 
The presented pronominalization algorithm 
has been implelnented ill the reusable module 
gnome-np, gnome-np consists of a colnponent 
for discom'se model lnanagement and one for 
NP form determination, it is designed to 1)e 
plugged ill ~~:\['1;(;1' text 1)lanning, coneeI)tualiza- 
ti(m, and sentence plalming, trot 1)etbre tactical 
generation. 
5 Eva luat ion  
A comparison of the t)erforlnance of our algo- 
r ithln with 1,he annotated MUS1.; corlms and Mc- 
Coy and Strube's newspatmr corlms is given in 
Table 1. The e, valuation has been carried out 
for the algorithm gnome-np without cm\])loying 
the rel)etith)n blocking rule and without; a line- 
grained discourse segmentation. Layout scg- 
lllell{;s Wel'e llse(l for the MUSE COl'l)llS. Beeal lse 
l/he munl)er of annotal;e(l seglnent OllSe\[;s Jill' the 
newsl)aper corpus is not easy to r('-estat)lish, wc 
giv(; here two figures fol" this eori)us: tirst with- 
out any segment, ons('t signalling (lower 1)ound), 
and second with the assulnt)l;ion that 15 short- 
distance definite (tcscriptions mark segment on- 
s<%s. The tigures include locally-herald l/re - 
nouns to yield J)(;tter cOlnl)arability wil;h McCoy 
and Sl;rul)e. '.\[lic, figur(,'s in l,hc, (:ohmms 'gnome- 
nil' represc, nl; I;\]lose NPs whose form is l)re(li(',led 
correctly 1)y 1;hi; new algoril;hm when evaluatc(l 
against l;h('~ a\]moi;at,(~(l corpora. 
The figures in T~d)le 1 show that  our al- 
gorithm performs very well in both domains, 
even without using a tiner discourse Seglnen- 
ration such as telnt)ol'al structure. Moreover, 
it; pertBrms better on McCoy and Stl'ul)e's cor- 
pus than their own algorithm, which success- 
fldly predicted the choice between realization by 
pronoml and realization by detinite description 
in 84.7% of all eases. The disagreements oc- 
('ur tirsl; tbr long distance t)rol~ouns (in our ter- 
lilino\]ogy: prollOtll lS lIlore than one clause dis- 
tanI;) and, second, ill hmger tel'trent chains with 
well established focus. For the latter, whereas 
gnome-np wouhl always suggest a tn'OlmUn, the 
real discourse swaps betweeli pronoun mid deft- 
nile description. Thus a finer segmentat ion or a 
repetit ion blocking rule could still improve the 
result fllrther. 
311 
MUSE gnome-r ip agreement  newspaper  gnome-np  agreement  
pronouus 112 101 90.2% 302 267 88.d% 
def descril)tions 101 86 85.1% 225 187 202 83.1% 89.70{o 
tota l  213 t87 87.8% 527 454 469 86.1% 89.0% 
'Dtble 1: Per formance compar ison 
6 Conc lus ions  
This  paper  has presented a new a lgor i thm tbr 
the pronomina l izat ion of third person discourse 
entit ies. The  Mgorithm, first, is imp lemented  
as a reusable module tbr generat ion systenls 
and, second, provides a theoret ical  account of 
pronomina l i zat ion  i general. 
The  proposed a lgor i thm provides a solution 
for quest ion \ [~  above by widening the defini- 
t ion of local Ibcus to be a set with possibly more 
than  one referent. The Mgor i thm also oilers a 
new solution fbr prob lem \ [ ~  above, aml)igu- 
ous pronoun generat ion.  Discom'se s t ructur ing 
(~\ ] )  is assumed as given. A sufficiently fine- 
grained discourse structur ing has been explored, 
for example,  by McCoy and Strube fbr their  do- 
n lmn of newspaper  articles, but  remMns an issue 
fbr future research fbr other domains.  We have 
shown that  next to 1)roxilnity, the discourse sta- 
tus of the mltecedcnt is a main cr iter ion for trig- 
gering pronomilml izat ion.  
The  suggested a lgor i thm general izes known 
fbcusing accounts. Gundel  et al (1993)'s cog- 
nit ive slat, us of being "in fbcus" is now approxi-  
mated  by the set of all discourse-old entit ies and 
the subject  of the previous utterance.  The  new 
focus determinat ion  is also a general izat ion of 
center ing's  Cb. The  focus so defined serves two 
funct ions s imultaneously:  to tr igger 1)ronomi- 
nMization, and to provide the set; of compet i tors  
for pronoun generat ion in ambiguous  contexts.  
A l though our t ra in ing corpus is too small  to jus- 
t i fy general  clMms, the ewduat ion with respect 
to tile newspaper  genre provides evidence that  
this f inding is valid for p lanned discourse ill gen- 
erM, independent  of the concrete genre. 
References 
Susan Brennan. 1998. Centering as a, psychological re- 
source for achieving joint reference in spontaneous 
discourse. In Marilyn A.  Walker, Aravind K..loshi, 
and Ellen F. Prince, editors, Centering Theory in Dis- 
course, pages 227 - 250. Clarendon Press, Oxtbrd. 
Noam Chomsky. 1981. Lectures on government and 
binding. Foris, l)ordrecht. 
Robert Dale. 1990. Generating referring cxpression.s. 
The MIT Press, Cambridge, Massachusetts. 
Fl'anti~ek Dane~. 1974. \]hmctional sentence perspective 
mid the orga.nisa.tion of the text. In Frantigek Dane~, 
editor, Papers on Functional Sentence Perspective, 
pages 106 128. Academia, Prague. 
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein- 
stein. 1995. Centering: A fl:amework for modelling 
the local coherence of discourse. Computational Lin- 
guistics, 21 (2):203 16d.  
Jeanette K. Gundel, Nancy Iledberg, and Ron Zaeharski. 
1993. Cognitive status aim the tbrm of rethr,:ing ex- 
pressions ill discourse. Lang'uagc , 69:27d 3(}7. 
Megulni Kameyama. 19.(t8. lntrasentcntial centering: A 
case study. In Marilyn A. Walker, Aravind K..Joshi, 
and Ellen F. Prince, editors, Centering 7'hcory in Dis- 
course, pages 89 -- 114. Clarendon Press, Oxtbrd. 
Ka.thlcen McCoy a,nd Michael Strube. 1999. Generating 
anaphorie xpressions: Pronoun or delinite descrip- 
tion? In Proceedings of ACL '99 Workshop: Refer- 
ence and discourse structure, pages 63 -- 71. 
J. Obcrlander, M. O'Donnell, A. Knott, and C. Mellish. 
1998. Conversation in the museuln: experiments in
dynamic hypermedia with the intelligent labelling ex- 
I)Iorcr. New l~,(:view of Multimedia and Hypermedia, 
pages 11 32. 
l/,ebecca Passonlmau. 1996. Using centering to re- 
lax gricean constraints on discourse anaphorie noun 
phrases. L(tngu, age wn, d ,qp('.ech, 39(2):229-- 264. 
Ellen F. Prince. 1992. The ZPC letter: Subjects, deli- 
niteness aim inforina.tion status. In W. C. Mam~ and 
S. A. Thompson, editors, Discourse desciption: Di- 
verse linguistic anab.lSCS of a flmd-raisi.ng text..lohn 
Benjamins, Amsterdam. 
Cmldace L. Sidner. 1979. 7bwards a computationally 
theory of definite anaphora comprehension i  English 
disourse. PhD thesis. 
Michael Strube and Udo ItMm. 1999. Functional cen- 
tering - grounding referential coherence in int'orma- 
tion structure. Computational Linguistics, 25(3):309 
- 344 .  
Michael Strube. 1998. Never look back: An alternative 
to centering. In Proceedings of Coling-ACL '98, pages 
1251 - 1257. 
Linda Z. Suri. 1993. Extending focussing frameworks to 
process complex sentences and to correct the written 
English of proficient signers of American Sign Lan- 
guage. PhD thesis. 
Enrico VMlduvi. 1993. lMbrmation packaging- a survey. 
Technical rel)ort, HCRC research Pal)er RP-d4. 
W. Vonk, G. Hustinx, and W. Simons. 1992. The use of 
referential expressions in structuring discourse. Lan- 
guage and Cognitive P~vcesses, 7(3 /4) :301  -333 .  
312 
 
	
	Specifying the Parameters of Centering Theory: a Corpus-Based
Evaluation using Text from Application-Oriented Domains
M. Poesio, H. Cheng, R. Henschel, J. Hitzeman,y R. Kibble,x and R. Stevenson
University of Edinburgh, ICCS and HCRC,
fpoesio,huac,henschelg@cogsci.ed.ac.uk
y The MITRE Corporation, hitz@linus.mitre.org
xUniversity of Brighton, ITRI, Rodger.Kibble@itri.bton.ac.uk
University of Durham, Psychology and HCRC, Rosemary.Stevenson@durham.ac.uk
Abstract
The definitions of the basic concepts,
rules, and constraints of centering the-
ory involve underspecified notions such
as ?previous utterance?, ?realization?,
and ?ranking?. We attempted to find the
best way of defining each such notion
among those that can be annotated reli-
ably, and using a corpus of texts in two
domains of practical interest. Our main
result is that trying to reduce the num-
ber of utterances without a backward-
looking center (CB) results in an in-
creased number of cases in which some
discourse entity, but not the CB, gets
pronominalized, and viceversa.
1 MOTIVATION
Centering Theory (Grosz et al, 1995; Walker et
al., 1998b) is best characterized as a ?parametric?
theory: its key definitions and claims involve no-
tions such as ?utterance?, ?realization?, and ?rank-
ing? which are not completely specified; their pre-
cise definition is left as a matter for empirical re-
search, and may vary from language to language.
A first goal of the work presented in this paper
was to find which way of specifying these param-
eters, among the many proposed in the literature,
would make the claims of centering theory most
accurate as predictors of coherence and pronomi-
nalization for English. We did this by annotating
a corpus of English texts with the sort of informa-
tion required to implement some of the most pop-
ular variants of centering theory, and using this
corpus to automatically check two central claims
of the theory, the claim that all utterances have a
backward looking center (CB) (Constraint 1), and
the claim that if any discourse entity is pronomi-
nalized, the CB is (Rule 1). In doing this, we tried
to make sure we would only use information that
could be annotated reliably.
Our second goal was to evaluate the predic-
tions of the theory in domains of interest for real
applications?natural language generation, in our
case. For this reason, we used texts in two gen-
res not yet studied, but of interest to developers of
NLG systems: instructional texts and descriptions
of museum objects to be displayed on Web pages.
The paper is organized as follows. We first re-
view the basic notions of the theory. We then dis-
cuss the methods we used: our annotation method
and how the annotation was used. In Section 4 we
present the results of the study. A discussion of
these results follows.
2 FUNDAMENTALS OF CENTERING
THEORY
Centering theory (Grosz et al, 1995; Walker et
al., 1998b) is an ?object-centered? theory of text
coherence: it attempts to characterize the texts
that can be considered coherent on the basis of
the way discourse entities are introduced and dis-
cussed.1 At the same time, it is also meant to
be a theory of salience: i.e., it attempts to pre-
dict which entities will be most salient at any
given time (which should be useful for a natural
language generator, since it is these entities that
are most typically pronominalized (Gundel et al,
1993)).
According to the theory, every UTTERANCE in
a spoken dialogue or written text introduces into
the discourse a number of FORWARD-LOOKING
CENTERS (CFs). CFs correspond more or less
1For a discussion of ?object-centered? vs. ?relation-
centered? notions of coherence, see (Stevenson et al, 2000).
to discourse entities in the sense of (Karttunen,
1976; Webber, 1978; Heim, 1982), and can be
linked to CFs introduced by previous or suc-
cessive utterances. Forward-looking centers are
RANKED, and because of this ranking, some CFs
acquire particular prominence. Among them, the
so-called BACKWARD-LOOKING CENTER (CB),
defined as follows:
Backward Looking Center (CB) CB(U
i+1
), the
BACKWARD-LOOKING CENTER of utter-
ance U
i+1
, is the highest ranked element of
CF(U
i
) that is realized in U
i+1
.
Utterance U
i+1
is classified as a CONTINUE if
CB(U
i+1
) = CB(U
i
) and CB(U
i+1
) is the most
highly ranked CF of U
i+1
; as a RETAIN if the CB
remains the same, but it?s not any longer the most
highly-ranked CF; and as a SHIFT if CB(U
i+1
) 6=
CB(U
i
).
The main claims of the theory are articulated in
terms of constraints and rules on CFs and CB.
Constraint 1: All utterances of a segment except
for the 1st have exactly one CB.
Rule 1: if any CF is pronominalized, the CB is.
Rule 2: (sequences of) continuations are pre-
ferred over (sequences of) retains, which are
preferred over (sequences of) shifts
Constraint 1 and Rule 2 express a preference for
utterances in a text to talk about the same ob-
jects; Rule 1 is the main claim of the theory about
pronominalization. In this paper we concentrate
on Constraint 1 and Rule 1.
One of the most unusual features of centering
theory is that the notions of utterance, previous
utterance, ranking, and realization used in the def-
initions above are left unspecified, to be appropri-
ately defined on the basis of empirical evidence,
and possibly in a different way for each language.
As a result, centering theory is best viewed as a
cluster of theories, each of which specifies the
parameters in a different ways: e.g., ranking has
been claimed to depend on grammatical function
(Kameyama, 1985; Brennan et al, 1987), on the-
matic roles (Cote, 1998), and on the discourse sta-
tus of the CFs (Strube and Hahn, 1999); there are
at least two definitions of what counts as ?previ-
ous utterance? (Kameyama, 1998; Suri and Mc-
Coy, 1994); and ?realization? can be interpreted
either in a strict sense, i.e., by taking a CF to be
realized in an utterance only if an NP in that utter-
ance denotes that CF, or in a looser sense, by also
counting a CF as ?realized? if it is referred to in-
directly by means of a bridging reference (Clark,
1977), i.e., an anaphoric expression that refers to
an object which wasn?t mentioned before but is
somehow related to an object that already has, as
in the vase . . . the handle (see, e.g., the discussion
in (Grosz et al, 1995; Walker et al, 1998b)).
3 METHODS
The fact that so many basic notions of centering
theory do not have a completely specified def-
inition makes empirical verification of the the-
ory rather difficult. Because any attempt at di-
rectly annotating a corpus for ?utterances? and
their CBs is bound to force the annotators to adopt
some specification of the basic notions of the the-
ory, previous studies have tended to study a par-
ticular variant of the theory (Di Eugenio, 1998;
Kameyama, 1998; Passonneau, 1993; Strube and
Hahn, 1999; Walker, 1989). A notable exception
is (Tetreault, 1999), which used an annotated cor-
pus to compare the performance of two variants
of centering theory.
The work discussed here, like Tetreault?s, is an
attempt at using corpora to compare different ver-
sions of centering theory, but considering also pa-
rameters of centering theory not studied in this
earlier work. In particular, we looked at different
ways of defining the notion of utterance, we stud-
ied the definition of realization, and more gener-
ally the role of semantic information. We did this
by annotating a corpus with information that has
been claimed by one or the other version of cen-
tering theory to play a role in the definitions of
its basic notions - e.g., the grammatical function
of an NP, anaphoric relations (including infor-
mation about bridging references) and how sen-
tences break up into clauses and subclausal units?
and then tried to find out the best way of specify-
ing these notions automatically, by trying out dif-
ferent configurations of parameters, and counting
the number of violations of the constraints and
rules that would result by adopting a particular
parameter configuration.
The Data
The aim of our project, which is called
GNOME and whose home page is at
http://www.hcrc.ed.ac.uk/ ~ gnome,
is to develop NP generation algorithms whose
generality is to be verified by incorporating
them in two distinct systems: the ILEX system
developed at the University of Edinburgh, that
generates Web pages describing museum objects
on the basis of the perceived status of its user?s
knowledge and of the objects she previously
looked at (Oberlander et al, 1998); and the
ICONOCLAST system, developed at the Univer-
sity of Brighton, that supports the creation of
patient information leaflets (Scott et al, 1998).
The corpus we collected includes texts from
both the domains we are studying. The texts
in the museum domain consist of descriptions
of museum objects and brief texts about the
artists that produced them; the texts in the
pharmaceutical domain are leaflets providing the
patients with the legally mandatory information
about their medicine. The total size of the corpus
is of about 6,000 NPs. For this study we used
about half of each subset, for a total number of
about 3,000 NPs, of which 103 are third person
pronouns (72 in the museum domain, 31 in the
pharmaceutical domain) and 61 are third-person
possessive pronouns (58 in the museum domain,
3 in the pharmaceutical domain).
Annotation
Previous empirical studies of centering theory
typically involved a single annotator annotat-
ing her corpus according to her own subjective
judgment (Passonneau, 1993; Kameyama, 1998;
Strube and Hahn, 1999). One of our goals was
to use for our study only information that could
be annotated reliably (Passonneau and Litman,
1993; Carletta, 1996), as we believe this will
make our results easier to replicate. The price
we paid to achieve replicability is that we could-
n?t test all hypotheses proposed in the literature,
especially about segmentation and about ranking.
We discuss some of the problems in what follows.
(The latest version of the annotation manual is
available from the GNOME project?s home page.)
We used eight annotators for the reliability study
and the annotation.
Utterances Kameyama (1998) noted that iden-
tifying utterances with sentences is problematic
in the case of multiclausal sentences: e.g., gram-
matical function ranking becomes difficult to
measure, as there may be more than one sub-
ject. She proposed to use all and only tensed
clauses instead of sentences as utterance units,
and then classified finite clauses into (i) utter-
ance units that constitute a ?permanent? update
of the local focus: these include coordinated
clauses and adjuncts) and (ii) utterance units that
result in updates that are then erased, much as
in the way the information provided by subor-
dinated discourse segments is erased when they
are popped. Kameyama called these EMBED-
DED utterance units, and proposed that clauses
that serve as verbal complements behave this way.
Suri and McCoy (1994) did a study that led them
to propose that some types of adjuncts?in particu-
lar, clauses headed by after and before?should be
treated as ?embedded? rather than as ?permanent
updates? as suggested by Kameyama; these re-
sults were subsequently confirmed by more con-
trolled experiments Pearson et al (2000). Nei-
ther Kameyama nor Suri and McCoy discuss par-
entheticals; Kameyama only briefly mentions rel-
ative clauses, but doesn?t analyze them in detail.
In order to evaluate these definitions of ut-
terance (sentences versus finite clauses), as well
as the different ways of defining ?previous utter-
ance?, we marked up in our corpus what we called
(DISCOURSE) UNITS. These include clauses, as
well as other sentence subconstituents which may
be treated as separate utterances, including paren-
theticals, preposed PPs, and (the second element
of) coordinated VPs. The instructions for mark-
ing up units were in part derived from (Marcu,
1999); for each unit, the following attributes were
marked:
 utype: whether the unit is a main clause,
a relative clause, appositive, a parenthetical,
etc.
 verbed: whether the unit contains a verb or
not.
 finite: for verbed units, whether the verb is
finite or not.
 subject: for verbed units, whether they have
a full subject, an empty subject (expletive,
as in there sentences), or no subject (e.g., for
infinitival clauses).
The agreement on identifying the boundaries of
units, using the  statistic discussed in (Carletta,
1996), was  = :9 (for two annotators and 500
units); the agreement on features(2 annotators
and at least 200 units) was follows:
Attribute  Value
utype .76
verbed .9
finite .81
subject .86
NPs Our instructions for identifying NP mark-
ables derive from those proposed in the MATE
project scheme for annotating anaphoric relations
(Poesio et al, 1999). We annotated attributes of
NPs which could be used to define their ranking,
including:
 The NP type, cat (pronoun, proper name,
etc.)
 A few other ?basic? syntactic features, num,
per, and gen, that could be used to identify
contexts in which the antecedent of a pro-
noun could be identified unambiguously;
 The grammatical function, gf;
 ani: whether the object denoted is animate
or inanimate
 deix: whether the object is a deictic refer-
ence or not
The agreement values for these attributes are as
follows:
Attribute  Value
ani .81
cat .9
deix .81
gen .89
gf .85
num .84
per .9
one of the features of NPs claimed to affect rank-
ing (Sidner, 1979; Cote, 1998) that we haven?t
so far been able to annotate because of failure
to reach acceptable agreement is thematic roles
( = :35).
Anaphoric information Finally, in order to
compute whether a CF from an utterance was re-
alized directly or indirectly in the following ut-
terance, we marked up anaphoric relations be-
tween NPs, again using a variant of the MATE
scheme. Theories of focusing such as (Sidner,
1979; Strube and Hahn, 1999), as well as our own
early experiments with centering, suggested that
indirect realization can play quite a crucial role in
maintaining the CB; however, previous work, par-
ticularly in the context of the MUC initiative, sug-
gested that while it?s fairly easy to achieve agree-
ment on identity relations, marking up bridging
references is quite hard; this was confirmed by,
e.g., Poesio and Vieira (1998). As a result we did
annotate this type of relations, but to achieve a
reasonable agreement, and to contain somehow
the annotators? work, we limited the types of re-
lations annotators were supposed to mark up, and
we specified priorities. Thus, besides identity
(IDENT) we only marked up three non-identity
(?bridging? (Clark, 1977)) relations, and only re-
lations between objects. The relations we mark
up are a subset of those proposed in the ?extended
relations? version of the MATE scheme (Poesio et
al., 1999) and include set membership (ELE-
MENT), subset (SUBSET), and ?generalized pos-
session? (POSS), which includes part-of relations
as well as more traditional ownership relations.
As expected, we achieved a rather good agree-
ment on identity relations. In our most recent
analysis (two annotators looking at the anaphoric
relations between 200 NPs) we observed no real
disagreements; 79.4% of these relations were
marked up by both annotators; 12.8% by only
one of them; and in 7.7% of the cases, one of
the annotators marked up a closer antecedent than
the other. Concerning bridges, limiting the re-
lations did limit the disagreements among an-
notators (only 4.8% of the relations are actually
marked differently) but only 22% of bridging ref-
erences were marked in the same way by both an-
notators; 73.17% of relations are marked by only
one or the other annotator. So reaching agreement
on this information involved several discussions
between annotators and more than one pass over
the corpus.
Segmentation Segmenting text in a reliable
fashion is still an open problem, and in addition
the relation between centering (i.e., local focus
shifts) and segmentation (i.e., global focus shifts)
is still not clear: some see them as independent
aspects of attentional structure, whereas other re-
searchers define centering transitions with respect
to segments (see, e.g., the discussion in the intro-
duction to (Walker et al, 1998b)). Our prelim-
inary experiments at annotating discourse struc-
ture didn?t give good results, either. Therefore,
we only used the layout structure of the texts
as a rough indication of discourse structure. In
the museum domain, each object description was
treated as a separate segment; in the pharmaceu-
tical domain, each subsection of a leaflet was
treated as a separate segment. We then identified
by hand those violations of Constraint 1 that ap-
peared to be motivated by too broad a segmenta-
tion of the text.2
Automatic computation of centering
information
The annotation thus produced was used to au-
tomatically compute utterances according to the
particular configuration of parameters chosen,
and then to compute the CFs and the CB (if any)
of each utterance on the basis of the anaphoric
information and according to the notion of rank-
ing specified. This information was the used to
find violations of Constraint 1 and Rule 1. The
behavior of the script that computes this informa-
tion depends on the following parameters:
utterance: whether sentences, finite clauses, or
verbed clauses should be treated as utter-
ances.
previous utterance: whether adjunct clauses
should be treated Kameyama-style or
Suri-style.
rank: whether CFs should be ranked according
to grammatical function or discourse status
in Strube and Hahn?s sense
2(Cristea et al, 2000) showed that it is indeed possible
to achieve good agreement on discourse segmentation, but
that it requires intensive training and repeated iterations; we
intend to take advantage of a corpus already annotated in this
way in future work.
realization: whether only direct realization
should be counted, or also indirect realiza-
tion via bridging references.
4 MAIN RESULTS
The principle we used to evaluate the different
configurations of the theory was that the best def-
inition of the parameters was the one that would
lead to the fewest violations of Constraint 1 and
Rule 1. We discuss the results for each principle.
Constraint 1: All utterances of a segment
except for the 1st have precisely one CB
Our first set of figures concerns Constraint 1:
how many utterances have a CB. This con-
straint can be used to evaluate how well cen-
tering theory predicts coherence, in the follow-
ing sense: assuming that all our texts are co-
herent, if centering were the only factor behind
coherence, all utterances should verify this con-
straint. The first table shows the results obtained
by choosing the configuration that comes clos-
est to the one suggested by Kameyama (1998):
utterance=finite, prev=kameyama, rank=gf, real-
ization=direct. The first column lists the number
of utterances that satisfy Constraint 1; the second
those that do not satisfy it, but are segment-initial;
the third those that do not satisfy it and are not
segment-initial.
CB Segment Initial NO CB Total Number
Museum 132 35 245 412
Pharmacy 158 13 198 369
Total 290 48 443 791
The previous table shows that with this config-
uration of parameters, most utterances do not sat-
isfy Constraint 1 in the strict sense even if we take
into account text segmentation (admittedly, a very
rough one). If we take sentences as utterances,
instead of finite clauses, we get fewer violations,
although about 25% of the total number of utter-
ances are violations:
CB Segment Initial NO CB Total Number
Museum 120 22 85 227
Pharmacy 152 8 51 211
Total 272 30 136 438
Using Suri and McCoy?s definition of previous
utterance, instead of Kameyama?s (i.e., treating
adjuncts as embedded utterances) leads to a slight
improvement over Kameyama?s proposal but still
not as good as using sentences:
CB Segment Initial NO CB Total Number
Museum 140 35 237 412
Pharmacy 167 14 188 369
Total 307 49 425 791
What about the finite clause types not consid-
ered by Kameyama or Suri and McCoy? It turns
out that we get better results if we do not treat as
utterances relative clauses (which anyway always
have a CB, under standard syntactic assumptions
about the presence of traces referring to the modi-
fied noun phrase), parentheticals, clauses that oc-
cur in subject position; and if we treat as a single
utterance matrix clauses with empty subjects and
their complements (as in it is possible that John
will arrive tomorrow).
CB Segment Initial NO CB Total Number
Museum 143 35 153 331
Pharmacy 161 14 159 334
Total 304 49 312 665
But by far the most significant improvement to the
percentage of utterances that satisfy Constraint 1
comes by adopting a looser definition of ?real-
izes?, i.e., by allowing a discourse entity to serve
as CB of an utterance even if it?s only referred to
indirectly in that utterance by means of a bridg-
ing reference, as originally proposed by Sidner
(1979) for her discourse focus. The following se-
quence of utterances explains why this could lead
to fewer violations of Constraint 1:
(1) (u1) These ?egg vases? are of exceptional
quality: (u2) basketwork bases support
egg-shaped bodies (u3) and bundles of straw
form the handles, (u4) while small eggs resting
in straw nests serve as the finial for each lid. (u5)
Each vase is decorated with inlaid decoration:
. . .
In (1), u1 is followed by four utterances. Only
the last of these directly refers to the set of egg
vases introduced in u1, while they all contain im-
plicit references to these objects. If we adopt this
looser notion of realization, the figures improve
dramatically, even with the rather restricted set of
relations on which our annotators agree. Now the
majority of utterances satisfy Constraint 1:
CB Segment Initial NO CB Total Number
Museum 225 35 71 331
Pharmacy 174 14 146 334
Total 399 49 217 665
And of course we get even better results by treat-
ing sentences as utterances:
CB Segment Initial NO CB Total Number
Museum 171 17 39 227
Pharmacy 168 7 36 211
Total 339 24 75 438
It is important, however, to notice that even un-
der the best configuration, at least 17% of utter-
ances violate the constraint. The (possibly, obvi-
ous) explanation is that although coherence is of-
ten achieved by means of links between objects,
this is not the only way to make texts coherent.
So, in the museum domain, we find utterances
that do not refer to any of the previous CFs be-
cause they express generic statements about the
class of objects of which the object under discus-
sion is an instance, or viceversa utterances that
make a generic point that will then be illustrated
by a specific object. In the following example,
the second utterance gives some background con-
cerning the decoration of a particular object.
(2) (u1) On the drawer above the door, gilt-bronze
military trophies flank a medallion portrait of
Louis XIV. (u2) In the Dutch Wars of 1672 -
1678, France fought simultaneously against the
Dutch, Spanish, and Imperial armies, defeating
them all. (u3) This cabinet celebrates the Treaty
of Nijmegen, which concluded the war.
Coherence can also be achieved by explicit
coherence relations, such as EXEMPLIFICA-
TION in the following example:
(3) (u1) Jewelry is often worn to signal membership
of a particular social group. (u2) The Beatles
brooch shown previously is another case in point:
Rule 1: if any NP is pronominalized, the CB is
In the previous section we saw that allowing
bridging references to maintain the CB leads to
fewer violations of Constraint 1. One should
not, however, immediately conclude that it would
be a good idea to replace the strict definition
of ?realizes? with a looser one, because there
is, unfortunately, a side effect: adopting an in-
direct notion of realizes leads to more viola-
tions of Rule 1. Figures are as follows. Us-
ing utterance=s, rank=gf, realizes=direct 22 pro-
nouns violating Rule 1 (9 museum, 13 pharmacy)
(13.4%), whereas with realizes=indirect we have
38 violations (25, 13) (23%); if we choose utter-
ance=finite, prev=suri, we have 23 violations of
rule 1 with realizes=direct (13 + 10) (14%), 32
with realizes=indirect (21 + 11) (19.5%). Using
functional centering (Strube and Hahn, 1999) to
rank the CFs led to no improvements, because of
the almost perfect correlation in our domain be-
tween subjecthood and being discourse-old. One
reason for these problems is illustrated by (4).
(4) (u1) A great refinement among armorial signets
was to reproduce not only the coat-of-arms but
the correct tinctures; (u2) they were repeated in
colour on the reverse side (u3) and the crystal
would then be set in the gold bezel.
They in u2 refers back to the correct tinctures (or,
possibly, the coat-of-arms), which however only
occurs in object position in a (non-finite) com-
plement clause in (u1), and therefore has lower
ranking than armorial signets, which is realized
in (u2) by the bridge the reverse side and there-
fore becomes the CB having higher rank in (u1),
but is not pronominalized.
In the pharmaceutical leaflets we found a num-
ber of violations of Rule 1 towards the end of
texts, when the product is referred to. A possi-
ble explanation is that after the product has been
mentioned sentence after sentence in the text, by
the end of the text it is salient enough that there
is no need to put it again in the local focus by
mentioning it explicitly. E.g., it in the following
example refers to the cream, not mentioned in any
of the previous two utterances.
(5) (u1) A child of 4 years needs about a third of
the adult amount. (u2) A course of treatment for
a child should not normally last more than five
days (u3) unless your doctor has told you to use
it for longer.
5 DISCUSSION
Our main result is that there seems to be a trade-
off between Constraint 1 and Rule 1. Allowing
for a definition of ?realizes? that makes the CB be-
have more like Sidner?s Discourse Focus (Sidner,
1979) leads to a very significant reduction in the
number of violations of Constraint 1.3 We also
noted, however, that interpreting ?realizes? in this
way results in more violations of Rule 1. (No
differences were found when functional center-
ing was used to rank CFs instead of grammati-
3Footnote 2, page 3 of the intro to (Walker et al, 1998b)
suggests a weaker interpretation for the Constraint: ?there is
no more than one CB for utterance?. This weaker form of
the Constraint does hold for most utterances, but it?s almost
vacuous, especially for grammatical function ranking, given
that utterances have at most one subject.
cal function.) The problem raised by these re-
sults is that whereas centering is intended as an
account of both coherence and local salience, dif-
ferent concepts may have to be used in Constraint
1 and Rule 1, as in Sidner?s theory. E.g., we might
have a ?Center of Coherence?, analogous to Sid-
ner?s discourse focus, and that can be realized in-
directly; and a ?Center of Salience?, similar to her
actor focus, and that can only be realized directly.
Constraint 1 would be about the Center of Coher-
ence, whereas Rule 1 would be about the Center
of Salience. Indeed, many versions of centering
theory have elevated the CP to the rank of a sec-
ond center.4
We also saw that texts can be coherent even
when Constraint 1 is violated, as coherence can
be ensured by other means (e.g., by rhetorical re-
lations). This, again, suggests possible revisions
to Constraint 1, requiring every utterance either
to have a center of coherence, or to be linked by a
rhetorical relation to the previous utterance.
Finally, we saw that we get fewer violations of
Constraint 1 by adopting sentences as our notion
of utterance; however, again, this results in more
violations of Rule 1. If finite clauses are used as
utterances, we found that certain types of finite
clauses not previously discussed, including rela-
tive clauses and matrix clauses with empty sub-
jects, are best not treated as utterances. We didn?t
find significant differences between Kameyama
and Suri and McCoy?s definition of ?previous ut-
terance?. We believe however more work is still
needed to identify a completely satisfactory way
of breaking up sentences in utterance units.
ACKNOWLEDGMENTS
We wish to thank Kees van Deemter, Barbara di
Eugenio, Nikiforos Karamanis and Donia Scott
for comments and suggestions. Massimo Poesio
is supported by an EPSRC Advanced Fellowship.
Hua Cheng, Renate Henschel and Rodger Kib-
ble were in part supported by the EPSRC project
GNOME, GR/L51126/01. Janet Hitzeman was in
part supported by the EPSRC project SOLE.
4This separation among a ?center of coherence? and a
?center of salience? is independently motivated by consid-
erations about the division of labor between the text planner
and the sentence planner in a generation system; see, e.g.,
(Kibble, 1999).
References
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987.
A centering approach to pronouns. In Proc. of the
25th ACL, pages 155?162, June.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2):249?254.
H. H. Clark. 1977. Inferences in comprehension. In
D. Laberge and S. J. Samuels, editors, Basic Pro-
cess in Reading: Perception and Comprehension.
Lawrence Erlbaum.
S. Cote. 1998. Ranking forward-looking centers. In
M. A. Walker, A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse, chapter 4, pages
55?70. Oxford.
D. Cristea, N. Ide, D. Marcu, and V. Tablan. 2000.
Discourse structure and co-reference: An empirical
study. In Proc. of COLING.
B. Di Eugenio. 1998. Centering in italian. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Cen-
tering Theory in Discourse, chapter 7, pages 115?
138. Oxford.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):202?225.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993.
Cognitive status and the form of referring expres-
sions in discourse. Language, 69(2):274?307.
I. Heim. 1982. The Semantics of Definite and In-
definite Noun Phrases. Ph.D. thesis, University of
Massachusetts at Amherst.
M. Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
M. Kameyama. 1998. Intra-sentential centering: A
case study. In M. A. Walker, A. K. Joshi, and
E. F. Prince, editors, Centering Theory in Dis-
course, chapter 6, pages 89?112. Oxford.
L. Karttunen. 1976. Discourse referents. In J. Mc-
Cawley, editor, Syntax and Semantics 7 - Notes from
the Linguistic Underground. Academic Press.
R. Kibble. 1999. Cb or not Cb? centering applied to
NLG. In Proc. of the ACL Workshop on discourse
and reference.
D. Marcu. 1999. Instructions for manually annotat-
ing the discourse structures of texts. Unpublished
manuscript, USC/ISI, May.
J. Oberlander, M. O?Donnell, A. Knott, and C. Mel-
lish. 1998. Conversation in the museum: Exper-
iments in dynamic hypermedia with the intelligent
labelling explorer. New Review of Hypermedia and
Multimedia, 4:11?32.
R. Passonneau and D. Litman. 1993. Feasibility of
automated discourse segmentation. In Proceedings
of 31st Annual Meeting of the ACL.
R. J. Passonneau. 1993. Getting and keeping the cen-
ter of attention. In M. Bates and R. M. Weischedel,
editors, Challenges in Natural Language Process-
ing, chapter 7, pages 179?227. Cambridge.
J. Pearson, R. Stevenson, and M. Poesio. 2000. Pro-
noun resolution in complex sentences. In Proc. of
AMLAP, Leiden.
M. Poesio and R. Vieira. 1998. A corpus-based inves-
tigation of definite description use. Computational
Linguistics, 24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The
MATE meta-scheme for coreference in dialogues in
multiple languages. In M. Walker, editor, Proc. of
the ACL Workshop on Standards and Tools for Dis-
course Tagging, pages 65?74.
D. Scott, R. Power, and R. Evans. 1998. Generation
as a solution to its own problem. In Proc. of the
9th International Workshop on Natural Language
Generation, Niagara-on-the-Lake, CA.
C. L. Sidner. 1979. Towards a computational theory
of definite anaphora comprehension in English dis-
course. Ph.D. thesis, MIT.
R. Stevenson, A. Knott, J. Oberlander, and S McDon-
ald. 2000. Interpreting pronouns and connectives.
Language and Cognitive Processes, 15.
M. Strube and U. Hahn. 1999. Functional centering?
grounding referential coherence in information
structure. Computational Linguistics, 25(3):309?
344.
L. Z. Suri and K. F. McCoy. 1994. RAFT/RAPR
and centering: A comparison and discussion of
problems related to processing complex sentences.
Computational Linguistics, 20(2):301?317.
J. R. Tetreault. 1999. Analysis of syntax-based pro-
noun resolution methods. In Proc. of the 37th ACL,
pages 602?605, University of Marylan, June. ACL.
M. A. Walker, A. K. Joshi, and E. F. Prince, editors.
1998b. Centering Theory in Discourse. Oxford.
M. A. Walker. 1989. Evaluating discourse process-
ing algorithms. In Proc. ACL-89, pages 251?261,
Vancouver, CA, June.
B. L. Webber. 1978. A formal approach to discourse
anaphora. Report 3761, BBN, Cambridge, MA.
An Empirical Analysis of Constructing Non.restrictive NP 
Modifiers to Express Semantic Relations 
Hua Cheng and  Chr i s  Me l l i sh  
D iv i s ion  of In fo rmat ics ,  Un ivers i ty  of  Ed inburgh  
80 South  Br idge,  Ed inburgh  EH1 1HN,  UK  
huac, chr ism @ dai. ed. ac. uk 
Abst rac t  
It is not a rare phenomenon for human written text 
to use non-restrictive NP modifiers to express es- 
sential pieces of information or support the situa- 
tion presented in the main proposition containing 
the NP, for example, "Private Eye, which couldn't 
afford the libel payment, had been threatened with 
closure." (from Wall Street Journal) Yet no previ- 
ous research in NLG investigates this in detail. This 
paper describes corpus analysis and a psycholinguis- 
tic experiment regarding the acceptability of using 
non-restrictive NP modifiers to express emantic re- 
lations that might normally be signalled by 'because' 
and 'then'. The experiment tests several relevant 
factors and enables us to accept or reject a number 
of hypotheses. The results are incorporated into an 
NLG system based on a Genetic Algorithm. 
1 In t roduct ion  
To produce natural language text, an NLG system 
must be able to choose among possible paraphrases 
one that satisfies the highest number of constraints 
in a certain context. Paraphrases can use various 
constructions, for example, using nominalisation i - 
stead of a clause for event representation. We are 
particularly interested in the use of non-restrictive 
(NR) modifiers within a referring expression to ex- 
press certain semantic relationQ other than object- 
attribute elaboration (in the sense defined in (Mann 
and Thompson, 1987)), for instance, causal rela- 
tions, which are normally expressed by separate 
clauses connected by cue phrases (Knott, 1996) such 
as 'because '. 
"A non-restrictive component gives additional in- 
formation to a head that has already been viewed 
as unique or as a member of a class that has been 
independently identified,-mud therefoee is not' essml; 
tial for the identification of the head" (Quirk et al, 
1985). This definition can be extended to account 
for modifiers of not only definite referring expres- 
sions, but also definite and indefinite NPs of var- 
ious types. In this paper, an NR modifier refers 
to any NP modifying component that is not essen- 
tial for identifying the object denoted by the head, 
including all modifiers of an NP that does not in- 
tend to identify (e.g. indefinite referring expressions 
and predicative phrases) (Kronfeld, 1990). Our dis- 
cussion focuses on definite referring expressions in- 
cluding proper-names because of the dominance of 
such examples in our corpus. However, we would 
expect no difficulty in applying our observation to 
other types of NPs. 
The semantic roles of NR modifiers, in particular 
NR clauses, are mentioned in many grammar and 
linguistics books. Quirk et al (1985) point out that 
an NR clause in a referring expression is usually neu- 
tral in its semantic role (i.e. it provides descriptive 
information about its head), but sometimes it can 
contribute to the semantics of the main clause in 
a variety of ways. They summarise three types of 
semantic relations that can be expressed by an NR 
clause (examples are given in Figure 1): 
? Causal, where the situation in the main clause 
is caused by that in the NR clause, e.g. (la). 
? Temporal, where the two clauses form a time 
sequence, e.g. (lb). 
? Circumstantial, where the NR clause sets a tem- 
poral or spatial framework for interpreting the 
main clause, e.g. (lc). 
Halliday (1985) mentions that a subordinate 
clause can elaborate a part of its primary clause 
through restating, clarifying, refining or adding a de- 
scriptive attribute or comment (see (2) of Figure 1). 
Halliday's notion of elaboration is much more gen- 
eral than that in other coherence theories like RST 
(Maim "andThompson; t987), and  :the rdat ion ex- 
pressed in (2) would not be treated as elaboration 
in most NLG systems. 
Similar phenomena were observed from the MUSE 
corpus 2, a corpus of museum exhibit labels, which 
l kVe are concerned with semant ic  ( informational) relations 
in this paper? Argumentat ive {intentional) relations are be- 
yond the scope of this paper. 
2This corpus is collected and annotated for the GNOME 
project (Poesio, 2000), which aims at developing eneral al- 
gor i thms for generat ing nominal expressions. 
108 
(1) a. 
b. 
C. 
He sent ahead the se,yeant, who was the most  exper ienced scout in the company.  
In 1960 he came to ,London4 .:wh.are. :he :haa~lived ? ever  ~in~ze. 
The boy, who had his satchel  trail ing behind him, ran past. 
(2) Inflation, which was necessary  fo r  the system,  became also "lethal. 
(3) In spite of his French name, Martin Carlin was born in Germany and emigrated to Paris to become 
an ebeniste. 
Figure 1: -Examples for NR modifiers .contributing. to the semantics of the main clauses 
? _ . . . . . . .  . ? ~ ' . ,  . . . . . .  _- - 
describe museum objects on display. For example, 
in (3) of Figure 1, the modifier French is not for 
identifying the name, but for establishing a conces- 
sion relation between the main proposition and the 
subordinate phrase to increase the reader's positive 
regard for where Martin Carlin was born. 
For the convenience of discussion, we define some 
terminology to be used throughout the paper: 
An NR construction/sentence : a sentence that has 
a main clause and a subordinate NR modifier 
attached to one of its NPs (e.g. (4b) of Fig- 
ure 2). 
A hypotactic onstruction/sentence : a sentence 
that has a main clause and a dependent clause, 
connected by a cue phrase. This is a common 
way of expressing semantic relations such as 
causality (e.g. (4a) of Figure 2). In this syn- 
tactic category, we single out a subclass of sen- 
tences according to one possible semantic on- 
nection between the two clauses. It is defined 
below. 
An elaboration realisation : a type of hypotactic 
construction where one clause elaborates the se- 
mantics of the other. We take cue phrases "as 
for" or "what is more" to signal elaboration re- 
lations 3. 
Previous research in NLG mainly focuses on us- 
ing NR constructions to realise elaboration relations 
but not other semantic relations (e.g. (Scott and 
de Souza, 1990) and (Hovy, 1993)). The NR modi- 
tier usually adds a descriptive attribute to the object 
denoted by the head. 
The linguistic research suggests for an NLG sys- 
tem the possibility to express certain semantic rela- 
cue phrases in most cases, and therefore could avoid 
using cues too heavily., This could be a better re- 
alisation under certain circumstances. Secondly, an 
NR construction enables a wider range of relations 
(especially those that are preferred to be expressed 
implicitly) to be selected for text structuring because 
the corresponding syntactic option is available. 
To understand how to enable an NLG system to 
generate such modifiers, we are faced with two ques- 
tions, which are not answered by linguistic research: 
1. Can this type of modifier be identified by human 
subjects, i.e. can humans tell the difference be- 
tween different NP modifier uses? 
2. Under what circumstances can an NR construc- 
tion be used in substitution of a hypotactic on- 
struction without changing the meaning dra- 
matically and how close are the meanings con- 
veyed by the two representations? 
An NLG system must come up with some solu- 
tions, simple or complex, to these two questions in 
order to choose among paraphrases. In this paper, 
we use cue phrases ms a signal of semantic relations 
rather than try to identify the relations directly. 
We describe systematically controlled experiments 
aimed at finding out the factors related to the gen- 
eration of this type of modifier in referring expres- 
sions. The result is intended to be reliable enough 
to be used by NLG systems in generating descriptive 
text. 
2 Corpus  annotat ion  
To answer the first question, we annotated the 
MUSE corpus, from which we have observed three 
types of modifier uses in an NP: 
tions through NR constructions, which is important . . . .  Firstly,. pro~i.ding .properties ?o .uniquely identify 
in two aspects. Firstly, an  NR construction gives--.--the objects or concepts denoted bythe-NP .Wi th -  
a more concise alternative realisation for a relation, 
where the relation is expressed implicitly rather than 
explicitly and usually more subtly. It does not need 
3\Ve acknowledge that these cue phrases are controversial 
in their semantic interpretations, but not using cue phrases 
would be even more ambiguous. Besides, our experiment does 
not heavily depend on these cue phrases. 
out these modifiers, the NP can denote more than 
one object/concept or sets of objects/concepts and 
is ambiguous in its interpretation, e.g. those in (6a). 
Such modifiers usually appear in phrases headed by 
the definite article 'the', which according to Loebner 
(1987) has the same meaning in all its uses, includ- 
ing in generic references and predicatives. Modifiers 
109 
(4) a. 
b. 
(5) a. 
b. 
Private Eye had been threatened with closure because it couldn't afford the libel payment. 
Private ~Ege;-',which. couldn~t~.a~o.rd.thevlibel. :paymen.t,.,: had:~been~threa~ned'with" closure. 
But P&G contends the new Cheer is a unique formula that also offers an ingredient hat prevents 
colors from fading. And retailers are expected to embrace the product, because it will take up less 
shelf space. 
And retailers are expected to embrace the product, which will take up less shelf  space. 
Figure 2: Examples for inferrability 
in other types of genericreferences, .g. indefini:tes; 
also belong here. 
This type subsumes the modifiers normally con- 
sidered by the referring expression generation mod- 
ule of an NLG system for uniquely identifying the 
referents (e.g. (Dale, 1992)). 
Secondly, having no effect in constraining a unique 
or unambiguous concept out of the NP which is ei- 
ther already unique or not required to have a unique 
interpretation, but being important o the situation 
presented in the main proposition containing the NP. 
This type includes the modifiers described in the 
previous section and many modifiers in indefinite 
predicatives, e.g. that in (6b). 
Thirdly, providing additional details about the 
referents of the NP, which functions the same way 
as the NP without these modifiers, e.g. those in 
(6c). The effect of such modifiers is usually local 
to the heads they describe rather than to the main 
propositions as a whole, which is the main difference 
between this and the second type of modifier. 
This type subsumes the modifiers normally gen- 
erated by an aggregation module, in particular one 
using embedding (e.g. (Shaw and McKeown, 1997), 
(Cheng, 1998)). 
(6) a. the decoration on this cabinet; the best 
looking food I ever saw 
b. This is a mighty  empty  country. 
c. the wide gilt bronze straps on the cof- 
fer fronts and sides; He lived in a five- 
room apartment in the Faubourg Saint-  
Anto ine .  
To find out whether the above distinctions make 
sense to human subjects, we designed an annotation 
scheme for modifiers in NPs, describing which ele- 
ments of an NP should be marked as a modifier and 
how to mark the features for a modifier. Apart from 
other features, each modifier should be anno/atecl 
with a pragmatic function feature (PRAGM), which 
specifies why a modifier is used it: an NP. The pos- 
sible values for this feature are unique, int and attr, 
corresponding to the three types of modifier uses de- 
scribed above (we will use the value names to refer 
to the different types of modifier in the rest of this 
paper). X.XlL was used as the markup language. 
We' had -two trained annotators mark the  NP  
modifiers in the MUSE corpus according to their 
understanding of the scheme. The agreement be- 
tween them on the PRAGM feature by means of the 
Kappa statistic (Caxletta, 1996) is .734, which means 
that the distinctions we are trying to make can be 
identified by human subjects to some extent. The 
main ambiguity exists between int and attr modi- 
fiers. There seems to be a gradual difference between 
them and where to draw the line is a bit arbitrary. 
In the MUSE corpus annotated so far, 19% of 1078 
modifiers in all types of NPs axe identified as int. So 
this is not a trivial phenomenon. 
3 An  exper iment  
We reduced the size of the problem of when to use 
an NR construction by focusing on two relations: a 
causal relation signalled by 'because' and a temporal 
relation signalled by 'then'. The reason for choosing 
these relations is that the possibilities of expressing 
them through NR constructions have already been 
shown by linguists. The two cue phrases are typical 
for the corresponding relations and can often substi- 
tute other cue phrases for the same relations. In the 
rest of this paper, we will still use the term causal 
or temporal relation, but what we actually mean is 
the specific relation signalled by 'because' or 'then'. 
3.1 Independent  var iab les  and  hypotheses  
From the generation point of view, our question is: 
given two facts and the semantic relation between 
them, what extra input do we need for making real- 
isation decisions? 
We collected examples of 'because' sentences from 
the MUSE corpus, and Wall .Street Journal source 
data, and transfered them to NR sentences by hand. 
Comparing the two constructions, we found some 
~, .An~eresting..vaxiation.:. _Eor.example,:compaxing the 
sentences in Figure 2, we found intuitively that the 
meanings of (4a) and (4b) are much closer than those 
of (5a) and (5b). In other words, (4b) can be used 
in substitution of (4a), whereas (5b) cannot, so easily 
41n (Carletta, 1996), a value of K between .8 and I in- 
dicates good agreement; a value between .6 and .8 indicates 
some agreement. 
110 
I ndependent  Var iables I\] Levels 
.Relation ...causal , temporal 
Inferrability strong weak 
Position initial final 
Order hypotactic vs. NR NR vs. hypotactic 
Subordination I nuc subordinate sat subordinate 
Cued/NoCue I use cue not use cue 
Table 1: Independent variables and their values 
substitute (5a). A simiiar pa~ttern can be foun(i in a 
number of other collected sentences. 
We claim that it is the degree ofinferrability of the 
relation between the semantics expressed through 
the two clauses that makes the difference. We define 
the inferrability of a causal/temporal relation as: 
Given two separate \]acts, the likeli- 
hood of human subjects inferring from their 
world knowledge that a causal/temporal 
connection between the \]acts might plausi- 
bly exist. 
In examples (4) and (5), the fact that Private Eye 
cannot afford the libel payment is very likely to di- 
rectly cause the closure threaten, whereas a prod- 
uct occupying less space is not usually a cause of 
it being accepted by retailers according to common 
sense. Therefore, the two realisations in (4) can be 
used in substitution of one another whereas those in 
(5) cannot. 
In\]errability is dynamic and user dependent. 
Given two facts, people with different background 
knowledge can infer the relation between them with 
different ease. If a relation is easily recognisable 
according to general world knowledge, we say that 
the inferrability of the relation is globally strong, 
in which case a hypotactic and an NR construction 
can express the relation almost equally well (if not 
considering rhetorical effect). Context can also con- 
tribute to the inferrability of a relation. A relation 
not easily recognisable from world knowledge may 
be identified by a reader with ease as the discourse 
proceeds. In this case, we say that the inferrabil- 
ity of the relation is locally strong, where the two 
constructions can express the relation equally well 
only in a certain context. In this paper, we mainly 
consider the global aspect of a relation and we will 
describe how we decided the value of inferrability in 
the next section. 
In Table 1, we summarise the factors (indepen- 
dent variables) that might play a role in the close- 
ness judgement between the semantics of a hypotac- 
tie construction and an NR construction. The levels 
are possible values of these factors. Besides Rela- 
tion and In\]errability. Position gives the location of 
the NP that contains the NR modifier. It can be the 
first (initial) or the last (final) phrase in a sentenceS; 
Order gives the order of presentation; a hypotactic 
sentence to be compared with an NR sentence or vice 
versa, which is used to balance the influence of cue 
phrases on human judgement; Subordination speci- 
fies whether the nucleus or the satellite is realised 
as an NR clause6; and Cued/NoCue means using a 
cue phrase in the NR clause or not, which is only 
applicable to the temporal relation, for example, 
(7) The health-care services announced the spinoff 
plan last January, which was then revised 
in May. 
Based on our observation of human written sen- 
tences, we have the following hypotheses: 
Hypothes is  ! For both causal and temporal rela- 
tions, the inferrability of the relation between the se- 
mantics of two \]acts contributes ignificantly to the 
semantic similarities between a hypotactic onstruc- 
tion and an NR construction. 
In other words, if the in\]errability of the relation 
between the two facts is strong, the semantic rela- 
tion can be expressed similarly through an NR con- 
struction, otherwise, the similarity is significantly re- 
duced. 
Hypothes is  2 For the causal relation, the satellite 
subordination bears significantly higher similarity m 
meaning to the hypotactic onstruction than the nu- 
cleus subordination does. 
For example, (4b) would be preferred to "Private 
Eye, which had been threatened with closure, couldn't 
afford the libel payment." 
Hypothes is  3 For the temporal relation, both the 
position of subordination and the use of an appro- 
priate cue phrase in the NR clause make a signifi- 
cant difference to the semantic similarities between 
? a hypotactic and an NR construction. - 
This hypothesis prefers Example (7) to the reali- 
sation that does not have 'then'. 
5|n our implementation, we restrict ourselves to sentences 
with two NPs. 
aWe assume that in the causal relation, the clause bearing 
'because'is always the satellite. Since the temporal relation 
is a multinuclear relation, this factor does not apply. 
111 
Dependent  Variables 
Naturalness Similarity. 
exactly the same _~ 
very similar 
more similar than di~erent 
N/A 
natural 
fairly natural 
more different han similar so-so 
very different fairly unnatural 
totally different unnatural 
Table 2: Dependent variables and their values 
3.2 The  des ign of  the  exper iment  
To assess a semantic similarity, which is thought to 
be influenced by the independent variables, we use 
human subjects to judge the following two depen- 
dent variables: 
Naturalness : how fluent a sentence is on its own. 
Similarity : how similar the meanings of two sen- 
tences are without considering their natural- 
ness. 
The scales of the variables are selected such that 
all values on the scale have natural verbal descrip- 
tions that could be grasped easily by our subjects 
(see Table 2). Similar rating methods have been 
described in (Jordan et al, 1993) to compare the 
output of a machine translation system with that of 
expert humans. 
Since we want to measure different groups of 
similarity judgement based on different in\]errabil- 
ity, order or position levels, a between-groups de- 
sign (Hatch and Lazaraton, 1991) seems to be most 
appropriate. The design we used is illustrated in 
Table 3, where all possible combinations of the in- 
dependent variables are listed. In the table, para- 
phrases gives the types of alternative sentences each 
original sentence has. They should be scored by hu- 
man subjects for their similarities to the original sen- 
tences and their naturalness. 
We used a method similar to random selection 
to create a stratified random sample. The sample 
should contain 12 hypotactic sentences and 12 NR 
sentences: two for each combination of the causal re- 
lation and one for each combination of the temporal 
relation. These numbers were used to obtain as big 
a sample as possible which could still be judged by 
human subjects in a relatively short period of time 
(say less than 30 minutes). 
Using cue phrases as- the indicators o f ' the  se ..... 
mantic relations between clauses, we collected all 
the sentences containing 'because' or 'then' from the 
Wall Street Journal source data. and went through 
each of them to pick out those that actually signal 
the desired relations and can potentially have NR- 
realisations, i.e. where there is a coreference r lation 
between the two NPs in the two clauses. Sentences 
containing NR clauses signalled by ', which' or ', 
who ':~were~=coUected similarly<,<From: these~:seritcnces, 
we randomly selected one by category. If it realised 
an unused factor combination, it was kept in the 
sample. This process was repeated until we collected 
the right number of test items which instantiated all 
combinations of properties in Table 3. 
We asked two subjects to mark the 24 selected 
items with regard to their inferrability on a five- 
point scale: 5 for very likely, 4 for quite likely, 3 
for possibly, 2 for .even less possibly and 1 for un- 
known.-We~took values of 4 and 5 as Strong ahd"the  
others as weak. The subjects and an author agreed 
on 19 items, and the author's version was used for 
the experiment. 
For the test items, we manually produced the cor- 
responding paraphrases, which were then put into a 
questionnaire for human assessment of the two de- 
pendent variables for each paraphrase. 
3.3 Resu l ts  
We had ten native English speakers evaluating tile 
similarity and naturalness on the sample. 
3.3.1 Simi lar i ty  
Since the similarity data is ordinal data and departs 
significantly from a theoretical normal distribution 
according to One-Sample Komogorov-Smirnov Test, 
we chose Mann Whitney U, which is a test for com- 
paring two groups on the basis of their ranks above 
and below the median. The result is summarised in
Table 4, with statistically significant items in bold- 
face (taking the conventional .05 p level). The Z 
scores tell how many standard deviations above or 
below the mean an observation might be. Means 
gives the means of the similarity scores with respect 
to the values of the independent variables in Table 1. 
For the causal relation, there is a significant dif- 
ference between the means of similarities of the two 
groups of different inferrabilities (P<.0005). So we 
have high confidence to accept part of Hypothesis 1. 
i.e. the strong inferrability of the causal relation be- 
tween the semantics of two facts makes the semantic 
similarities between a hypotactic onstruction and 
an NR construction significantly higher than the 
weak case does. In the strong case, tile mean of 
similarity is 4.59, wilich is ,close to very similar. 
We treated order as a factor to be balanced and 
did not expect it to have a significant effect, but 
it does (P=.008). An NR paraphrase shows much 
higher similarity to its corresponding hypotactic sen-  .... 
tence (with a mean of 4.46) than the other way 
round (with a mean of 3.83), but the difference be- 
comes smaller for the strong inferrability case. This 
could be because the causal relations expressed in 
NR sentences generally sound weaker than those in 
hypotactic sentences and the cue phrase has a big 
influence on the perceptibility of a relation. 
112 
Independent  Variables I 
Relation \[ Order  I inferrabflity I.Position 
causal 
temporal 
strong initial 
hypotactic vs. final 
NR sentence weak initial 
final 
strong initial 
NR sentence final 
vs. hypotactic weak initial 
final 
strong initial 
~ypot, actic vs . . . . . . .  5finAl 
NR sentence weak initial 
final 
strong initial 
NR sentence final 
vs. hypotactic weak initial 
final 
Paraphrases 
nuc & sat subordination 
NR sentence 
nuc & sat subordination 
NR sentence 
causal & 
elaboration hypotactic 
? cued & not 
cued NR sentence 
temporal & 
elaboration hypotactic 
Table 3: A between-groups 
Relat ion DependVar  \[ Factors 
causal 
(160 cases) 
temporal 
(80 cases) 
Similarity 
Similarity 
(cued) 
design 
Means  Z 2-tai led P 
Inferrability 4.59/3.70 -4.1015 <.0005 
Order 4.46/3.83 -2.6400 .0083 
Position 4.11/4.18 -.2136 .8308 
Inferrability 4.88/5.00 -.1022 .9086 
Order 5.08/4.80 -1.1756 .2398 
Position 4.80/5.08 -2.0649 I .0389 
Table 4: The output of Mann 
For the temporal relation, position is the only sig- 
nificant factor (P=.0389). So part of Hypothesis 3 is 
confirmed, that is, the final position subordination 
makes an NR paraphrase significantly more similar 
to the corresponding hypotactic onstruction than 
the initial position does. 
We do not have enough evidence to accept the 
claim that the inferrability of the temporal relation 
contributes ignificantly to the similarity judgement 
(as in Hypothesis 1). However, when we calculated 
the similarity mean for the alternative sentences us- 
ing cue phrases, strong or weak in inferrability, we 
got 4.94 (very similar). Comparing this with that of 
the strong causal case using the Mann Whitney U 
test, we get a significance l vel of 0.0294. This means 
that we have strong confidence to believe that the 
similarity mean for the temporal relation if using a 
cue phrase is significantly . higher. -than, that for the 
strong causal relation. Therefore, the temporal re- 
lation can always be realised by an NR construction 
as long as an appropriate cue phrase is used in the 
NR clause. 
The assumption of normality is also not met by 
the subset of the data related to Hypothesis 2 and 3 
(i.e. the similarity scores for nucleus/satellite subor- 
Whitney U on the similarity data 
dination paraphrases and cued/nocue paraphrases). 
We used the Wilcoxon Matched-Pairs Signed-Ranks 
Test because we were comparing pairs of para- 
phrases. The result is given in Table 5. We accept 
the hypothesis that the similarity means of nucleus 
and satellite subordination are significantly different 
in the initial position (Hypothesis 2). This confirms 
the linguistic observation that information of greater 
importance should be presented in a main position 
rather than a subordinate position. We can also ac- 
cept the hypothesis that for the temporal relation, 
using cue phrases in NR clauses can significantly im- 
prove the similarity score of the NR construction 
(Hypothesis 3). 
3.3.2 Natura lness  
~,?e -used the Mann Whitney U test on naturalness 
with regards to order, inferrability and position, and 
found no significant connection. Figure 3 shows the 
distribution of naturalness assessment of the para- 
phrases for the causal and temporal relation respec- 
tively. The majority of the NR constructions are 
natural or fairly natural, which suggests that they 
could be good alternative realisations. 
113 
causal 
temporal 
D.. 
Paired Var iab les~ Means \] Z value \] 2-tail Sig \] 
 ua7evaa -3.o2 .oo3 
Relation \[ 
Table 5: The output of the Wilcoxon Matched-Pairs Signed-Ranks Test 
 because  to NR clause 
~NR clause to because 
60 
50- 
?) 
O 
0.. 
60- 
50- 
 thee  to NR clause 
~NR clause to then 
Figure 3: The naturalness of the causal paraphrases (left) and the temporal paraphrases (right) 
3.3.3 Summary  
We briefly summarise the heuristics drawn from the 
experiment for expressing the causal and temporal 
relations with an NR construction. This is an ac- 
ceptable realisation in the following circumstances: 
e the causal relation holds between two facts and 
the inferrability of the relation is strong, in 
which case satellite subordination should be 
used; or 
? the temporal relation holds between two facts, 
in which case a final position subordination and 
an appropriate cue phrase, like 'then', should be 
used in the NR clause. 
We also found that an NR construction can ex- 
press the causal/temporal relation and the object- 
attribute elaboration relation at the same time, ir- 
respective of the inferrability of the relation. Gen- 
erally speaking, a semantic relation expressed by an 
NR construction sounds weaker than a hypotactic 
realisation with a cue phrase. Therefore, if a rela- 
tion is to be emphasised, NR constructions should 
not be used. 
4 Imp lement ing  the  resu l t s  in  a 
OA-based  text  p lanner  
int-modifiers have a mixed character, i.e. like attr- 
modifiers they are not essential for identifying the 
referents, but like unique-modifiers they are not op- 
tional. Because of their role in supporting the se- 
mantics of the main propositions, the selection of 
int-modifiers hould be a part of the text planning 
process, where a text structure is constructed to ful- 
fill the overall goals for producing the text. How- 
ever, compared with unique-modifiers, int-modifiers 
are less essential for an NP and they can only be 
added if there are available syntactic slots. 
Since embedding deals with attr-modifiers at both 
a content selection and an abstract realisation level, 
it could coordinate the addition of int-modifiers. 
Therefore, the text planner could consult the embed- 
ding module as to whether a property can be realised 
as an NP modifier, under the constraints from the 
NP type and the unique-modifiers that are already 
there. In other words, the text planner chooses facts 
to satisfy certain goals and the embedding process 
decides if the facts can be realised as NP modifiers 
in an abstract sense. 
We need a generation architecture that allows a 
certain degree of interaction between text planning, 
referring expression generation and embedding. So 
we chose the Genetic Algorithm based text planner 
described in (Mellish et el., 1998). Their task is, 
given a set of "facts and-relations between facts, 'to 
produce a legal RST tree using all the facts and some 
relations. Tile text planning is basically a two step 
process. Firstly sequences of facts are generated by 
applying GA operators, and secondly the rhetorical 
structure trees built from these sequences are evalu- 
ated and the good sequences are kept for producing 
better offspring. 
114 
We extended the text planner by adding a GA op- 
erator called embedding mutation, .which ~andomly 
selects two items mentioning a common entity from 
a sequence and assumes an embedding on them. Em- 
beddings are evaluated together with the other prop- 
erties an RST tree has. In this way, embedding is 
performed uring text planning. The ultimate score 
of a tree is the sum of positive and negative scores 
for all the good and bad properties it bears. Since 
good embeddings are scored higher, they are kept in 
the sequences for producing,better offspring and. are 
very likely to be included in the final output. 
We incorporated the results from the experiment 
into the GA planner by using them as preferences 
for evaluating RST trees. We treated inferrability 
as an input to the system. If a good embedding can 
be formed from two facts connected by an RST re- 
lation (i.e. either of the two cases in Section 3.3.3 
is satisfied and the required syntactic slot is free), 
the embedding is scored higher than the hypotactic 
realisation. However, this emphasis on embedding 
might not be appropriate. In a real application en- 
vironment, other communicative intentions hould 
be incorporated to balance the scoring for differ- 
ent realisations. And generally, inferrability has to 
be implemented based on limited domain-dependent 
knowledge and user configuration. 
5 Conc lus ion  and  fu ture  work  
This paper investigates the use of NR modifiers in 
referring expressions to express certain semantic re- 
lations. This is a commonly used strategy by human 
authors, which has not been explored by an NLG 
system before. Our experiment shows that when the 
conditions for inferrability etc. are satisfied, certain 
relations can be expressed through an NR construc- 
tion as well as a normally used hypotactic onstruc- 
tion with little difference in semantics. This facili- 
tates for an NLG system a way of expressing these 
semantic relations more concisely and subtly which 
could not be achieved by other means. 
Our experiment is restricted in many ways. One 
possible xtension is to use more cue phrases to cover 
a wider range of cases for each semantic relation. In 
reality, the application domain should decide which 
relations need to be tested. 
Re ferences  
Jean Carletta. 1996. Assessing agreement on classi- 
fication tasks: the kappa statistic,. Computational 
Linguistics, 22(2):249-254. 
Hua Cheng. 1998. Embedding new information into 
referring expressions. In Proceedings of COLING- 
A CL '98, pages 1478-1480, Montreal, Canada. 
Robert Dale. 1992. Generating Referring Expres- 
sions: Constructing Descriptions in a Domain of 
Objects and Processes. The MIT Press. 
M.A.K. Halliday. 1985. An Introduction to Func- 
tianal- Grammar. Edward..,&rnold (.PUblishers) 
Ltd., London, UK. 
Evelyn Hatch and Anne Lazaraton. 1991. The Re- 
search Manual: Design and Statistics for Applied 
Linguistics. Newbury House Publishers. 
Eduard Hovy. 1993. Automated iscourse genera- 
tion using discourse structure relations. Artificial 
Intelligence 63, Special Issue on Natural Language 
Processing, 1. 
: ~Pamela: Jordan,:~:~Bonnie: Dorr, _and John Benoit. 
..... 1993: A first-pass approach for evaluating ma- 
chine translation systems. Machine Translation, 
8(1-2):49-58. 
Alistair Knott. 1996. A Data-Driven Methodol- 
ogy for Motivating a Set of..Coherence Relations. 
Ph.D. thesis, Department ofArtificial Intelligence, 
University of Edinburgh, Edinburgh. 
Amichai Kronfeld. 1990. Reference and Compu- 
tation. Studies in Natural Language Processing. 
Cambridge University Press. 
Sebastian Loebner. 1987. Definites. Journal of Se- 
mantics, 4:279-306. 
William Mann and Sandra Thompson. 1987. 
Rhetorical structure theory: A theory of text or- 
ganization. Technical Report ISI/RR-87-190, In- 
formation Sciences Institute, University of South- 
ern California. 
Chris Mellish, Alistair Knott, Jon Oberlander, 
and Mick O'Donnell. 1998. Experiments using 
stochastic search for text planning. In Proceed- 
ings of the 9th International Workshop on Natural 
Language Generation, Ontario, Canada. 
Massimo Poesio. 2000. Annotating a corpus to de- 
velop and evaluate discourse ntity realization al- 
gorithms: Issues and preliminary results. In Pro- 
ceedings of LREC, Athens, May. 
Randolph Quirk, Sidney Greenbaum, Geoffrey 
Leech, and Jan Svartvik. 1985. A Grammar of 
Contemporary English. Longman Group Ltd. 
Donia Scott and Clarisse Sieckenius de Souza. 1990. 
Getting the message across in rst-based text gen- 
eration. In R. Dale, C. Mellish, and M. Zock, edi- 
tors, Current Research in Natural Language Gen- 
eration, pages 47-73. Academic Press. 
James Shaw and Kathleen McKeown. 1997. An ar- 
chitecture for aggregation i  text generation. In 
Proceedings of the Fifteenth International Joint 
Conference on Artificial Intelligence, Poster Ses- 
sion, Japan. 
115 
Capturing the Interaction between Aggregation and Text 
Planning in Two Generation Systems 
Hua Cheng and Chris Mel l i sh  
Division of Informatics, University of Edinburgh 
80 South Bridge, Ed inburgh EH1 1HN, UK 
huac, chrism@dai, ed. ac. uk 
Abst ract  
In natural language generation, different gener- 
ation tasks often interact with each other in a 
complex way. We think that how to resolve the 
complex interactions inside and between tasks 
is more important to the generation of a co- 
herent text than how to model each individual 
factor. This paper focuses on the interaction be- 
tween aggregation and text planning, and tries 
to explore what preferences exist among the fea- 
tures considered by the two tasks. The prefer- 
ences are implemented in two generation sys- 
tems, namely ILEX-TS and a text planner us- 
ing a Genetic Algorithm. The evaluation em- 
phasises the second implementation and shows 
that capturing these preferences properly can 
lead to coherent ext. 
1 D iscourse  coherence  and  
aggregat ion  
hi NLG, theories based on domain-independent 
rhetorical relations, in particular, Rhetorical 
Structure Theory (Mann and Thompson, 1987), 
are often used in text planning, whose task 
is to select the relevant information to be ex- 
pressed and organise it into a hierarchical struc- 
ture which captures certain discourse prefer- 
ences such as preferences for the use of rhetori- 
cal relations. 
In the theory of discourse structure developed 
by Grosz and Sidner (1986), each discourse seg- 
ment exhibits two types of coherence: local co- 
herence among utterances inside the segment, 
and global coherence between this segment and 
other discourse segments. Discourse segments 
are connected by either a dominaTzce relation or 
a satisfaction-precedence relation. 
There has been an effort to synthesise tile 
two accounts of discourse structure. X loser and 
Moore (1996) argue that the two theories have 
considerable common ground, which lies in the 
correspondence between the notion of domi- 
nance and nuclearity. It is possible to map 
between Grosz and Sidner's linguistic structure 
and RST text structure, and relation-based co- 
herence and global coherence capture similar 
discourse properties. 
Oberlander et al (1999) propose a dis- 
tinction between two types of discourse coher- 
ence: proposition-based coherence, which ex- 
ists between text spans connected by RST re- 
lations except for object-attribute laboration, 
and entity-based coherence, which exists be- 
tween spans of text in virtue of shared entities. 
entity-based coherence captures the coherence 
among adjacent propositions, which resembles 
local coherence in Grosz and Sidner's theory. 
To generate a coherent ext, the text planning 
process must try to achieve both local (entity- 
based) and global (relation-based) coherence. 
Since the task of aggregation is to combine sinl- 
ple representations together to form a complex 
one, which in the mean time leads to a shorter 
text as a whole, aggregation could affect the or- 
dering of text plans and the length of the whole 
text.. Therefore, it is closely related to tile task 
of maintaining both types of coherence. Here 
we treat embedding as a type of aggregation. 
There is no consensus as to where aggregation 
should happen or how it is related to other gen- 
eration processes (Wilkinson, 1995; Reape and 
Mellish, 1999). In many NLG systems, aggre- 
gation is a post planning process whose prefer- 
ences are only partially taken into account by 
the text planner. 
1.1 Aggregat ion  and local coherence  
In a structured text plan produced by the text 
planner, local coherence is normally maintained 
through the ordering of the selected facts, where 
186 
certain types of center transition (e.g. cen- 
ter continuation) :are preferred :over:others (eig,. -; 
center shifting) (Centering Theory (Grosz et al, 
1995)). Aggregation may affect text planning 
by taking away facts from a sequence featuring 
preferred center movements for embedding or 
subordination. As a result, the preferred cen- 
ter transitions in the original sequences could 
be cut off. For example, comparing the first 
two descriptions of.a necklace in Figure 1, 2 is 
less coherent than 1 because of the shifting from 
the description of the necklace to that of the de- 
signer, which is a side effect of embedding. 
Since the centers of sentences are normally 
NPs and embedding adds non-restrictive com- 
ponents into an NP, it could affect the way a Cb 
is realised (e.g. preventing it from being a pro- 
noun). As pointed out in (Grosz et al, 1995), 
different realisations (e.g. pronoun vs. definite 
description) are not equivalent with respect o 
their effect on coherence. Therefore, embedding 
could influence local coherence by forcing a dif- 
ferent realisation from that preferred by Center- 
ing Theory. There is an obvious need to balance 
the consideration for local coherence and stylis- 
tic preferences. 
1.2 Aggregat ion  and  global coherence  
Different types of aggregation eed to be com- 
patible among themselves, in particular, embed- 
ding and semantic parataxis and hypotaxis. Us- 
ing the abstraction of RST, semantic parataxis 
concerns facts related by explicit multi-nuclear 
semantic relations (e.g. sequence and contrast) 
or by implicit connections like parallel common 
parts. If two facts have at least two identi- 
cal parallel components, we say that a conjunct 
or disjunct relation exists between them, and 
these relations are multi-nuclear relations. Se- 
mantic hypotaxis concerns facts connected by 
nucleus-satellite r lations (e.g. cause). Seman- 
tic parataxis and hypotaxis feature in relation- 
based coherence and they depend on the text 
planner to put the related facts next to each 
other in order to perform a combination. 
(Cheng, 1998) describes interactions that 
need to be taken into account in aggrega- 
tion. Firstly, complex embedded components 
like non-restrictive clauses may interrupt tile 
semantic onnection or syntactic similarity be- 
tween a set of clauses. Secondly, the possibilities 
of other types of aggregation should be consid- 
ered for both the main fact and the fact to be 
-embedded . uring .:embedding .decision. maki ng... 
And thirdly, performing parataxis inside a hy- 
potaxis could convey wrong information. 
We argue that the effect of aggregation is not 
limited to the particular NP or sentence where 
aggregation happens, but to the coherence of 
the text as a whole. The complex interactions 
demand the features of aggregation to be eval- 
uated .together with other coherence~ features 
and aggregation to be planned as a part of text 
structuring. This requires better coordination 
between aggregation and other generation tasks 
as well as among different ypes of aggregation 
than is present in current NLG systems. 
In this paper, we describe how to capture the 
above interactions as preferences among related 
features, and the implementation f the prefer- 
ences in two very different generation architec- 
tures to produce descriptions of museum objects 
on display. 
2 P re ferences  among coherence  
features  
We claim that it is the relative preferences 
among features rather than the absolute magni- 
tude of each individual one that play the crucial 
role in the production of a coherent text. In this 
section we discuss the preferences among fea- 
tures related to text planning, based on which 
those for embedding can be introduced. 
2.1 P re ferences  for global coherence  
A semantic relation other than conjunct or dis- 
junet is preferred to be used whenever possible 
because it usually conveys interesting informa- 
tion about domain objects and leads to a coher- 
ent text span. If a conjunct relation shares a fact 
with a semantic relation, the conjunct should 
be suppressed. For example, in 3 of Figure 1. 
apart from other relations, there is an amplifica- 
tion relation signalled by indeed and a conjunct 
between the last two propositions. Compared 
with 3, 4 is less preferred because it misses tile 
amplification and the center transition from the 
necklace to an Arts and Crafts style jewel is not 
so smooth, whereas 3 expresses the amplifica- 
tion explicitly and the conjunct implicitly. 
However, a semantic relation can only be used 
if the knowledge assumed to be shared by the 
hearer is introduced in the previous discourse 
(Mellish et al. 1998a). \Ve assume the strategy 
187 
1. This necklace is in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate 
design. They tend to have floral motifs. For instance, this necklace has floral motifs. It was designed 
by Jessie King. King was Scottish. She once lived in London. 
2. This necklace, which was designed by Jessie King, is in the Arts and Crafts style. Arts and 
Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance, 
this necklace has floral motifs. King was Scottish. She once lived in London. 
3. The necklace is in the Arts and Crafts style. It is set with jewels in that it features cabuchon 
stones. Indeed, an Arts and Crafts style jewel usually uses cabuchon stones. It usually uses oval 
stones. 
4. The necklace is in the Arts and Crafts style. It is set. with jewels in that it features cabuchon 
stones. An Arts and Crafts style jewel usually uses cabuchon stones and oval stones. 
Figure 1: Aggregation examples 
of (Mellish et al, 1998a) which uses a joint re- 
lation to connect every two text spans that do 
not have a semantic relation other than object- 
attribute elaboration and conjunct/disjunct in 
between. Although joint is not preferred when 
other relations are present, it is better than 
missing presuppositions or embedding a con- 
junct relation inside a semantic relation. There- 
fore, we have the following heuristics, where 
"A>B" means that A is preferred over B. 
Heur i s t i c  1 Preferences among features for 
global coherence: 
a semantic relation > Conjunct/Disjunct > 
Joint > presuppositions not met 
Joint > Conjunct inside a semantic relation 
2.2 Pre ferences  for local coherence  
One way to achieve local coherence is to con- 
trol center transitions among utterances. In 
Centering Theory, Rule 2 specifies preferences 
among center movement in a locally coherent 
discourse segment: sequences of continuation 
are preferred over sequences of retaining; which 
are then preferred over sequences of shifting. 
Brennan et el. (1987) also describe typical 
discourse topic movements in terms of center 
transitions between pairs of utterances. They 
argue that the order of coherence among the 
transitions is continuing > retaining > smooth 
shifting > abrupt shifting. Instead of claiming 
that these are the best models, we use them 
simply as an example of linguistic models being 
used for evaluating features of text planning. 
A type of center transition that appears fre- 
quently in descriptive text is that the descrit)- 
tion starts with an object, but shifts to associ- 
ated objects or perspectives of that object. This 
is a type of abrupt shifting, but it is appropriate 
as long as the objects are highly associated to 
the original object (Schank, 1977). This phe- 
nomenon is handled in the system of (Grosz, 
1977), where subparts of an object are included 
into a focus space as the implicit foci when the 
object itself is to be included. 
We call this center movement an associate 
shifting, where the center moves from a trig- 
ger entity to a closely associated entity. Our 
informal observation from museum descriptions 
shows that associate shifting is preferred by hu- 
man writers to all other types of center move- 
ments except for continuation. There are two 
types of associate shifting: where the trigger 
is in the previous utterance or two entities in 
two adjacent utterances have the same trigger. 
There is no preference between them. 
Heuristic 2 summarises the above preferences. 
We admit that these are strict heuristics and 
that human texts are sometimes more flexible. 
Heur i s t i c  2 Preferences among center transi- 
tions: 
Continuation > Associate shifting > RetaiTI- 
ing > Smooth shifting > Abrupt shifting 
2.3 Pre ferences  for both  types  of 
coherence 
Two propositions can be connected in differ- 
ent ways, e.g. through a semmxtic relation or a 
smooth center transition only. Since a semantic 
relation is always preferred, we have the follow- 
ing heuristic: 
Heur i s t i c  3 Preferences among semantic rela- 
tions and center transitions: 
a semantic relation > Joint ? Continuation 
188 
2.4 P re ferences  for embedd ing  Good embedding > Normal embedding > 
We distinguish between.a.-good,.rwrmal,and-bad Jo int  > Bad embedding . . . . .  =:--..~ .:-- ~ .--:.: ........ 
embedding based on the features it bears. We do Continuation + Smooth shifting + Joint > 
not claim that the set of features is complete. 
In a different context, more criteria might have 
to be considered. 
A good embedding is one satisfying all the fol- 
lowing conditions: 
1. The referring part is an indefinite, a demon- 
strative or a bridging description (as de- 
fined in (Poesio et al, 1997)). 
2. The embedded part can be realised as an 
adjective or a prepositional phrase (Scott 
and de Souza, 1990). 
3. In the resulting text, the embedded part 
does not lie between text spans connected 
by semantic parataxis and hypotaxis. 
4. There is an available syntactic slot to hold 
the embedded part. 
A good embedding is highly preferred and 
should be performed whenever possible. A nor- 
mal embedding is one satisfying condition 1, 3 
and 4 and the embedded part is a relative clause 
which provides additional information about 
the referent. Bad embeddings are all those left, 
for example, if there is no available syntactic 
slot for the embedded part. 
Since semantic parataxis has a higher priority 
than embedding (Cheng, 1998), a good embed- 
ding should be less preferred than using a con- 
junct relation, but it should be preferred over a 
center continuation for it to happen. 
To decide the interaction between an embed- 
ding and a center transition, we use the first two 
examples in Figure 1 again. The only difference 
between I and 2 is the position of the sentence 
"This necklace was de.signed by Jessie King", 
which can be represented in terms of features of 
local coherence and embedding as follows: 
the last three sentences in 1: Joint + Contin- 
uation + Joint + Smooth shifting 
the last two sentences plus embedding in 2: 
Joint + Abrupt shifting + Normal embedding 
1 is preferred over 2 because the center inoves 
more smoothly in 1. The heuristics derived from 
the above discussions are summarised below: 
Heur i s t i c  4 Preferences among features for 
embedding and center transition: 
Abrupt shifting + Normal embedding 
Good embedding > Continuation + Joint 
Conjunct > Good embedding 
The '+' symbol can be interpreted in different 
ways, depending on how the features are used 
in an NLG system. In a traditional system, it 
means the coexistence of two features. In a sys- 
tem using numbers for planning, it can have the 
same meaning as the arithmetic symbol. 
3 Captur ing  the  pre ferences  in I LEX  
The architecture of text planning has a great 
effect on aggregation possibilities. In object de- 
scriptive text generation, there lacks a central 
overriding communicative goal which could be 
decomposed in a structured way into subgoals. 
The main goal is to provide interesting infor- 
mation about the target object. There are gen- 
erally only a small number of relations, mainly 
object-attribute elaboration and joint. For such a 
genre, a domain-dependent bottom-up lanner 
(Marcu, 1997) or opportunistic planner (Mel- 
lish et al, 1998b) suits better than a domain- 
independent top-down planner. In these archi- 
tectures, aggregation is important o text plan- 
ning because it changes the order in which infor- 
mation is expressed. The first implementation 
we will describe is based on ILEX (Oberlander 
et al, 1998). 
ILEX is an adaptive hypertext generation 
system, providing natural anguage descriptions 
for museum objects. The bottom-up text plan- 
ning is fulfilled in two steps: a content selection 
procedure, where a set of fact nodes with high 
relevance is selected from the Content Potential 
(following a search algorithm), and a content 
structuring procedure, where selected facts are 
reorganised to form entity-chains (based on the 
theory of entity-based coherence), which repre- 
sent a coherent ext arrangement. 
To make it possible for the ILEX planner to 
take into account aggregation, we use a revised 
version of Meteer's Text Structure (Meteer, 
1992; Panaget, 1997) as the intermediate l vel of 
representation between text planning and sen- 
tence rcalisation to provkte abstract syntactic 
constraints to the planning. We call this sys- 
tem ILEX-TS (ILEX based on Text Structure). 
189 
In ILEX-TS, abstract referring expression de- 
termination and.aggxegation are performed ur - . . :  
ing text structuring. For each fact whose Text 
Structure is being built, if an NP in the fact can 
take modifiers, the embedding process will find 
a list of elaboration facts to the referent and 
make embedding decisions based on the con- 
straints imposed by the NP form. The decisions 
include what to embed and what syntactic form 
the embedded part should use. 
Heuristic 1, 2 and 3 are followed naturally ~ 
by the ILEX text planner, which calculates the 
best RS tree and puts facts connected by the 
imaginary conjunct relation next to each other. 
It tries to feature center continuations as often 
as possible. When it needs to shift topic, it uses 
a smooth shifting. 
ILEX-TS has a set of embedding rules, where 
those rules featuring good embedding are al- 
ways used first, then a rule featuring a normal 
embedding. Bad embedding is not allowed at 
all. To coordinate different types of aggrega- 
tion, the algorithm checks parataxis and hy- 
potaxis possibilities for each nucleus fact and 
the fact to be embedded before it applies an 
embedding rule. These realise most of Heuris- 
tic 4 (except for the second set). However, be- 
cause the various factors are optimised in order 
(with no backtracking), there is no guarantee 
that the best overall text will be found. In addi- 
tion, complex interactions between aggregation 
and center transition cannot be easily captured. 
4 Text  p lann ing  us ing  a GA 
Although most heuristics can be followed in 
ILEX-TS, some interactions are missing, for ex- 
ample, 9 of Figure 1 will probably be generated. 
For better coordination, we adopt the text plan- 
ner based on a Genetic Algorithm (GA) as de- 
scribed in (Mellish et al, 1998a). The task is. 
given a set of facts and a set of relations between 
facts, to produce a legal rhetoricalstrncture tree 
using all the facts and some relations. 
A fact is represented in terms of a subject, 
a verb and a complement (as well as a unique 
identifier). A relation is represented in terms of 
the relation name, the two facts that are con- 
nected t) 3" the relation and a list of precondition 
facts which need to have been mentioned before 
the relation can be used i. 
1As this is an experimental system, the ability of the 
A genetic algorithm is suitable for such a 
problem.because,:the..numher-.of.-possihle-com- 
binations is huge and the search space is not 
perfectly smooth and unimodal (there can be 
many good combinations). Also the generation 
task does not require a global optimum to be 
found. What we need is a combination that is 
coherent enough for people to understand. 
(Mellish et al, 1998a) summarises the genetic 
algorithm roughly as follows: 
1. Enumerate a set of random initial se- 
quences by loosely following sequences of 
facts where consecutive facts mention the 
same entity. 
2. Evaluate sequences by evaluating the 
rhetorical structure trees they give rise to. 
3. Perform mutation and crossover on the se- 
quences. 
4. Stop after a given number of iterations, and 
return the tree for the "best" sequence. 
The advantage of this approach is that it pro- 
vides a mechanism to integrate planning factors 
in the evaluation function and search for the 
best combinations of them. So it is an excellent 
framework for experimenting with the interac- 
tion between aggregation and text planning. 
In the algorithm, the RS trees are right- 
branching and are almost deterministically built 
from sequences of facts. Given two sequences, 
crossover inserts a random segment from one 
sequence in a random position in the other to 
produce two new sequences. Mutation selects 
a random segment of a sequence and moves it 
into a random position in the same sequence. 
To explore the whole space of aggregation. 
we decide not to perform aggregation on struc- 
tured facts or on adjacent facts in a linear se- 
quence because they might restrict the possibil- 
ities and even miss out good candidates. In- 
stead, we define a third operator called embed- 
ding mutation. Suppose we have a sequence 
\[U1,U2,...,Ui,...,U.\], where we call each element 
of the sequence a unit, which can be either a fact 
or a list of facts or units with no depth limit. 
For a list, we call its very first fact the main fact, 
system is limited in all aspects. It does not have a real 
realisation component,  so the parts we are less interested 
in are realised by canned phrases for readability. 
190 
Features /Factors  
Semant ic  re la t ions  
a jo int  
a conjunct or dis junct  
a relat ion other  than  jo int ,  con junct  or  d is junct  
a con junct  ,inside o ther  semant ic  re la t ions  
a precondi t ion  not  sat isf ied 
Focus  moves  
a cont inu ing  
an assoc iate  sh i f t ing 
a smooth shifting 
resuming  a previous focus 
Embedd ing  
a good embedding 
a normal embedd ing  
a bad embedding 
Others  
topic not mentioned in the first sentence 
Va lues  ( ra ters )  
1 \] 2 .. 
-20 -46 
10 11 
21 69 
-50 -63 
-30 -61 
20 7 
16 1 
14 -3 
6 -43 
6 3 
3 0 
-30 -64 
-10 -12 
Table 1: Two different raters satisfying the same constraints 
/ x I % / \ 
i i  ~ - - ~ t  ! x x i I x 
I x 
I I ~ x i I 
/ I  x l 
_ m  
, -h-- . . . . .  X . . . . . .  g . . . .  -i~ . . . . . . .  T . . . . . . .  i; . . . . . . .  ~ .......... ,o 
Figure 2: Scores for four museum descriptive texts 
into which the remaining facts in the list are to 
be embedded. The embedding mutat ion ran- 
domly selects a unit Ui from the sequence and 
an entity in its main fact. It then collects all 
the units mentioning this entity and randomly 
chooses one Uk. The list containing these two 
units \[Ui,Uk\] represents a random embedding 
and will be treated as a single unit in later op- 
erations. It takes the. position of Ui to produce 
a new sequence \[U~,U2,...,\[Ui,Uk\],...,U,\] and all 
repetit ions outside \[Ui,U~:\] are removed. This 
sequence is then evaluated and ordered in the 
populat ion. 
The probabil it ies of apl)lying the three opera- 
tots are: 65% for crossow'r. 30% for embedding 
mutat ion and 5% for normal umtation. This  is 
because the first two are more likely to produce 
sequences bearing desired properties by e i ther  
combining the good bits of two sequences or 
performing a reasonable amount of embedding,  
whereas normal mutat ion is entirely random 2. 
5 Jus t i fy ing  the  GA eva luat ion  
funct ion  
The  linguistic theories discussed in Section 2 
only give evidence in qualitative terms. For a 
GA-based planner to work, we have to come up 
with actual numbers that can be used to evalu- 
2The values for crossover and mutation rate used m 
our algorithm are fairly standard. 
191 
The smal l  por tab le  throne f rom the t ime o f  the  Qianlong Emperor  1736-95 is mad e . . . .  
of ~acquer~d`wo~d~.wit~T.de~rati~n-in~-g~d`~and.red~It-was-use~-in-the-private.apartrn~r~`~ ..... "......... 
of the Imperial Palaces. The cover f rom the  re ign o f  J iaquing,  1796-1820 is woven in yellow 
silk, wh ich  is the  imper ia l  colour o f  the Q ing  Dynasty ,1644-1911.  It would have covered 
the throne when not in use. 
The design on the seat is a imper ia l  f ive c lawed dragon in a circular medal l ion.  On the 
inside of the arm pieces are small shelves. Precious possessions can be placed in small shelves 
and can be studied as an aid to contemplation. 
Figure 3: A generated text scored the h ighest ,  w i th  the  embedded parts  highlighted 
Score2 Score3 Score4 Score5 Score6 
Score1 .9567 .9337 .9631 .9419 .9515 
Score2 .9435 .8819 .9280 .9185 
Score3 .8650 .8462 .9574 
Score4 .9503 .8940 
Score5 .8486 
Table 2: Correlations between six raters 
ate an RS tree. Mellish et al (1998a) present 
some scores for evaluating the basic features of a 
tree, but they make it clear that the scores are 
there for descriptive purpose, not for making 
any serious claim about the best way of evalu- 
ating RS trees. 
The methodology we adopted was that we 
took the existing evaluation function and ex- 
tended it to take into account features for local 
coherence, embedding and semantic paratmxis. 
This resulted in rater 1 in Table 1, which sat- 
isfied all the heuristics mentioned in Section 2. 
We manually broke down four human written 
museum descriptions into individual facts and 
relations and reconstructed sequences of facts 
with the same orderings and aggregations a  in 
the original texts. We then used our evaluation 
flmction to score the RS trees built from these 
sequences. In the mean time. we ran the GA 
algorithm for 5000 iterations on the facts and 
relations for 10 times. The results are shown in 
Figure 2, where the four line styles correspond 
to the four texts. The jagged lines represent-the 
scores of the machine generated texts and the 
straight lines represent the scores for the corre- 
sponding human texts. 
All human texts were scored among the high- 
est and machine generated texts can get scores 
very close to human ones sometimes. Since the 
human texts were written and revised bv mu- 
seum experts, they can be treated as "'nearly 
best  texts". The figure shows that the evalu- 
ation function based on our heuristics can find 
good and correct combinations. The reason for 
a relatively bad text being generated sometimes 
might be that really bad sequences were pro- 
duced at the beginning. This could be improved 
by using certain heuristics to get better initial 
sequences. Also when the number of facts be- 
comes larger, more iterations are needed to get 
readable texts. Figure 3 gives a text generated 
using rater 1. 
To justify our claim that it is the preferences 
among generation factors that decide the coher- 
ence of a text, we fed the preferences into a con- 
straint based program. If a feature can take a 
range of values, the program randomly selects 
a number in that range. A number of raters 
compatible with the constraints were generated 
and one of them is given in Table 1 as rater 2. 
We then generated all possible combinations, in- 
cluding embedding, of seven facts from a human 
text and used six randomly produced raters to 
score each of them. 
The .qualities .of the generated texts are nor- 
real distributed according to all raters. The 
raters distinguish between good and bad texts 
and they classify the majority of texts as of 
moderate quality and only very small percent- 
ages as very good or very bad texts. The be- 
haviours of the raters are very similar as the 
histograms are of roughly the same shape. 
192 
To see to what extent he six raters agree with 
each other, we calculated the  Pearson correla- 
tion coefficient between them, which is shown 
in Table 2. We can claim from the table that 
for this data, the scores from the six raters cor- 
relate, and we have a fairly good chance to be- 
lieve that the six raters, randomly produced in 
a sense, agree with each other on evaluating the 
text and they measure basically the same thing. 
Daniel Marcu. 1997. From local to global coherence: 
. A_ bottom=up.approach' to. text:planning.. 'In Pro- 
ceedings of the Fourteenth National Conference on 
Artificial Intelligence, pages 629-635, Providence, 
Rhode Island. 
Chris Mellish, Alistair Knott, Jon Oberlander, 
and Mick O'Donnell. 1998a. Experiments using 
stochastic search for text planning. In Proceed- 
ings of the 9th International Workshop on Natural 
Language Generation, Ontario, Canada. 
6 Conc lus ions  and  fu ture  work  
. . . . .  Chris:MeUish,: Mick O'_Donnell,:.Jon Oberlander, and 
Alistair Knott. 1998b. An architecture for op- 
This paper describes an experiment with the 
preferences among features concerning aggrega- 
tion and text planning, in particular, we present 
an mechanism for how relevant features can be 
scored to contribute together to the planning of 
a coherent text. The statistical results partially 
justify our claim that it is the preferences among 
generation features that decide the coherence of 
a text. 
Our experiment could be extended in many 
ways, for example, validating the evaluation 
function through empirical analysis of human 
assessments of the generated texts, and us- 
ing more texts to test the correlation between 
raters. The architecture based on the Genetic 
Algorithm can also be used for testing interac- 
tions between or within other text generation 
modules. 
Re ferences  
Susan Brennan, Marilyn Walker Friedman, and Carl 
Pollard. 1987. A centering apporach to pronouns. 
In Proceedings of the 25th Annual Meeting of the 
Association for Computational Linguistics, pages 
155-162, Stanford, CA. 
Hua Cheng. 1998. Embedding new information i to 
referring expressions. In Proceedings ofCOLING- 
A CL '98, pages 1478-1480, Montreal, Canada. 
Barbara Grosz and Candace Sidner. 1986. Atten- 
tions, intentions and the structure of discourse. 
Computational Linguistics, 12:175-204. 
Barbara Grosz, Aravind Joshi, and Scott Weinstein. 
1995. Centering: A framework for modelling the 
local coherence of discourse. Computational Lin- 
guistics, 21(2):203-226. 
Barbara Grosz. 1977. T-he representation a d use of 
focus in dialogue understanding. Technical report 
151, SRI International. 
William Mann and Sandra Thompson. 198.7. 
Rhetorical structure theory: A theory of text or- 
ganization. Technical Report ISI/RR-87-190, In- 
formation Sciences Institute, University of South- 
ern California. 
portunistic text generation. In Proceedings of the 
9th International Workshop on Natural Language 
Generation, Ontario, Canada. 
Marie Meteer. 1992. Expressibility and The Prob- 
lem of Efficient Text Planning. Communication 
in Artificial Intelligence. Pinter Publishers Lim- 
ited, London. 
Megan Moser and Johanna Moore. 1996. Toward a 
synthesis of two accounts of discourse structure. 
Computational Linguistics, 22(3):409-419. 
Jon Oberlander, Mick O'Donnell, Ali Knott, and 
Chris Mellish. 1998. Conversation i  the mu- 
seum: Experiments in dynamic hypermedia with 
the intelligent labelling explorer. New Review of 
Hypermedia nd Multimedia, 4:11-32. 
Jon Oberlander, Alistair Knott, Mick O'Donnell, 
and Chris Mellish. 1999. Beyond elaboration: 
Generating descriptive texts containing it-clefts. 
In T Sanders, J Schilperoord, and W Spooren, 
editors, Text Representation: Linguistic and Psy- 
cholinguistic Aspects. Benjamins, Amsterdam. 
Franck Panaget. 1997. Micro-planning: A uni- 
fied representation f lexical and grammatical re- 
sources. In Proceeding of the 6th European Work- 
shop on Natural Language Generation, pages 97- 
106. 
Massimo Poesio, Renata Vieira, and Simone Teufel. 
1997. Resolving bridging references in unre- 
stricted text. Research paper hcrc-rp87, Centre 
for Cognitive Science, University of Edinburgh. 
Michael Reape and Chris Mellish. 1999. Just what 
is aggregation anyway? In Proceedings of the 7th 
European Workshop on Natural Language Gener- 
ation, pages 20-29, Toulouse, France. 
Roger Schank. 1977. Rules and topics in conversa- 
tion. Cognitive Science, 1(1):421-441. 
Donia Scott and Clarisse Sieckenius de Souza. 1990. 
Getting the-message across in rst-based text gen- 
eration. In R. Dale, C. Mellish, and M. Zock, edi- 
tors, Current Research in Natural Language Gen- 
eration, pages 47-73. Academic Press. 
John Wilkinson. 1995. Aggregation i Natural Lan- 
guage Generation: Another Look. Technical re- 
port, Computer Science Department, University 
of \Vnterloo. 
193 
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
Automatic Semantic Grouping in a Spoken Language User Interface 
Toolkit 
 
Hassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, Huy 
Nguyen, Fuad Rahman, Yuliya Tarnikova, Timotius Tjahjadi and Che Wilcox 
 
BCL Technologies Inc. 
Santa Clara, CA 95050 U.S.A. 
fuad@bcltechnologies.com 
 
 
Abstract 
With the rapid growth of real 
application domains for NLP systems, 
there is a genuine demand for a general 
toolkit from which programmers with no 
linguistic knowledge can build specific 
NLP systems. Such a toolkit should 
provide an interface to accept sample 
sentences and convert them into 
semantic representations so as to allow 
programmers to map them to domain 
actions. In order to reduce the workload 
of managing a large number of semantic 
forms individually, the toolkit will 
perform what we call semantic grouping 
to organize the forms into meaningful 
groups. In this paper, we present three 
semantic grouping methods: similarity-
based, verb-based and category-based 
grouping, and their implementation in 
the SLUI toolkit. We also discuss the 
pros and cons of each method and how 
they can be utilized according to the 
different domain needs. 
1 Introduction and Motivation 
With the improvement of natural language 
processing (NLP) and speech recognition 
techniques, spoken language will become the 
input of choice for software user interfaces, as 
it is the most natural way of communication. In 
the mean time, the domains for NLP systems, 
especially those handling speech input, have 
grown rapidly in recent years. However, most 
computer programmers do not have enough 
linguistic knowledge to develop an NLP 
system to handle speech input. There is a 
genuine demand for a general toolkit from 
which programmers with no linguistic 
knowledge can rapidly build speech based 
NLP systems to handle their domain specific 
problems more accurately (Alam, 2000). The 
toolkit will allow programmers to generate 
Spoken Language User Interface (SLUI) front 
ends for new and existing applications using, 
for example, a program-through-example 
method. In this methodology, the programmer 
will specify a set of sample input sentences or 
a domain corpus for each task. The toolkit will 
then organize the sentences by meaning and 
even generate a large set of syntactic variations 
for a given sentence. It will also generate the 
code that takes a user?s spoken request and 
executes a command on an application. This 
methodology is similar to using a GUI toolkit 
to develop a graphical user interface so that 
programmers can develop GUI without 
learning graphics programming. Currently this 
is an active research area, and the present work 
is funded by the Advanced Technology 
Program (ATP) of the National Institute of 
Standards and Technology (NIST). 
In the program-through-example approach, 
the toolkit should provide an interface for the 
programmers to input domain specific corpora 
and then process the sentences into semantic 
representations so as to capture the semantic 
meanings of the sentences. In a real world 
application, this process results in a large 
number of semantic forms. Since the 
programmers have to manually build the links 
between these forms and their specific domain 
actions, they are likely to be overwhelmed by 
the workload imposed by the large number of 
individual semantic forms. In order to 
significantly reduce this workload, we can 
organize these forms in such a way so that the 
programmers can manipulate them as groups 
rather than as individual items. This will speed 
up the generation process of the domain 
specific SLUI system. We call this process the 
semantic grouping process. 
One straightforward way to group is to 
organize different syntactic forms expressing 
the same meaning together. For example,  
 
(1.1) I want to buy this book online. 
(1.2) Can I order this book online? 
(1.3) How can I purchase this book online? 
(1.4) What do I need to do to buy this book 
online?  
 
The semantic forms of the above sentences 
may not be the same, but the action the 
programmer has in mind in an e-business 
domain is more or less the same: to actually 
buy the book online. In addition to the above 
sentences, there are many variations that an 
end-user might use. The embedded NLP 
system should be able to recognize the 
similarity among the variations so that the 
SLUI system can execute the same command 
upon receiving the different queries. This 
requires a group to contain only sentences with 
the same meaning. However in real 
applications, this might be difficult to achieve 
because user requests often have slight 
differences in meaning. 
This difficulty motivates a different style 
for semantic grouping: organizing the semantic 
forms into groups so that those in the same 
group can be mapped roughly to the same 
action. The action can be either a command, 
e.g., buy something, or concerning an object, 
e.g., different ways of gathering information 
about an object. For example, sentence (1.5) 
would be grouped together with the above 
example sentences because it poses the same 
request: buy books; and sentences (1.6) to (1.8) 
would be in one group because they are all 
about price information. 
 
(1.5) I want to buy the latest book about e-
business. 
 
(1.6) Please send me a price quote. 
(1.7) What is the reseller price? 
(1.8) Do you have any package pricing for 
purchasing multiple products at once? 
This type of grouping is the focus of this 
paper. We propose three grouping methods: 
similarity-based grouping, verb-based 
grouping and category-based grouping. The 
process of grouping semantic forms is domain 
dependent and it is difficult to come up with a 
generally applicable standard to judge whether 
a grouping is appropriate or not. Different 
grouping techniques can give programmers 
different views of their data in order to satisfy 
different goals.  
This paper is organized into 6 sections. In 
Section 2, we briefly describe the system for 
which the grouping algorithms are proposed 
and implemented. Section 3 presents the three 
grouping methods in detail. In Section 4, we 
describe how the algorithms are implemented 
in our system. We test the methods using a set 
a sentences from our corpus and discuss the 
pros and cons of each method in Section 5. 
Finally, in Section 6, we draw conclusions and 
propose some future work. 
2 SLUITK 
As mentioned in the previous section, the 
Spoken Language User Interface Toolkit 
(SLUITK) allows programmers with no 
linguistic knowledge to rapidly develop a 
spoken language user interface for their 
applications. The toolkit should incorporate 
the major components of an NLP front 
end, such as a spell checker, a parser and a 
semantic representation generator. Using 
the toolkit, a programmer will be able to create 
a system that incorporates complex NLP 
techniques such as syntactic parsing and 
semantic understanding.  
2.1 The Work Flow 
Using an Automatic Speech Recognition 
(ASR) system, the SLUITK connects user 
input to the application, allowing spoken 
language control of the application. The 
SLUITK generates semantic representations of 
each input sentence. We refer to each of these 
semantic representations as a frame, which is 
basically a predicate-argument representation 
of a sentence.  
The SLUITK is implemented using the 
following steps: 
1. SLUITK begins to create a SLUI by 
generating semantic representations of 
sample input sentences provided by the 
programmer. 
2. These representations are expanded using 
synonym sets and other linguistic devices, 
and stored in a Semantic Frame Table 
(SFT). The SFT becomes a 
comprehensive database of all the 
possible commands a user could request a 
system to do. It has the same function as 
the database of parallel translations in an 
Example-based machine translation 
system (Sumita and Iida, 1991). 
3. The toolkit then creates methods for 
attaching the SLUI to the back end 
applications. 
4. When the SLUI enabled system is 
released, a user may enter an NL 
sentence, which is translated into a 
semantic frame by the system. The SFT is 
then searched for an equivalent frame. If a 
match is found, the action or command 
linked to this frame is executed. 
 
In a real application, a large number of 
frames might be generated from a domain 
corpus. The semantic grouper takes the set of 
frames as the input and outputs the same 
frames organized in a logical manner.  
2.2 The Corpus 
We use a corpus of email messages from our 
customers for developing and testing the 
system. These email messages contain 
questions, comments and general inquiries 
regarding our document-conversion products.  
We modified the raw email programmatically 
to delete the attachments, HTML and other 
tags, headers and sender information. In 
addition, we manually deleted salutations, 
greetings and any information that was not 
directly related to customer support. The 
corpus contains around 34,640 lines and 
170,000 words. We constantly update it with 
new email from our customers.  
We randomly selected 150 sentential 
inquiries to motivate and test the semantic 
grouping methods discussed in this paper. 
3 Semantic Grouping 
We have mentioned in Section 1 that grouping 
semantic frames is domain dependent.  
Grouping depends on the nature of the 
application and also the needs of the domain 
programmer. Since this is a real world 
problem, we have to consider the efficiency of 
grouping. It is not acceptable to let the 
programmer wait for hours to group one set of 
semantic forms. The grouping should be fairly 
fast, even on thousands of frames. 
These different considerations motivate 
several grouping methods: similarity-based 
grouping, verb-based grouping and category-
based grouping. In this section, we describe 
each of these methods in detail. 
3.1 Similarity-based Grouping  
Similarity-based grouping gathers sentences 
with similar meanings together, e.g., sentences 
(1.1) to (1.4). There is a wide application for 
this method. For example, in open domain 
question-answering systems, questions need to 
be reformulated so that they will match 
previously posted questions and therefore use 
the cached answers to speed up the process 
(Harabagiu et al, 2000).   
The question reformulation algorithm of 
Harabagiu et al tries to capture the similarity 
of the meanings expressed by two sentences. 
For a given set of questions, the algorithm 
formulates a similarity matrix from which 
reformulation classes can be built. Each class 
represents a class of equivalent questions. 
The algorithm for measuring the similarity 
between two questions tries to find lexical 
relationships between every two questions that 
do not contain stop words. The algorithm 
makes use of the WordNet concept hierarchy 
(Fellbaum, 1998) to find synonym and 
hypernym relations between words. 
This algorithm does not infer information 
about the meanings of the questions, but rather 
uses some kind of similarity measurement in 
order to simulate the commonality in meaning. 
This is a simplified approach. Using different 
threshold, they can achieve different degrees of 
similarity, from almost identical to very 
different. 
This method can be used for similarity-
based grouping to capture the similarity in 
meanings expressed by different sentences. 
3.2 Verb-based Grouping 
Among the sentences normally used in the e-
business domain, imperative sentences often 
appear in sub-domains dominated by 
command-and-control requests. In such an 
application, the verb expresses the command 
that the user wants to execute and therefore 
plays the most important role in the sentence. 
Based on this observation, a grouping can be 
based on the verb or verb class only. For 
example, sentences with buy or purchase etc. 
as the main verbs are classified into one group 
whereas those with download as the main verb 
are classified into a different group, even when 
the arguments of the verbs are the same. 
This is similar to sorting frames by the 
verb, taking into account simple verb synonym 
information.  
3.3 Category-based Grouping 
Since SLUITK is a generic toolkit whereas the 
motivation for grouping is application 
dependent, we need to know how the 
programmer wants the groups to be organized. 
We randomly selected 100 sentences from our 
corpus and asked two software engineers to 
group them in a logical order. They came up 
with very different groups, but their thoughts 
behind the groups are more or less the same. 
This motivates the category-based grouping. 
This grouping method puts less emphasis 
on each individual sentence, but tries to 
capture the general characteristics of a given 
corpus.  For example, we want to group by the 
commands (e.g., buy) or objects (e.g., a 
software) the corpus is concerned with. If a 
keyword of a category appears in a given 
sentence, we infer that sentence belongs to the 
category. For example, sentences (1.6) to (1.8) 
will be grouped together because they all 
contain the keyword price. 
These sentences will not be grouped 
together by the similarity-based method 
because their similarity is not high enough, nor 
by the verb-based method because the verbs 
are all different. 
4 Grouping in SLUITK 
Because we cannot foresee the domain needs 
of the programmer, we implemented all three 
methods in SLUITK so that the programmer 
can view their data in several different ways.  
The programmer is able to choose which type 
of grouping scheme to implement. 
In the question reformulation algorithm of 
(Harabagiu, et al 2000), all words are treated 
identically in the question similarity 
measurement. However, our intuition from 
observing the corpus is that the verb and the 
object are more important than other 
components of the sentence and therefore 
should be given more weight when measuring 
similarity. In Section 4.1, we describe our 
experiment with the grouping parameters to 
test our intuition. 
4.1 Experimenting with Parameters 
We think that there are two main 
parameters affecting the grouping result: the 
weight of the syntactic components and the 
threshold for the similarity measurement in the 
similarity-based method. Using 100 sentences 
from our corpus, we tried four different types 
of weighting scheme and three thresholds with 
the category-based methods. Human judgment 
on the generated groups confirmed our 
intuition that the object plays the most 
important role in grouping and the verb is the 
second most important. The differences in 
threshold did not seem to have a significant 
effect on the similarity-based grouping.  This 
is probably due to the strict similarity 
measurement. 
This experiment gives us a relatively 
optimal weighting scheme and threshold for 
the similarity-based grouping. 
One relevant issue concerns the 
simplification of the semantic frames. For a 
sentence with multiple verbs, we can simplify 
the frame based on the verbs used in the 
sentence. The idea is that some verbs such as 
action verbs are more interesting in the e-
business domain than others, e.g., be and have. 
If we can identify such differences in the verb 
usage, we can simplify the semantic frames by 
only keeping the interesting verb frames. For 
example, in the following sentences, the verb 
buy is more interesting than be and want, and 
the generated semantic frames should contain 
only the frame for buy. 
 
(4.1) Is it possible to buy this software online? 
(4.2) I want to buy this software online.  
 
Figure 1: A screen shot of SLUITK 
We make use of a list of stop-words from 
(Frakes, 1992) in order to distinguish between 
interesting and uninteresting verbs. We look 
for frames headed by stop-words and follow 
some heuristics to remove the sub-frame of the 
stop-word. For example, if there is at least one 
verb that is not a stop-word, we remove all 
other stop-words from the frame. In the 
sentence [Is it possible to] buy the software in 
Germany?, be is a stop-word, so only the 
frame for buy is kept. This process removes the 
redundant part of a frame so that the grouping 
algorithm only considers the most important 
part of a frame. 
4.2 Implementation in SLUITK 
Figure 1 shows a screen shot of the interface of 
the SLUITK, which shows several grouped 
semantic frames. In this section, we give more 
detail about the implementation of the three 
grouping methods used in SLUITK.  
 
Similarity-based grouping 
 
Similar to (Harabagiu, et al 2001), our 
similarity-based grouping algorithm calculates 
the similarity between every two frames in the 
input collection. If the similarity is above a 
certain threshold, the two frames are 
considered similar and therefore should be 
grouped together. If two frames in two 
different groups are similar, then the two 
groups should be combined to a single group. 
The central issue here is how to measure the 
similarity between two frames. 
Since we have found that some syntactic 
components are more important to grouping 
than others, we use a weighted scheme to 
measure similarity. For each frame, all words 
(except for stop-words) are extracted and used 
for similarity calculation. We give different 
weights to different sentence components. 
Since in an e-business domain, the verb and 
the object of a sentence are usually more 
important than other components because they 
express the actions that the programmers want 
to execute, or the objects for which they want 
to get more information, the similarity of these 
components are emphasized through the 
weighting scheme. The similarity score of two 
frames is the summation of the weights of the 
matched words.  
There is a match between two words when 
we find a lexical relationship between them. 
We extend the method of (Harabagiu, et al 
2000) and define a lexical relationship between 
two words W1 and W2 as in the following: 
Table 1 : Comparison of grouping methods 
 
 
1. If W1 and W2 have a common 
morphological root. Various stemming 
packages can be used for this purpose, for 
example, Porter Stemmer (Porter, 1997). 
2. If W1 and W2 are synonyms, i.e., W2 is 
in the WordNet synset of W1. 
3. If the more abstract word is a WordNet 
hypernym of the other. 
4. If one word is the WordNet holonym of 
the other (signaling part of, member of 
and substance of relations); 
5. If W1 is the WordNet antonym of W2.  
 
Domain specific heuristics can also be used 
to connect words. For example, in the e-
business domain, you and I can be treated as 
antonyms in the following sentences: 
 
(4.3) Can I buy this software? 
(4.4) Do you sell this software? 
 
When none of the above is true, there is no 
lexical relation between two given words. 
Because the similarity-based grouping 
needs to consult WordNet frequently for 
lexical relations, it becomes very slow for even 
a few hundred frames. We have to change the 
algorithm to speed up the process, as it is too 
slow for real world applications.  
Instead of comparing every two frames, we 
put all the words from an existing group 
together. When a new frame is introduced, we 
compare the words in this new frame with the 
word collection of each group. The similarity 
scores are added up as before, but it needs to 
be normalized over the number of words in the 
collection. When the similarity is above a 
certain threshold, the new frame is classified as 
a member of the group. This significantly 
reduces the comparison needed for classifying 
a frame, and therefore reduces the number of 
times WordNet needs to be consulted. 
We compared this improved algorithm with 
the original one on 30 handcrafted examples; 
the generated groups are very similar. 
 
Verb-based grouping 
 
The verb-based grouping implementation is 
fairly straightforward and has been described 
in Section 3.2. 
 
Category-base grouping 
 
For the category-based method, we first count 
all the non stop-words in a given corpus and 
retrieve a set of most frequent words and their 
corresponding word classes from the corpus. 
This process also makes use of the WordNet 
synonym, hypernym, holonym and antonym 
information. These word classes form the 
categories of each group. We then check the 
verbs and objects of each sentence to see if 
they match these words. That is, if a category 
word or a lexically related word appears as the 
verb or the object of a sentence, the sentence is 
classified as a member of that group. For 
example, we can pick the most frequent 20 
words and divide the corpus into 21 groups, 
where the extra group contains all sentences 
that cannot be classified. The programmer can 
decide the number of groups they want. This 
gives the programmer more control over the 
grouping result. 
5 Discussion 
We tested the three methods on 100 sentences 
from our corpus. We had 5 people evaluate the 
generated groups. They all thought that 
grouping was a very useful feature of the 
toolkit. Based on their comments, we 
summarize the pros and cons of each method 
in Table 1. 
The similarity-based grouping produces a 
large number of groups, most of which contain 
only one sentence. This is because there are 
usually several unrelated words in each 
sentence, which decreases the similarity 
scores. In addition, using WordNet we 
sometimes miss the connections between 
lexical items. The verb-based grouping 
 Similarity-based Verb-based Category-based 
Group Size small small large 
Number of Groups large large variable 
Speed slow on large corpus fast slow on large corpus 
Application general command-and-control only general 
produces slightly larger groups, but also 
produces many single sentence groups. 
Another problem is that when sentences 
contain only stop-word verbs, e.g., be, the 
group will look rather arbitrary. For example, a 
group of sentences with be as the main verb 
can express completely different semantic 
meanings. The small group size is a 
disadvantage of both methods. The number of 
groups of the category-based grouping can 
change according to the user specification. In 
general it produces less groups than the other 
methods and the group size is much larger, but 
the size becomes smaller for less frequent 
category words. 
Both the similarity-based and category-
based grouping methods are slow because they 
frequently need to use WordNet to identify 
lexical relationships. The verb-based method is 
much faster, which is the primary advantage of 
this method. 
The verb-based method should be used in a 
command-and-control domain because it 
requires at least one non stop-word verb in the 
sentence. However, it will have a hard time in 
a domain that needs to handle questions. From 
the point of view of assigning a domain 
specific action to a group, this grouping is the 
best because each verb can be mapped to an 
action.  Therefore, the programmer can link an 
action to each group rather than to each 
individual frame. When the group size is 
relatively large, this can greatly reduce the 
workload of the programmer.  
The category-based method produces a 
better view of the data because the sentences in 
each group seem to be consistent with the 
keywords of the category. The disadvantage is 
that it is difficult to link a group to a single 
action, and the programmer might have to re-
organize the groups during action assignment.  
The similarity-based method did not 
perform well on the testing corpus, but it might 
work better on a corpus containing several 
different expressions of the same semantic 
information. 
In summary, each method has its 
advantages and disadvantages. The decision of 
which one to choose depends mainly on the 
needs of the domain programmer and the 
composition of the input corpus. 
 
6 Conclusions and Future Work 
In this paper we propose semantic grouping as 
a way to solve the problem of manipulating 
semantic frames in developing a general 
Spoken Language User Interface Toolkit 
(SLUITK). We introduced three methods for 
grouping semantic frames generated by the 
NLP components of the toolkit. We tested the 
methods and discussed the advantages and 
disadvantages of each method. Since the 
judgment of the grouping result is application 
dependent, the methods co-exist in our 
SLUITK to suit the requirement of different 
applications. 
Future work includes improving the 
efficiency and accuracy of the methods and 
testing them on a larger corpus. 
 
References 
Alam H. (2000) Spoken Language Generic 
User Interface (SLGUI). Technical Report 
AFRL-IF-RS-TR-2000-58, Air Force Research 
Laboratory, Rome.  
Fellbaum C. (1998) WordNet, An 
Electronic Lexical Database, The MIT Press, 
Cambridge, Massachusetts. 
Frakes W. and Baeza-Yates R. (1992) 
Information Retrieval, Data Structures and 
Algorithms, Prentice-Hall. 
HaraBagiu S. and Moldovan D. and Pasca 
M. and Mihalcea R. and Surdeanu M. and 
Bunescu R. and Girju R. and Rus V. and 
Morarescu P. (2000) FALCON: Boosting 
Knowledge for Answer Engines, TREC 9. 
Porter M. (1997) An algorithm for suffix 
stripping, in Readings in Information 
Retrieval, Karen Sparck Jones and Peter Willet 
(ed), San Francisco: Morgan Kaufmann. 
Sumita E. and Iida H. (1991) Experiments 
and Prospects of Example-Based Machine 
Translation. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics, pp. 185-192. 
 
Extending A Broad-Coverage Parser for a General NLP Toolkit 
 
Hassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, Fuad 
Rahman, Yuliya Tarnikova, Timotius Tjahjadi and Che Wilcox  
 
BCL Technologies Inc. 
Santa Clara, CA 95050 U.S.A. 
fuad@bcltechnologies.com 
 
Abstract 
With the rapid growth of real world 
applications for NLP systems, there 
is a genuine demand for a general 
toolkit from which programmers 
with no linguistic knowledge can 
build specific NLP systems. Such a 
toolkit should have a parser that is 
general enough to be used across 
domains, and yet accurate enough for 
each specific application. In this 
paper, we describe a parser that 
extends a broad-coverage parser, 
Minipar (Lin, 2001), with an 
adaptable shallow parser so as to 
achieve both generality and accuracy 
in handling domain specific NL 
problems. We test this parser on our 
corpus and the results show that the 
accuracy is significantly higher than 
a system that uses Minipar alone. 
1 Introduction 
With the improvement of natural language 
processing (NLP) techniques, domains for 
NLP systems, especially those handling speech 
input, are rapidly growing. However, most 
computer programmers do not have enough 
linguistic knowledge to develop NLP systems. 
There is a genuine demand for a general toolkit 
from which programmers with no linguistic 
knowledge can rapidly build NLP systems that 
handle domain specific problems more 
accurately (Alam, 2000). The toolkit will allow 
programmers to generate natural language 
front ends for new and existing applications 
using, for example, a program-through-
example method. In this methodology, the 
programmer will specify a set of sample input 
sentences or a domain corpus for each task. 
The toolkit will then organize the sentences by 
similarity and generate a large set of syntactic 
variations of a given sentence. It will also 
generate the code that takes a user?s natural 
language request and executes a command on 
an application. Currently this is an active 
research area, and the Advanced Technology 
Program (ATP) of the National Institute of 
Standards and Technology (NIST) is funding 
part of the work.  
In order to handle natural language input, 
an NLP toolkit must have a parser that maps a 
sentence string to a syntactic structure. The 
parser must be both general and accurate. It 
has to be general because programmers from 
different domains will use the toolkit to 
generate their specific parsers. It has to be 
accurate because the toolkit targets commercial 
domains, which usually require high accuracy.  
The accuracy of the parser directly affects the 
accuracy of the generated NL interface. In the 
program-through-example approach, the 
toolkit should convert the example sentences 
into semantic representations so as to capture 
their meanings. In a real world application, this 
process will involve a large quantity of data. If 
the programmers have to check each syntactic 
or semantic form by hand in order to decide if 
the corresponding sentence is parsed correctly, 
they are likely to be overwhelmed by the 
workload imposed by the large number of 
sentences, not to mention that they do not have 
the necessary linguistic knowledge to do this. 
Therefore the toolkit should have a broad-
coverage parser that has the accuracy of a 
parser designed specifically for a domain. 
One solution is to use an existing parser 
with relatively high accuracy.  Using existing 
parsers such as (Charniak, 2000; Collins, 
1999) would eliminate the need to build a 
parser from scratch. However, there are two 
problems with such an approach. First, many 
parsers claim high precision in terms of the 
number of correctly parsed syntactic relations 
rather than sentences, whereas in commercial 
applications, the users are often concerned 
with the number of complete sentences that are 
parsed correctly. The precision might drop 
considerably using this standard. In addition, 
although many parsers are domain 
independent, they actually perform much 
better in the domains they are trained on or 
implemented in. Therefore, relying solely on a 
general parser would not satisfy the accuracy 
needs for a particular domain.  
Second, since each domain has its own 
problems, which cannot be foreseen in the 
design of the toolkit, customization of the 
parser might be needed. Unfortunately, using 
an existing parser does not normally allow this 
option. One solution is to build another parser 
on top of the general parser that can be 
customized to address domain specific parsing 
problems such as ungrammatical sentences. 
This domain specific parser can be built 
relatively fast because it only needs to handle a 
small set of natural language phenomena. In 
this way, the toolkit will have a parser that 
covers wider applications and in the mean time 
can be customized to handle domain specific 
phenomena with high accuracy. In this paper 
we adopt this methodology. 
The paper is organized into 6 sections. In 
Section 2, we briefly describe the NLP toolkit 
for which the parser is proposed and 
implemented. Section 3 introduces Minipar, 
the broad-coverage parser we choose for our 
toolkit, and the problems this parser has when 
parsing a corpus we collected in an IT domain. 
In Section 4, we present the design of the 
shallow parser and its disadvantages. We 
describe how we combine the strength of the 
two parsers and the testing result in Section 5. 
Finally, in Section 6, we draw conclusions and 
propose some future work. 
2 NLP Toolkit 
In the previous section, we mentioned a 
Natural Language Processing Toolkit 
(NLPTK) that allows programmers with no 
linguistic knowledge to rapidly develop natural 
language user interfaces for their applications. 
The toolkit should incorporate the major 
components of an NLP system, such as a spell 
checker, a parser and a semantic representation 
generator. Using the toolkit, a software 
engineer will be able to create a system that 
incorporates complex NLP techniques such as 
syntactic parsing and semantic understanding.  
In order to provide NL control to an 
application, the NLPTK needs to generate 
semantic representations for input sentences. 
We refer to each of these semantic forms as a 
frame, which is basically a predicate-argument 
representation of a sentence.  
The NLPTK is implemented using the 
following steps: 
 
1. NLPTK begins to create an NLP front end 
by generating semantic representations of 
sample input sentences provided by the 
programmer. 
2. These representations are expanded using 
synonym sets and stored in a Semantic 
Frame Table (SFT), which becomes a 
comprehensive database of all the 
possible commands a user could request 
the system to do.  
3. The toolkit then creates methods for 
attaching the NLP front end to the back 
end applications. 
4. When the NLP front end is released, a user 
may enter an NL sentence, which is 
translated into a semantic frame by the 
system. The SFT is then searched for an 
equivalent frame. If a match is found, the 
action or command linked to this frame is 
executed. 
In order to generate semantic 
representations in Step 1, the parser has to 
parse the input sentences into syntactic trees. 
During the process of building an NLP system, 
the programmer needs to customize the parser 
of the toolkit for their specific domain. For 
example, the toolkit provides an interface to 
highlight the domain specific words that are 
not in the lexicon.  The toolkit then asks the 
programmer for information that helps the 
system insert the correct lexical item into the 
lexicon. The NLPTK development team must 
handle complicated customizations for the 
programmer. For example, we might need to 
change the rules behind the domain specific 
parser to handle certain natural language input. 
In Step 4, when the programmer finishes 
building an NLP application, the system will 
implement a domain specific parser. The 
toolkit has been completely implemented and 
tested.  
We use a corpus of email messages from 
our customers for developing the system. 
These emails contain questions, comments and 
general inquiries regarding our document-
conversion products. We modified the raw 
email programmatically to delete the 
attachments, HTML tags, headers and sender 
information. In addition, we manually deleted 
salutations, greetings and any information not 
directly related to customer support. The 
corpus contains around 34,640 lines and 
170,000 words. We constantly update it with 
new emails from our customers.  
From this corpus, we created a test corpus 
of 1000 inquiries to test existing broad-
coverage parsers and the parser of the toolkit. 
3 Minipar in NLPTK 
We choose to use Minipar (Lin, 2001), a 
widely known parser in commercial domains, 
as the general parser of NLPTK. It is worth 
pointing out that our methodology does not 
depend on any individual parser, and we can 
use any other available parser.  
3.1 Introduction to Minipar 
Minipar is a principle-based, broad-coverage 
parser for English (Lin, 2001). It represents its 
grammar as a network of nodes and links, 
where the nodes represent grammatical 
categories and the links represent types of 
dependency relationships. The grammar is 
manually constructed, based on the Minimalist 
Program (Chomsky, 1995). 
Minipar constructs all possible parses of an 
input sentence. It makes use of the frequency 
counts of the grammatical dependency 
relationships extracted by a collocation 
extractor (Lin, 1998b) from a 1GB corpus 
parsed with Minipar to resolve syntactic 
ambiguities and rank candidate parse trees. 
The dependency tree with the highest ranking 
is returned as the parse of the sentence.  
The Minipar lexicon contains about 
130,000 entries, derived from WordNet 
(Fellbaum, 1998) with additional proper 
names. The lexicon entry of a word lists all 
possible parts of speech of the word and its 
subcategorization frames (if any).  
Minipar achieves about 88% precision and 
80% recall with respect to dependency 
relationships (Lin, 1998a), evaluated on the 
SUSANNE corpus (Sampson, 1995), a subset 
of the Brown Corpus of American English. 
3.2 Disadvantages of Minipar 
In order to see how well Minipar performs in 
our domain, we tested it on 584 sentences from 
our corpus. Instead of checking the parse trees, 
we checked the frames corresponding to the 
sentences, since the accuracy of the frames is 
what we are most concerned with. If any part 
of a frame was wrong, we treated it as an error 
of the module that contributed to the error. We 
counted all the errors caused by Minipar and 
its accuracy in terms of correctly parsed 
sentences is 77.6%. Note that the accuracy is 
actually lower because later processes fix some 
errors in order to generate correct frames. 
The majority of Minipar errors fall in the 
following categories: 
 
1. Tagging errors: some nouns are mis-
tagged as verbs. For example, in Can I get 
a copy of the batch product guide?, guide 
is tagged as a verb. 
2. Attachment errors: some prepositional 
phrases (PP) that should be attached to 
their immediate preceding nouns are 
attached to the verbs. For example, in Can 
Drake convert the PDF documents in 
Japanese?, in Japanese is attached to 
convert. 
3. Missing lexical entries: some domain 
specific words such as download and their 
usages are not in the Minipar lexicon. 
This introduces parsing errors because 
such words are tagged as nouns by 
default.  
4. Inability to handle ungrammatical 
sentences: in a real world application, it is 
unrealistic to expect the user to enter only 
grammatical sentences. Although Minipar 
still produces a syntactic tree for an 
ungrammatical sentence, the tree is ill 
formed and cannot be used to extract the 
semantic information being expressed. 
 
In addition, Minipar, like other broad-
coverage parsers, cannot be adapted to specific 
applications. Its accuracy does not satisfy the 
needs of our toolkit. We have to build another 
parser on top of Minipar to enable domain 
specific customizations to increase the parsing 
accuracy. 
4 The Shallow Parser 
Our NLPTK maps input sentences to action 
requests. In order to perform an accurate 
mapping the toolkit needs to get information 
such as the sentence type, the main predicate, 
the arguments of the predicate, and the 
modifications of the predicate and arguments 
from a sentence. In other words, it mostly 
needs local dependency relationships. 
Therefore we decided to build a shallow parser 
instead of a full parser. A parser that captures 
the most frequent verb argument structures in a 
domain can be built relatively fast. It takes less 
space, which can be an important issue for 
certain applications. For example, when 
building an NLP system for a handheld 
platform, a light parser is needed because the 
memory cannot accommodate a full parser. 
4.1 Introduction 
We built a KWIC (keyword in context) verb 
shallow parser. It captures only verb predicates 
with their arguments, verb argument modifiers 
and verb adjuncts in a sentence. The resulting 
trees contain local and subjacent dependencies 
between these elements. 
The shallow parser depends on three levels 
of information processing: the verb list, 
subcategorization (in short, subcat) and 
syntactic rules. The verb subcat system is 
derived from Levin?s taxonomy of verbs and 
their classes (Levin, 1993). We have 24 verb 
files containing 3200 verbs, which include all 
the Levin verbs and the most frequent verbs in 
our corpus. A verb is indexed to one or more 
subcat files and each file represents a particular 
alternation semantico-syntactic sense. We have 
272 syntactic subcat files derived from the 
Levin verb semantic classes. The syntactic 
rules are marked for argument types and 
constituency, using the Penn Treebank tagset 
(Marcus, 1993). They contain both generalized 
rules, e.g.,  .../NN, and specified rules, e.g., 
purchase/VBP. An example subcat rule for the 
verb purchase looks like this: .../DT .../JJ 
.../NN, .../DT .../NN from/RP .../NN for/RP 
.../NN. The first element says that purchase 
takes an NP argument, and the second says that 
it takes an NP argument and two PP adjuncts. 
We also encoded specific PP head class 
information based on the WordNet concepts in 
the rules for some attachment disambiguation. 
The shallow parser works like this: it first 
tags an incoming sentence with Brill tagger 
(Brill, 1995) and matches verbs in the tagged 
sentence with the verb list. If a match is found, 
the parser will open the subcat files indexed to 
that verb and gather all the syntactic rules in 
these specific subcat files. It then matches the 
verb arguments with these syntactic rules and 
outputs the results into a tree. The parser can 
control over-generation for any verb because 
the syntactic structures are limited to that 
particular verb's syntactic structure set from 
the Levin classes. 
4.2 Disadvantages of Shallow Parser 
The disadvantages of the shallow parser are 
mainly due to its simplified design, including: 
1. It cannot handle sentences whose main 
verb is be or phrasal sentences without a 
verb because the shallow parser mainly 
targets command-and-control verb 
argument structures.  
2. It cannot handle structures that appear 
before the verb. Subjects will not appear 
in the parse tree even though it might 
contain important information. 
3. It cannot detect sentence type, for 
example, whether a sentence is a question 
or a request. 
4. It cannot handle negative or passive 
sentences. 
We tested the shallow parser on 500 
sentences from our corpus and compared the 
results with the output of Minipar. We 
separated the sentences into five sets of 100 
sentences. After running the parser on each set, 
we fixed the problems that we could identify.  
This was our process of training the parser. 
Table 1 shows the data obtained from one such 
cycle. Since the shallow parser cannot handle 
sentences with the main verb be, these 
sentences are excluded from the statistics. So 
the test set actually contains 85 sentences. 
In Table 1, the first column and the first 
row show the statistics for the shallow parser 
and Minipar respectively. The upper half of the 
table is for the unseen data, where 55.3% of 
the sentences are parsed correctly and 11.8% 
incorrectly (judged by humans) by both 
parsers. 18.9% of the sentences are parsed 
correctly by Minipar, but incorrectly by the 
shallow parser, and 14.1% vise versa. The 
lower half of the table shows the result after 
fixing some shallow parser problems, for 
example, adding a new syntactic rule. The 
accuracy of the parser is significantly 
improved, from 69.4% to 81.2%. This shows 
the importance of adaptation to specific 
domain needs, and that in our domain, the 
shallow parser outperforms Minipar. 
 
SP/MP Correct 
(74.1%) 
Wrong 
(25.9%) 
Correct (69.4%) 47 (55.3%) 12 (14.1%) 
Wrong (30.6%) 16 (18.9%) 10 (11.8%) 
SP/MP Correct 
(74.1%) 
Wrong 
(25.9%) 
Correct (81.2%) 53 (62.4%) 16 (18.8%) 
Wrong (18.8%) 10 (11.8%) 6 (7.1%) 
Table 1: Comparison of the shallow 
parser with Minipar on 85 sentences 
The parsers do not perform equally well on 
all sets of sentences. For some sets, the 
accuracies of Minipar and the shallow parser 
drop to 60.9% and 67.8% respectively. 
5 Extending Minipar with the 
Shallow Parser 
Each parser has pros and cons. The advantage 
of Minipar is that it is a broad-coverage parser 
with relatively high accuracy, and the 
advantage of the shallow parser is that it is 
adaptable. For this reason, we intend to use 
Minipar as our primary parser and the shallow 
parser a backup. Table 1 shows only a small 
percentage of sentences parsed incorrectly by 
both parsers (about 7%). If we always choose 
the correct tree between the two outputs, we 
will have a parser with much higher accuracy. 
Therefore, combining the advantages of the 
two parsers will achieve better performance in 
both coverage and accuracy. Now the question 
is how to decide if a tree is correct or not.  
5.1 Detecting Parsing Errors 
In an ideal situation, each parser should 
provide a confidence level for a tree that is 
comparable to each other. We would choose 
the tree with higher confidence. However, this 
is not possible in our case because weightings 
of the Minipar trees are not publicly available, 
and the shallow parser is a rule-based system 
without confidence information.  
Instead, we use a few simple heuristics to 
decide if a tree is right or wrong, based on an 
analysis of the trees generated for our test 
sentences. For example, given a sentence, the 
Minipar tree is incorrect if it has more than one 
subtree connected by a top-level node whose 
syntactic category is U (unknown).  A shallow 
parser tree is wrong if there are unparsed 
words at the end of the sentence after the main 
verb (except for interjections). We have three 
heuristics identifying a wrong Minipar tree and 
two identifying a wrong shallow parser tree. If 
a tree passes these heuristics, we must label the 
tree as a good parse.  This may not be true, but 
we will compensate for this simplification 
later. The module implementing these 
heuristics is called the error detector. 
We tested the three heuristics for Minipar 
trees on a combination of 84 requestive, 
interrogative and declarative sentences. The 
results are given in the upper part of Table 2. 
The table shows that 45 correct Minipar trees 
(judged by humans) are identified as correct by 
the error detector and 18 wrong trees are 
identified as wrong, so the accuracy is 75%. 
Tagging errors and some attachment errors 
cannot be detected. 
 
MP/ED Correct 
(76.2%) 
Wrong 
(23.8%) 
Correct (56%) 45 (53.6%) 2 (2.4%) 
Wrong (44%) 19 (22.6%) 18 (21.4%) 
SP/ED Correct 
(73%) 
Wrong 
(26%) 
Correct (59%) 58 (58%) 1 (1%) 
Wrong (40%) 15 (15%) 25 (25%) 
Table 2: The performance of the parse 
tree error detector 
We tested the two heuristics for shallow 
parser trees on 100 sentences from our corpus 
and the result is given in the lower part of 
Table 2. The accuracy is about 83%. We did 
not use the same set of sentences to test the 
two sets of heuristics because the coverage of 
the two parsers is different. 
5.2 Choosing the Better Parse Trees 
We run the two parsers in parallel to generate 
two parse trees for an input sentence, but we 
cannot depend only on the error detector to 
decide which tree to choose because it is not 
accurate enough. Table 2 shows that the error 
detector mistakenly judges some wrong trees 
as correct, but not the other way round. In 
other words, when the detector says a tree is 
wrong, we have high confidence that it is 
indeed wrong, but when it says a tree is 
correct, there is some chance that the tree is 
actually wrong. This motivates us to 
distinguish three cases: 
1. When only one of the two parse trees is 
detected as wrong, we choose the correct 
tree, because no matter what the correct 
tree actually is, the other tree is definitely 
wrong so we cannot choose it. 
2. When both trees are detected as wrong, we 
choose the Minipar tree because it handles 
more syntactic structures. 
3. When both trees are detected as correct, 
we need more analysis because either 
might be wrong. 
We have mentioned in the previous sections 
the problems with both parsers. By comparing 
their pros and cons, we come up with 
heuristics for determining which tree is better 
for the third case above.  
The decision flow for selecting the better 
parse is given in Figure 1. Since the shallow 
parser cannot handle negative and passive 
sentences as well as sentences with the main 
verb be, we choose the Minipar trees for such 
sentences. The shallow parser outperforms 
Minipar on tagging and some PP attachment 
because it checks the WordNet concepts. So, 
when we detect differences concerning part-of-
speech tags and PP attachment in the parse 
trees, we choose the shallow parser tree as the 
output. In addition, we prefer the parse with 
bigger NP chunks.  
We tested these heuristics on 200 sentences 
and the result is shown in Table 3. The first 
row specifies whether a Minipar tree or a 
shallow parser tree is chosen as the final 
output. The first column gives whether the 
final tree is correct or incorrect according to 
human judgment. 88% of the time, Minipar 
trees are chosen and they are 82.5% accurate. 
The overall contribution of Minipar to the 
accuracy is 73.5%. The improvement from just 
using Minipar is about 7%, from about 75.5% 
to 82.5%. This is a significant improvement. 
The main computational expense of 
running two parsers in parallel is time. Since 
our shallow parser has not been optimized, the 
extended parser is about 2.5 times slower than 
Minipar alone. We hope that with some 
optimization, the speed of the system will 
increase considerably. Even in the current time 
frame, it takes less than 0.6 second to parse a 
15 word sentence. 
 
Final tree MP tree 
(88%) 
SP tree 
(11%) 
Correct (82.5%) 73.5% 9% 
Wrong (16.5%) 14.5% 2% 
Table 3: Results for the extended parser 
6 Conclusions and Future Work 
In this paper we described a parser that extends 
a broad-coverage parser, Minipar, with a 
domain adaptable shallow parser in order to 
achieve generality and higher accuracy at the 
same time. This parser is an important 
component of a general NLP Toolkit, which 
helps programmers quickly develop an NLP 
front end that handles natural language input 
from their end users. We tested the parser on 
200 sentences from our corpus and the result 
shows significant improvement over using 
Minipar alone. 
Future work includes improving the 
efficiency and accuracy of the shallow parser. 
Also, we will test the parser on a different 
domain to see how much work is required to 
switch to a new domain. 
 
References 
Alam H. (2000) Spoken Language Generic 
User Interface (SLGUI). Technical Report 
AFRL-IF-RS-TR-2000-58, Air Force Research 
Laboratory, Rome.  
Brill E. (1992) A Simple Rule-based Part of 
Speech Tagger. In Proceedings of the 3rd 
Conference on Applied Natural Language 
Processing. 
Charniak E. (2000) A Maximum-Entropy-
Inspired Parser. In Proceedings of the 1st 
Meeting of NAACL. Washington. 
Chomsky N. (1995) Minimalist Program. 
MIT Press. 
Collins M. (1999) Head-Driven Statistical 
Models for Natural Language Parsing. PhD 
Dissertation, University of Pennsylvania. 
Is the sentence
passive?
Is the SP tree empty?
Is the SP  tree correct?
Adding the sentence type
and subject of the M P tree to
the SP tree
No
No
Yes
Accept M P tree
M inipar (M P) tree Shallow Parser (SP) tree
Final parse tree
Yes
No
Is the sentence
negative?
No
Yes
Yes
Yes
No
No
Yes
No
Is the M P tree correct?
No
Is the size of the M P
tree bigger than or
equal to that of the
SP  tree?
Is the length of an
NP chunk in  the M P
tree longer?
Yes
Does the M P tree
have less verb  tags?
Yes
No
No
Yes
Yes
No
Yes Does SP finds a verb
when M P assigns a
sentence type as NP?
Are the verb  tags in
the two trees
inconsistent?
Are there unequal
number of verb tags
in the trees?
Figure 1: Decision flow for parse tree selection 
 
Levin B. (1993) English Verb Classes and 
Alternations: A Preliminary Investigation. 
University of Chicago Press, Chicago. 
Lin D. (1998a) Dependency-based 
Evaluation of Minipar. In Workshop on the 
Evaluation of Parsing Systems, Spain. 
Lin D. (1998b) Extracting Collocations 
from Text Corpora. In Workshop on 
Computational Terminology, Montreal, 
Canada, pp. 57-63. 
Lin D. (2001) Latat: Language and Text 
Analysis Tools. In Proceedings of Human 
Language Technology Conference, CA, USA. 
Marcus M., Santorini B. and Marcinkiewicz 
M. (1993) Building a Large Annotated Corpus 
of English: The Penn Treebank, Computational 
Linguistics, vol. 19, no. 2, pp. 313-330.  
Sampson G. (1995) English for the 
Computer. Oxford University Press. 
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploring Semantic Constraints for Document Retrieval 
 
Hua Cheng, Yan Qu, Jesse Montgomery, David A. Evans 
Clairvoyance Corporation 
5001 Baum Blvd., Suite 700, Pittsburgh, PA 15213, U.S.A. 
{H.Cheng, Y.Qu, J.Montgomery, dae}@clairvoyancecorp.com 
 
  
 
Abstract 
In this paper, we explore the use of struc-
tured content as semantic constraints for 
enhancing the performance of traditional 
term-based document retrieval in special 
domains. First, we describe a method for 
automatic extraction of semantic content 
in the form of attribute-value (AV) pairs 
from natural language texts based on 
domain models constructed from a semi-
structured web resource. Then, we ex-
plore the effect of combining a state-of-
the-art term-based IR system and a sim-
ple constraint-based search system that 
uses the extracted AV pairs. Our evalua-
tion results have shown that such combi-
nation produces some improvement in IR 
performance over the term-based IR sys-
tem on our test collection. 
1 Introduction 
The questions of where and how sophisticated 
natural language processing techniques can im-
prove traditional term-based information re-
trieval have been explored for more than a dec-
ade. A considerable amount of work has been 
carried out that seeks to leverage semantic in-
formation for improving traditional IR. Early 
TREC systems such as INQUERY handled both 
natural language and semi-structured queries and 
tried to search for constraint expressions for 
country and time etc. in queries (Croft et al, 
1994). Later work, as discussed in (Strzalkowski 
et al, 1996), has focused on exploiting semantic 
information at the word level, including various 
attempts at word-sense disambiguation, e.g., 
(Voorhees, 1998), or the use of special-purpose 
terms; other approaches have looked at phrase-
level indexing or full-text query expansion. No 
approaches to date, however, have sought to em-
ploy semantic information beyond the word 
level, such as that expressed by attribute-value 
(AV) pairs, to improve term-based IR.  
Attribute-value pairs offer an abstraction for 
instances of many application domains. For ex-
ample, a person can be represented by a set of 
attributes such as name, date-of-birth, job title, 
and home address, and their associated values; a 
house has a different set of attributes such as ad-
dress, size, age and material; many product 
specifications can be mapped directly to AV 
pairs. AV pairs represent domain specific seman-
tic information for domain instances. 
Using AV pairs as semantic constraints for re-
trieval is related to some recent developments in 
areas such as Semantic Web retrieval, XML 
document retrieval, and the integration of IR and 
databases. In these areas, structured information 
is generally assumed. However, there is abundant 
and rich information that exists in unstructured 
text only. The goal of this work includes first to 
explore a method for automatically extracting 
structured information in the form of AV pairs 
from text, and then to utilize the AV pairs as se-
mantic constraints for enhancing traditional 
term-based IR systems. 
The paper is organized as follows. Section 2 
describes our method of adding AV annotations 
to text documents that utilizes a domain model 
automatically extracted from the Web. Section 3 
presents two IR systems using a vector space 
model and semantic constraints respectively, as 
well as a system that combines the two. Section 4 
describes the data set and topic set for evaluating 
the IR systems. In Section 5, we compare the 
performance of the three IR systems, and draw 
initial conclusions on how NLP techniques can 
improve traditional IR in specific domains. 
2 Domain-Driven AV Extraction 
This section describes a method that automati-
cally discovers attribute-value structures from 
unstructured texts, the result of which is repre-
sented as texts annotated with semantic tags.   
33
We chose the digital camera domain to illus-
trate and evaluate the methodology described in 
this paper. We expect this method to be applica-
ble to all domains whose main features can be 
represented as a set of specifications.  
2.1 Construction of Domain Model 
A domain model (DM) specifies a terminology 
of concepts, attributes and values for describing 
objects in a domain. The relationships between 
the concepts in such a model can be heterogene-
ous (e.g., the link between two concepts can 
mean inheritance or containment).  In this work, 
a domain model is used for establishing a vo-
cabulary as well as for establishing the attribute-
value relationship between phrases.  
For the digital camera domain, we automati-
cally constructed a domain model from existing 
Web resources. Web sites such as epinions.com 
and dpreview.com generally present information 
about cameras in HTML tables generated from 
internal databases. By querying these databases 
and extracting table content from the dynamic 
web pages, we can automatically reconstruct the 
databases as domain models that could be used 
for NLP purposes. These models can optionally 
be organized hierarchically. Although domain 
models generated from different websites of the 
same domain are not exactly the same, they often 
share many common features.  
From the epinions.com product specifications 
for 1157 cameras, we extracted a nearly compre-
hensive domain model for digital cameras, con-
sisting of a set of attributes (or features) and their 
possible values. A portion of the model is repre-
sented as follows: 
{Digital Camera} 
    <Brand> <Price> <Lens> 
{Brand} 
    (57) Canon 
    (33) Nikon 
{Price} $ 
    (136) 100 - 200 
    (100) >= 400 
{Lens} 
    <Optical Zoom> <Focus Range> 
{Optical Zoom} x 
    (17) 4 
    (3) 2.5 
{Focus Range} in., ? 
    (2) 3.9 - infinity 
    (1) 12 - infinity 
In this example, attributes are shown in curly 
brackets and sub-attributes in angle brackets. 
Attributes are followed by possible units for their 
numerical values. Values come below the attrib-
utes, headed by their frequencies in all specifica-
tions. The frequency information (in parentheses) 
is used to calculate term weights of attributes and 
values. 
Specifications in HTML tables generally do 
not specify explicitly the type restrictions on val-
ues (even though the types are typically defined 
in the underlying databases).  As type restrictions 
contain important domain information that is 
useful for value extraction, we recover the type 
restrictions by identifying patterns in values. For 
example, attributes such as price or dimension 
usually have numerical values, which can be ei-
ther a single number (?$300?), a range (?$100 - 
$200?), or a multi-dimensional value (?4 in. x 3 
in. x 2 in.?), often accompanied by a unit, e.g., $ 
or inches, whereas attributes such as brand and 
accessory usually have string values, e.g., 
?Canon? or ?battery charger?.  
We manually compile a list of units for identi-
fying numerical values, which is partially do-
main general. We identify range and multi-
dimensional values using such patterns as ?A ? 
B?, ?A to B?, ?less than A?, and ?A x B?, etc. 
Numerical values are then normalized to a uni-
form format. 
2.2 Identification of AV Pairs 
Based on the constructed domain model, we can 
identify domain values in unstructured texts and 
assign attribute names and domains to them. We 
focus on extracting values of a domain attribute.  
Attribute names appearing by themselves are not 
of interest here because attribute names alone 
cannot establish attribute-value relations. How-
ever, identification of attribute names is neces-
sary for disambiguation. 
The AV extraction procedure contains the fol-
lowing steps: 
1. Use MINIPAR (Lin, 1998) to generate 
dependency parses of texts. 
2. For all noun phrase chunks in parses, it-
eratively match sub-phrases of each 
chunk with the domain model to find all 
possible matches of attribute names and 
values above a threshold: 
? A chunk contains all words up to 
the noun head (inclusive); 
? Post-head NP components (e.g., 
PP and clauses) are treated as 
separate chunks. 
3. Disambiguate values with multiple at-
tribute assignments using the sentence 
context, with a preference toward closer 
context based on dependency. 
34
4. Mark up the documents with XML tags 
that represent AV pairs. 
Steps 2 and 3 are the center of the AV extrac-
tion process, where different strategies are em-
ployed to handle values of different types and 
where ambiguous values are disambiguated. We 
describe these strategies in detail below. 
Numerical Value 
Numerical values are identified based on the unit 
list and the range and multi-dimensional number 
patterns described earlier in Section 2.1. The 
predefined mappings between units and attrib-
utes suggest attribute assignment. It is possible 
that one unit can be mapped to multiple attrib-
utes.  For example, ?x? can be mapped to either 
optical zoom or digital zoom, both of which are 
kept as possible candidates for future disam-
biguation. For range and multi-dimensional 
numbers, we find all attributes in the domain 
model that have at least one matched range or 
multi-dimensional value, and keep attributes 
identified by either a unit or a pattern as candi-
dates. Numbers without a unit can only be 
matched exactly against an existing value in the 
domain model. 
String Value 
Human users often refer to a domain entity in 
different ways in text. For example, a camera 
called ?Canon PowerShot G2 Black Digital 
Camera? in our domain model is seldom men-
tioned exactly this way in ads or reviews, but 
rather as ?Canon PowerShot G2?, ?Canon G2?, 
etc.  However, a domain model generally only 
records full name forms rather than their all pos-
sible variations. This makes the identification of 
domain values difficult and invalidates the use of 
a trained classifier that needs training samples 
consisting of a large variety of name references. 
An added difficulty is that web texts often 
contain grammatical errors and incomplete sen-
tences as well as large numbers of out-of-
vocabulary words and, therefore, make the de-
pendency parses very noisy. As a result, effec-
tiveness of extraction algorithms based on certain 
dependency patterns can be adversely affected.  
Our approach makes use of the more accurate 
parser functionalities of part-of-speech tagging 
and phrase boundary detection, while reducing 
the reliance on low level dependency structures. 
For noun phrase chunks extracted from parse 
trees, we iteratively match all sub-phrases of 
each chunk with the domain model to find 
matching attributes and values above a threshold. 
It is often possible to find multiple AV pairs in a 
single NP chunk. 
Assigning domain attributes to an NP is essen-
tially a classification problem. In our domain 
model, each attribute can be seen as a target class 
and its values as the training set. For a new 
phrase, the idea is to find the value in the domain 
model that is most similar and then assign the 
attribute of this nearest neighbor to the phrase. 
This motivates us to adopt K Nearest Neighbor 
(KNN) (Fix and Hodges, 1951) classification for 
handling NP values. The core of KNN is a simi-
larity metric. In our case, we use word editing 
distance (Wagner and Fischer, 1974) that takes 
into account the cost of word insertions, dele-
tions, and substitutions. We compute word edit-
ing distance using dynamic programming tech-
niques. 
Intuitively, words do not carry equal weights 
in a domain. In the earlier example, words such 
as ?PowerShot? and ?G2? are more important 
than ?digital? and ?camera?, so editing costs for 
such words should be higher. This draws an 
analogy to the metric of Inverse Document Fre-
quency (IDF) in the IR community, used to 
measure the discriminative capability of a term 
in a document collection. If we regard each value 
string as a document, we can use IDF to measure 
the weight of each term in a value string to em-
phasize important domain terms and de-
emphasize more general ones. The normalized 
cost is computed as: 
)log(/)/log( TNNTN  
where TN is the total number of values for an 
attribute, and N is the number of values where a 
term occurs. This equation assigns higher cost to 
more discriminative terms and lower cost to 
more general terms.  It is also used to compute 
costs of terms in attribute names.  For words not 
appearing in a class the cost is 1, the maximum 
cost. 
The distance between a new phrase and a DM 
phrase is then calculated using word editing cost 
based on the costs of substitution, insertion, and 
deletion, where 
Costsub = (CostDM + Costnew) / 2 
Costins = Costnew 
Costdel = CostDM 
Costedit = min(Costsub, Costins, Costdel) 
 
where CostDM is the cost of a word in a domain 
value (i.e., its normalized IDF score), and Costnew 
35
is that of a word in the new phrase. The cost is 
also normalized by the larger of the weighted 
lengths of the two phrases.  We use a threshold 
of 0.6 to cut off phrases with higher cost.  
For a phrase that returns only a couple of 
matches, the similarity, i.e., the matching prob-
ability, is computed as 1 - Costedit; otherwise, the 
similarity is the maximum likelihood of an at-
tribute based on the number of returned values 
belonging to this attribute. 
Disambiguation by Sentence Context 
The AV identification process often returns mul-
tiple attribute candidates for a phrase that needs 
to be further disambiguated. The words close to 
the phrase usually provide good indications of 
the correct attribute names. Motivated by this 
observation, we design the disambiguation pro-
cedure as follows. First we examine the sibling 
nodes of the target phrase node in the depend-
ency structure for a mention of an attribute name 
that overlaps with a candidate. Next, we recur-
sively traverse upwards along the dependency 
tree until we find an overlap or reach the top of 
the tree. If an overlap is found, that attribute be-
comes the final assignment; otherwise, the at-
tribute with the highest probability is assigned. 
This method gives priority to the context closest 
(in terms of dependency) to the target phrase. For 
example, in the sentence ?The 4x stepless digital 
zoom lets you capture intricate details? (parse 
tree shown below), ?4x? can be mapped to both 
optical zoom and digital zoom, but the sentence 
context points to the second candidate.  
 
3 Document Retrieval Systems 
This section introduces three document retrieval 
systems: the first one retrieves unstructured texts 
based on vector space models, the second one 
takes advantage of semantic structures con-
structed by the methods in Section 2, and the last 
one combines the first two systems. 
3.1 Term-Based Retrieval (S1) 
Our system for term-based retrieval from un-
structured text is based on the CLARIT system, 
implementing a vector space retrieval model (Ev-
ans and Lefferts, 1995; Qu et al, 2005). The 
CLARIT system identifies terms in documents 
and constructs its index based on NLP-
determined linguistic constituents (NPs, sub-
phrases and words). The index is built upon full 
documents or variable-length subdocuments. We 
used subdocuments in the range of 8 to 12 sen-
tences as the basis for indexing and scoring 
documents in our experiments. 
Various similarity measures are supported in 
the model. For the experiments described in the 
paper, we used the dot product function for com-
puting similarities between a query and a docu-
ment: 
 
where WQ(t) is the weight associated with the 
query term t and WD(t) is the weight associated 
with the term t in the document D. The two 
weights were computed as follows: 
 
 
where IDF and TF are standard inverse docu-
ment frequency and term frequency statistics, 
respectively. IDF(t) was computed with the tar-
get corpus for retrieval. The coefficient C(t) is an 
?importance coefficient?, which can be modified 
either manually by the user or automatically by 
the system (e.g., updated during feedback). 
For term-based document retrieval, we have 
also experimented with pseudo relevance feed-
back (PRF) with various numbers of retrieved 
documents and various numbers of terms from 
such documents for query expansion. While PRF 
did result in improvement in performance, it was 
not significant. This is probably due to the fact 
that in this restricted domain, there is not much 
vocabulary variation and thus the advantage of 
using query expansion is not fully realized. 
3.2 Constraint-Based Retrieval (S2) 
The constraint-based retrieval approach searches 
through the AV-annotated document collection 
based on the constraints extracted from queries. 
Given a query q, our constraint-based system 
scores each document in the collection by com-
paring the extracted AV pairs with the con-
straints in q. Suppose q has a constraint c(a, v) 
that restricts the value of the attribute a to v, 
where v can be either a concrete value (e.g., 5 
megapixels) or a range (e.g., less than $400). If a 
)()()()( tIDFtTFtCtW QQ ??=
).()()( tIDFtTFtW DD ?=
).()(),( tWtWDQsim D
DQt
Q ?= ?
??
36
is present in a document d with a value v? that 
satisfies v, that is, v?= v if v is a concrete value or 
v? falls in the range defined by v, d is given a 
positive score w. However, if v? does not satisfy 
v, then d is given a negative score -w. No men-
tion of a does not change the score of d, except 
that, when c is a string constraint, we use a back-
off model that awards d a positive score w if it 
contains v as a substring. The final score of d 
given q is the sum of all scores for each con-
straint in q, normalized by the maximum score 
for q: ?
=
n
i
iiwc
1
, where ci is one of the n con-
straints specified in q and wi its score. 
We rank the documents by their scores. This 
scoring schema facilitates a sensible cutoff point, 
so that a constraint-based retrieval system can 
return 0 or fewer than top N documents when a 
query has no or very few relevant documents.  
3.3 Combined Retrieval (S3) 
Lee (1997) analyzed multiple post-search data 
fusion methods using TREC3 ad hoc retrieval 
data and explained the combination of different 
search results on the grounds that different runs 
retrieve similar sets of relevant documents, but 
different sets of non-relevant documents. The 
combination methods therefore boost the ranks 
of the relevant documents. One method studied 
was the summation of individual similarities, 
which bears no significant difference from the 
best approach (i.e., further multiply the summa-
tion with the number of nonzero similarities). 
Our system therefore adopts the summation 
method for its simplicity. Because the scores 
from term-based and constraint-based retrieval 
are normalized, we simply add them together for 
each document retrieved by both approaches and 
re-rank the documents based on their new scores. 
More sophisticated combination methods can be 
explored here, such as deciding which score to 
emphasize based on the characterizations of the 
queries, e.g., whether a query has more numeri-
cal values or string values. 
4 Experimental Study 
In this section, we describe the experiments we 
performed to investigate combining terms and 
semantic constraints for document retrieval. 
4.1 Data Sets 
To construct a domain corpus, we used search 
results from craigslist.org.  We chose the ?for 
sale ? electronics? section for the ?San Francisco 
Bay Area?.  We then submitted the search term 
?digital camera? in order to retrieve advertise-
ments.  After manually removing duplicates and 
expired ads, our corpus consisted of 437 ads 
posted between 2005-10-28 and 2005-11-07.  A 
typical ad is illustrated below, with a small set of 
XML tags specifying the fields of the title of the 
ad (title), date of posting (date), ad body (text), 
ad id (docno), and document (doc).  The length 
of the documents varies considerably, from 5 or 
6 sentences to over 70 (with specifications cop-
ied from other websites).  The ads have an aver-
age length of 230 words. 
 
<doc> 
<docno>docid519</docno> 
<title>brand new 12 mega pixel digital cam-
era</title> 
<date>2005-11-07,  8:27AM PST</date> 
<text> 
BRAND NEW 12 mega pixel digital cam-
era..............only $400, 
-12 Mega pixels (4000x3000) Max Resolution 
-2.0 Color LCD Display 
-8x Digital Zoom 
-16MB Built-In (internal) Memory 
-SD or MMC card (external) Memory 
-jpeg picture format 
ALSO COMES WITH SOFTWARE & CABLES 
</text> 
</doc> 
 
The test queries were constructed based on 
human written questions from the Digital Pho-
tography Review website (www.dpreview.com) 
Q&A forums, which contain discussions from 
real users about all aspects of digital photogra-
phy. Often, users ask for suggestions on purchas-
ing digital cameras and formulate their needs as a 
set of constraints. These queries form the base of 
our topic collection.  
The following is an example of such a topic 
manually annotated with the semantic constraints 
of interest to the user:  
<topic> 
<id>1</id> 
<query> 
I wanted to know what kind of Digital SLR cam-
era I should buy. I plan to spend nothing higher 
than $1500. I was told to check out the Nikon 
D70.  
</query> 
<constraint> 
<hard: type = ?SLR? /> 
<hard: price le $1500 /> 
<soft: product_name = ?Nikon D70? /> 
</constraint> 
</topic> 
37
In this example, the user query text is in the 
query field and the manually extracted AV con-
straints based on the domain model are in the 
constraint field. Two types of constraints are 
distinguished: hard and soft. The hard constraints 
must be satisfied while the soft constraints can be 
relaxed. Manual determination of hard vs. soft 
constraints is based on the linguistic features in 
the text. Automatic constraint extraction goes 
one step beyond AV extraction for the need to 
identify relations between attributes and values, 
for example, ?nothing higher than? indicates a 
?<=? relationship. Such constraints can be ex-
tracted automatically from natural text using a 
pattern-based method. However, we have yet to 
produce a rich set of patterns addressing con-
straints. In addition, such query capability can be 
simulated with a form-based parametric search 
interface. 
In order to make a fair comparison between 
systems, we use only phrases in the manually 
extracted constraints as queries to system S1. For 
the example topic, S1 extracted the NP terms 
?SLR?, ?1500? and ?Nikon D70?. During re-
trieval, a term is further decomposed into its sub-
terms for similarity matching. For instance, the 
term ?Nikon D70? is decomposed into subterms 
?Nikon? and ?D70? and thus documents that 
mention the individual subterms can be retrieved. 
For this topic, the system S2 produced annota-
tions as those shown in the constraint field.  
Table 1 gives a summary of the distribution 
statistics of terms and constraints for 30 topics 
selected from the Digital Photography Review 
website. 
 
 Average Min Max 
No. of terms 13.2 2 31 
No. of constraints 3.2 1 7 
No. of hard constraints 2.4 1 6 
No. of soft constraints 0.8 0 3 
No. of string constraints 1.4 0 5 
No. of numerical constraints 1.8 0 4 
Table 1: Summary of the distribution statistics of 
terms and constraints in the test topics 
 
4.2 Relevance Judgments 
Instead of using human subjects to give rele-
vance judgments for each document and query 
combination, we use a human annotator to mark 
up all AV pairs in each document, using the 
GATE annotation tool (Cunningham et al 2002). 
The attribute set contains the 40 most important 
attributes for digital cameras based on automati-
cally computed term distributions in our data set. 
The inter-annotator agreement (without annotator 
training) as measured by Kappa is 0.72, which 
suggests satisfactory agreement.  
Annotating AV pairs in all documents gives us 
the capability of making relevance judgments 
automatically, based on the number of matches 
between the AV pairs in a document and the 
constraints in a topic. This automatic approach is 
reasonable because unlike TREC queries which 
are short and ambiguous, the queries in our ap-
plication represent very specific information 
needs and are therefore much longer. The lack of 
ambiguity makes our problem closer to boolean 
search with structured queries like SQL than tra-
ditional IR search. In this case, a human assessor 
should give the same relevance judgments as our 
automatic system if they follow the same instruc-
tions closely. An example instruction could be ?a 
document is relevant if it describes a digital cam-
era whose specifications satisfy at least one con-
straint in the query, otherwise it is not relevant? 
(similar to the narrative field of a TREC topic).  
We specify two levels of relevance: strict and 
relaxed. Strict means that all hard constraints of a 
topic have to be satisfied for a document to be 
relevant to the topic, whereas relaxed means that 
at least half of the hard constraints have to be 
satisfied. Soft constraints play no role in a rele-
vance judgment. The advantage of the automatic 
approach is that when the levels of relevance are 
modified for different application purposes, the 
relevance judgment can be recomputed easily, 
whereas in the manual approach, the human as-
sessor has to examine all documents again. 
 
0
20
40
60
80
100
120
140
160
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Topic Number
N
um
be
r o
f R
el
ev
an
t D
oc
s
strict_judgments relaxed_judgments   
Figure 1: Distribution of relevant documents 
across topics for relaxed and strict judgments 
 
Figure 1 shows the distributions of the rele-
vant documents for the test topic set.  With strict 
judgments, only 20 out of the 30 topics have 
relevant documents, and among them 6 topics 
38
have fewer than 10 relevant documents. The top-
ics with many constraints are likely to result in 
low numbers of relevant documents. The average 
numbers of relevant documents for the set are 
57.3 for relaxed judgments, and 18 for strict 
judgments.  
5  Results and Discussion 
Our goal is to explore whether using semantic 
information would improve document retrieval, 
taking into account the errors introduced by se-
mantic processing. We therefore evaluate two 
aspects of our system: the accuracy of AV ex-
traction and the precision of document retrieval.  
5.1 Evaluate AV Extraction 
We tested the AV extraction system on a portion 
of the annotated documents, which contains 253 
AV pairs.  Of these pairs, 151 have string values, 
and the rest have numerical values. 
The result shows a prediction accuracy of 
50.6%, false negatives (missing AV pairs) 
35.2%, false positives 11%, and wrong predica-
tions 3%. Some attributes such as brand and 
resolution have higher extraction accuracy than 
other attributes such as shooting mode and di-
mension. An analysis of the missing pairs reveals 
three main sources of error: 1) an incomplete 
domain model, which misses such camera Con-
dition phrases as ?minor surface scratching?; 2) a 
noisy domain model, due to the automatic nature 
of its construction; 3) parsing errors caused by 
free-form human written texts. Considering that 
the predication accuracy is calculated over 40 
attributes and that no human labor is involved in 
constructing the domain model, we consider our 
approach a satisfactory first step toward explor-
ing the AV extraction problem. 
5.2 Evaluate AV-based Document Retrieval 
The three retrieval systems (S1, S2, and S3) each 
return top 200 documents for evaluation. Figure 
2 summarizes the precision they achieved against 
both the relaxed and strict judgments, measured 
by the standard TREC metrics (PN ? Precision at 
N, MAP ? Mean Average Precision, RP ? R-
Precision)1. For both judgments, the combined 
                                                 
1 Precision at N is the precision at N document cutoff point; 
Average Precision is the average of the precision value ob-
tained after each relevant document is retrieved, and Mean 
Average Precision is the average of AP over all topics; R-
Precision is the precision after R documents have been re-
trieved, where R is the number of relevant documents for 
the topic. 
system S3 achieved higher precision and recall 
than S1 and S2 by all metrics. In the case of re-
call, the absolute scores improve at least nine 
percent. Table 2 shows a pairwise comparison of 
the systems on three of the most meaningful 
TREC metrics, using paired T-Test; statistically 
significant results are highlighted. The table 
shows that the improvement of S3 over S1 and 
S2 is significant (or very nearly) by all metrics 
for the relaxed judgment. However, for the strict 
judgment, none of the improvements are signifi-
cant. The reason might be that one third of the 
topics have no relevant documents in our data set. 
This reduces the actual number of topics for 
evaluation. In general, the performance of all 
three systems for the strict judgment is worse 
than that for the relaxed, likely due to the lower 
number of relevant documents for this category 
(averaged at 18 per topic), which makes it a 
harder IR task. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P10 MAP RP Recall
S1_strict S2_strict S3_strict S1_relaxed S2_relaxed S3_relaxed  
Figure 2: System performance as measured by 
TREC metrics, averaged over all topics with non-
zero relevant documents 
 
Paired T-Test (p) P10 AP RP 
(S1,S2) .22 .37 .65 
(S2,S3) 1 .004 .10 
strict 
(S1,S3) .17 .48 .45 
(S1,S2) .62 .07 .56 
(S2,S3) .056 <.0001 .0007 
relaxed 
(S1,S3) .04 .02 .03 
Table 2: Paired T-Test (with two-tailed distribu-
tion) between systems over all topics 
 
The constraint-based system S2 produces 
higher initial precision than S1 as measured by 
P10. However, semantic constraints contribute 
less and less as more documents are retrieved. 
The performance of S2 is slightly worse than S1 
as measured by AP and RP, which is likely due 
to errors from AV extraction. None of the met-
rics is statistically significant.  
39
Topic-by-topic analysis gives us a more de-
tailed view of the behavior of the three systems.  
Figure 3 shows the performance of the systems 
measured by P10, sorted by that of S3. In gen-
eral, the performance of S1 and S2 deviates sig-
nificantly for individual topics. However, the 
combined system, S3, seems to be able to boost 
the good results from both systems for most top-
ics.  We are currently exploring the factors that 
contribute to the performance boost. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
16 8 15 18 4 26 30 11 24 28 7 10 2 5 13 20 21 27 3 25 1 12 17 29 19 6 23 14 22
Topic Numbers
Pr
ec
is
io
n 
@
 1
0
S1_relaxed S2_relaxed S3_relaxed  
Figure 3: Precision@10 for relaxed judgment 
 
A closer look at topics where S3 improves 
significantly over S1 and S2 at P10 reveals that 
the combined lists are biased toward the docu-
ments returned by S2, probably due to the higher 
scores assigned to documents by S2 than those 
by S1. This suggests the need for better score 
normalization methods that take into account the 
advantage of each system. 
In conclusion, our results show that using se-
mantic information can improve IR results for 
special domains where the information need can 
be specified as a set of semantic constraints. The 
constraint-based system itself is not robust 
enough to be a standalone IR system, and has to 
be combined with a term-based system to 
achieve satisfactory results. The IR results from 
the combined system seem to be able to tolerate 
significant errors in semantic annotation, consid-
ering that the accuracy of AV-extraction is about 
50%. It remains to be seen whether similar im-
provement in retrieval can be achieved in general 
domains such as news articles. 
6 Summary 
This paper describes our exploratory study of 
applying semantic constraints derived from at-
tribute-value pair annotations to traditional term-
based document retrieval. It shows promising 
results in our test domain where users have spe-
cific information needs. In our ongoing work, we 
are expanding the test topic set for the strict 
judgment as well as the data set, improving AV 
extraction accuracy, analyzing how the combined 
system improves upon individual systems, and 
exploring alternative ways of combining seman-
tic constraints and terms for better retrieval. 
References 
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva and Valentin Tablan.  2002. GATE: A 
Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. Pro-
ceedings of the 40th Anniversary Meeting of the 
Association for Computational Linguistics 
(ACL'02). Philadelphia. 
Bruce Croft, James Callan and John Broglio. 1994. 
TREC-2 Routing and Ad-Hoc Retrieval Evaluation 
Using the INQUERY System. In Proceedings of 
the 2nd Text Retrieval Conference, NIST Special 
Putlication 500-215. 
David A. Evans and Robert Lefferts. 1995.  CLARIT-
TREC experiments.  Information Processing and 
Management, 31(3), 385-395. 
E. Fix and J. Hodges. 1951. Discriminatory Analysis, 
Nonparametric Discrimination: Consistency Prop-
erties. Technical Report, USAF School of Aviation 
Medicine, Texas. 
Joon Ho Lee. 1997. Analyses of Multiple Evidence 
Combination. Proceedings of the 20th Annual In-
ternational ACM-SIGIR Conference on Research 
and Development in Information Retrieval. Phila-
delphia, pp. 267-276. 
Dekang Lin. 1998. Dependency-based Evaluation of 
MINIPAR. Workshop on the Evaluation of Parsing 
Systems, Spain.  
Yan Qu, David A. Hull, Gregory Grefenstette, David 
A. Evans, et al 2005. Towards Effective Strategies 
for Monolingual and Bilingual Information Re-
trieval: Lessons Learned from NTCIR-4. ACM 
Transactions on Asian Language Information 
Processing, 4(2): 78-110. 
Robert Wagner and Michael Fischer. 1974. The 
String-to-string Correction Problem. Journal of the 
Association for Computing Machinery, 21(1):168-
173.  
Tomek Strzalkowski, Louise Guthrie, Jussi Karigren, 
Jim Leistensnider, et al 1996. Natural language in-
formation retrieval, TREC-5 report. In Proceedings 
of the 5th Text Retrieval Conference (TREC-5), pp. 
291-314, Gaithersburg, Maryland. 
Ellen Voorhees. 1998. Using WordNet for text re-
trieval. In Wordnet, an Electronic Lexical Data-
base, pp 285-303. The MIT Press. 
 
40
