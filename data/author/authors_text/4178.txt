Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1060?1068, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Extending Machine Translation Evaluation Metrics with Lexical Cohesion
To Document Level
Billy T. M. Wong and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong SAR, P. R. China
{tmwong,ctckit}@cityu.edu.hk
Abstract
This paper proposes the utilization of lexical
cohesion to facilitate evaluation of machine
translation at the document level. As a linguis-
tic means to achieve text coherence, lexical
cohesion ties sentences together into a mean-
ingfully interwoven structure through words
with the same or related meaning. A compar-
ison between machine and human translation
is conducted to illustrate one of their critical
distinctions that human translators tend to use
more cohesion devices than machine. Various
ways to apply this feature to evaluate machine-
translated documents are presented, including
one without reliance on reference translation.
Experimental results show that incorporating
this feature into sentence-level evaluation met-
rics can enhance their correlation with human
judgements.
1 Introduction
Machine translation (MT) has benefited a lot from
the advancement of automatic evaluation in the past
decade. To a certain degree, its progress is also con-
fined to the limitations of evaluation metrics in use.
Most efforts devoted to evaluate the quality of MT
output so far have still focused on the sentence level
without sufficient attention to how a larger text is
structured. This is notably reflected in the represen-
tative MT evaluation metrics, such as BLEU (Pap-
ineni et al 2002), METEOR (Banerjee and Lavie,
2005) and TER (Snover et al 2006), that adopt a
sentence-by-sentence fashion to score MT outputs.
The evaluation result for a document by any of them
is usually a simple average of its sentence scores. A
drawback of this kind of sentence-based evaluation
is the neglect of document structure. There is no
guarantee for the coherence of a text if it is produced
by simply putting together stand-alone sentences, no
matter how well-translated, without adequate inter-
sentential connection. As a consequence, MT sys-
tem optimized this way to any of these metrics can
only have a very dim chance of producing translated
document that reads as natural as human writing.
The accuracy of MT output at the document level
is particularly important to MT users, for they care
about the overall meaning of a text in question more
than the grammatical correctness of each sentence
(Visser and Fuji, 1996). Post-editors particularly
need to ensure the quality of a whole document of
MT output when revising its sentences. The con-
nectivity of sentences is surely a significant factor
contributing to the understandability of a text as a
whole.
This paper studies the inter-sentential linguistic
features of cohesion and coherence and presents
plausible ways to incorporate them into the
sentence-based metrics to support MT evaluation at
the document level. In the Framework for MT Eval-
uation in the International Standards of Language
Engineering (FEMTI) (King et al 2003), coherence
is defined as ?the degree to which the reader can de-
scribe the role of each individual sentence (or group
of sentences) with respect to the text as a whole?.
The measurement of coherence has to rely on cohe-
sion, referring to the ?relations of meaning that exist
within the text? (Halliday and Hasan, 1976). Cohe-
sion is realized via the interlinkage of grammatical
and lexical elements across sentences. Grammatical
1060
cohesion refers to the syntactic links between text
items, while lexical cohesion is achieved through the
word choices in a text. This paper focuses on the
latter. A quantitative comparison of lexical cohesion
devices between MT output and human translation
is first conducted, to examine the weakness of cur-
rent MT systems in handling this feature. Different
ways of exploiting lexical cohesion devices for MT
evaluation at the document level are then illustrated.
2 Related Works
Cohesion and coherence are both necessary mono-
lingual features in a target text. They can hardly
be evaluated in isolation and have to be conjoined
with other quality criteria such as adequacy and flu-
ency. A survey of MT post-editing (Vasconcellos,
1989) suggests that cohesion and coherence serve
as higher level quality criteria beyond many others
such as syntactic well-formedness. Post-editors tend
to correct syntactic errors first before any amend-
ment for improving the cohesion and coherence of
an MT output. Also, as Wilks (1978)1 noted, it
is rather unlikely for a sufficiently large sample of
translations to be coherent and totally wrong at the
same time. Cohesion and coherence are appropri-
ate to serve as criteria for the overall quality of MT
output.
Previous researches in MT predominantly focus
on specific types of cohesion devices. For grammat-
ical cohesion, a series of works, including Nakaiwa
and Ikehara (1992), Nakaiwa et al(1995), and
Nakaiwa and Shirai (1996), present approaches to
resolving Japanese zero pronouns and to integrat-
ing them into a Japanese-English transferred-based
MT system. Peral et al(1999) propose an inter-
lingual mechanism for pronominal anaphora gen-
eration by exploiting a rich set of lexical, syntac-
tic, morphologic and semantic information. Mu-
rata and Nagao (1993) and Murata et al(2001) de-
velop a rule base to identify the referential prop-
erties of Japanese noun phrases, so as to facilitate
anaphora resolution for Japanese and article gen-
eration for English during translation. A recent
COMTIS project (Cartoni et al 2011) begins to ex-
ploit inter-sentential information for statistical MT.
A phase of its work is to have grammatical devices,
1As cited in van Slype (1979).
such as verbal tense/aspect/mode, discourse connec-
tives and pronouns, manually annotated in multilin-
gual corpora, in hopes of laying a foundation for the
development of automatic labelers for them that can
be integrated into an MT model.
For lexical cohesion, it has been only partially and
indirectly addressed in terms of translation consis-
tency in MT output. Different approaches to main-
taining consistency in target word choices are pro-
posed (Itagaki et al 2007; Gong et al 2011; Xiao
et al 2011). Carpuat (2009) also observes a general
tendency in human translation that a given sense is
usually lexicalized in a consistent manner through-
out the whole translation.
Nevertheless there are only a few evaluation
methods explicitly targeting on the quality of a docu-
ment. Miller and Vanni (2001) devise a human eval-
uation approach to measure the comprehensibility
of a text as a whole, based on the Rhetorical Struc-
ture Theory (Mann and Thompson, 1988), a theory
of text organization specifying coherence relations
in an authentic text. Snover et al(2006) proposes
HTER to assess post-editing effort through human
annotation. Its automatic versions TER and TERp
(Snover et al 2009), however, remain sentence-
based metrics. Comelles et al(2010) present a
family of automatic MT evaluation measures, based
on the Discourse Representation Theory (Kamp and
Reyle, 1993), that generate semantic trees to put to-
gether different text entities for the same referent ac-
cording to their contexts and grammatical connec-
tions. Apart from MT evaluation, automated essay
scoring programs such as E-rater (Burstein, 2003)
also employ a rich set of discourse features for as-
sessment. However, the parsing process needed for
these linguistic-heavy approaches may suffer seri-
ously from grammatical errors, which are unavoid-
able in MT output. Hence their accuracy and reli-
ability inevitably fluctuate in accord with different
evaluation data.
Lexical cohesion has far been neglected in both
MT and MT evaluation, even though it is the single
most important form of cohesion devices, account-
ing for nearly half of the cohesion devices in En-
glish (Halliday and Hasan, 1976). It is also a signif-
icant feature contributing to translation equivalence
of texts by preserving their texture (Lotfipour-Saedi,
1997). The lexical cohesion devices in a text can be
1061
represented as lexical chains conjoining related en-
tities. There are many methods of computing lexical
chains for various purposes, e.g., Morris and Hirst
(1991), Barzilay and Elhadad (1997), Chan (2004),
Li et al(2007), among many others. Contrary to
grammatical cohesion highly depending on syntac-
tic well-formedness of a text, lexical cohesion is less
affected by grammatical errors. Its computation has
to rely on a thesaurus, which is usually available for
almost every language. In this research, a number
of formulations of lexical cohesion, with or without
reliance on external language resource, will be ex-
plored for the purpose of MT evaluation.
3 Lexical Cohesion in Machine and
Human Translation
This section presents a comparative study of MT and
human translation (HT) in terms of the use of lexi-
cal cohesion devices. It is an intuition that more co-
hesion devices are used by humans than machines
in translation, as part of the superior quality of HT.
Two different datasets are used to ensure the relia-
bility and generality of the comparison. The results
confirm the incapability of MT in handling this fea-
ture and the necessity of using lexical cohesion in
MT evaluation.
3.1 Data
The MetricsMATR 2008 development set (Przy-
bocki et al 2009) and the Multiple-Translation Chi-
nese (MTC) part 4 (Ma, 2006) are used for this
study. They consist of MT outputs of different
source languages in company with reference trans-
lations. The data of MetricsMATR is selected from
the NIST Open MT 2006 evaluation, while MTC4 is
from the TIDES 2003 MT evaluation. Both datasets
include human assessments of MT output, from
which the part of adequacy assessment is selected
for this study. Table 1 provides overall statistics of
the datasets.
3.2 Identification of Lexical Cohesion Devices
Lexical cohesion is achieved through word choices
of two major types: reiteration and collocation. Re-
iteration can be realized in a continuum or a cline of
specificity, with repetition of the same lexical item at
one end and the use of a general noun to point to the
MetricsMATR MTC4
Number of systems 8 6
Number of documents 25 100
Number of segments 249 919
Number of references 4 4
Source language Arabic Chinese
Genre Newswire Newswire
Table 1: Information about the datasets in use
same referent at the other. In between the two ends
is to use a synonym (or near-synonym) and superor-
dinate. Collocation refers to those lexical items that
share the same or similar semantic relations, includ-
ing complementarity, antonym, converse, coordinate
term, meronym, troponym, and so on.
In this study, lexical cohesion devices are defined
as content words (i.e., tokens after stopword having
been removed) that reiterate once or more times in
a document, including synonym, near-synonym and
superordinate, besides those repetition and colloca-
tion. Repetition refers to the same words or stems
in a document. Stems are identified with the aid of
Porter stemmer (1980).
To classify the semantic relationships of words,
WordNet (Fellbaum, 1998) is used as a lexical re-
source, which clusters words of the same sense (i.e.,
synonyms) into a semantic group, namely a synset.
Synsets are interlinked in WordNet according to
their semantic relationships. Superordinate and col-
location are formed by words in a proximate se-
mantic relationship, such as bicycle and vehicle (hy-
pernym), bicycle and wheel (meronym), bicycle and
car (coordinate term), and so on. They are defined
as synset pairs with a distance of 1 in WordNet.
The measure of semantic distance (Wu and Palmer,
1994) is also applied to identify near-synonyms, i.e.,
words that are synonyms in a broad sense but not
grouped in the same synset. It quantifies the seman-
tic similarity of word pairs as a real number in be-
tween 0 and 1 (the higher the more similar) as
sim(c1, c2) =
2 d(lcs(c1, c2))
d(c1) + d(c2)
where c1 and c2 are the concepts (synsets) that the
two words in question belong to, d is the distance
in terms of the shortest path from a concept to the
1062
Word type
MetricsMATR MTC4
MT HT Difference (%) MT HT Difference (%)
Content word 4428 4636 208 (4.7) 16162 16982 830 (5.1)
- Not lexical cohesion device 2403 2381 -22 (-1.0) 8657 8814 157 (1.8)
- Lexical cohesion device 2025 2255 230 (11.4) 7505 8168 663 (8.9)
- Repetition 1297 1445 148 (11.4) 4888 5509 621 (12.7)
- Synonym and near-synonym 318 350 32 (10.1) 1323 1311 -12 (-0.9)
- Superordinate and collocation 410 460 50 (12.4) 1294 1348 54 (4.2)
Table 2: Statistics of lexical cohesion devices in machine versus human translation (average frequencies per version
of MT/HT)
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
MetricsMATR 
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
MTC4 
RC (MT)
RC (HT)
LC (MT)
LC (HT)
V
al
ue
s 
of
 R
C
 &
 L
C
 
Figure 1: Use of lexical cohesion devices in machine versus human translation
global root node in WordNet, and lcs is the least
common subsumer (i.e., the most specific ancestor
concept) of c1 and c2. A threshold is set to 0.96
for words to be considered near-synonyms of each
other, based on the empirical observation in a previ-
ous study (Wong, 2010).
3.3 Results
The difference between MT and HT (reference
translation) in terms of the frequencies of lexical co-
hesion devices in MetricsMATR and MTC4 datasets
is presented in Table 2. The frequencies are aver-
aged by the number of MT/HT versions. A further
categorization breaks down content words into lex-
ical cohesion devices and those that are not. The
count of each type of lexical cohesion device is also
provided. In general the two datasets provide highly
similar statistics. There are 4.7?5.1% more content
words in HT than in MT. The numbers of ordinary
content words (i.e., not lexical cohesion devices) are
close in MT and HT. The difference of content words
in HT and MT is mostly due to that of lexical co-
hesion devices, which are mostly repetition. 8.9?
11.4% more lexical cohesion devices are found in
HT than in MT in the datasets.
A further analysis is carried out to investigate into
the use of lexical cohesion devices in each version
of MT and HT in terms of the following two ratios,
LC = lexical cohesion devices / content words,
RC = repetition / content words.
A higher LC or RC ratio means that a greater pro-
portion of content words are used as lexical cohesion
devices.
Figure 1 illustrates the RC and LC ratios in the
two datasets. The ratios of different MT systems
are presented in an ascending order in each graph
from left to right, according to their human assess-
ment results. The distributions of these values show
a strong similarity between the two datasets. First,
most of the RC and LC ratios are within an observ-
able range, i.e., 0.25?0.35 for the former and 0.40?
0.50 for the latter, except a particularly low LC for
1063
MT 1
1 Chine scrambled research on 16 key technical
2 These techniques are from within headline everyones boosting science and technology and achiev-
ing goals and contend of delivered on time bound through achieving breakthroughs in essential
technology and complimentarity resources . national
BLEU: 0.224 (1-gram:7, 2-gram:0, 3-gram:2, 4-gram:1)
LC: 0.107 (number of lexical cohesion devices: 5)
Human assessment: 2.67
MT 2
1 China is accelerating research 16 main technologies
2 These technologies are within the important realm to promote sciences and technology and
achieve national goals and must be completed in a timely manner through achieving main dis-
coveries in technology and integration of resources .
BLEU: 0.213 (1-gram:5, 2-gram:3, 3-gram:2, 4-gram:1)
LC: 0.231 (number of lexical cohesion devices: 9)
Human assessment: 4.33
Reference
1 China Accelerates Research on 16 Main Technologies
2 These technologies represent a significant part in the development of science and technology and
the achievement of national goals. They must be accomplished within a fixed period of time by
realizing breakthroughs in essential technologies and integration of resources.
Table 3: An example of MT outputs of different quality (underlined: matched n-grams; italic: lexical cohesion devices)
one MT system. Second, the ratios in those differ-
ent HT versions are very stable in comparison with
those of MT. Especially, all four HT versions in the
MetricsMATR dataset share the sameRC ratio 0.31.
This shows a typical level of the use of lexical cohe-
sion device. Third, the ratios in MT are lower than or
at most equal to those in HT, suggesting their corre-
lation with translation quality: the closer their RC
and LC ratios to those in HT, the better the MT.
These results verify our assumption that lexical co-
hesion can serve as an effective proxy of the level of
translation quality.
4 MT Evaluation at Document Level
As a feature at the discourse level, lexical cohesion
is a good complement to current evaluation met-
rics focusing on features at the sentence level. Ta-
ble 3 illustrates an example selected from the Met-
ricsMATR dataset, consisting two versions of MT
output for a short document of two segments only.
The n-grams matched with the reference are under-
lined, while the lexical cohesion devices are itali-
cized. The two MT outputs have a similar num-
ber of matched n-grams and hence receive similar
BLEU scores. These scores, however, do not reflect
their real difference in quality: the second version is
better, according to human assessment of adequacy.
Instead, their LC ratios seem to represent such a
variation more accurately. The theme of the second
output is also highlighted through the lexical chains,
including main/important, technology/technologies
and achieve/achieving, which create a tight texture
between the two sentences, a crucial factor of text
quality.
To perform MT evaluation at the document level,
the LC and RC ratios can be used alone or in-
tegrated into a sentence-level metric. The former
way has an advantage that it does not have to rely
on any reference translation. LC mainly requires
a thesaurus for computing semantic relation, while
RC only needs a morphological processor such as
stemmer, both of which are available for most lan-
1064
guages. Its drawback, however, lies in the risk of
relying on a single discourse feature. Although lex-
ical cohesion gives a strong indication of text co-
herence, it is not indispensable, because a text can
be coherent without any surface cohesive clue. Fur-
thermore, the quality of a document is also reflected
in that of its sentences. A coherent translation may
be mistranslated, and on the other hand, a text con-
taining lots of sentence-level errors would make it
difficult to determine its document-level quality. A
previous study comparing MT evaluation at the sen-
tence versus document level (Wong et al 2011) re-
ports a poor consistency in the evaluation results at
these two levels when the sentence-level scores of
MT output are low. In regard of these, how to inte-
grate these two levels of MT evaluation is particu-
larly worth studying.
5 Experiments
We examine, through experiments, the effectiveness
of using LC and RC ratios alone and integrating
them into other evaluation metrics for MT evalua-
tion at the document and system levels. Three evalu-
ation metrics, namely BLEU, TER and METEOR,2
are selected for testing. They represent three dis-
tinctive types of evaluation metrics: n-gram, edit-
distance, and unigram with external language re-
sources, respectively. These metrics are evaluated in
terms of their correlation with human assessments,
using Pearson?s r correlation coefficient. The Met-
ricsMATR and MTC4 datasets and their adequacy
assessments are used as evaluation data. Note that
the adequacy assessment is in fact an evaluation
method for the sentence level. We have to rely on
an assumption that this evaluation data may emulate
document-level quality, since its MT outputs were
assessed sentence by sentence in sequence as in a
document. All experiments are performed under a
setting of multiple reference translations.
The integration of the two ratios into an evaluation
metric follows a simple weighted average approach.
A hybrid metric H is formulated as
H = ? mdoc + (1? ?) mseg
where mdoc refers to the document-level feature in
2METEOR 1.0 with default parameters optimized over the
adequacy assessments.
use (i.e., LC or RC), mseg to a sentence-level met-
ric, and ? to a weight controlling their proportion.
The MetricsMATR dataset is used as training data to
optimize the values of ? for different metrics, while
the MTC4 is used as evaluation data. Table 4 shows
the optimized weights for the metrics for evaluation
at the document level.
Metrics RC LC
BLEU 0.28 0.29
TER 0.40 0.38
METEOR 0.19 0.18
Table 4: Optimized weights for the integration of dis-
course feature into sentence-level metrics
Table 5 presents the correlation rates of evalua-
tion metrics obtained in our experiments under dif-
ferent settings, with their 95% conference intervals
(CI) provided. The LC and RC ratios are found to
have strong correlations with human assessments at
the system level even when used alone, highly com-
parable to BLEU and TER. At the document level,
however, they are not as good as the others. They
show their advantages when integrated into other
metrics, especially BLEU and TER. LC raises the
correlation of BLEU from 0.447 to 0.472 and from
0.861 to 0.905 at the document and system levels,
respectively. It improves TER even more signifi-
cantly, in that the correlation rates are boosted up
from -0.326 to -0.390 at the document level, and
even from -0.601 to -0.763 at the system level. Since
there are only six systems in the MTC4 data, such a
dramatic change may not be as meaningful as the
smooth improvement at the document level. ME-
TEOR is a special case in this experiment. Its corre-
lation cannot be improved by integrating LC orRC,
and is even slightly dropped at the document level.
The cause for this is yet to be identified. Neverthe-
less, these results confirm the close relationship of
an MT system?s capability to appropriately generate
lexical cohesion devices with the quality of its out-
put.
Table 6 presents the Pearson correlations between
evaluation results at the document level using dif-
ferent evaluation metrics in the MTC4 data. It il-
lustrates the homogeneity/heterogeneity of different
metrics and helps explain the performance change
1065
Document System
Metrics Correlation 95% CI Correlation 95% CI
RC 0.243 (0.167, 0.316) 0.873 (0.211, 0.985)
LC 0.267 (0.192, 0.339) 0.818 (0.020, 0.979)
BLEU 0.447 (0.381, 0.508) 0.861 (0.165, 0.984)
BLEU+RC 0.463 (0.398, 0.523) 0.890 (0.283, 0.987)
BLEU+LC 0.472 (0.408, 0.531) 0.905 (0.352, 0.989)
TER -0.326 (-0.253, -0.395) -0.601 (-0.411, -0.949)
TER+RC -0.370 (-0.299, -0.437) -0.740 (-0.179, -0.969)
TER+LC -0.390 (-0.320, -0.455) -0.763 (-0.127, -0.972)
METEOR 0.557 (0.500, 0.609) 0.961 (0.679, 0.995)
METEOR+RC 0.555 (0.498, 0.608) 0.960 (0.672, 0.995)
METEOR+LC 0.556 (0.499, 0.609) 0.962 (0.687, 0.995)
Table 5: Correlation of different metrics with adequacy assessment in MTC4 data
BLEU 1
TER -0.699 1
METEOR 0.834 -0.510 1
RC 0.287 -0.204 0.405 1
LC 0.263 -0.097 0.437 0.736 1
BLEU TER METEOR RC LC
Table 6: Correlation between the evaluation results of different metrics
by combining sentence- and document-level met-
rics. The table shows that the two ratios LC and
RC highly correlate with each other, as if they are
two variants of quantifying lexical cohesion devices.
The three sentence-level metrics, BLEU, TER and
METEOR, also show strong correlations with each
other, especially between BLEU and METEOR. The
correlations are generally weaker between sentence-
and document-level metrics, for instance, 0.263 be-
tween BLEU and LC and only -0.097 between TER
and LC, showing that they are quite heterogeneous
in nature. This accounts for the significant perfor-
mance gain from their combination: their difference
allows them to complement each other. It is also
worth noting that between METEOR and LC the
correlation of 0.437 is mildly strong, explaining the
negative result of their integration. On the one hand,
lexical cohesion is word choice oriented, which is
only sensitive to the reiteration and semantic relat-
edness of words in MT output. On the other hand,
METEOR is strong in unigram matching, with mul-
tiple strategies to maximize the matching rate be-
tween MT output and reference translation. In this
sense they are homogeneous to a certain extent, ex-
plaining the null effect of their combination.
6 Discussion and Conclusion
In this study we have attempted to address the prob-
lem that most existing MT evaluation metrics dis-
regard the connectivity of sentences in a document.
By focusing on a typical type of cohesion, i.e., lexi-
cal cohesion, we have shown that its use frequency is
a significant factor to differentiate HT from MT and
MT outputs of different quality from each other. The
high correlation rate of its use with translation ade-
quacy also suggests that the more lexical cohesion
devices in use, the better the quality of MT output.
Accordingly we have used two ratios, LC and RC,
to capture such correlativity. Our experimental re-
sults have confirmed the effectiveness of this feature
in accounting for the document-level quality of MT
output. The performance of two evaluation metrics,
BLEU and TER, is highly improved through incor-
porating this document-level feature, in terms of the
1066
change of their correlation with human assessments.
This finding is positive and sheds light on a region
of MT research that is still severely under-explored.
Our approach to extending the granularity of MT
evaluation from sentence to document through lex-
ical cohesion is highly applicable to different lan-
guages. It has a relatively weak demand for lan-
guage resource in comparison with the processing of
other discourse features like grammatical cohesion.
It is also much unaffected by grammatical problems
or errors commonly seen in natural languages and,
in particular, MT outputs.
Our future work will continue to explore the re-
lationship of lexical cohesion to translation quality,
so as to identify, apart from its use frequency, other
significant aspects for MT evaluation at the docu-
ment level. A frequent use of cohesion devices in
a text is not necessarily appropriate, because an ex-
cess of them may decrease the quality and readabil-
ity of a text. Human writers can strategically change
the ways of expression to achieve appropriate coher-
ence and also avoid overuse of the same lexical item.
To a certain extent, this is one of the causes for the
unnaturalness of MT output: it may contain a large
number of lexical cohesion devices which are sim-
ply direct translation of those in a source text that
do not fit in the target context. How to use lexical
cohesion devices appropriately instead of frequently
is thus an important issue to tackle before we can
adopt them in MT and MT evaluation by a suitable
means.
Acknowledgments
The research described in this paper was substan-
tially supported by the Research Grants Council
(RGC) of Hong Kong SAR, P. R. China through
GRF grant 144410.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Jill Burstein. 2003. The E-rater scoring engine: Auto-
mated essay scoring with natural language processing.
In Mark D. Shermis and Jill Burstein, editors, Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive, chapter 7, pages 113?122. Lawrence Erlbaum As-
sociates.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Future
Directions, pages 19?27, Boulder, Colorado.
Bruno Cartoni, Andrea Gesmundo, James Henderson,
Cristina Grisot, Paola Merlo, Thomas Meyer, Jacques
Moeschler, Sandrine Zufferey, and Andrei Popescu-
Belis. 2011. Improving MT coherence through text-
level processing of input texts: The COMTIS project.
In Tralogy, Paris.
Samuel W. K. Chan. 2004. Extraction of sailent tex-
tual patterns: Synergy between lexical cohesion and
contextual coherence. IEEE Transactions on Systems,
Man and Cybernetics, Part A: Systems and Humans,
34(2):205?218.
Elisabet Comelles, Jesus Gime?nez, Llu?`s Ma?rquez, Irene
Castello`n, and Victoria Arranz. 2010. Document-
level automatic MT evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 333?338, Uppsala.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In EMNLP 2011, pages 909?919, Edinburgh,
Scotland.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. London: Longman.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology translation
consistency with statistical method. In MT Summit XI,
pages 269?274.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Dordrecht: Kluwer.
Margaret King, Andrei Popescu-Belis, and Eduard Hovy.
2003. FEMTI: Creating and using a framework for
MT evaluation. In MT Summit IX, pages 224?231,
New Orleans.
Jing Li, Le Sun, Chunyu Kit, and Jonathan Webster.
2007. A query-focused multi-document summarizer
based on lexical chains. In DUC 2007, Rochester,
New York.
1067
Kazem Lotfipour-Saedi. 1997. Lexical cohesion and
translation equivalence. Meta, 42(1):185?192.
Xiaoyi Ma. 2006. Multiple-Translation Chinese (MTC)
part 4. Linguistic Data Consortium.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Keith J. Miller and Michelle Vanni. 2001. Scaling
the ISLE taxonomy: Development of metrics for the
multi-dimensional characterisation of machine trans-
lation quality. In MT Summit VIII, pages 229?238.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In TMI 1993, pages 218?225, Kyoto.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hi-
toshi Isahara. 2001. A machine-learning approach to
estimating the referential properties of Japanese noun
phrases. In CICLING 2001, pages 142?153, Mexico-
City.
Hiromi Nakaiwa and Satoru Ikehara. 1992. Zero pro-
noun resolution in a machine translation system by us-
ing Japanese to English verbal semantic attributes. In
ANLP 1992, pages 201?208.
Hiromi Nakaiwa and Satoshi Shirai. 1996. Anaphora
resolution of Japanese zero pronouns with deictic ref-
erence. In COLING 1996, pages 812?817, Copen-
hagen.
Hiromi Nakaiwa, Satoshi Shirai, Satoru Ikehara, and
Tsukasa Kawaok. 1995. Extrasentential resolution
of Japanese zero pronouns using semantic and prag-
matic constraints. In Proceedings of the AAAI 1995
Spring Symposium Series: Empirical Methods in Dis-
course Interpretation and Generation, pages 99?105,
Stanford.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL 2002, pages
311?318.
Jesu?s Peral, Manuel Palomar, and Antonio Ferra`ndez.
1999. Coreference-oriented interlingual slot structure
and machine translation. In Proceedings of the ACL
Workshop on Coreference and its Applications, pages
69?76, College Park, MD.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Mark Przybocki, Kay Peterson, and Se?bastien Bronsart.
2009. 2008 NIST metrics for machine translation
(MetricsMATR08) development data. Linguistic Data
Consortium.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA 2006, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on Statistical Machine Translation, pages 259?
268, Athens.
Georges van Slype. 1979. Critical Study of Methods for
Evaluating the Quality of Machine Translation. Tech-
nical report, Bureau Marcel van Dijk / European Com-
mission, Brussels.
Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E. Alatis, editor, Georgetown University Round
Table on Languages and Linguistics 1989: Language
Teaching, Testing, and Technology: Lessons from the
Past with a View Toward the Future, pages 89?105.
Georgetown University Press.
Eric M. Visser and Masaru Fuji. 1996. Using sentence
connectors for evaluating MT output. In COLING
1996, pages 1066?1069.
Yorick Wilks. 1978. The Value of the Monolingual
Component in MT Evaluation and its Role in the Bat-
telle. Report on Systran, Luxembourg CEC Memoran-
dum.
Billy T. M. Wong, Cecilia F. K. Pun, Chunyu Kit, and
Jonathan J. Webster. 2011. Lexical cohesion for eval-
uation of machine translation at document level. In
NLP-KE 2011, pages 238?242, Tokushima.
Billy Tak-Ming Wong. 2010. Semantic evaluation of ma-
chine translation. In LREC 2010, pages 2884?2888,
Valletta.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In ACL 1994, pages 133?138,
Las Cruces.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In MT summit XIII, pages 131?138,
Xiamen.
1068
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?5,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Higher-Order Constituent Parsing and Parser Combination?
Xiao Chen and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong SAR, China
{cxiao2,ctckit}@cityu.edu.hk
Abstract
This paper presents a higher-order model for
constituent parsing aimed at utilizing more lo-
cal structural context to decide the score of
a grammar rule instance in a parse tree. Ex-
periments on English and Chinese treebanks
confirm its advantage over its first-order ver-
sion. It achieves its best F1 scores of 91.86%
and 85.58% on the two languages, respec-
tively, and further pushes them to 92.80%
and 85.60% via combination with other high-
performance parsers.
1 Introduction
Factorization is crucial to discriminative parsing.
Previous discriminative parsing models usually fac-
tor a parse tree into a set of parts. Each part is scored
separately to ensure tractability. In dependency
parsing (DP), the number of dependencies in a part
is called the order of a DP model (Koo and Collins,
2010). Accordingly, existing graph-based DP mod-
els can be categorized into tree groups, namely, the
first-order (Eisner, 1996; McDonald et al, 2005a;
McDonald et al, 2005b), second-order (McDonald
and Pereira, 2006; Carreras, 2007) and third-order
(Koo and Collins, 2010) models.
Similarly, we can define the order of constituent
parsing in terms of the number of grammar rules
in a part. Then, the previous discriminative con-
stituent parsing models (Johnson, 2001; Henderson,
2004; Taskar et al, 2004; Petrov and Klein, 2008a;
?The research reported in this paper was partially supported
by the Research Grants Council of HKSAR, China, through the
GRF Grant 9041597 (CityU 144410).
Petrov and Klein, 2008b; Finkel et al, 2008) are the
first-order ones, because there is only one grammar
rule in a part. The discriminative re-scoring models
(Collins, 2000; Collins and Duffy, 2002; Charniak
and Johnson, 2005; Huang, 2008) can be viewed as
previous attempts to higher-order constituent pars-
ing, using some parts containing more than one
grammar rule as non-local features.
In this paper, we present a higher-order con-
stituent parsing model1 based on these previous
works. It allows multiple adjacent grammar rules
in each part of a parse tree, so as to utilize more
local structural context to decide the plausibility of
a grammar rule instance. Evaluated on the PTB
WSJ and Chinese Treebank, it achieves its best F1
scores of 91.86% and 85.58%, respectively. Com-
bined with other high-performance parsers under
the framework of constituent recombination (Sagae
and Lavie, 2006; Fossum and Knight, 2009), this
model further enhances the F1 scores to 92.80% and
85.60%, the highest ones achieved so far on these
two data sets.
2 Higher-order Constituent Parsing
Discriminative parsing is aimed to learn a function
f : S ? T from a set of sentences S to a set of valid
parses T according to a given CFG, which maps an
input sentence s ? S to a set of candidate parses
T (s). The function takes the following discrimina-
tive form:
f(s) = arg max
t?T (s)
g(t, s) (1)
1http://code.google.com/p/gazaparser/
1
thea portion of
DT
will32$ million realized from the sales be ...
VPNP NPQP PPVBN
IN PP
begin(b) split(m) end(e)...
Figure 1: A part of a parse tree centered at NP? NP VP
where g(t, s) is a scoring function to evaluate the
event that t is the parse of s. Following Collins
(2002), this scoring function is formulated in the lin-
ear form
g(t, s) = ? ??(t, s), (2)
where ?(t, s) is a vector of features and ? the vector
of their associated weights. To ensure tractability,
this model is factorized as
g(t, s) =
?
r?t
g(Q(r), s) =
?
r?t
? ? ?(Q(r), s), (3)
where g(Q(r), s) scores Q(r), a part centered at
grammar rule instance r in t, and ?(Q(r), s) is the
vector of features for Q(r). Each Q(r) makes its
own contribution to g(t, s). A part in a parse tree
is illustrated in Figure 1. It consists of the center
grammar rule instance NP? NP VP and a set of im-
mediate neighbors, i.e., its parent PP ? IN NP, its
children NP ? DT QP and VP ? VBN PP, and its
sibling IN ? of. This set of neighboring rule in-
stances forms a local structural context to provide
useful information to determine the plausibility of
the center rule instance.
2.1 Feature
The feature vector ?(Q(r), s) consists of a series
of features {?i(Q(r), s))|i ? 0}. The first feature
?0(Q(r), s) is calculated with a PCFG-based gen-
erative parsing model (Petrov and Klein, 2007), as
defined in (4) below, where r is the grammar rule in-
stance A ? B C that covers the span from the b-th
to the e-th word, splitting at the m-th word, x, y and
z are latent variables in the PCFG-based model, and
I(?) and O(?) are the inside and outside probabili-
ties, respectively.
All other features ?i(Q(r), s) are binary func-
tions that indicate whether a configuration exists in
Q(r) and s. These features are by their own na-
ture in two categories, namely, lexical and structural.
All features extracted from the part in Figure 1 are
demonstrated in Table 1. Some back-off structural
features are used for smoothing, which cannot be
presented due to limited space. With only lexical
features in a part, this parsing model backs off to a
first-order one similar to those in the previous works.
Adding structural features, each involving a least a
neighboring rule instance, makes it a higher-order
parsing model.
2.2 Decoding
The factorization of the parsing model allows us to
develop an exact decoding algorithm for it. Follow-
ing Huang (2008), this algorithm traverses a parse
forest in a bottom-up manner. However, it deter-
mines and keeps the best derivation for every gram-
mar rule instance instead of for each node. Be-
cause all structures above the current rule instance
is not determined yet, the computation of its non-
local structural features, e.g., parent and sibling fea-
tures, has to be delayed until it joins an upper level
structure. For example, when computing the score
of a derivation under the center rule NP ? NP VP
in Figure 1, the algorithm will extract child features
from its children NP ? DT QP and VP ? VBN PP.
The parent and sibling features of the two child rules
can also be extracted from the current derivation and
used to calculate the score of this derivation. But
parent and sibling features for the center rule will
not be computed until the decoding process reaches
the rule above, i.e., PP? IN NP.
This algorithm is more complex than the approx-
imate decoding algorithm of Huang (2008). How-
ever, its efficiency heavily depends on the size of the
parse forest it has to handle. Forest pruning (Char-
?0(Q(r), s) =
?
x
?
y
?
z
O(Ax, b, e)P(Ax ? By Cz)I(By, b,m)I(Cz,m, e)
I(S, 0, n)
(4)
2
Template Description Comments
Lexical
feature
N-gram on inner
/outer edge
wb/e+l(l=0,1,2,3,4) & b/e & l & NP
Similar to the distributional
similarity cluster bigrams
features in Finkel et al (2008)
wb/e?l(l=1,2,3,4,5) & b/e & l & NP
wb/e+lwb/e+l+1(l=0,1,2,3) & b/e & l & NP
wb/e?l?1wb/e?l(l=1,2,3,4) & b/e & l & NP
wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) & b/e & l & NP
wb/e?l?2wb/e?l?1wb/e?l(l=1,2,3) & b/e & l & NP
Bigram on edges wb/e?1wb/e & NP Similar to the lexical span
features in Taskar et al (2004)
and Petrov and Klein (2008b)
Split pair wm?1wm & NP? NP VP
Inner/Outer pair
wbwe?1 & NP? NP VP
wb?1we & NP? NP VP
Rule bigram
Left & NP & NP Similar to the bigrams features
in Collins (2000)Right & NP & NP
Structural
feature
Parent PP? IN NP & NP? NP VP
Similar to the grandparent
rules features in Collins (2000)
Child
NP? DT QP & VP? VBN PP & NP? NP VP
NP? DT QP & NP? NP VP
VP? VBN PP & NP? NP VP
Sibling Left & IN? of & NP? NP VP
Table 1: Examples of lexical and structural feature
niak and Johnson, 2005; Petrov and Klein, 2007)
is therefore adopted in our implementation for ef-
ficiency enhancement. A parallel decoding strategy
is also developed to further improve the efficiency
without loss of optimality. Interested readers can re-
fer to Chen (2012) for more technical details of this
algorithm.
3 Constituent Recombination
Following Fossum and Knight (2009), our con-
stituent weighting scheme for parser combination
uses multiple outputs of independent parsers. Sup-
pose each parser generates a k-best parse list for an
input sentence, the weight of a candidate constituent
c is defined as
?(c) =
?
i
?
k
?i?(c, ti,k)f(ti,k), (5)
where i is the index of an individual parser, ?i
the weight indicating the confidence of a parser,
?(c, ti,k) a binary function indicating whether c is
contained in ti,k, the k-th parse output from the i-
th parser, and f(ti,k) the score of the k-th parse as-
signed by the i-th parser, as defined in Fossum and
Knight (2009).
The weight of a recombined parse is defined as the
sum of weights of all constituents in the parse. How-
ever, this definition has a systematic bias towards se-
lecting a parse with as many constituents as possible
English Chinese
Train. Section 2-21 Art. 1-270,400-1151
Dev. Section 22/24 Art. 301-325
Test. Section 23 Art. 271-300
Table 2: Experiment Setup
for the highest weight. A pruning threshold ?, simi-
lar to the one in Sagae and Lavie (2006), is therefore
needed to restrain the number of constituents in a re-
combined parse. The parameters ?i and ? are tuned
by the Powell?s method (Powell, 1964) on a develop-
ment set, using the F1 score of PARSEVAL (Black
et al, 1991) as objective.
4 Experiment
Our parsing models are evaluated on both English
and Chinese treebanks, i.e., the WSJ section of Penn
Treebank 3.0 (LDC99T42) and the Chinese Tree-
bank 5.1 (LDC2005T01U01). In order to compare
with previous works, we opt for the same split as
in Petrov and Klein (2007), as listed in Table 2. For
parser combination, we follow the setting of Fossum
and Knight (2009), using Section 24 instead of Sec-
tion 22 of WSJ treebank as development set.
In this work, the lexical model of Chen and Kit
(2011) is combined with our syntactic model under
the framework of product-of-experts (Hinton, 2002).
A factor ? is introduced to balance the two models.
It is tuned on a development set using the gold sec-
3
English Chinese
R(%) P(%) F1(%) R(%) P(%) F1(%)
Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22
First-order 91.33 91.79 91.56 84.14 86.23 85.17
Higher-order 91.62 92.11 91.86 84.24 86.54 85.37
Higher-order+? 91.60 92.13 91.86 84.45 86.74 85.58
Stanford parser - - - 77.40 79.57 78.47
C&J parser 91.04 91.76 91.40 - - -
Conbination 92.02 93.60 92.80 82.44 89.01 85.60
Table 3: The performance of our parsing models on the English and Chinese test sets.
System F1(%) EX(%)
Single
Charniak (2000) 89.70
Berkeley parser 89.87 36.7
Bod (2003) 90.70
Carreras et al (2008) 91.1
Re-scoring
Collins (2000) 89.70
Charniak and Johnson (2005) 91.02
The parser of Charniak and Johnson 91.40 43.54
Huang (2008) 91.69 43.5
Combination
Fossum and Knight (2009) 92.4
Zhang et al (2009) 92.3
Petrov (2010) 91.85 41.9
Self-training
Zhang et al (2009) (s.t.+combo) 92.62
Huang et al (2010) (single) 91.59 40.3
Huang et al (2010) (combo) 92.39 43.1
Our single 91.86 40.89
Our combo 92.80 41.60
Table 4: Performance comparison on the English test set
tion search algorithm (Kiefer, 1953). The parame-
ters ? of each parsing model are estimated from a
training set using an averaged perceptron algorithm,
following Collins (2002) and Huang (2008).
The performance of our first- and higher-order
parsing models on all sentences of the two test sets
is presented in Table 3, where ? indicates a tuned
balance factor. This parser is also combined with
the parser of Charniak and Johnson (2005)2 and the
Stanford. parser3 The best combination results in
Table 3 are achieved with k=70 for English and
k=100 for Chinese for selecting the k-best parses.
Our results are compared with the best previous ones
on the same test sets in Tables 4 and 5. All scores
2ftp://ftp.cs.brown.edu/pub/nlparser/
3http://nlp.stanford.edu/software/lex-parser.shtml
System F1(%) EX(%)
Single
Charniak (2000) 80.85
Stanford parser 78.47 26.44
Berkeley parser 83.22 31.32
Burkett and Klein (2008) 84.24
Combination
Zhang et al (2009) (combo) 85.45
Our single 85.56 31.61
Our combo 85.60 29.02
Table 5: Performance comparison on the Chinese test set
listed in these tables are calculated with evalb,4
and EX is the complete match rate.
5 Conclusion
This paper has presented a higher-order model for
constituent parsing that factorizes a parse tree into
larger parts than before, in hopes of increasing its
power of discriminating the true parse from the oth-
ers without losing tractability. A performance gain
of 0.3%-0.4% demonstrates its advantage over its
first-order version. Including a PCFG-based model
as its basic feature, this model achieves a better
performance than previous single and re-scoring
parsers, and its combination with other parsers per-
forms even better (by about 1%). More importantly,
it extends the existing works into a more general
framework of constituent parsing to utilize more
lexical and structural context and incorporate more
strength of various parsing techniques. However,
higher-order constituent parsing inevitably leads to
a high computational complexity. We intend to deal
with the efficiency problem of our model with some
advanced parallel computing technologies in our fu-
ture works.
4http://nlp.cs.nyu.edu/evalb/
4
References
E. Black, S. Abney, D. Flickenger, R. Grishman, P. Har-
rison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans,
M. Liberman, M. Marcus, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of DARPA Speech and Nat-
ural Language Workshop, pages 306?311.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In EACL 2003, pages 19?26.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In EMNLP
2008, pages 877?886.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In CoNLL 2008, pages
9?16.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP-CoNLL
2007, pages 957?961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In ACL 2005, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In NAACL 2000, pages 132?139.
Xiao Chen and Chunyu Kit. 2011. Improving part-of-
speech tagging for context-free parsing. In IJCNLP
2011, pages 1260?1268.
Xiao Chen. 2012. Discriminative Constituent Parsing
with Localized Features. Ph.D. thesis, City University
of Hong Kong.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In ACL
2002, pages 263?270.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In ICML 2000, pages 175?182.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In EMNLP 2002, pages
1?8.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996, pages 340?345.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In ACL-HLT 2008, pages 959?
967.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In NAACL-HLT 2009, pages 253?
256.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In ACL 2004, pages
95?102.
Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771?1800.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable gram-
mars. In EMNLP 2010, pages 12?22.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL-HLT 2008,
pages 586?594.
Mark Johnson. 2001. Joint and conditional estimation
of tagging and parsing models. In ACL 2001, pages
322?329.
J. Kiefer. 1953. Sequential minimax search for a maxi-
mum. Proceedings of the American Mathematical So-
ciety, 4:502?506.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL 2010, pages 1?11.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In EACL 2006, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In ACL 2005, pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In EMNLP-HLT
2005, pages 523?530.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT 2007, pages
404?411.
Slav Petrov and Dan Klein. 2008a. Discriminative log-
linear grammars with latent variables. In NIPS 20,
pages 1?8.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing. In
EMNLP 2008, pages 867?876.
Slav Petrov. 2010. Products of random latent variable
grammars. In NAACL-HLT 2010, pages 19?27.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Computer Journal, 7(2):155?
162.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In NAACL-HLT 2006, pages 129?132.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
EMNLP 2004, pages 1?8.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In EMNLP 2009, pages 1552?1560.
5
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 622?630,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Non-Monotonic Sentence Alignment via Semisupervised Learning
Xiaojun Quan, Chunyu Kit and Yan Song
Department of Chinese, Translation and Linguistics
City University of Hong Kong, HKSAR, China
{xiaoquan,ctckit,[yansong]}@[student.]cityu.edu.hk
Abstract
This paper studies the problem of non-
monotonic sentence alignment, motivated
by the observation that coupled sentences
in real bitexts do not necessarily occur
monotonically, and proposes a semisuper-
vised learning approach based on two as-
sumptions: (1) sentences with high affinity
in one language tend to have their counter-
parts with similar relatedness in the other;
and (2) initial alignment is readily avail-
able with existing alignment techniques.
They are incorporated as two constraints
into a semisupervised learning framework
for optimization to produce a globally op-
timal solution. The evaluation with real-
world legal data from a comprehensive
legislation corpus shows that while exist-
ing alignment algorithms suffer severely
from non-monotonicity, this approach can
work effectively on both monotonic and
non-monotonic data.
1 Introduction
Bilingual sentence alignment is a fundamental
task to undertake for the purpose of facilitating
many important natural language processing ap-
plications such as statistical machine translation
(Brown et al, 1993), bilingual lexicography (Kla-
vans et al, 1990), and cross-language informa-
tion retrieval (Nie et al, 1999). Its objective is to
identify correspondences between bilingual sen-
tences in given bitexts. As summarized by Wu
(2010), existing sentence alignment techniques
rely mainly on sentence length and bilingual lex-
ical resource. Approaches based on the former
perform effectively on cognate languages but not
on the others. For instance, the statistical cor-
relation of sentence length between English and
Chinese is not as high as that between two Indo-
European languages (Wu, 1994). Lexicon-based
approaches resort to word correspondences in a
bilingual lexicon to match bilingual sentences. A
few sentence alignment methods and tools have
also been explored to combine the two. Moore
(2002) proposes a multi-pass search procedure us-
ing both sentence length and an automatically-
derived bilingual lexicon. Hunalign (Varga et al,
2005) is another sentence aligner that combines
sentence length and a lexicon. Without a lexicon,
it backs off to a length-based algorithm and then
automatically derives a lexicon from the align-
ment result. Soon after, Ma (2006) develops the
lexicon-based aligner Champollion, assuming that
different words have different importance in align-
ing two sentences.
Nevertheless, most existing approaches to sen-
tence alignment follow the monotonicity assump-
tion that coupled sentences in bitexts appear in
a similar sequential order in two languages and
crossings are not entertained in general (Langlais
et al, 1998; Wu, 2010). Consequently the task of
sentence alignment becomes handily solvable by
means of such basic techniques as dynamic pro-
gramming. In many scenarios, however, this pre-
requisite monotonicity cannot be guaranteed. For
example, bilingual clauses in legal bitexts are of-
ten coordinated in a way not to keep the same
clause order, demanding fully or partially crossing
pairings. Figure 1 shows a real excerpt from a leg-
islation corpus. Such monotonicity seriously im-
pairs the existing alignment approaches founded
on the monotonicity assumption.
This paper is intended to explore the problem of
non-monotonic alignment within the framework
of semisupervised learning. Our approach is mo-
tivated by the above observation and based on
the following two assumptions. First, monolin-
gual sentences with high affinity are likely to have
their translations with similar relatedness. Follow-
ing this assumption, we propose the conception
of monolingual consistency which, to the best of
622
British Overseas citizen" (??????) means a person who has the
status of a British Overseas citizen under the British Nationality Act
1981 (1981 c. 61 U.K.)
British protected person" (???????) means a person who has
the status of a British protected person under the British Nationality Act
1981 (1981 c. 61 U.K.)
...
1. Interpretation of words and expressions
British citizen" (???? ) means a person who has the status of a
British citizen under the British Nationality Act 1981 (1981 c. 61 U.K.)
British Dependent Territories citizen" (??????) means a person
who has or had the status of a British Dependent Territories citizen
under the British Nationality Act 1981 (1981 c. 61 U.K.)
British enactment" and "imperial enactment" (??????) Mean-
(a) any Act of Parliament; (b) any Order in Council; and (c) any rule,
regulation, proclamation, order, notice, rule of court, by-law or other
instrument made under or by virtue of any such Act or Order in Council
???????(British Overseas citizen)????1981?????
???(1981 c. 61 U.K.)????????????
???????(British Dependent Territories citizen)????1981
????????(1981 c. 61 U.K.)?????????????
???
1.????????
???????????????????????????????????????????????
??????????????????????????????????
?????(British citizen)????1981????????(1981
c. 61 U.K.)??????????
???????(British enactment, imperial enactment)??(a)??
????????(b)?????????(c)?????????
???????????????????????????
??????????????
...
"
"
"
"
"
?
?
?
?
?
Figure 1: A real example of non-monotonic sentence alignment from BLIS corpus.
our knowledge, has not been taken into account in
any previous work of alignment. Second, initial
alignment of certain quality can be obtained by
means of existing alignment techniques. Our ap-
proach attempts to incorporate both monolingual
consistency of sentences and bilingual consistency
of initial alignment into a semisupervised learning
framework to produce an optimal solution. Ex-
tensive evaluations are performed using real-world
legislation bitexts from BLIS, a comprehensive
legislation database maintained by the Depart-
ment of Justice, HKSAR. Our experimental results
show that the proposed method can work effec-
tively while two representatives of existing align-
ers suffer severely from the non-monotonicity.
2 Methodology
2.1 The Problem
An alignment algorithm accepts as input a bi-
text consisting of a set of source-language sen-
tences, S = {s1, s2, . . . , sm}, and a set of target-
language sentences, T = {t1, t2, . . . , tn}. Dif-
ferent from previous works relying on the mono-
tonicity assumption, our algorithm is generalized
to allow the pairings of sentences in S and T
to cross arbitrarily. Figure 2(a) illustrates mono-
tonic alignment with no crossing correspondences
in a bipartite graph and 2(b) non-monotonic align-
ment with scrambled pairings. Note that it is rela-
tively straightforward to identify the type of many-
to-many alignment in monotonic alignment using
techniques such as dynamic programming if there
is no scrambled pairing or the scrambled pairings
are local, limited to a short distance. However,
the situation of non-monotonic alignment is much
more complicated. Sentences to be merged into a
bundle for matching against another bundle in the
other language may occur consecutively or discon-
tinuously. For the sake of simplicity, we will not
consider non-monotonic alignment with many-to-
many pairings but rather assume that each sen-
tence may align to only one or zero sentence in
the other language.
Let F represent the correspondence relation be-
tween S and T , and therefore F ? S ? T . Let
matrix F denote a specific alignment solution of
F , where Fij is a real score to measure the likeli-
hood of matching the i-th sentence si in S against
the j-th sentence tj in T . We then define an align-
ment function A : F ? A to produce the final
alignment, where A is the alignment matrix for S
and T , with Aij = 1 for a correspondence be-
tween si and tj and Aij = 0 otherwise.
2.2 Semisupervised Learning
A semisupervised learning framework is intro-
duced to incorporate the monolingual and bilin-
gual consistency into alignment scoring
Q(F ) = Qm(F ) + ?Qb(F ), (1)
where Qm(F ) is the term for monolingual con-
straint to control the consistency of sentences with
high affinities, Qb(F ) for the constraint of initial
alignment obtained with existing techniques, and
? is the weight between them. Then, the optimal
alignment solution is to be derived by minimizing
the cost function Q(F ), i.e.,
F ? = argmin
F
Q(F ). (2)
623
s1
s2
s3
s4
s5
s6
t1
t2
t3
t4
t5
t6
(a)
s1
s2
s3
s4
s5
s6
t1
t2
t3
t4
t5
t6
(b)
Figure 2: Illustration of monotonic (a) and non-monotonic alignment (b), with a line representing the
correspondence of two bilingual sentences.
In this paper, Qm(F ) is defined as
1
4
m?
i,j=1
Wij
n?
k,l=1
Vkl
(
Fik?
DiiEkk
? Fjl?
DjjEll
)2
, (3)
whereW and V are the symmetric matrices to rep-
resent the monolingual sentence affinity matrices
in S and T , respectively, and D and E are the di-
agonal matrices with entries Dii = ?jWij and
Eii =
?
j Vij . The idea behind (3) is that to min-
imize the cost function, the translations of those
monolingual sentences with close relatedness re-
flected inW and V should also keep similar close-
ness. The bilingual constraint term Qb(F ) is de-
fined as
Qb(F ) =
m?
i=1
n?
j=1
(
Fij ? A?ij
)2
, (4)
where A? is the initial alignment matrix obtained
by A : F? ? A?. Note that F? is the initial relation
matrix between S and T .
The monolingual constraint term Qm(F ) de-
fined above corresponds to the smoothness con-
straint in the previous semisupervised learning
work by Zhou et al (2004) that assigns higher
likelihood to objects with larger similarity to share
the same label. On the other hand, Qb(F ) corre-
sponds to their fitting constraint, which requires
the final alignment to maintain the maximum con-
sistency with the initial alignment.
Taking the derivative of Q(F ) with respect to
F , we have
?Q(F )
?F = 2F ? 2SFT + 2?F ? 2?A?, (5)
where S and T are the normalized matrices of W
and V , calculated by S = D?1/2WD?1/2 and
T = E?1/2V E?1/2. Then, the optimal F ? is to
be found by solving the equation
(1 + ?)F ? ? SF ?T = ?A?, (6)
which is equivalent to ?F ? ? F ?? = ? with
? = (1 + ?)S?1, ? = T and ? = ?S?1A?.
This is in fact a Sylvester equation (Barlow et al,
1992), whose numerical solution can be found by
many classical algorithms. In this research, it is
solved using LAPACK,1 a software library for nu-
merical linear algebra. Non-positive entries in F ?
indicate unrealistic correspondences of sentences
and are thus set to zero before applying the align-
ment function.
2.3 Alignment Function
Once the optimal F ? is acquired, the remaining
task is to design an alignment function A to con-
vert it into an alignment solution. An intuitive ap-
proach is to use a heuristic search for local op-
timization (Kit et al, 2004), which produces an
alignment with respect to the largest scores in
each row and each column. However, this does not
guarantee a globally optimal solution. Figure 3 il-
lustrates a mapping relation matrix onto an align-
ment matrix, which also shows that the optimal
alignment cannot be achieved by heuristic search.
Banding is another approach frequently used to
convert a relation matrix to alignment (Kay and
Ro?scheisen, 1993). It is founded on the observa-
tion that true monotonic alignment paths usually
lie close to the diagonal of a relation matrix. How-
ever, it is not applicable to our task due to the non-
monotonicity involved. We opt for converting a
relation matrix into specific alignment by solving
1http://www.netlib.org/lapack/
624
al
ig
nm
en
t
m
at
ri
x
re
la
ti
on
m
at
ri
x
2
1 2 43 5 6 7
1
3
4
5
6
00.4 0 0.5 0 00
0.30 0 0.6 0 00
00 0 0 0 00
0.40 0 0 0.2 00
0.50 0 0 0 00.6
00.1 0 0 0 00.8
2
1 2 43 5 6 7
1
3
4
5
6
01 0 0 0 00
00 0 1 0 00
00 0 0 0 00
00 0 0 1 00
10 0 0 0 00
00 0 0 0 01
Figure 3: Illustration of sentence alignment from relation matrix to alignment matrix. The scores marked
with arrows are the best in each row/column to be used by the heuristic search. The right matrix repre-
sents the corresponding alignment matrix by our algorithm.
the following optimization
A =argmax
X
m?
i=1
n?
j=1
XijFij (7)
s.t.
m?
i=1
Xij ? 1,
n?
j=1
Xij ? 1, Xij ? {0, 1}
This turns sentence alignment into a problem to
be resolved by binary linear programming (BIP),
which has been successfully applied to word align-
ment (Taskar et al, 2005). Given a scoring matrix,
it guarantees an optimal solution.
2.4 Alignment Initialization
Once the above alignment function is available,
the initial alignment matrix A? can be derived from
an initial relation matrix F? obtained by an avail-
able alignment method. This work resorts to an-
other approach to initializing the relation matrix.
In many genres of bitexts, such as government
transcripts or legal documents, there are a certain
number of common strings on the two sides of bi-
texts. In legal documents, for example, transla-
tions of many key terms are usually accompanied
with their source terms. Also, common number-
ings can be found in enumerated lists in bitexts.
These kinds of anchor strings provide quite reli-
able information to link bilingual sentences into
pairs, and thus can serve as useful cues for sen-
tence alignment. In fact, they can be treated as a
special type of highly reliable ?bilexicon?.
The anchor strings used in this work are derived
by searching the bitexts using word-level inverted
indexing, a basic technique widely used in infor-
mation retrieval (Baeza-Yates and Ribeiro-Neto,
2011). For each index term, a list of postings is
created. Each posting includes a sentence identi-
fier, the in-sentence frequency and positions of this
term. The positions of terms are intersected to find
common anchor strings. The anchor strings, once
found, are used to calculate the initial affinity F?ij
of two sentences using Dice?s coefficient
F?ij =
2|C1i ? C2j |
|C1i|+ |C2j |
(8)
where C1i and C2j are the anchor sets in si and tj ,
respectively, and | ? | is the cardinality of a set.
Apart from using anchor strings, other avenues
for the initialization are studied in the evaluation
section below, i.e., using another aligner and an
existing lexicon.
2.5 Monolingual Affinity
Although various kinds of information from a
monolingual corpus have been exploited to boost
statistical machine translation models (Liu et al,
2010; Su et al, 2012), we have not yet been
exposed to any attempt to leverage monolingual
sentence affinity for sentence alignment. In our
framework, an attempt to this can be made through
the computation of W and V . Let us take W as an
example, where the entry Wij represents the affin-
ity of sentence si and sentence sj , and it is set to
0 for i = j in order to avoid self-reinforcement
during optimization (Zhou et al, 2004).
When two sentences in S or T are not too short,
or their content is not divergent in meaning, their
semantic similarity can be estimated in terms of
common words. Motivated by this, we define Wij
(for i 6= j) based on the Gaussian kernel as
Wij = exp
(
? 12?2
(
1? v
T
i vj
?vi? ?vj?
)2)
(9)
625
where ? is the standard deviation parameter, vi
and vj are vectors of si and sj with each com-
ponent corresponding to the tf-idf value of a par-
ticular term in S (or T ), and ??? is the norm of
a vector. The underlying assumption here is that
words appearing frequently in a small number of
sentences but rarely in the others are more signifi-
cant in measuring sentence affinity.
Although semantic similarity estimation is a
straightforward approach to deriving the two affin-
ity matrices, other approaches are also feasible. An
alternative approach can be based on sentence
length under the assumption that two sentences
with close lengths in one language tend to have
their translations also with close lengths.
2.6 Discussion
The proposed semisupervised framework for non-
monotonic alignment is in fact generalized be-
yond, and can also be applied to, monotonic align-
ment. Towards this, we need to make use of sen-
tence sequence information. One way to do it is
to incorporate sentence positions into Equation (1)
by introducing a position constraint Qp(F ) to en-
force that bilingual sentences in closer positions
should have a higher chance to match one another.
For example, the new constraint can be defined as
Qp(F ) =
m?
i=1
n?
j=1
|pi ? qj |F 2ij ,
where pi and qj are the absolute (or relative) posi-
tions of two bilingual sentences in their respective
sequences. Another way follows the banding as-
sumption that the actual couplings only appear in
a narrow band along the main diagonal of relation
matrix. Accordingly, all entries of F ? outside this
band are set to zero before the alignment function
is applied. Kay and Ro?scheisen (1993) illustrate
that this can be done by modeling the maximum
deviation of true couplings from the diagonal as
O(?n).
3 Evaluation
3.1 Data Set
Our data set is acquired from the Bilingual
Laws Information System (BLIS),2 an electronic
database of Hong Kong legislation maintained
by the Department of Justice, HKSAR. BLIS
2http://www.legislation.gov.hk
provides Chinese-English bilingual texts of ordi-
nances and subsidiary legislation in effect on or af-
ter 30 June 1997. It organizes the legal texts into a
hierarchy of chapters, sections, subsections, para-
graphs and subparagraphs, and displays the con-
tent of a such hierarchical construct (usually a sec-
tion) on a single web page.
By web crawling, we have collected in total
31,516 English and 31,405 Chinese web pages,
forming a bilingual corpus of 31,401 bitexts after
filtering out null pages. A text contains several to
two hundred sentences. Many bitexts exhibit par-
tially non-monotonic order of sentences. Among
them, 175 bitexts are randomly selected for man-
ual alignment. Sentences are identified based on
punctuations. OpenNLP Tokenizer3 is applied to
segment English sentences into tokens. For Chi-
nese, since there is no reliable segmenter for this
genre of text, we have to treat each Chinese char-
acter as a single token. In addition, to calculate the
monolingual sentence affinity, stemming of En-
glish words is performed with the Porter Stemmer
(Porter, 1980) after anchor string mining.
The manual alignment of the evaluation data set
is performed upon the initial alignment by Hu-
nalign (Varga et al, 2005), an effective sentence
aligner that uses both sentence length and a bilex-
icon (if available). For this work, Hunalign re-
lies solely on sentence length. Its output is then
double-checked and corrected by two experts in
bilingual studies, resulting in a data set of 1747
1-1 and 70 1-0 or 0-1 sentence pairs.
The standard deviation ? in (9) is an important
parameter for the Gaussian kernel that has to be
determined empirically (Zhu et al, 2003; Zhou et
al., 2004). In addition, theQ function also involves
another parameter ? to adjust the weight of the
bilingual constraint. This work seeks an approach
to deriving the optimal parameters without any ex-
ternal training data beyond the initial alignment. A
three-fold cross-validation is thus performed on
the initial 1-1 alignment and the parameters that
give the best average performance are chosen.
3.2 Monolingual Consistency
To demonstrate the validity of the monolingual
consistency, the semantic similarity defined by
vTi vj
?vi??vj? is evaluated as follows. 500 pairs of En-glish sentences with the highest similarities are se-
lected, excluding null pairings (1-0 or 0-1 type).
3http://opennlp.apache.org/
626
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 10.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Similarity of English sentence pair
Simi
larity
 of C
hine
se s
ente
nce 
pair
Figure 4: Demonstration of monolingual consis-
tency. The horizontal axis is the similarity of En-
glish sentence pairs and the vertical is the similar-
ity of the corresponding pairs in Chinese.
Type Total initAlign NonmoAlignPred Corr Pred Corr
1-0 70 662 66 70 50
1-1 1747 1451 1354 1747 1533
Table 1: Performance of the initial alignment and
our aligner, where the Pred and Corr columns are
the numbers of predicted and correct pairings.
All of these high-affinity pairs have a similarity
score higher than 0.72. A number of duplicate
sentences (e.g., date) with exceptionally high sim-
ilarity 1.0 are dropped. Also, the similarity of the
corresponding translations of each selected pair
is calculated. These two sets of similarity scores
are then plotted in a scatter plot, as in Figure 4.
If the monolingual consistency assumption holds,
the plotted points would appear nearby the diag-
onal. Figure 4 confirms this, indicating that sen-
tence pairs with high affinity in one language do
have their counterparts with similarly high affinity
in the other language.
3.3 Impact of Initial Alignment
The 1-1 initial alignment plays the role of labeled
instances for the semisupervised learning. It is
of critical importance to the learning performance.
As shown in Table 1, our alignment function pre-
dicts 1451 1-1 pairings by virtue of anchor strings,
among which 1354 pairings are correct, yielding
a relatively high precision in the non-monotonic
circumstance. It also predicts null alignment for
many sentences that contain no anchor. This ex-
plains why it outputs 662 1-0 pairings when there
20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100%0
200
400
600
800
1000
1200
1400
1600
Percentage of initial 1?1 alignment
Corr
ectly
 dete
cted
 1?1
 pair
ings
 
 
NonmoAligninitAlign
Figure 5: Performance of non-monotonic align-
ment along the percentage of initial 1-1 alignment.
are only 70 1-0 true ones. Starting from this initial
alignment, our aligner (let us call it NonmoAlign)
discovers 179 more 1-1 pairings.
A question here is concerned with how the scale
of initial alignment affects the final alignment. To
examine this, we randomly select 20%, 40%, 60%
and 80% of the 1451 1-1 detected pairings as the
initial alignments for a series of experiments. The
random selection for each proportion is performed
ten times and their average alignment performance
is taken as the final result and plotted in Figure 5.
An observation from this figure is that the aligner
consistently discovers significantly more 1-1 pair-
ings on top of an initial 1-1 alignment, which has
to be accounted for by the monolingual consis-
tency. Another observation is that the alignment
performance goes up along the increase of the
percentage of initial alignment while performance
gain slows down gradually. When the percentage
is very low, the aligner still works quite effectively.
3.4 Non-Monotonic Alignment
To test our aligner with non-monotonic sequences
of sentences, we have them randomly scrambled
in our experimental data. This undoubtedly in-
creases the difficulty of sentence alignment, espe-
cially for the traditional approaches critically rely-
ing on monotonicity.
The baseline methods used for comparison are
Moore?s aligner (Moore, 2002) and Hunalign
(Varga et al, 2005). Hunalign is configured with
the option [-realign], which triggers a three-step
procedure: after an initial alignment, Hunalign
heuristically enriches its dictionary using word co-
occurrences in identified sentence pairs; then, it
re-runs the alignment process using the updated
627
Type Moore Hunalign NonmoAlignP R F1 P R F1 P R F1
1-1 0.104 0.104 0.104 0.407 0.229 0.293 0.878 0.878 0.878
1-0 0.288 0.243 0.264 0.033 0.671 0.062 0.714 0.714 0.714
Micro 0.110 0.110 0.110 0.184 0.246 0.210 0.871 0.871 0.871
Table 2: Performance comparison with the baseline methods.
dictionary. According to Varga et al(2005), this
setting gives a higher alignment quality than oth-
erwise. In addition, Hunalign can use an external
bilexicon. For a fair comparison, the identified an-
chor set is fed to Hunalign as a special bilexicon.
The performance of alignment is measured by pre-
cision (P), recall (R) and F-measure (F1). Micro-
averaged performance scores of precision, recall
and F-measure are also computed to measure the
overall performance on 1-1 and 1-0 alignment.
The final results are presented in Table 2, show-
ing that both Moore?s aligner and Hunalign under-
perform ours on non-monotonic alignment. The
particularly poor performance of Moore?s aligner
has to be accounted for by its requirement of more
than thousands of sentences in bitext input for re-
liable estimation of its parameters. Unfortunately,
our available data has not reached that scale yet.
3.5 Partially Non-Monotonic Alignment
Full non-monotonic bitexts are rare in practice.
But partial non-monotonic ones are not. Unlike
traditional alignment approaches, ours does not
found its performance on the degree of monotonic-
ity. To test this, we construct five new versions of
the data set for a series of experiments by ran-
domly choosing and scrambling 0%, 10%, 20%,
40%, 60% and 80% sentence parings. In the-
ory, partial non-monotonicity of various degrees
should have no impact on the performance of our
aligner. It is thus not surprised that it achieves
the same result as reported in last subsection.
NonmoAlign initialized with Hunalign (marked
as NonmoAlign Hun) is also tested. The experi-
mental results are presented in Figure 6. It shows
that both Moore?s aligner and Hunalign work rel-
atively well on bitexts with a low degree of non-
monotonicity, but their performance drops dra-
matically when the non-monotonicity is increased.
Despite the improvement at low non-monotonicity
by seeding our aligner with Hunalign, its per-
formance decreases likewise when the degree of
non-monotonicity increases, due to the quality de-
0 % 10% 20% 30% 40% 50% 60% 70% 80%0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Non?monotonic ratio
Micr
o?F
1
 
 
NonmoAlignHunalignMooreNonmoAlign_Hun
Figure 6: Performance of alignment approaches at
different degrees of non-monotonicity.
crease of the initial alignment by Hunalign.
3.6 Monotonic Alignment
The proposed alignment approach is also expected
to work well on monotonic sentence alignment.
An evaluation is conducted for this using a mono-
tonic data set constructed from our data set by
discarding all its 126 crossed pairings. Of the
two strategies discussed above, banding is used
to help our aligner incorporate the sequence in-
formation. The initial relation matrix is built with
the aid of a dictionary automatically derived by
Hunalign. Entries of the matrix are derived by
employing a similar strategy as in Varga et al
(2005). The evaluation results are presented in Ta-
ble 3, which shows that NonmoAlign still achieves
very competitive performance on monotonic sen-
tence alignment.
4 Related Work
The research of sentence alignment originates in
the early 1990s. Gale and Church (1991) and
Brown (1991) report the early works using length
statistics of bilingual sentences. The general idea
is that the closer two sentences are in length, the
more likely they are to align. A notable difference
of their methods is that the former uses sentence
628
Type Moore Hunalign NonmoAlignP R F1 P R F1 P R F1
1-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.987
1-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729
Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976
Table 3: Performance of monotonic alignment in comparison with the baseline methods.
length in number of characters while the latter in
number of tokens. Both use dynamic program-
ming to search for the best alignment. As shown in
Chen (1993) and Wu (1994), however, sentence-
length based methods suffer when the texts to be
aligned contain small passages, or the languages
involved share few cognates. The subsequent stage
of sentence alignment research is accompanied by
the advent of a handful of well-designed alignment
tools. Moore (2002) proposes a three-pass proce-
dure to find final alignment. Its bitext input is ini-
tially aligned based on sentence length. This step
generates a set of strictly-selected sentence pairs
for use to train an IBM translation model 1 (Brown
et al, 1993). Its final step realigns the bitext using
both sentence length and the discovered word cor-
respondences. Hunalign (Varga et al, 2005), orig-
inally proposed as an ingredient for building paral-
lel corpora, has demonstrated an outstanding per-
formance on sentence alignment. Like many other
aligners, it employs a similar strategy of combin-
ing sentence length and lexical data. In the ab-
sence of a lexicon, it first performs an initial align-
ment wholly relying on sentence length and then
automatically builds a lexicon based on this align-
ment. Using an available lexicon, it produces a
rough translation of the source text by converting
each token to the one of its possible counterparts
that has the highest frequency in the target corpus.
Then, the relation matrix of a bitext is built of sim-
ilarity scores for the rough translation and the ac-
tual translation at sentence level. The similarity of
two sentences is calculated in terms of their com-
mon pairs and length ratio.
To deal with noisy input, Ma (2006) proposes
a lexicon-based sentence aligner - Champollion.
Its distinctive feature is that it assigns different
weights to words in terms of their tf-idf scores,
assuming that words with low sentence frequen-
cies in a text but high occurrences in some local
sentences are more indicative of alignment. Un-
der this assumption, the similarity of any two sen-
tences is calculated accordingly and then a dy-
namic programming algorithm is applied to pro-
duce final alignment. Following this work, Li et
al. (2010) propose a revised version of Champol-
lion, attempting to improve its speed without per-
formance loss. For this purpose, the input bitexts
are first divided into smaller aligned fragments be-
fore applying Champollion to derive finer-grained
sentence pairs. In another related work by Deng et
al. (2007), a generative model is proposed, accom-
panied by two specific alignment strategies, i.e.,
dynamic programming and divisive clustering. Al-
though a non-monotonic search process that toler-
ates two successive chunks in reverse order is in-
volved, their work is essentially targeted at mono-
tonic alignment.
5 Conclusion
In this paper we have proposed and tested
a semisupervised learning approach to non-
monotonic sentence alignment by incorporating
both monolingual and bilingual consistency. The
utility of monolingual consistency in maintain-
ing the consonance of high-affinity monolingual
sentences with their translations has been demon-
strated. This work also exhibits that bilingual con-
sistency of initial alignment of certain quality is
useful to boost alignment performance. Our eval-
uation using real-world data from a legislation
corpus shows that the proposed approach outper-
forms the baseline methods significantly when the
bitext input is composed of non-monotonic sen-
tences. Working on partially non-monotonic data,
this approach also demonstrates a superior per-
formance. Although initially proposed for non-
monotonic alignment, it works well on monotonic
alignment by incorporating the constraint of sen-
tence sequence.
Acknowledgments
The research described in this paper was substan-
tially supported by the Research Grants Council
(RGC) of Hong Kong SAR, China, through the
GRF grant 9041597 (CityU 144410).
629
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 2011.
Modern Information Retrieval: The Concepts
and Technology Behind Search, 2nd ed., Harlow:
Addison-Wesley.
Jewel B. Barlow, Moghen M. Monahemi, and Dianne P.
O?Leary. 1992. Constrained matrix Sylvester equa-
tions. In SIAM Journal on Matrix Analysis and Ap-
plications, 13(1):1-9.
Peter F. Brown, Jennifer C. Lai, Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of ACL?91, pages 169-176.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263-
311.
Stanley F. Chen. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings of
ACL?93, pages 9-16.
Yonggang Deng, Shankar Kumar, and William Byrne.
2007. Segmentation and alignment of parallel text
for statistical machine translation. Natural Lan-
guage Engineering, 13(3): 235-260.
William A. Gale, Kenneth Ward Church. 1991. A Pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of ACL?91, pages 177-184.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121-142.
Chunyu Kit, Jonathan J. Webster, King Kui Sin, Haihua
Pan, and Heng Li. 2004. Clause alignment for bilin-
gual HK legal texts: A lexical-based approach. In-
ternational Journal of Corpus Linguistics, 9(1):29-
51.
Chunyu Kit, Xiaoyue Liu, King Kui Sin, and Jonathan
J. Webster. 2005. Harvesting the bitexts of the laws
of Hong Kong from the Web. In The 5th Workshop
on Asian Language Resources, pages 71-78.
Judith L. Klavans and Evelyne Tzoukermann. 1990.
The bicord system: Combining lexical information
from bilingual corpora and machine readable dictio-
naries. In Proceedings of COLING?90, pages 174-
179.
Philippe Langlais, Michel Simard, and Jean Ve?ronis.
1998. Methods and practical issues in evaluating
alignment techniques. In Proceedings of COLING-
ACL?98, pages 711-717.
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.
2010. Improving statistical machine translation with
monolingual collocation. In Proceedings of ACL
2010, pages 825-833.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In LREC 2006, pages 489-492.
Peng Li, Maosong Sun, Ping Xue. 2010. Fast-
Champollion: a fast and robust sentence alignment
algorithm. In Proceedings of ACL 2010: Posters,
pages 710-718.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA 2002, page 135-144.
Jian-Yun Nie, Michel Simard, Pierre Isabelle and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. In Proceedings
of SIGIR?99, pages 74-81.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3): 130-137.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xi-
aodong Shi, Huailin Dong, Qun Liu. 2012. Transla-
tion model adaptation for statistical machine trans-
lation with monolingual topic information. In Pro-
ceedings of ACL 2012, Vol. 1, pages 459-468.
Ben Taskar, Simon Lacoste-Julien and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73-80.
Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, Viktor
Nagy, La?szlo? Ne?meth, Viktor Tro?n. 2005. Parallel
corpora for medium density languages. In Proceed-
ings of RANLP 2005, pages 590-596.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In Proceed-
ings of ACL?94, pages 80-87.
Dekai Wu. 2010. Alignment. Handbook of Natural
Language Processing, 2nd ed., CRC Press.
Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-
son Weston, Bernhard Schlkopf. 2004. Learning
with local and global consistency. Advances in Neu-
ral Information Processing Systems, 16:321-328.
Xiaojin Zhu, Zoubin Ghahramani and John Lafferty.
2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML 2003, pages 912-919.
630
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 138?143,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Finding More Bilingual Webpages with High Credibility via
Link Analysis
Chengzhi Zhang?
Nanjing University of Science and Technology
Nanjing, China
Xuchen Yao?
Johns Hopkins University
Baltimore, MD, USA
Chunyu Kit
City University of Hong Kong, Hong Kong SAR, China
Abstract
This paper presents an efficient approach
to finding more bilingual webpage pairs
with high credibility via link analysis, us-
ing little prior knowledge or heuristics.
It extends from a previous algorithm that
takes the number of bilingual URL pairs
that a key (i.e., a URL pairing pattern) can
match as the objective function to search
for the best set of keys yielding the greatest
number of webpage pairs within targeted
bilingual websites. Enhanced algorithms
are proposed to match more bilingual web-
pages following the credibility based on
statistical analysis of the link relationship
of the seed websites available. With about
12,800 seed websites as test set, the en-
hanced algorithms improve precision over
baseline by more than 5%, from 94.06%
to 99.40%, and hence find above 20%
more true bilingual URL pairs, illustrating
that significantly more bilingual webpages
with high credibility can be mined with the
help of the link analysis.
1 Introduction
Parallel corpora of bilingual text (bitext) are indis-
pensable language resources for many data-driven
tasks of natural language processing, such as sta-
tistical machine translation (Brown et al, 1990),
cross-language information retrieval (Davis and
Dunning, 1995; Oard, 1997), and bilingual lexi-
cal acquisition (Gale and Church, 1991; Melamed,
1997; Jiang et al, 2009), to name but a few. A
general way to develop such corpora from web
texts starts from exploring the structure of known
bilingual websites, which are usually organized
?Performed while a research associate at City University
of Hong Kong.
?Performed while a visiting student at City University of
Hong Kong.
by their web masters in a way to facilitate both
navigation and maintenance (Nie, 2010). The
most common strategy is to create a parallel struc-
ture in terms of URL hierarchies, exploiting some
known naming conventions for webpages of corre-
sponding languages (Huang and Tilley, 2001; Nie,
2010). Following available structures and nam-
ing conventions, researchers have been exploring
various means to mine parallel corpora from the
web and a good number of such systems have
demonstrated the feasibility and practicality in au-
tomatic acquisition of parallel corpora from bilin-
gual and/or multilingual web sites, e.g., STRAND
(Resnik, 1998; Resnik, 1999; Resnik and Smith,
2003), BITS (Ma and Liberman, 1999), PTMiner
(Chen and Nie, 2000), PTI (Chen et al, 2004),
WPDE (Zhang et al, 2006), the DOM tree align-
ment model (Shi et al, 2006), PagePairGetter (YE
et al, 2008) and Bitextor (Espla`-Gomis and For-
cada, 2010).
Most of these systems are run in three steps:
first, bilingual websites are identified and crawled;
second, pairs of parallel webpages are extracted;
and finally, the extracted pairs are validated (Kit
and Ng, 2007). Among them, prior knowledge
about parallel webpages, mostly in the form of ad
hoc heuristics for identifying webpage languages
or pre-defined patterns for matching or comput-
ing similarity between webpages, is commonly
used for webpage pair extraction (Chen and Nie,
2000; Resnik and Smith, 2003; Zhang et al, 2006;
Shi et al, 2006; Yulia and Shuly, 2010; Toma?s
et al, 2008). Specifically, these systems exploit
search engines and heuristics across webpage an-
chors to locate candidate bilingual websites and
then identify webpage pairs based on pre-defined
URL matching patterns. However, ad hoc heuris-
tics cannot exhaust all possible patterns. Many
webpages do not even have any language label
in their anchors, not to mention many untrust-
worthy labels. Also, using a limited set of pre-
138
defined URL patterns inevitably means to give up
all reachable bilingual webpages that fall outside
their coverage.
Addressing such weaknesses of the previous ap-
proaches, we instead present an efficient bilingual
web mining system based on analyzing link rela-
tionship of websites without resorting to prior ad
hoc knowledge. This approach extends, on top of
re-engineering, the previous work of Kit and Ng
(2007). It aims at (1) further advancing the idea
of finding bilingual webpages via automatic dis-
covery of non-ad-hoc bilingual URL pairing pat-
terns, (2) applying the found pairing patterns to
dig out more bilingual webpage pairs, especially
those involving a deep webpage unaccessible by
web crawling, (3) discovering more bilingual web-
sites (and then more bilingual webpages) with
high credibility via statistical analysis of bilingual
URL patterns and link relationship of available
seed websites. The results from our experiments
on 12, 800 seed websites show that the proposed
algorithms can find considerably more bilingual
webpage pairs on top of the baseline, achieving
a significant improvement of pairing precision by
more than 5%.
2 Algorithm
This section first introduces the idea of unsuper-
vised detection of bilingual URL pairing patterns
(?2.1) and then continues to formulate the use of
the detected patterns to explore more websites, in-
cluding deep webpages (?2.2), and those not in-
cluded in our initial website list (?2.3).
2.1 Bilingual URL Pattern Detection
Our current research is conducted on top of the
re-implementation of the intelligent web agent to
automatically identify bilingual URL pairing pat-
terns as described in Kit and Ng (2007). The un-
derlying assumption for this approach is that rather
than random matching, parallel webpages have
static pairing patterns assigned by web masters for
engineering purpose and these patterns are put in
use to match as many pairs of URLs as possible
within the same domain. Given a URL u from the
set U of URLs of the same domain, the web agent
goes through the set U?{u} of all other URLs and
finds among them all those that differ from u by a
single token1 ? a token is naturally separated by
1If language identification has been done on webpages, it
only needs to go through all URLs of the other language.
a special set of characters including slash /, dot .,
hyphen -, and underscore in a URL. Then, the
single-token difference of a candidate URL pairs
is taken as a candidate of URL paring pattern,
and all candidate patterns are put in competition
against each other in a way to allow a stronger one
(that matches more candidate URL pairs) to win
over a weaker one (that matches fewer). For in-
stance, the candidate pattern <en,zh> can be de-
tected from the following candidate URL pair:
www.legco.gov.hk/yr99-00/en/fc/esc/e0.htm
www.legco.gov.hk/yr99-00/zh/fc/esc/e0.htm
The re-implementation has achieved a num-
ber of improvements on the original algorithm
through re-engineering, including the following
major ones.
1. It is enhanced from token-based to character-
based URL matching. Thus, more gen-
eral patterns, such as <e,c>, can be aggre-
gated from a number of weaker ones like
<1e,1c>, <2e,2c>, ..., etc., many of which
may otherwise fail to survive the competition.
2. The original algorithm is speeded up from
O(|U |2) to O(|U |) time, by building in-
verted indices for URLs and establishing
constant lookup time for shortest matching
URL strings.2
3. The language detection component has been
expanded from bilingual to multi-lingual and
hence had the capacity to practically handle
multilingual websites such as those from EU
and UN.
When detected URL patterns are used to match
URLs in a web domain for identifying bilingual
webpages, noisy patterns (most of which are pre-
sumably weak keys) would better be filtered out.
A straightforward strategy to do this is by thresh-
olding the credibility of a pattern, which can be
defined as
C(p, w) = N(p, w)
|w|
.
where N(p, w) is the number of webpages
matched into pairs by pattern p within website w,
and |w| the size ofw in number of webpages. Note
that this is the local credibility of a key with re-
spect to a certain website w. Empirically, Kit and
2Achieved by utilizing SecondString http://second
string.sf.net/
139
Ng (2007) set a threshold of 0.1 to rule out weak
noisy keys.
Some patterns happen to generalize across do-
mains. The global credibility of such a pattern p is
thus computed by summing over all websites in-
volved, in a way that each webpage matched by p
is counted in respect to the local credibility of p in
the respective website:
C(p) =
?
w
C(p, w)N(p, w).
Interestingly, it is observed that many weak keys
ruled out by the threshold 0.1 are in fact good pat-
terns with a nice global credibility value. In prac-
tice, it is important to ?rescue? a local weak key
with strong global credibility. A common practice
is to do it straightforwardly with a global credibil-
ity threshold, e.g., C(p)> 500 as for the current
work.
Finally, the bilingual credibility of a website is
defined as
C(w) = max
p
C(p, w).
It will be used to measure the bilingual degree of a
website in a later phase of our work, for which an
assumption is that bilingual websites tend to link
with other bilingual websites.
2.2 Deep Webpage Recovery
Some websites contain webpages that cannot be
crawled by search engines. These webpages do
not ?exist? until they are created dynamically as
the result of a specific search, mostly triggered by
JavaScript or Flash actions. This kind of webpages
as a whole is called deep web. Specifically, we
are interested in the case where webpages in one
language are visible but their counterparts in the
other language are hidden. A very chance that we
may have to unearth these deep hidden webpages
is that their URLs follow some common naming
conventions for convenience of pairing with their
visible counterparts.
Thus for each of those URLs still missing a
paired URL after the URL matching using our
bilingual URL pattern collection, a candidate URL
will be automatically generated with each applica-
ble pattern in the collection for a trial to access its
possibly hidden counterpart. If found, then mark
them as a candidate pair. For example, the pattern
<english,tc chi> is found applicable to the
first URL in Table 1 and accordingly generates the
second as a candidate link to its English counter-
part, which turns out to be a valid page.
2.3 Incremental Bilingual Website
Exploration
Starting with a seed bilingual website list of size
N , bilingual URL pairing patterns are first mined,
and then used to reach out for other bilingual web-
sites. The assumption for this phase of work is
that bilingual websites are more likely to be ref-
erenced by other bilingual websites. Accordingly,
a weighted version of PageRank is formulated for
prediction.
Firstly, outgoing links and PageRank are used
as baselines. Linkout(w) is the total number of
outgoing links from website w, and the PageRank
of w is defined as (Brin and Page, 1998):
PageRank(w) = r
N
+(1?r)
?
w?M(w)
PageRank(w)
Linkout(w)
,
whereM(w) is the set of websites that link tow in
the seed set of N bilingual websites, and r? [0, 1]
a damping factor empirically set to 0.15. Initially,
the PageRank value of w is 1. In order to re-
duce time and space cost, both Linkout(w) and
PageRank(w) are computed only in terms of the
relationship of bilingual websites in the seed set.
The WeightedPageRank(w) is defined as the
PageRank(w) weighted by w?s credibility C(w).
To reach out for a related website s outside the
initial seed set of websites, our approach first
finds the set R(s) of seed websites that have
outgoing links to s, and then computes the sum
of these three values over each outgoing link,
namely,
?
wLinkout(w),
?
wPageRank(w), and
?
wWeightedPageRank(w) for each w?R(s), for
the purpose of measuring how ?likely? s is bilin-
gual. An illustration of link relationship of this
kind is presented in Figure 1.
In practice, the exploration of related websites
can be combined with bilingual URL pattern de-
tection to literately harvest both bilingual websites
and URL patterns, e.g., through the following pro-
cedure:
1. Starting from a seed set of websites as the
current set, detect bilingual URL patterns and
then use them to identify their bilingual web-
pages.
2. Select the top K linked websites from
the seed set according to either
?
Linkout,
?
PageRank, or
?
WeightedPageRank.
140
(1) http://www.fehd.gov.hk/tc chi/LLB web/cagenda 20070904.htm
(2) http://www.fehd.gov.hk/english/LLB web/cagenda 20070904.htm
Table 1: Illustration of URL generation for a deep webpage
 
 
Rel
ate
d w
ebs
ites
s 1
See
d w
ebs
ites
 
[1, 
0.1
2, 0
.08
] 
 [1
, 0.
21,
 0.1
3] 
[2, 
0.5
6, 0
.29
] 
[1, 
0.0
2, 0
.01
] 
[1, 
0.0
3, 0
.02
] 
 [0,
 0, 
0] 
 [1
, 0.
03,
 0.0
1] 
[1, 
0.1
2, 0
.08
] 
 [1
, 0.
21,
 0.1
3] 
 [3
, 0.
77,
 0.4
2] 
 [3
, 0.
59,
 0.1
3] 
[0, 
0, 0
] 
s 2 s 3 s 4 s 5
w 2
w 6
w 3
w 4
w 5 
w 7
w 1
Figure 1: Illustration of link relationship of seed websites and related websites, with associated
?
Linkout,
?
PageRank and
?
WeightedPageRank in square brackets and with arrows to indicate outgo-
ing links from a seed website to others.
3. Add the top K selected websites to the cur-
rent set, and repeat the above steps for desired
iterations.
3 Evaluation
The implementation of our method results in Pup-
Sniffer,3 a Java-based tool that has been released
for free. A series of experiments were conducted
with it to investigate the performance of the pro-
posed method on about 12, 800 seed websites. A
web interface was also implemented for evaluat-
ing the candidate bilingual webpage pairs identi-
fied by our system.
3.1 Seed Websites
The initial seed websites were collected from two
resources, namely
? Hong Kong Website Directory4 and
? Hong Kong World Wide Web Database.5
After the removal of invalid ones, 12, 800websites
were finally acquired as our seed set.6
3http://code.google.com/p/pupsniffer
4http://www.852.com
5http://www.cuhk.edu.hk/hkwww.htm
6http://mega.ctl.cityu.edu.hk/
?czhang22/pupsniffer-eval/Data/All_Seed_
Websites_List.txt
3.2 URL Pattern Detection and Deep
Webpage Recovery
The enhanced algorithm described in Section 2.1
above was ran to extract credible URL patterns. In
general, the extracted patterns are valid as long as
the threshold is not too low ? it is set to C(p, w)>
0.1 in our experiments. A number of strongest pat-
terns found are presented in Table 2 for demon-
stration. Most of them, especially <en,tc> and
<eng,chi>, are very intuitive patterns. A full
list of URL pairing patterns detected in our exper-
iments is also available.7 Particularly interesting is
that all these patterns were identified in an unsu-
pervised fashion without any manual heuristics.
Using these patterns, the original algorithm re-
trieved about 290K candidate bilingual webpage
pairs. By the simple trick of rescuing weak lo-
cal patterns with the global credibility threshold
C(p) > 500, 10K more webpage pairs were fur-
ther found. Additionally, other 16K webpage
pairs were dug out from deep webpages by auto-
matically generating paired webpages with the aid
of identified URL patterns.
7http://mega.ctl.cityu.edu.hk/
?czhang22/pupsniffer-eval/Data/Pattern_
Credibility_LargeThan100.txt
141
Pattern C(p)
<en,tc> 13997.36
<eng,tc> 12869.56
<english,tc chi> 11436.12
<english,chinese> 11032.46
<eng,chi> 7824.86
Table 2: Top 5 patterns with their global credibility
values.
Method Pairs Precision
Kit and Ng (2007) 290,247 94.06%
Weak key rescue 10,015 89.27%
Deep page recovery 15,825 95.02%
Incremental exploration 37,491 99.40%
Total 348, 058 94.72%
True pair increment 55, 674 20.76%
Table 3: Number of bilingual webpage pairs found
and their precision from sampled evaluation.
3.3 Website Exploration
To go beyond the original 12, 800 websites, the in-
cremental algorithm described in Section 2.3 was
run for one iteration to find outside bilingual web-
sites directly linked from the seeds. The top 500
of them, ranked by
?
Linkout,
?
PageRank and
?
WeightedPageRank, respectively, were manu-
ally checked by five students, giving the curves
of the total number of true bilingual websites and
overall precision per top N websites as plotted
in Figure 2. These results show that almost 50%
of the top 500 related outside websites ranked by
?
WeightedPageRank are true bilingual websites.
A higher precision indicates more bilingual web-
page pairs correctly matched by the URL patterns
in use.
After one iteration of the incremental algorithm,
37K more candidate bilingual webpage pairs were
found in the related outside websites, besides the
290K by the original algorithm. Table 3 presents
the number of webpage pairs identified by each
algorithm with a respective precision drawn from
random sampling. These results suggest that our
proposed enhancement is able to harvest above
20% more bilingual webpage pairs without de-
grading the overall precision. Error analysis shows
that around 80% of errors were due to mistakes
in language identification for webpages. For in-
stance, some Japanese webpages were mistakenly
recognized as Chinese ones.
?
?
?
?
?
0
50
100
150
200
250
300
50 100 150 200 250 300 350 400 450 500
# true 
bili
ngual w
ebs
ites
 
N
?Linkout ?PageRank ?WeightedPagerRank
0.40 
0.45 
0.50 
0.55 
0.60 
0.65 
0.70 
50 100 150 200 250 300 350 400 450 500
Precisi
on
N
?Linkout ?PageRank ?WeightedPagerRank
Figure 2: Number and precision of true bilingual
websites found per top N outside websites ranked
by various criteria.
4 Conclusion
In this paper we have presented an efficient ap-
proach to mining bilingual webpages via com-
puting highly credible bilingual URL pairing pat-
terns. With the aid of these patterns learned in
an unsupervised way, our research moves on to
exploring the possibility of rescuing weak local
keys by virtue of global credibility, uncovering
deep bilingual webpages by generating candidate
URLs using available keys, and also developing
an incremental algorithm for mining more bilin-
gual websites that are linked from the known bilin-
gual websites in our seed set. Experimental results
show that these several enhanced algorithms im-
prove the precision over the baseline from 94.06%
to 99.40% and, more importantly, help discover
above 20% more webpage pairs while maintain-
ing a high overall precision.
Acknowledgements
The research described in this paper was supported
in part by the Research Grants Council (RGC)
of Hong Kong SAR, China, through the GRF
142
grant 9041597 (CityU 144410), National Natural
Science Foundation of China through the grant
No. 70903032, and Project of the Education Min-
istry of China?s Humanities and Social Sciences
through the grant No. 13YJA870020.
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107?
117.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web
text mining for cross-language ir. In Proc. of RIAO,
pages 62?77.
Jisong Chen, Rowena Chau, and Chung-Hsing Yeh.
2004. Discovering parallel text from the world wide
web. In Proceedings of the second workshop on
Australasian information security, Data Mining and
Web Intelligence, and Software Internationalisation-
Volume 32, pages 157?161.
Mark W Davis and Ted E Dunning. 1995. A trec eval-
uation of query translation methods for multi-lingual
text retrieval. In Fourth Text Retrieval Conference,
pages 483?498.
Miquel Espla`-Gomis and Mikel L Forcada. 2010.
Combining content-based and URL-based heuris-
tics to harvest aligned bitexts from multilingual sites
with bitextor. The Prague Bulletin of Mathematical
Linguistics, 93(1):77?86.
William A Gale and Kenneth W Church. 1991. Iden-
tifying word correspondences in parallel texts. In
Proceedings of the workshop on Speech and Natural
Language, pages 152?157.
Shihong Huang and Scott Tilley. 2001. Issues of con-
tent and structure for a multilingual web site. In Pro-
ceedings of the 19th annual international conference
on Computer documentation, pages 103?110.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 2, pages 870?878.
Chunyu Kit and Jessica Yee Ha Ng. 2007. An in-
telligent web agent to mine bilingual parallel pages
via automatic discovery of URL pairing patterns.
In Proceedings of the 2007 IEEE/WIC/ACM Inter-
national Conferences on Web Intelligence and In-
telligent Agent Technology - Workshops: Workshop
on Agents and Data Mining Interaction (ADMI-07),
pages 526?529.
Xiaoyi Ma and Mark Liberman. 1999. BITS: A
method for bilingual text search over the web. In
Machine Translation Summit VII, pages 538?542.
I. Dan Melamed. 1997. A word-to-word model of
translational equivalence. In Proceedings of the
Eighth Conference on European Chapter of the As-
sociation for Computational Linguistics, pages 490?
497.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan and Claypool Publishers.
Douglas W Oard. 1997. Cross-language text re-
trieval research in the USA. In Proceedings of the
Third DELOS Workshop: Cross-Language Informa-
tion Retrieval, pages 7?16.
Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Philip Resnik. 1998. Parallel strands: A preliminary
investigation into mining the web for bilingual text.
In D. Farwell, L. Gerber, and E. Hovy, editors, Ma-
chine Translation and the Information Soup: Third
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-98), pages 72?82.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 527?534.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A DOM tree alignment model for min-
ing parallel data from the web. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 489?
496.
Jesu?s Toma?s, Jordi Bataller, Francisco Casacuberta,
and Jaime Lloret. 2008. Mining Wikipedia as a par-
allel and comparable corpus. In Language Forum,
volume 34.
Sha-ni YE, Ya-juan LV, Yun Huang, and Qun Liu.
2008. Automatic parallel sentences extraction from
web. Journal of Chinese Information Processing,
22:67?73.
T Yulia and W Shuly. 2010. Automatic acquisi-
tion of parallel corpora from website with dynamic
content. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC-2010), pages 3389?3392.
Ying Zhang, Ke Wu, Jianfeng Gao, and Phil Vines.
2006. Automatic acquisition of chinese?english par-
allel corpus from the web. In Advances in Informa-
tion Retrieval, pages 420?431. Springer.
143
