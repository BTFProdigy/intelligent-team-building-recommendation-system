Coling 2008: Companion volume ? Posters and Demonstrations, pages 127?130
Manchester, August 2008
Building a Bilingual Lexicon Using Phrase-based
Statistical Machine Translation via a Pivot Language
Takashi Tsunakawa? Naoaki Okazaki? Jun?ichi Tsujii??
?Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo 7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-0033 Japan
?School of Computer Science, University of Manchester / National Centre for Text Mining
131 Princess Street, Manchester, M1 7DN, UK
{tuna, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper proposes a novel method for
building a bilingual lexicon through a pivot
language by using phrase-based statisti-
cal machine translation (SMT). Given two
bilingual lexicons between language pairs
L
f
?L
p
and L
p
?L
e
, we assume these lexi-
cons as parallel corpora. Then, we merge
the extracted two phrase tables into one
phrase table between L
f
and L
e
. Fi-
nally, we construct a phrase-based SMT
system for translating the terms in the lex-
icon L
f
?L
p
into terms of L
e
and, ob-
tain a new lexicon L
f
?L
e
. In our experi-
ments with Chinese-English and Japanese-
English lexicons, our system could cover
72.8% of Chinese terms and drastically im-
prove the utilization ratio.
1 Introduction
The bilingual lexicon is a crucial resource for mul-
tilingual applications in natural language process-
ing including machine translation (Brown et al,
1990) and cross-lingual information retrieval (Nie
et al, 1999). A number of bilingual lexicons have
been constructed manually, despite their expensive
compilation costs. However, it is unrealistic to
build a bilingual lexicon for every language pair;
thus, comprehensible bilingual lexicons are avail-
able only for a limited number of language pairs.
One of the solutions is to build a bilingual lex-
icon of the source language L
f
and the target L
e
through a pivot language L
p
, when large bilingual
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
lexicons L
f
?L
p
and L
p
?L
e
are available. Numer-
ous researchers have explored the use of pivot lan-
guages (Tanaka and Umemura, 1994; Schafer and
Yarowsky, 2002; Zhang et al, 2005). This ap-
proach is advantageous because we can obtain a
bilingual lexicon between L
e
and L
f
, even if no
bilingual lexicon exists between these languages.
Pivot-based methods for dictionary construction
may produce incorrect translations when the word
w
e
is translated from a word w
f
by a polysemous
pivot word w
p
1
. Previous work addressed the poly-
semy problem in pivot-based methods (Tanaka and
Umemura, 1994; Schafer and Yarowsky, 2002).
Pivot-based methods also suffer from a mismatch
problem, in which a pivot word w
p
from a source
word w
f
does not exist in the bilingual lexicon L
p
?
L
e
2
. Moreover, a bilingual lexicon for technical
terms is prone to include a number of pivot terms
that are not included in another lexicon.
This paper proposes a method for building a
bilingual lexicon through a pivot language by us-
ing phrase-based statistical machine translation
(SMT) (Koehn et al, 2003). We build a transla-
tion model between L
f
and L
e
by assuming two
lexicons L
f
?L
p
and L
p
?L
e
as parallel corpora, in
order to increase the obtained lexicon size by han-
dling multi-word expressions appropriately. The
main advantage of this method is its ability to in-
corporate various translation models that associate
languages L
f
?L
e
; for example, we can further im-
prove the translation model by integrating a small
bilingual lexicon L
f
?L
e
.
1A Japanese term ????: dote, embankment, may be as-
sociated with a Chinese term ???,? y??ngha?ng: banking in-
stitution, using the pivot word bank in English.
2It is impossible to associate two translation pairs (???
??? (chikyu?-ondanka),? global warming), and (global heat-
ing, ????? (qua?nqiu?-bia`nnua?n)?) because of the differ-
ence in English (pivot) terms.
127
Lf-Lp lexicon Le-Lp lexicon
Lf-Lp translation 
phrase table
Le-Lp translation 
phrase table
Lf-Le translation 
phrase table
Phrase-based
SMT system
Word alignment &
grow-diag-final method
Merging phrase tables
Additional
features
INPUT OUTPUT
Le: translations of 
Lf-Lp lexicon
Figure 1: Framework of our approach
2 Merging two bilingual lexicons
We introduce phrase-based SMT for merging the
lexicons, in order to improve both the merged
lexicon size and its accuracy. Recently, several
researchers proposed the use of the pivot lan-
guage for phrase-based SMT (Utiyama and Isa-
hara, 2007; Wu and Wang, 2007). We employ a
similar approach for obtaining phrase translations
with the translation probabilities by assuming the
bilingual lexicons as parallel corpora. Figure 1 il-
lustrates the framework of our approach.
Let us suppose that we have two bilingual lex-
icons L
f
?L
p
and L
p
?L
e
. We obtain word align-
ments of these lexicons by applying GIZA++ (Och
and Ney, 2003), and grow-diag-final heuristics
(Koehn et al, 2007). Let w?
x
be a phrase that
represents a sequence of words in the language
L
x
. For phrase pairs (w?
p
, w?
f
) and (w?
e
, w?
p
), the
translation probabilities p(w?
p
|w?
f
) and p(w?
e
|w?
p
)
are computed using the maximum likelihood esti-
mation from the co-occurrence frequencies, con-
sistent with the word alignment in the bilingual
lexicons. We calculate the direct translation prob-
abilities between source and target phrases,
p(w?
e
|w?
f
) =
?
w?
p
p(w?
e
|w?
p
)p(w?
p
|w?
f
)
?
w?
?
e
?
w?
p
p(w?
?
e
|w?
p
)p(w?
p
|w?
f
)
. (1)
We employ the log-linear model of phrase-based
SMT (Och and Ney, 2002) for translating the
source term w?
f
in the lexicon L
f
?L
p
into the tar-
get language by finding a term ?w?
e
that maximizes
the translation probability,
?
w?
e
= argmax
w?
e
Pr(w?
e
|w?
f
)
= argmax
w?
e
M
?
m=1
?
m
h
m
(w?
e
, w?
f
), (2)
where we have M feature functions h
m
(w?
e
, w?
f
)
and model parameters ?
m
.
In addition to the typical features for the SMT
framework, we introduce two features: character-
based similarity, and additional bilingual lexicon.
We define a character-based similarity feature,
h
char sim
(w?
e
, w?
f
) = 1 ?
ED(w?
e
, w?
f
)
max(w?
e
, w?
f
)
, (3)
where ED(x, y) represents a Levenshtein distance
of characters between the two terms x and y3. We
also define an additional bilingual lexicon feature,
h
add lex
(w?
e
, w?
f
) =
?
i
log p
?
(w?
(i)
e
|w?
(i)
f
), (4)
where w?(i)
e
and w?(i)
f
represent an i-th translated
phrase pair on the term pair (w?
e
, w?
f
) during the
decoding, and p?(w?(i)
e
|w?
(i)
f
) represents the phrase
translation probabilities derived from the addi-
tional lexicon. The probability p?(w?(i)
e
|w?
(i)
f
) is cal-
culated using the maximum likelihood estimation.
3 Experiment
3.1 Data
For building a Chinese-to-Japanese lexicon, we
used the Japanese-English lexicon released by
JST4 (527,206 term pairs), and the Chinese-
English lexicon compiled by Wanfang Data5
(525,259 term pairs). Both cover a wide range
of named entities and technical terms that may
not be included in an ordinary dictionary. As an
additional lexicon, we used the Japanese-English-
Chinese trilingual lexicon6 (596,967 term pairs)
generated from EDR7 Japanese-English lexicon.
We lower-cased and tokenized all terms by the
following analyzers: JUMAN8 for Japanese, the
MEMM-based POS tagger9 for English, and cjma
(Nakagawa and Uchimoto, 2007) for Chinese.
3.2 The sizes and coverage of merged lexicons
Table 1 shows the distinct numbers of terms in
the original and merged lexicons, and the uti-
3We regard the different shapes of Han characters between
Chinese and Japanese as identical in our experiments.
4Japan Science and Technology Agency (JST)
http://pr.jst.go.jp/others/tape.html
5http://www.wanfangdata.com/
6This data was manually compiled by NICT, Japan.
7http://www2.nict.go.jp/r/r312/EDR/index.html
8http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
9http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/
128
Lexicon L
C
size L
E
size L
J
size
L
C
?L
E
375,990 429,807 -
L
E
?L
J
- 418,044 465,563
L
E
(distinct) - 783,414 -
Additional lex. 94,928 - 90,605
Exact matching 98,537 68,996 103,437
(26.2%) (22.2%)
Unique matching 4,875 4,875 4,875
(1.3%) (1.0%)
Table 1: The statistics of lexicons
lization ratio10 in the parentheses. For compari-
son, we prepared two baseline systems for build-
ing Chinese-Japanese lexicons. Exact matching
connects source and target terms that share at
least one common translation term in the pivot
language. Unique matching is an extreme ap-
proach for avoiding negative effects of polysemous
pivot terms: it connects source and target terms if
source, pivot, and target terms appear only once in
the corresponding lexicons.
Exact matching achieved 26.2% of the utiliza-
tion ratio in Japanese-to-Chinese translation, and
22.2% in Chinese-to-Japanese translation. These
figures imply that about 75% of the terms remained
unused in building the Japanese-Chinese lexicon.
With unique matching, as little as 1% of Japanese
and Chinese terms could be used. In contrast, our
method could cover 72.8% of Chinese terms by
generating Japanese terms, which was a drastic im-
provement in the utilization ratio.
3.3 Generating Japanese translations of the
Chinese-English lexicon
For evaluating the correctness of the merged lex-
icons, we assumed the lexicon generated by the
unique matching as a development/test set. Devel-
opment and test sets consist of about 2,400 term
pairs, respectively. Next, we input Chinese terms
in the development/test set into our system based
on Moses (Koehn et al, 2007), and obtained the
Japanese translations. We evaluated the perfor-
mance by using BLEU, NIST, and accuracy mea-
sures. Table 2 shows the evaluation results on the
test set. Our system could output correct trans-
lations for 68.5% of 500 input terms. The table
also reports that additional features were effective
in improving the performance.
We also conducted another experiment to gen-
erate Japanese translations for Chinese terms in-
cluded in an external resource. We randomly ex-
10The number of terms in the original lexicon used for
building the merged lexicon.
Features BLEU NIST Acc.
Typical features 0.4519 7.4060 0.676
w/ character similarity 0.4670 7.4963 0.682
w/ additional lexicon 0.4800 7.5907 0.674
All 0.4952 7.7046 0.685
Table 2: Translation performance on the test set
Features/Models Prec1 Prec10 MRR
Typical features 0.142 0.232 0.1719
w/ character similarity 0.136 0.224 0.1654
w/ additional lexicon 0.140 0.230 0.1704
All 0.140 0.230 0.1714
E-to-J translation 0.090 0.206 0.1256
Table 3: Evaluation results for the Eijiro dictionary
tracted 500 Chinese-English term pairs from the
Wanfang Data lexicon, for which the English term
cannot be mapped by the JST lexicon, but can be
mapped by another lexicon Eijiro11. Table 3 shows
the results for these 500 terms. Prec1 or Prec10 are
the precisions that the 1- or 10-best translations in-
clude the correct one, respectively. MRR (mean
reciprocal rank) is (1/500)?
i
(1/r
i
), where r
i
is
the highest rank of the correct translations for the
i-th term.
Since the input lexicons are Chinese-English
term pairs, their Japanese translations can be gen-
erated directly from the English terms by applying
an English-Chinese translation system. We com-
pared our system to an English-Japanese phrase-
based SMT system (E-to-J translation), con-
structed from the JST Japanese-English lexicon.
Table 3 shows that our system outperformed the
English-to-Japanese direct translation system.
Table 4 displays translation examples. The first
example shows that our system could output a cor-
rect translation (denoted by [T]); and the E-to-J
system failed to translate the source term ([F]),
because it could not reorder the source English
words and translate the word pubis correctly. In
the second example, our system could reproduce
Chinese characters ??? (fluid)?, but the E-to-J
system output a semantically acceptable but awk-
ward Japanese term. In the last example, the word
segmentation of the source Chinese term was in-
correct (???? (lumber)?? (lymph)?? is cor-
rect). Thus, our system received an invalid word ?
??? and could not find a translation for the word.
11http://www.eijiro.jp/
129
English Chinese Japanese (Eijiro) Japanese (C-to-J) Japanese (E-to-J)
symphysis pubis ???? ???? ???? [T] ??? (symphysis shame) [F]
ideal fluid dy-
namics
?? ??
???
?????? ?????? [T] ??? (fluid)?? [F]
intermediate
lumbar lymph
nodes
?? ??
??
??????? ?? ? ?? (inter-
mediate node [lumbar-
lymph]
INVALID
) [F]
??????? [T]
Table 4: Translation examples on Eijiro dictionary
4 Conclusion
This paper proposed a novel method for building a
bilingual lexicon by using a pivot language. Given
two bilingual lexicons L
f
?L
p
and L
p
?L
e
, we con-
structed a phrase-based SMT system from L
f
?L
e
by merging the lexicons into a phrase translation
table L
f
?L
e
. The experimental results demon-
strated that our method improves the utilization ra-
tio of given lexicons drastically. We also showed
that the pivot approach was more effective than the
SMT system that translates from L
p
to L
e
directly.
The future direction would be to introduce other
resources such as the parallel corpora and other
pivot languages into the SMT system for improv-
ing the precision and the coverage of the obtained
lexicon. We are also planning on evaluating a ma-
chine translation system that integrates our model.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan).
References
Brown, Peter F., John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology, pages 48?54.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics, demonstration ses-
sion, pages 177?180.
Nakagawa, Tetsuji and Kiyotaka Uchimoto. 2007. Hy-
brid approach to word segmentation and POS tag-
ging. In Companion Volume to the Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 217?220.
Nie, Jian-Yun, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language informa-
tion retrieval based on parallel texts and automatic
mining of parallel texts from the Web. In Proc. of
the 22nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 74?81.
Och, Franz Josef and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Schafer, Charles and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proc. of the 6th Conference
on Natural Language Learning, volume 20, pages 1?
7.
Tanaka, Kumiko and Kyoji Umemura. 1994. Construc-
tion of a bilingual dictionary intermediated by a third
language. In Proc. of the 15th International Confer-
ence on Computational Linguistics, pages 297?303.
Utiyama, Masao and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proc. of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 484?491.
Wu, Hua and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proc. of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 856?
863.
Zhang, Yujie, Qing Ma, and Hitoshi Isahara. 2005.
Construction of a Japanese-Chinese bilingual dictio-
nary using English as an intermediary. International
Journal of Computer Processing of Oriental Lan-
guages, 18(1):23?39.
130
Bilingual Synonym Identification with Spelling Variations
Takashi Tsunakawa? Jun?ichi Tsujii???
?Department of Computer Science,
Graduate School of Information Science and Technology, University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-0033 Japan
?School of Computer Science, University of Manchester
Oxford Road, Manchester, M13 9PL, UK
?National Centre for Text Mining 131 Princess Street, Manchester, M1 7DN, UK
{tuna, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper proposes a method for identify-
ing synonymous relations in a bilingual lex-
icon, which is a set of translation-equivalent
term pairs. We train a classifier for identi-
fying those synonymous relations by using
spelling variations as main clues. We com-
pared two approaches: the direct identifi-
cation of bilingual synonym pairs, and the
merger of two monolingual synonyms. We
showed that our approach achieves a high
pair-wise precision and recall, and outper-
forms the baseline method.
1 Introduction
Automatically collecting synonyms from language
resources is an ongoing task for natural language
processing (NLP). Most NLP systems have diffi-
culties in dealing with synonyms, which are differ-
ent representations that have the same meaning in
a language. Information retrieval (IR) could lever-
age synonyms to improve the coverage of search re-
sults (Qiu and Frei, 1993). For example, when we
input the query ?transportation in India? into an IR
system, the system can expand the query to its syn-
onyms; e.g. ?transport? and ?railway?, to find more
documents.
This paper proposes a method for the automatic
identification of bilingual synonyms in a bilingual
lexicon, with spelling variation clues. A bilingual
synonym set is a set of translation-equivalent term
pairs sharing the same meaning. Although a number
of studies have aimed at identifying synonyms, this
is the first study that simultaneously finds synonyms
in two languages, to our best knowledge.
Let us consider the case where a user enters the
Japanese query ?ko?jo?? (??, industrial plant) into a
cross-lingual IR system to find English documents.
After translating the query into the English trans-
lation equivalent, ?plant,? the cross-lingual IR sys-
tem may expand the query to its English synonyms,
e.g. ?factory,? and ?workshop,? and retrieve docu-
ments that include the expanded terms. However,
the term ?plant? is ambiguous; the system may also
expand the query to ?vegetable,? and the system is
prevented by the term which is different from our
intention. In contrast, the system can easily reject
the latter expansion, ?vegetable,? if we are aware of
bilingual synonyms, which indicate synonymous re-
lations over bilingual lexicons: (ko?jo?, plant)? (ko?jo?,
factory) and (shokubutsu1, plant) ? (shokubutsu,
vegetable)2 (See Figure 1). The expression of the
translation equivalent, (ko?jo?, plant), helps a cross-
lingual IR system to retrieve documents that include
the term ?plant,? used in the meaning for ko?jo?, or in-
dustrial plants.
We present a supervised machine learning ap-
proach for identifying bilingual synonyms. Design-
ing features for bilingual synonyms such as spelling
variations and bilingual associations, we train a clas-
sifier with a manually annotated bilingual lexicon
with synonymous information. In order to evaluate
the performance of our method, we carried out ex-
periments to identify bilingual synonyms by two ap-
proaches: the direct identification of bilingual syn-
onym pairs, and bilingual synonym pairs merged
from two monolingual synonym lists. Experimental
results show that our approach achieves the F-scores
1Shokubutsu (??) means botanical plant.
2??? represents the synonymous relation.
457
Figure 1: An example of an ambiguous term ?plant?,
and the synonyms and translation equivalents (TE)
89.3% in the former approach and 91.4% in the lat-
ter, thus outperforming the baseline method that em-
ploys only bilingual relations as its clues.
The remainder of this paper is organized as fol-
lows. The next section describes related work on
synonym extraction and spelling variations. Section
3 describes the overview and definition of bilingual
synonyms, the proposed method and employed fea-
tures. In Section 4 we evaluate our method and con-
clude this paper.
2 Related work
There have been many approaches for detecting syn-
onyms and constructing thesauri. Two main re-
sources for synonym extraction are large text cor-
pora and dictionaries.
Many studies extract synonyms from large mono-
lingual corpora by using context information around
target terms (Croach and Yang, 1992; Park and Choi,
1996; Waterman, 1996; Curran, 2004). Some re-
searchers (Hindle, 1990; Grefenstette, 1994; Lin,
1998) classify terms by similarities based on their
distributional syntactic patterns. These methods of-
ten extract not only synonyms, but also semantically
related terms, such as antonyms, hyponyms and co-
ordinate terms such as ?cat? and ?dog.?
Some studies make use of bilingual corpora or
dictionaries to find synonyms in a target language
(Barzilay and McKeown, 2001; Shimohata and
Sumita, 2002; Wu and Zhou, 2003; Lin et al, 2003).
Lin et al (2003) chose a set of synonym candidates
for a term by using a bilingual dictionary and com-
puting distributional similarities in the candidate set
to extract synonyms. They adopt the bilingual in-
formation to exclude non-synonyms (e.g., antonyms
and hyponyms) that may be used in the similar con-
texts. Although they make use of bilingual dictio-
naries, this study aims at finding bilingual synonyms
directly.
In the approaches based on monolingual dictio-
naries, the similarities of definitions of lexical items
are important clues for identifying synonyms (Blon-
del et al, 2004; Muller et al, 2006). For instance,
Blondel et al (2004) constructed an associated dic-
tionary graph whose vertices are the terms, and
whose edges from v1 to v2 represent occurrence of
v2 in the definition for v1. They choose synonyms
from the graph by collecting terms pointed to and
from the same terms.
Another strategy for finding synonyms is to con-
sider the terms themselves. We divide it into two
approaches: rule-based and distance-based.
Rule-based approaches implement rules with
language-specific patterns and detect variations by
applying rules to terms. Stemming (Lovins, 1968;
Porter, 1980) is one of the rule-based approaches,
which cuts morphological suffix inflections, and ob-
tains the stems of words. There are other types of
variations for phrases; for example, insertion, dele-
tion or substitution of words, and permutation of
words such as ?view point? and ?point of view? are
such variations (Daille et al, 1996).
Distance-based approaches model the similarity
or dissimilarity measure between two terms to find
similar terms. The edit distance (Levenshtein, 1966)
is the most widely-used measure, based on the mini-
mum number of operations of insertion, deletion, or
substitution of characters for transforming one term
into another. It can be efficiently calculated by using
458
Term pairs Concept
p1 = (sho?mei (??), light) c1
p2 = (sho?mei, lights) c1
p3 = (karui (??), light) c2
p4 = (raito (???), light) c1, c2
p5 = (raito, lights) c1
p6 = (raito, right) c3
p7 = (migi (?), right) c3
p8 = (raito, right fielder) c4
p9 = (kenri (??), right) c5
p10 = (kenri, rights) c5
Table 1: An Example of a bilingual lexicon and syn-
onym sets (concepts)
J terms E terms Description
c1 sho?mei, raito light, lights illumination
c2 karui, raito light lightweight
c3 migi, raito right right-side
c4 raito right fielder (baseball)
c5 kenri right, rights privilege
Table 2: The concepts in Table 1
a dynamic programming algorithm, and we can set
the costs/weights for each character type.
3 Bilingual Synonyms and Translation
Equivalents
This section describes the notion of bilingual syn-
onyms and our method for identifying the synony-
mous pairs of translation equivalents. We consider a
bilingual synonym as a set of translation-equivalent
term pairs referring to the same concept.
Tables 1 and 2 are an example of bilingual
synonym sets. There are ten Japanese-English
translation-equivalent term pairs and five bilingual
synonym sets in this example. A Japanese term
?raito? is the phonetic transcription of both ?light?
and ?right,? and it covers four concepts described by
the three English terms. Figure 2 illustrates the re-
lationship among these terms. The synonymous re-
lation and the translation equivalence are considered
to be similar in that two terms share the meanings.
Following synonymous relation between terms in
one language, we deal with the synonymous relation
between bilingual translation-equivalent term pairs
Figure 2: Relations among terms in Table 2
Solid lines show that two terms are translation
equivalents, while dotted lines show that two terms
are (monolingual) synonyms.
as bilingual synonyms. The advantage of manag-
ing the lexicon in the format of bilingual synonyms
is that we can facilitate to tie the concepts and the
terms.
3.1 Definitions
Let E and F be monolingual lexicons. We first as-
sume that a term e ? E (or f ? F ) refers to one
or more concepts, and define that a term e is a syn-
onym3 of e?(? E) if and only if e and e? share an
identical concept4. Let ??? represent the synony-
mous relation, and this relation is not transitive be-
cause a term often has several concepts:
e ? e? ? e? ? e?? 6=? e ? e??. (1)
We define a synonym set (synset) Ec as a set whose
elements share an identical concept c: Ec = {e ?
E|?e refers to c}. For a term set Ec(? E),
Ec is a synonym set (synset)
=? ?e, e? ? Ec e ? e? (2)
is true, but the converse is not necessarily true, be-
cause of the ambiguity of terms. Note that one term
can belong to multiple synonym sets from the defi-
nition.
Let D(? F ? E) be a bilingual lexicon defined
as a set of term pairs (f, e) (f ? F, e ? E) satis-
fying that f and e refer to an identical concept. We
3For distinguishing from bilingual synonyms, we often call
the synonym a monolingual synonym.
4The definition of concepts, that is, the criteria of deciding
whether two terms are synonymous or not, is beyond the fo-
cus of this paper. We do not assume that related terms such as
hypernyms, hyponyms and coordinates are kinds of synonyms.
In our experiments the criteria depend on manual annotation of
synonym IDs in the training data.
459
call these pairs translation equivalents, which refer
to concepts that both f and e refer to. We define
that two bilingual lexical items p and p?(? D) are
bilingual synonyms if and only if p and p? refer to an
identical concept in common with the definition of
(monolingual) synonyms. This relation is not tran-
sitive again, and if e ? e? and f ? f ?, it is not
necessarily true that p ? p?:
e ? e? ? f ? f ? 6=? p ? p? (3)
because of the ambiguity of terms. Similarly, we
can define a bilingual synonym set (synset) Dc as
a set whose elements share an identical meaning c:
Dc = {p ? D|?p refers to c}. For a set of transla-
tion eqiuvalents Dc,
Dc is a bilingual synonym set (synset)
=? ?p, p? ? Dc p ? p? (4)
is true, but the converse is not necessarily true.
3.2 Identifying bilingual synonym pairs
In this section, we describe an algorithm to identify
bilingual synonym pairs by using spelling variation
clues. After identifying the pairs, we can construct
bilingual synonym sets by assuming that the con-
verse of the condition (4) is true, and finding sets
of bilingual lexical items in which all paired items
are bilingual synonyms. We can see this method
as the complete-linkage clustering of translation-
equivalent term pairs. We can adopt another option
to construct them by assuming also that the bilingual
synonymous relation has transitivity: p ? p? ? p? ?
p?? =? p ? p??, and this can be seen as simple-
linkage clustering. This simplified method ignores
the ambiguity of terms, and it may construct a bilin-
gual synonym sets which includes many senses. In
spite of the risk, it is effective to find large synonym
sets in case the bilingual synonym pairs are not suf-
ficiently detected. In this paper we focus only on
identifying bilingual synonym pairs and evaluating
the performance of the identification.
We employ a supervised machine learning tech-
nique with features related to spelling variations
and so on. Figure 3 shows the framework for this
method. At first we prepare a bilingual lexicon with
synonymous information as training data, and gen-
erate a list consisting of all bilingual lexical item
Figure 3: Overview of our framework
pairs in the bilingual lexicon. The presence or ab-
sence of bilingual synonymous relations is attached
to each element of the list. Then, we build a classi-
fier learned by training data, using a maximum en-
tropy model (Berger et al, 1996) and the features
related to spelling variations in Table 3.
We apply some preprocessings for extracting
some features. For English, we transform all terms
into lower-case, and do not apply any other trans-
formations such as tokenization by symbols. For
Japanese, we apply a morphological analyzer JU-
MAN (Kurohashi et al, 1994) and obtain hiragana
representations5 as much as possible6. We may re-
quire other language-specific preprocessings for ap-
plying this method to other languages.
We employed binary or real-valued features de-
scribed in Table 3. Moreover, we introduce
the following combinatorial features: h1F ? h1E ,?
h2F ? h2E ,
?
h3F ? h3E , h5E ? h5F , h6 ? h2F and
h7 ? h2E .
3.2.1 Two approaches for identifying bilingual
synonym pairs
There are two approaches for identifying bilin-
gual synonym pairs: one is directly identifying
whether two bilingual lexical items are bilingual
synonyms (?bilingual? method), and another is first
5Hiragana is one of normalized representations of Japanese
terms, which denotes how to pronounce the term. Japanese vo-
cabulary has many of homonyms, which are semantically differ-
ent but have the same pronunciation. Despite the risk of classi-
fying homonyms into synonyms, we do not use original forms
of Japanese terms because they are typically too short to extract
character similarities.
6We keep unknown terms of JUMAN unchanged.
460
h1F , h1E : Agreement of the
first characters
Whether the first characters match or not
h2F , h2E : Normalized edit
distance
1? ED(w,w
?)
max(|w|,|w?|) , where ED(w,w
?) is a non-weighted edit distance between w and w? and
|w| is the number of characters in w
h3F , h3E : Bigram similarity |bigram(w)?bigram(w
?)|
max(|w|,|w?|)?1 , where bigram(w) is a multiset of character-based bigrams in w
h4F , h4E : Agreement or
known synonymous relation
of word sub-sequences
The count that sub-sequences of the target terms match as known terms or are in known
synonymous relation
h5F , h5E : Existence of cross-
ing bilingual lexical items
For bilingual lexical items (f1, e1) and (f2, e2), whether (f1, e2) (for h5F ) or (f2, e1) (for
h5E) is in the bilingual lexicon of the training set
h6: Acronyms Whether one English term is an acronym for another (Schwartz and Hearst, 2003)
h7: Katakana variants Whether one Japanese term is a katakana variant for another (Masuyama et al, 2004)
Table 3: Features used for identifying bilingual synonym pairs
hiF is the feature value when the terms w and w?(? F ) are compared in the i-th feature and so as hiE . h6 is
only for English and h7 is only for Japanese.
identifying monolingual synonyms in each language
and then merging them according to the bilingual
items (?monolingual? method). We implement these
two approaches and compare the results. For identi-
fying monolingual synonyms, we use features with
bilingual items as follows: For a term pair e1 and
e2, we obtain all the translation candidates F1 =
{f |(f, e1) ? D} and F2 = {f ?|(f ?, e2) ? D},
and calculate feature values related to F1 and/or F2
by obtaining the maximum feature value using F1
and/or F2. After that, if all the following four con-
ditions (p1 = (f1, e1) ? D, p2 = (f2, e2) ? D,
f1 ? e1 and f2 ? e2) are satisfied, we assume that
p1 and p2 are bilingual synonym pairs7.
4 Experiment
4.1 Experimental settings
We performed experiments to identify bilingual syn-
onym pairs by using the Japanese-English lexicon
with synonymous information8. The lexicon con-
sists of translation-equivalent term pairs extracted
from titles and abstracts of scientific papers pub-
lished in Japan. It contains many spelling variations
and synonyms for constructing and maintaining the
7Actually, these conditions are not sufficient to derive the
bilingual synonym pairs described in Section 3.1. We assume
this approximation because there seems to be few counter ex-
amples in actual lexicons.
8This data was edited and provided by Japan Science and
Technology Agency (JST).
Total train dev. test
|D| 210647 168837 20853 20957
|J | 136128 108325 13937 13866
|E| 115002 91057 11862 12803
Synsets 50710 40568 5071 5071
Pairs 814524 651727 77706 85091
Table 5: Statistics of the bilingual lexicon for our
experiment
|D|, |J |, and |E| are the number of bilingual lexi-
cal items, the number of Japanese vocabularies, and
the number of English vocabularies, respectively.
?Synsets? and ?Pairs? are the numbers of synonym
sets and synonym pairs, respectively.
thesaurus of scientific terms and improving the cov-
erage. Table 4 illustrates this lexicon.
Table 5 shows the statistics of the dictionary. We
used information only synonym IDs and Japanese
and English representations. We extract pairs of
bilingual lexical items, and treat them as events for
training of the maximum entropy method. The pa-
rameters were adjusted so that the performance is
the best for the development set. For a monolin-
gual method, we used Tb = 0.8, and for a bilingual
method, we used Tb = 0.7.
4.2 Evaluation
We evaluated the performance of identifying bilin-
gual synonym pairs by the pair-wise precision P ,
461
Synset ID J term E term
130213 ???? (shintai-bui) Body Regions
130213 ???? (shintai-bui) body part
130213 ???? (shintai-bui) body region
130213 ???? (shintai-bubun) body part
130217 Douglas? (Douglas-ka) Douglas? Pouch
130217 Douglas? (Douglas-ka) Douglas? Pouch
130217 ????? (Dagurasu-ka) pouch of Douglas
130217 ????? (Dagurasu-ka) pouch of Douglas
130217 ????? (chokucho?-shikyu?-ka) rectouterine pouch
130217 ????? (chokucho?-shikyu?-ka) rectouterine pouch
Table 4: A part of the lexicon used
Each bilingual synonym set consists of items that have the same synset ID. ?? (bubun) is a synonym of
?? (bui). ? (ka) is a hiragana representation of? (ka). ???? (Dagurasu) is a Japanese transcription
of ?Douglas?.
recall R and F-score F defined as follows:
P = C
T
,R = C
N
,F = 2PR
P +R
, (5)
where C, T and N are the number of correctly pre-
dicted pairs as synonyms, predicted pairs to become
synonyms, and synonym pairs in the lexicon9, re-
spectively.
We compared the results with the baseline and the
upper bound. The baseline assumes that each bilin-
gual lexical item is a bilingual synonym if either the
Japanese or English terms are identical. The upper
bound assumes that all the monolingual synonyms
are known and each bilingual item is a bilingual syn-
onym if the Japanese terms and the English terms
are synonymous. The baseline represents the per-
formance when we do not consider spelling varia-
tions, and the upper bound shows the limitation of
the monolingual approach.
4.3 Result
Table 6 shows the evaluation scores of our experi-
ments. The ?monolingual? and ?bilingual? methods
are described in Section 3.2.1. We obtained high
precision and recall scores, although we used fea-
tures primarily with spelling variations. Both meth-
ods significantly outperform the baseline, and show
the importance of considering spelling variations.
9N includes the number of synonym pairs filtered out from
training set by the bigram similarity threshold Tb.
Set Method Precision Recall F-score
dev. baseline 0.977
(31845/32581)
0.410
(31845/77706)
0.577
monolingual 0.911
(74263/81501)
0.956
(74263/77706)
0.932
bilingual 0.879
(72782/82796)
0.937
(72782/77706)
0.907
upper bound 0.984
(77706/78948)
1 0.992
test baseline 0.972
(33382/34347)
0.392
(33382/85091)
0.559
monolingual 0.900
(79099/87901)
0.930
(79099/85091)
0.914
bilingual 0.875
(77640/88714)
0.912
(77640/85091)
0.893
upper bound 0.979
(85091/86937)
1 0.989
Table 6: Evaluation scores
The ?monolingual? method achieved higher preci-
sion and recall than the ?bilingual? method. It in-
dicates that monolingual synonym identification is
effective in finding bilingual synonyms. The up-
per bound shows that there are still a few errors by
the assumption used by the ?monolingual? method.
However, the high precision of the upper bound rep-
resents the well-formedness of the lexicon we used.
We need more experiments on other bilingual lex-
icons to conclude that our method is available for
462
Features Precision Recall F-score
All 0.911 0.956 0.932
?h1F , h1E 0.911 0.974 0.941
?h2F , h2E 0.906 0.947 0.926
?h3F , h3E 0.939 0.930 0.934
?h4F , h4E 0.919 0.734 0.816
?h5F , h5E 0.869 0.804 0.831
?h6, h7 0.940 0.934 0.937
?combs. 0.936 0.929 0.932
Table 7: Evaluation scores of the bilingual method
with removing features on the development set
?h represents removing the feature h and combina-
torial features using h. ?combs. represents remov-
ing all the combinatorial features.
many kinds of lexicons.
To investigate the effectiveness of each feature,
we compared the scores when we remove several
features. Table 7 shows these results. Contrary to
our intuition, we found that features of agreement
of the first characters (h1) remarkably degraded the
recall without gains in precision. One of the rea-
sons for such results is that there are many cases
of non-synonyms that have the same first character.
We need to investigate more effective combinations
of features or to apply other machine learning tech-
niques for improving the performance. From these
results, we consider that the features of h4 are effec-
tive for improving the recall, and that the features of
h2 and h5 contribute improvement of both the pre-
cision and the recall. h3, h6, h7, and combinatorial
features seem to improve the recall at the expense
of precision. Which measure is important depends
on the importance of our target for using this tech-
nique. It depends on the requirements that we em-
phasize, but in general the recall is more important
for finding more bilingual synonyms.
5 Conclusion and future work
This paper proposed a method for identifying bilin-
gual synonyms in a bilingual lexicon by using clues
of spelling variations. We described the notion of
bilingual synonyms, and presented two approaches
for identifying them: one is to directly predict the
relation, and another is to merge monolingual syn-
onyms identified, according to the bilingual lexicon.
Our experiments showed that the proposed method
significantly outperformed the method that did not
use features primarily with spelling variations; the
proposed method extracted bilingual synonyms with
high precision and recall. In addition, we found that
merging monolingual synonyms by the dictionary is
effective for finding bilingual synonyms; there oc-
cur few errors through the assumption described in
Section 3.2.1.
Our future work contains implementing more fea-
tures for identifying synonymous relations, con-
structing bilingual synonym sets, and evaluating our
method for specific tasks such as thesaurus construc-
tion or cross-lingual information retrieval.
Currently, the features used do not include other
clues with spelling variations, such as the weighted
edit distance, transformation patterns, stemming and
so on. Another important clue is distributional infor-
mation, such as the context. We can use both mono-
lingual and bilingual corpora for extracting distribu-
tions of terms, and bilingual corpora are expected to
be especially effective for our goal.
We did not perform an experiment to construct
bilingual synonym sets from synonym pairs in this
paper. Described in Section 3.1, bilingual syn-
onym sets can be constructed from bilingual syn-
onym pairs by assuming some approximations. The
approximation that permits transitivity of bilingual
synonymous relations increases identified bilingual
synonyms, and thus causes an increase in recall and
decrease in precision. It is an open problem to find
appropriate strategies for constructing bilingual syn-
onym sets.
Finally, we plan to evaluate our method for spe-
cific tasks. For data-driven machine translation, it is
expected that data sparseness problem is alleviated
by merging the occurrences of low-frequency terms.
Another application is cross-lingual information re-
trieval, which can be improved by using candidate
expanded queries from bilingual synonym sets.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan). We thank
463
Japan Science and Technology Agency (JST) for
providing a useful bilingual lexicon with synony-
mous information. We acknowledge the anonymous
reviewers for helpful comments and suggestions.
References
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proc. of the
39th Annual Meeting of the Association for Computa-
tional Linguistics, pages 50?57.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
Vincent D. Blondel, Anah?? Gajardo, Maureen Heymans,
Pierre Senellart, and Paul Van Dooren. 2004. A mea-
sure of similarity between graph vertices: Applications
to synonym extraction and web searching. SIAM Re-
view, 46(4):647?666.
Carolyn J. Croach and Bokyung Yang. 1992. Experi-
ments in automatic statistical thesaurus construction.
In Proc. of the 15th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 77?88. ACM Press.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Be?atrice Daille, Beno??t Habert, Christian Jacquemin, and
Jean Royaute?. 1996. Empirical observation of term
variations and principles for their description. Termi-
nology, 3(2):197?258.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. of the 28th
Annual Meeting of the Association for Computational
Linguistics, pages 268?275.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyser JUMAN. In Proc. of Interna-
tional Workshop on Sharable Natural Language Re-
sources, pages 22?28.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proc. of the 2003 International Joint
Conference on Artificial Intelligence, pages 1492?
1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of the 17th International
Conference on Computational Linguistics, volume 2,
pages 768?774.
Julie B. Lovins. 1968. Development of a stemming al-
gorithm. Mechanical Translation and Computational
Linguistics, 11:22?31.
Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nak-
agawa. 2004. Automatic construction of Japanese
KATAKANA variant list from large corpus. In
Proc. of the 20th International Conference on Com-
putational Linguistics, volume 2, pages 1214?1219.
Philippe Muller, Nabil Hathout, and Bruno Gaume.
2006. Synonym extraction using a semantic distance
on a dictionary. In Proc. of TextGraphs: the 2nd Work-
shop on Graph Based Methods for Natural Language
Processing, pages 65?72.
Young C. Park and Key-Sun Choi. 1996. Automatic the-
saurus construction using Bayesian networks. Infor-
mation Processing and Management, 32(5):543?553.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Yonggang Qiu and Hans-Peter Frei. 1993. Concept-
based query expansion. In Proc. of SIGIR-93, 16th
ACM International Conference on Research and De-
velopment in Information Retrieval, pages 160?169.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proc. of the 8th Pacific Sympo-
sium on Biocomputing, pages 451?462.
Mitsuo Shimohata and Eiichiro Sumita. 2002. Auto-
matic paraphrasing based on parallel corpus for nor-
malization. In Proc. of the 3rd International Con-
ference on Language Resources and Evaluation, vol-
ume 2, pages 453?457.
Scott A. Waterman. 1996. Distinguished usage. In
Corpus Processing for Lexical Acquisition, pages 143?
172. MIT Press.
Hua Wu and Ming Zhou. 2003. Optimizing synonym
extraction using monolingual and bilingual resources.
In Proc. of the 2nd International Workshop on Para-
phrasing.
464
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1260?1268, Dublin, Ireland, August 23-29 2014.
 Enriching Wikipedia?s Intra-language Links  
by their Cross-language Transfer 
 
 
Takashi Tsunakawa  Makoto Araya  Hiroyuki Kaji 
Graduate School of Informatics, Shizuoka University 
3-5-1 Johoku, Naka-ku, Hamamatsu, Shizuoka 432-8011, Japan 
 
{tuna, araya, kaji}@inf.shizuoka.ac.jp 
 
  
 
Abstract 
Although hyperlinks enhance the utility of Wikipedia, embedding them in articles imposes a burden on 
contributors. To alleviate this burden as well as enrich hyperlinks in Wikipedia articles, we propose a 
method for transferring intra-language links between different-language articles linked via an inter-
language link. The method avoids anchor selection and disambiguation problems by which usual wikifi-
cation methods are affected, by exploiting the analogy between different language editions of Wikipedia. 
The effectiveness of the method was demonstrated through an experiment of transferring intra-language 
links from English to Japanese. It increased the number of intra-language links in Japanese articles by 
40.9%, and the accuracy of anchors selected was estimated to be 96.3%. 
1 Introduction 
Wikipedia is a Web-based encyclopedia constructed collaboratively by many contributors and contin-
ues to enlarge and improve daily. Because of its overwhelming scale, improved quality, and multilin-
gual nature, it has acquired a huge number of readers worldwide. One of the distinguishing features of 
Wikipedia is that it is a hypertext, which greatly enhances its usefulness and usability. That is, an arti-
cle is linked to its related articles in the same language via intra-language links as well as to its coun-
terpart articles in different languages via inter-language links (ILLs), and readers can navigate within 
millions of articles. 
Editing Wikipedia articles naturally includes linking them to their related articles, which imposes an 
additional burden on contributors. As a result, Wikipedia articles may remain incomplete; they some-
times lack important links as well as contain incorrect links. Thus, it is desirable to automate link-
related editorial tasks such as embedding links in new articles and verifying links in existing articles. 
Linking a plain text, usually non-Wikipedia articles, to Wikipedia articles is called wikification, and 
much effort has been devoted to developing a variety of wikification methods over the past decade 
(Mihalcea and Csomai, 2007; Milne and Witten, 2008a; Fogarolli, 2009; Ratinov et al., 2011). How-
ever, wikification methods are still immature and affected by two hard problems; anchor selection, 
which involves keyword extraction or term recognition, and destination-article determination, which is 
a kind of word sense disambiguation (WSD). 
We focused on the comparability of intra-language links between different language editions of 
Wikipedia, and developed a method for transferring intra-language links in one language edition to 
another language edition. Although the method is not applicable to texts other than Wikipedia articles, 
it avoids the problems of anchor selection and destination-article disambiguation by using analogy 
with different language editions. It does not require any language resources other than Wikipedia itself. 
When the target language is a morphologically rich one, a morphological analyzer is also required. 
Although the method is applicable to any language pairs, we evaluated its effectiveness through an 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1260
experiment of transferring intra-
language links from English to Japa-
nese. 
2 Basic Idea 
In Wikipedia, an article in one lan-
guage is often linked to another article 
in another language via an ILL. These 
two articles, which describe the same 
entity, concept or topic, are comparable. 
Note that this comparability holds not 
only for texts in articles but also for 
intra-language links, each of which 
links an anchor or an important term 
within an article to another same-
language article describing the entity, 
concept, or topic denoted by the an-
chor term. Figure 1 gives an example 
pair of ILL-linked articles; an English 
article ?Tata Motors? and a Japanese article ?????????.? The former has an intra-language 
link from an anchor ?Jaguar? to the English article ?Jaguar Cars,? while the latter has an intra-
language link from an anchor ?????? to the Japanese article ????? (???).? These two 
intra-language links are comparable: namely, the anchors are translations of one another and the desti-
nation articles are linked via an ILL.  
The above fact inspired us to develop a method for transferring intra-language links between ILL-
linked articles to enrich the intra-language links in each article. Suppose an extreme case in which an 
article ? in one language, which is linked to its counterpart ? in another language via an ILL, has no 
intra-language links. An intra-language link can be transferred from ? to ? as follows. First, following 
an intra-language link (? to ??) and then the ILL (?? to ??), the final destination article ?? is identi-
fied as that to be linked from ?. Second, the text of ? is searched for possible anchors for the destina-
tion article ??, which are learned from the entire Wikipedia beforehand. If two or more possible an-
chors are found, the most appropriate one will be selected according to a certain criterion. For example, 
suppose all intra-language links are missing from the Japanese article ?????????? in Figure 
1. The intra-language link from the English article ?Tata Motors? to ?Jaguar Cars? and the ILL from 
?Jaguar Cars? to ????? (???)? suggest that the Japanese article ?????????? should 
have an intra-language link to ????? (???).? The possible anchors for ????? (???),? 
which have been learned from all the Wikipedia articles linked to it, include ????? (???),? 
?????,? and others. Since the text of ?????????? contains ?????,? it is selected as 
the anchor for the destination article ????? (???).? 
It should be noted that our proposed method avoids the two hard problems in wikification, anchor 
selection and disambiguation, by exploiting the intra-language links provided by Wikipedia in another 
language. Resulting anchors are certainly important terms within ?, since their counterparts have been 
selected as anchors by the author of counterpart ? in another language.  Even if an anchor were an am-
biguous term, i.e., had two or more possible destination articles, it would be certainly linked to the ap-
propriate one due to the ?one sense per discourse? hypothesis (Gale et al., 1992). The hypothesis is 
extended to a pair of ILL-linked articles, ? and ?, as follows. A pair of corresponding anchors should 
be regarded as a single term and express the same sense in a discourse shared by ? and ?. In other 
words, they should be linked to articles that are linked via an ILL. Since the proposed method relies on 
this extended hypothesis, it will select correct destination articles for anchors in ? as long as anchors in 
? have been linked to their correct destination articles. 
It should also be noted that the proposed method first determines the destination articles then the 
anchors for them, while usual wikification methods first select anchors then determine their destina-
tion articles. The main reason for this is convenience of implementation; cross-language mapping of 
Figure 1. Transferring intra-language link. 
1261
destination articles is one-to-one (or one-to-zero), while that of anchors can be one-to-many. Deter-
mining destination articles prior to anchors, however, results in an additional advantage that allows a 
destination article to be proposed without an anchor for it. Since the pair ? and ? is not parallel but just 
comparable, the counterpart of an anchor in ? is not always found in ?. This is often the case when ? 
is incomplete, under construction, or written in a different style from that of ?. In such a case, our 
method proposes a destination article ?? without an anchor, and ? will be linked to ?? once ? is en-
larged to contain a term appropriate as the anchor for ??. 
3 Proposed Method 
The proposed method is divided into two steps; the preprocessing step for collecting possible anchors 
for all Wikipedia articles in a target language as well as estimating probabilities required in the suc-
ceeding step and the main step for transferring intra-language links in a source-language article ? to 
the target-language article ? linked to ? via an ILL. In this section, a triplet (?, ?, ??) denotes an intra-
language link from anchor ? in article ? to destination article ?? and, likewise, a triplet (?, ?, ??) does. 
Note that although an article can have two or more intra-language links from the same anchor at dif-
ferent positions in the text to the same destination article, they are treated as a single link. 
3.1 Preprocessing Step 
Collecting Possible Anchors for Wikipedia Articles 
The title of a Wikipedia article can be used as an anchor for the article. However, a title is often ac-
companied by a parenthesized note indicating the domain of the article to discriminate from other arti-
cles with the same title. The title ????? (???)? of an article that describes a car named Jaguar 
is an example; the parenthesized note ?(???)? discriminates the article from another article ???
???, which describes an animal belonging to the cat family. Such a title accompanied by a parenthe-
sized note rarely occurs in usual texts, and the title with the parenthesized note deleted is often marked 
as an anchor. Accordingly, we also regard a title with a parenthesized note deleted (e.g., ??????) 
as a possible anchor. Other terms, typically synonyms of the article title, are often used as anchors. 
Therefore, we collect terms that are actually used as anchors for each article from the entire Wikipedia. 
Finally, we threshold possible anchors by their keyphraseness to eliminate general words. The 
keyphraseness ?(?) of a term ? is defined as the probability that ? is used as an anchor in Wikipedia 
articles (Mihalcea and Csomai, 2007), i.e., 
?(?) =
|{?|??? . (?, ?, ??) ? ??}|
df(?)
, 
where ?? is a set consisting of all intra-language links in the target-language Wikipedia and df(?) is 
the number of Wikipedia articles in which ? occurs. 
In summary, a set of possible anchors A(??) are constructed for a target-language destination article 
?? as follows: 
A(??) = ({?????(??), ??????(??)} ? {?|??. (?, ?, ??) ? ??}) ? {?|?(?) ? ?}, 
where ?????(??) and  ??????(??) are ???s title with and without the parenthesized note, respectively, 
and ? is the threshold for the keyphraseness. 
Estimating Probabilities 
The following probabilities, which will be used to select one from among possible anchors for a desti-
nation article, are estimated from the entire Wikipedia. 
? The probability that the target-language anchor is ? on the condition that its source-language 
counterpart is ?, i.e., 
P(?|?) =
count(?, ?)
? count(?, ??)??
, 
1262
count(?, ?) = |{((?, ?), (?, ?))| 
??? . ??? . (?, ?, ??) ? ?? ? (?, ?, ??) ? ??
? (?, ?) ? ??? ? (?? , ??) ? ???
}| , 
where ?? is a set consisting of all intra-language links in the source-language Wikipedia, and 
??? is a set of all pairs of ILL-linked articles. 
? The probability that the anchor is ? on condition that the destination article is ??, i.e., 
P(?|??) =
|{?|(?, ?, ??) ? ??}|
|{?|???. (?, ??, ??) ? ??}|
 
? The probability that the destination article is ?? on condition that the anchor is ?, i.e., 
P(??|?) =
|{?| (?, ?, ??) ? ??}|
|{?|???
? . (?, ?, ??
? ) ? ??}|
 
3.2 Main Step 
Let ? and ? be source-language and target-language articles that are linked via an ILL, respectively. 
Intra-language links in ? are transferred to ? as follows: 
(i) For each source-language intra-language link (?, ?, ??), do (ii) to (v). 
(ii) If ?? has an ILL to an article in the target language, let ?? be the destination article of the ILL 
from ??. Otherwise, output ?NOT TRANSFERRED? and move to the next intra-language link. 
(iii) If A(??) is empty, output the transferred intra-language link (?, NULL, ??), which means that 
? should be linked to ?? but does not contain a term appropriate as the anchor, and move to 
the next intra-language link. 
(iv) For each possible anchor ? ? A(??), search the text of ? for ?. If found, let pos(?, ?) denote 
the position of its first occurrence in the text; otherwise, let pos(?, ?) = ?1. 
(v) If at least one possible anchor is found, choose the most appropriate one ??? according to an an-
chor priority score Score(?), i.e., 
??? = argmax
? s.t.  ??A(??)?pos(?,?)?0
Score(?). 
and output the transferred intra-language link (?, ???, ??). Otherwise, output the transferred in-
tra-language link (?, NULL, ??). 
We have the following five alternative anchor priority scores in step (v) above. 
? Anchor translation probability: Score1(?) = P(?|?). 
This score favors the anchor that occurs most frequently as counterpart to the source-language 
anchor. 
? Anchor probability: Score2(?) = P(?|??). 
This score favors the anchor by which the destination article is pointed most frequently. 
? Destination article likelihood: Score3(?) = P(??|?). 
This score favors the anchor that is most likely to point the destination article. 
? Spelling:  Score4(?) = 1 ? dist(?, ?????
?(??)) max{len(?), len(?????
?(??))}? , 
where dist(?, ??) is the Levenshtein distance between character strings ? and ?? (Levenshtein, 
1966), and len(?) is the length of character string ?. 
This score favors the anchor with the highest similarity to the article?s title without a parenthe-
sized note, which is the most representative term denoting the entity, concept, or topic described 
in the article. 
? Position: Score5(?) = 1 pos(?, ?)? . 
Note that in a Wikipedia article, among two or more occurrences of an important term, the first 
one tends to be marked as an anchor. 
1263
4 Experiment 
4.1 Experimental Settings 
We conducted an experiment on transferring 
intra-language links from the English edition 
to the Japanese edition of Wikipedia. 
Input Data 
The English edition of Wikipedia (2013-04-03 
dump), consisting of 4,241,324 articles, and 
the Japanese edition of Wikipedia (2013-03-28 
dump), consisting of 951,411 articles1, were 
used for the experiment. Intra-language links 
were extracted from each dump file, and ILLs 
were obtained from Wikidata (2013-03-28 
dump). Redirect pages were resolved prelimi-
narily, i.e., if the destination of an intra-
language link or ILL was a redirect page, the 
destination was replaced with an article pointed by the redirect page. 
From among a total of 366,358 pairs of English and Japanese articles linked by ILLs, 3,595 pairs 
were randomly selected as a test set. The remaining pairs were used as training data for constructing 
English and Japanese intra-language link sets, ?? and ??. The English articles in the test set contained 
179,963 intra-language links in total; these were input to the algorithm of the proposed method. 
Keyphraseness Threshold 
Limiting possible anchors to meaningful ones and gaining many links are in a trade-off relation ad-
justable by the keyphraseness threshold ?. In the experiment, ? was set to 0, 0.01, 0.05, and 0.1. 
Keyphraseness values of several anchors are listed in Table 1. Technical words (e.g., ?????
????????? ? Bayesian network) and uncommon proper names (e.g., ??????? ? Dic-
tionnaire Infernal) tend to have high keyphraseness values. Common words (e.g., ???? ? devil and 
???? ? conflict) and proper names (e.g., ???? ? Paris and ?????? ? Nicholas), especially 
identical to a general noun, have middle or low values according to their commonness. Although some 
functional words (e.g., ???? ? from) may be included in possible anchors for the Wikipedia articles 
of their homographic content words (e.g., ???? ? Yori (kana)), they naturally have extremely low 
values. By setting ? to a value slightly greater than zero, functional words could be removed from pos-
sible anchors.  
Comparison of Anchor Priority Scores 
To determine the most effective anchor priority score, the accuracy of anchors selected according to 
each score was evaluated, assuming the existing intra-language links in the original Japanese articles 
as gold standard. That is, anchor accuracy Acc is defined as the percentage of originally pointed desti-
nation articles for which correct anchors were selected, i.e.,  
Acc =
|?????? ? ???????|
|{(?, ?, ??) ? ??????|???. (?, ??, ??) ? ???????}|
 , 
where ?????? is a set consisting of all transferred intra-language links and ??????? is the gold stand-
ard intra-language link set. Table 2 lists the anchor accuracies for each anchor priority score and each 
?. Anchor translation probability exhibited the best results and, therefore, we adopted anchor transla-
tion probability as the anchor priority score. 
                                                 
1 Redirect pages and articles with no intra-language links were not included in these counts. 
Anchor English 
translation 
Keyphrase-
ness 
??????
?????? 
Bayesian 
network 
1 
????? 
Dictionnaire 
Infernal 
0.810 
??? demonology 0.678 
?? Paris 0.574 
???? occult 0.304 
?? devil 0.135 
???? Nicholas 0.039 
?? conflict 0.001 
?? half 7.8 ? 10?5  
?? Yori (kana) 4.4 ? 10?6 
Table 1. Example of keyphraseness values. 
1264
4.2 Experimental Results 
We inputted 179,963 English intra-language links to the algorithm of the proposed method and classi-
fied them into the following five classes. Examples of each class, except class B, are given in Figure 2, 
which is an excerpt from the results for the pair of English article ?Jacques Collin de Plancy? and Jap-
anese article ????????????.?  
A. Transferred to a Japanese intra-language link in the gold standard (bold underline in Figure 2) 
B. Transferred to a Japanese intra-language link whose anchor is not the same as the gold standard 
link to the same destination article 
C. Transferred to a Japanese intra-language link not in the gold standard (double underline in Fig-
ure 2) 
D. Transferred to a Japanese intra-language link without anchor (wavy underline in Figure 2) 
E. Not transferred to a Japanese intra-language link  (dashed underline in Figure 2) 
Table 3 lists the numbers of English intra-language links per class. The proposed method added 
many new intra-language links to Wikipedia articles. Since the total number of existing Japanese intra-
language links in the test-set articles was ? = 161,940, the increase rate of Japanese intra-language 
links was 100(? + ?) ?? = 100(13,916 + 52,275) 161,940? = 40.9%  (? = 0 ). When new links 
without anchors were excluded, the increase rate was 100? ?? = 100 ? 13,916 161,940? = 8.6% 
(? = 0). 
The anchor accuracy of existing links was 100? (? + ?) =? 100 ? 31,770/(31,770 + 1,219) =
96.3% (? = 0). Anchor accuracy of new intra-language links could not be calculated because of the 
unavailability of gold standard data. However, the proposed method specifies the anchor ? for destina-
tion article ?? only when possible anchors for it is found in the target-language article ?. The specified 
anchor ? is likely to be the counterpart of source-language anchor ? pointing to ?? that is the source-
language counterpart of ??, regardless of whether ? already points to ?? or not. Thus, the anchor accu-
racy of new links should be similar to that of existing links. 
Among the ? = 179,963 input English intra-language links, 100? ?? = 100 ? 52,275 179,963? =
29.0% (? = 0) were transferred to Japanese intra-language links with the anchor unspecified. This 
was because different language articles contain different contents even though they are linked via an 
ILL. The anchor-unspecified links are put in the ?????? sections (?See also? sections) of target-
language articles, and Wikipedia authors are expected to enlarge or revise the articles so that these an-
chor-unspecified links can be converted to anchor-specified links. Additionally, among the ? =
179,963  input English intra-language links, 100? ?? = 100 ? 80,783 179,963? = 44.9%  were not 
transferred to Japanese intra-language links. We assumed this was mainly due to missing Japanese 
articles. Note that the total number of Japanese articles is less than one-fourth that of English articles. 
The percentage of not-transferred links will decrease with the growing number of Japanese articles. 
Anchor priority score Anchor accuracy (%) 
? = 0 ? = 0.01 ? = 0.05 ? = 0.1 
Anchor translation probability 96.3 93.9 93.0 92.0 
Anchor probability 95.6 93.3 92.4 91.4 
Destination article likelihood 90.7 90.8 91.5 91.3 
Spelling 95.1 93.1 92.5 91.8 
Position 88.2 87.3 87.9 87.6 
Table 2. Anchor accuracy. 
1265
4.3 Additional Comments on Experimental Results 
Among alternative anchor priority scores, anchor translation probability seems most effective because 
this is a posterior probability of the target-language counterpart to the source-language anchor. Anchor 
probability is also useful because this is a posterior probability of the anchor for the destination. High-
er accuracy with spelling score indicates that Wikipedia editors tend to use the title of the destination 
as an anchor. This may be caused by manually specifying the anchor and destination independently. 
Contrary to expectations that the first occurrence likely becomes an anchor, position score exhibited 
the worst results. More detailed analysis of the context in which a term tends to be selected as an an-
chor is necessary. 
Table 2 shows that the anchor probability, unexpectedly, decreases with a rise of the keyphraseness 
threshold. It was caused by articles that have only one possible anchor with keyphraseness value be-
 Transfer result Number (percentage) 
Desti-
nation 
Anchor ? = 0 ? = 0.01 ? = 0.05 ? = 0.1 
A Existing Correct 31,770 (17.7%) 30,951 (17.2%) 30,661 (17.0%) 30,298 (16.8%) 
B Incorrect 1,219   (0.7%) 2,025   (1.1%) 2,298   (1.3%) 2,625   (1.5%) 
C New Found 13,916   (7.7%) 12,812   (7.1%) 11,421   (6.3%) 10,335   (5.7%) 
D Not found 52,275 (29.0%) 53,392 (29.7%) 54,800 (30.5%) 55,922 (31.1%) 
E Not transferred 80,783 (44.9%) 
Table 3. English intra-language links classified according to results. 
Figure 2. Example results of transferring intra-language links. 
Jacques Collin de Plancy 
 
Jacques Albin Simon Collin de Plancy 
(Plancy-l'Abbaye, 28 January 1793 ?Paris, 
1881) was a French occultist, demonologist 
and writer; he published several works on 
occultism and demonology.[1][2] 
 
He was born Jacques Albin Simon Collin on 
28 (in some sources 30) January 1793 in 
Plancy (presently Plancy-l'Abbaye) son of 
Edme-Aubin Collin and Marie-Anne Danton, 
sister of Georges-Jacques Danton who was 
executed the year after Jacques was born.[3] 
He later added the aristocratic "de Plancy" 
himself - an addition which would later cause 
accusations against his son in his career as a 
diplomat. He was a free-thinker influenced by 
Voltaire. He worked as a printer and publish-
er in Plancy-l'Abbaye and Paris. Between 
1830 and 1837, he resided in Brussels, and 
then in the Netherlands, before he returned to 
France after having converted to the Catholic 
religion. 
? 
In 1818 his best known work, Dictionnaire 
Infernal, was published. 
? 
??????????? 
 
????????????J. Collin de 
Plancy, 1794?????? 1793?? ? 
1881????? 1887???[1]????
19???????????????? 
? 
?????????????????? 
     ?Paris? 
???????1818?????????
????????????????? 
          ?Dictionnaire Infernal? 
??????????????? ??
??????????????????
?????????????????? 
                         ?occult? 
???? 3,799??????????  
? 
??????????????????
?????????????????? 
       ?demonology? 
?????????? 
? 
????  ?See also? 
??????   ?Brussels? 
??????   ?Voltaire? 
1266
low the threshold (e.g., ??? ? station). When the threshold was set high, the possible anchor set for 
such an article became empty and, as a results, the algorithm failed to reproduce the existing links to it. 
In this experiment, we transferred English links onto Japanese articles. Since the English edition of 
Wikipedia is richer than Japanese, it has been assumed that an English-to-Japanese direction is more 
effective than the inverse. However, among the 179,963 links in English and 161,940 links in Japanese 
extracted from the test set of English-Japanese article pairs, only 32,989 links are paired with their 
counterparts and others do not have counterparts. This fact indicates that a Japanese-to-English trans-
fer of links is also useful for enriching English articles. It also leads a low anchor recall, which is the 
percentage of correct links among existing links: 100? ?? = 100 ? 31,770 161,940? = 19.6% (? =
0). Combining usual wikification techniques should help improve the anchor recall. 
5 Discussion 
We now discuss two future directions, an extension to multiple language combination and a variation 
for inappropriate intra-language link detection. 
   The proposed method can be straightforwardly extended to three or more language combinations: 
Even if two source articles in two different languages are handled separately, the target article would 
be more enriched with the union of two transferred link sets. While this contributes to increasing the 
coverage of links, the reliability of links can also be improved by taking the intersection of the two 
transferred link sets. A more sophisticated combination of multiple source languages is a further prob-
lem. 
In the experiment, existing links were used as the gold standard for evaluation, despite the fact that 
they are not always appropriate because they are manually created by unspecified contributors. For 
example, there is a biology-related article containing an anchor ?translation? linking to an article 
?Translation? describing language translation, not to another article ?Translation (biology).? Such an 
incorrect intra-language link may be detected using a similar method as the proposed one. In the above 
example, suppose the Japanese counterpart article contains an anchor ???? linking to an article ??
? (???).? Two anchors ?translation? and ???? correspond to each other but their destination 
articles are not linked via an ILL. This inconsistency may be evidence for an inappropriate intra-
language link. Note that which of the English and Japanese links is inappropriate cannot be easily de-
termined. How to estimate the appropriateness of intra-language links is a problem to be solved. 
6 Related Work 
Wikification, which aims at linking mainly non-Wikipedia articles to Wikipedia articles, can be natu-
rally applied to linking between Wikipedia articles. There has been much research on wikification, 
most of which focused on disambiguation of destination articles (Milne and Witten, 2008a; Fogarolli, 
2009; Ratinov et al., 2011). Determining an appropriate destination article for an anchor term is a spe-
cial case of WSD. Although a variety of ideas for WSD have been adapted to wikification, their per-
formance is not satisfactory and there is room for further improvement. Another important issue with 
wikification is anchor selection, although most literature on wikification avoids the issue by selecting 
every term that is used as an anchor in any Wikipedia article. Anchor selection is a keyword extraction 
problem, which has been tackled using syntactic, statistical, and/or machine learning techniques but 
remains room for further improvement (Jacquemin and Bourigault, 2003). It should be added that our 
proposed method avoids both disambiguation and anchor selection problems by exploiting link infor-
mation in another language edition of Wikipedia. 
Adafre and de Rijke (2005) proposed a method for finding ?missing intra-language links? in a Wik-
ipedia article by assuming that an intra-language link represents the relatedness between concepts de-
scribed by the linked articles. Their method adds intra-language links to an article by using articles 
with similar link structures as that of the article in question. Similar methods that use the Wikipedia?s 
link structures as a semantic network have been proposed for entity linking (Milne and Witten, 2008b; 
Fogarolli, 2009; Ratinov et al., 2011). These still remain monolingual methods; the availability of oth-
er language editions cannot be assumed. 
A bilingual approach to improving quality of Wikipedia articles has also been studied. Sorg and 
Cimiano (2008) proposed a method for finding new ILLs by using a classifier whose features include 
1267
the number of ILLs between articles pointed by an article in one language and those pointed by an ar-
ticle in another language. Wang et al. (2013) improved the classifier by extending the intra-language 
links to increase the number of features. Both methods and our proposed method exploit the compara-
bility between intra-language links in different language editions. However, while the former find new 
ILLs, the latter finds new intra-language links. 
7 Conclusion 
We proposed a method for enriching intra-language links in Wikipedia articles. It transfers intra-
language links between a pair of different language articles linked by an inter-language link through 
the following two steps: first, determine destination articles to which the target-language article should 
be linked by following a source-language intra-language link and an ILL successively from each of the 
anchors in the source-language article; second, determine an anchor for each of the destination articles 
by searching the target-language article for possible anchors and selecting the most appropriate one 
according to the anchor translation probability criterion if two or more possible anchors are found. Un-
like usual wikification methods, our method avoids anchor selection and disambiguation problems by 
exploiting the comparability of intra-language links between different language editions of Wikipedia. 
   We conducted an experiment of transferring intra-language links from the English edition to the Jap-
anese edition to evaluate the effectiveness of our method. The method increased the number of intra-
language links in Japanese articles by 40.9%, and the accuracy of anchors selected was estimated to be 
96.3%. Future work includes an extension to multiple language combination and a variation for inap-
propriate intra-language link detection.  
References 
Adafre, Sisay Fissaha and Maarten de Rijke. 2005. Discovering missing links in Wikipedia. In Proceedings of 
the 3rd International Workshop on Link Discovery: Issues, Approaches and Applications, pages 90?97. 
Fogarolli, Angela. 2009. Word sense disambiguation based on Wikipedia link structure. In Proceedings of 2009 
IEEE International Conference on Semantic Computing, pages 77?82. 
Gale, William A., Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of 
HLT ?91 Workshop on Speech and Natural Language, pages 233?237. 
Jacquemin, Christian and Didier Bourigault. 2003. Term extraction and automatic indexing. In Ruslan Mitkov 
(Ed.), The Oxford Handbook of Computational Linguistics, pages 599?615. Oxford University Press. 
Levenshtein, Vladimir I. 1966. Binary codes capable of correcting deletions, insertions, and reversals. 
Cybernetics and Control Theory, 10(8):707?710. 
Mihalcea, Rada and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In 
Proceedings of the 16th ACM Conference on Information and Knowledge Management, pages 233?242. 
Milne, David and Ian H. Witten. 2008a. Learning to link with Wikipedia. In Proceedings of the 17th ACM 
Conference on Information and Knowledge Management, pages 509?518. 
Milne, David and Ian H. Witten. 2008b. An effective, low-cost measure of semantic relatedness obtained from 
Wikipedia links. In Proceedings of the Wikipedia and AI Workshop of AAAI, pages 25?30. 
Ratinov, Lev, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for 
disambiguation to Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for 
Computational Linguistics: Human Language Technologies, pages 1375?1384. 
Sorg, Philipp and Philipp Cimiano. 2008. Enriching the crosslingual link structure of Wikipedia - a 
classification-based approach. In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial 
Intelligence. 
Wang, Zhichun, Juanzi Li, and Jie Tang. 2013. Boosting cross-lingual knowledge linking via concept annotation. 
In Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pages 2733?2739. 
 
1268
Proceedings of the 8th Workshop on Asian Language Resources, pages 30?37,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Augmenting a Bilingual Lexicon with Information       
for Word Translation Disambiguation
Takashi Tsunakawa
Faculty of Informatics
Shizuoka University
tuna@inf.shizuoka.ac.jp
Hiroyuki Kaji
Faculty of Informatics
Shizuoka University
kaji@inf.shizuoka.ac.jp
Abstract
We describe a method for augmenting
a bilingual lexicon with additional in-
formation for selecting an appropriate 
translation word. For each word in the 
source language, we calculate a corre-
lation matrix of its association words 
versus its translation candidates. We 
estimate the degree of correlation by
using comparable corpora based on 
these assumptions: ?parallel word as-
sociations? and ?one sense per word 
association.? In our word translation 
disambiguation experiment, the results 
show that our method achieved 42% 
recall and 49% precision for Japa-
nese-English newspaper texts, and 45% 
recall and 76% precision for Chi-
nese-Japanese technical documents.
1 Introduction
The bilingual lexicon, or bilingual dictionary, 
is a fundamental linguistic resource for multi-
lingual natural language processing (NLP). For 
each word, multiword, or expression in the 
source language, the bilingual lexicon provides 
translation candidates representing the original 
meaning in the target language.
Selecting the right words for translation is a 
serious problem in almost all of multilingual 
NLP. One word in the source language almost 
always has two or more translation candidates 
in the target language by looking up them in 
the bilingual lexicon. Because each translation
candidate has a distinct meaning and property, 
we must be careful in selecting the appropriate 
translation candidate that has the same sense as 
the word inputted. This task is often called 
word translation disambiguation.
In this paper, we describe a method for add-
ing information for word translation disam-
biguation into the bilingual lexicon. Compara-
ble corpora can be used to determine which 
word associations suggest which translations 
of the word (Kaji and Morimoto, 2002). First, 
we extract word associations in each language 
corpus and align them by using a bilingual dic-
tionary. Then, we construct a word correlation 
matrix for each word in the source language. 
This correlation matrix works as information 
for word translation disambiguation.
We carried out word translation experiments 
on two settings: English-to-Japanese and Chi-
nese-to-Japanese. In the experiments, we tested 
Dice/Jaccard coefficients, pointwise mutual 
information, log-likelihood ratio, and Student?s
t-score as the association measures for extract-
ing word associations.
2 Constructing word correlation ma-
trices for word translation disam-
biguation
2.1 Outline of our method
In this section, we describe the method for 
calculating a word correlation matrix for each 
word in the source language. The correlation 
matrix for a word f consists of its association 
words and its translation candidates. Among 
the translation candidates, we choose the most 
acceptable one that is strongly suggested by its 
association words occurring around f.
We use two assumptions for this framework:
(i)  Parallel word associations:
Translations of words associated with 
each other in a language are also asso-
ciated with each other in another language 
30
(Rapp, 1995). For example, two English 
words ?tank? and ?soldier? are associated 
with each other and their Japanese trans-
lations ??? (sensha)? and ??? (hei-
shi)? are also associated with each other.
(ii)  One sense per word association:
A polysemous word exhibits only one 
sense of a word per word association 
(Yarowsky, 1993). For example, a poly-
semous word ?tank? exhibits the ?military
vehicle? sense of a word when it is asso-
ciated with ?soldier,? while it exhibits the 
?container for liquid or gas? sense when it 
is associated with ?gasoline.?
Under these assumptions, we determine which 
of the words associated with an input word 
suggests which of its translations by aligning 
word associations by using a bilingual dictio-
nary. Consider the associated English words 
(tank, soldier) and their Japanese translations 
(?? (sensha), ?? (heishi)). When we 
translate the word ?tank? into Japanese, the 
associated word ?soldier? helps us to translate 
it into ??? (sensha)?, not to translate it into 
???? (tanku)? which means ?a storage 
tank.?
This naive method seems to suffer from the 
following difficulties:
 A disparity in topical coverage between 
two corpora in two languages
 A shortage in the bilingual dictionary
 The existence of polysemous associated 
words that cannot determine the correct 
sense of the input word
For these difficulties, we use the tendency that 
the two words associated with a third word are 
likely to suggest the same sense of the third 
word when they are also associated with each 
other. For example, consider an English asso-
ciated word pair (tank, troop). The word ?troop?
cannot distinguish the different meanings be-
cause it can co-occur with the word ?tank? in 
both senses of the word. The third word ?sol-
dier,? which is associated with both ?tank? and 
?troop,? can suggest the translation ???
(sensha).?
The overview of our method is shown in
Figure 1. We first extract associated word pairs 
in the source and target languages from com-
parable corpora. Using a bilingual dictionary,
we obtain alignments of these word associa-
tions. Then, we iteratively calculate a correla-
tion matrix for each word in the source lan-
guage. Finally, we select the translation with 
the highest correlation from the translation 
candidates of the input word and the 
co-occurring words.
For each input word in the source language, 
we calculate correlation values between their 
translation candidates and their association 
words. The algorithm is shown in Figure 2.
In Algorithm 1, the initialization of correla-
tion values is based on word associations, 
where D is a set of word pairs in the bilingual 
dictionary, and Af and Ae are the sets of asso-
ciated word pairs. First, we retain associated 
words f?(i) when its translation e? exists and 
when e? is associated with e. In the iteration, 
the correlation values of associated words f?(i)
that suggest e(j) increase relatively by using 
association scores ((), ) and
Figure 1. Overview of our method.
31
((), ). In our experiments, we set the 
number of iterations Nr to 10.
2.2 Alternative association measures for 
extracting word associations
We extract co-occurring word pairs and calcu-
late their association scores. In this paper, we 
focus on some frequently used metrics for 
finding word associations based on their oc-
currence/co-occurrence frequencies.
Suppose that words x and y frequently 
co-occur. Let n1 and n2 be the occurrence fre-
quencies of x and y respectively, and let m be 
the frequency that x and y co-occur between w
content words. The parameter w is a window 
size that adjusts the range of co-occurrences.
Let N and M be the sum of occur-
rences/co-occurrences of all words/word pairs, 
respectively. The frequencies are summarized 
in Table 1.
The word association scores (, ) are de-
fined as follows:
 Dice coefficient (Smadja, 1993)
Dice(, ) =
2
	
 + 	
(1)
 Jaccard coefficient (Smadja et al, 1996)
Jaccard(, ) =

	
 + 	  
(2)
 Pointwise mutual information (pMI) 
(Church and Hanks, 1990)
pMI(, ) = log
/
(	
  )(	  )
(3)
 Log-likelihood ratio (LLR) (Dunning, 
1993)
LLR(, )
= 2logL(, 	
, )
+ logL(	  ,   	
, )
 logL(, 	
, 
)
 logL(	  ,   	
, ); (4)
logL(, 	, ) =
 log  + (	  ) log(1  ), (5)

 =

	

,  =
	  
  	

,  =
	

(6)
 Student?s t-score (TScore) (Church et al, 
1991)
TScore(, ) =
  	
	 

(7)
We calculate association scores for all pairs 
of words when their occurrence frequencies 
are not less than a threshold Tf and when their
x
occur
x not 
occur
Total
y
occur
m n2 ? m n2
y not 
occur
n1 ? m M ? n1
? n2 + m
N ? n2
Total n1 N ? n1 N
Table 1. Contingency matrix of occurrence 
frequencies.
Figure 2. Algorithm for calculating correlation 
matrices.
?
 ((), )  (, 