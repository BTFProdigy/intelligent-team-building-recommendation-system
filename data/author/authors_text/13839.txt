Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743?751,
Beijing, August 2010
Enhancing Morphological Alignment for Translating
Highly Inflected Languages ?
Minh-Thang Luong
School of Computing
National University of Singapore
luongmin@comp.nus.edu.sg
Min-Yen Kan
School of Computing
National University of Singapore
kanmy@comp.nus.edu.sg
Abstract
We propose an unsupervised approach uti-
lizing only raw corpora to enhance mor-
phological alignment involving highly in-
flected languages. Our method focuses on
closed-class morphemes, modeling their
influence on nearby words. Our language-
independent model recovers important
links missing in the IBM Model 4 align-
ment and demonstrates improved end-to-
end translations for English-Finnish and
English-Hungarian.
1 Introduction
Modern statistical machine translation (SMT)
systems, regardless of whether they are word-,
phrase- or syntax-based, typically use the word as
the atomic unit of translation. While this approach
works when translating between languages with
limited morphology such as English and French,
it has been found inadequate for morphologically-
rich languages like Arabic, Czech and Finnish
(Lee, 2004; Goldwater and McClosky, 2005;
Yang and Kirchhoff, 2006). As a result, a line
of SMT research has worked to incorporate mor-
phological analysis to gain access to information
encoded within individual words.
In a typical MT process, word aligned data is
fed as training data to create a translation model.
In cases where a highly inflected language is
involved, the current word-based alignment ap-
proaches produce low-quality alignment, as the
statistical correspondences between source and
?This work was supported by a National Research Foun-
dation grant ?Interactive Media Search? (grant # R-252-000-
325-279)
target words are diffused over many morpholog-
ical forms. This problem has a direct impact on
end translation quality.
Our work addresses this shortcoming by
proposing a morphologically sensitive approach
to word alignment for language pairs involving
a highly inflected language. In particular, our
method focuses on a set of closed-class mor-
phemes (CCMs), modeling their influence on
nearby words. With the model, we correct er-
roneous alignments in the initial IBM Model 4
runs and add new alignments, which results in im-
proved translation quality.
After reviewing related work, we give a case
study for morpheme alignment in Section 3. Sec-
tion 4 presents our four-step approach to construct
and incorporate our CCM alignment model into
the grow-diag process. Section 5 describes exper-
iments, while Section 6 analyzes the system mer-
its. We conclude with suggestions for future work.
2 Related Work
MT alignment has been an active research area.
One can categorize previous approaches into those
that use language-specific syntactic information
and those that do not. Syntactic parse trees
have been used to enhance alignment (Zhang and
Gildea, 2005; Cherry and Lin, 2007; DeNero
and Klein, 2007; Zhang et al, 2008; Haghighi et
al., 2009). With syntactic knowledge, modeling
long distance reordering is possible as the search
space is confined to plausible syntactic variants.
However, they generally require language-specific
tools and annotated data, making such approaches
infeasible for many languages. Works that follow
non-syntactic approaches, such as (Matusov et al,
743
i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614
-1 julistan2 euroopan3 parlamentin4 perjantaina5 136 joulukuuta7 19968 keskeytyneen9 istuntokauden10 uudelleen11 avatuksi12
Direct: 1-2 2-2 3-9 4-3 5-10 6-10 7-3 8-12 9-12 10-12 11-5 12-6 13-7 14-8Inverse: 1-1 2-2 8-3 9-4 10-5 12-6 13-7 14-8 10-9 10-10 10-11 10-12
(a) Gloss: -1 declare2 european3 parliament 4 on-friday5 136 december7 19968 adjourned9 session10 resumed11,12
i1 declare2 resume+3 d4 the5 session6 of7 the8 european9 parliament10 adjourn+11 ed12 on13 1314 december15 199616
- julist+ a+ n euroopa+ n parlament+ in perjantai+ n+ a 13 joulukuu+ ta 1996 keskeyty+ neen istunto+ kauden uude+ lle+ en avatuksi1   2      3   4        5          6          7           8        9          10 11 12      13        14   15         16           17        18            19        20       21  22       23Direct: 1-23 2-23 3-23 4-23 5-22 6-23 7-22 8-6 9-5 10-7 11-16 12-16 13-9 14-12 15-13 16-15Inverse: 1-1 2-2 2-3 5-4 9-5 8-6 10-7 10-8 11-9 0-10 7-11 14-12 15-13 15-14 16-15 11-16 11-17 11-18 11-19 11-20 11-21 0-22 11-23
(b)
Figure 1: Sample English-Finnish IBM Model 4 alignments: (a) word-level and (b) morpheme-level. Solid lines indicate
intersection alignments, while the exhaustive asymmetric alignments are listed below. In (a), translation glosses for Finnish
are given; the dash-dot line is the incorrect alignment. In (b), bolded texts are closed-class morphemes (CCM), while bolded
indices indicate alignments involving CCMs. The dotted lines are correct CCM alignments not found by IBM Model 4.
2004; Liang et al, 2006; Ganchev et al, 2008),
which aim to achieve symmetric word alignment
during training, though good in many cases, are
not designed to tackle highly inflected languages.
Our work differs from these by taking a middle
road. Instead of modifying the alignment algo-
rithm directly, we preprocess asymmetric align-
ments to improve the input to the symmetrizing
process later. Also, our approach does not make
use of specific language resources, relying only on
unsupervised morphological analysis.
3 A Case for Morpheme Alignment
The notion that morpheme based alignment would
be useful in highly inflected languages is intu-
itive. Morphological inflections might indicate
tense, gender or number that manifest as separate
words in largely uninflected languages. Capturing
these subword alignments can yield better word
alignments that otherwise would be missed.
Let us make this idea concrete with a case study
of the benefits of morpheme based alignment. We
show the intersecting alignments of an actual En-
glish (source) ? Finnish (target) sentence pair in
Figure 1, where (a) word-level and (b) morpheme-
level alignments are shown. The morpheme-
level alignment is produced by automatically seg-
menting words into morphemes and running IBM
Model 4 on the resulting token stream.
Intersection links (i.e., common to both direct
and inverse alignments) play an important role in
creating the final alignment (Och and Ney, 2004).
While there are several heuristics used in the sym-
metrizing process, the grow-diag(onal) process is
common and prevalent in many SMT systems,
such as Moses (Koehn et al, 2007). In the grow-
diag process, intersection links are used as seeds
to find other new alignments within their neigh-
borhood. The process continues iteratively, until
no further links can be added.
In our example, the morpheme-level intersec-
tion alignment is better as it has no misalignments
and adds new alignments. However it misses
some key links. In particular, the alignments of
closed-class morphemes (CCMs; later formally
defined) as indicated by the dotted lines in (b) are
overlooked in the IBM Model 4 alignment. This
difficulty in aligning CCMs is due to:
1. Occurrences of garbage-collector words
(Moore, 2004) that attract CCMs to align to
them. Examples of such links in (b) are 1?23
or 11?21 with the occurrences of rare words
adjourn+11 and avatuksi23. We further
characterize such errors in Section 6.1.
2. Ambiguity among CCMs of the same surface
that causes incorrect matchings. In (b), we
observe multiple occurrence of the and n
on the source and target sides respectively.
While the link 8?6 is correct, 5?4 is not as i1
should be aligned to n4 instead. To resolve
such ambiguity, context information should
be considered as detailed in Section 4.3.
The fact that rare words and multiple affixes
often occur in highly inflected languages exacer-
bates this problem, motivating our focus on im-
proving CCM alignment. Furthermore, having ac-
cess to the correct CCM alignments as illustrated
744
in Figure 1 guides the grow-diag process in find-
ing the remaining correct alignments. For exam-
ple, the addition of CCM links i1?n4 and d4?
lle21 helps to identify declare2?julist2
and resume3?avatuksi23 as admissible align-
ments, which would otherwise be missed.
4 Methodology
Our idea is to enrich the standard IBM Model 4
alignment by modeling closed-class morphemes
(CCMs) more carefully using global statistics and
context. We realize our idea by proposing a four-
step method. First, we take the input parallel cor-
pus and convert it into morphemes before training
the IBM Model 4 morpheme alignment. Second,
from the morpheme alignment, we induce auto-
matically bilingual CCM pairs. The core of our
approach is in the third and fourth steps. In Step 3,
we construct a CCM alignment model, and apply
it on the segmented input corpus to obtain an au-
tomatic CCM alignment. Finally, in Step 4, we in-
corporate the CCM alignment into the symmetriz-
ing process via our modified grow-diag process.
4.1 Step 1: Morphological Analysis
The first step presupposes morphologically seg-
mented input to compute the IBM Model 4 mor-
pheme alignment. Following Virpioja et al
(2007), we use Morfessor, an unsupervised an-
alyzer which learns morphological segmentation
from raw tokenized text (Creutz and Lagus, 2007).
The tool segments input words into labeled
morphemes: PRE (prefix), STM (stem), and SUF
(suffix). Multiple affixes can be proposed for
each word; word compounding is allowed as well,
e.g., uncarefully is analyzed as un/PRE+
care/STM+ ful/SUF+ ly/SUF. We append a
?+? sign to each nonfinal tag to distinguish word-
internal morphemes from word-final ones, e.g.,
?x/STM? and ?x/STM+? are considered different
tokens. The ?+? annotation enables the restoration
of the original words, a key point to enforce word
boundary constraints in our work later.
4.2 Step 2: Bilingual CCM Pairs
We observe that low and highly inflected lan-
guages, while intrinsically different, share more
en fi en fi en fi
the1 -n?1 in6 -ssa?15 me166 -ni?60-s2 -t?9 is7 on?2 me166 minun?282to3 -a?6 that8 etta??7 why168 siksi?187to3 maan91 that8 ettei?283 view172 mielta??162of4 -a4 we10 -mme?10 still181 viela??108of4 -en?5 we10 meida?n?52 where183 jossa?209of4 -sta?19 we10 me?113 same186 samaa?334and5 ja?3 we10 emme123 he187 ha?n?184and5 seka??122 we10 meilla??231 good189 hyva??321and5 eika?203 . . . . . . over-408 yli-?391
Table 1: English(en)-Finnish(fi) Bilingual CCM pairs
(N=128). Shown are the top 19 and last 10 of 168 bilingual
CCM pairs extracted. Subscript i indicates the ith most fre-
quent morpheme in each language. ? marks exact correspon-
dence linguistically, whereas ? suggests rough correspon-
dence w.r.t http://en.wiktionary.org/wiki/.
in common at the morpheme level. The many-
to-one relationships among words on both sides
is often captured better by one-to-one correspon-
dences among morphemes. We wish to model
such bilingual correspondence in terms of closed-
class morphemes (CCM), similar to Nguyen and
Vogel (2008)?s work that removes nonaligned af-
fixes during the alignment process. Let us now
formally define CCM and an associative measure
to gauge such correspondence.
Definition 1. Closed-class Morphemes (CCM)
are a fixed set of stems and affixes that ex-
hibit grammatical functions just like closed-class
words. In highly inflected languages, we observe
that grammatical meanings may be encoded in
morphological stems and affixes, rather than sep-
arate words. While we cannot formally identify
valid CCMs in a language-independent way (as
by definition they manifest language-dependent
grammatical functions), we can devise a good ap-
proximation. Following Setiawan et al (2007),
we induce the set of CCMs for a language as the
top N frequent stems together with all affixes1.
Definition 2. Bilingual Normalized PMI
(biPMI) is the averaged normalized PMI com-
puted on the asymmetric morpheme alignments.
Here, normalized PMI (Bouma, 2009), known to
be less biased towards low-frequency data, is de-
fined as: nPMI(x, y) = ln p(x,y)p(x)p(y))/- ln p(x, y),
where p(x), p(y), and p(x, y) follow definitions
in the standard PMI formula. In our case, we only
1Note that we employ length and vowel sequence heuris-
tics to filter out corpus-specific morphemes.
745
compute the scores for x, y being morphemes fre-
quently aligned in both asymmetric alignments.
Given these definitions, we now consider a pair
of source and target CCMs related and termed a
bilingual CCM pair (CCM pair, for short) if they
exhibit positive correlation in their occurrences
(i.e., positive nPMI2 and frequent cooccurrences).
We should note that relying on a hard thresh-
old of N as in (Setiawan et al, 2007) is brittle
as the CCM set varies in sizes across languages.
Our method is superior in the use of N as a start-
ing point only; the bilingual correspondence of the
two languages will ascertain the final CCM sets.
Take for example the en and fi CCM sets with
154 and 214 morphemes initially (each consist-
ing of N=128 stems). As morphemes not having
their counterparts in the other language are spu-
rious, we remove them by retaining only those in
the CCM pairs. This effectively reduces the re-
spective sizes to 91 and 114. At the same time,
these final CCMs cover a much larger range of top
frequent morphemes than N , up to 408 en and 391
fi morphemes, as evidenced in Table 1.
4.3 Step 3: The CCM Alignment Model
The goal of this model is to predict when appear-
ances of a CCM pair should be deemed as linking.
With an identified set of CCM pairs, we know
when source and target morphemes correspond.
However, in a sentence pair there can be many in-
stances of both the source and target morphemes.
In our example, the the?n pair corresponds to
definite nouns; there are two the and three -n in-
stances, yielding 2? 3=6 possible links.
Deciding which instances are aligned is a deci-
sion problem. To solve this, we inspect the IBM
Model 4 morpheme alignment to construct a CCM
alignment model. The CCM model labels whether
an instance of a CCM pair is deemed semantically
related (linked). We cast the modeling problem as
supervised learning, where we choose a maximum
entropy (ME) formulation (Berger et al, 1996).
We first discuss sample selection from the IBM
Model 4 morpheme alignment, and then give de-
tails on the features extracted. The processes de-
scribed below are done per sentence pair with fm1 ,
2nPMI has a bounded range of [?1, 1] with values 1 and
0 indicating perfect positive and no correlation, respectively.
en1 and U denoting the source, target sentences and
the union alignments, respectively.
Class labels. We base this on the initial IBM
Model 4 alignment to label each CCM pair in-
stance as a positive or negative example: Positive
examples are simply CCM pairs in U. To be pre-
cise, links j?i in U are positive examples if fj?ei
is a CCM pair. To find negative examples, we in-
ventory other potential links that share the same
lexical items with a positive one. That is, a link
j??i? not in U is a negative example, if a positive
link j?i such that fj = f ?j and ei = e?i exists.
We stress that our collection of positive exam-
ples contains high-precision but low-recall IBM
Model 4 links, which connect the reliable CCM
pairs identified before. The model then general-
izes from these samples to detect incorrect CCM
links and to recover the correct ones, enhancing
recall. We later detail this process in ?4.4.
Feature Set. Given a CCM pair instance, we
construct three feature types: lexical, monolin-
gual, and bilingual (See Table 2). These features
capture the global statistics and contexts of CCM
pairs to decide if they are true alignment links.
? Lexical features reflect the tendency of the
CCM pair being aligned to themselves. We use
biPMI, which aggregates the global alignment
statistics, to determine how likely source and tar-
get CCMs are associated with each other.
? Monolingual context features measure the
association among tokens of the same language,
capturing what other stems and affixes co-occur
with the source/target CCM:
1. within the same word (intra). The aim is to
disambiguate affixes as necessary in highly
inflected languages where same stems could
generate different roles or meanings.
2. outside the CCM?s word boundary (inter).
This potentially capture cues such as tense,
or number agreement. For example, in En-
glish, the 3sg agreement marker on verbs -s
often co-occurs with nearby pronouns e.g.,
he, she, it; whereas the same marker on
nouns (-s), often appears with plural deter-
miners e.g., these, those, many.
To accomplish this, we compute two monolin-
gual nPMI scores in the same spirit as biPMI, but
using the morphologically segmented input from
746
Feature Description Examples
Lexical ? biPMI: None [?1, 0], Low (0, 1/3], Medium (1/3, 2/3], High (2/3, 1] pmid?lle=LowMonolingual Context ? Capture morpheme cooccurrence with the src/tgt CCMIntra ? Within the same word srcWd?lle=resume, tgtWd?lle=en, tgtWd?lle=uudeInter ? To the Left & Right, in different words srcLd?lle=i, srcRd?lle=the, tgtRd?lle=avatuksiBilingual context ? Capture neighbor links? cooccurrence with the CCM pair linkbi0 ? Most descriptive, capturing in terms of surface forms only ? maybe sparse bi0d?lle=resume?avatuksibi1 ? Generalizes morphemes into relative locations (Left, Within, Right) bi1d?lle=W?avatuksi, bi1d?lle=resume?Rbi2 ? Most general, coupling token types (Close, Open) /w relative positions bi2d?lle=O?WR
Table 2: Maximum entropy feature set. Shown are feature types, descriptions and examples. Most examples are given for
the alignment d4?lle+21 of the same running example in ?3. Note that we only partially list the bilingual context features.
each language separately. Two morphemes are
?linked? if within a context window of wc words.
? Bilingual context features model cross-
lingual reordering, capturing the relationships be-
tween the CCM pair link and its neighbor3 links.
Consider a simple translation between an English
phrase of the form we ?verb? and the Finnish
one ?verb? -mme, where -mme is the 1pl verb
marker. We aim to capture movements such as
?the open-class morphemes on the right of we and
on the left of -mme are often aligned?. These will
function as evidence for the ME learner to align
the CCM pair (we, -mme). We encode the bilin-
gual context at three different granularities, from
most specific to most general ones (cf Table 2).
4.4 Step 4: Incorporate CCM Alignment
At test time, we apply the trained CCM alignment
model to all CCM pairs occurring in each sentence
pair to find CCM links. On our running exam-
ple in Figure 1, the CCM classifier tests 17 CCM
pairs, identifying 6 positive CCM links of which
4 are true positives (dotted lines in (b)).
Though mostly correct, we note that some of
the predicted links conflict: (d4?lle21, ed12?
neen17 and ed12?lle21) share alignment end-
points. Such sharing in CCM alignments is rare
and we believe should be disallowed. This moti-
vates us to resolve all CCM link conflicts before
incorporating them into the symmetrizing process.
Resolving link conflicts. As CCM pairs are
classified independently, they possess classifica-
tion probabilities which we use as evidence to re-
solve the conflicts. In our example, the classifica-
tion probabilities for (d4?lle21, ed12?neen17,
ed12?lle21) are (0.99, 0.93, 0.79) respectively.
We use a simple, ?best-first? greedy approach
3Within a context window of wc words as in monolingual.
to determine which links are kept and which are
dropped to satisfy our assumption. In our case,
we pick the most confident link, d4?lle21 with
probability 0.99. This precludes the incorrect link,
ed12?lle21, but admits the other correct one
ed12?neen17, probability 0.93. As a result, this
resolution successfully removes the incorrect link.
Modifying grow-diag. We incorporate the set
of conflict-resolved CCM links into the grow-diag
process. This step modifies the input alignments
as well as the growing process. U and I denote the
IBM Model 4 union and intersection alignments.
In our view, the resolved CCM links can serve
as a quality mark to ?upgrade? links before input
into the grow-diag process. We upgrade resolved
CCM links: (a) those ? U ? part of I , treating
them as alignment seeds; (b) those /? U ? part
of U , using them for exploration and growing. To
reduce spurious alignments, we discarded links in
U that conflict with the resolved CCM links.
In the usual grow-diag, links immediately adja-
cent to a seed link l are considered candidates to
be appended into the alignment seeds. While suit-
able for word-based alignment, we believe it is too
small a context when the input are morphemes.
For morpheme alignment, the candidate context
makes more sense in terms of word units. We thus
enforce word boundaries in our modified grow-
diag. We derive word boundaries for end points in
l using the morphological tags and the ?+? word-
end marker mentioned in ?4.1. Using such bound-
aries, we can then extend the grow-diag to con-
sider candidate links within a neighborhood of wg
words; hence, enhancing the candidate coverage.
5 Experiments
We use English-Finnish and English-Hungarian
data from past shared tasks (WPT05 and WMT09)
747
to validate our approach. Both Finnish and Hun-
garian are highly inflected languages, with numer-
ous verbal and nominal cases, exhibiting agree-
ment. Dataset statistics are given in Table 3.
en-fi # en-hu #
Train Europarl-v1 714K Europarl-v4 1,510K
LM Europarl-v1-fi 714K News-hu 4,209K
Dev wpt05-dev 2000 news-dev2009 2051
Test wpt05-test 2000 news-test2009 3027
Table 3: Dataset Statistics: the numbers of parallel sen-
tences for training, LM training, development and test sets.
We use the Moses SMT framework for our
work, creating both our CCM-based systems and
the baselines. In all systems built, we obtain
the IBM Model 4 alignment via GIZA++ (Och
and Ney, 2003). Results are reported using case-
insensitive BLEU (Papineni et al, 2001).
Baselines. We build two SMT baselines:
w-system: This is a standard phrase-based
SMT, which operates at the word level. The sys-
tem extracts phrases of maximum length 7 words,
and uses a 4-gram word-based LM.
wm-system: This baseline works at the word
level just like the w-system, but differs at the
alignment stage. Specifically, input to the IBM
Model 4 training is the morpheme-level corpus,
segmented by Morfessor and augmented with ?+?
to provide word-boundary information (?4.1). Us-
ing such information, we constrain the alignment
symmetrization to extract phrase pairs of 7 words
or less in length. The morpheme-based phrase ta-
ble is then mapped back to word forms. The pro-
cess continues identically as in the w-system.
CCM-based systems. Our CCM-based sys-
tems are similar in spirit to the wm system: train at
the morpheme, but decode at the word level. We
further enhance the wm-system at the alignment
stage. First, we train our CCM model based on
the initial IBM Model 4 morpheme alignment, and
apply it to the morpheme corpus to obtain CCM
alignment, which are input to our modified grow-
diag process. The CCM approach defines the set-
ting of three parameters: ?N , wc, wg? (Section 4).
Due to our resource constraints, we set N=128,
similar to (Setiawan et al, 2007), and wc=1 ex-
perimentally. We only focus on the choice of wg,
testing wg={1, 2} to explore the effect of enforc-
ing word boundaries in the grow-diag process.
5.1 English-Finnish results
We test the translation quality of both directions
(en-fi) and (fi-en). We present results in Table 4 for
7 systems, including: our baselines, three CCM-
based systems with word-boundary knowledge
wg={0, 1, 2} and two wm-systems wg={1, 2}.
Results in Table 4 show that our CCM approach
effectively improves the performance. Compared
to the wm-system, it chalks up a gain of 0.46
BLEU points for en-fi, and a larger improvement
of 0.93 points for the easier, reverse direction.
Further using word boundary knowledge in our
modified grow-diag process demonstrates that the
additional flexibility consistently enhances BLEU
for wg = 1, 2. We achieve the best performance
at wg = 2 with improvements of 0.67 and 1.22
BLEU points for en-fi and fi-en, respectively.
en-fi fi-en
w-system 14.58 23.56
wm-system 14.47 22.89
wm-system + CCM 14.93+0.46 23.82+0.93
wm-system + CCM + wg = 1 15.01 23.95
wm-system + CCM + wg = 2 15.14+0.67 24.11+1.22
wm-system + wg = 1 14.44 22.92
wm-system + wg = 2 14.28 23.01
(Ganchev, 2008) - Base 14.72 22.78
(Ganchev, 2008) - Postcat 14.74 23.43+0.65
(Yang, 2006) - Base N/A 22.0
(Yang, 2006) - Backoff N/A 22.3+0.3
Table 4: English/Finnish results. Shown are BLEU
scores (in %) with subscripts indicating absolute improve-
ments with respect to the wm-system baseline.
Interestingly, employing the word boundary
heuristic alone in the original grow-diag does not
yield any improvement for en-fi, and even worsens
as wg is enlarged (as seen in Rows 6?7). There
are only slight improvements for fi-en with larger
wg.This attests to the importance of combining the
CCM model and the modified grow-diag process.
Our best system outperforms the w-system
baseline by 0.56 BLEU points for en-fi, and yields
an improvement of 0.55 points for fi-en.
Compared to works experimenting en/fi trans-
lation, we note the two prominent ones by Yang
and Kirchhoff (2006) and recently by Ganchev
et al (2008). The former uses a simple back-off
method experimenting only fi-en, yielding an im-
provement of 0.3 BLEU points. Work in the op-
748
posite direction (en-fi) is rare, with the latter pa-
per extending the EM algorithm using posterior
constraints, but showing no improvement; for fi-
en, they demonstrate a gain of 0.65 points. Our
CCM method compares favorably against both ap-
proaches, which use the same datasets as ours.
5.2 English-Hungarian results
To validate our CCM method as language-
independent, we also perform preliminary exper-
iments on en-hu. Table 5 shows the results using
the same CCM setting and experimental schemes
as in en/fi. An improvement of 0.35 BLEU points
is shown using the CCM model. We further im-
prove by 0.44 points with word boundary wg=1,
but performance degrades for the larger window.
Due to time constraints, we leave experiments
for the reverse, easier direction as future work.
Though modest, the best improvement for en-hu
is statistical significant at p<0.01 according to
Collins? sign test (Collins et al, 2005).
System BLEU
w-system 9.63
wm-system 9.47
wm-system + CCM 9.82 +0.35
wm-system + CCM + wg = 1 9.91 +0.44
wm-system + CCM + wg = 2 9.87
Table 5: English/Hungarian results. Subscripts indicate
absolute improvements with respect to the wm-system.
We note that MT experiments for en/hu 4 are
very limited, especially for the en to hu direction.
Nova?k (2009) obtained an improvement of 0.22
BLEU with no distortion penalty; whereas Koehn
and Haddow (2009) enhanced by 0.5 points us-
ing monotone-at-punctuation reordering, mini-
mum Bayes risk and larger beam size decoding.
While not directly comparable in the exact set-
tings, these systems share the same data source
and splits similar to ours. In view of these com-
munity results, we conclude that our CCM model
does perform competitively in the en-hu task, and
indeed seems to be language independent.
6 Detailed Analysis
The macroscopic evaluation validates our ap-
proach as improving BLEU over both baselines,
4Hungarian was used in the ACL shared task 2008, 2009.
but how do the various components contribute?
We first analyze the effects of Step 4 in produc-
ing the CCM alignment, and then step backward
to examine the contribution of the different feature
classes in Step 3 towards the ME model.
6.1 Quality of CCM alignment
To evaluate the quality of the predicted CCM
alignment, we address the following questions:
Q1: What is the portion of CCM pairs being
misaligned in the IBM Model 4 alignment?
Q2: How does the CCM alignment differ from
the IBM Model 4 alignment?
Q3: To what extent do the new links introduced
by our CCM model address Q1?
Given that we do not have linguistic expertise in
Finnish or Hungarian, it is not possible to exhaus-
tively list all misaligned CCM pairs in the IBM
Model 4 alignment. As such, we need to find other
form of approximation in order to address Q1.
We observe that correct links that do not exist
in the original alignment could be entirely miss-
ing, or mistakenly aligned to neighboring words.
With morpheme input, we can also classify mis-
takes with respect to intra- or inter-word errors.
Figure 2 characterizes errors T1, T2 and T3, each
being a more severe error class than the previous.
Focusing on ei in the figure, links connecting ei
to fj? or fj?? are deemed T1 errors (misalignments
happen on one side). A T2 error aligns f ??j within
the same word, while a T3 error aligns it outside
the current word but still within its neighborhood.
This characterization is automatic, cheap and has
the advantage of being language-independent.
fj fj' fj??
1 word
T1T2T3
1 word
ei ei' ei??
Figure 2: Categorization of CCM missing links. Given
that a CCM pair link (fj?ei) is not present in the IBM Model
4, occurrences of any nearby link of the types T[1?3] can be
construed as evidence of a potential misalignment.
Statistics in Table 6(ii) answers Q1, suggest-
ing a fairly large number of missing CCM links:
3, 418K for en/fi and 6, 216K for en/hu, about
12.35% and 12.06% of the IBM Model 4 union
alignment respectively. We see that T1 errors con-
749
stitute the majority, a reasonable reflection of the
garbage- collector5 effect discussed in Section 3.
General (i) Missing CCM links (ii)en/fi en/hu en/fi en/hu
Direct 17,632K 34,312K T1 2,215K 3,487KInverse 18,681K 34,676K T2 358K 690K
D ? I 8,643K 17,441K T3 845K 2,039K
D ? I 27,670K 51,547K Total 3,418K 6,216K
Table 6: IBM Model 4 alignment statistics. (i) General
statistics. (ii) Potentially missing CCM links.
Q2 is addressed by the last column in Ta-
ble 7. Our CCM model produces about 11.98%
(1,035K/8,643K) new CCM links as compared to
the size of the IBM Model 4 intersection align-
ment for en/fi, and similarly, 9.52% for en/hu.
Orig. Resolved I U\I New
en/fi 5,299K 3,433K 1065K 1,332K 1,035K
en/hu 9,425K 6,558K 2,752K 2,146K 1,660K
Table 7: CCM vs IBM Model 4 alignments. Orig. and
Resolved give # CCM links predicted in Step 4 before and
after resolving conflicts. Also shown are the number of re-
solved links present in the Intersection, Union excluding I
(U\I) of the IBM Model 4 alignment and New CCM links.
Lastly, figures in Table 8 answer Q3, revealing
that for en/fi, 91.11% (943K/1,035K) of the new
CCM links effectively cover the missing CCM
alignments, recovering 27.59% (943K/3,418K) of
all missing CCM links. Our modified grow-diag
realizes a majority 76.56% (722K/943K) of these
links in the final alignment.
We obtain similar results in the en/hu pair for
link recovery, but a smaller percentage 22.59%
(330K/1,461K) are realized through the modified
symmetrization. This partially explains why im-
provements are modest for en/hu.
New CCM Links (i) Modified grow-diag (ii)
en/fi en/hu en/fi en/hu
T1 707K 1,002K 547K 228K
T2 108K 146K 79K 22K
T3 128K 313K 96K 80KTotal 943K 1,461K 722K 330K
Table 8: Quality of the newly introduced CCM links.
Shown are # new CCM links addressing the three error types
before (i) and after (ii) the modified grow-diag process.
6.2 Contributions of ME Feature Classes
We also evaluate the effectiveness the ME features
individually through ablation tests. For brevity,
5E.g., ei prefers f?j or f??j (garbage collectors) over fj .
we only examine the more difficult translation di-
rection, en to fi. Results in Table 9 suggest that
all our features are effective, and that removing
any feature class degrades performance. Balanc-
ing specificity and generality, bi1 is the most
influential feature in the bilingual context group.
For monolingual context, inter, which captures
larger monolingual context, outperforms intra.
The most important feature overall is pmi, which
captures global alignment preferences. As feature
groups, bilingual and monolingual context fea-
tures are important sources of information, as re-
moving them drastically decreases system perfor-
mance by 0.23 and 0.16 BLEU, respectively.
System BLEU
all (wm-system+CCM) 14.93
?bi2 14.90 ?intra 14.89
?bi1 14.84??0.09 ?pmi 14.81??0.12
?bi0 14.89 ?bi{2/1/0} 14.70??0.23
?inter 14.85 ?in{ter/tra} 14.77??0.16
Table 9: ME feature ablation tests for English-Finnish
experiments. ? mark results statistically significant at p <
0.05, differences are subscripted.
7 Conclusion and Future Work
In this work, we have proposed a language-
independent model that addresses morpheme
alignment problems involving highly inflected
languages. Our method is unsupervised, requiring
no language specific information or resources, yet
its improvement on BLEU is comparable to much
semantically richer, language-specific work. As
our approach deals only with input word align-
ment, any downstream modifications of the trans-
lation model also benefit.
As alignment is a central focus in this work, we
plan to extend our work over different and mul-
tiple input alignments. We also feel that better
methods for the incorporation of CCM alignments
is an area for improvement. In the en/hu pair, a
large proportion of discovered CCM links are dis-
carded, in favor of spurious links from the union
alignment. Automatic estimation of the correct-
ness of our CCM alignments may improve end
translation quality over our heuristic method.
750
References
Berger, Adam L., Stephen D. Della Pietra, and Vin-
cent J. D. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Bouma, Gerlof. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, Tu?bingen,
Gunter Narr Verlag.
Cherry, Colin and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In SSST.
Collins, Michael, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL.
Creutz, Mathias and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Trans. Speech Lang. Pro-
cess., 4(1):3.
DeNero, John and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In
ACL.
Ganchev, Kuzman, Joa?o V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
ACL-HLT.
Goldwater, Sharon and David McClosky. 2005. Im-
proving statistical mt through morphological analy-
sis. In HLT.
Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In ACL.
Koehn, Philipp and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In EACL.
Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL, Demonstration Session.
Lee, Young-Suk. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL.
Liang, Percy, Ben Taskar, and Dan Klein. 2006.
Alignment by agreement. In HLT-NAACL.
Matusov, Evgeny, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In COLING.
Moore, Robert C. 2004. Improving IBM word-
alignment model 1. In ACL.
Nguyen, Thuy Linh and Stephan Vogel. 2008.
Context-based Arabic morphological analysis for
machine translation. In CoNLL.
Nova?k, Attila. 2009. MorphoLogic?s submission for
the WMT 2009 shared task. In EACL.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Setiawan, Hendra, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
ACL.
Virpioja, Sami, Jaakko J. Vyrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In MT Summit
XI.
Yang, Mei and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In EACL.
Zhang, Hao and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In ACL.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL-HLT.
751
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 148?157,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Hybrid Morpheme-Word Representation
for Machine Translation of Morphologically Rich Languages?
Minh-Thang Luong Preslav Nakov Min-Yen Kan
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
{luongmin,nakov,kanmy}@comp.nus.edu.sg
Abstract
We propose a language-independent approach
for improving statistical machine translation
for morphologically rich languages using a
hybrid morpheme-word representation where
the basic unit of translation is the morpheme,
but word boundaries are respected at all stages
of the translation process. Our model extends
the classic phrase-based model by means
of (1) word boundary-aware morpheme-level
phrase extraction, (2) minimum error-rate
training for a morpheme-level translation
model using word-level BLEU, and (3) joint
scoring with morpheme- and word-level lan-
guage models. Further improvements are
achieved by combining our model with the
classic one. The evaluation on English to
Finnish using Europarl (714K sentence pairs;
15.5M English words) shows statistically sig-
nificant improvements over the classic model
based on BLEU and human judgments.
1 Introduction
The fast progress of statistical machine translation
(SMT) has boosted translation quality significantly.
While research keeps diversifying, the word remains
the atomic token-unit of translation. This is fine for
languages with limited morphology like English and
French, or no morphology at all like Chinese, but
it is inadequate for morphologically rich languages
like Arabic, Czech or Finnish (Lee, 2004; Goldwater
and McClosky, 2005; Yang and Kirchhoff, 2006).
?This research was sponsored in part by CSIDM (grant #
200805) and by a National Research Foundation grant entitled
?Interactive Media Search? (grant # R-252-000-325-279).
There has been a line of recent SMT research
that incorporates morphological analysis as part of
the translation process, thus providing access to the
information within the individual words. Unfortu-
nately, most of this work either relies on language-
specific tools, or only works for very small datasets.
Below we propose a language-independent ap-
proach to SMT of morphologically rich lan-
guages using a hybrid morpheme-word representa-
tion where the basic unit of translation is the mor-
pheme, but word boundaries are respected at all
stages of the translation process. We use unsuper-
vised morphological analysis and we incorporate its
output into the process of translation, as opposed to
relying on pre-processing and post-processing only
as has been done in previous work.
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work. Sections 3
and 4 present our morphological and phrase merging
enhancements. Section 5 describes our experiments,
and Section 6 analyzes the results. Finally, Section 7
concludes and suggests directions for future work.
2 Related Work
Most previous work on morphology-aware ap-
proaches relies heavily on language-specific tools,
e.g., the TreeTagger (Schmid, 1994) or the Buck-
walter Arabic Morphological Analyzer (Buckwal-
ter, 2004), which hampers their portability to
other languages. Moreover, the prevalent method
for incorporating morphological information is by
heuristically-driven pre- or post-processing. For
example, Sadat and Habash (2006) use different
combinations of Arabic pre-processing schemes
148
for Arabic-English SMT, whereas Oflazer and El-
Kahlout (2007) post-processes Turkish morpheme-
level translations by re-scoring n-best lists with a
word-based language model. These systems, how-
ever, do not attempt to incorporate their analysis as
part of the decoding process, but rather rely on mod-
els designed for word-token translation.
We should also note the importance of the trans-
lation direction: it is much harder to translate from a
morphologically poor to a morphologically rich lan-
guage, where morphological distinctions not present
in the source need to be generated in the target lan-
guage. Research in translating into morphologically
rich languages, has attracted interest for languages
like Arabic (Badr et al, 2008), Greek (Avramidis
and Koehn, 2008), Hungarian (Nova?k, 2009; Koehn
and Haddow, 2009), Russian (Toutanova et al,
2008), and Turkish (Oflazer and El-Kahlout, 2007).
These approaches, however, either only succeed in
enhancing the performance for small bi-texts (Badr
et al, 2008; Oflazer and El-Kahlout, 2007), or im-
prove only modestly for large bi-texts1.
3 Morphological Enhancements
We present a morphologically-enhanced version of
the classic phrase-based SMT model (Koehn et al,
2003). We use a hybrid morpheme-word representa-
tion where the basic unit of translation is the mor-
pheme, but word boundaries are respected at all
stages of the translation process. This is in con-
trast with previous work, where morphological en-
hancements are typically performed as pre-/post-
processing steps only.
In addition to changing the basic translation token
unit from a word to a morpheme, our model extends
the phrase-based SMT model with the following:
1. word boundary-aware morpheme-level phrase
extraction;
2. minimum error-rate training for a morpheme-
level model using word-level BLEU;
3. joint scoring with morpheme- and word-level
language models.
We first introduce our morpheme-level represen-
tation, and then describe our enhancements.
1Avramidis and Koehn (2008) improved by 0.15 BLEU over
a 18.05 English-Greek baseline; Toutanova et al (2008) im-
proved by 0.72 BLEU over a 36.00 English-Russian baseline.
3.1 Morphological Representation
Our morphological representation is based on the
output of an unsupervised morphological analyzer.
Following Virpioja et al (2007), we use Morfessor,
which is trained on raw tokenized text (Creutz and
Lagus, 2007). The tool segments words into mor-
phemes annotated with the following labels: PRE
(prefix), STM (stem), SUF (suffix). Multiple prefixes
and suffixes can be proposed for each word; word
compounding is allowed as well. The output can be
described by the following regular expression:
WORD = ( PRE* STM SUF* )+
For example, uncarefully is analyzed as
un/PRE+ care/STM+ ful/SUF+ ly/SUF
The above token sequence forms the input to our
system. We keep the PRE/STM/SUF tags as part
of the tokens, and distinguish between care/STM+
and care/STM. Note also that the ?+? sign is ap-
pended to each nonfinal tag so that we can distin-
guish word-internal from word-final morphemes.
3.2 Word Boundary-aware Phrase Extraction
The core translation structure of a phrase-based
SMT model is the phrase table, which is learned
from a bilingual parallel sentence-aligned corpus,
typically using the alignment template approach
(Och and Ney, 2004). It contains a set of bilingual
phrase pairs, each associated with five scores: for-
ward and backward phrase translation probabilities,
forward and backward lexicalized translation proba-
bilities, and a constant phrase penalty.
The maximum phrase length n is normally limited
to seven words; higher values of n increase the table
size exponentially without actually yielding perfor-
mance benefit (Koehn et al, 2003). However, things
are different when translating with morphemes, for
two reasons: (1) morpheme-token phrases of length
n can span less than n words; and (2) morpheme-
token phrases may only partially span words.
The first point means that morpheme-token
phrase pairs span fewer word tokens, and thus cover
a smaller context, which may result in fewer total
extracted pairs compared to a word-level approach.
Figure 1 shows a case where three Finnish words
consist of nine morphemes. Previously, this issue
was addressed by simply increasing the value of n
when using morphemes, which is of limited help.
149
SRC = theSTM newSTM , unPRE+ democraticSTM immigrationSTM policySTM
TGT = uusiSTM , ep?PRE+ demokraatSTM+ tSUF+ iSUF+ sSUF+ enSUF maahanmuuttoPRE+ politiikanSTM
(uusi=new  ,  ep?demokraattisen=undemocratic    maahanmuuttopolitiikan=immigration policy)
Figure 1: Example of English-Finnish bilingual fragments morphologically segmented by Morfessor. Solid links
represent IBM Model 4 alignments at the morpheme-token level. Translation glosses for Finnish are given below.
The second point is more interesting: morpheme-
level phrases may span words partially, making them
potentially usable in translating unknown inflected
forms of known source language words, but also
creates the danger of generating sequences of mor-
phemes that are not legal target language words.
For example, let us consider the phrase in Fig-
ure 1: unPRE+ democraticSTM. The original
algorithm will extract the spurious phrase epa?PRE+
demokraatSTM+ tSUF+ iSUF+ sSUF+, beside
the correct one that has enSUF appended at the
end. Such a spurious phrase does not generally help
in translating unknown inflected forms, especially
for morphologically-rich languages that feature mul-
tiple affixes, but negatively affects the translation
model in terms of complexity and quality.
We solve both problems by modifying the phrase-
pair extraction algorithm so that morpheme-token
phrases can extend longer than n, as long as they
span n words or less. We further require that
word boundaries be respected2, i.e., morpheme-
token phrases span a sequence of whole words. This
is a fair extension of the morpheme-token system
with respect to a word-token one since both are re-
stricted to span up to n word-tokens.
3.3 Morpheme-Token MERT Optimizing
Word-Token BLEU
Modern phrase-based SMT systems use a log-linear
model with the following typical feature functions:
language model probabilities, word penalty, distor-
tion cost, and the five parameters from the phrase ta-
ble. Their weights are set by optimizing BLEU score
(Papineni et al, 2001) directly using minimum error
rate training (MERT), as suggested by Och (2003).
In previous work, phrase-based SMT systems
using morpheme-token input/output naturally per-
2This means that we miss the opportunity to generate new
wordforms for known baseforms, but removes the problem of
proposing nonwords in the target language.
formed MERT at the morpheme-token level as well.
This is not optimal since the final expected system
output is a sequence of words, not morphemes. The
main danger is that optimizing a morpheme-token
BLEU score could lead to a suboptimal weight for
the word penalty feature function: this is because
the brevity penalty of BLEU is calculated with re-
spect to the number of morphemes, which may vary
for sentences with an identical number of words.
This motivates us to perform MERT at the word-
token level, although our input consists of mor-
phemes. In particular, for each iteration of MERT,
as soon as the decoder generates a morpheme-token
translation for a sentence, we convert it into a word-
token sequence, which is used to calculate BLEU.
We thus achieve MERT optimization at the word-
token level while translating a morpheme-token in-
put and generating a morpheme-token output.
3.4 Scoring with Twin Language Models
An SMT system that takes morpheme-token input
and generates morpheme-token output should natu-
rally use a morpheme-token language model (LM).
This has the advantage of alleviating the problem of
data sparseness, especially when translating into a
morphologically rich language, since the LM would
be able to handle some new unseen inflected forms
of known words. On the negative side, a morpheme-
token LM spans fewer word-tokens and thus has a
more limited word ?horizon? compared to one op-
erating at the word level. As with the maximum
phrase length, mechanically increasing the order of
the morpheme-token LM has a limited impact.
In order to address the issue in a more princi-
pled manner, we enhance our model with a second
LM that works at the word-token level. This LM is
used together with the morpheme-token LM, which
is achieved by using two separate feature functions
in the log-linear SMT model: one for each LM. We
further had to modify the Moses decoder so that
150
uusiSTM  , ep?PRE+ demokraatSTM+ tSUF+ iSUF+ sSUF+ enSUF maahanmuuttoPRE+ politiikanSTM 
? Score: ?sSUF+ enSUF maahanmuuttoPRE+?  ;  ?enSUF maahanmuuttoPRE+ politiikanSTM ?
? Concatenate: uusi , ep?demokraattisen maahanmuuttopolitiikan
? Score: ?, ep?demokraattisen maahanmuuttopolitiikan?
Previous hypotheses Current hypothesis
(i)
(ii)
(iii)
Figure 2: Scoring with twin LMs. Shown are: (i) The current state of the decoding process with the target phrases
covered by the current partial hypotheses. (ii, iii) Scoring with 3-gram morpheme-token and 3-gram word-token LMs,
respectively. For the word-token LM, the morpheme-token sequence is concatenated into word-tokens before scoring.
it can be enhanced with an appropriate word-token
?view? on the partial morpheme-level hypotheses3.
The interaction of the twin LMs is illustrated in
Figure 2. The word-token LM can capture much
longer phrases and more complete contexts such
as ?, epa?demokraattisen maahanmuuttopolitiikan?
compared to the morpheme-token LM.
Note that scoring with two LMs that see the out-
put sequence as different numbers of tokens is not
readily offered by the existing SMT decoders. For
example, the phrase-based model in Moses (Koehn
et al, 2007) allows scoring with multiple LMs, but
assumes they use the same token granularity, which
is useful for LMs trained on different monolingual
corpora, but cannot handle our case. While the fac-
tored translation model (Koehn and Hoang, 2007) in
Moses does allow scoring with models of different
granularity, e.g., lemma-token and word-token LMs,
it requires a 1:1 correspondence between the tokens
in the different factors, which clearly is not our case.
Note that scoring with twin LMs is conceptu-
ally superior to n-best re-scoring with a word-token
LM, e.g., (Oflazer and El-Kahlout, 2007), since it is
tightly integrated into decoding: it scores partial hy-
potheses and influenced the search process directly.
4 Enriching the Translation Model
Another general strategy for combining evidence
from the word-token and the morpheme-token rep-
resentations is to build two separate SMT systems
and then combine them. This can be done as a
post-processing system combination step; see (Chen
et al, 2009a) for an overview of such approaches.
3We use the term ?hypothesis? to collectively refer to the
following (Koehn, 2003): the source phrase covered, the cor-
responding target phrase, and most importantly, a reference to
the previous hypothesis that it extends.
However, for phrase-based SMT systems, it is theo-
retically more appealing to combine their phrase ta-
bles since this allows the translation models of both
systems to influence the hypothesis search directly.
We now describe our phrase table combination
approach. Note that it is orthogonal to the work pre-
sented in the previous section, which suggests com-
bining the two (which we will do in Section 5).
4.1 Building a Twin Translation Model
Figure 3 shows a general scheme of our twin trans-
lation model. First, we tokenize the input at differ-
ent granularities: (1) morpheme-token and (2) word-
token. We then build separate phrase tables (PT) for
the two inputs: a word-token PTw and a morpheme-
token PTm. Second, we re-tokenize PTw at the
morpheme level, thus obtaining a new phrase table
PTw?m, which is of the same granularity as PTm.
Finally, we merge PTw?m and PTm, and we input
the resulting phrase table to the decoder.
GIZA++
Decoding
Word alignment Morpheme alignment 
Word Morpheme
PTm
PTw?m
PTw
Morphological 
segmenta"on 
Phrase Extrac"on
PT merging
Phrase Extrac"on
GIZA++
Figure 3: Building a twin phrase table (PT). First, sep-
arate PTs are generated for different input granularities:
word-token and morpheme-token. Second, the word-
token PT is retokenized at the morpheme-token level. Fi-
nally, the two PTs are merged and used by the decoder.
151
4.2 Merging and Normalizing Phrase Tables
Below we first describe the two general phrase ta-
ble combination strategies used in previous work:
(1) direct merging using additional feature func-
tions, and (2) phrase table interpolation. We then
introduce our approach.
Add-feature methods. The first line of research
on phrase table merging is exemplified by (Niehues
et al, 2009; Chen et al, 2009b; Do et al, 2009;
Nakov and Ng, 2009). The idea is to select one of
the phrase tables as primary and to add to it all non-
duplicating phrase pairs from the second table to-
gether with their associated scores. For each entry,
features can be added to indicate its origin (whether
from the primary or from the secondary table). Later
in our experiments, we will refer to these baseline
methods as add-1 and add-2, depending on how
many additional features have been added. The val-
ues we used for these features in the baseline are
given in Section 5.4; their weights in the log-linear
model were set in the standard way using MERT.
Interpolation-based methods. A problem with
the above method is that the scores in the merged
phrase table that correspond to forward and back-
ward phrase translation probabilities, and forward
and backward lexicalized translation probabilities
can no longer be interpreted as probabilities since
they are not normalized any more. Theoretically,
this is not necessarily a problem since the log-linear
model used by the decoder does not assume that the
scores for the feature functions come from a normal-
ized probability distribution. While it is possible to
re-normalize the scores to convert them into prob-
abilities, this is rarely done; it also does not solve
the problem with the dropped scores for the dupli-
cated phrases. Instead, the conditional probabilities
in the two phrase tables are often interpolated di-
rectly, e.g., using linear interpolation. Representa-
tive work adopting this approach is (Wu and Wang,
2007). We refer to this method as interpolation.
Our method. The above phrase merging ap-
proaches have been proposed for phrase tables de-
rived from different sources. This is in contrast with
our twin translation scenario, where the morpheme-
token phrase tables are built from the same training
dataset; the main difference being that word align-
ments and phrase extraction were performed at the
word-token level for PTw?m and at the morpheme-
token level for PTm. Thus, we propose different
merging approaches for the phrase translation prob-
abilities and for the lexicalized probabilities.
In phrase-based SMT, phrase translation probabil-
ities are computed using maximum likelihood (ML)
estimation ?(f? |e?) = #(f? ,e?)?
f? #(f? ,e?)
, where #(f? , e?) is
the number of times the pair (f? , e?) is extracted from
the training dataset (Koehn et al, 2003). In order to
preserve the normalized ML estimations as much as
possible, we refrain from interpolation. Instead, we
use the raw counts for the two models #m(f? , e?) and
#w?m(f? , e?) directly as follows:
?(f? , e?) = #m(f? , e?) + #w?m(f? , e?)?
f? #m(f? , e?) +
?
f? #w?m(f? , e?)
For lexicalized translation probabilities, we would
like to use simple interpolation. However, we notice
that when a phrase pair belongs to only one of the
phrase tables, the corresponding lexicalized score
for the other table would be zero. This might cause
some good phrases to be penalized just because they
were not extracted in both tables, which we want to
prevent. We thus perform interpolation from PTm
and PTw according to the following formula:
lex(f? |e?) = ? ? lexm(f?m|e?m)
+ (1? ?)? lexw(f?w|e?w)
where the concatenation of f?m and e?m into word-
token sequences yields f?w and e?w, respectively.
If both (f?m, e?m) and (f?w, e?w) are present in PTm
and PTw, respectively, we have a simple interpola-
tion of their corresponding lexicalized scores lexm
and lexw. However, if one of them is missing, we
do not use a zero for its corresponding lexicalized
score, but use an estimate as follows.
For example, if only the entry (f?m, e?m) is present
in PTm, we first convert (f?m,e?m) into a word-token
pair (f?m?w,e?m?w), and then induce a correspond-
ing word alignment from the morpheme-token align-
ment of (f?m,e?m). We then estimate a lexicalized
phrase score using the original formula given in
(Koehn et al, 2003), where we plug this induced
word alignment and word-token lexical translation
probabilities estimated from the word-token dataset
The case when (f?w, e?w) is present in PTw, but
(f?m, e?m) is not, is solved similarly.
152
5 Experiments and Evaluation
5.1 Datasets
In our experiments, we use the English-Finnish data
from the 2005 shared task (Koehn and Monz, 2005),
which is split into training, development, and test
portions; see Table 1 for details. We further split
the training dataset into four subsets T1, T2, T3, and
T4 of sizes 40K, 80K, 160K, and 320K parallel sen-
tence pairs, which we use for studying the impact of
training data size on translation performance.
Sent. Avg. words Avg. morph.
en fi en fi
Train 714K 21.62 15.80 24.68 26.15
Dev 2K 29.33 20.99 33.40 34.94
Test 2K 28.98 20.72 33.10 34.47
Table 1: Dataset statistics. Shown are the number of
parallel sentences, and the average number of words and
Morfessor morphemes on the English and Finnish sides
of the training, development and test datasets.
5.2 Baseline Systems
We build two phrase-based baseline SMT systems,
both using Moses (Koehn et al, 2007):
w-system: works at the word-token level, extracts
phrases of up to seven words, and uses a 4-gram
word-token LM (as typical for phrase-based SMT);
m-system: works at the morpheme level, tok-
enized using Morfessor4 and augmented with ?+? as
described in Section 3.1.
Following Oflazer and El-Kahlout (2007) and Vir-
pioja et al (2007), we use phrases of up to 10
morpheme-tokens and a 5-gram morpheme-token
LM. None of the enhancements described previ-
ously is applied yet. After decoding, morphemes are
concatenated back to words using the ?+? markers.
To evaluate the translation quality, we compute
BLEU (Papineni et al, 2001) at the word-token
level. We further introduce a morpheme-token ver-
sion of BLEU, which we call m-BLEU: it first seg-
ments the system output and the reference trans-
lation into morpheme-tokens and then calculates a
BLEU score as usual. Table 2 shows the baseline re-
sults. We can see that the m-system achieves much
4We retrained Morfessor for Finnish/English on the
Finnish/English side of the training dataset.
w-system m-system
BLEU m-BLEU BLEU m-BLEU
T1 11.56 45.57 11.07 49.15
T2 12.95 48.63 12.68 53.78
T3 13.64 50.30 13.32 54.40
T4 14.20 50.85 13.57 54.70
Full 14.58 53.05 14.08 55.26
Table 2: Baseline system performance (on the test
dataset). Shown are word BLEU and morpheme m-
BLEU scores for the w-system and m-system.
higher m-BLEU scores, indicating that it may have
better morpheme coverage5. However, the m-system
is outperformed by the w-system on the classic word-
token BLEU, which means that it either does not
perform as well as the w-system or that word-token
BLEU is not capable of measuring the morpheme-
level improvements. We return to this question later.
5.3 Adding Morphological Enhancements
We now add our three morphological enhancements
from Section 3 to the baseline m-system:
phr (training) allow morpheme-token phrases to
get potentially longer than seven morpheme-tokens
as long as they cover no more than seven words;
tune (tuning) MERT for morpheme-token trans-
lations while optimizing word-token BLEU;
lm (decoding) scoring morpheme-token transla-
tion hypotheses with a 5-gram morpheme-token and
a 4-gram word-token LM.
The results are shown in Table 3 (ii). As we can
see, each of the three enhancements yields improve-
ments in BLEU score over the m-system, both for
small and for large training corpora. In terms of per-
formance ranking, tune achieves the best absolute
improvement of 0.66 BLEU points on T1 and of 0.47
points on the full dataset, followed by lm and phr.
Table 3 (iii) further shows that using phr and
lm together yields absolute improvements of 0.70
BLEU points on T1 and 0.50 points on the full train-
ing dataset. Further incorporating tune, however,
only helps when training on T1.
Overall, the morphological enhancements are on
par with the w-system baseline, and yield sizable im-
5Note that these morphemes were generated automatically
and thus many of them are erroneous.
153
System T1 (40K) Full (714K)
(i) w-system (w) 11.56 14.58
m-system (m) 11.07 14.08
(ii)
m+phr 11.44+0.37 14.43+0.35
m+tune 11.73+0.66 14.55+0.47
m+lm 11.58+0.51 14.53+0.45
(iii) m+phr+lm 11.77
+0.70 14.58+0.50
m+phr+lm+tune 11.90+0.83 14.39+0.31
Table 3: Impact of the morphological enhancements
(on test dataset). Shown are BLEU scores (in %) for
training on T1 and on the full dataset for (i) baselines,
(ii) enhancements individually, and (iii) combined. Su-
perscripts indicate absolute improvements w.r.t m-system.
provements over the m-system baseline: 0.83 BLEU
points on T1 and 0.50 on the full training dataset.
5.4 Combining Translation Tables
Finally, we investigate the effect of combining
phrase tables derived from a word-token and a
morpheme-token input, as described in Section 4.
We experiment with the following merging methods:
add-1: phrase table merging using one table as
primary and adding one extra feature6;
add-2: phrase table merging using one table as
primary and adding two extra features7;
interpolation: simple linear interpolation with
one parameter ?;
ourMethod: our interpolation-like merging
method described in Section 4.2.
Parameter tuning. We tune the parameters of the
above methods on the development dataset.
T1 (40K) Full (714K)
PTm is primary 11.99 13.45
PTw?m is primary 12.26 14.19
Table 4: Effect of selection of primary phrase table for
add-1 (on dev dataset): PTw?m, derived from a word-
token input, vs. PTm, from a morpheme-token input.
Shown is BLEU (in %) on T1 and the full training dataset.
For add-1 and add-2, we need to decide which
(PTw?m or PTm) phrase table should be consid-
6The feature values are e1, e2/3 or e1/3 (e=2.71828...);
when the phrase pair comes from both tables, from the primary
table only, and from the secondary table only, respectively.
7The feature values are (e1, e1), (e1, e0) or (e0, e1) when
the phrase pair comes from both tables, from the primary table
only, and from the secondary table only, respectively.
ered the primary table. Table 4 shows the results
when trying both strategies on add-1. As we can see,
using PTw?m as primary performs better on T1 and
on the full training dataset; thus, we will use it as
primary on the test dataset for add-1 and add-2.
For interpolation-based methods, we need to
choose a value for the interpolation parameters. Due
to time constraints, we use the same value for the
phrase translation probabilities and for the lexical-
ized probabilities, and we perform grid search for
? ? {0.3, 0.4, 0.5, 0.6, 0.7} using interpolate on the
full training dataset. As Table 5 shows, ? = 0.6
turns out to work best on the development dataset;
we will use this value in our experiments on the test
dataset both for interpolate and for ourMethod8.
? 0.3 0.4 0.5 0.6 0.7
BLEU 14.17 14.49 14.6 14.73 14.52
Table 5: Trying different values for interpolate (on dev
dataset). BLEU (in %) is for the full training dataset.
Evaluation on the test dataset. We integrate the
morphologically enhanced system m+phr+lm and
the word-token based w-system using the four merg-
ing methods above. The results for the full train-
ing dataset are shown in Table 6. As we can see,
add-1 and add-2 make little difference compared to
the m-system baseline. In contrast, interpolation and
ourMethod yield sizable absolute improvements of
0.55 and 0.74 BLEU points, respectively, over the
m-system; moreover, they outperform the w-system.
Merging methods Full (714K)
(i) m-system 14.08
w-system 14.58
(ii) add-1 14.25
+0.17
add-2 13.89?0.19
(iii) interpolation 14.63
+0.55
ourMethod 14.82+0.74
Table 6: Merging m+phr+lm and w-system (on test
dataset). BLEU (in %) is for the full training dataset. Su-
perscripts indicate performance gain/loss w.r.t m-system.
6 Discussion
Below we assess the significance of our results based
on micro-analysis and human judgments.
8Note that this might put ourMethod at disadvantage.
154
6.1 Translation Model Comparison
We first compare the following three phrase ta-
bles: PTm of m-system, maximum phrase length of
10 morpheme-tokens; PTw?m of w-system, maxi-
mum phrase length of 7 word-tokens, re-segmented
into morpheme-tokens; and PTm+phr ? morpheme-
token input using word boundary-aware phrase ex-
traction, maximum phrase length of 7 word-tokens.
Full (714K)
(i)
PTm 43.5M
PTw?m 28.9M
PTm+phr 22.5M
(ii) PTm+phr
?
PTm 21.4M
PTm+phr
?
PTw?m 10.7M
Table 7: Phrase table statistics. The number of phrase
pairs in (i) individual PTs and (ii) PT overlap, is shown.
PTm+phr versus PTm. Table 7 shows that
PTm+phr is about half the size of PTm. Still, as
Table 3 shows, m+phr outperforms the m-system.
Moreover, 95.07% (21.4M/22.5M) of the phrase
pairs in PTm+phr are also in PTm, which confirms
that boundary-aware phrase extraction selects good
phrase pairs from PTm to be retained in PTm+phr.
PTm+phr versus PTw?m. These two tables
are comparable in size: 22.5M and 28.9M pairs,
but their overlap is only 47.67% (10.7M/22.5M) of
PTm+phr. Thus, enriching the translation model
with PTw?m helps improve coverage.
6.2 Significance of the Results
Table 8 shows the performance of our system com-
pared to the two baselines: m-system and w-system.
We achieve an absolute improvement of 0.74 BLEU
points over the m-system, from which our system
evolved. This might look modest, but note that
the baseline BLEU is only 14.08, and thus the rel-
ative improvement is 5.6%, which is not trivial.
Furthermore, we outperform the w-system by 0.24
points (1.56% relative). Both improvements are sta-
tistically significant with p < 0.01, according to
Collins? sign test (Collins et al, 2005).
In terms of m-BLEU, we achieve an improvement
of 2.59 points over the w-system, which suggest our
system might be performing better than what stan-
dard BLEU suggests. Below we test this hypothesis
BLEU m-BLEU
ourSystem 14.82 55.64
m-system 14.08 55.26
w-system 14.58 53.05
Table 8: Our system vs. the two baselines (on the test
dataset): BLEU and m-BLEU scores (in %).
by means of micro-analysis and human evaluation.
Translation Proximity Match. We performed
automatic comparison based on corresponding
phrases between the translation output (out) and the
reference (ref), using the source (src) test dataset as
a pivot. The decoding log gave us the phrases used
to translate src to out, and we only needed to find
correspondences between src and ref, which we ac-
complished by appending the test dataset to training
and performing IBM Model 4 word alignments.
We then looked for phrase triples (src, out, ref ),
where there was a high character-level similarity be-
tween out and ref, measured using longest common
subsequence ratio with a threshold of 0.7, set ex-
perimentally. We extracted 16,262 triples: for 6,758
of them, the translations matched the references ex-
actly, while in the remaining triples, they were close
wordforms9. These numbers support the hypothesis
that our approach yields translations close to the ref-
erence wordforms but unjustly penalized by BLEU,
which only gives credit for exact word matches10.
Human Evaluation. We asked four native
Finnish speakers to evaluate 50 random test sen-
tences. Following (Callison-Burch et al, 2009), we
provided them with the source sentence, its refer-
ence translation, and the outputs of three SMT sys-
tems (m-system, w-system, and ourSystem), which
were shown in different order for each example and
were named sys1, sys2 and sys3 (by order of ap-
pearance). We asked for three pairwise judgments:
(i) sys1 vs. sys2, (ii) sys1 vs. sys3, and (iii) sys2 vs.
sys3. For each pair, a winner had to be designated;
ties were allowed. The results are shown in Table 10.
We can see that the judges consistently preferred
9Examples of such triples are (constitutional
structure, perustuslaillinen rakenne, perustuslaillisempi
rakenne) and (economic and social, taloudellisia ja
sosiaalisia, taloudellisten ja sosiaalisten)
10As a reference, the w-system yielded 15,673 triples, and
6,392 of them were exact matches. Compared to our system,
this means 589 triples and 366 exact matches less.
155
src: as a conservative , i am incredibly thrifty with taxpayers ? money .
ref: maltillisen kokoomuspuolueen edustajana suhtaudun erittain saastavaisesti veronmaksajien rahoihin .
our: konservatiivinen , olen erittain saastavaisesti veronmaksajien rahoja .
w : konservatiivinen , olen aarettoman tarkeaa kanssa veronmaksajien rahoja .
m : kuten konservatiivinen , olen erittain saastavaisesti veronmaksajien rahoja .
Comment: our  m  w. our uses better paraphrases, from which the correct meaning could be inferred. The part
?aarettoman tarkeaa kanssa? in w does not mention the ?thriftiness? and replaces it with ?important? (tarkeaa), which
is wrong. m introduces ?kuten?, which slightly alters the meaning towards ?like a conservative, ...?.
src: we were very constructive and we negotiated until the last minute of these talks in the hague .
ref: olimme erittain rakentavia ja neuvottelimme haagissa viime hetkeen saakka .
our: olemme olleet hyvin rakentavia ja olemme neuvotelleet viime hetkeen saakka naiden neuvottelujen haagissa .
w : olemme olleet hyvin rakentavia ja olemme neuvotelleet viime tippaan niin naiden neuvottelujen haagissa .
m : olimme erittain rakentavan ja neuvottelimme viime hetkeen saakka naiden neuvotteluiden haagissa .
Comment: our  m  w. In our, the meaning is very close to ref with only a minor difference in tense at the
beginning. m only gets the case wrong in ?rakentavan?, and the correct case is easily guessable. For w, the ?viime
tippaan? is in principle correct but somewhat colloquial, and the ?niin? is extra and somewhat confusing.
src: it would be a very dangerous situation if the europeans were to become logistically reliant on russia .
ref: olisi eritta?in vaarallinen tilanne , jos eurooppalaiset tulisivat logistisesti riippuvaisiksi vena?ja?sta? .
our: olisi eritta?in vaarallinen tilanne , jos eurooppalaiset tulee logistisesti riippuvaisia vena?ja?n .
w : se olisi eritta?in vaarallinen tilanne , jos eurooppalaisten tulisi logistically riippuvaisia vena?ja?n .
m : se olisi hyvin vaarallinen tilanne , jos eurooppalaiset haluavat tulla logistisesti riippuvaisia vena?ja?n .
Comment: our  w  m. our is almost correct except for the wrong inflections at the end. w is inferior since it
failed to translate ?logistically?. ?haluavat tulla? in m suggests that the Europeans would ?want to become logistically
dependent?, which is not the case. The ?se? (it), and ?hyvin? (a synonym of ?eritta?in?) are minor mistakes/differences.
Table 9: English-Finnish translation examples. Shown are the source (src), the reference (ref), and the transla-
tions of three systems (our, w, m). Text in bold indicates matches with respect to the ref, while italics show where a
system was judged inferior to the rest, as judged by native Finnish speakers.
(1) ourSystem to the m-system, (2) ourSystem to the
w-system, (3) w-system to the m-system. These pref-
erences are statistically significant, as found by the
sign test. Comparing to Table 8, we can see that
BLEU correlates with human judgments better than
m-BLEU; we plan to investigate this in future work.
our vs. m our vs. w w vs. m
Judge 1 25 18 19 12 21 19
Judge 2 24 16 19 15 25 14
Judge 3 27? 12 17 11 27? 15
Judge 4 25 20 26? 12 22 22
Total 101? 66 81? 50 95? 70
Table 10: Human judgments: ourSystem (our) vs. m-
system (m) vs. w-system (w). For each pair, we show
the number of times each system was judged better than
the other one, ignoring ties. Statistically significant dif-
ferences are marked with ? (p < 0.05) and ? (p < 0.01).
Finally, Table 9 shows some examples demon-
strating how our system improves over the w-system
and the m-system.
7 Conclusion and Future Work
In the quest towards a morphology-aware SMT that
only uses unannotated data, there are two key chal-
lenges: (1) to bring the performance of morpheme-
token systems to a level rivaling the standard word-
token ones, and (2) to incorporate morphological
analysis directly into the translation process.
This work satisfies the first challenge: we have
achieved statistically significant improvements in
BLEU for a large training dataset of 714K sentence
pairs and this was confirmed by human evaluation.
We think we have built a solid framework for the
second challenge, and we plan to extend it further.
Acknowledgements
We thank Joanna Bergstro?m-Lehtovirta (Helsinki
Institute for Information Technology), Katri Haveri-
nen (University of Turku and Turku Centre for Com-
puter Science), Veronika Laippala (University of
Turku), and Sampo Pyysalo (University of Tokyo)
for judging the Finnish translations.
156
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In ACL-HLT.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In ACL-HLT.
Tim Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer Version 2.0. Linguistic Data Consortium,
Philadelphia?.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
EACL.
Boxing Chen, Min Zhang, Haizhou Li, and Aiti Aw.
2009a. A comparative study of hypothesis alignment
and its improvement for machine translation system
combination. In ACL-IJCNLP.
Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi Zhang,
Sabine Hunsicker, Silke Theison, Christian Feder-
mann, and Hans Uszkoreit. 2009b. Combining multi-
engine translations with Moses. In EACL.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4(1):3.
Thi Ngoc Diep Do, Viet Bac Le, Brigitte Bigi, Laurent
Besacier, and Eric Castelli. 2009. Mining a compa-
rable text corpus for a Vietnamese-French statistical
machine translation system. In EACL.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
HLT.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared task
with reordering and speed improvements to Moses. In
EACL.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In EMNLP-CoNLL.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In WPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL, Demonstration Session.
Philipp Koehn. 2003. Noun phrase translation. Ph.D.
thesis, University of Southern California, Los Angeles,
CA, USA.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In HLT-NAACL.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In EMNLP.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and Alex
Waibel. 2009. The Universita?t Karlsruhe translation
system for the EACL-WMT 2009. In EACL.
Attila Nova?k. 2009. MorphoLogic?s submission for the
WMT 2009 shared task. In EACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kemal Oflazer and Ilknur El-Kahlout. 2007. Exploring
different representational units in English-to-Turkish
statistical machine translation. In StatMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Fatiha Sadat and Nizar Habash. 2006. Combination of
Arabic preprocessing schemes for statistical machine
translation. In ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In ACL-HLT.
Sami Virpioja, Jaakko J. Vyrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statisti-
cal machine translation based on morphs induced in an
unsupervised manner. In Machine Translation Summit
XI.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In EACL.
157
Parsing entire discourses as very long strings: Capturing topic continuity in
grounded language learning
Minh-Thang Luong
Department of Computer Science
Stanford University
Stanford, California
lmthang@stanford.edu
Michael C. Frank
Department of Psychology
Stanford University
Stanford, California
mcfrank@stanford.edu
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
Mark.Johnson@MQ.edu.au
Abstract
Grounded language learning, the task of map-
ping from natural language to a representation
of meaning, has attracted more and more in-
terest in recent years. In most work on this
topic, however, utterances in a conversation
are treated independently and discourse struc-
ture information is largely ignored. In the
context of language acquisition, this indepen-
dence assumption discards cues that are im-
portant to the learner, e.g., the fact that con-
secutive utterances are likely to share the same
referent (Frank et al, 2013). The current pa-
per describes an approach to the problem of
simultaneously modeling grounded language
at the sentence and discourse levels. We com-
bine ideas from parsing and grammar induc-
tion to produce a parser that can handle long
input strings with thousands of tokens, creat-
ing parse trees that represent full discourses.
By casting grounded language learning as a
grammatical inference task, we use our parser
to extend the work of Johnson et al (2012),
investigating the importance of discourse con-
tinuity in children?s language acquisition and
its interaction with social cues. Our model
boosts performance in a language acquisition
task and yields good discourse segmentations
compared with human annotators.
1 Introduction
Learning mappings between natural language (NL)
and meaning representations (MR) is an important
goal for both computational linguistics and cognitive
science. Accurately learning novel mappings is cru-
cial in grounded language understanding tasks and
such systems can suggest insights into the nature of
children language learning.
Two influential examples of grounded language
learning tasks are the sportscasting task, RoboCup,
where the NL is the set of running commentary and
the MR is the set of logical forms representing ac-
tions like kicking or passing (Chen and Mooney,
2008), and the cross-situational word-learning task,
where the NL is the caregiver?s utterances and the
MR is the set of objects present in the context
(Siskind, 1996; Yu and Ballard, 2007). Work
in these domains suggests that, based on the co-
occurrence between words and their referents in
context, it is possible to learn mappings between NL
and MR even under substantial ambiguity.
Nevertheless, contexts like RoboCup?where ev-
ery single utterance is grounded?are extremely
rare. Much more common are cases where a sin-
gle topic is introduced and then discussed at length
throughout a discourse. In a television news show,
for example, a topic might be introduced by present-
ing a relevant picture or video clip. Once the topic
is introduced, the anchors can discuss it by name
or even using a pronoun without showing a picture.
The discourse is grounded without having to ground
every utterance.
Moreover, although previous work has largely
treated utterance order as independent, the order of
utterances is critical in grounded discourse contexts:
if the order is scrambled, it can become impossible
to recover the topic. Supporting this idea, Frank et
al. (2013) found that topic continuity?the tendency
to talk about the same topic in multiple utterances
that are contiguous in time?is both prevalent and
informative for word learning. This paper examines
the importance of topic continuity through a gram-
matical inference problem. We build on Johnson et
al. (2012)?s work that used grammatical inference to
315
Transactions of the Association for Computational Linguistics, 1 (2013) 315?326. Action Editor: Mark Steedman.
Submitted 2/2013; Revised 6/2013; Published 7/2013. c?2013 Association for Computational Linguistics.

	
 









  	
 	   	 	 
	

	





	
	

	
	

	


	


	

	

	
	

	
	

	


	


	

	
	Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 166?169,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
WINGNUS: Keyphrase Extraction Utilizing Document Logical Structure
Thuy Dung Nguyen
Department of Computer Science
School of Computing
National University of Singapore
nguyen14@comp.nus.edu.sg
Minh-Thang Luong
Department of Computer Science
School of Computing
National University of Singapore
luongmin@comp.nus.edu.sg
Abstract
We present a system description of the
WINGNUS team work
1
for the SemEval-
2010 task #5 Automatic Keyphrase Ex-
traction from Scientific Articles. A key
feature of our system is that it utilizes an
inferred document logical structure in our
candidate identification process, to limit
the number of phrases in the candidate list,
while maintaining its coverage of impor-
tant phrases. Our top performing system
achieves an F
1
of 25.22% for the com-
bined keyphrases (author and reader as-
signed) in the final test data. We note that
the method we report here is novel and or-
thogonal from other systems, so it can be
combined with other techniques to poten-
tially achieve higher performance.
1 Introduction
Keyphrases are noun phrases (NPs) that capture
the primary topics of a document. While benefi-
cial for applications such as summarization, clus-
tering and indexing, only a minority of documents
have manually-assigned keyphrases, as it is a time-
consuming process. Automatic keyphrase genera-
tion is thus a focus for many researchers.
Most existing keyphrase extraction systems
view this task as a supervised classification task in
two stages: generating a list of candidates ? can-
didate identification; and using answer keyphrases
to distinguish true keyphrases ? candidate selec-
tion. The selection model uses a set of features that
capture the saliency of a phrase as a keyphrase.
A major challenge of the keyphrase extraction
task lies in the candidate identification process.
A narrow candidate list will overlook some true
1
This work was supported by a National Research Foun-
dation grant ?Interactive Media Search? (grant # R-252-000-
325-279).
keyphrases (favoring precision), whereas a broad
list will produce more errors and require more pro-
cessing in latter selection stage (favoring recall).
In our previous system (Nguyen and Kan,
2007), we made use of the document logical struc-
ture in the proposed features. The premise of this
earlier work was that keyphrases are distributed
non-uniformly in different logical sections of a pa-
per, favoring sections such as introduction, and
related work. We introduced features indicating
which sections a candidate occurrs in. For our
fielded system in this task (Kim et al, 2010), we
further leverage the document logical structure for
both candidate identification and selection stages.
Our contributions are as follows: 1) We suggest
the use of Google Scholar-based crawler to auto-
matically find PDF files to enhance logical struc-
ture extraction; 2) We provide a keyphrase distri-
bution study with respect to different logical struc-
tures; and 3) From the study result, we propose a
candidate identification approach that uses logical
structures to effectively limit the number of candi-
dates considered while ensuring good coverage.
2 Preprocessing
Although we have plain text for all test input, we
posit that logical structure recovery is much more
robust given the original richly-formatted docu-
ment (e.g., PDF), as font and formatting informa-
tion can be used for detection. As a bridge be-
tween plain text data provided by the organizer
and PDF input required to extract formatting fea-
tures, we first describe our Google Scholar-based
crawler to find PDFs given plain texts. We then
detail on the logical structure extraction process.
Google Scholar-based Paper Crawler
Our crawler
2
takes inputs as titles to query Google
Scholar (GS) by means of web scraping. It pro-
2
http://wing.comp.nus.edu.sg/
?
lmthang/GS/
166
cesses GS results and performs approximate ti-
tle matching using character-based Longest Com-
mon Subsequence similarity. Once a matching ti-
tle with high similarity score (> 0.7 experimen-
tally) is found, the crawler retrieves the list of
available PDFs, and starts downloading until one
is correctly stored. We enforce that PDFs accepted
should have the OCR texts closely match the pro-
vided plain texts in terms of lines and tokens.
In the keyphrase task, we approximate the title
inputs to our crawler by considering the first two
lines of each plain text provided. For 140 train
and 100 test input documents, the crawler down-
loaded 117 and 80 PDFs, of which 116 and 76 files
are correct, respectively. This yields an accept-
able level of performance in terms of (Precision,
Recall) of (99.15%, 82.86%) for train and (95%,
76%) for test data.
Logical Structure Extraction
Logical structure is defined as ?a hierarchy of log-
ical components, for example, titles, authors, affil-
iations, abstracts, sections, etc.? in (Mao et al,
2003). Operationalizing this definition, we em-
ploy an in-house software, called SectLabel (Lu-
ong et al, to appear), to obtain comprehensive
logical structure information for each document.
SectLabel classifies each text line in a scholarly
document with a semantic class (e.g., title, header,
bodyText). Header lines are furthered classified
into generic roles (e.g., abstract, intro, method).
A prominent feature of SectLabel is that it is
capable of utilizing rich information, such as font
format and spatial layout, from an optical char-
acter recognition (OCR) output if PDF files are
present
3
. In case PDFs are unavailable, SectLa-
bel still handles plain text based logical structure
discovery, but with degraded performance.
3 Candidate Phrase Identification
Phrase Distribution Study
We perform a study of keyphrase distribution on
the training data over different logical structures
(LSs) to understand the importance of each sec-
tion within documents. These LSs include: ti-
tle, headers, abstract, introduction (intro), related
work (rw), conclusion, and body text
4
(body).
3
We note that the PDFs have author assigned keyphrases
of the document, but we filtered this information before pass-
ing to our keyphrases system to ensure a fair test.
4
We utilize the comprehensive output of our logical struc-
ture system to filter out copyright, email, equation, figure,
We make a key observation that within a para-
graph, important phrases occur mostly in the first
n sentences. To validate our hypothesis, we con-
sider keyphrase distribution over body
n
, which is
the subset of all of the body LS, limited to the first
n sentences of each paragraph (n = 1, 2, 3 experi-
mentally).
Ath Rder Com Sent Den
title 142 175 251 122 2.06
headers 158 342 425 1,893 0.22
abstract 276 745 897 1,124 0.80
intro 335 984 1,166 4,338 0.27
rw 160 345 443 1,945 0.23
concl 227 488 616 1,869 0.33
body 398 1,175 1,411 39,179 0.04
full 465 1,720 1,994 50,512 0.04
body
1
333 839 1,035 11,280 0.09
body
2
366 980 1,197 20,024 0.06
body
3
382 1,042 1,269 26,163 0.05
fulltext 480 1,773 2,059 166,471 0.01
Table 1: Keyphrase distribution over different log-
ical structures computed from the 144 training
documents. The type counts of author-assigned
(ath), reader-assigned (rder) and combined (comb)
keyphrases are shown. Sent indicates the number
of sentences in each LS. The Den column gives the
density of keyphrases for each LS.
Results in Table 1 show that individual LSs
(title, headers, abstract, intro, rw, concl) con-
tain a high concentration (i.e., density > 0.2)
of keyphrases, with title and abstract having the
highest density, and intro being the most dominant
LS in terms of keyphrase count. With all these
LSs and body, we obtain the full setting, covering
1994/2059=96.84% of all keyphrases appearing in
the original text, fulltext, while effectively reduc-
ing the number of processed sentences by more
than two-thirds.
Considering only the first sentence of each para-
graph in the body text, body
1
, yields fair keyphrase
coverage of 1035/1411=73.35% relative to that of
fulltext. The number of lines to be processed is
much smaller, about a third, which validates our
aforementioned hypothesis.
Keyphrase Extraction
Results from the keyphrase distribution study mo-
tivates us to further explore the use of logical
structures (LS). The idea is to limit the search
scope of our candidate identification system while
maintaining coverage. We propose a new ap-
caption, footnote, and reference lines.
167
proach, which extracts candidates according to the
regular expression rules discussed in (Kim and
Kan, 2009). However, instead of using the whole
document text as input, we abridge the input text
at different levels from full to minimal.
Input Description Cand Com Recall
minimal
title + headers
30,702 1,312 63.72%
+ abs + intro
medium
min + rw
44,975 1,414 68.67%
+ conclusion
full
1
med + body
1
73,958 1,580 76.74%
full
2
med + body
2
90,624 1,635 79.41%
full
3
med + body
3
101,006 1,672 81.20%
full med + body 121,378 1,737 84.36%
fulltext original text 148,411 1,766 85.77%
Table 2: Different levels of abridged inputs com-
puted on the training data. Cand shows the
number of candidate keyphrases extracted for
each input type; Com gives the number of cor-
rect keyphrases appear as candidates; Recall is
computed with respect to the total number of
keyphrases in the original texts (2059).
Results in Table 2 show that we could gather
a recall of 63.72% when considering a signifi-
cantly abridged form of the input culled from ti-
tle, headers, abstract (abs) and introduction (in-
tro) ? minimal. Further adding related work (rw)
and conclusion ? medium ? enhances the recall by
4.95%. When adding only the first line of each
paragraph in the body text, we achieve a good re-
call of 76.74% while effectively reducing the num-
ber of candidate phrases to be process by a half
with respect to the fulltext input. Even though
full
2
, full
3
, and full show further improvements in
terms of recall, we opt to use full
1
in our experi-
mental runs, which trades off recall for less com-
putational complexity, which may influence down-
stream classification.
4 Candidate Phrase Selection
Following (Nguyen and Kan, 2007), we use the
Na??ve Bayes model implemented in Weka (Hall et
al., 2009) for candidate phrase selection. As dif-
ferent learning models have been discussed much
previous work, we just list the different features
with which we experimented with. Our features
5
are as follows (where n indicates a numeric fea-
ture; b, a boolean one):
5
Detailed feature definitions are described in (Nguyen and
Kan, 2007; Kim and Kan, 2009).
F1-F3 (n): TF?IDF, term frequency, term fre-
quency of substrings.
F4-F5 (n): First and last occurrences (word off-
set).
F6 (n): Length of phrases in words.
F7 (b): Typeface attribute (available when PDF
is present) ? Indicates if any part of the candidate
phrase has appeared in the document with bold
or italic format, a good hint for its relevance as
a keyphrase.
F8 (b): InTitle ? shows whether a phrase is also
part of the document title.
F9 (n): TitleOverlap ? the number of times
a phrase appears in the title of other scholarly
documents (obtained from a dump of the DBLP
database).
F10-F14 (b): Header, Abstract, Intro, RW,
Concl ? indicate whether a phrase appears in head-
ers, abstract, introduction, related work or conclu-
sion sections, respectively.
F15-F19 (n): HeaderF, AbstractF, IntroF, RWF,
ConclF - indicate the frequency of a phrase in
the headers, abstract, introduction, related work or
conclusion sections, respectively.
5 Experiments
5.1 Datasets
For this task (Kim et al, 2010), we are given two
datasets: train (144 docs) and test (100 docs) with
detailed answers for train. To tune our system,
we split the train dataset into train and validation
subsets: train
t
(104 docs) and train
v
(40 docs).
Once the best setting is derived from train
t
-train
v
,
we obtain the final model trained on the full data,
and apply it to the test set for the final results.
5.2 Evaluation
Our evaluation process is accomplished in two
stages: we first experiment different feature com-
binations by using the input types fulltext and full
1
.
We then fix the best feature set, and vary our dif-
ferent abridged inputs to find the optimal one.
Feature Combination
To evaluate the performance of individual features,
we define a base feature set, as F
1,4
, and measure
the performance of each feature added separately
to the base. Results in Table 3 have highlighted
the set of positive features, which is F
3,5,6,13,16
.
From the positive set F
3,5,6,13,16
, we tried dif-
ferent combinations for the two input types shown
168
System F Score System F Score
base 23.42% + F
11
23.42%
+ F
2
21.13% + F
12
23.42%
+ F
3
24.57% + F
13
23.75%
+ F
5
24.08% + F
14
22.28%
+ F
6
25.06% + F
15
22.11%
+ F
7
23.42% + F
16
23.59%
+ F
8
22.77% + F
17
22.60%
+ F
9
22.28% + F
18
23.26%
+ F
10
23.42% + F
19
21.95%
Table 3: Performance of individual features (on
fulltext) added separately to the base set F
1,4
.
in Table 4. The results indicate that while fulltext
obtains the best performance with F
3,6,5
added, us-
ing full
1
shows superior performance at 28.18% F
Score with F
3,6
added. Hence, we have identified
our best feature set as F
1,3,4,6
.
fulltext full
1
base (F
1,4
) 23.42% 22.60%
+ F
3,6
25.88% 28.18%
+ F
3,6,5
26.21% 26.21%
+ F
3,6,5,13
24.90% 26.21%
+ F
3,6,5,16
24.24% 26.70%
+ F
3,6,5,13,16
23.42% 26.70%
Table 4: Performance (F
1
) over difference feature
combinations for fulltext and full
1
inputs.
Abridged Inputs
Table 5 gives the performance for the abridged
inputs we tried with the best feature set F
1,3,4,6
.
All full
1
, full
2
, full
3
and full show improved per-
formance compared to those on the fulltext. We
achieve our best performance with full
1
at 28.18%
F Score. These results validate the effectiveness
of our approach in utilizing logical structure for
the candidate identification. We report our results
submitted in Table 6. These figures are achieved
using the best feature combination F
1,3,4,6
.
6 Conclusion
We have described and evaluated our keyphrase
extraction system for the SemEval-2 Task #5.
With the use of logical structure in the candidate
identification, our system has demonstrated its su-
perior performance over systems that do not use
such information. Moreover, we have effectively
reduced the numbers of text lines and candidate
Input @5 @10 @15 Fscore
min 62 110 145 23.75%
med 79 130 158 25.88%
full
1
84 135 172 28.18%
full
2
90 132 164 26.86%
full
3
89 134 162 26.54%
full 84 130 164 26.86%
fulltext 82 127 158 25.88%
Table 5: Performance over different abridged in-
puts using the best feature set F
1,3,4,6
. ?@N? indi-
cates the number of top N keyphrase matches.
System Description F@5 F@10 F@15
WINGNUS
1
full, F
1,3,4,6
20.65% 24.66% 24.95%
WINGNUS
2
full
1
, F
1,3,4,6
20.45% 24.73% 25.22%
Table 6: Final results on the test data.
phrases to be processed in the candidate identifi-
cation and selection respectively by about half.
Our system takes advantage of the logical struc-
ture analysis but not to the extent we had hoped.
We had hypothesized that formatting features (F
7
)
such as bold and italics, would help discriminate
key phrases, but in our limited experiments for
this task did not validate this. Similarly, external
knowledge should help in the keyphrase task, but
the prior knowledge about keyphrase likelihood
(F
9
) in DBLP hurt performance in our tests. We
plan to further explore these issues for the future.
References
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In MWE ?09.
Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Task 5: Automatic
keyphrase extraction from scientific articles. In Se-
mEval.
Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen
Kan. to appear. Logical structure recovery in schol-
arly articles with rich document features. IJDLS.
Forthcoming, accepted for publication.
Song Mao, Azriel Rosenfeld, and Tapas Kanungo.
2003. Document structure analysis algorithms: a lit-
erature survey. In Proc. SPIE Electronic Imaging.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
ICADL.
169
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104?113,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Better Word Representations with Recursive Neural Networks for
Morphology
Minh-Thang Luong Richard Socher Christopher D. Manning
Computer Science Department Stanford University, Stanford, CA, 94305
{lmthang, manning}@stanford.edu richard@socher.org
Abstract
Vector-space word representations have
been very successful in recent years at im-
proving performance across a variety of
NLP tasks. However, common to most
existing work, words are regarded as in-
dependent entities without any explicit re-
lationship among morphologically related
words being modeled. As a result, rare and
complex words are often poorly estimated,
and all unknown words are represented
in a rather crude way using only one or
a few vectors. This paper addresses this
shortcoming by proposing a novel model
that is capable of building representations
for morphologically complex words from
their morphemes. We combine recursive
neural networks (RNNs), where each mor-
pheme is a basic unit, with neural language
models (NLMs) to consider contextual
information in learning morphologically-
aware word representations. Our learned
models outperform existing word repre-
sentations by a good margin on word sim-
ilarity tasks across many datasets, includ-
ing a new dataset we introduce focused on
rare words to complement existing ones in
an interesting way.
1 Introduction
The use of word representations or word clusters
pretrained in an unsupervised fashion from lots of
text has become a key ?secret sauce? for the suc-
cess of many NLP systems in recent years, across
tasks including named entity recognition, part-of-
speech tagging, parsing, and semantic role label-
ing. This is particularly true in deep neural net-
work models (Collobert et al, 2011), but it is also
true in conventional feature-based models (Koo et
al., 2008; Ratinov and Roth, 2009).
Deep learning systems give each word a
distributed representation, i.e., a dense low-
dimensional real-valued vector or an embedding.
The main advantage of having such a distributed
representation over word classes is that it can cap-
ture various dimensions of both semantic and syn-
tactic information in a vector where each dimen-
sion corresponds to a latent feature of the word. As
a result, a distributed representation is compact,
less susceptible to data sparsity, and can implicitly
represent an exponential number of word clusters.
However, despite the widespread use of word
clusters and word embeddings, and despite much
work on improving the learning of word repre-
sentations, from feed-forward networks (Bengio
et al, 2003) to hierarchical models (Morin, 2005;
Mnih and Hinton, 2009) and recently recurrent
neural networks (Mikolov et al, 2010; Mikolov et
al., 2011), these approaches treat each full-form
word as an independent entity and fail to cap-
ture the explicit relationship among morphologi-
cal variants of a word.1 The fact that morphologi-
cally complex words are often rare exacerbates the
problem. Though existing clusterings and embed-
dings represent well frequent words, such as ?dis-
tinct?, they often badly model rare ones, such as
?distinctiveness?.
In this work, we use recursive neural networks
(Socher et al, 2011b), in a novel way to model
morphology and its compositionality. Essentially,
we treat each morpheme as a basic unit in the
RNNs and construct representations for morpho-
logically complex words on the fly from their mor-
phemes. By training a neural language model
(NLM) and integrating RNN structures for com-
plex words, we utilize contextual information in
1An almost exception is the word clustering of (Clark,
2003), which does have a model of morphology to encour-
age words ending with the same suffix to appear in the same
class, but it still does not capture the relationship between a
word and its morphologically derived forms.
104
an interesting way to learn morphemic semantics
and their compositional properties. Our model
has the capability of building representations for
any new unseen word comprised of known mor-
phemes, giving the model an infinite (if still in-
complete) covered vocabulary.
Our learned representations outperform pub-
licly available embeddings by a good margin on
word similarity tasks across many datasets, which
include our newly released dataset focusing on
rare words (see Section 5). The detailed analysis
in Section 6 reveals that our models can blend well
syntactic information, i.e., the word structure, and
the semantics in grouping related words.2
2 Related Work
Neural network techniques have found success in
several NLP tasks recently such as sentiment anal-
ysis at the sentence (Socher et al, 2011c) and
document level (Glorot et al, 2011), language
modeling (Mnih and Hinton, 2007; Mikolov and
Zweig, 2012), paraphrase detection (Socher et al,
2011a), discriminative parsing (Collobert, 2011),
and tasks involving semantic relations and compo-
sitional meaning of phrases (Socher et al, 2012).
Common to many of these works is use of a
distributed word representation as the basic input
unit. These representations usually capture lo-
cal cooccurrence statistics but have also been ex-
tended to include document-wide context (Huang
et al, 2012). Their main advantage is that they
can both be learned unsupervisedly as well as be
tuned for supervised tasks. In the former training
regiment, they are evaluated by how well they can
capture human similarity judgments. They have
also been shown to perform well as features for
supervised tasks, e.g., NER (Turian et al, 2010).
While much work has focused on different ob-
jective functions for training single and multi-
word vector representations, very little work has
been done to tackle sub-word units and how they
can be used to compute syntactic-semantic word
vectors. Collobert et al (2011) enhanced word
vectors with additional character-level features
such as capitalization but still can not recover
more detailed semantics for very rare or unseen
words, which is the focus of this work.
This is somewhat ironic, since working out cor-
2The rare word dataset and trained word vectors can be
found at http://nlp.stanford.edu/?lmthang/
morphoNLM.
rect morphological inflections was a very central
problem in early work in the parallel distributed
processing paradigm and criticisms of it (Rumel-
hart and McClelland, 1986; Plunkett and March-
man, 1991), and later work developed more so-
phisticated models of morphological structure and
meaning (Gasser and Lee, 1990; Gasser, 1994),
while not providing a compositional semantics nor
working at the scale of what we present.
To the best of our knowledge, the work clos-
est to ours in terms of handing unseen words are
the factored NLMs (Alexandrescu and Kirchhoff,
2006) and the compositional distributional seman-
tic models (DSMs) (Lazaridou et al, 2013). In
the former work, each word is viewed as a vec-
tor of features such as stems, morphological tags,
and cases, in which a single embedding matrix is
used to look up all of these features.3 Though
this is a principled way of handling new words in
NLMs, the by-product word representations, i.e.
the concatenations of factor vectors, do not en-
code in them the compositional information (they
are stored in the NN parameters). Our work does
not simply concatenate vectors of morphemes, but
rather combines them using RNNs, which cap-
tures morphological compositionality.
The latter work experimented with different
compositional DSMs, originally designed to learn
meanings of phrases, to derive representations for
complex words, in which the base unit is the mor-
pheme similar to ours. However, their models can
only combine a stem with an affix and does not
support recursive morpheme composition. It is,
however, interesting to compare our neural-based
representations with their DSM-derived ones and
cross test these models on both our rare word
similarity dataset and their nearest neighbor one,
which we leave as future work.
Mikolov et al (2013) examined existing word
embeddings and showed that these representations
already captured meaningful syntactic and seman-
tic regularities such as the singular/plural relation
that xapple - xapples ? xcar - xcars. However,we believe that these nice relationships will not
hold for rare and complex words when their vec-
tors are poorly estimated as we analyze in Sec-
tion 6. Our model, on the other hand, explicitly
represents these regularities through morphologi-
cal structures of words.
3(Collobert et al, 2011) used multiple embeddings, one
per discrete feature type, e.g., POS, Gazeteer, etc.
105
 
	

	




Figure 1: Morphological Recursive Neural Net-
work. A vector representation for the word ?un-
fortunately? is constructed from morphemic vec-
tors: unpre, fortunatestm, lysuf. Dotted nodes are
computed on-the-fly and not in the lexicon.
3 Morphological RNNs
Our morphological Recursive Neural Network
(morphoRNN) is similar to (Socher et al, 2011b),
but operates at the morpheme level instead of at
the word level. Specifically, morphemes, the mini-
mum meaning-bearing unit in languages, are mod-
eled as real-valued vectors of parameters, and are
used to build up more complex words. We assume
access to a dictionary of morphemic analyses of
words, which will be detailed in Section 4.
Following (Collobert and Weston, 2008), dis-
tinct morphemes are encoded by column vectors
in a morphemic embedding matrix We ? Rd?|M|,
where d is the vector dimension and M is an or-
dered set of all morphemes in a language.
As illustrated in Figure 1, vectors of morpho-
logically complex words are gradually built up
from their morphemic representations. At any lo-
cal decision (a dotted node), a new parent word
vector (p) is constructed by combining a stem vec-
tor (xstem) and an affix vector (xaffix) as follow:
p = f(Wm[xstem;xaffix] + bm) (1)
Here, Wm ? Rd?2d is a matrix of morphemic pa-
rameters while bm ? Rd?1 is an intercept vector.
We denote an element-wise activation function as
f , such as tanh. This forms the basis of our mor-
phoRNNmodels with ? = {We,Wm, bm} being
the parameters to be learned.
3.1 Context-insensitive Morphological RNN
Our first model examines how well morphoRNNs
could construct word vectors simply from the mor-
phemic representation without referring to any
context information. Input to the model is a refer-
ence embedding matrix, i.e. word vectors trained
by an NLM such as (Collobert and Weston, 2008)
and (Huang et al, 2012). By assuming that these
reference vectors are right, the goal of the model
is to construct new representations for morpholog-
ically complex words from their morphemes that
closely match the corresponding reference ones.
Specifically, the structure of the context-
insensitive morphoRNN (cimRNN) is the same as
the basic morphoRNN. For learning, we first de-
fine a cost function s for each word xi as the
squared Euclidean distance between the newly-
constructed representation pc(xi) and its refer-
ence vector pr(xi): s (xi) = ?pc(xi) ? pr(xi)?22.
The objective function is then simply the sum of
all individual costs over N training examples, plus
a regularization term, which we try to minimize:
J(?) =
N?
i=1
s (xi) +
?
2 ???
2
2 (2)
3.2 Context-sensitive Morphological RNN
The cimRNN model, though simple, is interesting
to attest if morphemic semantics could be learned
solely from an embedding. However, it is lim-
ited in several aspects. Firstly, the model has
no chance of improving representations for rare
words which might have been poorly estimated.
For example, ?distinctness? and ?unconcerned?
are very rare, occurring only 141 and 340 times
in Wikipedia documents, even though their corre-
sponding stems ?distinct? and ?concern? are very
frequent (35323 and 26080 respectively). Trying
to construct exactly those poorly-estimated word
vectors might result in a bad model with parame-
ters being pushed in wrong directions.
Secondly, though word embeddings learned
from an NLM could, in general, blend well both
the semantic and syntactic information, it would
be useful to explicitly model another kind of syn-
tactic information, the word structure, as we train
our embeddings. Motivated by these limitations,
we propose a context-sensitive morphoRNN (csm-
RNN) which integrates RNN structures into NLM
training, allowing for contextual information be-
ing taken into account in learning morphemic
compositionality. Specifically, we adopt the NLM
training approach proposed in (Collobert et al,
2011) to learn word embeddings, but build rep-
resentations for complex words from their mor-
phemes. During learning, updates at the top level
of the neural network will be back-propagated all
the way till the morphemic layer.
106
	
    	
 
 
 	
 	 

 



Figure 2: Context-sensitive morphological RNN
has two layers: (a) themorphological RNN, which
constructs representations for words from their
morphemes and (b) the word-based neural lan-
guage which optimizes scores for relevant ngrams.
Structure-wise, we stack the NLM on top of our
morphoRNN as illustrated in Figure 2. Complex
words like ?unfortunately? and ?closed? are con-
structed from their morphemic vectors, unpre +
fortunatestm + lysuf and closestm + dsuf, whereas
simple words4, i.e. stems, and affixes could be
looked up from the morphemic embedding ma-
trix We as in standard NLMs. Once vectors of all
complex words have been built, the NLM assigns
a score for each ngram ni consisting of words
x1, . . . , xn as follows:
s (ni) = ??f(W [x1; . . . ;xn] + b)
Here, xj is the vector representing the word xj .
We follow (Huang et al, 2012) to use a sim-
ple feed-forward network with one h-dimensional
hidden layer. W ? Rh?nd, b ? Rh?1, and
? ? Rh?1 are parameters of the NLM, and f is
an element-wise activation function as in Eq. (1).
We adopt a ranking-type cost in defining our ob-
jective function to minimize as below:
J(?) =
N?
i=1
max{0, 1? s (ni) + s (ni)} (3)
Here, N is the number of all available ngrams in
the training corpus, whereas ni is a ?corrupted?
ngram created from ni by replacing its last word
with a random word similar in spirit to (Smith
and Eisner, 2005). Our model parameters are
? = {We,Wm, bm,W , b,?}.
Such a ranking criterion influences the model
to assign higher scores to valid ngrams than to
4?fortunate?, ?the?, ?bank?, ?was?, and ?close?.
invalid ones and has been demonstrated in (Col-
lobert et al, 2011) to be both efficient and effective
in learning word representations.
3.3 Learning
Our models alternate between two stages: (1) for-
ward pass ? recursively construct morpheme trees
(cimRNN, csmRNN) and language model struc-
tures (csmRNN) to derive scores for training ex-
amples and (2) back-propagation pass ? compute
the gradient of the corresponding object function
with respect to the model parameters.
For the latter pass, computing the objective gra-
dient amounts to estimating the gradient for each
individual cost ?s(x)?? , where x could be either aword (cimRNN) or an ngram (csmRNN). We have
the objective gradient for the cimRNN derived as:
?J(?)
?? =
N?
i=1
?s (xi)
?? + ??
In the case of csmRNN, since the objective
function in Eq. (3) is not differentiable, we use the
subgradient method (Ratliff et al, 2007) to esti-
mate the objective gradient as:
?J(?)
?? =
?
i:1?s(ni)+s(ni)>0
??s (ni)?? +
?s (ni)
??
Back-propagation through structures (Goller
and Ku?chler, 1996) is employed to compute the
gradient for each individual cost with similar for-
mulae as in (Socher et al, 2010). Unlike their
RNN structures over sentences, where each sen-
tence could have an exponential number of deriva-
tions, our morphoRNN structure per word is, in
general, deterministic. Each word has a single
morphological tree structure which is constructed
from the main morpheme (the stem) and gradu-
ally appended affixes in a fixed order (see Sec-
tion 4 for more details). As a result, both our
forward and backward passes over morphologi-
cal structures are efficient with no recursive calls
implementation-wise.
4 Unsupervised Morphological
Structures
We utilize an unsupervised morphological seg-
mentation toolkit, named Morfessor by Creutz and
Lagus (2007), to obtain segmentations for words
in our vocabulary. Morfessor segments words in
107
two stages: (a) it recursively splits words to min-
imize an objective inspired by the minimum de-
scription length principle and (b) it labels mor-
phemes with tags pre (prefixes), stm (stems),
and suf (suffixes) using hidden Markov models.
Morfessor captures a general word structure of
the form (pre? stm suf?)+, which is handy
for words in morphologically rich languages like
Finnish or Turkish. However, such general form is
currently unnecessary in our models as the mor-
phoRNNs assume input of the form pre? stm
suf? for efficient learning of the RNN structures:
a stem is always combined with an affix to yield a
new stem.5 We, thus, postprocess as follows:
(1) Restrict segmentations to the form pre?
stm{1, 2} suf?: allow us to capture compounds.
(2) Split hyphenated words A-B as Astm Bstm.
(3) For a segmentation with two stems, pre?
Astm Bstm suf?, we decide if one could be a main
stem while the other could functions as an affix.6
Otherwise, we reject the segmentation. This will
provide us with more interesting morphemes such
as alpre in Arabic names (al-jazeera, al-salem) and
relatedsuf in compound adjectives (health-related,
government-related).
(4) To enhance precision, we reject a segmen-
tation if it has either an affix or an unknown stem
(not a word by itself) whose type count is below a
predefined threshold7.
The final list of affixes produced is given in Ta-
ble 1. Though generally reliable, our final seg-
mentations do contain errors, most notably non-
compositional ones, e.g. depre faultstm edsuf or
repre turnstm ssuf. With a sufficiently large num-
ber of segmentation examples, we hope that the
model would be able to pick up general trends
from the data. In total, we have about 22K com-
plex words out of a vocabulary of 130K words.
Examples of words with interesting affixes are
given in Table 2. Beside conventional affixes, non-
conventional ones like ?0? or ?mc? help further
categorize rare or unknown words into meaningful
groups such as measurement words or names.
5When multiple affixes are present, we use a simple
heuristic to first merge suffixes into stems and then combine
prefixes. Ideally, we would want to learn and generate an
order for such combination, which we leave for future work.
6We first aggregate type counts of pairs (A, left) and (B,
right) across all segmentations with two stems. Once done,
we label A as stm and B as suf if count (B, right) > 2 ?
count (A, left), and conversely, we label them as Apre Bstm if
count (A, left) > 2 ? count(B, right). Our rationale was that
Prefixes Suffixes
0 al all anti auto co
counter cross de dis
electro end ex first five
focus four half high hy-
per ill im in inter ir jan
jean long low market mc
micro mid multi neuro
newly no non off one
over post pre pro re sec-
ond self semi seven short
six state sub super third
three top trans two un
under uni well
able al ally american ance
ate ation backed bank
based born controlled d
dale down ed en er es field
ford free ful general head
ia ian ible ic in ing isation
ise ised ish ism ist ity ive
ization ize ized izing land
led less ling listed ly made
making man ment ness off
on out owned related s ship
shire style ton town up us
ville wood
Table 1: List of prefixes and suffixes discovered ?
conventional affixes in English are italicized.
Affix Words
0 0-acre, 0-aug, 0-billion, 0-centistoke
anti anti-immigrant, antipsychotics
counter counterexample, counterinsurgency
hyper hyperactivity, hypercholesterolemia
mc mcchesney, mcchord, mcdevitt
bank baybank, brockbank, commerzbank
ford belford, blandford, carlingford
land adventureland, bodoland, bottomland
less aimlessly, artlessness, effortlessly
owned bank-owned, city-owned disney-owned
Table 2: Sample affixes and corresponding words.
5 Experiments
As our focus is in learning morphemic seman-
tics, we do not start training from scratch, but
rather, initialize our models with existing word
representations. In our experiments, we make
use of two publicly-available embeddings (50-
dimensional) provided by(Collobert et al, 2011)
(denoted as C&W)8 and Huang et al (2012) (re-
ferred as HSMN)9.
Both of these representations are trained on
Wikipedia documents using the same ranking-type
cost function as in Eq. (3). The latter further uti-
lizes global context and adopts a multi-prototype
approach, i.e. each word is represented by mul-
tiple vectors, to better capture word semantics in
various contexts. However, we only use their
single-prototype embedding10 and as we train, we
affixes occur more frequently than stems.
7Set to 15 and 3 for affixes and stems respectively.
8http://ronan.collobert.com/senna/.
9http://www-nlp.stanford.edu/?ehhuang/.10The embedding obtained just before the clustering step
to build multi-prototype representation.
108
do not consider the global sentence-level context
information. It is worth to note that these aspects
of the HSMN embedding ? incorporating global
context and maintaining multiple prototypes ? are
orthogonal to our approach, which would be inter-
esting to investigate in future work.
For the context-sensitive morphoRNN model,
we follow Huang et al (2012) to use the April
2010 snapshot of the Wikipedia corpus (Shaoul
and Westbury, 2010). All paragraphs containing
non-roman characters are removed while the re-
maining text are lowercased and then tokenized.
The resulting clean corpus contains about 986 mil-
lion tokens. Each digit is then mapped into 0, i.e.
2013 will become 0000. Other rare words not in
the vocabularies of C&W and HSMN are mapped
to an UNKNOWN token, and we use <s> and
</s> for padding tokens representing the begin-
ning and end of each sentence.
Follow (Huang et al, 2012)?s implementation,
which our code is based on initially, we use 50-
dimensional vectors to represent morphemic and
word embeddings. For cimRNN, the regulariza-
tion weight ? is set to 10?2. For csmRNN, we use
10-word windows of text as the local context, 100
hidden units, and no weight regularization.
5.1 Word Similarity Task
Similar to (Reisinger and Mooney, 2010) and
(Huang et al, 2012), we evaluate the quality of our
morphologically-aware embeddings on the popu-
lar WordSim-353 dataset (Finkelstein et al, 2002),
WS353 for short. In this task, we compare corre-
lations between the similarity scores given by our
models and those rated by human.
To avoid overfitting our models to a single
dataset, we benchmark our models on a vari-
ety of others including MC (Miller and Charles,
1991), RG (Rubenstein and Goodenough, 1965),
SCWS?11 (Huang et al, 2012), and our new rare
word (RW) dataset (details in ?5.1.1). Information
about these datasets are summarized in Table 3
We also examine these datasets from the
?rareness? aspect by looking at distributions of
words across frequencies as in Table 4. The first
bin counts unknown words in each dataset, while
the remaining bins group words based on their
11SCWS? is a modified version of the Stanford?s contex-
tual word similarities dataset. The original one utilizes sur-
rounding contexts in judging word similarities and includes
pairs of identical words, e.g. financial bank vs. river bank.
We exclude these pairs and ignore the provided contexts.
pairs type raters scale Complex wordstoken type
WS353 353 437 13-16 0-10 24 17
MC 30 39 38 0-4 0 0
RG 65 48 51 0-4 0 0
SCWS? 1762 1703 10 0-10 190 113
RW (new) 2034 2951 10 0-10 987 686
Table 3: Word similarity datasets and their
statistics: number of pairs/raters/type counts as
well as rating scales. The number of complex
words are shown as well (both type and token
counts). RW denotes our new rare word dataset.
frequencies extracted from Wikipedia documents.
It is interesting to observe that WS353, MC, RG
contain very frequent words and have few complex
words (only WS353 has).12 SCWS? and RW have
a more diverse set of words in terms of frequencies
and RW has the largest number of unknown and
rare words, which makes it a challenging dataset.
All words Complex words
WS353 0 | 0 / 9 / 87 / 341 0 | 0 / 1 / 6 / 10
MC 0 | 0 / 1 / 17 / 21 0 | 0 / 0 / 0 / 0
RG 0 | 0 / 4 / 22 / 22 0 | 0 / 0 / 0 / 0
SCWS? 26 | 2 / 140 / 472 / 1063 8 | 2 / 22 / 44 / 45
RW 801 | 41 / 676 / 719 / 714 621 | 34 / 311 / 238 / 103
Table 4: Word distribution by frequencies ? dis-
tinct words in each dataset are grouped based on
frequencies and counts are reported for the fol-
lowing bins : unknown | [1, 100] / [101, 1000] /
[1001, 10000] / [10001, ?). We report counts for
all words in each dataset as well as complex ones.
5.1.1 Rare Word Dataset
As evidenced in Table 4, most existing word sim-
ilarity datasets contain frequent words and few of
them possesses enough rare or morphologically
complex words that we could really attest the ex-
pressiveness of our morphoRNN models. In fact,
we believe a good embedding in general should be
able to learn useful representations for not just fre-
quent words but also rare ones. That motivates us
to construct another dataset focusing on rare words
to complement existing ones.
Our dataset construction proceeds in three
stages: (1) select a list of rare words, (2) for each
of the rare words, find another word (not neces-
sarily rare) to form a pair, and (3) collect human
judgments on how similar each pair is.
12All these counts are with respect to the vocabulary list in
the C&W embedding (we obtain similar figures for HSMN).
109
(5, 10] (10, 100] (100, 1000]
un- untracked unrolls undissolved unrehearsed unflagging unfavourable unprecedented unmarried uncomfortable-al apocalyptical traversals bestowals acoustical extensional organismal directional diagonal spherical-ment obtainment acquirement retrenchments discernment revetment rearrangements confinement establishment management
word1 untracked unflagging unprecedented apocalyptical organismal diagonal obtainment discernment confinementword2 inaccessible constant new prophetic system line acquiring knowing restraint
Table 5: Rare words (top) ? word1 by affixes and frequencies and sample word pairs (bottom).
Rare word selection: our choices of rare words
(word1) are based on their frequencies ? based on
five bins (5, 10], (10, 100], (100, 1000], (1000,
10000], and the affixes they possess. To create a
diverse set of candidates, we randomly select 15
words for each configuration (a frequency bin, an
affix). At the scale of Wikipedia, a word with
frequency of 1-5 is most likely a junk word, and
even restricted to words with frequencies above
five, there are still many non-English words. To
counter such problems, each word selected is re-
quired to have a non-zero number of synsets in
WordNet(Miller, 1995).
Table 5 (top) gives examples of rare words se-
lected and organized by frequencies and affixes. It
is interesting to find out that words like obtainment
and acquirement are extremely rare (not in tradi-
tional dictionaries) but are perfectly understand-
able. We also have less frequent words like revet-
ment from French or organismal from biology.
Pair construction: following (Huang et al,
2012), we create pairs with interesting relation-
ships for each word1 as follow. First, a Word-
Net synset of word1 is randomly selected, and we
construct a set of candidates which connect to that
synset through various relations, e.g., hypernyms,
hyponyms, holonyms, meronyms, and attributes.
A word2 is then randomly selected from these can-
didates, and the process is repeated another time
to generate a total of two pairs for each word1.
Sample word pairs are given in Table 5 in which
word2 includes mostly frequent words, implying
a balance of words in terms of frequencies in our
dataset. We collected 3145 pairs after this stage
Human judgment: we use Amazon Mechani-
cal Turk to collect 10 human similarity ratings on
a scale of [0, 10] per word pair.13 Such procedure
has been demonstrated by Snow et al (2008) in
replicating ratings for the MC dataset, achieving
close inter-annotator agreement with expert raters.
Since our pairs contain many rare words which are
13We restrict to only US-based workers with 95% approval
rate and ask for native speakers to rate 20 pairs per hit.
challenging even to native speakers, we ask raters
to indicate for each pair if they do not know the
first word, the second word, or both. We use such
information to collect reliable ratings by either dis-
card pairs which many people do not know or col-
lect additional ratings to ensure we have 10 rat-
ings per pair.14 As a result, only 2034 pairs are
retained.
5.2 Results
We evaluate the quality of our morphoRNN em-
beddings through the word similarity task dis-
cussed previously. The Spearman?s rank correla-
tion is used to gauge how well the relationship be-
tween two variables, the similarity scores given by
the NLMs and the human annotators, could be de-
scribed using a monotonic function.
Detailed performance of the morphoRNN em-
beddings trained from either the HSMN or the
C&W embeddings are given in Table 7 for
all datasets. We also report baseline results
(rows HSMN, C&W) using these initial embed-
dings alone, which interestingly reveals strengths
and weaknesses of existing embeddings. While
HSMN is good for datasets with frequent words
(WS353, MC, and RG), its performances for those
with more rare and complex words (SCWS? and
RW) are much inferior than those of C&W, and
vice versa. Additionally, we consider two slightly
more competitive baselines (rows +stem) based
on the morphological segmentation of unknown
words: instead of using a universal vector repre-
senting all unknown words, we use vectors rep-
resenting the stems of unknown words. These
baselines yield slightly better performance for the
SCWS? and RW datasets while the trends we men-
tioned earlier remain the same.
Our first model, the context-insensitive mor-
phoRNN (cimRNN), outperforms its correspond-
ing baseline significantly over the rare word
14In our later experiments, an aggregated rating is derived
for each pair. We first discard ratings not within one standard
deviation of the mean, and then estimate a new mean from
the remaining ones to use as an aggregated rating.
110
Words C&W C&W + cimRNN C&W + csmRNNcommenting insisting insisted focusing hinted republishing accounting expounding commented comments criticizingcomment commentary rant statement remark commentary rant statement remark rant commentary statement anecdote
distinctness morphologies pesawat clefts modality indistinct tonality spatiality indistinct distinctiveness largeness uniquenessdistinct different distinctive broader narrower different distinctive broader divergent divergent diverse distinctive homogeneous
unaffected unnoticed dwarfed mitigated disaffected unconstrained uninhibited undesired unhindered unrestrictedaffected caused plagued impacted damaged disaffected unaffected mitigated disturbed complicated desired constrained reasonedunaffect ? affective affecting affectation unobserved affective affecting affectation restrictiveaffect exacerbate impacts characterize affects affectation exacerbate characterize decrease arise complicate exacerbateheartlessness ? fearlessness vindictiveness restlessness depersonalization terrorizes sympathizesheartless merciless sadistic callous mischievous merciless sadistic callous mischievous sadistic callous merciless hideousheart death skin pain brain life blood death skin pain brain life blood death brain blood skin lung mouth
saudi-owned avatar mohajir kripalani fountainhead saudi-based somaliland al-jaber saudi-based syrian-controlled syrian-backedshort-changed kindled waylaid endeared peopled conformal conformist unquestionable short-termism short-positions self-sustainable
Table 6: Nearest neighbors. We showmorphologically related words and their closest words in different
representations (?unaffect? is a pseudo-word; ? marks no results due to unknown words).
WS353 MC RG SCWS? RW
HSMN 62.58 65.90 62.81 32.11 1.97
+stem 62.58 65.90 62.81 32.11 3.40
+cimRNN 62.81 65.90 62.81 32.97 14.85
+csmRNN 64.58 71.72 65.45 43.65 22.31
C&W 49.77 57.37 49.30 48.59 26.75
+stem 49.77 57.37 49.30 49.05 28.03
+cimRNN 51.76 57.37 49.30 47.00 33.24
+csmRNN 57.01 60.20 55.40 48.48 34.36
Table 7: Word similarity task ? shown are Spear-
man?s rank correlation coefficient (? ? 100) be-
tween similarity scores assigned by neural lan-
guage models and by human annotators. stem in-
dicates baseline systems in which unknown words
are represented by their stem vectors. cimRNN and
csmRNN refer to our context insensitive and sensi-
tive morphological RNNs respectively.
dataset. The performance is constant for MC and
RG (with no complex words) and modestly im-
proved for MC (with some complex words ? see
Table 4). This is expected since the cimRNN
model only concerns about reconstructing the
original embedding (while learning word struc-
tures), and the new representation mostly differs
at morphologically complex words. For SCWS?,
the performance, however, decreases when train-
ing with C&W, which perhaps is due to: (a) the
baseline performance of C&W for SCWS? is com-
petitive and (b) the model trades off between learn-
ing syntactics (the word structure) and capturing
semantics, which requires context information.
On the other hand, the context-sensitive mor-
phoRNN (csmRNN) consistently improves corre-
lations over the cimRNN model for all datasets,
demonstrating the effectiveness of using surround-
ing contexts in learning both morphological syn-
tactics and semantics. It also outperforms the
corresponding baselines by a good margin for all
datasets (except for SCWS?). This highlights the
fact that our method is reliable and potentially ap-
plicable for other embeddings.
6 Analysis
To gain a deeper understanding of how our mor-
phoRNN models have ?moved? word vectors
around, we look at nearest neighbors of sev-
eral complex words given by various embed-
dings, where cosine similarity is used as a dis-
tance metric. Examples are shown in Table 6
for three representations: C&W and the context-
insensitive/sensitive morphoRNN models trained
on the C&W embedding.15
Syntactically, it is interesting to observe that
the cimRNN model could well enforce structural
agreement among related words. For example, it
returns V-ing as nearest neighbors for ?comment-
ing? and similarly, JJ-ness for ?fearlessness?, an
unknown word that C&W cannot handle. How-
ever, for those cases, the nearest neighbors are
badly unrelated.
On the semantic side, we notice that when
structural agreement is not enforced, the cimRNN
model tends to cluster words sharing the same
stem together, e.g., rows with words of the form
affect .16 This might be undesirable when we
want to differentiate semantics of words sharing
the same stem, e.g. ?affected? and ?unaffected?.
The csmRNN model seems to balance well be-
tween the two extremes (syntactic and seman-
tic) by taking into account contextual information
15Results of HSMN-related embeddings are not shown, but
similar trends follow.
16?unaffect? is a pseudo-word that we inserted.
111
when learning morphological structures. It returns
neighbors of the same structure un ed for ?unaf-
fected?, but does not include any negation of ?af-
fected? in the top 10 results when ?affected? is
queried.17 Even better, the answers for ?distinct-
ness? have blended well both types of results.
7 Conclusion
This paper combines recursive neural networks
(RNNs) and neural language models (NLMs) in
a novel way to learn better word representa-
tions. Each of these components contributes to
the learned syntactic-semantic word vectors in a
unique way. The RNN explicitly models the mor-
phological structures of words, i.e., the syntactic
information, to learn morphemic compositional-
ity. This allows for better estimation of rare and
complex words and a more principled way of han-
dling unseen words, whose representations could
be constructed from vectors of knownmorphemes.
The NLMs, on the other hand, utilize surround-
ing word contexts to provide further semantics
to the learned morphemic representations. As
a result, our context-sensitive morphoRNN em-
beddings could significantly outperform existing
embeddings on word similarity tasks for many
datasets. Our analysis reveals that the model could
blend well both the syntactic and semantic infor-
mation in clustering related words. We have also
made available a word similarity dataset focusing
on rare words to complement existing ones which
tend to include frequent words.
Lastly, as English is still considered limited
in terms of morphology, our model could poten-
tially yield even better performance when applied
to other morphologically complex languages such
as Finnish or Turkish, which we leave for future
work. Also, even within English, we expect our
model to be value to other domains, such as bio-
NLP with complicated but logical taxonomy.
Acknowledgements
We thank the anonymous reviewers for their feed-
back and Eric Huang for making his various pieces
of code available to us as well as answering our
questions on different datasets. Stanford Uni-
versity gratefully acknowledges the support of
the Defense Advanced Research Projects Agency
(DARPA) Deep Exploration and Filtering of Text
17?disaffected? is ranked 5th for the first query while ?af-
fecting? occurs at position 8 for the latter.
(DEFT) Program under Air Force Research Lab-
oratory (AFRL) contract no. FA8750-13-2-0040
and the DARPA Broad Operational Language
Translation (BOLT) program through IBM. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In NAACL.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In EACL.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
R. Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In AISTATS.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4:3:1?3:34.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Michael Gasser and Chan-Do Lee. 1990. A short-
term memory architecture for the learning of mor-
phophonemic rules. In NIPS.
Michael Gasser. 1994. Acquiring receptive morphol-
ogy: A connectionist model. In ACL.
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In ICML.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backprop-
agation through structure. IEEE Transactions on
Neural Networks, 1:347?352.
112
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving word representations via global
context and multiple word prototypes. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In ACL.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In NAACL-HLT.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
G.A. Miller. 1995. WordNet: A Lexical Database for
English. Communications of the ACM.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In ICML.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In NIPS.
Frederic Morin. 2005. Hierarchical probabilistic neu-
ral network language model. AIstats?05. In AIS-
TATS.
K. Plunkett and V. Marchman. 1991. U-shaped learn-
ing and frequency effects in a multi-layered percep-
tron: implications for child language acquisition.
Cognition, 38(1):43?102.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2007. Online subgradient methods for
structured prediction.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In NAACL.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
David E. Rumelhart and James L. McClelland. 1986.
On learning the past tenses of English verbs. In J. L.
McClelland, D. E. Rumelhart, and PDP Research
Group, editors, Parallel Distributed Processing. Vol-
ume 2: Psychological and Biological Models, pages
216?271. MIT Press.
Cyrus Shaoul and Chris Westbury. 2010. The West-
bury lab wikipedia corpus. Edmonton, AB: Univer-
sity of Alberta.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In ACL.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In EMNLP.
Richard Socher, Christopher Manning, and Andrew
Ng. 2010. Learning continuous phrase represen-
tations and syntactic parsing with recursive neural
networks. In NIPS*2010 Workshop on Deep Learn-
ing and Unsupervised Feature Learning.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic pooling and un-
folding recursive autoencoders for paraphrase detec-
tion. In NIPS.
R. Socher, Cliff C. Lin, A. Y. Ng, and C. D. Manning.
2011b. Parsing natural scenes and natural language
with recursive neural networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. In EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In ACL.
113
