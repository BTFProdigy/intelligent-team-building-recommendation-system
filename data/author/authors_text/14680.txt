Coling 2010: Poster Volume, pages 919?927,
Beijing, August 2010
A Study on Position Information in Document Summarization 
You Ouyang       Wenjie Li       Qin Lu       Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University 
{csyouyang,cswjli,csluqin,csrzhang}@comp.polyu.edu.hk 
Abstract 
Position information has been proved to 
be very effective in document 
summarization, especially in generic 
summarization. Existing approaches 
mostly consider the information of 
sentence positions in a document, based 
on a sentence position hypothesis that 
the importance of a sentence decreases 
with its distance from the beginning of 
the document. In this paper, we consider 
another kind of position information, i.e., 
the word position information, which is 
based on the ordinal positions of word 
appearances instead of sentence 
positions. An extractive summarization 
model is proposed to provide an 
evaluation framework for the position 
information. The resulting systems are 
evaluated on various data sets to 
demonstrate the effectiveness of the 
position information in different 
summarization tasks. Experimental 
results show that word position 
information is more effective and 
adaptive than sentence position 
information. 
1 Introduction 
Position information has been frequently used in 
document summarization. It springs from 
human?s tendency of writing sentences of 
greater topic centrality at particular positions in 
a document. For example, in newswire 
documents, topic sentences are usually written 
earlier. A sentence position hypothesis is then 
given as: the first sentence in a document is the 
most important and the importance decreases as 
the sentence gets further away from the 
beginning. Based on this sentence position 
hypothesis, sentence position features are 
defined by the ordinal position of sentences. 
These position features have been proved to be 
very effective in generic document 
summarization. In more recent summarization 
tasks, such as query-focused and update 
summarization tasks, position features are also 
widely used.  
Although in these tasks position features may 
be used in different ways, they are all based on 
the sentence position hypothesis. So we regard 
them as providing the sentence position 
information. In this paper, we study a new kind 
of position information, i.e., the word position 
information. The motivation of word position 
information comes from the idea of assigning 
different importance to multiple appearances of 
one word in a document.  
As to many language models such as the bag-
of-words model, it is well acknowledged that a 
word which appears more frequently is usually 
more important. If we take a closer look at all 
the appearances of one word, we can view this 
as a process that the different appearances of the 
same word raise the importance of each other. 
Now let?s also take the order of the appearances 
into account. When reading a document, we can 
view it as a word token stream from the first 
token to the last. When a new token is read, we 
attach more importance to previous tokens that 
have the same lemma because they are just 
repeated by the new token. Inspired by this, we 
postulate a word position hypothesis here: for 
all the appearances of a fixed word, the 
importance of each appearance depends on all 
its following appearances. Therefore, the first 
appearance of a word is the most important and 
the importance decreases with the ordinal 
919
positions of the appearances. Then, a novel kind 
of position features can be defined for the word 
appearances based on their ordinal positions. 
We believe that these word position features 
have some advantages when compared to 
traditional sentence position features. According 
to the sentence position hypothesis, sentence 
position features generally prefer earlier 
sentences in a document. As to the word 
position features that attempt to differentiate 
word appearances instead of sentences, a 
sentence which is not the first one in the 
document may still not be penalized as long as 
its words do not appear in previous sentences. 
Therefore, word position features are able to 
discover topic sentences in deep positions of the 
document. On the other hand, the assertion that 
the first sentence is always the most important is 
not true in actual data. It depends on the writing 
style indeed. For example, some authors may 
like to write some background sentences before 
topic sentences. In conclusion, we can expect 
word position features  to be more adaptive to 
documents with different structures.  
In the study of this paper, we define several 
word position features based on the ordinal 
positions of word appearances. We also develop 
a word-based summarization system to evaluate 
the effectiveness of the proposed word position 
features on a series of summarization data sets. 
The main contributions of our work are: 
(1) representation of word position information, 
which is a new kind of position information in 
document summarization area. 
(2) empirical results on various data sets that 
demonstrate the impact of position information 
in different summarization tasks. 
2 Related Work 
The use of position information in document 
summarization has a long history. In the seminal 
work by (Luhn, 1958), position information was 
already considered as a good indicator of 
significant sentences. In (Edmundson, 1969), a 
location method was proposed that assigns 
positive weights to the sentences to their ordinal 
positions in the document. Position information 
has since been adopted by many successful 
summarization systems, usually in the form of 
sentence position features. For example, Radev 
et al (2004) developed a feature-based system 
MEAD based on word frequencies and sentence 
positions. The position feature was defined as a 
descending function of the sentence position. 
The MEAD system performed very well in the 
generic multi-document summarization task of 
the DUC 2004 competition. Later, position 
information is also applied to more 
summarization tasks. For example, in query-
focused task, sentence position features are 
widely used in learning-based summarization 
systems as a component feature for calculating 
the composite sentence score (Ouyang et al 
2007; Toutanova et al 2007). However, the 
effect of position features alone was not studied 
in these works.  
There were also studies aimed at analyzing 
and explaining the effectiveness of position 
information. Lin and Hovy (1997) provided an 
empirical validation on the sentence position 
hypothesis. For each position, the sentence 
position yield was defined as the average value 
of the significance of the sentences with the 
fixed position. It was observed that the average 
significance at earlier positions was indeed 
larger. Nenkova (2005) did a conclusive 
overview on the DUC 2001-2004 evaluation 
results. It was reported that position information 
is very effective in generic summarization. In 
generic single-document summarization, a lead-
based baseline that simply takes the leading 
sentences as the summary can outperform most 
submitted summarization system in DUC 2001 
and 2002. As in multi-document summarization, 
the position-based baseline system is 
competitive in generating short summaries but 
not in longer summaries. Schilder and 
Kondadadi (2008) analyzed the effectiveness of 
the features that are used in their learning-based 
sentence scoring model for query-focused 
summarization. By comparing the ROUGE-2 
results of each individual feature, it was 
reported that position-based features are less 
effective than frequency-based features. In 
(Gillick et al, 2009), the effect of position 
information in the update summarization task 
was studied. By using ROUGE to measure the 
density of valuable words at each sentence 
position, it was observed that the first sentence 
of newswire document was especially important 
for composing update summaries. They defined 
a binary sentence position feature based on the 
920
observation and the feature did improve the 
performance on the update summarization data. 
3 Methodology 
In the section, we first describe the word-based 
summarization model. The word position 
features are then defined and incorporated into 
the summarization model. 
3.1 Basic Summarization Model 
To test the effectiveness of position information 
in document summarization, we first propose a 
word-based summarization model for applying 
the position information. The system follows a 
typical extractive style that constructs the target 
summary by selecting the most salient sentences.  
Under the bag-of-words model, the 
probability of a word w in a document set D can 
be scaled by its frequency, i.e., p(w)=freq(w)/|D|, 
where freq(w) indicates the frequency of w in D 
and |D| indicates the total number of words in D. 
The probability of a sentence s={w1, ?, wN} is 
then calculated as the product of the word 
probabilities, i.e., p(s)=?i p(wi). Moreover, the 
probability of a summary consisting a set of 
sentences, denoted as S={s1, ?, sM}, can be 
calculated by the product of the sentence 
probabilities, i.e., p(S)=?j p(sj). To obtain the 
optimum summary, an intuitive idea is to select 
the sentences to maximize the overall summary 
probability p(S), equivalent to maximizing 
log(p(S)) = ?j?i log(p(wji)) = ?j?i (logfreq(wji)- 
log|D|) = ?j?i log freq(wji) - |S|?log |D|,  
where wji indicates the ith word in sj and |S| 
indicates the total number of words in S. As to 
practical summarization tasks, a maximum 
summary length is usually postulated. So here 
we just assume that the length of the summary 
is fixed. Then, the above optimization target is 
equivalent to maximizing ?j?i logfreq(wji). 
From the view of information theory, the sum 
can also be interpreted as a simple measure on 
the total information amount of the summary. In 
this interpretation, the information of a single 
word wji is measured by log freq(wji) and the 
summary information is the sum of the word 
information. So the optimization target can also 
be interpreted as including the most informative 
words to form the most informative summary 
given the length limit.  
In extractive summarization, summaries are 
composed by sentence selection. As to the 
above optimization target, the sentence scoring 
function for ranking the sentences should be 
calculated as the average word information, i.e., 
score(s) = ?i log freq(wi) / |s|. 
After ranking the sentences by their ranking 
scores, we can select the sentences into the 
summary by the descending order of their score 
until the length limit is reached. By this process, 
the summary with the largest  p(S) can be 
composed.  
3.2 Word Position Features 
With the above model, word position features 
are defined to represent the word position 
information and are then incorporated into the 
model. According to the motivation, the features 
are defined by the ordinal positions of word 
appearances, based on the position hypothesis 
that earlier appearances of a word are more 
informative. Formally, for the ith appearance 
among the total n appearances of a word w, four 
position features are defined based on i and n 
using different formulas as described below. 
(1) Direct proportion (DP) With the word 
position hypothesis, an intuitive idea is to regard 
the information degree of the first appearance as 
1 and the last one as 1/n, and then let the degree 
decrease linearly to the position i. So we can 
obtain the first position feature defined by the 
direct proportion function, i.e., f(i)=(n-i+1)/n. 
(2) Inverse proportion (IP). Besides the linear 
function, other functions can also be used to 
characterize the relationship between the 
position and the importance. The second 
position feature adopts another widely-used 
function, the inversed proportion function, i.e., 
f(i)=1/i. This measure is similar to the above 
one, but the information degree decreases by the 
inverse proportional function. Therefore, the 
degree decreases more quickly at smaller 
positions, which implies a stronger preference 
for leading sentences. 
(3) Geometric sequence (GS). For the third 
feature, we make an assumption that the degree 
of every appearance is the sum of the degree of 
all the following appearances, i.e., f(i) = f(i+1)+ 
f(i+2)+?+ f(n). It can be easily derived that the 
sequence also satisfies f(i) = 2?f(i-1). That is, the 
information degree of each new appearance is 
921
halved. Then the feature value of the ith 
appearance can be calculated as f(i) = (1/2)i-1.  
(4) Binary function (BF). The final feature is a 
binary position feature that regards the first 
appearance as much more informative than the 
all the other appearances, i.e., f(i)=1, if i=1; ? 
else, where ? is a small positive real number.  
3.3 Incorporating the Position Features  
To incorporate the position features into the 
word-based summarization model, we use them 
to adjust the importance of the word appearance. 
For the ith appearance of a word w, its original 
importance is multiplied by the position feature 
value, i.e., log freq(w)?pos(w, i), where pos(w, i) 
is calculated by one of the four position features 
introduced above. By this, the position feature is 
also incorporated into the sentence scores, i.e., 
score?(s) = ?i [log freq(wi) ? pos(wi)] / |s| 
3.4 Sentence Position Features 
In our study, another type of position features, 
which model sentence position information, is 
defined for comparison with the word position 
features. The sentence position features are also 
defined by the above four formulas. However, 
for each appearance, the definition of i and n in 
the formulas are changed to the ordinal position 
of the sentence that contains this appearance 
and the total number of sentences in the 
document respectively. In fact, the effects of the 
features defined in this way are equivalent to 
traditional sentence position features. Since i 
and n are now defined by sentence positions, the 
feature values of the word tokens in the same 
sentence s are all equal. Denote it by pos(s), and 
the sentence score with the position feature can 
be written as  
score?(s) = ( ?w in slogfreq(w) ? pos(s))/|s|  
= pos(s)?(? logw in s freq(w)/|s|), 
which can just be viewed as the product of the 
original score and a sentence position feature. 
3.5 Discussion 
By using the four functions to measure word or 
sentence position information, we can generate 
a total of eight position features. Among the 
four functions, the importance drops fastest 
under the binary function and the order is BF > 
GS > IP > DP. Therefore, the features based on 
the binary function are the most biased to the 
leading sentences in the document and the 
features based on the direct proportion function 
are the least. On the other hand, as mentioned in 
the introduction, sentence-based features have 
larger preferences for leading sentences than 
word-based position features.  
An example is given below to illustrate the 
difference between word and sentence position 
features. This is a document from DUC 2001. 
1. GENERAL ACCIDENT, the leading British 
insurer, said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as 
much as Dollars 40m.'  
2. Lord Airlie, the chairman who was 
addressing an extraordinary shareholders' 
meeting, said: 'On the basis of emerging 
information, General Accident advise that the 
losses to their US operations arising from 
Hurricane Andrew, which struck Florida and 
Louisiana, might in total reach the level at 
which external catastrophe reinsurance covers 
would become exposed'.  
3. What this means is that GA is able to pass on 
its losses to external reinsurers once a certain 
claims threshold has been breached.  
4. It believes this threshold may be breached in 
respect of Hurricane Andrew claims.  
5. However, if this happens, it would suffer a 
post-tax loss of Dollars 40m (Pounds 20m).  
6. Mr Nelson Robertson, GA's chief general 
manager, explained later that the company has a  
1/2 per cent share of the Florida market.  
7. It has a branch in Orlando.  
8. The company's loss adjusters are in the area 
trying to estimate the losses.  
9. Their guess is that losses to be faced by all 
insurers may total more than Dollars 8bn.  
10. Not all damaged property in the area is 
insured and there have been estimates that the 
storm caused more than Dollars 20bn of 
damage.  
11. However, other insurers have estimated that 
losses could be as low as Dollars 1bn in total. 
12 Mr Robertson said: 'No one knows at this 
time what the exact loss is'. 
For the word ?threshold? which appears 
twice in the document, its original importance is 
log(2), for the appearance of ?threshold? in the 
4th sentence, the modified score based on word 
position feature with the direct proportion 
function is 1/2?log(2). In contrast, the score 
based on sentence position feature with the 
922
same function is 9/12?log(2), which is larger. 
For the appearance of the word ?estimate? in the 
8th sentence, its original importance is log(3) 
(the three boldfaced tokens are regarded as one 
word with stemming). The word-based and 
sentence-based scores are log(3) and 5/12?log(3) 
respectively. So its importance is larger under 
word position feature. Therefore, the system 
with word position features may prefer the 8th 
sentence that is in deeper positions but the 
system with sentence position feature may 
prefer the 4th sentence. As for this document, the 
top 5 sentences selected by sentence position 
feature are {1, 4, 3, 5, 2} and the those selected 
by the word position features are {1, 8, 3, 6, 9}. 
This clearly demonstrates the difference 
between the position features. 
4 Experimental Results 
4.1 Experiment Settings 
We conduct the experiments on the data sets 
from the Document Understanding Conference 
(DUC) run by NIST. The DUC competition 
started at year 2001 and has successfully 
evaluated various summarization tasks up to 
now. In the experiments, we evaluate the 
effectiveness of position information on several 
DUC data sets that involve various 
summarization tasks. One of the evaluation 
criteria used in DUC, the automatic 
summarization evaluation package ROUGE, is 
used to evaluate the effectiveness of the 
proposed word position features in the context 
of document summarization1. The recall scores 
of ROUGE-1 and ROUGE-2, which are based 
on unigram and bigram matching between 
system summaries and reference summaries, are 
adopted as the evaluation criteria.  
In the data sets used in the experiments, the 
original documents are all pre-processed by 
sentence segmentation, stop-word removal and 
word stemming. Based on the word-based 
summarization model, a total of nine systems 
are evaluated in the experiments, including the 
system with the original ranking model (denoted 
as None), four systems with each word position 
feature (denoted as WP) and four systems with 
each sentence position feature (denoted as SP). 
                                                 
1 We run ROUGE-1.5.5 with the parameters ?-x -m -
n 2 -2 4 -u -c 95 -p 0.5 -t 0? 
For reference, the average ROUGE scores of all 
the human summarizers and all the submitted 
systems from the official results of NIST are 
also given (denoted as Hum and NIST 
respectively).  
4.2 Redundancy Removal 
To reduce the redundancy in the generated 
summaries, we use an approach similar to the 
maximum marginal relevance (MMR) approach 
in the sentence selection process (Carbonell and 
Goldstein, 1998). In each round of the sentence 
selection, the candidate sentence is compared 
against the already-selected sentences. The 
sentence is added to the summary only if it is 
not significantly similar to any already-selected 
sentence, which is judged by the condition that 
the cosine similarity between the two sentences 
is less than 0.7. 
4.3 Generic Summarization 
In the first experiment, we use the DUC 2001 
data set for generic single-document 
summarization and the DUC 2004 data set for 
generic multi-document summarization. The 
DUC 2001 data set contains 303 document-
summary pairs; the DUC 2004 data set contains 
45 document sets, with each set consisting of 10 
documents. A summary is required for each 
document set. Here we need to adjust the 
ranking model for the multi-document task, i.e., 
the importance of a word is calculated as its 
total frequency in the whole document set 
instead of a single document. For both tasks, the 
summary length limit is 100 words. 
Table 1 and 2 below provide the average 
ROUGE-1 and ROUGE-2 scores (denoted as R-
1 and R-2) of all the systems. Moreover, we 
used paired two sample t-test to calculate the 
significance of the differences between a pair of 
word and sentence position features. The bolded 
score in the tables indicates that that score is 
significantly better than the corresponding 
paired one. For example, in Table 1, the bolded 
R-1 score of system WP DP means that it is 
significantly better than the R-1 score of system 
SP DP. Besides the ROUGE scores, two 
statistics, the number of ?first sentences 2 ? 
among the selected sentences (FS-N) and the 
                                                 
2 A ?first sentence? is the sentence at the fist position 
of a document.  
923
average position of the selected sentences (A-
SP), are also reported in the tables for analysis.  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.4473 0.1942 301 4.00 
SP DP 0.4396 0.1844 300 3.69 
WP IP 0.4543 0.2023 290 4.30 
SP IP 0.4502 0.1964 303 3.08 
WP GS 0.4544 0.2041 278 4.50 
SP GS 0.4509 0.1974 303 2.93 
WP BF 0.4544 0.2036 253 5.57 
SP BF 0.4239 0.1668 303 9.64 
None 0.4193 0.1626 265 10.06
NIST 0.4445 0.1865 - - 
Hum 0.4568 0.1740 - - 
Table 1. Results on the DUC 2001 data set  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.3728 0.0911 89 4.16 
SP DP 0.3724 0.0908 112 2.68 
WP IP 0.3756 0.0912 108 3.77 
SP IP 0.3690 0.0905 201 1.01 
WP GS 0.3751 0.0916 110 3.67 
SP GS 0.3690 0.0905 201 1.01 
WP BF 0.3740 0.0926 127 3.14 
SP BF 0.3685 0.0903 203 1 
None 0.3550 0.0745 36 10.98
NIST 0.3340 0.0686 - - 
Hum 0.4002 0.0962 - - 
Table 2. Results on the DUC 2004 data set 
 
From Table 1 and Table 2, it is observed that 
position information is indeed very effective in 
generic summarization so that all the systems 
with position features performed better than the 
system None which does not use any position 
information. Moreover, it is also clear that the 
proposed word position features consistently 
outperform the corresponding sentence position 
features. Though the gaps between the ROUGE 
scores are not large, the t-tests proved that word 
position features are significantly better on the 
DUC 2001 data set. On the other hand, the 
advantages of word position features over 
sentence position features are less significant on 
the DUC 2004 data set. One reason may be that 
the multiple documents have provided more 
candidate sentences for composing the summary. 
Thus it is possible to generate a good summary 
only from the leading sentences in the 
documents. According to Table 2, the average-
sentence-position of system SP BF is 1, which 
means that all the selected sentences are ?first 
sentences?. Even under this extreme condition, 
the performance is not much worse. 
The two statistics also show the different 
preferences of the features. Compared to word 
position features, sentence position features are 
likely to select more ?first sentences? and also 
have smaller average-sentence-positions. The 
abnormally large average-sentence-position of 
SP BF in DUC 2001 is because it does not 
differentiate all the other sentences except the 
first one. The corresponding word-position-
based system WP BF can differentiate the 
sentences since it is based on word positions, so 
its average-sentence-position is not that large. 
4.4 Query-focused Summarization 
Since year 2005, DUC has adopted query-
focused multi-document summarization tasks 
that require creating a summary from a set of 
documents to a given query. This task has been 
specified as the main evaluation task over three 
years (2005-2007). The data set of each year 
contains about 50 DUC topics, with each topic 
including 25-50 documents and a query. In this 
experiment, we adjust the calculation of the 
word importance again for the query-focused 
issue. It is changed to the total number of the 
appearances that fall into the sentences with at 
least one word in the query. Formally, given the 
query which is viewed as a set of words 
Q={w1, ?, wT}, a sentence set SQ is defined as 
the set of sentences that contain at least one wi 
in Q. Then the importance of a word w is 
calculated by its frequency in SQ. For the query-
focused task, the summary length limit is 250 
words. 
Table 3 below provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
DUC 2005-2007 data sets. The boldfaced terms 
in the tables indicate the best results in each 
column. According to the results, on query-
focused summarization, position information 
seems to be not as effective as on generic 
summarization. The systems with position 
features can not outperform the system None. In 
fact, this is reasonable due to the requirement 
specified by the pre-defined query. Given the 
query, the content of interest may be in any 
924
position of the document and thus the position 
information becomes less meaningful.  
On the other hand, we find that though the 
systems with word position features cannot 
outperform the system None, it does 
significantly outperform the systems with 
sentence position features. This is also due to 
the role of the query. Since it may refer to the 
specified content in any position of the 
documents, sentence position features are more 
likely to fail in discovering the desired 
sentences since they always prefer leading 
sentences. In contrast, word position features 
are less sensitive to this problem and thus 
perform better. Similarly, we can see that the 
direct proportion (DP), which has the least bias 
for leading sentences, has the best performance 
among the four functions. 
System 2005 2006 2007 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3791 0.0805 0.3909 0.0917 0.4158 0.1135 
SP DP 0.3727 0.0776 0.3832 0.0869 0.4118 0.1103 
WP IP 0.3772 0.0791 0.3830 0.0886 0.4106 0.1121 
SP IP 0.3618 0.0715 0.3590 0.0739 0.3909 0.1027 
WP GS 0.3767 0.0794 0.3836 0.0879 0.4109 0.1119 
SP GS 0.3616 0.0716 0.3590 0.0739 0.3909 0.1027 
WP BF 0.3740 0.0741 0.3642 0.0796 0.3962 0.1037 
SP BF 0.3647 0.0686 0.3547 0.0742 0.3852 0.1013 
NONE 0.3788 0.0791 0.3936 0.0924 0.4193 0.1140 
NIST 0.3353 0.0592 0.3707 0.0741 0.0962 0.3978 
Hum 0.4392 0.1022 0.4532 0.1101 0.4757 0.1402 
Table 3. Results on the DUC 2005 - 2007 data sets 
 
System 2008 A 2008 B 2009 A 2009 B R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3687 0.0978 0.3758 0.1036 0.3759 0.1015 0.3693 0.0922 
SP DP 0.3687 0.0971 0.3723 0.1011 0.3763 0.1031 0.3704 0.0946 
WP IP 0.3709 0.1014 0.3741 0.1058 0.3758 0.1030 0.3723 0.0906 
SP IP 0.3619 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP GS 0.3705 0.1004 0.3732 0.1048 0.3770 0.1051 0.3731 0.0917 
SP GS 0.3625 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP BF 0.3661 0.0975 0.3678 0.0992 0.3720 0.1069 0.3650 0.0936 
SP BF 0.3658 0.0965 0.3674 0.0980 0.3683 0.1043 0.3654 0.0945 
NONE 0.3697 0.0978 0.3656 0.0915 0.3653 0.0934 0.3595 0.0834 
NIST 0.3389 0.0799 0.3192 0.0676 0.3468 0.0890 0.3315 0.0761 
Hum 0.4105 0.1156 0.3948 0.1134 0.4235 0.1249 0.3901 0.1059 
Table 4. Results on the TAC 2008 - 2009 data sets 
 
4.5 Update Summarization 
Since year 2008, the DUC summarization track 
has become a part of the Text Analysis 
Conference (TAC). In the update summarization 
task, each document set is divided into two 
ordered sets A and B. The summarization target 
on set A is the same as the query-focused task in 
DUC 2005-2007. As to the set B, the target is to 
write an update summary of the documents in 
set B, under the assumption that the reader has 
already read the documents in set A. The data 
set of each year contains about 50 topics, and 
each topic includes 10 documents for set A, 10 
documents for set B and an additional query. 
For set A, we follow exactly the same method 
used in section 4.4; for set B, we make an 
additional novelty check for the sentences in B 
with the MMR approach. Each candidate 
sentence for set B is now compared to both the 
selected sentences in set B and in set A to 
925
ensure its novelty. In the update task, the 
summary length limit is 100 words.  
Table 4 above provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
TAC 2008-2009 data sets. The results on set A 
and set B are shown individually. For the task 
on set A which is almost the same as the DUC 
2005-2007 tasks, the results are also very 
similar. A small difference is that the systems 
with position features perform slightly better 
than the system None on these two data sets. 
Also, the difference between word position 
features and sentence position features becomes 
smaller. One reason may be that the shorter 
summary length increases the chance of 
generating good summaries only from the 
leading sentences. This is somewhat similar to 
the results reported in (Nenkova, 2005) that 
position information is more effective for short 
summaries. 
For the update set B, the results show that 
position information is indeed very effective. In 
the results, all the systems with position features 
significantly outperform the system None. We 
attribute the reason to the fact that we are more 
concerned with novel information when 
summarizing update set B. Therefore, the effect 
of the query is less on set B, which means that 
the effect of position information may be more 
pronounced in contrast. On the other hand, 
when comparing the position features, we can 
see that though the difference of the position 
features is quite small, word position features 
are still better in most cases.  
4.6 Discussion 
Based on the experiments, we briefly conclude 
the effectiveness of position information in 
document summarization. In different tasks, the 
effectiveness varies indeed. It depends on 
whether the given task has a preference for the 
sentences at particular positions. Generally, in 
generic summarization, the position hypothesis 
works well and thus the ordinal position 
information is effective. In this case, those 
position features that are more distinctive, such 
as GS and BF, can achieve better performances. 
In contrast, in the query-focused task that relates 
to specified content in the documents, ordinal 
position information is not so useful. Therefore, 
the more distinctive a position feature is, the 
worse performance it leads to. However, in the 
update summarization task that also involves 
queries, position information becomes effective 
again since the role of the query is less 
dominant on the update document set.   
On the other hand, by comparing the sentence 
position features and word position features on 
all the data sets, we can draw an overall 
conclusion that word position features are 
consistently more appreciated. For both generic 
tasks in which position information is effective 
and query-focused tasks in which it is not so 
effective, word position features show their 
advantages over sentence position features. This 
is because of the looser position hypothesis 
postulated by them. By avoiding arbitrarily 
regarding the leading sentences as more 
important, they are more adaptive to different 
tasks and data sets. 
5 Conclusion and Future Work 
In this paper, we proposed a novel kind of word 
position features which consider the positions of 
word appearances instead of sentence positions. 
The word position features were compared to 
sentence position features under the proposed 
sentence ranking model. From the results on a 
series of DUC data sets, we drew the conclusion 
that the word position features are more 
effective and adaptive than traditional sentence 
position features. Moreover, we also discussed 
the effectiveness of position information in 
different summarization tasks. 
In our future work, we?d like to conduct more 
detailed analysis on position information. 
Besides the ordinal positions, more kinds of 
position information can be considered to better 
model the document structures. Moreover, since 
position hypothesis is not always correct in all 
documents, we?d also like to consider a pre-
classification method, aiming at identifying the 
documents for which position information is 
more suitable. 
 
Acknowledgement The work described in 
this paper was supported by Hong Kong RGC 
Projects (PolyU5217/07E). We are grateful to 
professor Chu-Ren Huang for his insightful 
suggestions and discussions with us. 
926
References
Edmundson, H. P.. 1969. New methods in automatic 
Extracting. Journal of the ACM, volume 16, issue 
2, pp 264-285. 
Gillick, D., Favre, B., Hakkani-Tur, D., Bohnet, B., 
Liu, Y., Xie, S.. 2009. The ICSI/UTD 
Summarization System at TAC 2009. Proceedings 
of Text Analysis Conference 2009.  
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for 
reordering documents and producing summaries. 
Proceedings of the 21st annual international ACM 
SIGIR conference on Research and development 
in information retrieval, pp 335-336. 
Lin, C. and Hovy, E.. 1997. Identifying Topics by 
Position. Proceedings of the fifth conference on 
Applied natural language processing 1997, pp 
283-290. 
Luhn, H. P.. 1958. The automatic creation of 
literature abstracts. IBM J. Res. Develop. 2, 2, pp 
159-165. 
Nenkova. 2005. Automatic text summarization of 
newswire: lessons learned from the document 
understanding conference. Proceedings of the 
20th National Conference on Artificial 
Intelligence, pp 1436-1441. 
Ouyang, Y., Li, S., Li, W.. 2007. Developing 
learning strategies for topic-based summarization. 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp 79-86. 
Radev, D., Jing, H., Sty?s, M. and Tam, D.. 2004. 
Centroid-based summarization of multiple 
documents. Information Processing and 
Management, volume 40, pp 919?938. 
Schilder, F., Kondadadi, R.. 2008. FastSum: fast and 
accurate query-based multi-document 
summarization. Proceedings of the 46th Annual 
Meeting of the Association for Computational 
Linguistics on Human Language Technologies, 
short paper session, pp 205-208. 
Toutanova, K. et al 2007. The PYTHY 
summarization system: Microsoft research at 
DUC 2007. Proceedings of Document 
Understanding Conference 2007.  
 
927
Coling 2010: Poster Volume, pages 1489?1497,
Beijing, August 2010
Sentence Ordering with Event-Enriched Semantics and Two-
Layered Clustering for Multi-Document News Summarization
Renxian Zhang                Wenjie Li                   Qin Lu      
Department of Computing, the Hong Kong Polytechnic University
{csrzhang,cswjli,csluqin}@comp.polyu.edu.hk
Abstract
We propose an event-enriched model to 
alleviate the semantic deficiency 
problem in the IR-style text processing 
and apply it to sentence ordering for 
multi-document news summarization.
The ordering algorithm is built on event 
and entity coherence, both locally and 
globally. To accommodate the event-
enriched model, a novel LSA-integrated 
two-layered clustering approach is 
adopted. The experimental result shows 
clear advantage of our model over 
event-agonistic models.
1 Introduction
One of the crucial steps in multi-document 
summarization (MDS) is information ordering, 
right after content selection and before sentence 
realization (Jurafsky and Martin, 2009:832?
834). Problems with this step are the culprit for 
much of the dissatisfaction with automatic 
summaries. While textual order may guide the 
ordering in single-document summarization, no 
such guidance is available for MDS ordering. 
A sensible solution is ordering sentences by 
enhancing coherence since incoherence is the 
source of disorder. Recent researches in this 
direction mostly focus on local coherence by 
studying lexical cohesion (Conroy et al, 2006) 
or entity overlap and transition (Barzilay and 
Lapata, 2008). But global coherence, i.e., 
coherence between sentence groups with the 
whole text in view, is largely unaccounted for 
and few efforts are made at levels higher than 
entity or word in measuring sentence coherence.
On the other hand, event as a high-level 
construct has proved useful in MDS content 
selection (Filatova and Hatzivassiloglou, 2004; 
Li et al, 2006). But the potential of event in 
summarization has not been fully gauged and 
few publications report using event in MDS 
information ordering. We will argue that event 
is instrumental for MDS information ordering, 
especially multi-document news summarization 
(MDNS). Ordering algorithms based on event 
and entity information outperform those based 
only on entity information.
After related works are surveyed in section 2, 
we will discuss in section 3 the problem of 
semantic deficiency in IR-based text processing, 
which motivates building event information into 
sentence representation. The details of such 
representation are provided in section 4. In 
section 5, we will explicate the ordering 
algorithms, including layered clustering and 
cluster-based ordering. The performance of the 
event-enriched model will be extensively 
evaluated in section 6. Section 7 will conclude 
the work with directions to future work.
2 Related Work
In MDS, information ordering is often realized 
on the sentence level and treated as a coherence 
enhancement task. A simple ordering criterion 
is the chronological order of the events 
represented in the sentences, which is often 
augmented with other ordering criteria such as 
lexical overlap (Conroy et al, 2006), lexical
cohesion (Barzilay et al, 2002) or syntactic 
features (Lapata 2003).
A different way to capture local coherence in 
sentence ordering is the Centering Theory (CT, 
Grosz et al 1995)-inspired entity-transition 
approach, advocated by Barzilay and Lapata 
(2005, 2008). In their entity grid model, 
syntactic roles played by entities and transitions 
between these syntactic roles underlie the 
coherence patterns between sentences and in the 
1489
whole text. An entity-parsed corpus can be used 
to train a model that prefers the sentence 
orderings that comply with the optimal entity 
transition patterns.
Another important clue to sentence ordering 
is the sentence positional information in a 
source document, or ?precedence relation?, 
which is utilized by Okazaki et al (2004) in 
combination with topical clustering.
Those works are all relevant to the current 
work because we seek ordering clues from 
chronological order, lexical cohesion, entity 
transition, and sentence precedence. But we also 
add an important member to the panoply ? event.  
Despite its intuitive and conceptual appeal, 
event is not as extensively used in 
summarization as term or entity. Filatova and 
Hatzivassiloglou (2004) use ?atomic events? as 
conceptual representations in MDS content 
selection, followed by Li et al (2006) who treat 
event terms and named entities as graph nodes 
in their PageRank algorithm. Yoshioka and 
Haraguchi (2004) report an event reference-
based approach to MDS content selection for 
Japanese articles. Although ?sentence 
reordering? is a component of their model, it 
relies merely on textual and chronological order. 
Few published works report using event 
information in MDS sentence ordering.
Our work will represent text content at two 
levels: event vectors and sentence vectors. This 
is close in spirit to Bromberg?s (2006) enriched 
LSA-coherence model, where both sentence and 
word vectors are used to compute a centroid as 
the topic of the text. 
3 Semantic Deficiency in IR-Style Text 
Processing
As automatic summarization traces its root to 
Information Retrieval (IR), it inherits the vector 
space model (VSM) of text representation,
according to which a sentence is treated as a bag 
of words or stoplist-filtered terms. The order or 
relation among the terms is ignored. For 
example,
1a) The storm killed 120,000 people in Jamaica 
and five in the Dominican Republic before moving 
west to Mexico.
1b) [Dominican, Mexico, Jamaica, Republic, five,
kill, move, people, storm, west]
1c) [Dominican Republic, Mexico, Jamaica,
people, storm]
1b) and 1c) are the term-based and entity-
based representations of 1a) respectively. They
only indicate what the sentence is about (i.e., 
some happening, probably a storm, in some 
place that affects people), but ?aboutness? is a 
far cry from informativeness. For instance, no 
message about ?people in which place, Mexico 
or Jamaica, are affected? or ?what moves to 
where? can be gleaned from 1b) although such 
message is clearly conveyed in 1a). In other 
words, the IR-style text representation is 
semantically deficient. 
We argue that a natural text, especially a 
news article, is not only about somebody or 
something. It also tells what happened to 
somebody or something in a temporal-spatial 
manner. A natural approach to meeting the 
?what happened? requirement is to introduce 
event.
4 Event-Enriched SentenceRepresentation 
In summarization, an event is an activity or 
episode associated with participants, time, place, 
and manner. Conceptually, event bridges 
sentence and term/entity and partially fills the 
semantic gap in the sentence representation.
4.1 Event Structure and Extraction
Following (Li et al 2006), we define an event E
as a structured semantic unit consisting of one 
event term Term(E) and a set of event entities 
Entity(E). In the news domain, event terms are 
typically action verbs or deverbal nouns. Light 
verbs such as ?take?, ?give?, etc. (Tan et al,
2006) are removed.
Event entities include named entities and 
high-frequency entities. Named entities denote 
people, locations, organizations, dates, etc. 
High-frequency entities are common nouns or 
NPs that frequently participate in news events. 
Filatova and Hatzivassiloglou (2004) take the 
top 10 most frequent entities and Li et al (2006)
take the entities with frequency > 10. Rather 
than using a fixed threshold, we reformulate 
?high-frequency? as relative statistics based on 
(assumed) Gaussian distribution of the entities 
and consider those with z-score > 1 as candidate 
event entities. 
Event extraction begins with shallow parsing 
and named entity recognition, analyzing each 
1490
sentence S into ordered lists of event terms {t1,
t2, ?}. Low-frequency common entities are 
removed. If a noun is decided to be an event 
term, it cannot be (the head noun of) an entity.
The next step is to identify events with event 
terms and entities. Filatova and 
Hatzivassiloglou (2003) treat events as triplets 
with two event entities sandwiching one 
connector (event term). But the number 
restriction on entities is counterintuitive and is 
dropped in our method. We first identify n + 1
Segi segmented by n event terms tj.
? t1 ? ? tj-1 ? tj ? tj+1 ? ? tn ?
Figure 1. Segments among Event Terms
For each tj, the corresponding event Ej are 
extracted by taking tj and the event entities in its 
nearest entity-containing Segp and Segq.
Ej = [tj, Entity(Segp)?Entity(Segq)]            (Eq. 1)
where p = argmax?????????????(????) ? ? and q
= argmin?????????????(????) ? ? if such p and q
exist. 1d) is the event-extracted result of 1a).
1d) {[killed, [storm, people, Jamaica, Dominican
Republic]], [moving, [people, Jamaica, Dominican
Republic, west, Mexico]]}
From this representation, it is easy to identify 
the two events in sentence 1a) led by the event 
terms ?killed? and ?moving?. Unlike the triplets 
(two named entities and one connector) in 
(Filatova and Hatzivassiloglou 2003), an event 
in our model can have an unlimited number of 
event entities, as is often the real case. 
Moreover, we can tell that the ?killing? involves
?people?, ?storm?, ?Jamaica?, etc. and the 
?moving? involves ?Jamaica?, ?Dominique 
Republic?, etc.
The shallow parsing-based approach is 
admittedly coarse-grade (e.g., ?storm? is 
missing from the ?moving? event), but the 
extracted event-enriched representations help to 
alleviate the semantic deficiency problem in IR.
4.2 Event Relations
The relations between two events include event 
term relation and event entity relation. Two 
events are similar if their event terms are similar 
and/or their event entities are similar. Such
similarities are in turn defined on the word level. 
For event terms, we first find the root verbs of 
deverbal nouns and then measure verb similarity 
by using the fine-grained relations provided by 
VerbOcean (Chklovski and Pantel, 2004), 
which has proved useful in summarization (Liu 
et al, 2007). But unlike (Liu et al, 2007), we 
count in all the verb relations except antonymy
because considering two antonymous verbs as 
similar is counterintuitive. The other four 
relations ? similarity, strength, enablement,
before ? are all considered in our measurement 
of verb similarity. If we denote the normalized 
score of two verbs on relation i as VOi(V1, V2)
with i = 1, 2, 3, 4 corresponding to the above 
four relations, the term similarity of two events
?t(E1, E2) is defined as in Eq. 2, where ? is a 
small number to suppress zeroes. ? = 0.01 if
VOi(V1, V2) = 1 and otherwise ? = 0.
?t(E1, E2) = ?t(Term(E1), Term(E2)) = 1 ?
? (1 ? ? ??(???)???),???)???)) + ????? ) (Eq. 2)
Entity similarity is measured by the shared 
entities between two events. Li et al (2006) 
define entity similarity as the number of shared 
entities, which may unfairly assign high scores 
to events with many entities in our model. So 
we decide to use the normalized result as shown 
in Eq. 3, where ?e(E1, E2) denotes the event 
entity-based similarity between events E1 and E2.
?e(E1, E2) = 
|??????(??)???????(??)|
|??????(??)???????(??)|
(Eq. 3)
?(E1, E2), the score of event similarity, is a 
linear combination of ?t(E1, E2) and ?e(E1, E2).
?(E1, E2) = ?1 ? ?t(E1, E2) + (1 ? ?1) ? ?e(E1, E2) (Eq. 4)
4.3 Statistical Evidence for News Events
In this work, we introduce events as a middle-
layer representation between words and 
sentences under the assumptions that 1) events 
are widely distributed in a text and that 2) they 
are natural clusters of salient information in a 
text. They guarantee the relevance of event to 
our task ? summaries are condensed collections 
of salient information in source documents.
In order to confirm them, we scan the whole 
dataset in our experiment, which consists of 42 
200w human extracts and 39 400w human 
extracts for the DUC 02 multi-document extract 
task. Detailed information about the dataset can 
be found in Section 6. Table 1 lists the statistics.
200w 400w
200w +
400w
Source
Docs
Entity/Sent 8.78 8.48 8.47 6.01
Entity/Word 0.34 0.33 0.33 0.30
Event/Sent 2.43 2.26 2.28 1.42
SegnSegj-1 SegjSeg0
1491
Event/Word 0.09 0.09 0.09 0.07
Sents with
events/Sents
86.9% 85.1% 84.6% 71.3%
Table 1. Statistics from DUC 02 Dataset
There are on average 1.42 events per sentence 
in the source documents, and more than 70% of 
all the sentences contain events. The high event 
density confirms our first assumption about the 
distribution of events. For the 200w+400w 
category consisting of all the human-selected 
sentences, there are on average 2.28 events per
sentence, a 60% increase from the same ratio in 
the source documents. The proportion of event-
containing sentences reaches 84.6%, 13% 
higher than that in the source documents. Such 
is evidence that events count into the extract-
worthiness of sentences, which confirms our 
second assumption about the relevance of 
events to summarization. The data also show 
higher entity density in the extracts than in the 
source documents. As entities are still reliable 
and domain-independent clues of salient content,
we will consider both event and entity in the 
following ordering algorithm.
5 MDS Sentence Ordering with Event 
and Entity Coherence
In this section, we discuss how event can 
facilitate MDS sentence ordering with layered 
clustering on the event and sentence levels and 
then how event and entity information can be 
integrated in a coherence-based algorithm to 
order sentences based on sentence clusters.
5.1 Two-layered Clustering
After sentences are represented as collections of 
events, we need to vectorize events and 
sentences to facilitate clustering and cluster-
based sentence ordering. 
For a document set, event vectorization 
begins with aggregating all the event terms and 
entities in a set of event units (eu). Given m
distinct event terms, n distinct named entities, 
and p distinct high-frequency common entities, 
the m + n + p eu?s are a concatenation of the 
event terms and entities such that eui is an event 
term for 1 ? i ? m, a named entity for m + 1 ? i
? m + n, and a high-frequency entity for m + n +
1 ? i ? m + n + p. The eu?s define the m + n + p
dimensions of an event vector in an eu-by-event 
matrix E = [eij], as shown in Figure 2.
?
?
?
?
?
?
?
??? ? ???
? ? ?
??? ? ???
? ?
????,? ? ????,?
? ?
??????,? ? ??????,??
?
?
?
?
?
?
Figure 2. eu-by-Event Matrix
We further define EntityN(Ej) and EntityH(Ej)
to be the set of named entities and set of high-
frequency entities of Ej. Then,
??(???,???)???)) 1 ? i ? m
eij =
? ??(???,?)?????????(??)
????????(??)?
m + 1 ? i ? m + n
? ??(???,?)?????????(??)
????????(??)?
m + n + 1 ? i ?
m + n + p (Eq. 5)
                     2 w1 is identical to w2
?n(w1, w2) =  1 w1 (w2) is a part of w2 (w1) or they 
are in a hypernymy / holonymy 
relationship
             0 otherwise                          (Eq. 6)
1 w1 is identical to w2
?h(w1, w2) = 0.5 w1 are w2 are synonyms
0 otherwise                       (Eq. 7)
In Eq. 5, ?t(w1, w2) is defined as in Eq. 2.
Both the entity-based ?n(w1, w2) and ?h(w1, w2)
are measured in terms of total equivalence 
(identity) and partial equivalence. For named 
entities, partial equivalence applies to structural 
subsumption (e.g., ?Britain? and ?Great Britain?) 
and hypernymy/holonymy (e.g., ?South Africa? 
and ?Zambia?). For common entities, it applies 
to synonymy (e.g., ?security? and ?safety?). 
Partial equivalence is considered because of the 
lexical variations frequently employed in 
journalist writing. The named entity scores are 
doubled because they represent the essential 
elements of a news story.
Since the events are represented as vectors, 
sentence vectorization based on events is not as 
straightforward as on entities or terms. In this 
work we propose a novel approach of two-
layered clustering for the purpose. The basic 
idea is clustering events at the first layer and 
then using event clusters as a feature to 
vectorize and cluster sentences at the second 
E1, E2, ? Eq
eu1
?
eum
?
eum+n
...
eum+n+p
1492
layer. Hard clustering of events, such as K-
means, not only results in binary values in event 
vectors and data sparseness but also is 
inappropriate. For example, if EC1 clusters 
events all with event terms similar to t* and EC2
clusters events all with event entity sets similar 
to e* (a set), what about event {t*, e*}? 
Assigning it to either EC1 or EC2 is problematic
as it is partially similar to both. So we decide to 
do soft clustering at the first layer.
A well-studied soft clustering technique is the 
Expectation-Maximization (EM) algorithm 
which iteratively estimates the unknown 
parameters in a probability mixture model. We 
assume a Gaussian mixture model for the q
event vectors V1, V2, ?, Vq, with hidden 
variables Hi, initial means Mi, priors ?i, and 
covariance matrix Ci. The E-step is to calculate 
the hidden variables ??
? for each Vt and the M-
step re-estimates the new priors ?i?, means Mi?,
and covariance matrix Ci
?. We iterate the two 
steps until the log-likelihood converges within a 
threshold = 10-6. The performance of the EM 
algorithm is sensitive to the initial means, which 
are pre-computed by a conventional K-means.
In a preliminary study, we found that the 
event vectors display pronounced sparseness. A 
solution to this problem in an effort to leverage 
the latent ?event topics? among eu?s is the 
Latent Semantic Analysis (LSA, Landauer and 
Dumais, 1997) approach. We apply LSA-style 
dimensionality reduction to the eu-by-event 
matrix E by doing Singular Value 
Decomposition (SVD). A problem is with the 
number h of the largest singular values, which 
affects the performance of dimensionality 
reduction. In this work, we adopt a utility-based 
metric to find the best h* by maximizing intra-
cluster similarity (?h) and minimizing inter-
cluster similarity (?h) corresponding to the h-
dimensionality reduction
h* = argmax? ?? ???                               (Eq. 8)
?h is defined as the mean of average cluster 
similarities measured by cosine distance and ?h
is the mean of cluster centroid similarities. 
Because the EM clustering assigns a probability 
to every event vector, we also take those 
probabilities into account when calculating ?h
and ?h.
Based on the EM clustering of events, we 
vectorize a sentence by summing up the 
probabilities of its constituent event vectors 
over all event clusters (ECs) and obtaining an 
EC-by-sentence (Sn) matrix S = [sij].
                     ?
??? ? ???
? ? ?
??? ? ???
?
Figure 3. EC-by-Sentence Matrix
sij = ? P(????????????)????? where ?????? is Er?s vector.
At the sentence layer, hard clustering is 
sufficient because we need definitive, not 
probabilistic, membership information for the 
next step ? sentence ordering. We use K-means 
for the purpose. The LSA-style dimensionality 
reduction is still in order as possible 
performance gain is expected from the 
discovery of latent EC ?topics?. The decision of 
the best dimensionality is the same as before,
except that no probabilities are included.
5.2 Coherence-Based Sentence Ordering
Our ordering algorithm is based on sentence 
clusters, which is designed on the observation
that human writers and summarizers organize 
sentences by blocks (paragraphs). Sentences 
within a block are conceptually close to each 
other and adjacent sentences cohere with each 
other. Local coherence is thus realized within 
blocks. On the other hand, blocks are not 
randomly ordered. Two blocks are put next to 
each other if their contents are close enough to 
ensure text-level coherence. So text-level, or 
global coherence is realized among blocks. 
We believe in MDNS, the block-style 
organization is a sensible strategy taken by 
human extractors to sort sentences from 
different sources. Sentence clusters are 
simulations of such blocks and our ordering 
algorithm will be based on local coherence and 
global coherence described above. 
First we have to pinpoint the leading sentence 
for an extract. Using the heuristic of time and 
textual precedence, we first generate a set of 
possible leading sentences L = {Li} as the 
intersection of the document-leading extract 
sentence set LDoc and the time-leading sentence 
set LTime. Note that |LDoc| = the number of 
documents, LTime is in fact a sentence collection 
of time-leading documents, and LDoc ? LTime ? ?.
S1, S2, ? Sn
EC1
?
ECm
1493
If L is a singleton, finding the leading 
sentence SL is trivial. If not, SL is decided to be 
the sentence in L most similar to all the other 
sentences in the extract sentence set P so that it 
qualifies as a good topic sentence.
SL = argmax???? ? ??????(?? ,??)????\{??} (Eq. 9)
where ??????(??, ??) is the similarity between S1
and S2 in terms of their event similarity ?(S1, S2)
and entity similarity ?(S1, S2). ?(S1, S2) is an 
extended version of ?(E1, E2) (Eq. 4) by 
averaging the ?t(Ei, Ej) and ?e(Ei, Ej) for all (Ei,
Ej) pairs in S1 ?S2.
?(S1, S2) = ?2 ?
? ??(??,??)?????,?????
|?????(??)??????(??)|
+
(1 ? ?2)?
? ??(??,??)?????,?????
|?????(??)??????(??)|
              (Eq. 10)
where Event(S) is the set of all events in S. Next, 
?(S1, S2) is the cosine similarity between their 
entity vectors ?????? and ?????? with entity weights 
constructed according to Eq. 6 and 7. Then,
??????(??, ??) = ?3??(S1, S2) +(1 ? ?3)??(S1, S2) (Eq. 11)
After the leading sentence is determined, we 
identify the leading cluster it belongs to and our 
local coherence-based ordering starts with this 
cluster. We adopt a greedy algorithm, which 
selects each time from the unordered sentence 
set a sentence that best coheres with the 
sentence just selected, called anchor sentence.
Matching each candidate sentence with the 
anchor sentence only in terms of ?????? would 
assume that the sentences are isolated and 
decontextualized. But the anchor sentence did 
not come from nowhere and in order to find its 
best successor, we should also seek clues from 
its source context, which is inspired by the 
?sentence precedence? by Okazaki et al (2004).
More formally, given an anchor sentence Si at 
the end of the ordered sentence list, we select 
the next best sentence Si+1 according to their 
associative similarity and substitutive 
similarity, two crucial measures invented by us.
Associative similarity SimASS(Si, Sj) measures 
how Si and Sj associate with each other in terms 
of their event and entity coherence, which 
almost is ?????????, ???. But to better capture the 
transition between entities and the flow of topic, 
we also consider a topic-continuity score tc(Si,
Sj) according to the Centering Theory. If the 
topic continuity is measured in terms of entity 
change, local coherence can be captured by the 
centering transitions (CB and CP) in adjacent 
sentences. Based on (Taboada and Wiesemann,
2009), we assign 0.2 to the Establish and 
Continue transitions, 0.1 to Smooth Shift and 
Retain, and 0 to other centering transitions.
Since tc(Si, Sj) only applies to entities, it is 
treated as a bonus affiliated to ?(Si, Sj).
??????? ??, ??? = ?4 ? ?(Si, Sj) + (1 ? ?4) ? ?(Si, Sj)
? (1 + tc(Si, Sj))                                                 (Eq. 12)
Substitutive similarity accommodates what 
we earlier emphasized about the ?source context?
of the extracted sentences by measuring to what 
degree Si and Sj resemble each other?s relevant 
source context. More formally, let LC(Si) and 
RC(Si) be the left and right source contexts of Si
respectively, and the substitutive similarity 
SimSUB(Si, Sj) is defined as follows.
??????? ??, S?? = ??????? ??, ??( ??)? +
?????????( ??), S??                                       (Eq. 13)
In this work, we simply take LC(Si) and RC(Si)
to be the left adjacent sentence and right 
adjacent sentence of Si in the source document. 
Note that tc(Si, Sj) does not apply here. In view 
of the chronological order widely accepted in 
MDS ordering, a time penalty, tp(Si, Sj), is used 
to discount the score by 0.8 if Si?s document 
date is later than Sj?s document date. Finally, Eq.
14 summarizes our intra-cluster ordering 
method in a sentence cluster SCk.
Si+1 = argmax??????\{??} ??? ? ??????? ??, ??? +
(1 ? ??) ? ??????? ??, ???? ? ??( ??, ??) (Eq. 14)
After all the sentences in the current sentence 
cluster are ordered, we move on by considering 
the similarity of sentence clusters. Given a 
processed sentence cluster SCi, the next best 
sentence cluster SCi+1 is the one that maximizes 
the cluster similarity SimCLU(SCi, SCj) among 
the set of all clusters U. Since clusters are 
collections of sentences, their similarity is the 
mean of cross-cluster pairwise sentence 
similarities, each calculated according to Eq. 14.
Eq. 15 shows how SCi+1 is computed.
SCi+1=argmax?????\{???}??????(??? , ???) (Eq. 15)
This is how we incorporate (block-style) 
global coherence into MDS sentence ordering. 
Starting from the second chosen sentence 
cluster, we choose the first sentence in the 
current cluster with reference to the last 
sentence in the previous processed cluster and 
apply Eq. 14. We continue the whole process 
until all the extract sentences are ordered.
1494
6 Evaluation
In this section, we report the experimental result 
on the DUC 02 dataset.
6.1 Data
We use the dataset of the DUC 02 
summarization track for MDS because it 
includes an extraction task for which model 
extracts are provided. For every document set, 2 
model extracts are provided each for the 200w 
and 400w length categories. We use 1 randomly 
chosen model extract per document set per 
length category as the gold standard.
We intended to use all the 59 document sets 
on DUC 02 but found that for some categories, 
both model extracts contain material from 
sections such as the title, lead, or even byline.
Those extracts are incompatible with our design 
tailored for news body extracts. Therefore we 
have to filter them and retain only those extracts 
with all units selected from the news body. As a 
result, we collect 42 200w extracts and 39 400w 
extracts as our experimental dataset.
6.2 Peer Orderings
We evaluate the role played by various key 
elements in our approach, including event, topic 
continuity, time penalty, and LSA-style 
dimensionality reduction. In addition, we 
produce a random ordering and a baseline 
ordering according to chronological and textual 
order only. Table 2 lists the 9 peer orderings to 
be evaluated, with their codes.
A Random
B Baseline (time order + textual order)
C Entity only (no LSA)
D Event only (no LSA)
E Entity + Event ? topic continuity (no LSA)
F Entity + Event ? time penalty (no LSA)
G Entity + Event (no LSA)
H Entity + Event (event clustering LSA)
I Entity + Event (event + sentence clustering LSA)
Table 2. Peer Orderings
6.3 Metrics
A popular metric used in sequence evaluation 
is Kendall?s ? (Lapata, 2006), which measures 
ordering differences in terms of the number of 
adjacent sentence inversions necessary to 
convert a test ordering to the reference ordering.
? = 4m/(n(n ? 1))             (Eq. 16)
where m is the number of inversions described 
above and n is the total number of sentences.
The second metric we use is the Average 
Continuity (AC) developed by Bollegala et al
(2006), which captures the intuition that the 
ordering quality can be estimated by the number 
of correctly arranged continuous sentences.
?? = exp( ?
???
? log( ?? + ?)????                (Eq. 17)
where k is the maximum number of continuous 
sentences, ? is a small value in case Pn = 1. Pn,
the proportion of continuous sentences of length 
n in an ordering, is defined as m/(N ? n + 1) 
where m is the number of continuous sentences 
of length n in both the test and reference 
orderings and N is the total number of sentences. 
We set k = 4 and ? = 0.01.
6.4 Result
We empirically determine all the parameters (?
i
)
and produce all the peer orderings. Table 3 lists
the result, where we also show the statistical 
significance between the full model peer
ordering ?I? and all other versions, marked by * 
(p < .05) and ** (p < .01) on a two-tailed t-test.
Peer 
Code
200w 400w
Kendall?s ? AC Kendall?s ? AC
A 0.014** 0.009** -0.019** 0.004**
B 0.387 0.151* 0.259** 0.151*
C 0.369* 0.128* 0.264* 0.156*
D 0.380 0.163 0.270* 0.158*
E 0.375* 0.156* 0.267* 0.157*
F 0.388 0.159* 0.264* 0.157*
G 0.385 0.158* 0.269* 0.162
H 0.384 0.164 0.292* 0.170
I 0.395 0.170 0.350 0.176
Table 3. Evaluation Result
Almost all versions with entity and event 
information outperform the baseline. The LSA-
style dimensionality reduction proves effective 
for our task, as the full model (Peer I) ranks first 
and significantly beats versions without event
information, topic continuity, or LSA. Applying
LSA to both event and sentence clustering is 
better than applying it only to event clustering
(Peer H), which produces unstable results and is 
sometimes outperformed by no-LSA versions
(Peer G).
Event (Peer D) proves to be more valuable 
than entity (Peer C) as the event-only versions 
outperform the entity-only version in all 
categories, which is predicable because events
1495
are high-level constructs that incorporate most 
of the document-level important entities.
When entity is used, extra bonus can be 
gained from topic continuity concerns from CT
(Peer E vs. Peer G) because the centering 
transition effectively captures the coherence 
pattern between adjacent sentences. The effect 
of the chronological order seems less clear (Peer 
F vs. P) as removing it hurts longer extracts 
rather than short extracts. Therefore
chronological clues are more valuable for 
arranging more sentences from the same source 
document.
Our ordering algorithm achieves even better 
result with long extracts because the importance 
of order and coherence grows with text length. 
Measured by Kendall?s ?, the full model 
ordering in the 400w category is significantly
better than all other orderings.
For a qualitative evaluation, we select the 
200w extract d080ae and list all the sentences in 
Figure 4. The event terms are boldfaced and the 
event entities are underlined.
Limited by space, let?s focus on the baseline
(1 2 3 4 5 6), entity-only (3 5 2 4 6 1), and full-
model versions (3 5 4 2 1 6). The news extract 
is about the acquitting of child molesters. Both 
the ?acquitting? and ?molesting? events are 
found in 1) and 3) but only the latter qualifies as
the topic sentence because it contains important 
event entities. Choosing 3) instead of 1) as the 
leading sentence shows the advantage of our 
event-enriched model over the baseline. The
same choice is made by the entity-only version 
because 3) happens to be also entity-intensive. 
In order to see the advantage of the full model 
over the entity-only model, let?s consider 2) and 
4). 2) is chosen by the entity-only model after 5) 
because of the heavy entity overlap between 5) 
a
because of the heavy entity overlap between 5) 
and 2). But semantically, 2) is not as close to 5) 
as 4) because only 4) contains entities for both 
the ?acquitting? (?juror?) and ?molesting?
(?children?) events and intuitively, 4) continues 
the main trial-acquittal event topic but 2) 
supplies only secondary information. We
examined the sentence clusters before the 
ordering and found that 3), 5), and 4) are 
clustered together only by the full model,
leading to better coherence, locally and globally.
7 Conclusion and Future Work
We set out by realizing the semantic deficiency 
of IR and propose a low-cost approach of 
building event semantics into sentence 
representation. Event extraction relies on 
shallow parsing and external knowledge sources. 
Then we propose a novel approach of two-
layered clustering to use event information,
coupled with LSA-style dimensionality
reduction. MDS sentence ordering is guided by 
local and global coherence to simulate the 
block-style writing and is realized by a greedy 
algorithm. The evaluation shows clear 
advantage of our event-enriched model over
baseline and event-agonistic models, 
quantitatively and qualitatively.
The extraction approach can be refined by 
deep parsing and rich verb (frame) semantics. In 
a follow-up project, we will expand our dataset 
and experiment with more data and incorporate 
human evaluation in comparative tasks.
Acknowledgment
The work described in this paper was partially 
supported by a grant from the HK RGC (Project 
Number: PolyU5217/07E).
1) Thursday's acquittals in the McMartin Pre-School molestation case outraged parents who said prosecutors botched it, 
while those on the defense side proclaimed a triumph of justice over hysteria and hype.
2) Originally, there were seven defendants, including Raymond Buckey's sister, Peggy Ann Buckey, and Virginia McMartin, 
the founder of the school, mother of Mrs. Buckey and grandmother of Raymond Buckey.
3) Seven jurors who spoke with reporters in a joint news conference after acquitting Raymond Buckey and his mother, 
Peggy McMartin Buckey, on 52 molestation charges Thursday said they felt some children who testified may have been 
molested _ but not at the family-run McMartin Pre-School.
4) ``The children were never allowed to say in their own words what happened to them,'' said juror John Breese.
5) Ray Buckey and his mother, Peggy McMartin Buckey, were found not guilty Thursday of molesting children at the 
family-run McMartin Pre-School in Manhattan Beach, a verdict which brought to a close the longest and costliest criminal 
trial in history .
6) As it becomes apparent that McMartin cases will stretch out for years to come, parents and the former criminal defendants
alike are trying to resign themselves to the inevitability that the matter may be one they can never leave behind.
Figure 4. Extract sentences of d80ae, 200w
1496
References
Barzilay, R., Elhadad, N., and McKeown, K. 2002. 
Inferring Strategies for Sentence Ordering in 
Multidocument News Summarization. Journal of 
Artificial Intelligence Research, 17:35?55.
Barzilay, R., and Lapata, M. 2005. Modeling Local 
Coherence: An Entity-based Approach. In 
Proceedings of the 43rd Annual Meeting of the 
ACL, 141-148. Ann Arbor.
Barzilay, R., and Lapata, M. 2008. Modeling Local 
Coherence: An Entity-Based Approach. 
Computational Linguistics, 34:1?34.
Bollegala, D, Okazaki, N., and Ishizuka, M. 2006. A 
Bottom-up Approach to Sentence Ordering for 
Multi-document Summarization. In Proceedings 
of the 21st International Conference on 
Computational Linguistics and 44th Annual 
Meeting of the ACL, 385?392. Sydney, Australia.
Bromberg, I. 2006. Ordering Sentences According to 
Topicality. Presented at the Midwest 
Computational Linguistics Colloquium.
Chklovski, T., and Pantel, P. 2004. VerbOcean: 
Mining the Web for Fine-Grained Semantic Verb 
Relations. In Proceedings of Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-04). 11?13. Barcelona, 
Spain.
Conroy, J. M., Schlesinger, J. D., and Goldstein, J. 
2006. CLASSY Tasked Based Summarization: 
Back to Basics. In proceedings of the Document 
Understanding Conference (DUC-06).
Filatova, E., and Hatzivassiloglou, V. 2003. Domain-
independent detection, extraction, and labeling of 
atomic events. In Proceedings of RANLP, 145?
152, Borovetz, Bulgaria.
Filatova, E., and Hatzivassiloglou, V. 2004. Event-
Based Extractive Summarization. In Proceedings 
of the ACL-04, 104?111.
Grosz, B. J., Aravind K. J., and Scott W. 1995. 
Centering: A framework for Modeling the Local 
Coherence of Discourse. Computational 
Linguistics, 21(2):203?225.
Jurafsky D., and Martin, J. H. 2009. Speech and 
Language Processing, Second Edition. Upper 
Saddle River, NJ: Pearson Education International.
Landauer, T., and Dumais, S. 1997. A solution to 
Plato?s problem: The latent semantic analysis 
theory of the acquisition, induction, and 
representation of knowledge. Psychological 
Review, 104.
Lapata, M. 2003. Probabilistic Text Structuring: 
Experiments with Sentence Ordering. In 
Proceedings of the Annual Meeting of ACL, 545-
552. Sapporo, Japan.
Li, W., Wu, M., Lu, Q., Xu, W., and Yuan, C. 2006. 
Extractive Summarization Using Inter- and Intra-
Event Relevance. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL,
369?376. Sydney.
Liu, M., Li, W., Wu, M., and Lu, Q. 2007. Extractive 
Summarization Based on Event Term Clustering. 
In Proceedings of the ACL 2007 Demo and Poster
Sessions, 185?188. Prague.
Okazaki, N., Matsuo, Y., and Ishizuka, M. 2004. 
Improving Chronological Ordering by Precedence 
Relation. In Proceedings of 20th International 
Conference on Computational Linguistics 
(COLING 04), 750?756.
Taboada, M., and Wiesemann, L., Subjects and 
topics in conversation. Journal of Pragmatics
(2009), doi:10.1016/j.pragma.2009.04.009.
Tan, Y. F., Kan, M., and Cui, H. 2006. Extending 
corpus-based identification of light verb 
constructions using a supervised learning 
framework. In Proceedings of the EACL 2006 
Workshop on Multi-word-expressions in a 
multilingual context, 49?56, Trento, Italy.
Yoshioka, M., and Haraguchi, M. 2004. Multiple 
News Articles Summarization Based on Event 
Reference Information. In Working Notes of 
NTCIR-4, Tokyo.
1497
Proceedings of the ACL-HLT 2011 Student Session, pages 6?11,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Sentence Ordering Driven by Local and Global Coherence                    
for Summary Generation 
Renxian Zhang 
Department of Computing 
The Hong Kong Polytechnic University 
csrzhang@comp.polyu.edu.hk 
 
Abstract 
In summarization, sentence ordering is 
conducted to enhance summary readability by 
accommodating text coherence. We propose a 
grouping-based ordering framework that 
integrates local and global coherence concerns. 
Summary sentences are grouped before 
ordering is applied on two levels: group-level 
and sentence-level. Different algorithms for 
grouping and ordering are discussed. The 
preliminary results on single-document news 
datasets demonstrate the advantage of our 
method over a widely accepted method. 
1 Introduction and Background 
The canonical pipeline of text summarization 
consists of topic identification, interpretation, and 
summary generation (Hovy, 2005). In the simple 
case of extraction, topic identification and 
interpretation are conflated to sentence selection 
and concerned with summary informativeness. In 
comparison, summary generation addresses 
summary readability and a frequently discussed 
generation technique is sentence ordering. 
It is implicitly or explicitly stated that sentence 
ordering for summarization is primarily driven by 
coherence. For example, Barzilay et al (2002) use 
lexical cohesion information to model local 
coherence. A statistical model by Lapata (2003) 
considers both lexical and syntactic features in 
calculating local coherence. More globally biased 
is Barzilay and Lee?s (2004) HMM-based content 
model, which models global coherence with word 
distribution patterns. 
Whilst the above models treat coherence as 
lexical or topical relations, Barzilay and Lapata 
(2005, 2008) explicitly model local coherence with 
an entity grid model trained for optimal syntactic 
role transitions of entities. 
Although coherence in those works is modeled 
in the guise of ?lexical cohesion?, ?topic 
closeness?, ?content relatedness?, etc., few 
published works simultaneously accommodate 
coherence on the two levels: local coherence and 
global coherence, both of which are intriguing 
topics in text linguistics and psychology. For 
sentences, local coherence means the well-
connectedness between adjacent sentences through 
lexical cohesion (Halliday and Hasan, 1976) or 
entity repetition (Grosz et al, 1995) and global 
coherence is the discourse-level relation 
connecting remote sentences (Mann and 
Thompson, 1995; Kehler, 2002). An abundance of 
psychological evidences show that coherence on 
both levels is manifested in text comprehension 
(Tapiero, 2007). Accordingly, an apt sentence 
ordering scheme should be driven by such 
concerns.  
We also note that as sentence ordering is usually 
discussed only in the context of multi-document 
summarization, factors other than coherence are 
also considered, such as time and source sentence 
position in Bollegala et al?s (2006) ?agglomerative 
ordering? approach. But it remains an open 
question whether sentence ordering is non-trivial 
for single-document summarization, as it has long 
been recognized as an actual strategy taken by 
human summarizers (Jing, 1998; Jing and 
McKeown, 2000) and acknowledged early in work 
on sentence ordering for multi-document 
summarization (Barzilay et al, 2002). 
In this paper, we outline a grouping-based 
sentence ordering framework that is driven by the 
concern of local and global coherence. Summary 
sentences are grouped according to their 
conceptual relatedness before being ordered on two 
levels: group-level ordering and sentence-level 
ordering, which capture global coherence and local 
coherence in an integrated model. As a preliminary 
study, we applied the framework to single-
6
document summary generation and obtained 
interesting results. 
The main contributions of this work are: (1) we 
stress the need to channel sentence ordering 
research to linguistic and psychological findings 
about text coherence; (2) we propose a grouping-
based ordering framework that integrates both 
local and global coherence; (3) we find in 
experiments that coherence-driven sentence 
ordering improves the readability of single-
document summaries, for which sentence ordering 
is often considered trivial. 
In Section 2, we review related ideas and 
techniques in previous work. Section 3 provides 
the details of grouping-based sentence ordering. 
The preliminary experimental results are presented 
in Section 4. Finally, Section 5 concludes the 
whole paper and describes future work. 
2 Grouping-Based Ordering  
Our ordering framework is designed to capture 
both local and global coherence. Globally, we 
identify related groups among sentences and find 
their relative order. Locally, we strive to keep 
sentence similar or related in content close to each 
other within one group. 
2.1 Sentence Representation 
As summary sentences are isolated from their 
original context, we retain the important content 
information by representing sentences as concept 
vectors. In the simplest case, the ?concept? is 
equivalent to content word. A drawback of this 
practice is that it considers every content word 
equally contributive to the sentence content, which 
is not always true. For example, in the news 
domain, entities realized as NPs are more 
important than other concepts. 
To represent sentences as entity vectors, we 
identify both common entities (as the head nouns 
of NPs) and named entities. Two common entities 
are equivalent if their noun stems are identical or 
synonymous. Named entities are usually equated 
by identity. But in order to improve accuracy, we 
also consider: 1) structural subsumption (one is 
part of another); 2) hypernymy and holonymy (the 
named entities are in a superordinate-subordinate 
or part-whole relation). 
Now with summary sentence Si and m entities eik 
(k = 1 ? m), Si = (wf(ei1), wf(ei2), ?, wf(eim)), 
where wf(eik) = wk?f(eik), f(eik) is the frequency of 
eik and wk is the weight of eik. We define wk = 1 if 
eik is a common entity and wk = 2 if eik is a named 
entity. We give double weight to named entities 
because of their significance to news articles. After 
all, a news story typically contains events, places, 
organizations, people, etc. that denote the news 
theme. Other things being equal, two sentences 
sharing a mention of named entities are 
thematically closer than two sentences sharing a 
mention of common entities. 
Alternatively, we can realize the ?concept? as 
?event? because events are prevalent semantic 
constructs that bear much of the sentence content 
in some domains (e.g., narratives and news reports). 
To represent sentences as event vectors, we can 
follow Zhang et al?s (2010) method at the cost of 
more complexity.  
2.2 Sentence Grouping 
To meet the global need of identifying sentence 
groups, we develop two grouping algorithms by 
applying graph-based operation and clustering. 
Connected Component Finding (CC) 
This algorithm treats grouping sentences as 
finding connected components (CC) in a text graph 
TG = (V, E), where V represents the sentences and 
E the sentence relations weighted by cosine 
similarity. Edges with weight < t, a threshold, are 
removed because they represent poor sentence 
coherence.  
The resultant graph may be disconnected, in 
which we find all of its connected components, 
using depth-first search. The connected 
components are the groups we are looking for. 
Note that this method cannot guarantee that every 
two sentences in such a group are directly linked, 
but it does guarantee that there exists a path 
between every sentence pair. 
Modified K-means Clustering (MKM) 
Observing that the CC method finds only 
coherent groups, not necessarily groups of 
coherent sentences, we develop a second algorithm 
using clustering. A good choice might be K-means 
as it is efficient and outperforms agglomerative 
clustering methods in NLP applications (Steibach 
et al, 2000), but the difficulty with the 
conventional K-means is the decision of K.  
Our solution is modified K-means (MKM) based 
on (Wilpon and Rabiner, 1985). Let?s denote 
7
cluster i by CLi and cluster similarity by Sim(CLi) 
=
, ( ( , ))im in i im inS S CLMin Sim S S?
, where ( , )im inSim S S is their 
cosine. The following illustrates the algorithm. 
 
1. CL1 = all the sentence vectors; 
2. Do the 1-means clustering by assigning all the 
vectors to CL1; 
3. While at least 1 cluster has at least 2 sentences and 
Min(Sim(CLi)) <  t, do: 
  3.1 If Sim(Sm, Sn) = Min(Sim(CLi)), create two new 
centroids as Sm and Sn; 
  3.2 Do the conventional K-means clustering until 
clusters stabilize; 
 
The above algorithm stops iterating when each 
cluster contains all above-threshold-similarity 
sentence pairs or only one sentence. Unlike CC, 
MKM results in more strongly connected groups, 
or groups of coherence sentences.  
2.3 Ordering Algorithms 
After the sentences are grouped, ordering is to be 
conducted on two levels: group and sentence. 
Composed of closely related sentences, groups 
simulate high-level textual constructs, such as 
?central event?, ?cause?, ?effect?, ?background?, 
etc. for news articles, around which sentences are 
generated for global coherence. For an intuitive 
example, all sentences about ?cause? should 
immediately precede all sentences about ?effect? to 
achieve optimal readability. We propose two 
approaches to group-level ordering. 1) If the group 
sentences come from the same document, group 
(Gi) order is decided by the group-representing 
sentence (gi) order (  means ?precede?) in the text.  
i j i jg g G G?  
2) Group order is decided in a greedy fashion in 
order to maximize the connectedness between 
adjacent groups, thus enhancing local coherence. 
Each time a group is selected to achieve maximum 
similarity with the ordered groups and the first 
ordered group (G1) is selected to achieve 
maximum similarity with all the other groups. 
1 '
arg max ( , ')G G GG Sim G G?? ?
 
? ?
1
unordered groups 1
arg max ( , )ii jG jG Sim G G
?
? ?
? ?
 (i > 1) 
where Sim(G, G?) is the average sentence cosine 
similarity between G and G?. 
Within the ordered groups, sentence-level 
ordering is aimed to enhance local coherence by 
placing conceptually close sentences next to each 
other. Similarly, we propose two approaches. 1) If 
the sentences come from the same document, they 
are arranged by the text order. 2)  Sentence order is 
greedily decided. Similar to the decision of group 
order, with ordered sentence Spi in group Gp: 
1 '
arg max ( , ')
p
p S G S S
S Sim S S
? ?
? ?
 
? ?
1
unordered sentences in 1
arg max ( , )
p
i
pi pj
S G j
S Sim S S?
? ?
? ?
(i > 1) 
Note that the text order is used as a common 
heuristic, based on the assumption that the 
sentences are arranged coherently in the source 
document, locally and globally. 
3 Experiments and Preliminary Results  
Currently, we have evaluated grouping-based 
ordering on single-document summarization, for 
which text order is usually considered sufficient. 
But there is no theoretical proof that it leads to 
optimal global and local coherence that concerns 
us. On some occasions, e.g., a news article 
adopting the ?Wall Street Journal Formula? (Rich 
and Harper, 2007) where conceptually related 
sentences are placed at the beginning and the end, 
sentence conceptual relatedness does not 
necessarily correlate with spatial proximity and 
thus selected sentences may need to be rearranged 
for better readability. We are not aware of any 
published work that has empirically compared 
alternative ways of sentence ordering for single-
document summarization. The experimental results 
reported below may draw some attention to this 
taken-for-granted issue. 
3.1 Data and Method 
We prepared 3 datasets of 60 documents each, the 
first (D400) consisting of documents of about 400 
words from the Document Understanding 
Conference (DUC) 01/02 datasets; the second 
(D1k) consisting of documents of about 1000 
words manually selected from popular English 
journals such as The Wall Street Journal, The 
Washington Post, etc; the third (D2k) consisting of 
documents of about 2000 words from the DUC 
01/02 dataset. Then we generated 100-word 
summaries for D400 and 200-word summaries for 
D1k and D2k. Since sentence selection is not our 
8
focus, the 180 summaries were all extracts 
produced by a simple but robust summarizer built 
on term frequency and sentence position (Aone et 
al., 1999). 
Three human annotators were employed to each 
provide reference orderings for the 180 summaries 
and mark paragraph (of at least 2 sentences) 
boundaries, which will be used by one of the 
evaluation metrics described below.  
In our implementation of the grouping-based 
ordering, sentences are represented as entity 
vectors and the threshold t = ( ( ), )m nAvg Sim S S c?, 
the average sentence similarity in a group 
multiplied by a coefficient empirically decided on 
separate held-out datasets of 20 documents for 
each length category. The ?group-representing 
sentence? is the textually earliest sentence in the 
group. We experimented with both CC and MKM 
to generate sentence groups and all the proposed 
algorithms in 2.3 for group-level and sentence-
level orderings, resulting in 8 combinations as test 
orderings, each coded in the format of ?Grouping 
(CC/MKM) / Group ordering (T/G) / Sentence 
ordering (T/G)?, where T and G represent the text 
order approach and the greedy selection approach 
respectively. For example, ?CC/T/G? means 
grouping with CC, group ordering with text order, 
and sentence ordering with the greedy approach. 
We evaluated the test orderings against the 3 
reference orderings  and compute the average 
(Madnani et al, 2007) by using 3 different metrics. 
The first metric is Kendall?s ? (Lapata 2003, 
2006), which has been reliably used in ordering 
evaluations (Bollegala et al, 2006; Madnani et al, 
2007). It measures ordering differences in terms of 
the number of adjacent sentence inversions 
necessary to convert a test ordering to the reference 
ordering. 
41 ( 1)
m
N N? ? ? ?
 
In this formula, m represents the number of 
inversions described above and N is the total 
number of sentences. 
The second metric is the Average Continuity 
(AC) proposed by Bollegala et al (2006), which 
captures the intuition that the quality of sentence 
orderings can be estimated by the number of 
correctly arranged continuous sentences. 
2
lo AC (1/ ( 1 g( )) )k n
n
ex Pp k ?
?
? ?? ?
 
In this formula, k is the maximum number of 
continuous sentences, ? is a small value in case Pn 
= 1. Pn, the proportion of continuous sentences of 
length n in an ordering, is defined as m/(N ? n + 1) 
where m is the number of continuous sentences of 
length n in both the test and reference orderings 
and N is the total number of sentences. Following 
(Bollegala et al, 2006), we set k = Min(4, N) and ? 
= 0.01. 
We also go a step further by considering only 
the continuous sentences in a paragraph marked by 
human annotators, because paragraphs are local 
meaning units perceived by human readers and the 
order of continuous sentences in a paragraph is 
more strongly grounded than the order of 
continuous sentences across paragraph boundaries. 
So in-paragraph sentence continuity is a better 
estimation for the quality of sentence orderings. 
This is our third metric: Paragraph-level Average 
Continuity (P-AC). 
2
 loP-AC g((1/ ( 1) ))k n
n
Pexp Pk ?
?
?? ? ?
 
Here PPn = m?/(N ? n + 1), where m? is the number 
of continuous sentences of length n in both the test 
ordering and a paragraph of the reference ordering. 
All the other parameters are as defined in AC and 
Pn. 
3.2 Results 
The following tables show the results measured by 
each metric. For comparison, we also include a 
?Baseline? that uses the text order. For each 
dataset, two-tailed t-test is conducted between the 
top scorer and all the other orderings and statistical 
significance (p < 0.05) is marked with *. 
 
 
? AC P-AC 
Baseline 0.6573* 0.4452* 0.0630 
CC/T/T 0.7286 0.5688 0.0749 
CC/T/G 0.7149 0.5449 0.0714 
CC/G/T 0.7094 0.5449 0.0703 
CC/G/G 0.6986 0.5320 0.0689 
MKM/T/T 0.6735 0.4670* 0.0685 
MKM/T/G 0.6722 0.4452* 0.0674 
MKM/G/T 0.6710 0.4452* 0.0660 
MKM/G/G 0.6588* 0.4683* 0.0682 
Table 1: D400 Evaluation 
 
 
9
 
? AC P-AC 
Baseline 0.3276 0.0867* 0.0428* 
CC/T/T 0.3324 0.0979 0.0463* 
CC/T/G 0.3276 0.0923 0.0436* 
CC/G/T 0.3282 0.0944 0.0479* 
CC/G/G 0.3220 0.0893* 0.0428* 
MKM/T/T 0.3390 0.1152 0.0602 
MKM/T/G 0.3381 0.1130 0.0588 
MKM/G/T 0.3375 0.1124 0.0576 
MKM/G/G 0.3379 0.1124 0.0581 
Table 2: D1k Evaluation 
 
 
? AC P-AC 
Baseline 0.3125* 0.1622 0.0213 
CC/T/T 0.3389 0.1683 0.0235 
CC/T/G 0.3281 0.1683 0.0229 
CC/G/T 0.3274 0.1665 0.0226 
CC/G/G 0.3279 0.1672 0.0226 
MKM/T/T 0.3125* 0.1634 0.0216 
MKM/T/G 0.3125* 0.1628 0.0215 
MKM/G/T 0.3125* 0.1630 0.0216 
MKM/G/G 0.3122* 0.1628 0.0215 
Table 3: D2k Evaluation 
 
In general, our grouping-based ordering scheme 
outperforms the baseline for news articles of 
various lengths and statistically significant 
improvement can be observed on each dataset. 
This result casts serious doubt on the widely 
accepted practice of taking the text order for 
single-document summary generation, which is a 
major finding from our study. 
The three evaluation metrics give consistent 
results although they are based on different 
observations. The P-AC scores are much lower 
than their AC counterparts because of its strict 
paragraph constraint. 
Interestingly, applying the text order posterior to 
sentence grouping for group-level and sentence-
level ordering leads to consistently optimal 
performance, as the top scorers on each dataset are 
almost all ?__/T/T?. This suggests that the textual 
realization of coherence can be sought in the 
source document if possible, after the selected 
sentences are rearranged. It is in this sense that the 
general intuition about the text order is justified. It 
also suggests that tightly knit paragraphs (groups), 
where the sentences are closely connected, play a 
crucial role in creating a coherence flow. Shuffling 
those paragraphs may not affect the final 
coherence1. 
                                                          
1 I thank an anonymous reviewer for pointing this out. 
The grouping method does make a difference. 
While CC works best for the short and long 
datasets (D400 and D2k), MKM is more effective 
for the medium-sized dataset D1k. Whether the 
difference is simply due to length or 
linguistic/stylistic subtleties is an interesting topic 
for in-depth study. 
4 Conclusion and Future Work  
We have established a grouping-based ordering 
scheme to accommodate both local and global 
coherence for summary generation. Experiments 
on single-document summaries validate our 
approach and challenge the well accepted text 
order by the summarization community. 
Nonetheless, the results do not necessarily 
propagate to multi-document summarization, for 
which the same-document clue for ordering cannot 
apply directly. Adapting the proposed scheme to 
multi-document summary generation is the 
ongoing work we are engaged in. In the next step, 
we will experiment with alternative sentence 
representations and ordering algorithms to achieve 
better performance.  
We are also considering adapting more 
sophisticated coherence-oriented models, such as 
(Soricut and Marcu, 2006; Elsner et al, 2007), to 
our problem so as to make more interesting 
comparisons possible. 
Acknowledgements 
The reported work was inspired by many talks with 
my supervisor, Dr. Wenjie Li, who saw through 
this work down to every writing detail. The author 
is also grateful to many people for assistance. You 
Ouyang shared part of his summarization work and 
helped with the DUC data. Dr. Li Shen, Dr. Naishi 
Liu, and three participants helped with the 
experiments. I thank them all.  
The work described in this paper was partially 
supported by Hong Kong RGC Projects (No. 
PolyU 5217/07E).  
10
References  
Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, 
B. 1999. A Trainable Summarizer with Knowledge 
Acquired from Robust NLP Techniques. In I. Mani 
and M. T. Maybury (eds.), Advances in Automatic 
Text Summarization. 71?80. Cambridge, 
Massachusetts: MIT Press. 
Barzilay, R., Elhadad, N., and McKeown, K. 2002. 
Inferring Strategies for Sentence Ordering in 
Multidocument News Summarization. Journal of 
Artificial Intelligence Research, 17: 35?55. 
Barzilay, R. and Lapata, M. 2005. Modeling Local 
Coherence: An Entity-based Approach. In 
Proceedings of the 43rd Annual Meeting of the ACL, 
141?148. Ann Arbor. 
Barzilay, R. and Lapata, M. 2008. Modeling Local 
Coherence: An Entity-Based Approach. 
Computational Linguistics, 34: 1?34. 
Barzilay, R. and Lee L. 2004. Catching the Drift: 
Probabilistic Content Models, with Applications to 
Generation and Summarization. In HLT-NAACL 
2004: Proceedings of the Main Conference. 113?120. 
Bollegala, D, Okazaki, N., and Ishizuka, M. 2006. A 
Bottom-up Approach to Sentence Ordering for Multi-
document Summarization. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL, 
385?392. Sydney. 
Elsner, M., Austerweil, j. & Charniak E. 2007. ?A 
Unified Local and Global Model for Discourse 
Coherence?. In Proceedings of NAACL HLT 2007, 
436-443. Rochester, NY. 
Grosz, B. J., Aravind K. J., and Scott W. 1995. 
Centering: A framework for Modeling the Local 
Coherence of Discourse. Computational Linguistics, 
21(2):203?225. 
Halliday, M. A. K., and Hasan, R. 1976. Cohesion in 
English. London: Longman. 
Hovy, E. 2005. Automated Text Summarization. In R. 
Mitkov (ed.), The Oxford Handbook of 
Computational Linguistics, pp. 583?598. Oxford: 
Oxford University Press. 
Jing, H. 2000. Sentence Reduction for Automatic Text 
Summarization. In Proceedings of the 6th Applied 
Natural Language Processing Conference, Seattle, 
WA, pp. 310?315. 
Jing, H., and McKeown, K. 2000. Cut and Paste Based 
Text Summarization. In Proceedings of the 1st 
NAACL, 178?185. 
Kehler, A. 2002. Coherence, Reference, and the Theory 
of Grammar. Stanford, California: CSLI Publications. 
Lapata, M. 2003. Probabilistic Text Structuring: 
Experiments with Sentence Ordering. In Proceedings 
of the Annual Meeting of ACL, 545?552. Sapporo, 
Japan. 
Lapata, M. 2006. Automatic evaluation of information 
ordering: Kendall?s tau. Computational Linguistics, 
32(4):1?14. 
Madnani, N., Passonneau, R., Ayan, N. F., Conroy, J. 
M., Dorr, B. J., Klavans, J. L., O?leary, D. P., and 
Schlesinger, J. D. 2007. Measuring Variability in 
Sentence Ordering for News Summarization. In 
Proceedings of the Eleventh European Workshop on 
Natural Language Generation, 81?88. Germany. 
Mann, W. C. and Thompson, S. 1988. Rhetorical 
Structure Theory: Toward a Functional Theory of 
Text Organization. Text, 8:243?281. 
Rich C., and Harper, C. 2007. Writing and Reporting 
News: A Coaching Method, Fifth Edition. Thomason 
Learning, Inc. Belmont, CA. 
Soricut, R. and Marcu D. 2006. Discourse Generation 
Using Utility-Trained Coherence Models. In 
Proceedings of the COLING/ACL 2006 Main 
Conference Poster Sessions, 803?810. 
Steibach, M., Karypis, G., and Kumar V. 2000. A 
Comparison of Document Clustering Techniques. 
Technical Report 00-034. Department of Computer 
Science and Engineering, University of Minnesota. 
Tapiero, I. 2007. Situation Models and Levels of 
Coherence: Towards a Definition of Comprehension. 
Mahwah, New Jersey: Lawrence Erlbaum Associates. 
Wilpon, J. G. and Rabiner, L. R. 1985. A Modified K-
means Clustering Algorithm for Use in Isolated 
Word Recognition. In IEEE Trans. Acoustics, Speech, 
Signal Proc. ASSP-33(3), 587?594. 
Zhang R., Li, W., and Lu, Q. 2010. Sentence Ordering 
with Event-Enriched Semantics and Two-Layered 
Clustering for Multi-Document News Summarization. 
In COLING 2010: Poster Volume, 1489?1497, 
Beijing. 
 
 
11
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 567?571,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Sequential Summarization: A New Application for Timely Updated 
Twitter Trending Topics 
Dehong Gao, Wenjie Li, Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University, Hong Kong 
{csdgao, cswjli, csrzhang}@comp.polyu.edu.hk 
 
Abstract 
The growth of the Web 2.0 technologies has 
led to an explosion of social networking 
media sites. Among them, Twitter is the most 
popular service by far due to its ease for real-
time sharing of information. It collects 
millions of tweets per day and monitors what 
people are talking about in the trending topics 
updated timely. Then the question is how 
users can understand a topic in a short time 
when they are frustrated with the 
overwhelming and unorganized tweets. In 
this paper, this problem is approached by 
sequential summarization which aims to 
produce a sequential summary, i.e., a series 
of chronologically ordered short sub-
summaries that collectively provide a full 
story about topic development. Both the 
number and the content of sub-summaries are 
automatically identified by the proposed 
stream-based and semantic-based approaches. 
These approaches are evaluated in terms of 
sequence coverage, sequence novelty and 
sequence correlation and the effectiveness of 
their combination is demonstrated.  
1 Introduction and Background 
Twitter, as a popular micro-blogging service, 
collects millions of real-time short text messages 
(known as tweets) every second. It acts as not 
only a public platform for posting trifles about 
users? daily lives, but also a public reporter for 
real-time news. Twitter has shown its powerful 
ability in information delivery in many events, 
like the wildfires in San Diego and the 
earthquake in Japan. Nevertheless, the side effect 
is individual users usually sink deep under 
millions of flooding-in tweets. To alleviate this 
problem, the applications like whatthetrend 1 
have evolved from Twitter to provide services 
that encourage users to edit explanatory tweets 
about a trending topic, which can be regarded as 
topic summaries. It is to some extent a good way 
to help users understand trending topics. 
                                                          
1 whatthetrend.com 
There is also pioneering research in automatic 
Twitter trending topic summarization. (O'Connor 
et al, 2010) explained Twitter trending topics by 
providing a list of significant terms. Users could 
utilize these terms to drill down to the tweets 
which are related to the trending topics. (Sharifi 
et al, 2010) attempted to provide a one-line 
summary for each trending topic using phrase 
reinforcement ranking. The relevance model 
employed by (Harabagiu and Hickl, 2011) 
generated summaries in larger size, i.e., 250-
word summaries, by synthesizing multiple high 
rank tweets. (Duan et al, 2012) incorporate the 
user influence and content quality information in 
timeline tweet summarization and employ 
reinforcement graph to generate summaries for 
trending topics. 
Twitter summarization is an emerging 
research area. Current approaches still followed 
the traditional summarization route and mainly 
focused on mining tweets of both significance 
and representativeness. Though, the summaries 
generated in such a way can sketch the most 
important aspects of the topic, they are incapable 
of providing full descriptions of the changes of 
the focus of a topic, and the temporal information 
or freshness of the tweets, especially for those 
newsworthy trending topics, like earthquake and 
sports meeting. As the main information 
producer in Twitter, the massive crowd keeps 
close pace with the development of trending 
topics and provide the timely updated 
information. The information dynamics and 
timeliness is an important consideration for 
Twitter summarization. That is why we propose 
sequential summarization in this work, which 
aims to produce sequential summaries to capture 
the temporal changes of mass focus. 
Our work resembles update summarization 
promoted by TAC 2  which required creating 
summaries with new information assuming the 
reader has already read some previous 
documents under the same topic. Given two 
chronologically ordered documents sets about a 
topic, the systems were asked to generate two 
                                                          
2 www.nist.gov/tac 
567
summaries, and the second one should inform the 
user of new information only. In order to achieve 
this goal, existing approaches mainly emphasized 
the novelty of the subsequent summary (Li and 
Croft, 2006; Varma et al, 2009; Steinberger and 
Jezek, 2009). Different from update 
summarization, we focus more on the temporal 
change of trending topics. In particular, we need 
to automatically detect the ?update points? 
among a myriad of related tweets.  
It is the goal of this paper to set up a new 
practical summarization application tailored for 
timely updated Twitter messages. With the aim 
of providing a full description of the focus 
changes and the records of the timeline of a 
trending topic, the systems are expected to 
discover the chronologically ordered sets of 
information by themselves and they are free to 
generate any number of update summaries 
according to the actual situations instead of a 
fixed number of summaries as specified in 
DUC/TAC. Our main contributions include 
novel approaches to sequential summarization 
and corresponding evaluation criteria for this 
new application. All of them will be detailed in 
the following sections. 
2 Sequential Summarization 
Sequential summarization proposed here aims to 
generate a series of chronologically ordered sub-
summaries for a given Twitter trending topic. 
Each sub-summary is supposed to represent one 
main subtopic or one main aspect of the topic, 
while a sequential summary, made up by the sub-
summaries, should retain the order the 
information is delivered to the public. In such a 
way, the sequential summary is able to provide a 
general picture of the entire topic development. 
2.1 Subtopic Segmentation 
One of the keys to sequential summarization is 
subtopic segmentation. How many subtopics 
have attracted the public attention, what are they, 
and how are they developed? It is important to 
provide the valuable and organized materials for 
more fine-grained summarization approaches. 
We proposed the following two approaches to 
automatically detect and chronologically order 
the subtopics. 
2.1.1 Stream-based Subtopic Detection and 
Ordering 
Typically when a subtopic is popular enough, it 
will create a certain level of surge in the tweet 
stream. In other words, every surge in the tweet 
stream can be regarded as an indicator of the 
appearance of a subtopic that is worthy of being 
summarized. Our early investigation provides 
evidence to support this assumption. By 
examining the correlations between tweet content 
changes and volume changes in randomly 
selected topics, we have observed that the 
changes in tweet volume can really provide the 
clues of topic development or changes of crowd 
focus.  
The stream-based subtopic detection approach 
employs the offline peak area detection (Opad) 
algorithm (Shamma et al, 2010) to locate such 
surges by tracing tweet volume changes. It 
regards the collection of tweets at each such 
surge time range as a new subtopic.  
Offline Peak Area Detection (Opad) Algorithm 
1: Input: TS (tweets stream, each twi with timestamp ti); 
peak interval window ?? (in hour), and time 
step? (? ? ??); 
2: Output: Peak Areas PA. 
3: Initial: two time slots: ?? = ? = ?0 + ??;  
Tweet numbers: ?? = ? = ?????(?) 
4: while (?? = ? + ?) < ???1 
5:      update ?? = ?? + ?? and ?
? = ?????(??) 
6:      if (?? < ? And up-hilling)  
7: output one peak area ???  
8: state of down-hilling 
9:      else  
10: update ? = ?? and ? =  ?? 
11: state of up-hilling 
12: 
13: function ?????(?) 
14: Count tweets in time interval T 
The subtopics detected by the Opad algorithm 
are naturally ordered in the timeline. 
2.1.2 Semantic-based Subtopic Detection and 
Ordering 
Basically the stream-based approach monitors 
the changes of the level of user attention. It is 
easy to implement and intuitively works, but it 
fails to handle the cases where the posts about 
the same subtopic are received at different time 
ranges due to the difference of geographical and 
time zones. This may make some subtopics 
scattered into several time slots (peak areas) or 
one peak area mixed with more than one 
subtopic. 
In order to sequentially segment the subtopics 
from the semantic aspect, the semantic-based 
subtopic detection approach breaks the time 
order of tweet stream, and regards each tweet as 
an individual short document. It takes advantage 
of Dynamic Topic Modeling (David and Michael, 
2006) to explore the tweet content.  
568
DTM in nature is a clustering approach which 
can dynamically generate the subtopic 
underlying the topic. Any clustering approach 
requires a pre-specified cluster number. To avoid 
tuning the cluster number experimentally, the 
subtopic number required by the semantic-based 
approach is either calculated according to 
heuristics or determined by the number of the 
peak areas detected from the stream-based 
approach in this work. 
Unlike the stream-based approach, the 
subtopics formed by DTM are the sets of 
distributions of subtopic and word probabilities. 
They are time independent. Thus, the temporal 
order among these subtopics is not obvious and 
needs to be discovered. We use the probabilistic 
relationships between tweets and topics learned 
from DTM to assign each tweet to a subtopic that 
it most likely belongs to. Then the subtopics are 
ordered temporally according to the mean values 
of their tweets? timestamps. 
2.2 Sequential Summary Generation 
Once the subtopics are detected and ordered, the 
tweets belonging to each subtopic are ranked and 
the most significant one is extracted to generate 
the sub-summary regarding that subtopic. Two 
different ranking strategies are adopted to 
conform to two different subtopic detection 
mechanisms. 
For a tweet in a peak area, the linear 
combination of two measures is considered to 
evaluate its significance to be a sub-summary: (1) 
subtopic representativeness measured by the 
cosine similarity between the tweet and the 
centroid of all the tweets in the same peak area; 
(2) crowding endorsement measured by the times 
that the tweet is re-tweeted normalized by the 
total number of re-tweeting. With the DTM 
model, the significance of the tweets is evaluated 
directly by word distribution per subtopic.  
MMR (Carbonell and Goldstein, 1998) is used 
to reduce redundancy in sub-summary generation.  
3 Experiments and Evaluations 
The experiments are conducted on the 24 Twitter 
trending topics collected using Twitter APIs 3 . 
The statistics are shown in Table 1. 
Due to the shortage of gold-standard 
sequential summaries, we invite two annotators 
to read the chronologically ordered tweets, and 
write a series of sub-summaries for each topic 
                                                          
3https://dev.twitter.com/ 
independently. Each sub-summary is up to 140 
characters in length to comply with the limit of 
tweet, but the annotators are free to choose the 
number of sub-summaries. It ends up with 6.3 
and 4.8 sub-summaries on average in a 
sequential summary written by the two 
annotators respectively. These two sets of 
sequential summaries are regarded as reference 
summaries to evaluate system-generated 
summaries from the following three aspects. 
 
Category #TT 
Trending Topic 
Examples 
Tweets 
Number 
News 6 
Minsk, Libya 
Release 
25145 
Sports 6 
#bbcf1, 
Lakers/Heat 
17204 
Technology 5 Google Fiber 13281 
Science 2 AH1N1, Richter 10935 
Entertainment 2 Midnight Club, 6573 
Meme 2 
#ilovemyfans, 
Night Angels 
14595 
Lifestyle 1 Goose Island 6230 
Total 24 ------------ 93963 
Table 1. Data Set 
? Sequence Coverage 
Sequence coverage measures the N-gram match 
between system-generated summaries and 
human-written summaries (stopword removed 
first). Considering temporal information is an 
important factor in sequential summaries, we 
propose the position-aware coverage measure by 
accommodating the position information in 
matching. Let S={s1, s2, ?, sk} denote a 
sequential summary and si the ith sub-summary, 
N-gram coverage is defined as: 
????????
=
1
|???|
?
? ? ??????????(?-????)?-???????,????????
??? ? ? ? ?????(?-????)?-???????????????????
 
where,  ??? = |? ? ?| + 1, i and j denote the serial 
numbers of the sub-summaries in the system-
generated summary ???  and the human-written 
summary ??? , respectively. ?  serves as a 
coefficient to discount long-distance matched 
sub-summaries. We evaluate unigram, bigram, 
and skipped bigram matches. Like in ROUGE 
(Lin, 2004), the skip distance is up to four words. 
? Sequence Novelty 
Sequence novelty evaluates the average novelty 
of two successive sub-summaries. Information 
content (IC) has been used to measure the 
novelty of update summaries by (Aggarwal et al, 
2009). In this paper, the novelty of a system-
569
generated sequential summary is defined as the 
average of IC increments of two adjacent sub-
summaries,  
??????? =
1
|?| ? 1
?(???? ? ????, ???1)
?>1
 
where |?| is the number of sub-summaries in the 
sequential summary. ???? = ? ??????? . ????, ???1 =
? ???????????1  is the overlapped information in the 
two adjacent sub-summaries. ??? = ????  ?
?????????(?, ???) where w is a word, ???? is the 
inverse tweet frequency of w, and ??? is all the 
tweets in the trending topic. The relevance 
function is introduced to ensure that the 
information brought by new sub-summaries is 
not only novel but also related to the topic.  
? Sequence Correlation 
Sequence correlation evaluates the sequential 
matching degree between system-generated and 
human-written summaries. In statistics, 
Kendall?s tau coefficient is often used to measure 
the association between two sequences (Lapata, 
2006). The basic idea is to count the concordant 
and discordant pairs which contain the same 
elements in two sequences. Borrowing this idea, 
for each sub-summary in a human-generated 
summary, we find its most matched sub-
summary (judged by the cosine similarity 
measure) in the corresponding system-generated 
summary and then define the correlation 
according to the concordance between the two 
matched sub-summary sequences. 
???????????
=
2(|#???????????????| ? |#???????????????|)
?(? ? 1)
 
where n is the number of human-written sub-
summaries.  
Tables 2 and 3 below present the evaluation 
results. For the stream-based approach, we set 
?t=3 hours experimentally. For the semantic-
based approach, we compare three different 
approaches to defining the sub-topic number K: 
(1) Semantic-based 1: Following the approach 
proposed in (Li et al, 2007), we first derive the 
matrix of tweet cosine similarity. Given the 1-
norm of eigenvalues  ??
???? (? = 1, 2, ? , ?) of the 
similarity matrix and the ratios ?? = ??
????/?2 , 
the subtopic number ? = ? + 1  if ?? ? ??+1 > ? 
(? = 0.4 ). (2) Semantic-based 2: Using the rule 
of thumb in (Wan and Yang, 2008), ? = ?? , 
where n is the tweet number. (3) Combined: K is 
defined as the number of the peak areas detected 
from the Opad algorithm, meanwhile we use the 
tweets within peak areas as the tweets of DTM. 
This is our new idea. 
The experiments confirm the superiority of the 
semantic-based approach over the stream-based 
approach in summary content coverage and 
novelty evaluations, showing that the former is 
better at subtopic content modeling. The sub-
summaries generated by the stream-based 
approach have comparative sequence (i.e., order) 
correlation with the human summaries. 
Combining the advantages the two approaches 
leads to the best overall results.  
 
Coverage Unigram  Bigram  
Skipped 
Bigram 
Stream-
based(?t=3) 
0.3022 0.1567 0.1523 
Semantic-
based1(?=0.5) 
0.3507 0.1684 0.1866 
Semantic-based 2 0.3112 0.1348 0.1267 
Combined(?t=3) 0.3532 0.1699 0.1791 
Table 2. N-Gram Coverage Evaluation 
Approaches Novelty Correlation 
Stream-based (?t=3) 0.3798 0.3330 
Semantic-based 1 (?=0.4) 0.7163 0.3746 
Semantic-based 2 0.7017 0.3295 
Combined (?t=3) 0.7793 0.3986 
Table 3. Novelty and Correlation Evaluation 
4 Concluding Remarks 
We start a new application for Twitter trending 
topics, i.e., sequential summarization, to reveal 
the developing scenario of the trending topics 
while retaining the order of information 
presentation. We develop several solutions to 
automatically detect, segment and order 
subtopics temporally, and extract the most 
significant tweets into the sub-summaries to 
compose sequential summaries. Empirically, the 
combination of the stream-based approach and 
the semantic-based approach leads to sequential 
summaries with high coverage, low redundancy, 
and good order. 
Acknowledgments 
The work described in this paper is supported by 
a Hong Kong RGC project (PolyU No. 5202/12E) 
and a National Nature Science Foundation of 
China (NSFC No. 61272291). 
References  
Aggarwal Gaurav, Sumbaly Roshan and Sinha Shakti. 
2009. Update Summarization. Stanford: CS224N 
Final Projects. 
570
Blei M. David and Jordan I. Michael. 2006. Dynamic 
topic models. In Proceedings of the 23rd 
international conference on Machine learning, 113-
120.  Pittsburgh, Pennsylvania. 
Carbonell Jaime and Goldstein Jade. 1998. The use of 
MMR, diversity based reranking for reordering 
documents and producing summaries. In 
Proceedings of the 21st Annual International 
Conference on Research and Development in 
Information Retrieval, 335-336. Melbourne, 
Australia. 
Duan Yajuan, Chen Zhimin, Wei Furu, Zhou Ming 
and Heung-Yeung Shum. 2012. Twitter Topic 
Summarization by Ranking Tweets using Social                
Influence and Content Quality. In Proceedings of 
the 24th International Conference on Computational 
Linguistics, 763-780. Mumbai, India. 
Harabagiu Sanda and Hickl Andrew. 2011. Relevance 
Modeling for Microblog Summarization. In 
Proceedings of 5th International AAAI Conference 
on Weblogs and Social Media. Barcelona, Spain. 
Lapata Mirella. 2006. Automatic evaluation of 
information ordering: Kendall?s tau. Computational 
Linguistics, 32(4):1-14.  
Li Wenyuan, Ng Wee-Keong, Liu Ying and Ong 
Kok-Leong. 2007. Enhancing the Effectiveness of 
Clustering with Spectra Analysis. IEEE 
Transactions on Knowledge and Data Engineering, 
19(7):887-902. 
Li Xiaoyan and Croft W. Bruce. 2006. Improving 
novelty detection for general topics using sentence 
level information patterns. In Proceedings of the 
15th ACM International Conference on Information 
and Knowledge Management, 238-247. New York, 
USA. 
Lin Chin-Yew. 2004. ROUGE: a Package for 
Automatic Evaluation of Summaries. In 
Proceedings of the ACL Workshop on Text 
Summarization Branches Out, 74-81. Barcelona, 
Spain. 
Liu Fei, Liu Yang and Weng Fuliang. 2011. Why is 
?SXSW ? trending? Exploring Multiple Text 
Sources for Twitter Topic Summarization. In 
Proceedings of the ACL Workshop on Language in 
Social Media, 66-75. Portland, Oregon. 
O'Connor Brendan, Krieger Michel and Ahn David. 
2010. TweetMotif: Exploratory Search and Topic 
Summarization for Twitter. In Proceedings of the 
4th International AAAI Conference on Weblogs 
and Social Media, 384-385. Atlanta, Georgia. 
Shamma A. David, Kennedy Lyndon and Churchill F. 
Elizabeth. 2010. Tweetgeist: Can the Twitter 
Timeline Reveal the Structure of Broadcast Events? 
In Proceedings of the 2010 ACM Conference on 
Computer Supported Cooperative Work, 589-593. 
Savannah, Georgia, USA. 
Sharifi Beaux, Hutton Mark-Anthony and Kalita Jugal. 
2010. Summarizing Microblogs Automatically. In 
Human Language Technologies: the 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 685-
688. Los Angeles, California. 
Steinberger Josef and Jezek Karel. 2009. Update 
summarization based on novel topic distribution. In 
Proceedings of the 9th ACM Symposium on 
Document Engineering, 205-213. Munich, 
Germany. 
Varma Vasudeva, Bharat Vijay, Kovelamudi Sudheer, 
Praveen Bysani, Kumar K. N, Kranthi Reddy, 
Karuna Kumar and Nitin Maganti. 2009. IIIT 
Hyderabad at TAC 2009. In Proceedings of the 
2009 Text Analysis Conference. GaithsBurg, 
Maryland. 
Wan Xiaojun and Yang Jianjun. 2008. Multi-
document summarization using cluster-based link 
analysis. In Proceedings of the 31st Annual 
International Conference on Research and 
Development in Information Retrieval, 299-306. 
Singapore, Singapore. 
 
571
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 142?145,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
273. Task 5. Keyphrase Extraction Based on Core Word                 
Identification and Word Expansion 
You Ouyang        Wenjie Li        Renxian Zhang 
The Hong Kong Polytechnic University 
{csyouyang,cswjli,csrzhang}@comp.polyu.edu.hk 
Abstract 
This paper provides a description of the Hong 
Kong Polytechnic University (PolyU) System 
that participated in the task #5 of SemEval-2, 
i.e., the Automatic Keyphrase Extraction from 
Scientific Articles task. We followed a novel 
framework to develop our keyphrase 
extraction system, motivated by differentiating 
the roles of the words in a keyphrase. We first 
identified the core words which are defined as 
the most essential words in the article, and 
then expanded the identified core words to the 
target keyphrases by a word expansion 
approach.  
1 Introduction 
The task #5 in SemEval-2 requires extracting the 
keyphrases for scientific articles. According to 
the task definition, keyphrases are the words that 
capture the main topic of the given document. 
Currently, keyphrase extraction is usually carried 
out by a two-stage process, including candidate 
phrase identification and key phrase selection. 
The first stage is to identify the candidate phrases 
that are potential keyphrases. Usually, it is 
implemented as a process that filters out the 
obviously unimportant phrases. After the 
candidate identification stage, the target 
keyphrases can then be selected from the 
candidates according to their importance scores, 
which are usually estimated by some features, 
such as word frequencies, phrase frequencies, 
POS-tags, etc.. The features can be combined 
either by heuristics or by learning models to 
obtain the final selection strategy. 
In most existing keyphrase extraction methods, 
the importance of a phrase is estimated by a 
composite score of the features. Different 
features indicate preferences to phrases with 
specific characteristics. As to the common 
features, the phrases that consist of important and 
correlated words are usually preferred. Moreover, 
it is indeed implied in these features that the 
words are uniform in the phrase, that is, their 
degrees of importance are evaluated by the same 
criteria. However, we think that this may not 
always be true. For example, in the phrase ?video 
encoding/decoding?, the word ?video? appears 
frequently in the article and thus can be easily 
identified by simple features, while the word 
?encoding/decoding? is very rare and thus is very 
hard to discover. Therefore, a uniform view on 
the words is not able to discover this kind of 
keyphrases. On the other hand, we observe that 
there is usually at least one word in a keyphrase 
which is very important to the article, such as the 
word ?video? in the above example. In this paper, 
we call this kind of words core words. For each 
phrase, there may be one or more core words in 
it, which serve as the core component of the 
phrase. Moreover, the phrase may contain some 
words that support the core words, such as 
?encoding/decoding? in the above example. 
These words may be less important to the article, 
but they are highly correlated with the core word 
and are able to form an integrated concept with 
the core words. Motivated by this, we consider a 
new keyphrase extraction framework, which 
includes two stages: identifying the core words 
and expanding the core words to keyphrases. The 
methodology of the proposed approaches and the 
performance of the resulting system are 
introduced below. We also provide further 
discussions and modifications.  
2 Methodology 
According to our motivation, our extraction 
framework consists of three processes, including 
(1) The pre-processing to obtain the necessary 
information for the following processes; 
(2) The core word identification process to 
discover the core words to be expanded; 
(3) The word expansion process to generate the 
final keyphrases.  
 In the pre-processing, we first identify the text 
fields for each scientific article, including its title, 
abstract and main text (defined as all the section 
titles and section contents). The texts are then 
processed by the language toolkit GATE 1  to 
carry out sentence segmentation, word stemming 
and POS (part-of-speech) tagging. Stop-words 
                                                 
1 Publicly available at http://gate.ac.uk/gate 
142
are not considered to be parts of the target 
keyphrases. 
2.1 Core Word Identification 
Core words are the words that represent the 
dominant concepts in the article. To identify the 
core words, we consider the features below.  
Frequencies: In a science article, the words with 
higher frequencies are usually more important. 
To differentiate the text fields, in our system we 
consider three frequency-based features, i.e., 
Title-Frequency (TF), Abstract-Frequency 
(AF) and MainText-Frequency (MF), to 
represent the frequencies of one word in different 
text fields. For a word w in an article t, the 
frequencies are denoted by 
TF(w) = Frequency of  w in the title of t;  
AF(w) = Frequency of w in the abstract of t;  
MF(w) = Frequency of w in the main text of t. 
POS tag: The part-of-speech tag of a word is a 
good indicator of core words. Here we adopt a 
simple constraint, i.e., only nouns or adjectives 
can be potential core words. 
In our system, we use a progressive algorithm 
to identify all the core words. The effects of 
different text fields are considered to improve the 
accuracy of the identification result. First of all, 
for each word w in the title, it is identified to be a 
core word when satisfying  
{ TF(w)> 0 ? AF(w) > 0 } 
Since the abstract is usually less indicative 
than the title, we use stricter conditions for the 
words in the abstract by considering their co-
occurrence with the already-identified core 
words in the title. For a word w in the abstract, a 
co-occurrence-based feature COT(w) is defined 
as |S(w)|, where S(w) is the set of sentences 
which contain both w and at least one title core 
word. For a word w in the abstract, it is identified 
as an abstract core word when satisfying 
{ AF(w)> 0 ? MF(w) > ?1 ? COT (w) > ?2} 
Similarly, for a word w in the main text, it is 
identified as a general core word when satisfying 
{ MF(w) > ?1 ? COTA (w) >?2} 
where COTA (w) = |S?(w)| and S?(w) is the set of 
sentences which contain both w and at least one 
identified title core word or abstract core word. 
With this progressive algorithm, new core 
words can be more accurately identified with the 
previously identified core words. In the above 
heuristics, the parameters ? and ? are pre-defined 
thresholds, which are manually assigned2.  
                                                 
2 (?1, ?2, ?1, ?2) = (10, 5, 20, 10) in the system 
As a matter of fact, this heuristic-based 
identification approach is simple and preliminary. 
More sophisticated approaches, such as training 
machine learning models to classify the words, 
can be applied for better performance. Moreover, 
more useful features can also be considered. 
Nevertheless, we adopted the heuristic-based 
implementation to test the applicability of the 
framework as an initial study.  
An example of the identified core words is 
illustrated in Table 1 below: 
Type Core Word 
Title grid, service, discovery, UDDI 
Abstract distributed, multiple, web, computing, 
registry, deployment, scalability, DHT, 
DUDE, architecture 
Main proxy, search, node, key, etc. 
Table 1: Different types of core words 
2.2 Core Word Expansion 
Given the identified core words, the keyphrases 
can then be generated by expanding the core 
words. An example of the expansion process is 
illustrated below as 
grid ? grid service ? grid service discovery ? 
scalable grid service discovery  
For a core word, each appearance of it can be 
viewed as a potential expanding point. For each 
expanding point of the word, we need to judge if 
the context words can form a keyphrase along 
with it. Formally, for a candidate word w and the 
current phrase e (here we assume that w is the 
previous word, the case for the next word is 
similar), we consider the following features to 
judge if e should be expanded to w+e. 
Frequencies: the frequency of w (denoted by 
Freq(w)) and the frequency of the combination 
of w and e (denoted by phraseFreq(w, e)) which 
reflects the degree of w and e forming an 
integrated phrase. 
POS pattern: The part-of-speech tag of the 
word w is also considered here, i.e., we only try 
to expand w to w+e when w is a noun, an 
adjective or the specific conjunction ?of?. 
A heuristic-based approach is adopted here 
again. We intend to define some loose heuristics, 
which prefer long keyphrases. The heuristics 
include (1) If w and e are in the title or abstract, 
expand e to e+w when w satisfies the POS 
constraint and Freq(w) > 1; (2) If w and e are in 
the main text, expand e to e+w when w satisfies 
the POS constraint and phraseFreq(w, e) >1.  
More examples are provided in Table 2 below. 
 
 
143
Core Word Expanded Key Phrase 
grid scalable grid service discovery, 
grid computing 
UDDI UDDI registry, UDDI key 
web web service,  
scalability Scalability issue 
DHT DHT node 
Table 2: Core words and corresponding key phrases 
3 Results 
3.1 The Initial PolyU System in SemEval-2 
In the Semeval-2 test set, a total of 100 articles 
are provided. Systems are required to generate 15 
keyphrases for each article. Also, 15 keyphrases 
are generated by human readers as standard 
answers. Precision, recall and F-value are used to 
evaluate the performance. 
To generate exactly 15 keyphrases with the 
framework, we expand the core words in the title, 
abstract and main text in turn. Moreover, the core 
words in one fixed field are expanded following 
the descending order of frequency. When 15 
keyphrases are obtained, the process is stopped.  
For each new phrase, a redundancy check is 
also conducted to make sure that the final 15 
keyphrases can best cover the core concepts of 
the article, i.e.,  
(1) the new keyphrase should contain at least one 
word that is not included in any of the selected 
keyphrases; 
(2) if a selected keyphrase is totally covered by 
the new keyphrase, the covered keyphrase will 
be substituted by the new keyphrase. 
    The resulting system based on the above 
method is the one we submitted to SemEval-2. 
3.2 Phrase Filtering and Ranking 
Initially, we intend to use just the proposed 
framework to develop our system, i.e., using the 
expanded phrases as the keyphrases. However, 
we find out later that it must be adjusted to suit 
the requirement of the SemEval-2 task. In our 
subsequent study, we consider two adjustments, 
i.e., phrase filtering and phrase ranking.  
In SemEval-2, the evaluation criteria require 
exact match between the phrases. A phrase that 
covers a reference keyphrase but is not equal to it 
will not be counted as a successful match. For 
example, the candidate phrase ?scalable grid 
service discovery? is not counted as a match 
when compared to the reference keyphrase ?grid 
service discovery?. We call this the ?partial 
matching problem?. In our original framework, 
we followed the idea of ?expanding the phrase as 
much as possible? and adopted loose conditions. 
Consequently, the partial matching problem is 
indeed very serious. This unavoidably affects its 
performance under the criteria in SemEval-2 that 
requires exact matches. Therefore, we consider a 
simple filtering strategy here, i.e., filtering any 
keyphrase which only appears once in the article.  
Another issue is that the given task requires a 
total of exactly 15 keyphrases. Naturally we need 
a selection process to handle this. As to our 
framework, a keyphrase ranking process is 
necessary for discovering the best 15 keyphrases, 
not the best 15 core words. For this reason, we 
also try a simple method that re-ranks the 
expanded phrases by their frequencies. The top 
15 phrases are then selected finally. 
3.3 Results 
Table 3 below shows the precision, recall and F-
value of our submitted system (PolyU), the best 
and worst systems submitted to SemEval-2 and 
the baseline system that uses simple TF-IDF 
statistics to select keyphrases. 
On the SemEval-2 test data, the performance 
of the PolyU system was not good, just a little 
better than the baseline. A reason is that we just 
developed the PolyU system with our past 
experiences but did not adjust it much for better 
performance (since we were focusing on 
designing the new framework). After the 
competition, we examined two refined systems 
with the methods introduced in section 3.2. 
First, the PolyU system is adapted with the 
phrase filtering method. The performance of the 
resulting system (denoted by PolyU+) is given in 
Table 4. As shown in Table 4, the performance is 
much better just with this simple refinement to 
meet the requirement on extract matches for the 
evaluation criteria. Then, the phrase ranking 
method is also incorporated into the system. The 
performance of the resulting system (denoted by 
PolyU++) is also provided in Table 4. The 
performance is again much improved with the 
phrase ranking process. 
3.4 Discussion 
In our participation in SemEval-2, we submitted 
the PolyU system with the proposed extraction 
framework, which is based on expanding the 
core words to keyphrases. However, the PolyU 
system did not perform well in SemEval-2. 
However, we also showed later that the 
framework can be much improved after some
144
Simple but necessary refinements are made 
according to the given task. The final PolyU++ 
system with two simple refinements is much 
better. These refinements, including phrase 
filtering and ranking, are similar to traditional 
techniques. So it seems that our expansion-based 
framework is more applicable along with some 
traditional techniques. Though this conflicts our 
initial objective to develop a totally novel 
framework, the framework shows its ability of 
finding those keyphrases which contain different 
types of words. As to the PolyU++ system, when 
adapted with just two very simple post-
processing methods, the extracted candidate 
phrases can already perform quite well in 
SemEval-2. This may suggest that the framework 
can be considered as a new way for candidate 
keyphrase identification for the traditional 
extraction process. 
4 Conclusion and future work 
In this paper, we introduced our system in our 
participation in SemEval-2. We proposed a new 
framework for the keyphrase extraction task, 
which is based on expanding core words to 
keyphrases. Heuristic approaches are developed 
to implement the framework. We also analyzed 
the errors of the system in SemEval-2 and 
conducted some refinements. Finally, we 
concluded that the framework is indeed 
appropriate as a candidate phrase identification 
method. Another issue is that we just consider 
some simple information such as frequency or 
POS tag in this initial study. This indeed limits 
the power of the resulting systems. In future 
work, we?d like to develop more sophisticated 
implementations to testify the effectiveness of 
the framework. More syntactic and semantic 
features should be considered. Also, learning 
models can be applied to improve both the core 
word identification approach and the word 
expansion approach. 
 
Acknowledgments 
The work described in this paper is supported by 
Hong Kong RGC Projects (PolyU5217/07E and 
PolyU5230/08E). 
References  
Frank, E., Paynter, G.W., Witten, I., Gutwin, C. and 
Nevill-Manning, C.G.. 1999. Domain Specific 
Keyphrase Extraction. Proceedings of the IJCAI 
1999, pp.668--673. 
Medelyan, O. and Witten, I. H.. 2006. Thesaurus 
based automatic keyphrase indexing. Proceedings 
of the JCDL 2006, Chapel Hill, NC, USA. 
Medelyan, O. and Witten, I. H.. 2008. Domain 
independent automatic keyphrase indexing with 
small training sets. Journal of American Society for 
Information Science and Technology. Vol. 59 (7), 
pp. 1026-1040 
SemEval-2. Evaluation Exercises on Semantic 
Evaluation. http://semeval2.fbk.eu/ 
Turney, P.. 1999. Learning to Extract Keyphrases 
from Text. National Research Council, Institute for 
Information Technology, Technical Report ERB-
1057. (NRC \#41622), 1999. 
Wan, X. Xiao, J.. 2008. Single document keyphrase 
extraction using neighborhood knowledge. In 
Proceedings of AAAI 2008, pp 885-860. 
 
System 5 Keyphrases 10 Keyphrases 15 Keyphrases P R F P R F P R F 
Best 34.6% 14.4% 20.3% 26.1% 21.7% 23.7% 21.5% 26.7% 23.8%
Worst 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%
PolyU 13.6% 5.65% 7.98% 12.6% 10.5% 11.4% 12.0% 15.0% 13.3%
Baseline 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%
Table 3: Results from SemEval-2 
 
System 5 Keyphrases 10 Keyphrases 15 Keyphrases P R F P R F P R F 
PolyU 13.6% 5.65% 7.98% 12.6% 10.5% 11.4% 12.0% 15.0% 13.3%
PolyU+ 21.2% 8.8% 12.4% 16.9% 14.0% 15.3% 13.9% 17.3% 15.4%
PolyU++ 31.2% 13.0% 18.3% 22.1% 18.4% 20.1% 20.3% 20.6% 20.5%
Table 4: The performance of the refined systems 
 
145
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 18?27,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Towards Scalable Speech Act Recognition in Twitter:  
Tackling Insufficient Training Data 
 
 
Renxian Zhang Dehong Gao Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University 
{csrzhang, csdgao, cswjli}@comp.polyu.edu.hk 
 
 
 
 
Abstract 
Recognizing speech act types in Twitter is of 
much theoretical interest and practical use. 
Our previous research did not adequately 
address the deficiency of training data for this 
multi-class learning task. In this work, we set 
out by assuming only a small seed training set 
and experiment with two semi-supervised 
learning schemes, transductive SVM and 
graph-based label propagation, which can 
leverage the knowledge about unlabeled data. 
The efficacy of semi-supervised learning is 
established by our extensive experiments, 
which also show that transductive SVM is 
more suitable than graph-based label 
propagation for our task. The empirical 
findings and detailed evidences can 
contribute to scalable speech act recognition 
in Twitter. 
1. Introduction 
The social media platform of Twitter makes 
available a plethora of data to probe the 
communicative act of people in a social network 
woven by interesting events, people, topics, etc. 
Communicative acts such as disseminating 
information, asking questions, or expressing 
feelings all fall in the purview of ?speech act?, a 
long established area in pragmatics (Austin 
1962). The automatic recognition of speech act 
in tons of tweets has both theoretical and 
practical appeal. Practically, it helps tweeters to 
find topics to read or tweet about based on 
speech act compositions. Theoretically, it 
introduces a new dimension to study social 
media content as well as providing real-life data 
to validate or falsify claims in the speech act 
theory. 
Different taxonomies of speech act have been 
proposed by linguists and computational 
linguists, ranging from a few to over a hundred 
types. In this work, we adopt the 5 types of 
speech act used in our previous work (Zhang et 
al. 2011), which are in turn inherited from 
(Searle 1975): statement, question, suggestion, 
comment, and miscellaneous. Our choice is 
based on the fact that unlike face-to-face 
communication, twittering is more in a 
?broadcasting? style than on a personal basis. 
Statement and comment, which are usually 
intended to make one?s knowledge, thought, and 
sentiment known, thus befit Twitter?s 
communicative style. Question and suggestion 
on Twitter are usually targeted at other tweeters 
in general or one?s followers. More interpersonal 
speech acts such as ?threat? or ?thank? as well as 
rare speech acts in Twitter (Searle?s (1975) 
?commissives? and ?declaratives?) are relegated 
to ?miscellaneous?. Some examples from our 
experimental datasets are provided in Table 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
18
Tweet Speech Act 
Libya Releases 4 Times 
Journalists - 
http://www.photozz.com/?104k 
Statement 
#sincewebeinghonest why u so 
obsessed with what me n her 
do?? Don't u got ya own 
man???? Oh wait..... 
Question 
RT @NaonkaMixon: I will 
donate 10 $ to the Red Cross 
Japan Earthquake fund for 
every person that retweets this! 
#PRAYFORJAPAN 
Suggestion 
is enjoying this new season of 
#CelebrityApprentice.... Nikki 
Taylor = Yum!! 
Comment 
65. I want to get married to 
someone i meet in highschool. 
#100factsaboutme 
Miscellaneous 
Table 1. Example Tweets with Speech acts 
 
Assuming one tweet demonstrates only one 
speech act, the automatic recognition of those 
speech act types in Twitter is a multi-class 
classification task. We concede that this 
assumption may not always hold in real 
situations. But given the short length of tweets, 
multi-speech act tweets are rare and we find this 
simplifying assumption effective in reducing the 
complexity of our problem. A major problem 
with this task is the deficiency of training data. 
Tweeters as well as face-to-face interlocutors do 
not often identify their speech acts; human 
annotation is costly and time-consuming. 
Although our previous research (Zhang et al 
2011) sheds light on the preparation of training 
data, it did not adequately address this problem. 
Our contribution in this work is to directly 
address the problem of training data deficiency 
by using two well-known semi-supervised 
learning techniques that leverage the relationship 
between a small seed of training data and a large 
body of unlabeled data: transductive SVM and 
graph-based label propagation. The empirical 
results show that the knowledge about unlabeled 
data provides promising solutions to the data 
deficiency problem, and that transductive SVM 
is more competent for our task. Our exploration 
with different training/unlabeled data ratios for 
three major Twitter categories and a mixed-type 
category provides solid evidential support for 
future research. 
The rest of the paper is organized as follows. 
Section 2 reviews works related to speech act 
recognition and semi-supervised learning; 
Section 3 briefly discusses supervised learning of 
speech act types developed in our earlier work 
and complementing the previous findings with 
learning curves. The technical details of semi-
supervised learning are presented in Section 4. 
Then we report and discuss the results of our 
experiments in Section 5. Finally, Section 6 
concludes the paper and outlines future 
directions. 
2. Related Work  
The automatic recognition of speech act, also 
known as ?dialogue act?, has attracted sustained 
interest in computational linguistics and speech 
technology for over a decade (Searle 1975; 
Stolcke et al 2000). A few annotated corpora 
such as Switchboard-DAMSL (Jurafsky et al 
1997) and Meeting Recorder Dialog Act (Dhillon 
et al 2004) are widely used, with data 
transcribed from telephone or face-to-face 
conversation. 
Prior to the flourish of microblogging services 
such as Twitter, speech act recognition has been 
extended to electronic media such as email and 
discussion forum (Cohen et al 2004; Feng et al 
2006) in order to study the behavior of email or 
message senders. 
The annotated corpora for ordinary verbal 
communications and the methods developed for 
email, or discussion forum cannot be directly 
used for our task because Twitter text has a 
distinctive Netspeak style that is situated 
between speech and text but resembles neither 
(Crystal 2006, 2011). Compared with email or 
forum post, it is rife with linguistic noises such 
as spelling mistakes, random coinages, mixed 
use of letters and symbols. 
Speech act recognition in Twitter is a fairly 
new task. In our pioneering work (Zhang et al 
2011), we show that Twitter text normalization is 
unnecessary and even counterproductive for this 
task. More importantly, we propose a set of 
useful features and draw empirical conclusion 
about the scope of this task, such as recognizing 
speech act on the coarse-grade category level 
works as well as on the fine-grade topic level. In 
this work, we continue to adopt this framework 
including other learning details (speech act types 
and feature selection for tweets), but the new 
quest starts where the old one left: tackling 
insufficient training data. 
19
As in many practical applications, sufficient 
annotated data are hard to obtain. Therefore, 
unsupervised and semi-supervised learning 
methods are actively pursued. While 
unsupervised sentence classification is rule-based 
and domain-dependent (Deshpande et al 2010), 
semi-supervised methods that both alleviate the 
data deficiency problem and leverage the power 
of state-of-the-art classifiers hold more promises 
for different domains (Medlock and Briscoe 
2007; Erkan et al 2007). 
In the machine learning literature, a classic 
semi-supervised learning scheme is proposed by 
Yarowsky (1995), which is a classical self-
teaching process that makes no use of labeled 
data before they are classified. More theoretical 
analyses are made by (Culp and Michailidis 2007) 
and (Haffari and Sarkar 2007).  
Transductive SVM (Joachims 1999) extends 
the state-of-the-art inductive SVM by explicitly 
considering the relationship between labeled and 
unlabeled data. The graph-based label 
propagation model (Zhu et al 2003; Zhou et al 
2004) using a harmonic function also 
accommodates the knowledge about unlabeled 
data. We will adapt both of them to our multi-
class classification task. 
Jeong et al (2009) report a semi-supervised 
approach to classifying speech acts in emails and 
online forums. But their subtree-based method is 
not applicable to our task because Twitter?s noisy 
textual quality cannot be found in the much 
cleaner email or forum texts. 
3. Supervised Learning of Speech Act 
Types  
Supervised learning of speech act types in 
Twitter relies heavily on a good set of features 
that capture the textual characteristics of both 
Twitter and speech act utterances. As in our 
previous work, we use speech act-specific cues, 
special words (abbreviations and acronyms, 
opinion words, vulgar words, and emoticons), 
and special characters (Twitter-specific 
characters and a few punctuations). Tweet-
external features such as tweeter profile may also 
help, but that is beyond the focus of this paper. 
Although it has been empirically shown that 
speech act recognition in Twitter can be done 
without using training data specific to topics or 
even categories, it is not clear how much training 
data is needed to achieve desirable performance. 
In order to answer this question, we adopt the 
same experimental setup and datasets as reported 
in (Zhang et al 2011) and plot the learning 
curves shown in Figure 1. 
 
 
Figure 1. Learning Curves of Each Category and 
All Tweets 
 
For all individual experiments, the test data are 
a randomly sampled 10% set of all annotated 
data. When training data reach 90%, we actually 
duplicate the reported results. However, Figure 1 
shows that it is unnecessary to use so much 
training data to achieve good classification 
performance. For News and Entity, the 
classification makes little noticeable 
improvement after the training data ratio reaches 
40% (training : test = 4 : 1). For Mixed (the 
aggregate of the News, Entity, LST datasets) and 
LST, performance peaks even earlier at 20% 
training data (training : test = 2 : 1) and 10% 
(training : test = 1 : 1).  
It is delightful to see that only a moderate 
number of annotated data are needed for speech 
act recognition. But even that number (for the 
Mixed dataset, 10% training data are over 800 
annotated tweets) may not be available and in 
many situations, test data may be much more 
than training data. Taking this challenge is the 
next important step we make. 
4. Semi-Supervised Learning of Speech 
Act Types  
The problem setting of a small seed training 
(labeled) set and a much larger test (labeled) set 
fits the semi-supervised learning scheme. Classic 
semi-supervised learning approaches such as 
self-teaching methods (e.g., Yarowsky 1995) are 
mainly concerned with incrementing high-
confidence labeled data in each round of training. 
They do not, however, directly take into account 
the knowledge about unlabeled data. The recent 
research emphasis is on leveraging knowledge 
about unlabeled data during training. In this 
section, we discuss two such approaches. 
20
4.1 Transductive SVM 
The standard SVM classifier popularly used in 
text classification is also known as inductive 
SVM as a model is induced from training data. 
The model is solely dependent on the training 
data and agnostic about the test data. In contrast, 
transductive SVM (Vapnik 1998; Joachims 1999) 
predicts test labels by using the knowledge about 
test data. In the case of test (unlabeled) data far 
outnumbering training (labeled) data, 
transductive SVM provides a feasible scheme of 
semi-supervised learning. 
For a single-class classification problem {xi, yi} 
that focuses on only one speech act type, where 
xi is the ith tweet and yi is the corresponding 
label and { 1, 1}iy ? ? ?  denotes whether xi 
contains the speech act or not, inductive SVM is 
formulated to find an optimal hyperplane 
sign(w?xi ? b) to maximize the soft margin 
between positive and negative objects, or to 
minimize: 
 
21/ 2 iiC ?? ?w
 
s.t. ( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
 
where
i? is a slack variable. Adopting the same 
formulation, transductive SVM further considers 
test data xi* during training by finding a labeling 
yj* and a hyperplane to maximize the soft margin 
between both training and test data, or to 
minimize: 
 
2
1 21/ 2 i ii iC C? ?? ?? ?w
 
s.t. ( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
      * *( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
 
where
i? is a slack variable for the test data. In 
fact, labeling test data is done during training. 
As the maximal margin approach proves very 
effective for text classification, its transductive 
variant that effectively uses the knowledge about 
test data holds promises of handling the 
deficiency of labeled data. 
4.2 Graph-based Label Propagation 
An alternative way of using unlabeled data in 
semi-supervised learning is based on the intuition 
that similar objects should belong to the same 
class, which can be translated into label 
smoothness on a graph with weights indicating 
object similarities. This is the idea underlying 
Zhu et al?s (2003) graph-based label propagation 
model using Gaussian random fields.   
We again focus on a single-class classification 
problem. Formally, {x1, ? xN} are N tweets, 
having their actual speech act labels y = {y1, ? 
yL, ? yN} (yi ?{1, 0} denoting whether xi 
contains the speech act or not) with the first L of 
them known, and f = {f1, ? fL, ? fN} are their 
predicted labels. Let L = {x1, ? xL} and U = 
{xL+1, ? xN} and the task is to determine 
{fL+1, ? fN} for U. We further define a graph G = 
(V, E), where V = L?U and E is weighted by W 
= [wij]N?N  with wij denoting the similarity 
between xi and xj. Preferring label smoothness on 
G and preserving the given labels, we want to 
minimize the loss function: 
 
2
,
( ) 1/ 2 ( ) Tij i j
i j L U
E w f f
? ?
? ? ??f f ?f
 
s.t. fi = yi (i = 1, ?, L) 
 
where ? = D ? W is the combinatorial graph 
Laplacian with D being a diagonal matrix [dij]N?N 
and 
ii ijj
d w??
. 
This can be expressed as a harmonic function, 
h = argmin fL = yLE(f), which satisfies the 
smoothness property on the graph: 
( ) 1/ ( ( ))ii ikkh i d w h k? ?
. If we define 
/ij ij ikkp w w? ?
and collect pij and h(i) into 
matrix P and column vector h, solving ?h = 0 s.t. 
hL = yL is equivalent to solving h = Ph. 
To find the solution, we can use L and U to 
partition h and P: 
L
U
? ?? ? ?? ?
hh h
, ,
,
LL LU
UL UU
? ?? ? ?? ?
P PP P P
 
and it can be shown that 1( )U UU UL L?? ?h I P P y. 
To get the final classification result, those 
elements in hU that are greater than a threshold 
(0.5) become 1 and the others become 0. 
This approach propagates labels from labeled 
data to unlabeled data on the principle of label 
smoothness. If the assumption about similar 
tweets having same speech acts holds, it should 
work well for our problem. 
4.3 Multi-class Classification 
In the previous formulations, we emphasized 
?single-class classification? because both 
21
transductive SVM and graph-based label 
propagation are inherently one class-oriented. 
Since our problem is a multi-class one, we 
transform the problem to single-class 
classifications by using the one-vs-all scheme.  
Specifically, for each class (speech act type) ci, 
we label all training instances belonging to ci as 
+1 and all those belonging to other classes as ?1 
and then do binary classification. For our 
problem with 5 speech act types, we make 5 such 
transformations. The final prediction is made by 
choosing the class with the highest classification 
score from the 5 binary classifiers. Both 
transductive SVM and graph-based label 
propagation produce real-valued classification 
scores and are amenable to this scheme. 
5. Experiments  
Our experiments are designed to answer two 
questions: 1) How useful is semi-supervised 
speech act learning in comparison with 
supervised learning? 2) Which semi-supervised 
learning approach is more appropriate for our 
problem? 
5.1 Experimental Setup 
We use the 6 datasets in our previous study1 , 
which fall into 3 categories: News, Entity, Long-
standing Topic (LST). Each of the total 8613 
tweets is labeled with one of the following 
speech act types: sta (statement), que (question), 
sug (suggestion), com (comment), mis 
(miscellaneous). In addition, we randomly select 
1000 tweets from each of the categories to create 
a Mixed category of 3000 tweets. Figures 2 to 5 
illustrate the distributions of the speech act types 
in the 3 original categories and the Mixed 
category. 
 
 
Figure 2. Speech Act Distribution (News) 
 
                                                          
1 http://www4.comp.polyu.edu.hk/~csrzhang 
 
Figure 3. Speech Act Distribution (Entity) 
 
 
Figure 4. Speech Act Distribution (LST) 
 
 
Figure 5. Speech Act Distribution (Mixed) 
 
For each category, we use two 
labeled/unlabeled data settings, with labeled data 
accounting for 5% and 10% of the total so that 
the labeled/unlabeled ratios are set at 
approximately 1:19 and 1:9. The labeled data in 
each category are randomly selected in a 
stratified way: using the same percentage to 
select labeled data with each speech act type. The 
stratified selection is intended to keep the speech 
act distributions in both labeled and unlabeled 
data. Table 2 and Table 3 list the details of data 
splitting using the two settings. 
 
Category # Labeled # Unlabeled Total 
News 155 2995 3150 
Entity 72 1391 1463 
LST 198 3802 4000 
Mixed 147 2853 3000 
Table 2. Stratified Data Splitting with 5% as 
Labeled 
 
22
Category # Labeled # Unlabeled Total 
News 312 2838 3150 
Entity 144 1319 1463 
LST 399 3601 4000 
Mixed 298 2702 3000 
Table 3. Stratified Data Splitting with 10% as 
Labeled 
 
For comparison with supervised learning, we 
also use inductive SVM. The inductive and 
transductive SVM classifications are 
implemented by using the SVMlight tool2 with a 
linear kernel. For the graph-based label 
propagation method, we populate the similarity 
matrix W with weights calculated by a Gaussian 
function. Given two tweets xi and xj,  
 
2
2exp( )2
i j
ijw ?
?? ? x x
 
 
where ?.? is the L2 norm. Empirically, the 
Gaussian function measure leads to better results 
than other measures such as cosine. Then we 
convert the graph to an ?NN graph (Zhu and 
Goldberg 2009) by removing edges with weight 
less than a threshold because the ?NN graph 
empirically outperforms the fully connected 
graph. The threshold is set to be ? + ?, the mean 
of all weights plus one standard deviation. 
5.2 Results 
To better evaluate the performance of semi-
supervised learning on speech act recognition in 
Twitter, we report the classification scores for 
both multi-class and individual classes, as well as 
confusion matrices. 
 
Multi-class Evaluation 
Table 4 lists the macro-average F scores and 
weighted average F scores for all classifiers and 
all categories at the 5% labeled data setting. 
Macro-average F is chosen because it gives equal 
weight to all classes. Since some classes (e.g., sta) 
have much more instances than others (e.g., que), 
macro-average F ensures that significant score 
change on minority classes will not be 
overshadowed by small score change on majority 
classes. In contrast, weighted average F is 
calculated according to class instance numbers, 
which is chosen mainly because we want to 
compare the result with supervised learning 
(reported in Zhang et al 2011 and Figure 1). In 
                                                          
2 http://svmlight.joachims.org/ 
this and the following tables, iSVM, tSVM, and 
GLP denote inductive SVM, transductive SVM, 
and graph-based label propagation. 
 
 
Macro-average F Weighted average F 
iSVM tSVM GLP iSVM tSVM GLP 
News .374 .502 .285 .702 .759 .643 
Entity .312 .395 .329 .493 .534 .436 
LST .295 .360 .216 .433 .501 .376 
Mixed .383 .424 .245 .539 .537 .391 
Table 4. Multi-class F scores (5% labeled data) 
 
Almost without exception, transductive SVM 
achieves the best performance. Measured by 
macro-average F, it outperforms inductive SVM 
with a gain of 10.7% (Mixed) to 34.2% (News). 
Consistent with supervised learning results, 
semi-supervised learning results degrade with 
News > Entity > LST, indicating that both semi-
supervised learning and supervised learning are 
sensitive to dataset characteristics. More uniform 
tweet set (e.g., News) leads to better 
classification and greater improvement by semi-
supervised learning. That also explains why the 
Mixed category, composed of the most 
diversified tweets, benefits least from semi-
supervised learning. 
Conversely, supervised learning (inductive 
SVM) on the Mixed category benefits from the 
data hodgepodge even though the test data are 19 
times the training data. Its macro-average F is 
higher than the other categories although it does 
not have the most training data. Its weighted-
average F using inductive SVM is even higher 
than using transductive SVM. 
It is a little surprising to find that the graph-
based label propagation performs very poorly. In 
all but one place, the GLP score is lower than its 
iSVM counterpart. This may indicate that the 
graph method cannot adapt well to the multi-
class scenario and we will show more evidences 
in the next two sections. 
To understand the effectiveness of semi-
supervised learning, a better way than doing 
numerical calculation is juxtaposing semi-
supervised data settings with their comparable 
supervised data settings, which is shown in Table 
5. The supervised data settings are of those with 
the closest weighted average F (waF) to the 
semi-supervised (tSVM) waF from our previous 
results (Figure 1). 
 
23
 # labeled labeled :unlabeled waF 
 Semi-supervised (tSVM) 
News 155 1 : 19 .759 
Entity 72 1 : 19 .534 
LST 198 1 : 19 .501 
Mixed 147 1 : 19 .537 
 Supervised (with closest waF) 
News 945 1 : 0.3 .768 
Entity 146 1 : 1 .589 
LST 800 1 : 0.5 .501 
Mixed 861 1 : 1 .596 
 
Table 5. Semi-supervised Learning vs. 
Supervised Learning 
 
Obviously semi-supervised learning by 
transductive SVM can achieve classification 
performance comparable to supervised learning 
by inductive SVM, with less training data and 
much lower labeled/unlabeled ratio. This shows 
that semi-supervised learning such as 
transductive SVM holds much promise for 
scalable speech act recognition in Twitter. 
It is tempting to think that with more labeled 
data and higher labeled/unlabeled ratio, semi-
supervised learning performance should improve. 
To put this conjecture to test, we double the 
labeled data (from 5% to 10%) and 
labeled/unlabeled ratio (from 1/19 to 1/9), with 
results in Table 6. 
 
 
Macro-average F Weighted average F 
iSVM tSVM GLP iSVM tSVM GLP 
News .403 .524 .298 .731 .762 .647 
Entity .441 .440 .311 .587 .575 .406 
LST .335 .397 .216 .459 .512 .384 
Mixed .435 .463 .284 .557 .553 .415 
Table 6. Multi-class F scores (10% labeled data) 
 
Compared with Table 4, increased labeled data 
does lead to some improvement, but not much as 
we would expect, the largest gain being 15.9% 
(macro-average F on Mixed, using GLP). Note 
that this is achieved at the cost of labeling twice 
as much data and predicting half as much. In 
contrast, the inductive SVM performance is 
improved by as much as 41.3% (macro-average 
F on Entity). Such evidence shows that semi-
supervised learning of speech acts in Twitter 
benefits disproportionately little from increased 
labeled data, or at least the gain is not worth the 
pain. In fact, this is good news for scalable 
speech act recognition. 
 
Individual Class Evaluation 
For more microscopic inspection, we also report 
the classification results on individual classes for 
all categories. In Table 7, we list the rankings of 
F measures by each classifier for each speech act 
type and each category. The one-letter notations i, 
t, g are short for iSVM, tSVM, and GLP. 
Therefore, t > g > i means tSVM outperforms 
GLP, which outperforms iSVM, in terms of F 
measure. The labeled data are 5%. 
 
 Sta Que Sug Com Mis 
News t >g>i t >i>g t >i>g t >i>g t >g>i 
Entity t >g>i t >i>g g >t>i i >t>g t >g>i 
LST i >g>t t >i>g i >t>g t >i>g t >g>i 
Mixed i >t>g t >i>g t >i>g i >t>g t >g>i 
Table 7. Classifier Rankings for Each Speech 
Act Type and Category (5% Labeled Data) 
 
In 15 out of the 20 rankings, transductive 
SVM or graph-based label propagation beats 
inductive SVM, which shows the efficacy of 
semi-supervised learning in this class-based 
perspective. Transductive SVM is the champion, 
claiming 14 top places.  
We also find that the overall performance of 
graph-based label propagation is the poorest, 
claiming 12 out of 20 bottom places. After 
inspecting the data, we observe that the 
underlying assumption of GLP that similar 
objects belong to the same class is questionable 
for speech act recognition in Twitter. Tweets 
with different speech acts (e.g., question and 
comment) may appear very similar on the graph. 
The maximal margin approach is apparently 
more appropriate for our problem.  
On the other hand, the GLP performance 
evaluated on individual classes is better than 
evaluated on the multi-class if we compare Table 
7 and Table 4, where GLP is almost always the 
lowest achiever. This indicates that in multi-class 
classification, GLP suffers further from the one-
vs-all converting scheme, a point we will make 
clearer in the following. 
 
 
 
24
Confusion matrices 
Confusion matrix provides another perspective to 
understand the multi-class classification 
performance. For brevity?s sake, we present the 
confusion matrices of the three classifiers on the 
News category with 5% labeled data in Figure 6 
to Figure 8. Similar patterns are also observed for 
the other categories and with 10% labeled data. 
Note that the rows represent true classes and the 
columns represent predicted classes. 
 
 Sta Que Sug Com Mis 
Sta 2043 0 5 14 0 
Que 46 7 2 9 0 
Sug 211 1 61 21 0 
Com 276 2 10 164 0 
Mis 120 0 1 2 0 
Figure 6. Confusion Matrix of iSVM (News, 5% 
Labeled Data) 
 
 Sta Que Sug Com Mis 
Sta 1848 4 56 90 64 
Que 19 17 7 20 1 
Sug 95 0 158 31 10 
Com 143 5 19 275 10 
Mis 94 3 4 15 7 
Figure 7. Confusion Matrix of tSVM (News, 5% 
Labeled Data) 
 
 Sta Que Sug Com Mis 
Sta 1852 0 4 11 195 
Que 19 6 0 0 39 
Sug 123 0 25 2 144 
Com 134 0 0 47 271 
Mis 102 0 0 1 20 
Figure 8. Confusion Matrix of GLP (News, 5% 
Labeled Data) 
 
The News category is typically biased towards 
the statement speech act, which accounts for 
69% of the total tweets according to Figure 2. As 
a result, the iSVM tends to classify tweets of the 
other speech acts as statement. Figure 6 also 
shows that the prediction accuracy is correlated 
with the training amount. The two classes with 
the least training data, question and 
miscellaneous, demonstrate the lowest accuracy. 
Clearly, supervised learning suffers from training 
data deficiency. 
Both tSVM and GLP show the effect of 
leveraging unlabeled data as they assign new 
labels to some instances wrongly classified as 
statement. Transductive SVM is more successful 
in that it moves most of the Sug and Com 
instances to the diagonal. The situation for Que 
and Mis is also better, though the prediction 
accuracy still suffers from lack of training data. 
Figure 8, however, reveals an intrinsic problem 
of applying graph-based label propagation to 
multi-class classification. Most instances are 
predicted as either Sta or Mis. The wrong 
prediction as Mis cannot be explained by 
imbalance of training data. Rather, it is due to the 
fact that the single-class scores for Mis after 
smoothing on the graph are generally higher than 
those for Que, Sug, or Com. In other words, the 
graph-based method is highly sensitive to class 
differences when multi-class prediction is 
converted from single-class predictions on a 
scheme like one-vs-all. 
In contrast, transductive SVM does not suffer 
much from class differences according to Figure 
7, proving to be more suitable for multi-class 
classification than graph-based label propagation. 
5.3 Summary 
For the task of recognizing speech acts in Twitter, 
we have made some interesting findings from the 
extensive empirical study. To wrap up, let?s 
summarize the most important of them in the 
following. 
1) Semi-supervised learning approaches, 
especially transductive SVM, perform 
comparably to supervised learning approaches, 
such as inductive SVM, with considerably less 
training data and lower training/test ratio. 
Increasing training data cannot improve 
performance proportionately. 
2) Transductive SVM proves to be more 
effective than graph-based label propagation for 
our task. The performance of the latter is hurt by 
two factors: a) the inappropriate assumption 
about similar tweets having the same speech act 
and b) its vulnerability to class differences under 
the one-vs-all multi-class conversion scheme. 
3) For supervised learning as well as semi-
supervised learning for multi-class classification, 
training data imbalance poses no lesser threat 
than training data deficiency. 
25
6. Conclusion and Future Work  
Speech act recognition in Twitter facilitates 
content-based user behavior study. Realizing that 
it is obsessed with insufficient training data, we 
start where previous research left. 
We are not aware of previous study of semi-
supervised learning of speech acts in Twitter and 
in this paper we contribute to scalable speech act 
recognition by drawing conclusions from 
extensive experiments. Specifically, we 
1) extend the work of (Zhang et al 2011) by 
establishing the practicality of semi-supervised 
learning that leverages the knowledge of 
unlabeled data as a promising solution to 
insufficient training data;  
2) show that transductive SVM is more 
effective than graph-based label propagation for 
our problem, which aptly extends the maximal 
margin approach to unlabeled data and is more 
amenable to the multi-class scenario; 
3) provide detailed empirical evidences of 
multi-class and single-class results, which can 
inform future extensions in this direction and 
design of practical systems. 
At this stage, we are not sure whether the one-
vs-all scheme is a bottleneck to one class-
oriented classifiers (it appears to be so for the 
graph-based method). Therefore we will next 
explore other multi-class conversion schemes 
and also consider semi-supervised learning using 
inherently multi-class classifiers such as Na?ve 
Bayes or Decision Tree. In the future, we will 
also explore unsupervised approaches to 
recognizing speech acts in Twitter. 
Acknowledgments 
The work described in this paper was supported 
by the grants GRF PolyU 5217/07E and PolyU 
5230/08E. 
 
26
References  
Austin, J. 1962. How to Do Things with Words. 
Oxford: Oxford University Press. 
Cohen, W., Carvalho, V., and Mitchell, T. 2004. 
Learning to Classify Email into ?Speech Acts?. In 
Proceedings of Empirical Methods in Natural 
Language Processing (EMNLP-04), 309?316.  
Crystal, D. 2006. Language and the Internet, 2nd 
edition. Cambridge, UK: Cambridge University 
Press. 
Crystal, D. 2011. Internet linguistics. London: 
Routledge. 
Culp M. and Michailidis, G. 2007. An Iterative 
Algorithm for Extending Learners to a 
Semisupervised Setting. In The 2007 Joint 
Statistical Meetings (JSM). 
Deshpande S. S., Palshikar, G. K., and Athiappan, G. 
2010. An Unsupervised Approach to Sentence 
Classification, In International Conference on 
Management of Data (COMAD 2010), Nagpur, 
India. 
Dhillon, R., Bhagat, S., Carvey, H., and Shriberg, E. 
2004. Meeting Recorder Project: Dialog Act 
Labeling Guide. Technical report, International 
Computer Science Institute. 
Erkan, G., ?zg?r, A., and Radev, D. 2007. Semi-
Supervised Classification for Extracting Protein 
Interaction Sentences Using Dependency Parsing. 
In Proceedings of the 2007 Joint Conference on 
Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 228?237. 
Feng, D., Shaw, E., Kim, J., and Hovy. E. H. 2006. 
Learning to Detect Conversation Focus of 
Threaded Discussions. In Proceedings of HLT-
NAACL, 208?215. 
Haffari G.R. and Sarkar. A. 2007. Analysis of semi-
supervised learning with the Yarowsky algorithm. 
In 23rd Conference on Uncertainty in Artificial 
Intelligence (UAI). 
Jeong, M., Lin, C-Y., and Lee, G. 2009. Semi-
supervised Speech Act Recognition in Emails and 
Forums. In Proceedings of EMNLP, pages 1250?
1259. 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. In 
Proceedings of the 16th International Conference 
on Machine Learning (ICML). 
Jurafsky, D., Shriberg, E., and Biasca, D. 1997. 
Switchboard SWBD-DAMSL Labeling Project 
Coder?s Manual, Draft 13. Technical report, 
University of Colorado Institute of Cognitive 
Science. 
Medlock, B., and Briscoe, T. 2007. Weakly 
Supervised Learning for Hedge Classification in 
Scientific Literature. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics, 992?999. 
Searle, J. 1975. Indirect speech acts. In P. Cole and J. 
Morgan (eds.), Syntax and semantics, vol. iii: 
Speech acts (pp. 59?82). New York: Academic 
Press. 
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, 
R., Jurafsky, D., Taylor, P., Martin, R. Van Ess-
Dykema, C., and Meteer, M. 2000. Dialogue Act 
Modeling for Automatic Tagging and Recognition 
of Conversational Speech. Computational 
Linguistics, 26(3):339?373. 
Vapnik, V. 1998. Statistical Learning Theory. New 
York: John Wiley & Sons. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL-
1995), 189?196. 
Zhang, R., Gao, D., and Li, W. 2011. What Are 
Tweeters Doing: Recognizing Speech Acts in 
Twitter. In AAAI-11 Workshop on Analyzing 
Microtext. 
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and 
Scholkopf, B. 2004. Learning with Local and 
Global Consistency. Advances in Neural 
Information Processing Systems (NIPS), vol. 16, 
Cambridge, MA: MIT Press. 
Zhu, X., Ghahramani, Z., and Lafferty, J. D. 2003. 
Semi-supervised Learning Using Gaussian Fields 
and Harmonic Functions. In Proceedings of the 
Twentieth International Conference on Machine 
Learning (ICML), 912?919, Washington, DC. 
Zhu, X. and Goldberg, A. B., 2009. Introduction to 
Semi-Supervised Learning. Morgan & Claypool 
Publishers. 
 
27
