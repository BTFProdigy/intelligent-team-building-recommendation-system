Inducing Information Extraction Systems for New Languages
via Cross-Language Projection
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Charles Schafer and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{cschafer,yarowsky}@cs.jhu.edu
Abstract
Information extraction (IE) systems are costly to
build because they require development texts, pars-
ing tools, and specialized dictionaries for each ap-
plication domain and each natural language that
needs to be processed. We present a novel
method for rapidly creating IE systems for new lan-
guages by exploiting existing IE systems via cross-
language projection. Given an IE system for a
source language (e.g., English), we can transfer its
annotations to corresponding texts in a target lan-
guage (e.g., French) and learn information extrac-
tion rules for the new language automatically. In
this paper, we explore several ways of realizing both
the transfer and learning processes using off-the-
shelf machine translation systems, induced word
alignment, attribute projection, and transformation-
based learning. We present a variety of experiments
that show how an English IE system for a plane
crash domain can be leveraged to automatically cre-
ate a French IE system for the same domain.
1 Introduction
Information extraction (IE) is an important appli-
cation for natural language processing, and recent
research has made great strides toward making IE
systems easily portable across domains. However,
IE systems depend on parsing tools and specialized
dictionaries that are language specific, so they are
not easily portable across languages. In this re-
search, we explore the idea of using an information
extraction system designed for one language to au-
tomatically create a comparable information extrac-
tion system for a different language.
To achieve this goal, we rely on the idea of cross-
language projection. The basic approach is the fol-
lowing. First, we create an artificial parallel cor-
pus by applying an off-the-shelf machine translation
(MT) system to source language text (here, English)
to produce target language text (here, French). Or
conversely, in some experiments we generate a par-
allel corpus by applying MT to a French corpus
to produce artificial English. We then run a word
alignment algorithm over the parallel corpus. Next,
we apply an English IE system to the English texts
and project the IE annotations over to the corre-
sponding French words via the induced word align-
ments. In effect, this produces an automatically an-
notated French corpus. We explore several strate-
gies for transferring the English IE annotations to
the target language, including evaluation of the
French annotations produced by the direct projec-
tion alone, as well as the use of transformation-
based learning to create French extraction rules
from the French annotations.
2 Information Extraction
The goal of information extraction systems is to
identify and extract facts from natural language text.
IE systems are usually designed for a specific do-
main, and the types of facts to be extracted are de-
fined in advance. In this paper, we will focus on the
domain of plane crashes and will try to extract de-
scriptions of the vehicle involved in the crash, vic-
tims of the crash, and the location of the crash.
Most IE systems use some form of extraction
patterns to recognize and extract relevant informa-
tion. Many techniques have been developed to gen-
erate extraction patterns for a new domain automat-
ically, including PALKA (Kim & Moldovan, 1993),
AutoSlog (Riloff, 1993), CRYSTAL (Soderland et
al., 1995), RAPIER (Califf, 1998), SRV (Freitag,
1998), meta-bootstrapping (Riloff & Jones, 1999),
and ExDisco (Yangarber et al, 2000). For this
work, we will use AutoSlog-TS (Riloff, 1996b) to
generate IE patterns for the plane crash domain.
AutoSlog-TS is a derivative of AutoSlog that auto-
matically generates extraction patterns by gathering
statistics from a corpus of relevant texts (within the
domain) and irrelevant texts (outside the domain).
Each extraction pattern represents a linguistic ex-
pression that can extract noun phrases from one of
three syntactic positions: subject, direct object, or
object of a prepositional phrase. For example, the
following patterns could extract vehicles involved
in a plane crash: ?<subject> crashed?, ?hijacked
<direct-object>?, and ?wreckage of <np>?.
We trained AutoSlog-TS using AP news stories
about plane crashes as the relevant text, and AP
news stories that do not mention plane crashes as
the irrelevant texts. AutoSlog-TS generates a list
of extraction patterns, ranked according to their as-
sociation with the domain. A human must review
this list to decide which patterns are useful for the
IE task and which ones are not. We manually re-
viewed the top patterns and used the accepted pat-
terns for the experiments described in this paper. To
apply the extraction patterns to new text, we used a
shallow parser called Sundance that also performs
information extraction.
3 Cross-Language Projection
3.1 Motivation and Previous Projection Work
Not all languages have received equal investment
in linguistic resources and tool development. For
a select few, resource-rich languages such as En-
glish, annotated corpora and text analysis tools are
readily available. However, for the large majority
of the world?s languages, resources such as tree-
banks, part-of-speech taggers, and parsers do not
exist. And even for many of the better-supported
languages, cutting edge analysis tools in areas such
as information extraction are not readily available.
One solution to this NLP-resource disparity is
to transfer linguistic resources, tools, and do-
main knowledge from resource-rich languages to
resource-impoverished ones. In recent years, there
has been a burst of projects based on this paradigm.
Yarowsky et al (2001) developed cross-language
projection models for part-of-speech tags, base
noun phrases, named-entity tags, and morpholog-
ical analysis (lemmatization) for four languages.
Resnik et al (2001) developed related models for
projecting dependency parsers from English to Chi-
nese. There has also been extensive work on the
cross-language transfer and development of ontolo-
gies and WordNets (e.g., (Atserias et al, 1997)).
3.2 Mechanics of Projection
The cross-language projection methodology em-
ployed in this paper is based on Yarowsky et al
(2001), with one important exception. Given the
absence of available naturally occurring bilingual




































 
 
 
 
LOCATION
VICTIM
tuant ses  20 occupants
was crushed Thursday evening in the south?east of Haiti  ,
A two?motor aircraft Beechcraft of the Air?Saint?Martin company
Un avion bi?moteur Beechcraft de la compagnie Air?Saint?Martin
VEHICLE
killing its 20 occupants 
s? est ?cras? jeudi soir dans le sud?est   d?   Haiti    ,
.
.
Figure 1: French text word aligned with its English
machine translation (extractions highlighted)
corpora in our target domain, we employ commer-
cial, off-the-shelf machine translation to generate
an artificial parallel corpus. While machine transla-
tion errors present substantial problems, MT offers
great opportunities because it frees cross-language
projection research from the relatively few large
existing bilingual corpora (such as the Canadian
Hansards). MT allows projection to be performed
on any corpus, such as the domain-specific plane-
crash news stories employed here. Section 5 gives
the details of the MT system and corpora that we
used.
Once the artificial parallel corpus has been cre-
ated, we apply an English IE system to the English
texts and transfer the IE annotations to the target
language as follows:
1. Sentence align the parallel corpus.1
2. Word-align the parallel corpus using the
Giza++ system (Och and Ney, 2000).
3. Transfer English IE annotations and noun-
phrase boundaries to French via the mecha-
nism described in Yarowsky et al (2001),
yielding annotated sentence pairs as illustrated
in Figure 1.
4. Train a stand-alone IE tagger on these pro-
jected annotations (described in Section 4).
4 Transformation-Based Learning
We used transformation-based learning (TBL)
(Brill, 1995) to learn information extraction rules
for French. TBL is well-suited for this task because
it uses rule templates as the basis for learning, which
can be easily modeled after English extraction pat-
terns. However, information extraction systems typ-
ically rely on a shallow parser to identify syntactic
elements (e.g., subjects and direct objects) and verb
1This is trivial because each sentence has a numbered an-
chor preserved by the MT system.
constructions (e.g., passive vs. active voice). Our
hope was that the rules learned by TBL would be ap-
plicable to new French texts without the need for a
French parser. One of our challenges was to design
rule templates that could approximate the recogni-
tion of syntactic structures well enough to duplicate
most of the functionality of a French shallow parser.
When our TBL training begins, the initial state is
that no words are annotated. We experimented with
two sets of ?truth? values: Sundance?s annotations
and human annotations. We defined 56 language-
independent rule templates, which can be broken
down into four sets designed to produce different
types of behavior. Lexical N-gram rule templates
change the annotation of a word if the word(s) im-
mediately surrounding it exactly match the rule. We
defined rule templates for 1, 2, and 3-grams. In
Table 1, Rules 1-3 are examples of learned Lexi-
cal N-gram rules. Lexical+POS N-gram rule tem-
plates can match exact words or part-of-speech tags.
Rules 4-5 are Lexical+POS N-gram rules. Rule 5
will match verb phrases such as ?went down in?,
?shot down in?, and ?came down in?.
One of the most important functions of a parser is
to identify the subject of a sentence, which may be
several words away from the main verb phrase. This
is one of the trickest behaviors to duplicate without
the benefit of syntactic parsing. We designed Sub-
ject Capture rule templates to identify words that
are likely to be a syntactic subject. As an example,
Rule 6 looks for an article at the beginning of a sen-
tence and the word ?crashed? a few words ahead2,
and infers that the article belongs to a vehicle noun
phrase. (The NP Chaining rules described next will
extend the annotation to include the rest of the noun
phrase.) Rule 7 attempts relative pronoun disam-
biguation when it finds the three tokens ?COMMA
which crashed? and infers that the word preceding
the comma is a vehicle.
Without the benefit of a parser, another challenge
is identifying noun phrase boundaries. We designed
NP Chaining rule templates to look at words that
have already been labelled and extend the bound-
aries of the annotation to cover a complete noun
phrase. As examples, Rules 8 and 9 extend loca-
tion and victim annotations to the right, and Rule 10
extends a vehicle annotation to the left.
2? is a start-of-sentence token. w
4?7
means that the item
occurs in the range of word
4
through word
7
.
Rule Condition Rule Effect
1. w
1
=crashed w
2
=in w
3
is LOC.
2. w
1
=wreckage w
2
=of w
3
is VEH.
3. w
1
=injuring w
2
is VIC.
4. w
1
=NOUN w
2
=crashed w
1
is VEH.
5. w
1
=VERB w
2
=down w
3
=in w
4
is LOC.
6. w
1
=? w
2
=ART w
4?7
=crashed w
2
is VEH.
7. w
2
=COMMA w
3
=which w
4
=crashed w
1
is VEH.
8. w
1
=in w
2
=LOCATION w
3
=NOUN w
3
is LOC.
9. w
1
=VERB w
2
=VICTIM w
3
=NOUN w
3
is VIC.
10. w
1
=ART w
2
=VEHICLE w
1
is VEH.
Table 1: Examples of Learned TBL Rules
(LOC.=location, VEH.=vehicle, VIC.=victim)
5 Resources
The corpora used in these experiments were ex-
tracted from English and French AP news stories.
We created the corpora automatically by searching
for articles that contain plane crash keywords. The
news streams for the two languages came from dif-
ferent years, so the specific plane crash events de-
scribed in the two corpora are disjoint. The En-
glish corpus contains roughly 420,000 words, and
the French corpus contains about 150,000 words.
For each language, we hired 3 fluent university
students to do annotation. We instructed the anno-
tators to read each story and mark relevant entities
with SGML-style tags. Possible labels were loca-
tion of a plane crash, vehicle involved in a crash,
and victim (any persons killed, injured, or surviv-
ing a crash). We asked the annotators to align their
annotations with noun phrase boundaries. The an-
notators marked up 1/3 of the English corpus and
about 1/2 of the French corpus.
We used a high-quality commercial machine
translation (MT) program (Systran Professional
Edition) to generate a translated parallel corpus for
each of our English and French corpora. These will
henceforth be referred to as MT-French (the Systran
translation of the English text) and MT-English (the
Systran translation of our French text).
6 Experiments and Evaluation
6.1 Scoring and Annotator Agreement
We explored two ways of measuring annotator
agreement and system performance. (1) The
exact-word-match measure considers annotations to
match if their start and end positions are exactly the
same. (2) The exact-NP-match measure is more for-
giving and considers annotations to match if they
both include the head noun of the same noun phrase.
The exact-word-match criterion is very conservative
because annotators may disagree about equally ac-
ceptable alternatives (e.g., ?Boeing 727? vs. ?new
Boeing 727?). Using the exact-NP-match measure,
?Boeing 727? and ?new Boeing 727? would con-
stitute a match. We used different tools to identify
noun phrases in English and French. For English,
we applied the base noun phrase chunker supplied
with the fnTBL toolkit (Ngai & Florian, 2001). In
French, we ran a part-of-speech tagger (Cucerzan
& Yarowsky, 2000) and applied regular-expression
heuristics to detect the heads of noun phrases.
We measured agreement rates among our human
annotators to assess the difficulty of the IE task. We
computed pairwise agreement scores among our 3
English annotators and among our 3 French anno-
tators. The exact-word-match scores ranged from
16-31% for French and 24-27% for English. These
relatively low numbers suggest that the exact-word-
match criterion is too strict. The exact-NP-match
agreement scores were much higher, ranging from
43-54% for French and 51-59% for English3.
These agreement numbers are still relatively low,
however, which partly reflects the fact that IE is a
subjective and difficult task. Inspection of the data
revealed some systematic differences of approach
among annotators. For example, one of the French
annotators marked 4.5 times as many locations as
another. On the English side, the largest disparity
was a factor of 1.4 in the tagging of victims.
6.2 Monolingual English & French Evaluation
As a key baseline for our cross-language projec-
tion studies, we first evaluated the AutoSlog-TS
and TBL training approaches on monolingual En-
glish and French data. Figure 2 shows (1) English
training by running AutoSlog-TS on unannotated
texts and then applying its patterns to the human-
annotated English test data, (2) English training and
testing by applying TBL to the human-annotated
English data with 5-fold cross-validation, (3) En-
glish training by applying TBL to annotations pro-
duced by Sundance (using AutoSlog-TS patterns)
and then testing the TBL rules on the human-
annotated English data, and (4) French training and
testing by applying TBL to human annotated French
data with 5-fold cross-validation.
Table 2 shows the performance in terms of Pre-
cision (P), Recall (R) and F-measure (F). Through-
3Agreement rates were computed on a subset of the data
annotated by multiple people; systems were scored against the
full corpus, of which each annotator provided the standard for
one third.
out our experiments, AutoSlog-TS training achieves
higher precision but lower recall than TBL training.
This may be due to the exhaustive coverage pro-
vided by the human annotations used by TBL, com-
pared to the more labor-efficient but less-complete
AutoSlog-TS training that used only unannotated
data.
English TEST
140K words
(English)
  SUNDANCE
English (plain)
English (plain)
(English)
  SUNDANCE
4/5Eng TEST Eng TEST1/5
French TEST1/5
French TEST4/5
  TBLES
S1
ES1TS1Train TBL
(1)
(3)
Test TBL
(4) (French)  TBL TF0
Train TBL
Test TBL
English TESTS0
140K words
280K words
280K words
Autoslo
g?TS
Autos
log?T
S
[+ 280K words irrel. text]
(2) (English)  TBL T0
112K words 28K words
Train TBL
Test TBL
[+ 280K words irrel. text]
[cross validation]
[cross validation]
16K words
64K words
Figure 2: Monolingual IE Evaluation pathways4
Monolingual Training Route P R F
English
(1) Train AutoSlog-TS on English-plain (ASE)
S0: Apply ASE to English Test .44 .42 .43
(2) Train TBL on 4/5 of English-Test (TBLE)
T0: Apply TBLE to 1/5 of English Test .35 .62 .45
(perform in 5-fold cross-validation)
(3) Train AutoSlog-TS on English-plain (ASE)
S1: Apply ASE to English-plain .31 .40 .35
TS1 Train TBL on Sundance annotations
ES1: Apply TBLES to English Test
French
(4) Train TBL on 4/5 of French-Test (TBLF)
TF0: Apply TBLF to 1/5 of French Test .47 .66 .54
(perform in 5-fold cross-validation)
Table 2: Monolingual IE Baseline Performance
6.3 TBL-based IE Projection and Induction
As noted in Section 5, both the English and French
corpora were divided into unannotated (?plain?)
and annotated (?antd? or ?Tst?) sections. Figure
3 illustrates these native-language data subsets in
white. Each native-language data subset alo has
a machine-translated mirror in French/English re-
spectively (shown in black), with an identical num-
ber of sentences to the original. By word-aligning
these 4 native/MT pairs, each becomes a potential
vehicle for cross-language information projection.
Consider the pathway TE1?P1 ? TF1 as a rep-
resentative example pathway for projection. Here
an English TBL classifier is trained on the 140K-
word human annotated data and the learned TBL
rules are applied to the unannotated English sub-
corpus. The annotations are then projected across
the Giza++ word alignments to their MT-French
mirror. Next, a French TBL classifier (TBL1) is
trained on the projected MT-French annotations and
the learned French TBL rules are subsequently ap-
plied to the native-French test data.
An alternative path (TE4 ? P4 ? French-Test)
is more direct, in that the English TBL classifier
is applied immediately to the word-aligned MT-
English translation of the French test data. The MT-
English annotations can then be directly projected
to the French test data, so no additional training
is necessary. Another short direct projection path
(PHA2 ? THA2 ? French-test) skips the need to
train an English TBL model by projecting the En-
glish human annotations directly onto MT-French
texts, which can then be used to train a French TBL
system which can be applied to the French test data.
HUMAN
Annotators
  TBL (English)
French TBL
Training and
Transfer to
Test Data
English
Annotation
P1
English (plain)
P    2HA
TBL1
T    2HA
TBL2h
English (antd)
TB
L 
Tr
ai
ni
ng
T  1
ET  1
F
Cross?
Language
Projection
P3
TBL3
French (plain)
T  3F
T   3E
P4
MT?English Tst
T   4E
(plain)
(plain)
MT?English
MT?French
MT?French
(annotated) French Test
Figure 3: TBL-based IE projection pathways
Table 3 shows the results of our TBL-based ex-
periments. The top performing pathway is the
TE4 ? P4 two-step projection pathway shown in
Figure 3. Note the F-measure of the best pathway
is .45, which is equal to the highest F-measure for
monolingual English and only 9% lower than the F-
measure for monolingual French.
4The irrelevant texts are needed to train AutoSlog-TS, but
not TBL.
Projection and Training Route P R F
TE1: Apply TBLE to English-plain
P1: Project to MT-French(English-Plain) .69 .24 .36
TF1: Train TBL & Apply to FrTest
? Use human Annos from Eng Antd
Pha2: Project to MT-French(English Antd) .56 .29 .39
Tha2: Train TBL & Apply to FrTest
TE3: Apply TBLE to MT-Eng(FrenchPlain)
P3: Project to French-Plain .49 .34 .40
TF3: Train TBL & Apply to FrTest
TE4: Apply TBLE to MT-Eng(FrenchTest)
P4: Direct Project to French-Test .49 .41 .45
Table 3: TBL-based IE projection performance
6.4 Sundance-based IE Projection and
Induction
Figure 4 shows the projection and induction model
using Sundance for English IE annotation, which is
almost isomorphic to that using TBL. One notable
difference is that Sundance was trained by apply-
ing AutoSlog-TS to the unannotated English text
rather than the human-annotated data. Figure 4 also
shows an additional set of experiments (SMT 3 and
SMT 4) in which AutoSlog-TS was trained on the
English MT translations of the unannotated French
data. The motivation was that native-English extrac-
tion patterns tend to achieve low recall when applied
to MT-English text (given frequent mistranslations
such as ?to crush? a plane rather than ?to crash? a
plane). By training AutoSlog-TS on the sentences
generated by an MT system (seen in the SMT 3 and
SMT 4 pathways), the F-measure increases.5
French TBL
Training and
Transfer to
Test Data
English
Annotation
P2P1
T1
T2
S1
S2
  SUNDANCE
(English)
English (plain)
TBL1
TBL2
English (antd)
Projection
Language
Cross?
P4
(MT?English)
SUNDANCE
MT?English Tst
P     3
S     4
P     4
MT
MT MT
French (plain)
S4
S     3MT
S3
P3
TBL3
T3 TBL3m
T     3MT
Au
tos
log
?T
S
Au
to
slo
g 
 ?T
S
(plain)
MT?French
MT?English
(plain)
MT?French
(annotated) French Test
Figure 4: Sundance-based projection pathways
5This is a ?fair? gain, in that the MT-trained AutoSlog-TS
patterns didn?t use translations of any of the French test data.
Projection and Training Route P R F
AutoSlog-TS trained on native English (AS E)
S2: Apply ASE to English-Antd
P2: Project to MT-French(English-Antd) .39 .24 .29
T2: Train TBLFP2 & Apply to FrTest
S(1+2): Apply ASE to English Antd+Plain
P (1+2): Project to MT-French(Eng-Ant+Pl) .43 .23 .30
T (1+2): Train TBLFP1+2 & Apply to FrTest
S3: Apply ASE to MT-Eng(FrenchPlain)
P3: Project to French-Plain .45 .04 .07
T3: Train TBLFP3 & Apply to FrTest
S4: Apply ASE to MT-Eng(FrenchTest)
P4: Direct Project to French-Test .48 .07 .13
AutoSlog-TS trained on MT English (AS MTE)
SMT 3: Apply ASMTE to MT-Eng(FrPlain)
PMT 3: Project to French-Plain .46 .25 .32
TMT 3: Train TBLFMT3 & Apply to FrTest
SMT 4: Apply ASMTE to MT-Eng(FrTest)
PMT 4: Direct Project to French-Test .55 .28 .37
Table 4: Sundance-based IE projection performance 6
Table 4 shows that the best Sundance pathway
achieved an F-measure of .37. Overall, Sundance
averaged 7% lower F-measures than TBL on com-
parable projection pathways. However, AutoSlog-
TS training required only 3-4 person hours to review
the learned extraction patterns while TBL training
required about 150 person-hours of manual IE an-
notations, so this may be a viable cost-reward trade-
off. However, the investment in manual English IE
annotations can be reused for projection to new for-
eign languages, so the larger time investment is a
fixed cost per-domain rather than per-language.
6.5 Analysis and Implications
? For both TBL and Sundance, the P1, P2 and
P3-family of projection paths all yield stand-alone
monolingual French IE taggers not specialized for
any particular test set. In contrast, the P4 series of
pathways (e.g. PMT 4 for Sundance), were trained
specifically on the MT output of the target test data.
Running an MT system on test data can be done au-
tomatically and requires no additional human lan-
guage knowledge, but it requires additional time
(which can be substantial for MT). Thus, the higher
performance of the P4 pathways has some cost.
? The significant performance gains shown by
Sundance when AutoSlog-TS is trained on MT-
English rather than native-English are not free be-
cause the MT data must be generated for each new
language and/or MT system to optimally tune to
6S(1+2) combines the training data in S1 (280K) and S2
(140K), yielding a 420K-word sample.
its peculiar language variants. No target-language
knowledge is needed in this process, however, and
reviewing AutoSlog-TS? patterns can be done suc-
cessfully by imaginative English-only speakers.
? In general, recall and F-measure drop as the
number of experimental steps increases. Averaged
over TBL and Sundance pathways, when compar-
ing 2 and 3-step projections, mean recall decreases
from 26.8 to 21.8 (5 points), and mean F-measure
drops from 32.6 to 28.8 (3.8 points). Viable extrac-
tion patterns may simply be lost or corrupted via too
many projection and retraining phases.
? One advantage of the projection path families
of P1 and P2 is that no domain-specific documents
in the foreign language are required (as they are in
the P3 family). A collection of domain-specific En-
glish texts can be used to project and induce new IE
systems even when no domain-specific documents
exist in the foreign language.
6.6 Multipath Projection
Finally, we explored the use of classifier combina-
tion to produce a premium system. We considered a
simple voting scheme over sets of individual IE sys-
tems. Every annotation of a head noun was consid-
ered a vote. We tried 4 voting combinations: (1) the
systems that used Sundance with English extraction
patterns, (2) the systems that used Sundance with
MT-English extraction patterns, (3) the systems that
used TBL trained on English human annotations,
(4) all systems. For each combination of n sys-
tems, n answer sets were produced using the voting
thresholds Tv = 1..n. For example, for Tv = 2 ev-
ery annotation receiving >= 2 votes (picked by at
least 2 individual systems) was output in the answer
set. This allowed us to explore a precision/recall
tradeoff based on varying levels of consensus.
Figure 5 shows the precision/recall curves. Vot-
ing yields some improvement in F-measure and pro-
vides a way to tune the system for higher preci-
sion or higher recall by choosing the Tv threshold.
When using all English knowledge sources, the F-
measure at Tv=1 (.48) is nearly 3% higher than the
strongest individual system. Figure 5 also shows
the performance of a 5th system (5), which is a
TBL system trained directly from the French anno-
tations under 5-fold cross-validation. It is remark-
able that the most effective voting-based projection
system from English to French comes within 6% F-
measure of the monolingually trained system, given
that this cross-validated French monolingual system
was trained directly on data in the same language
and source as the test data. This suggests that cross-
language projection of IE analysis capabilities can
successfully approach the performance of dedicated
systems in the target language.
Precision
Recall
(5)
(2)
(1) (3) (4)
(5) TBL Trained from French Annotations 
(4) English TBL + Sundance pathways
(3) English TBL pathways
(2) Sundance?MT pathways
(1) Sundance pathways
[under 5?fold cross?validation]
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Figure 5: Precision/Recall curves for voting systems. Each
point represents performance for a particular voting threshold.
In all cases, precision increases and recall decreases as the
threshold is raised.
French Test-Set Performance P R F
Multipath projection from all English resources .43 .54 .48
Table 5: Best multipath English-French Projection Per-
formance (from English TBL and Sundance pathways)
7 Conclusions
We have used IE systems for English to automati-
cally derive IE systems for a second language. Even
with the quality of MT available today, our results
demonstrate that we can exploit translation tools to
transfer information extraction expertise from one
language to another. Given an IE system for a
source language, an MT system that can translate
between the source and target languages, and a word
alignment algorithm, our approach allows a user to
create a functionally comparable IE system for the
target language with very little human effort. Our
experiments demonstrated that the new IE system
can achieve roughly the same level of performance
as the source-language IE system. French and En-
glish are relatively close languages, however, so
how well these techniques will work for more dis-
tant language pairs is still an open question.
Additional performance benefits could be
achieved in two ways: (1) put more effort into
obtaining better resources for English, or (2)
implement (minor) specializations per language.
While it is expensive to advance the state of the art
in English IE or to buy annotated data for a new
domain, these additions will improve performance
not only in English but for other languages as
well. On the other hand, with minimal effort
(hours) it is possible to custom-train a system
such as Autoslog/Sundance to work relatively
well on noisy MT-English, providing a substantial
performance boost for the IE system learned for the
target language, and further gains are achieved via
voting-based classifier combination.
References
J. Atserias, S. Climent, X. Farreres, G. Rigau and H. Rodriguez.
1997. Combining multiple methods for the automatic con-
struction of multilingual WordNets. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
M. E. Califf. 1998. Relational learning techniques for natural
language information extraction. Ph.D. thesis, Tech. Rept.
AI98-276, Artificial Intelligence Laboratory, The University
of Texas at Austin.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL-2000, pages 270-277.
D. Freitag. 1998. Toward general-purpose learning for in-
formation extraction. In Proceedings of COLING-ACL?98,
pages 404-408.
J. Kim and D. Moldovan. 1993. Acquisition of semantic pat-
terns for information extraction from corpora. In Proceed-
ings of the Ninth IEEE Conference on Artificial Intelligence
for Applications, pages 171?176.
G. Ngai and R. Florian. 2001. Transformation-based learning
in the fast lane. In Proceedings of NAACL, pages 40-47.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages 440?447.
E. Riloff. 1993. Automatically Constructing a dictionary for
information extraction tasks. In Proceedings of the Eleventh
National Conference on Artificial Intelligence, pages 811?
816.
E. Riloff. 1996b. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence, pages 1044?
1049. AAAI Press/MIT Press.
E. Riloff and R. Jones. 1999. Learning dictionaries for infor-
mation extraction by multi-level bootstrapping. In Proceed-
ings of the Sixteenth National Conference on Artificial Intel-
ligence, pages 474?479.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings of the Fourteenth International Joint Conference on Ar-
tificial Intelligence, pages 1314?1319.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Automatic acquisiton of domain knowledge for infor-
mation extraction. In Proceedings of COLING-2000, pages
940-946.
Yarowsky, D., G. Ngai and R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection across
aligned corpora. In Proceedings of HLT-01, pages 161?168.
Exploiting Aggregate Properties of Bilingual Dictionaries For Distinguishing
Senses of English Words and Inducing English Sense Clusters
Charles SCHAFER and David YAROWSKY
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD, 21218, USA
{cschafer,yarowsky}@cs.jhu.edu
Abstract
We propose a novel method for inducing monolingual
semantic hierarchies and sense clusters from numerous
foreign-language-to-English bilingual dictionaries. The
method exploits patterns of non-transitivity in transla-
tions across multiple languages. No complex or hierar-
chical structure is assumed or used in the input dictio-
naries: each is initially parsed into the ?lowest common
denominator? form, which is to say, a list of pairs of the
form (foreign word, English word). We then propose a
monolingual synonymy measure derived from this ag-
gregate resource, which is used to derive multilingually-
motivated sense hierarchies for monolingual English
words, with potential applications in word sense classifi-
cation, lexicography and statistical machine translation.
1 Introduction
In this work we consider a learning resource compris-
ing over 80 foreign-language-to-English bilingual dictio-
naries, collected by downloading electronic dictionaries
from the Internet and also scanning and running optical
character recognition (OCR) software on paper dictio-
naries. Such a diverse parallel lexical data set has not,
to our knowledge, previously been assembled and exam-
ined in its aggregate form as a lexical semantics training
resource. We show that this aggregate data set admits
of some surprising applications, including discovery of
synonymy relationships between words and automatic
induction of high-quality hierarchical word sense clus-
terings for English.
We perform and describe several experiments deriving
synonyms and sense groupings from the aggregate bilin-
gual dictionary, and subsequently suggest some possible
applications for the results.
Finally, we propose that sense taxonomies of the kind
introduced here, being of different provenance from
those produced explicitly by lexicographers or using un-
supervised corpus-driven methods, have significant value
because they add diversity to the set of available re-
sources.
2 Resources
First we collected, from Internet sources and via scan-
ning and running OCR on print dictionaries, 82 dictio-
naries between English and a total of 44 distinct foreign
languages from a variety of language families.
Over 213K distinct English word types were present
in a total of 5.5M bilingual dictionary entries, for an av-
fair
blond justS
SS are synonymous with
fair
differing senses of
blond and just
Figure 1: Detecting asynonymy via unbalanced synonymy relation-
ships among 3 words. The derived synonymy relation S holds between
fair and blond, and between fair and just. S does not hold between
blond and fair. We can infer that fair has at least 2 senses and, further,
we can represent them by blond and just.
English French Spanish German
fair blond, blondo, blond,
juste licito, recto gerecht
blond blond blondo blond
just juste licito; recto gerecht
Figure 2: This excerpt from the data set illustrates the kind of support
the aggregate bilingual dictionary provides for partitioning the mean-
ings of fair into distinct senses: blond and just.
erage of 26 and a median of 3 foreign entries per English
word. Roughly 15K English words had at least 100 for-
eign entries; over 64K had at least 10 entries.
No complex or hierarchical structure was assumed or
used in our input dictionaries. Each was initially parsed
into the ?lowest common denominator? form. This con-
sisted of a list of pairs of the form (foreign word, English
word). Because bilingual dictionary structure varies
widely, and even the availability and compatibility of
part-of-speech tags for entries is uncertain, we made the
decision to compile the aggregate resource only with data
that could be extracted from every individual dictionary
into a universally compatible format. The unique pairs
extracted from each dictionary were then converted to 4-
tuples of the form:
<foreign language, dictionary name, foreign word, English word>
before being inserted into the final, combined dictionary
data set.
3 A Synonymy Relation
We began by using the above-described data set to obtain
a synonymy relation between English words.
In general, in a paper bilingual dictionary, each for-
eign word can be associated with a list of English words
which are possible translations; in our reduced format
each entry lists a single foreign word and single possible
English translation, though taking a union of all English
translations for a particular foreign word recreates this
list.
We use the notion of coentry to build the synonymy
relation between English words. The per-entry coentry
count Cper?entry(e1,e2) for two English words e1 and e2
is simply the number of times e1 and e2 both appear as
the translation of the same foreign word (over all foreign
words, dictionaries and languages). The per-dictionary
coentry count Cper?dict(e1,e2), ignores the number
of individual coentries within a particular dictionary
and merely counts as 1 any number of coentries inside
a particular dictionary. Finally, per-language coentry
count Cper?lang(e1,e2) counts as 1 any number of
coentries for e1 and e2 for a particular language. Thus,
for the following snippet from the database:
Eng. Wd. Foreign Wd. Foreign Language Dict. ID
hit schlagen GERMAN ger.dict1
pound schlagen GERMAN ger.dict1
hit schlag GERMAN ger.dict1
pound schlag GERMAN ger.dict1
hit schlag GERMAN ger.dict2
pound schlag GERMAN ger.dict2
hit battere ITAL ital.dict1
pound battere ITAL ital.dict1
Cper?entry(hit,pound) = 4, while
Cper?dict(hit,pound) = 3, since the two individ-
ual coentries in ger.dict1 are only counted once.
Cper?lang(hit,pound) = 2; hit and pound are coentries in
the Italian and German languages. We found the more
conservative per-dictionary and per-language counts to
be a useful device, given that some dictionary creators
appear sometimes to copy and paste identical synonym
sets in a fairly indiscriminate fashion, spuriously
inflating the Cper?entry(e1,e2) counts.
Our algorithm for identifying synonyms was sim-
ple: we sorted all pairs of English words by decreas-
ing Cper?dict(e1,e2) and, after inspection of the resulting
list, cut it off at a per-dictionary and per-language count
threshold1 yielding qualitatively strong results. For all
word pairs e1,e2 above threshold, we say the symmetric
synonymy relation S(e1,e2) holds. The following tables
provide a clarifying example showing how synonymy
can be inferred from multiple bilingual dictionaries in a
way which is impossible with a single such dictionary
(because of idiosyncratic foreign language polysemy).
Lang. Dict. ID Foreign Wd English Translations
GERMAN ger.dict1 absetzen deposit drop deduct sell
GERMAN ger.dict1 ablagerung deposit sediment settlement
The table above displays entries from one
German-English dictionary. How can we tell
that ?sediment? is a better synonym for ?de-
posit? than ?sell?? We can build and examine the
1The threshold was 10 and 5 respectively for per-dictionary and per-
language coentry counts.
coentry counts Cper?lang(deposit,sediment) and
Cper?lang(deposit,sell) using dictionaries from many
languages, as illustrated below:
FRENCH fre.dict1 de?po?t arsenal deposit depository
depot entrusting filing
sludge store trust submission
repository scale sediment
TURKISH tk.dict1 tortu sediment deposit faeces
remainder dregs crust
CZECH cz.dict1 sedlina clot deposit sediment warp
Polysemy which is specific to German ? ?deposit?
and ?sell? senses coexisting in a particular word
form ?absetzen? ? will result in total coentry counts
Cper?lang(deposit,sell), over all languages and dictio-
naries, which are low. In fact, ?deposit? and ?sell?
are coentries under only 2 out of 44 languages in our
database (German and Swedish, which are closely re-
lated). On the other hand, near-synonymous English
translations of a particular sense across a variety of lan-
guages will result in high coentry counts, as is the case
with Cper?lang(deposit,sediment). As illustrated in the
tables, German, French, Czech and Turkish all support
the synonymy hypothesis for this pair of English words.
?deposit? Coentries Per Entry Per Dict. Per Lang.
sell 4 4 2
sediment 68 40 18
The above table, listing the various coentry counts
for ?deposit?, demonstrates the empirical motivation in
the aggregate dictionary for the synonymy relationship
between deposit and sediment, while the aggregate ev-
idence of synonymy between deposit and sell is weak,
limited to 2 languages, and is most likely the result of a
word polysemy restricted to a few Germanic languages.
4 Different Senses: Asymmetries of
Synonymy Relations
After constructing the empirically derived synonymy re-
lation S described in the previous section, we observed
that one can draw conclusions from the topology of the
graph of S relationships (edges) among words (vertices).
Specifically, consider the case of three words e1,e2, e3
for which S(e1,e2) and S(e1,e3) hold, but S(e2,e3) does
not. Figure 1 illustrates this situation with an example
from data (e1 = ?fair?), and more examples are listed
in Table 1. As Figure 1 suggests and inspection of the
random extracts presented in Table 1 will confirm, this
topology can be interpreted as indicating that e2 and e3
exemplify differing senses of e1.
We decided to investigate and apply it with more gen-
erality. This will be discussed in the next section.
5 Inducing Sense Taxonomies: Clustering
with Synonym Similarity
With the goal of using the aggregate bilingual dictionary
to induce interesting and useful sense distinctions of En-
glish words, we investigated the following strategy.
syn1(W) W syn2(W)
quiet still yet
desire want lack
delicate tender offer
conceal hide skin
nice kind sort
assault charge load
filter strain stretch
flow run manage
cloth fabric structure
blond fair just
foundation base ignoble
deny decline fall
hurl cast mould
bright clear open
harm wrong incorrect
crackle crack fissure
impeach charge load
enthusiastic keen sharp
coarse rough difficult
fling cast form
firm fast speedy
fashion mold mildew
incline lean meagre
arouse raise increase
digit figure shape
dye paint picture
spot stain tincture
shape cast toss
claim call shout
earth ground groundwork
associate fellow guy
arrest stop plug
Table 1: A representative sampling of high-confidence sense
distinctions derived via unbalanced synonymy relationships among
three words, W and two of its synonyms syn1(W) & syn2(W),
such that Cper?dict(W,syn1(W)) and Cper?dict(W,syn2(W)) are
high, whereas Cper?dict(syn1(W),syn2(W)) is low (0). Ex-
tracted from a list sorted by descending Cper?dict(W,syn1(W))
? Cper?dict(W,syn2(W)) / Cper?dict(syn1(W),syn2(W)) (counts
were smoothed to prevent division by zero).
For each target word Wt in English having a suffi-
ciently high dictionary occurrence count to allow inter-
esting results2, a list of likely synonym words Ws was
induced by the method described in Section 33. Addi-
tionally, we generated a list of all words Wc having non-
zero Cper?dict(Wt,Wc).
The synonym words Ws ? the sense exemplars for
target words Wt ? were clustered based on vectors of
coentry counts Cper?dict(Ws,Wc). This restriction on
vector dimension to only words that have nonzero co-
entries with the target word helps to exclude distractions
such as coentries of Ws corresponding to a sense which
doesn?t overlap with Wt. The example given in the fol-
lowing table shows an excerpt of the vectors for syn-
onyms of strike. The hit synonym overlaps strike in the
beat/bang/knock sense. Restricting the vector dimension
as described will help prevent noise from hit?s common
2For our experiments, English words occurring in at least 15 distinct
source dictionaries were considered.
3Again, the threshold for synonyms was 10 and 5 respectively for
per-dictionary and per-language coentry counts.
chart-topper/recording/hit single sense. The following
table also illustrates the clarity with which major sense
distinctions are reflected in the aggregate dictionary. The
induced clustering for strike (tree as well as flat cluster
boundaries) is presented in Figure 4.
attack bang hit knock walkout find
attack - 4 18 7 0 0
bang - 38 43 2 0 0
hit - 44 2 29
knock - 2 0
walkout - 0
find -
We used the CLUTO clustering toolkit (Karypis,
2002) to induce a hierarchical agglomerative clustering
on the vectors for Ws. Example results for vital and
strike are in Figures 3 and 4 respectively4. Figure 4 also
presents flat clusters automatically derived from the tree,
as well as a listing of some foreign words associated with
particular clusters.
Figure 3: Induced sense hierarchy for the word ?vital?
6 Related Work
There is a distinguished history of research extracting lexical
semantic relationships from bilingual dictionaries (Copestake
et al, 1995; Chen and Chang, 1998). There is also a long-
standing goal of mapping translations and senses in multiple
languages in a linked ontology structure (Resnik and Yarowsky,
1997; Risk, 1989; Vossen, 1998). The recent work of Ploux and
Ji (2003) has some similarities to the techniques presented here
in that it considers topological properties of the graph of syn-
onymy relationships between words. The current paper can be
distinguished on a number of dimensions, including our much
greater range of participating languages, and the fundamental
algorithmic linkage between multilingual translation distribu-
tions and monolingual synonymy clusters.
4In both ?vital? and ?strike? examples, the rendered hierarchical
clusterings were pruned (automatically) in order to fit in this paper.
Figure 4: Induced sense hierarchy for the word ?strike? and some translations of individual ?strike? synonyms. Flat clusters
automatically derived from the tree are denoted by the horizontal lines.
7 Analysis and Conclusions
This is the first presentation of a novel method for the induc-
tion of word sense inventories, which makes use of aggregate
information from a large collection of bilingual dictionaries.
One possible application of the induced sense inventories
presented here is as an aid to manual construction of mono-
lingual dictionaries or thesauri, motivated by translation dis-
tinctions across numerous world languages. While the desired
granularity of sense distinction will vary according to the re-
quirements of taste and differing applications, treating our out-
put as a proposal to be assessed and manually modified would
be a valuable labor-saving tool for lexicographers.
Another application of this work is a supplemental resource
for statistical machine translation (SMT). It is possible, as
shown graphically in Figure 4, to recover the foreign words
associated with a cluster (not just a single word). Given that
the clusters provide a more complete coverage of English word
types for a given sense than the English side of a particular
bilingual dictionary, clusters could be used to unify bitext co-
occurrence counts of foreign words with English senses in a
way that typical bilingual dictionaries cannot. Unifying counts
in this way would be a useful way of reducing data sparsity in
SMT training.
Finally, evaluation of induced sense taxonomies is always
problematic. First of all, there is no agreed ?correct? way to
classify the possible senses of a particular word. To some de-
gree this is because human experts disagree on particular judg-
ments of classification, though a larger issue, as pointed out
in Resnik and Yarowsky 1997, is that what constitutes an ap-
propriate set of sense distinctions for a word is, emphatically, a
function of the task at hand. The sense-distinction requirements
of English-to-French machine translation differ from those of
English-to-Arabic machine translation (due to differing degrees
of parallel polysemy across the language pairs), and both differ
from those of English dictionary construction.
We believe that the translingually-motivated word-sense tax-
onomies developed here will prove useful for the a variety
of tasks including those mentioned above. The fact that they
are derived from a novel resource, not constructed explicitly
by humans or derived in fully unsupervised fashion from text
corpora, makes them worthy of study and incorporation in fu-
ture lexicographic, machine translation, and word sense disam-
biguation efforts.
References
J. Chen and J. Chang. 1998. Topical Clustering of MRD
Senses Based on Information Retrieval Techniques.
Computational Linguistic, 29(2):61-95.
A. Copestake, E. Briscoe, P. Vossen, A. Ageno, I.
Castellan, F. Ribas, G. Rigau, H. Rodriguez and A.
Samiotou. 1995. Acquisition of Lexical Translation
Relations from MRDs. Machine Translation: Special
Issue on the Lexicon, 9(3):33-69.
G. Karypis. 2002. CLUTO: A Clustering Toolkit. Tech
Report 02-017, Dept. of Computer Science, University
of Minnesota. Available at http://www.cs.umn.edu?cluto
S. Ploux and H. Ji. 2003. A Model for Matching
Semantic Maps Between Languages (French/English,
English/French). Computational Linguistics, 29(2):155-
178.
P. Resnik and D. Yarowsky. 1997. A Perspective
on Word Sense Disambiguation Methods and Their
Evaluation. In Proceedings of SIGLEX-1997, pp. 79-86.
O. Risk. 1989. Sense Disambiguation of Word Trans-
lations in Bilingual Dictionaries: Trying to Solve The
Mapping Problem Automatically. RC 14666, IBM T.J.
Watson Research Center. Yorktown Heights.
P. Vossen (ed.). 1998. EUROWORDNET: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers. Dordrecht, The Netherlands.
 	

Statistical Machine Translation Using Coercive Two-Level Syntactic
Transduction
Charles Schafer and David Yarowsky
Center for Language and Speech Processing / Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
{cschafer,yarowsky}@cs.jhu.edu
Abstract
We define, implement and evaluate a novel model for
statistical machine translation, which is based on shal-
low syntactic analysis (part-of-speech tagging and phrase
chunking) in both the source and target languages. It
is able to model long-distance constituent motion and
other syntactic phenomena without requiring a full parse
in either language. We also examine aspects of lexical
transfer, suggesting and exploring a concept of transla-
tion coercion across parts of speech, as well as a transfer
model based on lemma-to-lemma translation probabili-
ties, which holds promise for improving machine trans-
lation of low-density languages. Experiments are per-
formed in both Arabic-to-English and French-to-English
translation demonstrating the efficacy of the proposed
techniques. Performance is automatically evaluated via
the Bleu score metric.
1 Introduction
In this work we define, implement and evaluate a novel
model for statistical machine translation (SMT).
Our goal was to produce a SMT system for translat-
ing foreign languages into English which utilizes some
syntactic information in both the foreign language and
English without, however, requiring a full parse in either
language. Some advantages of not relying on full parses
include that (1) there is a lack of availability of parsers
for many languages of interest; (2) parsing time com-
plexity represents a potential bottleneck for both model
training and testing.
Intuitively, the explicit modeling of syntactic phenom-
ena should be of benefit in the machine translation task;
the ability to handle long-distance motion in an intelli-
gently constrained way is a salient example of such a
benefit. Allowing unconstrained translation reorderings
at the word level generates a very large set of permu-
tations that pose a difficult search problem at decoding
time. We propose a model that makes use of shallow
parses (text chunking) to support long-distance motion
of phrases without requiring deeper analysis of syntax.
The resources required to train this system on a new lan-
guage are minimal, and we gain the ability to model
long-distance movement and some interesting proper-
ties of lexical translation across parts of speech. One of
the source languages we examine in this paper, Arabic,
has a canonical sentence-level order of Verb-Subject-
Object, which means that translation into English (with
a standard ordering of Subject-Verb-Object) commonly
requires motion of entire phrasal constituents, which is
not true of French-to-English translation, to cite one lan-
guage pair whose characteristics have wielded great in-
fluence in the history of work on statistical machine
translation. A key motivation for and objective of this
work was to build a translation model and feature space
to handle the above-described phenomenon effectively.
2 Prior Work
Statistical machine translation, as pioneered by IBM
(e.g. Brown et al, 1993), is grounded in the noisy chan-
nel model. And similar to the related channel problems
of speech and handwriting recognition, the original SMT
language pair French-English exhibits a relatively close
linear correlation in source and target sequence. Much
common local motion that is observed for French, such
as adjective-noun swapping, is adequately modeled by
the relative-position-based distortion models of the clas-
sic IBM approach. Unfortunately, these distortion mod-
els are less effective for languages such as Japanese or
Arabic, which have substantially different top-level sen-
tential word orders from English, and hence longer dis-
tance constituent motion.
Wu (1997) and Jones and Havrilla (1998) have sought
to more closely tie the allowed motion of constituents
between languages to those syntactic transductions sup-
ported by the independent rotation of parse tree con-
stituents. Yamada and Knight (2000, 2001) and Alshawi
et al (2000) have effectively extended such syntactic
transduction models to fully functional SMT systems,
based on channel model tree transducers and finite state
head transducers respectively. While these models are
well suited for the effective handling of highly divergent
sentential word orders, the above frameworks have a lim-
itation shared with probabilistic context free grammars
that the preferred ordering of subtrees is insufficiently
constrained by their embedding context, which is espe-
cially problematic for very deep syntactic parses.
In contrast, Och et al (1999) have avoided the con-
straints of tree-based syntactic models and allow the rel-
atively flat motion of empirically derived phrasal chunks,
which need not adhere to traditional constituent bound-
aries.
Our current paper takes a middle path, by grounding
motion in syntactic transduction, but in a much flatter 2-
level model of syntactic analysis, based on flat embed-
ded noun-phrases in a flat sentential constituent-based
chunk sequence that can be driven by syntactic brack-
eters and POS tag models rather than a full parser, facili-
tating its transfer to lower density languages. The flatter
2-level structures also better support transductions condi-
tioned to full sentential context than do deeply embedded
tree models, while retaining the empirically observed ad-
vantages of translation ordering independence of noun-
phrases.
Another improvement over Och et al and Yamada and
Knight is the use of the finite state machine (FSM) mod-
elling framework (e.g. Bangalore and Riccardi, 2000),
which offers the considerable advantage of a flexible
framework for decoding, as well as a representation
which is suitable for the fixed two-level phrasal mod-
elling employed here.
Finally, the original cross-part-of-speech lexical coer-
cion models presented in Section 4.3.3 have related work
in the primarily-syntactic coercion models utilized by
Dorr and Habash (2002) and Habash and Door (2003),
although their induction and modelling are quite differ-
ent from the approach here.
3 Resources
As in other SMT approaches, the primary training re-
source is a sentence-aligned parallel bilingual corpus.
We further require that each side of the corpus be part-
of-speech (POS) tagged and phrase chunked; our lab
has previously developed techniques for rapid training
of such tools (Cucerzan and Yarowsky, 2002). Our trans-
lation experiments were carried out on two languages:
Arabic and French. The Arabic training corpus was a
subset of the United Nations (UN) parallel corpus which
is being made available by the Linguistic Data Consor-
tium. For French-English training, we used a portion of
the Canadian Hansards. Both corpora utilized sentence-
level alignments publicly distributed by the Linguistic
Data Consortium.
POS tagging and phrase chunking in English were
done using the trained systems provided with the fnTBL
Toolkit (Ngai and Florian, 2001); both were trained
from the annotated Penn Treebank corpus (Marcus et al,
1993). French POS tagging was done using the trained
French lexical tagger also provided with the fnTBL soft-
ware. For Arabic, we used a colleague?s POS tagger and
tokenizer (clitic separation was also performed prior to
POS tagging), which was rapidly developed in our lab-
oratory. Simple regular-expression-based phrase chun-
kers were developed by the authors for both Arabic and
French, requiring less than a person-day each using ex-
isting multilingual learning tools.
A further input to our system is a set of word alignment
links on the parallel corpus. These are used to compute
word translation probabilities and phrasal alignments.
The word alignments can in principle come from any
source: a dictionary, a specialized alignment program,
or another SMT system. We used alignments generated
by Giza++ (Och and Ney, 2000) by running it in both di-
rections (e.g., Arabic ? English and English ? Arabic)
on our parallel corpora. The union of these bidirectional
alignments was used to compute cross-language phrase
correspondences by simple majority voting, and for pur-
poses of estimating word translation probabilities, each
link in this union was treated as an independent instance
of word translation.
4 Translation Model
Now we turn to a detailed description of the proposed
translation model. The exposition will give a formal
specification and also will follow a running example
throughout, using one of the actual Arabic test set sen-
tences. This example, its gloss, system translation and
reference human translation are shown in Table 1.
The translation model (TM) we describe is trained di-
rectly from counts in the data, and is a direct model, not
a noisy channel model. It consists of three nested com-
ponents: (1) a sentence-level model of phrase correspon-
dence and reordering, (2) a model of intra-phrase trans-
lation, and (3) models of lexical transfer, or word transla-
tion. We make a key assumption in our construction that
translation at each of these three levels is independent of
the others.
4.1 Sentence Translation
As mentioned, both the foreign language and English
corpora are input with ?hard? phrase bracketings and la-
beled with ?hard? phrase types (e.g., NP, VP1, PPNP2,
etc.) as given by the output of the phrase chunker. These
are denoted in the top-level model presentation in Table
2(1). Given word alignment links, as described in Sec-
tion 2, we compute phrasal alignments on training data.
We contrain these to have cardinality
(foreign)N ? 1(English). Next, we collect counts over
aligned phrase sequences and use the relative frequen-
cies to estimate the probability distribution in Table 2(2).
Particularly for smaller training corpora, unseen foreign-
language phrase sequences are a problem, so we imple-
mented a simple backoff method which assigns proba-
bility to translations of unseen foreign-language phrase
sequences. Table 2(3) encapsulates the remainder of the
translation model, which is described below.
As an example, Table 3 shows the most probable
aligned English phrase sequence generations given an
Arabic simple sentence having the canonical VSO or-
dering. Also, note that all probabilities in the following
1VP in our parlance is perhaps more properly called a verb chunk:
it consists of a verb, its auxiliaries, and contiguous adverbs.
2PPNP consists of a NP with its prepositional head attached.
Arabic Example Sentence From Test Set
(ARABIC) twSy Al- ljnp Al- sAdsp Al- jmEyp Al- EAmp b- AEtmAd m$rwE Al- mqrr Al- tAly :
(PHR.-BRACKETED AR.) [twSy] [Al- ljnp Al- sAdsp] [Al- jmEyp Al- EAmp] [b- AEtmAd m$rwE Al- mqrr Al- tAly] [:]
(AN ENG. GLOSS) [recommends] [the committee the sixth] [the assembly the general] [to adoption draft the decision the following] [:]
(ENG. MT OUTPUT) [the sixth committee] [recommends] [the general assembly] [in the adoption of the following draft resolution] [:]
(REFERENCE TRANS.) the sixth committee recommends to the general assembly the adoption of the following draft decision :
Table 1: An Arabic translation from the test set. We revisit portions of this example throughout the text. All Arabic
strings in this paper are rendered in the reversible Buckwalter transliteration. In addition, all words or symbols referring
to Arabic and French in this paper are italicized.
figures and tables are from the actual Arabic and French
trained systems.
Arabic Phrase Aligned English Prob.
Sequence Phrase Sequence
VP1 NP2 NP3 NP2 VP1 NP3 0.23
VP1 NP2 NP3 VP1 NP2 PP3 0.10
VP1 NP2 NP3 NP3 VP1,2 0.06
Table 3: Top learned sentence-level reorderings for Ara-
bic, for canonical Arabic simple sentence structure VP
(verb) NP (subject) NP (object). Subscripts in English
phrase sequence are alignments to positions in the corre-
sponding Arabic phrase sequence.
4.2 Phrase Translation
Given an Arabic test sentence, a distribution of aligned
English phrase sequences is proposed by the sentence-
level model described in the previous section and in Ta-
ble 2. Each proposed English phrase in each of the phrase
sequence possibilities, therefore, comes to the middle
level of the translation model with access to the identity
of the French phrases aligned to it. Phrase translation is
implemented as shown in Table 4. The phrase transla-
tion model is structured with several levels of backoff: if
no observations exist from training data for a particular
level, the model backs off to the next-more-general level.
In all cases, generation of an English phrase is condi-
tioned on the foreign phrase as well as the type
(NP, VP, etc.) of the English phrase.
Table 4 (1) describes the initial phrase translation
model. It comes into play if the precise sequence of
foreign words has been observed aligning to an En-
glish phrase of the appropriate type. In the example,
we are trying to generate an NP given the Arabic word
string ?Al- ljnp Al- sAdsp? (literally: ?the committee the
sixth?). If this has been observed in data, then that rela-
tive frequency distribution serves as the translation prob-
ability distribution. Table 11 contains examples of some
of these literal phrase translations from the French data.
The next stage of backoff from the above, literal level
is a model that generates aligned English POS tag se-
quences given foreign POS tag sequences: details and
an example can be found in Table 4(2). The sequence
alignments determine the position in English phrase and
the part-of-speech into which we translate the foreign
word. Again, translation is also conditioned on the En-
glish phrase type. Table 5 and Table 6 show the most
probable aligned English sequence generations for two
of the phrases in the example sentence.
If there were no counts for (foreign-POS-sequence,
english-phrase-type) then we back off to counts
collected over (foreign-coarse-POS-sequence, english-
phrase-type), where a coarse POS is, for example, N in-
stead of NOUN-SG. This is shown in Table 4(3).
In case further backoff is needed, as shown in Table
4(4), we begin stripping POS-tags off the ?less signifi-
cant? (non-head) end of the foreign POS-sequence until
we are left with a phrase sequence that has been seen in
training, and from this a corresponding English phrase
distribution is observable. We define the ?less signifi-
cant? end of a phrase to be the end if it is head-initial,
or the beginning if it is head-final, and at this point ig-
nore issues such as nested structure in French and Arabic
NP?s.
Aligned English POS-tag Sequence Translation Probabilities
(conditioned on Arabic POS-tag sequence from NP in example)
P ( DT? JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.22
P ( JJ4 NN1 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.20
P ( DT? NN1 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.13
P ( DT? VBN4 NNS2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.13
P ( DT1 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.04
P ( DT3 JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.03
P ( DT1 VBN4 NNS2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.03
P ( DT? NN4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( JJ4 NNS2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( DT1 JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( NN4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
Table 5: From the running Arabic example, top English
NP generations given an Arabic phrase DET NOUN-SG
DET ADJ. Note: ? denotes a null alignment (generation
from null). Generation from a null alignment is allowed
for specified parts of speech, such as determiners and
prepositions.
4.3 Lexical Transfer
4.3.1 The Basic Model
In the basic model of word generation, phrases may be
translated directly as single atomic entities (as in Table
4(1)), or via phrasal decomposition to individual words
translated independently, conditioned only on the source
word and target POS. Word translation in the latter case
Top-level Definition of Translation Model
Example Instantiation of Model Variables Model Description
P ( the sixth committee recommends the general assembly .. | P ( english words | foreign words ) =
twSy Al- ljnp Al- sAdsp Al- jmEyp Al- EAmp .. ) =
P ( [twSy]V P1 [Al- ljnp Al- sAdsp]NP1 [Al- jmEyp Al- EAmp]NP2 .. | (1) P ( foreign bracketing , foreign phrase sequence | foreign words )
twSy Al- ljnp Al- sAdsp Al- jmEyp Al- EAmp .. )
?P ( NP2 VP1 NP3 PPNP4 PUNC5 | (2) P ( english phrase sequence , phrase alignment matrix |
VP1 NP2 NP3 PPNP4 PUNC5 ) foreign phrase sequence )
?P ( [the sixth committee]NP2 [recommends]V P1 (3) P ( english words , english bracketing , english phrase sequence |
[the general assembly]NP3 .. | foreign words , foreign bracketing , foreign phrase sequence ,
[twSy]V P1 [Al- ljnp Al- sAdsp]NP1 [Al- jmEyp Al- EAmp]NP2 .. , english phrase sequence , phrase alignment matrix )
NP2 VP1 NP3 PPNP4 PUNC5 )
Table 2: Statement of the translation model at top level.
.
Phrase Translation Model with Backoff Pathways
Example Instantiations Model Statement
P ( the sixth committee | Al- ljnp Al- sAdsp , NP ) =
P ( the sixth committee | Al- ljnp Al- sAdsp , NP ) (1) P ( WE1 WE2 .. WEn | WF1 WF2 .. WFm , phr typeE )
? ? (backoff if C( WF1 WF2 .. WFm , phr typeE) = 0)
P ( DT1 JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP ) (2) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TfineF1
TfineF2
.. TfineFm
, phr typeE )
. ?P ( the | Al- , DT ) . ?P ( WE1 | WF?i(1)
, TfineE1
)
. ?P ( committee | ljnp , NN ) . ?P ( WE2 | WF?i(2)
, TfineE2
)
. ?P ( sixth | sAdsp , JJ ) . ?.. ? P ( WEn | WF?i(n)
, TfineEn
)
? ? (backoff if C( TfineF1
TfineF2
.. TfineFm
, phr typeE) = 0)
P ( DT1 JJ4 NN2 | D1 N2 D3 A4 , NP ) (3) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TcoarseF1
TcoarseF2
.. TcoarseFm
, phr typeE )
. ?P ( the | Al- , DT ) . ?P ( WE1 | WF?i(1)
, TfineE1
)
. ?P ( committee | ljnp , NN ) . ?P ( WE2 | WF?i(2)
, TfineE2
)
. ?P ( sixth | sAdsp , JJ ) . ?.. ? P ( WEn | WF?i(n)
, TfineEn
)
? ? (backoff if C( TcoarseF1 TcoarseF2 .. TcoarseFm , phr typeE ) = 0)
P ( ? | D1 N2 D3 , NP ) (4) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TcoarseF1
TcoarseF2
.. TcoarseFm?1
, phr typeE )
. * ? * .. * ? . * ? * .. * ?
? ? (backoff if C( TcoarseF1 TcoarseF2 .. TcoarseFm?1 , phr typeE) = 0)
P ( ? | D1 N2 , NP ) (4) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TcoarseF1
TcoarseF2
.. TcoarseFm?2
, phr typeE )
. * ? * .. * ? . * ? * .. * ?
? ? (backoff if C( TcoarseF1 TcoarseF2 .. TcoarseFm?2 , phr typeE) = 0)
.... ....
Table 4: The phrase translation model, with backoff. Examples on the left side are from one of the Arabic test
sentences. (1) is the direct, lexical translation level. (2) - (4) constitute the backoff path to handle detailed phenomena
unseen in the training set. (2) is a model of fine POS-tag reordering and lexical generation; (3) is similar, but conditions
generation on coarse POS-tag sequences in the foreign language. (4) is a model for progressively stripping off POS-
tags from the ?less significant? end of a foreign sequence. The idea is to do this until we reach a subsequence that has
been seen in training data, and which we therefore have a distribution of valid generatons for. The term ?i in (2) - (4)
is a position alignment matrix. At all times, we generate not just an English POS-tag sequence, but rather an aligned
sequence. Similarly, in the lexical transfer probabilities shown in this table, there is a function ?i() which takes an
English sequence position index and returns the (unique) foreign word position to which it is aligned4.
Aligned English POS-tag Sequence Translation Probabilities
(conditioned on Arabic POS-tag sequence from VP in example)
P ( VBZ1 | VERB-IMP1 , VP ) = .28
P ( VBP1 | VERB-IMP1 , VP ) = .17
P ( VBD1 | VERB-IMP1 , VP ) = .09
...
P ( MD? VB1 | VERB-IMP1 , VP ) = .06
Table 6: From the Arabic example, top English VP gen-
erations given an Arabic phrase VERB-IMP.
is done in the context that the model has already pro-
posed a sequence of POS tags for the phrase. Thus we
know the English POS of the word we are trying to gen-
erate in addition to the foreign word that is generating it.
Consequently, we condition translation on English POS
as well as the foreign word. Table 7 describes the backoff
path for basic lexical transfer and presents a motivating
example in the French word droit. Translation probabili-
ties for one of the words in the example Arabic sentence
can be found in Table 8.
4.3.2 Generation via a Lemma Model
To counter sparse data problems in estimating word
translation probabilities, we also implemented a lemma-
Word Generation
Examples Model with Backoff Pathways
P (WE | droit , NNS) P (WE |WF , TfineE )
rights 0.4389 p(rights | droit , NNS)
benefits 0.0690
people 0.0533
laws 0.0188
? (backoff if C(WF , TfineE ) = 0)
P (WE | droit , N) P (WE |WF , TcoarseE )
right 0.4970
law 0.1318
rights 0.0424 p(rights | droit , N)
property 0.0115
? (backoff if C(WF , TcoarseE ) = 0)
P (WE | droit) P (WE |WF )
right 0.2919
entitled 0.0663
law 0.0652
the 0.0249
to 0.0240
rights 0.0210 p( rights | droit )
? (backoff if C(WF ) = 0)
p( UNKNOWN WORD |WF ) = 1
Table 7: Description of the conditioning for different lev-
els of backoff in the lexical transfer model. The exam-
ple shows translations for the French word droit (?right?)
conditioned on decreasingly specific values. The pro-
gressively lower ranking of the correct translation as we
move from fine, to coarse, to no POS, illustrates the ben-
efit of conditioning generation on the English part of
speech.
Arabic Word English POS English Wd. Prob.
ljnp NN committee 0.591
ljnp NN commission 0.233
ljnp NN subcommittee 0.035
ljnp NN acc 0.013
ljnp NN report 0.005
ljnp NN ece 0.004
ljnp NN icrc 0.004
ljnp NN aalcc 0.004
ljnp NN escap 0.004
ljnp NN escwa 0.004
ljnp NN eca 0.003
ljnp NNS members 0.088
ljnp NNS recommendations 0.033
ljnp NNS copuos 0.033
ljnp NNS questions 0.027
ljnp NNS representatives 0.024
ljnp N committee 0.577
ljnp N commission 0.227
ljnp N subcommittee 0.035
Table 8: From running example, translation probabilities for Arabic
noun ljnp, ?committee?.
based model for word translation. Under this model,
translation distributions are estimated by counting word
alignment links between foreign and English lemmas, as-
suming a lemmatization of both sides of the parallel cor-
pus as input. The form of the model is illustrated below:
P ( WE | WF ,TcoarseF ,TfineE ) =
P ( WE | lemmaE , TcoarseF , TfineE )?
P ( lemmaE | lemmaF , TcoarseF , TfineE )?
P ( lemmaF | WF , TcoarseF , TfineE )
? approximated by
P ( WE | lemmaE , TfineE )?
P ( lemmaE | lemmaF , TcoarseE )?
P ( lemmaF | WF , TcoarseF )
First, note that P ( lemmaF | WF , TcoarseF ) is very
simply a hard lemma assignment by the foreign lan-
guage lemmatizer. Second, English word generation
from English lemma and coarse POS (P ( WE | lemmaE
, TfineE )) is programmatic, and can be handled by
means of rules in conjunction with a lookup table for
irregular forms. The only distribution here that must be
estimated from data is P ( lemmaE | lemmaF , TcoarseE
). This is done as described above. Furthermore, given
an electronic translation dictionary, even this distribution
can be pre-loaded: indeed, we expect this to be an
advantage of the lemma model, and an example of
a good opportunity for integrating compiled human
knowledge about language into an SMT system. Some
examples of the lemma model combating sparse data
problems inherent in the basic word-to-word models can
be found in Table 9.
4.3.3 Coercion
Lexical coercion is a phenomenon that sometimes occurs
when we condition translation of a foreign word on the
word and the English part-of-speech. We find that the
system we have described frequently learns this behav-
ior: specifically, the model learns in some cases how
to generate, for instance, a nominal form with similar
meaning from a French adjective, or an adjectival real-
ization of a French verb?s meaning; some examples of
this phenomenon are shown in Table 10. We find this
coercion effect to be of interest because it identifies in-
teresting associations of meaning. For example, in Table
10 ?willing? and ?ready? are both sensible ways to re-
alize the meaning of the action ?to accept? in a passive,
descriptive mode. droit behaves similarly. Though the
English verb ?to right? or ?to be righted? does not have
the philosophical/judicial entitlement sense of the noun
?right?, we see that the model has learned to realize the
meaning in an active, verbal form: e.g., VBG ?receiving?
and VB ?qualify?.
5 Decoding
Decoding was implemented by constructing finite-state
machines (FSMs) per evaluation sentence to encode
relevant portions (for the individual sentence in ques-
tion) of the component translation distributions described
above. Operations on these FSMs are performed using
the AT&T FSM Toolkit (Mohri et al, 1997). The FSM
constructed for a test sentence is subsequently composed
with a FSM trigram language model created via the SRI
Language Modeling Toolkit (Stolcke, 2002). Thus we
use the trigram language model to implement rescoring
of the (direct) translation probabilities for the English
word sequences in the translation model lattice.
We found that using the finite-state framework and the
general-purpose AT&T toolkit greatly facilitates decoder
development by freeing the implementation from details
of machine composition and best-path searching, etc.
The structure of the translation model finite-state ma-
chines is as illustrated in Figure 1. The sentence-level
(aligned phrase sequence generation) and phrase-level
(aligned intra-phrase sequence generation) translation
probabilities are encoded on epsilon arcs in the ma-
chines. Word translation probabilities are placed onto
arcs emitting the word as an output symbol (in the fig-
ure, note the arcs emitting ?committee?, ?the?, etc.). The
FSM in Figure 1 corresponds to the Arabic example sen-
tence used throughout this paper. In the portion of the
machine shown, the (best) path which generated the ex-
ample sentence is drawn in bold. Finally, Figure 2 is
a rendering of the actual FSM (aggressively pruned for
display purposes) that generated the example Arabic sen-
tence; although labels and details are not visible, it may
provide a visual aid for better understanding the structure
of the FSM lattices generated here.
As a practical matter in decoding, during translation
model FSM construction we modified arc costs for out-
put words in the following way: a fixed bonus was as-
signed for generating a ?content? word translating to a
?content? word. Determining what qualifies as a con-
tent word was done on the basis of a list of content POS
tags for each language. For example, all types of nouns,
verbs and adjectives were listed as content tags; deter-
miners, prepositions, and most other closed-class parts of
speech were not. This implements a reasonable penalty
on undesirable output sentence lengths. Without such a
penalty, translation outputs tend to be very short: long
sentence hypotheses are penalized de facto merely by
containing many word translation probabilities. An ad-
ditional trick in decoding is to use only the N-best trans-
lation options for sentence-level, phrase-level, and word-
level translation. We found empirically (and very consis-
tently) in dev-test experiments that restricting the syntac-
tic transductions to a 30-best list and word translations to
a 15-best list had no negative impact on Bleu score. The
benefit, of course, is that the translation lattices are dra-
matically reduced in size, speeding up composition and
search operations.
.
.
.
.
.
.
etc ...
... next phrase,
.
.
.
.
.
.
S
P(the | NULL)
P(an | NULL) P(committee | ljnp)
P(commission | ljnp)
NULLP( DT       NN      |   NOUN?SG    )1
1 1
P(NP    VP    NP  .. |VP   NP   NP  .. )
32
"commission"
"committee"
"the"
"an"
1
2 3
Figure 1: An illustration of the translation model structure for an
Arabic test sentence.
319 321W__:the/-0
0
1
S__:<s>/5.411
2
S__:<s>/5.629
3
S__:<s>/5.835
4
S__:<s>/5.868
5
S__:<s>/5.940
6
S__:<s>/6.226
7
S__:<s>/6.381
8
S__:<s>/6.385
9
S__:<s>/6.396
10
S__:<s>/6.529
11S__:<s>/6.546
12
S__:<s>/6.620
13
S__:<s>/6.657
14S__:<s>/6.698
15
S__:<s>/6.711
16
S__:<s>/6.717
17S__:<s>/6.748
18
S__:<s>/6.825
19S__:<s>/6.875
20S__:<s>/6.942
21
S__:<s>/6.946
116
137
W__:committee/-0
224
247P__ADJP:<epsilon>/-0
412 439T__-:<epsilon>/0.129
702
714
W__:of/1.969
W__:in/2.215
22
P__NP:<epsilon>/-0
117
138
G__:<epsilon>/-0
225 248G__:<epsilon>/-0
272 273W__:general/-0
413
440T__recommends:<epsilon>/0.227
474
501W__:the/1.188
W__:a/1.736
W__:this/2.266
703
715
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:after/4.880
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
521
555W__:of/1.969
558
590
W__:the/0.132
631
630
W__:rapporteur/0.699
W__:decision/1.000
23
P__NP:<epsilon>/-0
118
139G__:<epsilon>/-0
226
227
W__:committee/-0
322
W__:assembly/-0
414 441T__recommends:<epsilon>/0.227
522 556W__:of/1.969
W__:in/2.215
525
559
P__O:<epsilon>/-0
24
P__NP:<epsilon>/-0
119 140P__O:<epsilon>/-0
249
G__:<epsilon>/-0
415
442
T__2:<epsilon>/0.631
443
T__2#xslax/NULL:<epsilon>/1.194
632 647P__NP:<epsilon>/-0
367
G__:<epsilon>/-0
523
557
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
592
T__::<epsilon>/0.028
704
716
W__:of/1.969
W__:in/2.215
W__:that/2.614
W__:for/2.660
25
P__O:<epsilon>/-0
228 229
W__:sixth/-0
416
444
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
445T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
446
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
475 502G__:<epsilon>/-0
633
648
P__VP:<epsilon>/-0
120
141
P__NP:<epsilon>/-0
560
561
W__:the/-0
705
717
P__O:<epsilon>/-0
26
P__NP:<epsilon>/-0
417 447T__-:<epsilon>/0.129
476 503P__NP:<epsilon>/-0
634
635
W__:draft/0.404
W__:project/1.841
121
142
G__:<epsilon>/-0
230
251
P__VP:<epsilon>/-0
323
325
W__:by/-0
27P__NP:<epsilon>/-0
524
W__:of/1.969
W__:in/2.215
593W__:sixth/-0
122 143G__:<epsilon>/-0
324 W__:to/-0
477
504W__:adoption/0.711
W__:provision/1.929
532
G__:<epsilon>/-60
706
718W__:police/-0
28
P__ADJP:<epsilon>/-0
231
252
G__:<epsilon>/-0
418
448
W__:-/-0
526
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
T__sixth#committee:<epsilon>/1.871
562
534
W__:by/-0
123 144
P__O:<epsilon>/-0
29P__NP:<epsilon>/-0
232
253
W__:adoption/0.711
W__:provision/1.929
527 T__by/NULL#the/NULL#general#assembly:<epsilon>/1.085
563T__to/NULL#the/NULL#general#assembly:<epsilon>/1.254 W__:to/-0
636 649P__ADJP:<epsilon>/-0
327W__:the/-0
419 W__:2/-0
30
P__NP:<epsilon>/-0
124 145P__O:<epsilon>/-0
528 565T__-:<epsilon>/0.129
573
W__:-/-0
637
650
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
326
328
W__:the/-0
420 449W__:2/-0
707
696
W__:in/-0
31
P__NP:<epsilon>/-0
32
P__VP:<epsilon>/-0
33
P__SBAR:<epsilon>/-0
529 566
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:about/3.426
594
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:this/2.266
638 651
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
125
146
G__:<epsilon>/-0
421
450
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
34P__NP:<epsilon>/-0
329
T__the/NULL#general#assembly:<epsilon>/0.938
369
T__had#decided:<epsilon>/2.442
370
T__will#consider:<epsilon>/2.442
639
652
W__:in/1.425
708 719
W__:world/-0
104 126G__:<epsilon>/-0
422
451
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
530 567
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
35P__NP:<epsilon>/-0
330
W__:decided/-0
478
505
W__:adoption/0.711
W__:provision/1.929
640
653
W__:on/2.046
W__:by/1.908
W__:in/1.425
W__:with/1.853
709 720P__VP:<epsilon>/-0
423
452
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
36
P__NP:<epsilon>/-0
233
254W__:adoption/0.711
W__:provision/1.929
331
W__:consider/-0
531 568
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
641
654W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
710
721
P__O:<epsilon>/-0
147P__VP:<epsilon>/-0
148
P__PPNP:<epsilon>/-0
424
453
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
37
P__O:<epsilon>/-0
332
307W__:to/-0
533 570
T__accountable:<epsilon>/0.693
571T__present/NULL:<epsilon>/0.693
642
655
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
127
P__PPNP:<epsilon>/-0
149P__NP:<epsilon>/-0
425
454W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
38
P__O:<epsilon>/-0
333 371
W__:with/-0
537
W__:the/-0
711
722
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:every/4.241
W__:another/4.082
128 P__VP:<epsilon>/-0
426
455
W__:in/1.425
39
P__O:<epsilon>/-0
535
538
W__:the/-0
595W__:following/0.227
129 150
P__O:<epsilon>/-0
334
372
W__:next/1.854
W__:following/0.227
427
456
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
643
656
T__to/NULL#2/NULL:<epsilon>/1.299
T__to/NULL#2:<epsilon>/2.397
657T__with/NULL#article/NULL#2:<epsilon>/2.397
658
T__under/NULL#review/NULL:<epsilon>/3.091
659
T__by/NULL#2#police/NULL#officers/NULL:<epsilon>/3.091
660T__in/NULL#only/NULL#2:<epsilon>/3.091
661
T__in/NULL#order/NULL:<epsilon>/3.091
662T__for/NULL#just/NULL#2:<epsilon>/3.091
663
T__on/NULL#2:<epsilon>/3.091
664T__at/NULL#2:<epsilon>/3.091
665
T__with/NULL#regard/NULL:<epsilon>/3.091
666
T__for/NULL#2:<epsilon>/3.091
667
T__to/NULL#2.7/NULL#in/NULL:<epsilon>/3.091
668
T__in/NULL#document:<epsilon>/3.091
669
T__into/NULL#the/NULL#world/NULL#economy/NULL:<epsilon>/3.091
40
P__NP:<epsilon>/-0
479
506
W__:adoption/0.711
W__:provision/1.929
428
457
W__:in/1.425
536 572
W__:the/-0
41
P__PPNP:<epsilon>/-0
130 151P__NP:<epsilon>/-0
234
255
W__:adoption/0.711
W__:provision/1.929
429
458
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
42P__VP:<epsilon>/-0
131
152G__:<epsilon>/-0
539
574
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
644 670T__-:<epsilon>/0.129
43
P__NP:<epsilon>/-0
132
153P__PPNP:<epsilon>/-0
645
646
W__:the/-0
44
P__NP:<epsilon>/-0
133
154
P__VP:<epsilon>/-0
480
508
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
235
257
P__NP:<epsilon>/-0
335
373
W__:rapporteur/0.699
W__:decision/1.000
W__:the/0.132
45
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
46
T__sixth#committee:<epsilon>/1.871
134
155
P__NP:<epsilon>/-0
671W__:sixth/-0
236
258
P__NP:<epsilon>/-0
47
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
48T__sixth#committee:<epsilon>/1.871
135
156
G__:<epsilon>/-0
430
459
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
336
374W__:2/-0
49T__2:<epsilon>/0.631
50
T__2#xslax/NULL:<epsilon>/1.194
136
157
G__:<epsilon>/-0
237
259
G__:<epsilon>/-0
337
375
W__:2/-0
51
T__-:<epsilon>/0.129
T__-/NULL:<epsilon>/2.988
158
G__:<epsilon>/-0
238
260
G__:<epsilon>/-0
569P__O:<epsilon>/-0
287
338
W__:sixth/-0
672
T__2:<epsilon>/0.631
673
T__2#xslax/NULL:<epsilon>/1.194
52T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
53
T__sixth#committee:<epsilon>/1.871
159
P__VP:<epsilon>/-0
239
261
G__:<epsilon>/-0
596T__::<epsilon>/0.028
376W__:committee/-0
674
T__recommends:<epsilon>/0.227
54T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
55T__sixth#committee:<epsilon>/1.871
160
P__VP:<epsilon>/-0
240
262
P__PPNP:<epsilon>/-0
481
509
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:accountable/-0
431
460
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
540
575
W__:next/1.854
W__:following/0.227
56T__asean/NULL:<epsilon>/1.098
57
T__imf-2:<epsilon>/1.098
58
T__concerned/NULL#with/NULL:<epsilon>/1.098
161T__-:<epsilon>/0.129
241
263P__NP:<epsilon>/-0
339
377
W__:committee/-0
W__:present/-0
59
T__2:<epsilon>/0.631
60T__2#xslax/NULL:<epsilon>/1.194
162
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
163
T__sixth#committee:<epsilon>/1.871
242
264
T__by/NULL#the/NULL#general#assembly:<epsilon>/1.085
T__by/NULL#the#general#assembly:<epsilon>/2.725
265T__to/NULL#the/NULL#general#assembly:<epsilon>/1.254
T__to/NULL#the#general#assembly:<epsilon>/2.748
266
T__with/NULL#general#assembly#decision/NULL:<epsilon>/3.313
340 342W__:the/-0
W__:general/-0
675
T__asean/NULL:<epsilon>/1.098
676T__imf-2:<epsilon>/1.098
677T__concerned/NULL#with/NULL:<epsilon>/1.098
61
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
62
T__sixth#committee:<epsilon>/1.871
164
P__VP:<epsilon>/-0
243
267
P__VP:<epsilon>/-0
341
343
W__:the/-0
W__:assembly/-0
678
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:every/4.241
W__:another/4.082
63T__728#xslax/NULL:<epsilon>/1.421
64
T__the/NULL#committee:<epsilon>/1.981 65
T__the/NULL#advisory#committee:<epsilon>/2.268
66T__729#xslax/NULL:<epsilon>/2.674
165P__VP:<epsilon>/-0
244
268
P__NP:<epsilon>/-0
564
W__:wish/-0
344 346
W__:the/-0
67
T__recommends:<epsilon>/0.227
166
T__-:<epsilon>/0.129
245
269
P__NP:<epsilon>/-0
432
461
W__:the/1.188
345
347
W__:the/-0
597
G__:<epsilon>/-0
712
723
W__:next/1.854
W__:following/0.227
68T__that:<epsilon>/-0
167T__-:<epsilon>/0.129
246
270
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
271
T__the/NULL#assembly:<epsilon>/1.941
433
462
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
482
510
W__:adoption/0.711
348
368
W__:accountable/-0
598
W__:next/1.854
W__:following/0.227
69T__2:<epsilon>/0.631
70T__2#xslax/NULL:<epsilon>/1.194
168P__VP:<epsilon>/-0
274T__accountable:<epsilon>/0.693
275T__present/NULL:<epsilon>/0.693
434
463
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
483
511
W__:adoption/0.711
W__:provision/1.929
349 W__:present/-0
72T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
73T__sixth#committee:<epsilon>/1.871
169T__the/NULL#sixth#committee:<epsilon>/0.154
170
T__will#establish:<epsilon>/1.945
276P__PPNP:<epsilon>/-0
435
464
W__:in/1.425
350
380
W__:2/-0
541
W__:the/0.132
679
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:every/4.241
W__:another/4.082
74T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
75
T__the/NULL#assembly:<epsilon>/1.941
171T__by/NULL#the/NULL#sixth#committee:<epsilon>/0.490
T__by/NULL#the/NULL#sixth/NULL#committee:<epsilon>/1.589
436
465W__:in/1.425
277
P__NP:<epsilon>/-0
351
W__:article/-0
78T__-:<epsilon>/0.129
172
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
173
T__sixth#committee:<epsilon>/1.871
437
466
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
250
W__:committee/-0
352
W__:review/-0
71
T__-:<epsilon>/0.129
438
467
W__:in/1.425
W__:with/1.853
278
G__:<epsilon>/-0
353
381
W__:2/-0
79
T__-:<epsilon>/0.129
174
T__-:<epsilon>/0.129
468
W__:-/-0
599
W__:rapporteur/0.699
W__:decision/1.000
T__the/NULL#general#assembly:<epsilon>/0.938
279
T__had#decided:<epsilon>/2.442
280T__will#consider:<epsilon>/2.442
354 W__:only/-0
80
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871 81
T__sixth#committee:<epsilon>/1.871
175
T__2:<epsilon>/0.631
176T__2#xslax/NULL:<epsilon>/1.194
W__:recommends/-0
484
512
W__:adoption/0.711
281
P__PPNP:<epsilon>/-0
355 W__:order/-0
82
T__to/NULL#2/NULL:<epsilon>/1.299
177
P__VP:<epsilon>/-0
469W__:recommends/-0
485
514
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
576
600W__::/-0
680
W__:the/1.188
282
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
356 W__:just/-0
507
542
P__O:<epsilon>/-0
83T__add:<epsilon>/1.704
84
T__take/NULL:<epsilon>/2.397
178
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
179
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
180
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
470
W__:2/-0
577
601
W__:next/1.854
W__:following/0.227
681
W__:the/1.188
W__:a/1.736
T__::<epsilon>/0.028
85
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
86
T__sixth#committee:<epsilon>/1.871
181T__add:<epsilon>/1.704
182T__take/NULL:<epsilon>/2.397
283
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:per/3.988
471W__:2/-0
357 W__:regard/-0
486
515
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
543
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
87
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
88T__sixth#committee:<epsilon>/1.871
183T__2:<epsilon>/0.631
184T__2#xslax/NULL:<epsilon>/1.194
472
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:as/2.616
284
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:about/3.426
W__:if/3.704
682
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:another/4.082
W__:the/-0
185
P__VP:<epsilon>/-0
358 382W__:2.7/-0
473
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:as/2.616
487
516
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
186
P__VP:<epsilon>/-0
286
T__the/NULL#sixth#committee:<epsilon>/0.454
359 W__:document/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
89
W__:sixth/-0
488
517
W__:of/1.969
187
P__VP:<epsilon>/-0
288T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
289
T__sixth#committee:<epsilon>/1.871
360
383
W__:the/-0
W__:-/-0
W__:the/-0
490
519
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
188T__recommends:<epsilon>/0.227
189T__recommended:<epsilon>/2.759
290
P__NP:<epsilon>/-0
361
384
G__:<epsilon>/-0
G__:<epsilon>/-0
578
602
W__:rapporteur/0.699
W__:decision/1.000
713 724W__:the/1.188
90
W__:sixth/-0
190T__recommends:<epsilon>/0.227
191T__recommended:<epsilon>/2.759
192T__recommend:<epsilon>/3.200
291
P__NP:<epsilon>/-0
362
W__:xslax/-0
W__:xslax/-0
580 604W__:sponsors/1.131
91
W__:2/-0
193W__:-/-0
292P__ADJP:<epsilon>/-0
683
W__:the/1.188
W__:a/1.736
725
W__:following/0.227
363
385W__:assembly/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
92W__:2/-0
W__:the/-0
293
T__to/NULL#2/NULL:<epsilon>/1.299
T__to/NULL#2:<epsilon>/2.397
294
T__with/NULL#article/NULL#2:<epsilon>/2.397
295T__under/NULL#review/NULL:<epsilon>/3.091
296T__by/NULL#2#police/NULL#officers/NULL:<epsilon>/3.091
297T__in/NULL#only/NULL#2:<epsilon>/3.091
298
T__in/NULL#order/NULL:<epsilon>/3.091
299T__for/NULL#just/NULL#2:<epsilon>/3.091
300T__on/NULL#2:<epsilon>/3.091
301
T__at/NULL#2:<epsilon>/3.091
302
T__with/NULL#regard/NULL:<epsilon>/3.091
303
T__for/NULL#2:<epsilon>/3.091
304
T__to/NULL#2.7/NULL#in/NULL:<epsilon>/3.091
305T__in/NULL#document:<epsilon>/3.091
306T__into/NULL#the/NULL#world/NULL#economy/NULL:<epsilon>/3.091
579 603W__:following/0.227
364
386
G__:<epsilon>/-0
W__:the/0.132
W__:a/3.066
93
W__:-/-0
T__2:<epsilon>/0.631
308T__2#xslax/NULL:<epsilon>/1.194
309
T__it/NULL:<epsilon>/3.818
581 605W__:2/-0
194W__:sixth/-0
W__:by/-0
582 606W__:2/-0
W__:to/-0
W__:the/-0
195
T__recommends:<epsilon>/0.227
316
317
W__:general/-0
491
520
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:to/-0
583
607W__:rapporteur/0.699
W__:decision/1.000
685
W__:with/-0
196T__recommends:<epsilon>/0.227
197
T__recommended:<epsilon>/2.759
365
W__:assembly/-0
94
W__:sixth/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
198
W__:-/-0
544
W__:next/1.854
W__:following/0.227
584
608
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
686
W__:under/-0
W__:the/-0
310W__:with/-0
387
G__:<epsilon>/-0
199W__:-/-0
687W__:by/-0
311T__recommends:<epsilon>/0.227
312
T__recommended:<epsilon>/2.759
313T__recommend:<epsilon>/3.200
320
W__:general/-0
585
586
W__:draft/0.404
95W__:sixth/-0
200T__recommends:<epsilon>/0.227
688W__:in/-0
314
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
315
T__the/NULL#assembly:<epsilon>/1.941
366W__:assembly/-0
587 610G__:<epsilon>/-0
96
W__:asean/-0
201W__:the/-0
689
W__:in/-0
318
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
588
611
W__:next/1.854
W__:following/0.227
W__:imf-2/-0
202W__:will/-0
388G__:<epsilon>/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:an/2.837
W__:some/3.097
W__:this/2.266
492
W__:adoption/0.711
690W__:for/-0
W__:the/-0
97
W__:concerned/-0
203W__:by/-0
389
P__PPNP:<epsilon>/-0
493
W__:adoption/0.711
W__:on/-0
W__:the/-0
98W__:2/-0
494 W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:at/-0
W__:the/-0
99W__:2/-0
W__:accountable/-0
W__:general/-0
691W__:with/-0
W__:the/-0 204
W__:sixth/-0
W__:present/-0
W__:assembly/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:for/-0
205
W__:-/-0
T__by/NULL#the/NULL#general#assembly:<epsilon>/1.085
T__by/NULL#the#general#assembly:<epsilon>/2.725
T__to/NULL#the/NULL#general#assembly:<epsilon>/1.254
T__to/NULL#the#general#assembly:<epsilon>/2.748
692W__:to/-0
100
W__:sixth/-0
390
G__:<epsilon>/-0
589
612
W__:rapporteur/0.699
W__:decision/1.000
W__:2/-0
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
693W__:in/-0
101W__:728/-0
W__:the/0.132
206W__:2/-0
P__VP:<epsilon>/-0
391
W__:had/-0
545 W__:following/0.227
613
W__:following/0.227
694W__:into/-0
102
W__:the/-0
207
T__recommends:<epsilon>/0.227
392W__:will/-0
495
W__:adoption/0.711
546
W__:the/0.132
591
614
W__:sponsors/1.131
684W__:-/-0
103W__:the/-0
W__:had/-0
208
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
496 P__NP:<epsilon>/-0
726
T__::<epsilon>/0.028
W__:729/-0
W__:will/-0
695W__:committee/-0
209
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
497 P__PPNP:<epsilon>/-0
615W__::/-0
W__:officers/-0
105W__:recommends/-0
T__to/NULL#2/NULL:<epsilon>/1.299
T__to/NULL#2:<epsilon>/2.397
T__with/NULL#article/NULL#2:<epsilon>/2.397
W__:article/-0
W__:2/-0
210
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
498 P__O:<epsilon>/-0
106
W__:that/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201 393W__:rapporteur/0.699
W__:decision/1.000
616
W__:committee/-0
697
W__:2/-0
W__:economy/-0
211W__:add/-0
107W__:2/-0
499 W__:adoption/0.711
W__:provision/1.929
W__:recommends/-0
728T__recommends:<epsilon>/0.227
W__:take/-0
394W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:of/-0
108W__:2/-0
W__:asean/-0
727
T__-:<epsilon>/0.129
T__-/NULL:<epsilon>/2.988
212
W__:2/-0
W__:the/1.188
W__:the/-0
395G__:<epsilon>/-0
W__:imf-2/-0
729
W__:next/1.854
W__:following/0.227
213W__:2/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
500
W__:adoption/0.711
W__:provision/1.929
617
W__:following/0.227
W__:xslax/-0
698W__:concerned/-0
110W__:sixth/-0
214T__recommends:<epsilon>/0.227
396
G__:<epsilon>/-0
699
W__:adoption/0.711
W__:provision/1.929
W__:approval/3.758
W__:appropriation/3.108
W__:accreditation/3.18
W__:dependence/3.758
W__:reliance/3.746
76W__:the/-0
215
T__recommends:<epsilon>/0.227
W__:adoption/0.711
547 W__:the/0.132
397
G__:<epsilon>/-0
513 548P__NP:<epsilon>/-0
618
W__:rapporteur/0.699
W__:decision/1.000
77W__:the/-0
216
T__recommends:<epsilon>/0.227
W__:general/-0
T__2:<epsilon>/0.631
T__2#xslax/NULL:<epsilon>/1.194
217W__:recommends/-0
P__ADJP:<epsilon>/-0
619W__::/-0
112
W__:-/-0
378W__:assembly/-0
W__:the/1.188
549
W__:next/1.854
W__:following/0.227
W__:recommended/-0
W__:next/1.854
W__:following/0.227
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
T__the#assembly:<epsilon>/3.641
T__the/NULL#wish/NULL#of/NULL#the/NULL#general#assembly:<epsilon>/3.421
109
W__:-/-0
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
218
W__:recommends/-0
398G__:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
113
W__:-/-0
620
P__PPNP:<epsilon>/-0
730
W__:rapporteur/0.699
W__:decision/1.000
W__:recommended/-0
W__:general/-0
W__:the/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:per/3.988
621
W__:rapporteur/0.699
W__:decision/1.000
W__:recommend/-0
379
W__:assembly/-0
731
W__:following/0.227
700
W__:adoption/0.711
W__:provision/1.929
W__:approval/3.758
W__:appropriation/3.108
W__:accreditation/3.18
W__:dependence/3.758
W__:reliance/3.746
114W__:sixth/-0
219G__:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:about/3.426
W__:if/3.704
622W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
732
W__:sponsors/1.131
399
G__:<epsilon>/-0
W__:to/-0
220
W__:committee/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:during/4.572
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
550
W__:rapporteur/0.699
W__:decision/1.000
623
G__:<epsilon>/-0
W__:add/-0
221
W__:recommends/-0
400G__:<epsilon>/-0
W__:adoption/0.711
W__:provision/1.929
624W__:rapporteur/0.699
W__:decision/1.000
733
W__::/-0
W__:take/-0
222
W__:recommends/-0
W__:the/0.132
551
W__:following/0.227
W__:the/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
625
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:recommended/-0
401W__:police/-0
552 W__::/-0
W__:recommends/-0
115
W__:sixth/-0
223G__:<epsilon>/-0
553
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:draft/0.404
W__:-/-0
W__:the/-0
G__:<epsilon>/-0
W__:of/1.969
626
W__:of/1.969
701W__:adoption/0.711
734
W__:rapporteur/0.699
W__:decision/1.000
W__:recommends/-0
W__:in/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
627G__:<epsilon>/-0
W__:adoption/0.711
W__:sixth/-0
W__:sixth/-0
W__:adoption/0.711
W__:provision/1.929
W__:xslax/-0
735
W__:articles/3.975
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:co-sponsors/3.808
402W__:world/-0
W__:of/1.969
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:committee/-0
W__:establish/-0
256
285P__NP:<epsilon>/-0
W__:draft/0.404
W__:project/1.841
403P__O:<epsilon>/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:draft/0.404
W__:the/-0
T__2:<epsilon>/0.631
T__2#xslax/NULL:<epsilon>/1.194
W__:committee/-0
489G__:<epsilon>/-60
736
W__:of/1.969
W__:in/2.215
W__:that/2.614
W__:for/2.660
W__:the/-0
W__:decision/-0G__:<epsilon>/-0
W__:committee/-0 737G__:<epsilon>/-0
W__:the/-0
404P__NP:<epsilon>/-0
W__:xslax/-0
G__:<epsilon>/-0
609
W__:of/1.969
405
P__PPNP:<epsilon>/-0
W__:adoption/0.711
G__:<epsilon>/-0
W__:sixth/-0
W__:rapporteur/0.699
W__:draft/0.404
W__:resolution/3.339
W__:project/1.841
W__:xslax/-0
406
P__PPNP:<epsilon>/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
628P__NP:<epsilon>/-0
W__:committee/-0
W__:recommends/-0
407T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
408
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
409
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
410
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
W__:adoption/0.711
G__:<epsilon>/-60
T__the/NULL#general#assembly:<epsilon>/0.530
T__the/NULL#assembly:<epsilon>/1.941
629
W__:rapporteur/0.699
W__:decision/1.000
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
411
P__PPNP:<epsilon>/-0
W__:committee/-0
T__accountable:<epsilon>/0.693
T__present/NULL:<epsilon>/0.693
W__:decided/-0
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:co-sponsors/3.808
W__:article/-0
G__:<epsilon>/-0
W__:to/-0
W__:consider/-0
W__:review/-0
W__:with/-0
W__:draft/0.404
W__:rapporteur/0.699
W__:decision/1.000
W__:with/-0
554
W__:next/1.854
W__:following/0.227
W__:2/-0
W__:draft/0.404
W__:project/1.841
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:of/1.969
W__:in/2.215
G__:<epsilon>/-0
W__:under/-0
W__:only/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:next/1.854
W__:following/0.227
G__:<epsilon>/-0
W__:xslax/-0
W__:by/-0
G__:<epsilon>/-60
W__:order/-0
738/0<epsilon>:</s>/-0
G__:<epsilon>/-0
W__:in/-0
W__:just/-0
W__:committee/-0
W__:rapporteur/0.699
W__:decision/1.000
W__:in/-0
P__O:<epsilon>/-0
W__:xslax/-0
W__:implications/2.139
W__:sponsors/1.131
W__:for/-0
W__:regard/-0
W__:committee/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:an/2.837
W__:some/3.097
W__:this/2.266
P__VP:<epsilon>/-0
W__:on/-0
W__:the/1.188
G__:<epsilon>/-0
W__:advisory/-0
P__VP:<epsilon>/-0
W__:2.7/-0
W__:at/-0
W__:the/1.188
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NGEN+IN#NN:<epsilon>/4.443
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NNS#NGEN+IN#NN:<epsilon>/4.644
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
P__NP:<epsilon>/-0
W__:document/-0
G__:<epsilon>/-0
W__:with/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:draft/0.404
W__:project/1.841
P__PPNP:<epsilon>/-0
W__:the/-0
G__:<epsilon>/-0
W__:for/-0
G__:<epsilon>/-0
P__O:<epsilon>/-0
G__:<epsilon>/-60
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:to/-0
W__:officers/-0
W__:the/0.132
G__:<epsilon>/-0
W__:xslax/-0
G__:<epsilon>/-0
W__:in/-0
P__PPNP:<epsilon>/-0
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:xslax/-0
W__:into/-0
W__:economy/-0
W__:draft/0.404
W__:project/1.841
W__:xslax/-0
W__:recommends/-0
W__:2/-0
T__-:<epsilon>/0.129
T__-/NULL:<epsilon>/2.988
W__:the/1.188
W__:the/1.188
W__:committee/-0
W__:recommends/-0
W__:2/-0
W__:following/0.227
G__:<epsilon>/-60
W__:general/-0
T__2:<epsilon>/0.631
T__2#xslax/NULL:<epsilon>/1.194
G__:<epsilon>/-0
W__:recommends/-0
W__:it/-0
W__:with/-0
111W__:assembly/-0
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
W__:the/0.132
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:after/4.880
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:up/5.208
W__:without/5.054
W__:since/5.054
W__:so/5.103
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
W__:upon/5.054
W__:across/5.208
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NNS#NGEN+IN#NN:<epsilon>/4.644
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
W__:rapporteur/0.699
G__:<epsilon>/-0
W__:general/-0
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
P__O:<epsilon>/-0
G__:<epsilon>/-0
W__:recommends/-0
W__:xslax/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
G__:<epsilon>/-0
W__:recommended/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:this/2.266
W__:the/0.132
P__PPNP:<epsilon>/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
518P__O:<epsilon>/-0
W__:recommend/-0
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:after/4.880
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:without/5.054
W__:since/5.054
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
W__:upon/5.054
W__:committee/-0
G__:<epsilon>/-0
W__:in/1.425
T__::<epsilon>/0.028
W__:the/-0
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
T__sixth#committee:<epsilon>/1.871
G__:<epsilon>/-0
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NGEN+IN#NN:<epsilon>/4.443
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NNS#NGEN+IN#NN:<epsilon>/4.644
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
W__:the/-0
W__:draft/0.404
W__:project/1.841
G__:<epsilon>/-0
W__:the/1.188
W__:a/1.736
W__:this/2.266
W__:committee/-0
W__:the/-0
G__:<epsilon>/-60
P__NP:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:of/1.969
W__:in/2.215
Figure 2: A portion of the translation model for an Arabic test sen-
tence, compacted and aggressively pruned by path probability for dis-
play purposes.
6 Evaluation
Results Tables A and B below list evaluation results for
translation on the Arabic and French test sets respec-
tively. In each case, results for a comparison system ?
the Giza++ IBM Model 4 implementation (Och and Ney,
2000) with the ReWrite decoder (Marcu and Germann,
2002) ? are included as a benchmark. Results were gen-
erated for training corpora of varying sizes. For Arabic,
we ran our system on two large subsets of the UN cor-
pus and evaluated on a 200-sentence held-out set (refer
to Results Table A below). For the 150K sentence Ara-
bic training set, Giza++ and the shallow syntax model
achieved very similar performance. We were unable to
obtain evaluation numbers for Giza++/ReWrite on the
large Arabic training set, however, since its language
model component has a vocabulary size limit which was
exceeded in the larger corpus. In French we observed the
systems to perform similarly on the small training sets
we used (Results Table B). We performed some exper-
iments in classifier combination using the two compat-
ible (150K-training-sentence) Arabic systems, wherein
a small devtest set was used to identify simple system
combination parameters based on model confidence and
sentence length. In situations where our system was con-
fident we used its output, and used Giza++ output other-
wise. We achieved a 3% boost in Bleu score over Giza++
performance on the evaluation set with these very sim-
ple classifier combination techniques, and anticipate that
research in this direction ? classifier combination of di-
versely trained SMT systems ? could yield significant
performance improvements.
Bleu Score
System 150K 500K
Trn. Sent. Trn. Sent.
Giza++/ReWrite Decoder 0.17 *
2-level Syntax Model 0.17 0.18
Results Table A: Results comparison for Arabic to English
translation on the UN corpus, with a 200-sentence evaluation
set. Note that Giza++/ReWrite cannot be run for the 500K
sentence training set; the CMU Language Modeling Toolkit,
which ReWrite uses, has a vocabulary size limit which is
exceeded in the 500K corpus.
Bleu Score
System 5K 20K
Trn. Sent. Trn. Sent.
Giza++/ReWrite Decoder 0.08 0.11
2-level Syntax Model 0.08 0.09
Results Table B: Results comparison for French to English
translation on the Canadian Hansards corpus (200-sentence
evaluation set).
7 Conclusions
We have described and implemented an original syntax-
based statistical translation model that yields baseline re-
sults which compete successfully with other state-of-the-
art SMT models. This is particularly encouraging in that
the authors are not well-versed in Arabic or French and
it appears that the quality of the rule-based phrase chun-
kers we developed in a single person-day offers substan-
tial room for improvement. We expect to be able to at-
tain improved bracketings from native speakers and, in
addition, via translingual projection of existing brack-
eters. Secondly, the lemma model we have proposed for
lexical transfer provides an efficient framework for in-
tegrating electronic dictionaries into SMT models. Al-
though we have at this time no large electronic dictionar-
ies for either Arabic or French, efforts are underway to
acquire electronic or scanned paper dictionaries for this
purpose. We did evaluate the lemma models in isola-
tion for French and Arabic without dictionary inclusion,
but in each experiment the results did not differ signifi-
cantly from the word-specific lexical transfer models, de-
spite their substantially reduced dimensionality. We an-
ticipate that the relatively seamless direct incorporation
of dictionaries into the lemma-based models will be par-
ticularly effective for translating low-density languages,
which suffer from data sparseness in the face of limited
parallel text. Finally, we incorporated lexical translation
coercion models into this full SMT framework, the in-
duction of which is a phenomenon of interest in its own
right.
8 Acknowledgements
This work was supported in part by NSF grant number IIS-9985033. In
addition, we owe many thanks to colleagues who generously lent their
time and insights. David Smith shared his tools for Arabic part-of-
speech tagging and morphological analysis and answered many ques-
tions about the Arabic language. Thanks to Skankar Kumar and San-
jeev Khudanpur for numerous helpful discussions.
9 References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning depen-
dency translation models as collections of finite state head transducers
Computational Linguistics, 26(1), 45?60.
S. Bangalore and G. Riccardi. 2000. Stochastic finite-state models for
spoken language machine translation. In Proceedings of the Workshop
on Embedded Machine Translation Systems., pp. 52?59.
P. Brown, S. Della Pietra, V. Della Pietra and R. Mercer. 1993. The
mathematics of statistical machine translation: Parameter estimation.
Computational Linguistics, 12(2), 263?312.
S. Cucerzan and D. Yarowsky. 2002. Bootstrapping a Multilingual
Part-of-speech Tagger in One Person-day. Proceedings of the Sixth
Conference on Natural Language Learning (CoNLL), Taipei, 2002.
B. Dorr and N. Habash. 2002. Interlingua approximation: A
generation-heavy approach. In Proceedings of AMTA-2002.
W. A. Gale and K. W. Church. 1991. A Program for Aligning
Sentences in Bilingual Corpora. In 29th Annual Meeting of the ACL,
Berkeley, CA.
N. Habash and B. Dorr. 2003. A categorial variation database for
English. In Proceedings of NAACL-HLT 2003
D. Jones and R. Havrilla. 1998. Twisted pair grammar: Support for
rapid development of machine translation for low density languages.
In Proceedings of AMTA98, pp. 318?332.
D. Marcu and U. Germann. 2002. The ISI ReWrite Decoder Release
0.7.0b. http://www.isi.edu/licensed-sw/rewrite-decoder/.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a
large annotated corpus of English: the Penn Treebank. Computational
Linguistics, Vol. 19.
M. Mohri, F. Pereira, and M. Riley. 1997.
ATT General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
G. Ngai and R. Florian. Transformation-based learning in the fast lane.
In Proceedings of North Americal ACL 2001, pages 40-47, June 2001.
F. J. Och and H. Ney. 2000. Improved statistical alignment models.
In Proceedings of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 440?447.
F.J. Och, C. Tillmann, H. Ney. Improved Alignment Models for
Statistical Machine Translation. In Proceedings of EMNLP 1999, pp.
20-28.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a method
for automatic evaluation of machine translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
A. Stolcke. 2002. SRILM - an extensible language modeling
toolkit. In Proceedings of the International Conference on Spo-
ken Language Processing, pages 901-904. Denver, CO, USA.
http://www.speech.sri.com/projects/srilm/.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Linguistics,
23(3), 377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL-2001, pp. 523?529.
K. Yamada and K. Night. 2002. A decoder for syntax-based statistical
MT In Proceedings of ACL-2002, pp. 303?310.
Word Translation Probabilities
Word translation for mangeait conditioned on
French Word, EnglishPOS
mangeait VBG eating 1.00
mangeait VB go 0.50
mangeait VB anticipate 0.50
mangeait VBD were 1.00
mangeait VBP knelt 1.00
mangeait NN bill 1.00
Word translation for mangeait conditioned on
French Word, English Coarse POS
mangeait V eating 0.44
mangeait V were 0.22
mangeait V knelt 0.11
mangeait V go 0.11
mangeait V anticipate 0.11
mangeait N bill 1.00
Word translation for mangeait conditioned on
French Word only
mangeait eating 0.29
mangeait were 0.14
mangeait go 0.07
mangeait bill 0.07
Word translation for mangeant
conditioned on French Word, EnglishPOS
mangeant RB mostly 1.00
mangeant JJ final 1.00
mangeant VBN obtained 1.00
mangeant VBG eating 1.00
mangeant WP who 1.00
mangeant IN through 1.00
mangeant NN lard 1.00
mangeant VBZ eats 0.50
mangeant VBZ comes 0.50
Lemma Translation Probabilities
Generation of a verb lemma given manger
manger V eat 0.60
manger V feed 0.05
manger V have 0.04
Generation of a noun lemma given manger
manger N meal 0.06
manger N trough 0.04
manger N loaf 0.04
manger N food 0.04
Generation of an adj. lemma given manger
manger J hungry 0.33
Raw lemma translation probabilities
(ignoring English Coarse POS)
manger eat 0.28
manger to 0.03
manger feed 0.03
manger out 0.02
manger have 0.02
manger are 0.02
manger , 0.02
manger you 0.01
manger meal 0.01
Table 9: Direct generation (word-to-word translation probabilities
at the various levels of backoff) is contrasted with lemma generation.
Manger (?to eat?) is a relatively rare word in the Hansards. Note that
due to low counts, the desired verb POS (target of generation) for ?eat?
may not have been observed as a translation in training data. In addi-
tion, in this situation, noisy word alignments may cause an incorrect
translation to have similar estimated translation probability. This prob-
lem is addressed by the lemma model; note the much sharper probabil-
ity distribution for verb lemmas given manger. Generation of English
inflections given lemma and target POS is algorithmic (and irregular
exceptions are handled via a lookup table).
French Wd. Eng. POS Eng. Wd. Prob.
accepter JJ unacceptable 0.12
accepter JJ acceptable 0.12
accepter JJ willing 0.11
accepter JJ ready 0.03
accepter NN acceptance 0.09
accepter NN amendment 0.03
droit VBN entitled 0.66
droit VBN allowed 0.09
droit VBN denied 0.03
droit VBN given 0.02
droit VBN permitted 0.02
droit VBN justified 0.01
droit VBN qualified 0.01
droit VBN allotted 0.01
droit VB qualify 0.14
droit VB be 0.11
droit VB have 0.09
droit VB receive 0.08
droit VB get 0.07
droit VB expect 0.03
droit VBG receiving 0.11
droit VBG getting 0.08
droit NNS rights 0.44
droit NNS benefits 0.69
Table 10: Examples of word translation coercions. Co-
ercions of the French verb accepter ?to accept? and the
French noun droit ?right? (there is parallel polysemy be-
tween the two languages for this word, but the predom-
inant sense in our corpus is the philosophical/judicial
sense, as opposed to the direction).
Eng. Phrase French Eng. Prob.
Type Phrase Phrase
NP dans le cas pre?sent a situation 0.25
NP dans le cas pre?sent the subject of debate 0.25
NP dans le cas pre?sent the position 0.25
NP dans le cas pre?sent it 0.25
VP dans le cas pre?sent should apply 1.00
ADVP dans le cas pre?sent really 1.00
PPNP dans le cas pre?sent in this case 0.63
PPNP dans le cas pre?sent in this instance 0.04
PPNP dans le cas pre?sent in this actual case 0.04
PPNP dans le cas pre?sent in this particular case 0.04
PPNP dans le cas pre?sent in that case 0.04
PPNP dans le cas pre?sent in the present circumstances 0.04
VP acceptons accept 0.48
VP acceptons agree 0.14
NP acceptons this consent 1.00
PPNP par an per year 0.67
PPNP par an in each year 0.03
PPNP par an for a year 0.03
ADVP par an annually 1.00
NP par an a year 0.79
NP par an each year 0.02
NP un discours a speech 0.83
NP un discours an address 0.05
VP un discours to speak 1.00
Table 11: Examples of direct phrase translations (see Ta-
ble 4(1)), including some coercions.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 79?82,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Models for Inuktitut-English Word Alignment
Charles Schafer and Elliott Franco Dr?abek
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{cschafer,edrabek}@cs.jhu.edu
Abstract
This paper presents a set of techniques for bitext word align-
ment, optimized for a language pair with the characteristics of
Inuktitut-English. The resulting systems exploit cross-lingual
affinities at the sublexical level of syllables and substrings, as
well as regular patterns of transliteration and the tendency to-
wards monotonicity of alignment. Our most successful systems
were based on classifier combination, and we found different
combination methods performed best under the target evalua-
tion metrics of F-measure and alignment error rate.
1 Introduction
Conventional word-alignment methods have been suc-
cessful at treating many language pairs, but may be lim-
ited in their ability to generalize beyond the Western Eu-
ropean language pairs for which they were originally
developed, to pairs which exhibit more complex diver-
gences in word order, morphology and lexical granular-
ity. Our approach to Inuktitut-English alignment was to
carefully consider the data in identifying difficulties par-
ticular to Inuktitut-English as well as possible simplify-
ing assumptions. We used these observations to construct
a novel weighted finite-state transducer alignment model
as well as a specialized transliteration model. We com-
bined these customized systems with 3 systems based
on IBM Model 4 alignments under several methods of
classifier combination. These combination strategies al-
lowed us to produce multiple submissions targeted at the
distinct evaluation measures via a precision/recall trade-
off.
2 Special Characteristics of the
Inuktitut-English Alignment Problem
Guided by the discussion of Inuktitut in Mallon (1999),
we examined the Nunavut Hansards training and hand-
labeled trial data sets in order to identify special chal-
lenges and exploitable characteristics of the Inuktitut-
English word alignment problem. We were able to iden-
tify three: (1) Importance of sublexical Inuktitut units;
(2) 1-to-N Inuktitut-to-English alignment cardinality; (3)
Monotonicity of alignments.
2.1 Types and TokensInuktitut has an extremely productive agglutinative mor-
phology, and an orthographic word may combine very
many individual morphemes. As a result, in Inuktitut-
English bitext we observe Inuktitut sentences with many
fewer word tokens than the corresponding English sen-
tences; the ratio of English to Inuktitut tokens in the
training corpus is 1.85.1 This suggests the importance of
looking below the Inuktitut word level when computing
lexical translation probabilities (or alignment affinities).
To reinforce the point, consider that the ratio of training
corpus types to tokens is 0.007 for English, and 0.194 for
Inuktitut. In developing a customized word alignment
solution for Inuktitut-English, a major goal was to han-
dle the huge number of Inuktitut word types seen only
once in the training corpus (337798 compared to 8792
for English), without demanding the development of a
morphological analyzer.
2.2 AlignmentConsidering English words in English sentence order,
4.7% of their alignments to Inuktitut were found to be
retrograde; that is, involving a decrease in Inuktitut word
position with respect to the previous English word?s
aligned Inuktitut position. Since this method of counting
retrograde alignments would assign a low count to mass
movements of large contiguous chunks, we also mea-
sured the number of inverted alignments over all pairs
of English word positions. That is, the sum
?e?a=|e|?1a=1 ?
b=|e|
b=a+1?i1?I(e,a)?i2?I(e,b)(1 if i1 > i2)was computed over all Inuktitut alignment sets I(e, x),
for e the English sentence and x the English word po-
sition. Dividing this sum by the obvious denominator
(replacing (1 if i1 > i2) with (1) in the sum) yielded avalue of 1.6% inverted alignments.
Table 1 shows a histogram of alignment cardinalities
for both English and Inuktitut. Ninety-four percent of
English word tokens, and ninety-nine percent of those
having a non-null alignment, align to exactly one Inuk-
titut word. In development of a specialized word aligner
for this language pair (Section 3), we made use of the
observed reliability of these two properties, monotonic-
ity and 1-to-N cardinality.
3 Alignment by Weighted Finite-State
Transducer Composition
We designed a specialized alignment system to handle
the above-mentioned special characteristics of Inuktitut-
1Though this ratio increases to 2.21 when considering only longer
sentences (20 or more English words), ignoring common short, formu-
laic sentence pairs such as ( Hudson Bay ) ( sanikiluaq ) .
79
% Words Having Specified Alignment Cardinality
NULL 1 2 3 4 5 6 7
English 5 94 <1 <1 0 0 0 0
Inuktitut 3 43 20 14 10 5 3 2
Table 1: Alignment cardinalities for English-Inuktitut word
alignment, computed over the trial data.
English alignment. Our weighted finite-state transducer
(WFST) alignment model, illustrated in Figure 1, struc-
turally enforces monotonicity and 1-to-N cardinality, and
exploits sublexical information by incorporating associ-
ation scores between English words and Inuktitut word
substrings, based on co-occurrence in aligned sentences.
For each English word, an association score was com-
puted not only with each Inuktitut word, but also with
each Inuktitut character string of length ranging from
2 to 10 characters. This is similar to the technique de-
scribed in Martin et al (2003) as part of their construc-
tion of a bilingual glossary from English-Inuktitut bi-
text. However, our goal is different and we keep all the
English-Inuktitut associations, rather than selecting only
the ?best? ones using a greedy method, as do they. Addi-
tionally, before extracting all substrings from each Inuk-
titut word, we added a special character to the word?s
beginning and end (e.g., makkuttut ? makkuttut ), in
order to exploit any preferences for word-initial or -final
placement.
The heuristic association score chosen was
p(worde |wordi) ? p(wordi |worde), computed over allthe aligned sentence pairs. We have in the past observed
this to be a useful indicator of word association, and it
has the nice property of being in the range (0,1].
The WFST aligner is a composition of 4 transduc-
ers.2 The structure of the entire WFST composition en-
forces monotonicity, Inuktitut-to-English 1-N cardinal-
ity, and Inuktitut word fertilities ranging between 1 and
7. This model was implemented using the ATT finite-
state toolkit (Mohri et al, 1997). In Figure 1, [1] is
a linear transducer mapping each English position in a
particular English test sentence to the word at that posi-
tion. It is constructed so as to force each English word
to participate in exactly 1 alignment. [2] is a single-state
transducer mapping English word to Inuktitut substrings
(or full words) with weights derived from the association
scores.3 [3] is a transducer mapping Inuktitut substrings
(and full words) to their position in the Inuktitut test sen-
tence. Its construction allows a single Inuktitut position
to correspond to multiple English positions, while en-
forcing monotonicity. [4] is a transducer regulating the
allowed ?fertility? values of Inuktitut words; each Inuk-
titut word is permitted a fertility of between 1 and 7. The
fertility values are assigned the probabilities correspond-
ing to observed relative frequencies in the trial data, and
2Bracketed numbers in the following discussion refer to the compo-
nent transducers as illustrated in Figure 1.
3Transducers [2] and [4] are shared across all sentence decodings.
1
1 1
1 1 1
<epsilon> (0.82)
<epsilon> 
(1.56)
<epsilon> 
(1.90)
<epsilon> 
<epsilon> 
<epsilon> 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
illug/1
_pijjut/1
atuqa/2
_inna/2
_amm/3
_am/3
kkutt/4
_makku/4
ajunga/5
akainnar/5
<epsilon><epsilon> <epsilon> <epsilon> <epsilon>
_pijjutigillugu_/1 _innatuqait_/2 _amma_/3 _makkuttut_/4 _uqausiqakainnarumajunga_/5
1/in 2/regards 3/to 4/elders
. . .
6/youth5/and
in regards to elders and youth i want to make general comments  
pijjutigillugu innatuqait amma makkuttut uqausiqakainnarumajunga
and/_am (.54)
youth/_makku (1.10)
youth/_makkuttut_ (3.89)
regards/_pijjutigillugu_ (3.49)
regards/_pijjuti (2.98)
.
.
.
.
.
.
[1]
elders/_inna (0.90)
elders/_innat (1.09)
general/_uqausi (4.54)
and/_amma (.49)
and/_amm (.49)
. . .
[2]
[3]
[4]
(1.90)
2
2 2
2 2 2
<epsilon> (0.82)
<epsilon> 
(1.56)
<epsilon> 
<epsilon> 
<epsilon> 
<epsilon> 
. . .
Figure 1: WFST alignment system in composition order, in-
stantiated for an example sentence from the development (trial)
data. To save space, only a representative portion of each ma-
chine is drawn. Transition weights are costs in the tropical
(min,+) semiring, derived from negative logs of probabilities
and association scores. Nonzero costs are indicated in paren-
theses.
are not conditioned on the identity of the Inuktitut word.
4 English-Inuktitut Transliteration
Although in this corpus English and Inuktitut are both
written in Roman characters, English names are signifi-
cantly transformed when rendered in Inuktitut text. Con-
sider the following English/Inuktitut pairs from the train-
ing corpus: Chartrand/saaturaan, Chretien/kurittian
and the set of training corpus-attested Inuktitut render-
ings of Williams, Campbell, and McLean shown in Ta-
ble 2(A) (which does not include variations containing
the common -mut lexeme, meaning ?to [a person]? (Mal-
lon, 1999)).
Clearly, not only does the English-to-Inuktitut trans-
formation radically change the name string, it does so
in a nondeterministic way which appears to be influ-
enced not only by the phonological preferences of Inuk-
titut but also by differing pronunciations of the name in
question and possibly by differing conventions of trans-
lators (note, for example, maklain versus mikliin for
McLean).
We trained a probabilistic finite-state transducer
(FST) to identify English-Inuktitut transliterated pairs
in aligned sentences. Training string pairs were ac-
quired from the training bitext in the following manner.
Whenever single instances of corresponding honorifics
were found in a sentence pair ? these included the cor-
respondences (Ms , mis); (Mrs , missa/missis); (Mr ,
80
(A) (B)
Williams McLean k sh
ailiams makalain k -4.2 s -7.2
uialims makkalain q -6.2
uilialums maklaain w
uiliam maklain b ui -5.8
uiliammas maklainn p -4.3 v -6.1
uiliams maklait v -5.0
uilians makli o
uliams maklii z a -4.2
viliams makliik j -5.2 aa -4.6
makliin s -5.8 uu -4.9
Campbell maklin u -5.1
kaampu malain ch
kaampul matliin s -5.6 u
kaamvul miklain k -6.8 uu -5.5
kamvul mikliin u -5.6
miklin a -6.2
Table 2: (A) Training-corpus-attested renderings of Williams,
Campbell, and McLean. (B) Top learned Inuktitut substi-
tutions and their log probabilities for several English (shown
underlined) orthographic characters (and character sequences).
Where top substitutions for English characters are shown, none
equal or better were omitted.
mista/mistu) ? the immediately following capitalized En-
glish words (up to 2) were extracted and the same num-
ber of Inuktitut words were extracted to be used as train-
ing pairs. Thus, given the appearance in aligned sen-
tences of ?Mr. Quirke? and ?mista kuak?, the training
pair (Quirke,kuak) would be extracted. Common dis-
tractions such as ?Mr Speaker? were filtered out. In or-
der to focus on the native English name problem (Inuk-
titut name rendering into English is much less noisy) the
English extractions were required to have appeared in a
large, news-corpus-derived English wordlist. This pro-
cedure resulted in a conservative, high-quality list of 434
unique name pairs. The probabilistic FST model we se-
lected was that of a memoryless (single-state) transducer
representing a joint distribution over character substitu-
tions, English insertions, and Inuktitut insertions. This
model is identical to that presented in Ristad and Yianilos
(1997). Prior to training, common English digraphs (e.g.,
?th? and ?sh?) were mapped to unique single characters,
as were doubled consonants. Inuktitut ?ng? and common
two-vowel sequences were also mapped to unique single
characters to elicit higher-quality results from the memo-
ryless transduction model employed. Some results of the
transducer training are displayed in Table 2(B). Proba-
bilistic FST weight training was accomplished using the
Dyna modeling language and DynaMITE parameter op-
timization toolkit (Eisner et al 2004). The translitera-
tion modeling described here differs from such previous
transliteration work as Stalls and Knight (1998) in that
there is no explicit modeling of pronunciation, only a di-
rect transduction between written forms.
In applying transliteration on trial/test data, the
following criteria were used to select English words for
transliteration: (1) Word is capitalized (2) Word is not in
the exclusion list.4 For the top-ranked transliteration of
the English word present in the Inuktitut sentence, all
occurrences of that word in that sentence are marked as
aligned to the English word.
We have yet to evaluate English-Inuktitut translitera-
tion in isolation on a large test set. However, accuracy
on the workshop trial data was 4/4 hypotheses correct,
and on test data 2/6 correct. Of the 4 incorrect test
hypotheses, 2 were mistakes in identifying the correct
transliteration, and 2 mistakes resulted from attempting
to transliterate an English word such as ?Councillors?
which should not be transliterated. Even with a rela-
tively low accuracy, the transliteration model, which is
used only as an individual voter in combination systems,
is unlikely to vote for the incorrect choice of another sys-
tem. Its purpose under system combination is to push a
good alignment link hypothesis up to the required vote
threshold.5
5 IBM Model 4 Alignments
As a baseline and contributor to our combination sys-
tems, we ran GIZA++ (Och and Ney, 2000), to produce
alignments based on IBM Model 4. The IBM align-
ment models are asymmetric, requiring that one lan-
guage be idenitifed as the ?e? language, whose words
are allowed many links each, and the other as the ?f? lan-
guage, whose words are allowed at most one link each.
Although the observed alignment cardinalities naturally
suggest identifying Inuktitut as the ?e? language and En-
glish as the ?f? language, we ran both directions for com-
pleteness.
As a crude first attempt to capture sublexical corre-
spondences in the absence of a method for morpheme
segmentation, we developed a rough syllable segmenter
(spending approximately 2 person-hours), ran GIZA++
to produce alignments treating the syllables as words,
and chose, for each English word, the Inuktitut word or
words the largest number of whose syllables were linked
to it.
In the nomenclature of our results tables, giza++ syl-
labized refers to the latter system, giza++ E(1)-I(N) rep-
resents GIZA++ run with English as the ?e? language,
and giza++ E(N)-I(1) sets English as the ?f? language.
6 System Performance and Combination
Methods
We observed the 4 main systems (3 GIZA++ variants and
WFST) to have significantly different performance pro-
files in terms of precision and recall. Consistently, WFST
4Exclusion list was compiled as follows: (a) capitalized words in
2000 randomly selected English training sentences were examined,
Words such as Clerk, Federation, and Fisheries, which are frequently
capitalized but should not be transliterated, were put into the exclusion
list; in addition, any word with frequency > 50 in the training corpus
was excluded, on the rationale that common-enough words would have
well-estimated translation probabilities already. 50 may seem like a
high threshold until one considers the high variability of the transliter-
ation process as demonstrated in Table 2(A).
5Refer to Section 6 for detailed descriptions of voting.
81
SYSTEM P R F AER |H|/|T |
Individual system performance Trial Data
giza++ E(1)-I(N) 63.4 26.6 37.5 32.9 0.42
giza++ E(N)-I(1) 68.2 59.4 63.5 28.6 0.87
giza++ syllabized 83.6 44.5 58.1 18.3 0.53
WFST 70.3 72.7 71.5 27.8 1.03
Combination system performance Trial Data
F/AER Emphasis 85.4 63.5 72.9 12.3 0.74
AER Emphasis (1) 92.6 44.2 59.9 8.8 0.48
AER Emphasis (2) 95.1 38.0 54.3 9.5 0.40
F Emphasis 74.8 77.6 76.2 21.9 1.04
Recall Emphasis 66.9 82.1 73.8 28.9 1.23
Individual system performance Test Data
giza++ E(1)-I(N) 49.7 18.6 27.0 45.2 0.37
giza++ E(N)-I(1) 64.6 56.2 60.1 32.7 0.87
giza++ syllabized 84.9 44.0 57.9 15.6 0.52
WFST 65.4 68.3 66.8 33.7 1.04
(submitted) Combination system performance Test Data
F/AER Emphasis 84.4 58.6 69.2 14.3 0.69
AER Emphasis (1) 90.7 39.4 54.9 11.5 0.43
AER Emphasis (2) 96.7 32.3 48.4 9.5 0.33
F Emphasis 70.7 73.8 72.2 26.7 1.04
Recall Emphasis 62.6 81.7 70.1 34.2 1.31
Table 3: System performance evaluated on trial and test data.
The precision, recall and F-measure cited are the unlabeled
version (?probable,? in the nomenclature of this shared task).
The gold standard truth for trial data contained 710 alignments.
The test gold standard included 1972 alignments. The column
|H|/|T | lists ratio of hypothesis set size to truth set size for each
system.
won out on F-measure while giza++ syllabized attained
better alignment error rate (AER). Refer to Table 3 for
details of performance on trial and test data.
We investigated a number of system combination
methods, three of which were finally selected for use
in submitted systems. There were two basic methods of
combination: per-link voting and per-English-word vot-
ing.6 In per-link voting, an alignment link is included if
it is proposed by at least a certain number of the partic-
ipating individual systems. In per-English-word voting,
the best outgoing link is chosen for each English word
(the link which is supported by the greatest number of in-
dividual systems). Any ties are broken using the WFST
system choice. A high-recall variant of per-English-word
voting was included in which ties at vote-count 1 (in-
dicating a low-confidence decision) are not broken, but
rather all systems? choices are submitted as hypotheses.
The transliteration model described in Section 4 was
included as a voter in each combination system, though it
made few hypotheses (6 on the test data). Composition of
the submitted systems was as follows: F/AER Empha-
6Combination methods we elected not to submit included voting
with trained weights and various stacked classifiers. The reasoning was
that with such a small development data set ? 25 sentences ? it was
unsafe to put faith in any but the simplest of classifier combination
schemes.
sis - per-link voting with decision criterion >= 2 votes,
over all 5 described systems (WFST, 3 GIZA++ vari-
ants, transliteration). AER Emphasis (I) per-link voting,
>= 2 votes, over all systems except giza++ E(N)-I(1).
AER Emphasis (II) per-link voting, >= 3 votes, over
all systems. F Emphasis per-English-word voting, over
all systems, using WFST as tiebreaker. Recall Empha-
sis per-English-word voting, over all systems, high-recall
variant.
We elected to submit these systems because each
tailors to a distinct evaluation criterion (as suggested
by the naming convention). Experiments on trial data
convinced us that minimizing AER and maximizing F-
measure in a single system would be difficult. Mini-
mizing AER required such high-precision results that the
tradeoff in recall greatly lowered F-measure. It is inter-
esting to note that system combination does provide a
convenient means for adjusting alignment precision and
recall to suit the requirements of the problem or evalua-
tion standard at hand.
7 Conclusions
We have presented several individual and combined sys-
tems for word alignment of Inuktitut-English bitext. The
most successful individual systems were those targeted
to the specific characteristics of the language pair. The
combined systems generally outperformed the individual
systems, and different combination methods were able to
optimize for performance under different evaluation met-
rics. In particular, per-English-word voting performed
well on F-measure, while per-link voting performed well
on AER.
Acknowledgements: Many thanks to Eric Goldlust, David
Smith, and Noah Smith for help in using the Dyna language.
References
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declarative
language for implementing dynamic programs. In Proceedings of the
42nd Annual Meeting of the Association for Computational Linguistics
(ACL 2004), Companion Volume, pages 218-221.
M. Mallon. 1999. Inuktitut linguistics for technocrats. Technical re-
port, Ittukuluuk Language Programs, Iqaluit, Nunavut, Canada.
J. Martin, H. Johnson, B. Farley, and A. Maclachlan. 2003. Align-
ing and using an English-Inuktitut parallel corpus. In Proceedings of
Workshop on Building and Using Parallel Texts: Data Driven Machine
Translation and Beyond, HLT-NAACL 2003.
M. Mohri, F. Pereira, and M. Riley. 1997.
ATT General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2000. Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440?447.
E. S. Ristad and P. N. Yianilos. 1997. Learning string edit distance. In
Machine Learning: Proceedings of the Fourteenth International Con-
ference, pages 287?295.
B. Stalls and K. Knight. 1998. Translating names and technical terms
in arabic text. In Proceedings of the COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages.
82
