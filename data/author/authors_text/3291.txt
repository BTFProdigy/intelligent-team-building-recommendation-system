147
148
149
150
151
152
153
154
187
188
189
190
A Spoken Dialogue Interface to a Geologist?s Field Assistant
John Dowding and James Hieronymus
Mail Stop T-27A-2
NASA Ames Research Center
Moffett Field, CA 94035-1000
{jdowding,jimh}@riacs.edu
Abstract
We will demonstrate a spoken dialogue inter-
face to a Geologist?s Field Assistant that is
being developed as part of NASA?s Mobile
Agents project. The assistant consists of a
robot and an agent system which helps an as-
tronaut wearing a planetary space suit while
conducting a geological exploration. The pri-
mary technical challanges relating to spoken
dialogue systems that arise in this project are
speech recognition in noise, open-microphone,
and recording voice annotations. This system
is capable of discriminating between speech in-
tended for the system and for other purposes.
1 Introduction
The Geologist?s Field Assistant is one component of Mo-
bile Agents, a NASA project studying technologies, tech-
niques, and work practices for sophisticated human-agent
and human-robot cooperation in space and planetary ex-
ploration environments, such as the surface of Mars. The
evolution, development and evaluation of this project oc-
curs in a series of increasingly complex field tests in Mars
analog environments (deserts and artic sites) on Earth.
The Spoken Dialogue component assists an astronaut
wearing a space suit while conducting a geological ex-
ploration, by tagging samples by spoken discriptions,
commanding the taking of pictures, recording descriptive
voice annotations, and tracking the associations between
these samples, images, and annotations. The assistant
will also help track the astronaut?s location and progress
though the survey, and help track their body exertion level
(heart and respiration rate).
The Spoken Dialogue interface is one of several of Mo-
bile Agents, each with different goals and evaluation met-
rics. Other components include: Brahms work-practice
modelling and simulation system (NASA Ames); MEX
mobile wireless networking (Ames); robots (Johnson
Space Center (JSC) and Georgia Tech); Spacesuits (JSC)
Start tracking my g p s coordinates.
Start logging my bio sensors every fifteen seconds.
Where is my my current location?
Call this location Asparagus.
Create a new sample bag and label it sample bag three.
Take a voice note
Please begin recording voice note now:
This sample originated in a dry creek bed. [pause]
Would you like to continue recording the voice note?
no
Voice note has been created.
Associate that voice note with sample bag three.
Play the voice note associated with sample bag three.
Table 1: Example utterances
and Biomedical sensors (Stanford University); Satellite
Internet services (Goddard Space Flight Center); and Ge-
ologists (US Geological Survey).
The primary technical challanges relating to spoken
dialogue systems that arise in this project are open-
microphone speech recognition and understanding which
decides which agent receives, and responds to a particular
utterance and space suit noise.
2 Example Dialogue
The language capabilities developed so far are largely di-
rect commanding with the user controlling task initiative.
A sample of user commands is given in Table 1. A system
response is always given, but is usually omitted below for
the sake of brevity. When given, the system response ap-
pears in italics.
3 Architecture
This spoken dialogue system shares a common architec-
ture with several prior systems: CommandTalk (Stent et
al., 1999), PSA (Rayner et al, 2000), WITAS (Lemon et
al., 2001), and the Intelligent Procedure Assistant (Aist
et al, 2002). The architecure has been well described in
                                                               Edmonton, May-June 2003
                                                             Demonstrations , pp. 9-10
                                                         Proceedings of HLT-NAACL 2003
prior work. The critical feature of the architecture rel-
evant to this work is the use of a grammar-based lan-
guage model for speech recognition that is automatically
derived from the same Unification Grammar that is used
for parsing and interpretation.
4 Data Collection
The Mobile Agents project conducted two field tests in
2002: a one week dress rehearsal at JSC in the Mars yard
in May, and a two week field test in the Arizona desert
in September, split between two sites of geological inter-
est, one near the Petrified Forest National Park, and the
other on the ejecta field at Meteor Crater. We collected
approxmimately 5,000 recorded sound files from 8 sub-
jects during the September tests, some from space-suit
subjects, and the rest in shirt-sleeve walk-throughs (still
a high wind condition). We transcribed 1059 wave files.
All conditions were performed open-mic and all sounds
that were picked up by the microphone were recorded, so
not all of these files contained within-domain utterances
intended. Of the transcribed sound files, 208 contained
no speech (mostly wind noise) and 243 contained out-of-
domain speech that was intended for other hearers. That
left 608 within-domain utterances that were split 80%-
20% into test and training utterances.
5 Technical Challanges
The Geologist?s Field Assitant requires the ability to
make voice notes that can be stored and transmitted.
We implemented this by adding a recording mode to
the speech recognizer agent, and temporarily increas-
ing the speech end-pointing interval. This allows us to
record multi-sentence voice notes without treating inter-
sentence pauses as end-of-voice-note markers. Entering
recording mode is triggered by specific speach acts like
Take a voice note or Annotate sample bag one.
When considering recognition accuracy in the open-
mic condition, we consider additional metrics beyond
word-error rate (WER). Since the recognizer can fail to
find a hypothesis for an utterance, we compute the false-
rejection rate (FREJ) for within-domain utterances and
adjusted word-error (AWER) counting only the word er-
rors on the non-rejected utterances. We also consider
misrecognitions of out-of-domain utterance as within-
domain, and compute the false-accept rate (FACC). Table
2 gives the performance results for the grammar-based
language model that was used in the September test. This
model gives reasonable performance on within-domain
utterances, but falsely accepts 25.5% of out-of-domain
utterances. After the September test, we used the training
data we had collected to build a Probabilistic Context-
Free Grammar using the compute-grammar-probs
tool that comes with Nuance (Nuance, 2002). Using only
485 utterances of training data, there was improvement
in both the AWER and FACC rates, resulting in a lan-
guage model where both FREF and FACC were under
10%. There was also a substantial improvement in recog-
nition speed, as measured in multiples of CPU real-time.
Version WER FREJ AWER FACC xCPUrt
(%) (%) (%) (%) (%)
Baseline CFG Language Model
Training 12.56 4.54 7.72 ? 58.9
Test 9.5 3.25 7.5 25.5 57.5
Probabilistic CFG Language Model
Training 9.97 5.57 4.6 ? 19.4
Test 8.99 7.32 3.7 9.09 19.0
Table 2: Comparing Baseline and Probabilistic CFG
6 Conclusions
We will demonstrate a dialogue system that has an im-
proved ability to discriminate between speech that is in-
tended for different purposes, treating some as data ob-
jects to be saved, and others identified as being out-of-
domain. With probabilities on the rules the system has an
acceptably low false accept rate and is fast and accurate.
References
G. Aist, J. Dowding, B.A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astro-
naut training and support. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (demo track), Philadelphia, PA.
O. Lemon, A. Bracy, A. Gruenstein, and S. Peters. 2001.
Multimodal dialogues with intelligent agents in dy-
namic environments: the WITAS conversational in-
terface. In Proceedings of 2nd Meeting of the North
American Association for Computational Linguistics,
Pittsburgh, PA.
Nuance, 2002. http://www.nuance.com. As of 15
November 2002.
M. Rayner, B.A. Hockey, and F. James. 2000. A com-
pact architecture for dialogue management based on
scripts and meta-outputs. In Proceedings of the 6th Ap-
plied Natural Language Processing Conference, Seat-
tle, WA.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of the Thirty-Seventh Annual Meeting of
the Association for Computational Linguistics, pages
183?190.
An Intelligent Procedure Assistant
Built Using REGULUS 2 and ALTERF
Manny Rayner, Beth Ann Hockey, Jim Hieronymus, John Dowding, Greg Aist
Research Institute for Advanced Computer Science (RIACS)
NASA Ames Research Center
Moffet Field, CA 94035
{mrayner,bahockey,jimh,jdowding,aist}@riacs.edu
Susana Early
DeAnza College/NASA Ames Research Center
searly@mail.arc.nasa.gov
Abstract
We will demonstrate the latest version of
an ongoing project to create an intelli-
gent procedure assistant for use by as-
tronauts on the International Space Sta-
tion (ISS). The system functionality in-
cludes spoken dialogue control of nav-
igation, coordinated display of the pro-
cedure text, display of related pictures,
alarms, and recording and playback of
voice notes. The demo also exempli-
fies several interesting component tech-
nologies. Speech recognition and lan-
guage understanding have been devel-
oped using the Open Source REGULUS
2 toolkit. This implements an approach
to portable grammar-based language mod-
elling in which all models are derived
from a single linguistically motivated uni-
fication grammar. Domain-specific CFG
language models are produced by first
specialising the grammar using an au-
tomatic corpus-based method, and then
compiling the resulting specialised gram-
mars into CFG form. Translation between
language centered and domain centered
semantic representations is carried out by
ALTERF, another Open Source toolkit,
which combines rule-based and corpus-
based processing in a transparent way.
1 Introduction
Astronauts aboard the ISS spend a great deal of their
time performing complex procedures. This often in-
volves having one crew member reading the proce-
dure aloud, while while the other crew member per-
forms the task, an extremely expensive use of as-
tronaut time. The Intelligent Procedure Assistant is
designed to provide a cheaper alternative, whereby a
voice-controlled system navigates through the pro-
cedure under the control of the astronaut perform-
ing the task. This project has several challenging
features including: starting the project with no tran-
scribed data for the actual target input language, and
rapidly changing coverage and functionality. We
are using REGULUS 2 and ALTERF to address these
challenges. Together, they provide an example-
based framework for constructing the portion of the
system from recognizer through intepretation that
allows us to make rapid changes and take advan-
tage of both rule-base and corpus-based information
sources. In this way, we have been able to extract
maximum utility out of the small amounts of data
initial available to the project and also smoothly ad-
just as more data has been accumulated in the course
of the project.
The following sections describe the procedure as-
sistant application and domain, REGULUS 2 and AL-
TERF.
2 Application and domain
The system, an early version of which was described
in (Aist et al, 2002), is a prototype intelligent voice
enabled personal assistant, intended to support astro-
nauts on the International Space Station in carrying
out complex procedures. The first production ver-
sion is tentatively scheduled for introduction some
time during 2004. The system reads out each pro-
cedure step as it reaches it, using a TTS engine, and
also shows the corresponding text and supplemen-
tary images in a visual display. Core functionality
consists of the following types of commands:
? Navigation: moving to the following step or
substep (?next?, ?next step?, ?next substep?),
going back to the preceding step or substep
(?previous?, ?previous substep?), moving to a
named step or substep (?go to step three?, ?go
to step ten point two?).
? Visiting non-current steps, either to preview fu-
ture steps or recall past ones (?read step four?,
?read note before step nine?). When this func-
tionality is invoked, the non-current step is dis-
played in a separate window, which is closed
on returning to the current step.
? Recording, playing and deleting voice notes
(?record voice note?, ?play voice note on step
three point one?, ?delete voice note on substep
two?).
? Setting and cancelling alarms (?set alrm for
five minutes from now?, ?cancel alarm at ten
twenty one?).
? Showing or hiding pictures (?show the small
waste water bag?, ?hide the picture?).
? Changing the TTS volume (?increase/decrease
volume?).
? Querying status (?where are we?, ?list voice
notes?, ?list alarms?).
? Undoing and correcting commands (?go back?,
?no I said increase volume?, ?I meant step
four?).
The system consists of a set of modules, written
in several different languages, which communicate
with each other through the SRI Open Agent Ar-
chitecture (Martin et al, 1998). Speech recogni-
tion is carried out using the Nuance Toolkit (Nuance,
2003).
3 REGULUS 2
REGULUS 2 (Rayner et al, 2003; Regulus, 2003)
is an Open Source environment that supports effi-
cient compilation of typed unification grammars into
speech recognisers. The basic intent is to provide
a set of tools to support rapid prototyping of spo-
ken dialogue applications in situations where little
or no corpus data exists. The environment has al-
ready been used to build over half a dozen appli-
cations with vocabularies of between 100 and 500
words.
The core functionality provided by the REGU-
LUS 2 environment is compilation of typed unifi-
cation grammars into annotated context-free gram-
mar language models expressed in Nuance Gram-
mar Specification Language (GSL) notation (Nu-
ance, 2003). GSL language models can be con-
verted into runnable speech recognisers by invoking
the Nuance Toolkit compiler utility, so the net result
is the ability to compile a unification grammar into
a speech recogniser.
Experience with grammar-based spoken dialogue
systems shows that there is usually a substantial
overlap between the structures of grammars for dif-
ferent domains. This is hardly surprising, since they
all ultimately have to model general facts about the
linguistic structure of English and other natural lan-
guages. It is consequently natural to consider strate-
gies which attempt to exploit the overlap between
domains by building a single, general grammar valid
for a wide variety of applications. A grammar of this
kind will probably offer more coverage (and hence
lower accuracy) than is desirable for any given spe-
cific application. It is however feasible to address
the problem using corpus-based techniques which
extract a specialised version of the original general
grammar.
REGULUS implements a version of the grammar
specialisation scheme which extends the Explana-
tion Based Learning method described in (Rayner
et al, 2002). There is a general unification gram-
mar, loosely based on the Core Language Engine
grammar for English (Pulman, 1992), which has
been developed over the course of about ten individ-
ual projects. The semantic representations produced
by the grammar are in a simplified version of the
Core Language Engine?s Quasi Logical Form nota-
tion (van Eijck and Moore, 1992).
A grammar built on top of the general grammar is
transformed into a specialised Nuance grammar in
the following processing stages:
1. The training corpus is converted into a ?tree-
bank? of parsed representations. This is done
using a left-corner parser representation of the
grammar.
2. The treebank is used to produce a specialised
grammar in REGULUS format, using the EBL
algorithm (van Harmelen and Bundy, 1988;
Rayner, 1988).
3. The final specialised grammar is compiled into
a Nuance GSL grammar.
4 ALTERF
ALTERF (Rayner and Hockey, 2003) is another Open
Source toolkit, whose purpose is to allow a clean
combination of rule-based and corpus-driven pro-
cessing in the semantic interpretation phase. There
is typically no corpus data available at the start
of a project, but considerable amounts at the end:
the intention behind ALTERF is to allow us to shift
smoothly from an initial version of the system which
is entirely rule-based, to a final version which is
largely data-driven.
ALTERF characterises semantic analysis as a task
slightly extending the ?decision-list? classification
algorithm (Yarowsky, 1994; Carter, 2000). We start
with a set of semantic atoms, each representing a
primitive domain concept, and define a semantic
representation to be a non-empty set of semantic
atoms. For example, in the procedure assistant do-
main we represent the utterances
please speak up
show me the sample syringe
set an alarm for five minutes from now
no i said go to the next step
respectively as
{increase volume}
{show, sample syringe}
{set alrm, 5, minutes}
{correction, next step}
where increase volume, show,
sample syringe, set alrm, 5, minutes,
correction and next step are semantic
atoms. As well as specifying the permitted semantic
atoms themselves, we also define a target model
which for each atom specifies the other atoms with
which it may legitimately combine. Thus here, for
example, correction may legitimately combine
with any atom, but minutes may only combine
with correction, set alrm or a number.1.
Training data consists of a set of utterances, in
either text or speech form, each tagged with its in-
tended semantic representation. We define a set of
feature extraction rules, each of which associates an
utterance with zero or more features. Feature ex-
traction rules can carry out any type of processing.
In particular, they may involve performing speech
recognition on speech data, parsing on text data, ap-
plication of hand-coded rules to the results of pars-
ing, or some combination of these. Statistics are
then compiled to estimate the probability p(a | f)
of each semantic atom a given each separate feature
f , using the standard formula
p(a | f) = (Naf + 1)/(Nf + 2)
where Nf is the number of occurrences in the train-
ing data of utterances with feature f , and N af is the
number of occurrences of utterances with both fea-
ture f and semantic atom a.
The decoding process follows (Yarowsky, 1994)
in assuming complete dependence between the fea-
tures. Note that this is in sharp contrast with the
Naive Bayes classifier (Duda et al, 2000), which as-
sumes complete independence. Of course, neither
assumption can be true in practice; however, as ar-
gued in (Carter, 2000), there are good reasons for
preferring the dependence alternative as the better
option in a situation where there are many features
extracted in ways that are likely to overlap.
We are given an utterance u, to which we wish to
assign a representation R(u) consisting of a set of
semantic atoms, together with a target model com-
prising a set of rules defining which sets of seman-
1The current system post-processes Alterf semantic atom
lists to represent domain dependancies between semantic
atoms more directly before passing on the result. e.g.
(correction, set alrm, 5, minutes) is repack-
aged as (correction(set alrm(time(0,5))))
tic atoms are consistent. The decoding process pro-
ceeds as follows:
1. Initialise R(u) to the empty set.
2. Use the feature extraction rules and the statis-
tics compiled during training to find the set of
all triples ?f, a, p? where f is a feature associ-
ated with u, a is a semantic atom, and p is the
probability p(a | f) estimated by the training
process.
3. Order the set of triples by the value of p, with
the largest probabilities first. Call the ordered
set T .
4. Remove the highest-ranked triple ?f, a, p? from
T . Add a to R(u) iff the following conditions
are fulfilled:
? p ? pt for some pre-specified threshold
value pt.
? Addition of a to R(u) results in a set
which is consistent with the target model.
5. Repeat step (4) until T is empty.
Intuitively, the process is very simple. We just
walk down the list of possible semantic atoms, start-
ing with the most probable ones, and add them to
the semantic representation we are building up when
this does not conflict with the consistency rules in
the target model. We stop when the atoms suggested
are too improbable, that is, they have probabilies be-
low a cut-off threshold.
5 Summary and structure of demo
We have described a non-trivial spoken language di-
alogue application built using generic Open Source
tools that combine rule-based and corpus-driven
processing. We intend to demo the system with par-
ticular reference to these tools, displaying intermedi-
ate results of processing and showing how the cover-
age can be rapidly reconfigured in an example-based
fashion.
References
G. Aist, J. Dowding, B.A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astro-
naut training and support. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (demo track), Philadelphia, PA.
D. Carter. 2000. Choosing between interpretations. In
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors, The Spoken Language Translator.
Cambridge University Press.
R.O. Duda, P.E. Hart, and H.G. Stork. 2000. Pattern
Classification. Wiley, New York.
D. Martin, A. Cheyer, and D. Moran. 1998. Building
distributed software systems with the open agent ar-
chitecture. In Proceedings of the Third International
Conference on the Practical Application of Intelligent
Agents and Multi-Agent Technology, Blackpool, Lan-
cashire, UK.
Nuance, 2003. http://www.nuance.com. As of 25 Febru-
ary 2003.
S.G. Pulman. 1992. Syntactic and semantic process-
ing. In H. Alshawi, editor, The Core Language En-
gine, pages 129?148. MIT Press, Cambridge, Mas-
sachusetts.
M. Rayner and B.A. Hockey. 2003. Transparent com-
bination of rule-based and data-driven approaches in a
speech understanding architecture. In Proceedings of
the 10th EACL, Budapest, Hungary.
M. Rayner, B.A. Hockey, and J. Dowding. 2002. Gram-
mar specialisation meets language modelling. In Pro-
ceedings of the 7th International Conference on Spo-
ken Language Processing (ICSLP), Denver, CO.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner. 1988. Applying explanation-based general-
ization to natural-language processing. In Proceedings
of the International Conference on Fifth Generation
Computer Systems, pages 1267?1274, Tokyo, Japan.
Regulus, 2003. http://sourceforge.net/projects/regulus/.
As of 24 April 2003.
J. van Eijck and R. Moore. 1992. Semantic rules for
English. In H. Alshawi, editor, The Core Language
Engine, pages 83?116. MIT Press.
T. van Harmelen and A. Bundy. 1988. Explanation-
based generalization = partial evaluation (research
note). Artificial Intelligence, 36:401?412.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution. In Proceedings of the 32nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 88?95, Las Cruces, New Mexico.
A procedure assistant for astronauts
in a functional programming architecture,
with step previewing and spoken correction of dialogue moves
Gregory Aist
1
, Manny Rayner
1
, John Dowding
1
,
Beth Ann Hockey
1
, Susana Early
2
, and Jim Hieronymus
3
1
Research Institute for Advanced Computer Science
2
Foothill/DeAnza College
3
NASA Ames Research Center
M/S T35B-1, Moffett Field CA 94035
{aist, mrayner, jdowding, bahockey, jimh}@riacs.edu; searly@mail.arc.nasa.gov
Abstract
We present a demonstration of a proto-
type system aimed at providing support
with procedural tasks for astronauts on
board the International Space Station.
Current functionality includes navigation
within the procedure, previewing steps,
requesting a list of images or a particular
image, recording voice notes and spoken
alarms, setting parameters such as audio
volume. Dialogue capabilities include
handling spoken corrections for an entire
dialogue move, reestablishing context in
response to a user request, responding to
user barge-in, and help on demand. The
current system has been partially reim-
plemented for better efficiency and in re-
sponse to feedback from astronauts and
astronaut training personnel. Added fea-
tures include visual and spoken step pre-
viewing, and spoken correction of
dialogue moves. The intention is to intro-
duce the system into astronaut training as
a prelude to flight on board the Interna-
tional Space Station.
1 Introduction
Astronauts on board the International Space Sta-
tion engage in a wide variety of tasks on orbit in-
cluding medical procedures, extra vehicular
activity (E V A), scientific payloads, and station
repair and maintenance. These human space flight
activities require extensive and thorough proce-
dures. These procedures are written down in the
form of a number of steps and, with various notes,
cautions, and warnings interspersed throughout the
procedure. Each step may have one or more sub
steps.  Procedures also include branch points, call-
outs to other procedures, and instructions to com-
municate with mission control.  Since December
2001, the RIALIST group has been developing a
spoken dialogue system for providing assistance
with space station procedures. Aist and Hockey
(2002) and Aist et al (2002) described the first
version of the system, which operated on a simpli-
fied (and invented) procedure for unpacking and
operating a digital camera and included speech
input and speech output only. Aist et al (2003)
described a second version of the system with an
XML-based display, and that included support for
not only procedures, but also voice notes and re-
corded alarms, and parameter settings such as in-
creasing and decreasing volume. In this paper, we
describe the third version of the system, with a
reimplemented architecture based on a functional
specification of the domain-specific aspects of the
system combined with an event-driven generic ar-
chitectural framework. We also describe two new
features: previewing of steps, and spoken correc-
tion of dialogue moves.
2 System Description
The March 2003 version of the Intelligent Proce-
dure Assistant is shown in Figure 1, just after
loading a procedure. The March 2003 version pro-
vides the following functions:
Loading a procedure by specifying its name, for
example, ?Load water procedure.?
Sequential navigation through individual steps, for
example, ?Next step? or ?Previous step.?
Navigation to arbitrary steps, for example, ?Go to
step two point one.?
Setting system parameters, such as ?Increase vol-
ume? or ?Decrease volume.?
Handling annotations, such as voice notes or
alarms (?Record a voice note?), or pictures (?Show
the small waste water bag.?).
Previewing steps; for example, ?Read step three?.
Issuing spoken corrections (of entire commands),
for example, ?I meant go to step two.?
We will discuss previewing steps and issuing spo-
ken corrections in turn.
2.1 Previewing steps (Reading mode)
Besides acting on the current step, astronauts indi-
cated that they would like a spoken preview of the
next step. Currently this functionality is imple-
mented as displaying a second procedure window
in the upper right corner of the screen. Further-
more, steps are prefixed with a spoken indication
of previewing, for example, ?Reading mode. Note
before step two?? To transition back into normal
(execution) mode, the user may say ?Stop read-
ing.? Figure 2 shows the resulting display for the
reading mode.  
2.2 Issuing spoken corrections
In the March 2003 version of the Checklist system,
the user may issue a spoken correction in the case
of an incorrectly given command, or in the case of
a speech recognition error (e.g. ?read me step
three? ? ?repeat step three?). The dialogue history
is represented as a list of the prior dialogue states.
Currently we model a correction as a change in the
information state, a rollback of the previous action
plan, and then an application of the new action
plan. Figure 3 shows the display after issuing a
correction, ?I meant the wash cloth?. Reading
mode has been exited, and a picture of the wash-
cloth is displayed.
Figure 1. Loading a procedure.
Figure 2. Preview mode, step three.
Figure 3. A subsequent correction, resulting in a
return to execution mode, and the implementation
of the other command.
Figure 4. Checklist dialogue system architecture.
3  Architecture, or, How to write
a dialogue system in three easy steps
There are three main sections to the dialogue han-
dling code: the input manager, dialogue manager,
and the output manager (Figure 4). These are
similar divisions to those proposed in Allen et al
(2000). Here, we also adopt a further division of
the code into application-specific code and generic
code. Application-specific code computes the fol-
lowing function for each component, as a compila-
tion step:
Input manager: Input ? Event
Dialogue manager: (Event, State)
? (Action, State)
Output manager: Action ? (Output, Inverse)
The Output and Inverse computed by the Input
manager are the multimodal output plans and their
multimodal inverses, respectively.  The multi-
modal inverses are used when applying a correc-
tion ? in conjunction with a return to a previous
state on the history list.
The generic code is an interpretation (or execu-
tion) step; the input manager?s code collects in-
coming events and dispatches the events to the
dialogue manager. The dialogue manager?s code
collects the incoming events, retrieves the previous
state, applies the application-specific function,
saves the new state, and then dispatches the new
action. The output manager takes the action, ap-
plies the application-specific function to compute
the output and its inverse, and then dispatches the
output plan one action at a time. Each action is rep-
resented as an OAA solvable, and dispatched se-
quentially to be handled by the appropriate agent
such as the text-to-speech agent.
The entire dialogue manager is side-effectfree.
(With the minor exception of loading a procedure
file, which causes a change in the ?last accessed?
time of the file.) In a more typical dialogue system
architecture such as that shown in Figure 5, the
side effects are represented separately. The inte-
gration of side effects into the output plan has
positive benefits for robustness, since they will be
represented in one place (and thus modified at the
same time when programming changes are made).
Figure 5.  A more typical dialogue system ar-
chitecture, with the side effects executed separately
from the spoken output.
4 Related Research and Future Work
Rudnicky, Reed, and Thayer (1996) describe a
system for supporting vehicle maintenance with
speech interfaces. Schreckenghost et al (2003)
describe a scenario involving similar tasks (life
Speech
Recognizer
Parser
Input
Manager
Output
Manager
Speech
Synthesizer
Visual
Display
Dialogue
Manager
I: input ? event
D: (event, state)
? (action, state)
O: action
? (output, inverse)
Speech
Recognizer
Parser Input
Manager
Output
Manager
Speech
Synthesizer
DB
Dialogue
Manager
support / maintenance related) but with the com-
puter in more control of the actual task. S & K
Electronics (n.d.) mention a procedure develop-
ment environment for rapidly developing and veri-
fying on-orbit procedures
(http://sk-web.sk-tech.com/proj.html).
Possible future work includes adding procedures
involving inventory management and robot arm
assistance, automating dialogue system construc-
tion from XML procedures, integrating with te-
lemetry to monitor execution of the procedure and
develop error recovery options, improving natural-
ness of the speech output, modeling dialogue to
include dialogue moves and expected user re-
sponses, and improving speech recognition to be
robust to ISS noise.
References
G. Aist. J. Dowding, B. A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astronaut
training and support. Proceedings of the 40
th
 Annual
Meeting of the Association for Computational Lin-
guistics, refereed demonstration track.
G. Aist and B. A. Hockey. 2002. Generating Training
and Assistive Dialogues for Astronauts from Interna-
tional Space Station Technical Documentation. ITS
2002 Workshop on Integrating Technical and Train-
ing Documentation. Presented along with system
demonstration.
G. Aist, J. Dowding, B. A. Hockey, M. Rayner, J. Hi-
eronymus, D. Bohus, B. Boven, N. Blaylock, E.
Campana, S. Early, G. Gorrell, and S. Phan. 2003.
European Association for Computational Linguistics
(EACL) 2003 meeting, Software Demonstration, Bu-
dapest, Hungary, April 2003.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L.
Galescu, and A. Stent. 2000. An architecture for a
generic dialogue shell. Natural Language Engineer-
ing, Special issue on Best Practice in Spoken Lan-
guage Dialogue Systems Engineering, pp. 323-340.
A. Rudnicky, S. Reed, and E. H. Thayer. 1996.
SpeechWear: A mobile speech system.
http://www.speech.cs.cmu.edu/air/papers/speechwear.ps
D. Schreckenghost, C. Thronesbery, P. Bonasso, D.
Kortenkamp and C. Martin, Intelligent Control of
Life Support for Space Missions, in IEEE Intelligent
Systems Magazine, September/October, 2002.
Portions of the dialogue systems described in this paper
were constructed with Rayner, Hockey, and Dowding?s
Regulus open source toolkit. Interested readers may find
the toolkit and supporting documentation online at:
http://sourceforge.net/projects/regulus/.
