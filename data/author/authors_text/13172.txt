Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1040?1047,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
An Exact A* Method for Deciphering Letter-Substitution Ciphers
Eric Corlett and Gerald Penn
Department of Computer Science
University of Toronto
{ecorlett,gpenn}@cs.toronto.edu
Abstract
Letter-substitution ciphers encode a docu-
ment from a known or hypothesized lan-
guage into an unknown writing system or
an unknown encoding of a known writing
system. It is a problem that can occur in
a number of practical applications, such as
in the problem of determining the encod-
ings of electronic documents in which the
language is known, but the encoding stan-
dard is not. It has also been used in rela-
tion to OCR applications. In this paper, we
introduce an exact method for decipher-
ing messages using a generalization of the
Viterbi algorithm. We test this model on a
set of ciphers developed from various web
sites, and find that our algorithm has the
potential to be a viable, practical method
for efficiently solving decipherment prob-
lems.
1 Introduction
Letter-substitution ciphers encode a document
from a known language into an unknown writ-
ing system or an unknown encoding of a known
writing system. This problem has practical sig-
nificance in a number of areas, such as in reading
electronic documents that may use one of many
different standards to encode text. While this is not
a problem in languages like English and Chinese,
which have a small set of well known standard en-
codings such as ASCII, Big5 and Unicode, there
are other languages such as Hindi in which there
is no dominant encoding standard for the writing
system. In these languages, we would like to be
able to automatically retrieve and display the in-
formation in electronic documents which use un-
known encodings when we find them. We also
want to use these documents for information re-
trieval and data mining, in which case it is impor-
tant to be able to read through them automatically,
without resorting to a human annotator. The holy
grail in this area would be an application to ar-
chaeological decipherment, in which the underly-
ing language?s identity is only hypothesized, and
must be tested. The purpose of this paper, then,
is to simplify the problem of reading documents
in unknown encodings by presenting a new algo-
rithm to be used in their decipherment. Our algo-
rithm operates by running a search over the n-gram
probabilities of possible solutions to the cipher, us-
ing a generalization of the Viterbi algorithm that
is wrapped in an A* search, which determines at
each step which partial solutions to expand. It
is guaranteed to converge on the language-model-
optimal solution, and does not require restarts or
risk falling into local optima. We specifically con-
sider the problem of finding decodings of elec-
tronic documents drawn from the internet, and
we test our algorithm on ciphers drawn from ran-
domly selected pages of Wikipedia. Our testing
indicates that our algorithm will be effective in this
domain.
It may seem at first that automatically decoding
(as opposed to deciphering) a document is a sim-
ple matter, but studies have shown that simple al-
gorithms such as letter frequency counting do not
always produce optimal solutions (Bauer, 2007).
If the text from which a language model is trained
is of a different genre than the plaintext of a cipher,
the unigraph letter frequencies may differ substan-
tially from those of the language model, and so
frequency counting will be misleading. Because
of the perceived simplicity of the problem, how-
ever, little work was performed to understand its
computational properties until Peleg and Rosen-
feld (1979), who developed a method that repeat-
edly swaps letters in a cipher to find a maximum
probability solution. Since then, several different
approaches to this problem have been suggested,
some of which use word counts in the language
to arrive at a solution (Hart, 1994), and some of
1040
which treat the problem as an expectation max-
imization problem (Knight et al, 2006; Knight,
1999). These later algorithms are, however, highly
dependent on their initial states, and require a
number of restarts in order to find the globally op-
timal solution. A further contribution was made by
(Ravi and Knight, 2008), which, though published
earlier, was inspired in part by the method pre-
sented here, first discovered in 2007. Unlike the
present method, however, Ravi and Knight (2008)
treat the decipherment of letter-substitution ci-
phers as an integer programming problem. Clever
though this constraint-based encoding is, their pa-
per does not quantify the massive running times
required to decode even very short documents
with this sort of approach. Such inefficiency indi-
cates that integer programming may simply be the
wrong tool for the job, possibly because language
model probabilities computed from empirical data
are not smoothly distributed enough over the space
in which a cutting-plane method would attempt to
compute a linear relaxation of this problem. In
any case, an exact method is available with a much
more efficient A* search that is linear-time in the
length of the cipher (though still horribly exponen-
tial in the size of the cipher and plain text alpha-
bets), and has the additional advantage of being
massively parallelizable. (Ravi and Knight, 2008)
also seem to believe that short cipher texts are
somehow inherently more difficult to solve than
long cipher texts. This difference in difficulty,
while real, is not inherent, but rather an artefact of
the character-level n-gram language models that
they (and we) use, in which preponderant evidence
of differences in short character sequences is nec-
essary for the model to clearly favour one letter-
substitution mapping over another. Uniform char-
acter models equivocate regardless of the length of
the cipher, and sharp character models with many
zeroes can quickly converge even on short ciphers
of only a few characters. In the present method,
the role of the language model can be acutely per-
ceived; both the time complexity of the algorithm
and the accuracy of the results depend crucially on
this characteristic of the language model. In fact,
we must use add-one smoothing to decipher texts
of even modest lengths because even one unseen
plain-text letter sequence is enough to knock out
the correct solution. It is likely that the method
of (Ravi and Knight, 2008) is sensitive to this as
well, but their experiments were apparently fixed
on a single, well-trained model.
Applications of decipherment are also explored
by (Nagy et al, 1987), who uses it in the con-
text of optical character recognition (OCR). The
problem we consider here is cosmetically related
to the ?L2P? (letter-to-phoneme) mapping prob-
lem of text-to-speech synthesis, which also fea-
tures a prominent constraint-based approach (van
den Bosch and Canisius, 2006), but the constraints
in L2P are very different: two different instances
of the same written letter may legitimately map to
two different phonemes. This is not the case in
letter-substitution maps.
2 Terminology
Substitution ciphers are ciphers that are defined
by some permutation of a plaintext alphabet. Ev-
ery character of a plaintext string is consistently
mapped to a single character of an output string
using this permutation. For example, if we took
the string ?hello world? to be the plaintext, then
the string ?ifmmp xpsme? would be a cipher
that maps e to f , l to m, and so on. It is easy
to extend this kind of cipher so that the plaintext
alphabet is different from the ciphertext alphabet,
but still stands in a one to one correspondence to
it. Given a ciphertext C, we say that the set of
characters used inC is the ciphertext alphabet ?C ,
and that its size is nC . Similarly, the entire possi-
ble plaintext alphabet is ?P , and its size is is nP .
Since nC is the number of letters actually used
in the cipher, rather than the entire alphabet it is
sampled from, we may find that nC < nP even
when the two alphabets are the same. We refer to
the length of the cipher string C as clen. In the
above example, ?P is { , a, . . . z} and nP = 27,
while ?C = { , e, f, i,m, p, s, x}, clen = 11 and
nC = 8.
Given the ciphertext C, we say that a partial
solution of size k is a map ? = {p1 : c1, . . . pk :
ck}, where c1, . . . , ck ? ?C and are distinct, and
p1, . . . , pk ? ?P and are distinct, and where k ?
nC . If for a partial solution ??, we have that ? ?
??, then we say that ?? extends ?. If the size of ?? is
k+1 and ? is size k, we say that ?? is an immediate
extension of ?. A full solution is a partial solution
of size nC . In the above example, ?1 = { : , d :
e} would be a partial solution of size 2, and ?2 =
{ : , d : e, g : m} would be a partial solution
of size 3 that immediately extends ?1. A partial
solution ?T { : , d : e, e : f, h : i, l : m, o :
1041
p, r : s, w : x} would be both a full solution and
the correct one. The full solution ?T extends ?1
but not ?2.
Every possible full solution to a cipher C will
produce a plaintext string with some associated
language model probability, and we will consider
the best possible solution to be the one that gives
the highest probability. For the sake of concrete-
ness, we will assume here that the language model
is a character-level trigram model. This plain-
text can be found by treating all of the length clen
strings S as being the output of different charac-
ter mappings from C. A string S that results from
such a mapping is consistent with a partial solu-
tion ? iff, for every pi : ci ? ?, the character posi-
tions of C that map to pi are exactly the character
positions with ci in C.
In our above example, we had C =
?ifmmp xpsme?, in which case we had
clen = 11. So mappings from C to
?hhhhh hhhhh? or ? hhhhhhhhhh? would
be consistent with a partial solution of size 0,
while ?hhhhh hhhhn? would be consistent with
the size 2 partial solution ? = { : , n : e}.
3 The Algorithm
In order to efficiently search for the most likely so-
lution for a ciphertext C, we conduct a search of
the partial solutions using their trigram probabil-
ities as a heuristic, where the trigram probability
of a partial solution ? of length k is the maximum
trigram probability over all strings consistent with
it, meaning, in particular, that ciphertext letters not
in its range can be mapped to any plaintext letter,
and do not even need to be consistently mapped to
the same plaintext letter in every instance. Given
a partial solution ? of length n, we can extend ?
by choosing a ciphertext letter c not in the range
of ?, and then use our generalization of the Viterbi
algorithm to find, for each p not in the domain of
?, a score to rank the choice of p for c, namely the
trigram probability of the extension ?p of ?. If we
start with an empty solution and iteratively choose
the most likely remaining partial solution in this
way, storing the extensions obtained in a priority
heap as we go, we will eventually reach a solution
of size nC . Every extension of ? has a probabil-
ity that is, at best, equal to that of ?, and every
partial solution receives, at worst, a score equal
to its best extension, because the score is poten-
tially based on an inconsistent mapping that does
not qualify as an extension. These two observa-
tions taken together mean that one minus the score
assigned by our method constitutes a cost function
over which this score is an admissible heuristic in
the A* sense. Thus the first solution of size nC
will be the best solution of size nC .
The order by which we add the letters c to par-
tial solutions is the order of the distinct cipher-
text letters in right-to-left order of their final oc-
currence in C. Other orderings for the c, such as
most frequent first, are also possible though less
elegant.1
Algorithm 1 Search Algorithm
Order the letters c1 . . . cnC by rightmost occur-
rence in C, rnC < . . . < r1.
Create a priority queue Q for partial solutions,
ordered by highest probability.
Push the empty solution ?0 = {} onto the
queue.
while Q is not empty do
Pop the best partial solution ? from Q.
s = |?|.
if s = nC then
return ?
else
For all p not in the range of ?, push the
immediate extension ?p onto Q with the
score assigned to table cell G(rs+1, p, p)
by GVit(?, cs+1, rs+1) if it is non-zero.
end if
end while
Return ?Solution Infeasible?.
Our generalization of the Viterbi algorithm, de-
picted in Figure 1, uses dynamic programming to
score every immediate extension of a given partial
solution in tandem, by finding, in a manner con-
sistent with the real Viterbi algorithm, the most
probable input string given a set of output sym-
bols, which in this case is the cipher C. Unlike the
real Viterbi algorithm, we must also observe the
constraints of the input partial solution?s mapping.
1We have experimented with the most frequent first regi-
men as well, and it performs worse than the one reported here.
Our hypothesis is that this is due to the fact that the most fre-
quent character tends to appear in many high-frequency tri-
grams, and so our priority queue becomes very long because
of a lack of low-probability trigrams to knock the scores of
partial solutions below the scores of the extensions of their
better scoring but same-length peers. A least frequent first
regimen has the opposite problem, in which their rare oc-
currence in the ciphertext provides too few opportunities to
potentially reduce the score of a candidate.
1042
A typical decipherment involves multiple runs of
this algorithm, each of which scores all of the im-
mediate extensions, both tightening and lowering
their scores relative to the score of the input par-
tial solution. A call GVit(?, c, r) manages this by
filling in a table G such that for all 1 ? i ? r, and
l, k ? ?P , G(i, l, k) is the maximum probability
over every plaintext string S for which:
? len(S) = i,
? S[i] = l,
? for every p in the domain of ?, every 1 ? j ?
i, if C[j] = ?(p) then S[j] = p, and
? for every position 1 ? j ? i, if C[j] = c,
then S[j] = k.
The real Viterbi algorithm lacks these final two
constraints, and would only store a single cell at
G(i, l). There, G is called a trellis. Ours is larger,
so so we will refer to G as a greenhouse.
The table is completed by filling in the columns
from i = 1 to clen in order. In every column i,
we will iterate over the values of l and over the
values of k such that k : c and l : are consistent
with ?. Because we are using a trigram character
model, the cells in the first and second columns
must be primed with unigram and bigram proba-
bilities. The remaining probabilities are calculated
by searching through the cells from the previous
two columns, using the entry at the earlier column
to indicate the probability of the best string up to
that point, and searching through the trigram prob-
abilities over two additional letters. Backpointers
are necessary to reference one of the two language
model probabilities. Cells that would produce in-
consistencies are left at zero, and these as well as
cells that the language model assigns zero to can
only produce zero entries in later columns.
In order to decrease the search space, we add the
further restriction that the solutions of every three
character sequence must be consistent: if the ci-
phertext indicates that two adjacent letters are the
same, then only the plaintext strings that map the
same letter to each will be considered. The num-
ber of letters that are forced to be consistent is
three because consistency is enforced by remov-
ing inconsistent strings from consideration during
trigram model evaluation.
Because every partial solution is only obtained
by extending a solution of size one less, and ex-
tensions are only made in a predetermined order
of cipher alphabet letters, every partial solution is
only considered / extended once.
GVit is highly parallelizable. The nP ?nP cells
of every column i do not depend on each other ?
only on the cells of the previous two columns i?1
and i?2, as well as the language model. In our im-
plementation of the algorithm, we have written the
underlying program in C/C++, and we have used
the CUDA library developed for NVIDIA graphics
cards to in order to implement the parallel sections
of the code.
4 Experiment
The above algorithm is designed for application to
the transliteration of electronic documents, specif-
ically, the transliteration of websites, and it has
been tested with this in mind. In order to gain re-
alistic test data, we have operated on the assump-
tion that Wikipedia is a good approximation of the
type of language that will be found in most inter-
net articles. We sampled a sequence of English-
language articles from Wikipedia using their ran-
dom page selector, and these were used to create
a set of reference pages. In order to minimize the
common material used in each page, only the text
enclosed by the paragraph tags of the main body of
the pages were used. A rough search over internet
articles has shown that a length of 1000 to 11000
characters is a realistic length for many articles, al-
though this can vary according to the genre of the
page. Wikipedia, for example, does have entries
that are one sentence in length. We have run two
groups of tests for our algorithm. In the first set
of tests, we chose the mean of the above lengths
to be our sample size, and we created and decoded
10 ciphers of this size (i.e., different texts, same
size). We made these cipher texts by appending
the contents of randomly chosen Wikipedia pages
until they contained at least 6000 characters, and
then using the first 6000 characters of the result-
ing files as the plaintexts of the cipher. The text
length was rounded up to the nearest word where
needed. In the second set of tests, we used a single
long ciphertext, and measured the time required
for the algorithm to finish a number of prefixes of
it (i.e., same text, different sizes). The plaintext for
this set of tests was developed in the same way as
the first set, and the input ciphertext lengths con-
sidered were 1000, 3500, 6000, 8500, 11000, and
13500 characters.
1043
Greenhouse Array
(a) (b) (c) (d)
...
l
m
n
...
z
l w ? ? ? y t g ? ? ? g u
? ? ? e f g ? ? ? z
Figure 1: Filling the Greenhouse Table. Each cell in the greenhouse is indexed by a plaintext letter and
a character from the cipher. Each cell consists of a smaller array. The cells in the array give the best
probabilities of any path passing through the greenhouse cell, given that the index character of the array
maps to the character in column c, where c is the next ciphertext character to be fixed in the solution. The
probability is set to zero if no path can pass through the cell. This is the case, for example, in (b) and (c),
where the knowledge that ? ? maps to ? ? would tell us that the cells indicated in gray are unreachable.
The cell at (d) is filled using the trigram probabilities and the probability of the path at starting at (a).
In all of the data considered, the frequency of
spaces was far higher than that of any other char-
acter, and so in any real application the character
corresponding to the space can likely be guessed
without difficulty. The ciphers we have consid-
ered have therefore been simplified by allowing
the knowledge of which character corresponds to
the space. It appears that Ravi and Knight (2008)
did this as well. Our algorithm will still work with-
out this assumption, but would take longer. In the
event that a trigram or bigram would be found in
the plaintext that was not counted in the language
model, add one smoothing was used.
Our character-level language model used was
developed from the first 1.5 million characters of
the Wall Street Journal section of the Penn Tree-
bank corpus. The characters used in the lan-
guage model were the upper and lower case let-
ters, spaces, and full stops; other characters were
skipped when counting the frequencies. Further-
more, the number of sequential spaces allowed
was limited to one in order to maximize context
and to eliminate any long stretches of white space.
As discussed in the previous paragraph, the space
character is assumed to be known.
When testing our algorithm, we judged the time
complexity of our algorithm by measuring the ac-
tual time taken by the algorithm to complete its
runs, as well as the number of partial solutions
placed onto the queue (?enqueued?), the number
popped off the queue (?expanded?), and the num-
ber of zero-probability partial solutions not en-
queued (?zeros?) during these runs. These latter
numbers give us insight into the quality of trigram
probabilities as a heuristic for the A* search.
We judged the quality of the decoding by mea-
suring the percentage of characters in the cipher
alphabet that were correctly guessed, and also the
word error rate of the plaintext generated by our
solution. The second metric is useful because a
low probability character in the ciphertext may be
guessed wrong without changing as much of the
actual plaintext. Counting the actual number of
word errors is meant as an estimate of how useful
or readable the plaintext will be. We did not count
the accuracy or word error rate for unfinished ci-
phers.
We would have liked to compare our results
with those of Ravi and Knight (2008), but the
method presented there was simply not feasible
1044
Algorithm 2 Generalized Viterbi Algorithm
GVit(?, c, r)
Input: partial solution ?, ciphertext character c,
and index r into C.
Output: greenhouse G.
Initialize G to 0.
i = 1
for All (l, k) such that ? ? {k : c, l : Ci} is
consistent do
G(i, l, k) = P (l).
end for
i = 2
for All (l, k) such that ? ? {k : c, l : Ci} is
consistent do
for j such that ? ? {k : c, l : Ci, j : Ci?1} is
consistent do
G(i, l, k) = max(G(i, l, k), G(0, j, k)?
P (l|j))
end for
end for
i = 3
for (l, k) such that ? ? {k : c, l : Ci} is consis-
tent do
for j1, j2 such that ??{k : c, j2 : C[i?2], j1 :
C[i? 1], l : Ci} is consistent do
G(i, l, k) = max(G(i, l, k), G(i?2, j2, k)
? P (j1|j2)? P (l|j2j1)).
end for
end for
for i = 4 to r do
for (l, k) such that ? ? {k : c, l : Ci} is con-
sistent do
for j1, j2 such that ? ? {k : c, j2 :
C[i?2], j1 : C[i?1], l : Ci} is consistent
do
G(i, l, k) = max(G(i, l, k),
G(i?2, j2, k)?P (j1|j2j2(back))
? P (l|j2j1)).
end for
end for
end for
on texts and (case-sensitive) alphabets of this size
with the computing hardware at our disposal.
5 Results
In our first set of tests, we measured the time con-
sumption and accuracy of our algorithm over 10
ciphers taken from random texts that were 6000
characters long. The time values in these tables are
given in the format of (H)H:MM:SS. For this set
of tests, in the event that a test took more than 12
hours, we terminated it and listed it as unfinished.
This cutoff was set in advance of the runs based
upon our armchair speculation about how long one
might at most be reasonably expected to wait for
a web-page to be transliterated (an overnight run).
The results from this run appear in Table 1. All
running times reported in this section were ob-
tained on a computer running Ubuntu Linux 8.04
with 4 GB of RAM and 8 ? 2.5 GHz CPU cores.
Column-level subcomputations in the greenhouse
were dispatched to an NVIDIA Quadro FX 1700
GPU card that is attached through a 16-lane PCI
Express adapter. The card has 512 MB of cache
memory, a 460 MHz core processor and 32 shader
processors operating in parallel at 920 MHz each.
In our second set of tests, we measured the time
consumption and accuracy of our algorithm over
several prefixes of different lengths of a single
13500-character ciphertext. The results of this run
are given in Table 2.
The first thing to note in this data is that the ac-
curacy of this algorithm is above 90 % for all of
the test data, and 100% on all but the smallest 2
ciphers. We can also observe that even when there
are errors (e.g., in the size 1000 cipher), the word
error rate is very small. This is a Zipf?s Law effect
? misclassified characters come from poorly at-
tested character trigrams, which are in turn found
only in longer, rarer words. The overall high ac-
curacy is probably due to the large size of the
texts relative to the uniticity distance of an En-
glish letter-substitution cipher (Bauer, 2007). The
results do show, however, that character trigram
probabilities are an effective indicator of the most
likely solution, even when the language model and
test data are from very different genres (here, the
Wall Street Journal and Wikipedia, respectively).
These results also show that our algorithm is ef-
fective as a way of decoding simple ciphers. 80%
of our runs finished before the 12 hour cutoff in
the first experiment.
1045
Cipher Time Enqueued Expanded Zeros Accuracy Word Error Rate
1 2:03:06 964 964 44157 100% 0%
2 0:13:00 132 132 5197 100% 0%
3 0:05:42 91 91 3080 100% 0%
4 Unfinished N/A N/A N/A N/A N/A
5 Unfinished N/A N/A N/A N/A N/A
6 5:33:50 2521 2521 114283 100% 0%
7 6:02:41 2626 2626 116392 100% 0%
8 3:19:17 1483 1483 66070 100% 0%
9 9:22:54 4814 4814 215086 100% 0%
10 1:23:21 950 950 42107 100% 0%
Table 1: Time consumption and accuracy on a sample of 10 6000-character texts.
Size Time Enqueued Expanded Zeros Accuracy Word Error Rate
1000 40:06:05 119759 119755 5172631 92.59% 1.89%
3500 0:38:02 615 614 26865 96.30% 0.17%
6000 0:12:34 147 147 5709 100% 0%
8500 8:52:25 1302 1302 60978 100% 0%
11000 1:03:58 210 210 8868 100% 0%
13500 0:54:30 219 219 9277 100% 0%
Table 2: Time consumption and accuracy on prefixes of a single 13500-character ciphertext.
As far as the running time of the algorithm goes,
we see a substantial variance: from a few minutes
to several hours for most of the longer ciphers, and
that there are some that take longer than the thresh-
old we gave in the experiment. Specifically, there
is substantial variability in the the running times
seen.
Desiring to reduce the variance of the running
time, we look at the second set of tests for possible
causes. In the second test set, there is a general
decrease in both the running time and the number
of solutions expanded as the length of the ciphers
increases. Running time correlates very well with
A* queue size.
Asymptotically, the time required for each
sweep of the Viterbi algorithm increases, but this
is more than offset by the decrease in the number
of required sweeps.
The results, however, do not show that running
time monotonically decreases with length. In par-
ticular, the length 8500 cipher generates more so-
lutions than the length 3500 or 6000 ones. Recall
that the ciphers in this section are all prefixes of
the same string. Because the algorithm fixes char-
acters starting from the end of the cipher, these
prefixes have very different character orderings,
c1, . . . , cnC , and thus a very different order of par-
tial solutions. The running time of our algorithm
depends very crucially on these initial conditions.
Perhaps most interestingly, we note that the
number of enqueued partial solutions is in ev-
ery case identical or nearly identical to the num-
ber of partial solutions expanded. From a the-
oretical perspective, we must also remember the
zero-probability solutions, which should in a sense
count when judging the effectiveness of our A*
heuristic. Naturally, these are ignored by our im-
plementation because they are so badly scored
that they could never be considered. Neverthe-
less, what these numbers show is that scores based
on character-level trigrams, while theoretically ad-
missible, are really not all that clever when it
comes to navigating through the search space of
all possible letter substitution ciphers, apart from
their very keen ability at assigning zeros to a
large number of partial solutions. A more com-
plex heuristic that can additionally rank non-zero
probability solutions with more prescience would
likely make a very great difference to the running
time of this method.
1046
6 Conclusions
In the above paper, we have presented an algo-
rithm for solving letter-substitution ciphers, with
an eye towards discovering unknown encoding
standards in electronic documents on the fly. In
a test of our algorithm over ciphers drawn from
Wikipedia, we found its accuracy to be 100% on
the ciphers that it solved within a threshold of 12
hours, this being 80% of the total attempted. We
found that the running time of our algorithm is
highly variable depending on the order of char-
acters attempted, and, due to the linear-time the-
oretical complexity of this method, that running
times tend to decrease with larger ciphertexts due
to our character-level language model?s facility at
eliminating highly improbable solutions. There is,
however, a great deal of room for improvement in
the trigram model?s ability to rank partial solutions
that are not eliminated outright.
Perhaps the most valuable insight gleaned from
this study has been on the role of the language
model. This algorithm?s asymptotic runtime com-
plexity is actually a function of entropic aspects of
the character-level language model that it uses ?
more uniform models provide less prominent sep-
arations between candidate partial solutions, and
this leads to badly ordered queues, in which ex-
tended partial solutions can never compete with
partial solutions that have smaller domains, lead-
ing to a blind search. We believe that there is a
great deal of promise in characterizing natural lan-
guage processing algorithms in this way, due to the
prevalence of Bayesian methods that use language
models as priors.
Our approach makes no explicit attempt to ac-
count for noisy ciphers, in which characters are
erroneously mapped, nor any attempt to account
for more general substitution ciphers in which a
single plaintext (resp. ciphertext) letter can map to
multiple ciphertext (resp. plaintext) letters, nor for
ciphers in which ciphertext units corresponds to
larger units of plaintext such syllables or words.
Extensions in these directions are all very worth-
while to explore.
References
Friedrich L. Bauer. 2007. Decrypted Secrets.
Springer-Verlag, Berlin Heidelberg.
George W. Hart. 1994. To Decode Short Cryptograms.
Communications of the ACM, 37(9): 102?108.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics, 25(4):607?615.
Kevin Knight, Anish Nair, Nishit Rathod, Kenji Ya-
mada. Unsupervised Analysis for Decipherment
Problems. Proceedings of the COLING/ACL 2006,
2006, 499?506.
George Nagy, Sharad Seth, Kent Einspahr. 1987.
Decoding Substitution Ciphers by Means of Word
Matching with Application to OCR. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
9(5):710?715.
Shmuel Peleg and Azriel Rosenfeld. 1979. Breaking
Substitution Ciphers Using a Relaxation Algorithm.
Communications of the ACM, 22(11):589?605.
Sujith Ravi, Kevin Knight. 2008. Attacking Decipher-
ment Problems Optimally with Low-Order N-gram
Models Proceedings of the ACL 2008, 812?819.
Antal van den Bosch, Sander Canisius. 2006. Im-
proved Morpho-phonological Sequence Processing
with Constraint Satisfaction Inference Proceedings
of the Eighth Meeting of the ACL Special Interest
Group on Computational Phonology at HLT-NAACL
2006, 41?49.
1047
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 83?92,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Why Letter Substitution Puzzles are Not Hard to Solve: A Case Study in
Entropy and Probabilistic Search-Complexity
Eric Corlett
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
ecorlett@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
In this paper we investigate the theoretical
causes of the disparity between the theo-
retical and practical running times for the
A? algorithm proposed in Corlett and Penn
(2010) for deciphering letter-substitution
ciphers. We argue that the difference seen
is due to the relatively low entropies of the
probability distributions of character tran-
sitions seen in natural language, and we
develop a principled way of incorporat-
ing entropy into our complexity analysis.
Specifically, we find that the low entropy
of natural languages can allow us, with
high probability, to bound the depth of the
heuristic values expanded in the search.
This leads to a novel probabilistic bound
on search depth in these tasks.
1 Introduction
When working in NLP, we can find ourselves
using algorithms whose worst-case running time
bounds do not accurately describe their empiri-
cally determined running times. Specifically, we
can often find that the algorithms that we are us-
ing can be made to run efficiently on real-world
instances of their problems despite having theo-
retically high running times. Thus, we have an ap-
parent disparity between the theoretical and prac-
tical running times of these algorithms, and so we
must ask why these algorithms can provide results
in a reasonable time frame. We must also ask to
what extent we can expect our algorithms to re-
main practical as we change the downstream do-
mains from which we draw problem instances.
At a high level, the reason such algorithms can
work well in the real world is that the real world
applications from which we draw our inputs do
not tend to include the high complexity inputs. In
other words, our problem space either does not
cover all possible inputs to the algorithm, or it
does, but with a probability distribution that gives
a vanishingly small likelihood to the ?hard? inputs.
Thus, it would be beneficial to incorporate into our
running time analysis the fact that our possible in-
puts are restricted, even if only restricted in rela-
tive frequency rather than in absolute terms.
This means that any running time that we ob-
serve must be considered to be dependent on the
distribution of inputs that we expect to sample
from. It probably does not come as a surprise that
any empirical analysis of running time carries with
it the assumption that the data on which the tests
were run are typical of the data which we expect
to see in practice. Yet the received wisdom on the
asymptotic complexity of algorithms in computa-
tional linguistics (generally what one might see
in an advanced undergraduate algorithms curricu-
lum) has been content to consider input only in
terms of its size or length, and not the distribution
from which it was sampled. Indeed, many algo-
rithms in NLP actually take entire distributions as
input, such as language models. Without a more
mature theoretical understanding of time complex-
ity, it is not clear exactly what any empirical run-
ning time results would mean. A worst-case com-
plexity result gives a guarantee that an algorithm
will take no more than a certain number of steps
to complete. An average-case result gives the ex-
pected number of steps to complete. But an empir-
ical running time found by sampling from a distri-
bution that is potentially different from what the
algorithm was designed for is only a lesson in how
truly different the distribution is.
It is also common for the theoretical study of
asymptotic time complexity in NLP to focus on
the worst-case complexity of a problem or algo-
rithm rather than an expected complexity, in spite
of the existence for now over 20 years of methods
for average-case analysis of an algorithm. Even
these, however, often assume a uniform distribu-
83
tion over input, when in fact the true expectation
must consider the probability distribution that we
will draw the inputs from. Uniform distributions
are only common because we may not know what
the distribution is beforehand.
Ideally, we should want to characterize the run-
ning time of an algorithm using some known prop-
erties of its input distribution, even if the precise
distribution is not known. Previous work that at-
tempts this does exist. In particular, there is a vari-
ant of analysis referred to as smoothed analysis
which gives a bound on the average-case running
time of an algorithm under the assumption that all
inputs are sampled with Gaussian measurement er-
ror. As we will argue in Section 2, however, this
approach is of limited use to us.
We instead approach the disparity of theoretical
and practical running time by making use of statis-
tics such as entropy, which are taken from the in-
put probability distributions, as eligible factors in
our analysis of the running time complexity. This
is a reasonable approach to the problem, in view of
the numerous entropic studies of word and charac-
ter distributions dating back to Shannon.
Specifically, we analyze the running time of the
A? search algorithm described in Corlett and Penn
(2010). This algorithm deciphers text that has
been enciphered using a consistent letter substitu-
tion, and its running time is linear in the length of
the text being deciphered, but theoretically expo-
nential in the size of the input and output alpha-
bets. This na??ve theoretical analysis assumes that
characters are uniformly distributed, however. A
far more informative bound is attainable by mak-
ing reference to the entropy of the input. Be-
cause the algorithm takes a language model as one
of its inputs (the algorithm is guaranteed to find
the model-optimal letter substitution over a given
text), there are actually two input distributions: the
distribution assumed by the input language model,
and the distribution from which the text to be de-
ciphered was sampled. Another way to view this
problem is as a search for a permutation of letters
as the outcomes of one distribution such that the
two distributions are maximally similar. So our
informative bound is attained through reference to
the cross-entropy of these two distributions.
We first formalize our innate assumption that
these two distributions are similar, and build an
upper bound for the algorithm?s complexity that
incorporates the cross-entropy between the two
distributions. The analysis concludes that, rather
than being exponential in the length of the input or
in the size of the alphabets, it is merely exponen-
tial in the cross-entropy of these two distributions,
thus exposing the importance of their similarity.
Essentially, our bound acts as a probability distri-
bution over the necessary search depth.
2 Related Work
The closest previous work to the analysis pre-
sented here is the use of smoothed analysis to ex-
plain the tractable real-world running time of a
number of algorithms with an exponential worst-
case complexity. These algorithms include the
simplex algorithm, as described by Spielman and
Teng (2004), the k-means clustering algorithm, as
described by Arthur et al (2009) and others. As
in our current approach, smoothed analysis works
by running a general average-case analysis of the
algorithms without direct knowledge of the distri-
bution from which the problem inputs have been
drawn. The assumption made in smoothed anal-
ysis is that every input has been read with some
Gaussian measurement error. That is, in a typi-
cal worst-case analysis, we may have an adversary
choose any input for our algorithm, after which we
must calculate how bad the resulting running time
might be, but in a smoothed analysis, the adver-
sary gives us input by placing it into the real world
so that we may measure it, and this measurement
adds a small error drawn from a Gaussian dis-
tribution to the problem instance. The point of
smoothed analysis is to find the worst average-case
running time, under these conditions, that the ad-
versary can subject us to. Thus the analysis is an
average case, subject to this error, of worst cases.
In the papers cited above, this method of analysis
was able to drop running times from exponential
to polynomial.
It is unfortunate that this approach does not
readily apply to many of the algorithms that we
use in NLP. To see why this is, simply note that
we can only add a small Gaussian error to our in-
puts if our inputs themselves are numerical. If the
inputs to our algorithms are discrete, say, in the
form of strings, then Gaussian errors are not mean-
ingful. Rather, we must ask what sort of error we
can expect to see in our inputs, and to what extent
these errors contribute to the running time of our
algorithms. In the case of decipherment, ?error?
is committed by substituting one character for an-
84
other consistently.
The strongest known result on the search com-
plexity of A? is given in Pearl (1984). This work
found that, under certain assumptions, a bound on
the absolute error between the heuristic used and
the true best cost to reach the goal yields a polyno-
mial worst-case depth for the search. This happens
when the bound is constant across search instances
of different sizes. On the other hand, if the relative
error does not have this constant bound, the search
complexity can still be exponential. This analy-
sis assumes that the relative errors in the heuristic
are independent between nodes of the search tree.
It is also often very difficult even to calculate the
value of a heuristic that possesses such a bound,
as it might involve calculating the true best cost,
which can be as difficult as completely solving a
search problem instance (Korf et al, 2001). Thus,
most practical heuristics still give rise to theoreti-
cally exponential search complexities in this view.
In Korf and Reid (1998) and Korf et al (2001),
on the other hand, several practical problems are
treated, such as random k-SAT, Rubik?s cubes, or
sliding tile puzzles, which are not wholly unlike
deciphering letter substitution puzzles in that they
calculate permutations, and therefore can assume,
as we do, that overall time complexity directly cor-
responds to the number of nodes visited at differ-
ent depths in the search tree that have a heuris-
tic low enough to guarantee node expansion. But
their analysis assumes that it is possible to both es-
timate and use a probability distribution of heuris-
tic values on different nodes of the search graph,
whereas in our task, this distribution is very dif-
ficult to sample because almost every node in the
search graph has a worse heuristic score than the
goal does, and would therefore never be expanded.
Without an accurate idea of what the distribution
of the heuristic is, we cannot accurately estimate
the complexity of the algorithm. On the other
hand, their analysis makes no use of any estimates
of the cost of reaching the goal, because the prac-
tical problems that they consider do not allow for
particularly accurate estimates. In our treatment,
we find that the cost to reach the goal can be esti-
mated with high probability, and that this estimate
is much less than the cost of most nodes in the
search graph. These different characteristics allow
us to formulate a different sort of bound on the
search complexity for the decipherment problem.
3 The Algorithm
We now turn to the algorithm given in Corlett and
Penn (2010) which we will investigate, and we ex-
plain the model we use to find our bound.
The purpose of the algorithm is to allow us to
read a given ciphertext C which is assumed to
be generated by putting an unknown plaintext P
through an unknown monoalphabetic cipher.
We will denote the ciphertext alphabet as ?c
and the plaintext alphabet as ?p. Given any string
T , we will denote n(T ) as the length of T . Fur-
thermore, we assume that the plaintext P is drawn
from some string distribution q. We do not assume
q to be a trigram distribution, but we do require it
to be a distribution from which trigrams can be
calculated (e.g, a 5-gram corpus will in general
have probabilities that cannot be predicted using
the associated trigrams, but the associated trigram
corpus can be recovered from the 5-grams).
It is important to realize in the algorithm de-
scription and analysis that q may also not be
known exactly, but we only assume that it exists,
and that we can approximate it with a known tri-
gram distribution p. In Corlett and Penn (2010),
for example, p is the trigram distribution found us-
ing the Penn treebank. It is assumed that this is a
good approximation for the distribution q, which
in Corlett and Penn (2010) is the text in Wikipedia
from which ciphers are drawn. As is common
when dealing with probability distributions over
natural languages, we assume that both p and q
are stationary and ergodic, and we furthermore as-
sume that p is smooth enough that any trigram that
can be found in any string generated by q occurs in
p (i.e., we assume that the cross entropyH(p, q) is
finite).
The algorithm works in a model in which, for
any run of the algorithm, the plaintext string P
is drawn according to the distribution q. We do
not directly observe P , but instead its encoding
using the cipher key, which we will call piT . We
observe the ciphertext C = pi?1T (P ). We note that
piT is unknown, but that it does not change as new
ciphertexts are drawn.
Now, the way that the algorithm in Corlett and
Penn (2010) works is by searching over the pos-
sible keys to the cipher to find the one that maxi-
mizes the probability of the plaintext according to
the distribution p. It does so as follows.
In addition to the possible keys to the cipher,
85
weakened cipher keys called partial solutions are
added to the search space. A partial solution of
size k (denoted as pik) is a section of a possible full
cipher key which is only defined on k character
types in the cipher. We consider the character
types to be fixed according to some preset order,
and so the k fixed letters in pik do not change
between different partial solutions of size k.
Given a partial solution pik, a string pi
n(C)
k (C)
is defined whose probability we use as an upper
bound for the probability of the plaintext when-
ever the true solution to the cipher contains pik
as a subset. The string pin(C)k (C) is the most
likely string that we can find that is consistent
with C on the letters fixed by pik. That is, we
define the set ?k so that S ? ?k iff whenever
si and ci are the characters at index i in S and
C, then si = pik(ci) if ci is fixed in pik. Note
that if ck is not fixed in pik, we let si take any
value. We extend the partial character function
to the full string function pin(C)k on ?
n(C)
c so that
pin(C)k (C) = argmax(S??k)probp(S).
In Corlett and Penn (2010), the value pin(C)k (C)
is efficiently computed by running it through
the Viterbi algorithm. That is, given C, p and
pik, a run of the Viterbi algorithm is set up in
which the letter transition probabilities are those
that are given in p. In order to describe the
emission probabilities, suppose that we partition
the ciphertext alphabet ?c into two sets ?1 and
?2, where ?1 is the set of ciphertext letters fixed
by pik. For any plaintext letter y ? ?p, if there
is a ciphertext letter x ? ?1 such that y ? x is
a rule in pik, then the emission probability that y
will be seen as x is set to 1, and the probability
that y will be seen as any other letter is set to 0.
On the other hand, if there is no rule y ? x in
pik for any ciphertext letter x, then the emission
probability associated with y is uniform over the
letters x ? ?2 and 0 for the letters x ? ?1.
The search algorithm described in Corlett and
Penn (2010) uses the probability of the string
pin(C)k (C), or more precisely, the log probabil-
ity ?logprobp(pi
n(C)
k (C)), as an A
? heuristic over
the partial solutions pik. In this search, an edge
is added from a size k partial solution pik to a
size k + 1 partial solution pik+1 if pik agrees with
pik+1 wherever it is defined. The score of a node
pik is the log probability of its associated string:
?logprobp(pi
n(C)
k (C)). We can see that if pik has
an edge leading to pik+1, then ?k+1 ? ?k, so that
?logprobp(pi
n(C)
k+1 (C)) ? ?logprobp(pi
n(C)
k (C)).
Thus, the heuristic is nondecreasing. Moreover,
by applying the same statement inductively we can
see that any full solution to the cipher that has pik
as a subset must have a score at least as great as
that of pik. This means that the score never over-
estimates the cost of completing a solution, and
therefore that the heuristic is admissible.
4 Analysis
The bound that we will prove is that for any k > 0
and for any ?, ? > 0, there exists an n ? N such
that if the length n(C) of the cipher C is at least
n, then with probability at least 1 ? ?, the search
for the key to the cipher C requires no more than
2n?(H(p,q)+?) expansions of any partial solution of
size k to complete. Applying the same bound over
every size k of partial solution will then give us
that for any ?, ? > 0, there exists a n0 > 0 such
that if the length n(C) of the cipher C is at least
n, then with probability at least 1 ? ?, the search
for the key to the cipher C requires no more than
2n(H(p,q)+?) expansions of any partial solution of
size greater than 0 to complete (note that there is
only one partial solution of size 0).
Let pi? be the solution that is found by the
search. This solution has the property that it is the
full solution that induces the most probable plain-
text from the cipher, and so it produces a plaintext
that is at least as likely as that of the true solution
P . Thus, we have that ?logprobp(pi?n(C)(C)) ?
?logprobp(pi
n(C)
T (C)) = ?logprobp(P ).
We find our bound by making use of the fact that
an A? search never expands a node whose score
is greater than that of the goal node pi?. Thus, a
partial solution pik is expanded only if
?logprobp(pi
n(C)
k (C)) ? ?logprobp(pi
?n(C)(C)).
Since
?logprobp(pi
?n(C)(C)) ? ?logprobp(P ),
we have that pik is expanded only if
?logprobp(pi
n(C)
k (C)) ? ?logprobp(P ).
So we would like to count the number of solutions
satisfying this inequality.
86
We would first like to approximate the value of
?logprobp(P ), then. But, since P is drawn from
an ergodic stationary distribution q, this value
will approach the cross entropy H(p, q) with high
probability: for any ?1, ?1 > 0, there exists an
n1 > 0 such that if n(C) = n(P ) > N1, then
| ? logprobp(P )/n(C)?H(p, q)| < ?1
with probability at least 1 ? ?1. In this case, we
have that ?logprobp(P ) < n(C)(H(p, q) + ?1).
Now, if k is fixed, and if pik and pi?k are two dif-
ferent size k partial solutions, then pik and pi?k must
disagree on at least one letter assignment. Thus,
the sets ?k and ??k must be disjoint. But then we
also have that pin(C)k (C) 6= pi
n(C)?
k (C). Therefore,
if we can find an upper bound for the size of the
set
{S ? ?n(C)p |S = pi
n(C)
k (C) for some pik},
we will have an upper bound on the number of
times the search will expand any partial solution
of size k. We note that under the previous assump-
tions, and with probability at least 1? ?1, none of
these strings can have a log probability larger than
n(C)(H(p, q) + ?1).
For any plaintext string C drawn from q, we let
aPb be the substring of P between the indices a
and b. Similarly, we let aSb be the substring of
S = pin(C)k (C) between the indices a and b.
We now turn to the proof of our bound: Let
?, ? > 0 be given. We give the following three
bounds on n:
(a) As stated above, we can choose n1 so that for
any string P drawn from q with length at least
n1,
| ? logprobp(P )/n(P )?H(p, q)| < ?1/2
with probability at least 1? ?/3.
(b) We have noted that if k is fixed then any two
size k partial solutions must disagree on at
least one of the letters that they fix. So if we
have a substring aPb of P with an instance of
every letter type fixed by the partial solutions
of size k, then the substrings aSb of S must
be distinct for every S ? {S ? ?n(C)p |S =
pin(C)k (C) for some pik}. Since q is ergodic,
we can find an n2 such that for any string P
drawn from q with length at least n2, every
letter fixed in pik can be found in some length
n2 substring P2 of P , with probability at least
1? ?/3.
(c) By the Lemma below, there exists an n? > 0
such that for all partial solutions pik, there ex-
ists a trigram distribution rk on the alphabet
?p such that if S = pi
n(C)
k (C) and b ? a =
n > n?, then
?
?
?
?
?logprob(aSb)
n
?H(p, rk)
?
?
?
? < ?/4
with a probability of at least 1? ?/3.
Let n = max(n1, n2, n?). Then, the probability
of any single one of the properties in (a), (b) or (c)
failing in a string of length at least n is at most ?/3,
and so the probability of any of them failing is at
most ?. Thus, with a probability of at least 1??, all
three of the properties hold for any string P drawn
from q with length at least n. Let P be drawn from
q, and suppose n(P ) > n. Let aPb be a length n
substring of P containing a token of every letter
type fixed by the size k partial solutions.
Suppose that pik is a partial solution such that
?logprobp(pi
n(C)
k (C)) ? n(P )(H(p, q) + ?/2).
Then, letting S = pin(C)k (C), we have that if
?
?
?
?
?logprob(S)
n(P )
?H(p, rk)
?
?
?
? < ?/4
and
?
?
?
?
?logprob(aSb)
n
?H(p, rk)
?
?
?
? < ?/4
it follows that
?
?
?
?
?logprob(S)
n(P )
+
logprob(aSb)
n
?
?
?
?
?
?
?
?
?
?logprob(S)
n(P )
?H(p, rk)
?
?
?
?
+
?
?
?
??H(p, rk)?
logprob(aSb)
n
?
?
?
?
? ?/4 + ?/4 = ?/2
But then,
?
logprob(aSb)
n
<
?logprob(S)
n(P )
+ ?/2
?
n(P )(H(p, q) + ?/2)
n(P )
+ ?/2
= H(p, q) + ?.
87
So, for our bound we will simply need to find the
number of substrings aSb such that
? log probp(aSb) < n(H(p, q) + ?).
Letting IH(aSb) = 1 if ?logprobp(aSb) <
n(H(p, q) + ?) and 0 otherwise, the number of
strings we need becomes
X
aSb??
n(C)
p
IH(aSb) = 2
n?(H(p,q)+?)
X
aSb??
n(C)
p
IH(aSb)2
?n?(H(p,q)+?)
<2n?(H(p,q)+?)
X
aSb??
n(C)
p
IH(aSb)probp(aSb)
(since ? log probp(aSb) < n(H(p, q) + ?)
implies probp(aSb) > 2?n?(H(p,q)+?))
? 2n?(H(p,q)+?)
X
aSb??
n(C)
p
probp(aSb)
= 2n?(H(p,q)+?)
Thus, we have a bound of 2n?(H(p,q)+?) on
the number of substrings of length n satisfying
? log probp(aSb) < n(H(p, q) + ?). Since we
know that with probability at least 1? ?, these are
the only strings that need be considered, we have
proven our bound. 
4.1 Lemma:
We now show that for any fixed k > 0
and ??, ?? > 0, there exists some n? > 0
such that for all partial solutions pik, there
exists a trigram distribution rk on the al-
phabet ?p such that if S = pi
n(C)
k (C) and
b ? a = n > n?, |?logprob(aSb)n ? H(p, rk)| < ?
?
with a probability of at least 1? ??.
Proof of Lemma: Given any partial solution pik,
it will be useful in this section to consider the
strings S = pin(C)k (C) as functions of the plain-
text P rather than the ciphertext C. Since C =
pi?1T (P ), then, we will compose pi
n(C)
k and pi
?1
T
to get pin(C)?k (P ) = pi
n(C)
k (pi
?1
T (P )). Now, since
piT is derived from a character bijection between
?c and ?p, and since pi
n(C)
k fixes the k character
types in ?c that are defined in pik, we have that
pin(C)?k fixes k character types in ?p. Let ?P1 be
the set of k character types in ?p that are fixed by
pin(C)?k , and let ?P2 = ?p \?P1 . We note that ?P1
and ?P2 do not depend on which pik we use, but
only on k.
Now, any string P which is drawn from q
can be decomposed into overlapping substrings
by splitting it whenever it has see two adjacent
characters from ?P1 . When we see a bigram in
P of this form, say, y1y2, we split P so that both
the end of the initial string and the beginning of
the new string are y1y2. Note that when we have
more than two adjacent characters from ?P1 we
will split the string more than once, so that we get
a series of three-character substrings of P in our
decomposition. As a matter of bookkeeping we
will consider the initial segment to begin with two
start characters s with indices corresponding to 0
and ?1 in P . As an example, consider the string
P = friends, romans, countrymen, lend me
your ears
Where ?P1 = {? ?, ?, ?, ?a?, ?y?}. In this case,
we would decompose P into the strings ?ssfriends,
?, ?, romans, ?, ?, countrymen, ?, ?, lend me ?, ?e y?,
? your e? and ? ears?.
Let M be the set of all substrings that can be
generated in this way by decomposing strings P
which are drawn from q. Since the end of any
string m ?M contains two adjacent characters in
?P1 and since the presence of two adjacent char-
acters in ?P1 signals a position at which a string
will be decomposed into segments, we have that
the set M is prefix-free. Every string m ? M
is a string in ?p, and so they will have probabili-
ties probq(m) in q. It should be noted that for any
m ? M the probability probq(m) may be differ-
ent from the trigram probabilities predicted by q,
but will instead be the overall probability in q of
seeing the string m.
For any pair T, P of strings, let #(T, P ) be the
number of times T occurs in P . Since we as-
sume that the strings drawn from q converge to
the distribution q, we have that for any ?3, ?3 >
0 and any n4 > 0, there exists an n3 > 0
such that for any substring P3 of P of length
at least n3, where P is drawn from q, and for
any m ? M of length at most n4, the number
|#(m,P )/len(P3) ? probq(m)| < ?3 with prob-
ability greater than 1? ?3.
Now suppose that for some P drawn from q
we have a substring aPb of P such that aPb =
m,m ? M . If S = pin(C)?k (P ), consider the sub-
string aSb of S. Recall that the string function
pin(C)?k can map the characters in P to S in one
of two ways: if a character xi ? ?P1 is found at
index i in P , then the corresponding character in S
88
is pik(xi). Otherwise, xi is mapped to whichever
character yi in ?P maximizes the probability in p
of S given pin(C)?k (xi?2)pi
n(C)?
k (xi?1)yi. Since the
values of pin(C)?k (xi?2), pi
n(C)?
k (xi?1) and yi are in-
terdependent, and since pin(C)?k (xi?2) is dependent
on its previous two neighbors, the value that yi
takes may be dependent on the values taken by
pin(C)?k (xj) for indices j quite far from i. How-
ever, we see that no dependencies can cross over
a substring in P containing two adjacent charac-
ters in ?P1 , since these characters are not trans-
formed by pin(C)?k in a way that depends on their
neighbors. Thus, if aPb = m ? M , the endpoints
of aPb are made up of two adjacent characters in
?P1 , and so the substring aSb of S depends only
on the substring aPb of P . Specifically, we see that
aSb = pi
n(C)?
k (aPb).
Since we can decompose any P into overlap-
ping substrings m1,m2, . . . ,mt in M , then, we
can carry over this decomposition into S to break
S into pin(C)?k (m1), pi
n(C)?
k (m2), . . . , pi
n(C)?
k (mt).
Note that the score generated by S in
the A? search algorithm is the sum
?
1?i? logprobp(yi?2yi?1yi), where yi is
the ith character in S. Also note that ev-
ery three-character sequence yi?2yi?1yi
occurs exactly once in the decomposition
pin(C)?k (m1), pi
n(C)?
k (m2), . . . , pi
n(C)?
k (mt). Since
for anym the number of occurrences of pin(C)?k (m)
in S under this decomposition will be equal to the
number of occurrences of m in P , we have that
?logprobp(S) =
X
1?i?n(P )
logprobp(yi?2yi?1yi)
=
X
m?M
#(m,P ) ? (?logprobp(pi
n(C)?
k (m))).
Having finished these definitions, we can
now define the distribution rk. In princi-
ple, this distribution should be the limit of
the frequency of trigram counts of the strings
S = pin(C)?k (P ), where n(P ) approaches infin-
ity. Given a string S = pin(C)?k (P ), where P
is drawn from q, and given any trigram y1y2y3
of characters in ?p, this frequency count is
#(y1y2y3,S)
n(P ) . Breaking S into its component sub-
strings pin(C)?k (m1), pi
n(C)?
k (m2), . . . , pi
n(C)?
k (mt),
as we have done above, we see that any instance
of the trigram y1y2y3 in S occurs in exactly one of
the substrings pin(C)?k (mi), 1 ? i ? t. Grouping
together similar mis, we find
#(y1y2y3, S)
n(P )
=
tP
i=1
#(y1y2y3, pi
n(C)?
k (mi))
n(P )
=
P
m?M
#(y1y2y3, pi
n(C)?
k (m)) ?#(m,P )
n(P )
As n(P ) approaches infinity, we find that #(m,P )n(P )
approaches probq(m), and so we can write
probrk (y1y2y3) =
X
m?M
#(y1y2y3, pi
n(C)?
k (m))probq(m).
Since 0 ?
?
m?M #(y1y2y3, pi
n(C)?
k (m))probq(m)
when P is sampled from q we have that
X
y1y2y3
probrk (y1y2y3)
=
X
y1y2y3
X
m?M
#(y1y2y3, pi
n(C)?
k (m))probq(m)
= lim
n(P )??
X
y1y2y3
X
m?M
#(y1y2y3, pi
n(C)?
k (m))
#(m,P )
n(P )
= lim
n(P )??
X
m?M
X
y1y2y3
#(y1y2y3, pi
n(C)?
k (m))
#(m,P )
n(P )
= lim
n(P )??
X
m?M
(n(pin(C)?k (m))? 2)#(m,P )
n(P )
= lim
n(P )??
X
m?M
(n(m)? 2)#(m,P )
n(P )
= lim
n(P )??
n(P )
n(P )
= 1,
so we have that probrk is a valid probability distri-
bution. In the above calculation we can rearrange
the terms, so convergence implies absolute conver-
gence. The sum
?
y1y2y3 #(y1y2y3, pi
n(C)?
k (m))
gives (n(pin(C)?k (m)) ? 2) because there is one
trigram for every character in pin(C)?k (m), less two
to compensate for the endpoints. However, since
the different m overlap by two in a decomposition
from P , the sum (n(m) ? 2)#(m,P ) just gives
back the length n(P ), allowing for the fact that
the initial m has two extra start characters.
Having defined rk, we can now find the value of
H(p, rk). By definition, this term will be
89
Xy1y2y3
?logprobp(y1y2y3)probrk (y1y2y3)
=
X
y1y2y3
?logprobp(y1y2y3)
X
m?M
#(y1y2y3, pi
n(C)?
k (m))probq(m)
=
X
m?M
X
y1y2y3
?logprobp(y1y2y3)#(y1y2y3, pi
n(C)?
k (m))probq(m)
=
X
m?M
?logprobp(m)probq(m).
Now, we can finish the proof of the Lemma.
Holding k fixed, let ??, ?? > 0 be given. Since we
have assumed that p does not assign a zero proba-
bility to any trigram generated by q, we can find a
trigram x1x2x3 generated by q whose probability
in p is minimal. Let X = ?logprobp(x1x2x3),
and note that probp(x1x2x3) > 0 implies
X < ?. Since we know by the argu-
ment above that when P is sampled from q,
limn(P )??(
?
m?M
(npin(C)?k (m)?2)?#(m,P )
n(P ) ) = 1,
we have that
?
m?M
(npin(C)?k (m)? 2)probq(m) = 1.
Thus, we can choose n4 so that
?
m?M,n(m)?n4
(npin(C)?k (m)? 2)probq(m)
> 1? ??/4X.
Let Y = |{m ? M,n(m) ? n4}|, and choose
n? such that if P is sampled from q and aPb is a
substring of P with length greater than n?, then
with probability at least 1 ? ??, for every m ? M
we will have that
?
?
?
?
#(m, aPb)
n(aPb)
? probq(m)
?
?
?
? < ?
?/4XY (n4 ? 2).
Let pik be any partial solution of length k, and let
rk be the trigram probability distribution described
above. Then let P be sampled from q, and let S =
pin(C)k (C) = pi
n(C)?
k (P ), and let a, b be indices of
S such that b ? a = n > n?. Finally, we will
partition the set M as follows: we let M ? be the
set {m ?M |n(n) ? n4} andM ?? be the set {m ?
M |n(m) > n4}. Thus, we have that
?
?
?
?
?logprob(aSb)
n
?H(p, rk)
?
?
?
?
=
?
?
?
?
?
P
m?M #(m, aPb)(?logprobp(pi
n(C)?
k (m))
n
?
X
m?M
probq(m) ? (?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
.
Grouping the terms of these sums into the index
sets M ? and M ??, we find that this value is at most
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
?
#(m, aPb)
n
? probq(m)
?
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
Furthermore, we can break up the sum over the
index M ?? to bound this value by
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
#(m, aPb)
n
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
probq(m)(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
Now, for any m ? M , we have that
the score ?logprobp(pi
n(C)?
k (m) equals?
1?i?n(m)?2?logprobp(yiyi+1yi+2), where yi
is the character at the index i in pin(C)?k (m).
Taking the maximum possible values for
?logprobp(yiyi+1yi+2), we find that this sum is
at most (n(m)? 2)X . Applying this bound to the
previous formula, we find that it is at most
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(n(m)? 2)X
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
probq(m) ? (n(m)? 2)X
?
?
?
?
?
.
We can bound each of these three terms separately.
Looking at the first sum in this series, we find that
with probability at least 1? ??,
90
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(n(m)? 2)X
?
?
?
?
?
(*)
?
X
m?M?
?
?
?
?
#(m, aPb)
n
? probq(m)
?
?
?
? (n(m)? 2)X
?
X
m?M?
?
?
?
?
??
4(n4 ? 2)XY
?
?
?
? ? (n(m)? 2)X
?
X
m?M?
?
?
?
?
??
4Y
?
?
?
?
=
??
4Y
X
m?M?
1 =
??
4Y
Y = ?/4.
In order to bound the second sum, we make use
of the fact that
?
m?M #(m, aPb)(n(m) ? 2) =
n(aPb) = n to find that once again, with probabil-
ity greater than 1? ??,
?
?
?
?
?
X
m?M??
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
?
?
X
m?M??
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
? .
Since M ?? = M ?M ?, this value is
X
m?M
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
?
X
m?M?
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
=X ?
X
m?M?
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
? .
This value can further be split into
=X?
X
m?M?
?
?
?
?
?
#(m, aPb)
n
+(1?1)probq(m)
?
(n(m)?2)X
?
?
?
?
?X ?
 
X
m?M?
|probq(m)(n(m)? 2)X|
?
X
m?M?
?
?
?
?
#(m, aPb)
n
? probq(m)
?
?
?
? (n(m)? 2)X
!
Using our value for the sum in (*), we find that
this is
=X ?
X
m?M?
|probq(m)(n(m)? 2)X|
+
X
m?M?
?
?
?
?
#(m, aPb)
n
? probq(m)
?
?
?
? (n(m)? 2)X
?X ?
X
m?M?
|probq(m)(n(m)? 2)X|+
??
4
,
Using our definition of n4, we can further bound
this value by
=X
 
1?
X
m?M?
probq(m)(n(m)? 2)
!
+
??
4
<X
?
1?
?
1?
??
4X
??
+
??
4
=X
??
4X
+
??
4
=
??
2
.
Finally, we once again make use of the definition
of n4 to find that the last sum is
?
?
?
?
?
X
m?M??
probq(m) ? (n(m)? 2)X
?
?
?
?
?
=
X
m?M??
probq(m) ? (n(m)? 2)X
= X
X
m?M??
probq(m) ? (n(m)? 2)
< X
??
4X
=
??
4
.
Adding these three sums together, we get
??
4
+
??
2
+
??
4
= ??.
Thus,
?
?
?
?logprob(aSb)
n ?H(p, rk)
?
?
? < ?? with prob-
ability greater than 1? ??, as required. 
5 Conclusion
In this paper, we discussed a discrepancy between
the theoretical and practical running times of cer-
tain algorithms that are sensitive to the entropies
of their input, or the entropies of the distributions
from which their inputs are sampled. We then
used the algorithm from Corlett and Penn (2010)
as a subject to allow us to investigate ways to
talk about average-case complexity in light of
this discrepancy. Our analysis was sufficient
to give us a bound on the search complexity
of this algorithm which is exponential in the
cross-entropy between the training distribution
and the input distribution. Our method in effect
yields a probabilistic bound on the depth of the
search heuristic used. This leads to an exponen-
tially smaller search space for the overall problem.
We must note, however, that our analysis does
not fully reconcile the discrepancy between the
91
theoretical and practical running time for this
algorithm. In particular, our bound still does not
explain why the number of search nodes expanded
by this algorithm tends to converge on one per
partial solution size as the length of the string
grows very large. As such, we are interested in
further studies as to how to explain the running
time of this algorithm. It is our opinion that this
can be done by refining our description of the sets
?k to exclude strings which cannot be considered
by the algorithm. Not only would this allow us
to reduce the overall number of strings we would
have to count when determining the bound, but
we would also have to consider fewer strings
when determining the value of n?. Both changes
would reduce the overall complexity of our bound.
This general strategy may have the potential to
illuminate the practical time complexities of ap-
proximate search algorithms as well.
References
David Arthur, Bodo Manthey, and Heiko Ro?glin.
k-means has polynomial smoothed complex-
ity. In The 50th Annual Symposium on Foun-
dations of Computer Science. IEEE Computer
Society Technical Committee on Mathematical
Foundations of Computing, 2009. URL http:
//arxiv.org/abs/0904.1113.
Eric Corlett and Gerald Penn. An exact A? method
for deciphering letter-substitution ciphers. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 1040?1047, 2010.
Richard E Korf and Michael Reid. Complexity
analysis of admissible heuristic search. In Pro-
ceedings of the Fifteenth National Conference
on Artificial Intelligence, 1998.
Richard E Korf, Michael Reid, and Stefan
Edelkamp. Time complexity of iterative-
deepening-A?. Artificial Intelligence, 129(1?2):
199?218, 2001.
Judea Pearl. Heuristics: Intelligent Search Strate-
gies for Computer Problem Solving. Addison-
Wesley, 1984.
Daniel A Spielman and Shang-Hua Teng.
Smoothed analysis of algorithms: Why the
simplex algorithm usually takes polynomial
time. Journal of the ACM, 51(3):385?463,
2004.
92
