Quantitative and Qualitative Evaluation of the OntoLearn Ontology Learning
System
Roberto NAVIGLI, Paola VELARDI
Dipartimento di Informatica
Universit? ?La Sapienza?
via Salaria 113
Roma, Italy, 00198
{velardi,navigli}@di.uniroma1.it
Alessandro CUCCHIARELLI, Francesca NERI
DIIGA
Universit? Politecnica delle Marche
via Brecce Bianche 12
Ancona, Italy, 60131
{cucchiarelli, neri}@diiga.univpm.it
Abstract
Ontology evaluation is a critical task, even more so
when the ontology is the output of an automatic
system, rather than the result of a conceptualisation
effort produced by a team of domain specialists
and knowledge engineers. This paper provides an
evaluation of the OntoLearn ontology learning
system. The proposed evaluation strategy is
twofold: first, we provide a detailed quantitative
analysis of the ontology learning algorithms, in
order to compute the accuracy of OntoLearn under
different learning circumstances. Second, we
automatically generate natural language
descriptions of formal concept specifications, in
order to facilitate per-concept qualitative analysis
by domain specialists.
1 Evaluating ontologies
Automatic methods for ontology learning and
population have been proposed in recent literature
(e.g. ECAI-2002 and KCAP-2003 workshops1) but
a co-related issue then becomes the evaluation of
such automatically generated ontologies, not only
with the goal of comparing the different
approaches (Hovy, 2001) and ontology-based tools
(Angele and Sure, 2002), but also to verify whether
an automatic process may actually compete with
the typically human process of converging on an
agreed conceptualization of a given domain.
Ontology construction, apart from the technical
aspects of a knowledge representation task (i.e.
choice of representation languages, consistency
and correctness with respect to axioms, etc.), is a
consensus building process, one that implies long
and often harsh discussions among the specialists
of a given domain. Can an automatic method
simulate this process? Can we provide domain
specialists with a means to measure the adequacy
of a specific set of concepts as a model of a given
                                                       
1ECAI-2002 http://www-sop.inria.fr/acacia/WORKSHOPS/
ECAI2002-OLT/accepted-papers.html
KCAP-2003 http://km.aifb.uni-karlsruhe.de/ws/semannot
2003/papers.html
domain?, Specialists are often unable to evaluate
the formal content of a computational ontology
(e.g. the denotational theory, the formal notation,
the knowledge representation system capabilities
like property inheritance, consistency, etc.).
Evaluation of the formal content is rather tackled
by computational scientists, or by automatic
verification systems. The role of the specialists is
instead to compare their intuition of a domain with
the description of this domain, as provided by the
ontology concepts. To facilitate one such
qualitative per-concept evaluation, we devised a
method for automatic generation of textual
explanations (glosses) of automatically learned
concepts. Glosses provide a description, in natural
language, of the formal specifications assigned to
the learned concepts. An expert can easily compare
his intuition with these natural language
descriptions.
The objective of the gloss-based evaluation is, as
previously remarked, to obtain a judgement, by
domain specialists, concerning the adequacy of an
automatically derived domain conceptualisation.
On the computational side, an ontology learning
tool is based on a battery of software programs
aimed at extracting and formalising domain
knowledge, usually starting from unstructured
data. Therefore, it is equally important to produce a
detailed evaluation of these programs, on a
quantitative ground, in order to gain insight on the
internal and external contingencies that may affect
the result of an ontology learning process.
In what follows, we firstly provide a quantitative
evaluation of the OntoLearn ontology learning
system, under different learning circumstances.
Secondly, we describe the gloss-based per-concept
evaluation method. Both evaluation strategies are
experimented in two application domains: Tourism
and Economy.
The subsequent section provides a sketchy
description of the OntoLearn algorithms. Details
are found in (Navigli and Velardi, 2004) and
(Navigli, Velardi and Gangemi, 2003). Sections 3
and 4 are dedicated to the quantitative and
qualitative analyses of OntoLearn.
2 Summary of the OntoLearn system
OntoLearn is an ontology population method
based on text mining and machine learning
techniques. OntoLearn starts with an existing
generic ontology (we use WordNet, though other
choices are possible) and a set of documents in a
given domain, and produces a domain extended
and trimmed version of the initial ontology. The
ontology generated by OntoLearn is anchored to
texts, it can be therefore classified as a linguistic
ontology (G?mez-P?rez et al 2004).
OntoLearn has been applied to different domains
(tourism, computer networks, economy) and in
several European projects2.
Concept learning is achieved in the following
three phases:
1) Terminology Extraction: A list of domain
multi-word expressions (MWE hereafter) is
extracted from a set of documents that are
judged representative of a given domain.
MWEs are extracted using natural language
processing and statistical techniques.
Contrastive corpora and glossaries in different
domains are used to prune terminology that is
not domain-specific. Domain MWEs are
selected also on the basis of an entropy-based
measure that simulates specialist consensus on
concepts choice: in words, the probability
distribution of a ?good? domain MWE must be
uniform across the individual documents of the
domain corpus.
2) Semantic interpretation of MWEs: Semantic
interpretation is based on a principle,
compositional interpretation, and on a novel
algorithm, called structural semantic
interconnections (SSI) .  Composi t ional
interpretation signifies that the meaning of a
multi-word expression (MWE) can be derived
compositionally from its components3, e.g. the
meaning of business plan is derived first, by
associating the appropriate concept identifier,
with reference to the initial top ontology, to the
component terms (i.e. sense #2 of business and
sense #1 of plan in WordNet), and then, by
identifying the semantic relations holding
among the involved concepts (e.g.
                                                       
2
 E.g. : Harmonize IST-2000-29329  and the INTEROP network of
excellence, started on december 2003.
3
 In the literature, multi word expressions are classified as
compositional, idiosyncratically compositional and non-
compositional.  In mid-technical domains, compositional MWEs
cover about 60-70% of MWE (we cannot support with data our
statitics for sake of space)
plan#1 topic? ? ? ? business# 2 ).
3) Extending and trimming the initial
o n t o l o g y : Once the terms have been
semantically interpreted, they are organized in
sub-trees, and appended under the appropriate
node  o f  t he  i n i t i a l  on to logy,
e.g. business _ plan# 1 kind _ of? ? ? ? ? ? plan# 1 .
Furthermore, certain upper and lower nodes of
the initial ontology are pruned to create a
domain-view  of the ontology. The final
ontology is output in OWL language.
SSI lies in the area of syntactic pattern matching
algorithms (Bunke and Sanfeliu, 1990). It is a word
sense disambiguation algorithm used to determine
the correct sense (with reference to the initial
ontology) for each component of a complex MWE.
The algorithm is based on building a graph
representation for alternative senses of each MWE
component4, and then selecting the appropriate
senses on the basis of detected s e m a n t i c
interconnection patterns between graph pairs. The
SSI algorithm seeks for semantic interconnections
among the words of a context T. Contexts Ti are
generated from groups of partially overlapping
complex MWEs (extracted during phase 1 of the
OntoLearn procedure) sharing the same syntactic
head . For example, given the list of complex
MWEs securities portfolio, investment portfolio,
real-estate portfolio, junk-bond portfolio,
diversified portfolio, stock portfolio, bond
portfolio, loan portfolio, the following list of term
components is created:
T=[security, investment, real-estate, estate, bond,
junk-bond, diversified, stock, portfolio, loan ]
Relevant pattern types  are described by a
context free grammar G. An example of rule in G
is the following (S1 S2 and S are concepts, i.e.
synsets in WordNet):
Rule Name:gloss+hyperonymy/meronymy (S1,S2):
Def: :SynsetsG?? S1
gloss
? ? ? ?  S
 and there is a
hyperonymy/meronymy path between S and S2
For instance, in railways company, the gloss of
railway#1 contains the word organization, and
there is an hyperonymy path of length 2 between
company#1 and organization#1. That is:
railway#1 gloss? ? ? ?  o r g a n i z a t i o n # 1 ,  a n d :
company#1
  
kind _of
? ? ? ? ? ? institution#1 
  
kind _of
? ? ? ? ? ? 
organization#1. This pattern (an instance of the
gloss+hypeonymyr/meronymy rule) cumulates
                                                       
4
 We remark again that a detailed description of the SSI algorithm
is in (Navigli & Velardi, 2004) and (Navigli, Velardi and Gangemi,
2003). Graphs are generated on the basis of lexico-semantic
information in WordNet and in a variety of on-line resources, see the
mentioned papers for details.
evidence for senses #1 of both ra i lway and
company.
In SSI, the correct sense St for a term t?T is
selected depending upon the number and weight of
patterns matching with rules in G. The weights of
patterns are automatically learned using a
perceptron5 model. The weight function is given
by:
)
_
1
()()1(
j
jjj patternlength
patternweight ?? +=
where ? j  is the weight of rule j in G, and the
second addend is a smoothing parameter inversely
proportional to the length of the matching pattern
(e.g. 2 in the previous example, since 2 is the
minimal length of the rule, and the actual length of
the pattern is 3). The perceptron has been trained
on the SemCor6 semantically annotated corpus.
In order to complete the semantic interpretation
process, OntoLearn then attempts to determine the
semantic relations that hold between the
components of a complex concept. In order to do
this, it was first necessary to select an inventory of
semantic relations. We examined several
proposals, like EuroWordnet (Vossen, 1999),
DOLCE (Masolo et al, 2002), FrameNet
(Ruppenhofer Fillmore & Baker, 2002) and others.
 As also remarked in (Hovy, 2001), no
systematic methods are available in literature to
compare the different sets of relations. Since our
objective was to define an automatic method for
semantic relation extraction, our final choice was
to use a reduced set of FrameNet relations, which
seemed general enough to cover our application
domains (tourism, computer networks, economy).
The choice of FrameNet is motivated by the
availability of a sufficiently large set of annotated
examples of conceptual relations7, that we used to
train an available machine learning algorithm,
TiMBL (Daelemans et al, 2002). The relations
used are: Material, Purpose, Use, Topic, Product,
Constituent Parts, Attribute8. Examples for each
relation are the following:
net # 1 attribute? ? ? ? ? ? loss# 3
takeover# 2 topic? ? ? ? proposal# 1
sand# 1 material? ? ? ? ? ? beach# 1
merger# 1 purpose? ? ? ? ? ? agreement# 1
                                                       
5
 http://www.cs.waikato.ac.nz/ml/weka/
6
 http://www.cs.unt.edu/~rada/downloads.html#semcor
7
 The choice of FrameNet was motivated more by availability than
appropriateness.
8
 The relation Attribute is not in FrameNet, however it was a
useful relation for terminological strings of the adjective_noun type.
meeting# 1 use? ? ? ? ro om# 1
bond# 2 const _ part? ? ? ? ? ? ? market# 1
c om puter# 1 product? ? ? ? ?  com pany# 1
We represented training instances as pairs of
concepts annotated with the appropriate conceptual
relation, e.g.:
[(computer#1,maker#3),Product]
Each concept is in turn represented by a feature-
vec to r  where attributes are the concept?s
hyperonyms in WordNet, e.g.:
(computer#1,maker#3):
((computer#1,machine#1,device#1
,instrumentality#3),(maker#3,business
#1,enterprise#2,organization#1))
3 Quantitative Evaluation of OntoLearn
This section provides a quantitative evaluation of
OntoLearn s main algorithms. We believe that a
quantitative evaluation is particularly important in
complex learning systems, where errors can be
produced at almost any stage. Even though some
of these errors (e.g. subtle sense distinctions) may
not have a percievable effect on the final ontology,
as shown by the results of the qualitative
evaluation in Section 4.2, it is nevertheless
important to gain insight on the actual system
capabilities, as well as on the pararmeters and
external circumstances that may positively or
negatively influence the final performance.
3.1 Evaluating the MWE extraction
algorithm
The terminology extraction algorithm has been
evaluated in the context of the European project
Harmonise on Tourism interoperability. We first
collected a corpus of about 1 million words of
tourism documents, mainly descriptions of travel
and tourism sites. From this corpus, a syntactic
parser extracted an initial list of 14,383 candidate
complex MWEs from which the statistical filters
selected a list of 3,840 domain-relevant complex
MWEs, that were submitted to the domain
specialists. The Harmonise ontology partners were
not skilled to evaluate the OntoLearn semantic
interpretation of MWEs, therefore we let them
evaluate only the domain appropriateness of the
terms. The gloss generation method described in
Section 4 was subsequently concieved to overcome
this limitation.
We obtained a precision ranging from 72.9% to
about 80% and a recall of 52.74%. The precision
shift is due to the well-known fact that experts may
have different intuitions about the relevance of a
concept. The recall estimate was produced by
manually inspecting 6,000 of the initial 14,383
candidate MWEs, asking the experts to mark all
the MWEs judged as ?good? domain MWEs, and
comparing the obtained list with the list of terms
automatically filtered by OntoLearn.
We ran similar experiments on an Economy
corpus and a Computer Network corpus, but in this
case the evaluation was performed by the authors.
Overall, the performance of the MWE extraction
task appears to be influenced by the dimension and
the focus of the starting corpus (e.g. ?generic
tourism? vs. ?hotel accomodation descriptions?).
Small and unfocused corpora do not favor the
efficacy of statistical analysis. However, the
availability of sufficiently large and focused
corpora seems a realistic requirement for most
applications.
3.2 Evaluating the ontology learning
algorithms
The distinctive task performed by OntoLearn is
semantic disambiguation. The performance of the
SSI algorithm critically depends upon two factors:
the first is the ability to detect semantic
interrelations among concepts associated to the
words of complex MWEs, the second is the
dimension of the context T available to start the
disambiguation process.
As for the first factor, there are two possible
ways of enhancing reliable identification of
semantic interconnections: one is to tune at best the
weight of individual rules in G (e.g. formula (1) in
Section 2), the second is to enrich the semantic
information associated to alternative word senses.
The latter is an on-going research activity.
As far as the context T is concerned, the
intuition is that, with a larger T , there are higher
chances of detecting semantic patterns among the
?correct? senses of the terms in T. However, the
dimension of contexts Ti is an external
contingency, it depends upon the available corpus.
Accordingly, we evaluated the SSI algorithm
using as parameters the dimension of T, T , and
the weights associated to rules in G. We ran several
experiments over the full terminology extracted
from the Economy and Tourism corpora, but
performances are computed only on, respectively,
453 and 638 manually disambiguated terms. This
means that in a context Ti including, e.g. k terms,
we evaluate OntoLearn?s sense choices only for
the fragment of j ? k terms, for which the ?true?
sense has been manually assigned.
Table 1 shows the performance of SSI (precision
and recall) when using only patterns whose weight,
computed with formula (1) is over a threshold ? .
The ?Core? column in Table 1 shows the
performance of SSI when accepting only these core
patterns, while the third column refers to all
matching patterns. With ? = 0,7  a subset of 7-9
rules9 in G (over a total of 20) are used by the
algorithm. Interestingly enough, these rules have a
high probability of being hired, as shown by the
relatively low difference in recall. The Baseline
tower in Table 1 is computed selecting always the
first sense (senses in WordNet are ordered by
probability in everyday language).
Table 2 shows that performance of SSI is indeed
affected by the dimension of T. Large T , as
expected, improves the performance, however,
overly large contexts (>80 terms) may favor the
detection of non-relevant patterns.
In general, both experiments show that the
Economy corpus performs better than the Tourism,
since the latter is less technical (the baseline is
quite high), rather unfocused, and contexts Ti are
much less populated.
Table 1. Performances as a function of pattern?s
weight
Table 2. Performances as a function of |T|
We remark that SSI performs better than
standard WSD (word sense disambiguation) tasks
but this is also motivated by the fact that context
words in T are more interrelated than co-occurring
words in generic sentences. The SSI algorithm, by
                                                       
9
 in formula (1), ? , that depends upon the rule, has a much
higher influence than ? , that depends upon the matching pattern)
86.40%
57.17% 53.76%
59.72%
80.21%76.48%
81.77%
85.48%
71.30% 67.33%
0%
20%
40%
60%
80%
100%
Baseline Core All Rules Baseline Core All Rules
Prec.
Recall Finance Tourism
78.57% 81.82% 82.84% 80.29% 80%
58.51% 63.28%
73.16%
55.46%
73.20%
0%
20%
40%
60%
80%
100%
|T| < 35 35 <= |T| < 65 |T| >= 65 |T| < 35 |T| >= 35
Prec. Recall
Finance Tourism
its very nature, is favored by focused and large
contexts. In any case, it is worth mentioning that
SSI received the second best score in the latest
SenSeval-310, gloss disambiguation exercise,
placed about 1% below the first and about 11%
before the third participant.
3.3 Evaluating the semantic annotation
algorithm
To test the semantic relation annotation task, we
used a learning set (including selected annotated
examples from FrameNet (FN), Tourism (Tour),
and Economy (Econ)), and a test set with a
distribution of examples shown in Table 3.
Table 3. Distribution of examples in the learning
and test set for the semantic annotation task
Learning Set Test Set
Sem_Rel FN Tour Econ Tot FN Tour Econ Tot
MATERIAL 8 3 0 11 5 2 0 7
USE 9 32 2 43 6 20 1 27
TOPIC 52 79 100 231 29 43 50 122
C_PART 3 7 12 22 2 4 6 12
PURPOSE 26 64 22 112 14 34 11 59
PRODUCT 3 1 6 10 1 1 4 6
Total 101 186 142 429 57 104 72 233
Notice that the relation Attribute is generated
whenever the term associated to one of the
concepts is an adjective. Therefore, this semantic
relation is not included in the evaluation
experiment, since it would artificially increase
performances. We then tested the learner on test
sets for individual domains11, leading to the
results shown in Table 4 a and b.
Table 4 Performance of the semantic annotation
task on a) Tourism b) Economy
d<=10% d<=30% d<=100%
Precision MACRO 0,958 0,875 0,847
Recall MACRO 0,283 0,636 0,793
F1 MACRO 0,437 0,737 0,819
Precision micro 0,900 0,857 0,798
Recall micro 0,087 0,635 0,798
F1 micro 0,158 0,721 0,798
d<=10% d<=30% d<=100%
Precision MACRO 1,000 0,804 0,651
Recall MACRO 0,015 0,403 0,455
F1 MACRO 0,030 0,537 0,536
Precision micro 1,000 0,758 0,750
Recall micro 0,042 0,653 0,750
F1 micro 0,080 0,701 0,750
The performance measures are those adopted in
TREC competitions12. The parameter d  in the
above Tables is a confidence factor defined in the
TiMBL algorithm. This parameter can be used to
                                                       
10
 SensEval?3   http://www.senseval.org/senseval3
11
 This of course penalised the results (the performance over a test
set composed by examples of all the three domains is much higher),
but provides a more realistic test bed of the generality of the approach.
12
 http://trec.nist.gov/
increase system?s robustness in the following way:
whenever the confidence associated by TiMBL to
the classification of a new instance is lower than a
given threshold, we output a ?generic? conceptual
relation, named Relatedness. We experimentally
fixed the threshold for d  around 30% (central
column of Table 4).
Table 4 demonstrates rather good performances,
however the main problem with semantic relation
annotation is the unavailability of an agreed set of
conceptual relations, and a sufficiently large and
balanced training set. Consequently, we need to
update the set of used relations whenever we
analyse a new domain, and re-run the training
phase enriching the training corpus with manually
tagged examples from the new domain (as for in
Table 2).
4  Qualitative evaluation: Evaluating the
generated ontology on a per-concept basis
The lesson learned during the Harmonise EC
project was that the domain specialists, tourism
operators in our case, can hardly evaluate the
formal aspects of a computational ontology. When
presented with the domain extended and trimmed
version of WordNet (OntoLearn?s phase 3 in
Section 2), they were only able to express a generic
judgment on each node of the hierarchy, based on
the concept label. These judgments were used to
evaluate the terminology extraction task, but the
experiment suggested that, indeed, it was necessary
to provide a better description for the learned
concepts.
4.1 Gloss generation grammar
To help human evaluation on a per-concept
basis, we decided to enhance OntoLearn with a
gloss generation algorithm. The idea is to generate
glosses in a way that closely reflects the key
aspects of the concept learning process, i.e.
semantic disambiguation and annotation with a
conceptual relation.
The gloss generation algorithm is based on the
definition of a grammar with distinct generation
rules for each type of semantic relation.
Let Sih
sem _ rel
? ? ? ? ? ? Sj
k
 be the complex concept
associated to a complex term whw k (e.g. jazz
festival, or long-term debt), and let:
<H>= the syntactic head of whwk (e.g. festival,
debt)
<M> = the syntactic modifier of whwk (e.g. jazz,
long-term)
<GNC>= be the gloss of the new complex concept
Shk
<HYP>= the selected sense of <H>(e.g.
respectively, festival#1 and debt#1).
<MSGHYP>= the main sentence13 of the
WordNet gloss of <HYP>
<MSGM>= the main sentence of the WordNet
gloss of the selected sense for <M>
Here we provide two examples of rules for
generating GNCs:
If sem_rel=Topic, <GNC>:: = a kind of <HYP>,
<MSGHYP>, relating to the <M>, <MSGM>.
e,g.: GNC(jazz festival): a kind of festival,  a
day or period of time set aside for feasting and
celebration,  relating to the jazz, a style of dance
music popular in the 1920.
If sem_rel=Attribute, <GNC>:= a kind of <HYP>,
<MSGHYP>, <MSGM>.
e.g.:GNC(long term debt)= a kind of debt, the
state of owing something (especially money),
relating to or extending over a relatively long time.
4.2 Per-concept evaluation experiment
To verify the utility of gloss generation, the
automatically generated glosses were submitted for
evaluation to two human experts, a tourism
specialist from ECCA14, and an economist from
the University of Ancona. The specialists were not
aware of the method used to generate glosses; they
have been presented with a list of concept-gloss
pairs and asked to fill in an evaluation form (see
Appendix) as follows: vote 1 means
?unsatisfactory definition?, vote 2 means ?the
definition is helpful?, vote 3 means ?the definition
is fully acceptable?. Whenever he was not fully
happy with a definition (vote 2 or 1), the specialist
was asked to provide a brief explanation. For
comparison, Appendix 2 shows also glossary
definitions extracted from the web for the same
MWEs, that were not shown to the specialists.
Table 5 provides a summary of the
evaluation..
Table 5. Evaluation of glosses by domain
specialists.
vote =1 vote=2 vote=3 uncertai
n
average
Tourism
total
(97)
33
(34.7)
14
(14.4)
45
(46.3)
5 (5.1) 2,13
Ecomo
my total
(134)
52
(38.8)
16
(11.9)
66
(49.2)
- 2.10
The following conclusions can be drawn from
this experiment:
1 .  Overall, the two domain specialists fully
accepted the system?s choices in 45-49% of the
cases, and were reasonably satisfied in 12-14%
                                                       
13
 The main sentence is the gloss pruned of subordinates,
examples, etc.
14
 ECCA ? eTourism Competence Center Austria.
of the cases. The average vote is above 2 in
both cases.
2. As expected, if a MWE is compositional, the
generated definition is more often accepted or
fully accepted (e.g. examples 25_E and 14_T
in Appendix 2). When a compositional
interpretation is not accepted (vote=1), this is
motivated either by an OntoLearn
interpretation error (wrong sense or wrong
conceptual relations) or by the unavailability
of a correct sense in WordNet, despite the fact
that the sense is not idiosyncratic. OntoLearn
errors for compositional MWEs are 7 (5%) in
Economy and 12 (13%) in Tourism.  Examples
of OntoLearn errors and core ontology
?misses? are the definitions 14_T (wrong sense
of form) and 19_E (no good sense for bilateral
in WordNet), respectively.
3.  Sometimes the specialists found it acceptable
also an idiosyncratic or non compositional
definition. This happens in 16 cases for the
Tourism domain (16%) and in 19 cases for the
Economy domain (13%). Examples are the
MWEs 45_E and 76_E, both idiosyncratically
decomposable, in Appendix 2.
One of the specialists is particularly involved in
ontology building projects, therefore we report his
valuable comment: ?some of the descriptions
would not be appropriate to take them over in a
tourism ontology just as they are. But most of them
are quite helpful as basis for building the ontology.
The most important problem from my point of view
is the too detailed descriptions of the components
itself instead of the meaning of the overall term in
this context. Best example is the term ?bed tax?.
Nobody would expect a definition of a bed or a
tax.? In other terms, he found disturbing the fact
that a definition extensively reports the definitions
of its components. On the other side, our objective
is not only to produce concept definitions, but also
to organize concepts in hierarchies. Showing the
definitions of individual components is a ?natural?
mean to verify that the correct senses have been
selected (e.g. the correct senses of bed and tax).
This is clearly the case, since, for example in
definition 14_T (booking form), the specialist was
immediately able to diagnose a sense
disambiguation error for form , though he was
unaware of the OntoLearn methodology.
5 Concluding remarks
This paper presented an in-depth evaluation of
the Ontolearn ontology learning system. The three
basic algorithms (terminology extraction, sense
disambiguation and annotation with semantic
relation) have been individually evaluated in two
domains, under different parametrizations, to
obtain a realistic and comprehensible picture of
system?s capabilities. The critical algorithm, SSI,
has very good performances that are favored by the
fact that word sense disambiguation is applied to
group of words (domain MWEs) that are strongly
semantically related, unlike for generic WSD tasks
(e.g. Senseval). The performance of the SSI
algorithm can be further improved through an
extension of the grammar G, which is an on-going
research activity.
6 Acknowledgements
Our thanks go to Dr. Wolfram H?pken, from
ECCA ? eTourism Competence Center Austria
(wolfram@hoepken.org ) and Dr. Renato
Iacobucci, from the University of Ancona, who
gave up their precious time to evaluate our glosses.
This work has been in part supported by the
INTEROP Network of Excellence IST-2003-
508011
References
J. Angele and Y. Sure (2002) ?Whitepaper:
Evaluation of Ontology-based Tools?, Workshop
on evaluation of ontology-based tools
(EON2002), at the 13th Int. EKAW 2002,
Sigueza (Spain), September 2002.
H. Bunke and A. Sanfeliu (editors) (1990).
Syntactic and Structural pattern Recognition:
Theory and Applications, World Scientific,
Series in Computer Science vol. 7, 1990.
Daelemans,W. Zavrel, J. Van den Sloot, K. & Van
den Bosch, A. (2002). TiMBL: Tilburg Memory
Based Learner. Version 4.3 Reference Guide.
Tilburg University.
G?mez-P?rez, A., Fern?ndez-Lopez M. and
Corcho O. (2004). Ontological Engineering,
Springer Verlag, London, 2004.
Hovy, E. (2001). Comparing Sets of Semantic
relations in Ontologies. In R. Geen, C.A. Bean
and S. Myaeng Semantic of relations. Kluwer.
Masolo, C., Borgo, S., Gangemi, A., Guarino, N.
Oltramari, A. & Schneider, L. (2002).
Sweetening Ontologies with DOLCE.
Proceedings of the 13th International Conference
on Knowledge Engineering and Knowledge
Management. Ontologies and the Semantic Web.
Navigli, R. & Velardi, P. (2004). Learning Domain
Ontologies from Document Warehouses and
Dedicated Web Sites. Computational Linguistics,
MIT press, (50)2.
Navigli, R., Velardi, P. Gangemi, A. (2003).
Corpus Driven Ontology Learning: a Method
and its Application to Automated Terminology
Translation. IEEE Intelligent Systems (18)1.22-
31.
Ruppenhofer, J., Fillmore, C.J. & Baker, C.F.
(2002). Collocational Information in the
FrameNet Database. In Braasch, A. and Povlsen,
C. (eds.), Proceedings of the Tenth Euralex
International Congress. Copenhagen, Denmark.
Vol. I: 359--369, 2002.
Vossen, P. (1999). EuroWordNet: General
D o c u m e n t .  V e r s i o n  3  F i n a l .
http://www.hum.uva.nl/~ewn
APPENDIX: Excerpt of the per-concept evaluation form
Concept #: 25_E Term: business_plan Synt: N-N Rel<w1,w2>: Topic
Gloss: a kind of plan, a series of steps to be carried out or goals to be accomplished, relating to the business, the activity
of providing goods and services involving financial and commercial and industrial aspects.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: a written report that states what a company (or a part of a company) aims to do increase sales,
develop new products, etc. within a certain period, and how it will obtain the necessary finances and resources.
Concept #: 2_T Term: affiliated_hotel Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of hotel, a building where travellers can pay for lodging and meals and other services, being joined in close
association.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: a hotel that is a member of a chain, franchise, or referral system. Membership provides special
advantages, particularly a national reservation system.
Concept #: 14_T Term: booking_form Synt: N-N Rel<w1,w2>: Purpose
Gloss: a kind of form, alternative names for the body of a human being, for booking, the act of reserving (a place or
passage) or engaging the services of (a person or group).
Specialist vote: 1
Comment by Specialist: definition of form  wrong in this context
Diagnose: OntoLearn disambiguation error for form
Glossary definition: a document which purchasers of tours must complete to give the operator full particulars about who
is buying the tour.
Concept #: 19_E Term: bilateral_aid Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, having
identical parts on each side of an axis.
Specialist vote: 1
Comment by Specialist: Fully wrong definition.
Diagnose: WordNet gloss of bilateral  is not adequate to domain (no better definition is available in WordNet).
Glossary definition:  assistance given by one country to another.
Concept #: 45_E Term: cyclical_uneployment Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of unemployment, the state of being unemployed or not having a job, recurring in cycles.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: workers are without a job because of a lack of aggregate demand due to a down turn in economic
activity.
Concept #: 76_E Term: foreign_aid Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, of
concern to or concerning the affairs of other nations .
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary defonition: the international transfer of public and private funds in the form of loans or grants from donor
countries to recipient countries.
Squibs and Discussions 
Unsupervised Named Entity Recognition 
Using Syntactic and Semantic Contextual 
Evidence 
Alessandro Cucchiarelli* 
Universita di Ancona 
Paola Velardi t
Universit~i di Roma 'La Sapienza' 
Proper nouns form an open class, making the incompleteness of manually or automatically earned 
classification rules an obvious problem. The purpose of this paper is twofold:first, o suggest the use 
of a complementary "backup" method to increase the robustness of any hand-crafted ormachine- 
learning-based NE tagger; and second, to explore the effectiveness of using more fine-grained 
evidence--namely, s ntactic and semantic ontextual knowledge--in classifying NEs. 
1. Proper Noun Classification 
In this paper we present a corpus-driven statistical technique that uses a learning 
corpus to acquire contextual classification cues, and then uses the results of this 
phase to classify unrecognized proper nouns (PN) in an unlabeled corpus. Training 
examples of proper nouns are obtained using any available named entity (NE) recog- 
nizer (in our experiments we used a rule-based recognizer and a machine-learning- 
based recognizer). The contextual model of PN categories i learned without supervi- 
sion. 
The approach described in this paper is complementary to current methods for 
NE recognition: our objective is to improve, without additional manual effort, the 
robustness of any available NE system through the use of more "fine-grained" con- 
textual knowledge, best exploited at a relatively late stage of analysis. The method is 
particularly useful when an available NE system must be rapidly adapted to another 
language or to another domain, provided the shift is not dramatic. 
Furthermore, our study provides experimental evidence relating to two issues 
still under debate: i) the effectiveness, in practical NLP applications, of using syntactic 
relations (most systems use plain collocations and morphological features), and ii) 
context expansion based on thesauri. While we do not provide a definitive argument 
in favor of syntactic ontexts and semantic expansion for word sense disambiguation 
tasks in general, we do show that they can be successfully used for unknown proper 
noun classification. Proper nouns have particular characteristics, such as low or zero 
ambiguity, which makes it easier to characterize their contexts. 
2. Description of the U_PN Classification Method 
In this section we briefly summarize the corpus-based tagging technique for the classi- 
fication of unknown proper nouns (for more details, see Cucchiarelli, Luzi, and Velardi 
\[1998\]). 
* Istituto di Informatica, Via Brecce Bianche 1-60131 Ancona, Italy. E-mail: alex@inform.unian.it 
t Dipartimento di Scienze dell'Informazione, Via Salaria 113, 1-00198 Roma, Italy. E-mail: velardi@ 
dsi.uniromal.it 
Computational Linguistics Volume 27, Number 1 
2.1 Learning Contextual Sense Indicators 
Our method proceeds as follows: first, by means of any available NE recognition 
technique (which we will call an early NE classifier), at least some examples of PNs in 
each category are detected. Second, through an unsupervised corpus-based technique, 
typical PN syntactic and semantic contexts are learned. Syntactic and semantic cues can 
then be used to extend the coverage of the early NE classifier, increasing its robustness 
to the limitations of the gazetteers (PN dictionaries) and domain shifts. 
In phase one, a learning corpus in the application domain is morphologically 
processed. The gazetteer lookup and the early NE classifier are then used to detect 
PNs. At the end of this phase, "some" PNs are recognized and classified, depending 
upon the size of the gazetteer and the actual performance (in the domain) of the NE 
classifier. 
In phase two, the objective is to learn a contextual model of each PN category, 
augmented with syntactic and semantic features. Since the algorithm is unsupervised, 
statistical techniques are applied to smooth the weight of acquired examples as a 
function of semantic and syntactic ambiguity. 1 
Syntactic processing is applied over the corpus. A shallow parser (see details in 
Basili, Pazienza, and Velardi \[1994\]) extracts from the learning corpus elementary syn- 
tactic relations such as Subject-Object, Noun-Preposition-Noun, etc. 2 An elementary 
syntactic link (esl) is represented as: 
esl(wi, mod( typei, Wk ) ) 
where wj is the headword, Wk is the modifier, and type i is the type of syntactic relation 
(e.g. Prepositional Phrase, Subject-Verb, Verb-Direct-Object, e c.). For example, esl(close 
mod(G_N_V_Act Xerox)) reads: Xerox is the modifier of the head close in a Subject-Verb 
(G_N_V_Act) syntactic relation. 
In our study, the context of a word w in a sentence S is represented by the esls 
including w as one of its arguments (wj or Wk). The esls that include semantically 
classified PNs as one of their arguments are grouped in a database, called PN_esl. 
This database provides contextual evidence for assigning a category to unknown PNs. 
2.2 Tagging Unknown PNs 
A corpus-driven algorithm is used to classify unknown proper nouns recognized as 
such, but not semantically classified by the early NE recognizer. 3 
? Let U_PN be an unknown proper noun, i.e., a single word or a complex 
nominal. Let Cpn = (Cp~l, Cpn2 . . . . .  CpnN) be the set of semantic ategories 
for proper nouns (e.g. Person, Organization, Product, etc.). Finally, let 
ESL be the set of esls (often more than one in a text) that include U_PN 
as one of their arguments. 
? For each esli in ESL let: 
esli( wj, mod( typei, Wk )) = esli( x, U_PN) 
1 We say the algorithm is unsupervised because neither the NE items detected by the early recognizer 
nor the extracted syntactic ontexts are inspected for correctness. 
2 Shallow, or partial parsers are a well-established technique for corpus parsing. Several partial parsers 
are readily available---for example, the freely downloadable LINK parser. 
3 A standard POS tagger augmented with simple heuristics is used to detect possible instances of PNs. 
Errors are originated only by ambiguous entence beginners, as "Owens Illinois" or "Boots Plc" 
causing partial recognition. 
124 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
where x = w\] or x = Wk and U-PN=wk or wj (the unknown PN can be 
either the head or the modifier), type i is the syntactic type of esl (e.g. 
N-of-N, NAN, V-for-N, etc.), and furthermore l t: 
pl(esli(x, U_PN) ) 
be the plausibi l i ty of a detected esl. Plausibility is a measure of the 
statistical evidence of a detected syntactic relation (Basili, Marziali, and 
Pazienza 1994; Grishman and Sterling 1994) that depends upon local 
(i.e., sentence-level) syntactic ambiguity and global corpus evidence. The 
plausibility accounts for the uncertainty arising from syntactic ambiguity. 
,. Finally, let: 
- -  ESLA be a set of esls in PN_esl (the previously learned 
contextual model) defined as follows: for each esli(x, Uff)N) in 
ESL, put in ESLA the set of eslj(x, PNj) with typej = type i, x in 
the same position as esli, and PNj a known proper noun, in 
the same position as U_PN in esli. 
ESLB be a set of esls in PN_esl defined as follows: for each 
esli(x, U_PN) in ESL put in ESLB the set of eslj(w, PNj) with 
type\] -- type i, w in the same position as x in esli, Sim(w,x) > 6, 
and PNj a known proper noun, in the same position as U_PN 
in esli. Sim(w, x) is a similarity measure between x and w. In 
our experiments, Sim(w,x) > ~ iff w and x have a common 
hyperonym H in WordNet. The generality of H (i.e., the 
number of levels from x to H) is made parametric, to analyze 
the effect of generalization. 
? For each semantic ategory Cp,j compute vidence(Cp,j) as: 
E weightq (x)D(x, C(PNj)) 
esliC ESLA,C( PNj)=Cpn j 
evidence(Cp~j) = a + 
E weight~j (x)D(x, C(PNj)) 
esliEESLA 
E weightq (x)D(x, C(PNj)) 
esli E ESLB,C( PNj) =Cpn j fl 
E weightiy(x)D(x'C(PNJ )) 
esli6 ESLB 
where: 
weightq(x) = weight q( esli(x, PNj) ) = pl( esli(x, PNj) ) ? (1 - ~(~)-1~_1 , 
u weightij(w ) = weightij(esli(w, PNj) ) = pl(esli(w, PNj)). (1 - amb(w)-l~k_\] 2 
pl(esli(x, PNj)) is the plausibility and arab(x) is the ambiguity 
of x in esli 
k is a constant factor used to incrementally reduce the influence 
of ambiguous words. The smoothing is tuned to be higher in 
ESLB 
a and fl are parametric, and can be used to study the evidence 
provided by ESLA and ESLB 
125 
Computational Linguistics Volume 27, Number 1 
D(x, C(PNj)) is a discrimination factor used to determine the 
saliency (Yarowsky 1992) of a context esli(x, _) for a category 
C(PNj), i.e., how good a context is at discriminating between 
C(PNj)and the other categories. 4 
The selected category for U~N is 
C = argmax(evidence(Cp~k)) 
When grouping all the evidence of a U_PN in a text, the underlying hypothesis i
that, in a given linguistic domain (finance, medicine, etc.), a PN has a unique sense. This 
is a reasonable restriction for Proper Nouns, supported by empirical evidence, though 
we would be more skeptical about the applicability of the one-sense-per-discourse 
paradigm (Gale, Church, and Yarowsky 1992) to generic words. We believe that it is 
precisely this restriction that makes the use of syntactic and semantic ontexts effective 
for PNs. 
Notice that the formula of the evidence has several smoothing factors that work to- 
gether to reduce the influence of unreliable or uninformative contexts. The formula also 
has parameters (k, ~, fl), estimated by running systematic experiments. Standard sta- 
tistical techniques have been used to balance xperimental conditions and the sources 
of variance. 
3. Using WordNet for Context Generalization 
One of the stated objectives of this paper is to investigate the effect of context gen- 
eralization (the addend ESLB in the formula of the evidence) on our sense tagging 
task. 
The use of on-line thesauri for context generalization has already been investigated 
with limited success (Hearst and Schuetze 1993; Brill and Resnik 1994; Resnik 1997; 
Agirre and Rigau 1996). Though the idea of using thesauri for context expansion is 
quite common, there are no clear indications that this is actually useful in terms of 
performance. However, studying the effect of context expansion for a PN tagging task 
in particular is relevant because: 
PNs may be hypothesized to have a unique sense in a text, and even in a 
domain corpus. Therefore, we can reliably consider as potential sense 
indicators all the contexts in which a PN appears. The only source of 
ambiguity is then the word wi co-occurring in a syntactic ontext with a 
PN, esli(wi, U_PN), but since in ESLB we group several contexts, 
hopefully spurious hyperonyms of wi will gain lower evidence. For 
example, consider the context "division of Americand3randsdnc". Division 
is a highly ambiguous word, but, when generalizing it, the majority of 
its senses appearing in the same type of syntactic relation with a Proper 
Noun (e.g. branch of Drexel_ Burnhamd,ambert_Group dnc, part of Nationale_ 
Nederlanden_Group) are indeed pertinent senses. 
4 For example, a Subject_Verb phrase with the verb make (e.g., Ace made acontract) isfound with almost 
equal probability with Person and Organization names. We used a simple conditional probability 
model for D(x, c(PNj)), but we believe that more refined measures could improve performance. 
126 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
? PN categories (e.g., Person, Location, Product) exhibit a more stable and 
less ambiguous contextual behavior than other more vague categories, 
such as psychological_feature. 5 
? We can study the degree of generalization at which an opt imum 
performance is achieved. 
4. Experimental Discussion 
The purpose of experimental evaluation is twofold: 
To test the improvement in robustness of a state-of-the-art NE recognizer. 
To study the effectiveness of syntactic ontexts and of a "cautious" 
context generalization on the performance of the U_PN tagger, analyzed 
in isolation. The effect of generalization is studied by gradually relaxing 
the notion of similarity in the formula of evidence and by tuning, 
through the factors a and fl, the contribution of generalized contexts to 
the formula of evidence. 
In our experiment, we used the Italian Sole24Ore half-million-word corpus on 
financial news, the one-mill ion-word Wall Street Journal corpus, and WordNet, as stan- 
dard on-line available resources, as well as a series of computational tools made avail- 
able for our research: 
? the VIE system (Humphreys et al 1996) for initial detection of Proper 
Nouns from the learning corpus; for the same purpose we also used a 
machine learning method based on decision lists, described in Paliouras, 
Karkaletsis, and Spyropolous (1998). 
? the SSA shallow syntactic analyzer (Basili, Pazienza, and Velardi 1994) 
for surface corpus parsing. 6
? the tool described in Cucchiarelli and Velardi (1998) for corpus-driven 
WordNet pruning. 7
4.1 Experiment 1: Improving Robustness of NE Recognizers 
The objective of Experiment 1 is to verify the improvement in robustness of existing 
NE recognizers, through the use of our tagger. In Figure 1, three testing experiments 
are shown. The table measures the local performance of the NE tagging task achieved 
by the early NE recognizer, by our untrained tagger, and finally, the joint performance 
of the two methods. 
In the first test, we used the Italian Sole24Ore corpus. Due to the unavailability of 
WordNet in Italian, we used a dictionary of strict synonyms for context expansion. In 
this test, we "loosely" adapted the English VIE system (as used in MUC-6) to Italian. 
5 In Velardi and Cucchiarelli (2000) we formally studied the relation between category type and 
learnability of contextual cues for WSD. 
6 We also used the GATE partial parser. We were not as successful with this parser because it is not 
designed for high-performance VP3?P and NP-PP detection, but prepositional contexts are often the 
most informative indicators. 
7 This method produces a20-30% reduction of the initial WordNet ambiguity, depending on the specific 
corpus. 
127 
Computational Linguistics Volume 27, Number 1 
A B C D E F G H I J K L 
Test 1 239 355 67.32% 339 70.50% 60 83 72.29% 75 80.00% 84.23% 88.20% 
Test 2 650 793 81.90% 759 85.63% 67 83 80.72% 80 83.75% 90.42% 94.47% 
Test 3 3,040 4,168 72.94% 3,233 94.03% 585 935 62.57% 810 72.22% 86.97% 89.66% 
Legend 
A: PNs correctly tagged by the early NE recognizer 
B: Total PNs in the Test Corpus 
C: Local Recall of the early NE recognizer (A/B) 
D: Total PNs detected by the early NE recognizer (D = A + A1 (errors) + G(unknown) 
E: Local Precision of the early NE recognizer (A/D) 
F: UPNs correctly tagged by the UPN tagger in the Test Corpus 
G: Total UPNs not detected by the early NE recognizer 
H: Local recall of UPN tagger (Phase2) (F/G) 
I: Total UPNs for which a decision was possible by the UPN tagger 
\]: Local precision of the UPN tagger 
K: Joint Recall of the two methods (A + F)/B 
L: Joint Precision of the two methods (A+F)/D 
Figure 1 
Outline of results on the Sole24Ore corpus. 
We used the English gazetteer as it was and we appl ied simple " language porting" to 
the NE grammar  (e.g., replacing English words and preposit ions with corresponding 
Italian words, and little more), s This justifies the low performance of the rule-based 
classifier. Note that our context-based tagger produces a considerable improvement  in
performance (around 18%), therefore the global performance (column K and L) turns 
out to be comparable with state-of-the-art systems, without a significant readaptation 
effort. 
In the second test, we used again VIE, on the English Wall Street Journal corpus. 
We used a version of VIE that was designed to detect NE in a management  succession 
domain (we are testing the effect of a domain shift here). Local performance was 
somewhat  lower than in MUC-6. Again, we measured a 9% improvement  using our 
tagger, and very high global performance. 
The third test was the most demanding. Here, we used only half of the named 
entity gazetteer used in previous experiments. The purpose of this test was also to 
verify the effect on performance of a poorly populated gazetteer. In this test, rather than 
using LASIE, we used a machine learning method described in Paliouras, Karkaletsis 
and Spyropolous (1998). This method uses as a training set the available half of the 
gazetteer to learn a context-based decision list for NE classification. 
As shown in Test 3, column B, the initial number  of PNs in the test corpus is now 
considerably higher. The decision-list classifier is tuned to classify with high precision 
and lower recall. Therefore, only the "hardest" cases are submitted to our untrained 
classifier. In fact, local performance of our classifier is around 10% lower than for pre- 
vious tests, but nevertheless, global performance (in terms of joint precision and recall) 
shows an improvement.  Finally, we observe that the performance figures reported in 
Figure 1 say nothing about the various sources of errors. Errors and misses occur both 
during the off-line learning phase (as we said, NE instances and syntactic contexts 
8 Most location and company names known worldwide (e.g., NewYork, IBM) are in fact mentioned in 
economic journals regardless of the language. 
128 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
are not inspected for correctness, therefore the contextual knowledge base is error 
prone) and prior to the U_PN tagging phase: a compound PN may be incompletely 
recognized uring POS tagging, causing the generation of an uninformative syntactic 
context (e.g., "Owens Illinois" at the beginning of a sentence is recognized as "owens 
Illinois", causing a spurious NdN(owen,Illinois) context o be generated). 
Because all these "external" sources of noise are not filtered out, we may then 
reliably conclude that our tagger is effective at improving the robustness of proper 
noun classification, though clearly the amount of improvement depends upon the 
baseline performances of the early method used for PN classification. 
Although the classification evidence provided by syntactic ontexts is somewhat 
noise prone, it proves to be useful as a "backup," when other "simpler" contextual 
evidence does not allow a reliable decision. 
4.2 Effectiveness of Syntactic and Semantic Cues for Semantic Classification 
In a second experiment, we used the experimental set up of Test 2 (WSJ+VIE described 
above) to evaluate the effectiveness of context expansion on system performance. We 
applied a pruning method on WordNet (Cucchiarelli and Velardi 1998) to reduce initial 
ambiguity of contexts. This pruning method allowed an average of 27% reduction in 
the initial ambiguity of the total number of the 13,428 common nouns in the Wall 
Street Journal corpus. The objective of this experiment was to allow a more detailed 
evaluation of our method, with respect o several parameters. 
We built four test sets with the same distribution of PN categories and frequency 
distribution as in the application corpus. We selected four frequency ranges (1, 2, 3-9, 
> 10) and in each range we selected 100 PNs, reflecting the frequency distribution 
in the corpus of the three main PN semantic ategories--Person, Organization, and 
Location. We then built another test set, called TSAll, with 400 PNs again reflecting the 
frequency and category distribution of the corpus. The 400 PNs were then removed 
from the set of 37,018 esls extracted by our parser and from the gazetteer (whenever 
included). 
In this experiment, we wanted to measure the performance of the U_PN tagger 
over the 400 words in the test set, in terms of F-measure, according to several varying 
factors: 
? the category type; 
? the amount of initial contextual evidence (i.e., the frequency range, 
reflected by the different test sets); 
? the factors oe and fl, i.e., the influence of local and generalized contexts; 
? the level of generalization L. 
Figures 2 summarizes the results of the experiment. Figure 2(a) shows the increase 
in performance as a function of the values of oe and fl and the generalization level. N 
means no generalization, only the evidence provided by ESLA is computed; 0 means 
that ESLB collects the evidence provided by contexts in which w is a strict synonym of 
x according to WordNet; 1, 2, and 3 refer to incremental levels of generalization in the 
(pruned) WordNet hierarchy. The figure shows that context generalization produces up 
to 7% improvement in performance. Best results are obtained with L = 2 and ~ = 0.7, 
fl = 0.3. Further generalization may cause a drop in performance. High ambiguity is 
the cause of this behavior, despite WordNet pruning (without WordNet pruning, we 
observed a performance inversion at level 1; this experiment is not reported ue to 
129 
Computational Linguistics Volume 27, Number 1 
hi1% i . . , "  . ,"  "/ /( l=o ?, \]3 0 3 
~% 4 o 3 I~ 41 7 
76% 
9S~,  
I 
N " Leve l  of  Gen~ra l i za t lon  
(a )  
Figure 2 
Evaluation of the effectiveness of context expansion. 
f:2 
t~ 
(b)  
limitations of space). Figure 2(b) illustrates the influence of initial contextual evidence. 
Recognition of singleton PNs remains almost constant as the contribution of gener- 
alized and nongeneralized contexts varies. Looking more in detail, we observe that 
recall increases with fl -- (1 -  c~), but precision decreases. Generalization on the basis of 
a unique context does not allow any filtering of spurious senses, while when grouping 
several contexts, spurious senses gain lower evidence (as anticipated in Section 3). 
Finally, we designed an experiment to evaluate the influence of the test set com- 
position on the U_PN tagger performances. We performed an analysis of variance 
(ANOVA test \[Hoel 1971\]) on the results obtained by processing nine different est 
sets of 400 PNs each, selected randomly. In all our experiments the details of which 
we omit, for lack of space), we found that the U-PN tagging method performances 
were independent of the variations of the test set. 
References 
Agirre, Eneko and German Rigau. 1996. 
Word Sense Disambiguation using 
Conceptual Density. In Proceedings ofthe 
16th International Conference on 
Computational Linguistics (COLING '96), 
Copenhagen, Denmark. 
Basili, Roberto, Alessandro Marziali, and 
Maria Teresa Pazienza. 1994. Modelling 
syntax uncertainty in lexical acquisition 
from texts. Journal of Quantitative 
Linguistics, 1(1). 
Basili, Roberto, Maria Teresa Pazienza, and 
Paola Velardi. 1994. A (not-so) shallow 
parser for collocational nalysis. In 
Proceedings ofthe 15th International 
Conference on Computational Linguistics 
(COLING '94), Kyoto, Japan. 
Brill, Erik and Philip Resnik. 1994. A 
transformation-based approach to 
prepositional phrase attachment 
disambiguation. I  Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING '94), Kyoto, Japan. 
Cucchiarelli, Alessandro, Danilo Luzi, and 
Paola Velardi. 1998. Automatic semantic 
tagging of unknown proper names. In 
COLING-ACL "98: 36th Annual Meeting of 
the Association for Computational Linguistics 
and I7th International Conference on 
Computational Linguistics, Montreal, 
Canada. 
Cucchiarelli, Alessandro and Paola Velardi. 
1998. Finding a domain-appropriate sense 
inventory for semantically tagging a 
corpus. International Journal on Natural 
Language Engineering, December. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992. One sense per discourse. 
In Proceedings ofthe DARPA Speech and 
Natural Language Workshop. Harriman, NY. 
Grishrnan, Ralph and John Sterling. 1994. 
Generalizing automatically generated 
selectional patterns. Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING "94), Kyoto, Japan. 
Hearst, Marti and Hinrich Schuetze. 1993. 
Customizing a lexicon to better suite a 
computational task. In Proceedings of
ACL-SIGLEX Workshop on Lexical 
Acquisition from Text. Columbus, OH. 
Hoel, Paul Gerhard. 1971. Introduction to 
130 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
Mathematical Statistics. John Wiley & Sons 
Inc., New York. 
Humphreys, Kevin, Robert Gaizauskas, 
Hamish Cunningam, and Sheila Azzan. 
1996. Technical Specifications, 1996/10/1815. 
ILASH, University of Sheffield, UK. 
Paliouras, George, Vangelis Karkaletsis, and 
Constantine Spyropolous. 1998. Results 
from the named entity recognition task. In 
Deliverable 3.2.1 of the European project 
ECRAN LE 2110. Available at: http://  
www2.echo.lu/langeng/en/lel/ecran/ 
ecran.html. 
Resnik, Philip. 1997. Selectional reference 
and sense disambiguation. I  Proceedings 
of the ACL Workshop Tagging Text with 
Lexical Semantics: Why, What, and How? 
Washington, DC. 
Velardi, Paola and Alessandro Cucchiarelli. 
2000. A theoretical nalysis of 
contextual-based l arning algorithms for 
word sense disambiguation. I  Proceedings 
of ECA12000, Berlin, Germany. (To 
appear.) 
Yarowsky, David. 1992 Word-sense 
disambiguation using statistical models of 
Roget's categories trained on large 
corpora. In Proceedings ofthe 14th 
International Conference on Computational 
Linguistics (COLING "92), Nantes, France. 
131 
c? 2004 Association for Computational Linguistics
Learning Domain Ontologies from
Document Warehouses and Dedicated
Web Sites
Roberto Navigli? Paola Velardi
Universita` di Roma ?La Sapienza? Universita` di Roma ?La Sapienza?
We present a method and a tool, OntoLearn, aimed at the extraction of domain ontologies from
Web sites, and more generally from documents shared among the members of virtual organiza-
tions. OntoLearn first extracts a domain terminology from available documents. Then, complex
domain terms are semantically interpreted and arranged in a hierarchical fashion. Finally, a
general-purpose ontology, WordNet, is trimmed and enriched with the detected domain concepts.
The major novel aspect of this approach is semantic interpretation, that is, the association of a
complex concept with a complex term. This involves finding the appropriate WordNet concept
for each word of a terminological string and the appropriate conceptual relations that hold among
the concept components. Semantic interpretation is based on a new word sense disambiguation
algorithm, called structural semantic interconnections.
1. Introduction
The importance of domain ontologies is widely recognized, particularly in relation to
the expected advent of the Semantic Web (Berners-Lee 1999). The goal of a domain on-
tology is to reduce (or eliminate) the conceptual and terminological confusion among
the members of a virtual community of users (for example, tourist operators, commer-
cial enterprises, medical practitioners) who need to share electronic documents and
information of various kinds. This is achieved by identifying and properly defining a
set of relevant concepts that characterize a given application domain. An ontology is
therefore a shared understanding of some domain of interest (Uschold and Gruninger
1996). The construction of a shared understanding, that is, a unifying conceptual frame-
work, fosters
? communication and cooperation among people
? better enterprise organization
? interoperability among systems
? system engineering benefits (reusability, reliability, and specification)
Creating ontologies is, however, a difficult and time-consuming process that involves
specialists from several fields. Philosophical ontologists and artificial intelligence lo-
gicians are usually involved in the task of defining the basic kinds and structures
of concepts (objects, properties, relations, and axioms) that are applicable in every
? Dipartimento di Informatica, Universita` di Roma ?La Sapienza,? Via Salaria, 113 - 00198 Roma, Italia.
E-mail: {navigli, velardi}@di.uniroma1.it.
152
Computational Linguistics Volume 30, Number 2
 
 
     Core Ontology  
Foundational  Ontology  
Specific Domain Ontology
Figure 1
The three levels of generality of a domain ontology.
possible domain. The issue of identifying these very few ?basic? principles, now often
referred to as foundational ontologies (FOs) (or top, or upper ontologies; see Figure 1)
(Gangemi et al 2002), meets the practical need of a model that has as much generality
as possible, to ensure reusability across different domains (Smith and Welty 2001).
Domain modelers and knowledge engineers are involved in the task of identify-
ing the key domain conceptualizations and describing them according to the organi-
zational backbones established by the foundational ontology. The result of this effort
is referred to as the core ontology (CO), which usually includes a few hundred ap-
plication domain concepts. While many ontology projects eventually succeed in the
task of defining a core ontology,1 populating the third level, which we call the specific
domain ontology (SDO), is the actual barrier that very few projects have been able to
overcome (e.g., WordNet [Fellbaum 1995], Cyc [Lenat 1993], and EDR [Yokoi 1993]),
but they pay a price for this inability in terms of inconsistencies and limitations.2
It turns out that, although domain ontologies are recognized as crucial resources
for the Semantic Web, in practice they are not available and when available, they are
rarely used outside specific research environments.
So which features are most needed to build usable ontologies?
? Coverage: The domain concepts must be there; the SDO must be
sufficiently (for the application purposes) populated. Tools are needed to
extensively support the task of identifying the relevant concepts and the
relations among them.
? Consensus: Decision making is a difficult activity for one person, and it
gets even harder when a group of people must reach consensus on a
given issue and, in addition, the group is geographically dispersed.
When a group of enterprises decide to cooperate in a given domain, they
have first to agree on many basic issues; that is, they must reach a
consensus of the business domain. Such a common view must be
reflected by the domain ontology.
? Accessibility: The ontology must be easily accessible: tools are needed to
easily integrate the ontology within an application that may clearly show
1 Several ontologies are already available on the Internet, including a few hundred more or less
extensively defined concepts.
2 For example, it has been claimed by several researchers (e.g., Oltramari et al, 2002) that in WordNet
there is no clear separation between concept-synsets, instance-synsets, relation-synsets, and
meta-property-synsets.
153
Navigli and Velardi Learning Domain Ontologies
its decisive contribution, e.g., improving the ability to share and
exchange information through the web.
In cooperation with another research institution,3 we defined a general architecture
and a battery of systems to foster the creation of such ?usable? ontologies. Consensus
is achieved in both an implicit and an explicit way: implicit, since candidate concepts
are selected from among the terms that are frequently and consistently employed in
the documents produced by the virtual community of users; explicit, through the use
of Web-based groupware aimed at consensual construction and maintenance of an
ontology. Within this framework, the proposed tools are OntoLearn, for the automatic
extraction of domain concepts from thematic Web sites; ConSys, for the validation
of the extracted concepts; and SymOntoX, the ontology management system. This
ontology-learning architecture has been implemented and is being tested in the con-
text of several European projects,4 aimed at improving interoperability for networked
enterprises.
In Section 2, we provide an overview of the complete ontology-engineering archi-
tecture. In the remaining sections, we describe in more detail OntoLearn, a system that
uses text mining techniques and existing linguistic resources, such as WordNet and
SemCor, to learn, from available document warehouses and dedicated Web sites, do-
main concepts and taxonomic relations among them. OntoLearn automatically builds
a specific domain ontology that can be used to create a specialized view of an exist-
ing general-purpose ontology, like WordNet, or to populate the lower levels of a core
ontology, if available.
2. The Ontology Engineering Architecture
Figure 2 reports the proposed ontology-engineering method, that is, the sequence of
steps and the intermediate outputs that are produced in building a domain ontol-
ogy. As shown in the figure, ontology engineering is an iterative process involving
concept learning (OntoLearn), machine-supported concept validation (ConSys), and
management (SymOntoX).
The engineering process starts with OntoLearn exploring available documents
and related Web sites to learn domain concepts and detect taxonomic relations among
them, producing as output a domain concept forest. Initially, we base concept learning
on external, generic knowledge sources (we use WordNet and SemCor). In subsequent
cycles, the domain ontology receives progressively more use as it becomes adequately
populated.
Ontology validation is undertaken with ConSys (Missikoff and Wang 2001), a
Web-based groupware package that performs consensus building by means of thor-
ough validation by the representatives of the communities active in the application
domain. Throughout the cycle, OntoLearn operates in connection with the ontology
management system, SymOntoX (Formica and Missikoff 2003). Ontology engineers use
this management system to define and update concepts and their mutual connections,
thus allowing the construction of a semantic net. Further, SymOntoX?s environment
can attach the automatically learned domain concept trees to the appropriate nodes of
the core ontology, thereby enriching concepts with additional information. SymOntoX
3 The LEKS-CNR laboratory in Rome.
4 The Fetish EC project, ITS-13015 (http://fetish.singladura.com/index.php) and the Harmonise EC
project, IST-2000-29329 (http://dbs.cordis.lu), both in the tourism domain, and the INTEROP Network
of Excellence on interoperability IST-2003-508011.
154
Computational Linguistics Volume 30, Number 2
 
Domain 
Concept 
Forest 
OntoLearn ConSys  Domain 
Ontology 
Application domain  
Communities 
Domain 
Web sites 
Generic 
Knowledge  
Sources 
Self-learning 
SymOntoX 
Validation 
Self-learni
S ntoX
Figure 2
The ontology-engineering chain.
also performs consistency checks. The self-learning cycle in Figure 2 consists, then, of
two steps: first, domain users and experts use ConSys to validate the automatically
learned ontology and forward their suggestions to the knowledge engineers, who im-
plement them as updates to SymOntoX. Then, the updated domain ontology is used
by OntoLearn to learn new concepts from new documents.
The focus of this article is the description of the OntoLearn system. Details on
other modules of the ontology-engineering architecture can be found in the referenced
papers.
3. Architecture of the OntoLearn System
Figure 3 shows the architecture of the OntoLearn system. There are three main phases:
First, a domain terminology is extracted from available texts in the application domain
(specialized Web sites and warehouses, or documents exchanged among members of
a virtual community), and filtered using natural language processing and statistical
techniques. Second, terms are semantically interpreted (in a sense that we clarify in
Section 3.2) and ordered according to taxonomic relations, generating a domain con-
cept forest (DCF). Third, the DCF is used to update the existing ontology (WordNet
or any available domain ontology).
In a ?stand-alone? mode, OntoLearn automatically creates a specialized view of
WordNet, pruning certain generic concepts and adding new domain concepts. When
used within the engineering chain shown in Figure 2, ontology integration and up-
dating is performed by the ontology engineers, who update an existing core ontology
using SymOntoX.
In this article we describe the stand-alone procedure.
3.1 Phase 1: Terminology Extraction
Terminology is the set of words or word strings that convey a single (possibly complex)
meaning within a given community. In a sense, terminology is the surface appearance,
in texts, of the domain knowledge of a community. Because of their low ambiguity
and high specificity, these words are also particularly useful for conceptualizing a knowledge
domain or for supporting the creation of a domain ontology. Candidate terminological
expressions are usually captured with more or less shallow techniques, ranging from
stochastic methods (Church and Hanks 1989; Yamamoto and Church 2001) to more
sophisticated syntactic approaches (Jacquemin 1997).
155
Navigli and Velardi Learning Domain Ontologies
WordNet
domain
 corpus
contrastive
  corpora
terminology extraction
candidate
extraction
terminology
   filtering
semantic interpretation
     semantic
disambiguation
   identification of
taxonomic relations
   identification of
conceptual relations
Inductive
  learner
  Natural
 Language
Processor
ontology integration
     and updating
3
2
1
  Lexical
Resources
Domain Concept Forest
Figure 3
The architecture of OntoLearn.
Obviously, richer syntactic information positively influences the quality of the
result to be input to the statistical filtering. In our experiments we used the linguis-
tic processor ARIOSTO (Basili, Pazienza, and Velardi 1996) and the syntactic parser
CHAOS (Basili, Pazienza, and Zanzotto 1998). We parsed the available documents in
the application domain in order to extract a list Tc of syntactically plausible termino-
logical noun phrases (NPs), for example, compounds (credit card), adjective-NPs (local
tourist information office), and prepositional-NPs (board of directors). In English, the first
two constructs are the most frequent.
OntoLearn uses a novel method for filtering ?true? terminology, described in detail
in (Velardi, Missikoff, and Basili 2001). The method is based on two measures, called
Domain Relevance (DR) and Domain Consensus (DC), that we introduce hereafter.
156
Computational Linguistics Volume 30, Number 2
High frequency in a corpus is a property observable for terminological as well as
nonterminological expressions (e.g., last week or real time). We measure the specificity of
a terminological candidate with respect to the target domain via comparative analysis
across different domains. To this end a specific DR score has been defined. A quantita-
tive definition of the DR can be given according to the amount of information captured
within the target corpus with respect to a larger collection of corpora. More precisely,
given a set of n domains {D1, . . . , Dn} and related corpora, the domain relevance of a
term t in class Dk is computed as
DRt,k =
P(t|Dk)
max
1?j?n
P(t|Dj)
(1)
where the conditional probabilities (P(t|Dk)) are estimated as
E(P(t|Dk)) =
ft,k
?
t??Dk
ft?,k
where ft,k is the frequency of term t in the domain Dk (i.e., in its related corpus).
Terms are concepts whose meaning is agreed upon by large user communities in a
given domain. A more selective analysis should take into account not only the overall
occurrence of a term in the target corpus but also its appearance in single documents.
Domain terms (e.g., travel agent) are referred to frequently throughout the documents
of a domain, while there are certain specific terms with a high frequency within single
documents but completely absent in others (e.g., petrol station, foreign income). Dis-
tributed usage expresses a form of consensus tied to the consolidated semantics of a
term (within the target domain) as well as to its centrality in communicating domain
knowledge.
A second relevance indicator, DC, is then assigned to candidate terms. DC mea-
sures the distributed use of a term in a domain Dk. The distribution of a term t in
documents d ? Dk can be taken as a stochastic variable estimated throughout all
d ? Dk. The entropy of this distribution expresses the degree of consensus of t in Dk.
More precisely, the domain consensus is expressed as follows:
DCt,k =
?
d?Dk
(
Pt(d) log
1
Pt(d)
)
(2)
where
E(Pt(dj)) =
ft,j
?
dj?Dk
ft,j
Nonterminological (or nondomain) candidate terms are filtered using a combination
of measures (1) and (2).
For each candidate term the following term weight is computed:
TWt,k = ?DRt,k + ?DCnormt,k
where DCnormt,k is a normalized entropy and ?,? ? (0, 1). We experimented with several
thresholds for ? and ?, with consistent results in two domains (Velardi, Missikoff, and
Basili 2001). Usually, a value close to 0.9 is to be chosen for ?. The threshold for
157
Navigli and Velardi Learning Domain Ontologies
Table 1
The first 10 terms from a tourism (left) and
finance (right) domain.
tourism finance
travel information vice president
shopping street net income
airline ticket executive officer
booking form composite trading
bus service stock market
car rental interest rate
airport transfer million share
contact detail holding company
continental breakfast third-quarter net
tourist information office chief executive
service
ferry service boat service
car ferry service
bus service transport
service
public transport
service
coach
service
taxi service
express servicetrain service
car service customer
service
Figure 4
A lexicalized tree in a tourism domain.
? depends upon the number N of documents in the training set of Dk. When N is
sufficiently large, ?good? values are between 0.35 and 0.25. Table 1 shows some of the
accepted terms in two domains, ordered by TW.
3.2 Phase 2: Semantic Interpretation
The set of terms accepted by the filtering method described in the previous section are
first arranged in subtrees, according to simple string inclusion.5 Figure 4 is an example
of what we call a lexicalized tree T . In absence of semantic interpretation, it is not
possible to fully capture conceptual relationships between concepts (for example, the
taxonomic relation between bus service and public transport service in Figure 4).
Semantic interpretation is the process of determining the right concept (sense) for
each component of a complex term (this is known as sense disambiguation) and then
identifying the semantic relations holding among the concept components, in order to
build a complex concept. For example, given the complex term bus service, we would
like to associate a complex concept with this term as in Figure 5, where bus#1 and
service#1 are unique concept names taken from a preexisting concept inventory (e.g.,
WordNet, though other general-purpose ontologies could be used), and INSTR is a
semantic relation indicating that there is a service, which is a type of work (service#1),
operated through (instrument) a bus, which is a type of public transport (bus#1).
5 Inclusion is on the right side in the case of compound terms (the most common syntactic construct for
terminology in English).
158
Computational Linguistics Volume 30, Number 2
bus#1 service#1
INSTR
Figure 5
A complex term represented as a complex concept.
This kind of semantic interpretation is indeed possible if the meaning of a new
complex concept can be interpreted compositionally from its components. Clearly, this
is not always possible. Furthermore, some of the component concepts may be absent
in the initial ontology. In this case, other strategies can be adopted, as sketched in
Section 6.
To perform semantic disambiguation, we use available lexical resources, like Word-
Net and annotated corpora, and a novel word sense disambiguation (WSD) algorithm
called structural semantic interconnection. A state-of-art inductive learner is used to
learn rules for tagging concept pairs with the appropriate semantic relation.
In the following, we first describe the semantic disambiguation algorithm (Sec-
tions 3.2.1 to 3.2.4). We then describe the semantic relation extractor (Section 3.2.5).
3.2.1 The Structural Semantic Interconnection Algorithm. OntoLearn is a tool for
extending and trimming a general-purpose ontology. In its current implementation,
it uses a concept inventory taken from WordNet. WordNet associates one or more
synsets (e.g., unique concept names) to over 120,000 words but includes very few
domain terms: for example, bus and service are individually included, but not bus
service as a unique term.
The primary strategy used by OntoLearn to attach a new concept under the ap-
propriate hyperonym of an existing ontology is compositional interpretation. Let
t = wn ? . . . ? w2 ? w1 be a valid multiword term belonging to a lexicalized tree T .
Let w1 be the syntactic head of t (e.g., the rightmost word in a compound, or the
leftmost in a prepositional NP). The process of compositional interpretation associates
the appropriate WordNet synset Sk with each word wk in t. The sense of t is hence
compositionally defined as
S(t) = [Sk|Sk ? Synsets(wk), wk ? t]
where Synsets(wk) is the set of senses provided by WordNet for word wk, for instance:
S (?transport company??) = [{transportation#4, shipping#1, transport#3},
{company#1}]
corresponding to sense 1 of company (an institution created to conduct business) and
sense 3 of transport (the commercial enterprise of transporting goods and materials).
Compositional interpretation is a form of word sense disambiguation. In this sec-
tion, we define a new approach to sense disambiguation called structural semantic
interconnections (SSI).
The SSI algorithm is a kind of structural pattern recognition. Structural pattern
recognition (Bunke and Sanfeliu 1990) has proven to be effective when the objects to
be classified contain an inherent, identifiable organization, such as image data and
time-series data. For these objects, a representation based on a ?flat? vector of fea-
tures causes a loss of information that has a negative impact on classification per-
159
Navigli and Velardi Learning Domain Ontologies
coachvehicle transport passenger
PATIENTPURPOSEKIND-OF
(vehicle, passenger, transport) (a)
(b)
Figure 6
Two representations of the same concept: (a) as a feature vector and (b) as a semantic graph.
formances. The classification task in a structural pattern recognition system is imple-
mented through the use of grammars that embody precise criteria to discriminate
among different classes. The drawback of this approach is that grammars are by their
very nature application and domain specific. However, automatic learning techniques
may be adopted to learn from available examples.
Word senses clearly fall under the category of objects that are better described
through a set of structured features. Compare for example the following two feature-
vector (a) and graph-based (b) representations of the WordNet definition of coach#5 (a
vehicle carrying many passengers, used for public transport) in Figure 6. The graph
representation shows the semantic interrelationships among the words in the defini-
tion, in contrast with the flat feature vector representation.
Provided that a graph representation for alternative word senses in a context is
available, disambiguation can be seen as the task of detecting certain ?meaningful? intercon-
necting patterns among such graphs. We use a context-free grammar to specify the type
of patterns that are the best indicators of a semantic interrelationship and to select the
appropriate sense configurations accordingly.
In what follows, we first describe the method to obtain a graph representation
of word senses from WordNet and other available resources. Then, we illustrate the
disambiguation algorithm.
Creating a graph representation for word senses. A graph representation of word senses is
automatically built using a variety of knowledge source:
1. WordNet. In WordNet, in addition to synsets, the following information
is provided:
(a) a textual sense definition (gloss);
(b) hyperonymy links (i.e., kind-of relations: for example, bus#1
is a kind of public transport#1);
(c) meronymy relations (i.e., part-of relations: for example, bus#1
has part roof#2 and window#2);
(d) other syntactic-semantic relations, as detailed later, not
systematically provided throughout the lexical knowledge
base.
2. Domain labels6 extracted by a semiautomatic methodology described in
Magnini and Cavaglia (2000) for assigning domain information (e.g.,
tourism, zoology, sport) to WordNet synsets.
3. Annotated corpora providing examples of word sense usages in contexts:
6 Domain labels have been kindly made available by the IRST to our institution for research purposes.
160
Computational Linguistics Volume 30, Number 2
(a) SemCor7 is a corpus in which each word in a sentence is
assigned a sense selected from the WordNet sense inventory for
that word. Examples of a SemCor document are the following:
Color#1 was delayed#1 until 1935, the widescreen#1 until the early#1
fifties#1.
Movement#7 itself was#7 the chief#1 and often#1 the only#1
attraction#4 of the primitive#1 movies#1 of the nineties#1.
(b) LDC/DSO8 is a corpus in which each document is a collection of
sentences having a certain word in common. The corpus
provides a sense tag for each occurrence of the word within the
document. Examples from the document focused on the noun
house are the following:
Ten years ago, he had come to the house#2 to be interviewed.
Halfway across the house#1, he could have smelled her morning
perfume.
(c) In WordNet, besides glosses, examples are sometimes provided
for certain synsets. From these examples, as for the LDC and
SemCor corpora, co-occurrence information can be extracted.
Some examples are the following:
Overnight accommodations#4 are available.
Is there intelligent#1 life in the universe?
An intelligent#1 question.
The use of other semantic knowledge repositories (e.g., FrameNet9 and Verbnet10)
is currently being explored, the main problem being the need of harmonizing these
resources with the WordNet sense and relations inventory.
The information available in WordNet and in the other resources described in the
previous section is used to automatically generate a labeled directed graph (digraph)
representation of word senses. We call this a semantic graph.
Figure 7 shows an example of the semantic graphs generated for senses 1 (coach)
and 2 (conductor) of bus; in the figure, nodes represent concepts (WordNet synsets)
and edges are semantic relations. In each graph in the figure, we include only nodes
with a maximum distance of three from the central node, as suggested by the dashed
oval. This distance has been experimentally established.
The following semantic relations are used: hyperonymy (car is a kind of vehicle,
denoted with kind?of?? ), hyponymy (its inverse, has?kind?? ), meronymy (room has-part wall,
has?part?? ), holonymy (its inverse, part?of?? ), pertainymy (dental pertains-to tooth, pert??), at-
tribute (dry value-of wetness, att?), similarity (beautiful similar-to pretty, sim?), gloss (gloss??),
7 http://www.cs.unt.edu/?rada/downloads.html#semcor
8 http://www.ldc.upenn.edu/
9 http://www.icsi.berkeley.edu/?framenet/
10 http://www.cis.upenn.edu/verbnet/
161
Navigli and Velardi Learning Domain Ontologies
bus#1
public
transport#1
transport#1
school bus#1 window#2
instrumentation#1
roof#2 vehicle#1
passenger#1
traveler#1
express#2
gloss
window frame#1
protection#2
framework#3
pane#1
covering#2
kin
d-o
f kind-of
has-
part
has-part
kind
-of
has-kind
kin
d-o
f kind-of
kin
d-o
fglo
ss
kind
-of
has-p
art has-part
kind-of
plate glass#1
person#1
kind-of
has-
kind
gloss
bus#2
conductor#4
device#1electrical#2
instrumentality#3
computer#1
connection#2
wiring#1
machine#1
calculator#2
has-kind
union#4
kind
-of
kind-of
kind-of
gloss
electrical device#1
part-of
gloss
electricity#1
gloss
circuit#1
kind-
of
inter
connection#1
has-kindkind-
of
gloss
has-kind
glo
ss
kind-of
state#4
kind-
of
kin
d-
of
pert
connected#6
(a)
(b)
Figure 7
Graph representations for (a) sense 1 and (b) sense 2 of bus.
topic (
topic??), and domain ( dl?). All these relations are explicitly encoded in WordNet, ex-
cept for the last three. Topic, gloss, and domain are extracted from annotated corpora,
sense definitions, and domain labels, respectively. Topic expresses a co-occurrence rela-
tion between concepts in texts, extracted from annotated corpora and usage examples.
Gloss relates a concept to another concept occurring in its natural language defini-
tion. Finally, domain relates two concepts sharing the same domain label. In parsing
glosses, we use a stop list to eliminate the most frequent words.
The SSI algorithm. The SSI algorithm is a knowledge-based iterative approach to
word sense disambiguation. The classification problem can be stated as follows:
? t is a term
? T (the context of t) is a list of co-occurring terms, including t.
? I is a structural representation of T (the semantic context).
162
Computational Linguistics Volume 30, Number 2
? St1, St2, . . . , Stn are structural specifications of the possible senses for t
(semantic graphs).
? G is a grammar describing structural relations (semantic
interconnections) among the objects to be analyzed.
? Determine how well the structure of I matches that of each of
St1, S
t
2, . . . , S
t
n, using G.
? Select the best matching.
Structural representations are graphs, as previously detailed. The SSI algorithm con-
sists of an initialization step and an iterative step.
In a generic iteration of the algorithm, the input is a list of co-occurring terms
T = [t1, . . . , tn] and a list of associated senses I = [St1 , . . . , Stn ], that is, the semantic
interpretation of T, where Sti 11 is either the chosen sense for ti (i.e., the result of a
previous disambiguation step) or the empty set (i.e., the term is not yet disambiguated).
A set of pending terms is also maintained, P = {ti|Sti = ?}. I is referred to as the semantic
context of T and is used, at each step, to disambiguate new terms in P.
The algorithm works in an iterative way, so that at each stage either at least
one term is removed from P (i.e., at least one pending term is disambiguated) or
the procedure stops because no more terms can be disambiguated. The output is the
updated list I of senses associated with the input terms T.
Initially, the list I includes the senses of monosemous terms in T. If no monosemous
terms are found, the algorithm uses an initialization policy described later.
During a generic iteration, the algorithm selects those terms t in P showing an
interconnection between at least one sense S of t and one or more senses in I. The
likelihood that a sense S will be the correct interpretation of t, given the semantic
context I, is estimated by the function fI : Synsets ? T ? , where Synsets is the set of
all the concepts in WordNet, and defined as follows:
fI(S, t) =
{
?({?(S, S?)|S?? I}) if S ? Senses(t) ? Synsets
0 otherwise
where Senses(t) is the subset of synsets in WordNet associated with the term t, and
?(S, S?) = ??({w(e1, e2, . . . , en)|S
e1? S1
e2? . . . en?1? Sn?1
en? S?}), that is, a function (??) of
the weights (w) of each path connecting S with S?, where S and S? are represented
by semantic graphs. A semantic path between two senses S and S?, S e1? S1
e2? . . . en?1?
Sn?1
en? S?, is represented by a sequence of edge labels e1, e2, . . . , en. A proper choice
for both ? and ?? may be the sum function (or the average sum function).
A context-free grammar G = (E, N, SG, PG) encodes all the meaningful semantic
patterns. The terminal symbols (E) are edge labels, while the nonterminal symbols (N)
encode (sub)paths between concepts; SG is the start symbol of G, and PG the set of its
productions.
We associate a weight with each production A ? ? ? PG, where A ? N and
? ? (N ? E)?, that is, ? is a sequence of terminal and nonterminal symbols. If the
sequence of edge labels e1, e2, . . . , en belongs to L(G), the language generated by the
grammar, and G is not ambiguous, then w(e1, e2, . . . , en) is given by the sum of the
11 Note that with Sti we refer interchangeably to the semantic graph associated with a sense or to the
sense label (i.e., the synset).
163
Navigli and Velardi Learning Domain Ontologies
weights of the productions applied in the derivation SG ?? e1, e2, . . . , en. (The grammar
G is described in the next subsection.)
Finally, the algorithm selects St =argmax
S?Synsets
fI(S, t) as the most likely interpretation of
t and updates the list I with the chosen concept. A threshold can be applied to fI(S, t)
to improve the robustness of the system?s choices.
At the end of a generic iteration, a number of terms are disambiguated, and each
of them is removed from the set of pending terms P. The algorithm stops with output
I when no sense S? can be found for the remaining terms in P such that fI(S?, t?) > 0,
that is, P cannot be further reduced. In each iteration, interconnections can be found
only between the sense of a pending term t and the senses disambiguated during the
previous iteration.
If no monosemous words are found, we explore two alternatives: either we provide
manually the synset of the root term h (e.g., service#1 in Figure 4: work done by one
person or group that benefits another), or we fork the execution of the algorithm into
as many processes as the number of senses of the root term h. Let n be such a number.
For each process i (i = 1, . . . , n), the input is given by Ii = [?, ?, . . . , Shi , . . . , ?], where
Shi is the ith sense of h in Senses(h). Each execution outputs a (partial or complete)
semantic context Ii. Finally, the most likely context Im is obtained by choosing
m = arg max
1?i?n
?
St?Ii
fIi(S
t, t)
Figure 8 provides pseudocode for the SSI algorithm.
3.2.2 The Grammar. The grammar G has the purpose of describing meaningful inter-
connecting patterns among semantic graphs representing concepts in the ontology. We
define a pattern as a sequence of consecutive semantic relations e1?e2?. . .?en where ei ? E,
the set of terminal symbols, that is, the vocabulary of conceptual relations. Two rela-
tions ei ? ei+1 are consecutive if the edges labeled with ei and ei+1 are incoming and/or
outgoing from the same concept node, for example,
ei? S ei+1? , ei? S ei+1? , ei? S ei+1? , ei? S ei+1? .
A meaningful pattern between two senses S and S? is a sequence e1 ? e2 ? . . . ? en that
belongs to L(G).
In its current version, the grammar G has been defined manually, inspecting the
intersecting patterns automatically extracted from pairs of manually disambiguated
word senses co-occurring in different domains. Some of the rules in G are inspired
by previous work in the eXtended WordNet12 project. The terminal symbols ei are
the conceptual relations extracted from WordNet and other on-line lexical-semantic
resources, as described in Section 3.2.1.
G is defined as a quadruple (E, N, SG, PG), where E = { ekind-of, ehas-kind, epart-of,
ehas-part, egloss, eis-in-gloss, etopic, . . .}, N = { SG, Ss, Sg, S1, S2, S3, S4, S5, S6, E1, E2, . . .}, and
PG includes about 50 productions. An excerpt from the grammar is shown in Table 2.
As stated in the previous section, the weight w(e1, e2, . . . , en) of a semantic path
e1, e2, . . . , en is given by the sum of the weights of the productions applied in the
derivation SG ?? e1, e2, . . . , en. These weights have been experimentally established on
standard word sense disambiguation data, such as the SemCor corpus, and have been
normalized so that the weight of a semantic path always ranges between 0 and 1.
The main rules in G are as follows (S1 and S2 are two synsets in I):
12 http://xwn.hlt.utdallas.edu/papers.html.
164
Computational Linguistics Volume 30, Number 2
SSI(T : list of terms, I : initial list of interpretation synsets)
{
for each t ? T
if (t is monosemous) I[t] = the only sense of t
P := {t ? T : I[t] = ?}
{ while there are more terms to disambiguate }
do
{
P? := P
for each t ? P? { for each pending term }
{
bestSense := ?
maxValue := 0
{ for each possible interpretation of t }
for each sense S of t in WordNet
{
f [S] := 0
for each synset S? ? I
{
? := 0
for each semantic path e1e2 . . . en between S and S?
? := ? + w(e1e2 . . . en)
f [S] := f [S] + ?
}
if (f [S] > maxValue)
{
maxValue := f [S]
bestSense := S
}
}
if (maxValue > 0)
{
I[t] := bestSense
P := P \ {t}
}
}
} while(P = P?)
return I
}
Figure 8
The SSI algorithm in pseudocode.
165
Navigli and Velardi Learning Domain Ontologies
Table 2
Excerpt from the context-free grammar for the recognition of semantic
interconnections.
SG ? Ss|Sg (all the rules)
Ss ? S1|S2|S3 (simple rules)
S1 ? E1S1|E1 (hyperonymy/meronymy)
E1 ? ekind?of|epart?of
S2 ? E2S2|E2 (hyponymy/holonymy)
E2 ? ehas?kind|ehas?part
S3 ? ekind?ofS3ehas?kind|ekind?ofehas?kind (parallelism)
Sg ? eglossSs|S4|S5|S6 (gloss rules)
S4 ? egloss (gloss rule)
S5 ? etopic (topic rule)
S6 ? eglosseis?in?gloss (gloss + gloss?1 rule)
1. color, if S1 is in the same adjectival cluster as chromatic#3 and S2 is a
hyponym of a concept that can assume a color like physical object#1 and
food#1 (e.g., S1 ? yellow#1 and S2 ? wall#1)
2. domain, if the gloss of S1 contains one or more domain labels and S2 is a
hyponym of those labels (for example, white#3 is defined as ?(of wine)
almost colorless,? therefore it is the best candidate for wine#1 in order to
disambiguate the term white wine)
3. synonymy, if
(a) S1 ? S2 or (b) ?N ? Synsets : S1
pert?? N ? S2
(for example, in the term open air, both the words belong to synset
{ open#8, air#2, . . . , outdoors#1 })
4. hyperonymy/meronymy path, if there is a sequence of
hyperonymy/meronymy relations (for example, mountain#1
has-part??
mountain peak#1 kind-of?? top#3 provides the right sense for each word of
mountain top)
5. hyponymy/holonymy path, if there is a sequence of
hyponymy/holonymy relations (for example, in sand beach, sand#1
part-of??
beach#1);
6. parallelism, if S1 and S2 have a common ancestor (for example, in
enterprise company, organization#1 is a common ancestor of both
enterprise#2 and company#1)
7. gloss, if S1
gloss?? S2 (for example, in web site, the gloss of web#5 contains
the word site; in waiter service, the gloss of restaurant attendant#1,
hyperonym of waiter#1, contains the word service)
8. topic, if S1
topic?? S2 (for example, in the term archeological site, in which
both words are tagged with sense 1 in a SemCor file; notice that
WordNet provides no mutual information about them; also consider
picturesque village: WordNet provides the example ?a picturesque village?
for sense 1 of picturesque)
166
Computational Linguistics Volume 30, Number 2
9. gloss+hyperonymy/meronymy path, if ?G ? Synsets : S1
gloss?? G and there
is a hyperonymy/meronymy path between G and S2 (for example, in
railway company, the gloss of railway#1 contains the word organization and
company#1 kind-of?? institution#1 kind-of?? organization#1)
10. gloss+parallelism, if ?G ? Synsets : S1
gloss?? G and there is a parallelism
path between G and S2 (for example, in transport company, the gloss of
transport#3 contains the word enterprise and organization#1 is a common
ancestor of both enterprise#2 and company#1)
11. gloss+gloss, if ?G ? Synsets : S1
gloss?? G gloss?? S2 (for example, in mountain
range, mountain#1 and range#5 both contain the word hill so that the
right senses can be chosen)
12. hyperonymy/meronymy+gloss path, if ?G ? Synsets : G gloss?? S2 and there
is a hyperonymy/meronymy path between S1 and G
13. parallelism+gloss, if ?G ? Synsets : G gloss?? S2 and there is a parallelism
path between S1 and G.
3.2.3 A Complete Example. We now provide a complete example of the SSI algorithm
applied to the task of disambiguating a lexicalized tree T . With reference to Figure 4,
the list T is initialized with all the component words in T , that is, [service, train, ferry,
car, boat, car-ferry, bus, coach, transport, public transport, taxi, express, customer].
Step 1. In T there are four monosemous words, taxi, car-ferry, public transport, and
customer; therefore, we have
I = [taxi#1, car ferry#1, public transport#1, customer#1 ]
P = {service, train, ferry, car, boat, bus, coach, transport, express}.
Step 2. During the second iteration, the following rules are matched:13
{taxi} kind-of?? {car, auto}(hyper)
{taxi} kind-of?? {car, auto} kind-of?? {motor vehicle,automotive vehicle}
kind-of?? {vehicle} gloss?? {bus, autobus, coach}(hyper + gloss)
{taxi} kind-of?? {car, auto} kind-of?? {motor vehicle,automotive vehicle} kind-of?? {vehicle}
gloss?? {ferry, ferryboat}(hyper + gloss)
{bus, autobus, coach} kind-of?? {public transport}(hyper)
{car ferry} kind-of?? {ferry, ferryboat}(hyper)
13 More than one rule may contribute to the disambiguation of a term. We list here only some of the
detected patterns.
167
Navigli and Velardi Learning Domain Ontologies
{customer, client} topic?? {service}(topic)
{service} gloss?? {person, someone} has-kind?? {consumer}
has-kind?? {customer, client}(gloss + hypo)
{train, railroad train} kind-of?? {public transport}(hyper)
{express, expressbus} kind-of?? {bus, autobus, coach} kind-of?? {public transport}(hyper)
{conveyance, transport} has-kind?? {public transport}(hypo)
obtaining:
I = [taxi#1, car ferry#1, public transport#1, customer#1, car#1, ferry#1, bus#1,
coach#5, train#1, express#2, transport#1, service#1] 14
P = {boat}.
Step 3.
{boat} has-kind?? {ferry, ferryboat}(hypo)
I = [taxi#1, car ferry#1, public transport#1, customer#1, car#1, ferry#1, bus#1,
coach#5, train#1, express#2, boat#1, transport#1, service#1 ]
P = ?.
Then the algorithm stops since the list P is empty.
3.2.4 Creating Domain Trees. During the execution of the SSI algorithm, (possibly)
all the terms in a lexicalized tree T are disambiguated. Subsequently, we proceed as
follows:
a. Concept clustering: Certain concepts can be clustered in a unique
concept on the basis of pertainymy, similarity, and synonymy (e.g.,
manor house and manorial house, expert guide and skilled guide, bus service
and coach service, respectively); notice again that we detect semantic
relations between concepts, not words. For example, bus#1 and coach#5
are synonyms, but this relation does not hold for other senses of these
two words.
b. Hierarchical structuring: Taxonomic information in WordNet is used to
replace syntactic relations with kind-of relations (e.g., ferry service kind-of??
boat service), on the basis of hyperonymy, rather than string inclusion as
in T .
14 Notice that bus#1 and coach#5 belong to the same synset, therefore they are disambiguated by the same
rule.
168
Computational Linguistics Volume 30, Number 2
service
transport service
car service public transport service car service#2 boat service
coach service, bus service train servicebus service#2 taxi service
coach service#2
express service#2express service
coach service#3 ferry service
car-ferry service
customer service
Figure 9
Domain concept tree.
Each lexicalized tree T is finally transformed into a domain concept tree ?. Fig-
ure 9 shows the concept tree obtained from the lexicalized tree of Figure 4. For the
sake of legibility, in Figure 9 concepts are labeled with the associated terms (rather
than with synsets), and numbers are shown only when more than one semantic in-
terpretation holds for a term. In fact, it is possible to find more than one matching
hyperonymy relation. For example, an express can be a bus or a train, and both inter-
pretations are valid, because they are obtained from relations between terms within
the domain.
3.2.5 Adding Conceptual Relations. The second phase of semantic interpretation in-
volves finding the appropriate semantic relations holding among concept components.
In order to extract semantic relations, we need to do the following:
? Select an inventory of domain-appropriate semantic relations.
? Learn a formal model to select the relations that hold between pairs of
concepts, given ontological information on these concepts.
? Apply the model to semantically relate the components of a complex
concept.
First, we selected an inventory of semantic relations types. To this end, we con-
sulted John Sowa?s (1984) formalization on conceptual relations, as well as other
studies conducted within the CoreLex,15 FrameNet, and EuroWordNet (Vossen 1998)
projects. In the literature, no systematic definitions are provided for semantic relations;
therefore we selected only the more intuitive and widely used ones.
To begin, we selected a kernel inventory including the following 10 relations,
which we found pertinent (at least) to the tourism and finance16 domains: place (e.g.,
room PLACE?? service, which reads ?the service has place in a room? or ?the room is
the place of service?), time (afternoon TIME?? tea), matter (ceramics MATTER?? tile), topic (art
TOPIC?? gallery), manner (bus MANNER?? service), beneficiary (customer BENEF?? service), purpose
(booking PURPOSE?? service), object (wine OBJ?? production), attribute (historical ATTR?? town),
15 http://www.cs.brandeis.edu/?paulb/CoreLex/corelex.html
16 Financial terms are extracted from the Wall Street Journal.
169
Navigli and Velardi Learning Domain Ontologies
characteristics (first-class CHRC?? hotel). This set can be easily adapted or extended to
other domains.
In order to associate the appropriate relation(s) that hold among the components
of a domain concept, we decided to use inductive machine learning. In inductive
learning, one has first to manually tag with the appropriate semantic relations a subset
of domain concepts (this is called the learning set) and then let an inductive learner
build a tagging model. Among the many available inductive learning programs, we
experimented both with Quinlan?s C4.5 and with TiMBL (Daelemans et al 1999).
An inductive learning system requires selecting a set of features to represent in-
stances in the learning domain. Instances in our case are concept-relation-concept
triples (e.g., wine
OBJ?? production), where the type of relation is given only in the learning
set.
We explored several alternatives for feature selection. We obtained the best result
when representing each concept component by the complete list of its hyperonyms
(up to the topmost), as follows:
feature ? vector[[list of hyperonyms]?modifier[list of hyperonyms]head]
For example, the feature vector for tourism operator, where tourism is the modifier and
operator is the head, is built as the sequence of hyperonyms of tourism#1: [tourism#1,
commercial enterprise#2, commerce#1, transaction#1, group-action#1, act#1, human-
action#1], followed by the sequence of hyperonyms for operator#2 [operator#2, capi-
talist#2, causal agent#1, entity#1, life form#1, person#1, individual#1].
Features are converted into a binary representation to obtain vectors of equal
length. We ran several experiments, using a tagged set of 405 complex concepts, a
varying fragment of which were used for learning, the remainder for testing (we used
two-fold cross-validation). Overall, the best experiment provided a 6% error rate over
405 examples and produced around 20 classification rules.
The following are examples of extracted rules (from C4.5), along with their confi-
dence factor (in parentheses) and examples:
If in modifier [knowledge domain#1, knowledge base#1 ]
= 1 then relation THEME(63%)
Examples : arts festival, science center
If in modifier [building material#1 ] = 1 then relation MATTER(50%)
Examples : stave church, cobblestone street
If in modifier [conveyance#3, transport#1 ] = 1 and in head[act#1,human act#1 ]
= 1 then relation MANNER(92.2%)
Examples : bus service, coach tour
Selection and extraction of conceptual relations is one of the active research areas in
the OntoLearn project. Current research is directed toward the exploitation of on-line
resources (e.g., the tagged set of conceptual relations in FrameNet) and the automatic
170
Computational Linguistics Volume 30, Number 2
generation of glosses for complex concepts (e.g., for travel service we have travel#1
PURPOSE?? service#1: ?a kind of service, work done by one person or group that benefits
another, for travel, the act of going from one place to another?). Automatic generation
of glosses (see Navigli et al [2004] for preliminary results) relies on the compositional
interpretation criterion, as well as the semantic information provided by conceptual
relations.
3.3 Phase 3: Ontology Integration
The domain concept forest generated by OntoLearn is used to trim and update Word-
Net, creating a domain ontology. WordNet is pruned and trimmed as follows:
? After the domain concept trees are attached to the appropriate nodes in
WordNet in either a manual or an automatic manner, all branches not
containing a domain node can be removed from the WordNet hierarchy.
? An intermediate node in WordNet is pruned whenever the following
conditions all hold:
1. It has no ?brother? nodes.
2. It has only one direct hyponym.
3. It is not the root of a domain concept tree.
4. It is not at a distance greater than two from a WordNet unique
beginner (this is to preserve a ?minimal? top ontology).
Figure 10 shows an example of pruning the nodes located over the domain concept
tree rooted at wine#1. The appendix shows an example of a domain-adapted branch
of WordNet in the tourism domain.
4. Evaluation
The evaluation of ontologies is recognized to be an open problem.17 Though the num-
ber of contributions in the area of ontology learning and construction has considerably
increased in the past few years, especially in relation to the forthcoming Semantic Web,
experimental data on the utility of ontologies are not available, other than those in Far-
quhar et al (1998), in which an analysis of user distribution and requests is presented
for the Ontology Server system. A better performance indicator would have been the
number of users that access the Ontology Server on a regular basis, but the authors
mention that regular users account for only a small percentage of the total. Efforts
have recently being made on the side of ontology evaluation tools and methods, but
available results are on the methodological rather than on the experimental side. The
ontology community is still in the process of assessing an evaluation framework.
We believe that, in absence of a commonly agreed-upon schema for analyzing the
properties of an ontology, the best way to proceed is evaluating an ontology within
some existing application. Our current work is precisely in this direction: The results of
a terminology translation experiment appear in Navigli, Velardi, and Gangemi (2003),
while preliminary results on a query expansion task are presented in Navigli and
Velardi (2003).
17 OntoWeb D.1.3 Tools (2001), ?Whitepaper: Ontology Evaluation Tools,? available at
http://www.aifb.unikarlsruhe.de/WBS/ysu/publications/eon2002 whitepaper.pdf
171
Navigli and Velardi Learning Domain Ontologies
drug of abuse#1
object#1
substance#1
fluid#1
liquid#1
 
 
artifact#1
drug#1
wine#1
food#1
beverage#1
alchool#1
entity#1(a)
object#1
substance#1
fluid#1
liquid#1
 
 
artifact#1
drug#1
drug of abuse#1
wine#1
food#1
beverage#1
alchool#1
entity#1(b)
object#1
substance#1
fluid#1
liquid#1
  
  
artifact#1
drug#1
wine#1
food#1
entity#1(d)
object#1
substance#1
fluid#1
liquid#1
 
 
artifact#1
drug#1
drug of abuse#1
wine#1
food#1
beverage#1
entity#1(c)
Figure 10
Pruning steps over the domain concept tree for wine1.
In this evaluation section we proceed as follows: First, we provide an account of
the feedback that we obtained from tourism experts participating in the Harmonise
EC project on interoperability in the tourism domain. Then, we evaluate in detail the
SSI algorithm, which is the ?heart? of the OntoLearn methodology.
4.1 OntoLearn as a Support for Ontology Engineers
During the first year of the Harmonise project, a core ontology of about three hundred
concepts was developed using ConSys and SymOntoX. In parallel, we collected a
corpus of about one million words from tourism documents, mainly descriptions of
travels and tourism sites. From this corpus, OntoLearn extracted an initial list of 14,383
172
Computational Linguistics Volume 30, Number 2
candidate terms (the first phase of terminology extraction in Section 3.1), from which
the system derived a domain concept forest of 3,840 concepts, which were submitted
to the domain experts for ontology updating and integration.
The Harmonise ontology partners lacked the requisite expertise to evaluate the
WordNet synset associations generated by OntoLearn for each complex term, therefore
we asked them to evaluate only the domain appropriateness of the terms, arranged in
a hierarchical fashion (as in Figure 9). We obtained a precision ranging from 72.9% to
about 80% and a recall of 52.74%.18 The precision shift is due to the well-known fact
that experts may have different intuitions about the relevance of a concept for a given
domain. The recall estimate was produced by manually inspecting 6,000 of the initial
14,383 candidate terms, asking the experts to mark all the terms judged as ?good?
domain terms, and comparing the obtained list with the list of terms automatically
filtered by OntoLearn (the phase of terminology filtering described in Section 3.1).
As a result of the feedback obtained from the tourism experts, we decided that
experts? interpretation difficulties could indeed be alleviated by associating a textual
definition with each new concept proposed by OntoLearn. This new research (auto-
matic generation of glosses) was mentioned in Section 3.2.5. We still need to produce
an in-field evaluation of the improved readability of the ontology enriched with textual
definitions.
In any case, OntoLearn favored a considerable speed up in ontology development,
since shortly after we provided the results of our OntoLearn tool, the Harmonise
ontology reached about three thousand concepts. Clearly, the definition of an initial
set of basic domain concepts is sufficiently crucial, to justify long-lasting and even
heated discussions. But once an agreement is reached, filling the lower levels of the
ontology can still take a long time, simply because it is a tedious and time-consuming
task. Therefore we think that OntoLearn revealed itself indeed to be a useful tool
within Harmonise.
4.2 Evaluation of the SSI Word Sense Disambiguation Algorithm
As we will argue in Section 5, one of the novel aspects of OntoLearn with respect
to current ontology-learning literature is semantic interpretation of extracted terms.
The SSI algorithm described in section 3.2 was subjected to several evaluation exper-
iments by the authors of this article. The output of these experiments was used to
tune certain heuristics adopted by the algorithm, for example, the dimension of the
semantic graph (i.e., the maximum distance of a concept S? from the central concept
S) and the weights associated with grammar rules. To obtain a domain-independent
tuning, tuning experiments were performed applying the SSI algorithm on standard
word sense disambiguation data,19 such as SemCor and Senseval all-words.20
However, OntoLearn?s main task is terminology disambiguation, rather than plain
word sense disambiguation. In complex terms, words are likely to be more tightly se-
mantically related than in a sentence; therefore the SSI algorithm seems more
appropriate.21 To test the SSI algorithm, we selected 650 complex terms from the set of
3,840 concepts mentioned in Section 4.1, and we manually assigned the appropriate
18 In a paper specifically dedicated to terminology extraction and evaluation (Velardi, Missikoff, and
Basili 2001) we performed an evaluation also on an economics domain, with similar results.
19 In standard WSD tasks, the list T in input to the SSI algorithm is the set of all words in a sentence
fragment to be disambiguated.
20 http://www.itri.brighton.ac.uk/events/senseval/ARCHIVE/resources.html#test
21 For better performance on a standard WSD task, it would be essential to improve lexical knowledge of
verbs (e.g. by integrating VerbNet and FrameNet, as previously mentioned), as well as to enhance the
grammar.
173
Navigli and Velardi Learning Domain Ontologies
81
.1
5%
79
.7
8%
83
.1
2%
84
.5
6%
79
.0
5%
75
.1
8%
73
.2
2%
77
.8
3%
79
.7
6%
72
.2
4%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
a
ll 
ru
le
s
e
xc
l. 
7,
 9
,
10
, 1
2,
 1
3
e
xc
l. 
9
e
xc
l. 
11
,
12
, 1
3
e
xc
l. 
7,
 8
,
9,
 1
0,
 1
2,
13
including monosemous terms
excluding monosemous terms
Figure 11
Different runs of the semantic disambiguation algorithm when certain rules in the grammar G
are removed.
WordNet synset to each word composing the term. We used two annotators to ensure
some degree of objectivity in the test set. In this task we experienced difficulties al-
ready pointed out by other annotators, namely, that certain synsets are very similar, to
the point that choosing one or the other?even with reference to our specific tourism
domain?seemed a mere guess. Though we can?t say that our 650 tagged terms are
a ?gold standard,? evaluating OntoLearn against this test set still produced interest-
ing outcomes and a good intuition of system performance. Furthermore, as shown by
the example of Section 3.2.3, OntoLearn produces a motivation for its choices, that
is, the detected semantic patterns. Though it was not feasible to analyze in detail all
the output of the system, we found more than one example in which the choices
of OntoLearn were more consistent22 and more convincing than those produced by
the annotators, to the point that OntoLearn could also be used to support human
annotators in disambiguation tasks.
First, we evaluated the effectiveness of the rules in G (Section 3.2.2) in regard to the
disambiguation algorithm. Since certain rules are clearly related (for example, rules 4
and 5, rules 9 and 11), we computed the precision of the disambiguation when adding
or removing groups of rules. The results are shown in Figure 11. The shaded bars in
the figure show the results obtained when those terms containing unambiguous words
are removed from the set of complex terms.
We found that the grammar rules involving the gloss and hyperonym relations
contribute more than others to the precision of the algorithm. Certain rules (not listed
in 3.2.2 since they were eventually removed) were found to produce a negative effect.
All the rules described in 3.2.2 were found to give more or less a comparable positive
contribution to the final performance.
22 Consistent at least with respect to the lexical knowledge encoded in WordNet.
174
Computational Linguistics Volume 30, Number 2
84
.5
6%
82
.2
5%
75
.7
4%
73
.8
0%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
manually dis. head fully automatic
precision recall
Figure 12
Precision and recall for the terminology disambiguation task: manual disambiguation of the
head and fully automatic disambiguation.
The precision computed in Figure 11 refers to the case in which the head node of
each term tree is sense-tagged manually. In Figure 12 the light and dark bars represent
precision and recall, respectively, of the algorithm when the head (i.e., the root) of
a term tree is manually assigned and when the disambiguation is fully automatic.
The limited drop in performance (2%) of the fully automated task with respect to
manual head disambiguation shows that, indeed, the assumption of a strong semantic
interrelationship between the head and the other terms of the term tree is indeed
justified.
Finally, we computed a baseline, comparing the performance of the algorithm with
that obtained by a method that always chooses the first synset for each word in a com-
plex term. (We remind readers that in WordNet, the first sense is the most probable.)
The results are shown in Figure 13, where it is seen, as expected, that the increment
in performance with respect to the baseline is higher (around 5%) when only polyse-
mous terms are considered. A 5% difference (3% with respect to the fully automatic
disambiguation) is not striking, however, the tourism domain is not very technical,
and often the first sense is the correct one. We plan in the future to run experiments
with more technical domains, for example, economics or software products.
5. Related Work
Comprehensive ontology construction and learning has been an active research field
in the past few years. Several workshops23 have been dedicated to ontology learning
and related issues. The majority of papers in this area propose methods for extending
an existing ontology with unknown words (e.g., Agirre et al 2000 and Alfonseca and
Manandhar 2002). Alfonseca and Manandhar present an algorithm to enrich WordNet
with unknown concepts on the basis of hyponymy patterns. For example, the pattern
hypernism(N2, N1) :?appositive(N2, N1) captures a hyponymy relation between Shake-
speare and poet in the appositive NP ?Shakespeare, the poet.? This approach heavily
23 ECAI-2000 First Workshop on Ontology Learning (http://ol2000.aifb.uni-karlsruhe.de/) and
IJCAI-2001 Second Workshop on Ontology Learning (http://ol2001.aifb.uni-karlsruhe.de/).
175
Navigli and Velardi Learning Domain Ontologies
84
.5
6%
80
.6
1%
79
.7
6%
73
.9
6%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
best result baseline
including monosemous terms
excluding monosemous terms
Figure 13
Comparison with a baseline.
depends upon the ability of discovering such patterns, however, it appears a useful
complementary strategy with respect to OntoLearn. OntoLearn, in fact, is unable to
analyze totally unknown terms (though ongoing research is in progress to remedy this
limitation). Berland and Charniak (1999) propose a method for extracting whole-part
relations from corpora and enrich an ontology with this information. Few papers pro-
pose methods of extensively enriching an ontology with domain terms. For example,
Vossen (2001) uses statistical methods and string inclusion to create lexicalized trees,
as we do (see Figure 4). However, no semantic disambiguation of terms is performed.
Very often, in fact, ontology-learning papers regard domain terms as concepts. A statis-
tical classifier for automatic identification of semantic roles between co-occuring terms
is presented in Gildea and Jurafsky (2002). In order to tag texts with the appropriate
semantic role, Gildea and Jurafsky use a training set of fifty thousand sentences man-
ually annotated within the FrameNet semantic labeling project. Finally, in Maedche
and Staab (2000, 2001), an architecture is presented to help ontology engineers in the
difficult task of creating an ontology. The main contribution of this work is in the
area of ontology engineering, although machine-learning methods are also proposed
to automatically enrich the ontology with semantic relations.
6. Conclusions and Ongoing Developments
We believe that the OntoLearn system is innovative in several respects:
1. in presenting an overall ontology development system.
2. in stressing the importance of appropriate terminology extraction to the
ontology-building enterprise.
3. in avoiding a common confusion between domain terms and domain
concepts, since it performs a semantic interpretation of terms. This is indeed
the strongest aspect of our method.
176
Computational Linguistics Volume 30, Number 2
4. in presenting a new structural approach to sense classification (SSI). This
method is general and has been applied to other sense disambiguation
tasks, such as sense-based query expansion (Navigli and Velardi 2003)
and gloss disambiguation (Gangemi, Navigli, and Velardi 2003).
Ontology learning is a complex enterprise, and much is left to be done. We list
here some of the drawbacks and gaps of our method, along with hints for ongoing
and future developments. OntoLearn is in fact a fully active area of research within
our group.
1. The SSI method presupposes that each term component has at least one
synset in WordNet. In our ongoing research, we try to cope with this
limitation, parsing textual definitions in glossaries (e.g., in a computer
network application) whenever a term cannot be interpreted
compositionally in WordNet. Terms in glossaries are first arranged in
trees according to detected taxonomic relations, then the head terms of
each tree are attached to the appropriate node of WordNet, if an
appropriate node indeed exists. Rule-based and algebraic methods are
jointly used to construct term trees and to compute measures of the
similarity between the textual definitions in glossaries and those in
WordNet.
2. OntoLearn detects taxonomic relations between complex concepts and
other types of semantic relations among the components of a complex
concept. However, an ontology is more than a taxonomy. The result of
concept disambiguation in OntoLearn is more than an ordered list of
synsets, since we obtain semantic nets and intersecting patterns among
them (Section 3.2.2). This information is not currently exploited to
generate richer concept definitions. A preliminary attempt to generate
formal concept definitions from informal ones is described in Gangemi,
Navigli, and Velardi (2003). Furthermore, an automatic gloss generation
algorithm has been defined (Navigli et al 2004).
3. A large-scale evaluation is still to be done. As we have already pointed
out, evaluation of ontologies is recognized as an open problem, and few
results are available, mostly on the procedural (?how to?) side. We partly
evaluated OntoLearn in an automatic translation task (Navigli, Velardi,
and Gangemi 2003), and the SSI algorithm in generic WSD tasks as
mentioned in item 4 of the previous list. In addition, it would be
interesting to run OntoLearn on different domains, in order to study the
effect of higher or lower levels of ambiguity and technicality on the
output domain ontology.
Appendix: A Fragment of Trimmed WordNet for the Tourism Domain
{ activity%1 }
{ work%1 }
{ project:00508925%n }
{ tourism project:00193473%n }
{ ambitious project:00711113%a }
{ service:00379388%n }
177
Navigli and Velardi Learning Domain Ontologies
{ travel service:00191846%n }
{ air service#2:00202658%n }
{ air service#4:00194802%n }
{ transport service:00716041%n }
{ ferry service#2:00717167%n }
{ express service#3:00716943%n }
{ exchange service:02413424%n }
{ guide service:04840928%n }
{ restaurant service:03233732%n }
{ rail service:03207559%n }
{ customer service:07197309%n }
{ guest service:07304921%n }
{ regular service#2:07525988%n }
{ outstanding customer service:02232741%a }
{ tourism service:00193473%n }
{ waiter service:07671545%n }
{ regular service:02255650%a,scheduled service:02255439%a }
{ personalized service:01703424%a,personal service:01702632%a }
{ secretarial service:02601509%a }
{ religious service:02721678%a }
{ church service:00666912%n }
{ various service:00462055%a }
{ helpful service:02376874%a }
{ quality service:03714294%n }
{ air service#3:03716758%n }
{ room service:03250788%n }
{ maid service:07387889%n }
{ laundry service:02911395%n }
{ car service#5:02364995%n }
{ hour room service:10938063%n }
{ transport service#2:02495376%n }
{ car service:02383458%n }
{ bus service#2:02356871%n }
{ taxi service:02361877%n }
{ coach service#2:02459686%n }
{ public transport service:03184373%n }
{ bus service:02356526%n,coach service:02356526%n }
{ express service#2:02653414%n }
{ local bus service:01056664%a }
{ train service:03528724%n }
{ express service:02653278%n }
{ car service#2:02384604%n }
{ coach service#3:03092927%n }
{ boat service:02304226%n }
{ ferry service:02671945%n }
{ car-ferry service:02388365%n }
{ air service:05270417%n }
{ support service:05272723%n }
178
Computational Linguistics Volume 30, Number 2
References
Agirre, Eneko, Olatz Ansa, Eduard, Hovy,
and David Mart??nez. 2000. Enriching very
large ontologies using the WWW. In ECAI
Ontology Learning Workshop 2000, available
at http://ol2000.aifb.uni-karlsruhe.de/
Alfonseca, Enrique and Suresh Manandhar.
2002. Improving an ontology refinement
Method with hyponymy patterns. In
Language Resources and Evaluation
(LREC-2002), LasPalmas, Spain, May.
Basili, Roberto, Maria Teresa Pazienza, and
Paola Velardi. 1996. An empirical
symbolic approach to natural language
processing, Artificial Intelligence, 85(1?2):
59?99.
Basili, Roberto, Maria Teresa Pazienza, and
Fabio Massimo Zanzotto. 1998. A robust
parser for information extraction. In
Proceedings of the European Conference on
Artificial Intelligence (ECAI ?98), Brighton,
U.K., August.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the the 37th Annual Meeting
of the Association for Computational
Linguistics (ACL-99), College Park, MD.
Berners-Lee, Tim. 1999. Weaving the Web.
Harper, San Francisco.
Bunke, Horst and Alberto Sanfeliu, editors.
1990. Syntactic and Structural Pattern
Recognition: Theory and Applications. World
Scientific.
Church, Kenneth Ward and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In ACL-89,
Vancouver, British Columbia, Canada.
Daelemans, Walter, Jakub Zavrel, Ko van
der Sloot, and Antal van den Bosch. 1999.
TiMBL: Tilburg Memory Based Learner,
version 2.0, reference manual. Technical
Report ILK-9901, ILK, Tilburg University,
Tilburg, the Netherlands.
Farquhar, Adam, Richard Fikes, Wanda
Pratt, and James Rice. 1998.
?Collaborative Ontology Construction for
Information Integration.? http://www-
ksl-svc.stanford.edu:5915/doc/project-
papers.html.
Fellbaum, Christiane, editor. 1995. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Formica, Anna, and Michele Missikoff. 2003.
Ontology Validation in OPAL. In 2003
International Conference on Web Services
(ICWS? 03), Las Vegas, NV. Springer.
Gangemi, Aldo, Nicola Guarino, Claudio
Masolo, Alessandro Oltramari, and Luc
Schneider. 2001. Sweetening ontologies
with DOLCE. In Proceedings of EKAW02,
Siguenza, Spain. Springer, pages 166?181
Gangemi, Aldo, Roberto Navigli, and Paola
Velardi. 2003. Axiomatising WordNet: A
hybrid methodology. In Workshop on
Human Language Technology for the Semantic
Web and Web Services, Held in Conjunction
with Second International Semantic Web
Conference, Sanibel Island, FL.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3): 245?288.
Jacquemin, Christian. 1997. Guessing
morphology from terms and corpora. In
Proceedings of the 20th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR?
97), Philadelphia, PA, pages 156?167.
Lenat, Douglas. 1993. CYC: A large scale
investment in knowledge infrastructure.
Communications of the ACM, 3(11).
Maedche, Alexander and Steffen Staab.
2000. Semi-automatic engineering of
ontologies from text. In Proceedings of the
12th International Conference on Software
Engineering and Knowledge Engineering
(SEKE?2000), Chicago, IL.
Maedche, Alexander and Steffen Staab 2001.
Ontology learning for the semantic web.
IEEE Intelligent Systems, 16(2): 72?79.
Magnini, Bernardo and Gabriella Cavaglia.
2000. Integrating subject field codes into
WordNet. In Proceedings of the second
International Conference on Language
Resources and Evaluation (LREC2000),
Atenes.
Missikoff, Michele and X.F. Wang. 2001.
Consys?A group decision-making
support system for collaborative ontology
building. In Proceedings of Group Decision &
Negotiation 2001 Conference, La Rochelle,
France.
Navigli, Roberto, and Paola Velardi. 2003.
An analysis of ontology-based query
expansion strategies. Workshop on Adaptive
Text Extraction and Mining, held in
conjunction with ECML 2003, Cavtat
Dubrovnik, Croatia, September 22.
Navigli, Roberto, Paola Velardi, Alessandro
Cucchiarelli, and Francesca Neri. 2004.
Extending and enriching WordNet with
OntoLearn. In Second Global WordNet
Conference, Brno, Czech Republic, January
20?23. Springer-Verlag.
Navigli, Roberto, Paola Velardi, and Aldo
Gangemi. 2003. Corpus driven ontology
learning: A method and its application to
automated terminology translation, IEEE
Intelligent Systems, 18(1): 11?27.
179
Navigli and Velardi Learning Domain Ontologies
Oltramari, Alessandro, Aldo Gangemi,
Nicola Guarino, and Claudio Masolo.
2002. Restructuring WordNet?s top-level:
The OntoClean approach. In Proceedings of
the International Conference on Language
Resources and Evaluation (LREC2002), Las
Palmas, Spain.
Smith, Barry and Christopher A. Welty.
2001. Ontology: Towards a new synthesis.
Formal Ontology in Information Systems,
ACM Press.
Sowa, John F. 1984. Conceptual Structures:
Information Processing in Mind and Machine.
Addison-Wesley, Reading, MA.
Uschold, Mike and Michael Gruninger.
1996. Ontologies: Principles, methods and
applications. Knowledge Engineering
Review, 11(2).
Velardi, Paola, Michele Missikoff, and
Roberto Basili. 2001. Identification of
relevant terms to support the construction
of domain ontologies. In ACL-EACL
Workshop on Human Language Technologies,
Toulouse, France, July.
Vossen, Piek, editor. 1998. EuroWordNet: A
Multilingual Database with Lexical Semantic
Networks. Kluwer Academic, Dordrecht,
Netherlands.
Vossen, Piek. 2001. Extending, trimming and
fusing WordNet for technical documents.
In NAACL 2001 Workshop on WordNet and
Other Lexical Resources, Pittsburgh, July.
Yamamoto, Mikio and Kenneth W. Church.
2001. Using suffix arrays to compute term
frequency and document frequency for all
substrings in a corpus. Computational
Linguistics, 27(1): 1?30.
Yokoi, Toshio. 1993. The EDR electronic
dictionary. Communications of the ACM,
38(11): 42?44.
Dependency of context-based Word Sense Disambiguation from 
representation and domain complexity 
Paola Velardi 
Dipartimento di Scienze dell' Informazione 
University "La Sapienza" 
Roma 
Velardi@dsi.uniromal.it 
Alessandro Cucchiarelli 
Istituto di Informatica 
University of Ancona 
Ancona 
alex@inform.unian.it 
Abstract 
Word Sense Disambiguation (WSD) is a 
central task in the area of Natural 
Language Processing. In the past few years 
several context-based probabilistic and 
machine learning methods for WSD have 
been presented in literature. However, an 
important area of research that has not 
been given the attention it deserves is a 
formal analysis of the parameters affecting 
the performance of the learning task faced 
by these systems. Usually performance is 
estimated by measuring precision and 
recall of a specific algorithm for specific 
test sets and environmental conditions. 
Therefore, a comparison among different 
learning systems and an objective 
estimation of the difficulty of the learning 
task is extremely difficult. 
In this paper we propose, in the framework 
of Computational Learning theory, a 
formal analysis of the relations between 
accuracy of a context-based WSD system, 
the complexity of the context 
representation scheme, and the 
environmental conditions (e.g. the 
complexity of language domain and 
concept inventory ) . 
1 Introduction 
In the literature (see Computational Linguistics 
(1998) for some recent results), there is a rather 
vast repertoire of supervised and unsupervised 
learning algorithms for WSD, most of which 
are based on a formal characterization f the 
surrounding context of a word or linguistic 
concept 1, and a function f to compute the 
membership of a word to a category, given its 
context in running texts. 
Despite the rich literature, none of these 
algorithms exhibit an "acceptable" 
performance with reference to the needs of 
real-world computational task (e.g. 
Information Retrieval, Information Extraction, 
Machine Translation etc.), except for 
particularly straightforward cases. 
A very interesting WSD experiment is 
Senseval (1998), a large:-scale exercise in 
evaluating WSD programs. One of the 
objectives of this experiment was to identify 
correlations between performance of the 
various systems and the parameters of the 
WSD task. Though the scoring of systems 
appears ensitive to certain factors, such as the 
degree of polysemy and the entropy of sense 
distributions, these correlations could not be 
consistently observed. There are words with 
fewer senses (e.g. bet, consume, generous) 
causing troubles to most systems, while there 
are words with a very high polysemy and 
entropy (e.g. shake) on which all systems 
obtain good performance. The justification that 
the Senseval coordinator Adam Kilgariff 
provides for shake is very interesting in the 
light of what we will discuss later in this paper: 
"The items (means contexts) for shake involve 
multi-word expressions, uch as shake one's 
head. (...) Over 50% of the items for shake 
involve some multi-word expression or other." 
In other words, the contexts for shake are very 
1 The inventory of linguistic concepts is usually 
extracted from on-line resources like WordNet, the 
Longman dictionary (LDOCE), or HECTOR. 
28 
repetitive in the training set, therefore all 
systems could easily learn a sense 
discrimination model. 
Furthermore, in Senseval (but also in other 
reported evaluations experiments) it appears 
that performances for individual 
words/concepts are extremely uneven within 
the same system. This scarce homogeneity of 
results suggests that performance is not solely 
related with the "cleverness" of a given 
learning algorithm. 
Clearly, the performances of WSD systems are 
related to a variety of parameters, but the 
formal nature of these dependencies is not fully 
understood. 
The Senseval experiment highlighted the 
necessity of a more accurate analysis of the 
correlations between performance of WSD 
systems and the parameters that may affect his 
task. In absence, a comparison of the various 
WSD algorithms and an estimation of their 
performance under different environmental 
conditions is extremely difficult. 
In the next sections we briefly present a 
computational model of learning, called PAC 
theory (Anthony and Biggs (1997), Kearns and 
Vazirani (1994), Valiant (1984)), and we then 
show that this theory may be used to determine 
the formal relations between performance of 
context-based WSD models and environmental 
conditions, such as the complexity of the 
context representation scheme, and the the 
complexity of language domain and concept 
inventory. 
2 A relation between sample size and 
complexity of learning task 
Formally, the problem of example-based 
learning of WSD models can be stated as 
follows: 
1 Given a class C of concepts Cl (where C 
is either a hierarchy or a "flat" concept 
inventory), 
2 Given a context-based representadon 
class H for a concept class C, where H: 
~*--~C and ~ is a finite alphabet of 
symbols (e.g. words or word tags), 
3 Given an input space X~*  of 
encodings of instances in the learner's 
world, e.g. feature vectors representing 
contexts around words wj, where wj is a 
member of Ct, 
Given a training sample S of length m: 
S=((xl,bl)...(xm,bm)) xi eX, ~ e{O,l} 
where bl=l if xi is a positive example of Cl, 
characterize formally a function h (C~)e H that 
assigns a word w to a concept Cl, given the 
sentence context x of w. The hypothesis may 
have the form of a Hidden Markov Model with 
estimated transition probabilities, a decision 
list, a cluster of points in a representation 
space, a logic formula, etc. 
The complexity of this learning task is related 
to several aspects, such as selecting an 
appropriate representation space H, an 
appropriate grain for the concept inventory C, 
and finally, a sufficiently representative 
training sample S. 
As first, H must be a "reasonable" 
representation space for C. Quite intuitively, if 
we represent a linguistic concept as the set of 
possible morphologic tags pairs in a ?1 
window, we will not be able to predict much, 
simply because surrounding morphologic tags 
are not sufficient to determine the semantic 
category of a word. 
On the opposite, if we select an overly 
complex representation model, including 
irrelevant features, we run through the so 
called overfitdng problem. 
Thirdly, some of the features used in a 
representation may be dependent from other 
features, and again the model would result 
unnecessarily complex. 
The problem of noise and overfitting are well 
known in the area of Machine Learning 
(Russell and Norvig (1999)), therefore we will 
not discuss the matter in detail here. An 
analysis of this issue as applied to probabilistic 
WSD learners may be found in Bruce and 
Wiebe (1999). 
For the purpose of this paper, we assume that 
the representation space H is optimized with 
respect to the choice of the relevant model 
parameters. Our objective will be to determine 
the size of S, given H and C, and given certain 
performance objectives. 
As we said, the aim of a WSD learning 
process, when instructed with a sequence S of 
examples in X, is to produce an hypothesis h
which, in some sense, "corresponds" to the 
29 
concept under consideration. Because S is a 
finite sequence, only concepts with a finite 
number of positive examples can be learned 
with total success, i.e. the learner can output an 
hypothesis h= C~ . In general, and this is the 
case for linguistic oncepts, we can only hope 
that h is a good approximation of Ci.. In our 
problem at hand, it is worth noticing that even 
humans may provide only approximate 
definitions of linguistic oncepts ! 
The theory of Probably Approximately Correct 
(PAC) learning, a relatively recent field at the 
borderline between Artificial Intelligence and 
Information Theory, states the conditions 
under which h reaches this objective, i.e. the 
conditions under which a computer derived 
hypothesis h 'probably' represents Ct 
'approximately'. 
Definition 1 (PAC learning). Let C be a 
concept class over X. Let D be a fixed 
probability distribution over the instance space 
X, and EX(Ci,D) be a procedure reflecting the 
probability distribution of the population we 
whish to learn about. We say that C is PAC 
learnable if there exists an algorithm L with 
the following property: For every Ci~C, for 
every distribution D on X, and for all 0<e<l/2 
and 0<8<1/2, if L is given access to EX(C~,D) 
and inputs e and 8, then with probability at 
least (1-8), L outputs a hypothesis h for 
concept Cl, satisfying error(h)<e. The 
parameters e and 5 have the following 
meaning: e is the probability that the learner 
produces a generalization of the sample that 
does not coincide with the target concept, 
while 5 is the probability, given D, that a 
particularly unrepresentative (or noisy) training 
sample is drawn. The objective of PAC theory 
is to predict the performance of learning 
systems by deriving a lower bound for m, as a 
function of the performance parameters e and 
6. 
Figure 1 (from Russell and Norvig (1999)) 
illustrates the "intuitive" meaning of PAC 
definition. After seeing m examples, the 
probability that Hbad includes consistent 
hypotheses is:
P(Hbad~Hco.s)-<\[ Hbad \[(l-l~)m-<lH\[(l-~) m 
H 
Hbad @ 
Figure I : e-sphere around the "true" 
function Ci 
And we want this to be: 
IHl(1-e)m_<6 
we hence obtain a lower bound for the number 
of examples we need to submit o the learner in 
order to obtain the required accuracy: 
(1) m_>~(ln~ +1 ~ I~ 
The inequality (1) establishes a sort of worst- 
case general bound, relating the size of the 
learning set with the complexity of the 
representation space \[HI. Unfortunately this 
bound turns out to have limited utility in 
practical applications. 
For example, if the hypothesis pace for a 
linguistic concept Ci is the classic "bag of 
words", i.e. a set of at least k "typical" context 
words selected by a probabilistic learner, after 
observing m samples of the ?n words around 
words we Cl 
(e.g. x = (W.n,W.n+l,.. W .. . .  Wn-l,Wn) ) 
then H is any choice of _<k_<lV\[ words over IVI 
elements, where \[V\[ (--10 s) is the size of the 
vocabulary. We then have: 
the above expression, used in inequality (1), 
produces an overly high bound for m, that can 
be hardly pursued especially in case the 
learning algorithm is supervised! 
In PAC literature, the bound for m is often 
derived "ad hoc" for specific algorithms, in 
order to exploit knowledge on the precise 
learning conditions. 
It is also worth noticing that PAC literature has 
mostly a theoretical emphasis, and most 
applications concentrated onthe field of neural 
networks and natural learning systems 
(Hanson, Petsche, Kearns, Rivest (1994)). To 
the knowledge of the authors, the utility of this 
theory in the area of computer learning of 
natural language has not been explored. 
30 
In the following, we will derive a probabilistic 
expression for m in the track of (1), for the 
case of a context-based WSD probabflistic 
learner, a learning method that includes a 
rather wide class of algorithms in the area of 
WSD. We believe that adapting our analysis to 
other example-based WSD systems will not 
require a significant effort. This relation allows 
it to establish, upon an a-priori analysis of the 
chosen conceptual model and of the language 
domain, a more precise relation between 
performance, complexity of the learning 
algorithm, and environmental conditions (e.g. 
complexity of the language domain). 
Our objective is to show that an a-priori 
analysis of the learning model and language 
domain may help to tune precisely a WSD 
experiment and allows a more uniform 
comparison between different WSD systems. 
3. A formal estimate of accuracy for 
context-based probability WSD models 
A probabilistic context-based WSD learner 
may be described as follows: 
Let X be a space of feature vectors: 
fk=( f(all=vl,a21=v2 .... ani=Vn)e ~n, bik)), 
b\[ =1 if fk is a positive example of Ct under H. 
Each vector describes the context in which a 
word we Cl is found, with variable degree of 
complexity. For examples, arguments may be 
any combination of plain words and their 
morphologic, syntactic and semantic tags. 
We assume that arguments are not statistically 
independent (in case they are, the 
representation f a concept is more simple, see 
Bruce and Wiebe, (1999)). 
An example (Cucchiarelli, Luzi and Velardi 
(1998)) is the case in which fk represents a
syntactic relation between we C~ and another 
word in its context. For example, given the 
compound istrict banks the following feature 
is generated as an example of the category 
organization: 
((N_N district bank), organization(bank)) 
We further assume that observations of 
contexts are noisy, and the noise may be 
originated by several factors, such as tags 
ambiguity, and semantic ambiguity of the word 
whose context is observed. 
In the above feature vector, the syntactic tag 
(first argument) could be wrong because of 
syntactic ambiguity and limited coverage of 
available parsers, and the ambiguous word 
bank could not be, in a specific context, an 
instance of the category organization, though it 
is in the example above. 
Probabilistic learners usually associate to 
uncertain information a measure of the 
confidence the system has in that information. 
Therefore, we assume that each feature fk is 
associated to a concept Cl with a confidence 
qb(i,k). 
The confidence may be calculated in several 
ways, depending upon the type of selected 
features for fk. For example, the Mutual 
Information measures the strength of a 
correlation between co-occurring arguments, 
and the Plausibility (Cucchiarelli, Luzi and 
Velardi (1998)) assigns a weight to a feature 
vector, depending upon the degree of 
ambiguity of its arguments and the frequency 
of its observations in a corpus. We assume here 
that ~ is adjusted to be a probability, i.e. 
~l~(i,k)=l. The factor ~(i,k) represents hence 
an estimate of the probability that fk. is indeed 
a context of Ci. 
Under these hypotheses, a representation he H 
for a concept Ct is the following: 
h(Cl):{fll..flm,} 
(2) fk-~h(Cl ) iff qb(i,k) > y 
A concept is hence represented by a set of 
features with associated probabilities 2. Policy 
(2) establishes that only features with a 
probability higher than a threshold y are 
assigned to a category model. 
Given an unknown word w' occurring in a 
context represented by f'k, the WSD algorithm 
assigns w' to the category in C that maximizes 
the similarity between f'k and one of its 
members. Again, see Cucchiarelli, Luzi and 
Velardi (1998) and Bruce and Wiebe, (1999) 
for examples of similarity functions. 
2 Note that in case of statistical independence 
among the features in a vector, a model for a 
concept would be a set of features, rather than 
feature vectors, but most of  what we discuss in this 
section would still apply with simple changes. 
31 
Given the above, the probabilistic WSD model 
for a category Ct may fail because: 
1 Cl includes falsepos#Jves (fp), e.g. feature 
vectors erroneously assigned to Cl 
2 There are false negatives (fn), i.e. feature 
vectors erroneously discarded because of a 
low value qb(i,k) 
3 The context f'k of the word w' has never 
been observed around members of Ct, nor 
it is similar (in the precise sense of 
similarity established by a given 
algorithm) to any of the vectors in the 
contextual models. 
We then have3: 
(3) P(w' is misclassified on the basis of 
f'k)= 
P(f'kE fp in C0+P(f'kE fn outside C0+P(f'kis 
unseen in C O 
Let: 
m be the total number of feature vectors 
extracted from a corpus 
m k the total number of occurrences of a feature 
fk 
k the number of times the context fk occurred m i 
with a word w' member of Cl 
Notice that ~irnik~m k, since, because of 
i 
ambiguity, a context may be assigned to more 
than one concept (or to none). 
We can then estimate the three probabilities in 
expression (3) as follows: 
L 
(3.1) ~ (fp in Ct)= E -~-(l-dp(i, k) 
~( i, k~? 
m. k
(3.2) ~ (fn outside C~)= X 1 0(i,k) 
?(i, k~y m 
(3.3) ~ (unseen in CO= 
(1 ~mk).(~ Emik ).(~(i)) =~m Emik,(i, k 
m Vm =l k k 
The third probability is computed as the 
product of three estimated factors: the 
probability ~ of unseen contexts 4 in the 
3 In the expression 3) the three events are clearly 
mutually exclusive. 
4 We here assume for simplicity that the similarity 
function is an identity. A multtnomial or a more 
corpus, the probability of extracting contexts 
around members of Cl, and the average 
confidence of a feature vector in Cl. 
Classic methods uch as Chernoff bounds may 
be applied to obtain good approximations for 
the three probabilities above. Notice however 
that in order to obtain a given accuracy of 
estimate, Chernoff bounds (and other methods) 
again impose a bound on the number of 
observed examples (Kearns and Vazirani 
(1994)) 
Since in (3.1) (1-~(i,k))<y, in (3.2) ~(i,k))>y, 
and in (3.3) ~(i,k))_<l, we obtain the bound: 
P(w' is misclassified on the basis of f'k)= 
<_ Mi m -Ni (l-y)+_~ty +l~m ~m 
The expression (3) establishes interesting 
dependencies between the accuracy of a 
context-based probabilistic WSD model and 
certain environmental conditions. 
3.1 Dependency upon the corpus and 
linguistic concepts 
In a complex language domain (e.g. newspaper 
articles) linguistic phenomena re far less 
repetitive than in a restricted language (e.g. 
airline reservations). However, even in a 
relatively unrestricted domain certain 
categories are used in a more narrow sense. 
Let us consider the probabilistic ontext-based 
algorithm in Cucchiarelli, Luzi and Velardi 
(1998), where a feature is defined by: 
fk: (syntactic_relation, wl, wi) (e.g. (N_N 
district bank)) 
fk ~C~ if w i reaches the hyperonym C~ in the 
WordNet on-line taxonomy, and ~(i,k) > y 
Using the 1 million word Wall Street Journal 
corpus, we estimated the following 
probabilities (3.3) of unseen feature vectors (m 
in this experiment is O(105)): 
P(unseen in artifact)=0,7692 
P(unseen in person)= 0,7161 
P(unseen in psychological feature)=0.8598 
complex function must be used in case contexts are 
considered similar if, for example, co-occurring 
words have some common hyperonym. See 
Cucchiarelli, Luzi and Velardi (1998) for 
examples. 
32 
The linguistic concepts artifact, person and 
psychological feature are three hyperonyms of 
the on-line WordNet taxonomy. The above 
figures show that the more "vague" concept 
psychological feature occurs in more variable 
contexts, though the distribution of words in 
the three categories i approximately even. 
3.2 Dependency on the representat ion 
model 
The representation model H also affects the 
estimates of erroneous classifications. For 
example, if we modify the contextual model by 
removing the information on wi (that is to say, 
the feature vectors in the contextual model now 
only includes the syntactic relation type and 
the co-occurring word wl), we obtain the 
following values for the probabilies (3.3): 
P (unseen in artifact) =0,1778 
P (unseen in person) = 0,1714 
P(unseen in psychological feature)=O,2139 
The probability of "unseens" in this simpler 
model is considerably lower (we removed an 
attribute, wi, that assumes values over V), but 
clearly, the probability of false positives and 
false negatives increases. 
The motivation is that we now assume that a 
context for a word belonging (also to) Ct is a 
valid context for any word in that category. 
Regardless of the specific adopted formula for 
O(i,k), the confidence ~b(i,k) in such a 
generalization depends on the number of 
different words w~ in occurring in a given 
context fk. If this number is low, or is just 1, 
then the value of dp(i,k) must be low, 
accordingly. The selected threshold y then 
determines the different contribution of false 
positives and false negatives to the total model 
accuracy. 
A preliminary experiment is illustrated in 
Figure 2. The figure computes (1-P(fp in C1) 
for the category artifact, as a function of m and 
~(i,k), evaluated on a test set of 78 words. 
The figure shows that when y is >_0,5 the 
number of false positives is rather low, after 
observing sufficient examples. 
On the other side, P(fn outside Ci) (not shown 
here for sake of space) has a specular 
behavour. For 7=0,9, the probability of false 
negative is as low as 0,6. 
4. Conclusion 
By no means the work presented in this paper 
needs more investigation, especially on the 
experimental side. However, we believe that 
learnability analysis of WSD models has 
strong practical implications. 
The quantitative and (preliminary) 
experimental results of Section 2 put in 
evidence that : 
? In order to acquire statistically stable 
contextual models of linguistic concepts, 
the dimension of the analyzed corpora 
must be considerably high. Paradoxically, 
untrained probabilistic systems are in 
better shape in this regard. Very large 
repositories of language samples can be 
now obtained from the WWW. 
? The experimental setting (i.e. size of the 
training set) must be tuned for each 
category and language domain, because the 
variability of contextual behavior may be 
significantly different, depending on 
domain complexity, e.g. the type and grain 
of the selected category, and the more or 
less restricted language domain 
? it is possible and indeed advisable, for a 
given WSD algorithm, to determine in a 
formal way the relation between expected 
accuracy of the WSD model and the 
domain and representation complexity. 
This would allow a better comparison 
among systems, and an a-priori tuning of 
the parameters of the disambiguation 
model. 
References 
Anthony M. and Biggs, N. (1997) Computational 
Learning Theory Cambridge University Press, 
1997 
Bruce R. and Wiebe J., (1999) Decomposable 
Modeling in Natural Language Processing, 
Computational Linguistics vol. 25, N. 2. 199 
Computational Linguistics (1998) Special Issue on 
Word Sense Disamblguatlon, Vol. 24 (1) March 
1988 
33 
Cucchiarelli A. Luzi D. and Velardi P. (1998) 
Automatic Semantic Tagging of Unknown Proper 
Names Proc. of joint 36 ? ACL-17 ? COLING, 
Montreal, August 1998 
Hanson S.J., Petsche T., Kearns M., Rivest R.L. 
(1994) Computational Learning Theory and 
Natural Learning Systems, Vol. II, MIT Press, 
1994 
Kearns M.J. and Vazirani U.V. (1994) An 
Introduction to Computational Learning Theory 
MIT Press, 1994 
Russell S.J and Norvig P (1999). Chapter 18: 
Learning from Observations in: Artificial 
Intelligence: a modern approach Prentice-hall 
1999 
Senseval (1998) homepage: http://www.itri. 
brtghton.ac.uk/events/senseval\] 
Valiant L. (1984) A Theory of Learnable 
Communications of the ACM, 27(11), 1984 
Figure 2: (1 -P( fp) )  VS. Corpus Dim. For the category Ar t i fact  
100 ? 
95  
90  .l 
'! 
I~. 85  
80 ~ 
70 
24943 4988S 74827 99770 124712 149654 
Corpus Dim.  
34 
Identification of relevant terms to support the construction of Domain
Ontologies
Paola Velardi? and Michele Missikoff? and Roberto Basili+
? Universit? di Roma ?La Sapienza?
Velardi@dsi.uniroma1.it
?IASI-CNR, Roma
missikoff@iasi.rm.cnr.it
+ Roberto Basili
Universit? di Roma, Tor Vergata
basili@info.uniroma2.it
Abstract
Though the utility of domain
Ontologies is now widely
acknowledged in the IT (Information
Technology) community, several
barriers must be overcome before
Ontologies become practical and
useful tools. One important
achievement would be to reduce the
cost of identifying and manually
entering several thousand-concept
descriptions. This paper describes a
text mining technique to aid an
Ontology Engineer to identify the
important concepts in a Domain
Ontology.
1 Introduction
In cooperating to work together (or even in
interacting in social settings), people and
organizations must communicate among
themselves. However, due to different contexts
and backgrounds, there can be different
viewpoints, assumptions and needs regarding
the same domain or the same problem. They
may use different jargon and terminology,
sometimes even confused, overlapping, and
they may use concepts and evaluation methods
that are mismatched or poorly defined.
The consequence is the lack of a shared
understanding that leads to a poor
communication between people and
organizations. In particular, when IT solutions
are involved, this lack of a shared
understanding impacts on:
?  Effectiveness of people?s cooperation
?  Flaws in enterprise organization
?  The identification of the requirements for
the system specification
?  The inter-operability among systems and
?  The possibility of re-using and sharing of
systems components.
The goals of an Ontology is to reduce (or
eliminate) conceptual and terminological
confusion. This is achieved by identifying and
properly defining a set of relevant concepts
that characterize a given application domain.
With respect to a Thesaurus:
An Ontology aims at describing concepts,
whereas a Thesaurus aims at describing terms;
An Ontology can be seen as an enriched
Thesaurus where, besides the definitions and
relationships among terms of a given domain,
more conceptual knowledge, by means of
richer semantic relationships, is represented.
With respect to a Knowledge Base (KB):
An Ontology can be seen as a KB whose goal
is the description of the concepts necessary for
talking about domains;
A KB, in addition, includes the knowledge
needed to model and elaborate a problem,
derive new knowledge, prove theorems, or
answer to intentional queries about a domain.
Though the utility of domain Ontologies is
now widely acknowledged in the IT
community, several barriers must be overcome
before Ontologies become practical and useful
tools for shared knowledge management.
We envisage three main areas where
innovative computational solutions could
significantly reduce the cost and effort of
Ontology construction:
?  provide effective support for collaborative
development of consensus Ontologies,
since consensus is the first condition  to be
met in order to obtain the desired benefits
from an Ontology
?  enable distributed development and access
to Ontologies, since wide-spread usage of
a resource outweighs the cost of
development
?  develop tools to identify the relevant
concepts and (semi-)automatically  enrich
with semantic information the nodes of the
Ontology, thus reducing the cost and
complexity of manually defining several
thousand concepts
In this paper, we describe SymOntos, an
Ontology management system under
development at our institution since the last
several years.  In designing SymOntos, we
have been working to define innovative
solutions concerning the three critical issues
listed above. These solutions are currently
being experimented in the context of the
European project FETISH1, aimed at the
definition of an interoperability platform for
Small Medium Enterprises in the tourism
sector.
Though we will (very) briefly present
SymOntos, this paper is concerned with the
third issue, that is, the description of text
mining methods and tools to automatically
enrich the concept Ontology.
In the FETISH Project, we decided to explore
the possibility to support the extraction of
initial shared/able knowledge from on-line
textual documentation accessible from the
Web.
2 SymOntos: a symbolic Ontology
management  system
SymOntos (SymOntos 2000) is an Ontology
management system under development at
IASI_CNR. It supports the construction of an
Ontology  following the OPAL (Object,
Process, and Actor modeling Language)
methodology. OPAL is a methodology for the
modeling and management of the Enterprise
Knowledge Base and, in particular, it allows
the representation of the semi-formal
knowledge of an enterprise. As already
mentioned, an Ontology gathers a set of
concepts that are considered relevant to a given
domain. Therefore, in SymOntos the
construction of an Ontology is performed by
defining a set of concepts. In essence, in
SymOntos a concept is characterized by:
a term, that denotes the concept,
a definition, explaining the meaning of the
concept, generally in natural language,
a set of relationships with other concepts.
                                                     
1 The interested reader may access the Web site
reported in the bibliography
Figure 1 shows an example of  filled concept
form in the Tourism domain. The Domain
Ontology is called OntoTour. Concept
relationships play a key role since they allow
concepts to be inter-linked according to their
semantics. The set of concepts, together with
their links, forms a semantic network
(Brachman 1979).
In a semantically rich Ontology, both concepts
and semantic relationships are categorized.
Semantic relationships are distinguished
according to three main categories2 namely,
Broader Terms, Similar Terms, Related Terms,
that are described below.
The Broader Terms relationship allows a set of
concepts to be organized according to a
generalization hierarchy (corresponding in the
literature to the well-known ISA hierarchy).
With the Similar Terms relationship, a set of
concepts that are similar to the concept being
defined are given, each of which annotated
with a similarity degree. For instance, the
concept Hotel can have as similar concepts
Bed&Breackfast, , with similarity degree 0.6,
and camping, with similarity degree 0.4.
Finally, the Related Terms relationship allows
the definition of a set of concepts that are
semantically related to the concept being
defined. Related concepts may be of different
kinds, but they must be defined in the
Ontology.
For instance, TravelAgency, Customer, or
CreditCard, are concepts that are semantically
related to the Hotel concept.
In SymOntos, Broader relations are also
referred to as  ?vertical? , while Related and
Similar are called ?horizontal? relations.
SymOntos is equipped with functions to ensure
concept management, verification and
                                                     
2 The represented information is in fact quite more
rich, but we omit a detailed description for sake of
space
Ontology closure, and a web interface to help
developing consensus definitions in a given
user community (Missikoff and Wang, 2000).
These functions are not described here since
they are outside the purpose of the paper.
Hotel
Def: A place where a
tourist can stay
XML tag: <htl>
Gen: Accommodation
Spec: Country_ Guest_
house, motel
Part-of:
receptivity _system
Has-part:
fitness_facilities,
restaurant, garage
Related-objects: Reservation,
payment, deposit
Related-actors: Htl-manager,
cashier, room_service
Related-processes: reserving,
paying, billing,
airport_transfer
Similar-concepts: B&B[0.6],
camping[0.4],
holiday_apartment[0.7]
Figure 1 ? the Hotel concept in OntoTour
3 Text Mining tools to construct a
Domain Ontology
In Section 2 we illustrated the main features of
the SymOntos system, and provided an
example of  concept definition in the Tourism
domain.
The techniques described in this Section are
intended to significantly improve human
productivity in the process that a group of
domain experts accomplish in order to find an
agreement on:
?  the identification of the key concepts and
relationships in the domain of interest
?  providing an explicit representation of the
conceptualization captured in the previous
stage
To reduce time, cost (and, sometimes, harsh
discussions) it is highly advisable to refer to
the documents available in the field.  In this
paper we show that text-mining tools may be
of great help in this task.
At the present state of the project, natural
language processing tools have been used for
the following tasks:
1. Identification of  thesauric information, i.e.
discovery of terms that are good candidate
names for the concepts in the Ontology.
2. Identification of taxonomic relations
among these terms.
3. Identification of related terms
For sake of space, only the first method is
described in this paper. Details of the other
methods may be found in (Missikoff et al
2001).
To mine texts, we used  a corpus processor
named ARIOSTO (Basili et al 1996) whose
performance has been improved with the
addition of a Named Entity recognizer
(Cucchiarelli et al 1998) (Paliouras et al
2000) and a chunk parser CHAOS (Basili et al
1998). In the following, we will refer to this
enhanced release of the system as ARIOSTO+.
Figure 2 provides an example of final output
(simplified for sake of readability) produced by
ARIOSTO+  on a Tourism text. Interpreting
the output predicates of Figure 2 is rather
straightforward.
The main principles underlying the CHAOS
parsing technology are decomposition and
lexicalization. Parsing is carried out in four
steps: (1) POS tagging, (2) Chunking, (3) Verb
argument structure matching and (4) Shallow
grammatical analysis. .
Chunks are defined via prototypes. These are
sequences of morphosyntactical labels mapped
to specific grammatical functions, called chunk
types. Examples of labels for the inner
components are Det, N, Adj, and Prep
while types are related to traditional
constituents, like NP, PP, etc.
The definition of chunk prototypes in CHAOS
is implemented through regular expressions.
Chunks are the first types of output shown in
Figure 2. The link(..) predicates represent the
result of shallow parsing. Whenever the
argument structure information cannot be used
to link chunks, a plausibility measure is
computed, which is inversely proportional  to
the number of colliding syntactic attachments
(see the referred papers for details). The first
phase of the Ontology building process
consists in the identification of the key
concepts of the application domain.
______________________________________
The  Colorado River Trail   follows the  Colorado River
across 600 miles of beautiful  Texas Country  - from the
pecan orchards   of  San Saba   to the  Gulf of Mexico  .
[ 1 , Nom , [The,Colorado_River_Trail] ]
[ 2 , VerFin , [follows] ]
[ 3 , Nom , [the,Colorado_River] ]
[ 4 , Prep , [across,600_miles] ]
[ 5 , Prep , [of,beautiful,Texas_Country] ]
(more follows..)
link(0,2,'Sentence').
link(2,1,'V_Sog', plaus(1.0)).
link(2,3,'V_Obj', plaus(1.0)).
link(3,4,'NP_PP',plaus(0.5)).
link(2,4,'V_PP',plaus(0.5)).
link(4,5,'PP_PP',plaus(0.3333333333333333)).
link(3,5,'NP_PP',plaus(0.3333333333333333)).
link(2,5,'V_PP',plaus(0.3333333333333333)).
 (?morefollows?)
______________________________________________
Figure 2. An example of parsed Tourism
text
Though concept names do not always have a
lexical correspondent in natural language,
especially at the most general levels of the
Ontology, one such correspondence may be
naturally drawn among the more specific
concept names and  domain-specific words and
complex nominals, like:
?  Domain Named Entities (e.g., gulf of
Mexico, Texas Country, Texas Wildlife
Association)
?  Domain-specific complex nominals   (e.g.,
travel agent, reservation list, historic site,
preservation area)
?  Domain-specific singleton words (e.g.,
hotel, reservation, trail, campground)
We denote these singleton and complex words
as Terminology.
Terminology is the set of words or word
strings , which convey a single, possibly
complex, meaning within a given community.
In a sense, Terminology  is the surface
appearance, in texts, of  the domain knowledge
in a given domain. Because of their low
ambiguity and high specificity, these words are
also particularly useful to conceptualize a
knowledge domain, but on the other side, these
words are often not found in Dictionaries. We
now describe how the different types of
Terminology are captured  using NLP
techniques.
3.1 Detection of Named Entities
Proper names are the instances of domain
concepts, therefore they populate the leaves of
the Ontology.
Proper names are pervasive in texts. In the
Tourism domain, as in most domains, Named
Entities (NE) represent more than 20% of  the
total occurring words.
To detect NE, we used a module already
available in ARIOSTO+. A detailed
description of the method summarized
hereafter may be found in (Cucchiarelli et al
1998) (Paliouras et al 2000). In ARIOSTO+,
NE are detected and semantically tagged
according to three main conceptual categories:
locations (objects in OPAL), organizations and
persons (actors in OPAL) . When contextual
cues are sufficiently strong (e.g. "lake Tahoe is
located.".), names of locations are further sub-
categorized (city, bank, hotel, geographic
location, ..), therefore the Ontology Engineer is
provided with semantic cues to correctly place
the instance under  the appropriate concept
node of the Ontology.
Named Entity recognition is based on a set of
contextual rules (e.g.  "a complex or simple
proper name followed by the trigger word
authority is a organization named entity").
Rules are manually entered or machine learned
using decision lists. If a complex nominal does
not match  any  contextual  rule in the NE rule
base, the decision is delayed until syntactic
parsing. A classification based on syntactically
augmented context similarity is later
ttempted.
The NE tagger is also used to automatically
enrich the Proper Names dictionary, thus
leading to increasingly better coverage as long
as new texts are analyzed.
As reported in the referred papers, the F-
measure (combined recall and precision with a
weight factor w=0,5) of this method is
consistently (i.e. with different experimental
settings) around  89%, a performance that
compares very well with other NE recognizers
described in the literature3.
3.2 Detection of domain-specific words
and complex nominals
NEs are word strings in part or totally
capitalized, and they often appear in well-
characterized contexts. Therefore, the task of
NE recognition is relatively well assessed in
literature. Other not-named terminological
patterns (that we will refer hereafter again with
the word "terminology" though in principle
t rminology includes also NEs) are rather more
difficult to capture since the notion of erm is
mostly underspecified.
In the literature (see Bourigault et al (1998)
for an overview of recent research) the
following steps are in general adopted:
?  Detecting terminological candidates from
texts
?  Selecting the specific entries that can be
members of a terminological glossary in
the target domain of knowledge.
Candidates terminological expressions are
                                                     
3 ftp.muc.saic.com/proceedings/score_reports_index.html
usually captured with more or less shallow
techniques, ranging from stochastic methods
(Church, 1988) to more sophisticated syntactic
approaches (e.g. Jacquemin, 1997).
Obviously, richer syntactic information
positively influences the quality of the result to
be input to the statistical filtering. In our
research, we used the CHAOS parser to select
candidate terminological patterns. Nominal
expressions usually denoting terminological
items are very similar to chunk instances.
Specific chunk prototypes have been used to
match  terminological structures.
A traditional problem of purely syntactic
approaches to term extraction is
overgeneration. The available candidates that
satisfy grammatical constraints are far more
than the true terminological entries. Extensive
studies suggest that statistical filters be always
faced with 50-80% of non-terminological
candidates.
Filtering of true terms can be done by
estimating the strength of an association
among words in a candidate terminological
expression. Commonly used association
measures are the Mutual Information (Fano,
1961) and the Dice factor (Smadja et al 1996).
In both formulas, the denominator combines
the marginal probability of each word
appearing in the candidate term. If one of these
words is particularly frequent, both measures
tend to be low. This is indeed not desirable,
because certain very prominent domain words
appear in many terminological patterns. For
example, in the Tourism domain, the term visa
appears both in isolation and in many
multiword patterns, e.g.: business visa,
extended visa, multiple entry business visa,
transit visa, student visa, etc.?Such patterns
are usually not captured by standard
association measures, because of the high
marginal probability of visa.
Another widely used measure is the inverse
document frequency, idf.
  
idfi =log2
N
dfi
Where dfi is the number of documents in a
domain Di that include a term t, and N is the
total number of documents in a collection of n
domains (D1, ?, Dn). The idea underlying this
measure is to capture words that are frequent in
a subset of documents representing a given
domain, but are relatively rare in a collection
of  generic documents. This measure captures
also words that appear just one time in a
domain, which is in principle correct, but is
also a major source of noise.
Other corpus-driven studies suggested that
pure frequency as a ranking score (i.e. a
measure of the plausibility of any candidate to
be a term) is a good metrics (Daille 1994).
However, frequency alone cannot be taken as a
good indicator: several very frequent
expressions (e.g. last week) are perfect
candidates from a grammatical point of view
but they are totally irrelevant as terminological
expressions. It is worth noticing that this is true
for two independent reasons. First, they are not
related to specific knowledge, pertinent to the
target domain, but are language specific:
different languages express with different
syntactic structures  (adverbial vs. nominal
phrases) similar temporal or spatial
expressions. As a result such expressions have
similar distributions in different domain
corpora. True terminology is tightly related to
specific concepts so that their use in the target
corpus is highly different wrt other corpora.
Second, common sense expressions are only
occasionally used, their meaning depending on
factual rather than on conceptual information.
They occur often once in a document and tend
not to repeat throughout the discourse. Their
appearance is thus evenly spread throughout
documents of any corpus. Conversely, true
terms are central elements in discourses and
they tend to recur in the documents where they
appear. They are thus expected to show more
skewed (i.e. low entropy) distributions.
The above issues suggest the application of
two different evaluation (utility) functions.
Although both are related to the widely
employed notion of term probability, they
capture more specific aspects and provide a
more effective ranking.
3.2.1 Modeling Relevance in domains
As observed above, high frequency in a corpus
is a property observable for terminological as
well as non-terminological expressions (e.g.
"last week" or "real time"). The specificity of a
terminological candidate with respect to the
target domain (Tourism in our case) is
measured via comparative analysis across
different domains. A specific score, called
Domain Relevance (DR), has been defined.
More precisely, given a set of n d mains4 (D1,
?, Dn)  the domain relevance of a term t is
computed as:
(1)
  
D R ( t , D i )= P ( t | D i )
P(t | D i )
i=1..n
?
where the conditional probabilities (P(t|Di))
are estimated as:
  
E ( P ( t| D i ) )=
freq(t in Di)
freq(t in Di)
i=1..n
?
3.2.2 Modeling Consensus about a term
Terms are concepts whose meaning is agreed
upon large user communities in a given
domain. A more selective analysis should take
into account not only the overall occurrence in
the target corpus but also its appearance in
                                                     
4 ? domains ? are (pragmatically) represented by
texts collections in different areas, e.g. medicine,
finance, tourism, etc.
single documents. Domain concepts (e.g.
ravel agent) are referred frequently
throughout the documents of a domain, while
there are certain specific terms with a high
frequency within single documents but
completely absent in others (e.g. petrol station,
foreign income).
Distributed usage expresses a form of
consensus tied to the consolidated semantics of
a term (within the target domain) as well as to
its centrality in communicating domain
knowledge. A second indicator to be assigned
to candidate terms can thus be defined.
Domain consensus measures the distributed
use of a term in a domain Di.. The distribution
of a term t in documents dj can be taken as a
stochastic variable estimated throughout all dj
? Di. The entropy H of this distribution
expresses the degree of consensus of t in Di.
More precisely, the domain consensus is
expressed as follows
(2)     
DC(t,Di) =H(P(t,dj)=
P(t,dj)
dj? Di
? log2 1P(t,dj)
? 
? 
? ? 
? 
? 
?  
Where:
  
E(P(t ,dj))=
freq(t in dj)
f req( t in dj)
dj? Di
?
Pruning of not terminological (or not-domain)
candidate terms is performed using a
combination of the measures (1) and (2). We
experimented several combinations of these
two measures, with similar results. The results,
discussed in the next Section, have been
obtained applying a threshold a to the set of
terms ranked according to (1) and then
eliminating the candidates with a rank (2)
lower than b.
4 Experiments
An obvious problem of any automatic method
for concept extraction is to provide objective
performance evaluation.
?  Firstly, a "golden standard" tourism
terminology would be necessary to
formally measure the accuracy of the
method. One such standard is not
available, and determining this standard is
one of the objectives of FETISH.
Moreover, the notion of "term" is too
vague to consider available terminological
databases as "closed" sets, unless the
domain is extremely specific.   
?  Secondly, no formal methods to evaluate a
terminology are available in literature. The
best way to evaluate a "basic" linguistic
component (i.e. a module that performs
some basic task, such as POS tagging,
terminology extraction, etc.) within a
larger NLP application (information
extraction, document classification, etc.) is
to compute the difference in performance
with and without the basic component. In
our case, since Ontology does not perform
any measurable task, adopting a similar
approach is not straightforward. As a
matter of facts, an Ontology is a basic
component itself, therefore it can be
formally evaluated only in the context of
some specific usage of the Ontology itself.
Having in mind all these inherent difficulties,
we performed two sets of experiments. In the
first, we extracted the terminology from a
collection of texts in the Tourism domain, and
we manually evaluated the results, with the
help of other participants in the FETISH
project (see the FETISH web site). In the
second, we attempted to assess the generality
of our approach. We hence extracted the
terminology from a financial corpus (the Wall
Street journal) and then we both manually
evaluated the result, and compared the
extracted terminology with an available
thesaurus in a (approximately) similar domain.
As a reference set of terms we used the
Washington Post5 (WP) dictionary of
economic and financial terms.
To compute the Domain Relevance, we first
collected corpora in several domains: tourism
announcements and hotel descriptions,
economic prose (Wall Street Journal), medical
news (Reuters), sport news (Reuters), a
balanced corpus (Brown Corpus) and four
novels by Wells. Overall, about 3,2 million
words were collected.
In the first experiment, we used the Tourism
corpus as a ?target? domain for term
extraction.
The Tourism corpus was manually built using
the WWW and currently has only about
200,000 words, but it is rapidly growing.
Table 1 is a summary of the experiment. It is
seen that only 2% terms are extracted from the
initial list of candidates. This extremely high
filtering rate is due to the small corpus: many
candidates are found just one time in the
corpus. However, candidates are extracted with
high precision (over 85%).
N. of candidate multiword terms (after
parsing)
14.383
N. of extracted terms (with a=0.35 and
b=0.50)
288
% correct (3 human judges) 85.20%
Number of subtrees (of which with
depth>0)
177
(54)
Table 1. Summary results for the term
extraction task in the Tourism domain
Table 2 shows the 15 most highly rated
multiword terms, ordered by Consensus
(Relevance is 1 for all the terms in the list).
Table 3 illustrates the effectiveness of Domain
Consensus at pruning irrelevant terms: all the
                                                     
5http://www.washingtonpost.com/wp-
srv/business/longterm/glossary/indexag.htm
candidate terms in the list have DR>a, but
DC<b.
Terms Domain Consensus
credit card
tourist information
travel agent
swimming pool
service charge
car rental
credit card number
card number
room rate
information centre
beach hotel
tourist area
tour operator
standard room
video camera
0.846913
0.696701
0.686668
0.664041
0.640951
0.635580
0.616671
0.616671
0.596764
0.579662
0.571898
0.565462
0.543419
0.539450
0.523142
Table 2: The 15 most highly ranked
multiword Tourism terms
Domain
Relevance
Domain
Consensus
english cyclist
manual work
petrol station
school diploma
western movie
white cloud
false statement
best price
council decision
foreign income
gay community
mortgage interest
substantial discount
typical day
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
0.621369
0.612948
0.612948
0.441907
0.441907
0.441907
0.441907
0.441907
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.224244
0.000000
0.000000
0.224244
0.000000
0.224244
0.224244
Table 3. Terms with high Domain Relevance
and low Domain Consensus
In the second experiment, we used the one-
million-word Wall Street journal (WSJ) and
the Washington Post (WP) reference
terminology.
The WP  includes 1270 terms, but only 214
occur at least once in the WSJ. We used these
214 as the "golden standard" (Test1), but we
performed different experiments eliminating
terms with a frequency lower than 2 (Test2), 5
(Test5) and 10 (Test10). This latter set includes
only 73 terms.
During syntactic processing, 41,609 chunk
prototypes have been extracted as eligible
terminology.
The Tables 4 and 5 compare our method with t
with Mutual Information, Dice factor, and pure
frequency. Clearly, these measures are applied
on the same set of eligible candidates extracted
by the CHAOS chunker. The results reported
in each line are those obtained using the best
threshold for each adopted measure6. For our
method (DR+DC), the threshold is given by
the values a and b. As remarked in the
introduction, a comparison against a golden
standard may be unfair, since, on one side,
many terms may be present in the observed
documents, and not present in the terminology.
On the other side, low frequency terms in the
reference terminology are difficult to capture
using statistical filters. Due to these problems,
the F-measure is in general quite low, though
our method outperforms Mutual Information
and Dice factor.  As remarked by Daille
(1994), the frequency emerges as a reasonable
indicator, especially as for the Recall value,
which is a rather obvious result.
However pure frequency implies the problems
outlined in the previous section. Upon manual
inspection, we found that, as obvious,
undesired terms increase rapidly in the
frequency ranked term list, as the frequency
decreases. Manually inspecting the first 100
highly ranked terms produced a score of 87,5
precision for our method, and 77,5 for the
frequency measure. For the subsequent 100
terms, the discrepancy gets much higher
(18%).
Note that the precision score is in line with that
obtained for the Tourism corpus. Notice also
                                                     
6 as a matter of fact, for our method we are not
quite using the best value for b, as emarked later.
that the values of a and b are the same in the
two experiments.  In practice, we found that
the threshold a=0,35  for the Domain
Relevance is a generally ?good? value, while a
little tuning may be necessary for the Domain
Consensus. In the Tourism domain, where
statistical evidence is lower, a lower value for
b produces higher precision (+1, 2%).
Method Threshol
d
Prec Recall F
DR+DC 0.35 0.49 17.18 17.61 17.39
MI 0.00009 6.68 32.08 11.05
Dice 0.034 7.48 23.90 11.39
Freq 22 14.19 25.79 18.30
Table 4 WSJ/WP experiment on Test1
Method Threshol
d
Prec Recall F
DR+DC 0.35 0.57 23.80 19.42 21.39
MI 0.00009 6.42 47.57 11.30
Dice 0.057 8.22 23.30 12.15
Freq 22 14.19 39.81 20.92
Table 5 WSJ/WP experiment on Test5
5 Conclusion and Future Work
The text mining techniques  proposed in this
paper are meant to increase the productivity of
an Ontology Engineer during the time
consuming task of populating a Domain
Ontology.   The work presented in this paper is
in part well assessed, in part still under
development. We are designing new
algorithms and techniques to widen the
spectrum of information that can be extracted
from texts and from other on-line resources,
such as dictionaries and lexical taxonomies
(like EuroWordnet, a multilingual version of
Wordnet). An on-going extension of this
research is to detect similarity relations among
concepts on the basis of contextual similarity.
Similarity is one of the fields (see Figure 1) in
a concept definition form that are currently
filled by humans.
One admittedly weak part of the research
presented in this paper is evaluation: we could
produce a numerical evaluation of certain
specific subtasks (extraction of Named Entities
and extraction of thesauric information), but
we did not evaluate the overall effect that our
text mining tools produce on the Ontology.
However, we are not aware of any  assessed
Ontology evaluation methodology in the
literature, besides (Farquhar et al 1996) where
an analysis of Ontology Server user
distribution and requests is presented.  A better
performance indicator would have been the
number of users that access Ontology Server
on a regular basis, but the authors mention that
regular users are only a small percentage7. As
remarked in Subsection 3.1.2, an objective
evaluation of an Ontology as a stand-alone
artifact is not feasible: the only  possible
success indicator is the (subjective)
acceptance/rejection rate of the Ontology
Engineer when inspecting the automatically
extracted information. An Ontology  can only
be evaluated in a context in which many users
of a community (e.g. Tourism operators in our
application) access the Ontology  on a regular
basis and use this shared knowledge to
increase their ability to communicate, access
prominent information and documents,
improve collaboration. Though  a field
evaluation of OntoTour is foreseen during the
last months of the project, we believe that wide
accessibility and  a long-lasting monitoring of
user behaviors would provide the basis for a
sound evaluation of  the OntoTour system.
                                                     
7 The system described by Farquhar and his colleagues,
however, is not a specific Ontology, but a tool, Ontology
Server, to help publishing, editing and browsing an
Ontology.
6 References
E. Agirre, O. Ausa, E. Havy and D. Martinez
"Enriching very large ontologies using the WWW"
ECAI2000 workshop on Ontology Learning,
http://ol2000.aifb.uni-karlsruhe.de/, Berlin, August 2000
 R.J. Brachman ?On the epistemological status of
semantic networks?; in "Associative Networks -
Representation and use of Knowledge by Computers",
N.V.Findler (Ed.); Academic Press, New York, 1979.
Basili R., M.T. Pazienza, F.M. Zanzotto (1998), A
Robust Parser for Information Extraction, Proceedings of
the European Conference on Artificial Intelligence (ECAI
'98), Brighton (UK), August 1998.
Bourigault, D. , C. Jacquemin and M.C. L'Homme
(1998), Eds. Proceedings of the first Workshop on
Computational Terminology, jointly held with COLING-
98, Montreal, 1998
A. Cucchiarelli, D. Luzi and P. Velardi "Semantic
tagging of Unknown Proper Noun"s in Natural Language
Engineering, December 1998.
V. Paliouras Cucchiarelli A., Karkaletsis G.
Spyropolous C. Velardi P. "Automatic adaptation of
Proper Noun Dictionaries through cooperation of
machine learning and probabilistic methods" 23rd annual
SIGIR, Athens, June 2000
Basili, R., M.T. Pazienza, P. Velardi,  An Empyrical
Symbolic Approach to Natural Language Processing,
Artificial Intelligence, 85, 59-99,  August 1996
R. Basili, G. De Rossi, M.T. Pazienza "Inducing
Terminology for Lexical Acquisition" Proc. of the
Second Conference on Empirical Methods in Natural
Lanaguge Processing, Providence, USA, August 1997
R. Basili, M.T. Pazienza F. Zanzotto, "Customizable
Modular Lexicalized Parsing Extraction" proc. of  Int.
Workshop on  Parsing Technology, Povo (Trento)
February 2000
B. Daille "Study and Implementation of Combined
Techniques for Automatic Extraction of Terminology"
Proc. of  ACL94 Workshop "The Balancing Act:
combining Symbolic and Statistical Approaches to
Language" , New Mexico State University, July 1994.
R. Fano "Trasmission of Information, MIT press, 1961
Farquhar, R. Fikes, W. Pratt, J. Rice "Collaborative
Ontology Construction for Information Integration"
http://www-ksl-svc.stanford.edu:5915/doc/project-
papers.html
FETISH Groupware (2001)
http://liss.uni.net/QuickPlace/trial/Main.nsf?OpenDatabas
e
Jacquemin, C. (1997). Variation terminologique.
Memoire d'Habilitation Diriger des Recherces and
Informatique Fondamentale. Universit? de Nantes,
Nantes, France.
Justenson J.S. and S.M. Katz (1995) Technical
terminology: some linguistic properties and an algorithm
for identification in text. Natural language engineering,
Vol. 1, Part 1, March 1995
Klavans, J (2001). Text Mining Techniques for Fully
Automatic Glossary Construction, Proceedings of the
HTL2001 Conference, San Diego (CA), March, 2001.
Miller A. "WordNet: An on-line lexical resource"
Special issue of the Journal of Lexicography, 3(4) 1990
M.Missikoff, XF. Wang, ?Consys ? A Web System for
Collaborative Ontology Building?, submitted, Dic. 2000.
Missikoff M., Velardi P. and Fabriani P. (2001) ?Text
Mining Techniques to Automatically Enrich a Domain
Ontology? to appear on Applied Intelligence, Special
Issue on Text and Web Mining.
A. Maedche and S. Staab "Learning Ontologies for the
Semantic Web" http://www.aifb.uni-
karlsruhe.de/WBS/ama/publications.html
Pustejovsky  "The generative lexicon : a theory of
computational lexical semantics" MIT press 1993
Smadja, F, K. McKeown and V. Hatzivassiloglou
(1996) Translating Collocations for Bilingual Lexicons: a
statistical approach, Computational Linguistics, 22:1
SymOntos (2001), a Symbolic Ontology Management
System. http://www.symontos.org
A. Wagner "Enriching a Lexical Semantic Net with
Selectional Prefernces by means of Statistical Corpus
Analysis" ECAI2000 workshop on Ontology Learning,
ibidem
Y. Wilks, B. Slator and L. Guthrie "Electric Words:
Dictionaries, Computers, and Meaning", MIT Press,
Cambridge, MA, 1999
Structural Semantic Interconnection: a knowledge-based approach to Word 
Sense Disambiguation 
 
Roberto NAVIGLI 
Dipartimento di Informatica, 
Universit? di Roma ?La Sapienza? 
Via Salaria, 113 - 00198 Roma, Italy 
navigli@di.uniroma1.it 
 
Paola VELARDI 
Dipartimento di Informatica, 
Universit? di Roma ?La Sapienza? 
Via Salaria, 113 - 00198 Roma, Italy 
velardi@di.uniroma1.it 
 
Abstract 
In this paper we describe the SSI algorithm, a 
structural pattern matching algorithm for 
WSD. The algorithm has been applied to the 
gloss disambiguation task of Senseval-3. 
1 Introduction 
Our approach to WSD lies in the structural 
pattern recognition framework. Structural or 
syntactic pattern recognition (Bunke and Sanfeliu, 
1990) has proven to be effective when the objects 
to be classified contain an inherent, identifiable 
organization, such as image data and time-series 
data. For these objects, a representation based on a 
?flat? vector of features causes a loss of 
information that negatively impacts on 
classification performances. Word senses clearly 
fall under the category of objects that are better 
described through a set of structured features.  
The classification task in a structural pattern 
recognition system is implemented through the 
use of grammars that embody precise criteria to 
discriminate among different classes. Learning a 
structure for the objects to be classified is often a 
major problem in many application areas of 
structural pattern recognition. In the field of 
computational linguistics, however, several efforts 
have been made in the past years to produce large 
lexical knowledge bases and annotated resources, 
offering an ideal starting point for constructing 
structured representations of word senses. 
2 Building structural representations of 
word senses 
We build a structural representation of word 
senses using a variety of knowledge sources, i.e. 
WordNet, Domain Labels (Magnini and Cavaglia, 
2000), annotated corpora like SemCor and LDC-
DSO1. We use this information to automatically 
 
1 LDC http://www.ldc.upenn.edu/ 
generate labeled directed graphs (digraphs)
representations of word senses. We call these 
semantic graphs, since they represent alternative 
conceptualizations for a lexical item. 
Figure 1 shows an example of the semantic 
graph generated for senses #1 of market, where 
nodes represent concepts (WordNet synsets), and 
edges are semantic relations. In each graph, we 
include only nodes with a maximum distance of 3 
from the central node, as suggested by the dashed 
oval in Figure 1. This distance has been 
experimentally established.  
market#1
goods#1
trading#1
gloss
gloss
merchandise
 #1
k ind-of
monopoly#1
kind
-of
export#1
has
-kin
dactivity#1has-kind
consumer
 goods#1
grocery#2
kind-of
kind-of
load#3
kind
-of
commercial
enterprise#2
has-part
commerce#1 kind -of
transportation#5
has-p
art
business
activity#1
glo
ss
service#1
gloss to p
ic
industry#2
kind-of
h as
-pa
rt
gloss
kind-of
food#1
clothing#1
glo
ss
glos
s
enterprise#1
kind-of
production#1
artifact#1
k i n d -o f
express#1
kind-of
consumption#1
gloss
Figure 1. Graph representations for sense #1 of market.
All the used semantic relations are explicitly 
encoded in WordNet, except for three relations 
named topic, gloss and domain, extracted 
respectively from annotated corpora, sense 
definitions and domain labels.  
3 Summary description of the SSI algorithm  
The SSI algorithm consists of an initialization step 
and an iterative step.  
In a generic iteration of the algorithm the input 
is a list of co-occurring terms T = [ t1, ?, tn ] and 
a list of associated senses I = ],...,[ 1 ntt SS , i.e. the 
semantic interpretation of T, where itS 2 is either 
the chosen sense for ti (i.e., the result of a previous 
 
2 Note that with itS we refer interchangeably to the semantic 
graph associated with a sense or to the sense name.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
disambiguation step) or the empty set (i.e., the 
term is not yet disambiguated).  
A set of pending terms is also maintained, P =
}|{ =iti St . I is named the semantic context of T
and is used, at each step, to disambiguate new 
terms in P. 
The algorithm works in an iterative way, so that 
at each stage either at least one term is removed 
from P (i.e., at least a pending term is 
disambiguated) or the procedure stops because no 
more terms can be disambiguated. The output is 
the updated list I of senses associated with the 
input terms T.
Initially, the list I includes the senses of 
monosemous terms in T. If no monosemous terms 
are found, the algorithm makes an initial guess 
based on the most probable sense of the less 
ambiguous term. The initialisation policy is 
adjusted depending upon the specific WSD task 
considered. Section 5 describes the policy adopted 
for the task of gloss disambiguation in WordNet. 
During a generic iteration, the algorithm selects 
those terms t in P showing an interconnection 
between at least one sense S of t and one or more 
senses in I. The likelihood for a sense S of being 
the correct interpretation of t, given the semantic 
context I, is estimated by the function 
CxTfI : , where C is the set of all the 
concepts in the ontology O, defined as follows: 


 	


=
otherwise
SynsetstSensesSifISSS
tSf I 0
)(})'|)',(({
),(

where Senses(t) is the subset of concepts C in O
associated with the term t, and 
})'...|)...(({')',( 1121
121 SSSSeeewSS nn
e
n
eee
n = 
 ,
i.e. a function (?) of the weights (w) of each path 
connecting S with S?, where S and S? are 
represented by semantic graphs. A semantic path 
between two senses S and S?, '... 11
121 SSSS nn
e
n
eee  
 ,
is represented by a sequence of edge labels 
neee  ...21 . A proper choice for both  and ? may 
be the sum function (or the average sum function). 
A context-free grammar G = (E, N, SG, PG)
encodes all the meaningful semantic patterns. The 
terminal symbols (E) are edge labels, while the 
non-terminal symbols (N) encode (sub)paths 
between concepts; SG is the start symbol of G and 
PG the set of its productions. 
We associate a weight with each production 
A in PG, where NA
 and *)( EN 
 , i.e. 
 is a sequence of terminal and non-terminal 
symbols. If the sequence of edge labels neee  ...21
belongs to L(G), the language generated by the 
grammar, and provided that G is not ambiguous, 
then )...( 21 neeew  is given by the sum of the 
weights of the productions applied in the 
derivation nG eeeS + ...21 . The grammar G is 
described in the next section. 
Finally, the algorithm selects ),(maxarg tSfI
CS

as 
the most likely interpretation of t and updates the 
list I with the chosen concept. A threshold can be 
applied to ),( tSf to improve the robustness of 
system?s choices. 
At the end of a generic iteration, a number of 
terms is disambiguated and each of them is 
removed from the set of pending terms P. The 
algorithm stops with output I when no sense S can 
be found for the remaining terms in P such that 
0),( >tSfI , that is, P cannot be further reduced. 
In each iteration, interconnections can only be 
found between the sense of a pending term t and 
the senses disambiguated during the previous 
iteration.  
A special case of input for the SSI algorithm is 
given by ]..., ,,[ =I , that is when no initial 
semantic context is available (there are no 
monosemous words in T). In this case, an 
initialization policy selects a term t 
 T and the 
execution is forked into as many processes as the 
number of senses of t.
4 The grammar 
The grammar G has the purpose of describing 
meaningful interconnecting patterns among 
semantic graphs representing conceptualisations 
in O. We define a pattern as a sequence of 
consecutive semantic relations neee  ...21 where 
Eei 
 , the set of terminal symbols, i.e. the 
vocabulary of conceptual relations in O. Two 
relations 1+ii ee are consecutive if the edges 
labelled with ie and 1+ie are incoming and/or 
outgoing from the same concept node, that is 
1)( + ii ee S , 1)( + ii ee S , 1)( + ii ee S , 1)( + ii ee S . A meaningful 
pattern between two senses S and S? is a sequence 
neee  ...21 that belongs to L(G). 
In its current version, the grammar G has been 
defined manually, inspecting the intersecting 
patterns automatically extracted from pairs of 
manually disambiguated word senses co-occurring 
in different domains. Some of the rules in G are 
inspired by previous work on the eXtended 
WordNet project described in (Milhalcea and 
Moldovan, 2001). The terminal symbols ei are the 
conceptual relations extracted from WordNet and 
other on-line lexical-semantic resources, as 
described in Section 2. 
G is defined as a quadruple (E, N, SG, PG), 
where E = { ekind-of, ehas-kind, epart-of, ehas-part, egloss, eis-
in-gloss, etopic, ? }, N = { SG, Ss, Sg, S1, S2, S3, S4, S5,
S6, E1, E2, ? }, and PG includes about 50 
productions.  
As stated in previous section, the weight 
)...( 21 neeew  of a semantic path neee  ...21 is given 
by the sum of the weights of the productions 
applied in the derivation nG eeeS + ...21 . These 
weights have been learned using a perceptron 
model, trained with standard word sense 
disambiguation data, such as the SemCor corpus. 
Examples of the rules in G are provided in the 
subsequent Section 5. 
5 Application of the SSI algorithm to the 
disambiguation of WordNet glosses 
For the gloss disambiguation task, the SSI 
algorithm is initialized as follows: In step 1, the 
list I includes the synset S whose gloss we wish to 
disambiguate, and the list P includes all the terms 
in the gloss and in the gloss of the hyperonym of 
S. Words in the hyperonym?s gloss are useful to 
augment the context available for disambiguation.  
In the following, we present a sample execution of 
the SSI algorithm for the gloss disambiguation 
task applied to sense #1 of retrospective: ?an
exhibition of a representative selection of an 
artist?s life work?. For this task the algorithm uses 
a context enriched with the definition of the synset 
hyperonym, i.e. art exhibition#1: ?an exhibition of 
art objects (paintings or statues)?.  
Initially we have: 
I = { retrospective#1 }3
P = { work, object, exhibition, life, statue, artist, 
selection, representative, painting, art }
At first, I is enriched with the senses of 
monosemous words in the definition of 
retrospective#1 and its hyperonym: 
I = { retrospective#1, statue#1, artist#1 }
P = { work, object, exhibition, life, selection, 
representative, painting, art }
since statue and artist are monosemous terms in 
WordNet. During the first iteration, the algorithm 
finds three matching paths4:
retrospective#1 2  ofkind exhibition#2, statue#1 
3  ofkind  art#1 and statue#1 
3 For convenience here we denote I as a set rather 
than a list. 
4 With S R  i S? we denote a path of i consecutive 
edges labeled with the relation R interconnecting S
with S?.
6  ofkind object#1 
This leads to: 
I = { retrospective#1, statue#1, artist#1, 
exhibition#2, object#1, art#1 }
P = { work, life, selection, representative, painting 
}
During the second iteration, a 
hyponymy/holonymy path (rule S2) is found:  
art#1 2  kindhas painting#1 (painting is a kind 
of art)which leads to: 
I = { retrospective#1, statue#1, artist#1, 
exhibition#2, object#1, art#1, painting#1 }
P = { work, life, selection, representative }
The third iteration finds a co-occurrence (topic 
rule) path between artist#1 and sense 12 of life 
(biography, life history): 
artist#1 topic  life#12 
then, we get: 
I = { retrospective#1, statue#1, artist#1, 
exhibition#2, object#1, art#1, painting#1, life#12 
}
P = { work, selection, representative }
The algorithm stops because no additional 
matches are found. The chosen senses concerning 
terms contained in the hyperonym?s gloss were of 
help during disambiguation, but are now 
discarded. Thus we have: 
GlossSynsets(retrospective#1) = { artist#1, 
exhibition#2, life#12, work#2 }
6 Evaluation  
The SSI algorithm is currently tailored for noun 
disambiguation. Additional semantic knowledge 
and ad-hoc rules would be needed to detect 
semantic patterns centered on concepts associated 
to verbs. Current research is directed towards 
integrating in semantic graphs information from 
FrameNet and VerbNet, but the main problem is 
harmonizing these knowledge bases with 
WordNet?s senses and relations inventory.  A 
second problem of SSI, when applied to 
unrestricted WSD tasks, is that it is designed to 
disambiguate with high precision, possibly low 
recall. In many interesting applications of WSD, 
especially in information retrieval, improved 
document access may be obtained even when only 
few words in a query are disambiguated, but the 
disambiguation precision needs to be well over 
the 70% threshold. Supporting experiments are 
described in (Navigli and Velardi, 2003). 
The results obtained by our system in Senseval-
3 reflect these limitations (see Figure 2).  
The main run, named OntoLearn, uses a 
threshold to select only those senses with a weight 
over a given threshold. OntoLearnEx uses a non-
greedy version of the SSI algorithm. Again, a 
threshold is used to accepts or reject sense 
choices. Finally, OntoLearnB uses the ?first 
sense? heuristics to select a sense, every since a 
sense choice is below the threshold (or no patterns 
are found for a given word).  
82.60% 75.30%
37.50%
68.50%
68.40%
32.30%39.10%
49.70%
99.90%
0%
20%
40%
60%
80%
100%
OntoLearn OntoLearnB OntoLearnEx
Precision Recall Attempted
Figure 2. Results of three runs submitted to Senseval-3. 
Table 1 shows the precision and recall of 
OntoLearn main run by syntactic category. It 
shows that, as expected, the SSI algorithm is 
currently tuned for noun disambiguation. 
 
Nouns Verbs Adj. 
Precision 86.0% 69.4% 78.6% 
Recall 44.7% 13.5% 26.2% 
Attempted 52.0% 19.5% 33.3% 
Table 1. Precision and Recall by syntactic category. 
The official Senseval-3 evaluation has been 
performed against a set of so called ?golden 
glosses? produced by Dan Moldovan and its 
group5. This test set however had several 
problems, that we partly detected and submitted to 
the organisers. 
Besides some technical errors in the data set 
(presence of WordNet 1.7 and 2.0 senses, missing 
glosses, etc.) there are sense-tagging 
inconsistencies that are very evident. 
For example, one of our highest performing 
sense tagging rules in SSI is the direct 
hyperonymy path.  This rule reads as follows: ?if 
the word wj appears in the gloss of a synset Si, and 
if one of the synsets of wj, Sj, is the direct 
hyperonym of Si, then, select Sj as the correct 
sense for wj?. 
An example is custom#4 defined as ?habitual 
patronage?. We have that: 
{custom-n#4} kind _ of  {trade,patronage-n#5} 
 
5 http://xwn.hlt.utdallas.edu/wsd.html 
therefore we select sense #5 of patronage, while 
Moldovan?s ?golden? sense is #1. 
We do not intend to dispute whether the 
?questionable? sense assignment is the one 
provided in the golden gloss or rather the 
hyperonym selected by the WordNet 
lexicographers. In any case, the detected patterns 
show a clear inconsistency in the data.  
These patterns (313) have been submitted to the 
organisers, who then decided to remove them 
from the data set.   
7 Conclusion 
The interesting feature of the SSI algorithm, 
unlike many co-occurrence based and statistical 
approaches to WSD, is a justification (i.e. a set of 
semantic patterns) to support a sense choice. 
Furthermore, each sense choice has a weight 
representing the confidence of the system in its 
output. Therefore SSI can be tuned for high 
precision (possibly low recall), an asset that we 
consider more realistic for practical WSD 
applications. 
Currently, the system is tuned for noun 
disambiguation, since we build structural 
representations of word senses using lexical 
knowledge bases that are considerably richer for 
nouns. Extending semantic graphs associated to 
verbs and adding appropriate interconnection 
rules implies harmonizing WordNet and available 
lexical resources for verbs, e.g. FrameNet and 
VerbNet. This extension is in progress. 
References  
H. Bunke and A. Sanfeliu (editors) (1990) 
Syntactic and Structural pattern Recognition: 
Theory and Applications World Scientific, Series 
in Computer Science vol. 7, 1990. 
A. Gangemi, R. Navigli and P. Velardi (2003) 
?The OntoWordNet Project: extension and 
axiomatization of conceptual relations in 
WordNet?, 2nd Int. Conf. ODBASE, ed. Springer 
Verlag, 3-7 November 2003, Catania, Italy. 
B. Magnini and G. Cavaglia (2000) 
?Integrating Subject Field Codes into WordNet?, 
Proceedings of  LREC2000, Atenas 2000. 
Milhalcea R., Moldovan D. I. (2001) 
?eXtended WordNet: progress report?. NAACL 
2001 Workshop on WordNet and other lexical 
resources, Pittsburg, June 2001. 
Navigli R. and Velardi P. (2003) ?An Analysis 
of Ontology-based Query Expansion Strategies?, 
Workshop on Adaptive Text Extraction and 
Mining September 22nd, 2003 Cavtat-Dubrovnik 
(Croatia), held in conjunction with ECML 2003. 
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 1?9,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Enriching a formal ontology with a thesaurus: an application in the cultural heritage domain 
 Roberto Navigli, Paola Velardi Dipartimento di Informatica,  Universit? ?La Sapienza?, Italy  navigli,velardi@di.uniroma1.it   Abstract This paper describes a pattern-based method to automatically enrich a core ontology with the definitions of a domain glossary. We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology. To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others. 1 Introduction Large-scale, automatic semantic annotation of web documents based on well established domain ontologies would allow various Semantic Web applications to emerge and gain acceptance. Wide coverage ontologies are indeed available for general-purpose domains (e.g. WordNet, CYC, SUMO1), however semantic annotation in unconstrained areas seems still out of reach for state of art systems. Domain-specific ontologies are preferable since they limit the domain and make the applications feasible. Furthermore, real-world applications (e.g tourism, cultural heritage, e-commerce) are dominated by the requirements of the related web communities, who began to believe in the benefits deriving from the application of Semantic Web techniques.  These communities are interested in extracting from texts specific types of information, rather than general-purpose relations. Accordingly, they produced remarkable efforts to conceptualize their competence domain through the definition of a core ontology2.                                                 1 WordNet: http://wordnet.princeton.edu, CYC: http://www.opencyc.org, SUMO: http://www.ontologyportal.org 2 a core ontology is a very basic ontology consisting only of the minimal concepts relations and axioms 
Relevant examples are in the area of enterprise modeling (Fox et al 1997) (Uschold et al 1998) and cultural heritage (Doerr, 2003). Core ontologies are indeed a necessary starting point to model in a principled way the basic concepts, relations and axioms of a given domain. But in order for an ontology to be really usable in applications, it is necessary to enrich the core structure with the thousands of concepts and instances that ?make? the domain.  In this paper we present a methodology to automatically annotate a glossary G with the semantic relations of an existing core ontology O. Glosses are then converted into formal concepts, used to enrich O. The annotation of glossary definitions is performed using regular expressions, a widely adopted text mining approach. However, while in the literature regular expressions seek mostly for patterns at the lexical and part of speech level, we defined more complex expressions enriched with syntactic and semantic constraints.  A word sense disambiguation algorithm, SSI (Velardi and Navigli, 2005), is used to automatically replace the high level semantic constraints specified in the core ontology with fine?grained sense restrictions, using the sense inventory of a general purpose lexicalized ontology, WordNet.  We experimented our methodology in the cultural heritage domain, since for this domain several well-established resources are available, like the CIDOC-CRM core ontology, the AAT art and architecture thesaurus, and others. The paper is organized as follows: in Section 2 we briefly present the CIDOC and the other resources used in this work. In Section 3 we describe in detail the ontology enrichment algorithm. Finally, in Section 4 we provide a performance evaluation on a subset of CIDOC                                                                     required to understand the other concepts in the domain. 
1
properties and a sub-tree of the AAT thesaurus. Related literature is examined in Section 5. 2 Semantic and lexical resources in the cultural heritage domain  In this section we briefly describe the resources that have been used in this work. 2.1 The CIDOC CRM The core ontology O is the CIDOC CRM (Doerr, 2003), a formal core ontology whose purpose is to facilitate the integration and exchange of cultural heritage information between heterogeneous sources. It is currently being elaborated to become an ISO standard. In the current version (4.0) the CIDOC includes 84 taxonomically structured concepts (called entities) and a flat set of 141 semantic relations, called properties. Properties are defined in terms of domain (the class for which a property is formally defined) and range (the class that comprises all potential values of a property), e.g.:  P46 is composed of (forms part of) Domain:  E19 Physical Object Range:  E42 Object Identifier  The CIDOC is an ?informal? resource. To make it usable by a computer program, we replaced specifications written in natural language with formal ones. For each property R, we created a tuple R(Cd,Cr) where Cd and Cr are the domain and range entities specified in the CIDOC reference manual. 2.2 The AAT thesaurus The domain glossary G is the Art and Architecture Thesaurus (AAT) a controlled vocabulary for use by indexers, catalogers, and other professionals concerned with information management in the fields of art and architecture. In its current version3 it includes more than 133,000 terms, descriptions, bibliographic citations, and other information relating to fine art, architecture, decorative arts, archival materials, and material culture.  An example is the following:  maest? Note: Refers to a work of a specific iconographic type, depicting the Virgin Mary and Christ Child enthroned in                                                 3 http://www.getty.edu/research/conducting_research/ vocabularies/aat/ 
the center with saints and angels in adoration to each side. The type developed in Italy in the 13th century and was based on earlier Greek types. Works of this type are typically two-dimensional, including painted panels (often altarpieces), manuscript illuminations, and low-relief carvings. Hierarchical Position:  Objects Facet  .... Visual and Verbal Communication  ........ Visual Works  ............ <visual works>  ................ <visual works by subject type>  .................... maest?  We manually mapped the top CIDOC entities to AAT concepts, as shown in Table 1.  AAT topmost CIDOC entities Top concept of AAT  CRM Entity (E1), Persistent Item (E77) Styles and Periods Period (E4) Events Event (E5) Activities Facet Activity (E7) Processes/Techniques Beginning of Existence (E63) Objects Facet Physical Stuff (E18), Physical Object (E19) Artifacts Physical Man-Made Stuff (E24) Materials Facet Material  (E57) Agents Facet Actor (E39) Time Time-Span (E52) Place Place (E53) Table 1: mapping between AAT and CIDOC. 2.3 Additional resources A general purpose lexicalised ontology, WordNet, is used to bridge the high level concepts defined in the core ontology with the words in a fragment of text. As better clarified later, WordNet  is used to verify that certain words in a string of text f satisfy the range constraints R(Cd,Cr) in the CIDOC. In order to do so, we manually linked the WordNet topmost concepts to the CIDOC entities. For example, the concept E19 (Physical Object) is mapped to the WordNet synset ?object, physical object?. Furthermore, we created a gazetteer I of named entities extracting names from the Dmoz4, a large human-edited directory of the web, the Union List of Artist Names (ULAN) and the Getty Thesaurus of Geographic Names (GTG) provided by the Getty institute, along with the AAT. Named entities often occur in AAT definitions, therefore, NE recognition is relevant for our task. 
                                                4 http://dmoz.org/about.html 
2
3 Enriching the CIDOC CRM with the AAT thesaurus In this Section we describe in detail the method for automatic semantic annotation and ontology enrichment in the cultural heritage domain.  We start with an example of the task to be performed: given a gloss G of a term t in the glossary G, the first objective is to annotate certain gloss fragments with CIDOC relations. For example, the following gloss fragment for ?vedute? is annotated with a CIDOC relation, as follows: [..]The first vedute probably were <carried-out-by>painted by northern European artists</carried-out-by> [...] Then, for each annotated fragment, we extract a semantic relation instance R(Ct,Cw), where R is a relation in O, Ct and Cw are respectively the domain and range of R. The concept Ct corresponds to its lexical realization t, while Cw is the concept associated to the ?head? word w in the annotated segment of the gloss.  In the previous example, the relation instance is: R carried_out_by(vedute,European_artist) The annotation process allows to automatically enrich O with an existing glossary in the same domain of O, since each pair of term and gloss (t,G) in the glossary G is transformed into a formal definition, compliant with O. Furthermore, the very same method used to annotate definitions can be used to annotate free text with the relations of the enriched ontology O?.  We now describe the method in detail. Let G be a glossary, t a term in G and G the corresponding natural language definition (gloss).  The main steps of the algorithm are the following: 1. Part-of-Speech analysis. Each input gloss is processed with a part-of-speech tagger, TreeTagger5. As a result, for each gloss G = w1 w2 ? wn, a string of part-of-speech tags p1 p2 ? pn is produced, where pi 
? 
?P is the part-of-speech tag chosen by TreeTagger for word wi, and P = { N, A, V, J, R, C, P, S, W } is a simplified set of syntactic categories (respectively, nouns, articles, verbs, adjectives, adverbs, conjunctions, prepositions,                                                 5 TreeTagger is available at: http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger.  
symbols, wh-words). Terminological strings (european artist) are detected using our Term Extractor tool, already described in (Navigli and Velardi, 2004).  2. Named Entity recognition. We augmented TreeTagger with the ability to capture named entities of locations, organizations, persons, numbers and time expressions. In order to do so, we use regular expressions (Friedl, 1997) in a rather standard way, therefore we omit details. When a named entity string wi wi+1 ? wi+j is recognized, it is transformed into a single term and a specific part of speech denoting the kind of entity is assigned to it (L for cities (e.g. Venice), countries and continents, T for time and historical periods (e.g. Middle Ages), O for organizations and persons (e.g. Leonardo Da Vinci), B for numbers). 3. Annotation of sentence segments with CIDOC properties. Once the text has been parsed, we use manually defined regular expressions to capture relevant fragments. The regular expressions are used to annotate gloss segments with properties grounded on the CIDOC-CRM relation model. Given a gloss G and a property6 R, we define a relation checker cR taking in input G and producing in output a set FR of fragments of G annotated with the property R: <R>f</R>. The selection of a fragment f to be included in the set FR is based on three different kinds of constraints:  ? a part-of-speech constraint p(f, pos-string) matches the part-of-speech (pos) string associated with the fragment f against a regular expression (pos-string), specifying the required syntactic structure. ? a lexical constraint l(f, k, lexical-constraint) matches the lemma of the word in k-th position of f against a regular expression (lexical-constraint), constraining the lexical conformation of words occurring within the fragment f. ? semantic constraints on domain and range sD(f, semantic-domain) and s(f, k, semantic-range) are valid, respectively, if the term t and the word in the k-th position of f match the semantic constraints on domain and range imposed by the CIDOC, i.e. if there exists at least one sense of t Ct and one sense of w Cw such that:                                                 6 In what follows, we adopt the CIDOC terminology for relations and concepts, i.e. properties and entities. 
3
Rkind-of*(Cd, Ct) and Rkind-of*(Cr, Cw)7  More formally, the annotation process is defined as follows: A relation checker cR for a property R is a logical expression composed with constraint predicates and logical connectives, using the following production rules:  cR ? sD(f, semantic-domain) 
? 
? cR? cR? ? ?cR?| (cR? ? cR?) | (cR? 
? 
? cR?) cR? ? p(f, pos-string) | l(f, k, lexical-constraint)  | s(f, k, semantic-range)  where f is a variable representing a sentence fragment. Notice that a relation checker must always specify a semantic constraint sD on the domain of the relation R being checked on fragment f. Optionally, it must also satisfy a semantic constraint s on the k-th element of f, the range of R. For example, the following excerpt of the checker for the is-composed-of relation (P46):  (1) cis-composed-of(f) = sD(f, physical object#1)  
? 
? p(f, ?(V)1(P)2R?A?[CRJVN]*(N)3?)  
? 
? l(f, 1,  ?^(consisting|composed|comprised|constructed)$?)  
? 
? l(f, 2, ?of?) 
? 
? s(f, 3, physical_object#1)  reads as follows: ?the fragment f is valid if it consists of a verb in the set { consisting, composed, comprised, constructed }, followed by a preposition ?of?, a possibly empty number of adverbs, adjectives, verbs and nouns, and terminated by a noun interpretable as a physical object in the WordNet concept inventory?. The first predicate, sD, requires that also the term t whose gloss contains f (i.e., its domain) be interpretable as a physical object. Notice that some letter in the regular expression specified for the part-of-speech constraint is enclosed in parentheses. This allows it to identify the relative positions of words to be matched against lexical and semantic constraints, as shown graphically in Figure 1. 
(V)
1
(P)
2
R?A?[CRJVN]*(N)
3
(composed)
1
 (of)
2
 two or more (negatives)
3
part-of-speech string
gloss fragmentFigure 1. Correspondence between parenthesized part-of-speech tags and words in a gloss fragment.  Checker (1) recognizes, among others, the following fragments (the words whose part-of-                                                7 Rkind-of* denotes zero, one, or more applications of Rkind-of. 
speech tags are enclosed in parentheses are indicated in bold):  (consisting)1 (of)2 semi-precious (stones)3 (matching part-of-speech string: (V)1(P)2 J(N)3) (composed)1 (of)2 (knots)3 (matching part-of-speech string: (V) 1(P)2(N)3) As a second example, an excerpt of the checker for the consists-of (P45) relation is the following:  (2) cconsists-of(f) = sD(f, physical object#1)  
? 
?p(f, ?(V)1(P)2A?[JN,VC]*(N)3?) 
? 
? l(f, 1, ?^(make|do|produce|decorated)$?)  
? 
? l(f, 2, ?^(of|by|with)$?)  
? 
? ?s(f, 3, color#1)
? 
? ?s(f, 3, activity#1)  
? 
? (s(f, 3, material#1) 
? 
? s(f, 3, solid#1)  
? 
? s(f, 3, liquid#1))  recognizing, among others, the following phrases: ? (made)1 (with)2 the red earth pigment (sinopia)3 (matching part-of-speech string: (V)1(P)2AJNN(N)3) ? (decorated)1 (with)2 red, black, and white (paint)3 (matching part-of-speech string: (V)1(P)2JJCJ(N)3) Notice that in both checkers (1) and (2) semantic constraints are specified in terms of WordNet sense numbers (material#1, solid#1 and liquid#1), and can also be negative (?color#1 and ?activity#1). The motivation is that CIDOC constraints are coarse-grained due to the small number of available core concepts: for example, the property P45 consists of simply requires that the range belongs to the class Material (E57). Using these coarse grained constraints would produce false positives in the annotation task, as discussed later. Using WordNet for semantic constraints has two advantages: first, it is possible to write more fine-grained (and hence more reliable) constraints, second, regular expressions can be re-used, at least in part, for other domains and ontologies. In fact, several CIDOC properties are rather general-purpose.  Notice that, as remarked in section 2.3, replacing coarse CIDOC sense restrictions with WordNet fine-grained restrictions is possible since we mapped the 84 CIDOC entities onto WordNet topmost concepts. 4. Formalisation of glosses. The annotations generated in the previous step are the basis for extracting property instances to enrich the CIDOC CRM with a conceptualization of the AAT terms. In general, for each gloss G defining a concept Ct, 
4
and for each fragment f ? FR of G annotated with the property R: <R>f</R>, it is possible to extract one or more property instances in the form of a triple R(Ct, Cw), where Cw is the concept associated with a term or multi-word expression w occurring in f (i.e. its language realization) and Ct is the concept associated to the defined term t in AAT. For example, from the definition of tatting (a kind of lace) the algorithm automatically annotates the phrase composed of knots, suggesting that this phrase specifies the range of the is-composed-of property for the term tatting: Ris-composed-of(Ctatting, Cknot) In this property instance, Ctatting is the domain of the property (a term in the AAT glossary) and Cknot is the range (a specific term in the definition G of tatting).  Selecting the concept associated to the domain is rather straightforward: glossary terms are in general not ambiguous, and, if they are, we simply use a numbering policy to identify the appropriate concept. In the example at hand, Ctatting=tatting#1 (the first and only sense in AAT). Therefore, if Ct matches the domain restrictions in the regular expression for R, then the domain of the relation is considered to be Ct. Selecting the range of a relation is instead more complicated. The first problem is to select the correct words in a fragment f. Only certain words of an annotated gloss fragment can be exploited to extract the range of a property instance. For example, in the phrase ?depiction of fruit, flowers, and other objects? (from the definition of still life), only fruit, flowers, objects represent the range of the property instances of kind depicts (P62). When writing relation checkers, as described in the previous paragraph of this Section, we can add markers of ontological relevance by specifying a predicate r(f, k) for each relevant position k in a fragment f. The purpose of these markers is precisely to identify words in f whose corresponding concepts are in the range of a property. For instance, the checker (1) cis-composed-of from the previous paragraph is augmented with the conjunction: 
? 
? r(f, 3). We added the predicate r(f, 3) because the third parenthesis in the part-of-speech string refers to an ontologically relevant element (i.e. the candidate range of the is-composed-of property).  The second problem is that words that are candidate ranges can be ambiguous, and they 
often are, especially if they do not belong to the domain glossary G. Considering the previous example of the property depicts, the word fruit is not a term of the AAT glossary, and it has 3 senses in WordNet (the fruit of a plant, the consequence of some action, an amount of product). The property depicts, as defined in the CIDOC, simply requires that the range be of type Entity (E1). Therefore, all the three senses of fruit in WordNet satisfy this constraint. Whenever the range constraints in a relation checker do not allow a full disambiguation, we apply the SSI algorithm (Navigli and Velardi, 2005), a semantic disambiguation algorithm based on structural pattern recognition, available on-line8. The algorithm is applied to the words belonging to the segment fragment f and is based on the detection of relevant semantic interconnection patterns between the appropriate senses. These patterns are extracted from a lexical knowledge base that merges WordNet with other resources, like word collocations, on-line dictionaries, etc. For example, in the fragment ?depictions of fruit, flowers, and other objects? the following properties are created for the concept still_ life#1: 
 Rdepicts(still_ life#1, fruit#1) Rdepicts (still_ life#1, flower#2) Rdepicts (still_ life#1, object#1)  Some of the semantic patterns supporting this sense selection are shown in Figure 2. A further possibility is that the range of a relation R is a concept instance. We create concept instances if the word w extracted from the fragment f is a named entity. For example, the definition of Venetian lace is annotated as ?Refers to needle lace created <current-or-former-location> in Venice</current-or-former-location> [?]?. As a result, the following triple is produced: Rhas-current-or-former-location(Venetian_lace#1, Venice:city#1) where Venetian_ lace#1 is the concept label generated for the term Venetian lace in the AAT and Venice is an instance of the concept city#1 (city, metropolis, urban center) in WordNet.                                                  8 SSI is an on-line knowledge-based WSD algorithm accessible from http://lcl.di.uniroma1.it/ssi. The on-line version also outputs the detected semantic connections (as those in Figure 2). 
5
fruit#1
flower#2
object#1
depiction#1
bunch#1
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
flower
head#1
related-to
h
a
s
-
p
a
r
t
cyme#1
r
e
l
a
t
e
d
-
t
o
inflorescence
#2
k
i
n
d
-
o
f
r
e
l
a
t
e
d
-
t
o
still life#1
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
description#1
k
i
n
d
-
o
f
statement#1
k
i
n
d
-
o
f
thing#5
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
appearance#1
portrayal#1
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
k
i
n
d
-
o
f
forest#2
land#3
k
i
n
d
-
o
f
r
e
l
a
t
e
d
-
t
o
r
e
l
a
t
e
d
-
t
o
plant#1
r
e
l
a
t
e
d
-
t
o
organism#1
living thing#1
k
i
n
d
-
o
f
kind-of
k
i
n
d
-
o
f
 Figure 2. Semantic Interconnections selected by the SSI algorithm when given the word list: ?depiction, fruit, flower, object?. 4 Evaluation Since the CIDOC-CRM model formalizes a large number of fine-grained properties (precisely, 141), we selected a subset of properties for our experiments (reported in Table 2). We wrote a relation checker for each property in the Table. By applying the checkers in cascade to a gloss G, a set of annotations is produced. The following is an example of an annotated gloss for the term ?vedute?:  Refers to detailed, largely factual topographical views, especially <has-time-span>18th-century</has-time-span> Italian paintings, drawings, or prints of cities. The first vedute probably were <carried-out-by>painted by northern European artists</carried-out-by> who worked <has former-or-current-location>in Italy</has former-or-current-location><has-time-span>in the 16th century</has-time-span>. The term refers more generally to any painting, drawing or print <depicts>representing a landscape or town view</depicts> that is largely topographical in conception.  Figure 3 shows a more comprehensive graph representation of the outcome for the concepts vedute#1 and maest?#1 (see the gloss in Section 2.2). To evaluate the methodology described in Section 3 we considered 814 glosses from the Visual Works sub-tree of the AAT thesaurus, containing a total of 27,925 words. The authors wrote the relation checkers by tuning them on a subset of 122 glosses, and tested their generality on the remaining 692. The test set was manually tagged with the subset of the CIDOC-CRM properties shown in Table 2 by two annotators with adjudication (requiring a careful comparison of the two sets of annotations). We performed two experiments: in the first, we evaluated the gloss annotation task, in the 
second the property instance extraction task, i.e. the ability to identify the appropriate domain and range of a property instance. In the case of the gloss annotation task, for evaluating each piece of information we adopted the measures of ?labeled? precision and recall. These measures are commonly used to evaluate parse trees obtained by a parser (Charniak, 1997) and allow the rewarding of good partial results. Given a property R, labeled precision is the number of words annotated correctly with R over the number of words annotated automatically with R, while labeled recall is the number of words annotated correctly with R over the total number of words manually annotated with R. Table 3 shows the results obtained by applying the checkers to tag the test set (containing a total number of 1,328 distinct annotations and 5,965 annotated words). Note that here we are evaluating the ability of the system to assign the correct tag to every word in a gloss fragment f, according to the appropriate relation checker. We choose to evaluate the tag assigned to single words rather than to a whole phrase, because each misalignment would count as a mistake even if the most part of a phrase was tagged correctly by the automatic annotator. The second experiment consisted in the evaluation of the property instances extracted. Starting from 1,328 manually annotated fragments of 692 glosses, the checkers extracted an overall number of 1,101 property instances. We randomly selected a subset of 160 glosses for evaluation, from which we manually extracted 344 property instances. Two aspects of the property instance extraction task had to be assessed: ? the extraction of the appropriate range words in a gloss, for a given property instance ? the precision and recall in the extraction of the appropriate concepts for both domain and range of the property instance.  An overall number of 233 property instances were automatically collected by the checkers, out of which 203 were correct with respect to the first assessment (87.12% precision (203/233), 59.01% recall (203/344)). In the second evaluation, for each property instance R(Ct, Cw) we assessed the semantic correctness of both the concepts Ct and Cw. The appropriateness of the concept Ct chosen 
6
for the domain must be evaluated, since, even if a term t satisfies the semantic constraints of the domain for a property R, still it can be the case that a fragment f in G does not refer to t, like in the following example: 
pastels (visual works) -- Works of art, typically on a paper or vellum support, to which designs are applied using crayons made of ground pigment held together with a binder, typically oil or water and gum. 
 Code Name Domain Range Example P26 moved to Move Place P26(installation of public sculpture, public place) P27 moved from Move Place P27(removal of cornice pictures, wall) P53 has former/current location Physical Stuff Place P53(fancy pictures, London) P55 has current location Physical Object Place P55(macrame, Genoa) P46 is composed of (is part of) Physical Stuff Physical Stuff P46(lace, knot) P62 depicts Physical Man-Made Stuff Entity P62(still life, fruit) P4 has time span Temporal Entity Time Span P4(pattern drawings, Renaissance) P14 carried out by (performed) Activity Actor P14(blotted line drawings, Andy Warhol) P92 brought into existence by Persistent Item Beginning of Existence P92(aquatints, aquatint process) P45 consists of (incorporated in) Physical Stuff Material P45(sculpture, stone) Table 2: A subset of the relations from the CIDOC-CRM model.  
maest?
Virgin Mary
Christ child
Italy
13
th
 century
painted panel
carving
altarpiece
illuminations
d
e
p
i
c
t
s
d
e
p
i
c
t
s
i
s
-
c
o
m
p
o
s
e
d
-
o
f
i
s
-
c
o
m
p
o
s
e
d
-
o
f
i
s
-
c
o
m
p
o
s
e
d
-
o
f
is-c
om
po
se
d-o
f
h
a
s
-
c
u
r
r
e
n
t
o
r
-
f
o
r
m
e
r
-
l
o
c
a
t
i
o
n
h
a
s
t
i
m
e
-
s
p
a
n
vedute
landscape
town view
Italy
18
th
 century
artist
d
e
p
i
c
t
s
d
e
p
i
c
t
s
c
a
r
r
i
e
d
-
o
u
t
b
y
h
a
s
-
c
u
r
r
e
n
t
o
r
-
f
o
r
m
e
r
-
l
o
c
a
t
i
o
n
h
a
s
t
i
m
e
-
s
p
a
n
16
th
 century
h
a
s
t
i
m
e
-
s
p
a
n
work
h
a
s
-
t
y
p
e
topographical
views
h
a
s
-
t
y
p
e  Figure 3. Extracted conceptualisation (in graphical form) of the terms maest?#1 and vedute#1 (sense numbers are omitted for clarity). In this example, ground pigment refers to crayons (not to pastels). The evaluation of the semantic correctness of the domain and range of the property instances extracted led to the final figures of 81.11% (189/233) precision and 54.94% (189/344) recall, due to 9 errors in the choice of Ct as a domain for an instance R(Ct, Cw) and 5 errors in the semantic disambiguation of range words w not appearing in AAT, but encoded in WordNet (as described in the last part of Section 3). A final experiment was performed to evaluate the generality of the approach presented in this paper. As already remarked, the same procedure used for annotating the glosses of a thesaurus can be used to annotate web documents. Our objective in this third experiment was to: ? Evaluate the ability of the system to annotate fragments of web documents with CIDOC relations ? Evaluate the domain dependency of the relation checkers, by letting the system annotate documents not in the cultural heritage domain.  
We then selected 5 documents at random from an historical archive and an artist?s biographies archive9 including about 6,000 words in total, about 5,000 of which in the historical domain. We then ran the automatic annotation procedure on these documents and we evaluated the result, using the same criteria as in Table 3.  Property Precision Recall  P26 ? moved to 84.95% (79/93) 64.23% (79/123)  P27 ? moved from 81.25% (39/48) 78.00% (39/50)  P53 - has former or  current location 78.09% (916/1173) 67.80% (916/1351)  P55 ? has current  location 100.00% (8/8) 100.00% (8/8)  P46 ?composed of 87.49% (944/1079) 70.76% (944/1334)  P62 ? depicts 94.15%  (370/393) 65.26% (370/567)  P4 ? has time span 91.93% (547/595) 76.40% (547/716)  P14 - carried out by 91.71% (343/374) 71.91% (343/477)  P92 ? brought into  existence 89.54% (471/526) 62.72% (471/751)  P45 ? consists of 74.67% (398/533) 57.60% (398/691) Avg. performance 85.34% (4115/4822) 67.81% (4115/6068) Table 3: Precision and Recall of the gloss annotation task. Table 4 presents the results of the experiment. Only 5 out of 10 properties had at least one                                                 9 http://historicaltextarchive.com and http://www.artnet.com/library 
7
instance in the analysed documents. It is remarkable that, especially for the less domain-dependent properties, the precision and recall of the algorithm is still high, thus showing the generality of the method. Notice that the historical documents influenced the result much more than the artist biographies, because of their dimension. In Table 4 the recall of P14 (carried out by) is omitted. This is motivated by the fact that this property, in a generic domain, corresponds to the agent relation (?an active animate entity that voluntarily initiates an action?10), while in the cultural heritage domain it has a more narrow interpretation (an example of this relation in the CIDOC handbook is: ?the painting of the Sistine Chapel (E7) was carried out by Michelangelo Buonarroti (E21) in the role of master craftsman (E55)?). However, the domain and range restrictions for P14 correspond to an agent relation, therefore, in a generic domain, one should annotate as ?carried out by? almost any verb phrase with the subject (including pronouns and anaphoric references) in the class Human.  Property Precision Recall P53 ? has former or current location 79.84% (198/248) 77.95% (198/254) P46 ? composed of 83.58% (112/134) 96.55% (112/116) P4 ? has time span 78.32% (112/143) 50.68% (112/221) P14 ? carried out by 60.61% (40/66) - - P45 ? consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task.  5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al 2005; Valarakos et al 2004)) use regular expressions to identify named entities, i.e. concept instances. Other methods extract hypernym11 relations using syntactic and lexical                                                 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is the same as in AAT, and in 26% of the cases, either the extracted hypernym is more general than the one defined in AAT, or the contrary, 
patterns (Snow et al 2005; Morin and Jaquemin 2004) or supervised clustering techniques (Kashyap et al 2003).  In our work, we automatically learn formal concepts, not simply instances or taxonomies (e.g. the graphs of Figure 3) compliant with the semantics of a well-established core ontology, the CIDOC. The method is unsupervised, in the sense that it does not need manual annotation of a significant fragment of text. However, it relies on a set of manually written regular expressions, based on lexical, part-of-speech, and semantic constraints. The structure of regular expressions is rather more complex than in similar works using regular expressions, especially for the use of automatically verified semantic constraints. This complexity is indeed necessary to identify non-trivial relations in an unconstrained text and without training. The issue is however how much this method generalizes to other domains:  ? A first problem is the availability of lexical and semantic resources used by the algorithm. The most critical requirement of the method is the availability of sound domain core ontologies, which hopefully will be produced by other web communities stimulated by the recent success of CIDOC CRM. On the other side, in absence of an agreed conceptual reference model, no large scale annotation is possible at all. As for the other resources used by our algorithm, glossaries, thesaura and gazetteers are widely available in ?mature? domains. If not, we developed a methodology, described in (Navigli and Velardi, 2005b), to automatically create a glossary in novel domains (e.g. enterprise interoperability), extracting definition sentences from domain-relevant documents and authoritative web sites. ? The second problem is about the generality of regular expressions. Clearly, the relation checkers that we defined are tuned on the CIDOC properties. This however is consistent with our target: in specific domains users are interested to identify specific relations, not general purpose. Certain relevant application domains ?like cultural heritage, e-commerce, or tourism- are those that dictate specifications for real-world applications of NLP techniques. However, several CIDOC properties are rather general (especially locative and                                                                        wrt the AAT hierarchy. 
8
temporal relations) therefore some relation checkers easily apply to other domains, as demonstrated by the experiment on automatic annotation of historical archives in Table 4. Furthermore, the method used to verify semantic constraints is fully general, since it is based on WordNet and a general-purpose, untrained semantic disambiguation algorithm, SSI.   ? Finally, the authors believe with some degree of convincement that automatic pattern-learning methods often require non-trivial human effort just like manual methods (because of the need of annotated data, careful parameter setting, etc.), and furthermore they are unable to combine in a non-trivial way different types of features (e.g. lexical, syntactic, semantic). To make an example, a recent work on learning hypernymy patterns (Morin and Jacquemin, 2004) provides the full list of learned patterns. The complexity of these patterns is certainly lower than the regular expression structures used in this work, and many of them are rather intuitive.   In the literature the tasks on which automatic methods have been tested are rather constrained, and do not convincingly demonstrate the superiority of automatic with respect to manually defined patterns. For example, in Senseval-3 (automated labeling of semantic roles12), participating systems are requested to identify semantic roles in a sentence fragment for which the ?frame semantics? is given, therefore the possible semantic relations to be identified are quite limited.  However, we believe that our method can be automated to some degree (for example, machine learning methods can be used to bootstrap the syntactic patterns, and to learn semantic constraints), a research line we are currently exploring. References  M. E. Califf and R.J. Mooney, ?Bottom-up relational learning of pattern matching rules for information extraction? Machine Learning research, 4 (2)177-210, 2004. E. Charniak, ?Statistical Techniques for Natural Language Parsing?, AI Magazine 18(4), 33-44, 1997. P. Cimiano, G. Ladwig and S. Staab, ?Gimme the context: context-driven automatic semantic                                                 12 http://www.clres.com/SensSemRoles.html 
annotation with C-PANKOW? In: Proceedings of the 14th International WWW Conference, WWW 2005, Chiba, Japan, May, 2005. ACM Press. M. Doerr, ?The CIDOC Conceptual Reference Module: An Ontological Approach to Semantic Interoperability of Metadata?. AI Magazine, Volume 24, Number 3, Fall 2003. M. S. Fox, M. Barbuceanu, M. Gruninger, and J. Lin, "An Organisation Ontology for Enterprise Modeling", In Simulating Organizations: Computational Models of Institutions and Groups, M. Prietula, K. Carley & L. Gasser (Eds), Menlo Park CA: AAAI/MIT Press, pp. 131-152. 1997 J.E. F. Friedl ?Mastering Regular Expressions? O?Reilly eds., ISBN: 1-56592-257-3, First edition January 1997. V. Kashyap, C. Ramakrishnan, T. Rindflesch. "Toward (Semi)-Automatic Generation of Bio-medical Ontologies", Proceedings of American Medical Informatics Association, 2003 G. A. Miller, ``WordNet: a lexical database for English.'' In: Communications of the ACM 38 (11), November 1995, pp. 39 - 41.  E. Morin and C. Jacquemin ?Automatic acquisition and expansion of hypernym links? Computer and the Humanities, 38: 363-396, 2004 R. Navigli, P. Velardi. Learning Domain Ontologies from Document Warehouses and Dedicated Websites, Computational Linguistics (30-2), MIT Press, June, 2004. R. Navigli and P. Velardi, ?Structural Semantic Interconnections: a knowledge-based approach to word sense disambiguation?, Special Issue-Syntactic and Structural Pattern Recognition, IEEE TPAMI, Volume: 27, Issue: 7, 2005. R. Navigli, P. Velardi. Automatic Acquisition of a Thesaurus of Interoperability Terms, Proc. of 16th IFAC World Congress, Praha, Czech Republic, July 4-8th, 2005b. R. Snow, D. Jurafsky, A. Y. Ng, "Learning syntactic patters for automatic hypernym discovery", NIPS 17, 2005. M. Thelen, E. Riloff, "A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts", Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2002. M. Uschold, M. King, S. Moralee and Y. Zorgios, ?The Enterprise Ontology?, The Knowledge Engineering Review , Vol. 13, Special Issue on Putting Ontologies to Use (eds. Uschold. M. and Tate. A.), 1998 Valarakos, G. Paliouras, V. Karkaletsis, G. Vouros, ?Enhancing Ontological Knowledge through Ontology Population and Enrichment? in Proceedings of the 14th EKAW conf., LNAI, Vol. 3257, pp. 144-156, Springer Verlag, 2004. 
9
OntoLearn Reloaded: A Graph-Based
Algorithm for Taxonomy Induction
Paola Velardi?
Sapienza University of Rome
Stefano Faralli?
Sapienza University of Rome
Roberto Navigli?
Sapienza University of Rome
In 2004 we published in this journal an article describing OntoLearn, one of the first systems
to automatically induce a taxonomy from documents and Web sites. Since then, OntoLearn has
continued to be an active area of research in our group and has become a reference work within
the community. In this paper we describe our next-generation taxonomy learning methodol-
ogy, which we name OntoLearn Reloaded. Unlike many taxonomy learning approaches in the
literature, our novel algorithm learns both concepts and relations entirely from scratch via the
automated extraction of terms, definitions, and hypernyms. This results in a very dense, cyclic
and potentially disconnected hypernym graph. The algorithm then induces a taxonomy from
this graph via optimal branching and a novel weighting policy. Our experiments show that we
obtain high-quality results, both when building brand-new taxonomies and when reconstructing
sub-hierarchies of existing taxonomies.
1. Introduction
Ontologies have proven useful for different applications, such as heterogeneous data
integration, information search and retrieval, question answering, and, in general, for
fostering interoperability between systems. Ontologies can be classified into three main
types (Sowa 2000), namely: i) formal ontologies, that is, conceptualizations whose cat-
egories are distinguished by axioms and formal definitions, stated in logic to support
complex inferences and computations; ii) prototype-based ontologies, which are based
on typical instances or prototypes rather than axioms and definitions in logic; iii) lexical-
ized (or terminological) ontologies, which are specified by subtype-supertype relations
and describe concepts by labels or synonyms rather than by prototypical instances.
Here we focus on lexicalized ontologies because, in order to enable natural
language applications such as semantically enhanced information retrieval and ques-
tion answering, we need a clear connection between our formal representation of the
? Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy.
E-mail: {velardi,faralli,navigli}@di.uniroma1.it.
Submission received: 17 December 2011; revised submission received: 28 July 2012; accepted for publication:
10 October 2012.
doi:10.1162/COLI a 00146
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
domain and the language used to express domain meanings within text. And, in turn,
this connection can be established by producing full-fledged lexicalized ontologies for
the domain of interest. Manually constructing ontologies is a very demanding task,
however, requiring a large amount of time and effort, even when principled solutions
are used (De Nicola, Missikoff, and Navigli 2009). A quite recent challenge, referred
to as ontology learning, consists of automatically or semi-automatically creating a
lexicalized ontology using textual data from corpora or the Web (Gomez-Perez and
Manzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al 2011). As
a result of ontology learning, the heavy requirements of manual ontology construction
can be drastically reduced.
In this paper we deal with the problem of learning a taxonomy (i.e., the backbone
of an ontology) entirely from scratch. Very few systems in the literature address this
task. OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in this
area. In OntoLearn taxonomy learning was accomplished in four steps: terminology
extraction, derivation of term sub-trees via string inclusion, disambiguation of domain
terms using a novel Word Sense Disambiguation algorithm, and combining the sub-
trees into a taxonomy. The use of a static, general-purpose repository of semantic
knowledge, namely, WordNet (Miller et al 1990; Fellbaum 1998), prevented the system
from learning taxonomies in technical domains, however.
In this paper we present OntoLearn Reloaded, a graph-based algorithm for learning
a taxonomy from the ground up. OntoLearn Reloaded preserves the initial step of
our 2004 pioneering work (Navigli and Velardi 2004), that is, automated terminology
extraction from a domain corpus, but it drops the requirement for WordNet (thereby
avoiding dependence on the English language). It also drops the term compositionality
assumption that previously led to us having to use a Word Sense Disambiguation
algorithm?namely, SSI (Navigli and Velardi 2005)?to structure the taxonomy. Instead,
we now exploit textual definitions, extracted from a corpus and the Web in an iterative
fashion, to automatically create a highly dense, cyclic, potentially disconnected hyper-
nym graph. An optimal branching algorithm is then used to induce a full-fledged tree-
like taxonomy. Further graph-based processing augments the taxonomy with additional
hypernyms, thus producing a Directed Acyclic Graph (DAG).
Our system provides a considerable advancement over the state of the art in
taxonomy learning:
 First, excepting for the manual selection of just a few upper nodes, this
is the first algorithm that has been experimentally shown to build from
scratch a new taxonomy (i.e., both concepts and hypernym relations)
for arbitrary domains, including very technical ones for which
gold-standard taxonomies do not exist.
 Second, we tackle the problem with no simplifying assumptions: We cope
with issues such as term ambiguity, complexity of hypernymy patterns,
and multiple hypernyms.
 Third, we propose a novel algorithm to extract an optimal branching
from the resulting hypernym graph, which?after some recovery
steps?becomes our final taxonomy. Taxonomy induction is the
main theoretical contribution of the paper.
 Fourth, the evaluation is not limited, as it is in most papers, to the number
of retrieved hypernymy relations that are found in a reference taxonomy.
666
Velardi, Faralli, and Navigli OntoLearn Reloaded
Instead, we also analyze the extracted taxonomy in its entirety;
furthermore, we acquire two ?brand new? taxonomies in the
domains of ARTIFICIAL INTELLIGENCE and FINANCE.
 Finally, our taxonomy-building workflow is fully implemented and
the software components are either freely available from our Web
site,1 or reproducible.
In this paper we extend our recent work on the topic (Navigli, Velardi, and Faralli
2011) as follows: i) we describe in full detail the taxonomy induction algorithm; ii) we
enhance our methodology with a final step aimed at creating a DAG, rather than a strict
tree-like taxonomical structure; iii) we perform a large-scale multi-faceted evaluation
of the taxonomy learning algorithm on six domains; and iv) we contribute a novel
methodology for evaluating an automatically learned taxonomy against a reference
gold standard.
In Section 2 we illustrate the related work. We then describe our taxonomy-
induction algorithm in Section 3. In Section 4 we present our experiments, and discuss
the results. Evaluation is both qualitative (on new ARTIFICIAL INTELLIGENCE and
FINANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies). Sec-
tion 5 is dedicated to concluding remarks.
2. Related Work
Two main approaches are used to learn an ontology from text: rule-based and distri-
butional approaches. Rule-based approaches use predefined rules or heuristic patterns
to extract terms and relations. These approaches are typically based on lexico-syntactic
patterns, first introduced by Hearst (1992). Instances of relations are harvested from text
by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y).
Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999;
Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques
(Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case,
a number of term pairs in the wanted relation are manually picked and the relation is
sought within text corpora or the Web. Other rule-based approaches learn a taxonomy
by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci,
and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computa-
tional lexicons such as WordNet (Ponzetto and Navigli 2009).
Distributional approaches, instead, model ontology learning as a clustering or
classification task, and draw primarily on the notions of distributional similarity (Pado
and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon
and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such
approaches are based on the assumption that paradigmatically-related concepts2 appear
in similar contexts and their main advantage is that they are able to discover relations
that do not explicitly appear in the text. They are typically less accurate, however, and
the selection of feature types, notion of context, and similarity metrics vary considerably
depending on the specific approach used.
1 http://lcl.uniroma1.it/ontolearn reloaded and http://ontolearn.org.
2 Because we are concerned with lexical taxonomies, in this paper we use the words concepts and terms
interchangeably.
667
Computational Linguistics Volume 39, Number 3
Recently, Yang and Callan (2009) presented a semi-supervised taxonomy induc-
tion framework that integrates contextual, co-occurrence, and syntactic dependencies,
lexico-syntactic patterns, and other features to learn an ontology metric, calculated
in terms of the semantic distance for each pair of terms in a taxonomy. Terms are
incrementally clustered on the basis of their ontology metric scores. In their work, the
authors assume that the set of ontological concepts C is known, therefore taxonomy
learning is limited to finding relations between given pairs in C. In the experiments,
they only use the word senses within a particular WordNet sub-hierarchy so as to avoid
any lexical ambiguity. Their best experiment obtains a 0.85 precision rate and 0.32 recall
rate in replicating is-a links on 12 focused WordNet sub-hierarchies, such as PEOPLE,
BUILDING, PLACE, MILK, MEAL, and so on.
Snow, Jurafsky, and Ng (2006) propose the incremental construction of taxonomies
using a probabilistic model. In their work they combine evidence from multiple
supervised classifiers trained on very large training data sets of hyponymy and cousin
relations. Given the body of evidence obtained from all the relevant word pairs in
a lexico-syntactic relation, the taxonomy learning task is defined probabilistically as
the problem of finding the taxonomy that maximizes the probability of having that
evidence (a supervised logistic regression model is used for this). Rather than learning
a new taxonomy from scratch, however, this approach aims at attaching new concepts
under the appropriate nodes of an existing taxonomy (i.e., WordNet). The approach is
evaluated by manually assessing the quality of the single hypernymy edges connecting
leaf concepts to existing ones in WordNet, with no evaluation of a full-fledged struc-
tured taxonomy and no restriction to a specific domain. A related, weakly supervised
approach aimed at categorizing named entities, and attaching them to WordNet leaves,
was proposed by Pasca (2004). Other approaches use formal concept analysis (Cimiano,
Hotho, and Staab 2005), probabilistic and information-theoretic measures to learn tax-
onomies from a folksonomy (Tang et al 2009), and Markov logic networks and syntactic
parsing applied to domain text (Poon and Domingos 2010).
The work closest to ours is that presented by Kozareva and Hovy (2010). From an
initial given set of root concepts and basic level terms, the authors first use Hearst-like
lexico-syntactic patterns iteratively to harvest new terms from the Web. As a result a
set of hyponym?hypernym relations is obtained. Next, in order to induce taxonomic
relations between intermediate concepts, the Web is searched again with surface pat-
terns. Finally, nodes from the resulting graph are removed if the out-degree is below
a threshold, and edges are pruned by removing cycles and selecting the longest path
in the case of multiple paths between concept pairs. Kozareva and Hovy?s method has
some limitations, which we discuss later in this paper. Here we note that, in evalu-
ating their methodology, the authors discard any retrieved nodes not belonging to a
WordNet sub-hierarchy (they experiment on PLANTS, VEHICLES, and ANIMALS), thus
it all comes down to Yang and Callan?s (2009) experiment of finding relations between a
pre-assigned set of nodes.
In practice, none of the algorithms described in the literature was actually applied
to the task of creating a new taxonomy for an arbitrary domain of interest truly from
scratch. Instead, what is typically measured is the ability of a system to reproduce as
far as possible the relations of an already existing taxonomy (a common test is WordNet
or the Open Directory Project3), when given the set of domain concepts. Evaluating
against a gold standard is, indeed, a reasonable validation methodology. The claim to be
3 http://www.dmoz.org/.
668
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 1
The OntoLearn Reloaded taxonomy learning workflow.
?automatically building? a taxonomy needs also to be demonstrated on new domains
for which no a priori knowledge is available, however. In an unknown domain, tax-
onomy induction requires the solution of several further problems, such as identifying
domain-appropriate concepts, extracting appropriate hypernym relations, and detect-
ing lexical ambiguity, whereas some of these problems can be ignored when evaluating
against a gold standard (we will return to this issue in detail in Section 4). In fact,
the predecessor of OntoLearn Reloaded, that is, OntoLearn (Navigli and Velardi 2004),
suffers from a similar problem, in that it relies on the WordNet taxonomy to establish
paradigmatic connections between concepts.
3. The Taxonomy Learning Workflow
OntoLearn Reloaded starts from an initially empty directed graph and a corpus for the
domain of interest (e.g., an archive of artificial intelligence papers). We also assume
that a small set of upper terms (entity, abstraction, etc.), which we take as the end
points of our algorithm, has been manually defined (e.g., from a general purpose taxon-
omy like WordNet) or is available for the domain.4 Our taxonomy-learning workflow,
summarized in Figure 1, consists of five steps:
1. Initial Terminology Extraction (Section 3.1): The first step applies a term
extraction algorithm to the input domain corpus in order to produce an
initial domain terminology as output.
2. Definition & Hypernym Extraction (Section 3.2): Candidate definition
sentences are then sought for the extracted domain terminology. For each
term t, a domain-independent classifier is used to select well-formed
definitions from the candidate sentences and extract the corresponding
hypernyms of t.
4 Although very few domain taxonomies are available, upper (core) concepts have been defined in several
domains, such as MEDICINE, ART, ECONOMY, and so forth.
669
Computational Linguistics Volume 39, Number 3
3. Domain Filtering (Section 3.3): A domain filtering technique is applied
to filter out those definitions that do not pertain to the domain of interest.
The resulting domain definitions are used to populate the directed graph
with hypernymy relations connecting t to the extracted hypernym h.
Steps (2) and (3) are then iterated on the newly acquired hypernyms,
until a termination condition occurs.
4. Graph Pruning (Section 3.4): As a result of the iterative phase we obtain
a dense hypernym graph that potentially contains cycles and multiple
hypernyms for most nodes. In this step we combine a novel weighting
strategy with the Chu-Liu/Edmonds algorithm (Chu and Liu 1965;
Edmonds 1967) to produce an optimal branching (i.e., a tree-like
taxonomy) of the initial noisy graph.
5. Edge Recovery (Section 3.5): Finally, we optionally apply a recovery
strategy to reattach some of the hypernym edges deleted during the
previous step, so as to produce a full-fledged taxonomy in the form
of a DAG.
We now describe in full detail the five steps of OntoLearn Reloaded.5
3.1 Initial Terminology Extraction
Domain terms are the building blocks of a taxonomy. Even though in many cases an
initial domain terminology is available, new terms emerge continuously, especially
in novel or scientific domains. Therefore, in this work we aim at fully automatizing
the taxonomy induction process. Thus, we start from a text corpus for the domain
of interest and extract domain terms from the corpus by means of a terminology
extraction algorithm. For this we use our term extraction tool, TermExtractor,6 that
implements measures of domain consensus and relevance to harvest the most relevant
terms for the domain from the input corpus.7 As a result, an initial domain terminol-
ogy T(0) is produced that includes both single- and multi-word expressions (such as,
respectively, graph and flow network). We add one node to our initially empty graph
Gnoisy = (Vnoisy, Enoisy) for each term in T(0)?that is, we set Vnoisy := T(0) and Enoisy := ?.
In Table 1 we show an excerpt of our ARTIFICIAL INTELLIGENCE and FINANCE
terminologies (cf. Section 4 for more details). Note that our initial set of domain terms
(and, consequently, nodes) will be enriched with the new hypernyms acquired during
the subsequent iterative phase, described in the next section.
3.2 Definition and Hypernym Extraction
The aim of our taxonomy induction algorithm is to learn a hypernym graph by means of
several iterations, starting from T(0) and stopping at very general terms U, that we take
as the end point of our algorithm. The upper terms are chosen from WordNet topmost
5 A video of the first four steps of OntoLearn Reloaded is available at
http://www.youtube.com/watch?v=-k3cOEoI Dk.
6 http://lcl.uniroma1.it/termextractor.
7 TermExtractor has already been described in Sclano and Velardi (2007) and in Navigli and Velardi (2004);
therefore the interested reader is referred to these papers for additional details.
670
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 1
An excerpt of the terminology extracted for the ARTIFICIAL INTELLIGENCE and FINANCE
domains.
ARTIFICIAL INTELLIGENCE
acyclic graph parallel corpus flow network
adjacency matrix parse tree pattern matching
artificial intelligence partitioned semantic network pagerank
tree data structure pathfinder taxonomic hierarchy
FINANCE
investor shareholder open economy
bid-ask spread profit maximization speculation
long term debt shadow price risk management
optimal financing policy ratings profit margin
synsets. In other words, U contains all the terms in the selected topmost synsets. In
Table 2 we show representative synonyms of the upper-level synsets that we used for
the ARTIFICIAL INTELLIGENCE and FINANCE domains. Seeing that we use high-level
concepts, the set U can be considered domain-independent. Other choices are of course
possible, especially if an upper ontology for a given domain is already available.
For each term t ? T(i) (initially, i = 0), we first check whether t is an upper term (i.e.,
t ? U). If it is, we just skip it (because we do not aim at extending the taxonomy beyond
an upper term). Otherwise, definition sentences are sought for t in the domain corpus
and in a portion of the Web. To do so we use Word-Class Lattices (WCLs) (Navigli and
Velardi 2010, introduced hereafter), which is a domain-independent machine-learned
classifier that identifies definition sentences for the given term t, together with the
corresponding hypernym (i.e., lexical generalization) in each sentence.
For each term in our set T(i), we then automatically extract definition candidates
from the domain corpus, Web documents, and Web glossaries, by harvesting all the
sentences that contain t. To obtain on-line glossaries we use a Web glossary extraction
system (Velardi, Navigli, and D?Amadio 2008). Definitions can also be obtained via a
lightweight bootstrapping process (De Benedictis, Faralli, Navigli 2013).
Finally, we apply WCLs and collect all those sentences that are classified as defini-
tional. We show some terms with their definitions in Table 3 (first and second column,
respectively). The extracted hypernym is shown in italics.
Table 2
The set of upper concepts used in OntoLearn Reloaded for AI and FINANCE (only representative
synonyms from the corresponding WordNet synsets are shown).
ability#n#1 abstraction#n#6 act#n#2 code#n#2
communication#n#2 concept#n#1 data#n#1 device#n#1
discipline#n#1 entity#n#1 event#n#1 expression#n#6
research#n#1 instrumentality#n#1 knowledge#n#1 knowledge domain#n#1
language#n#1 methodology#n#2 model#n#1 organization#n#1
person#n#1 phenomenon#n#1 process#n#1 property#n#2
quality#n#1 quantity#n#1 relation#n#1 representation#n#2
science#n#1 system#n#2 technique#n#1 theory#n#1
671
Computational Linguistics Volume 39, Number 3
Table 3
Some definitions for the ARTIFICIAL INTELLIGENCE domain (defined term in bold, extracted
hypernym in italics).
Term Definition Weight Domain?
adjacency matrix an adjacency matrix is a zero-one matrix 1.00 
flow network in graph theory, a flow network is a directed graph 0.57 
flow network global cash flow network is an online company that
specializes in education and training courses in
teaching the entrepreneurship
0.14 ?
Table 4
Example definitions (defined terms are marked in bold face, their hypernyms in italics).
[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of . . . ]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.
[Myrtales]DF [are an order of]VF [ flowering plants]GF [placed as a basal group . . . ]REST.
3.2.1 Word-Class Lattices. We now describe our WCL algorithm for the classification of
definitional sentences and hypernym extraction. Our model is based on a formal notion
of textual definition. Specifically, we assume a definition contains the following fields
(Storrer and Wellinghoff 2006):
 The DEFINIENDUM field (DF): this part of the definition includes the
definiendum (that is, the word being defined) and its modifiers
(e.g., ?In computer science, a pixel?);
 The DEFINITOR field (VF): which includes the verb phrase used to
introduce the definition (e.g., ?is?);
 The DEFINIENS field (GF): which includes the genus phrase (usually
including the hypernym, e.g., ?a dot?);
 The REST field (RF): which includes additional clauses that further
specify the differentia of the definiendum with respect to its genus
(e.g., ?that is part of a computer image?).
To train our definition extraction algorithm, a data set of textual definitions was
manually annotated with these fields, as shown in Table 4.8 Furthermore, the single-
or multi-word expression denoting the hypernym was also tagged. In Table 4, for each
sentence the definiendum and its hypernym are marked in bold and italics, respectively.
Unlike other work in the literature dealing with definition extraction (Hovy et al 2003;
Fahmi and Bouma 2006; Westerhout 2009; Zhang and Jiang 2009), we covered not only
a variety of definition styles in our training set, in addition to the classic X is a Y pattern,
but also a variety of domains. Therefore, our WCL algorithm requires no re-training
when changing the application domain, as experimentally demonstrated by Navigli and
Velardi (2010). Table 5 shows some non-trivial patterns for the VF field.
8 Available on-line at: http://lcl.uniroma1.it/wcl.
672
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 5
Some nontrivial patterns for the VF field.
is a term used to describe is a specialized form of
is the genus of was coined to describe
is a term that refers to a kind of is a special class of
can denote is the extension of the concept of
is commonly used to refer to is defined both as
Starting from the training set, the WCL algorithm learns generalized definitional
models as detailed hereafter.
Generalized sentences. First, training and test sentences are part-of-speech tagged with the
TreeTagger system, a part-of-speech tagger available for many languages (Schmid 1995).
The first step in obtaining a definitional pattern is word generalization. Depending on
its frequency we define a word class as either a word itself or its part of speech. Formally,
let T be the set of training sentences. We first determine the set F of words in T whose
frequency is above a threshold ? (e.g., the, a, an, of ). In our training sentences, we replace
the defined term with the token ?TARGET? (note that ?TARGET? ? F).
Given a new sentence s = t1, t2, . . . , tn, where ti is the i-th token of s, we generalize
its words ti to word classes t?i as follows:
t?i =
{
ti if ti ? F
POS(ti) otherwise
that is, a word ti is left unchanged if it occurs frequently in the training corpus (i.e.,
ti ? F); otherwise it is replaced with its part of speech (POS(ti)). As a result we obtain a
generalized sentence s?. For instance, given the first sentence in Table 4, we obtain the
corresponding generalized sentence: ?In NNS, a ?TARGET? is a JJ NN,? where NN and
JJ indicate the noun and adjective classes, respectively. Generalized sentences are dou-
bly beneficial: First, they help reduce the annotation burden, in that many differently
lexicalized sentences can be caught by a single generalized sentence; second, thanks
to their reduction of the definition variability, they allow for a higher-recall definition
model.
Star patterns. Let T again be the set of training sentences. In this step we associate a
star pattern ?(s) with each sentence s ? T . To do so, let s ? T be a sentence such that
s = t1, t2, . . . , tn, where ti is its i-th token. Given the set F of most frequent words in T ,
the star pattern ?(s) associated with s is obtained by replacing with * all the tokens ti 
? F,
that is, all the tokens that are non-frequent words. For instance, given the sentence ?In
arts, a chiaroscuro is a monochrome picture,? the corresponding star pattern is ?In *, a
?TARGET? is a *,? where ?TARGET? is the defined term.
Sentence clustering. We then cluster the sentences in our training set T on the basis of
their star pattern. Formally, let ? = (?1, . . . ,?m) be the set of star patterns associated
with the sentences in T . We create a clustering C = (C1, . . . , Cm) such that Ci = {s ? T :
?(s) = ?i}, that is, Ci contains all the sentences whose star pattern is ?i.
As an example, assume ?3 = ?In *, a ?TARGET? is a *.? The first three sentences
reported in Table 4 are all grouped into cluster C3. We note that each cluster Ci contains
673
Computational Linguistics Volume 39, Number 3
sentences whose degree of variability is generally much lower than for any pair of
sentences in T belonging to two different clusters.
Word-class lattice construction. The final step consists of the construction of a WCL for
each sentence cluster, using the corresponding generalized sentences. Given such a
cluster Ci ? C, we apply a greedy algorithm that iteratively constructs the WCL.
Let Ci = {s1, s2, . . . , s|Ci|} and consider its first sentence s1 = t1, t2, . . . , tn. Initially, we
create a directed graph G = (V, E) such that V = {t1, . . . , tn} and E = {(t1, t2), (t2, t3), . . . ,
(tn?1, tn)}. Next, for each j = 2, . . . , |Ci|, we determine the alignment between sentence sj
and each sentence sk ? Ci such that k < j according to the following dynamic program-
ming formulation (Cormen, Leiserson, and Rivest 1990, pages 314?319):
Ma,b = max {Ma?1,b?1 + Sa,b, Ma,b?1, Ma?1,b}, (1)
where a ? {0, . . . , |sk|} and b ? {0, . . . , |sj|}, Sa,b is a score of the matching between the
a-th token of sk and the b-th token of sj, and M0,0, M0,b and Ma,0 are initially set to 0 for
all values of a and b.
The matching score Sa,b is calculated on the generalized sentences s?k and s
?
j as
follows:
Sa,b =
{
1 if t?k,a = t
?
j,b
0 otherwise
where t?k,a and t
?
j,b are the a-th and b-th tokens of s
?
k and s
?
j , respectively. In other words, the
matching score equals 1 if the a-th and the b-th tokens of the two generalized sentences
have the same word class.
Finally, the alignment score between sk and sj is given by M|sk|,|sj|, which calculates
the minimal number of misalignments between the two token sequences. We repeat this
calculation for each sentence sk (k = 1, . . . , j ? 1) and choose the one that maximizes its
alignment score with sj. We then use the best alignment to add sj to the graph G: We add
to the set of nodes V the tokens of s?j for which there is no alignment to s
?
k and we add to
E the edges (t?1, t
?
2), . . . , (t
?
|sj|?1, t
?
|sj|).
Example. Consider the first three definitions in Table 4. Their star pattern is ?In *,
a ?TARGET? is a *.? The corresponding WCL is built as follows: The first part-
of-speech tagged sentence, ?In/IN arts/NN , a/DT ?TARGET?/NN is/VBZ a/DT
monochrome/JJ picture/NN,? is considered. The corresponding generalized sentence is
?In NN1 , a ?TARGET? is a JJ NN2.? The initially empty graph is thus populated with one
node for each word class and one edge for each pair of consecutive tokens, as shown in
Figure 2a. Note that we use a rectangle to denote the hypernym token NN2 . We also add
to the graph a start node and an end node ?, and connect them to the corresponding
initial and final sentence tokens. Next, the second sentence, ?In mathematics, a graph
is a data structure that consists of...,? is aligned to the first sentence. The alignment
is perfect, apart from the NN3 node corresponding to ?data.? The node is added to
the graph together with the edges ?a?? NN3 and NN3 ? NN2 (Figure 2b, node and
edges in bold). Finally, the third sentence in Table 4, ?In computer science, a pixel is a
dot that is part of a computer image,? is generalized as ?In NN4 NN1 , a ?TARGET?
is a NN2.? Thus, a new node NN4 is added, corresponding to ?computer? and new
674
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 2
The Word-Class Lattice construction steps on the first three sentences in Table 4. We show in
bold the nodes and edges added to the lattice graph as a result of each sentence alignment step.
The support of each word class is reported beside the corresponding node.
edges are added that connect node ?In? to NN4 and NN4 to NN1. Figure 2c shows the
resulting lattice.
Variants of the WCL model. So far we have assumed that our WCL model learns lattices
from the training sentences in their entirety (we call this model WCL-1). We also consid-
ered a second model that, given a star pattern, learns three separate WCLs, one for each
of the three main fields of the definition, namely: definiendum (DF), definitor (VF), and
definiens (GF). We refer to this latter model as WCL-3. Note that our model does not
take into account the REST field, so this fragment of the training sentences is discarded.
The reason for introducing the WCL-3 model is that, whereas definitional patterns are
highly variable, DF, VF, and GF individually exhibit a lower variability, thus WCL-3
improves the generalization power.
Once the learning process is over, a set of WCLs is produced. Given a test sentence
s, the classification phase for the WCL-1 model consists of determining whether there
exists a lattice that matches s. In the case of WCL-3, we consider any combination of
definiendum, definitor, and definiens lattices. Given that different combinations might
match, for each combination of three WCLs we calculate a confidence score as follows:
score(s, lDF, lVF, lGF ) = coverage ? log2(support + 1) (2)
where s is the candidate sentence, lDF, lVF, and lGF are three lattices (one for
each definition field), coverage is the fraction of sentence tokens covered by the
675
Computational Linguistics Volume 39, Number 3
third lattice, and support is the total number of sentences in the corresponding star
pattern.
WCL-3 selects, if any, the combination of the three WCLs that best fits the sentence
in terms of coverage and support from the training set. In fact, choosing the most
appropriate combination of lattices impacts the performance of hypernym extraction.
Given its higher performance (Navigli and Velardi 2010), in OntoLearn Reloaded we
use WCL-3 for definition classification and hypernym extraction.
3.3 Domain Filtering and Creation of the Hypernym Graph
The WCLs described in the previous section are used to identify definitional sentences
and harvest hypernyms for the terms obtained as a result of the terminology extraction
phase. In this section we describe how to filter out non-domain definitions and create a
dense hypernym graph for the domain of interest.
Given a term t, the common case is that several definitions are found for it (e.g.,
the flow network example provided at the beginning of this section). Many of these
will not pertain to the domain of interest, however, especially if they are obtained
from the Web or if they define ambiguous terms. For instance, in the COMPUTER
SCIENCE domain, the cash flow definition of flow network shown in Table 3 was not
pertinent. To discard these non-domain sentences, we weight each definition candidate
d(t) according to the domain terms that are contained therein using the following
formula:
DomainWeight(d(t)) =
|Bd(t) ? D|
|Bd(t)|
(3)
where Bd(t) is the bag of content words in the definition candidate d(t) and D is given
by the union of the initial terminology T(0) and the set of single words of the terms in
T(0) that can be found as nouns in WordNet. For example, given T(0) = { greedy algo-
rithm, information retrieval, minimum spanning tree }, our domain terminology D = T(0) ?
{ algorithm, information, retrieval, tree }. According to Equation (3), the domain weight
of a definition is normalized by the total number of content words in the definition, so
as to penalize longer definitions. Domain filtering is performed by keeping only those
definitions d(t) whose DomainWeight(d(t)) ? ?, where ? is an empirically tuned thresh-
old.9 In Table 3 (third column), we show some values calculated for the corresponding
definitions (the fourth column reports a check mark if the domain weight is above
the threshold, an ? otherwise). Domain filtering performs some implicit form of Word
Sense Disambiguation (Navigli 2009), as it aims at discarding senses of hypernyms
which do not pertain to the domain.
Let Ht be the set of hypernyms extracted with WCLs from the definitions of term t
which survived this filtering phase. For each t ? T(i), we add Ht to our graph Gnoisy =
(Vnoisy, Enoisy), that is, we set Vnoisy := Vnoisy ? Ht. For each t, we also add a directed
edge (h, t)10 for each hypernym h ? Ht, that is, we set Enoisy := Enoisy ? {(h, t)}. As a result
9 Empirically set to 0.38, as a result of tuning on several data sets of manually annotated definitions in
different domains.
10 In what follows, (h, t) or h ? t reads ?t is-a h.?
676
Velardi, Faralli, and Navigli OntoLearn Reloaded
of this step, the graph contains our domain terms and their hypernyms obtained from
domain-filtered definitions. We now set:
T(i+1) :=
?
t?T(i)
Ht \
i
?
j=1
T(j) (4)
that is, the new set of terms T(i+1) is given by the hypernyms of the current set of terms
T(i) excluding those terms that were already processed during previous iterations of
the algorithm. Next, we move to iteration i + 1 and repeat the last two steps, namely,
we perform definition/hypernym extraction and domain filtering on T(i+1). As a result
of subsequent iterations, the initially empty graph is increasingly populated with new
nodes (i.e., domain terms) and edges (i.e., hypernymy relations).
After a given number of iterations K, we obtain a dense hypernym graph Gnoisy
that potentially contains more than one connected component. Finally, we connect all
the upper term nodes in Gnoisy to a single top node . As a result of this connecting
step, only one connected component of the noisy hypernym graph?which we call
the backbone component?will contain an upper taxonomy consisting of upper
terms in U.
The resulting graph Gnoisy potentially contains cycles and multiple hypernyms for
the vast majority of nodes. In order to eliminate noise and obtain a full-fledged taxon-
omy, we perform a step of graph pruning, as described in the next section.
3.4 Graph Pruning
At the end of the iterative hypernym harvesting phase, described in Sections 3.2 and 3.3,
the result is a highly dense, potentially disconnected, hypernymy graph (see Section 4
for statistics concerning the experiments that we performed). Wrong nodes and edges
might stem from errors in any of the definition/hypernym extraction and domain filter-
ing steps. Furthermore, for each node, multiple ?good? hypernyms can be harvested.
Rather than using heuristic rules, we devised a novel graph pruning algorithm, based
on the Chu-Liu/Edmonds optimal branching algorithm (Chu and Liu 1965; Edmonds
1967), that exploits the topological graph properties to produce a full-fledged taxonomy.
The algorithm consists of four phases (i.e., graph trimming, edge weighting, optimal
branching, and pruning recovery) that we describe hereafter with the help of the noisy
graph in Figure 3a, whose grey nodes belong to the initial terminology T(0) and whose
bold node is the only upper term.
3.4.1 Graph Trimming. We first perform two trimming steps. First, we disconnect ?false?
roots, i.e., nodes which are not in the set of upper terms and with no incoming edges
(e.g., image in Figure 3a). Second, we disconnect ?false? leaves, namely, leaf nodes which
are not in the initial terminology and with no outgoing edges (e.g., output in Figure 3a).
We show the disconnected components in Figure 3b.
3.4.2 Edge Weighting. Next, we weight the edges in our noisy graph Gnoisy. A policy based
only on graph connectivity (e.g., in-degree or betweenness, see Newman [2010] for a
complete survey) is not sufficient for taxonomy learning.11 Consider again the graph in
11 As also remarked by Kozareva and Hovy (2010), who experimented with in-degree.
677
Computational Linguistics Volume 39, Number 3
Figure 3
A noisy graph excerpt (a), its trimmed version (b), and the final taxonomy resulting from
pruning (c).
Figure 3: In choosing the best hypernym for the term token sequence, a connectivity-based
measure might select collection rather than list, because the former reaches more nodes.
In taxonomy learning, however, longer hypernymy paths should be preferred (e.g., data
structure ? collection ? list ? token sequence is better than data structure ? collection ?
token sequence).
We thus developed a novel weighting policy aimed at finding the best trade-off
between path length and the connectivity of traversed nodes. It consists of three steps:
i) Weight each node v by the number of nodes belonging to the initial
terminology that can be reached from v (potentially including v itself).12
Let w(v) denote the weight of v (e.g., in Figure 3b, node collection reaches
list and token sequence, thus w(collection) = 2, whereas w(graph) = 3).
All weights are shown in the corresponding nodes in Figure 3b.
ii) For each node v, consider all the paths from an upper root r to v.
Let ?(r, v) be the set of such paths. Each path p ? ?(r, v) is weighted
by the cumulative weight of the nodes in the path, namely:
?(p) =
?
v??p
w(v?) (5)
iii) Assign the following weight to each incoming edge (h, v) of v (i.e., h is one
of the direct hypernyms of v):
w(h, v) = max
r?U
max
p??(r,h)
?(p) (6)
This formula assigns to edge (h, v) the value ?(p) of the highest-weighting
path p from h to any upper root ? U. For example, in Figure 3b, w(list) = 2,
w(collection) = 2, w(data structure) = 5. Therefore, the set of paths ?(data
structure, list) = { data structure ? list, data structure ? collection ? list },
whose weights are 7 (w(data structure) + w(list)) and 9 (w(data structure) +
w(collection) + w(list)), respectively. Hence, according to Formula 6, w(list,
token sequence) = 9. We show all edge weights in Figure 3b.
12 Nodes in a cycle are visited only once.
678
Velardi, Faralli, and Navigli OntoLearn Reloaded
3.4.3 Optimal Branching. Next, our goal is to move from a noisy graph to a tree-like
taxonomy on the basis of our edge weighting strategy. A maximum spanning tree
algorithm cannot be applied, however, because our graph is directed. Instead, we need
to find an optimal branching, that is, a rooted tree with an orientation such that every
node but the root has in-degree 1, and whose overall weight is maximum. To this end,
we first apply a pre-processing step: For each (weakly) connected component in the
noisy graph, we consider a number of cases, aimed at identifying a single ?reasonable?
root node to enable the optimal branching to be calculated. Let R be the set of candidate
roots, that is, nodes with no incoming edges. We perform the following steps:
i) If |R| = 1 then we select the only candidate as root.
ii) Else if |R| > 1, if an upper term is in R, we select it as root, else we choose
the root r ? R with the highest weight w according to the weighting
strategy described in Section 3.4.2. We also disconnect all the unselected
roots, that is, those in R \ {r}.
iii) Else (i.e., if |R| = 0), we proceed as for step (ii), but we search candidates
within the entire connected component and select the highest weighting
node. In contrast to step (ii), we remove all the edges incoming to the
selected node.
This procedure guarantees not only the selection but also the existence of a single
root node for each component, from which the optimal branching algorithm can start.
We then apply the Chu-Liu/Edmonds algorithm (Chu and Liu 1965; Edmonds 1967) to
each component Gi = (Vi, Ei) of our directed weighted graph Gnoisy in order to find an
optimal branching. The algorithm consists of two phases: a contraction phase and an
expansion phase. The contraction phase is as follows:
1. For each node which is not a root, we select the entering edge with the
highest weight. Let S be the set of such |Vi| ? 1 edges;
2. If no cycles are formed in S, go to the expansion phase. Otherwise,
continue;
3. Given a cycle in S, contract the nodes in the cycle into a pseudo-node k,
and modify the weight of each edge entering any node v in the cycle from
some node h outside the cycle, according to the following equation:
w(h, k) = w(h, v) + (w(x(v), v) ? minv(w(x(v), v))) (7)
where x(v) is the predecessor of v in the cycle and w(x(v), v) is the weight
of the edge in the cycle which enters v;
4. Select the edge entering the cycle which has the highest modified weight
and replace the edge which enters the same real node in S by the new
selected edge;
5. Go to step 2 with the contracted graph.
The expansion phase is applied if pseudo-nodes have been created during step 3.
Otherwise, this phase is skipped and Ti = (Vi, S) is the optimal branching of component
679
Computational Linguistics Volume 39, Number 3
Gi (i.e., the i-th component of Gnoisy). During the expansion phase, pseudo-nodes are
replaced with the original cycles. To break the cycle, we select the real node v into which
the edge selected in step 4 enters, and remove the edge entering v belonging to the
cycle. Finally, the weights on the edges are restored. For example, consider the cycle
in Figure 4a. Nodes pagerank, map, and rank are contracted into a pseudo-node, and
the edges entering the cycle from outside are re-weighted according to Equation (7).
According to the modified weights (Figure 4b), the selected edge, that is, (table, map),
is the one with weight w = 13. During the expansion phase, the edge (pagerank, map) is
eliminated, thus breaking the cycle (Figure 4c).
The tree-like taxonomy resulting from the application of the Chu-Liu/Edmonds
algorithm to our example in Figure 3b is shown in Figure 3c.
3.4.4 Pruning Recovery. The weighted directed graph Gnoisy input to the Chu-Liu/
Edmonds algorithm might contain many (weakly) connected components. In this case,
an optimal branching is found for each component, resulting in a forest of taxonomy
trees. Although some of these components are actually noisy, others provide an impor-
tant contribution to the final tree-like taxonomy. The objective of this phase is to recover
from excessive pruning, and re-attach some of the components that were disconnected
during the optimal branching step. Recall from Section 3.3 that, by construction, we
have only one backbone component, that is, a component which includes an upper tax-
onomy. Our aim is thus to re-attach meaningful components to the backbone taxonomy.
To this end, we apply Algorithm 1. The algorithm iteratively merges non-backbone trees
to the backbone taxonomy tree T0 in three main steps:
 Semantic reconnection step (lines 7?9 in Algorithm 1): In this step we
reuse a previously removed ?noisy? edge, if one is available, to reattach a
non-backbone component to the backbone. Given a root node rTi of a
non-backbone tree Ti (i > 0), if an edge (v, rTi ) existed in the noisy graph
Gnoisy (i.e., the one obtained before the optimal branching phase), with
v ? T0, then we connect the entire tree Ti to T0 by means of this edge.
Figure 4
A graph excerpt containing a cycle (a); Edmonds? contraction phase: a pseudo-node enclosing
the cycle with updated weights on incoming edges (b); and Edmonds? expansion phase: the
cycle is broken and weights are restored (c).
680
Velardi, Faralli, and Navigli OntoLearn Reloaded
Algorithm 1 PruningRecovery(G, Gnoisy)
Require: G is a forest
1: repeat
2: Let F := {T0, T1, . . . , T|F|} be the forest of trees in G = (V, E)
3: Let T0 ? F be the backbone taxonomy
4: E? ? E
5: for all T in F \ {T0} do
6: rT ? rootOf (T)
7: if ?v ? T0 s.t. (v, rT ) ? Gnoisy then
8: E ? E ? {(v, rT )}
9: break
10: else
11: if out-degree(rT ) = 0 then
12: if ?v ? T0 s.t. v is the longest right substring of rT then
13: E := E ? {(v, rT )}
14: break
15: else
16: E ? E \ {(rT, v) : v ? V}
17: break
18: until E = E?
 Reconnection step by lexical inclusion (lines 11?14): Otherwise, if Ti is a
singleton (the out-degree of rTi is 0) and there exists a node v ? T0 such
that v is the longest right substring of rTi by lexical inclusion,
13 we connect
Ti to the backbone tree T0 by means of the edge (v, rTi ).
 Decomposition step (lines 15?17): Otherwise, if the component Ti is not a
singleton (i.e., if the out-degree of the root node rTi is > 0) we disconnect
rTi from Ti. At first glance, it might seem counterintuitive to remove edges
during pruning recovery. Reconnecting by lexical inclusion within a
domain has already been shown to perform well in the literature (Vossen
2001; Navigli and Velardi 2004), but we want to prevent any cascading
errors on the descendants of the root node, and at the same time free up
other pre-existing ?noisy? edges incident to the descendants.
These three steps are iterated on the newly created components, until no change
is made to the graph (line 18). As a result of our pruning recovery phase we return the
enriched backbone taxonomy. We show in Figure 5 an example of pruning recovery that
starts from a forest of three components (including the backbone taxonomy tree on top,
Figure 5a). The application of the algorithm leads to the disconnection of a tree root,
that is, ordered structure (Figure 5a, lines 15?17 of Algorithm 1), the linking of the trees
rooted at token list and binary search tree to nodes in the backbone taxonomy (Figures 5b
and 5d, lines 7?9), and the linking of balanced binary tree to binary tree thanks to lexical
inclusion (Figure 5c, lines 11?14 of the algorithm).
13 Similarly to our original OntoLearn approach (Navigli and Velardi 2004), we define a node?s string
v = wnwn?1 . . .w2w1 to be lexically included in that of a node v? = w?mw
?
m?1 . . .w
?
2w
?
1 if m > n and
wj = w?j for each j ? {1, . . . , n}.
681
Computational Linguistics Volume 39, Number 3
Figure 5
An example starting with three components, including the backbone taxonomy tree on the
top and two other trees on the bottom (a). As a result of pruning recovery, we disconnect ordered
structure (a); we connect token sequence to token list by means of a ?noisy? edge (b); we connect
binary tree to balanced binary tree by lexical inclusion (c); and finally binary tree to binary search
tree by means of another ?noisy? edge (d).
3.5 Edge Recovery
The goal of the last phase was to recover from the excessive pruning of the optimal
branching phase. Another issue of optimal branching is that we obtain a tree-like tax-
onomic structure, namely, one in which each node has only one hypernym. This is not
fully appropriate in taxonomy learning, because systematic ambiguity and polysemy
often require a concept to be paradigmatically related to more than one hypernym. In
fact, a more appropriate structure for a conceptual hierarchy is a DAG, as in WordNet.
For example, two equally valid hypernyms for backpropagation are gradient descent search
682
Velardi, Faralli, and Navigli OntoLearn Reloaded
procedure and training algorithm, so two hypernym edges should correctly be incident to
the backpropagation node.
We start from our backbone taxonomy T0 obtained after the pruning recovery
phase described in Section 3.4.4. In order to obtain a DAG-like taxonomy we apply
the following step: for each ?noisy? edge (v, v?) ? Enoisy such that v, v? are nodes in T0
but the edge (v, v?) does not belong to the tree, we add (v, v?) to T0 if:
i) it does not create a cycle in T0;
ii) the absolute difference between the length of the shortest path from v to
the root rT0 and that of the shortest path from v
? to rT0 is within an interval
[m, M]. The aim of this constraint is to maintain a balance between the
height of a concept in the tree-like taxonomy and that of the hypernym
considered for addition. In other words, we want to avoid the connection
of an overly abstract concept with an overly specific one.
In Section 4, we experiment with three versions of our OntoLearn Reloaded algo-
rithm, namely: one version that does not perform edge recovery (i.e., which learns a
tree-like taxonomy [TREE], and two versions that apply edge recovery (i.e., which learn
a DAG) with different intervals for constraint (ii) above (DAG[1, 3] and DAG[0, 99]; note
that the latter version virtually removes constraint (ii)). Examples of recovered edges
will be presented and discussed in the evaluation section.
3.6 Complexity
We now perform a complexity analysis of the main steps of OntoLearn Reloaded. Given
the large number of steps and variables involved we provide a separate discussion of
the main costs for each individual step, and we omit details about commonly used data
structures for access and storage, unless otherwise specified. Let Gnoisy = (Vnoisy, Enoisy)
be our noisy graph, and let n = |Vnoisy| and m = |Enoisy|.
1. Terminology extraction: Assuming a part-of-speech tagged corpus as
input, the cost of extracting candidate terms by scanning the corpus with a
maximum-size window is in the order of the word size of the input
corpus. Thus, the application of statistical measures to our set of candidate
terms has a computational cost that is on the order of the square of the
number of term candidates (i.e., the cost of calculating statistics for each
pair of terms).
2. Definition and hypernym extraction: In the second step, we first retrieve
candidate definitions from the input corpus, which costs on the order of
the corpus size.14 Each application of a WCL classifier to an input
candidate sentence s containing a term t costs on the order of the word
length of the sentence, and we have a constant number of such classifiers.
So the cost of this step is given by the sum of the lengths of the candidate
sentences in the corpus, which is lower than the word size of the corpus.
14 Note that this corpus consists of both free text and Web glossaries (cf. Section 3.2).
683
Computational Linguistics Volume 39, Number 3
3. Domain filtering and creation of the graph: The cost of domain filtering
for a single definition is in the order of its word length, so the running time
of domain filtering is in the order of the sum of the word size of the
acquired definitions. As for the hypernym graph creation, using an
adjacency-list representation of the graph Gnoisy, the dynamic addition of a
newly acquired hypernymy edge costs O(n), an operation which has to be
repeated for each (hypernymy, term) pair.
4. Graph pruning, consisting of the following steps:
 Graph trimming: This step requires O(n) time in order to identify
false leaves and false roots by iterating over the entire set of nodes.
 Edge weighting: i) We perform a DFS (O(n + m)) to weight all the
nodes in the graph; ii) we collect all paths from upper roots to any
given node, totalizing O(n!) paths in the worst case (i.e., in a
complete graph). In real domains, however, the computational cost
of this step will be much lower. In fact, over our six domains, the
average number of paths per node ranges from 4.3 (n = 2107,
ANIMALS) to 3175.1 (n = 2616, FINANCE domain): In the latter,
worst case, in practice, the number of paths is in the order of n, thus
the cost of this step, performed for each node, can be estimated by
O(n2) running time; iii) assigning maximum weights to edges costs
O(m) if in the previous step we keep track of the maximum value
of paths ending in each node h (see Equation (6)).
 Optimal branching: Identifying the connected components of our
graph costs O(n + m) time, identifying root candidates and
selecting one root per component costs O(n), and finally applying
the Chu-Liu/Edmonds algorithm costs O(m ? log2n) for sparse
graphs, O(n2) for dense ones, using Tarjan?s implementation
(Tarjan 1977).
5. Pruning recovery: In the worst case, m iterations of Algorithm 1 will be
performed, each costing O(n) time, thus having a total worst-case cost of
O(mn).
6. Edge recovery: For each pair of nodes in T0 we perform i) the
identification of cycles (O(n + m)) and ii) the calculation of the shortest
paths to the root (O(n + m)). By precomputing the shortest path for each
node, the cost of this step is O(n(n + m)) time.
Therefore, in practice, the computational complexity of OntoLearn Reloaded is
polynomial in the main variables of the problem, namely, the number of words in the
corpus and nodes in the noisy graph.
4. Evaluation
Ontology evaluation is a hard task that is difficult even for humans, mainly because
there is no unique way of modeling the domain of interest. Indeed several different
taxonomies might model a particular domain of interest equally well. Despite this
difficulty, various evaluation methods have been proposed in the literature for assessing
684
Velardi, Faralli, and Navigli OntoLearn Reloaded
the quality of a taxonomy. These include Brank, Mladenic, and Grobelnik (2006) and
Maedche, Pekar, and Staab (2002):
a) automatic evaluation against a gold standard;
b) manual evaluation performed by domain experts;
c) structural evaluation of the taxonomy;
d) application-driven evaluation, in which a taxonomy is assessed on the
basis of the improvement its use generates within an application.
Other quality indicators have been analyzed in the literature, such as accuracy,
completeness, consistency (Vo?lker et al 2008), and more theoretical features (Guarino
and Welty 2002) like essentiality, rigidity, and unity. Methods (a) and (b) are by far the
most popular ones. In this section, we will discuss in some detail the pros and cons of
these two approaches.
Gold standard evaluation. The most popular approach for the evaluation of lexicalized
taxonomies (adopted, e.g., in Snow, Jurafsky, and Ng 2006; Yang and Callan 2009;
and Kozareva and Hovy 2010) is to attempt to reconstruct an existing gold standard
(Maedche, Pekar, and Staab 2002), such as WordNet or the Open Directory Project.
This method is applicable when the set of taxonomy concepts are given, and the
evaluation task is restricted to measuring the ability to reproduce hypernymy links
between concept pairs. The evaluation is far more complex when learning a specialized
taxonomy entirely from scratch, that is, when both terms and relations are unknown.
In reference taxonomies, even in the same domain, the granularity and cotopy15 of an
abstract concept might vary according to the scope of the taxonomy and the expertise
of the team who created it (Maedche, Pekar, and Staab 2002). For example, both the
terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in
the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective
and shading techniques whereas collage is classified under image-making processes and
techniques. As long as common-sense, non-specialist knowledge is considered, it is still
feasible for an automated system to replicate an existing classification, because the
Web will provide abundant evidence for it. For example, Kozareva and Hovy (2010,
K&H hereafter) are very successful at reproducing the WordNet sub-taxonomy for
ANIMALS, because dozens of definitional patterns are found on the Web that classify,
for example, lion as a carnivorous feline mammal, or carnivorous, or feline. As we show
later in this section, however, and as also suggested by the previous AA&T example,
finding hypernymy patterns in more specialized domains is far more complex. Even in
simpler domains, however, it is not clear how to evaluate the concepts and relations not
found in the reference taxonomy. Concerning this issue, Zornitsa Kozareva comments
that: ?When we gave sets of terms to annotators and asked them to produce a taxonomy,
people struggled with the domain terminology and produced quite messy organization.
Therefore, we decided to go with WordNet and use it as a gold truth? (personal
communication). Accordingly, K&H do not provide an evaluation of the nodes and
relations other than those for which the ground truth is known. This is further clarified
in a personal communication: ?Currently we do not have a full list of all is-a outside
15 The cotopy of a concept is the set of its hypernyms and hyponyms.
16 http://www.getty.edu/vow/AATHierarchy.
685
Computational Linguistics Volume 39, Number 3
WordNet. [...] In the experiments, we work only with the terms present in WordNet
[...] The evaluation is based only on the WordNet relations. However, the harvesting
algorithm extracts much more. Currently, we do not know how to evaluate the Web
taxonomization.?
To conclude, gold standard evaluation has some evident drawbacks:
 When both concepts and relations are unknown, it is almost impossible to
replicate a reference taxonomy accurately.
 In principle, concepts not in the reference taxonomy can be either wrong
or correct; therefore the evaluation is in any case incomplete.
Another issue in gold standard evaluation is the definition of an adequate evalu-
ation metric. The most common measure used in the literature to compare a learned
with a gold-standard taxonomy is the overlapping factor (Maedche, Pekar, and Staab
2002). Given the set of is-a relations in the two taxonomies, the overlapping factor
simply computes the ratio between the intersection and union of these sets. Therefore
the overlapping factor gives a useful global measure of the similarity between the
two taxonomies. It provides no structural comparison, however: Errors or differences
in grouping concepts in progressively more general classes are not evidenced by this
measure.
Comparison against a gold standard has been analyzed in a more systematic way
by Zavitsanos, Paliouras, and Vouros (2011) and Brank, Mladenic, and Grobelnik (2006).
They propose two different strategies for escaping the ?naming? problem that we have
outlined. Zavitsanos, Paliouras, and Vouros (2011) propose transforming the ontology
concepts and their properties into distributions over the term space of the source data
from which the ontology has been learned. These distributions are used to compute
pairwise concept similarity between gold standard and learned ontologies.
Brank, Mladenic, and Grobelnik (2006) exploit the analogy between ontology learn-
ing and unsupervised clustering, and propose OntoRand, a modified version of the
Rand Index (Rand 1971) for computing the similarity between ontologies. Morey and
Agresti (1984) and Carpineto and Romano (2012), however, demonstrated a high de-
pendency of the Rand Index (and consequently of OntoRand itself) upon the number of
clusters, and Fowlkes and Mallows (1983) show that the Rand Index has the undesirable
property of converging to 1 as the number of clusters increases, even in the unrealistic
case of independent clusterings. These undesired outcomes have also been experienced
by Brank, Mladenic, and Grobelnik (2006, page 5), who note that ?the similarity of an
ontology to the original one is still as high as 0.74 even if only the top three levels of
the ontology have been kept.? Another problem with the OntoRand formula, as also
remarked in Zavitsanos, Paliouras, and Vouros (2011), is the requirement of comparing
ontologies with the same set of instances.
Manual evaluation. Comparison against a gold standard is important because it repre-
sents a sort of objective evaluation of an automated taxonomy learning method. As
we have already remarked, however, learning an existing taxonomy is not particularly
interesting in itself. Taxonomies are mostly needed in novel, often highly technical do-
mains for which there are no gold standards. For a system to claim to be able to acquire
a taxonomy from the ground up, manual evaluation seems indispensable. Nevertheless,
none of the taxonomy learning systems surveyed in Section 2 performs such evaluation.
Furthermore, manual evaluation should not be limited to an assessment of the acquired
686
Velardi, Faralli, and Navigli OntoLearn Reloaded
hypernymy relations ?in isolation,? but must also provide a structural assessment
aimed at identifying common phenomena and the overall quality of the taxonomic
structure. Unfortunately, as already pointed out, manual evaluation is a hard task.
Deciding whether or not a concept belongs to a given domain is more or less feasible
for a domain expert, but assessing the quality of a hypernymy link is far more complex.
On the other hand, asking a team of experts to blindly reconstruct a hierarchy, given a
set of terms, may result in the ?messy organization? reported by Zornitsa Kozareva. In
contrast to previous approaches to taxonomy induction, OntoLearn Reloaded provides
a natural solution to this problem, because is-a links in the taxonomy are supported by
one or more definition sentences from which the hypernymy relation was extracted. As
shown later in this section, definitions proved to be a very helpful feature in supporting
manual analysis, both for hypernym evaluation and structural assessment.
The rest of this section is organized as follows. We first describe the experimen-
tal set-up (Section 4.1): OntoLearn Reloaded is applied to the task of acquiring six
taxonomies, four of which attempt to replicate already existing gold standard sub-
hierarchies in WordNet17 and in the MeSH medical ontology,18 and the other two are
new taxonomies acquired from scratch. Next, we present a large-scale multi-faceted
evaluation of OntoLearn Reloaded focused on three of the previously described eval-
uation methods, namely: comparison against a gold standard, manual evaluation, and
structural evaluation. In Section 4.2 we introduce a novel measure for comparing an
induced taxonomy against a gold standard one. Finally, Section 4.3 is dedicated to a
manual evaluation of the six taxonomies.
4.1 Experimental Set-up
We now provide details on the set-up of our experiments.
4.1.1 Domains. We applied OntoLearn Reloaded to the task of acquiring six taxonomies:
ANIMALS, VEHICLES, PLANTS, VIRUSES, ARTIFICIAL INTELLIGENCE, and FINANCE.
The first four taxonomies were used for comparison against three WordNet sub-
hierarchies and the viruses sub-hierarchy of MeSH. The ANIMALS, VEHICLES, and
PLANTS domains were selected to allow for comparison with K&H, who experimented
on the same domains. The ARTIFICIAL INTELLIGENCE and FINANCE domains are ex-
amples of taxonomies truly built from the ground up, for which we provide a thorough
manual evaluation. These domains were selected because they are large, interdisci-
plinary, and continuously evolving fields, thus representing complex and specialized
use cases.
4.1.2 Definition Harvesting. For each domain, definitions were sought in Wikipedia and
in Web glossaries automatically obtained by means of a Web glossary extraction system
(Velardi, Navigli, and D?Amadio 2008). For the ARTIFICIAL INTELLIGENCE domain we
also used a collection consisting of the entire IJCAI proceedings from 1969 to 2011 and
the ACL archive from 1979 to 2010. In what follows we refer to this collection as the ?AI
corpus.? For FINANCE we used a combined corpus from the freely available collection
of Journal of Financial Economics from 1995 to 2012 and from Review Of Finance from 1997
to 2012 for a total of 1,575 papers.
17 http://wordnet.princeton.edu.
18 http://www.nlm.nih.gov/mesh/.
687
Computational Linguistics Volume 39, Number 3
4.1.3 Terminology. For the ANIMALS, VEHICLES, PLANTS, and VIRUSES domains, the
initial terminology was a fragment of the nodes of the reference taxonomies,19 sim-
ilarly to, and to provide a fair comparison with, K&H. For the AI domain instead,
the initial terminology was selected using our TermExtractor tool20 on the AI corpus.
TermExtractor extracted over 5,000 terms from the AI corpus, ranked according to a
combination of relevance indicators related to the (direct) document frequency, domain
pertinence, lexical cohesion, and other indicators (Sclano and Velardi 2007). We manu-
ally selected 2,218 terms from the initial set, with the aim of eliminating compounds
like order of magnitude, empirical study, international journal, that are frequent but not
domain relevant. For similar reasons a manual selection of terms was also applied to the
terminology automatically extracted for the FINANCE domain, obtaining 2,348 terms21
from those extracted by TermExtractor. An excerpt of extracted terms was provided in
Table 1.
4.1.4 Upper Terms. Concerning the selection of upper terms U (cf. Section 3.2), again
similarly to K&H, we used just one concept for each of the four domains focused
upon: ANIMALS, VEHICLES, PLANTS, and VIRUSES. For the AI and FINANCE domains,
which are more general and complex, we selected from WordNet a core taxonomy of
32 upper concepts U (resulting in 52 terms) that we used as a stopping criterion for
our iterative definition/hypernym extraction and filtering procedure (cf. Section 3.2).
The complete list of upper concepts was given in Table 2. WordNet upper concepts are
general enough to fit most domains, and in fact we used the same set U for AI and
FINANCE. Nothing, however, would have prevented us from using a domain-specific
core ontology, such as the CRM-CIDOC core ontology for the domain of ART AND
ARCHITECTURE.22
4.1.5 Algorithm Versions and Structural Statistics. For each of the six domains we ran the
three versions of our algorithm: without pruning recovery (TREE), with [1, 3] recovery
(DAG[1, 3]), and with [0, 99] recovery (DAG[0, 99]), for a total of 18 experiments. We
remind the reader that the purpose of the recovery process was to reattach some of the
edges deleted during the optimal branching step (cf. Section 3.5).
Figure 6 shows an excerpt of the AI tree-like taxonomy under the node data structure.
Notice that, even though the taxonomy looks good overall, there are still a few errors,
such as ?neuron is a neural network? and overspecializations like ?network is a digraph.?
Figure 7 shows a sub-hierarchy of the FINANCE tree-like taxonomy under the concept
value.
In Table 6 we give the structural details of the 18 taxonomies extracted for our six
domains. In the table, edge and node compression refers to the number of surviving
nodes and edges after the application of optimal branching and recovery steps to the
noisy hypernymy graph. To clarify the table, consider the case of VIRUSES, DAG[1, 3]:
we started with 281 initial terms, obtaining a noisy graph with 1,174 nodes and 1,859
edges. These were reduced to 297 nodes (i.e., 1,174?877) and 339 edges (i.e., 1,859?1,520)
after pruning and recovery. Out of the 297 surviving nodes, 222 belonged to the initial
19 For ANIMALS, VEHICLES, and PLANTS we used precisely the same seeds as K&H.
20 http://lcl.uniroma1.it/termextractor.
21 These dimensions are quite reasonable for large technical domains: as an example, The Economist?s
glossary of economic terms includes on the order of 500 terms (http://www.economist.com/
economics-a-to-z/).
22 http://cidoc.mediahost.org/standard crm(en)(E1).xml.
688
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 6
An excerpt of the ARTIFICIAL INTELLIGENCE taxonomy.
terminology; therefore the coverage over the initial terms is 0.79 (222/281). This means
that, for some of the initial terms, either no definitions were found, or the definition
was rejected in some of the processing steps. The table also shows, as expected, that the
term coverage is much higher for ?common-sense? domains like ANIMALS, VEHICLES,
and PLANTS, is still over 0.75 for VIRUSES and AI, and is a bit lower for FINANCE
(0.65). The maximum and average depth of the taxonomies appears to be quite variable,
with VIRUSES and FINANCE at the two extremes. Finally, Table 6 reports in the last
column the number of glosses (i.e., domain definitional sentences) obtained in each
run. We would like to point out that providing textual glosses for the retrieved domain
hypernyms is a novel feature that has been lacking in all previous approaches to
ontology learning, and which can also provide key support to much-needed manual
validation and enrichment of existing semantic networks (Navigli and Ponzetto 2012).
4.2 Evaluation Against a Gold Standard
In this section we propose a novel, general measure for the evaluation of a learned
taxonomy against a gold standard. We borrow the Brank, Mladenic, and Grobelnik
689
Computational Linguistics Volume 39, Number 3
Figure 7
An excerpt of the FINANCE taxonomy.
(2006) idea of exploiting the analogy with unsupervised clustering but, rather than
representing the two taxonomies as flat clusterings, we propose a measure that takes
into account the hierarchical structure of the two analyzed taxonomies. Under this
perspective, a taxonomy can be transformed into a hierarchical clustering by replacing
each label of a non-leaf node (e.g., perspective and shading techniques) with the transitive
closure of its hyponyms (e.g., cangiatismo, chiaroscuro, foreshortening, hatching).
4.2.1 Evaluation Model. Techniques for comparing clustering results have been surveyed
in Wagner and Wagner (2007), although the only method for comparing hierarchical
clusters, to the best of our knowledge, is that proposed by Fowlkes and Mallows (1983).
Suppose that we have two hierarchical clusterings H1 and H2, with an identical set of n
objects. Let k be the maximum depth of both H1 and H2, and Hij a cut of the hierarchy,
where i ? {0, . . . , k} is the cut level and j ? {1, 2} selects the clustering of interest. Then,
for each cut i, the two hierarchies can be seen as two flat clusterings Ci1 and C
i
2 of the n
concepts. When i = 0 the cut is a single cluster incorporating all the objects, and when
i = k we obtain n singleton clusters. Now let:
 n11 be the number of object pairs that are in the same cluster in both Ci1
and Ci2;
 n00 be the number of object pairs that are in different clusters in both Ci1
and Ci2;
 n10 be the number of object pairs that are in the same cluster in Ci1 but
not in Ci2;
690
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 6
Structural evaluation of three versions of our taxonomy-learning algorithm on six different
domains.
Experiment Term Coverage Depth |V| |E| V Compress. E Compress. Glosses
A
I
TREE 75.51% 12 max 2,387 2,386 43.00% 67.31% 1,249(1,675/2,218) 6.00 avg (1,801/4,188) (4,915/7,301)
DAG [1,3] 75.51% 19 max 2,387 3,554 43.00% 51.32% 2,081(1,675/2,218) 8.27 avg (1,801/4,188) (3,747/7,301)
DAG [0,99] 75.51% 20 max 2,387 3,994 43.00% 45.29% 2,439(1,675/2,218) 8.74 avg (1,801/4,188) (3,307/7,301)
FI
N
A
N
C
E
TREE 65.20% 14 max 2,038 2,037 22.09% 47.99% 1,064(1,533/2,348) 6.83 avg (578/2,616) (1,880/3,917)
DAG [1,3] 65.20% 38 max 2,038 2,524 22.09% 35.56% 1,523(1,533/2,348) 18.82 avg (578/2,616) (1,393/3,917)
DAG [0,99] 65.20% 65 max 2,038 2,690 22.09% 31.32% 1,677(1,533/2,348) 33.54 avg (578/2,616) (1,227/3,917)
V
IR
U
SE
S
TREE 79.00% 5 max 297 296 74.70% 84.07% 172(222/281) 2.13 avg (877/1,174) (1,563/1,859)
DAG [1,3] 79.00% 5 max 297 339 74.70% 81.76% 212(222/281) 2.20 avg (877/1,174) (1,520/1,859)
DAG [0,99] 79.00% 5 max 297 360 74.70% 80.63% 233(222/281) 2.32 avg (877/1,174) (1,563/1,859)
A
N
IM
A
L
S
TREE 93.56% 10 max 900 899 57.28% 66.96% 724(640/684) 4.35 avg (1,207/2,107) (1,822/2,721)
DAG [1,3] 93.56% 16 max 900 1,049 57.28% 61.44% 872(640/684) 5.21 avg (1,207/2,107) (1,672/2,721)
DAG [0,99] 93.56% 16 max 900 1,116 57.28% 58.98% 939(640/684) 5.39 avg (1,207/2,107) (1,605/2,721)
P
L
A
N
T
S
TREE 96.57% 19 max 710 709 72.69% 84.53% 638(535/554) 5.85 avg (1,890/2,600) (3,877/4,586)
DAG [1,3] 96.57% 19 max 710 922 72.69% 79.89% 851(535/554) 6.65 avg (1,890/2,600) (3,664/4,586)
DAG [0,99] 96.57% 19 max 710 1,242 72.69% 72.91% 1,171(535/554) 6.54 avg (1,890/2,600) (3,344/4,586)
V
E
H
IC
L
E
S
TREE 95.72% 8 max 169 168 71.50% 80.48% 150(112/117) 3.44 avg (424/593) (693/861)
DAG [1,3] 95.72% 8 max 169 200 71.50% 76.77% 182(112/117) 3.94 avg (424/593) (661/861)
DAG [0,99] 95.72% 10 max 169 231 71.50% 73.17% 213(112/117) 4.48 avg (424/593) (630/861)
 n01 be the number of object pairs that are in the same cluster in Ci2 but not
in Ci1;
The generalized Fowlkes and Mallows (F&M) measure of cluster similarity for the
cut i (i ? {0, . . . , k}), as reformulated by Wagner and Wagner (2007), is defined as:
Bi1,2 =
ni11
?
(ni11 + n
i
10) ? (ni11 + ni01)
. (8)
Note that the formula can be interpreted as the geometric mean of precision and
recall of an automated method in clustering the same concept pairs as in a gold-standard
691
Computational Linguistics Volume 39, Number 3
clustering. This formula has a few undesirable properties: first, the value of Bi1,2 gets
close to its maximum 1.0 as we approach the root of the hierarchy (i = 0); second, the
two hierarchies need to have the same maximum depth k; third, the hierarchies need to
have the same number of initial objects and a crisp classification.
In order to apply the F&M measure to the task of comparing a learned and a gold-
standard taxonomy, we need to mitigate these problems. Equation (8) copes with the
third problem without modifications. In fact, if the sets of objects in H1 and H2 are
different, the integers n10 and n01 can be considered as also including objects that belong
to one hierarchy and not to the other. In this case, the value of B01,2 will provide a measure
of the overlapping objects in the learned taxonomy and the gold standard one. In order
to take into account multiple (rather than crisp) classifications, again, there is no need
to change the formula, which is still meaningful if an object is allowed to belong to
more than one cluster. As before, mismatches between H1 and H2 would result in higher
values of n10 and n01 and lower Bi1,2.
A more serious problem with Equation (8) is that the lower the value of i, the higher
the value of the formula, whereas, ideally, we would like to reward similar clusterings
when the clustering task is more difficult and fine-grained, that is, for cuts that are close
to the leaf nodes. To assign a reward to ?early? similarity values, we weight the values
of Bi1,2 with a coefficient
i+1
k . We can then compute a cumulative measure of similarity
with the following formula:
B1,2 =
?k?1
i=0
i+1
k B
i
1,2
?k?1
i=0
i+1
k
=
?k?1
i=0
i+1
k B
i
1,2
k+1
2
. (9)
Finally, to solve the problem of different depths of the two hierarchies, we define a
policy that penalizes a learned taxonomy that is less structured than the gold standard
one, and rewards?or at least does not penalize?the opposite case.
As an example, consider Figure 8, which shows two taxonomies H1 and H2, with
non-identical sets of objects {a, b, c, d, e, f} and {a, b, c, d, e, g}. In the figure each edge is
labeled by its distance from the root node (the value i in the F&M formula). Notice that
H1 and H2 have multiple classifications (i.e., multiple hypernyms in our case) for the
object e, thus modeling the common problem of lexical ambiguity and polysemy. Let
us suppose that H1 is the learned taxonomy, and H2 the gold standard one. We start
comparing the clusterings at cut 0 and stop at cut kr ? 1, where kr is the depth of the
Figure 8
Two hierarchical clusters of n non-identical objects.
692
Velardi, Faralli, and Navigli OntoLearn Reloaded
gold standard taxonomy. This means that if the learned taxonomy is less structured
we replicate the cut kl ? 1 for kr ? kl times (where kl is the maximum depth of the
learned taxonomy), whereas if it is more structured we stop at cut kr ? 1. In contrast to
previous evaluation models, our aim is to reward (instead of penalize) more structured
taxonomies provided they still match the gold standard one.
Table 7 shows the cuts from 0 to 3 of H1 and H2 and the values of Bi1,2. For i = 2 the
B value is 0, if H2 is the learned taxonomy, and is not defined, if H2 is the gold standard.
Therefore, when computing the cumulative Equation (9), we obtain the desired effect of
penalizing less the structured learned taxonomies. Note that, when the two hierarchies
have different depths, the value k ? 1 in Equation (9) is replaced by kr ? 1.
Finally, we briefly compare our evaluation approach with the OntoRand index,
introduced by Brank, Mladenic, and Grobelnik (2006). The Rand Index measures the
similarity between two clusterings Cl and Cr by the formula:
R(Cl, Cr) =
2(n11 + n00)
n(n ? 1) (10)
where n11, n00, and n have the same meaning described earlier. In Brank, Mladenic,
and Grobelnik (2006), a clustering is obtained from an ontology by associating each
ontology instance to its concept. The set of clusters is hence represented by the set of
leaf concepts in the hierarchy, namely, according to our notation, the clustering Ck?1i . In
order to take into account the hierarchical structure, they define the OntoRand formula.
This measure, rather than summing up to 1 or 0, depending on whether or not two
given instances i and j belong to the same cluster in the compared ontologies, returns a
real number in [0, 1] depending upon the distance between i and j in terms of common
ancestors. In other terms, if i and j do not belong to the same concept but have a very
close common ancestor, the OntoRand measure returns a value still close to 1.
Our measure has several advantages over the OntoRand index:
i) It allows for a comparison at different levels of depth of the hierarchy,
and the cumulative similarity measure penalizes the contribution of the
highest cuts of the hierarchy.
ii) It does not require that the two hierarchies have the same depth, nor that
they have the same number of leaf nodes.
iii) The measure can be extended to lattices (e.g., it is not required that each
object belongs precisely to one cluster).
Table 7
Application of the evaluation method to the hierarchies of Figure 8. The values of Bi1,2 are shown
both when H1 and H2 are the learned taxonomy (penultimate and last column, respectively).
i C1 C2 n11 n10 n01 H1 H2
Bi1,2
0 {a,b,c,d,e,f} {a,b,c,d,e,g} 10 5 5 0.67 0.67
1 {a,b,c,d,e},{e,f} {a,b,c,d,e},{e},{g} 10 1 0 0.95 0.95
2 {a,b},{c,d},{e},{f} {a},{b},{c},{d},{e},{g} 0 2 0 n.a. 0
3 {a},{b},{c},{d},{e},{f} {a},{b},{c},{d},{e},{g} 0 0 0 n.a. n.a.
693
Computational Linguistics Volume 39, Number 3
iv) It is not dependent, as the Rand Index is, on the number n00, the value of
which has the undesirable effect of producing an ever higher similarity as
the number of singleton clusters grows (Morey and Agresti 1984).
4.2.2 Results. This section presents the results of the F&M evaluation model for gold
standard evaluation, therefore we focus on four domains and do not consider AI and
FINANCE. The three WordNet sub-hierarchies are also compared with the taxonomies
automatically created by Kozareva and Hovy (2010) in the same domains, kindly made
available by the authors. It is once more to be noted that Kozareva and Hovy, during hy-
pernym extraction, reject all the nodes not belonging to WordNet, whereas we assume
no a-priori knowledge of the domain, apart from adopting the same set of seed terms
used by K&H.
Figure 9 shows, for each domain (ANIMALS, PLANTS, VEHICLES, and VIRUSES), and
for each cut level of the hierarchy, a plot of Bi1,2 values multiplied by the penalty factor.
As far as the comparison with K&H is concerned, we notice that, though K&H obtain
better performance in general, OntoLearn has higher coverage over the domain, as is
shown by the highest values for i = 0, and has a higher depth of the derived hierarchy,
especially with DAG[0, 99]. Another recurrent phenomenon is that K&H curves grace-
fully degrade from the root to the leaf nodes, possibly with a peak in the intermediate
levels, whereas OntoLearn has a hollow in the mid-high region (see the region 4?6 for
ANIMALS and 1?2 for the other three hierarchies) and often a relative peak in the lowest
Figure 9
Gold standard evaluation of our three versions of OntoLearn Reloaded against WordNet
(ANIMALS, PLANTS, and VEHICLES) and MeSH (VIRUSES). A comparison with K&H is also
shown for the first three domains.
694
Velardi, Faralli, and Navigli OntoLearn Reloaded
levels. In the manual evaluation section we explain this phenomenon, which also occurs
in the ARTIFICIAL INTELLIGENCE taxonomy.
The generally decreasing values of Bi1,2 in Figure 9 show that, as expected, mim-
icking the clustering criteria of a taxonomy created by a team of experts proves very
difficult at the lowest levels, while performance grows at the highest levels. At the
lowest taxonomy levels there are two opposing phenomena: overgeneralization and
overspecialization. For example, macaque has monkey as a direct hypernym in WordNet,
and we find short-tailed monkey as a direct hypernym of macaque. An opposite case is
ganoid, which is a taleostan in WordNet and simply a fish in our taxonomy. The first
case does not reward the learned taxonomy (though, unlike for the overlapping factor
[Maedche, Pekar, and Staab 2002], it does not cause a penalty), whereas the second is
quite penalizing. More of these examples will be provided in Section 4.3.
Finally, in Table 8 we show the cumulative B1,2 values for the four domains, ac-
cording to Equation (9). Here, except for the VEHICLES domain, the unconstrained
DAG[0, 99] performs best.
4.3 Manual Evaluation
This section is dedicated to the manual assessment of the learned ontologies. The
section is divided in three parts: Section 4.3.1 is concerned with the human validation of
hypernymy relations, Section 4.3.2 examines the global learned taxonomic structure in
the search for common phenomena across the six domains, and finally Section 4.3.3 in-
vestigates the possibility of enhancing our hypernymy harvesting method with K&H?s
Hearst-like patterns, applying their method to the AI domain and manually evaluating
the extracted hypernyms.
4.3.1 Hypernym Evaluation. To reduce subjectivity in taxonomy evaluation, we asked
three annotators, only one of whom was a co-author, to validate, for each of the three
experiments of each of the six domains, a random sample of hypernymy relations. For
each relation the definition(s) supporting the relation were also provided. This was
especially helpful for domains like VIRUSES, but also PLANTS and ANIMALS, in which
the annotators were not expert. The size of each random sample was 300 for the (larger)
AI and FINANCE domains and 100 for the others.
Each evaluator was asked to tag incorrect relations, regardless of whether the error
was due to the selection of non-domain definitions (e.g., for VEHICLES: ?a driver is a
golf club with a near vertical face that is used for hitting long shots from the tee?), to
a poor definition (e.g., for AI: ?a principle is a fundamental essence, particularly one
producing a given quality?) or to a wrong selection of the hypernym. As an example of
the latter, in the PLANTS domain, we extracted the hypernym species from the sentence:
?geranium is a genus of 422 species of flowering annual, biennial, and perennial plants
Table 8
Values of B1,2 for the domains of VIRUSES, ANIMALS, PLANTS, and VEHICLES.
Experiment VIRUSES ANIMALS PLANTS VEHICLES
TREE 0.093 0.064 0.059 0.065
DAG [1,3] 0.101 0.062 0.072 0.069
DAG [0,99] 0.115 0.097 0.080 0.103
K&H n.a. 0.067 0.068 0.158
695
Computational Linguistics Volume 39, Number 3
Table 9
Precision of hypernym edges on six domains (calculated on a majority basis) and inter-annotator
agreement on the corresponding sample of relations.
Experiment # of Sample Precision ?
AI
TREE 300 80.3% [241/300] 0.45
DAG [1,3] 300 73.6% [221/300] 0.42
DAG [0,99] 300 73.0% [219/300] 0.41
FINANCE
TREE 300 93.6% [281/300] 0.40
DAG [1,3] 300 93.0% [279/300] 0.43
DAG [0,99] 300 92.6% [278/300] 0.41
VIRUSES
TREE 100 99.0% [99/100] 0.49
DAG [1,3] 100 99.0% [99/100] 0.39
DAG [0,99] 100 99.0% [99/100] 0.32
ANIMALS
TREE 100 92.0% [92/100] 0.53
DAG [1,3] 100 92.0% [92/100] 0.36
DAG [0,99] 100 90.0% [90/100] 0.56
PLANTS
TREE 100 89.0% [89/100] 0.49
DAG [1,3] 100 85.0% [85/100] 0.53
DAG [0,99] 100 97.0% [97/100] 0.26
VEHICLES
TREE 100 92.0% [92/100] 0.64
DAG [1,3] 100 92.0% [92/100] 0.49
DAG [0,99] 100 91.0% [91/100] 0.44
? Interpretation
< 0 Poor agreement
0.01?0.20 Slight agreement
0.21?0.40 Fair agreement
0.41?0.60 Moderate agreement
0.61?0.80 Substantial agreement
0.81?1.00 Almost perfect agreement
that are commonly known as the cranesbills? since, in the WCL verb set, we have ?is
a * species of? and ?is a * genus of?, but not the concatenation of these two patterns.
Annotators could mark with ? a hyponym?hypernym pair for which they felt uncertain.
Though it would have been useful to distinguish between the different types of error,
we found that regarding many error types there was, anyway, very low inter-annotator
agreement. Indeed the annotation task would appear to be intrinsically complex and
controversial. In any case, an assessment of the definition and hypernym extraction
tasks in isolation was already provided by Navigli and Velardi (2010).
Table 9 summarizes the results. Precision of each classification was computed on a
majority basis, and we used Fleiss? kappa statistics (Fleiss 1971) to measure the inter-
annotator agreement. In general, the precision is rather good, though it is lower for the
AI domain, probably due to its high ?vitality? (many new terms continuously arise, and
for some of them it is difficult to find good quality definitions). In general, precision is
higher in focused domains (VIRUSES, ANIMALS, PLANTS, and VEHICLES) than in wide-
range domains (AI and FINANCE). The former domains, however, have just one quite
696
Velardi, Faralli, and Navigli OntoLearn Reloaded
?narrow? upper concept (virus for VIRUSES, etc.), whereas AI and FINANCE have several
upper concepts (e.g., person or abstraction), and furthermore they are less focused. This
means that there is an inherently higher ambiguity and this may be seen as justifying
the lower performance. In Table 9 we also note that TREE structures achieve in general a
higher precision, except for PLANTS, whereas the DAG has the advantage of improving
recall (see also Section 4.2.2).
Note that high precision here does not contradict the results shown in Section 4.2.2:
In this case, each single relation is evaluated in isolation, therefore overgenerality or
overspecificity do not imply a penalty, provided the relation is judged to be correct.
Furthermore, global consistency is not considered here: for example, distance metric
learning ? parametric technique, and eventually ends up in technique, whereas belief
network learning ? machine learning algorithm ends up in algorithm and then in procedure.
In isolation, these hypernymy patterns are acceptable, but within a taxonomic structure
one would like to see a category node grouping all terms denoting machine learning
algorithms. This behavior should be favored by the node weighting strategy described
in Section 3.4, aimed at attracting nodes with multiple hypernyms towards the most
populated category nodes. As in the previous example, however, there are category
nodes that are almost equally ?attractive? (e.g., algorithm and technique), and, further-
more, the taxonomy induction algorithm can only select among the set of hypernyms
extracted during the harvesting phase. Consequently, when no definition suggests that
distance metric learning is a hyponym of machine learning algorithm, or of any other
concept connected to machine learning algorithm, there is no way of grouping distance
metric learning and belief network learning in the desired way. This task must be postponed
to manual post-editing.
Concerning the kappa statistics, we note that the values range from moderate to
substantial in most cases. These numbers are apparently low, but the task of evaluating
hypernymy relations is quite a complex one. Similar kappa values were obtained in
Yang and Callan (2008) in a human-guided ontology learning task.
4.3.2 Structural Assessment. In addition to the manual evaluation summarized in
Table 9, a structural assessment was performed to identify the main sources of error.
To this end, one of the authors analyzed the full AI and FINANCE taxonomies and a
sample of the other four domains in search of recurring errors. In general, our optimal
branching algorithm and weighting schema avoids many of the problems highlighted in
well-known studies on taxonomy acquisition from dictionaries (Ide and Ve?ronis 1993),
like circularity, over-generalization, and so forth. There are new problems to be faced,
however.
The main sources of error are the following:
 Ambiguity of terms, especially at the intermediate levels
 Low quality of definitions
 Hypernyms described by a clause rather than by a single- or multi-word
expression
 Lack of an appropriate WCL to analyze the definition
 Difficulty of extracting the correct hypernym string from phrases with
identical syntactic structure
We now provide examples for each of these cases.
697
Computational Linguistics Volume 39, Number 3
Figure 10
Error distribution of the TREE version of our algorithm on the ARTIFICIAL INTELLIGENCE
domain.
Ambiguity. Concerning ambiguity of terms, consider Figures 10 and 11, which show the
distribution of errors at the different levels of the learned AI and FINANCE taxonomies
for the TREE experiment. The figures provide strong evidence that most errors are
located in the intermediate levels of the taxonomy. As we move from leaf nodes to
the upper ontology, the extracted terms become progressively more general and con-
sequently more ambiguous. For these terms the domain heuristics may turn out to be
inadequate, especially if the definition is a short sentence.
But why are these errors frequent at the intermediate levels and not at the highest
levels? To understand this, consider the following example from the AI domain: For
the term classifier the wrong hypernym is selected from the sentence ?classifier is a
person who creates classifications.? In many cases, wrong hypernyms do not accumulate
sufficient weight and create ?dead-end? hypernymy chains, which are pruned during
the optimal branching step. But, unfortunately, a domain appropriate definition is
Figure 11
Error distribution of the TREE version of our algorithm on the FINANCE domain.
698
Velardi, Faralli, and Navigli OntoLearn Reloaded
found for person: ?person is the more general category of an individual,? due to the
presence of the domain word category. On the other hand, this new sentence produces
an attachment that, in a sense, recovers the error, because category is a ?good? domain
concept that eventually ends up in subsequent iterations to the upper node abstraction.
Therefore, what happens is that the upper taxonomy nodes, with the help of the domain
heuristic, mitigate the ?semantic drift? caused by out-of-domain ambiguity, recovering
the ambiguity errors of the intermediate levels. This phenomenon is consistently found
in all domains, as shown by the hollow that we noticed in the graphs of Section 4.2.2.
An example in the ANIMALS domain is represented by the hypernymy sequence
fawn ? color ? race ? breed ? domestic animal, where the wrong hypernym color was
originated by the sentence ?fawn is a light yellowish brown color that is usually used in
reference to a dog?s coat color.? Only in VIRUSES is the phenomenon mitigated by the
highly specific and very focused nature of the domain.
In addition to out-of-domain ambiguity, we have two other phenomena: in-domain
ambiguity and polysemy. In-domain ambiguity is rare, but not absent (Agirre et al
2010; Faralli and Navigli 2012). Consider the example of Figure 12a, from the VEHICLES
domain: tractor has two definitions corresponding to two meanings, which are both
correct. The airplane meaning is ?tractor is an airplane where the propeller is located in
front of the fuselage,? whereas the truck meaning is ?tractor is a truck for pulling a semi-
trailer or trailer.? Here the three hyponyms of tractor (see the figure) all belong to the
truck sense. We leave to future developments the task of splitting in-domain ambiguous
nodes in the appropriate way.
Another case is systematic polysemy, which is shown in Figure 13. The graph in
the figure, from the AI domain, captures the fact that a semantic network, as well as
its hyponyms, are both a methodology and a representation. Another example is shown
in Figure 12b for the PLANTS domain, where systematic polysemy can be observed
for terms like olive, orange, and breadfruit, which are classified as evergreen tree and
fruit. Polysemy, however, does not cause errors, as it does for in-domain ambiguity,
because hyponyms of polysemous concepts inherit the polysemy: In the two graphs
of Figures 13 and 12b, both partitioned semantic network and tangerine preserve the
polysemy of their ancestors. Note that in-domain ambiguity and polysemy are only
captured by the DAG structure; therefore this can be seen as a further advantage (in
addition to higher recall) of the DAG model over and against the more precise TREE
structure.
Figure 12
An example of in-domain ambiguity (a) and an example of systematic polysemy (b). Dashed
edges were added to the graph as a result of the edge recovery phase (see Section 3.5).
699
Computational Linguistics Volume 39, Number 3
Figure 13
An example of systematic polysemy. Dashed edges were added to the graph as a result of the
edge recovery phase (see Section 3.5).
Low quality of definitions. Often textual definitions, especially if extracted from the
Web, do not have a high quality. Examples are: ?artificial intelligence is the next big
development in computing? or ?aspectual classification is also a necessary prerequi-
site for interpreting certain adverbial adjuncts.? These sentences are definitions on a
syntactic ground, but not on a semantic ground. As will be shown in Section 4.3.3,
this problem is much less pervasive than for Hearst-like lexico-syntactic patterns,
although, neither domain heuristics nor the graph pruning could completely eliminate
the problem. We can also include overgeneralization in this category of problems: Our
algorithm prefers specific hypernyms to general hypernyms, but for certain terms no
specific definitions are found. The elective way to solve this problem would be to assign
a quality confidence score to the definition source (document or Web page), for example,
by performing an accurate and stable classification of its genre (Petrenz and Webber
2011).
Hypernym is a clause. There are cases in which, although very descriptive and good
quality definitions are found, it is not possible to summarize the hypernym with a
term or multi-word expression. For example ?anaphora resolution is the process of
determining whether two expressions in natural language refer to the same real world
entity.? OntoLearn extracts process of determining which ends up in procedure, process.
This is not completely wrong, however, and in some case is even fully acceptable, as
for ?summarizing is a process of condensing or expressing in short something you
have read, watched or heard?: here, process of condensing is an acceptable hypernym.
An example for FINANCE is: ?market-to-book ratio is book value of assets minus book
value of equity plus market value of equity,? where we extracted book value, rather than
the complete formula. Another example is: ?roa is defined as a ratio of operating income
to book value of assets,? from which we extracted ratio, which is, instead, acceptable.
Lack of an appropriate definitional pattern. Though we acquired hundreds of different
definitional patterns, there are still definitions that are not correctly parsed. We already
700
Velardi, Faralli, and Navigli OntoLearn Reloaded
mentioned the geranium example in the PLANTS domain. An example in the AI domain
is ?execution monitoring is the robot?s process of observing the world for discrepancies
between the actual world and its internal representation of it,? where the extracted
hypernym is robot because we have no WCL with a Saxon genitive.
Wrong hypernym string. This is the case in which the hypernym is a superstring or
substring of the correct one, like: ?latent semantic analysis is a machine learning proce-
dure.? Here, the correct hypernym is machine learning procedure, but OntoLearn extracts
machine because learning is POS tagged as a verb. In general, it is not possible to evaluate
the extent of the hypernym phrase except case-by-case. The lattice learner acquired a
variety of hypernymy patterns, but the precision of certain patterns might be quite low.
For example, the hypernymy pattern ?* of *? is acceptable for ?In grammar, a lexical
category is a linguistic category of words? or ?page rank is a measure of site popularity?
but not for ?page rank is only a factor of the amount of incoming and outgoing links
to your site? nor for ?pattern recognition is an artificial intelligence area of considerable
importance.? The same applies to the hypernymy pattern ADJ NN: important algorithm is
wrong, although greedy algorithm is correct.
4.3.3 Evaluation of Lexico-Syntactic Patterns. As previously remarked, Kozareva and Hovy
(2010) do not actually apply their algorithm to the task of creating a new taxonomy, but
rather they try to reproduce three WordNet taxonomies, under the assumption that the
taxonomy nodes are known (cf. Section 4). Therefore, there is no evidence of the preci-
sion of their method on new domains, where the category nodes are unknown. On the
other hand, if Hearst?s patterns, which are at the basis of K&H?s hypernymy harvesting
algorithm, could show adequate precision, we would use them in combination with
our definitional patterns. This section investigates the matter.
As briefly summarized in Section 2, K&H create a hypernym graph in three steps.
Given a few root concepts (e.g., animal) and basic level concepts or instances (e.g.,
lion), they:
1) harvest new basic and intermediate concepts from the Web in an iterative
fashion, using doubly anchored patterns (DAP) like ??root? such as ?seed?
and ?? and inverse DAP (i.e., DAP?1) like ?? such as ?term1? and ?term2??.
The procedure is iterated until no new terms can be found;
2) rank the nodes extracted with DAP by out-degree and those extracted
with inverse DAP by in-degree, so as to prune out less promising terms;
3) induce the final taxonomic structure by positioning the intermediate nodes
between basic level and root terms using a concept positioning procedure
based on a variety of Hearst-like surface patterns. Finally, they eliminate
cycles, as well as nodes with no predecessor or no successor, and they
select the longest path in the case of multiple paths between node pairs.
In this section we apply their method23 to the domain of AI in order to manually
analyze the quality of the extracted relations. To replicate the first two steps of K&H
algorithm we fed the algorithm with a growing set of seed terms randomly selected
from our validated terminology, together with their hierarchically related root terms
23 We followed the exact procedure described in Figure 2 of Kozareva & Hovy (2010).
701
Computational Linguistics Volume 39, Number 3
Table 10
K&H performance on the AI domain.
number of root/seed pairs 1 10 100 1,000
# new concepts 131 163 227 247
# extracted is-a relations 114 146 217 237
correct and in-domain 21.05% 24.65% 18.89% 18.56%
(24/114) (36/146) (41/217) (44/237)
in the upper taxonomy (e.g., unsupervised learning is a method or maximum entropy is a
measure). We then performed the DAP and DAP?1 steps iteratively until no more terms
could be retrieved, and we manually evaluated the quality of the harvested concepts
and taxonomic relations using the same thresholding formula described in K&H.24 We
give the results in Table 10.
As we said earlier, our purpose here is mainly to evaluate the quality of Hearst
patterns in more technical domains, and the efficacy of DAP and DAP?1 steps in
retrieving domain concepts and relations. Therefore, replicating step (3) above is not
useful in this case since, rather than adding new nodes, step (3) is aimed, as in our
optimal branching and pruning recovery steps, at reorganizing and trimming the final
graph.
Table 10 should be compared with the first three rows (AI) of Table 9: It shows that
in the absence of a priori knowledge on the domain concepts the quantity and quality
of the is-a links extracted by the K&H algorithm is much lower than those extracted by
OntoLearn Reloaded. First, the number of new nodes found by the K&H algorithm is
quite low: For the same domain of ARTIFICIAL INTELLIGENCE, our method, as shown in
Table 9, is able to extract from scratch 2,387 ? 52 = 2,335 nodes,25 in comparison with the
247 new nodes of Table 10, obtained with 1,000 seeds. Second, many nodes extracted by
the K&H algorithm, like fan speed, guidelines, chemical engineering, and so on, are out-of-
domain and many hypernym relations are incorrect irrespective of their direction, like
computer program is a slow and data mining is a contemporary computing problem. Third, the
vast majority of the retrieved hypernyms are overgeneral, like discipline, method, area,
problem, technique, topic, and so forth, resulting in an almost flat hypernymy structure. A
high in-degree threshold and a very high number of seeds do not mitigate the problem,
demonstrating that Hearst-like patterns are not very good at harvesting many valid
hypernym relations in specialized domains.26
Following this evaluation, we can outline several advantages of our method over
K&H?s work (and, as a consequence, over Hearst?s patterns):
i) We obtain higher precision and recall when no a priori knowledge is
available on the taxonomy concepts, because hypernyms are extracted
from expert knowledge on the Web (i.e., technical definitions rather than
patterns reflecting everyday language).
24 The technique is based on the in-degree and out-degree of the graph nodes.
25 Remember that the 52 domain-independent upper terms are manually defined (cf. Section 4.1.4).
26 This result is in line with previous findings in a larger, domain-balanced experiment (Navigli and Velardi
2010) in which we have shown that WCLs outperform Hearst patterns and other methods in the task of
hypernym extraction.
702
Velardi, Faralli, and Navigli OntoLearn Reloaded
ii) We cope better with sense ambiguity via the domain filtering step.27
iii) We use a principled algorithmic approach to graph pruning and cycle
removal.28
iv) Thanks to the support provided by textual definitions, we are able to
cope with the problem of manually evaluating the retrieved concepts
and relations, even in the absence of a reference taxonomy.
4.3.4 Summary of Findings. We here summarize the main findings of our manifold
evaluation experiments:
i) With regard to the two versions of our graph pruning algorithm, we found
that TREE structures are more precise, whereas DAGs have a higher recall.
ii) Errors are mostly concentrated in the mid-level of the hierarchy, where
concepts are more ambiguous and the ?attractive? power of top nodes is
less influential. This was highlighted by our quantitative (F&M) model
and justified by means of manual analysis.
iii) The quality and number of definitions is critical for high performance.
Less-focused domains in which new terms continuously emerge are the
most complex ones, because it is more difficult to retrieve high-quality
definitions for them.
iv) Definitions, on the other hand, are a much more precise and high-coverage
source of knowledge for hypernym extraction than (Hearst-like) patterns
or contexts, because they explicitly represent expert knowledge on a
given domain. Furthermore, they are a very useful support for manual
validation and structural analysis.
5. Conclusions
In this paper we presented OntoLearn Reloaded, a graph-based algorithm for learning
a taxonomy from scratch using highly dense, potentially disconnected, hypernymy
graphs. The algorithm performs the task of eliminating noise from the initial graph
remarkably well on arbitrary, possibly specialized, domains, using a weighting scheme
that draws both on the topological properties of the graph and on some general prin-
ciples of taxonomic structures. OntoLearn Reloaded provides a considerable advance-
ment over the state of the art in taxonomy learning. First, it is the first algorithm that
experimentally demonstrates its ability to build a new taxonomy from the ground up,
without any a priori assumption on the domain except for a corpus and a set of (possibly
general) upper terms. The majority of existing systems start from a set of concepts
and induce hypernymy links between these concepts. Instead, we automatically learn
both concepts and relations via term extraction and iterative definition and hypernym
27 In the authors? words (Kozareva and Hovy 2010, page 1,115): ?we found that the learned terms in the
middle ranking do not refer to the meaning of vehicle as a transportation device, but to the meaning of
vehicle as media (i.e., seminar, newspapers), communication and marketing.?
28 Again in the authors? words (Kozareva and Hovy 2010, page 1,115): ?we found that in-degree is not
sufficient by itself. For example, highly frequent but irrelevant hypernyms such as meats and others are
ranked at the top of the list, while low frequent but relevant ones such as protochordates, hooved-mammals,
homeotherms are discarded.?
703
Computational Linguistics Volume 39, Number 3
extraction. Second, we cope with issues such as term ambiguity, complexity, and
multiplicity of hypernymy patterns. Third, we contribute a multi-faceted evaluation,
which includes a comparison against gold standards, plus a structural and a manual
evaluation. Taxonomy induction was applied to the task of creating new ARTIFICIAL
INTELLIGENCE and FINANCE taxonomies and four taxonomies for gold-standard
comparison against WordNet and MeSH.29
Our experimental analysis shows that OntoLearn Reloaded greatly simplifies the
task of acquiring a taxonomy from scratch: Using a taxonomy validation tool,30 a team
of experts can correct the errors and create a much more acceptable taxonomy in a
matter of hours, rather than man-months, also thanks to the automatic acquisition of
textual definitions for our concepts. As with any automated and unsupervised learning
tool, however, OntoLearn does make errors, as we discussed in Section 4. The accuracy
of the resulting taxonomy is clearly related to the number and quality of discovered
definitional patterns, which is in turn related to the maturity and generality of a domain.
Even with good definitions, problems might arise due to in- and out-domain ambiguity,
the latter being probably the major source of errors, together with complex definitional
structures. Although we believe that there is still room for improvement to OntoLearn
Reloaded, certain errors would appear unavoidable, especially for less focused and
relatively dynamic domains like ARTIFICIAL INTELLIGENCE and FINANCE, in which
new terms arise continuously and have very few, or no definitions on the Web.
Future work includes the addition of non-taxonomical relations along the lines of
ReVerb (Etzioni et al 2011) and WiSeNet (Moro and Navigli 2012), and a more sophis-
ticated rank-based method for scoring textual definitions. Finally, we plan to tackle
the issue of automatically discriminating between in-domain ambiguity and systematic
polysemy (as discussed in Section 4.3.2).
Acknowledgments
Stefano Faralli and Roberto Navigli
gratefully acknowledge the support of the
ERC Starting Grant MultiJEDI No. 259234.
The authors wish to thank Jim McManus for
his valuable comments on the paper, and
Zornitsa Kozareva and Eduard Hovy for
making their data available.
References
Agirre, Eneko, Oier Lo?pez de Lacalle,
Christiane Fellbaum, Shu-Kai Hsieh,
Maurizio Tesconi, Monica Monachini,
Piek Vossen, and Roxanne Segers.
2010. SemEval-2010 Task 17: All-words
Word Sense Disambiguation on a
specific domain. In Proceedings of the
5th International Workshop on Semantic
Evaluation (SemEval-2010), pages 75?80,
Uppsala.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large
corpora. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 57?64, College
Park, MD.
Biemann, Chris. 2005. Ontology learning
from text?A survey of methods.
LDV-Forum, 20(2):75?93.
Brank, Janez, Dunja Mladenic, and
Marko Grobelnik. 2006. Gold standard
based ontology evaluation using instance
assignment. In Proceedings of 4th Workshop
Evaluating Ontologies for the Web (EON),
Edinburgh.
Carpineto, Claudio and Giovanni Romano.
2012. Consensus Clustering Based on
a New Probabilistic Rand Index with
Application to Subtopic Retrieval.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 34(12):2315?2326.
Chu, Yoeng-Jin and Tseng-Hong Liu.
1965. On the shortest arborescence
of a directed graph. Science Sinica,
14:1396?1400.
Cimiano, Philipp, Andreas Hotho, and
Steffen Staab. 2005. Learning concept
29 Data sets are available at: http://lcl.uniroma1.it/ontolearn reloaded.
30 For example, http://lcl.uniroma1.it/tav/.
704
Velardi, Faralli, and Navigli OntoLearn Reloaded
hierarchies from text corpora using
formal concept analysis. Journal of
Artificial Intelligence Research,
24(1):305?339.
Cohen, Trevor and Dominic Widdows.
2009. Empirical distributional semantics:
Methods and biomedical applications.
Journal of Biomedical Informatics,
42(2):390?405.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction
to Algorithms. MIT Electrical Engineering
and Computer Science. MIT Press,
Cambridge, MA.
De Benedictis, Flavio, Stefano Faralli, and
Roberto Navigli. 2013. GlossBoot:
Bootstrapping Multilingual Domain
Glossaries from the Web. In Proceedings
of the 51st Annual Meeting of the
Association for Computational Linguistics
(ACL), Sofia.
De Nicola, Antonio, Michele Missikoff,
and Roberto Navigli. 2009. A software
engineering approach to ontology
building. Information Systems,
34(2):258?275.
Edmonds, Jack. 1967. Optimum branchings.
Journal of Research of the National Bureau of
Standards, 71B:233?240.
Etzioni, Oren, Anthony Fader, Janara
Christensen, Stephen Soderland, and
Mausam. 2011. Open information
extraction: The second generation. In
Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI),
pages 3?10, Barcelona.
Fahmi, Ismail and Gosse Bouma. 2006.
Learning to identify definitions using
syntactic features. In Proceedings of
the EACL 2006 workshop on Learning
Structured Information in Natural
Language Applications, pages 64?71,
Trento.
Faralli, Stefano and Roberto Navigli.
2012. A new minimally supervised
framework for domain Word Sense
Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL),
pages 1,411?1,422, Jeju.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fleiss, Joseph L. 1971. Measuring
nominal scale agreement among
many raters. Psychological Bulletin,
76(5):378?382.
Fountain, Trevor and Mirella Lapata. 2012.
Taxonomy induction using hierarchical
random graphs. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics: Human Language
Technologies (HLT-NAACL), pages 466?476,
Montre?al.
Fowlkes, Edward B. and Colin L. Mallows.
1983. A method for comparing two
hierarchical clusterings. Journal of the
American Statistical Association,
78(383):553?569.
Girju, Roxana, Adriana Badulescu, and
Dan Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Gomez-Perez, Asuncio?n and David
Manzano-Mancho. 2003. A survey of
ontology learning methods and
techniques. OntoWeb Delieverable 1.5.
Universidad Polite?cnica de Madrid.
Guarino, Nicola and Chris Welty. 2002.
Evaluating ontological decisions with
OntoClean. Communications of the ACM,
45(2):61?65.
Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora.
In Proceedings of the 14th International
Conference on Computational Linguistics
(COLING), pages 539?545, Nantes.
Hovy, Eduard, Andrew Philpot,
Judith Klavans, Ulrich Germann, and
Peter T. Davis. 2003. Extending metadata
definitions by automatically extracting
and organizing glossary definitions. In
Proceedings of the 2003 Annual National
Conference on Digital Government Research,
pages 1?6, Boston, MA.
Ide, Nancy and Jean Ve?ronis. 1993.
Extracting knowledge bases from
machine-readable dictionaries: Have
we wasted our time? In Proceedings
of the Workshop on Knowledge Bases and
Knowledge Structures, pages 257?266,
Tokyo.
Kozareva, Zornitsa and Eduard Hovy. 2010.
A semi-supervised method to learn and
construct taxonomies using the Web.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 1,110?1,118,
Cambridge, MA.
Kozareva, Zornitsa, Ellen Riloff, and
Eduard Hovy. 2008. Semantic class
learning from the Web with hyponym
pattern linkage graphs. In Proceedings
of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 1,048?1,056, Columbus, OH.
705
Computational Linguistics Volume 39, Number 3
Maedche, Alexander, Viktor Pekar, and
Steffen Staab. 2002. Ontology learning
part one?on discovering taxonomic
relations from the Web. In N. Zhong,
J. Liu, and Y. Y. Yao, editors, Web
Intelligence. Springer Verlag, Berlin,
pages 301?322.
Maedche, Alexander and Steffen Staab.
2009. Ontology learning. In Steffen Staab
and Rudi Studer, editors, Handbook on
Ontologies. Springer, Berlin, pages 245?268.
Miller, George A., R. T. Beckwith,
Christiane D. Fellbaum, D. Gross, and
K. Miller. 1990. WordNet: An online
lexical database. International Journal of
Lexicography, 3(4):235?244.
Morey, Leslie C. and Alan Agresti. 1984. The
measurement of classification agreement:
An adjustment to the Rand statistic for
chance agreement. Educational and
Psychological Measurement, 44:33?37.
Moro, Andrea and Roberto Navigli. 2012.
WiSeNet: Building a Wikipedia-based
semantic network with ontologized
relations. In Proceedings of the 21st
ACM Conference on Information and
Knowledge Management (CIKM 2012),
pages 1,672?1,676, Maui, HI.
Navigli, Roberto. 2009. Word Sense
Disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Navigli, Roberto, and Simone Paolo
Ponzetto. 2012. BabelNet: The automatic
construction, evaluation and application of
a wide-coverage multilingual semantic
network. Artificial Intelligence 193,
pp. 217?250.
Navigli, Roberto and Paola Velardi. 2004.
Learning domain ontologies from
document warehouses and dedicated
websites. Computational Linguistics,
30(2):151?179.
Navigli, Roberto and Paola Velardi. 2005.
Structural semantic interconnections:
A knowledge-based approach to Word
Sense Disambiguation. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
27(7):1075?1088.
Navigli, Roberto and Paola Velardi. 2010.
Learning Word-Class Lattices for
definition and hypernym extraction.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 1,318?1,327,
Uppsala.
Navigli, Roberto, Paola Velardi, and Stefano
Faralli. 2011. A graph-based algorithm for
inducing lexical taxonomies from scratch.
In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI),
pages 1,872?1,877, Barcelona.
Newman, Mark E. J. 2010. Networks: An
Introduction. Oxford University Press.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pantel, Patrick and Marco Pennacchiotti.
2006. Espresso: Leveraging generic
patterns for automatically harvesting
semantic relations. In Proceedings of
44th Annual Meeting of the Association for
Computational Linguistics joint with 21st
Conference on Computational Linguistics
(COLING-ACL), pages 113?120, Sydney.
Pasca, Marius. 2004. Acquisition of
categorized named entities for web search.
In Proceedings of the 13th ACM International
Conference on Information and Knowledge
Management (CIKM), pages 137?145,
Washington, DC.
Petasis, Georgios, Vangelis Karkaletsis,
Georgios Paliouras, Anastasia Krithara,
and Elias Zavitsanos. 2011. Ontology
population and enrichment: State of the
art. In Georgios Paliouras, Constantine
Spyropoulos, and George Tsatsaronis,
editors, Knowledge-Driven Multimedia
Information Extraction and Ontology
Evolution, volume 6050 of Lecture Notes
in Computer Science. Springer, Berlin /
Heidelberg, pages 134?166.
Petrenz, Philipp and Bonnie L. Webber.
2011. Stable classification of text genres.
Computational Linguistics, 37(2):385?393.
Ponzetto, Simone Paolo and Roberto Navigli.
2009. Large-scale taxonomy mapping for
restructuring and integrating Wikipedia.
In Proceedings of the 21st International Joint
Conference on Artificial Intelligence (IJCAI),
pages 2,083?2,088, Pasadena, CA.
Ponzetto, Simone Paolo and Michael Strube.
2011. Taxonomy induction based on a
collaboratively built knowledge repository.
Artificial Intelligence, 175:1737?1756.
Poon, Hoifung and Pedro Domingos. 2010.
Unsupervised ontology induction from
text. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 296?305, Uppsala.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846?850.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
706
Velardi, Faralli, and Navigli OntoLearn Reloaded
Sclano, Francesco and Paola Velardi. 2007.
TermExtractor: A Web application to
learn the shared terminology of emergent
Web communities. In Proceedings of the 3th
International Conference on Interoperability
for Enterprise Software and Applications
(I-ESA), pages 287?290, Funchal.
Snow, Rion, Dan Jurafsky, and Andrew Ng.
2006. Semantic taxonomy induction from
heterogeneous evidence. In Proceedings of
44th Annual Meeting of the Association for
Computational Linguistics joint with 21st
Conference on Computational Linguistics
(COLING-ACL), pages 801?808, Sydney.
Sowa, John F. 2000. Knowledge Representation:
Logical, Philosophical, and Computational
Foundations. Brooks Cole Publishing Co.,
Pacific Grove, CA.
Storrer, Angelika and Sandra Wellinghoff.
2006. Automated detection and annotation
of term definitions in German text corpora.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 2,373?2,376,
Genova.
Suchanek, Fabian M., Gjergji Kasneci,
and Gerhard Weikum. 2008. YAGO:
A large ontology from Wikipedia and
WordNet. Journal of Web Semantics,
6(3):203?217.
Tang, Jie, Ho Fung Leung, Qiong Luo,
Dewei Chen, and Jibin Gong. 2009.
Towards ontology learning from
folksonomies. In Proceedings of the
21st International Joint Conference on
Artificial Intelligence (IJCAI),
pages 2,089?2,094, Pasadena, CA.
Tarjan, Robert Endre. 1977. Finding optimum
branchings. Networks, 7(1):25?35.
Velardi, Paola, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent
Systems, 23(5):18?25.
Vo?lker, Johanna, Denny Vrandec?ic?,
York Sure, and Andreas Hotho. 2008.
AEON?An approach to the automatic
evaluation of ontologies. Journal of
Applied Ontology, 3(1-2):41?62.
Vossen, Piek. 2001. Extending, trimming
and fusing WordNet for technical
documents. In Proceedings of the North
American Chapter of the Association
for Computational Linguistics Workshop
on WordNet and Other Lexical
Resources: Applications, Extensions
and Customizations (NAACL),
pages 125?131, Pittsburgh, PA.
Wagner, Silke and Dorothea Wagner. 2007.
Comparing clusterings: An overview.
Technical Report 2006-04, Faculty of
Informatics, Universita?t Karlsruhe (TH).
Westerhout, Eline. 2009. Definition extraction
using linguistic and structural features.
In Proceedings of the RANLP Workshop
on Definition Extraction, pages 61?67,
Borovets.
Yang, Hui and Jamie Callan. 2008.
Human-guided ontology learning.
In Proceedings of Human-Computer
Interaction and Information Retrieval
(HCIR), pages 26?29, Redmond, WA.
Yang, Hui and Jamie Callan. 2009. A
metric-based framework for automatic
taxonomy induction. In Proceedings of
the 47th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 271?279, Suntec.
Zavitsanos, Elias, Georgios Paliouras,
and George A. Vouros. 2011. Gold
standard evaluation of ontology learning
methods through ontology transformation
and alignment. IEEE Transactions on
Knowledge and Data Engineering,
23(11):1635?1648.
Zhang, Chunxia and Peng Jiang. 2009.
Automatic extraction of definitions.
In Proceedings of 2nd IEEE International
Conference on Computer Science and
Information Technology (ICCSIT),
pages 364?368, Beijing.
707

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318?1327,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Word-Class Lattices for Definition and Hypernym Extraction
Roberto Navigli and Paola Velardi
Dipartimento di Informatica
Sapienza Universita` di Roma
{navigli,velardi}@di.uniroma1.it
Abstract
Definition extraction is the task of au-
tomatically identifying definitional sen-
tences within texts. The task has proven
useful in many research areas including
ontology learning, relation extraction and
question answering. However, current ap-
proaches ? mostly focused on lexico-
syntactic patterns ? suffer from both low
recall and precision, as definitional sen-
tences occur in highly variable syntactic
structures. In this paper, we propose Word-
Class Lattices (WCLs), a generalization of
word lattices that we use to model tex-
tual definitions. Lattices are learned from
a dataset of definitions from Wikipedia.
Our method is applied to the task of def-
inition and hypernym extraction and com-
pares favorably to other pattern general-
ization methods proposed in the literature.
1 Introduction
Textual definitions constitute a fundamental
source to look up when the meaning of a term is
sought. Definitions are usually collected in dictio-
naries and domain glossaries for consultation pur-
poses. However, manually constructing and up-
dating glossaries requires the cooperative effort of
a team of domain experts. Further, in the presence
of new words or usages, and ? even worse ? new
domains, such resources are of no help. Nonethe-
less, terms are attested in texts and some (usually
few) of the sentences in which a term occurs are
typically definitional, that is they provide a formal
explanation for the term of interest. While it is not
feasible to manually search texts for definitions,
this task can be automatized by means of Machine
Learning (ML) and Natural Language Processing
(NLP) techniques.
Automatic definition extraction is useful not
only in the construction of glossaries, but also
in many other NLP tasks. In ontology learning,
definitions are used to create and enrich concepts
with textual information (Gangemi et al, 2003),
and extract taxonomic and non-taxonomic rela-
tions (Snow et al, 2004; Navigli and Velardi,
2006; Navigli, 2009a). Definitions are also har-
vested in Question Answering to deal with ?what
is? questions (Cui et al, 2007; Saggion, 2004).
In eLearning, they are used to help students as-
similate knowledge (Westerhout and Monachesi,
2007), etc.
Much of the current literature focuses on the use
of lexico-syntactic patterns, inspired by Hearst?s
(1992) seminal work. However, these methods
suffer both from low recall and precision, as defi-
nitional sentences occur in highly variable syntac-
tic structures, and because the most frequent def-
initional pattern ? X is a Y ? is inherently very
noisy.
In this paper we propose a generalized form of
word lattices, called Word-Class Lattices (WCLs),
as an alternative to lexico-syntactic pattern learn-
ing. A lattice is a directed acyclic graph (DAG), a
subclass of non-deterministic finite state automata
(NFA). The lattice structure has the purpose of
preserving the salient differences among distinct
sequences, while eliminating redundant informa-
tion. In computational linguistics, lattices have
been used to model in a compact way many se-
quences of symbols, each representing an alter-
native hypothesis. Lattice-based methods differ
in the types of nodes (words, phonemes, con-
cepts), the interpretation of links (representing ei-
ther a sequential or hierarchical ordering between
nodes), their means of creation, and the scor-
ing method used to extract the best consensus
output from the lattice (Schroeder et al, 2009).
In speech processing, phoneme or word lattices
(Campbell et al, 2007; Mathias and Byrne, 2006;
Collins et al, 2004) are used as an interface be-
tween speech recognition and understanding. Lat-
1318
tices are adopted also in Chinese word segmenta-
tion (Jiang et al, 2008), decompounding in Ger-
man (Dyer, 2009), and to represent classes of
translation models in machine translation (Dyer et
al., 2008; Schroeder et al, 2009). In more com-
plex text processing tasks, such as information re-
trieval, information extraction and summarization,
the use of word lattices has been postulated but is
considered unrealistic because of the dimension of
the hypothesis space.
To reduce this problem, concept lattices have
been proposed (Carpineto and Romano, 2005;
Klein, 2008; Zhong et al, 2008). Here links repre-
sent hierarchical relations, rather than the sequen-
tial order of symbols like in word/phoneme lat-
tices, and nodes are clusters of salient words ag-
gregated using synonymy, similarity, or subtrees
of a thesaurus. However, salient word selection
and aggregation is non-obvious and furthermore
it falls into word sense disambiguation, a notori-
ously AI-hard problem (Navigli, 2009b).
In definition extraction, the variability of pat-
terns is higher than for ?traditional? applications
of lattices, such as translation and speech, how-
ever not as high as in unconstrained sentences.
The methodology that we propose to align patterns
is based on the use of star (wildcard *) charac-
ters to facilitate sentence clustering. Each clus-
ter of sentences is then generalized to a lattice of
word classes (each class being either a frequent
word or a part of speech). A key feature of our
approach is its inherent ability to both identify def-
initions and extract hypernyms. The method is
tested on an annotated corpus of Wikipedia sen-
tences and a large Web corpus, in order to demon-
strate the independence of the method from the
annotated dataset. WCLs are shown to general-
ize over lexico-syntactic patterns, and outperform
well-known approaches to definition and hyper-
nym extraction.
The paper is organized as follows: Section 2
discusses related work, WCLs are introduced in
Section 3 and illustrated by means of an example
in Section 4, experiments are presented in Section
5. We conclude the paper in Section 6.
2 Related Work
Definition Extraction. A great deal of work
is concerned with definition extraction in several
languages (Klavans and Muresan, 2001; Storrer
and Wellinghoff, 2006; Gaudio and Branco, 2007;
Iftene et al, 2007; Westerhout and Monachesi,
2007; Przepio?rkowski et al, 2007; Dego?rski et
al., 2008). The majority of these approaches use
symbolic methods that depend on lexico-syntactic
patterns or features, which are manually crafted
or semi-automatically learned (Zhang and Jiang,
2009; Hovy et al, 2003; Fahmi and Bouma, 2006;
Westerhout, 2009). Patterns are either very sim-
ple sequences of words (e.g. ?refers to?, ?is de-
fined as?, ?is a?) or more complex sequences of
words, parts of speech and chunks. A fully au-
tomated method is instead proposed by Borg et
al. (2009): they use genetic programming to learn
simple features to distinguish between definitions
and non-definitions, and then they apply a genetic
algorithm to learn individual weights of features.
However, rules are learned for only one category
of patterns, namely ?is? patterns. As we already
remarked, most methods suffer from both low re-
call and precision, because definitional sentences
occur in highly variable and potentially noisy syn-
tactic structures. Higher performance (around 60-
70% F1-measure) is obtained only for specific do-
mains (e.g., an ICT corpus) and patterns (Borg et
al., 2009).
Only few papers try to cope with the general-
ity of patterns and domains in real-world corpora
(like the Web). In the GlossExtractor web-based
system (Velardi et al, 2008), to improve precision
while keeping pattern generality, candidates are
pruned using more refined stylistic patterns and
lexical filters. Cui et al (2007) propose the use
of probabilistic lexico-semantic patterns, called
soft patterns, for definitional question answering
in the TREC contest1. The authors describe two
soft matching models: one is based on an n-gram
language model (with the Expectation Maximiza-
tion algorithm used to estimate the model param-
eter), the other on Profile Hidden Markov Mod-
els (PHMM). Soft patterns generalize over lexico-
syntactic ?hard? patterns in that they allow a par-
tial matching by calculating a generative degree
of match probability between the test instance and
the set of training instances. Thanks to its gen-
eralization power, this method is the most closely
related to our work, however the task of defini-
tional question answering to which it is applied is
slightly different from that of definition extraction,
so a direct performance comparison is not possi-
1Text REtrieval Conferences: http://trec.nist.
gov
1319
ble2. In fact, the TREC evaluation datasets cannot
be considered true definitions, but rather text frag-
ments providing some relevant fact about a target
term. For example, sentences like: ?Bollywood is
a Bombay-based film industry? and ?700 or more
films produced by India with 200 or more from
Bollywood? are both ?vital? answers for the ques-
tion ?Bollywood?, according to TREC classifica-
tion, but the second sentence is not a definition.
Hypernym Extraction. The literature on hy-
pernym extraction offers a higher variability of
methods, from simple lexical patterns (Hearst,
1992; Oakes, 2005) to statistical and machine
learning techniques (Agirre et al, 2000; Cara-
ballo, 1999; Dolan et al, 1993; Sanfilippo and
Poznan?ski, 1992; Ritter et al, 2009). One of the
highest-coverage methods is proposed by Snow et
al. (2004). They first search sentences that con-
tain two terms which are known to be in a taxo-
nomic relation (term pairs are taken from Word-
Net (Miller et al, 1990)); then they parse the sen-
tences, and automatically extract patterns from the
parse trees. Finally, they train a hypernym clas-
sifer based on these features. Lexico-syntactic pat-
terns are generated for each sentence relating a
term to its hypernym, and a dependency parser is
used to represent them.
3 Word-Class Lattices
3.1 Preliminaries
Notion of definition. In our work, we rely on
a formal notion of textual definition. Specifically,
given a definition, e.g.: ?In computer science, a
closure is a first-class function with free variables
that are bound in the lexical environment?, we as-
sume that it contains the following fields (Storrer
and Wellinghoff, 2006):
? The DEFINIENDUM field (DF): this part of
the definition includes the definiendum (that
is, the word being defined) and its modifiers
(e.g., ?In computer science, a closure?);
? The DEFINITOR field (VF): it includes the
verb phrase used to introduce the definition
(e.g., ?is?);
2In the paper, a 55% recall and 34% precision is achieved
with the best experiment on TREC-13 data. Furthermore, the
classifier of Cui et al (2007) is based on soft patterns but also
on a bag-of-word relevance heuristic. However, the relative
influence of the two methods on the final performance is not
discussed.
? The DEFINIENS field (GF): it includes the
genus phrase (usually including the hyper-
nym, e.g., ?a first-class function?);
? The REST field (RF): it includes additional
clauses that further specify the differentia of
the definiendum with respect to its genus
(e.g., ?with free variables that are bound in
the lexical environment?).
Further examples of definitional sentences an-
notated with the above fields are shown in Table
1. For each sentence, the definiendum (that is, the
word being defined) and its hypernym are marked
in bold and italic, respectively. Given the lexico-
syntactic nature of the definition extraction mod-
els we experiment with, training and test sentences
are part-of-speech tagged with the TreeTagger sys-
tem, a part-of-speech tagger available for many
languages (Schmid, 1995).
Word Classes and Generalized Sentences. We
now introduce our notion of word class, on which
our learning model is based. Let T be the set
of training sentences, manually bracketed with the
DF, VF, GF and RF fields. We first determine the
set F of words in T whose frequency is above a
threshold ? (e.g., the, a, is, of, refer, etc.). In our
training sentences, we replace the term being de-
fined with ?TARGET?, thus this frequent token is
also included in F .
We use the set of frequent words F to generalize
words to ?word classes?. We define a word class
as either a word itself or its part of speech. Given
a sentence s = w1, w2, . . . , w|s|, where wi is the
i-th word of s, we generalize its words wi to word
classes ?i as follows:
?i =
{
wi if wi ? F
POS(wi) otherwise
that is, a word wi is left unchanged if it occurs
frequently in the training corpus (i.e., wi ? F )
or is transformed to its part of speech (POS(wi))
otherwise. As a result, we obtain a general-
ized sentence s? = ?1, ?2, . . . , ?|s|. For instance,
given the first sentence in Table 1, we obtain the
corresponding generalized sentence: ?In NN, a
?TARGET? is a JJ NN?, where NN and JJ indicate
the noun and adjective classes, respectively.
3.2 Algorithm
We now describe our learning algorithm based
on Word-Class Lattices. The algorithm consists of
three steps:
1320
[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of . . . ]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.
Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italic).
? Star patterns: each sentence in the training
set is pre-processed and generalized to a star
pattern. For instance, ?In arts, a chiaroscuro
is a monochrome picture? is transformed to
?In *, a ?TARGET? is a *? (Section 3.2.1);
? Sentence clustering: the training sentences
are then clustered based on the star patterns
to which they belong (Section 3.2.2);
? Word-Class Lattice construction: for each
sentence cluster, a WCL is created by means
of a greedy alignment algorithm (Section
3.2.3).
We present two variants of our WCL model,
dealing either globally with the entire sentence or
separately with its definition fields (Section 3.2.4).
The WCL models can then be used to classify any
input sentence of interest (Section 3.2.5).
3.2.1 Star Patterns
Let T be the set of training sentences. In this step,
we associate a star pattern ?(s) with each sentence
s ? T . To do so, let s ? T be a sentence such that
s = w1, w2, . . . , w|s|, where wi is its i-th word.
Given the set F of most frequent words in T (cf.
Section 3.1), the star pattern ?(s) associated with
s is obtained by replacing with * all the words
wi 6? F , that is all the tokens that are non-frequent
words. For instance, given the sentence ?In arts,
a chiaroscuro is a monochrome picture?, the cor-
responding star pattern is ?In *, a ?TARGET? is a
*?, where ?TARGET? is the defined term.
Note that, here and in what follows, we discard
the sentence fragments tagged with the REST field,
which is used only to delimit the core part of defi-
nitional sentences.
3.2.2 Sentence Clustering
In the second step, we cluster the sentences in our
training set T based on their star patterns. For-
mally, let ? = (?1, . . . , ?m) be the set of star
patterns associated with the sentences in T . We
create a clustering C = (C1, . . . , Cm) such that
Ci = {s ? T : ?(s) = ?i}, that is Ci contains all
the sentences whose star pattern is ?i.
As an example, assume ?3 = ?In *, a
?TARGET? is a *?. The sentences reported in Ta-
ble 1 are all grouped into cluster C3. We note that
each cluster Ci contains sentences whose degree
of variability is generally much lower than for any
pair of sentences in T belonging to two different
clusters.
3.2.3 Word-Class Lattice Construction
Finally, the third step consists of the construction
of a Word-Class Lattice for each sentence cluster.
Given such a cluster Ci ? C, we apply a greedy
algorithm that iteratively constructs the WCL.
Let Ci = {s1, s2, . . . , s|Ci|} and consider
its first sentence s1 = w11, w
1
2, . . . , w
1
|s1|
(wji
denotes the i-th token of the j-th sentence).
We first produce the corresponding general-
ized sentence s?1 = ?
1
1, ?
1
2, . . . , ?
1
|s1|
(cf. Sec-
tion 3.1). We then create a directed graph
G = (V,E) such that V = {?11, . . . , ?
1
|s1|
} and
E = {(?11, ?
1
2), (?
1
2, ?
1
3), . . . , (?
1
|s1|?1
, ?1|s1|)}.
Next, for the subsequent sentences in Ci, that
is, for each j = 2, . . . , |Ci|, we determine the
alignment between the sentence sj and each
sentence sk ? Ci such that k < j based on the
following dynamic programming formulation
(Cormen et al, 1990, pp. 314?319):
Ma,b = max {Ma?1,b?1 +Sa,b,Ma,b?1,Ma?1,b}
where a ? {1, . . . , |sk|} and b ? {1, . . . , |sj |},
Sa,b is a score of the matching between the a-th
token of sk and the b-th token of sj , and M0,0,
M0,b and Ma,0 are initially set to 0 for all a and b.
The matching score Sa,b is calculated on the
generalized sentences s?k of sk and s
?
j of sj as fol-
lows:
Sa,b =
{
1 if ?ka = ?
j
b
0 otherwise
where ?ka and ?
j
b are the a-th and b-th word classes
of s?k and s
?
j , respectively. In other words, the
matching score equals 1 if the a-th and the b-th
tokens of the two original sentences have the same
word class.
Finally, the alignment score between sk and sj
is given by M|sk|,|sj |, which calculates the mini-
1321
In
arts
science
mathematics
NN1
NN4
computer
, a ?TARGET?
pixel
graph
chiaroscuro
is a
monochrome
JJ NN2
structure
picture
dot
NN3
data
Figure 1: The Word-Class Lattice for the sentences in Table 1. The support of each word class is reported
beside the corresponding node.
mal number of misalignments between the two to-
ken sequences. We repeat this calculation for each
sentence sk (k = 1, . . . , j ? 1) and choose the
one that maximizes its alignment score with sj .
We then use the best alignment to add sj to the
graph G. Such alignment is obtained by means
of backtracking from M|sk|,|sj | to M0,0. We add
to the set of vertices V the tokens of the gen-
eralized sentence s?j for which there is no align-
ment to s?k and we add to E the edges (?
j
1, ?
j
2),
. . . , (?j|sj |?1, ?
j
|sj |
). Furthermore, in the final lat-
tice, nodes associated with the hypernym words in
the learning sentences are marked as hypernyms
in order to be able to determine the hypernym of a
test sentence at classification time.
3.2.4 Variants of the WCL Model
So far, we have assumed that our WCL model
learns lattices from the training sentences in
their entirety (we call this model WCL-1). We
now propose a second model that learns separate
WCLs for each field of the definition, namely:
the DEFINIENDUM (DF), DEFINITOR (VF) and
DEFINIENS (GF) fields (see Section 3.1). We re-
fer to this latter model as WCL-3. Rather than ap-
plying the WCL algorithm to the entire sentence,
the very same method is applied to the sentence
fragments tagged with one of the three definition
fields. The reason for introducing the WCL-3
model is that, while definitional patterns are highly
variable, DF, VF and GF individually exhibit a
lower variability, thus WCL-3 should improve the
generalization power.
3.2.5 Classification
Once the learning process is over, a set of WCLs is
produced. Given a test sentence s, the classifica-
tion phase for the WCL-1 model consists of deter-
mining whether it exists a lattice that matches s. In
the case of WCL-3, we consider any combination
of DEFINIENDUM, DEFINITOR and DEFINIENS
lattices. While WCL-1 is applied as a yes-no clas-
sifier as there is a single WCL that can possibly
match the input sentence, WCL-3 selects, if any,
the combination of the three WCLs that best fits
the sentence. In fact, choosing the most appro-
priate combination of lattices impacts the perfor-
mance of hypernym extraction. The best combi-
nation of WCLs is selected by maximizing the fol-
lowing confidence score:
score(s, lDF, lVF, lGF) = coverage ? log(support)
where s is the candidate sentence, lDF, lVF and lGF
are three lattices one for each definition field, cov-
erage is the fraction of words of the input sentence
covered by the three lattices, and support is the
sum of the number of sentences in the star patterns
corresponding to the three lattices.
Finally, when a sentence is classified as a def-
inition, its hypernym is extracted by selecting the
words in the input sentence that are marked as ?hy-
pernyms? in the WCL-1 lattice (or in the WCL-3
GF lattice).
4 Example
As an example, consider the definitions in Table
1. As illustrated in Section 3.2.2, their star pat-
tern is ?In *, a ?TARGET? is a *?. The corre-
sponding WCL is built as follows: the first part-
of-speech tagged sentence, ?In/IN arts/NN , a/DT
?TARGET?/NN is/VBZ a/DT monochrome/JJ pic-
ture/NN?, is considered. The corresponding gen-
eralized sentence is ?In NN , a ?TARGET? is a
JJ NN?. The initially empty graph is thus popu-
lated with one node for each word class and one
edge for each pair of consecutive tokens, as shown
in Figure 1 (the central sequence of nodes in the
graph). Note that we draw the hypernym token
NN2 with a rectangle shape. We also add to the
1322
graph a start node ? and an end node ??, and con-
nect them to the corresponding initial and final
sentence tokens. Next, the second sentence, ?In
mathematics, a graph is a data structure that con-
sists of...?, is aligned to the first sentence. The
alignment of the generalized sentence is perfect,
apart from the NN3 node corresponding to ?data?.
The node is added to the graph together with the
edges a? NN3 and NN3 ? NN2 . Finally, the
third sentence in Table 1, ?In computer science, a
pixel is a dot that is part of a computer image?,
is generalized as ?In NN NN , a ?TARGET? is
a NN?. Thus, a new node NN4 is added, corre-
sponding to ?computer? and new edges are added:
In?NN4 and NN4?NN1. Figure 1 shows the re-
sulting WCL-1 lattice.
5 Experiments
5.1 Experimental Setup
Datasets. We conducted experiments on two
different datasets:
? A corpus of 4,619 Wikipedia sentences, that
contains 1,908 definitional and 2,711 non-
definitional sentences. The former were ob-
tained from a random selection of the first
sentences of Wikipedia articles3. The de-
fined terms belong to different Wikipedia
domain categories4, so as to capture a
representative and cross-domain sample of
lexical and syntactic patterns for defini-
tions. These sentences were manually an-
notated with DEFINIENDUM, DEFINITOR,
DEFINIENS and REST fields by an expert
annotator, who also marked the hypernyms.
The associated set of negative examples
(?syntactically plausible? false definitions)
was obtained by extracting from the same
Wikipedia articles sentences in which the
page title occurs.
? A subset of the ukWaC Web corpus (Fer-
raresi et al, 2008), a large corpus of the En-
glish language constructed by crawling the
.uk domain of the Web. The subset includes
over 300,000 sentences in which occur any
of 239 terms selected from the terminology
of four different domains (COMPUTER SCI-
3The first sentence of Wikipedia entries is, in the large
majority of cases, a definition of the page title.
4en.wikipedia.org/wiki/Wikipedia:Cate-
gories
ENCE, ASTRONOMY, CARDIOLOGY, AVIA-
TION).
The reason for using the ukWaC corpus is that, un-
like the ?clean? Wikipedia dataset, in which rel-
atively simple patterns can achieve good results,
ukWaC represents a real-world test, with many
complex cases. For example, there are sentences
that should be classified as definitional according
to Section 3.1 but are rather uninformative, like
?dynamic programming was the brainchild of an
american mathematician?, as well as informative
sentences that are not definitional (e.g., they do not
have a hypernym), like ?cubism was characterised
by muted colours and fragmented images?. Even
more frequently, the dataset includes sentences
which are not definitions but have a definitional
pattern (?A Pacific Northwest tribe?s saga refers to
a young woman who [..]?), or sentences with very
complex definitional patterns (?white body cells
are the body?s clean up squad? and ?joule is also
an expression of electric energy?). These cases can
be correctly handled only with fine-grained pat-
terns. Additional details on the corpus and a more
thorough linguistic analysis of complex cases can
be found in Navigli et al (2010).
Systems. For definition extraction, we experi-
ment with the following systems:
? WCL-1 and WCL-3: these two classifiers
are based on our Word-Class Lattice model.
WCL-1 learns from the training set a lattice
for each cluster of sentences, whereas WCL-
3 identifies clusters (and lattices) separately
for each sentence field (DEFINIENDUM,
DEFINITOR and DEFINIENS) and classifies a
sentence as a definition if any combination
from the three sets of lattices matches (cf.
Section 3.2.4, the best combination is se-
lected).
? Star patterns: a simple classifier based on
the patterns learned as a result of step 1 of our
WCL learning algorithm (cf. Section 3.2.1):
a sentence is classified as a definition if it
matches any of the star patterns in the model.
? Bigrams: an implementation of the bigram
classifier for soft pattern matching proposed
by Cui et al (2007). The classifier selects as
definitions all the sentences whose probabil-
ity is above a specific threshold. The proba-
bility is calculated as a mixture of bigram and
1323
Algorithm P R F1 A
WCL-1 99.88 42.09 59.22 76.06
WCL-3 98.81 60.74 75.23 83.48
Star patterns 86.74 66.14 75.05 81.84
Bigrams 66.70 82.70 73.84 75.80
Random BL 50.00 50.00 50.00 50.00
Table 2: Performance on the Wikipedia dataset.
unigram probabilities, with Laplace smooth-
ing on the latter. We use the very same set-
tings of Cui et al (2007), including threshold
values. While the authors propose a second
soft-pattern approach based on Profile HMM
(cf. Section 2), their results do not show sig-
nificant improvements over the bigram lan-
guage model.
For hypernym extraction, we compared WCL-
1 and WCL-3 with Hearst?s patterns, a system
that extracts hypernyms from sentences based on
the lexico-syntactic patterns specified in Hearst?s
seminal work (1992). These include (hypernym
in italic): ?such NP as {NP ,} {(or | and)} NP?,
?NP {, NP} {,} or other NP?, ?NP {,} includ-
ing { NP ,} {or | and} NP?, ?NP {,} especially {
NP ,} {or | and} NP?, and variants thereof. How-
ever, it should be noted that hypernym extraction
methods in the literature do not extract hypernyms
from definitional sentences, like we do, but rather
from specific patterns like ?X such as Y?. There-
fore a direct comparison with these methods is not
possible. Nonetheless, we decided to implement
Hearst?s patterns for the sake of completeness. We
could not replicate the more refined approach by
Snow et al (2004) because it requires the annota-
tion of a possibly very large dataset of sentence
fragments. In any case Snow et al (2004) re-
ported the following performance figures on a cor-
pus of dimension and complexity comparable with
ukWaC: the recall-precision graph indicates preci-
sion 85% at recall 10% and precision 25% at re-
call of 30% for the hypernym classifier. A variant
of the classifier that includes evidence from coor-
dinate terms (terms with a common ancestor in a
taxonomy) obtains an increased precision of 35%
at recall 30%. We see no reasons why these figures
should vary dramatically on the ukWaC.
Finally, we compare all systems with the ran-
dom baseline, that classifies a sentence as a defi-
nition with probability 12 .
Algorithm P R?
WCL-1 98.33 39.39
WCL-3 94.87 56.57
Star patterns 44.01 63.63
Bigrams 46.60 45.45
Random BL 50.00 50.00
Table 3: Performance on the ukWaC dataset (? Re-
call is estimated).
Measures. To assess the performance of our
systems, we calculated the following measures:
? precision ? the number of definitional sen-
tences correctly retrieved by the system over
the number of sentences marked by the sys-
tem as definitional.
? recall ? the number of definitional sen-
tences correctly retrieved by the system over
the number of definitional sentences in the
dataset.
? the F1-measure ? a harmonic mean of preci-
sion (P) and recall (R) given by 2PRP+R .
? accuracy ? the number of correctly classi-
fied sentences (either as definitional or non-
definitional) over the total number of sen-
tences in the dataset.
5.2 Results and Discussion
Definition Extraction. In Table 2 we report
the results of definition extraction systems on the
Wikipedia dataset. Given this dataset is also used
for training, experiments are performed with 10-
fold cross validation. The results show very high
precision for WCL-1, WCL-3 (around 99%) and
star patterns (86%). As expected, bigrams and star
patterns exhibit a higher recall (82% and 66%, re-
spectively). The lower recall of WCL-1 is due to
its limited ability to generalize compared to WCL-
3 and the other methods. In terms of F1-measure,
star patterns and WCL-3 achieve 75%, and are
thus the best systems. Similar performance is ob-
served when we also account for negative sen-
tences ? that is we calculate accuracy (with WCL-
3 performing better). All the systems perform sig-
nificantly better than the random baseline.
From our Wikipedia corpus, we learned over
1,000 lattices (and star patterns). Using WCL-
3, we learned 381 DF, 252 VF and 395 GF lat-
tices, that then we used to extract definitions from
1324
Algorithm Full Substring
WCL-1 42.75 77.00
WCL-3 40.73 78.58
Table 4: Precision in hypernym extraction on the
Wikipedia dataset
the ukWaC dataset. To calculate precision on this
dataset, we manually validated the definitions out-
put by each system. However, given the large size
of the test set, recall could only be estimated. To
this end, we manually analyzed 50,000 sentences
and identified 99 definitions, against which recall
was calculated. The results are shown in Table 3.
On the ukWaC dataset, WCL-3 performs best, ob-
taining 94.87% precision and 56.57% recall (we
did not calculate F1, as recall is estimated). In-
terestingly, star patterns obtain only 44% preci-
sion and around 63% recall. Bigrams achieve
even lower performance, namely 46.60% preci-
sion, 45.45% recall. The reason for such bad
performance on ukWaC is due to the very dif-
ferent nature of the two datasets: for example, in
Wikipedia most ?is a? sentences are definitional,
whereas this property is not verified in the real
world (that is, on the Web, of which ukWaC is
a sample). Also, while WCL does not need any
parameter tuning5, the same does not hold for bi-
grams6, whose probability threshold and mixture
weights need to be best tuned on the task at hand.
Hypernym Extraction. For hypernym extrac-
tion, we tested WCL-1, WCL-3 and Hearst?s pat-
terns. Precision results are reported in Tables 4
and 5 for the two datasets, respectively. The Sub-
string column refers to the case in which the cap-
tured hypernym is a substring of what the annota-
tor considered to be the correct hypernym. Notice
that this is a complex matter, because often the se-
lection of a hypernym depends on semantic and
contextual issues. For example, ?Fluoroscopy is
an imaging method? and ?the Mosaic was an in-
teresting project? have precisely the same genus
pattern, but (probably depending on the vagueness
of the noun in the first sentence, and of the adjec-
tive in the second) the annotator selected respec-
5WCL has only one threshold value ? to be set for deter-
mining frequent words (cf. Section 3.1). However, no tuning
was made for choosing the best value of ?.
6We had to re-tune the system parameters on ukWaC,
since with the original settings of Cui et al (2007) perfor-
mance was much lower.
Algorithm Full Substring
WCL-1 86.19 (206) 96.23 (230)
WCL-3 89.27 (383) 96.27 (413)
Hearst 65.26 (62) 88.42 (84)
Table 5: Precision in hypernym extraction on the
ukWaC dataset (number of hypernyms in paren-
theses).
tively imaging method and project as hypernyms.
For the above reasons it is difficult to achieve high
performance in capturing the correct hypernym
(e.g. 40.73% with WCL-3 on Wikipedia). How-
ever, our performance of identifying a substring
of the correct hypernym is much higher (around
78.58%). In Table 4 we do not report the preci-
sion of Hearst?s patterns, as only one hypernym
was found, due to the inherently low coverage of
the method.
On the ukWaC dataset, the hypernyms returned
by the three systems were manually validated and
precision was calculated. Both WCL-1 and WCL-
3 obtained a very high precision (86-89% and 96%
in identifying the exact hypernym and a substring
of it, respectively). Both WCL models are thus
equally robust in identifying hypernyms, whereas
WCL-1 suffers from a lack of generalization in
definition extraction (cf. Tables 2 and 3). Also,
given that the ukWaC dataset contains sentences
in which any of 239 domain terms occur, WCL-3
extracts on average 1.6 and 1.7 full and substring
hypernyms per term, respectively. Hearst?s pat-
terns also obtain high precision, especially when
substrings are taken into account. However, the
number of hypernyms returned by this method is
much lower, due to the specificity of the patterns
(62 vs. 383 hypernyms returned by WCL-3).
6 Conclusions
In this paper, we have presented a lattice-based ap-
proach to definition and hypernym extraction. The
novelty of our approach is:
1. The use of a lattice structure to generalize
over lexico-syntactic definitional patterns;
2. The ability of the system to jointly identify
definitions and extract hypernyms;
3. The generality of the method, which applies
to generic Web documents in any domain and
style, and needs no parameter tuning;
1325
4. The high performance as compared with the
best-known methods for both definition and
hypernym extraction. Our approach outper-
forms the other systems particularly where
the task is more complex, as in real-world
documents (i.e., the ukWaC corpus).
Even though definitional patterns are learned
from a manually annotated dataset, the dimension
and heterogeneity of the training dataset ensures
that training needs not to be repeated for specific
domains7, as demonstrated by the cross-domain
evaluation on the ukWaC corpus.
The datasets used in our experiments are avail-
able from http://lcl.uniroma1.it/wcl.
We also plan to release our system to the research
community. In the near future, we aim to apply the
output of our classifiers to the task of automated
taxonomy building, and to test the WCL approach
on other information extraction tasks, like hyper-
nym extraction from generic sentence fragments,
as in Snow et al (2004).
References
Eneko Agirre, Ansa Olatz, Xabier Arregi, Xabier Ar-
tola, Arantza Daz de Ilarraza Snchez, Mikel Ler-
sundi, David Martnez, Kepa Sarasola, and Ruben
Urizar. 2000. Extraction of semantic relations from
a basque monolingual dictionary using constraint
grammar. In Proceedings of Euralex.
Claudia Borg, Mike Rosner, and Gordon Pace. 2009.
Evolutionary algorithms for definition extraction. In
Proceedings of the 1st Workshop on Definition Ex-
traction 2009 (wDE?09).
William M. Campbell, M. F. Richardson, and D. A.
Reynolds. 2007. Language recognition with word
lattices and support vector machines. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP 2007),
pages 989?992, Honolulu, HI.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
120?126, Maryland, USA.
Claudio Carpineto and Giovanni Romano. 2005. Us-
ing concept lattices for text retrieval and mining. In
B. Ganter, G. Stumme, and R. Wille, editors, Formal
Concept Analysis, pages 161?179.
Christopher Collins, Bob Carpenter, and Gerald Penn.
2004. Head-driven parsing for word lattices. In Pro-
ceedings of the 42nd Meeting of the Association for
7Of course, it would need some additional work if applied
to languages other than English. However, the approach does
not need to be adapted to the language of interest.
Computational Linguistics (ACL?04), Main Volume,
pages 231?238, Barcelona, Spain, July.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to algorithms.
the MIT Electrical Engineering and Computer Sci-
ence Series. MIT Press, Cambridge, MA.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Transactions on Information
Systems, 25(2):8.
?ukasz Dego?rski, Micha? Marcinczuk, and Adam
Przepio?rkowski. 2008. Definition extraction us-
ing a sequential combination of baseline grammars
and machine learning classifiers. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco.
William Dolan, Lucy Vanderwende, and Stephen D.
Richardson. 1993. Automatically deriving struc-
tured knowledge bases from on-line dictionaries. In
Proceedings of the First Conference of the Pacific
Association for Computational Linguistics, pages 5?
14.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2008),
pages 1012?1020, Columbus, Ohio, USA.
Christopher Dyer. 2009. Using a maximum en-
tropy model to build segmentation lattices for mt.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2009), pages 406?414, Boul-
der, Colorado, USA.
Ismail Fahmi and Gosse Bouma. 2006. Learning to
identify definitions using syntactic features. In Pro-
ceedings of the EACL 2006 workshop on Learning
Structured Information in Natural Language Appli-
cations, pages 64?71, Trento, Italy.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large Web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4), Marrakech, Morocco.
Aldo Gangemi, Roberto Navigli, and Paola Velardi.
2003. The OntoWordNet project: Extension and ax-
iomatization of conceptual relations in WordNet. In
Proceedings of the International Conference on On-
tologies, Databases and Applications of SEmantics
(ODBASE 2003), pages 820?838, Catania, Italy.
Rosa Del Gaudio and Anto?nio Branco. 2007. Auto-
matic extraction of definitions in portuguese: A rule-
based approach. In Proceedings of the TeMa Work-
shop.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceed-
ings of the 14th International Conference on Com-
putational Linguistics (COLING), pages 539?545,
Nantes, France.
1326
Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich
Germann, and Peter T. Davis. 2003. Extending
metadata definitions by automatically extracting and
organizing glossary definitions. In Proceedings of
the 2003 Annual National Conference on Digital
Government Research, pages 1?6. Digital Govern-
ment Society of North America.
Adrian Iftene, Diana Trandaba?, and Ionut Pistol. 2007.
Natural language processing and knowledge repre-
sentation for elearning environments. In Proc. of
Applications for Romanian. Proceedings of RANLP
workshop, pages 19?25.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word
lattice reranking for chineseword segmentation and
part-of-speech tagging. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics (COLING 2008), pages 385?392, Manch-
ester, UK.
Judith Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully auto-
matic glossary construction. In Proc. of the Amer-
ican Medical Informatics Association (AMIA) Sym-
posium.
Michael Tully Klein. 2008. Understanding English
with Lattice-Learning, Master thesis. MIT, Cam-
bridge, MA, USA.
Lambert Mathias and William Byrne. 2006. Statis-
tical phrase-based speech translation. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP 2006),
Toulouse, France.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet:
an online lexical database. International Journal of
Lexicography, 3(4):235?244.
Roberto Navigli and Paola Velardi. 2006. Ontology
enrichment through automatic semantic annotation
of on-line glossaries. In Proceedings of the 15th In-
ternational Conference on Knowledge Engineering
and Knowledge Management (EKAW 2006), pages
126?140, Podebrady, Czech Republic.
Roberto Navigli, Paola Velardi, and Juana Mar??a Ruiz-
Mart??nez. 2010. An annotated dataset for extract-
ing definitions and hypernyms from the Web. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC 2010),
Valletta, Malta.
Roberto Navigli. 2009a. Using cycles and quasi-cycles
to disambiguate dictionary glosses. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2009), pages 594?602, Athens, Greece.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Michael P. Oakes. 2005. Using hearst?s rules for
the automatic acquisition of hyponyms for mining a
pharmaceutical corpus. In Proceedings of the Work-
shop Text Mining Research.
Adam Przepio?rkowski, Lukasz Dego?rski, Beata
Wo?jtowicz, Miroslav Spousta, Vladislav Kubon?,
Kiril Simov, Petya Osenova, and Lothar Lemnitzer.
2007. Towards the automatic extraction of defini-
tions in slavic. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing (in
ACL ?07), pages 43?50, Prague, Czech Republic.
Association for Computational Linguistics.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
Horacio Saggion. 2004. Identifying denitions in text
collections for question answering. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation (LREC 2004), Lisbon,
Portugal.
Antonio Sanfilippo and Victor Poznan?ski. 1992. The
acquisition of lexical knowledge from combined
machine-readable dictionary sources. In Proceed-
ings of the third Conference on Applied Natural Lan-
guage Processing, pages 80?87.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation. In
Proceedings of the European Chapter of the Asso-
ciation for Computation Linguistics (EACL 2009),
pages 719?727, Athens, Greece.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of Advances in Neural
Information Processing Systems, pages 1297?1304.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
german text corpora. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation (LREC 2006), Genova, Italy.
Paola Velardi, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent Systems,
23(5):18?25.
Eline Westerhout and Paola Monachesi. 2007. Extrac-
tion of dutch definitory contexts for eLearning pur-
poses. In Proceedings of CLIN.
Eline Westerhout. 2009. Definition extraction using
linguistic and structural features. In Proceedings
of the RANLP 2009 Workshop on Definition Extrac-
tion, pages 61?67.
Chunxia Zhang and Peng Jiang. 2009. Automatic ex-
traction of definitions. In Proceedings of 2nd IEEE
International Conference on Computer Science and
Information Technology, pages 364?368.
Zhao-man Zhong, Zong-tian Liu, and Yan Guan. 2008.
Precise information extraction from text based on
two-level concept lattice. In Proceedings of the
2008 International Symposiums on Information Pro-
cessing (ISIP ?08), pages 275?279, Washington,
DC, USA.
1327
