R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 588 ? 599, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Why Is Zero Marking Important in Korean? 
Sun-Hee Lee1,*, Donna K. Byron2, and Seok Bae Jang3 
1,2 395, Dreese Lab., 2015, Neil Avenue,Columbus, OH 43210 
shlee@ling.ohio-state edu 
dbyron@cse.ohio-state.edu 
3 37th and O Sts., NW, Washington, D.C, 20057 
sbj3@georgetown.edu 
Abstract. This paper argues for the necessity of zero pronoun annotations in 
Korean treebanks and provides an annotation scheme that can be used to de-
velop a gold standard for testing different anaphor resolution algorithms. Rele-
vant issues of pronoun annotation will be discussed by comparing the Penn Ko-
rean Treebank with zero pronoun mark-up and the newly developing Sejong 
Teebank without zero pronoun mark-up. In addition to supportive evidence for 
zero marking, necessary morphosyntactic and semantic features will be sug-
gested for zero annotation in Korean treebanks.   
1   Introduction 
This paper discusses the importance of zero pronoun marking in treebanks and investi-
gates what kind of linguistic features are needed for treebank annotation in order to 
increase the usability of annotated corpora. Zero pronouns refer to empty pronouns 
without phonological realization, which work in a similar manner as English pronouns.  
In the recent decade, there has been remarkable progress in the realm of building 
large corpora in Korean and applying them for linguistic research and natural lan-
guage processing. Based on the broad acknowledgement of the importance of corpus 
and applicative tools, the 21st century Sejong project was launched in 1998 and has 
been developing various database and relevant computational tools including elec-
tronic dictionaries, annotation tools, morphological analyzers, parsers, etc. As a part 
of the Sejong project, the syntactically annotated treebank of Korean has been under 
construction. In addition to the Sejong Treebank (henceforth, ST), the Penn Korean 
Treebank (Han et al[4] henceforth, PKT) has already been released and continues to 
be expanded. These treebanks with abundant linguistic information are expected to 
fulfill a function as informative databases in broad domains of theoretical linguistics 
and computational linguistics such as statistical approaches, machine learning, etc. 
A notable point is that there is a critical difference between annotations of ST and 
PKT with respect to marking zero elements including traces, zero pronouns, etc. The 
most current guidelines of ST specify that zeros are dropped in order to maintain the 
                                                          
*
  This work was supported by the Korea Research Foundation Grant (KRF-2004-037-A00098) 
for the author. 
 Why Is Zero Marking Important in Korean? 589 
consistency and efficiency of the treebank. In contrast, PKT advocates for  
representing zero elements. According to different approaches to zero marking, the 
structure of the following sentence (1a) is differently analyzed as in (1b) and (1c); in 
(1b) ST does not contain any missing subject while PKT marks the missing subject 
and object as pros in (1c)1 
(1) a. ???         12?-?      ?-?-???.  
          eceypam        12 si-ey       pat-ass-supnita.  
          last night        12 o?clock-at   receive-Past-E 
         ?Last night at 12 o?clock, (I/(s)he/they) received (it)?.   
      b. ST:  (VP   (AP    ???/MAG)  
                            (VP  (NP_AJT 12/SN + ?/NNB + ?/JKB) 
                                     (VP ?/VV+?/EP+???/EF.+ ./SF ))) 
      c. PKT :(S   (NP-SUBJ  *pro*) 
                           (VP  (NP-ADV ???/NNC) 
                                   (NP-ADV 12/NNU ?/NNX+?/PAD) 
                                   (VP  (NP-OBJ  *pro*) 
                                           ?/VV+?/EPF+???/EFN))) ./SFN). 
The sentence representation of (1b) does not fully present the subject-predicate re-
lation in contrast with (1c). In this paper, we argue that failure to mark zeros may 
cause a loss of valuable linguistic information such as filler-gap dependencies, argu-
ment-predication relations, semantic and discourse interpretations of sentences, etc. 
The ST style zero-less annotation will impose the burden of zero marking on the post-
annotation tasks, which utilize treebank resources for developing computational tools. 
This, however, is inconsistent with the purpose of developing treebanks. As pointed 
out in Dickinson & Meurers [3], treebanks have major usage for two types of lin-
guists; one is for theoretical linguists who search through the corpora in order to iden-
tify certain linguistic patterns. The other is for computational linguists who use com-
putational technology and develop statistical models from the annotated corpora in 
order to develop parsers and question-answer systems and to extract information such 
as subcategorization frames of predicates, event nouns, complex predicates, etc. In 
general, treebanks are manually or semi-manually annotated by humans. This guaran-
tees more sophisticated representations of sentence structure and reliable mark-ups for 
ambiguous morphosyntactic units. While focusing on the usability of treebanks, we 
propose an argument against dropping zero mark-ups in treebanks and investigate the 
empirical necessity of zero annotation in Korean treebanks.  
In this paper, we will discuss some significant problems of zero-less treebank 
annotation and explain why zero annotation is important in languages like Korean. 
Then we will present a general annotation scheme and features of zero pronouns 
that can be used to develop a gold standard for testing an anaphor resolution algo-
rithm. Adding zero mark-up will solidify accurate syntactic representation and in-
crease the usability of treebanks even though it takes strenuous efforts and time  
for development. 
                                                          
1
  The tagsets of ST and PKT are somewhat different (i.e., MAG represents an adverb, AP, an 
adverbial phrase, and NP-ADV , a nominal functioning as an adverbial modifier in ST). 
590 S.-H. Lee, D.K. Byron, and S.B. Jang 
2   Necessity of Zero Annotation in Korean Treebank 
In contrast with English where a repeated element tends to appear as a pronoun, in 
topic prominent languages like Korean, a repeated element has no surface realization. 
Thus, Korean zero elements are often called zero pronouns. 
(2) a. John met Mary yesterday.       
      b. Kim met her, too.  
(3) a. John-i           eycey          Mary-lul    mannassta.  
          John-Nom   yesterday  Mary-Acc  met 
         ?John met with Mary yesterday. . 
      b. Kim-to       ?       ,mannaassta.   
          Kim also    OBJ    met 
          ?Kim also met (zero=her).? 
The discrepancy between ST and PKT with respect to zero annotation brings us 
two different values with respect to corpus annotation; economy vs. usability. At the 
stage of annotating corpora, excluding all the missing subjects from the Korean tree-
banks may reduce the burden of annotation tasks such as classifying zeros, sorting 
markable zeros, training annotators and maintaining the legitimate level of inter-
annotator agreement. However, at the later stage zero marked treebanks have higher 
usability by higher level processing including anaphor resolution, extracting subcate-
gorization frames of predicates, discourse analysis, etc. 
More specifically, our arguments against zero-less treebanks can be presented as 
follows. First, zero-less treebanks may provide misleading representations with re-
spect to the general patterns of sentence realization. In so-called pro-drop languages 
such as Korean, Japanese, Spanish and Portuguese, basic units of sentence structure, 
such as subjects of matrix clauses, are frequently unrealized. Although missing sub-
ject information in languages like Spanish and Portuguese is recoverable from verb 
morphology, interpretations of missing arguments do not correspond to specific verb 
morphology in Korean. Thus, marking the place of a zero element is an inevitable 
process not only for structural representation but for processing the meaning of a 
sentence. Zero-less treebanks license various VP or S nodes without capturing correct 
argument-predicate relations. For example, the following sentence is simply repre-
sented as VP, which is inconsistent with the subcategorization frame of the main verb. 
(4) ??        ???     ???-?             ???-?  ?-?-???.  
       kunyang camcakho wuntongcang-man naytapo-ko  iss-ess-supnita.  
       just         silently      playground-only    look down-PreP-Past-E  
       ?(I/you/he/she/they) was only looking down the playground just silently.? 
     (VP  (AP ??/MAG)  
            (VP   (AP ???/MAG)  
                      (VP   (NP_OBJ ???/NNG + ?/JX)  
                               (VP   (VP ???/VV + ?/EC)  
                                         (VP ?/VX + ?/EP + ???/EF + ./SF))))) 
According to Hong [6], the rate of subject drop is 57% in spoken Korean, which is 
higher than other elements. In particular, when the subject refers to a nominal entity 
mentioned in the previous utterance, it naturally disappears in speech rather than ap-
 Why Is Zero Marking Important in Korean? 591 
pearing as a pronoun. This suggests that the number of VPs lacking subjects will be 
significantly high in the spoken corpora. We extracted only 100 sentences from the 
ST corpus containing natural spoken conversations and found that 81 sentences are 
represented as VPs or VNPs (predicate nominal phrases). However, it may derive a 
misleading generalization such that canonical sentence patterns in the given corpus 
are VPs or VNPs. In line with this, semantic interpretations of those incomplete VPs 
or VNPs subsume the meaning of the zero pronouns whose antecedents appear in the 
previous utterances. However, zero-less mark-up poses a difficulty in retrieving the 
complete sentential meaning from the given phrasal categories of VPs or VNPs.  
Second, zero-less treebanks make it difficult to extract certain constructions that 
linguists want to identify. For example, one of the most frequently discussed topics in 
Korean grammar is formation of Double Subject Constructions (DSCs), which license 
two subjects. However, zero-less treebanks do not correctly represent Double Subject 
Constructions and represent (5) and (6) differently in spite of their similarity in  
argument realization.  
(5) ??-?                   ?-?         ?       ?-?-???.  
      hayspam-i                mas-i          ssek    choh-ass-supnita.  
      new chestnut-Nom  taste-Nom   quite   good-Past-End 
      ?New chestnuts had pretty good taste.?  
    (S  (NP_SUB ??/NNG + ?/JKS)  
          (S  (NP_SBJ ?/NNG + ?/JKS)  
                (VP  (AP ?/MAG)  
                        (VP ?/VA + ?/EP + ???/EF + ./SF))))  
(6)  ???             ?-?        ?       ?-?-???.  
       yunanhi            mas-i         ssek    choh-ass-supnita.  
       particularly      taste-Nom  quite   good-Past-End 
     ?Particularly, the taste of (it) was pretty good.?  
    (S  (AP  ???/MAG) 
          (S (NP_SBJ ?/NNG + ?/JKS)  
                 (VP  (AP ?/MAG)  
                         (VP ?/VA + ?/EP + ???/EF + ./SF)))) 
According to the analysis of ST, (5) is represented as a DSC that licenses two sub-
jects, hayspam and mas. In contrast, (6) is represented as a complex clause that only 
licenses a single matrix subject, mas and the first zero subject referring to the same 
nominal entity in the preceding phrase has been ignored in the sentential representa-
tion. It is difficult to extract certain syntactic patterns from the zero-less treebanks 
because their structural representations do not reflex the accurate argument-predicate 
realization. It is because they focus on surface realization of arguments instead of 
considering lexical constraints of argument-predicate relations.   
The third critical problem of zero-less treebanks is related to discourse analysis. 
Unrealized arguments are important for tracking the attentional state of a discourse in 
topic-oriented languages like Korean and Japanese. Within the framework of center-
ing theory, e.g. Walker et al [9], Iida [7]), Hong [6], etc. it has been shown that a 
salient entity recoverable by inference from the context is frequently omitted, and 
therefore interpreting these zero pronouns allows one to follow the center of the atten-
tional state. Walker et al [9] applied the centering model, developed for pronoun 
592 S.-H. Lee, D.K. Byron, and S.B. Jang 
resolution in English, to zero pronoun resolution in Japanese. They argue that inter-
pretation of a zero pronoun is determined by discourse factors. This suggests that 
identifying occurrences of zero pronouns and retrieving their antecedents are impor-
tant in developing a computational model of discourse interpretations as well as syn-
tactic and semantic analyses. When it comes to topic information retrieval, the salient 
element under the discussion of the given discourse is realized as a zero. Grammatical 
roles and semantic restrictions provide crucial cues for the interpretations of them. 
However, without specifying the argument positions of these zeros, discourse proc-
essing of the given utterances is impossible. 
3   Relevant Issues of Zero Annotation 
Zero marked treebanks function as useful resource for researchers, especially the 
anaphora resolution community. For developing computational tools of anaphor reso-
lution, it is necessary to determine the distribution of zero pronouns and their link to 
other discourse properties. There has historically been a lack of annotated material 
available to the wider research community that would allow us to investigate these 
questions. Researchers in the past worked mainly with small amounts of hand-
constructed data rather than being able to do large-scale corpus analysis. This lack has 
been recently pointed out by Lee et al [8] evaluating the Penn Korean Treebank (Han 
et. al. [4]), which includes annotations indicating the position of zero pronouns. In 
PKT, annotations of zeros are problematic due to inconsistent mark-up for zero pro-
nouns and structural representation of trees. Inconsistent annotation of zero pronouns 
in PTK brings an imminent issue for developers of Treebanks and other annotated 
language resources; when and how should these unrealized elements be explicitly 
introduced into the linguistic material being developed? Unless these questions are 
resolved, treebanks cannot fulfill their potential as a source of linguistic knowledge 
about zero pronouns. Also, the same question should be taken into consideration by 
other teams developing similar resources in other languages.  
3.1   Argument vs. Adjunct 
Previous authors have pointed out that the antecedents of zero pronouns can often be 
determined by using various grammatical properties such as topicality, agreement, 
tense, and aspect as well as subcategorization information (Walker et al [9]; Iida [7]; 
Hong [6],  etc.). However, in order for these factors to be useful in developing anaph-
ora resolution algorithms, they must be reliably and consistently annotated into the 
source data. Thus, the first crucial step for zero pronoun resolution is identifying the 
exact positions of zero pronouns. Determining the positions of invisible zeros is a 
difficult task. This process needs to refer to the argument realization in a given utter-
ance and the previous utterances of the same discourse unit. The argument realization 
of a sentence is based upon argument structure of a predicate.  
(7) a.  John-i  .       kesil-eyse           swi-ko  iss-ess-ta? 
          John-Nom    living room-in    rest-Pres Prog-Past-E 
         ?John was resting in the living room.? 
      b.  sakwa-lul  mek-ess-ta.  
           apple-Acc  eat-past-E 
          ?(He) ate an apple.? 
 Why Is Zero Marking Important in Korean? 593 
In (7b), the argument structure of mekta ?eat? suggests that the subject is missing in 
a sentence sakwalul mekesse ?ate an apple?. However, do we need to mark the adjunct 
kesileyse  ?in the living room? in (7b)? In the given utterances, it seems to be possible 
for John to have eaten the apple in the living room but it is not necessarily true. The 
combinations of adjunct and predicate are not predictable by using argument structure 
of a predicate. With no specific guideline, identifying missing adjuncts complicates 
the annotation process. Thus, we argue that only missing arguments must be marked. 
As for zero argument annotation, the current annotation in PKT is somewhat prob-
lematic due to unclear distinction of obligatory argument vs. optional argument. Ac-
cording to the guidelines of PKT, only a missing obligatory argument should be anno-
tated as an empty element. Missing optional arguments and adjuncts are not. Thus, in 
PKT, missing subject or object elements were marked as zeros while missing locative 
arguments were not marked when they were omitted. However, the annotation 
method based on an obligatory vs. optional argument may result in the loss of crucial 
information needed at later stage of retrieving an antecedent of a zero element. For 
example, the locative argument, ?the 45th division-in? has not been marked up as a 
zero pronoun in the tagged sentence of (8b)  
(8) a: ?     45   ???  ?     ??-??  ????  ??? ? 
          the   45   division  again what-with   composed  be  
            ?What is the 45th Division composed of ??  
        (S  (NP-SBJ ?/XPF+45/NNU 
                            ??/NNC+?/PAU) 
              (VP (VP (ADVP ?/ADV) 
                              (VP (NP-COMP ??/NPN+??/PAD) 
                                      (VV ??/NNC+?/XSV+?/EAU))) 
                                         ?/VX+??/EFN)  ?/SFN) 
         b: ??          ???-?     ???? . 
              division    head-Nom      exist 
             ?The head division is (there). 
        (S (NP-SBJ ??/NNC 
                            ???/NNC+?/PCA) 
              (ADJP ?/VJ+???/EFN).  /SFN) 
In the given discourse segments, the adjective ?? issta requires a locative argu-
ment which has been treated as an optional argument in PKT. Thus, the information 
of the missing locative has not been represented even though it is crucial for retriev-
ing the meaning of the sentence. Another concern with respect to distinction of argu-
ment and adjunct is that ST classifies only subject and object as arguments and ex-
cludes other case-marked nominals as adjunct2. This classification may cause prob-
lems when zero pronouns are added in their treebanks.  
In identifying missing zero arguments, maintaining consistency is crucial. For this 
task, we can rely on a dictionary containing constant argument structure of predicates. 
Dictionaries with specific argument structure information can be used here, such as 
                                                          
2
  In addition to subject and object, nominals in front of predicates, toyta, anita and quotation 
clause have been included as arguments.  
594 S.-H. Lee, D.K. Byron, and S.B. Jang 
the Yonsei Korean dictionary, where different subcategorization frames are listed 
according to semantically disambiguated senses for each predicate. For correct identi-
fication of a zero pronoun in the given utterance, annotators need to examine the rele-
vant previous utterances in the same discourse unit and determine the exact verb sense 
of the relevant predicate by using the dictionary. In addition, checking inter-annotator 
agreement is also an essential task (Carletta [2]).  
3.2   Language Specific Properties of Korean 
Another notable point is that the developers need to pay attention to language specific 
properties of Korean. There are some notable morphosyntactic properties of Korean 
with respect to zero pronoun annotation. In order to maintain constant annotation of 
zero pronouns, it is important to carefully represent specific features related to zero 
pronouns. In this section, we will discuss specific properties that can be added for 
zero pronoun annotation. It will increase the applicability of the treebank to both 
theoretical research on anaphors and computational modeling of anaphor resolution. 
[1]   CASE MARKING  
In determining an antecedent of a zero pronoun, the existence of topics plays an im-
portant role in Korean. In the previous theoretical literature, it has been commonly 
assumed that the topic marked elements appear at the higher phrasal level than the 
phrasal combination of subject-predicate. At the discourse level, Walker et al [9] and 
Iida [7] provide evidence that topic marked elements function as antecedents of zero 
pronouns in Japanese. The similar property has been also observed with Korean by 
Hong [6]. As seen in the following examples, the sentence-initial topic functions as 
the antecedent of zero pronouns that appear in the latter utterances. This phenomenon 
suggests that the topic marker needs to be differentiated from other postpositions and 
that grammatical topics are to be differentiated from other grammatical arguments 
like subjects and objects.  
(9) a  Seyho-nun apeci-eykey   chingchan-ul pat-ca    ekkay-ka ussukhayci-pnita.  
          Seyho-Top  father-to        praise-Acc     receive-E be proud of 
        ?As for Seyho, he felt pride when he received praise from father.?  
      b. ?  20 ilman-ey  tut-nun   chingchan-ila kippum-un   hankyel tehaysssupnita  
               20days-in    hear-Rel  praise-since    pleasure-Top  far         more  
       ?Since it was the praise (he) heard for the first time in 20 days, his pleasure     
          was much more.?  
      c. ?  emeni-eykey-nun  nul         kkwucilam-kwa   cansoli-man   tulesssupnita.    
              mother-to-Top      always    scolding-and        lecture-only    heard 
       ?From his mother, (he) always heard only scolding and lecture.? 
In general, while the marker nun functions as a topic marker in a sentence initial 
position, it also works as an auxiliary postposition in a non-initial position of a sen-
tence in Korean. The first is classified as a grammatical topic marker while the latter 
is a contrastive topic marker in traditional Korean grammar. However, the current 
annotations of PKT and ST treat topic marker nun as the same auxiliary postposition, 
which is similar to other postpositions man ?only?, to ?also?, and mace ?even?. In par-
ticular, PKT and ST represent a subject NP with a topic marker as the subject, while 
 Why Is Zero Marking Important in Korean? 595 
an object with a topic marker is treated as a scrambled argument out of its canonical 
position. With respect to zero pronouns, the sentence initial topic marker needs to be 
distinctly marked from other postpositions. In addition, we claim that the structural 
position of a topicalized subject needs to be differentiated from a normal subject posi-
tion in parallel with a topicalized object and other element, which leave zero traces in 
their original positions. 
Another problem with case markers is subject marker -eyse, which only combines 
with nominals referring to a group or organization. In Korean, these group nominals 
do not take the nominative case i/ka but the case marker -eyse as in (12)  
(10) wuli hakkyo-eyse  wusung-ul         hayssta.  
        our school-Nom    winning-Acc     did 
        ?Our school won.? 
Although -eyse has been treated as a nominative case marker in traditional Korean 
grammar, both PKT and ST do not treat -eyse as a nominative marker. Instead, group 
or organization nominals with the case marker-eyse  are analyzed as NP adverbial 
phrases. This, however, mistakenly licenses a zero subject in the following example 
of PKT even though the subject with case marker -eyse exists. In order to eliminate 
redundant marking for the zero subjects, it is better to analyze the case marker, -eyse 
as a nominative case marker in Korean. 
(11) 2 ??-??          ??       ???-?               ???-?      ?-?? 
        2 taytay-eyse         etten        mwucenmang-ul     wunyonha-ko iss-ci? 
        2 squadron-Nom   which      radio network-Acc  use-PresP-Q 
       ?What kind of radio network is the 2nd squadron using??             
       (S  (NP-ADV  2/NNU 
                             ??/NNC+??/PAD) 
       (S     (NP-SBJ *pro*) 
                (VP (VP (ADVP ?/ADV) 
                                (VP (NP-OBJ ??/DAN 
                                                      ???/NNC+?/PCA) 
                                        (VV ??/NNC+?/XSV+?/EAU))) 
                                ?/VX+?/EFN) ?/SFN) 
[2]   SUBJECTLESS CONSTRUCTIONS  
Unlike English having expletive pronouns it or there, certain predicate constructions 
do not license subject positions at all in Korean. Some examples are presented in 
(12),which include incomplete predicates with few inflectional forms.  
(12) -ey tayhaye, ?regarding on?-ey kwanhaye, ?about? -ey uyhaye, ?in terms of?  
        -lo inhayse  ?due to~?, -wa tepwule  ?with ~? etc.  
In addition, some modal auxiliary verbs like sangkita, poita, toyta, etc. do not license 
subject positions and have already been classified as subjectless constructions in Ko-
rean grammar. While ST treats these modal verbs to be included in the preceding 
verbal clusters, PKT separates them from the preceding verbs and assigns zero sub-
jects for these verbs. Thus, the PKT approach redundantly assigns zero subjects for 
subjectless predicates.  
596 S.-H. Lee, D.K. Byron, and S.B. Jang 
[3]   VERBAL MORPHOLOGY OF SPEECH ACT  
As for zero pronoun resolution, verbal suffixes representing speech act can be a useful 
source. Thus, we argue for adding these morphosyntactic features in treebanks. It has 
been well known that in Korean certain speech acts such as declaration, request, ques-
tion, promise, etc. are associated with verb morphology; five different types of verbal 
inflections are used to indicate declaratives, interrogatives, imperatives, propositives, 
and exclamatives. Information of a missing subject can be retrieved from verbal mor-
phology. For example, the imperative verbal endings suggest that a missing subject 
refers to the hearer while promising verbal endings imply that a missing subject is the 
speaker. Thus, the missing subjects of the following examples are respectively inter-
preted as I, you and we based on the verbal suffixes representing a particular speech act.    
(13) a. ?   ka-llay. (Question) 
                 go-Q 
           ?Do (you) want to go?? 
        b. ?   ka-llay. (Declaration) 
                  go-will 
            ?(I) will go.?   
        c. ?    ka-ca. (Request) 
                   go-let?s 
             ?Let?s go.? 
Verbal endings of speech acts can be used to enhance the process of determining 
an antecedent of a zero pronoun subject. In the current annotations of PKT and ST, 
verbal suffixes do not subclassify the final endings. We argue that annotating the five 
classes of verbal suffixes differently will facilitate application of anaphor resolution 
algorithms on treebanks.   
[4]   WH-PRONOUN TAGGING  
Wh-pronouns in Korean include nwuka ?who?, mwues ?what?, encey ?when?, etise 
?where?, way ?why?, ettehkey ?how?, etc. Unfortunately, wh-pronouns are not dis-
tinctly tagged from other pronouns in the PKT and the ST. The information of wh-
pronouns can be useful for resolving the meaning of zero pronouns in the next an-
swering utterance As seen in (14b), a fragment directly related to a wh-pronoun nec-
essarily appears in the answering utterance while non-wh-elements previously men-
tioned are easily dropped.  This is because pairs of wh-question-answer tend to have 
the same predicates with the same argument structure. Therefore, answering utter-
ances of the wh-questions generally contain zero pronouns, whose antecedents appear 
in the preceding questioning utterances. 
(14) A:  John-i            Min-ul         mwe-la-ko               mitko  iss-ni?  
              John-Nom     Min-Acc      what-Co-Comp       believe being-Q 
             ?What does John believe Min to be?? 
        B:  ?          ?             kyoswu-la-ko          mitko   iss-nuntey.  
             SUBJ    OBJ         professor-Co-Comp   believe  being-END 
            ?(He) believes (her) to be a professor.?   
 Why Is Zero Marking Important in Korean? 597 
4   New Annotation Scheme of Korean Zero Pronouns  
Once zeros are identified by argument structure information of predicates and the 
previous utterances in the given discourse, the additional reference information can be 
added in treebanks to support anaphor resolution. Zeros in Korean can be classified 
into different classes according to properties of their reference. Anaphor resolution 
algorithms can be applied for certain types of pronouns. For example, in order to 
retrieve the meaning of a zero pronoun referring to a nominal entity in the previous 
utterance, the resolution algorithm will search nominal entities that appear in the pre-
vious utterance by making a list of antecedent candidates and selecting the most ap-
propriate candidate. In contrast, the searching algorithm does not need to apply for a 
zero element referring to an indefinite entity as in (15). 
(15) ?   holangi    kwul-ey      ka-ya    holangi-lul      capnunta.  
                  tiger         den-to         go-E     tiger-Acc        catch 
 (Lit.)    ?One should go to the tiger?s den in order to catch a tiger.  
     (Trans.)?Don?t hesitate but pursue what you need to do.? 
According to reference relation between a zero pronoun and its antecedent, zero 
pronouns in Korean can be divided into three classes as in Table 1; discourse ana-
phoric zeros, deictic and indexical zeros, and indefinite zeros. 
In the given classification, discourse anaphoric zeros take their reference from an-
tecedents in the previous utterances in the given discourse. This class is the main one 
that anaphor resolution systems aim to handle. The discourse anaphoric zeros can be 
divided into three subclasses according to the semantic properties of their antecedents. 
Table 1. Classification of Korean Zero Pronouns 
     Individual Entities 
Propositions 
 
Discourse Anaphoric Zeros 
Eventualities  
Deictic and Indexical Zeros 
Indefinite Zeros 
The first subclass of discourse anaphoric zeros refers to individual domain entities, 
the second, eventualities, and the third, propositions. The zeros of individual entities 
refer to entities that were introduced into the discourse via noun phrases. Most exam-
ples presented in the previous sections correspond to this class. The zeros of proposi-
tions refer to propositions introduced in the previous utterance as in (16). 
 (16) A: 108 yentay   cihwipwu-nun    hyencay eti-ey wichihako issnun-ka? 
             108 regiment   headquarter-TOP  now  where-at locate   being-Q 
             ?Where is the headquarter of the 108th regiment located?? 
        B:  ?1      ?2      molukeyss-supnita.  
              SUBJ   OBJ     not know-END 
             ?I don?t know.?    
      (?1 = ?B?, ?2= ?Where the headquarter of the 108th regiment is located ?) 
598 S.-H. Lee, D.K. Byron, and S.B. Jang 
The third class of zero anaphors referring to eventualities, i.e. action and event as 
in (17) (Asher [1]).    
 (17) A: Mary-ka              cip-ey -   ka-ko          sipheha-ci  anha.  
             Mary-Nom          home-to go-E           want-END  don?t 
            ?Children don?t want to go home.? 
        B: na-to      ?          silhe.       
             I-also                  hate 
            ?I also hate to go home.?       ( ? = the action of going home) 
The second class of zero pronouns includes deictic and indexical zeros that directly 
refer to entities that can be determined in the given spatiotemporal context, which 
generally include a speaker and an addressee. The third class includes indefinite zeros 
referring to general people, which corresponds to they, one, and you in English.  
Given the classification of zero pronouns, different coding systems can be provided 
for each class for annotating these elements. According to different classes of zeros, the 
resolution process varies. Zero anaphors of discourse anaphoric entities will be marked 
the same as their antecedents in the previous utterances. Anaphor resolution algorithms 
determine the antecedent of a zero anaphor by searching through the antecedent candi-
dates in different orders. Deictic and indexical zeros are dependent on discourse partici-
pants. In general, a zero anaphor can also refer to the speaker or the hearer. Overlapping 
mark-up for these zeros need to be allowed although resolution mechanisms for deictic 
and indexical zeros are different from those for anaphors. Indefinite zeros need to be 
marked but anaphor resolution algorithms do not need to be applied to them. 
5   Conclusion 
In this paper, we discussed why zero marking is necessary for Korean treebanks and 
how invisible zeros can be consistently marked in annotated corpora like treebanks. The 
importance of zero mark-up in Korean treebanks has been discussed with respect to 
correct linguistic analysis and efficient application of computational process. We also 
claimed that only missing arguments are marked as zeros and a dictionary like Yonsei 
Dictionary with full specification of argument-predicate relations can be a useful source 
for the annotation task. By examining PKT and the newly developing ST, we deter-
mined four linguistic features that are useful for anaphor resolution in Korean; case 
marking, subjectless construction, verb morphology of speech acts and wh-pronoun 
tagging. In addition, we provided a new annotation scheme that can be utilized for anno-
tating treebanks and testing anaphor resolution algorithms with annotated corpora.  
References  
1. Asher, N.:. Reference to Abstract Objects in Discourse. Kluwer Academic Publishers. 
(1993). 
2. Carletta, J. Assessing Agreement on Classification Tasks: the Kappa Statistic, Computa-
tional Linguistics 22(2) (1996) 249-254. 
3. Dickinson, M. and Meurers, D.: Detecting Inconsistencies in Treebanks in Proceedings of 
the Second Workshop on Treebanks and Linguistics Theories.(TLT 2003)..V?xj?.  
Sweden. (2003) 
 Why Is Zero Marking Important in Korean? 599 
4. Han, C-H., Han, N-R., Ko, E-S.and Palmer, M.: Development and Evaluation of a Korean 
Treebank and Its Application to NLP.in Proceedings of the 3rd International Conference on 
Language Resources and Evaluation (LREC).(2002) 
5. Han, N-R.: Korean Null Pronouns: Classification and Annotation in Proceedings of the 
ACL 2004 Workshop on Discourse Annotation. (2004) 33-40. 
6. Hong M.: Centering Theory and Argument Deletion in Spoken Korean. The Korean Jour-
nal  Cognitive Science. Vol. 11-1 (2000) 9-24. 
7. Iida, M.: Discourse Coherence and Shifting Centers in Japanese texts in Walker, M., Joshi 
A.K., Prince E.F. (Eds.) Centering Theory in Discourse. Oxford University Press, Oxford: 
UK..(1998) 161-182. 
8. Lee, S., Byron, D., and Gegg-Harrison, W.: Annotations of Zero Pronoun Resolution in 
Korean Using the Penn Korean Treebank in the 3rd Worksop on Treebanks and Linguistics 
Theories (TLT 2004). T?bingen. Germany. (2004)  75-88. 
9. Walker, M., Iida, M., .Cotes, S.: Japanese Discourse and the Process of Centering in Com-
putational Linguistics, Vol. 20-2.: (1994.) 193-232 
10. 10. Dictionary   Yonsei Korean Dictionary. (1999) Dong-A Publishing Co. 
11. Guidelines of the Sejong Treebank. Korea University 
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 157?160,
New York, June 2006. c?2006 Association for Computational Linguistics
Sentence Planning for Realtime Navigational Instructions
Laura Stoia and Donna K. Byron and
Darla Magdalene Shockley and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
2015 Neil Ave., Columbus, Ohio 43210
stoia|dbyron|shockley|fosler@cse.ohio-state.edu
Abstract
In the current work, we focus on systems that
provide incremental directions and monitor
the progress of mobile users following those
directions. Such directions are based on dy-
namic quantities like the visibility of reference
points and their distance from the user. An
intelligent navigation assistant might take ad-
vantage of the user?s mobility within the set-
ting to achieve communicative goals, for ex-
ample, by repositioning him to a point from
which a description of the target is easier to
produce. Calculating spatial variables over a
corpus of human-human data developed for
this study, we trained a classifier to detect con-
texts in which a target object can be felici-
tously described. Our algorithm matched the
human subjects with 86% precision.
1 Introduction and Related Work
Dialog agents have been developed for a variety of
navigation domains such as in-car driving directions
(Dale et al, 2003), tourist information portals (John-
ston et al, 2002) and pedestrian navigation (Muller,
2002). In all these applications, the human partner
receives navigation instructions from a system. For
these domains, contextual features of the physical
setting must be taken into account for the agent to
communicate successfully.
In dialog systems, one misunderstanding can of-
ten lead to additional errors (Moratz and Tenbrink,
2003), so the system must strategically choose in-
structions and referring expressions that can be
clearly understood by the user. Human cognition
studies have found that the in front of/behind axis
is easier to perceive than other relations (Bryant et
al., 1992). In navigation tasks, this suggests that de-
scribing an object when it is in front of the follower
is preferable to using other spatial relations. Studies
on direction-giving language have found that speak-
ers interleave repositioning commands (e.g. ?Turn
right 90 degrees?) designating objects of interest
(e.g. ?See that chair??) and action commands (e.g.
?Keep going?)(Tversky and Lee, 1999). The con-
tent planner of a spoken dialog system must decide
which of these dialog moves to produce at each turn.
A route plan is a linked list of arcs between nodes
representing locations and decision-points in the
world. A direction-giving agent must perform sev-
eral content-planning and surface realization steps,
one of which is to decide how much of the route
to describe to the user at once (Dale et al, 2003).
Thus, the system selects the next target destination
and must describe it to the user. In an interactive
system, the generation agent must not only decide
what to say to the user but also when to say it.
2 Dialog Collection Procedure
Our task setup employs a virtual-reality (VR) world
in which one partner, the direction-follower (DF),
moves about in the world to perform a series of
tasks, such as pushing buttons to re-arrange ob-
jects in the room, picking up items, etc. The part-
ners communicated through headset microphones.
The simulated world was presented from first-person
perspective on a desk-top computer monitor. The
DF has no knowledge of the world map or tasks.
His partner, the direction-giver (DG), has a paper
2D map of the world and a list of tasks to complete.
During the task, the DG has instant feedback about
157
video frame: 00:13:16
00:13:16 ?keep going forward?
video frame: 00:15:12
00:14:05 ?ok, stop?
00:15:20 ?turn right?
video frame: 00:17:07
00:17:19: ?and go through that door
[D6]?Figure 1: An example sequence with repositioning
DG: ok, yeah, go through that door [D9, locate]
turn to your right
?mkay, and there?s a door [D11, vague]
in there um, go through the one
straight in front of you [D11, locate]
ok, stop... and then turn around and look at
the buttons [B18,B20,B21]
ok, you wanna push the button that?s there
on the left by the door [B18]
ok, and then go through the door [D10]
look to your left
there, in that cabinet there [C6, locate]
Figure 2: Sample dialog fragment
the DF?s location in the VR world, via mirroring of
his partner?s screen on his own computer monitor.
The DF can change his position or orientation within
the virtual world independently of the DG?s direc-
tions, but since the DG knows the task, their collab-
oration is necessary. In this study, we are most inter-
ested in the behavior of the DG, since the algorithm
we develop emulates this role. Our paid participants
were recruited in pairs, and were self-identified na-
tive speakers of North American English.
The video output of DF?s computer was captured
to a camera, along with the audio stream from both
microphones. A logfile created by the VR engine
recorded the DF?s coordinates, gaze angle, and the
position of objects in the world. All 3 data sources
were synchronized using calibration markers. A
technical report is available (Byron, 2005) that de-
scribes the recording equipment and software used.
Figure 2 is a dialog fragment in which the DG
steers his partner to a cabinet, using both a sequence
of target objects and three additional repositioning
commands (in bold) to adjust his partner?s spatial
relationship with the target.
2.1 Developing the Training Corpus
We recorded fifteen dialogs containing a total of
221 minutes of speech. The corpus was transcribed
and word-aligned. The dialogs were further anno-
tated using the Anvil tool (Kipp, 2004) to create a
set of target referring expressions. Because we are
interested in the spatial properties of the referents
of these target referring expressions, the items in-
cluded in this experiment were restricted to objects
with a defined spatial position (buttons, doors and
cabinets). We excluded plural referring expressions,
since their spatial properties are more complex, and
also expressions annotated as vague or abandoned.
Overall, the corpus contains 1736 markable items,
of which 87 were annotated as vague, 84 abandoned
and 228 sets.
We annotated each referring expression with a
boolean feature called Locate that indicates whether
the expression is the first one that allowed the fol-
lower to identify the object in the world, in other
words, the point at which joint spatial reference was
achieved. The kappa (Carletta, 1996) obtained on
this feature was 0.93. There were 466 referring ex-
pressions in the 15-dialog corpus that were anno-
tated TRUE for this feature.
The dataset used in the experiments is a consensus
version on which both annotators agreed on the set
of markables. Due to the constraints introduced by
the task, referent annotation achieved almost perfect
agreement. Annotators were allowed to look ahead
in the dialog to assign the referent. The data used in
the current study is only the DG?s language.
3 Algorithm Development
The generation module receives as input a route plan
produced by a planning module, composed of a list
of graph nodes that represent the route. As each sub-
sequent target on the list is selected, content plan-
ning considers the tuple of variables   ID, LOC 
where ID is an identifier for the target and LOC is
the DF?s location (his Cartesian coordinates and ori-
entation angle). Target ID?s are always object id?s
to be visited in performing the task, such as a door
158
 = Visible area(  )
 = Angle to target
	 = distance to target
In this scene:
Distractors = 5

 B1, B2, B3, C1, D1 
VisDistracts = 3 
 B2, B3, C1 
VisSemDistracts = 2 
 B2, B3 
Figure 3: An example configuration with spatial context fea-
tures. The target obje ct is B4 and [B1, B2, B3, B4, C1, D1] are
perceptually accessible.
that the DF must pass through. The VR world up-
dates the value of LOC at a rate of 10 frames/sec.
Using these variables, the content planner must de-
cide whether the DF?s current location is appropriate
for producing a referring expression to describe the
object.
The following features are calculated from this in-
formation: absolute Angle between target and fol-
lower?s view direction, which implicitly gives the in
front relation, Distance from target, visible distrac-
tors (VisDistracts), visible distractors of the same
semantic category (VisSemDistracts), whether the
target is visible (boolean Visible), and the target?s
semantic category (Cat: button/door/cabinet). Fig-
ure 3 is an example spatial configuration with these
features identified.
3.1 Decision Tree Training
Training examples from the annotation data are tu-
ples containing the ID of the annotated description,
the LOC of the DF at that moment (from the VR en-
gine log), and a class label: either Positive or Nega-
tive. Because we expect some latency between when
the DG judges that a felicity condition is met and
when he begins to speak, rather than using spatial
context features that co-occur with the onset of each
description, we averaged the values over a 0.3 sec-
ond window centered at the onset of the expression.
Negative contexts are difficult to identify since
they often do not manifest linguistically: the DG
may say nothing and allow the user to continue mov-
ing along his current vector, or he may issue a move-
ment command. A minimal criterion for producing
an expression that can achieve joint spatial reference
is that the addressee must have perceptual accessi-
bility to the item. Therefore, negative training exam-
ples for this experiment were selected from the time-
periods that elapsed between the follower achiev-
ing perceptual access to the object (coming into the
same room with it but not necessarily looking at it),
but before the Locating description was spoken. In
these negative examples, we consider the basic felic-
ity conditions for producing a descriptive reference
to the object to be met, yet the DG did not produce
a description. The dataset of 932 training examples
was balanced to contain 50% positive and 50% neg-
ative examples.
3.2 Decision Tree Performance
This evaluation is based on our algorithm?s ability
to reproduce the linguistic behavior of our human
subjects, which may not be ideal behavior.
The Weka1 toolkit was used to build a decision
tree classifier (Witten and Frank, 2005). Figure 4
shows the resulting tree. 20% of the examples were
held out as test items, and 80% were used for train-
ing with 10 fold cross validation. Based on training
results, the tree was pruned to a minimum of 30 in-
stances per leaf. The final tree correctly classified
 of the test data.
The number of positive and negative examples
was balanced, so the first baseline is 50%. To incor-
porate a more elaborate baseline, we consider that a
description will be made only if the referent is visi-
ble to the DF. Marking all cases where the referent
was visible as describe-id and all the other examples
as delay gives a higher baseline of 70%, still 16%
lower than the result of our tree.2
Previous findings in spatial cognition consider an-
gle, distance and shape as the key factors establish-
ing spatial relationships (Gapp, 1995), the angle de-
viation being the most important feature for projec-
tive spatial relationship. Our algorithm also selects
Angle and Distance as informative features. Vis-
Distracts is selected as the most important feature
by the tree, suggesting that having a large number
of objects to contrast makes the description harder,
which is in sync with human intuition. We note that
Visible is not selected, but that might be due to the
fact that it reduces to Angle  . In terms of the
referring expression generation algorithm described
by (Reiter and Dale, 1992), in which the description
which eliminates the most distractors is selected, our
1http://www.cs.waikato.ac.nz/ml/weka/
2not all positive examples were visible
159
results suggest that the human subjects chose to re-
duce the size of the distractor set before producing a
description, presumably in order to reduce the com-
putational load required to calculate the optimal de-
scription.
VisDistracts <= 3
| Angle <= 33
| | Distance <=154: describe-id (308/27)
| | Distance > 154: delay (60/20)
| Angle > 33
| | Distance <= 90
| | | Angle <=83:describe-id(79/20)
| | | Angle > 83: delay (53/9)
| | Distance >90: delay(158/16)
VisDistracts > 3: delay (114/1)
Figure 4: The decision tree obtained.
Class Precision Recall F-measure
describe-id 0.822 0.925 0.871
delay 0.914 0.8 0.853
Table 1: Detailed Performance
The exact values of features shown in our deci-
sion tree are specific to our environment. However,
the features themselves are domain-independent and
are relevant for any spatial direction-giving task, and
their relative influence over the final decision may
transfer to a new domain. To incorporate our find-
ings in a system, we will monitor the user?s context
and plan a description only when our tree predicts it.
4 Conclusions and Future Work
We describe an experiment in content planning for
spoken dialog agents that provide navigation in-
structions. Navigation requires the system and the
user to achieve joint reference to objects in the envi-
ronment. To accomplish this goal human direction-
givers judge whether their partner is in an appropri-
ate spatial configuration to comprehend a reference
spoken to an object in the scene. If not, one strategy
for accomplishing the communicative goal is to steer
their partner into a position from which the object is
easier to describe.
The algorithm we developed in this study, which
takes into account spatial context features replicates
our human subject?s decision to produce a descrip-
tion with 86%, compared to a 70% baseline based
on the visibility of the object. Although the spatial
details will vary for other spoken dialog domains,
the process developed in this study for producing de-
scription dialog moves only at the appropriate times
should be relevant for spoken dialog agents operat-
ing in other navigation domains.
Building dialog agents for situated tasks provides
a wealth of opportunity to study the interaction be-
tween context and linguistic behavior. In the future,
the generation procedure for our interactive agent
will be further developed in areas such as spatial de-
scriptions and surface realization. We also plan to
investigate whether different object types in the do-
main require differential processing, as prior work
on spatial semantics would suggest.
5 Acknowledgements
We would like to thank the OSU CSE department for funding
this work, our participants in the study and to M. White and
our reviewers for useful comments on the paper. We also thank
Brad Mellen for building the virtual world.
References
D. J. Bryant, B. Tversky, and N. Franklin. 1992. Internal and
external spatial frameworks representing described scenes.
Journal of Memory and Language, 31:74?98.
D. K. Byron. 2005. The OSU Quake 2004 corpus of two-
party situated problem-solving dialogs. Technical Report
OSU-CISRC-805-TR57, The Ohio State University Com-
puter Science and Engineering Department, Sept., 2005.
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249?
254.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using natural
language generation for navigational assistance. In M. Oud-
shoorn, editor, Proceedings of the 26th Australasian Com-
puter Science Conference, Adelaide, Australia.
K. Gapp. 1995. Angle, distance, shape, and their relationship
to projective relations. Technical Report 115, Universitat des
Saarlandes.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of the 40  Annual Meeting of the Association for
Computational Linguistics (ACL ?02), pages 376?383.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
R. Moratz and T. Tenbrink. 2003. Instruction modes for
joint spatial reference between naive users and a mobile
robot. In Proc. RISSP 2003 IEEE International Conference
on Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on New Methods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
E. Reiter and R. Dale. 1992. A fast algorithm for the generation
of referring expressions. COLING.
B. Tversky and P. U. Lee. 1999. Pictorial and verbal tools for
conveying routes. Stade, Germany.
I. Witten and E. Frank. 2005. Data Mining: Practical machine
learning tools and techniques, 2nd Edition. Morgan Kauf-
mann, San Francisco.
160
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 40?47,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Identifying non-referential it: a machine learning approach incorporating
linguistically motivated patterns
Adriane Boyd
Department of Linguistics
The Ohio State University
1712 Neil Ave.
Columbus, OH 43210
adriane@ling.osu.edu
Whitney Gegg-Harrison & Donna Byron
Department of Computer Science and Engineering
The Ohio State University
2015 Neil Ave.
Columbus, OH 43210
{geggharr,dbyron}@cse.osu.edu
Abstract
In this paper, we present a machine learn-
ing system for identifying non-referential
it. Types of non-referential it are ex-
amined to determine relevant linguistic
patterns. The patterns are incorporated
as features in a machine learning system
which performs a binary classification of
it as referential or non-referential in a
POS-tagged corpus. The selection of rel-
evant, generalized patterns leads to a sig-
nificant improvement in performance.
1 Introduction
The automatic classification of it as either referen-
tial or non-referential is a topic that has been rel-
atively ignored in the computational linguistics lit-
erature, with only a handful of papers mentioning
approaches to the problem. With the term ?non-
referential it?, we mean to refer to those instances
of it which do not introduce a new referent. In the
previous literature these have been called ?pleonas-
tic?, ?expletive?, and ?non-anaphoric?. It is impor-
tant to be able to identify instances of non-referential
it to generate the correct semantic interpretation of
an utterance. For example, one step of this task is to
associate pronouns with their referents. In an auto-
mated pronoun resolution system, it is useful to be
able to skip over these instances of it rather than at-
tempt an unnecessary search for a referent for them,
The authors would like to thank the GE Foundation Faculty
for the Future grant for their support of this project. We would
also like to thank Detmar Meurers and Erhard Hinrichs for their
helpful advice and feedback.
only to end up with inaccurate results. The task of
identifying non-referential it could be incorporated
into a part-of-speech tagger or parser, or viewed as
an initial step in semantic interpretation.
We develop a linguistically-motivated classifi-
cation for non-referential it which includes four
types of non-referential it: extrapositional, cleft,
weather/condition/time/place, and idiomatic, each
of which will be discussed in more detail in Section
2. A subset of the BNC Sampler Corpus (Burnard,
1995) was chosen for our task because of its ex-
tended tagset and high tagging accuracy. Non-
referential it makes up a significant proportion of the
occurrences of it in our corpus, which contains a se-
lection of written texts of various genres, approxi-
mately one-third prose fiction, one-third newspaper
text, and one-third other non-fiction. In our corpus,
there are 2337 instances of it, 646 of which are non-
referential (28%). It appears in over 10% of the sen-
tences in our corpus. The corpus is described in fur-
ther detail in Section 3.
Previous research on this topic is fairly lim-
ited. Paice and Husk (1987) introduces a rule-based
method for identifying non-referential it and Lappin
and Leass (1994) and Denber (1998) describe rule-
based components of their pronoun resolution sys-
tems which identify non-referential it. Evans (2001)
describes a machine learning system which classi-
fies it into seven types based on the type of referent.
Their approaches are described in detail in Section
4. In Section 5 we describe our system which com-
bines and extends elements of the systems developed
by Paice and Husk (1987) and Evans (2001), and the
results are presented in Section 6.
40
2 Classification
The first step is to create a classification system for
all instances of it. Though the goal is the binary clas-
sification of it as referential or non-referential, an
annotation scheme is used which gives more detail
about each instance of non-referential it, since they
occur in a number of constructions. The main types
of non-referential it are taken from the Cambridge
Grammar of the English Language in the section
on ?Special uses of it?, Section 2.5, Huddleston and
Pullum (2002). Five main uses are outlined: extra-
positional, cleft, weather/condition/time/place, id-
iomatic, and predicative. As noted in the Cambridge
Grammar, predicative it seems to be more referen-
tial that the other types of non-referential it. Pred-
icative it can typically be replaced with a demonstra-
tive pronoun. Consider the example: It is a dreary
day. It can be replaced with This with no change in
grammaticality and no significant change in mean-
ing: This is a dreary day. In contrast, replacing the
other types of it with this results in nonsense, e.g.,
*This seems that the king is displeased.
For our purposes, if a particular it can be re-
placed with a demonstrative pronoun and the result-
ing sentence is still grammatical and has no signif-
icant change in meaning, this it is referential and
therefore annotated as referential. The demonstra-
tive pronoun replacement test is not quite perfect
(e.g., *This is a dreary day in Paris), but no such
instances of predicative it were found in the corpus
so predicative it is always classified as referential.
This leaves four types of it, each of which are de-
scribed in detail below. The main examples for each
type are taken from the corpus. See Section 3 for
details about the corpus.
2.1 Extrapositional
When an element of a sentence is extraposed, it is
often inserted as a placeholder in the original posi-
tion of the now extraposed element. Most often, it
appears in the subject position, but it can also ap-
pear as an object. Example (1) lists a few instances
of extrapositional it from our corpus.
(1) a. It has been confirmed this week that politi-
cal parties will no longer get financial sub-
sidies.
b. She also made it clear that Conductive Ed-
ucation is not the only method.
c. You lead life, it seems to me, like some rit-
ual that demands unerring performance.
The extraposed element is typically a subordinate
clause, and the type of clause depends on lexical
properties of verbs and adjectives in the sentence,
see (2).
(2) * It was difficult that X.
It was difficult to X.
* It was clear to X.
It was clear that X.
As (1c) shows, extrapositional it can also appear
as part of a truncated extrapositional phrase as a kind
of parenthetical comment embedded in a sentence.
2.2 Cleft
It appears as the subject of it-cleft sentences. When
an it-cleft sentence is formed, the foregrounded
phrase becomes the complement of the verb be and
the rest of sentence is backgrounded in a relative
clause. The foregrounded phrase in a cleft sentence
can be a noun phrase, prepositional phrase, adjective
phrase, adverb phrase, non-finite clause, or content
clause.
(3) a. It was the military district commander
who stepped in to avoid bloodshed. (noun
phrase)
b. It is on this point that the views of the
SACP and some Soviet policymakers di-
vide. (prepositional phrase)
c. ?Tis glad I am to ?ear it, me lord. (adjective
phrase)
Additionally, the foregrounded phrase can some-
times be fronted:
(4) He it was who ushered in the new head of state.
More context than the immediate sentence is
needed to accurately identify it-cleft sentences.
First, clefts with a foregrounded noun phrase are am-
biguous between cleft sentences (5a) and sentences
where the noun phrase and relative clause form a
constituent (5b).
41
(5) a. A: I heard that the general stepped in to
avoid bloodshed.
B: No, it was the military district comman-
der who stepped in.
b. A: Was that the general being interviewed
on the news?
B: No, it was the military district comman-
der who stepped in to avoid bloodshed.
Due to this ambiguity, we expect that it may be
difficult to classify clefts. In addition, there are dif-
ficulties because the relative clause does not always
appear in full. In various situations the relative pro-
noun can be omitted, the relative clause can be re-
duced, or the relative clause can be omitted entirely.
2.3 Weather/Condition/Time/Place
It appears as the subject of weather and other
related predicates involving condition, time, and
place/distance:
(6) a. It was snowing steadily outside.
b. It was about midnight.
c. It was no distance to Mutton House.
d. It was definitely not dark.
2.4 Idiomatic
In idioms, it can appear as the subject, object, or
object of a preposition.
(7) a. After three weeks it was my turn to go to
the delivery ward at Fulmer.
b. Cool it!
c. They have not had an easy time of it.
2.5 General Notes
Non-referential it is most often the subject of a sen-
tence, but in extrapositional and idiomatic cases, it
can also be the object. Idioms are the only cases
where non-referential it is found as the object of a
preposition.
3 Corpus
The BNC Sampler Corpus (Burnard, 1995) was cho-
sen for its extended tagset and high tagging accu-
racy. The C7 tagset used for this corpus has a unique
Prose fiction 32%
Newspaper text 38%
Other non-fiction 30%
Table 1: Text types in our corpus
# of Instances % of Inst.
Extrapositional 477 20.4%
Cleft 119 5.1%
Weather 69 2.9%
Idiomatic 46 2.0%
Referential 1626 69.6%
Total 2337 100%
Table 2: Instances of it in our corpus
tag for it, which made the task of identifying all oc-
currences of it very simple. We chose a subset con-
sisting of 350,000 tokens from written texts in vari-
ety of genres. The breakdown by text type can be
seen in Table 1.
The two lead authors independently annotated
each occurence with one of the labels shown in Ta-
ble 2 and then came to a joint decision on the fi-
nal annotation. The breakdown of the instances of it
in our corpus is shown in Table 2. There are 2337
occurrences of it, 646 of which are non-referential
(28%). Ten percent of the corpus, taken from all
sections, was set aside as test data. The remaining
section, which contains 2100 instances of it, became
our training data.
4 Previous Research
Paice and Husk (1987) reports a simple rule-based
system that was used to identify non-referential it in
the technical section of the Lancaster-Oslo/Bergen
Corpus. Because of the type of text, the distribution
of types of non-referential it is somewhat limited, so
they only found it necessary to write rules to match
extrapositional and cleft it (although they do men-
tion two idioms found in the corpus). The corpus
was plain text, so their rules match words and punc-
tuation directly.
Their patterns find it as a left bracket and search
for a right bracket related to the extrapositional and
cleft grammatical patterns (to, that, etc.). For the
extrapositional instances, there are lists of words
which are matched in between it and the right
42
Accuracy 92%
Precision 93%
Recall 97%
Table 3: Paice and Husk (1987): Results
Accuracy 79%
Precision 80%
Recall 31%
Table 4: Replicating Paice and Husk (1987)
bracket. The word lists are task-status words (STA-
TUS), state-of-knowledge words (STATE), and a list
of prepositions and related words (PREP), which is
used to rule out right brackets that could potentially
be objects of prepositions. Patterns such as ?it STA-
TUS to? and ?it !PREP that? were created. The
left bracket can be at most 27 words from the right
bracket and there can be either zero or two or more
commas or dashes between the left and right brack-
ets. Additionally, their system had a rule to match
parenthetical it: there is a match when it appears im-
mediately following a comma and another comma
follows within four words. Their results, shown in
Table 3, are impressive.
We replicated their system and ran it on our test-
ing data, see Table 4. Given the differences in text
types, it is not surprising that their system did not
perform as well on our corpus. The low recall seems
to show the limitations of fixed word lists, while the
reasonably high precision shows that the simple pat-
terns tend to be accurate in the cases where they ap-
ply.
Lappin and Leass (1994) and Denber (1998) men-
tion integrating small sets of rules to match non-
referential it into their larger pronoun resolution sys-
tems. Lappin and Leass use two words lists and
a short set of rules. One word list is modal adjec-
tives (necessary, possible, likely, etc.) and the other
is cognitive verbs (recommend, think, believe, etc.).
Their rules are as follows:
It is Modaladj that S
It is Modaladj (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S
NP makes/finds it Modaladj (for NP) to VP
Accuracy 71%
Precision 73%
Recall 69%
Table 5: Evans (2001): Results, Binary Classifica-
tion
It is time to VP
It is thanks to NP that S
Their rules are mainly concerned with extraposi-
tional it and they give no mention of cleft it. They
give no direct results for this component of their
system, so it is not possible to give a comparison.
Denber (1998) includes a slightly revised and ex-
tended version of Lappin and Leass?s system and
adds in detection of weather/time it. He suggests
using WordNet to extend word lists.
Evans (2001) begins by noting that a significant
percentage of instances of it do not have simple
nominal referents and describes a system which uses
a memory-based learning (MBL) algorithm to per-
form a 7-way classification of it by type of refer-
ent. We consider two of his categories, pleonas-
tic and stereotypic/idiomatic, to be non-referential.
Evans created a corpus with texts from the BNC
and SUSANNE corpora and chose to use a memory-
based learning algorithm. A memory-based learn-
ing algorithm classifies new instances on the basis of
their similarity to instances seen in the training data.
Evans chose the k-nearest neighbor algorithm from
the Tilburg Memory-Based Learner (TiMBL) pack-
age (Daelemans et al, 2003) with approximately 35
features relevant to the 7-way classification. Al-
though his system was created for the 7-way classi-
fication task, he recognizes the importance of the bi-
nary referential/non-referential distinction and gives
the results for the binary classification of pleonastic
it, see Table 5. His results for the classification of
idiomatic it (33% precision and 0.7% recall) show
the limitations of a machine learning system given
sparse data.
We replicated Evans?s system with a simplified set
of features to perform the referential/non-referential
classification of it. We did not include features that
would require chunking or features that seemed rel-
evant only for distinguishing kinds of referential it.
The following thirteen features are used:
43
Accuracy 76%
Precision 57%
Recall 60%
Table 6: Replicating Evans (2001)
1-8. four preceding and following POS tags
9-10. lemmas of the preceding and following verbs
11. lemma of the following adjective
12. presence of that following
13. presence of an immediately preceding preposi-
tion
Using our training and testing data with the same
algorithm from TiMBL, we obtained results similar
to Evans?s, shown in Table 6. The slightly higher
accuracy is likely due to corpus differences or the
reduced feature set which ignores features largely
relevant to other types of it.
Current state-of-the-art reference resolution sys-
tems typically include filters for non-referential
noun phrases. An example of such a system is Ng
and Cardie (2002), which shows the improvement
in reference resolution when non-referential noun
phrases are identified. Results are not given for the
specific task of identifying non-referential it, so a di-
rect comparison is not possible.
5 Method
As seen in the previous section, both rule-based
and machine learning methods have been shown to
be fairly effective at identifying non-referential it.
Rule-based methods look for the grammatical pat-
terns known to be associated with non-referential it
but are limited by fixed word lists; machine learning
methods can handle open classes of words, but are
less able to generalize about the grammatical pat-
terns associated with non-referential it from a small
training set.
Evans?s memory-based learning system showed a
slight integration of rules into the machine learning
system by using features such as the presence of fol-
lowing that. Given the descriptions of types of non-
referential it from Section 2, it is possible to create
more specific rules which detect the fixed grammat-
ical patterns associated with non-referential it such
as it VERB that or it VERB ADJ to. Many of these
patterns are similar to Paice and Husk?s, but hav-
ing part-of-speech tags allows us to create more gen-
eral rules without reference to specific lexical items.
If the results of these rule matches are integrated
as features in the training data for a memory-based
learning system along with relevant verb and ad-
jective lemmas, it becomes possible to incorporate
knowledge about grammatical patterns without cre-
ating fixed word lists. The following sections exam-
ine each type of non-referential it and describe the
patterns and features that can be used to help auto-
matically identify each type.
5.1 Extrapositional it
Extrapositional it appears in a number of fairly fixed
patterns, nine of which are shown below. Interven-
ing tokens are allowed between the words in the pat-
terns. F4-6 are more general versions of F1-3 but
are not as indicative of non-referential it, so it useful
to keep them separate even though ones that match
F1-3 will also match F4-6. F7 applies when it is the
object of a verb. To simplify patterns like F8, all
verbs in the sentence are lemmatized with morpha
(Minnen et al, 2001) before the pattern matching
begins.
F1 it VERB ADJ that
F2 it VERB ADJ
what/which/where/whether/why/how
F3 it VERB ADJ to
F4 it VERB that
F5 it VERB what/which/where/whether/why/how
F6 it VERB to
F7 it ADJ that/to
F8 it be/seem as if
F9 it VERB COMMA
For each item above, the feature consists of the
distance (number of tokens) between it and the end
of the match (the right bracket such that or to).
By using the distance as the feature, it is possible
to avoid specifying a cutoff point for the end of a
match. The memory-based learning algorithm can
adapt to the training data. As discussed in Sec-
tion 2.1, extraposition is often lexically triggered,
so the specific verbs and adjectives in the sentence
are important for its classification. For this reason,
it is necessary to include information about the sur-
rounding verbs and adjectives. The nearby full verbs
44
(as opposed to auxiliary and modal verbs) are likely
to give the most information, so we add features for
the immediately preceding full verb (for F7), the
following full verb (for F1-F6), and the following
adjective (for F1-3,7). The verbs were lemmatized
with morpha and added as features along with the
following adjective.
F10 lemma of immediately preceding full verb
F11 lemma of following full verb within current
sentence
F12 following adjective within current sentence
5.2 Cleft it
Two patterns are used for cleft it:
F13 it be who/which/that
F14 it who/which/that
As mentioned in the previous section, all verbs in
the sentence are lemmatized before matching. Like-
wise, these features are the distance between it and
the right bracket. Feature F14 is used to match a
cleft it in a phrase with inverted word order.
5.3 Weather/Condition/Time/Place it
Ideally, the possible weather predicates could be
learned automatically from the following verbs, ad-
jectives, and nouns, but the list is so open that it
is better in practice to specify a fixed list. The
weather/time/place/condition predicates were taken
from the training set and put into a fixed list. Some
generalizations were made (e.g., adding the names
of all months, weekdays, and seasons), but the list
contains mainly the words found in the training set.
There are 46 words in the list. As Denber men-
tioned, WordNet could be used to extend this list.
A feature is added for the distance to the nearest
weather token.
The following verb lemma feature (F10) added
for extrapositional it is the lemma of the follow-
ing full verb, but in many cases the verb following
weather it is the verb be, so we also added a binary
feature for whether the following verb is be.
F15 distance to nearest weather token
F16 whether the following verb is be
5.4 Idiomatic it
Idioms can be identified by fixed patterns. All verbs
in the sentence are lemmatized and the following
patterns, all found as idioms in our training data, are
used:
if/when it come to pull it off
as it happen fall to it
call it a NOUN ask for it
on the face of it be it not for
have it not been for like it or not
Short idiom patterns such as ?cool it? and ?watch
it? were found to overgeneralize, so only idioms in-
cluding at least three words were used. A binary
feature was added for whether an idiom pattern was
matched for the given instance of it (F17). In addi-
tion, two common fixed patterns were included as a
separate feature:
it be ... time
it be ... my/X?s turn
F17 whether an idiom pattern was matched
F18 whether an additional fixed pattern was
matched
5.5 Additional Restrictions
There are a few additional restrictions on the pattern
matches involving length and punctuation. The first
restriction is on the distance between the instance
of it and the right bracket (that, to, who, etc.). On
the basis of their corpus, Paice and Husk decided
that the right bracket could be at most 27 words
away from it. Instead of choosing a fixed distance,
features based on pattern matches are the distance
(number of tokens) between it and the right bracket.
The system looks for a pattern match between it
and the end of the sentence. The end of a sentence
is considered to be punctuation matching any of the
following: . ; : ? ! ) ] . (Right parenthesis or
bracket is only included if a matching left parenthe-
sis or bracket has not been found before it.) If there
is anything in paired parentheses in the remainder of
the sentence, it is omitted. Quotes are not consistent
indicators of a break in a sentence, so they are ig-
nored. If the end of a sentence is not located within
50 tokens, the sentence is truncated at that point and
the system looks for the patterns within those tokens.
45
As Paice and Husk noted, the presence of a sin-
gle comma or dash between it and the right bracket
is a good sign that the right bracket is not rele-
vant to whether the instance of it is non-referential.
When there are either zero or two or more commas
or dashes it is difficult to come to any conclusion
without more information. Therefore, when the to-
tal comma count or total dash count between it and
the right bracket is one, the pattern match is ignored.
Additionally, unless it occurs in an idiom, it is
also never the object of a preposition, so there is
an additional feature for whether it is preceded by
a preposition.
F19 whether the previous word is a preposition
Finally, the single preceding and five following
simplified part-of-speech tags were also included.
The part-of-speech tags were simplified to their first
character in the C7 tagset, adverb (R) and nega-
tive (X) words were ignored, and only the first in-
stance in a sequence of tokens of the same simplified
type (e.g., the first of two consecutive verbs) was in-
cluded in the set of following tags.
F20-25 surrounding POS tags, simplified
6 Results
Training and testing data were generated from our
corpus using the the 25 features described in the
previous section. Given Evans?s success and the
limited amount of training data, we chose to also
use TiMBL?s k-nearest neighbor algorithm (IB1).
In TiMBL, the distance metric can be calculated
in a number of ways for each feature. The nu-
meric features use the numeric metric and the re-
maining features (lemmas, POS tags) use the de-
fault overlap metric. Best performance is achieved
with gain ratio weighting and the consideration of
2 nearest distances (neighbors). Because of overlap
in the features for various types of non-referential
it and sparse data for cleft, weather, and idiomatic
it, all types of non-referential it were considered at
the same time and the output was a binary classifi-
cation of each instance of it as referential or non-
referential. The results for our TiMBL classifier
(MBL) are shown in Table 7 alongside our results
using a decision tree algorithm (DT, described be-
low) and the results from our replication of Evans
Our MBL
Classifier
Our DT
Classifier
Repl. of
Evans
Accuracy 88% 81% 76%
Precision 82% 82% 57%
Recall 71% 42% 60%
Table 7: Results
Extrapositional 81%
Cleft 45%
Weather 57%
Idiomatic 60%
Referential 94%
Table 8: Recall by Type for MBL Classifier
(2001). All three systems were trained and evalu-
ated with the same data.
All three systems perform a binary classifica-
tion of each instance of it as referential or non-
referential, but each instance of non-referential it
was additionally tagged for type, so the recall for
each type can be calculated. The recall by type can
been seen in Table 8 for our MBL system. Given that
the memory-based learning algorithm is using previ-
ously seen instances to classify new ones, it makes
sense that the most frequent types have the highest
recall. As mentioned in Section 2.2, clefts can be
difficult to identify.
Decision tree algorithms seem suited to this kind
of task and have been used previously, but C4.5
(Quinlan, 1993) decision tree algorithm did not per-
form as well as TiMBL on our data, compare the
TiMBL results (MBL) with the C4.5 results (DT) in
Table 7. This may be because the verb and adjective
lemma features (F10-F12) had hundreds of possible
values and were not as useful in a decision tree as in
the memory-based learning algorithm.
With the addition of more relevant, generalized
grammatical patterns, the precision and accuracy
have increased significantly, but the same cannot be
said for recall. Because many of the patterns are
designed to match specific function words as the
right bracket, cases where the right bracket is omit-
ted (e.g., extraposed clauses with no overt comple-
mentizers, truncated clefts, clefts with reduced rela-
tive clauses) are difficult to match. Other problem-
atic cases include sentences with a lot of intervening
46
material between it and the right bracket or simple
idioms which cannot be easily differentiated. The
results for cleft, weather, and idiomatic it may also
be due in part to sparse data. When only 2% of the
instances of it are of a certain type, there are fewer
than one hundred training instances, and it can be
difficult for the memory-based learning method to
be very successful.
7 Conclusion
The accurate classification of it as referential or non-
referential is important for natural language tasks
such as reference resolution (Ng and Cardie, 2002).
Through an examination of the types of construc-
tions containing non-referential it, we are able to de-
velop a set of detailed grammatical patterns associ-
ated with non-referential it. In previous rule-based
systems, word lists were created for the verbs and
adjectives which often occur in these patterns. Such
a system can be limited because it is unable to adapt
to new texts, but the basic grammatical patterns
are still reasonably consistent indicators of non-
referential it. Given a POS-tagged corpus, the rele-
vant linguistic patterns can be generalized over part-
of-speech tags, reducing the dependence on brittle
word lists. A machine learning algorithm is able
to adapt to new texts and new words, but it is less
able to generalize about the linguistic patterns from
a small training set. To be able to use our knowl-
edge of relevant linguistic patterns without having to
specify lists of words as indicators of certain types
of it, we developed a machine learning system which
incorporates the relevant patterns as features along-
side part-of-speech and lexical information. Two
short lists are still used to help identify weather it
and a few idioms. The k-nearest neighbors algo-
rithm from the Tilburg Memory Based Learner is
used with 25 features and achieved 88% accuracy,
82% precision, and 71% recall for the binary classi-
fication of it as referential or non-referential.
Our classifier outperforms previous systems in
both accuracy and precision, but recall is still a prob-
lem. Many instances of non-referential it are diffi-
cult to identify because typical clues such as com-
plementizers and relative pronouns can be omitted.
Because of this, subordinate and relative clauses
cannot be consistently identified given only a POS-
tagged corpus. Improvements could be made in the
future by integrating chunking or parsing into the
pattern-matching features used in the system. This
would help in identifying extrapositional and cleft it.
Knowledge about context beyond the sentence level
will be needed to accurately identify certain types of
cleft, weather, and idiomatic constructions.
References
L. Burnard, 1995. Users reference guide for the British
National Corpus. Oxford.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide. ILK
Technical Report 03-10. Technical report.
Michel Denber. 1998. Automatic resolution of anaphora
in English. Technical report, Imaging Science Divi-
son, Eastman Kodak Co.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of It. Literary and
Linguistic Computing, 16(1):45 ? 57.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Compu-
tational Linguistics, 20(4):535?561.
Guido Minnen, John Caroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. Proceedings of the 19th In-
ternational Conference on Computational Linguistics
(COLING-2002).
C. D. Paice and G. D. Husk. 1987. Towards an automatic
recognition of anaphoric features in English text; the
impersonal pronoun ?it?. Computer Speech and Lan-
guage, 2:109 ? 132.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
47
Proceedings of the Fourth International Natural Language Generation Conference, pages 81?88,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Noun Phrase Generation for Situated Dialogs
Laura Stoia and Darla Magdalene Shockley and Donna K. Byron and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
2015 Neil Ave., Columbus, Ohio 43210
stoia|shockley|dbyron|fosler@cse.ohio-state.edu
Abstract
We report on a study examining the gener-
ation of noun phrases within a spoken di-
alog agent for a navigation domain. The
task is to provide real-time instructions
that direct the user to move between a se-
ries of destinations within a large interior
space. A subtask within sentence plan-
ning is determining what form to choose
for noun phrases. This choice is driven by
both the discourse history and spatial con-
text features derived from the direction-
follower?s position, e.g. his view angle,
distance from the target referent and the
number of similar items in view. The al-
gorithm was developed as a decision tree
and its output was evaluated by a group of
human judges who rated 62.6% of the ex-
pressions generated by the system to be as
good as or better than the language origi-
nally produced by human dialog partners.
1 Introduction
In today?s world of mobile, context-aware com-
puting, intelligent software agents are being de-
ployed in a wide variety of domains to aid hu-
mans in performing navigation tasks. Exam-
ples include hand-held tourist information por-
tals (Johnston et al, 2002) campus tour guides
(Yang et al, 1999; Long et al, 1996; Striegnitz
et al, 2005), direction-giving avatars for visitors
to a building (Cassell et al, 2002; Chou et al,
2005), in-car driving direction systems (Dale et al,
2003; Wahlster et al, 2001), and pedestrian navi-
gation systems (Muller, 2002). These applications
present an exciting and challenging new frontier
for dialog agents, since attributes of the real-world
setting must be combined with other contextual
factors for the agent to communicate successfully.
In the current work, we focus on a scenario
in which the system provides incremental direc-
tions to a mobile user who is following the instruc-
tions as they are produced. Unlike the rigid di-
rections produced by applications like Mapquest,1
which describes the entire route from start to fin-
ish, this task requires realtime instructions issued
while monitoring the user?s progress. Instructions
are based on dynamic local context variables such
as the visibility of and distance to reference points.
In referring to items in the setting, human speak-
ers produce a wide variety of noun phrase forms,
including descriptions that are headed by a com-
mon noun and that employ a definite, indefinite, or
demonstrative determiner, one anaphors, and pro-
nouns such as it, this and that. Our goal in the
current work is to model that entire space of varia-
tion, which makes the task more difficult than the
noun phrase generation task defined in many pre-
vious studies that simplify the alternatives down to
description or pronoun.
In order to study this process, we developed a
task domain in which a human partner is directed
through an interior space (a graphically-presented
3D virtual world) to perform a sequence of ma-
nipulation tasks. In the first stages of the work, we
collected and annotated a corpus of human-human
dialogs from this domain. Then, using this data,
we trained a decision-tree classifier to utilize con-
text variables such as distance, target object visi-
bility, discourse history, etc., to determine lexical
properties of referring expressions to be produced
by the generation component of our dialog system.
2 Generation for Situated Tasks
Many previous projects, such as (Lauria et al,
2001; Moratz and Tenbrink, 2003; Skubic et al,
2002), inter alia, study interpretation of situated
language, e.g. for giving directions to a robot. The
focus of our work is rather on generating naviga-
tion instructions for a human partner to follow.
Linguistic studies have shown that speakers se-
lect noun phrase forms to refer to entities based on
a variety of factors. Some of the factors are intrin-
sic to the object being described, while others are
features of the context in which the expression is
spoken. The entity?s status within the discourse,
1www.mapquest.com
81
spatial position, and the presence of similar items
from which the target referent must be distin-
guished, have all been found to cause changes to
the lexical properties chosen for a particular refer-
ring expression (i.e. (Gundel et al, 1993; Prince,
1981; Grosz et al, 1995)). This variation is ex-
pressed in terms of the determiner chosen (e.g.
that/a), the head noun (e.g. that/door/one), and
the presence of additional modifiers such as pre-
nominal adjectives or prepositional phrases.
In natural language generation, the process of
generating referring expressions occurs in stages
(Reiter and Dale, 1992). The process we explore
in this paper is the sentence planning stage, which
determines whether the context supports generat-
ing a particular referring expression as a pronoun,
description, one-anaphor, etc.
There has been extensive research in both au-
tomatic route description and on general noun
phrase (NP) generation, but few projects consider
extra-linguistic information as part of the context
that influences dialog behavior. (Poesio et al,
1999) applies statistical techniques for the prob-
lem of NP generation. However, even though the
corpus used in that study contains descriptions of
museum items visually accessible to the user, the
features used in generation were mostly linguis-
tic, and included little information about the vi-
sual or spatial properties of the referent. Another
related study in statistical NP generation (Cheng et
al., 2001) focuses on choosing the modifiers to be
included. Again, no features derived from the sit-
uated world were used in that study. (Maass et al,
1995) use features from the world, including ob-
jects? color, height, width, and visibility, as well as
the user?s direction of travel and distance from ob-
jects, for generating instructions in a situated task.
However, their focus is on selecting landmarks and
descriptions under time pressure, rather than se-
lecting the linguistic form to be produced.
3 Data Collection
Our task setup is designed to elicit natural, spon-
taneous situated language examples from human
partners. The experimental platform employs a
virtual-reality (VR) world in which one partner,
the direction-follower (DF), moves about to per-
form a series of tasks, such as pushing buttons to
re-arrange objects in the room, finding and picking
up treasures, etc. The simulated world was pre-
sented from first-person perspective on a desk-top
computer monitor. The DF had no knowledge of
the world map or tasks.
DG: you can currently see three buttons... there?s
actually a fourth button that?s kind of hidden
DF: yeah
DG: by this cabinet on the right
DF: I know, yeah
DG: ok, um, so what you wanna do is you want to
go in and you?re gonna press one of the buttons
that?s on the right hand wall, so you wanna go
all the way straight into the room and then face
the wall
DF: mhm
DG: there with the two buttons
DF: yep
DG: um and you wanna push the one that?s on the left
Figure 1: Sample dialog fragment and accompanying video
frame
His partner, the direction-giver (DG), had a pa-
per 2D map of the world and a list of tasks to
complete. As they performed the task, the DG
had instant feedback about the DF?s location in
the VR world, via mirroring of the DF?s computer
screen on the DG?s computer monitor. The part-
ners communicated through headset microphones.
Our paid participants were self-identified native
speakers of North American English. Figure 1
shows an example view of the world and the ac-
companying dialog fragment.
The video output of DF?s computer was cap-
tured to a camera, along with the audio from both
microphones. A logfile created by the VR soft-
ware recorded the DF?s coordinates, gaze angle,
and the position of objects in the world 10 times
per second. These data sources were synchronized
using calibration markers. A technical report is
available that describes the recording equipment
and software used (Byron, 2005).
3.1 Corpus and Annotation Scheme
Using the above-described setup, we created a cor-
pus consisting of 15 dialogs containing a total of
221 minutes of speech. It was transcribed and
word-aligned using Praat 2 and SONIC.3 The di-
alogs were further annotated using the Anvil soft-
ware (Kipp, 2004) to identify a set of target refer-
ring expressions in the corpus. Because we are in-
2http://www.praat.org
3http://cslr.colorado.edu/beginweb/speech recognition/sonic.html/
82
terested in the spatial properties of the referents of
these target referring expressions, the items of in-
terest in this experiment were restricted to objects
with a defined spatial position.
Each object in the virtual world was assigned a
symbolic id, and the id of each target referring ex-
pression was added to the annotation. Referring
expressions with plural referents were marked as
Set, and were labeled with a list of the members
in the set. Expressions were also annotated as ei-
ther vague when the referent was not clear at the
time of utterance or abandoned in case the utter-
ance was cut short. Items that did not contain a
surface realization of the head of the NP (e.g., on
the left), were marked with the tag empty.
The corpus contains 1736 target expressions, of
which 221 were Vague, 45 were Empty, and 228
were Sets. The remaining 1242 form the set of test
items in the experiment described below. Vague
items were excluded since we do not wish for the
algorithm we develop to reproduce this behavior.
Set items were excluded in order to avoid the more
complex calculation of spatial properties associ-
ated with plural entities.
The data used in the experiments is a consensus
version on which both annotators, two of the au-
thors, agreed on the set of target expressions and
their properties. Due to the constraints introduced
by the task, referent annotation achieved almost
perfect agreement. The data used in this study is
only the DG?s language.
4 Algorithm Development
Our ultimate goal is to provide input to a surface
realization component for NP generation, given
the ID of a target referent and a vector of context
features. It is desirable for these context features
to be automatically derived, to limit the reliance
on human annotation, so we restricted out study to
features that either were derived automatically, or
required minimal human annotation.
One impact of this decision is that even though
the linguistic literature predicts that syntactic fea-
tures such as grammatical role are important in
selecting NP forms, these features were difficult
to obtain. Our corpus contains spontaneous spo-
ken discourse, which has no sentence boundaries
and relaxed structural constraints. Thus, automatic
parsing was problematic. With improved parsing
techniques, we may include syntactic information
in the decision process for NP generation in future,
but this was not included in the current study.
Following (Poesio et al, 1999), we consider the
det a, the, that, none
head it, that, one, noun, none
mod +, -
The possible values of each NP frame slot
[
det : none
head : it
mod : ?
] [
det : that
head : noun
mod : +
]
it that button on the right
NP frames for it and that button on the right
Figure 2: NP frame slot values and examples
information conveyed by an NP to be divided into
four slots which must be filled to be able to gen-
erate the NP form: a determiner/quantifier, a pre
or post-modifier and a head noun slot. There were
very few examples of premodifiers in the corpus,
so we collapsed the modifier feature. Therefore,
the output from our algorithm is an NP frame spec-
ifying values for the three slots for each target ex-
pression. Figure 2 shows the possible values in
each slot and example slot values for two NPs. The
number of occurrences in the entire corpus for the
NP frame slot values are shown in Table 2.4
In the experimental VR world developed for this
study, all items from the same category were de-
signed to look identical. This was intended to en-
courage the subjects to use referring expressions
that rely on spatial attributes or deictic reference
such as that one. The spatial properties of target
referents and distractors are used as inputs to the
content planning algorithm. Their values in this
study were calculated automatically based on ge-
ometric properties of the virtual world.
To form the training dataset, we processed each
target expression with a syntactic chunker.5 The
partial parse it produced was further processed
with a regular-expression matcher to isolate the
values corresponding to the three slots. Parser er-
rors caused some low-count NP frame values, so
we retained only items that occurred at least 10
times in the entire corpus. Any parser errors that
remained in the data were not hand corrected, in
order to minimize human intervention.
4.1 Context Features
Given the restrictions that we impose over what
is accessible to the learning algorithm, we devel-
oped a set of features for each referring expression
that characterize both the referent and the context
in which the expression was spoken. The context
4The two possible tags for Mod occurred in almost equal
proportion (49%/51%)
5http://www.ltg.ed.ac.uk/software/chunk/index.html
83
Dialog history features
1. Count and chainCount the mention counts for the referent over the dialog and inside a reference chaina
2. DeltaTime and DeltaTimeChain the time elapsed since it was last mentioned in the dialog overall or in a chain
3. PrevSpeaker the previous speaker that mentioned the ID (either DG or DF)
4. Mod
i?1
, Det
i?1
, Head
i?1
the values of the slots of the NP frame of the prior mention of the same referent
5. Mod
i?2
, Det
i?2
, Head
i?2
the previous-1 values of the slots
6. WordDistance and the number of words spoken by both speakers since the last mention of the ID
chainWordDistance overall or in the chain
7. Type
i?1
indicates if the previous mention was in a Set, was Vague, or was a test item
Spatial/Visual featuresb
8. Distance the distance between the referent and the DF?s VR coordinates
9. Angle the angle between the center of the DF?s view angle and the center of the referent
10. Visible a boolean value which indicates if the object is visible
Relation to other objects in the world
11. Visible Distractors the number of other objects besides the target referent in the field of view
12. SameCatVisDistractors the number of visible distractors of the same type as the referent
Object category and its information status
13. Cat the semantic category of the referent: door/cabinet/button
14. First Locate indicates if this is the first expression that allowed the DF to identify the object
in the world. The point where joint spatial reference is accomplished.
Table 1: The Context Features Used by the Algorithm
amention counts are not considered over vague or ambiguous tags, or over sets.
bnote that an Angle value smaller than 500 ensures the object is visible
Det Head
Value Count Percent Value Count Percent
the 364 39% noun 558 60%
that/this 264 29% one 166 18%
none 253 27% it 116 13%
a 46 5% that 57 6%
none 30 3%
Table 2: Distribution of Det and Head values in the corpus
v = Visible area(100o)
? = Angle to target
d = distance to target
In this scene:
VisDistr =3 {B2, B3, C1}
VisSemDistr =2 {B2, B3}
Figure 3: An example configuration with spatial context
features. The target object is B4.
features are not only linguistic but also derived
from the extralinguistic situation, including spatial
relations between the referent and the DF?s posi-
tion and orientation at that instant. The context
feature for each target expression includes these
automatically-calculated attributes as well as fea-
tures from the annotation described above. Table 1
describes the full set of context features, and Fig-
ure 3 shows a schematic of the spatial context fea-
tures.
The mention history of any target referent is im-
portant for determining the form to use in a subse-
quent referring expression. Ideally, the discourse
history feature should indicate whether a refer-
ent has already been discussed, and the distance
between a new mention and its antecedent. But
determining the discourse status of items in this
world was complicated by two factors. All ob-
jects in the world of the same semantic category
had identical visual features, and the VR world
in which the task is conducted is a maze, which
required the subjects to perform tasks, move to a
different portion of the maze, and possibly return
to a previously visited room. Due to the visual
and spatial confusion possible in this setting, there
is no guarantee that our subjects could accurately
calculate whether they were discussing the same
object they had encountered before, or remember
whether that object had been discussed. While
the subjects were focused on a task in a particular
room, however, it is reasonable to expect that they
could remember which items had been discussed.
Therefore, the discourse histories of target objects
were calculated using a re-initialization process.
Each time the subjects left a VR room to pursue a
different task, if more than 25s elapsed before the
next mention of objects in that room, those sub-
sequent expressions were considered to be in new
coreference chains. This time constant was estab-
lished by examining pronominal referring expres-
sions in the training dialogs.
These features were used as input to develop a
classifier to determine NP frames for unseen tar-
get referents in context. We chose decision trees
due to their ease of interpretation, but we plan to
test other machine learning techniques in the fu-
ture. 5 dialogs were held out as unseen data and
the remaining 10 were used to train and adjust the
parameters of the decision process. The first pro-
cedure was to test whether the three slot values
are interdependent. In contrast to previous work,
84
which focused on predicting the values for one of
the slots at a time, we hold that due to their inter-
dependence, these decisions should not be made
separately. For example, a noun form that has the
pronoun it as the head will never have a modifier
or a determiner. If the three slots are independent,
training three separate classifiers and then com-
bining their decisions will yield better results. On
the other hand, if they are dependent, better results
will be obtained through training a single classifier
on the combined label. Unfortunately, combining
the labels is problematic due to data sparsity. To
test these dependencies, we trained several deci-
sion trees, varying the independence assumptions:
Independent - a decision tree was trained for each
slot and their outputs combined at the end.
Joint - a decision tree was trained for the com-
bined label for all three slots
Conditional - three decision trees were trained
in sequence, each having access to the output of
the previous tree. For example, Mod-Det-Head
means that first the Mod tree was trained, then a
tree to classify Det, using the output from Mod,
and finally a tree for Head , using both the Det
and Mod values.
All possible orderings between Mod, Head and
Det were tested. The best result obtained was from
the ordering Mod-Det-Head, but the differences
between the orderings were not significant. The
10 fold cross-validation results are shown in Ta-
ble 3. There were 632 items in the data set. The
Conditional trees outperformed the Independent
trees by 9%, which is significant at the level of
(p < .0002).
As our training data suggests, we test the Mod-
Det-Head trees against our held out data. We
decided to use a leave one out method of train-
ing/testing due to the sparsity of data.
Independent Joint Mod-Det-Head
22.0 % 28.8 % 31.0 %
Table 3: Testing independence of the slot values
Decision tree classifiers offer the opportunity to
examine the relevance of particular features in the
final decision. Algorithm 1 and 2 show example
trees derived for the Mod and Det features (the
Head tree is not shown due to space limitations).
We found that there are significant dependencies
between the slots in the NP form. Each time one of
the slots? values was available to the decision pro-
cess, it was selected as most informative feature in
the next tree. The spatial features were selected as
informative in all the trees, most prevelantly in the
Algorithm 1 An example decision tree for Mod
if FirstLocate = True then
if V isibleDistractors = 0 then
if Distance ? 116 then
return Mod: -
else
return Mod:+
else
if SameCatV isDistractors = 0 then
if V isibleDistractors ? 2 then
if Angle ? 38 then
return Mod: -
else
return Mod: +
else
return Mod: +
else
return Mod: +
else
if chainWordDistance = 0 then
if prevMention 6= Set/AllV ague then
if firstMention = True then
return Mod: +
else
if Angle ? 27 then
return Mod: -
else
return Mod: +
else
if noprevMention then
return Mod: +
else {prevMention = Set/AllV ague}
return Mod: -
else
return Mod: -
Algorithm 2 An example decision tree for Det
if Mod : ? then
if FirstLocate = True then
return Det:that
else
if prevMention 6= Set/AllV ague then
if notV isible then
if Cat = Button/Cabinet then
return Det:none
else {Cat = Door}
return Det:that
else {isV isible}
if Head
i?1
= it then
return Det:none
else if Head
i?1
= noun then
if DeltaT ime ? 6.3 then
if Cat = Button/Cabinet then
return Det:none
else {Cat = Door}
return Det:that
else
return Det:the
else if Head
i?1
= one/none/low then
return Det:that
else {Head
i?1
= that}
return Det:none
else if noprevMention then
return Det:that
else {prevMention = Set/AllV ague}
return Det:none
else {target has modifier}
return Det:the
85
decision tree for Mod, suggesting that the decision
of including extra information is driven largely by
the spatial configuration. The information status
features and discourse history, such as First Lo-
cate, Type, and attributes of the prior mention,
were selected as good predictors for the Det slot.
5 Evaluation
We report several methods of evaluating the NP
frames produced using the process given by the
decision trees. First, we report the results of a
strict evaluation in which the system?s output must
exactly match expressions produced by the hu-
man subjects. We also compare this result with
a hand-crafted Centering-style generation algo-
rithm. Requiring the algorithm to exactly match
human performance is an overly-strict criterion,
since in many contexts several possible referring
expression forms could be equally felicitous in a
given context, so we also conducted a human judg-
ment study. The 5 test dialogs contain 295 target
expressions.
5.1 Exact Match Evaluation
The output of the decision tree classifier was com-
pared to the expressions observed in the test dia-
log. Table 4 reports the results of this evaluation.
The accuracy obtained was 31.2%. The most fre-
quent tag gives a 20.0% baseline performance us-
ing this strict match criterion.
Exact match results
Predicted All three features det mod head
Correct 31% 48% 72% 56%
Exact match: head feature per value
Predicted noun it none one that
Correct 65% 64% 0% 30% 38%
Exact match: det feature per value
Predicted a none that the
Correct 0% 49% 36% 66%
Table 4: Classifier results using Exact-match criterion
5.2 Comparison to Centering
For purposes of comparing the performance of our
generation algorithm to existing work on genera-
tion of NPs, we performed a manual evaluation of
the centering-style generation algorithm described
in (Kibble and Power, 2000) against our dialog
corpus. Algorithms developed according to the
centering framework use discourse coherence to
make decisions about pronominalization (Grosz et
al., 1995), where coherence is measured in terms
of topical continuity from one sentence to the next.
Centering designates the backward-looking cen-
ter (Cb) as the item in the current sentence that
was most topical in the previous sentence. There-
fore, to perform a centering-style evaluation, the
dialogs must be broken into sentence-like units,
and a ranking procedure must be devised for the
items mentioned in each unit.
The current evaluation corpus, being a spo-
ken dialog, has not been parsed to automatically
determine the syntactic or dependency structure,
but rather was manually segmented into utterance
units, where each unit contained a main predicate
and its satellites. The items mentioned in each unit
were ranked according to thematic roles, using the
ranking {AGENT > PATIENT > COMP > AD-
JUNCT}, and excluding references to the speakers
themselves, which often appear in AGENT posi-
tion (Byron and Stent, 1998). The Cb in each unit,
if there is one, is the highest-ranked item from the
prior unit?s list that is repeated in the current unit?s
list. Following a procedure similar to that reported
by Kibble and Power, our decision procedure rec-
ommends pronominalizing an item if it is the Cb
of its unit and if it is in Subject position, otherwise
a description is generated. Based on this rule, all
items that are being mentioned for the first time
in the discourse are predicted to require a descrip-
tion.
Although most prior studies take the recom-
mendation to pronominalize to mean that a per-
sonal pronoun (e.g. it) should be generated, due
to the demonstrative nature of our domain, the de-
cision to produce a pronoun can result in either a
demonstrative or a personal pronoun. Therefore,
we considered the algorithm?s output to match hu-
man production when the target expression in the
human corpus was either a personal or demonstra-
tive pronoun, and the algorithm generated either
category of pronoun. Table 5 shows the compari-
son of our system?s output and the output from the
centering algorithm on anaphoric mentions. The 5
dialogs used for testing in this study contained 145
such items. Both algorithms obtained a similar ac-
curacy (64.8% our system vs. 64.1% centering)
and over-generated pronoouns. Although our al-
gorithm does not outperform centering, it assumes
less structural analysis of the input text.
5.3 Human Judgment Evaluation
Evaluating generation studies by calculating their
similarity to human spontaneous speech may not
be the ideal performance metric, since several dif-
ferent realizations may be equally felicitous in a
86
Pron Desc Total
Human Production 28 117 145
Predicted by Our Algorithm 55 90
Predicted by Centering 64 81
Table 5: Comparison to Coherence-based Generation
Figure 4: The Anvil software tool used for judging
given context. Therefore, we also performed a
human judgement evaluation. In this evaluation,
judges compared the NPs generated by our algo-
rithm to the NPs produced by human subjects, and
to NPs with randomly generated feature assign-
ments. Judges viewed test NPs in the context of
the original test corpus.
To re-create the context in which the original
expression was produced, the video, audio, and
dialog transcript were played for the judges us-
ing the Anvil annotation tool (Kipp, 2004). The
judges could play or pause the video as they
wished. Using the word-alignments established
during the data annotation phase, the audio of the
test NPs was replaced by silence, and the words
were removed in the transcript shown in the time-
line viewer. For each test item, the judges were
presented with a selection box showing two pos-
sible referring expressions that they were asked to
compare using a qualitative ranking (option 1 is
better, option 2 is better, or they are equal), given
a particular target ID and the context. Figure 4
shows a screen-shot of the judges? annotation tool.
The judges did not know the source of the expres-
sions they evaluated (system, human production,
or random). The 10 judges were volunteers from
the university community who were self-identified
native speakers of English. They were not com-
pensated for their time.
The decision tree selected NP-frame slot val-
ues which were converted into realized NPs. The
Det and Head choices were directly translated into
surface forms (for Head=noun we chose a consis-
tent common noun for each semantic class: but-
All Items
System compared to Human Trials: 577
equal 45.9%
system preferred 16.6%
(system equal or preferred to human) (62.6%)
human preferred 37.4%
System compared to Random Trials: 289
equal 24.2%
system preferred 53.3%
(system equal or preferred to random) (77.5%)
random preferred 22.5%
Random compared to Human Trials: 292
equal 23.3%
random preferred 13.0%
(random equal or preferred to human) (36.3%)
human preferred 65.7%
Items with two judges & judges agreed
System against Human Trials: 197
equal 37.3%
system preferred 19.8%
(system equal or preferred to human) (57.1%)
human preferred 36.6%
Table 6: Results of Human Judging
ton, door or cabinet. If the system?s selection of
Mod feature matched the value from the corpus,
we used the expression produced by the original
speaker. If the original expression did not include
a modifier, but the system selected Mod:+, we lex-
icalized this feature to a simple but correct spatial
description like on the right, on the left or in front.
Table 6 shows the results of human judging.
The system?s output was either equal or preferred
to the original spontaneous language in 62.6%
of cases where these two choices were compared
directly. Interestingly, the randomly-generated
choice was preferred over the original spontaneous
language in 13.0% of trials, and was preferred over
the system?s output in 22.5% of trials. Aggregat-
ing over all judges, the system?s performance was
judged to be much better than random, but not as
good as the original human language.
Trials were balanced among judges so that each
target item was seen by four judges: with two
comparing the system?s response to the original
human language, one comparing the system to
random, and one comparing the human to random.
There were 282 trials for which 2 judges saw the
identical pair of choices. Out of these, the two
judges? responses agreed in 197 cases, producing
an inter-annotator reliability (kappa score) of 0.51,
with raw agreement of 69% and expected agree-
ment of 37%. Although this is a relatively low
kappa value, we believe that the aggregate judg-
ments of all of the judges over all of the test items
are still informative, since the scores of items for
which we have two judgements follow a very sim-
87
ilar pattern to the overal distribution of responses.
The low inter-annotator agreement may be due to
the substitutability of the expressions.
6 Conclusions and Future Work
In this paper we describe a generation study for
situated dialog and a novel evaluation setup of the
system?s output. The algorithm decides upon the
determiner, head and modifier values to be pro-
duced in a noun phrase describing an object in
a particular moment in the dialog. The decision
is influenced by dialog history, spatial and visual
relations and information status of the ID to be
described. Our algorithm achieved 31.2% exact
match with human language, but human evalua-
tors judged the output as good as or better than the
original human language 62.6% of the time.
For our future work, we intend to develop the
generation module of a dialog system that per-
forms the direction giver?s role. We plan to incor-
porate the results of this study in an extension of
(Reiter and Dale, 1992) algorithm that would take
into account other types of properties of the ob-
jects like visual salience, temporal attributes (for
example time elapsed between mentions), if it par-
ticipated in an action (like the case of a door open-
ing, or a button being pushed) or its importance to
the overall task completion.
Acknowledgments
The authors would like to thank our undergradu-
ate RA, Bradley Mellen, for building the virtual
world, the 11 judges who rated the system output,
and the anonymous reviewers.
References
D. Byron and A. Stent. 1998. A preliminary model of center-
ing in dialog. In Proceedings of ACL ?98, pp. 1475?1477.
D. Byron. 2005. The OSU Quake 2004 corpus of two-party
situated problem-solving dialogs. Technical Report OSU-
CISRC-805-TR57, The Ohio State University Computer
Science and Engineering Department, September.
J. Cassell, T. Stocky, T. Bickmore, Y. Gao, Y. Nakano,
K. Ryokai, D. Tversky, C. Vaucelle, and H. Vilhjalmsson.
2002. MACK: Media lab Autonomous Conversational
Kiosk. In Proceedings of IMAGINA?02, Monte Carlo, Jan-
uary.
H. Cheng, M. Poesio, R. Henschel, and C. Mellish. 2001.
Corpus-based NP modifier generation. In NAACL ?01,
pp. 1?8, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
S. Chou, W. Hsieh, F. Gandon, and N. Sadeh. 2005. Se-
mantic web technologies for context-aware museum tour
guide applications. In Proceedings of the 2005 Interna-
tional Workshop on Web and Mobile Information Systems.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using
natural language generation for navigational assistance.
In M. Oudshoorn, editor, Proceedings of the 26th Aus-
tralasian Computer Science Conference, Adelaide, Aus-
tralia.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering: A
framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?226.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive
status and the form of referring expressions in discourse.
Language, 69(2):274?307.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of ACL ?02, pp. 376?383.
R. Kibble and R. Power. 2000. An integrated framework for
text planning and pronominalisation. In Proceedings of
INLG?2000, pp. 77?84.
M. Kipp. 2004. Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Dis-
sertation.com.
S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein.
2001. Training personal robots using natural langauge in-
struction. IEEE Intelligent Systems, 16(5):2?9.
S. Long, R. Kooper, G. Abowd, and C. Atkesonet. 1996.
Rapid prototyping of mobile context-aware applications:
The cyberguide case study. In 2nd ACM International
Conference on Mobile Computing and Networking (Mo-
biCom?96), November 10-12.
W. Maass, J. Baus, and J. Paul. 1995. Visual grounding of
route descriptions in dynamic environments.
R. Moratz and T. Tenbrink. 2003. Instruction modes for joint
spatial reference between naive users and a mobile robot.
In Proc. RISSP 2003 IEEE International Conference on
Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on NewMethods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
M. Poesio, R. Henschel, J. Hitzeman, and R. Kibble. 1999.
Statistical NP generation: A first report. Utrecht, August.
E. Prince. 1981. On the inferencing of indefinite this NPs.
In Aravind K. Joshi, Bonnie Lynn Webber, and Ivan Sag,
editors, Elements of Discourse Understanding, pp. 231?
250. Cambridge University Press.
E. Reiter and R. Dale. 1992. A fast algorithm for the gen-
erations referring expressions. In Proceedings of COL-
ING ?92, pp. 232?238.
M. Skubic, D. Perzanowski, A. Schultz, and W. Adams.
2002. Using spatial language in a human-robot dialog.
In 2002 IEEE International Conference on Robotics and
Automation, Washington, D.C.
K. Striegnitz, P. Tepper, A. Lovett, and J. Cassell. 2005.
Knowledge representation for generating locating gestures
in route directions. In Proceedings of Workshop on Spa-
tial Language and Dialogue (5th Workshop on Language
and Space), Delmenhorst, Germany, October.
W. Wahlster, N. Reithinger, and A. Blocher. 2001.
Smartkom: Towards multimodal dialogues with anthropo-
morphic interface agents. In International Status Confer-
ence: Lead Projects HumanComputer -Interaction, Saar-
bruecken, Germany.
J. Yang, W. Yang, M. Denecke, and A. Waibel. 1999. Smart
sight: a tourist assistant system. In Proceedings of the
3rd International Symposium on Wearable Computers, pp.
73?78, San Francisco, California, 18-19 October.
88
Prosody and the Resolution of Pronominal Anaphora 
Maria Welters 
lnstitut l'i.ir Komlnunikationsforschung und 
Phonetik, Universitfit Bonn 
Poppelsdorfer Alice 47, D-53115 Bonn 
wolters@ikp.uni-bonn, de 
Donna K. Byron 
Department of Computer Science 
University of Rochester 
Re. Box 270226, Rochester, NY 14627 
dbyron@cs, rochester, edu 
Abstract 
In this paper, we investigate the acoustic prosodic mark- 
ing o\[" demonstrative and personal pronouns in task- 
oriented dialog. Although it has been hypothesized that 
acouslie marking affects pronoun resolution, we find 
flint {l~e prosodic information extracted from tile data is 
not sufficienl to predict antcceden! lype reliably. Inter- 
speaker variation accottnts for mt, ch of lhe prosodic vari- 
ation that we find in our data. We conclude that prosodic 
cues shot, ld be handled with care in robust, speaker- 
independenl dialog systems. 
1 I n t roduct ion  
Previous work on anaphora resolution has yieMed a rich 
basis of theories and heuristics for finding antecedenls. 
However, most research to date has neglecled an impor- 
tant potential cue that is only available in spoken data: 
prosody. Prosodic marking can be used to change the 
antecedent of a pronoun, as demonsh'ated by lifts clas- 
sic example from l.akoff ( 1971 ) (capitals indicalc a pitch 
accent): 
( I ) Johll i called J imj a Relmblican, then hei insulted 
himj. 
(2) Johlll called Jim./ a Republican, then lll{j ill- 
sulled lllMi. 
But exactly how the antecedent changes due to the 
prosodic marking on tile pronoun, and whefller this effect 
happens consistently, is an open question. If consislcnl 
elfecls do exisl, they would be useful for online pronoun 
inlerpretation i spoken dialog systems. 
Prosodic prominence directs tile attention of the lis- 
tener to what is important for understanding and inteF 
pretation. But how should this principle be applied when 
words that are normally not very prominent, such as 
prollouns, are accented? More generally, does acous- 
tic marking provide syslemalic ues to characteristics of
amecedents? IVlore specitically, does it imply that tile 
antecedent is "untmtml" in some wily'? These arc tile 
two hypofl/eses we investigate in this paper. ()ur data 
consists of 322 pronouns from a large corpus of spoma- 
neous lask-orientcd dialog, the TRAINS93 corpus (Hee~ 
man and Allen, 1995). This corpus allows us to study 
pronotms as they occur in spontaneous unscripted dis- 
COLlrse, al)d is erie of tile very few speech corporit o have 
been annotated with pronoun interpretation i fommtion. 
The remainder oF this paper is structured as follows: 
In Section 2, we SUllllllill'iZe relevant work on pl'OllOUn 
resolution and report on tile few proposals for integrat- 
ing prosody into pronoun resolution algorithms. Next, 
in Section 3, we present ile dialogs used for our study 
and the attributes awfilable in tile annotation data, while 
Section 4 describes file acoustic measures that were corn- 
puled automatically from the data. Section 5 explores 
whelher there are syslematic orrelations between these 
properties and tile acoustic measures fundamental fre- 
quency, duralion, and inlensity. For lhese measures, we 
find Ihat nlost correlations are in fact due to speaker vari- 
alien, and fl/a/ speakers differ greatly in their overall 
prosodic characleristics. Finally, we investigate whether 
it is possible to use Ihesc acoustic features to predict 
prope,'ties of tile antecedent using logistic regression. 
Again, we do not find acoustic features to be reliable 
prediclors for lhe l'catures of inleresl. Therefore, we con- 
chide in Section 6 lhat acoustic measures cannot be used 
in sl)eaker-independenl o ine anaphora resolution algo- 
rithms to predict lhe features under investigation here. 
2 Background and Related Work 
There is a rich literature ?)11 resolving personal pronouns. 
Many approaches arc based on a notion of attentional 
foctls. Entities in attentional focus are highly salient, 
and pronouns are assumed to refer to tile most salient 
entity in lhc discourse (el. (Brennan el al., 1987; Az- 
zam et ill., 1998; Strube, 1998)). Centering (Grosz et 
al., 1995) is a i}amework for predicting local attentional 
focus. It assumes that tile most salient entity from sen- 
tence ,3,,_\] that is realized in sentence ,5',, is most likely 
to be pronominalized in ,3,z. That entity is termed the Cb 
(backward-looking center) of sentence ,5',,. Finding ille 
preferred ranking criteria is an active area of research. 
Byron and Stem (1998) adapted this approach, which had 
previously been applied to text, for spoken dialogs, but 
wilh linfited st,ccess. 
\]n contrast to personal pronouns, demonstratives do 
not rely on calculalions of salience. In fact, Linde (1979) 
found lhat while it was preferred for entities within the 
919 
current local t'ocus, that was used for items outside the 
current focus of attention. Passonneau (1989) showed 
that personal and demonstrative pronouns are used in 
contrasting situations: personal pronouns are preferred 
when both the pronoun and its antecedent are in sub- 
ject position, while demonstrative pronouns are preferred 
when either the pronoun or its antecedent is not ill sub- 
ject position. She also found that personal pronouns tend 
to co-specify with pronouns or base noun phrases; the 
more clause- or seutence-likc the antecedent, he more 
likely the speaker is to choose a demonstrative pronoun. 
Pronoun resolution algoritlnns tend not to cover 
demonstratives. Notable exceptions are Webber's model 
for discotn'se deixis (Webbcr, 1991) and the model de- 
veloped for spoken dialog by Eekert and Strube (1999). 
This algorithm encompasses both personal and delnon- 
strative pronouns and exploits their contrastive usage pat- 
terns, relying on syntactic lues and verb subcategoriza- 
tions as input. Neither study investigated the intluence of 
prosodic prominence on resolution. 
Most previous work on prosody and pronotm resolu- 
tion has focussed on pitch accents and third person sin- 
gular pronouns that co-specify with persons. Nakatani 
(1997) examined the antecedents of personal pronouns 
in a 20-minute narrative monologue. She found that pro- 
nouns tend to be accented il' they occur in subject po- 
sition, and if the backward-looking center (Grosz et al, 
1995) was shifted to the referent of that pronoun. She 
then extended this result to a general theory of the in- 
teraction between l)rominencc and discourse structure. 
Cahu (1995)discusses accented prorJouns on the ba- 
sis of a theory about accentual correlates of salience. 
Kamcyama (1998) interprets a pitch accent on pronouns 
in the fl'amework of Ihe alternative semantics (Rooth, 
1992) theory o1' focus. She assumes that all potential an- 
tecedents are stored in a list. Pronouns arc then resolved 
to the most preferred antecedent on that list which is syn- 
tactically and semantically compatible with the pronoun. 
Preference is modeled by an ordering on the set ol' an- 
tecedents. An accent on lhe pronoun signals that pro- 
noun resolution should not be based on the default order- 
ing, where the default is computed by a nmnber of in- 
teracting syntactic, semantic, pragmatic, and attentional 
constraints. 
Compared to he and she, it and that lmve been some- 
what neglected. There are two reasons for this: First, it 
is not considered to be as accentable as he and she by 
native speakers of both British and American English, 
whereas that is more likely than it to beat" a pitch ac- 
cent. An informal study of the London-Lund corptts of 
spoken British English (Svartvik, 1990) confirmed that 
observation. Second, that fi'cquently does not lmve a 
co-specifying NP antecedent, and most research on co- 
speciticatiou has focussed on pronouns and NPs. Work 
on accented emonstratives and pronoun resolution is ex- 
tremely scarce. Pioneering studies were conducted by 
Ft'ethcim and his collaborators. They tested the effect of 
accented sentence-initial demonstratives that co-specify 
with the preceding sentence on the resolution of ambigu- 
ous personal pronouns, and found that the pronoun an- 
tecedents switched when the demonstrative was accented 
(Fretheim ct al., 1997). However, to otu" knowledge, 
there are no studies that compare the co-specification 
preferences of accented vs. unaccented demonstratives. 
3 The Corpus: TRAINS93 
Our data is taken from the TRAINS93 corpus of hunlun- 
human problem solving dialogs in the logistics phnuting 
domain. In these dialogs, one participant plays the role 
of the planning assistant and the other attempts to con- 
struct a plan for delivering specified cargo to its destina- 
tion. We used a subset of 18 TRAINS93 dialogs in which 
the referent and antecedent of third-person on-gendcrcd 
pronouns I had been attnotated in a previous study (By- 
ron and Allen, 1998). In the dialogs used for the present 
study, 322 pronouns (158 personal and 164 demonstra- 
live) have been annotated. Personal pronouns ill the di- 
alogs are it, its, itselJ; them, the3,, their and themselves. 
Demonstrative pronouns in the annotation data are that, 
this, these, those. There are live nmle and 11 fenmle 
speakers. One female speaker contributed 89 pronouns, 
two others produced more than 30 each (one female, one 
male), the rest is divided unevenly among tile remain- 
ing 13 speakers. The set of dialogs chosen for annota- 
tion intentionally included a variety of speakers o that 
no speaker's idiosyncratic discourse strategies would be 
prevalent ill the resulting data. 
Table 1 describes the attributes caplurcd for each 
pronoun. These features were chosen for tile annota- 
tion because many previous studies have shown them 
to be imporlant for pronoun resolution. Features ill- 
clude attributes of the pronoun, its antecedent ( he dis- 
cotu'se constituent Ihat previously triggered lhe refer- 
ent), and its referent (the entity that should be substi- 
tuted for the pronoun in a semantic representation of 
the sentence). Cb was annotated using Model3 from 
(Byron and Stent, 1998) with a linear model of dis- 
course  st ructure .  Note that anno la led  prononns  were  
not limited to those with NP antecedents, as is tile case 
with most other studies. In addition to NP antecedents, 
pronouns in this data set could have an antecedent of 
some other phrase or clause type, or no annomtablc an- 
tecedent at all. There are two categories of pronouns 
with no annotalable antecedent. Ill the simplest case, 
tim pronominal reference is the first mention of the ref- 
erent ill tile dialog. That happens when the referent is in- 
ferred liom the problem solving state. For example, af- 
ter" tile utterance send the engine to Coming  
and p ick  up the  boxcars ,  a new discourse n- 
I No gendcred entities exist in this co,'pus, so gendered pronouns 
wc,-c not inchtdcd. All dcmonst,'ativc pronouns were annolated; how- 
evcf, lhcre were only 5 occurrences of "this" in the selected ialogs, 
so eonstrasts between proxinml and distal dcmonslratives could not be 
studied. 
920 
Feature 11) l)escriplion 
I'RONTYPE Pronoutl Type 
I'RONSUB,I Pronoun is suljccl 
ANTI,\]I~()I{M Antecedenl form 
I)IST I)islance to antecedent 
ANTESUILI Antecedent is subjcc! 
CB Backward-looking center 
|trOllOU 11 
category 
Possible Values 
def= tile pronoun is one of {it, its, itself, them, dmy, thcin themselves} 
dcm = the inonoun is one of {that, this, these, fllose} 
Y = prOllOtltl subject of lllaill clause of its ullerance 
N : pronotm not subject of main clause 
I'I~,()NOUN = antecedent is pronoun 
NI' = antecedent is mse noun phrase 
N()N-NP = antecedent is other constituent, at most one utterance long 
NONE = pronotm is lit'st mention or antecedent length > one tttterance 
SAME = antecedent and pronoun in same utterance 
AI)J = antecedent and pronoun in adjacent utterances 
RI{MOTE = antecedent more than one utterance before pronoun 
Y = alSteccdel l l  subject o1' the lllain chmse of its tttterance 
N = antecedent not subject of a main clause 
Y = pronoun is Cb of its utterance 
N = pronoun is not Cb 
DIST  
cldj. I'CqllO\[? 
Table 1: The features avaihtble ill the annotation data set. 
ANTE ANTESUBJ  
NP/pmn.  non-NP  none yes no same 
75.9% 6.3% 17.8 % 37.3% 62.7% 29.1% 
28.0% 36.1t0% 36.0% 14.0% 86.11% 18.9% 
51.60{, 21.4% 27.0% 25.5% 74.5% 23.9% 
personal 33.5% 20.2% 
demonslrafive 29.9% 15.2% 
lolal 31.7% 17.7% 
3hble 2: Typical properties of antccedcnts lbr personal and demonst,'ative pronouns ill file corpus. All percentages 
are given relative to tile lolal ntnnber of pronouns in that category and rounded. Boldface: most frequent antecedent 
property. 
tity, tile train composed of tile engine and Ix)xcars, is 
awfilable for anaphoric reference. In the more subtle 
case, Ihe entity was built from a stretch (51" discourse 
longer than one utterance. In an effort to achieve an ac- 
ceptable level of inier-annotalor agreelnenl for the aw 
nohltion, the maxinmm size \[or a consfiluenl to serve as 
~tll ~ltllecedelll W\[lS de\[illed l(1 be OllC ullCl'~,lllCC, l)iscourse 
entities that are built fi'om longer she/chcs of lexl include 
objects uch as tile entire 131an or tile discourse itself, and 
such items are less reliable lo annotate. 
qaking the annotated dialogs as a whole, 21.4% of all 
prollouns have ;.l non-NP antecedent, and 27% do not 
have an mmolatal~le antecedent a  a11. qhble 2 shows thal 
tile default antecedenls o1' personal and denlonsh'alive 
pronouns follow the predictions of Schiffman (1985). 
The antecedent of personal pronouns i  most likely itself 
lo be a pronoun or a full NP, while demonstratives m'e 
most likely to have no antecedent, or if there is one, it is 
ntost likely to be a non-NR The main role of prosodic ill- 
lksr,nation is to help pronoun resolution algorithms iden- 
tify cases where flmse default predictions are false. 
4 Acoustic Prosodic Cues 
Our selection (51' acottstic measures covers three classic 
components of prosody: fundamental frequency (IV()), 
duration, and intensity (Lehiste, 1970). The relation- 
ship between those cues and prosodic pronlinencc has 
been demonstrated bye.g. (Fant and Kruckenberg, 1989; 
Heufl, 1999). Tile main correlate of English stress is F0, 
the second rues! imporlant is duration, and the least im- 
porlanl is inlensity (1,chisle, 1970). Therefore, we will 
pay more allelllioll lo F0 illeflsUl'eS. Although cxperi- 
menial results indicate flint 1;0 cues of pronlinencc can 
depend on the shape of file 1:0 conlour of the uucranec 
(c.f. (Gussenhoven cl al., 1997)), we do nol control for 
such illleraclions. \]llstead, we reslricl ourselves to cues 
that are easy to COnlpute fr(ml limiled dala, so that a run- 
ning spoken dialogue system might be able to compute 
them in real time. 
4.1 Acoustic Measures 
Duration: For duration, we found lhat 1he logarith- 
mic duration wllues a,'c nornmlly distributed, bolh pooled 
over all speakers and for lhoso speakers willl more than 
20 pronouns. Logariflmtic duration is also tile target vari- 
able of many duration models such as that of (van San- 
ten, 1992). We assume that speaker-related variation is 
covered by the w,'iance of lhis normal distribution; we 
can control for speaker effects by including a SPEAKER 
factor in our models. 
F0 variables: F0 was computed using the \]2ntropic 
ESPS Waves tool get_f0 with standard settings and a 
frame rate (51' 10 ms. All F0 wdues were transt'onned into 
lhe log-domain and then pooled imo mean, minimum, 
and maximum F0 values for each word and each utter- 
ance. This log donmin is well motiw~led psychoacousti- 
cally (Zwicker and lhtstl, 1990). F0 range was computed 
oil the values in tile log-domain. We assume lhat the Iog- 
m'ithm of F0 has a nomml distribution. Therefore, we 
921 
can nommlize for speaker-dependent differences in pitch 
range by using z-scores, and we can use standard statis- 
tical analysis methods uch as ANOVA. 
Intensity: Intensity is measured as the root-mean- 
square (RMS) of signal amplitudes. We measure 
RMS relative to a baseline as given by the formula 
log(l{MS/RMSb~olino). The baseline RMS was com- 
puted on the basis of a simple pause detection algorithm, 
which takes the first nmximum in the amplitude his- 
togram to be the average amplitude of background noise. 
The baseline RMS was slightly above that value. 
4.2 Inter-Speaker Differences 
Since we need to pool data from many different speak- 
ers, we qeed to control for inter-speaker differences. 
Tim number of pronouns we have fl'om each speaker 
varies between 1 for speaker GD and 86 for speaker 
CK. Speakers PH, male, and CK, female, are the 
only ones to lmve produced more than 15 personal 
pronouns and 15 demonstratives. In order to test 
whether the SPEAKER factor affects the choice be- 
tween personal pronouns and demonstratives, we tit- 
ted a logistic regression model with the target variable 
PRONTYPE (personal or demonstrative) and the predic- 
torsANTE, ANTESUBJ ,  DIST, REFCAT,  CBand 
SPEAKER (in this sequence). REFCAT is an additional 
variable that describes the senmntic category of a pro- 
noun's referent (eg. donmin objects vs. abstract enti- 
ties). Even though SPEAKER is the last factor in the 
model, an analysis of deviance shows a signilicant intlu- 
euce (p<0.005,F=2.51,df13).  A possible explanation 
for this is that some speakers prefer to use demonstra- 
tives in contexts where others would choose a personal 
pronotm, and vice versa, or perhaps the SPEAKER vari- 
able mediates the intluence of a far ,nore complex factor 
such as problem solving strategy. Resolving this ques- 
lion is beyond the scope of this paper. 
On the basis of F0, we can establish four groups of 
speakers: The first group consists of male speakers with 
a low mean F0 and a low F0 range. In the next group, 
we find both male and female speakers with a low mean 
F0, but a far higher range. Speaker PH belongs to this 
second group. Interestingly, for these speakers, the mean 
F0 on pronouns is lower titan for those of the first group. 
Groups 3 and 4 consist entirely of female speakers, with 
group 3 using a lower range than group 4. Speaker CK 
belongs to group 4. 
5 Exploring Prominent Pronouns 
If data about prosodic prominence is to be useful for pro- 
noun resolution, then there must be prosodic cues that 
carry information about properties of the antecedent. In 
this section, we investigate if there are such cues for the 
properties that we have available in the annotation data, 
defined in ~lable 1. More specitieally, we hypothesize 
that prosodic ues will be used if the antecedent is some- 
what unusual. For example, the results of Linde and 
Property 
ANTEFORFI  
D IST  
ANTESUBJ  
df 
all 
3 range 
3 none 
2 dur 
Dam Set 
peJw. dem. CK 
110110 none  none  
l lono none  none  
dur,  no l le  \])ors.: 
mean energy 
range 
Table 3: Significant Inlluences of Antecedent Proper- 
ties (p <0.05) on Prosodic Cues. inean=z-score mean 
F0, range=range of z-score F0, dur=logarithmic dura- 
tion, dem=demonstratives, pets=personal pronotms 
Passonneau would lead us to expect that personal pro- 
nouns with non-NP antecedents and demonstratives with 
NP and pronoun antecedents will be marked. Since the 
antecedents of pronouns tend to occur no more than 1-2 
clauses ago, we would also expect pronouns with more 
remote antecedents obe marked. A first qualitative look 
at the data suggets that even il' such these tendencies are 
present in the data, they might not turn out to be signifi- 
cant. For example, in Figure 1, the means of l zmeanf0  
behave roughly as predicted, but the variation is so large 
that these differences might well be due to chance. 
5.1 Correlations between Measures and Properties 
Next, we examine whether the measures delined in Sec- 
lion 4 correlate with any particular properties o1' the 
antecedent. More precisely, if a property is cued by 
some aspect ot' prosody (either duration, F0, or inten- 
sity), then the prosody of a pronoun depends to a cer- 
lain degree on its antecedent. In a statistical analysis, 
we should lind a significant effect of the relevant an- 
tecedent property on the prosodic measure. We selected 
ANOVA as our analysis method, because our prosodic 
target variables appear to have a normal distribution. For 
each of the antecedent features delined above, we ex- 
amined its inlluence on mean F0 (imeanf0), the z- 
score of mean F0 ( l zmeanf0) ,  the z-score of F0 range 
( l z rg f0 ) ,  logarithmic duration (dur) ,  and normalized 
energy (energy) .  In addition, we added the tactors, 
PRONTYPE and SPEAKER. 
Results: The results are summarized in Table 3. For 
i zmeanf0  and energy, the influence of SPEAKER 
is always considerable. There are also consistent ef- 
fects of the syntactic position of a pronoun: In general, 
demonstratives are shorter in subject position, and for 
CK, mean F0 on personal pronouns in subject position 
is higher than on non-subject ones (228 Hz vs. 190 Hz). 
But when we turn to the factors that interest us lnOSt, 
properties of the antecedent, we cannot lind any consis- 
tent correlates, although in ahnost every data set, there 
are some prosodic ues to ANTESUBJ for personal pro- 
nouns. But what these cues are may well depend on the 
speaker, as the results for CK show. Her pitch range on 
pronouns with a stdjcct antecedent is double the range 
on pronouns with an antecedent in non-su/lject position. 
922 
E 
P?l'SOll l l l  l i ro l lOUl lS 
T - - : -  
- -7  : : 
" L ~ 
o ~ 
o o o? 
o 
I I I ~ - -  
NI I lie alllt~ non-NP  I/lO 
"\['ype of AtllCci~dt~lll 
F~ 
7 
t ) I_qll O II,"i \[ fa t  i'? (~ P fo I IO I ln~ 
: 8 
NP IIO1|11\[~ no I I -NP  pfO 
Type of Alltcccdcnl 
y~ 
K 
8 
o 
- -  f 
t~on slkhi 
Perso l | l l l  P fo l lOt l l lS  
- \ ] - -  
1 ; 
o 
o 
o 
l - -  I 
II(II1L~ stilt 
Ai/tecctletll is Stlbjc'ct 
E 
o 
i 
Demonst ra t ive  P ronouns  
m 
Cl 
fi, 
8 
t - - \ ] - - -  i 
I|OII-SlIIj Ill)lie sulj 
AiItecctlct~l is Subject 
Figure 1: Distribution el: z-score of mean F0 for dilferellt values of ANTEFORM and ANTESUI3J 
Pronouns with subject antecedents are also considerably 
louder. All ill all, antecedent prol)ertics can only ac- 
COUllt for a very small percelltage of tile wtriatioll in 
these prosodic ues. Therefore, we should i~ot expect he 
prosodic ues to be slablc, robust indicators for predict- 
ins antecedent properlies ill spoken dialog systems. 
5.2  In ter -Speaker  Var ia t ion  
we have sccn that inter-speaker di ffcrcl~ces cxpl;~i n much 
of the variation in the prosodic measures. Table 4 gives 
an idea of the size and direction of these differences. 
On the complete data set, wc lilKl that personal pro- 
nouns are shorlor lhan demonslratives, they have a lower 
intensity and show a higher average 1;0 (3~tble 4). A 
closer examination reveals considerable inter-speaker 
variation in the data, illustrated in Table 4. CK is fairly 
ptototypical. PH barely shows the difference il~ F0, al~d 
for MF, the difference in intensity is actually reversed. 
MF also has rather shor! demonstratives. Such speaker- 
specilic wlriation callnot be eliminated by nomtalization. 
It has to be controlled for in the statistical lcsls. Dis- 
covering types of speakers is diflicult - two of the 15 
speakers, CK, and PH, con/ribute 48% of all pronouns. 
5.3  P red ic t ing  Proper t ies  o f  t i le  Antecedent  
Finally, we examine how much information prosodic 
cues yield about the ~tntecedent. For this purpose, we 
set till a prediction lask not unlike one that all actual 
NLU syslenl ~lces. The input variables arc the prosodic 
properties of the pronoun, whether the protloun is per- 
sonal or demonstrative (P\]R.ONTYPE), whether it is the 
subject (PRONSUBJ), and whether it is sentence-initial 
(PRONZNIT). From this, we now have to deduce l~roper - 
lies of thc antecedent: syntactic i'olc (ANTESrdBJ), fern1 
(ANTEFORM), and distance (DZST). For prediction, wc 
used logistic regression (Agresti, 1990). This has two ad- 
vantages: not only can wc compare how well the differ- 
cnt regression models lit the data, wc call also re-analyze 
the titled model to determine which factors have a signif- 
icant inlluence oll classiIication accuracy. 
Firsl, we conslrucl a model on the basis of 
PRONTYPE,  PRONSUBJ ,  and PRONIN IT .  Then, 
we conslruct a model with these three faclors plus 
SPEAKER.. finally, we train a model with PRONTYPE, 
923 
Speaker 
dis'c. 
all 156 Hz 
CK 188Hz 
PH 126 Hz 
MF 166 Hz 
mean F0 
pelw. dem. 
157 Hz 142 Hz 
208 Hz 187 Hz 
109 Hz 110 Hz 
184 Hz 182 Hz 
z-score mean 
pets. dent 
-0.04 -0.24 
0.31 0.00 
-0.43 -0.47 
0.32 0.26 
duration 
pelw. dem 
161 ms 206 ms 
151 ms 193 ms 
179ms 252 ms 
166 ms 164 ms 
intensity 
petw. dem 
2.36 2.38 
2.51 2.54 
2.57 2.84 
2.69 2.40 
Table 4: Inter-speaker variation in prosody, disc.: complete discourse. All speakers: 322 pronouns, CK: 41 personal, 
45 demonstrative, PH: 18 personal, 24 demonstrative, MF: 7 personal, 8 demonstrative 
PRONSUBJ, PRONINIT, SPEAKER and one of the 
three measures l zmeanf0 ,  dur ,  energy .  The mod- 
els are trained to predict whether there is an antecedent 
(task noAnte) ,  whether the antecedent is a non-NP 
(task nonNP), whether the antecedent is remote (task 
remote) ,  whether the antecedent is in subject position 
(task u j ante ) ,  and whether the antecedent is the current 
Cb (task cb). All models are computed over the full data 
set, because the data set for speaker CK is not suflicient 
? for estimating the regression coefficients. The models 
are then compared to see which step yielded a significant 
improvement: adding SPEAKER or adding the prosodic 
variable after we have accounted for SPEAKER variation. 
Results: The results arc summarized in Table 5. On 
all tasks except remote ,  PRONTYPE and PRONSUBJ 
performed well. Both features have ah'oady been shown 
to be reliable cnes for prononn resoluti(m (c.f. Sec- 
tion 2). On task cb, only PRONTYPE can explain a 
signilicant amount of wuiation. Models which include 
a speaker factor ahnost always fare better. In models 
without speaker information, F0-relaled measures yield 
a larger reduction in deviance than the duration measure. 
The reason for this is that the F0 measures preserve some 
information about the ditl'ercnt speaker strategies. Once 
SPEAKER has been included as well, only dur  leads 
to significant improvements on task nonNP (p<0.05). 
Both demonstratives and personal pronouns are shorter 
when the antecedent is a non-NR 
6 Conclusion and Outlook 
In this paper, we cxamincd patterns of acoustic prosodic 
highlighting of personal and demonstrative pronouns in 
a corpus of task-oriented spontaneous dialog. To our 
knowledge, this is the lirst comparative study of this 
kind. Wc used a straightforward, theory-neutral opera- 
tionalization of "prosodic highlighting" that does not de- 
pend on complex algorithms for F0 stylization or (focal) 
accent detection and is thus very easy to incorporate into 
any real-time spoken dialog system. We chose a spo- 
ken dialog corpus that includes demonstrativc pronouns 
because demonstratives are both a prominent feature of 
problem-solving dialogs and a sorely neglected lield of 
study. In particular, we asked two questions: 
Do Speakers Signal Antecedent Properties 
Acoustically? Based on our data, the answer to this 
question is: If they do,/hey do it in a highly idiosyncratic 
way. We cannot posit any safe generalizations over sev- 
eral speakers, and li"om the perspective of an NLP appli- 
cation, such generalizations might even be dangerous. In 
order to evaluate the impact of speaker strategies on the 
resolution of pronouns, we need more data - 150 to 200 
pronouns from 4-5 speakers each. Collecting this amount 
of data in a dedicated corpus is inefficient. Therefore, 
further acoustic investigations do not make much sense at 
this point; rather, the data should be examined carefully 
for tendencies which can form the basis for dedicated 
production and perception experiments which arc explic- 
itly designed for uncovering inter-speaker variation. 
Are Acoustic Features Useful for Pronoun 
Resolution? The answer is: probably not. At least for 
this corpus, we were not able to determine any numeri- 
cal heuristics that could be utilized to aid pronoun reso- 
lution. The logistic regression experiments show that on 
a speaker-independent basis, logarithmic duration might 
well be a reliable cue to certain aspects of a pronoun's 
antecedent. In order to incorporate prosodic cues into 
an actual algorithm, we will need more training material 
and a principled evaluation procedure. We will also need 
to take into account other influences, such as dialog acts 
and dialog structure. 
Acknowledgements. Wc would like to thank the three 
anonymous reviewers, Rebecca Passonneau, Lncicn 
Galescu, James Alhm, Michael Strube, Dictmar Lancd 
and Wolf gang Hess for their comments on earlier vet'- 
sions of this work. Donna K. Byron was funded by ONR 
research grant N00014-95-1-1088 and Columbia Univer- 
sity/NSF research grant OPG:1307. For all statistical 
analyses, wc used R (Ihaka and Gentleman, 1996). 
References 
A. Agresti. 1990. Categorical Data Analysis. John Wi- 
ley. 
S. Azzam, K. Humphreys, and R. Gaizauskas 1998. 
E?tending a Simple Co,'eference Algorithm with a 
Focusing Mechanism. In New Approaches to Dis- 
course Anaphora: Proceedings of the Second Collo- 
quium on Discoulwe Anaphora and Anaphor Resolu- 
tion (DAARC2), pages 15-27. 
S. Brennan, M. Friedman, and C. Pollard. 1987. A cen- 
tering approach to pronouns. In Proceedings of the 
25 th Ammal Meeting of the Association.fi~r Compu- 
tational Linguistics (ACL '87), pages 155-162. 
924 
\]hsk 
nonNP 
noAnte  
remote  
s jante  
cb 
significant illfluence 
PRONTYPE, PRONSUBJ, PRONINIT, dur 
PRONTYPE, PRONSUBJ, PRONINIT, SPEAKER. 
110110 
PRONTYPE, PRONSUBJ  
PRONTYPE,  SPEAKER 
Table 5: Perlbrmance of Reg,'ession Models on Tasks. Listed are factors which improve perfornmnce signilicantly 
(p < 0.05) 
D. Byron and J. Allen. 1998. Resolving demonstra- 
tive pronouns in the TRAtNS93 corpus. In New Ap- 
proaches to Discoume AnaFhora: Proceedings of 
the Second Colloquium on Discou/we AnaFhora attd 
Anaphor Resolution (DAARC2), pages 68 - 81. 
I). Byron and A. Stem. 1998. A preliminary model of 
centering in dialog. In Proceedings of tire 36 th An- 
total Meeting of the Association .for Computational 
Linguistics (A CL '98). 
3. Cahn. 1995. The effect of pitch accenting on pro- 
norm referent resolulion. In Proceedings of the 33 tj~ 
Ammal Meeting of" the Association./or Computatiomd 
?ingtdstics (ACL '95), pages 290-292. 
M. F, ckert and M. Strubc. 1999. Resolving discourse de- 
ictic anaphora in dialogs. 111 I~roceedings oJ"the 9 th 
Coq/'erence of the European Chapter of the Associa- 
tion Jbr Conq)utational Linguistics ( I';ACL '99). 
G. Fant and A. Kruckenberg. 1989. Preliminaries 
to tim study of Swedish prose reading and reading 
style. KT'II Speech 7)ansmission Laborato O, Quar- 
terly Progress and Status Report, 2: 1-83. 
T Frelheinl, W. wm 1)onmlelen, and K. 13orthen. 1997. 
lJnguislic constraints on relevance in reference reso- 
lution. In K. Singer, R. Eggert, and G. Anderson, edi- 
tors, CLS, volume 33, pages 99-113. 
B. Grosz, A. Joshi, and S. Weinstein. 1995. Cenlering: 
A framework for modeling the local coherence of dis- 
course. ComputationalLinguistics, 21(2):203-226. 
C. Gussenhoven, B.H. Repp, A. Rietveld, II. P, ump 
and J. Terken. 1997. The perceptual prominence of 
t'undanmntal frequency peaks. J. Acoust. Soc. Ame,:, 
102:3009-3022. 
P. Heeman and J. Allen. 1995. The Trains Spoken l)ia- 
log Corpus. CD-ROM, lJngt, istic Data Consortium. 
B. Heuft 1999. F, ine prominenzbasierte M thode zttt 
Prosodieanalyse trod -synthese. Peter Lang, Frank- 
furt. 
R. Ihaka and R. Gentlenmn (1996). R: A language for 
data analysis and graphics. Journal q/Co/nputational 
and Graphical Statistics, 5:299-314. 
M. Kameymna. 1998. Stressed Pronouns. 111 R Bosch, 
R. van Sandt, editors, The Focus Book, pages 89-112. 
()xford University Press, Oxford. 
G. lmkoff. 1971. P,'esuppositions and relative well- 
formedness. Iii Semantics: An InteMisciplinao, 
Reader in Philosophy, Linguistics, and l'sydtology, 
pages 329-340. Cambridge University Press. 
I. Lchiste. 1970. Suprasegmentals. Mrl" Press, Cam- 
bridge, Mass. 
C Limle. 1979. Focus o1' attention and the choice of 
pronouns in discourse. In qhhny Given, editor, &,max 
and Semantics 12: Discomwe arid ,S),ntax, New York. 
Academic Press. 
C. Nakatani. 1997. The Computational Processing of 
lntonatiolml Prominence: A Functional Prosody Per- 
spective. Ph.l). thesis, Harvard University. 
R. Passonneau. 1989. Gelling al discourse referents. In 
Proceedings of the 27 u' Ammal Meeting of the Associ- 
ation for Computational Linguistics (ACL '89), pages 
51-59. 
M. Rooth. 1992. A theory of focus interpretation. Natu- 
ral Language Semantics, 1:75-112. 
P,. Schiffnlan (Passo,mcau). 1985. DA'course con- 
strair2ts on 'it' and 'that': A study of la/tguage use in 
career-courtseling interviews. Ph.\]). thesis, Universily 
of Chicago. 
\]. Swulvik, editor. 1990. 7he London Coq)us o.fSl~oken 
English: Descril)tion and Reseamh. l.und Universily 
Press, Lund. 
M. Strube 1998. Never look back: An alternative to 
centering. In Proceedings (!/" the 36 th Amlual Meet- 
ing o.f the Association for Comtmtational Linguistics 
(ACL '98), pages 1251-1257. 
J. van Sanlen 1992 Contexlt, al effects on vowel dura- 
tion. Speech Co/mmmication, 11:513-546. 
B. Webber. 1991. Structure and ostension in the inter- 
pretation of discourse deixis. Language and Cognitive 
Processes, 6:107-135. 
E. Zwicker and H. Fastl 1990. Psychoacoustics. 
Springer, Be,'lin. 
925 
Squibs and Discussions 
The Uncommon Denominator: A Proposal for 
Consistent Reporting of Pronoun Resolution 
Results 
Donna K. Byron*  
University of Rochester 
Pronoun resolution studies compute performance inconsistently and describe results incom- 
pletely. We propose a new reporting standard that improves the exposition of individual results 
and the possibility for readers to compare techniques across tudies. We also propose an informative 
new performance metric, the resolution rate, for use in addition to precision and recall. 
1. Introduction 
To describe the merits of new pronoun resolution tect~niques, we often compare them 
with previous approaches using the performance metrics precision and recall. 
Precision P -- c where: 
C = pronouns resolved correctly 
A --- total pronouns attempted 
Recall R -- c where: 
C = pronouns resolved correctly 
T = total pronouns in the test data 
Precision computes how well a technique did what it was designed to do, and is not 
at issue here. Recall is intended as a more general performance measure, yet R scores 
are difficult to interpret due in part to varying methods of calculating T. T includes 
only the pronouns that were included in the study rather than all pronouns in the 
data set. But since different studies consider different sorts of pronouns to be in scope, 
R scores from different studies are difficult to compare. Also, since the pronouns in 
scope for a study might represent a large or small percentage of the pronouns in the 
corpus, R reveals little about a technique's utility for the general problem of pronoun 
resolution. 
This paper proposes a new reporting format and a new performance measure to 
supplement R and P. Pronoun resolution studies differ in many respects, such as the 
method of calculating C and the underlying semantic assumptions, and this proposal 
does not address ways to make the studies themselves more consistent (for discussion 
of these issues, see Walker 1989, van Deemter and Kibble 1999, Mitkov 2000). Instead, 
we propose a reporting format that clarifies the details of a study's test data (especially 
those details that tend to differ between studies) and explicitly derives the numbers 
used to compute performance measures. 
? Department of Computer Science, P. O. Box 270226, Rochester, NY 14627. E-mail: 
dbyron@cs.rochester.edu 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
2. Reporting Pronoun Resolution Performance 
This squib is necessary because past reports of pronoun resolution performance have 
included inconsistent amounts of detail. Some provide complete details of the exper- 
imental design and results, while others (e.g., Byron and Stent 1998) fail to answer 
even basic questions: What pronouns did this study address? Which pronouns were 
resolved correctly? In order for a reader to assess performance scores, a report must 
describe its test data so that the reader knows exactly what the study includes and what 
it excludes from T. This section briefly discusses what details should be provided. 
2.1 Describing the Test Data 
2.1.1 Corpus Type. Each pronoun resolution evaluation is carried out over an evalu- 
ation corpus, for example a set of human conversations or a number of pages from a 
book. Details about the evaluation corpus's genre (written or spoken, news or fiction, 
etc.) and size (e.g., word count, number of discourse units) should be provided to 
help the reader understand how the corpus chosen for evaluation affected the results 
obtained. 
2.1.2 Lexical Coverage. A report should clearly indicate which pronouns the study 
included, called the coverage, by listing each distinct pronoun type (e.g., it, itself, and 
its are shown separately). Some past reports give no coverage details at all, while 
others (e.g., Popescu-Belis and Robba 1997, page 97) precisely state their coverage: 
" / i l / , /e l le / , / l e / , / l a / , /1 ' / , / lu i / , / i l s / ,~ /e l les / .  '' A categorical description, such as 
"\[results are shown for\] personal and possessive pronouns" (Strube 1998, page 1256) 
is insufficient because the author might assume that his exclusion of certain pronouns 
(e.g., first person pronouns) need not be mentioned since they are excluded by most 
other studies. 
2.1.3 Exclusions. Before pronoun resolution is executed, any evaluation corpus must 
be brought into line with the goals of the study by marking individual pronoun tokens 
as included or excluded. 1 Even tokens of pronoun types covered in the study might be 
excluded from the evaluation. The reasons for considering tokens to be out of scope 
for a study are called exclusion criteria, and the set of pronouns remaining after all 
exclusions are applied to the corpus is the evaluation set. 
Different studies apply different exclusions, and pronoun tokens that were ex- 
cluded in one study might be counted as errors in another. Cataphors are a case in 
point. Some pronoun resolution techniques address cataphora (e.g., Lappin and Leass 
1994), so the cataphors are included when calculating the performance for these tech- 
niques. Other techniques are not designed to identify cataphors, and for some of those 
the authors exclude cataphors from their test data (e.g., Ge, Hale, and Charniak 1998) 
while others include them but count the cataphors as errors (e.g., Strube and Hahn 
1999). There are no standard guidelines for what exclusions are reasonable to apply, 
although it would be beneficial for such a standard to exist. Since performance mea- 
sures are based on the number of pronouns in the evaluation set, such inconsistencies 
make recall scores from separate studies difficult to compare. 
Because each study defines its own idiosyncratic set of exclusion criteria, it is 
important hat performance reports clearly list which criteria were applied. Some 
1 Items might be marked in the answer key or in the test corpus itself, for example, by using special 
part-of-speech tags. Space restrictions prevent us from discussing the additional issues of whether 
pleonastics and items to be resolved in the text are identified manually or automatically. 
570 
Byron The Uncommon Denominator 
Table 1 
Pleonastic onstructions in English. 
Extraposition 
Clefts 
Idioms 
Prop-it 
Extraposition moves a clausal subject to the predicate. Most nominal clauses 
can be extraposed, including participles, infinitives, relative clauses, and 
some prepositional c auses. Example: It's good that you cleaned up. 
Clefts provide contrastive stress with a dummy subject it and the focal NP 
placed after the verb. Example: It was Pat who gave us directions. 
Idioms often include vacuous pronouns, for example, hit it off. 
Prop-it is the ascription of properties to an entity with no existential 
force (Quirk and Greenbaum 1973). Examples: (weather) It is raining, 
(time) It is 5 o'clock, and (ambient environment) I  is hot in here. 
reports provide no exclusion details at all, and even when authors do provide them, 
the descriptions they use are often incomplete or confusing, as in these examples: 
? "7 of the pronouns were non-anaphoric and 16 exophoric" (Mitkov 1998, 
page 872). It is unclear what categories of pronouns this statement refers 
to, since exophoric pronouns are nonanaphoric. 
? "Pleonastic pronouns it (i.e. non-anaphoric it) have not been included in 
these results" (Peral, Palomar, and Ferr~ndez 1999, page 71). This 
assertion seems to incorrectly equate the categories pleonastic and 
nonanaphoric. 
? "'It' was not counted when referring to a syntactically recoverable 'that' 
clause or occurring in a time or weather construction" (Hobbs 1986, 
page 344). These are only some of the possible pleonastic onstructions. 
The reader is left to wonder whether all pleonastic items were excluded. 
Without clear and complete xclusion details, it is impossible for future researchers 
to begin with the same evaluation corpus and recreate results, or for readers of the 
report to determine whether they think that the exclusions applied were reasonable. 
To aid future researchers in providing clear and complete xclusion descriptions, the 
terminology important for describing exclusion criteria is briefly reviewed below. Ex- 
clusion categories for nonreferential nd referential items must be kept distinct. 
Nonreferential items include all items lexically identical to pronouns that do not 
refer and that should therefore be excluded from performance statistics for pronoun 
resolution. In English, lexical items called expletives or pleonastics look like pronouns 
but are semantically vacuous. Categories of pleonastic items are defined in Table 1. 
Postal and Pullum (1988) describe tests to discriminate pleonastic from ordinary NPs, 
since the distinction is not always straightforward. In other languages, forms that 
sometimes function as pronouns may also be used as other parts of speech, for exam- 
ple, l' in French. 
Besides pleonastic items, other tokens might be considered nonreferential by a 
particular study. For example, spontaneous discourse may contain pronouns in aban- 
doned fragments that are uninterpretable to humans. In So that'll have ok so you want all 
three boxcars from Dansville? (Heeman and Allen 1995, d93-10.1,utt29), the initial false 
start is discarded, so the abandoned token of that would probably be excluded. 
Referential pronoun tokens can be anaphoric, cataphoric, exophoric, or modified 
to form complete independent references (e.g., He that plants thorns must not expect o 
gather roses (Mitkov 2001). Anaphors "... point back to some previous item" (Halliday 
571 
Computational Linguistics Volume 27, Number 4 
and Hassan 1976, page 14) for their meaning. Many constituents besides pronouns can 
be anaphoric. Anaphors point to preceding discourse, while cataphors point to sub- 
sequent discourse. The stretch of discourse pointed to is a sponsor, and the pronoun 
and sponsor are said to corefer when they refer to the same, rather than to a related 
or inferred, entity. We reserve the term antecedent for coreferential base-NP sponsors. 
Exophors refer outside the discourse to entities in the discourse setting. Cornish (1986) 
and Mitkov (2000) note that the terms nonreferential and nonanaphoric are often con- 
flated, as are anaphoric and coreferential, but the above definitions explain why this 
is incorrect. 
Current research tends to focus only on anaphors, o nonanaphoric tokens are com- 
monly excluded. Anaphoric pronouns with certain properties may also be excluded. 
Some common reasons are: 
. 
. 
. 
. 
. 
Split antecedence: The pronoun is a plural pronoun whose referent must 
be constructed. Example: Pati went to Kimj' s house and theyi+j went dancing. 
Quoted speech: Either the pronoun or its sponsor occurs in reported 
speech. Example: Mr. Vinkeni exclaimed, "The guy ran right in front of reel." 
High-order entities: Pronouns referring to entities uch as propositions 
and events often have sponsors that are not base NPs. Example: \[He 
practiced the tuba all night\]i and iti almost drove me crazy. 
Noncoreference: The pronoun and its sponsor do not corefer. Example: 
The Bakersi+j arrived next. Shei's an astronaut and hej" s a teacher. 
Long-distance reference: The sponsor appears outside a preset window 
utilized by the algorithm. 
2.2 Measuring Performance 
In previous tudies, recall has been computed over the pronouns in scope for a study 
(e.g., only coreferential pronouns, only third person pronouns) rather than all refer- 
ential pronouns. This makes recall rates difficult to compare because the number of 
pronouns in scope for different studies varies. Also, because results are stated in terms 
of the items that were attempted, most studies report similarly high success rates. This 
author has been asked, "Why is work on pronoun resolution still needed when tech- 
nique X gets 93% of pronouns correct?" In fact, technique X correctly resolves 93% of 
singular personal pronouns that have coreferential noun phrase antecedents, which is 
only a fraction of the pronouns needing to be resolved. This hardly makes pronoun res- 
olution a solved problem. But one must read the report carefully to find these details, 
and the fact that the question was asked demonstrates the interpretation problems 
that result from the performance metrics currently in use. 
If the long-term goal of pronoun resolution research is to describe a process for 
interpreting all referential pronouns, there should be a performance number that in- 
dicates how a technique measures up against his goal. The metric we propose, res- 
olution rate, does that by computing the percentage of referential pronouns in the 
evaluation corpus that were resolved correctly. 
The resolution rate RR = ~+~ where: 
? C = number of pronouns resolved correctly 
? T = all pronouns in the evaluation set 
? E = all excluded referential pronouns 
572 
Byron The Uncommon Denominator 
The denominator f 1ttt includes all the pronouns that remain in the evaluation 
corpus after removing nonreferential items and before excluding referential tokens. 
Computing tttt for a technique's performance on a variety of corpora demonstrates the 
technique's sensitivity to its input data. 1111 also provides a way to reward techniques 
that attempt to resolve more sorts of pronouns, such as cataphora or event anaphora. 
Obviously, 1111 applies to techniques that claim general utility but not to those designed 
for specific circumstances, such as the one reported in Suri, McCoy, and DeCristofaro 
(1999) or a technique to handle a particular phenomenon such as cataphora. R and P 
are still useful to show a technique's performance on the in-scope items, and they are 
more informative because the reader knows what percentage of the total pronouns 
were in scope. R uses the above definition of T as its denominator, and P remains 
unchanged. 
All performance measures hould be reported separately for each pronoun type 
covered rather than just for the test corpus as a whole. This facilitates comparing 
results from studies with different coverage or with test data from different genres 
where the mix of pronoun types might be different. It also elucidates the effect hat 
the composition of the evaluation corpus had on the results. 
3. Proposed Reporting Format: The Standard Disclosure 
The standard disclosure includes important details, such as the coverage, performance 
metrics, the size and composition of the evaluation corpus, and the number of pro- 
nouns in each exclusion type, all in a user-friendly format. It includes these details in 
less space than would otherwise be required and spares the author from providing 
textual descriptions ofexclusions, uch as "We have only two examples of sentential or 
VP anaphora ltogether . . . .  Neither Hobbs algorithm nor BFP attempt to cover these 
examples" (Walker 1989, page 257). This leaves more space for commentary on the 
technique(s) being described. We describe the format as it applies to pronoun resolu- 
tion studies, but it can be adapted for other categories of referring expression (e.g., 
descriptive NPs). 
3.1 Explanation of the Format 
Table 2 is a sample disclosure for a fictional study comparing a new technique, Tech- 
nique Beta, with an existing baseline Technique Alpha, on the same English evaluation 
corpus. Footnotes in this example are provided to assist in explaining the format and 
would not be included in an actual disclosure. Italicized row and column headings 
indicate parts of the disclosure that will vary depending on the study being reported 
(they need not be italicized in an actual disclosure), while items not in italics are 
invariant portions of the format. 
The header to the disclosure lists the evaluation corpus used as well as its genre 
and size. In the table proper, a data column is provided for each lexical type covered 
by the study; all types that are not addressed in this study are summarized in the "Out 
of Scope" column. Because pronouns are a closed word class in English, pronoun types 
are best described by showing the different lexical forms. Some flexibility is allowed; 
for example, one might wish to collapse the categories for "He~She" or "Him-/Herself." 
In other languages, or for other forms of referring expressions such as descriptive 
noun phrases, column headings would instead be category labels. 
The first data row, "A: Raw Word Count," contains the count of all tokens of 
that lexical form in the evaluation corpus. The next section details nonreferential ex- 
clusions, resulting in subtotal row "B: Sum Nonreferential." More details could be 
provided in this section at the researcher's discretion; for example, different categories 
573 
Computational Linguistics Volume 27, Number 4 
Table 2 
Sample standard isclosure for a fictional study. 
Evaluation corpus name: Peanut dialogues (Babar et al 1994) 
Genre: Two-party problem-solving dialogues 
Size: 15 dialogues, 937 turns, 31 minutes total speaking time 
Out of 
Pronoun Lexical Types? Her She Herself He Him His Himself  It  Its Itself Scope b Total 
A: Raw Word Count 22 25 3 89 44 7 14 94 12 1 186 497 
Nonreferential Exclusions c
Pleonastic 0 0 0 0 0 0 0 6 0 0 2 8 
Abandoned Utterance 0 0 0 1 0 1 0 0 0 0 2 4 
B: Sum Nonreferential 0 0 0 1 0 1 0 6 0 0 4 12 
C: Total Referential (A--B) 22 25 3 88 44 6 14 88 12 1 182 485 
Referential Exclusions d
Plural 0 0 0 0 0 0 0 0 0 0 120 120 
Demonstrative 0 0 0 0 0 0 0 0 0 0 36 36 
lst/2nd Person 0 0 0 0 0 0 0 0 0 0 24 24 
Reported Speech 0 0 0 1 0 0 0 0 0 0 2 3 
Event Anaphora 0 0 0 0 0 0 0 15 0 0 0 15 
D: Sum Ref Exclusions 0 0 0 1 0 0 0 15 0 0 182 198 
E: Evaluation Set (C -D)  22 25 3 87 44 6 14 73 12 1 0 e 287 
Results 
Technique Alpha 
F:#Correct: Ante (Inter) 7/7 16/17 0/3 35/45 20/21 2/3  0/14 30/41 2/3 0/1 0 112 (82%) 
F:#Correct:Ante(Intra) 15/15 7/8 0/0 35/42 20/23 3/3  0/0  24/32 9/9 0/0 0 113 (86%) 
Errors: Cataphora 0 0 0 7/7 0 0 0 3/3 0 0 0 10 
Errors: Long Distance 0 2/2  0 4/4  0 0 0 4/4  0 0 0 10 
G:#Correct: Refs 21 22 0 67 38 5 0 52 11 0 0 216 (75%) 
Errors: Chaining 1 0 0 0 1 0 0 0 0 0 0 2 
Resolution Rate (G/C) 100% 88% 0% 76% 86% 83% 0% 59% 92% 0% 0% 45% 
New Technique Beta 
H:#Correct: Ante (Inter) 5/7 17/17 3/3  45/45 15/21 2/3 13/14 34/41 3/3 1/1 0 138 (90%) 
H:#Correct:Ante(Intra) 15/15 7/8 0/0 31/42 24/31 3/3 0/0 27/32 6/9 0/0  0 113 (85%) 
Errors: Cataphora 0 0 0 7/7  0 0 0 1/3 0 0 0 8 
Errors: Long Distance 0 2 0 4 0 0 0 4 0 0 0 10 
I:# Correct: Refs 20 23 3 76 38 5 13 61 8 1 0 248 (86%) 
Errors: Chaining 0 0 0 1 2 0 0 0 0 0 0 3 
Resolution Rate d (I/C) 90% 92% 100% 86% 86% 83% 93% 69% 67% 100% 0% 51% 
Notes on the format: 
~Pronouns shown as column headings are those included in this (fictional) study. Other studies would have different 
column headings depending on their coverage or the language of the evaluation corpus. 
bPlurals, demonstratives, l t /2nd  person, reported speech, and event anaphora in this example. 
CCategories in this section differ in different languages. For example, the French le is both a pronoun and a determiner, 
so a study using a French corpus would have an exclusion category for determiners. 
~These are the exclusions applied in our fictional study. For any particular study, the categories listed here may 
differ from these. 
eAll pronouns in the "Out of Scope" category have been explicitly listed, resulting in 0 "Out of Scope" pronouns 
remaining in the evaluation set. 
dThe numerator of RR is either correct referents or correct antecedents, depending on the researcher's goals. 
of pleonastics could be listed separately. Identifying all the nonreferential tokens is 
time-consuming, but need only be performed once for each evaluation corpus. The 
next row, "C: Total Referential," is simply A - B and is used as the denominator 
of/~/iL 
The next section lists referential pronouns excluded from the test set. All the exclu- 
sions applied in the study must be itemized. Categories of pronouns that are clumped 
together in the "Out of Scope" column, such as demonstratives and plurals in this 
example, are listed individually in this section. Row "D: Sum Ref Exclusions" shows 
574 
Byron The Uncommon Denominator 
the total tokens excluded, and the next row, "E: Evaluation Set," is C - D, the resulting 
count of pronouns that are in scope. Notice that because the table starts with raw 
word counts and works forward to the evaluation set, the researcher must explicitly 
account for each excluded token. 
The final section shows the performance of the technique(s) under study. For sys- 
tems that compute referents for the test pronouns, it is recommended that the correct 
antecedents (Ante) and correct referents (Ref) be shown separately to clarify the effect 
of chaining errors. We also recommend calculating performance separately for inter- 
sentential (Inter) and intrasentential (Intra) sponsors, since techniques tend to vary 
across this dimension. Separating the resolution details in this manner is informa- 
tive; however, it is optional. The table could instead show only one number for the 
total correct resolutions per type of pronoun, although that would be less useful to 
the reader. Recall would be included for techniques that do not resolve every item 
attempted. 
Error analysis is optional, but in light of the fact that pronouns that are excluded in 
one study often cause errors in another, it is highly recommended that error details be 
shown for classes of pronouns that are commonly excluded. Other categories of errors 
could be detailed as well if particular error categories are of interest in the study. The 
resolution rate is shown last, calculated as the number of correct resolutions divided 
by the number of referential pronouns in row C. If a technique reports high RR with 
this format, it is easy to tell whether its performance r sults from doing a few things 
well or from doing a mediocre job at everything. 
To summarize, the important features of this format are: 
1. The pronoun types included in the study are readily apparent. 
2. Categories and itemized counts of excluded tokens are clearly shown. 
3. /~/~ can be calculated because the referential exclusions are enumerated. 
3.2 The Benefits of the Standard Disc losure 
By combining details of the evaluation corpus's construction with performance statis- 
tics, the standard isclosure displays many important details in one place, making 
them easy for readers to find. Some authors in the past have stated their performance 
statistics eparately for each pronoun type, while others stated only one overall per- 
formance number per technique. Because a particular technique's performance can 
vary widely across pronoun types (for example, Hobbs's algorithm resolved 93% of 
instances of he but only 77% of instances of it; Hobbs 1986), reporting performance per 
pronoun type should become standard practice. Also, different studies choose different 
combinations of pronouns to investigate, and without detailed performance numbers 
one cannot know how the two techniques compare on the pronouns they have in 
common. Although the only sure way to compare two techniques i in a head-to-head 
test on the same corpus, results stated in the standard isclosure format leave the 
reader better able to judge, for example, if a technique might be appropriate to his 
corpus. 
Providing details on the exclusion criteria applied to the evaluation corpus pro- 
vides a sanity check so that the reader understands how the initial corpus was pared 
down to become the evaluation data set. If the table shows that an unexpectedly high 
percentage ofpronouns were excluded from testing, the reader might wonder whether 
the results obtained are reliable or if, on the other hand, the researcher might have 
overzealously tailored the evaluation set to the capabilities of the algorithm being 
tested. Because many past studies either did not discuss their exclusion categories at 
575 
Computational Linguistics Volume 27, Number 4 
all, described their exclusions with confusing descriptions of the sort listed in Sec- 
tion 2.1.3, or did not state the number of pronouns excluded, the reader must be 
guarded in interpreting the stated results. The tabular format suggested here does not 
guarantee consistent application of the exclusion categories across studies, but it does 
represent an improvement over current practices. Preparing exclusion data might at 
first seem like an extra burden. However, it must only be collected once per eval- 
uation corpus, and much of this information is already collected uring the corpus 
annotation process. As we demonstrated above, many authors already discuss exclu- 
sions in the body of a paper. We believe that the increased clarity that the standard 
disclosure format offers to the reader outweighs any small outlay of time required to 
prepare it. 
Finally, this format allows the researcher to compute RR for general-purpose al-
gorithms, giving the community a more realistic view of how an algorithm performs. 
While in the past the reader knew that a particular technique correctly resolved 93% 
of some subset of pronouns, he had no clear idea what that 93% represented because 
the process used to derive its denominator was so unclear. 
4. Summary 
The reporting format we propose has numerous benefits. Important details of a pro- 
noun resolution study are in one place and easy for readers to find. The information 
is organized to clearly state details that may differ from one study to another so that 
future researchers do not need to reimplement a echnique simply to remove these dif- 
ferences. Its tabular format consumes less space for this additional information, freeing 
up room in the body of a paper for analysis and discussion of the techniques under 
investigation. By tabulating the number of referential pronouns that are excluded, the 
format clarifies the composition of the test data set and enables the calculation of 
the resolution rate (RR), which is a more accurate general measure of performance. 
RR makes a nice addition to the performance metrics currently in use that state per- 
formance in terms of the in-scope pronouns. While it does not solve many of the 
difficulties involved in comparing techniques from different studies, this format does 
offer an incremental improvement over current practices. 
Acknowledgments 
This material is based on work supported 
by ONR Grant N00014-95-1-1088 and 
DARPA Grant F30602-98-2-0133. The author 
thanks James Allen, Nate Blaylock, Jason 
Eisner, Lucian Galescu, Brandon Sanders, 
Amanda Stent, and the anonymous 
reviewers for helpful comments on ideas 
developed here. 
References 
Byron, Donna and Amanda Stent. 1998. A 
preliminary model of centering in dialog. 
In Proceedings ofthe 36th Annual Meeting of 
the Association for Computational Linguistics 
and 17th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 1475-1477. 
Cornish, Francis. 1986. Anaphoric Relations in 
English and French. Croom Helm. 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical pproach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Very Large Corpora, 
pages 161-170. 
Halliday, M. A. K. and Ruqaiya Hassan. 
1976. Cohesion in English. Longman. 
Heeman, Peter A. and James Allen. 1995. 
The Trains spoken dialog corpus. 
CD-ROM, Linguistics Data Consortium. 
Hobbs, Jerry. 1986. Resolving pronoun 
reference. In Barbara J. Grosz, Karen 
Sparck Jones, and Bonnie Lynn Webber, 
editors, Readings in Natural Language 
Processing. Morgan Kaufmann, pages 
339-352. 
Lappin, Shalom and Herbert J. Leass. 1994. 
An algorithm for pronominal naphora 
resolution. Computational Linguistics, 
20(4):535-561. 
576 
Byron The Uncommon Denominator 
Mitkov, Ruslan. 1998. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 869-875. 
Mitkov, Ruslan. 2000. Towards a more 
consistent and comprehensive evaluation 
of anaphora resolution algorithms and 
systems. In Proceedings ofthe Discourse 
Anaphora nd Anaphora Resolution 
Conference (DAARC2000), pages 96-107. 
Mitkov, Ruslan. 2001. Anaphora Resolution. 
Longman. 
Peral, Jestis, Manuel Palomar, and Antonio 
Ferr~indez. 1999. Coreference-oriented 
interlingual slot structure and machine 
translation. In Proceedings ofthe Workshop 
on Coreference and Its Applications (ACL'99), 
pages 69-76. 
Popescu-Belis, Andrei and Isabelle Robba. 
1997. Cooperation between pronoun and 
reference resolution for unrestricted texts. 
In Proceedings ofthe ACL Workshop on 
Operational Factors in Practical, Robust 
Anaphora Resolution for Unrestricted Texts, 
pages 94-99. 
Postal, Paul M. and Geoffrey K. Pullum. 
1988. Expletive noun phrases in 
subcategorized positions. Linguistic 
Inquiry, 19:635-670. 
Quirk, Randolph and Sidney Greenbaum. 
1973. A University Grammar of English. 
Longman. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), 
pages 1251-1257. 
Strube, Michael and Udo Hahn. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Suri, Linda Z., Kathleen F. McCoy, and 
Jonathan D. DeCristofaro. 1999. A 
methodology for extending focusing 
frameworks. Computational Linguistics, 
25(2):173-194. 
van Deemter, Kees and Rodger Kibble. 1999. 
What is coreference, and what should 
coreference annotation be? In Proceedings 
of the Workshop on Coreference and Its 
Applications (ACL'99), pages 90-96. 
Walker, Marilyn A. 1989. Evaluating 
discourse processing algorithms. In 
Proceedings ofthe 27th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'89), pages 251-261. 
577 

 
	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Validating the web-based evaluation of NLG systems
Alexander Koller
Saarland U.
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Donna Byron
Northeastern U.
dbyron@ccs.neu.edu
Justine Cassell
Northwestern U.
justine@northwestern.edu
Robert Dale
Macquarie U.
Robert.Dale@mq.edu.au
Sara Dalzel-Job
U. of Edinburgh
S.Dalzel-Job@sms.ed.ac.uk
Jon Oberlander
U. of Edinburgh
Johanna Moore
U. of Edinburgh
{J.Oberlander|J.Moore}@ed.ac.uk
Abstract
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
1 Introduction
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al, 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system?s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al, 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects? behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al, 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
2 The GIVE Challenge
In the GIVE scenario (Byron et al, 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
301
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
3 The experiments
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn?t crash, the game
wasn?t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least ?expert? (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world ? World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as ?expert? or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p < 0.05. Significance was tested using a ?
2
-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise ?
2
and Tukey) to com-
pare the NLG systems pairwise.
Results: Subjective measures. Users were
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al, 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
302
Objective Measures Subjective Measures
task
success
instructions steps actions seconds overall
choice
of words
referring
expressions
timing
A 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% A
M 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABC
T 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% AB
U 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% C
W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC
A 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A B
M 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A B
T 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A B
U 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% A
W 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% B
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
didn?t fill in the ?overall evaluation? field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
4 Discussion
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures ? 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn?t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
Web
games reported mean
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
Lab
# games reported mean
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 ? ?
Figure 2: Skewed results for ?overall evaluation?.
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including ?overall eval-
uation? in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al, 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as ?expert?
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3?5 performed significantly better
on ?task success? than the group with proficiencies
1?2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the ?overall evaluation? question. Users who didn?t
complete the task successfully tended to judge the
303
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment ?
whether in the laboratory or on the Internet ? suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user?s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system?s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
5 Conclusion
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users? language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
A. Belz. 2009. That?s nice ... what can you do with it?
Computational Linguistics, 35(1):111?118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a ?virtual? maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73?87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
304
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
1 Introduction
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
2 The GIVE Challenge
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3?5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
165
Figure 1: What the user sees when playing with
the GIVE Challenge.
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
2.1 Why a new NLG evaluation paradigm?
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system?s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
2.2 Why study instruction-giving?
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al, 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale & Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it ? and specif-
ically the instruction-giving task ? can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system?s efficiency). But if
166
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
3 Evaluation Method and Logistics
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
3.1 Software architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
3.2 Subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 2: The GIVE architecture.
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject?s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
3.3 Materials
Figs. 3?5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
167
plant
chair
alarm
lamp
tutorial room
couch
safe
Figure 3: World 1
lamp
plant
chair
alarm
tutorial room
safe
Figure 4: World 2
plant
chair
lamp
safe
tutorial room
alarm
Figure 5: World 3
3.4 Timeline
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
3.5 NLG systems
Five NLG systems were evaluated in GIVE-1:
1. one system from the University of Texas at
Austin (?Austin? in the graphics below);
2. one system from Union College in Schenec-
tady, NY (?Union?);
3. one system from the Universidad Com-
plutense de Madrid (?Madrid?);
4. two systems from the University of Twente:
one serious contribution (?Twente?) and one
more playful one (?Warm-Cold?).
Of these systems, ?Austin? can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The ?Warm-
Cold? system generates very vague instructions
that only tell the user if they are getting closer
(?warmer?) to their next objective or if they are
moving away from it (?colder?). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team?s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
4 Results
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?), such as the ?Austin? baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
4.1 Demographics
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn?t crash, the game wasn?t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn?t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
168
037,5
75,0
112,5
150,0
N
o
v
 
7
D
e
c
 
1
J
a
n
 
1
F
e
b
 
1
F
e
b
 
5
# games per day
German
press release
US
press release
posted to
SIGGEN list
covered by
Chinese blog
Figure 6: Histogram of the connections per day.
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as ?good? or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
4.2 Objective measures
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
? task success (Did the player get the trophy?)
? instructions (Number of instructions pro-
duced by the NLG system.?)
? steps (Number of all player actions.?)
? actions (Number of object manipulation
action.?)
? second (Time in seconds.?)
?
Measured from the end of the tutorial until the
end of the game.
Figure 7: Objective measurements
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
success
40% 71% 35% 73% 18%
A A
B B
C
instructions
83.2 58.3 121.2 80.3 190.0
A
B B
C
D
steps
103.6 124.3 160.9 117.5 307.4
A A
B B
C
D
actions
11.2 8.7 14.3 9.0 14.3
A A
B
C C
seconds
129.3 174.8 207.0 175.2 312.2
A
B B
C
D
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don?t share a letter were found to be significantly
different with p < 0.05.
169
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user?s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don?t share the same letter, the dif-
ference between these two systems is significant
with p < 0.05. Significance was tested using a
?2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise ?2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
4.3 Subjective measures
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (?overall?
on a 7-point scale); the ?informativity? and ?tim-
ing? questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran ?2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise ?2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the ?play again? category, there is
no significant difference at all. Nevertheless,
?Austin? is shown to be particularly good at navi-
gation instructions and timing, whereas ?Madrid?
outperforms the rest of the field in ?informativ-
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 . . . 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system?s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system?s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
Nominal items:
informativity: Did you feel the amount of information you
were given was: too little / just right / too much
timing: Did the directions come ... too early / just at the right
time / too late
Figure 9: Questionnaire items
ity?. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, ?Warm-
Cold? again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on ?infor-
mativity? and ?timing? in terms of the number of
users who judged them as ?just right?, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
170
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
difficulty
4.3 4.3 4.0 4.3 3.5
A A A A
B
goal clarity
4.0 3.7 3.9 3.7 3.3
A A A A
B
play again
2.8 2.6 2.4 2.9 2.5
A A A A A
instruction
clarity
4.0 3.6 3.8 3.6 3.0
A A A
B B B
C
instruction
helpfulness
3.8 3.9 3.6 3.7 2.9
A A A A
B
informativity
46% 68% 51% 56% 51%
A
B B B B
overall
4.9 4.9 4.3 4.6 3.6
A A A
B B
C
choice of
words
4.2 3.8 4.1 3.7 3.5
A A
B B
C C C
referring
expressions
3.4 3.9 3.7 3.7 3.5
A A A
B B B B
navigation
instructions
4.6 4.0 4.0 3.7 3.2
A
B B B
C
timing
78% 62% 60% 62% 49%
A
B B B
C C
friendliness
3.4 3.8 3.1 3.6 3.1
A A A
B B B
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don?t share a letter were found
to be significantly different with p < 0.05.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. ?2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players? English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don?t manage to lead players with only
basic English skills to success as often as other
players, Union?s and Twente?s success rates do not
depend on the players? English skills (?2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
5 Conclusion
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
171
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
Figure 12: Effect of the players? English skills on
the success rate of the NLG systems.
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 ? by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
172
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
173
Proceedings of the EACL 2009 Demonstrations Session, pages 33?36,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
The Software Architecture for the
First Challenge on Generating Instructions in Virtual Environments
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Kristina Striegnitz
Union College
striegnk@union.edu
Abstract
The GIVE Challenge is a new Internet-
based evaluation effort for natural lan-
guage generation systems. In this paper,
we motivate and describe the software in-
frastructure that we developed to support
this challenge.
1 Introduction
Natural language generation (NLG) systems are
notoriously hard to evaluate. On the one hand,
simply comparing system outputs to a gold stan-
dard is not appropriate because there can be mul-
tiple generated outputs that are equally good, and
finding metrics that account for this variability and
produce results consistent with human judgments
and task performance measures is difficult (Belz
and Gatt, 2008; Stent et al, 2005; Foster, 2008).
On the other hand, lab-based evaluations with hu-
man subjects to assess each aspect of the system?s
functionality are expensive and time-consuming.
These characteristics make it hard to compare dif-
ferent systems and measure progress.
GIVE (?Generating Instructions in Virtual En-
vironments?) (Koller et al, 2007) is a research
challenge for the NLG community designed to
provide a new approach to NLG system evalua-
tion. In the GIVE scenario, users try to solve
a treasure hunt in a virtual 3D world that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual envi-
ronment. The challenge for the NLG system is
to generate, in real time, natural-language instruc-
tions that will guide the users to the successful
completion of their task (see Fig. 1). One cru-
cial advantage of this generation task is that the
NLG system and the user can be physically sepa-
rated. This makes it possible to carry out a task-
based evaluation over the Internet ? an approach
that has been shown to provide generous amounts
Figure 1: The GIVE Challenge.
of data in earlier studies (von Ahn and Dabbish,
2004; Orkin and Roy, 2007).
In this paper, we describe the software archi-
tecture underlying the GIVE Challenge. The soft-
ware connects each player in a 3D game world
with an NLG system over the Internet. It is imple-
mented and open source, and can be a used online
during EACL at www.give-challenge.org.
In Section 2, we give an introduction to the GIVE
evaluation methodology by describing the experi-
ence of a user participating in the evaluation, the
nature of the data we collect, and our scientific
goals. Then we explain the software architecture
behind the scenes and sketch the API that concrete
NLG systems must implement in Section 3. In
Section 4, we present some preliminary evaluation
results, before we conclude in Section 5.
2 Evaluation method
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
33
b2 b3b4 b5
b6
b7
b1
player
b8b9
b10
b11 b14b13b12
safe 
door
b1 opens doorto room 3
b9 moves picture to
b8: part of safe sequencereveal safe
? to win you have to retrieve the trophy from the safe in room 1? use button b9 to move the picture (and get access to the safe)
? if the alarm sounds, the game is over and you have lost
? press buttons b8, b6, b13, b13, b10 (in this order) to open the safe;if a button is pressed in the wrong order, the whole sequence is reset
b14 makes alarm soundb10, b13: part of safe sequence door to room 2b7 opens/closesstepping on this tiletriggers alarm
alarm
room 3
b2 turns off alarm tileb3 opens/closes door to room 2
b6: part of safe sequence
room 1
b5 makes alarm sound
room 2
door
door
lampcouch
chair
flower
pictu
retrophy
Figure 2: The map of a virtual world.
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system.
The map of one of the game worlds is shown in
Fig. 2: In this world, players must pick up a trophy,
which is in a wall safe behind a picture. In order
to access the trophy, they must first push a button
to move the picture to the side, and then push an-
other sequence of buttons to open the safe. One
floor tile is alarmed, and players lose the game
if they step on this tile without deactivating the
alarm first. There are also a number of distrac-
tor buttons which either do nothing when pressed
or set off an alarm. These distractor buttons are in-
tended to make the game harder and, more impor-
tantly, to require appropriate reference to objects
in the game world. Finally, game worlds can con-
tain a number of objects such as chairs and flowers
which are irrelevant for the task, but can be used
as landmarks by a generation system.
Users are asked to fill out a before- and after-
game questionnaire that collects some demo-
graphic data and asks the user to rate various as-
pects of the instructions they received. Every ac-
tion that players take in a game world, and every
instruction that a generation system generates for
them, is recorded in a database. In addition to the
questionnaire data, we are thus able to compute a
number of objective measures such as:
? the percentage of users each system leads to
a successful completion of the task;
? the average time, the average number of in-
structions, and the average number of in-
game actions that this success requires;
? the percentage of generated referring expres-
sions that the user resolves correctly; and
? average reaction times to instructions.
It is important to note that we have designed
the GIVE Challenge not as a competition, but as
a friendly evaluation effort where people try to
learn from each other?s successes. This is reflected
in the evaluation measures above, which are in
tension with one another: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?) will enjoy short reaction times, but it will re-
quire more instructions than a system that aggre-
gates these. To further emphasize this perspective,
we will also provide a number of diagnostic tools,
such as heat maps that show how much time users
spent on each tile, or a playback function which
displays an entire game run in real time.
In summary, the GIVE Challenge is a novel
evaluation effort for NLG systems. It is motivated
by real applications (such as pedestrian navigation
and the generation of task instructions), makes
no assumptions about the internal structure of an
NLG system, and emphasizes the situated genera-
tion of discourse in a simulated physical environ-
ment. The game world is scalable; it can be made
more complex and it can be adapted to focus on
specific issues in natural language generation.
3 Architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components:
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide.
34
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 3: The GIVE architecture.
When a user starts the client, it connects over
the Internet to the Matchmaker. The Matchmaker
then selects a game world and an NLG server at
random, and requests the NLG server to spawn
a new server instance. It then sends the game
world to the client and the server instance and dis-
connects from them, ready to handle new connec-
tions from other clients. The client and the server
instance play one game together: Whenever the
user does something, the client sends a message
about this to the server instance, and the server in-
stance can also send a message back to the client
at any time, which will then be displayed as an in-
struction. When the game ends, the client and the
server instance disconnect from each other. The
server instance sends a log of all game events to
the Matchmaker, and the client sends the ques-
tionnaire results to the Matchmaker; these then are
stored in the database for later analysis.
All of these components are implemented in
Java. This allows the client to be portable across
all major operating systems, and to be started di-
rectly from the website via Java Web Start without
the need for software installation. We felt it was
important to make startup of the client as effort-
less as possible, in order to maximize the num-
ber of users willing to play the game. Unsurpris-
ingly, we had to spend the majority of the pro-
gramming time on the 3D graphics (based on the
free jMonkeyEngine library) and the networking
code. We could have reduced the effort required
for these programming tasks by building upon an
existing virtual 3D world system such as Second
Life. However, we judged that the effort needed to
adapt such a system to our needs would have been
at least as high (in particular, we would have had
to ensure that the user could only move according
to the rules of the GIVE game and to instrument
the virtual world to obtain real-time updates about
events), and the result would have been less exten-
abstract class NlgSystem:
void connectionEstablished();
void connectionDisconnected();
void handleStatusInformation(Position playerPosition,
Orientation playerOrientation,
List?String? visibleObjects);
void handleAction(Atom actionInstance,
List?Formula? updates);
void handleDidNotUnderstand();
void handleMoveTurnAction(Direction direction);
. . .
Figure 4: The interface of an NLG system.
sible to future installments of the challenge.
Since we provided all the 3D, networking, and
database code, the research teams being evaluated
were able to concentrate on the development of
their NLG systems. Our only requirement was
that they implement a concrete subclass of the
class NlgSystem, shown in Fig. 4. This involves
overriding the six abstract callback methods in
this class with concrete implementations in
which the NLG system reacts to specific events.
The methods connectionEstablished
and connectionDisconnected are called
when users enter the game world and when
they disconnect from the game. The method
handleAction gets called whenever the user
performs some physical action, such as pushing a
button, and specifies what changed in the world
due to this action; handleMoveTurnAction
gets called whenever the user moves;
handleDidNotUnderstand gets called
whenever users press the H key to signal that
they didn?t understand the previous instruction;
and handleStatusInformation gets called
once per second and after each user action to
inform the server of the player?s position and
orientation and the visible objects. Ultimately,
each of these method calls gets triggered by a
message that the client sends over the network
in reaction to some event; but this is completely
hidden from the NLG system developer.
The NLG system can use the method send to
send a string to the client to be displayed. It also
has access to various methods querying the state of
the game world and to an interface to an external
planner which can compute a sequence of actions
leading to the goal.
4 First results
For this first installment of the GIVE Challenge,
four research teams from the US, the Netherlands,
35
and Spain provided generation systems, and a
number of other research groups expressed their
interest in participating, but weren?t able to partic-
ipate due to time constraints. Given that this was
the first time we organized this task, we find this
a very encouraging number. All four of the teams
consisted primarily of students who implemented
the NLG systems over the Northern-hemisphere
summer. This is in line with our goal of tak-
ing this first iteration as a ?dry run? in which we
could fine-tune the software, learn about the easy
and hard aspects of the challenge, and validate the
evaluation methodology.
Public involvement in the GIVE Challenge was
launched with a press release in early Novem-
ber 2008; the Matchmaker and the NLG servers
were then kept running until late January 2009.
During this time, online users played over 1100
games, which translates into roughly 75 game runs
for each experimental condition (i.e., five differ-
ent NLG systems paired with three different game
worlds). To our knowledge, this makes GIVE the
largest NLG evaluation effort yet in terms of ex-
perimental subjects.
While we have not yet carried out the detailed
evaluation, the preliminary results look promising:
a casual inspection shows that there are consider-
able differences in task success rate among the dif-
ferent systems.
While there is growing evidence from differ-
ent research areas that the results of Internet-based
evaluations are consistent with more traditional
lab-based experiments (e.g., (Keller et al, 2008;
Gosling et al, 2004)), the issue is not yet set-
tled. Therefore, we are currently conducting a lab-
based evaluation of the GIVE NLG systems, and
will compare those results to the qualitative and
quantitative data provided by the online subjects.
5 Conclusion
In this paper, we have sketched the GIVE Chal-
lenge and the software infrastructure we have de-
veloped for it. The GIVE Challenge is, to the
best of our knowledge, the largest-scale NLG eval-
uation effort with human experimental subjects.
This is made possible by connecting users and
NLG systems over the Internet; we collect eval-
uation data automatically and unobtrusively while
the user simply plays a 3D game. While we will
report on the results of the evaluation in more de-
tail at a later time, first results seem encouraging
in that the performance of different NLG systems
differs considerably.
In the future, we will extend the GIVE Chal-
lenge to harder tasks. Possibilities includ mak-
ing GIVE into a dialogue challenge by allowing
the user to speak as well as act in the world; run-
ning the challenge in a continuous world rather
than a world that only allows discrete movements;
or making it multimodal by allowing the NLG
system to generate arrows or virtual human ges-
tures. All these changes would only require lim-
ited changes to the GIVE software architecture.
However, the exact nature of future directions re-
mains to be discussed with the community.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008. Timing accuracy of web experiments: A case
study using the WebExp software package. Behav-
ior Research Methods, to appear.
A. Koller, J. Moore, B. di Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In M. White and R. Dale, editors, Work-
ing group reports of the Workshop on Shared Tasks
and Comparative Evaluation in Natural Language
Generation. Available at http://www.ling.
ohio-state.edu/nlgeval07/report.html.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
36
gency dispatcher, cooperating with the system to 
dynamically allocate resources to and make 
plans for solving problems as they arise in the 
world. The setting, Monroe County, NY, is con- 
siderably more complex than our previous do- 
mains (e.g. Pacifica, TRAINS), and raises new 
issues in knowledge representation a d refer- 
ence. Emergencies include requests for medical 
assistance, car accidents, civil disorder, and 
larger problems such as flooding and snow 
storms. Resources at the user's disposal may 
include road crews, electric crews, ambulances, 
police units and helicopters. Some of the in- 
crease in mixed-initiative interaction comes 
from givi-n~ the_ system more knowledge of the 
tasks being solved. Some comes from the fact 
that the solution to one problem may conflict 
with the solution to another, either because of 
scheduling conflicts, scarce resources, or aspects 
of the physical world (e.g. an ambulance can't go 
down a road that has not been plowed). The 
range of tasks and complexity of the world allow 
for problem solving at different levels of granu- 
larity, making it possible for the system to take 
as much control over the task as the user per- 
mits. 
4. Important  Contr ibut ions  
While a number of robust dialogue systems have 
been built in recent years, they mostly have op- 
erated in domains that require little if any rea- 
soning. Rather, the task is hard-coded into the 
system operation. One of the major goals of the 
TRIPS project has been to develop dialogue 
models and system architectures that support 
conversational interaction in domains where 
complex reasoning systems are required. One 
goal has been to build a fairly generic model in 
which different domains can then be specified 
fairly easily. On this front, we are seeing some 
success as we have now constructed versions of 
TRIPS in three different domains, and TRIPS? 
911 will be the fourth. In developing the system 
for new domains, the bulk of the work by far has 
been in system enhancements rather than in 
developing the domain models. 
The TRIPS-911 domain has forced a rethinking 
of the relationship between dialogue- 
management, problem-solving, the system's 
Figure 1: Monroe County map used in TRIPS-911 
own goal-pursuit and generation. The new ar- 
chitecture is designed to support research into 
mixed-initiative interactions, incremental gen- 
eration of content (in which the user might in- 
tervene before the system completes all it has to 
say), rich reference resolution models, and the 
introduction of plan monitoring and plan repair 
into the suite of plan management operations 
supported. The domain also can support longer 
and richer dialogues than in previous domains. 
More complex domains mean even more com- 
plex dialogues. The complexity arises from 
many factors. First, more complex dialogues 
will involve topic progression, development and 
resumption, and more complex referential phe- 
nomena. On the problem solving front, there will 
be more complex corrections, elaborations and 
modifications--forcing us to develop richer 
discourse models. In addition, the complexity of 
the domain demonstrates a need for better 
grounding behavior and a need for incremental 
dialogue-based generation. 
We have by no means solved these problems. 
Rather we have built a rich testbed, designed and 
implemented a plausible architecture, and have 
constructed an initial system to demonstrate 
basic capabilities in each of the problem areas. 
34 
5. Limitations 
TRIPS-911 is a first attempt at handling a do- 
main of this complexity. As such there are many 
capabilities that people have in such situations 
that are beyond the system's current capabilities. 
Some of the most important are: 
? Scale - we can only handle small domains 
and the existing techniques would not ex- 
tend directly to a realistic size 911 operation. 
To scale up we must face some difficult 
problems including reasoning about quanti- 
ties and aggregates, planning in large-scale 
domains (i.e., the real domains are beyond 
the capabilities of current plan technology), 
and performing intention recognition as the 
number of options increases. In addition, for 
an effective dialogue system, all this must be 
done in real-time. 
? Meta-talk - when faced with complex prob- 
lems, people often first generally discuss the 
problem and possible strategies for solving 
it, and later may explicitly direct attention to 
specific subproblems. The current TRIPS 
system does not support such discussion. 
? Time - in the 911 domain there are at least 
two temporal contexts that can be "used" by 
the conversants: there is the actual time (i.e., 
when they are talking), but there also is the 
time relative to a point of focus in a plan, or 
even simply talking about the past or the 
future. TRIPS-911 can currently interpret 
expressions with respect to the actual time. 
? Interleaved generation - when people are 
discussing complex issues, they often have 
to plan to communicate heir content across 
several different utterances. There is no 
guarantee that the other conversant will not 
"interrupt" (e.g., to clarify, correct, suggest 
alternatives, etc) before the entire content is 
conveyed. This requires a rethinking of cur- 
rent practice in generation to make it incre- 
mental and interactive. 
? True interruptions - people may interrupt the 
system while it is talking. It is unclear at this 
stage what the system should assume was 
conveyed. The strategies of assuming noth- 
ing was conveyed, or that all was conveyed 
have obvious faults. We are pursuing alter- 
natives based on knowing when speech was 
interrupted, but using this ififormation suc- 
cessfully remains adifficult problem. 
References 
Allen, James et al An Architecture for a Generic 
Dialogue Shell, to appear, J. Natural Language 
Engineering, 2000. 
Ferguson, George and J. Allen,-TRIPS: An Integrated 
Intelligent Problem-Solving Assistant, Proc. Na- 
tional Conference on AI (AAAI-98), Madison, WI, 
1998. 
35 
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
