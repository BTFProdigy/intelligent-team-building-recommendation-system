Proceedings of the 12th European Workshop on Natural Language Generation, pages 185?186,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Generation of Referring Expression with an Individual Imprint
Bernd Bohnet
International Computer Science Institute
1947 Center Street, CA 94704 Berkeley
bohnet@icsi.berkeley.edu
Abstract
A major outcome of the last Shared Tasks
for Referring Expressions Generation was
that each human prefers distinct proper-
ties, syntax and lexical units for building
referring expressions. One of the reasons
for this seems to be that entities might
be identified faster since the conversation
partner has already some knowledge about
how his conversation partner builds refer-
ring expressions. Therefore, artificial re-
ferring expressions should provide such
individual preferences as well so that they
become human like. With this contribu-
tion to the shared task, we follow this idea
again. For the development set, we got a
very good DICE score of 0.88 for the fur-
niture domain and of 0.79 for the people
domain.
1 Introduction
We expect that the test set does not provide the
information to which human a referring expres-
sion belongs. Therefore, we implemented a fall
back strategy in order to get still acceptable DICE
scores. In such cases, we select among all sets the
set of referring expressions which is most similar
to all others. We compute the similarity between
two sets as the average DICE score between all re-
ferring expression of two sets. The basis for our
algorithm is an extended full brevity implementa-
tion, cf. (Bohnet and Dale, 2005). IS-FP uses also
the nearest neighbor technique like the IS-FBN al-
gorithm that was introduced by Bohnet (2007).
With the nearest neighbor technique, IS-FP se-
lects the expressions which are most similar to
the referring expressions of the same human and
a human that builds referring expressions similar
or in the case that the human is unknown it uses
the most similar one to all others referring expres-
sions. The similarity is computed as the average of
all DICE scores between all combinations of the
available trails for two humans. From the result
of the nearest neighbor evaluation, FP selects the
shortest and if still more than one expressions re-
main then it computes the similarity among them
and chooses the most typical and finally, if still al-
ternatives remain, it selects one with the attributes
having the highest frequency. Table 1 shows the
results for IS-FP trained on the training set and ap-
plied to the development set.
Set Dice MASI Accuracy .
Furniture 0.880 0.691 51.25%
People 0.794 0.558 36.8%
Total 0.837 0.625 44%
Table 1: Results for the IS-FP algorithm
2 IS-GT: Realization with Graph
Transducers
We build the input dependency tree for the text
generator due to the statistical information that we
collect from the training data for each person. This
procedure is consistent with our referring expres-
sion generator IS-FP that reproduces the individ-
ual imprint in a referring expression for the target
person. We start with the realization of the refer-
ring expressions from a surface syntactic depen-
dency tree, cf. (Mel?c?uk, 1988). For the realiza-
tion of the text, we use the Text Generator and Lin-
guistic Environment MATE.
185
3 The Referring Expression Models
An algorithm learns a Referring Expression Model
for each person that contributed referring expres-
sion to the corpus. The model contains the follow-
ing information:
(1) The lexicalization for the values of a attribute
such as couch for the value sofa, man for
value person, etc.
(2) The preferred usage of determiners for the
type that can be definite (the), indefinite (a),
no article.
(3) The syntactic preferences such as the top left
chair, the chair at the bottom to the left, etc.
The information about the determiner and the
lexicalization is collected from the annotated word
string and the word string itself. We collect the
most frequent usage for each person in the corpus.
In order to collect the preferred syntax, we anno-
tated the word strings with syntactic dependency
trees. Each of the dependency tress contains ad-
ditional attributes, which describe the information
content of a branch outgoing from the root as well
as the possible value of the attribute at the nodes
which carry the information. The learning pro-
gram cuts the syntactic tree at edges starting at the
root node and stores the branches in the referring
expression model for the person.
4 Realization
For the realization, we use a handcrafted grammar
that generates out of the dependency trees topo-
logic graphs. The main task of the grammar is to
determine the word order. The system was devel-
oped only by using the training data without any
consideration of the development data. We used
as guide for the optimization cross validation of
training data.
5 IS-FP-GT: The Combination of
Attribute Selection and Realization
For the combination of the both methods, we com-
bine the two procedure in a pipeline architecture.
Table 2 shows the results.
6 Conclusion
The IS-FP algorithm reproduces the imprint of hu-
man referring expressions. When the test set con-
tains the reference to the human then the scores are
exceptional high.
Set Accuracy String ED Mean SED Blue 3
Furniture 15 % 3,8625 0.3826 0.3684
People 4,41 % 4,764 0.4817 0.2263
Total 9,71 4,313 0.4321 0.297
Table 2: Results for the TUNA-REG Task
References
B. Bohnet and R. Dale. 2005. Viewing referring
expression generation as search. In IJCAI, pages
1004?1009.
B. Bohnet. 2007. IS-FBN, IS-FBS, IS-IAC: The Adap-
tation of Two Classic Algorithms for the Generation
of Referring Expressions in order to Produce Ex-
pressions like Humans Do. In MT Summit XI, UC-
NLG+MT, pages 84?86.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press, Al-
bany.
186
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 67?72,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Parsing of Syntactic and Semantic Dependency Structures
Bernd Bohnet
International Computer Science Institute
1947 Center Street
Berkeley 94704, California
bohnet@icsi.Berkeley.edu
Abstract
In this paper, we describe our system for the
2009 CoNLL shared task for joint parsing of
syntactic and semantic dependency structures
of multiple languages. Our system combines
and implements efficient parsing techniques to
get a high accuracy as well as very good pars-
ing and training time. For the applications
of syntactic and semantic parsing, the pars-
ing time and memory footprint are very im-
portant. We think that also the development of
systems can profit from this since one can per-
form more experiments in the given time. For
the subtask of syntactic dependency parsing,
we could reach the second place with an ac-
curacy in average of 85.68 which is only 0.09
points behind the first ranked system. For this
task, our system has the highest accuracy for
English with 89.88, German with 87.48 and
the out-of-domain data in average with 78.79.
The semantic role labeler works not as well as
our parser and we reached therefore the fourth
place (ranked by the macro F1 score) in the
joint task for syntactic and semantic depen-
dency parsing.
1 Introduction
Depedendency parsing and semantic role labeling
improved in the last years significantly. One of the
reasons are CoNLL shared tasks for syntactic de-
pendency parsing in the years 2006, 2007 (Buch-
holz and Marsi, 2006; Nivre et al, 2007) and the
CoNLL shared task for joint parsing of syntactic and
semantic dependencies in the year 2008 and of cause
this shared task in 2009, cf. (Surdeanu et al, 2008;
Hajic? et al, 2009). The CoNLL Shared Task 2009
is to parse syntactic and semantic dependencies of
seven languages. Therefore, training and develop-
ment data in form of annotated corpora for Cata-
lan, Chinese, Czech, English, German, Japanese and
Spanish is provided, cf. (Taule? et al, 2008; Palmer
and Xue, 2009; Hajic? et al, 2006; Surdeanu et al,
2008; Burchardt et al, 2006; Kawahara et al, 2002).
There are two main approaches to dependency
parsing: Maximum Spanning Tree (MST) based de-
pendency parsing and Transition based dependency
parsing, cf. (Eisner, 1996; Nivre et al, 2004; Mc-
Donald and Pereira, 2006). Our system uses the first
approach since we saw better chance to improve the
parsing speed and additionally, the MST had so far
slightly better parsing results. For the task of seman-
tic role labeling, we adopted a pipeline architecture
where we used for each step the same learning tech-
nique (SVM) since we opted for the possibility to
build a synchronous combined parser with one score
function.
2 Parsing Algorithm
We adopted the second order MST parsing algo-
rithm as outlined by Eisner (1996). This algorithm
has a higher accuracy compared to the first order
parsing algorithm since it considers also siblings and
grandchildren of a node. Eisner?s second order ap-
proach can compute a projective dependency tree
within cubic time (O(n3)).
Both algorithms are bottom up parsing algorithms
based on dynamic programming similar to the CKY
chart parsing algorithm. The score for a dependency
tree is the score of all edge scores. The following
67
equation describes this formally.
score(S, t) =??(i,j)?E score(i, j)
The score of the sentence S and a tree t over S
is defined as the sum of all edge scores where the
words of S are w0...w1. The tree consists of set of
nodes N and set of edges E = ?N ?N?. The word
indices (0..n) are the elements of the node set N .
The expression (i, j) ? E denotes an edge which is
going from the node i to the node j.
The edge score (score(i, j)) is computed as the
scalar product of a feature vector representation of
each edge ??fS(i, j) with a weight vector ??w where
i, j are the indices of the words in a sentence. The
feature vector fS might take not only into account
the words with indices i and j but also additional
values such as the words before and after the words
wi and wj . The following equation shows the score
function.
score(i, j) = ??fS(i, j) ? ??w
Many systems encode the features as strings and
map the strings to a number. The number becomes
the index of the feature in the feature vector and
weight vector. In order to compute the weight vec-
tor, we reimplemented the support vector machine
MIRA which implements online Margin Infused Re-
laxed Algorithm, cf. (Crammer et al, 2003).
3 Labeled Dependency Parsing
The second order parsing algorithm builds an un-
labeled dependency tree. However, all dependency
tree banks of the shared task provide trees with edge
labels. The following two approaches are common
to solve this problem. An additional algorithm la-
bels the edges or the parsing algorithm itself is ex-
tended and the labeling algorithm is integrated into
the parsing algorithm. McDonald et al (2006) use
an additional algorithm. Their two stage model has
a good computational complexity since the label-
ing algorithm contributes again only a cubic time
complexity to the algorithm and keeps therefore the
joint algorithm still cubic. The algorithm selects
the highest scored label due to the score function
score(wi, label) + score(wj , label) and inserts the
highest scored label into a matrix. The scores are
also used in the parsing algorithms and added to
the edge scores which improves the overall pars-
ing results as well. In the first order parsing sce-
nario, this procedure is sufficient since no combi-
nation of edges are considered by the parsing algo-
rithm. However, in the second order parsing sce-
nario where more than one edge are considered by
the parsing algorithm, combinations of two edges
might be more accurate.
Johansson and Nugues (2008) combines the edge
labeling with the second order parsing algorithm.
This adds an additional loop over the edge labels.
The complexity is therefore O(n4). However, they
could show that a system can gain accuracy of about
2-4% which is a lot.
4 Non-Projective Dependency Parsing
The dependency parser developed in the last years
use two different techniques for non-projective de-
pendency parsing.
Nivre and Nilsson (2005) uses tree rewriting
which is the most common technique. With this
technique, the training input to the parser is first pro-
jectivized by applying a minimal number of lifting
operations to the non-projective edges and encoding
information about these lifts in edge labels. After
these operations, the trees are projective and there-
fore a projective dependency parser can be applied.
During the training, the parser learns also to built
trees with the lifted edges and so indirect to built
non-projective dependency trees by applying the in-
verse operations to the lifting on the projective tree.
McDonald and Pereira (2006) developed a tech-
nique to rearrange edges in the tree in a postpro-
cessing step after the projective parsing has taken
place. Their Approximate Dependency Parsing Al-
gorithm searches first the highest scoring projective
parse tree and then it rearranges edges in the tree
until the rearrangements does not increase the score
for the tree anymore. This technique is computa-
tionally expensive for trees with a large number of
non-projective edges since it considers to re-attach
all edges to any other node until no higher scoring
trees can be found. Their argument for the algo-
rithm is that most edges in a tree even in language
with lot of non-projective sentences, the portion of
non-projective edges are still small and therefore by
starting with the highest scoring projective tree, typ-
68
ically the highest scoring non-projective tree is only
a small number of transformations away.
Our experiments showed that with the non-
projective Approximate Dependency Parsing Algo-
rithm and a threshold for the improvment of score
higher than about 0.7, the parsing accuracy improves
even for English slightly. With a threshold of 1.1, we
got the highest improvements.
5 Learning Framework
As learning technique, we use Margin Infused Re-
laxed Algorithm (MIRA) as developed by Crammer
et al (2003) and applied to dependency parsing by
McDonald et al (2005). The online Algorithm in
Figure 1 processes one training instance on each it-
eration, and updates the parameters accordingly.
Algorithm 1: MIRA
? = {Sx, Tx}Xx=1 // The set of training data consists
// of sentences and the corresponding dependency trees
??w (0) = 0,??v = 0
for n = 1 to N
for x = 1 to X
wi+1 = update wi according to instance (Sx, Tx)
v = v + wi+1
i = i+ 1
end for
end for
w = v/(N ?X)
The inner loop iterates over all sentences x of the
training set while the outer loop repeats the train i
times. The algorithm returns an averaged weight
vector and uses an auxiliary weight vector v that ac-
cumulates the values of w after each iteration. At
the end, the algorithm computes the average of all
weight vectors by dividing it by the number of train-
ing iterations and sentences. This helps to avoid
overfitting, cf. (Collins, 2002).
The update function computes the update to the
weight vector wi during the training so that wrong
classified edges of the training instances are possibly
correctly classified. This is computed by increasing
the weight for the correct features and decreasing the
weight for wrong features of the vectors for the tree
of the training set ??fTx ? wi and the vector for the
predicted dependency tree ??fT ?x ? wi.
The update function tries to keep the change to
the parameter vector wi as small as possible for cor-
rectly classifying the current instance with a differ-
ence at least as large as the loss of the incorrect clas-
sifications.
6 Selected Parsing Features
Table 1, 4 and 2 give an overview of the selected
features for our system. Similar to Johansson and
Nugues (2008), we add the edge labels to each fea-
tures. In the feature selection, we follow a bit more
McDonald and Pereira (2006) since we have in addi-
tion the lemmas, morphologic features and the dis-
tance between the word forms.
For the parsing and training speed, most impor-
tant is a fast feature extraction beside of a fast pars-
ing algorithm.
Standard Features
h-f/l h-f/l, d-pos
h-pos h-pos, d-f/l
d-f/l h-f/l, d-f/l
d-pos h-pos, d-pos
h-f/l,h-pos h-f/l, d-f/l, h-pos
d-f/l,d-pos h-f/l, d-f/l, d-pos
h-pos, d-pos, h-f/l
h-pos, d-pos, d-f/l
h-pos, d-pos, h-f/l, d-f/l
Table 1: Selected standard parsing features. h is the ab-
brevation for head, d for dependent, g for grandchild, and
s for sibling. Each feature contains also the edge label
which is not listed in order to save some space. Addi-
tional features are build by adding the direction and the
distance plus the direction. The direction is left if the de-
pendent is left of the head otherwise right. The distance
is the number of words between the head and the depen-
dent, if ?5, 6 if >5 and 11 if >10. ? means that an
additional feature is built with the previous part plus the
following part. f/l represent features that are built once
with the form and once with the lemma.
Selected morphologic parsing features.
? h-morpheme ? head-morphologic-feature-set do
? d-morpheme ? dependent-morphologic-feature-set do
build-feautre: h-pos, d-pos, h-morpheme, d-morpheme
7 Implementation Aspects
In this section, we provide implementation details
considering improvements of the parsing and train-
ing time. The training of our system (parser) has
69
Linear Features Grandchild Features Sibling Features
h-pos, d-pos, h-pos + 1 h-pos, d-pos, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l ? dir(d,s) ?dist(d,s)
h-pos, d-pos, h-pos - 1 h-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ? dir(d,s) ?dist(d,s)
h-pos, d-pos, d-pos + 1 d-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ? dir(d,s)+ ? dist(d,s)
h-pos, d-pos, d-pos - 1 h-pos, g-f/l, dir(h,d), dir(d,g) d-pos, s-pos ?dir(d,s)?dist(d,s)
h-pos, d-pos, h-pos - 1, d-pos - 1 d-pos, g-f/l, dir(h,d), dir(d,g) h-pos, d-pos, s-pos, dir(h,d), dir(d,s) ?dist(h,s)
h-f/l, g-pos, dir(h,d), dir(d,g) h-f/l, s-f/l, dir(h,d), dir(d,s)?dist(h,s) h-pos, s-f/l, dir(h,d), dir(d,s)?dist(h,s)
d-f/l, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l, dir(h,d), dir(d,s) ?dist(h,s) d-pos, s-f/l, dir(h,d), dir(d,s)?dist(h,s)
h-f/l, s-pos, dir(h,d), dir(d,s)?dist(h,s)
d-f/l, s-pos, dir(h,d), dir(d,s)?dist(h,s)
Table 2: Selected Features.
three passes. The goal of the first two passes is to
collect the set of possible features of the training
set. In order to determine the minimal description
length, the feature extractor collects in the first pass
all attributes that the features can contain. For each
attribute (labels, part-of-speech, etc.), the extractor
computes a mapping to a number which is continous
from 1 to the count of elements without duplicates.
We enumerate in the same way the feature pat-
terns (e.g. h-pos, d-pos) in order to distinguish the
patterns. In the second pass, the extractor builds the
features for all training examples which occur in the
train set. This means for all edges in the training
examples.
We create the features with a function that adds it-
eratively the attributes of a feature to a number rep-
resented with 64 bits and shifts it by the minimal
number of bits to encode the attribute and then enu-
merates and maps these numbers to 32 bit numbers
to save even more memory.
Beside this, the following list shows an overview
of the most important implementation details to im-
prove the speed:
1. We use as feature vector a custom array imple-
mentation with only a list of the features that
means without double floating point value.
2. We store the feature vectors for
f(label, wi, wj), f(label, wi, wj , wg),
f(label, wi, wj , ws) etc. in a compressed
file since otherwise it becomes the bottleneck.
3. After the training, we store only the parameters
of the support vector machine which are higher
than a threshold of 1?10?7
Table 3 compares system regarding their perfor-
mance and memory usage. For the shared task, we
System (1) (2) (3)
Type 2nd order 2nd order 2nd order
Labeling separate separate integrated
System baseline this this
Training 22 hours 3 hours 15 hours
7GB 1.5 GB 3 GB
Parsing 2000 ms 50 ms 610 ms
700 MB 1 GB
LAS 0.86 0.86 0.88
Table 3: Performance Comparison. For the baseline sys-
tem (1), we used the system of McDonald and Pereira
(2006) on a MacPro 2.8 Ghz as well for our implementa-
tion (2). For system (3), we use a computer with Intel i7
3.2 Ghz which is faster than the MacPro. For all systems,
we use 10 training iterations for the SVM Mira.
use the system (3) with integrated labeling.
8 Semantic Role Labeling
The semantic role labeler is implemented as a
pipeline architecture. The components of the
pipeline are predicate selection (PS), argument iden-
tification (AI), argument classification (AC), and
word sense disambiguation (WSD).
In order to select the predicates, we look up the
lemmas in the Prob Bank, Nom Bank, etc. if avail-
able, cf. (Palmer et al, 2005; Meyers et al, 2004).
For all other components, we use the support vec-
tor machine MIRA to select and classify the seman-
tic role labels as well as to disambiguate the word
senese.
The AI component identifies the arguments of
each predicate. It iterates over the predicates and
over the words of a sentence. In the case that the
score function is large or equal to zero the argument
is added to the set of arguments of the predicate in
question. Table 5 lists for the attribute identification
and semantic role labeling.
The argument classification algorithm labels each
70
Language Catalan Chinese Czech English German Japanese Spanish Czech English German
Development Set
LAS 86.69 76.77 80.75 87.97 86.46 92.37 86.53
Semantic Unlabeled 93.92 85.09 94.05 91.06 91.61 93.90 93.87
Semantic Labeled 74.98 75.94 78.07 78.79 72.66 72.86 73.01
Macro (F1) 80.84 76.48 79.42 84.52 79.56 82.63 79.77
Test Set Out-of-domain data
LAS 86.35 76.51 80.11 @89.88 @87.48 92.21 87.19 76.40 @82.64 @77.34
Semantic Labeled 74.53 75.29 79.02 80.39 75.72 72.76 74.31 78.01 68.44 63.36
Macro (F1) 80.44 75.91 79.57 85.14 81.60 82.51 80.75 77.20 75.55 70.35
Table 4: Syntactic and Semantic Scores. @ indicate values that are the highest scores of all systems.
Features with part-of-speech tags Features with lemmas Features with rels
arg, path-len arg, p-lemma ? dir ? path-len arg, a-rel ? path-len
arg, p-pos arg, a-lemma, path, dir arg, a-pos, p-pos, p-rel ? path-len
arg, sub-cat, a-pos, p-pos arg, p-lemma - 1, a-pos, path-len, dir arg, p-rel, a-pos, lms-lemma
arg, p-pos, a-pos, a-rmc-pos arg, p-lemma + 1, a-lemma, path, dir arg, a-pos, p-pos, a-rel
arg, p-pos, a-pos, a-lmc-pos arg, p-lemma - 1, a-lemma, path, dir arg, path-rel
arg, p-pos, a-pos, a-lemma-1 arg, p-lemma - 2, a-lemma, path, dir arg, p-lemma, a-pos, path-rel
arg, sub-cat, a-lemma, dir, path-len arg, p-lemma, a-lemma, pathPos, dir
arg, a-pos, a-lemma + 1 arg, p-lemma, p-lemma + 1
arg, a-pos, a-lemma + 2 arg, p-lemma, p-lemma - 1, a-pos ? dir ? path-len
arg, a-pos, a-lemma-lmc arg, p-lemma, a-lms-lemma, a-pos ? dir ? path-len
arg, p-sub-cat, p-pos ? dir arg, p-lemma, path-len ? dir
arg, p-pos, path, p-parent-lemma arg, p-lemma, path-len ? dir
arg, p-pos, path, p-parent-pos ? dir arg, a-pos, path
arg, p-pos, a-pos, familyship(p,a) arg, p-pos, p-lemma, familyship(p,a)
arg, path-pos arg, a-pos, p-lemma, familyship(p,a)
arg, p-pos, a-lemma, familyship(p,a)
Table 5: Argument identification and semantic role labeling Features. p is the abbrivation for predicate and a for
argument. For the AI component, the attribute arg is either the value yes and no and for the SRL component, ars is
the role label. path is the path in terms of up?s and down?s. pathPos is a path plus the part-of-speech on the path. dir
is left, if the argument is left of the predicate, equal if the predicate and argument are equal, otherwise right. rmc is
the abbrivation for right most child, lmc for left most child, and lms left most sibling. familiship(x,y) is a function that
computes the relation between two words: self, parent, child, ancestor, decendant and none.
identified argument with a semantic role label. The
argument classification algorithm selects with a
beam search algorithm the combination of argu-
ments with the highest score.
The last component of our pipeline is the word
sense disambiguation. We put this against the in-
tuition at the end of our pipeline since experiments
showed that other components could not profit from
disambiguated word senses but on the other hand
the word sense disambiguation could profit from the
argument identification and argument classification.
In order to disambiguate, we iterate over the words
in the corpus that have more than one sense and take
the sense with the highest score.
The average time to execute the SRL pipeline on
a sentence is less than 0.15 seconds and the training
time for all languages less than 2 hours.
9 Conclusion
We provided a fast implementation with good pars-
ing time and memory footprint. Even if we traded
off a lot of the speed improvement by using a
more expensive decoder and more attributes to get
a higher accuracy.
For some languages, features are not provided or
the parser does not profit from using these features.
For instance, the English parser does not profit from
the lemmas and the Chinese as well as the Japanese
71
corpus does not have lemmas different from the
word forms, etc. Therefore, a possible further ac-
curacy and parsing speed improvement would be to
select different features sets for different languages
or to leave out some features.
Acknowledgments
This work was supported by the German Academic
Exchange Service (DAAD). We gratefully acknowl-
edge this support.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
In Proc. of CoNLL, pages 149?164.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online Passive-Aggressive Al-
gorithms. In Sixteenth Annual Conference on Neural
Information Processing Systems (NIPS).
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhaen.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zden?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu?is
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Miahi Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Dependen-
cies in Multiple Languages. In Proceedings of the 13th
CoNLL-2009, June 4-5, Boulder, Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008, Manchester,
UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In In Proc. of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-margin Training of Dependency
Parsers. In Proc. ACL, pages 91?98.
Ryan McDonald, Kevin Lerman, Koby Crammer, and
Fernando Pereira. 2006. Multilingual Dependency
Parsing with a Two-Stage Discriminative Parser. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 91?98.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Joakim. Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In Proceed-
ings of the 8th CoNLL, pages 49?56, Boston, Mas-
sachusetts.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Rayn McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The conll 2007 shared task on dependency pars-
ing. In Proc. of the CoNLL 2007 Shared Task. Joint
Conf. on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), June.
Martha Palmer and Nianwen Xue. 2009. Adding Seman-
tic Roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. volume 31, pages 71?106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
72
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 99?100,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
ICSI-CRF: The Generation of References to the Main Subject and Named
Entities using Conditional Random Fields
Benoit Favre and Bernd Bohnet
International Computer Science Institute
1947 Center Street. Suite 600
Berkeley, CA 94704, USA
{favre|bohnet}@icsi.berkeley.edu
Abstract
In this paper, we describe our contribution to
the Generation Challenge 2009 for the tasks of
generating Referring Expressions to the Main
Subject References (MSR) and Named Enti-
ties Generation (NEG). To generate the refer-
ring expressions, we employ the Conditional
Random Fields (CRF) learning technique due
to the fact that the selection of an expres-
sion depends on the selection of the previ-
ous references. CRFs fit very well to this
task since they are designed for the labeling
of sequences. For the MSR task, our system
has a String Accuracy of 0.68 and a REG08-
Type Accuracy of 0.76 and for the NEG task a
String Accuracy of 0.79 and REG08-Type Ac-
curacy of 0.83.
1 Introduction
The GREC Generation Challenge 2009 consists of
two tasks. The first task is to generate appropriate
references to an entity due to a given context which
is longer than a sentence. In the GREC-MSR task,
data sets are provided of possible referring expres-
sions which have to be selected. In the first shared
task on same topic (Belz and Varges, 2007), the main
task was to select the referring expression type cor-
rectly. In the GREC-MSR 2009 task, the main task
is to select the actual word string correctly, and the
main evaluation criterion is String Accuracy.
The GREC-NEG task is about the generation of
references to all person entities in a context longer
than a sentence. The NEG data also provides sets of
possible referring expressions to each entity (?he?),
groups of multiple entities (?they?) and nested refer-
ences (?his father?).
2 System Description
Our approach relies in mapping each input expres-
sion for a given reference to a class label. We use
the attributes of the REFEX tags as basic labels so
that, for instance, a REFEX with attributes REG08-
TYPE=?pronoun? CASE=?nominative? is mapped
to the label ?nominative pronoun?. In order to de-
crease the number of potential textual units for a pre-
dicted label, we derive extra label information from
the text itself. For instance a qualifier ?first name?
or ?family name? is added to the expressions rela-
tive to a person. Similarly, types of pronouns (he,
him, his, who, whose, whom, emphasis) are speci-
fied in the class label, which is very useful for the
NEG task. Only the person labels have been refined
this way. While we experimented with a few ap-
proaches to remove the remaining ambiguity (same
label for different text), they generally did not per-
form better than a random selection. We opted for a
deterministic generation with the last element in the
list of possibilities given a class label.
For prediction of attributes, our system uses Con-
ditional Random Fields, as proposed by (Lafferty et
al., 2001). We use chain CRFs to estimate the prob-
ability of a sequence of labels (Y = Y
1
. . . Y
n
) given
a sequence of observations (X = X
1
. . . X
m
).
P (Y |X) ? exp
?
?
n
?
j=1
m
?
i=1
?
i
f
i
(Y
j?1
, Y
j
, X)
?
?
(1)
Here, f
i
(?) are decision functions that depend on
99
MSR NEG
Evaluation Metric R1 R2 S1 S2 S2R S2O R1 R2 S1 S2 S2R S2O
REG08 Type Accuracy 0.36 1.00 0.74 0.75 0.75 0.75 0.40 1.00 0.83 0.83 0.83 0.83
String Accuracy 0.12 0.82 0.62 0.67 0.66 0.75 0.12 0.70 0.52 0.79 0.79 0.80
Mean Edit Distance 2.52 0.31 0.95 0.85 0.87 0.72 2.38 0.61 1.07 0.53 0.52 0.49
Mean Norm. Edit Dist. 0.79 0.09 0.31 0.28 0.28 0.24 0.84 0.22 0.43 0.19 0.20 0.19
BLEU 1 0.19 0.88 0.65 0.69 0.68 0.74 0.17 0.79 0.64 0.81 0.81 0.83
BLEU 2 0.14 0.76 0.55 0.60 0.59 0.71 0.18 0.75 0.69 0.83 0.83 0.85
BLEU 3 0.10 0.69 0.51 0.56 0.55 0.70 0.18 0.73 0.71 0.83 0.84 0.86
Table 1: Results for the GREC MSR and NEG tasks. Are displayed: a random
2
output (R1), a random output when
the attributes are guessed correctly (R2), the CRF system predicting basic attributes (S1), the CRF system predicting
refined attributes (S2), CRF-predicted attributes with random selection of text (S2R) and CRF-predicted attributes with
oracle selection of text (S2O).
the examples and a clique of boundaries close to Y
j
,
and ?
i
is the weight of f
i
estimated on training data.
For our experiments, we use the CRF++ toolkit,
1
which allows binary decision functions dependent
on the current label and the previous label.
All features are used for both MSR and NEG
tasks, where applicable:
? word unigram and bigram before and after the
reference
? morphology of the previous and next words (-
ed, -ing, -s)
? punctuation type, before and after (comma,
parenthesis, period, nothing)
? SYNFUNC, SYNCAT and SEMCAT
? whether or not the previous reference is about
the same entity as the current one
? number of occurrence of the entity since the be-
ginning of the text (quantized 1,2,3,4+)
? number of occurrence of the entity since the
last change of entity (quantized)
? beginning of paragraph indicator
In the MSR case, this list is augmented with the fea-
tures of the two previous references. In the NEG
case, we use the features of the previous reference
and those of the previous occurrence of the same en-
tity.
1
http://crfpp.sourceforge.net/
3 Results and Conclusion
Table 1 shows the results for the GREC MSR and
NEG tasks.
2
We observe that for both tasks, our sys-
tem exceeds the performance of a random
3
selection
(columns R1 vs. S2). In the MSR task, guessing
correctly the attributes seems more important than
in the NEG task, as suggested by the difference in
string accuracy when randomly selecting the refer-
ences with the right attributes (columns R2). Gener-
ating more specific attributes from the text is espe-
cially important for the NEG task (columns S1 vs.
S2). This was expected because we only refined
the attributes for person entities. We also observe
that a deterministic disambiguation of the references
with the same attributes is not distinguishable from
a random selection (columns S2 vs. S2R). However
it seems that selecting the right text, as in the ora-
cle experiment, would hardly help in the NEG task
while the gap is larger for the MSR task. This shows
that refined classes work well for person entities but
more refinements are needed for other types (city,
mountain, river...).
References
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML, pages 282-289
A. Belz and S. Varges. 2007 Generation of Repeated
References to Discourse Entities. In Proceedings of
the 11th European Workshop on Natural Language
Generation (ENLG07), pages 9-16.
2
Our system is available http://www.icsi.berkeley.edu/
?favre/grec/
3
All random experiments are averaged over 100 runs.
100
A Deve lopment  Env i ronment  for an MTT-Based  Sentence  
Generator  
Bernd Bohnet,  Andreas Langjahr and Leo Wanner 
Computer  Science Depar tment  
Univers i ty  of Stut tgar t  
Breitwiesenstr .  20-22 
-70565:Stut tgar t ,  Germany . . . . . . . . . .  " ........ 
{bohnet \ [ langjahr\[wanner}?informatik.uni-stuttgart .de 
1 I n t roduct ion  
With the rising standard of the state of the art in 
text generation and the increase of the number 
of practical generation applications, it becomes 
more and more important o provide means for 
the maintenance of the generator, i.e. its ex- 
tension, modification, and monitoring by gram- 
marians who are not familiar with its internals. 
However, only a few sentence and text gener- 
ators developed to date actually provide these 
means. One of these generators is KPML (Bate- 
man, 1997). I~PML comes with a Development 
Environment and there is no doubt about the 
contribution of this environment to the popular- 
ity of the systemic approach in generation. 
In the generation project at Stuttgart, the 
realization of a high quality development en- 
vironment (henceforth, DE) has been a central 
topic from the beginning. The De provides sup- 
port to the user with respect to writing, mod- 
ifying, testing, and debugging of (i) grammar 
rules. (ii) lexical information, and (iii) linguis- 
tic structures at different levels of abstraction. 
Furthermore, it automatically generalizes tile or- 
ganization of the lexica and the grammar. In 
what follows, we briefly describe oF,'s main fea- 
tures. The theoretical linguistic background of 
the DE is the Meaning-Text Theory (Mel'euk, 
1988: Polgu~re, 1998). However. its introduc- 
tion is beyond tile scope of this note: tile inter- 
ested reader is asked to consuh the above reg 
erences as well as further literature on the use 
Of MTT ill text generation---for illSlallCe, (Ior- 
danskaja cta l . ,  1992: I,avoie ?- Rainbow. 1997: 
(.'och. 1997). 
2 Globa l  V iew on the  DE 
In MTT, seven levels (or strata) of linguis- 
tic description are distinguished, of which 
five are relevant for generation: semantic 
(Sem), deep-syntactic (DSynt), surface-syntactic 
(SSynt), deep-morphologicM (DMorph) and 
surface-morphological (SMorph). In order to be 
able to generate starting from the data in a data 
base, we introduce an additional, the conceptual 
(Con) stratum. The input structure to DE is thus 
a conceptual structure (ConStr) derived from the 
data in the DB. The generation process consists 
of a series of structure mappings between adja- 
cent strata until the SMorph stratum is reached. 
At the SMorph stratum, the structure is a string 
of linearized word forms. 
The central module of the DE iS a compiler 
that maps a structure specified at one of tile five 
first of the above strata on a structure at the 
adjacent stratum. To support the user in the ex- 
amination of the internal information gathered 
during the processing of a structure, a debug- 
ger and an inspector are available. The user can 
interact with the compiler either via a graphic 
interface or via a text command interface. For 
the maintenance of the grammar, of the lexica 
and of the linguistic structures, the DE possesses 
separate ditors: a rule editor, a lexicon editor, 
and a structure editor. 
2.1 The  Ru le  Ed i tor  
The  Ru les .  Most of the grammatical rules 
in an MTT-based generator are two-level rules. 
.'\ two-level rule establishes a correspomlence 
260 
between minimal structures of two adjacent 
strata. Given that in generation five of 
MTT'S strata are used, four sets of two-level 
rules are available: (1) Sem=vDSynt-rules, (2) 
DSynt~SSynt-rules, (3) SSynt=vDMorph rules, 
and (4) DMorph~SMorph-rules. 
Formally, a two-level rule is defined by the 
optimize the organization of the grammar by a u - 
tomatic detection Of common parts in several 
rules and their extraction into abstract 'class' 
rules. The theoretical background and the proce- 
dure of rule generalization is described in detail 
in (Wanner & Bohnet, submitted) and will hence 
not be discussed in this note. 
quintuple (/2, Ctxt, Conds, 7~, Corr). ? specifies While editing a rule, the developer has the 
the lefthand side :of the r.,ule-~a,~minimal~so~rce~a stc nd~r.d,c.ommands:,:~t,,his/'her~ disposal. Rules 
substructure that is mapped by the rule onto its can be edited either in a text rule editor or via 
destination structure specified in 7~, the right- 
hand side of the rule. Ctxt specifies the wider 
context of the lefthand side in the input structure 
(note that by far not all rules contain context in- 
formation). Conds specifies the conditions that 
must be satisfied for the rule to be applicable to 
an input substructure matched by ?. Corr spec- 
ifies the correspondence between the individual 
nodes of the lefthand side and the righthand side 
structures. 
Consider a typical Sem=~,DSynt-rule, 
which maps the semantic relation '1' that 
holds between a property and an entity 
that possesses this property onto the deep- 
syntactic relation ATTR. The names begin- 
ning with a '7' are variables. The condition 
' Lex: : (Sem:: (?Xsem.sem). lex) .cat  = adj' 
requires that the lexicalization of the property 
is an adjective. '?Xsem ~ ?Xdsynt' and '?Ysem 
?:~ ?Ydsynt' mean that the semantic node ?Xsem 
is expressed at the deep-syntactic stratum by 
?Xdsynt, and ?Ysem by ?Ydsynt. 
property (Sem_DSynt) { 
leftside : 
?Xsem -i-+ ?Ysem 
condit ions : 
Sem: :?Xsem.sem,1:ype = property 
Lex: :(Sem::(?Xsem.sem).lex).cat = adj 
rightside: 
?Xds 
?Yds 
?Yds -ATTR-+?Xds 
correspondence : 
?Xsem ~ ?Xds 
?Ysem ~ ?Yds} 
The rule editor (l~t-~) has two main \['unctions: 
(i) to support the mai)~tenance (i.e. editing and 
examination) of grammatical rules, and (ii) to 
a graphic interface. Obviously incorrect rules 
can be detected during the syntax and the se- 
mantic rule checks. The syntax check exam- 
ines the correctness of the notation of the state- 
ments in a rule (i.e. of variables, relations, con- 
ditions, etc.)-- in the same way as a conventional 
compiler does. The semantic check examines 
the consistency of the conditions, relations, and 
attribute-feature pairs in a rule, the presence of 
an attribute's value in the set of values that are 
available to this attribute, etc. If, for instance 
in the above rule 'adj' is misspelled as 'adk' or 
erroneously a multiple correspondence between 
?gds and ?Xsem and ?Ysem is introduced, the 
rule editor draws the developer's attention to tile 
respective rror (see Figure 1). 
Ru le  Test ing.  Rule testing is usually a very 
time consuming procedure, this is so partly be- 
cause tile generator needs to be started as a 
whole again and again, partly because tlle re- 
suiting structure and the trace must be carefully 
inspected in order to find out whether tile rule 
in question fired and if it did not fire why it 
did not. The DE attempts to minimize this el'- 
fort. With "drag and drop' the developer can 
select one or several rules and apply them onto 
an input structure (which can be presented ei- 
ther graphically or in a textual format; see be- 
low). When a rule dropped onto the structure 
fires, the affected parts of the input structure are 
made visually prominent, and the resulting out- 
put (sub)structure appears in the corresponding 
window of the slructure editor. If a rule did not 
fire. the inspector indicates which conditions of 
tim rule in question were not satisfied. See also 
I)elow lhe description of the features of lhe in- 
spect or. 
261 
Figure 1: Error messages of the rule editor 
2.2 The  St ructure  Ed i tor  
The structure editor manages two types of win- 
dows: windows in which the input structures are 
presented and edited, and windows in which the 
resulting structures are presented. Both types of 
windows can be run in a text and in a graphic 
mode. The input structures can be edited in 
both modes, i.e., new nodes and new relations 
can be introduced, attribute-value pairs associ- 
ated with the nodes can be changed, etc. 
In the same way as rules, structures can be 
checked with respect to their syntax and se- 
mantics. Each structure can be exported into 
postscript files and thus conveniently be printed. 
2.3 The  Lex icon Ed i tor  
The main function of the lexicon editor is to sup- 
port the maintenance of the lexica. Several types 
of lexica are distinguished: conceptual lexica, se- 
mantic lexica, and lexico-syntactic lexica.. 
Besides tile standard editor functions, the 
lexicon editor provides the following options: (i) 
sorting of tile entries (either alphabetically or ac- 
cording to such criteria as 'category'); (ii) syntax 
check; (iii) finding information that. is common 
to several entries and extracting it into abstract 
entries (the result is a hierarchical organization 
of the resource). During the demonstration, each 
of these options will be shown ilt action. 
2.4 The  Inspector  
The inspector fulfils mainly three functions. 
First.  it presents in format ion  collected ( lur ing 
the application of the rules selected by the de- 
veloper Io :-ill inpul  st ructure.  This  i l l fornia-  
tion is especially useful for generation experts 
who are familiar with the internal processing. It 
concerns (i) the correspondences tablished be- 
tween nodes of the input structure and nodes of 
the resulting structure, (ii) the instantiation of 
the variables of those rules that are applied to- 
gether to the input structure in question, and 
(iii) the trace of all operations performed by the 
compiler during the application of the rules. 
Second, it indicates to which part of the input 
structure a specific rule is applicable and what 
its result at the destination side is. Third, it in- 
dicates which rules failed and why. The second 
and third kind of information is useful not only 
for generation experts, but also for grammarians 
with a pure linguistic background. 
Figure 2 shows a snapshot of the inspecLor 
editor interface. Sets of rules that can simulta: 
neously be applied together to an input struc- 
ture without causing conflicts are grouped dur- 
ing processing into so-called clusters. At the 
left side of the picture, we see two such clus- 
ters (Cluster 13 and Cluster 22). Tile instances 
of the rules of Cluster 13 are shown to the righl 
of the cluster pane. The cluster pane also con- 
tains sets of rules that failed (in the picture, the 
corresponding icon is not expanded). The left 
graph in Figure 2 is the input structure to which 
the rules are applied. For illustration, one of 
the rules, namely date,  has been selected for ap- 
plication: tile highlighted arcs and nodes of tile 
input structure are the part to which date  is ap- 
plicable. 'Pile result of its application is tile tree 
at the right. Beneath the graphical structures, 
we see tile correspondence between input nodes 
262 
SOUlCe s~'ucture:(  1 ) 
Eva luat ion  F al lecl  
C lu$1er  22  ~3 ? 
+ ..... , \ \ !  
IX . . . .  " + ,  .... \ \ 
? , . j  ~m .+ 2 
taq  tm, . :  a,,: ; ,t , i  
i i . i . i .~ i+_  
~Jt" l  
RI~? ~- ,~k- - ,~+- -Bg~t I l *  \ ' 
T 
A r r~ 
~4 
+ . . . . . . .  \ ] l i .  
N}~Y?P: ' - . .4 J  .~ ~ ,. -7" - -=- -  - -  
!p !e(? )  . . . . . . . .  I o? in  Ume ,~.~!e(?}  
? ~ :o9(.__~ . . . . . . . . . . . .  
ocal Jo ln(5)  . . . . . . .  : loc ln  .spaco(6) 
171 (g)  .371 (9) 
;~yerltB.~__) . . . . . . . . . . .  .VVOrI(I 1 ) _ __  . 
+.~(L0). ...... i.~7.(!_al ................. 
:CO(B)  ,CO(141 
. . . . . . . . . . .  ,,,, , ,  ,,,,, 
Figure 2: The inspector interface of the DE.  
and result nodes. The numbers in parentheses 
are for system use. 
2.5 The  Debugger  
In the rule editor, break points within individual 
rules can be set. When the compiler reaches a 
break point it stops and enters the debugger. In 
the debugger, the developer can execute the rules 
statement by statement. As in the inspector, the 
execution trace, the variable instantiation and 
node correspondences can be examined. During 
the demonstration, the function of the debugger 
will be shown in action. 
3 Cur rent  Work  
DE is written in ,Java 1.2 and has been tested on 
a SUN workstation and on a PC pentium with 
300 MHz and 128 MB of RAM. 
Currently, the described functions of the DE 
are consolidated and extended bv new features. 
The most important of these features aa'e the im- 
port and the export feature. The import feature 
allows for a transformation of grammatical rules 
and lexical information encoded in a different 
format into the format used bv our generator. 
Tests are being carried out with the import of 
RealPro (Lavoie ,~,: Rainbow. 1997) grammati- 
cal rules and lexical information (in particular 
ubcategorization a d diathesis information) en- 
coded in the DATR-formalism. The export fea- 
ture allows for a transformation f the rules and 
lexical information encoded in our format into 
external formats. 
B ib l iography  
Bateman, J.A. 1997. Enabling technology for mul- 
tilingual natural anguage generation: the KPblL 
development environment. Natural Language Engi- 
neering. 3.2:15-55. 
Coch, J. 1997. Quand l'ordinateur prend la plume :
la gdn4ration de textes. Document Numdrique. 1 :~. 
Iordanskaja, L.N., M. Kim, R. Kittredge, B. Lavoie 
~: A. Polgu6re. 1992. Generation of Extended Bilin- 
gual Statistical Reports. COLING-92. 1019 1022. 
Nantes. 
Lavoie, B. & O. Rainbow. 1997. A fast and portable 
realizer for text generation systems. Proceedings of
the Fifth Conference on Applied Natural Language 
Processing. Washington, DC. 
Mel'euk, I.A. 1988. Dependenc!l Syntaz:: Theory and 
Prac&ce. Albany: State University of New York 
Press. 
Polgu6re, A. 1998. La th~orie sens+textre. I)t- 
alangue,. 8-9:9-30. 
Wanner. L. ~ B. Bohnet. submitted. Inheritance in
the MTT-grammar. 
263 
 	
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97,
Beijing, August 2010
Very High Accuracy and Fast Dependency Parsing is not a Contradiction
Bernd Bohnet
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
bernd.bohnet@ims.uni-stuttgart.de
Abstract
In addition to a high accuracy, short pars-
ing and training times are the most impor-
tant properties of a parser. However, pars-
ing and training times are still relatively
long. To determine why, we analyzed the
time usage of a dependency parser. We il-
lustrate that the mapping of the features
onto their weights in the support vector
machine is the major factor in time com-
plexity. To resolve this problem, we im-
plemented the passive-aggressive percep-
tron algorithm as a Hash Kernel. The
Hash Kernel substantially improves the
parsing times and takes into account the
features of negative examples built dur-
ing the training. This has lead to a higher
accuracy. We could further increase the
parsing and training speed with a paral-
lel feature extraction and a parallel parsing
algorithm. We are convinced that the Hash
Kernel and the parallelization can be ap-
plied successful to other NLP applications
as well such as transition based depen-
dency parsers, phrase structrue parsers,
and machine translation.
1 Introduction
Highly accurate dependency parsers have high de-
mands on resources and long parsing times. The
training of a parser frequently takes several days
and the parsing of a sentence can take on average
up to a minute. The parsing time usage is impor-
tant for many applications. For instance, dialog
systems only have a few hundred milliseconds to
analyze a sentence and machine translation sys-
tems, have to consider in that time some thousand
translation alternatives for the translation of a sen-
tence.
Parsing and training times can be improved
by methods that maintain the accuracy level, or
methods that trade accuracy against better parsing
times. Software developers and researchers are
usually unwilling to reduce the quality of their ap-
plications. Consequently, we have to consider at
first methods to improve a parser, which do not in-
volve an accuracy loss, such as faster algorithms,
faster implementation of algorithms, parallel al-
gorithms that use several CPU cores, and feature
selection that eliminates the features that do not
improve accuracy.
We employ, as a basis for our parser, the second
order maximum spanning tree dependency pars-
ing algorithm of Carreras (2007). This algorithm
frequently reaches very good, or even the best la-
beled attachment scores, and was one of the most
used parsing algorithms in the shared task 2009
of the Conference on Natural Language Learning
(CoNLL) (Hajic? et al, 2009). We combined this
parsing algorithm with the passive-aggressive per-
ceptron algorithm (Crammer et al, 2003; McDon-
ald et al, 2005; Crammer et al, 2006). A parser
build out of these two algorithms provides a good
baseline and starting point to improve upon the
parsing and training times.
The rest of the paper is structured as follows. In
Section 2, we describe related work. In section 3,
we analyze the time usage of the components of
89
the parser. In Section 4, we introduce a new Ker-
nel that resolves some of the bottlenecks and im-
proves the performance. In Section 5, we describe
the parallel parsing algorithms which nearly al-
lowed us to divide the parsing times by the num-
ber of cores. In Section 6, we determine the opti-
mal setting for the Non-Projective Approximation
Algorithm. In Section 7, we conclude with a sum-
mary and an outline of further research.
2 Related Work
The two main approaches to dependency parsing
are transition based dependency parsing (Nivre,
2003; Yamada and Matsumoto., 2003; Titov and
Henderson, 2007) and maximum spanning tree
based dependency parsing (Eisner, 1996; Eisner,
2000; McDonald and Pereira, 2006). Transition
based parsers typically have a linear or quadratic
complexity (Nivre et al, 2004; Attardi, 2006).
Nivre (2009) introduced a transition based non-
projective parsing algorithm that has a worst case
quadratic complexity and an expected linear pars-
ing time. Titov and Henderson (2007) combined
a transition based parsing algorithm, which used a
beam search with a latent variable machine learn-
ing technique.
Maximum spanning tree dependency based
parsers decomposes a dependency structure into
parts known as ?factors?. The factors of the first
order maximum spanning tree parsing algorithm
are edges consisting of the head, the dependent
(child) and the edge label. This algorithm has a
quadratic complexity. The second order parsing
algorithm of McDonald and Pereira (2006) uses a
separate algorithm for edge labeling. This algo-
rithm uses in addition to the first order factors: the
edges to those children which are closest to the de-
pendent. The second order algorithm of Carreras
(2007) uses in addition to McDonald and Pereira
(2006) the child of the dependent occurring in the
sentence between the head and the dependent, and
the an edge to a grandchild. The edge labeling is
an integral part of the algorithm which requires
an additional loop over the labels. This algorithm
therefore has a complexity of O(n4). Johansson
and Nugues (2008) reduced the needed number of
loops over the edge labels by using only the edges
that existed in the training corpus for a distinct
head and child part-of-speech tag combination.
The transition based parsers have a lower com-
plexity. Nevertheless, the reported run times in
the last shared tasks were similar to the maxi-
mum spanning tree parsers. For a transition based
parser, Gesmundo et al (2009) reported run times
between 2.2 days for English and 4.7 days for
Czech for the joint training of syntactic and se-
mantic dependencies. The parsing times were
about one word per second, which speeds up
quickly with a smaller beam-size, although the ac-
curacy of the parser degrades a bit. Johansson and
Nugues (2008) reported training times of 2.4 days
for English with the high-order parsing algorithm
of Carreras (2007).
3 Analysis of Time Usage
We built a baseline parser to measure the time us-
age. The baseline parser resembles the architec-
ture of McDonald and Pereira (2006). It consists
of the second order parsing algorithm of Carreras
(2007), the non-projective approximation algo-
rithm (McDonald and Pereira, 2006), the passive-
aggressive support vector machine, and a feature
extraction component. The features are listed in
Table 4. As in McDonald et al (2005), the parser
stores the features of each training example in a
file. In each epoch of the training, the feature
file is read, and the weights are calculated and
stored in an array. This procedure is up to 5 times
faster than computing the features each time anew.
But the parser has to maintain large arrays: for
the weights of the sentence and the training file.
Therefore, the parser needs 3GB of main memory
for English and 100GB of disc space for the train-
ing file. The parsing time is approximately 20%
faster, since some of the values did not have to be
recalculated.
Algorithm 1 illustrates the training algorithm in
pseudo code. ? is the set of training examples
where an example is a pair (xi, yi) of a sentence
and the corresponding dependency structure. ??w
and ??v are weight vectors. The first loop ex-
tracts features from the sentence xi and maps the
features to numbers. The numbers are grouped
into three vectors for the features of all possible
edges ?h,d, possible edges in combination with
siblings ?h,d,s and in combination with grandchil-
90
te+s tr tp ta rest total te pars. train. sent. feat. LAS UAS
Chinese 4582 748 95 - 3 846 3298 3262 84h 22277 8.76M 76.88 81.27
English 1509 168 12.5 20 1.5 202 1223 1258 38.5h 39279 8.47M 90.14 92.45
German 945 139 7.7 17.8 1.5 166 419 429 26.7h 36020 9.16M 87.64 90.03
Spanish 3329 779 36 - 2 816 2518 2550 16.9h 14329 5.51M 86.02 89.54
Table 1: te+s is the elapsed time in milliseconds to extract and store the features, tr to read the features
and to calculate the weight arrays, tp to predict the projective parse tree, ta to apply the non-projective
approximation algorithm, rest is the time to conduct the other parts such as the update function, train. is
the total training time per instance (tr + tp + ta+rest ), and te is the elapsed time to extract the features.
The next columns illustrate the parsing time in milliseconds per sentence for the test set, training time
in hours, the number of sentences in the training set, the total number of features in million, the labeled
attachment score of the test set, and the unlabeled attachment score.
Algorithm 1: Training ? baseline algorithm
? = {(xi, yi)}Ii=1 // Training data??w = 0,??v = 0
? = E ? I // passive-aggresive update weight
for i = 1 to I
tss+e; extract-and-store-features(xi); tes+e;
for n = 1 to E // iteration over the training epochs
for i = 1 to I // iteration over the training examples
k ? (n? 1) ? I + i
? = E ? I ? k + 2 // passive-aggressive weight
tsr,k; A = read-features-and-calc-arrays(i,??w ) ; ter,k
tsp,k; yp = predicte-projective-parse-tree(A);tep,k
tsa,k; ya = non-projective-approx.(yp ,A); tea,k
update ??w , ??v according to ?(yp, yi) and ?
w = v/(E ? I) // average
dren ?h,d,g where h, d, g, and s are the indexes
of the words included in xi. Finally, the method
stores the feature vectors on the hard disc.
The next two loops build the main part of the
training algorithm. The outer loop iterates over
the number of training epochs, while the inner
loop iterates over all training examples. The on-
line training algorithm considers a single training
example in each iteration. The first function in the
loop reads the features and computes the weights
A for the factors in the sentence xi. A is a set of
weight arrays.
A = {??w ? ??f h,d,??w ?
??f h,d,s,??w ?
??f h,d,g}
The parsing algorithm uses the weight arrays
to predict a projective dependency structure yp.
The non-projective approximation algorithm has
as input the dependency structure and the weight
arrays. It rearranges the edges and tries to in-
crease the total score of the dependency structure.
This algorithm builds a dependency structure ya,
which might be non-projective. The training al-
gorithm updates ??w according to the difference
between the predicted dependency structures ya
and the reference structure yi. It updates ??v as
well, whereby the algorithm additionally weights
the updates by ?. Since the algorithm decreases
? in each round, the algorithm adapts the weights
more aggressively at the beginning (Crammer et
al., 2006). After all iterations, the algorithm com-
putes the average of ??v , which reduces the effect
of overfitting (Collins, 2002).
We have inserted into the training algorithm
functions to measure the start times ts and the
end times te for the procedures to compute and
store the features, to read the features, to pre-
dict the projective parse, and to calculate the non-
projective approximation. We calculate the aver-
age elapsed time per instance, as the average over
all training examples and epochs:
tx =
?E?I
k=1 t
e
x,k?tsx,k
E?I .
We use the training set and the test set of the
CoNLL shared task 2009 for our experiments. Ta-
ble 1 shows the elapsed times in 11000 seconds
(milliseconds) of the selected languages for the
procedure calls in the loops of Algorithm 1. We
had to measure the times for the feature extraction
in the parsing algorithm, since in the training al-
gorithm, the time can only be measured together
with the time for storing the features. The table
contains additional figures for the total training
time and parsing scores.1
The parsing algorithm itself only required, to
our surprise, 12.5 ms (tp) for a English sentence
1We use a Intel Nehalem i7 CPU 3.33 Ghz. With turbo
mode on, the clock speed was 3.46 Ghz.
91
on average, while the feature extraction needs
1223 ms. To extract the features takes about
100 times longer than to build a projective depen-
dency tree. The feature extraction is already im-
plemented efficiently. It uses only numbers to rep-
resent features which it combines to a long integer
number and then maps by a hash table2 to a 32bit
integer number. The parsing algorithm uses the
integer number as an index to access the weights
in the vectors ??w and ??v .
The complexity of the parsing algorithm is usu-
ally considered the reason for long parsing times.
However, it is not the most time consuming com-
ponent as proven by the above analysis. There-
fore, we investigated the question further, asking
what causes the high time consumption of the fea-
ture extraction?
In our next experiment, we left out the mapping
of the features to the index of the weight vectors.
The feature extraction takes 88 ms/sentence with-
out the mapping and 1223 ms/sentence with the
mapping. The feature?index mapping needs 93%
of the time to extract the features and 91% of the
total parsing time. What causes the high time con-
sumption of the feature?index mapping?
The mapping has to provide a number as an in-
dex for the features in the training examples and to
filter out the features of examples built, while the
parser predicts the dependency structures. The al-
gorithm filters out negative features to reduce the
memory requirement, even if they could improve
the parsing result. We will call the features built
due to the training examples positive features and
the rest negative features. We counted 5.8 times
more access to negative features than positive fea-
tures.
We now look more into the implementation de-
tails of the used hash table to answer the pre-
viously asked question. The hash table for the
feature?index mapping uses three arrays: one for
the keys, one for the values and a status array to
indicate the deleted elements. If a program stores
a value then the hash function uses the key to cal-
culate the location of the value. Since the hash
function is a heuristic function, the predicted lo-
cation might be wrong, which leads to so-called
2We use the hash tables of the trove library:
http://sourceforge.net/projects/trove4j.
hash misses. In such cases the hash algorithm
has to retry to find the value. We counted 87%
hash misses including misses where the hash had
to retry several times. The number of hash misses
was high, because of the additional negative fea-
tures. The CPU cache can only store a small
amount of the data from the hash table. Therefore,
the memory controller has frequently to transfer
data from the main memory into the CPU. This
procedure is relatively slow. We traced down the
high time consumption to the access of the key
and the access of the value. Successive accesses
to the arrays are fast, but the relative random ac-
cesses via the hash function are very slow. The
large number of accesses to the three arrays, be-
cause of the negative features, positive features
and because of the hash misses multiplied by the
time needed to transfer the data into the CPU are
the reason for the high time consumption.
We tried to solve this problem with Bloom fil-
ters, larger hash tables and customized hash func-
tions to reduce the hash misses. These techniques
did not help much. However, a substantial im-
provement did result when we eliminated the hash
table completely, and directly accessed the weight
vectors ??w and ??v with a hash function. This led
us to the use of Hash Kernels.
4 Hash Kernel
A Hash Kernel for structured data uses a hash
function h : J ? {1...n} to index ?, cf. Shi et
al. (2009). ? maps the observations X to a fea-
ture space. We define ?(x, y) as the numeric fea-
ture representation indexed by J . Let ?k(x, y) =
?j(x, y) the hash based feature?index mapping,
where h(j) = k. The process of parsing a sen-
tence xi is to find a parse tree yp that maximizes
a scoring function argmaxyF (xi, y). The learning
problem is to fit the function F so that the errors
of the predicted parse tree y are as low as possible.
The scoring function of the Hash Kernel is
F (x, y) = ??w ? ?(x, y)
where ??w is the weight vector and the size of ??w is
n.
Algorithm 2 shows the update function of the
Hash Kernel. We derived the update function
from the update function of MIRA (Crammer et
92
Algorithm 2: Update of the Hash Kernel
// yp = arg maxyF (xi, y)
update(??w,??v , xi, yi, yp, ?)
? = ?(yi, yp) // number of wrong labeled edges
if ? > 0 then
??u ? (?(xi, yi)? ?(xi, yp))
? = ??(F (xt,yi)?F (xi,yp))||??u ||2??w ? ??w + ? ? ??u
??v ? ~v + ? ? ? ? ??u
return ??w , ??v
al., 2006). The parameters of the function are
the weight vectors ??w and ??v , the sentence xi,
the gold dependency structure yi, the predicted
dependency structure yp, and the update weight
?. The function ? calculates the number of
wrong labeled edges. The update function up-
dates the weight vectors, if at least one edge is la-
beled wrong. It calculates the difference ??u of the
feature vectors of the gold dependency structure
?(xi, yi) and the predicted dependency structure
?(xi, yp). Each time, we use the feature represen-
tation ?, the hash function h maps the features to
integer numbers between 1 and |??w |. After that
the update function calculates the margin ? and
updates ??w and ??v respectively.
Algorithm 3 shows the training algorithm for
the Hash Kernel in pseudo code. A main dif-
ference to the baseline algorithm is that it does
not store the features because of the required time
which is needed to store the additional negative
features. Accordingly, the algorithm first extracts
the features for each training instance, then maps
the features to indexes for the weight vector with
the hash function and calculates the weight arrays.
Algorithm 3: Training ? Hash Kernel
for n? 1 to E // iteration over the training epochs
for i? 1 to I // iteration over the training exmaples
k ? (n? 1) ? I + i
? ? E ? I ? k + 2 // passive-aggressive weight
tse,k; A? extr.-features-&-calc-arrays(i,??w ) ; tee,k
tsp,k; yp? predicte-projective-parse-tree(A);tep,k
tsa,k; ya? non-projective-approx.(yp ,A); tea,k
update ??w , ??v according to ?(yp, yi) and ?
w = v/(E ? I) // average
For different j, the hash function h(j) might
generate the same value k. This means that the
hash function maps more than one feature to the
same weight. We call such cases collisions. Col-
lisions can reduce the accuracy, since the weights
are changed arbitrarily. This procedure is similar
to randomization of weights (features), which
aims to save space by sharing values in the weight
vector (Blum., 2006; Rahimi and Recht, 2008).
The Hash Kernel shares values when collisions
occur that can be considered as an approximation
of the kernel function, because a weight might
be adapted due to more than one feature. If the
approximation works well then we would need
only a relatively small weight vector otherwise
we need a larger weight vector to reduce the
chance of collisions. In an experiments, we
compared two hash functions and different hash
sizes. We selected for the comparison a standard
hash function (h1) and a custom hash function
(h2). The idea for the custom hash function h2 is
not to overlap the values of the feature sequence
number and the edge label with other values.
These values are stored at the beginning of a long
number, which represents a feature.
h1 ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|3
h2 ? |(l xor ((l >> 13) ? 0xffffffffffffe000) xor
((l >> 24) ? 0xffffffffffff0000) xor
((l >> 33) ? 0xfffffffffffc0000) xor
((l >> 40) ? 0xfffffffffff00000)) % size |
vector size h1 #(h1) h2 #(h2)
411527 85.67 0.41 85.74 0.41
3292489 87.82 3.27 87.97 3.28
10503061 88.26 8.83 88.35 8.77
21006137 88.19 12.58 88.41 12.53
42012281 88.32 12.45 88.34 15.27
115911564? 88.32 17.58 88.39 17.34
179669557 88.34 17.65 88.28 17.84
Table 2: The labeled attachment scores for differ-
ent weight vector sizes and the number of nonzero
values in the feature vectors in millions. ? Not a
prime number.
Table 2 shows the labeled attachment scores for
selected weight vector sizes and the number of
nonzero weights. Most of the numbers in Table
2 are primes, since they are frequently used to ob-
tain a better distribution of the content in hash ta-
3>> n shifts n bits right, and % is the modulo operation.
93
bles. h2 has more nonzero weights than h1. Nev-
ertheless, we did not observe any clear improve-
ment of the accuracy scores. The values do not
change significantly for a weight vector size of 10
million and more elements. We choose a weight
vector size of 115911564 values for further exper-
iments since we get more non zero weights and
therefore fewer collisions.
te tp ta r total par. trai.
Chinese 1308 - 200 3 1511 1184 93h
English 379 21.3 18.2 1.5 420 354 46h
German 209 12 15.3 1.7 238 126 24h
Spanish 1056 - 39 2 1097 1044 44h
Table 3: The time in milliseconds for the feature
extraction, projective parsing, non-projective ap-
proximation, rest (r), the total training time per
instance, the average parsing (par.) time in mil-
liseconds for the test set and the training time in
hours
0
1
2
3
0 5000 10000 15000
Spanish
Figure 1: The difference of the labeled attachment
score between the baseline parser and the parser
with the Hash Kernel (y-axis) for increasing large
training sets (x-axis).
Table 3 contains the measured times for the
Hash Kernel as used in Algorithm 2. The parser
needs 0.354 seconds in average to parse a sen-
tence of the English test set. This is 3.5 times
faster than the baseline parser. The reason for that
is the faster feature mapping of the Hash Kernel.
Therefore, the measured time te for the feature ex-
traction and the calculation of the weight arrays
are much lower than for the baseline parser. The
training is about 19% slower since we could no
longer use a file to store the feature indexes of
the training examples because of the large number
of negative features. We counted about twice the
number of nonzero weights in the weight vector of
the Hash Kernel compared to the baseline parser.
For instance, we counted for English 17.34 Mil-
lions nonzero weights in the Hash Kernel and 8.47
Millions in baseline parser and for Chinese 18.28
Millions nonzero weights in the Hash Kernel and
8.76 Millions in the baseline parser. Table 6 shows
the scores for all languages of the shared task
2009. The attachment scores increased for all lan-
guages. It increased most for Catalan and Span-
ish. These two corpora have the smallest training
sets. We searched for the reason and found that
the Hash Kernel provides an overproportional ac-
curacy gain with less training data compared to
MIRA. Figure 1 shows the difference between the
labeled attachment score of the parser with MIRA
and the Hash Kernel for Spanish. The decreasing
curve shows clearly that the Hash Kernel provides
an overproportional accuracy gain with less train-
ing data compared to the baseline. This provides
an advantage for small training corpora.
However, this is probably not the main rea-
son for the high improvement, since for languages
with only slightly larger training sets such as Chi-
nese the improvement is much lower and the gra-
dient at the end of the curve is so that a huge
amount of training data would be needed to make
the curve reach zero.
5 Parallelization
Current CPUs have up to 12 cores and we will
see soon CPUs with more cores. Also graphic
cards provide many simple cores. Parsing algo-
rithms can use several cores. Especially, the tasks
to extract the features and to calculate the weight
arrays can be well implemented as parallel algo-
rithm. We could also successful parallelize the
projective parsing and the non-projective approx-
imation algorithm. Algorithm 4 shows the paral-
lel feature extraction in pseudo code. The main
method prepares a list of tasks which can be per-
formed in parallel and afterwards it creates the
threads that perform the tasks. Each thread re-
moves from the task list an element, carries out
the task and stores the result. This procedure is
repeated until the list is empty. The main method
waits until all threads are completed and returns
the result. For the parallel algorithms, Table 5
shows the elapsed times depend on the number of
94
# Standard Features # Linear Features Linear G. Features Sibling Features
1 l,hf ,hp,d(h,d) 14 l,hp,h+1p,dp,d(h,d) 44 l,gp,dp,d+1p,d(h,d) 99 l,sl,hp,d(h,d)?r(h,d)
2 l,hf ,d(h,d) 15 l,hp,d-1p,dp,d(h,d) 45 l,gp,dp,d-1p,d(h,d) 100 l,sl,dp,d(h,d)?r(h,d)
3 l,hp,d(h,d) 16 l,hp,dp,d+1p,d(h,d) 46 l,gp,g+1p,d-1p,dp,d(h,d) 101 l,hl,dp,d(h,d)?r(h,d)
4 l,df ,dp,d(h,d) 17 l,hp,h+1p,d-1p,dp,d(h,d) 47 l,g-1p,gp,d-1p,dp,d(h,d) 102 l,dl,sp,d(h,d)?r(h,d)
5 l,hp,d(h,d) 18 l,h-1p,h+1p,d-1p,dp,d(h,d) 48 l,gp,g+1p,dp,d+1p,d(h,d) 75 l,?dm,?sm,d(h,d)
6 l,dp,d(h,d) 19 l,hp,h+1p,dp,d+1p,d(h,d) 49 l,g-1p,gp,dp,d+1p,d(h,d) 76 l,?hm,?sm,d(h,s)
7 l,hf ,hp,df ,dp,d(h,d) 20 l,h-1p,hp,dp,d-1p,d(h,d) 50 l,gp,g+1p,hp,d(h,d) Linear S. Features
8 l,hp,df ,dp,d(h,d) Grandchild Features 51 l,gp,g-1p,hp,d(h,d) 58 l,sp,s+1p,hp,d(h,d)
9 l,hf ,df ,dp,d(h,d) 21 l,hp,dp,gp,d(h,d,g) 52 l,gp,hp,h+1p,d(h,d) 59 l,sp,s-1p,hp,d(h,d)
10 l,hf ,hp,df ,d(h,d) 22 l,hp,gp,d(h,d,g) 53 l,gp,hp,h-1p,d(h,d) 60 l,sp,hp,h+1p,d(h,d)
11 l,hf ,df ,hp,d(h,d) 23 l,dp,gp,d(h,d,g) 54 l,gp,g+1p,h-1p,hp,d(h,d) 61 l,sp,hp,h-1p,d(h,d)
12 l,hf ,df ,d(h,d) 24 l,hf ,gf ,d(h,d,g) 55 l,g-1p,gp,h-1p,hp,d(h,d) 62 l,sp,s+1p,h-1p,d(h,d)
13 l,hp,dp,d(h,d) 25 l,df ,gf ,d(h,d,g) 56 l,gp,g+1p,hp,h+1p,d(h,d) 63 l,s-1p,sp,h-1p,d(h,d)
77 l,hl,hp,d(h,d) 26 l,gf ,hp,d(h,d,g) 57 l,g-1p,gp,hp,h+1p,d(h,d) 64 l,sp,s+1p,hp,d(h,d)
78 l,hl,d(h,d) 27 l,gf ,dp,d(h,d,g) Sibling Features 65 l,s-1p,sp,hp,h+1p,d(h,d)
79 l,hp,d(h,d) 28 l,hf ,gp,d(h,d,g) 30 l,hp,dp,sp,d(h,d) ?r(h,d) 66 l,sp,s+1p,dp,d(h,d)
80 l,dl,dp,d(h,d) 29 l,df ,gp,d(h,d,g) 31 l,hp,sp,d(h,d)?r(h,d) 67 l,sp,s-1p,dp,d(h,d)
81 l,dl,d(h,d) 91 l,hl,gl,d(h,d,g) 32 l,dp,sp,d(h,d)?r(h,d) 68 sp,dp,d+1p,d(h,d)
82 l,dp,d(h,d) 92 l,dp,gp,d(h,d,g) 33 l,pf ,sf ,d(h,d)?r(h,d) 69 sp,dp,d-1p,d(h,d)
83 l,dl,hp,dp,hl,d(h,d) 93 l,gl,hp,d(h,d,g) 34 l,pp,sf ,d(h,d)?r(h,d) 70 sp,s+1p,d-1p,dp,d(h,d)
84 l,dl,hp,dp,d(h,d) 94 l,gl,dp,d(h,d,g) 35 l,sf ,pp,d(h,d)?r(h,d) 71 s-1p,sp,d-1p,dp,d(h,d)
85 l,hl,dl,dp,d(h,d) 95 l,hl,gp,d(h,d,g) 36 l,sf ,dp,d(h,d)?r(h,d) 72 sp,s+1p,dp,d+1p,d(h,d)
86 l,hl,hp,dp,d(h,d) 96 l,dl,gp,d(h,d,g) 37 l,sf ,dp,d(h,d)?r(h,d) 73 s-1p,sp,dp,d+1p,d(h,d)
87 l,hl,dl,hp,d(h,d) 74 l,?dm,?gm,d(h,d) 38 l,df ,sp,d(h,d)?r(h,d) Special Feature
88 l,hl,dl,d(h,d) Linear G. Features 97 l,hl,sl,d(h,d)?r(h,d) 39 ?l,hp,dp,xpbetween h,d
89 l,hp,dp,d(h,d) 42 l,gp,g+1p,dp,d(h,d) 98 l,dl,sl,d(h,d)?r(h,d)
41 l,?hm,?dm,d(h,d) 43 l,gp,g-1p,dp,d(h,d)
Table 4: Features Groups. l represents the label, h the head, d the dependent, s a sibling, and g a
grandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance.
used cores. The parsing time is 1.9 times faster
on two cores and 3.4 times faster on 4 cores. Hy-
per threading can improve the parsing times again
and we get with hyper threading 4.6 faster parsing
times. Hyper threading possibly reduces the over-
head of threads, which contains already our single
core version.
Algorithm 4: Parallel Feature Extraction
A // weight arrays
extract-features-and-calc-arrays(xi)
data-list? {} // thread-save data list
for w1 ? 1 to |xi|
for w2 ? 1 to |xi|
data-list? data-list ?{(w1, w2)}
c? number of CPU cores
for t? 1 to c
Tt ? create-array-thread(t, xi,data-list)
start array-thread Tt// start thread t
for t? 1 to c
join Tt// wait until thread t is finished
A? A ? collect-result(Tt)
return A
//
array-thread T
d? remove-first-element(data-list)
if d is empty then end-thread
... // extract features and calculate part d of A
Cores te tp ta rest total pars. train.
1 379 21.3 18.2 1.5 420 354 45.8h
2 196 11.7 9.2 2.1 219 187 23.9h
3 138 8.9 6.5 1.6 155 126 16.6h
4 106 8.2 5.2 1.6 121 105 13.2h
4+4h 73.3 8.8 4.8 1.3 88.2 77 9.6h
Table 5: Elapsed times in milliseconds for differ-
ent numbers of cores. The parsing time (pars.)
are expressed in milliseconds per sentence and
the training (train.) time in hours. The last row
shows the times for 8 threads on a 4 core CPU
with Hyper-threading. For these experiment, we
set the clock speed to 3.46 Ghz in order to have
the same clock speed for all experiments.
6 Non-Projective Approximation
Threshold
For non-projective parsing, we use the Non-
Projective Approximation Algorithm of McDon-
ald and Pereira (2006). The algorithm rearranges
edges in a dependency tree when they improve
the score. Bohnet (2009) extended the algorithm
by a threshold which biases the rearrangement of
the edges. With a threshold, it is possible to gain
a higher percentage of correct dependency links.
We determined a threshold in experiments for
Czech, English and German. In the experiment,
we use the Hash Kernel and increase the thresh-
95
System Average Catalan Chinese Czech English German Japanese Spanish
Top CoNLL 09 85.77(1) 87.86(1) 79.19(4) 80.38(1) 89.88(2) 87.48(2) 92.57(3) 87.64(1)
Baseline Parser 85.10 85.70 76.88 76.93 90.14 87.64 92.26 86.12
this work 86.33 87.45 76.99 80.96 90.33 88.06 92.47 88.13
Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al (2009), (2) Bohnet (2009), (3) Che et
al. (2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel. The
numbers in bold face mark the top scores. We used for Catalan, Chinese, Japanese and Spanish the
projective parsing algorithm.
old at the beginning in small steps by 0.1 and later
in larger steps by 0.5 and 1.0. Figure 2 shows
the labeled attachment scores for the Czech, En-
glish and German development set in relation to
the rearrangement threshold. The curves for all
languages are a bit volatile. The English curve
is rather flat. It increases a bit until about 0.3
and remains relative stable before it slightly de-
creases. The labeled attachment score for Ger-
man and Czech increases until 0.3 as well and then
both scores start to decrease. For English a thresh-
old between 0.3 and about 2.0 would work well.
For German and Czech, a threshold of about 0.3
is the best choice. We selected for all three lan-
guages a threshold of 0.3.
74
76
78
80
82
84
86
88
0 1 2 3 4 5
Czech English German
Figure 2: English, German, and Czech labeled at-
tachment score (y-axis) for the development set in
relation to the rearrangement threshold (x-axis).
7 Conclusion and Future Work
We have developed a very fast parser with ex-
cellent attachment scores. For the languages of
the 2009 CoNLL Shared Task, the parser could
reach higher accuracy scores on average than the
top performing systems. The scores for Catalan,
Chinese and Japanese are still lower than the top
scores. However, the parser would have ranked
second for these languages. For Catalan and
Chinese, the top results obtained transition-based
parsers. Therefore, the integration of both tech-
niques as in Nivre and McDonald (2008) seems
to be very promising. For instance, to improve
the accuracy further, more global constrains cap-
turing the subcategorization correct could be inte-
grated as in Riedel and Clarke (2006). Our faster
algorithms may make it feasible to consider fur-
ther higher order factors.
In this paper, we have investigated possibilities
for increasing parsing speed without any accuracy
loss. The parsing time is 3.5 times faster on a sin-
gle CPU core than the baseline parser which has
an typical architecture for a maximum spanning
tree parser. The improvement is due solely to the
Hash Kernel. The Hash Kernel was also a prereq-
uisite for the parallelization of the parser because
it requires much less memory bandwidth which is
nowadays a bottleneck of parsers and many other
applications.
By using parallel algorithms, we could further
increase the parsing time by a factor of 3.4 on a
4 core CPU and including hyper threading by a
factor of 4.6. The parsing speed is 16 times faster
for the English test set than the conventional ap-
proach. The parser needs only 77 millisecond in
average to parse a sentence and the speed will
scale with the number of cores that become avail-
able in future. To gain even faster parsing times, it
may be possible to trade accuracy against speed.
In a pilot experiment, we have shown that it is
possible to reduce the parsing time in this way to
as little as 9 milliseconds. We are convinced that
the Hash Kernel can be applied successful to tran-
sition based dependency parsers, phrase structure
parsers and many other NLP applications. 4
4We provide the Parser and Hash Kernel as open source
for download from http://code.google.com/p/mate-tools.
96
References
Attardi, G. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings
of CoNLL, pages 166?170.
Blum., A. 2006. Random Projection, Margins, Ker-
nels, and Feature-Selection. In LNCS, pages 52?68.
Springer.
Bohnet, B. 2009. Efficient Parsing of Syntactic and
Semantic Dependency Structures. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009).
Carreras, X. 2007. Experiments with a Higher-order
Projective Dependency Parser. In EMNLP/CoNLL.
Che, W., Li Z., Li Y., Guo Y., Qin B., and Liu T. 2009.
Multilingual Dependency-based Syntactic and Se-
mantic Parsing. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009).
Collins, M. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
Crammer, K., O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2003. Online Passive-Aggressive Algo-
rithms. In Sixteenth Annual Conference on Neural
Information Processing Systems (NIPS).
Crammer, K., O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551?585.
Eisner, J. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?
345, Copenhaen.
Eisner, J., 2000. Bilexical Grammars and their Cubic-
time Parsing Algorithms, pages 29?62. Kluwer
Academic Publishers.
Gesmundo, A., J. Henderson, P. Merlo, and I. Titov.
2009. A Latent Variable Model of Syn-
chronous Syntactic-Semantic Parsing for Multiple
Languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), Boulder, Colorado, USA., June 4-5.
Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
Shared Task: Syntactic and Semantic Dependencies
in Multiple Languages. In Proceedings of the 13th
CoNLL-2009, June 4-5, Boulder, Colorado, USA.
Johansson, R. and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proceedings of the Shared Task
Session of CoNLL-2008, Manchester, UK.
McDonald, R. and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms.
In In Proc. of EACL, pages 81?88.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line Large-margin Training of Dependency Parsers.
In Proc. ACL, pages 91?98.
Nivre, J. and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers.
In ACL-08, pages 950?958, Columbus, Ohio.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-
Based Dependency Parsing. In Proceedings of the
8th CoNLL, pages 49?56, Boston, Massachusetts.
Nivre, J. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In 8th International
Workshop on Parsing Technologies, pages 149?160,
Nancy, France.
Nivre, J. 2009. Non-Projective Dependency Parsing in
Expected Linear Time. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP, pages 351?359, Suntec, Singapore.
Rahimi, A. and B. Recht. 2008. Random Features
for Large-Scale Kernel Machines. In Platt, J.C.,
D. Koller, Y. Singer, and S. Roweis, editors, Ad-
vances in Neural Information Processing Systems,
volume 20. MIT Press, Cambridge, MA.
Ren, H., D. Ji Jing Wan, and M. Zhang. 2009. Pars-
ing Syntactic and Semantic Dependencies for Mul-
tiple Languages with a Pipeline Approach. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boul-
der, Colorado, USA., June 4-5.
Riedel, S. and J. Clarke. 2006. Incremental Inte-
ger Linear Programming for Non-projective Depen-
dency Parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 129?137, Sydney, Australia, July.
Association for Computational Linguistics.
Shi, Q., J. Petterson, G. Dror, J. Langford, A. Smola,
and S.V.N. Vishwanathan. 2009. Hash Kernels for
Structured Data. In Journal of Machine Learning.
Titov, I. and J. Henderson. 2007. A Latent Variable
Model for Generative Dependency Parsing. In Pro-
ceedings of IWPT, pages 144?155.
Yamada, H. and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of IWPT, pages 195?206.
97
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 98?106,
Beijing, August 2010
Broad Coverage Multilingual Deep Sentence Generation with a
Stochastic Multi-Level Realizer
Bernd Bohnet1, Leo Wanner1,2, Simon Mille1, Alicia Burga1
1Department of Information and Communication Technologies
Pompeu Fabra University
2Institucio? Catalana de Recerca i Estudis Avanc?ats (ICREA)
{first-name.last-name}@upf.edu
Abstract
Most of the known stochastic sentence
generators use syntactically annotated
corpora, performing the projection to
the surface in one stage. However,
in full-fledged text generation, sentence
realization usually starts from semantic
(predicate-argument) structures. To be
able to deal with semantic structures,
stochastic generators require semantically
annotated, or, even better, multilevel an-
notated corpora. Only then can they
deal with such crucial generation issues as
sentence planning, linearization and mor-
phologization. Multilevel annotated cor-
pora are increasingly available for multi-
ple languages. We take advantage of them
and propose a multilingual deep stochastic
sentence realizer that mirrors the state-of-
the-art research in semantic parsing. The
realizer uses an SVM learning algorithm.
For each pair of adjacent levels of anno-
tation, a separate decoder is defined. So
far, we evaluated the realizer for Chinese,
English, German, and Spanish.
1 Introduction
Recent years saw a significant increase of inter-
est in corpus-based natural language generation
(NLG), and, in particular, in corpus-based (or
stochastic) sentence realization, i.e., that part of
NLG which deals with mapping of a formal (more
or less abstract) sentence plan onto a chain of in-
flected words; cf., among others, (Langkilde and
Knight, 1998; Oh and Rudnicky, 2000; Bangalore
and Rambow, 2000; Wan et al, 2009). The advan-
tage of stochastic sentence realization over tradi-
tional rule-based realization is mainly threefold:
(i) it is more robust, (ii) it usually has a signifi-
cantly larger coverage; (iii) it is per se language-
and domain-independent. Its disadvantage is that
it requires at least syntactically annotated corpora
of significant size (Bangalore et al, 2001). Given
the aspiration of NLG to start from numeric time
series or conceptual or semantic structures, syn-
tactic annotation even does not suffice: the cor-
pora must also be at least semantically annotated.
Up to date, deep stochastic sentence realization
was hampered by the lack of multiple-level an-
notated corpora. As a consequence, available
stochastic sentence generators either take syntac-
tic structures as input (and avoid thus the need for
multiple-level annotation) (Bangalore and Ram-
bow, 2000; Langkilde-Geary, 2002; Filippova
and Strube, 2008), or draw upon hybrid models
that imply a symbolic submodule which derives
the syntactic representation that is then used by
the stochastic submodule (Knight and Hatzivas-
siloglou, 1995; Langkilde and Knight, 1998).
The increasing availability of multilevel anno-
tated corpora, such as the corpora of the shared
task of the Conference on Computational Natu-
ral Language Learning (CoNLL), opens new per-
spectives with respect to deep stochastic sentence
generation?although the fact that these corpora
have not been annotated with the needs of genera-
tion in mind, may require additional adjustments,
as has been, in fact, in the case of our work.
98
In this paper, we present a Support Vector
Machine (SVM)-based multilingual dependency-
oriented stochastic deep sentence realizer that
uses multilingual corpora of the CoNLL ?09
shared task (Hajic?, 2009) for training. The sen-
tences of these corpora are annotated with shal-
low semantic structures, dependency trees, and
lemmata; for some of the languages involved,
they also contain morphological feature annota-
tions. The multilevel annotation allows us to take
into account all levels of representation needed
for linguistic generation and to model the pro-
jection between pairs of adjacent levels by sep-
arate decoders, which, in its turn, facilitates the
coverage of such critical generation tasks as sen-
tence planning, linearization, and morphologiza-
tion. The presented realizer is, in principle,
language-independent in that it is trainable on any
multilevel annotated corpus. In this paper, we dis-
cuss its performance for Chinese, English, Ger-
man, and Spanish.
The remainder of the paper is structured as fol-
lows. In Section 2, we discuss how the shallow se-
mantic annotation in the CoNLL ?09 shared task
corpora should be completed in order to be suit-
able for generation. Section 3 presents the train-
ing setup of our realizer. Section 4 shows the in-
dividual stages of sentence realization: from the
semantic structure to the syntactic structure, from
the syntactic structure to the linearized structure
and from the linearized structure to a chain of in-
flected word forms (if applicable for the language
in question). Section 5 outlines the experimental
set up for the evaluation of our realizer and dis-
cusses the results of this evaluation. In Section 6,
finally, some conclusions with respect to the char-
acteristics of our realizer and its place in the re-
search landscape are drawn.
The amount of the material which comes into
play makes it impossible to describe all stages
in adequate detail. However, we hope that the
overview provided in what follows still suffices to
fully assess our proposal.
2 Completing the Semantic Annotation
The semantic annotation of sentences in CoNLL
?09 shared task corpora follows the PropBank an-
notation guidelines (Palmer et al, 2005). Prob-
lematic from the viewpoint of generation is that
this annotation is not always a connected acyclic
graph. As a consequence, in these cases no valid
(connected) syntactic tree can be derived. The
most frequent cases of violation of the connectiv-
ity principle are not attached adjectival modifiers,
determiners, adverbs, and coordinations; some-
times, the verb is not connected with its argu-
ment(s). Therefore, prior to starting the training
procedure, the semantic annotation must be com-
pleted: non-connected adjectival modifiers must
be annotated as predicates with their syntactic
heads as arguments, determiners must be ?trans-
lated? into quantifiers, detached verbal arguments
must be connected with their head, etc.
Algorithm 1 displays the algorithm that com-
pletes the semantic annotations of the corpora.
Each sentence xi of the corpus I , with i =
1, . . . , |I|, is annotated with its dependency tree
yi and its shallow semantic graph si. The algo-
rithm traverses yi breath-first, and examines for
each node n in yi whether n?s corresponding node
in si is connected with the node corresponding to
the parent of n. If not, the algorithm connects both
by a directed labeled edge. The direction and the
label of the edge are selected consulting a look up
table in which default labels and the orientation
of the edges between different node categories are
specified.
Figure 1 shows the semantic representation of
a sample English sentence obtained after the ap-
plication of Algorithm 1. The solid edges are
the edges available in the original annotation; the
dashed edges have been introduced by the algo-
rithm. The edge labels ?A0? and ?A1? stand for
?first argument? and ?second argument? (of the
corresponding head), respectively, ?R-A0? for ?A0
realized as a relative clause?, and ?AM-MNR? for
?manner modifier?. As can be seen, 6 out of the
total of 14 edges in the complete representation
of this example have been added by Algorithm 1.
We still did not finish the formal evaluation of
the principal changes necessary to adapt the Prop-
Bank annotation for generation, nor the quality of
our completion algorithm. However, the need of
an annotation with generation in mind is obvious.
99
Algorithm 1: Complete semantic graph
//si is a semantic graph and yi a dependency tree
// si = ?Nsi , Lsi , Esi?, where Nsi is the set of nodes// Lsi the set of edge labels// Esi ? Ns ?Ns ? Ls is the set of edgesfor i? 1 to |I| // iteration over the training examples
let ry ? yi be the root node of the dependency tree
// initialization of the queue
nodeQueue ? children(ry)
while nodeQueue 6= ? do
ny ? removeFirst(nodeQueue)
// breath first: add nodes at the end of the queue
nodeQueue? nodeQueue ? children(ny)
nys ? sem(ny); pys ? sem(parent(ny))//get the semantic equivalents of ny and of its parent
if not exists path(nys , pys ) then
l? label(ny ,parent(ny))
ls ? look-up-sem-label(nys , pys , l)if look-up-sem-direction(nys , pys , ls) = ??? then// add the semantic edge
Es? Es ? (pys , nys , ls)else // direction of the edge ???
// add the semantic edge
Es? Es ? (nys , pys , ls)
3 Realizer Training Setup
Figure 2 shows the training setup of our realizer.
For each level of annotation, an SVM feature ex-
tractor and for each pair of adjacent levels of an-
notation, an SVM decoder is defined. The Sem-
Synt decoder constructs from a semantic graph
the corresponding dependency tree. The Synt-
Linearization decoder derives from a dependency
tree a chain of lemmata, i.e., determines the word
order within the sentence. The Linearization-
Morph decoder generates the inflected word form
for each lemma in the chain. Both the fea-
ture extractors and the decoders are language-
independent, which makes the realizer applicable
to any language for which multilevel-annotated
corpora are available.
To compute the score of the alternative realiza-
tions by each decoder, we apply MIRA (Margin
Infused Relaxed Algorithm) to the features pro-
vided by the feature extractors. MIRA is one
of the most successful large-margin training tech-
niques for structured data (Crammer et al, 2006).
It has been used, e.g., for dependency parsing,
semantic role labelling, chunking and tagging.
Since we have similar feature sets (of compara-
ble size) as those for which MIRA has proven to
work well, we assume that it will also perform
a an
A1
A1
A0
A1
A0
A1
A1
A1
A0
AM-MNR
A1
A2
A0
be
illustrate
but
Panama
that
substute
their
system
produce
gridlock
that
absurd
R-A0
Figure 1: Semantic representation of the sentence
But Panama illustrates that their substitute is a
system that produces an absurd gridlock. after
completion
well for sentence realization. Unfortunately, due
to the lack of space, we cannot present here the
instantiation of MIRA for all stages of our model.
For illustration, Algorithm 2 outlines it for mor-
phological realization.
The morphologic realization uses the minimal
string edit distance (Levenshtein, 1966) to map
lemmata to word forms. As input to the MIRA-
classifier, we use the lemmata of a sentence, its
dependency tree and the already ordered sentence.
The characters of the input strings are reversed
since most of the changes occur at the end of the
words and the string edit scripts work relatively
to the beginning of the string. For example, to
calculate the minimal string edit distance between
the lemma go and the form goes, both are first
reversed by the function compute-edit-dist and
then the minimal string edit script between og and
seog is computed. The resulting script is Ie0Is0.
It translates into the operations ?insert e at the po-
sition 0 of the input string? and ?insert s at the po-
sition 0?.
Before MIRA starts, we compute all mini-
mal edit distance scripts to be used as classes of
MIRA. Only scripts that occur more often than
twice are used. The number of the resulting edit
scripts is language-dependent; e.g., we get about
100
Semantic annotation
Syntactic annotation
Lineariz. annotation
Morphol. annotation
sem. feature extr.
synt. feature extr.
lineariz. feature extr.
morph. feature extr.
Sem-Synt DECODER
Synt-Lineariz. DECODER
Linear-Morph DECODER
SVM
Figure 2: Realizer training scenario setup
1500 scripts for English and 2500 for German.
The training algorithms typically perform 6 it-
erations (epochs) over the training examples. For
each training example, a minimal edit script is se-
lected. If this script is different from the gold
script, the features of the gold script are calcu-
lated and the weight vector of the SVM is adjusted
according to the difference between the predicted
vector and the gold feature vector. The classifi-
cation task consists then in finding the classifica-
tion script that maps the lemma to the correct word
form. For this purpose, the classifier scores each
of the minimal edit scripts according to the input,
choosing the one with the highest score.
4 Sentence Generation
Sentence generation that starts from a given se-
mantic structure as input consists in the applica-
tion of the previously trained SVM decoders in se-
quence in order to realize the following sequence
of mappings:
SemStr? SyntStr? LinearStr? Surface
4.1 Semantic Generation
Algorithm 3 shows the algorithm for semantic
generation, i.e., the derivation of a dependency
tree from a semantic structure. It is a beam search
that creates a maximum spanning tree. In the first
step, a seed tree consisting of one edge is built.
In each of the subsequent steps, this tree is ex-
tended by one node. For the decision, which node
Algorithm 2: Morphological realization
training with MIRA
// yi, li; yi is a dependency tree, li lemmatized sentence
script-list? {} //initialize the script-list
for i? 1 to |I| // iteration over the training examples
for l? 1 to |li| do//// iteration over the lemmata of li
lemmal? lower-case (li,l)
//ensure that all lemmata start with a lower case letter
script? compute-edit-dist-script(lemmal, form(li,l))
if script 6? script-list
script-list? script-list ? { script }
for k? 1 to E // E = number of traininig epochs
for i? 1 to |I| // iteration over the training examples
for l? 1 to |li| do
scriptp? predict-script(li,yi,l)
scriptg ? edit-dist-script(lemmal, form(li,l))
if scriptp 6= scriptg then
// update the weight vector v and the vector w, which
// averages over all collected weight vectors acc.
// to diff. of the predicted and gold feature vector
update w, v according to ?(?(scriptp), ?(scriptg))
//with ?(scriptp), ?(scriptg) as feature vectors of
//scriptp and scriptg , respectively
is to be attached next and to which node, we con-
sider the highest scoring options. This procedure
works well since nodes that are close in the se-
mantic structure are usually close in the syntactic
tree as well. Therefore subtrees that contain those
nodes are considered first.
Unlike the traditional n-gram based stochastic
realizers such as (Langkilde and Knight, 1998),
we use for the score calculation structured fea-
tures composed of the following elements: (i) the
lemmata, (ii) the distance between the starting
node s and the target node t, (iii) the direction
of the path (if the path has a direction), (iv) the
sorted bag of in-going edges labels without repi-
tition, (v) the path of edge labels between source
and target node.
The composed structured features are:
? label+dist(s, t)+dir
? label+dist(s, t)+lemmas+dir
? label+dist(s, t)+lemmat+dir
? label+dist(s, t)+lemmas+lemmat+dir
? label+dist(s, t)+bags+dir
? label+dist(s, t)+bagt+dir
? label+path(s, t)+dir
101
# word-pairs(w1,w2) # n-grams
1 labelw1+labelw2 13 PoS1+PoS2+PoS32 labelw1+lemma1 14 PoS1+PoS2+PoS3+dist3 labelw1+lemma2 15 lemma1+lemma2+lemma34 labelw2+lemma1 16 lemma1+lemma2+lemma3+dist5 labelw2+lemma2 17 lemma1+lemma3+head(w1,w2,w3)6 PoS1+PoS2 18 lemma1+lemma3+head(w1,w2,w3)+dist7 PoS1+PoS2+head(w1,w2) 19 label1+label2+label3+head(w1,w2,w3)8 labelw1+labelw2+PoS1+head(w1,w2) 20 label1+label2+label3+head(w1,w2,w3)+dist9 labelw1+labelw2+PoS2+head(w1,w2) 21 label1+label2+label3+lemma1+PoS2+head(w1,w2,w3)10 labelw1+labelw2+PoS1+PoS2+head(w1,w2) 22 label1+label2+label3+lemma1+PoS2+head(w1,w2,w3)+dist11 labelw1+labelw2+PoS1+#children2+head(w1,w2) 23 label1+label2+label3+lemma2+PoS1+head(w1,w2,w3)12 labelw1+labelw2+PoS2+#children1+head(w1,w2) 24 label1+label2+label3+lemma2+PoS1+head(w1,w2,w3)+dist
# global features for constituents
25 if |constituent| > 1 then label1st+labellast+labellast?1+PoSfirst+PoSlast+PoShead
26 if |constituent| > 2 then label1st+label2d+label3d+PoSlast+PoSlast?1+PoShead+contains-?
27 if |constituent| > 2 then label1st+label2d+label3d+PoSlast+PoSlast?1+lemmahead+contains-?
28 if |constituent| > 3 then PoS1st+PoS2d+PoS3d+PoS4th+PoSlast+labelhead+contains-?+pos-head
29 if |constituent| > 3 then PoSlast+PoSlast?1+PoSlast?2+PoSlast?3+PoSfirst+labelhead+contains-?+pos-head30 PoSfirst+PoSlast+lemmafirst+lemmalast+lemmahead+contains-?+pos-head
Table 1: Feature schemas used for linearization (labelw is the label of the in-going edge to a word w in
the dependency tree; lemmaw is the lemma of w, and PoSw is the part-of-speech tag of w; head(w1,w2,
. . . ) is a function which is 1 if w1 is the head, 2 if w2 is the head, etc. and else 0; dist is the position
within the constituent; contains-? is a boolean value which is true if the sentence contains a question
mark and false otherwise; pos-head is the position of the head in the constituent)
4.2 Dependency Tree Linearization
Since we use unordered dependency trees as syn-
tactic structures, our realizer has to find the opti-
mal linear order for the lexemes of each depen-
dency tree. Algorithm 4 shows our linearization
algorithm. To order the dependency tree, we use a
one classifier-approach for all languages?in con-
trast to, e.g., Filippova and Strube (2009), who use
a two-classifier approach for German.1
The algorithm is again a beam search. It starts
with an elementary list for each node of the depen-
dency tree. Each elementary list is first extended
by the children of the node in the list; then, the
lists are extended stepwise by the children of the
newly added nodes. If the number of lists during
this procedure exceeds the threshold of 1000, the
lists are sorted in accordance with their score, and
the first 1000 are kept. The remaining lists are
removed. Afterwards, the score of each list is ad-
justed according to a global score function which
takes into account complex features such as the
first word of a consitutent, last word, the head, and
the edge label to the head (cf. Table 1 for the list
of the features). Finally, the nodes of the depen-
1We decided to test at this stage of our work a uniform
technology for all languages, even if the idiosyncrasies of
some languages may be handled better by specific solutions.
dency tree are ordered with respect to the highest
ranked lists.
Only in a very rare case, the threshold of the
beam search is exceeded. Even with a rich feature
set, the procedure is very fast. The linearization
takes about 3 milliseconds in average per depen-
dency tree on a computer with a 2.8 Ghz CPU.
4.3 Morphological Realization
The morphological realization algorithm selects
the edit script in accordance with the highest score
for each lemma of a sentence obtained during
training (see Algorithm 2 above) and applies then
the scripts to obtain the word forms; cf. Algo-
rithm 5.
Table 2 lists the feature schemas used for mor-
phological realization.
5 Experiments
To evaluate the performance of our realizer, we
carried out experiments on deep generation of
Chinese, English, German and Spanish, starting
from CoNLL ?09 shared task corpora. The size of
the test sets is listed in Table 3.2
2As in (Langkilde-Geary, 2002) and (Ringger et al,
2004), we used Section 23 of the WSJ corpus as test set for
English.
102
Algorithm 3: Semantic generation
//si, y semantic graph and its dependency tree
for i? 1 to |I| // iteration over the training examples
// build an initial tree
for all n1 ? si do
trees? {} // initialize the constructed trees list
for all n2 ? si do
if n1 6= n2 then
for all l ? dependency-labels do
trees = trees ? {(synt(n1),synt(n2),l)}
trees? sort-trees-descending-to-score(trees)
trees? look-forward(1000,sublist(trees,20))
//assess at most 1000 edges of the 20 best trees
tree? get-best-tree-due-to-score(trees)
(s,t,l)? first-added-edge(tree)
// create the best tree
best-tree? (s,t,l)
// compute the nodes that still need to be attached
rest? nodes(si) - {s, t}
while rest 6= ? do
trees? look-forward(1000,best-tree,rest)
tree? get-best-tree-due-to-score(trees)
(s,t,l)? first-added-edge(tree)
best-tree? best-tree ? { (s,t,l) }
if (root(s,best-tree)) then rest? rest - {s}
else rest? rest - {t}
The performance of both the isolated stages and
the realizer as a whole has been assessed.
5.1 Evaluation Metrics
In order to measure the correctness of the se-
mantics to syntax mapping, we use the unlabeled
and labeled attachment score as it commonly used
in dependency parsing. The labeled attachment
score (LAS) is the proportion of tokens that are as-
signed both the correct head and the correct edge
label. The unlabeled attachment score (ULA) is
the proportion of correct tokens that are assigned
the correct head.
To assess the quality of linearization, we use
three different evaluation metrics. The first metric
is the per-phrase/per-clause accuracy (acc snt.),
which facilitates the automatic evaluation of re-
sults:
acc = correct constituentsall constituents
As second evaluation metric, we use a metric
related to the edit distance:
di = 1? mtotal number of words
(with m as the minimum number of deletions
combined with insertions to obtain the correct or-
der (Ringger et al, 2004)).
Algorithm 4: Dependency tree lineariza-
tion
//yi a dependency tree
for i? 1 to |I| // iteration over the training examples
// iterate over all nodes of the dependency tree yi
for n? 1 to |yi| do
subtreen? children(n) ? {n}
ordered-listsn? {} // initialize
for all m ? subtreen do
beam? {}
for all l ? ordered-lists do
beam? beam ? { append(clone(l),m)}
for all l ? ordered-lists do
score(l)? compute-score-for-word-list(l)
sort-lists-descending-to-score(beam,score)
if | beam | > beam-size then
beam? sublist(0,1000,beam)
ordered-listsn? beam
scoreg(l)? score(l) + compute-global-score(l)
sort-lists-descending-in-score(beam,scoreg)
Algorithm 5: Morphological realization
// yi a dependency tree, and li an ordered list of lemmata
for l? 1 to |li| do
scriptp? predict-script(li,yi,l)
forml? apply-edit-dist-script(lemmal, scriptp)
To be able to compare our results with (He et
al., 2009) and (Ringger et al, 2004), we use the
BLEU score as a third metric.
For the asessment of the quality of the word
form generation, we use the accuracy score. The
accuracy is the ratio between correctly generated
word forms and the entire set of generated word
forms.
For the evaluation of the sentence realizer as a
whole, we use the BLEU metric.
5.2 Experimental Results
Table 4 displays the results obtained for the iso-
lated stages of sentence realization and of the real-
ization as a whole, with reference to a baseline and
to some state-of-the-art works. The baseline is
the deep sentence realization over all stages start-
ing from the original semantic annotation in the
CoNLL ?09 shared task corpora.
Note, that our results are not fully comparable
with (He et al, 2009; Filippova and Strube, 2009)
and (Ringger et al, 2004), respectively, since the
data are different. Furthermore, Filippova and
Strube (2009) linearize only English sentences
103
# features
1 es+lemma
2 es+lemma+m.feats
3 es+lemma+m.feats+POS
4 es+lemma+m.feats+POS+position
5 es+lemma+(lemma+1)+m.feats
6 es+lemma+(lemma+1)+POS
7 es+lemma+(m.feats-1)+(POS-1)
8 es+lemma+(m.feats-1)+(POS-1)+position
9 es+m.feats+(m.feats-1)
10 es+m.feats+(m.feats+1)
11 es+lemma+(m.feats-1)
12 es+m.feats+(m.feats-1)+(m.feats-2)
13 es+m.feats+POS
14 es+m.feats+(m.feats+1)
15 es+m.feats+(m.feats+1)+lemma
16 es+m.feats
17 es+e0+e1+m.feats
18 es+e0+e1+e2+m.feats
19 es+e0+e1+e2+e3+m.feats
20 es+e0+e1+e2+e3+e4+m.feats
21 es+e0+m.feats
Table 2: Feature schemas used for morphological
realization
Chinese English German Spanish
2556 2400 2000 1725
Table 3: The number of sentences in the test sets
used in the experiments
that do not contain phrases that exceed 20,000 lin-
earization options?which means that they filter
out about 1% of the phrases.
For Spanish, to the best of our knowledge, no
linearization experiments have been carried out so
far. Therefore, we cannot contrast our results with
any reference work.
As far as morphologization is concerned, the
performance achieved by our realizer for English
is somewhat lower than in (Minnen et al, 2001)
(97.8% vs. 99.8% of accuracy). Note, however,
that Minnen et al describe a combined analyzer-
generator, in which the generator is directly de-
rived from the analyzer, which makes both ap-
proaches not directly comparable.
5.3 Discussion
The overall performance of our SVM-based deep
sentence generator ranges between 0.611 (for Ger-
man) and 0.688 (for Chinese) of the BLEU score.
HALogen?s (Langkilde-Geary, 2002) scores range
between 0.514 and 0.924, depending on the com-
pleteness of the input. The figures are not directly
comparable since HALogen takes as input syntac-
tic structures. However, it gives us an idea where
our generator is situated.
Traditional linearization approaches are rule-
based; cf., e.g., (Bro?ker, 1998; Gerdes and Ka-
hane, 2001; Duchier and Debusmann, 2001), and
(Bohnet, 2004). More recently, statistic language
models have been used to derive word order, cf.
(Ringger et al, 2004; Wan et al, 2009) and (Fil-
ippova and Strube, 2009). Because of its partially
free order, which is more difficult to handle than
fixed word order, German has often been worked
with in the context of linearization. Filippova and
Strube (2009) adapted their linearization model
originally developed for German to English. They
use two classifiers to determine the word order
in a sentence. The first classifier uses a trigram
LM to order words within constituents, and the
second (which is a maximum entropy classifier)
determines the order of constituents that depend
on a finite verb. For English, we achieve with
our SVM-based classifier a better performance.
As mentioned above, for German, Filippova and
Strube (2009)?s two classifier approach pays off
because it allows them to handle non-projective
structures for the Vorfeld within the field model.
It is certainly appropriate to optimize the perfor-
mance of the realizer for the languages covered in
a specific application. However, our goal has been
so far different: to offer an off-the-shelf language-
independent solution.
The linearization error analysis, first of all of
German and Spanish, reveals that the annotation
of coordinations in corpora of these languages as
?X ? and/or/. . .? Y? is a source of errors. The
?linear? annotation used in the PropBank (?X ?
and/or/. . .? Y?) appears to facilitate higher qual-
ity linearization. A preprocessing stage for au-
tomatic conversion of the annotation of coordi-
nations in the corpora would have certainly con-
tributed to a higher quality. We refrained from
doing this because we did not want to distort the
figures.
The morphologization error analysis indicates
a number of error sources that we will address
in the process of the improvement of the model.
Among those sources are: quotes at the beginning
of a sentence, acronyms, specific cases of start-
ing capital letters of proper nouns (for English and
Spanish), etc.
104
Chinese English German Spanish
Semantics-Syntax (ULA/LAS) 95.71/86.29 94.77/89.76 95.46/82.99 98.39/93.00
Syntax-Topology (di/acc) 0.88/64.74 0.91/74.96 0.82/50.5 0.83/52.77
Syntax-Topology (BLEU) 0.85 0.894 0.735 0.78
Topology-Morphology (accuracy=correct words/all words) ? 97.8 97.49 98.48
All stages (BLEU) 0.688 0.659 0.611 0.68
Baseline (BLEU) 0.12 0.18 0.11 0.14
Syntax-Topology (He et al, 2009) (di/acc) 0.89/? ? ? ?
Syntax-Topology (He et al, 2009) (BLEU) 0.887 ? ? ?
Syntax-Topology (Filippova and Strube, 2009) (di/acc) ? 0.88/67 0.87/61 ?
Syntax-Topology (Ringger et al, 2004) (BLEU) ? 0.836 ? ?
Table 4: Quality figures for the isolated stages of deep sentence realization and the complete process.
As far as the contrastive evaluation of the qual-
ity of our morphologization stage is concerned,
it is hampered by the fact that for the traditional
manually crafted morphological generators, it is
difficult to find thorough quantitative evaluations,
and stochastic morphological generators are rare.
As already repeatedly pointed out above, so far
we intentionally refrained from optimizing the in-
dividual realization stages for specific languages.
Therefore, there is still quite a lot of room for im-
provement of our realizer when one concentrates
on a selected set of languages.
6 Conclusions
We presented an SVM-based stochastic deep mul-
tilingual sentence generator that is inspired by the
state-of-the-art research in semantic parsing. It
uses similar techniques and relies on the same re-
sources. This shows that there is a potential for
stochastic sentence realization to catch up with
the level of progress recently achieved in parsing
technologies.
The generator exploits recently available
multilevel-annotated corpora for training. While
the availability of such corpora is a condition for
deep sentence realization that starts, as is usually
the case, from semantic (predicate-argument)
structures, we discovered that current annotation
schemata do not always favor generation such
that additional preprocessing is necessary. This
is not surprising since stochastic generation is a
very young field. An initiative of the generation
community would be appropriate to influence
future multilevel annotation campaigns or to feed
back the enriched annotations to the ?official?
resources.3
The most prominent features of our generator
are that it is per se multilingual, it achieves an ex-
tremely broad coverage, and it starts from abstract
semantic structures. The last feature allows us to
cover a number of critical generation issues: sen-
tence planning, linearization and morphological
generation. The separation of the semantic, syn-
tactic, linearization and morphological levels of
annotation and their modular processing by sep-
arate SVM decoders also facilitates a subsequent
integration of other generation tasks such as re-
ferring expression generation, ellipsis generation,
and aggregation. As a matter of fact, this gen-
erator instantiates the Reference Architecture for
Generation Systems (Mellish et al, 2006) for lin-
guistic generation.
A more practical advantage of the presented
deep stochastic sentence generator (as, in prin-
ciple, of all stochastic generators) is that, if
trained on a representative corpus, it is domain-
independent. As rightly pointed out by Belz
(2008), traditional wide coverage realizers such
as KPML (Bateman et al, 2005), FUF/SURGE
(Elhadad and Robin, 1996) and RealPro (Lavoie
and Rambow, 1997), which were also intended
as off-the-shelf plug-in realizers still tend to re-
quire a considerable amount of work for integra-
tion and fine-tuning of the grammatical and lexical
resources. Deep stochastic sentence realizers have
the potential to become real off-the-shelf modules.
Our realizer is freely available for download at
http://www.recerca.upf.edu/taln.
3We are currently working on a generation-oriented mul-
tilevel annotation of corpora for a number of languages. The
corpora will be made available to the community.
105
Acknowledgments
Many thanks to the three anonymous reviewers for
their very valuable comments and suggestions.
References
Bangalore, S. and O. Rambow. 2000. Exploiting a
Probabilistic Hierarchical Model for Generation. In
Proceedings of COLING ?00, pages 42?48.
Bangalore, S., J. Chen, and O. Rambow. 2001. Impact
of Quality and Quantity of Corpora on Stochastic
Generation. In Proceedings of the EMNLP Confer-
ence, pages 159?166.
Bateman, J.A., I. Kruijff-Korbayova?, and G.-J. Krui-
jff. 2005. Multilingual Resource Sharing Across
Both Related and Unrelated Languages: An Imple-
mented, Open-Source Framework for Practical Nat-
ural Language Generation. Research on Language
and Computation, 15:1?29.
Belz, A. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
Bohnet, B. 2004. A graph grammar approach to map
between dependency trees and topological models.
In Proceedings of the IJCNLP, pages 636?645.
Bro?ker, N. 1998. Separating Surface Order and Syn-
tactic Relations in a Dependency Grammar. In Pro-
ceedings of the COLING/ACL ?98.
Crammer, K., O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551?585.
Duchier, D. and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of lin-
ear precedence. In Proceedings of the ACL.
Elhadad, M. and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic real-
ization component. Technical Report TR 96-03,
Department of Mathematics and Computer Science,
Ben Gurion University.
Filippova, K. and M. Strube. 2008. Sentence fusion
via dependency graph compression. In Proceedings
of the EMNLP Conference.
Filippova, K. and M. Strube. 2009. Tree lineariza-
tion in English: Improving language model based
approaches. In Proceedings of the NAACL ?09 and
HLT, Short Papers, pages 225?228.
Gerdes, K. and S. Kahane. 2001. Word order in Ger-
man: A formal dependency grammar using a topo-
logical hierarchy. In Proceedings of the ACL.
Hajic?, J. et al 2009. The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple
Languages. In Proceedings of the CoNLL.
He, W., H. Wang, Y. Guo, and T. Liu. 2009. De-
pendency based chinese sentence realization. In
Proceedings of the ACL and of the IJCNLP of the
AFNLP, pages 809?816.
Knight, K. and V. Hatzivassiloglou. 1995. Two-level,
many paths generation. In Proceedings of the ACL.
Langkilde, I. and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Pro-
ceedings of the COLING/ACL, pages 704?710.
Langkilde-Geary, I. 2002. An empirical verification
of coverage and correctness for a general-purpose
sentence generator. In Proceedings of the Second
INLG Conference, pages 17?28.
Lavoie, B. and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on ANLP.
Levenshtein, V.I. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics, 10:707?710.
Mellish, C., D. Scott, L. Cahill, D. Paiva, R. Evans, and
M. Reape. 2006. A reference architecture for natu-
ral language generation systems. Natural Language
Engineering, 12(1):1?34.
Minnen, G., J. Carroll, and D. Pearce. 2001. Ap-
plied morphological processing for English. Nat-
ural Language Engineering, 7(3):207?223.
Oh, A.H. and A.I. Rudnicky. 2000. Stochastic lan-
guage generation for spoken dialogue systems. In
Proceedings of the ANL/NAACL Workshop on Con-
versational Systems, pages 27?32.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
Ringger, E., M. Gamon, R.C. Moore, D. Rojas,
M. Smets, and S. Corston-Oliver. 2004. Linguis-
tically informed statistical models of constituent
structure for ordering in sentence realization. In
Proceedings of COLING, pages 673?679.
Wan, S., M. Dras, Dale R., and C. Paris. 2009. Im-
proving Grammaticality in Statistical Sentence Gen-
eration: Introducing a Dependency Spanning Tree
Algorithm with an Argument Satisfaction Model. In
Proceedings of the EACL ?09, pages 852?860.
106
Coling 2010: Poster Volume, pages 1122?1130,
Beijing, August 2010
Informed ways of improving data-driven
dependency parsing for German
Wolfgang Seeker
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Bernd Bohnet
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
Bernd.Bohnet@ims.uni-stuttgart.de
Lilja ?vrelid
University of Potsdam
Institut fu?r Linguistik
ovrelid@uni-potsdam.de
Jonas Kuhn
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Abstract
We investigate a series of targeted modifi-
cations to a data-driven dependency parser
of German and show that these can be
highly effective even for a relatively well
studied language like German if they are
made on a (linguistically and methodolog-
ically) informed basis and with a parser
implementation that allows for fast and
robust training and application. Mak-
ing relatively small changes to a range
of very different system components, we
were able to increase labeled accuracy on
a standard test set (from the CoNLL 2009
shared task), ignoring gold standard part-
of-speech tags, from 87.64% to 89.40%.
The study was conducted in less than five
weeks and as a secondary project of all
four authors. Effective modifications in-
clude the quality and combination of auto-
assigned morphosyntactic features enter-
ing machine learning, the internal feature
handling as well as the inclusion of global
constraints and a combination of different
parsing strategies.
1 Introduction
The past years have seen an enormous surge of in-
terest in dependency parsing, mainly in the data-
driven paradigm, and with a particular emphasis
on covering a whole set of languages with a single
approach. The reasons for this interest are mani-
fold; the availability of shared task data from var-
ious CoNLL conferences (among others (Buch-
holz and Marsi, 2006; Hajic? et al, 2009)), com-
prising collections of languages based on a sin-
gle representation format, has certainly been in-
strumental. But likewise, the straightforward use-
fulness of dependency representations for a num-
ber of tasks plays an important role. The rela-
tive language independence of the representations
makes dependency parsing particularly attractive
for multilingually oriented work, including ma-
chine translation.
As data-driven approaches to dependency pars-
ing have reached a certain level of maturity, it may
appear as if further improvements of parsing per-
formance have to rely on relatively advanced tun-
ing procedures, such as sophisticated automatic
feature selection procedures or combinations of
different parsing approaches with complementary
strengths. It is indeed still hard to pinpoint the
structural properties of a language (or annotation
scheme) that make the parsing task easier for a
particular approach, so it may seem best to leave
the decision to a higher-level procedure.
This paper starts from the suspicion that
while sophisticated tuning procedures are cer-
tainly helpful, one should not underestimate the
potential of relatively simple modifications of the
experimental set-up, such as a restructuring of as-
pects of the dependency format, a targeted im-
provement of the quality of automatically as-
signed features, or a simplification of the feature
space for machine learning ? the modifications
just have to be made in an informed way. This
1122
presupposes two things: (i) a thorough linguistic
understanding of the issues at hand, and (ii) a rel-
atively powerful and robust experimental machin-
ery which allows for experimentation in various
directions and which should ideally support a fast
turn-around cycle.
We report on a small pilot study exploring the
potential of relatively small, informed modifica-
tions as a way of improving parsing accuracy
even for a language that has received considerable
attention in the parsing literature, including the
dependency parsing literature, namely German.
Within a timeframe of five weeks and spending
only a few hours a day on the project (between a
group of four people), we were able to reach some
surprising improvements in parsing accuracy.
By way of example, we experimented with
modifications in a number of rather different sys-
tem areas, which we will discuss in the course
of this paper after a brief discussion of related
work and the data basis in Section 2. Based on a
second-order maximum spanning tree algorithm,
we used a hash kernel to facilitate the mapping
of the features onto their weights for a very large
number of features (Section 3); we modified the
dependency tree representation for prepositional
phrases, adding hierarchical structure that facili-
tates the picking up of generalizations (Section 4).
We take advantage of a morphological analyzer
to train an improved part-of-speech tagger (Sec-
tion 5), and we use knowledge about the structure
of morphological paradigms and the morphology-
syntax interface in the feature design for machine
learning (Section 6). As is known from other stud-
ies, the combination of different parsing strategies
is advantageous; we include a relatively simple
parser stacking procedure in our pilot study (Sec-
tion 7), and finally, we apply Integer Linear Pro-
gramming in a targeted way to add some global
constraints on possible combinations of arc labels
with a single head (Section 8). Section 9 offers a
brief conclusion.
2 Related Work and Data Basis
We quickly review the situation in data-driven de-
pendency parsing in general and on applying it to
German specifically.
The two main approaches to data-driven de-
pendency parsing are transition based dependency
parsing (Nivre, 2003; Yamada and Matsumoto,
2003; Titov and Henderson, 2007) and maximum
spanning tree based dependency parsing (Eis-
ner, 1996; Eisner, 2000; McDonald and Pereira,
2006). Transition based parsers typically have
a linear or quadratic complexity (Attardi, 2006).
Nivre (2009) introduced a transition based non-
projective parsing algorithm that has a worst case
quadratic complexity and an expected linear pars-
ing time. Titov and Henderson (2007) combined
a transition based parsing algorithm, using beam
search, with a latent variable machine learning
technique.
Maximum spanning tree based dependency
parsers decompose a dependency structure into
factors. The factors of the first order maximum
spanning tree parsing algorithm are edges consist-
ing of the head, the dependent (child) and the edge
label. This algorithm has a quadratic complexity.
The second order parsing algorithm of McDonald
and Pereira (2006) uses a separate algorithm for
edge labeling. In addition to the first order fac-
tors, this algorithm uses the edges to those chil-
dren which are closest to the dependent and has a
complexity of O(n3). The second order algorithm
of Carreras (2007) uses in addition to McDonald
and Pereira (2006) the child of the dependent oc-
curring in the sentence between the head and the
dependent as well as the edge from the dependents
to a grandchild. The edge labeling is an integral
part of the algorithm which requires an additional
loop over the labels. This algorithm therefore has
a complexity of O(n4). Johansson and Nugues
(2008) reduced the required number of loops over
the edge labels by considering only the edges that
existed in the training corpus for a distinct head
and child part-of-speech tag combination.
Predating the surge of interest in data-based
dependency parsing, there is a relatively long
tradition of dependency parsing work on Ger-
man, including for instance Menzel and Schro?der
(1998) and Duchier and Debusmann (2001). Ger-
man was included in the CoNLL shared tasks in
2006 (Multilingual Dependency Parsing, (Buch-
holz and Marsi, 2006)) and in 2009 (Syntactic and
Semantic Dependencies in Multiple Languages,
(Hajic? et al, 2009)) with data based on the TIGER
1123
corpus (Brants et al, 2002) in both cases. Since
the original TIGER treebank is in a hybrid phrase-
structural/dependency format with a relatively flat
hierarchical structure, conversion to a pure depen-
dency format involves some non-trivial steps. The
2008 ACL Workshop on Parsing German included
a specific shared task on dependency parsing of
German (Ku?bler, 2008), based on two sets of data:
again the TIGER corpus ? however with a differ-
ent conversion routine than for the CoNLL tasks ?
and the Tu?Ba-D/Z corpus (Hinrichs et al, 2004).
In the 2006 CoNLL task and in the 2008 ACL
Workshop task, the task was dependency parsing
with given gold standard part-of-speech tags from
the corpus. This is a valid way of isolating the
specific subproblem of parsing, however it is clear
that the task does not reflect the application set-
ting which includes noise from automatic part-of-
speech tagging. In the 2009 CoNLL task, both
gold standard tags and automatically assigned tags
were provided. The auto-tagged version was cre-
ated with the standard model of the TreeTagger
(Schmid, 1995) (i.e., with no domain-specific tag-
ger training).
In our experiments, we used the data set from
the 2009 CoNLL task, for which the broadest
comparison of recent parsing approaches exists.
The highest-scoring system in the shared task was
Bohnet (2009) with a labeled accuracy (LAS) of
87.48%, on auto-tagged data. The highest-scoring
(in fact the only) system in the dependency pars-
ing track of the 2008 ACL Workshop on parsing
German was Hall and Nivre (2008) with an LAS
of 90.80% on gold-tagged data, and with a data
set that is not comparable to the CoNLL data.1
3 Hash Kernel
Our parser is based on a second order maximum
spanning tree algorithm and uses MIRA (Cram-
mer et al, 2006) as learning technique in combi-
nation with a hash kernel. The hash kernel has
a higher accuracy since it can use additional fea-
tures found during the creation of the dependency
1To get an idea of how the data sets compare, we trained
the version of our parser described in Section 3 (i.e., with-
out most of the linguistically informed improvements) on
this data, achieving labeled accuracy of 92.41%, compared
to 88.06% for the 2009 CoNLL task version.
tree in addition to the features extracted from the
training examples. The modification to MIRA is
simple: we replace the feature-index mapping that
maps the features to indices of the weight vector
by a random function. Usually, the feature-index
mapping in the support vector machine has two
tasks: The mapping maps the features to an index
and it filters out features that never occurred in a
dependency tree. In our approach, we do not filter
out these features, but use them as additional fea-
tures. It turns out that this choice improves pars-
ing quality. Instead of the feature-index mapping
we use the following hash function:2
h ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|
The Hash Kernel for structured data uses the hash
function h : J ? {1...n} to index ? where ?
maps the observations X to a feature space. We
define ?(x, y) as the numeric feature representa-
tion indexed by J . The learning problem is to fit
the function F so that the errors of the predicted
parse tree y are as low as possible. The scoring
function of the Hash Kernel is defined as:3
F (x, y) = ??w ? ?(x, y)
For different j, the hash function h(j) might gen-
erate the same value k. This means that the hash
function maps more than one feature to the same
weight which causes weight collisions. This pro-
cedure is similar to randomization of weights (fea-
tures), which aims to save space by sharing val-
ues in the weight vector (Blum, 2006; Rahimi
and Recht, 2008). The Hash Kernel shares values
when collisions occur that can be considered as
an approximation of the kernel function, because
a weight might be adapted due to more than one
feature. The approximation works very well with
a weight vector size of 115 million values.
With the Hash Kernel, we were able to improve
on a baseline parser that already reaches a quite
high LAS of 87.64% which is higher than the top
score for German (87.48%) in the CoNLL Shared
task 2009. The Hash Kernel improved that value
by 0.42 percentage points to 88.06%. In addition
to that, we obtain a large speed up in terms of pars-
ing time. The baseline parser spends an average of
426 milliseconds to parse a sentence of the test
2>> n shifts n bits right, and % is the modulo operation.
3??w is the weight vector and the size of ??w is n.
1124
set and the parser with Hash Kernel only takes
126 milliseconds which is an increase in speed
of 3.4 times. We get the large speed up because
the memory access to a large array causes many
CPU cache misses which we avoid by replacing
the feature-index mapping with a hash function.
As mentioned above, the speedup influences the
experimenters? opportunities for explorative de-
velopment since it reduces the turnaround time for
experimental trials.
4 Restructuring of PPs
In a first step, we applied a treebank transforma-
tion to our data set in order to ease the learning
for the parser. We concentrated on prepositional
phrases (PP) to get an idea how much this kind
of transformation can actually help a parser. PPs
are notoriously flat in the TIGER Treebank anno-
tation (from which our data are derived) and they
do not embed a noun phrase (NP) but rather attach
all parts of the noun phrase directly at PP level.
This annotation was kept in the dependency ver-
sion and it can cause problems for the parser since
there are two different ways of annotating NPs: (i)
for normal NPs where all dependents of the noun
are attached as daughters of the head noun and (ii)
for NPs in PPs where all dependents of the noun
are attached as daughters to the preposition thus
being sisters to their head noun. We changed the
annotation of PPs by identifying the head noun in
the PP and attaching all of its siblings to it. To find
the correct head, we used a heuristic in the style of
Magerman (1995). The head is chosen by taking
the rightmost daughter of the preposition that has
a category label according to the heuristic and is
labeled with NK (noun kernel element).
Table 1 shows the parser performance on the
data after PP-restructuring.4 The explanation for
the benefit of the restructuring is of course that
4Note that we are evaluating against a gold standard here
(and in the rest of the paper) which has been restructured as
well. With a different gold standard one could argue that the
absolute figures we obtain are not fully comparable with the
original CoNLL shared task. However, since we are doing
dependency parsing, the transformation does neither add nor
remove any nodes from the structure nor do we change any
labels. The only thing that is done during the transforma-
tion is the reattachment of some daughters of a PP. This is
only a small modification, and it is certainly linguistically
warranted.
now there is only one type of NP in the whole cor-
pus which eases the parser?s task to correctly learn
and identify them.
dev. set test set
LAS UAS LAS UAS
hash kernel 87.40 89.79 88.06 90.24
+restructured 87.49 89.97 88.30 90.44
Table 1: Parser performance on restructured data
Since restructuring parts of the corpus seems
beneficial, there might be other structures where
more consistent annotation could help the parser,
e. g., coordination or punctuation (like in the 2008
ACL Workshop data set, cp. Footnote 1).
5 Part-of-Speech Tagging
High quality part-of-speech (PoS) tags can greatly
improve parsing quality. Having a verb wrongly
analyzed as a noun and similar mistakes are very
likely to mislead the parser in its decision process.
A lot of the parser?s features include PoS tags and
reducing the amount of errors during PoS tagging
will therefore reduce misleading feature values as
well. Since the quality of the automatically as-
signed PoS tags in the German CoNLL ?09 data
is not state-of-the-art (see Table 2 below), we de-
cided to retag the data with our own tagger which
uses additional information from a symbolic mor-
phological analyzer to direct a statistical classifier.
For the assignment of PoS tags, we apply
a standard maximum entropy classification ap-
proach (see Ratnaparkhi (1996)). The classes of
the classifier are the PoS categories defined in the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999). We use standard binarized features like
the word itself, its last three letters, whether the
word is capitalized, contains a hyphen, a digit or
whether it consists of digits only. As the only non-
binary feature, word length is recorded. These
standard features are augmented by a number of
binary features that support the classification pro-
cess by providing a preselection of possible PoS
tags. Every word is analyzed by DMOR, a finite
state morphological analyzer, from whose output
analyses all different PoS tags are collected and
added to the feature set. For example, DMOR
assigns the PoS tags NN (common noun) and
ADJD (predicative adjective) to the word gegan-
1125
gen (gone). From these analyses two features are
generated, namely possible-tag:NN and possible-
tag:ADJD, which are strong indicators for the
classifier that one of these classes is very likely
to be the correct one. The main idea here is to
use the morphological analyzer as a sort of lexicon
that preselects the set of possible tags beforehand
and then use the classifier to do the disambigua-
tion (see Jurish (2003) for a more sophisticated
system based on Hidden-Markov models that uses
roughly the same idea). Since the PoS tags are in-
cluded in the feature set, the classifier is still able
to assign every class defined in STTS even if it is
not in the preselection. Where the morphological
analyzer does not know the word in question we
add features for every PoS tag representing a pro-
ductive word class in German, making the reason-
able assumption that the morphology knows about
all closed-class words and word forms. Finally,
we add word form and possible tag features for
the previous and the following word to the feature
set thus simulating a trigram tagger. We used the
method of Kazama and Tsujii (2005) which uses
inequality constraints to do a very efficient feature
selection5 to train the maximum entropy model.
We annotated the entire corpus with versions
of our own tagger, i.e., the training, development
and test data. In order to achieve a realistic be-
havior (including remaining tagging errors, which
the parser may be able to react to if they are sys-
tematic), it was important that each section was
tagged without any knowledge of the gold stan-
dard tags. For the development and test portion,
this is straightforward: we trained a model on the
gold PoS of the training portion of the data and
applied it to retag these two portions. Retagging
the training portion was a bit trickier since we
could not use a model trained on the same data,
but at the same time, we wanted to use a tagger
of similarly high quality ? i.e. one that has seen a
similar amount of training data. The training set
was therefore split into 20 different parts and for
every split, a tagging model was trained on the
other 19 parts which then was used to retag the
remaining 20th part. Table 2 shows the quality
of our tagger evaluated on the German CoNLL
5We used a width factor of 1.0.
?09 data in terms of accuracy and compares it
to the originally annotated PoS tags which have
been assigned by using the TreeTagger (Schmid,
1995) together with the German tagging model
provided from the TreeTagger website. Tagging
accuracy improves consistently by about 2 per-
centage points which equates to an error reduction
of 44.55 % to 49.0 %.
training development test
original 95.69 95.51 95.46
retagged 97.61 97.71 97.52
error red. 44.55% 49.00% 45.37%
Table 2: Tagging accuracy
Table 3 shows the parser performance when
trained on the newly tagged data. The consider-
able improvements in tagging accuracy visibly af-
fect parsing accuracy, raising both the labeled and
the unlabeled attachment score by 0.66 percentage
points (LAS) and 0.51 points (UAS) for the de-
velopment set and by 0.45 points (LAS) and 0.64
points (UAS) for the test set.
dev. set test set
LAS UAS LAS UAS
restructured 87.49 89.97 88.30 90.44
+retagged 88.15 90.48 88.75 91.08
Table 3: Parser performance on retagged data
6 Morphological Information
German, as opposed to English, exhibits a rela-
tively rich morphology. Predicate arguments and
nominal adjuncts are marked with special case
morphology which allows for a less restricted
word order in German. The German case system
comprises four different case values, namely nom-
inative, accusative, dative and genitive case. Sub-
jects and nominal predicates are usually marked
with nominative case, objects receive accusative
or dative case and genitive case is usually used
to mark possessors in possessive constructions.
There are also some temporal and spatial nominal
adjuncts which require certain case values. Since
case is used to mark the function of a noun phrase
in a clause, providing case information to a parser
might improve its performance.
The morphological information in the German
CoNLL ?09 data contains much more information
than case alone and previous models (baseline,
1126
hash kernel, retagged) have used all of it. How-
ever, since we aim to improve a syntactic parser,
we would like to exclude all morphological infor-
mation from the parsing process that is not obvi-
ously relevant to syntax, e. g. mood or tense. By
reducing the morphological annotations to those
that are syntactically relevant, we hope to reduce
the noise that is introduced by irrelevant informa-
tion. (One might expect that machine learning and
feature selection should ?filter out? irrelevant fea-
tures, but given the relative sparsity of unambigu-
ous instances of the linguistically relevant effects,
drawing the line based on just a few thousand sen-
tences of positive evidence would be extremely
hard even for a linguist.)
We annotated every case-bearing word in the
corpus with its case information using DMOR.
With case-bearing words, we mean nouns, proper
nouns, attributive adjectives, determiners and all
kinds of pronouns. Other types of morphologi-
cal information was discarded. We did not use
the manually annotated and disambiguated mor-
phological information already present in the cor-
pus for two reasons: the first one is the same as
with the PoS tagging. Since it is unrealistic to
have gold-standard annotation in a real-world ap-
plication which deals with unseen data, we want
the parser to learn from and hopefully adapt to
imperfectly annotated data. The second reason
is the German-inherent form syncretism in nom-
inal paradigms. The German noun inflection sys-
tem is with over ten different (productive and
non-productive) inflectional patterns quite com-
plicated, and to make matters worse, there are
only five different morphological markers to dis-
tinguish 16 different positions in the pronoun, de-
terminer and adjective paradigms and eight differ-
ent positions in the noun paradigms. Some po-
sitions in the paradigm will therefore always be
marked in the same way and we would like the
parser to learn that some word forms will always
be ambiguous with respect to their case value.
We also conducted experiments where we an-
notated number and gender values in addition to
case. The idea behind this is that number and gen-
der might help to further disambiguate case val-
ues. The downside of this is the increase in fea-
ture values. Combining case and number features
means a multiplication of their values creating
eight new feature values instead of four. Adding
gender annotation raises this number to 24. Be-
side the disambiguation of case, there is also an-
other reason why we might want to add num-
ber and gender: Inside a German noun phrase,
all parts have to agree on their case and number
feature in order to produce a well-formed noun
phrase. Furthermore, the head noun governs the
gender feature of the other parts. Thus, all three
features can be relevant to the construction of a
syntactic structure.6 Table 4 shows the results of
our experiments with morphological features.
dev. set test set
LAS UAS LAS UAS
retagged 88.15 90.48 88.75 91.08
no morph. 87.78 90.18 88.60 90.92
+case 88.04 90.48 88.77 91.13
+c+n 88.21 90.62 88.88 91.13
+c+n+g 87.96 90.33 88.73 90.99
Table 4: Parser performance with morph. infor-
mation (c=case, n=number, g=gender)
The no morph row in Table 4 shows, that
using no morphological information at all de-
creases parser performance. When only case val-
ues are annotated, the parser performance does
not change much in comparison to the retagged
model, so there is no benefit here. Adding num-
ber features on the other hand improves parsing
results significantly. This seems to support our in-
tuition that number helps in disambiguating case
values. However, adding gender information does
not further increase this effect but hurts parser per-
formance even more than case annotation alone.
This leaves us with a puzzle here. Annotating case
and number helps the parser, but case alone or
having case, number and gender together affects
performance negatively. A possible explanation
might be that the effect of the gender information
is masked by the increased number of feature val-
ues (24) which confuses the parsing algorithm.
7 Parser Stacking
Nivre and McDonald (2008) show how two dif-
ferent approaches to data-driven dependency pars-
6Person would be another syntactically relevant informa-
tion. However, since we are dealing with a newspaper cor-
pus, first and second person features appear very rarely.
1127
ing, the graph-based and transition-based ap-
proaches, may be combined and subsequently
learn to complement each other to achieve im-
proved parsing results for different languages.
MaltParser (Nivre et al, 2006) is a language-
independent system for data-driven dependency
parsing which is freely available.7 It is based on a
deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parsing
actions. MaltParser employs a rich feature repre-
sentation in order to guide parsing. For the train-
ing of the Malt parser model that we use in the
stacking experiments, we use learner and parser
settings identical to the ones optimized for Ger-
man in the CoNLL-X shared task (Nivre et al,
2006). Furthermore, we employ the technique
of pseudo-projective parsing described in Nilsson
and Nivre (2005) and a split prediction strategy for
predicting parse transitions and arc labels (Nivre
and Hall, 2008).8 In order to obtain automatic
parses for the whole data set, we perform a 10-
fold split. For the parser stacking, we follow the
approach of Nivre and McDonald (2008), using
MaltParser as a guide for the MST parser with the
hash kernel, i.e., providing the arcs and labels as-
signed by MaltParser as features. Table 5 shows
the scores we obtain by parser stacking. Although
our version of MaltParser does not quite have the
same performance as for instance the version of
Hall and Nivre (2008), its guidance leads to a
small improvement in the overall parsing results.
dev. set test set
LAS UAS LAS UAS
MaltParser 82.47 85.78 83.84 86.8
our parser 88.21 90.62 88.88 91.13
+stacking 88.42 90.77 89.28 91.40
Table 5: Stacked parser performance with guid-
ance by MaltParser
7http://maltparser.org
8The feature models make use of information about the
lexical form (FORM), the predicted PoS (PPOS) and the de-
pendency relation constructed thus far during parsing (DEP).
In addition, we make use of the predicted values for other
morphological features (PFEATS). We employ the arc-eager
algorithm (Nivre, 2003) in combination with SVM learners,
using LIBSVM with a polynomial kernel.
8 Relabeling
In the relabeling step, we pursue the idea that
some erroneous parser decisions concerning the
distribution of certain labels might be detected and
repaired in post-processing. In German and in
most other languages, there are syntactic restric-
tions on the number of subjects and objects that
a verb might select. The parser will learn this be-
havior during training. However, since it is using a
statistical model with a limited context, it can still
happen that two or more of the same grammati-
cal functions are annotated for the same verb. But
having two subjects annotated for a single verb
makes this particular clause uninterpretable for
subsequently applied tasks. Therefore, we would
like to detect those doubly annotated grammatical
functions and correct them in a controlled way.
The detection algorithm is simple: Running
over the words of the output parse, we check for
every word whether it has two or more daughters
annotated with the same grammatical function and
if we find one, we relabel all of its daughters.9 For
the relabeling, we applied a dependency-version
of the function labeler described in Seeker et al
(2010) which uses a maximum entropy classifier
that is restrained by a number of hard constraints
implemented as an Integer Linear Program. These
constraints model the aforementioned selectional
restrictions on the number of certain types of ver-
bal arguments. Since these are hard constraints,
the labeler is not able to annotate more than one
of those grammatical functions per verb. If we
count the number of sentences that contain doubly
annotated grammatical functions in the best pars-
ing results from the previous section, we get 189
for the development set and 153 for the test set.
About two thirds of the doubly annotated func-
tions are subjects and the biggest part of the re-
maining third are accusative objects which are the
most common arguments of German verbs.
Table 6 shows the final results after relabeling
the output of the best performing parser config-
uration from the previous section. The improve-
ments on the overall scores are quite small, which
9The grammatical functions we are looking for are SB
(subject), OA (accusative object), DA (dative), OG (genitive
object), OP (prepositional object), OC (clausal object), PD
(predicate) and OA2 (second accusative object).
1128
dev. set test set
LAS UAS LAS UAS
stacking 88.42 90.77 89.28 91.40
+relabeling 88.48 90.77 89.40 91.40
Table 6: Parse quality after relabeling
is partly due to the fact that the relabeling affects
only a small subset of all labels used in the data.
Furthermore, the relabeling only takes place if a
doubly annotated function is detected; and even
if the relabeling is applied we have no guarantee
that the labeler will assign the labels correctly (al-
though we are guaranteed to not get double func-
tions). Table 7 shows the differences in precision
and recall for the grammatical functions between
the original and the relabeled test set. As one can
see, scores stay mostly the same except for SB,
OA and DA. For OA, scores improve both in recall
and precision. For DA, we trade a small decrease
in precision for a huge improvement in recall and
vice versa for SB, but on a much smaller scale.
Generally spoken, relabeling is a local repair strat-
egy that does not have so much effect on the over-
all score but can help to get some important labels
correct even if the parser made the wrong deci-
sion. Note that the relabeler can only repair incor-
rect label decisions, it cannot help with wrongly
attached words.
original relabeled
rec prec rec prec
DA 64.2 83.2 74.7 79.6
OA 88.9 85.8 90.7 88.2
OA2 0.0 NaN 0.0 NaN
OC 95.2 93.5 95.1 93.7
OG 33.3 66.7 66.7 80.0
OP 54.2 80.8 54.2 79.9
PD 77.1 76.8 77.1 76.8
SB 91.0 90.6 90.7 93.7
Table 7: Improvements on grammatical functions
in the relabeled test set
9 Conclusion
We presented a sequence of modifications to a
data-driven dependency parser of German, depart-
ing from a state-of-the-art set-up in an imple-
mentation that allows for fast and robust train-
ing and application. Our pilot study tested what
can be achieved in a few weeks if the data-driven
technique is combined with a linguistically in-
formed approach, i.e., testing hypotheses of what
should be particularly effective in a very targeted
way. Most modifications were relatively small,
addressing very different dimensions in the sys-
tem, such as the handling of features in the Ma-
chine Learning, the quality and combination of
automatically assigned features and the ability to
take into account global constraints, as well as the
combination of different parsing strategies. Over-
all, labeled accuracy on a standard test set (from
the CoNLL 2009 shared task), ignoring gold stan-
dard part-of-speech tags, increased significantly
from 87.64% (baseline parser without hash ker-
nel) to 89.40%.10 We take this to indicate that a
targeted and informed approach like the one we
tested can have surprising effects even for a lan-
guage that has received relatively intense consid-
eration in the parsing literature.
Acknowledgements
We would like to thank Sandra Ku?bler, Yannick
Versley and Yi Zhang for their support. This work
was partially supported by research grants from
the Deutsche Forschungsgemeinschaft as part of
SFB 632 ?Information Structure? at the Univer-
sity of Potsdam and SFB 732 ?Incremental Speci-
fication in Context? at the University of Stuttgart.
References
Attardi, G. 2006. Experiments with a Multilanguage Non-
Projective Dependency Parser. In Proceedings of CoNLL,
pages 166?170.
Blum, A. 2006. Random Projection, Margins, Kernels, and
Feature-Selection. In LNCS, pages 52?68. Springer.
Bohnet, B. 2009. Efficient Parsing of Syntactic and Se-
mantic Dependency Structures. In Proceedings of CoNLL
2009).
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories, Sozopol.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In In Proc. of
CoNLL, pages 149?164.
Carreras, X. 2007. Experiments with a Higher-order Projec-
tive Dependency Parser. In EMNLP/CoNLL.
10?= 0.01, measured with a tool by Dan Bikel from
www.cis.upenn.edu/? dbikel/download/compare.pl
1129
Crammer, K., O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Journal of
Machine Learning Research, 7:551?585.
Duchier, Denys and Ralph Debusmann. 2001. Topologi-
cal dependency trees: a constraint-based account of linear
precedence. In Proceedings of ACL 2001, pages 180?
187, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Eisner, J. 1996. Three New Probabilistic Models for Depen-
dency Parsing: An Exploration. In Proceedings of Coling
1996, pages 340?345, Copenhaen.
Eisner, J., 2000. Bilexical Grammars and their Cubic-time
Parsing Algorithms, pages 29?62. Kluwer Academic
Publishers.
Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue,
and Y. Zhang. 2009. The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Proceedings of the 13th CoNLL-2009, June
4-5, Boulder, Colorado, USA.
Hall, Johan and Joakim Nivre. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the Workshop on Parsing Ger-
man, pages 47?54, Columbus, Ohio, June. Association for
Computational Linguistics.
Hinrichs, Erhard, Sandra Ku?bler, Karin Naumann, Heike
Telljohann, and Julia Trushkina. 2004. Recent develop-
ments in linguistic annotations of the tu?ba-d/z treebank.
In Proceedings of the Third Workshop on Treebanks and
Linguistic Theories, pages 51?62, Tu?bingen, Germany.
Johansson, R. and P. Nugues. 2008. Dependency-based
Syntactic?Semantic Analysis with PropBank and Nom-
Bank. In Proceedings of the Shared Task Session of
CoNLL-2008, Manchester, UK.
Jurish, Bryan. 2003. A hybrid approach to part-of-speech
tagging. Technical report, Berlin-Brandenburgische
Akademie der Wissenschaften.
Kazama, Jun?Ichi and Jun?Ichi Tsujii. 2005. Maximum en-
tropy models with inequality constraints: A case study on
text categorization. Machine Learning, 60(1):159?194.
Ku?bler, Sandra. 2008. The PaGe 2008 shared task on pars-
ing german. In Proceedings of the Workshop on Parsing
German, pages 55?63, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Magerman, David M. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL 1995, pages 276?
283, Morristown, NJ, USA. Association for Computa-
tional Linguistics Morristown, NJ, USA.
McDonald, R. and F. Pereira. 2006. Online Learning of Ap-
proximate Dependency Parsing Algorithms. In In Proc.
of EACL, pages 81?88.
Menzel, Wolfgang and Ingo Schro?der. 1998. Decision pro-
cedures for dependency parsing using graded constraints.
In Proceedings of the COLING-ACL ?98 Workshop on
Processing of Dependency-Based Grammars, pages 78?
87.
Nilsson, Jens and Joakim Nivre. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL 2005, pages
99?106.
Nivre, Joakim and Johan Hall. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the ACL Workshop on Parsing
German.
Nivre, J. and R. McDonald. 2008. Integrating Graph-Based
and Transition-Based Dependency Parsers. In ACL-08,
pages 950?958, Columbus, Ohio.
Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and
Svetoslav Marinov. 2006. Labeled pseudo-projective de-
pendency parsing with Support Vector Machines. In Pro-
ceedings of CoNLL 2006.
Nivre, J. 2003. An Efficient Algorithm for Projective De-
pendency Parsing. In 8th International Workshop on
Parsing Technologies, pages 149?160, Nancy, France.
Nivre, J. 2009. Non-Projective Dependency Parsing in Ex-
pected Linear Time. In Proceedings of the 47th Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 351?359, Suntec, Singapore.
Rahimi, A. and B. Recht. 2008. Random Features for
Large-Scale Kernel Machines. In Platt, J.C., D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural
Information Processing Systems, volume 20. MIT Press,
Cambridge, MA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP 1996,
volume 1, pages 133?142.
Schiller, Anne, Simone Teufel, and Christine Sto?ckert. 1999.
Guidelines fu?r das Tagging deutscher Textcorpora mit
STTS (Kleines und gro?es Tagset). Technical Report Au-
gust, Universita?t Stuttgart.
Schmid, Helmut. 1995. Improvements in part-of-speech tag-
ging with an application to German. In Proceedings of the
ACL SIGDAT-Workshop, volume 11.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn, and Josef Van
Genabith. 2010. Hard Constraints for Grammatical Func-
tion Labelling. In Proceedings of ACL 2010, Uppsala.
Titov, I. and J. Henderson. 2007. A Latent Variable Model
for Generative Dependency Parsing. In Proceedings of
IWPT, pages 144?155.
Yamada, H. and Y. Matsumoto. 2003. Statistical Depen-
dency Analysis with Support Vector Machines. In Pro-
ceedings of IWPT, pages 195?206.
1130
Coling 2010: Demonstration Volume, pages 33?36,
Beijing, August 2010
A High-Performance Syntactic and Semantic Dependency Parser
Anders Bjo?rkelund? Bernd Bohnet? Love Hafdell? Pierre Nugues?
?Department of Computer science ?Institute for Natural Language Processing
Lund University University of Stuttgart
anders.bjorkelund@cs.lth.se bohnet@ims.uni-stuttgart.de
love.hafdell@cs.lth.se
pierre.nugues@cs.lth.se
Abstract
This demonstration presents a high-
performance syntactic and semantic de-
pendency parser. The system consists of a
pipeline of modules that carry out the to-
kenization, lemmatization, part-of-speech
tagging, dependency parsing, and seman-
tic role labeling of a sentence. The sys-
tem?s two main components draw on im-
proved versions of a state-of-the-art de-
pendency parser (Bohnet, 2009) and se-
mantic role labeler (Bjo?rkelund et al,
2009) developed independently by the au-
thors.
The system takes a sentence as input and
produces a syntactic and semantic anno-
tation using the CoNLL 2009 format. The
processing time needed for a sentence typ-
ically ranges from 10 to 1000 millisec-
onds. The predicate?argument structures
in the final output are visualized in the
form of segments, which are more intu-
itive for a user.
1 Motivation and Overview
Semantic analyzers consist of processing
pipelines to tokenize, lemmatize, tag, and parse
sentences, where all the steps are crucial to their
overall performance. In practice, however, while
code of dependency parsers and semantic role
labelers is available, few systems can be run as
standalone applications and even fewer with a
processing time per sentence that would allow a
?Authors are listed in alphabetical order.
user interaction, i.e. a system response ranging
from 100 to 1000 milliseconds.
This demonstration is a practical semantic
parser that takes an English sentence as input
and produces syntactic and semantic dependency
graphs using the CoNLL 2009 format. It builds
on lemmatization and POS tagging preprocessing
steps, as well as on two systems, one dealing with
syntax and the other with semantic dependencies
that reported respectively state-of-the-art results
in the CoNLL 2009 shared task (Bohnet, 2009;
Bjo?rkelund et al, 2009). The complete system ar-
chitecture is shown in Fig. 1.
The dependency parser is based on Carreras?s
algorithm (Carreras, 2007) and second order span-
ning trees. The parser is trained with the margin
infused relaxed algorithm (MIRA) (McDonald et
al., 2005) and combined with a hash kernel (Shi et
al., 2009). In combination with the system?s lem-
matizer and POS tagger, this parser achieves an
average labeled attachment score (LAS) of 89.88
when trained and tested on the English corpus
of the CoNLL 2009 shared task (Surdeanu et al,
2008).
The semantic role labeler (SRL) consists of a
pipeline of independent, local classifiers that iden-
tify the predicates, their senses, the arguments of
the predicates, and the argument labels. The SRL
module achieves an average labeled semantic F1
of 80.90 when trained and tested on the English
corpus of CoNLL 2009 and combined with the
system?s preprocessing steps and parser.
2 The Demonstration
The demonstration runs as a web application and
is available from a server located at http://
33
	

	


	
	
	


	
	

Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 794?805, Dublin, Ireland, August 23-29 2014.
Automatic Feature Selection for Agenda-Based Dependency Parsing
Miguel Ballesteros
Natural Language Processing Group
Universitat Pompeu Fabra
Barcelona, Spain
miguel.ballesteros@upf.edu
Bernd Bohnet
School of Computer Science
University of Birmingham
Birmingham, United Kingdom
bohnetb@cs.bham.ac.uk
Abstract
In this paper we present an in-depth study on automatic feature selection for beam-search depen-
dency parsers. The search strategy is inherited from the one implemented in MaltOptimizer, but
searches in a much larger set of feature templates that could lead to a higher number of combina-
tions. Our models provide results that are on par with models trained with a larger set of feature
templates, and this implies that our models provide faster training and parsing times. Moreover,
the results establish the state of the art for some of the languages.
1 Introduction
Finding an optimal and accurate set of feature templates is crucial when training statistical parsers; in
fact it is essential when building any machine learning system (Smith, 2011). In dependency parsing, the
features are based on the linguistic information that is annotated within the words and the information
that is being calculated during the parsing process. Researchers normally tend to include a large set of
feature templates in their machine learning models, following the idea that more is always better; however
some recent research on feature selection for transition-based parsing (Ballesteros, 2013; Ballesteros and
Nivre, 2014) and graph-based parsing (He et al., 2013) have shown that more features are not always
better, at least in the case of dependency parsing; models containing more features are always slower in
parsing and training time and they do not always provide better results.
This indicates that a smart feature template selection could be the key in the trade-off for finding an
accurate and fast feature model for a given parsing model. On the one hand, we want a parser that should
provide the best results possible, while on the other hand, we want a parser that should provide the results
in the fastest way possible. For practical applications, a fast model is crucial.
In this paper, we report the results of feature selection experiments that we carried out with the in-
tention of obtaining accurate and faster feature models, for the transition-based Mate parser with and
without graph-based completion models. The Mate parser is a beam search parser that uses a hash kernel
for training, joint part-of-speech tagging, morphological tagging and dependency parsing. As a result of
this research, we provide a framework that allows to find an optimal feature template set for the Mate
parser (Bohnet et al., 2013). Moreover, our models provide some of the highest results ever reported for
a set of treebanks.
The paper is organized as follows. Section 2 describes related work including the used agenda-based
dependency parser. This section depicts the feature templates that can be used by a transition-based or
a graph-based parser. Section 3 describes the feature selection algorithm that we implemented for our
experiments. Section 4 shows the experimental set-up. Section 5 reports the main results of our experi-
ments. Section 6 provides the parsing times and memory requirements. Finally, Section 7 concludes.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
794
Transition Condition
LEFT-ARC
d
([?|i, j], B,?)? ([?|j], B,?[(j, i)?A, ?(j, i)=d]) i 6= 0
RIGHT-ARC
d
([?|i, j], B,?)? ([?|i], B,?[(i, j)?A, ?(i, j)=d])
SHIFT
p,m,l
(?, [i|?],?)? ([?|i], ?,?[pi(i)=p, ?(i)=m,?(i)= l])
SWAP ([?|i, j], ?,?)? ([?|j], [i|?],?) 0 < i < j
Figure 1: Transition set for joint morphological and syntactic analysis. The stack ? is represented as a
list with its head to the right (and tail ?) and the buffer B as a list with its head to the left (and tail ?).
2 Related Work
2.1 Mate Parser
For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs
joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing.
The parser employs a number of techniques that lead to very competitive accuracy such as beam-search
with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set,
a graph-based completion model that adds scores for tree parts which a transition-based parser would not
be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes
into account second and third order factors and obtains a score as soon as the tree parts are completed.
The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et
al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the
two models. The drawback of such a large feature set is a huge impact on the speed. Important research
questions include (1) whether the number of features could be reduced to speed up the parser and (2)
whether languages dependent feature sets would be beneficiary.
2.2 Features in transition-based dependency parsing
Every transition-based parser uses two data structures: (1) a buffer that contains at the beginning of the
parsing process all words of the sentence that have to be parsed, and (2) a stack.
The Mate parser that we used in our experiment follows Nivre?s arc-standard parsing algorithm plus
the SWAP transition to build non-projective dependency trees. Figure 1 depicts the transition system
formally; the SHIFT transition removes the first node from the buffer and puts it on the stack. The
LEFT-ARC
d
transition introduces a labeled dependency edge between the top element on the stack and
the second element of the stack with the label d. The second top element is removed from the stack.
The RIGHT-ARC
d
transition introduces a labeled dependency edge between the second element on the
stack and the top element with the label d while the top element is removed from the stack. The SWAP
transition swaps the position of the topmost nodes of the stack and the buffer.
A classifier selects transitions based on the feature templates that are composed of stack elements,
buffer elements, the already created parse, and the transition sequence. For instance, if the parser contains
the feature template LEMMA(S
1
), it means that it may use the lemma of the word that is in the first
position of the stack in any parsing state in order to select the best parsing action.
2.3 Features in graph-based dependency parsing
A Graph-based dependency parser performs an exhaustive search over trees of the words of a sentence.
Frequently, dynamic programming techniques are used to find the optimal tree for each span, considering
candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to
decide among alternative spans. The typical feature models are based on combinations of edges (as
known as, factors). A factor consists either of a single edge, two or three edges; which are called
first order, second and third order factors, respectively. The later are employed in more advanced and
recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo
and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the
795
vertexes involved in the factors. A feature template of a second order factor is composed of properties
drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted
as POS(H)+POS(D)+POS(C). In our experiments, we use in addition to the transition-based model, a
completion model that uses graph-based feature templates with up to third order factors to re-score the
beam.
2.4 Feature Selection
There has been some recent research on trying to manually find better feature models for dependency
parsers, such as Nivre et al. (2006), Hall et al. (2007), Hall (2008), Zhang and Nivre (2011), and
Agirre et al. (2011). There is also research on automatic feature selection in the case of transition-based
dependency parsing, a good example is MaltOptimizer (Ballesteros and Nivre, 2014) which implements
a search for the best feature model that it can find, following acquired previous experience and deep
linguistic knowledge (Hall et al., 2007; Nivre and Hall, 2010); Nilsson and Nugues (2010) also tried to
search for optimal feature sets in the case of transition-based parsing, starting from a reduced test set
using the concept of topological neighbors. Finally, He He et al. (2013) also tried automatic feature
selection but for a graph-based parsing algorithm, where they pruned the feature space, removing unused
features, in a first-order graph-based dependency parser, providing models that are equally accurate and
faster.
Zhang and Nivre (2011) pointed out that two different parsers based on the same algorithm may
need different feature templates since other design aspects of a parser might have an influence on the
usefulness of feature templates such as the learning technique or the use of beam search.
3 Feature Selection Algorithm
As in MaltOptimizer (Ballesteros and Nivre, 2014), our feature selection algorithm starts with a default
feature set that is based on the MaltParser?s default feature model for an arc-standard parsing algorithm
1
,
it first tests whether the features that are in the default model are actually useful, which means that
whenever we remove any of the features of the default set, the accuracy is still the same (or better).
Let F = {F
1
, . . . , F
n
} be the full set of features,
let M(X) be the evaluation metric for feature set X,
and let ? be the threshold.
1 X ? ?
2 while X 6= F
3 B ? 0
4 Y ? ?
5 for each X
i
? F \X
6 if M(X ? {X
i
}) + ? > B then
7 B ?M(X ? {X
i
})
8 Y ? X ? {X
i
}
9 if M(X) > B then
10 return X
11 else
12 X ? Y
13 return X
Figure 2: Algorithm for forward feature selection.
After that, one by one, the algorithm tries to
add feature templates to the feature set. For each
additional feature template a parser is trained for
testing and if the accuracy is higher than the ac-
curacy of the previous step plus a ? (threshold)
then the feature in question is added to the fea-
ture set. The selection process continues until
all features have been tested, and therefore each
feature has been either added or rejected. Most
of the feature selection is based on the forward
selection algorithm shown in Figure 2, although
there is also a bit of backward selection from the
default set.
The feature selection algorithm only has the
training set as an input, and it splits it into train-
ing and development to validate the outcomes of
the experiments.
2
After the feature selection, we
run the parser model on a held-out test set to measure its performance.
The feature selection is pruned following similar strategies to MaltOptimizer; there are features that are
deeply related and the system tries to avoid unnecessary tests when some features happen to be excluded.
For instance, the algorithm will not try to select the third position of the buffer for the part-of-speech, if
the second position was excluded by the feature selection algorithm.
1
http://www.maltparser.org/userguide.html
2
It makes a 80/20 division; 80% for training, 20% for development.
796
4 Experimental Set-Up
In order to set up the experiments for the feature selection algorithm, we carried out a series of tests
based on the parser settings. From these experiments, we obtained the best parser settings, the threshold
that provides the best results given a development set, and the best scoring method and some additional
configurations, that gave us reliable results and a fast outcome.
We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank
5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with
the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).
3
English: We used the WSJ section
of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the
labeling rules of Nivre (2006).
4
German: We used Tiger Treebank (Brants et al., 2002) in the improved
dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency
Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000;
Boguslavsky et al., 2002).
4.1 Parser settings
As outlined in Section 3, our feature selection experiments require the training of a large number of
parsing models and applying these to the development set.
5
Therefore, we aimed to find a training setup
for the parser that provided fast training times while maintaining a realistic training and optimization
scenario.
A major factor for the time usage is the beam size. The beam contains the alternative syntactic struc-
tures that are considered in the parsing process, and thus it requires more time and memory while it
normally provides better results. The parser uses two additional small beams to store the differently
tagged syntactic structures and morphological structures, for the joint models. We explored a number of
configurations and assessed the parsing performance by carrying out a set of experiments on the Penn
Treebank and the training settings of Bohnet et al. (2013);
6
the results are shown in Table 1.
transition-based model
beam 1 3 5 8 12 20 30 40 50
LAS 88.00 89.71 90.10 90.19 90.26 90.09 90.29 90.46 90.41
POS 96.88 97.02 97.03 97.00 96.94 96.95 97.02 96.92 97.00
TT 4 7 8 9 11 14 16 20 21
transition-based and graph-based completion model
beam 1 3 5 8 12 20 30 40 50
LAS 77.49 88.92 90.13 90.55 90.49 90.62 90.97 90.96 90.75
POS 96.71 96.93 96.97 96.97 96.97 97.05 96.99 97.00 97.04
TT 2 9 11 14 20 32 35 40 48
Table 1: Labeled Accuracy Score (LAS) in percent, Part-of-Speech tag accuracy POS in percent and
training time (TT) in milliseconds per sentence. The parser was applied on the development set and
trained over the Penn Treebank.
The table provides an overview of this preliminary experiment. The upper part of the table shows the
performance when only using the transition-based model. The accuracy improvements are small when
the beam-size becomes larger than 5. Even when we compared the results with the results of a beam size
of 30, we observed only a small accuracy improvement. Further, we observe with a larger beam size a
saturation where the accuracy does not improve and the parsing results show a small variance.
3
Training: 001?815, 1001?1136. Development: 886?931, 1148?1151. Test: 816?885, 1137?1147.
4
Training: 02-21. Development: 24. Test: 23.
5
All this experiments were carried out on a CPU Intel Xeon 3.4 Ghz with 6 cores.
6
We used 25 training iterations and we took the accuracy scores from the last iteration, we used the join parser, the two best
part-of-speech tags and morphological tags. The threshold for the inclusion of part-of-speech tags was set to 0.25 and that of
the morphological tagger to 0.1. We selected a beam size for the alternative POS tags and morphological tags of 4.
797
English German
? LAS UAS POS # LAS UAS POS MOR #
0.05 90.17 91.39 97.00 40 90.57 92.81 97.89 90.45 41
0.02 90.24 91.52 97.04 54 90.83 93.00 98.01 90.55 49
0.01 90.17 91.45 96.90 54 90.90 92.95 97.98 90.69 60
0.00 90.43 91.71 97.00 57 90.89 92.98 97.94 90.59 68
-0.01 90.26 91.47 97.06 69 90.92 93.09 98.02 90.72 79
-0.02 90.27 91.52 97.05 77 91.27 93.37 98.17 90.84 93
-0.05 90.49 91.66 97.01 98 91.02 93.11 98.11 90.69 116
-? 90.37 91.65 96.98 188 90.77 93.00 98.14 89.56 188
Figure 3: Accuracy scores depending on the threshold ?.
The feature selection starts with a default feature set that includes 20 features (cf. Section 3), and
all these features are derived from the default feature models for MaltParser (Nivre et al., 2007)
7
. In
total, the feature selection algorithm, for the transition-based model, may select 188 features. In Table 1
we show the training time (TT). We used this table to selected the optimal settings for the beam. After
considering the trade-off between accuracy and speed, we selected for the feature selection a beam size of
8, since it obtains 90.19 LAS which is close to the highest accuracy score 90.46 and with this beam size
the parser is fast. For a parser trained with all feature templates, the average parsing time per sentence is
9 milliseconds. With 20-60 features, we obtained a parsing time of 2-5 milliseconds per sentence, which
is a faster and more optimal setting for the feature selection. Moreover, with a beam size of 40, we get
parsing times that ranged depending on the number of features from 12 to 50 milliseconds per sentence,
this is impracticable for feature selection experiments.
4.2 Selecting an Optimal Threshold
Feature templates are selected when they provide a higher accuracy compared to the previous feature
set plus a threshold ?. To determine an optimal ? for the feature selection, we carried out a series of
experiments with different ? values. As a first step, we ran the feature selection algorithm starting from
0.05 and reducing the value stepwise to -0.05 (testing 0.05, 0.02, 0.01, 0.0, -0.01, -0.02, -0.05) with the
intention of obtaining accuracy scores for all these settings. Table 3 shows the scores for our experiments
on the development set for the English and German treebanks. We obtained an optimal trade-off between
score and number of features with a ? of 0.0. With higher thresholds, such as 0.02 or 0.05, the feature
selection algorithm was very restrictive, and resulted in lower accuracy scores. This indicates that there
are several features that are not included that could contribute to a higher accuracy; for instance, in the
German case, we see that the algorithm only selects 41 features. Moreover, the accuracy for English with
a ? of 0.0 is even higher compared with the results obtained when all features were included (cf. last
row: ??). For German, we see a highest accuracy score with threshold of?0.02. We might get the best
accuracy with this threshold when applied to the test set; however, the downside of this threshold is that
the algorithm selected 25 more feature templates, which leads to a slower parser.
Figure 4 illustrates the accuracy gain depending on the number of features included. The development
set of these graphs consist of 20% of the original training set. A negative ? leads to the inclusion of more
features, which seem to provide even slightly higher results while including much more features. This
outcome is not fully supported by the results from the development sets for English where we observed
slightly lower results for a ? of -0.02 compared to 0.0.
To determine the optimal threshold ? for a language would come with a high computational cost, we
carried out these experiments for English and German which show only small differences in accuracy
in the threshold range around 0. Therefore, we adopted 0.0 as threshold for our further experiments on
other languages as well, cf. Table 4.
7
http://maltparser.org
798
87	 ?
89	 ?
91	 ?
0	 ? 25	 ? 50	 ? 75	 ? 100	 ?
0.0	 ? 	 ?-??	 ?0.02	 ?
	 ?0.02	 ? 	 ?-??	 ?0.05	 ?
Figure 4: Selected features (x-axis) vs Labeled Accuracy Score (y-axis). Features: transition-based
English German
LAS UAS POS # LAS UAS POS MOR #
LAS 90.34 91.71 97.04 54 90.89 92.98 97.94 90.59 68
LUMP 90.38 91.57 97.09 55 90.82 92.88 98.11 90.65 53
PMLAS 90.12 91.38 97.02 40 89.27 91.66 98.01 90.66 31
Table 2: Experiments with evaluation metrics with a ? of 0.0 on the development sets. Features:
transition-based. The morphology results are only shown for German, because the English treebank
does not contain separate morphological features.
4.3 Selecting the Best Scoring Method
We carried out a number of experiments to determine the best criterion for the inclusion of features into
the model. We tested several evaluation measures that compute the results of each model, that are LAS
[labeled attachment score], LUMP
8
[(labeled attachment score + unlabeled attachment score + mor-
phology accuracy + part-of-speech accuracy)/4] and PMLAS
9
[labeled attachment score, morphology
accuracy and part-of-speech accuracy]. Table 2 shows the results of the feature selection for English and
German for all these scoring methods. We finally selected LAS as our scoring method given that it pro-
vides the best results for German and competitive results (at the same level) for English. LUMP is very
similar, however, it seems a bit more restrictive than LAS. Moreover, PMLAS was the most restrictive
measure, allowing only 31 features for German and 40 for English, which is the reason why there is a
significant lower accuracy for the models selected with PMLAS.
Finally, it is worth mentioning that we explored an alternative criterion for the inclusion of features
into the set. We explored the possibility to include only features that show a statistical significant im-
provement. However, this criterion is too strict as only very few features showed a statistical significant
improvement on its own.
4.4 Selection of Feature Templates of the Graph-based Completion Model
The graph-based completion model re-scores the beam incrementally and leads to a higher accuracy.
We tried to select the graph-based feature templates of the completion model after the selection of the
8
LMP [(labeled attachment score + morphology accuracy + part-of-speech accuracy)/3] would have been another alternative.
However, we wanted to give the syntax still a higher weight in the feature selection process.
9
See (Bohnet et al., 2013)
799
transition-based feature templates. This approach could not reach the accuracy gain shown by Bohnet
and Kuhn (2012). We attempted to compensate this by starting the selection procedure from the default
set with the intention of maximizing potential accuracy gains. However, this procedure did not lead to a
better accuracy when later combined with the selected transition-based feature templates. We tried also
to relax the threshold to -0.02 in order to include more features and to achieve a higher accuracy. Since
this leads to better results, we performed the feature selection for the graph-based completion model with
this setting.
4.5 Morphology for English
The Penn Treebank is annotated with part-of-speech tags that include morphological features such as
NNS (plural noun) or VBD (verb past tense). The corpus does not include separate morphological features.
Splitting up these features could be useful because: (1) the parser might be able to generalize better when
we use the word categories separated from morphological features, and (2) we might take advantage
of the ability of the parser to predict morphology and part-of-speech based on the interaction with the
syntax. Table 3 summarizes the results. Our transition-based parsing model shows only small differences
between the scores for the original POS tag set and the tag set that separates the category and morphology.
transition-based model
LAS UAS POS MOR POS&MOR
baseline dev 90.13 91.44 ? ? 96.97
separate dev 90.11 91.26 97.66 98.81 97.08
baseline test 92.11 93.16 ? ? 97.41
separate test 92.07 93.09 97.88 97.93 97.35
transition-based model with completion model
baseline test 92.41 93.35 ? ? 97.41
separate test 92.53 93.49 97.85 98.89 97.28
Table 3: Experiments on Penn Treebank with separate representation of word category and morphology.
The results of the transition-based model, including the graph-based model shows some larger differ-
ences
The labeled and unlabeled accuracy scores are not statistically significant and we concluded that (1)
and (2) do not probably hold. Splitting up the morphology is a neutral operation in terms of labeled
and unlabeled accuracy scores; however, it is worth noting that our results with the separate test for the
completion model is more competitive, providing an improvement of 0.14 UAS.
5 Experiments: Feature Selection
We applied the feature selection algorithm with the parameters determined in the previous sections on
the corpora of Chinese, English, German, Hungarian and Russian, and we applied the outcome to parse
the held-out test sets with a beam size of 40 and 25 training iterations. Table 4 shows the accuracy scores
and the number of features selected for each language. The threshold for inclusion of the feature was set
to 0, cf. section 4.
The first row (Full) shows the accuracy scores for the full set of features, that includes all 188 feature
templates of the transition-based feature set. The second row gives the accuracy scores that have been
obtained with the reduced feature set gained by the feature selection algorithm described in Section 3.
For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar-
ian and Russian higher labeled and unlabeled accuracy scores. The scores for German are very similar
to the ones obtained with the full set and the scores for English are slightly worse. In the case of the
transition-based parser with graph-based completion model, the results are the same for Chinese, and
slightly worse for the rest of the languages, with the parser at least twice as fast. It is worth noting that
the number of feature templates is reduced by 2/3 across all languages which leads to a much faster
parsing and training time, thus freeing up a huge amount of main memory.
800
German Hungarian Russian
LAS UAS POS MOR # LAS UAS POS MOR # LAS UAS POS MOR #
Transition-based features
Full 91.39 93.39 97.96 90.36 188 87.67 90.38 97.83 96.39 188 86.73 92.24 98.88 94.66 188
Select 91.34 93.36 97.88 90.48 68 87.94 90.51 97.87 96.38 71 87.21 92.40 98.88 94.74 64
Transition-based and graph-based features
Full+Cmp 91.77 93.63 98.14 90.77 326 88.88 91.33 97.84 96.41 326 87.66 92.84 98.82 94.56 326
Sel+Cmp 91.81 93.72 97.85 90.44 206 88.67 91.16 97.83 96.39 209 87.93 93.01 98.89 94.73 202
Sel+Sel 91.60 93.61 97.85 90.39 91 88.40 90.50 97.86 96.39 97 87.57 92.76 98.88 94.59 75
Chinese English
LAS UAS POS # LAS UAS POS #
Transition-based features
Full 77.81 81.13 94.11 188 92.13 93.18 97.40 188
Select 78.04 81.20 94.17 56 91.89 92.93 97.38 57
Transition-based and graph-based features
Full+Cmp 78.34 81.46 94.19 326 92.41 93.35 97.41 326
Sel+Cmp 78.74 81.86 94.13 197 92.22 93.19 97.37 195
Sel+Sel 78.74 81.77 94.28 67 92.08 93.05 97.44 74
Table 4: Labeled attachment score (LAS), unlabeled attachment score (UAS), part-of-speech accuracy
(POS) and morphology accuracy (MOR) per language and model. The first two rows refer only to
transition-based features while the last two rows include transition-based and graph-based features. Full
refers to a model with all transition-based features. Select refers to a model with selected transition-based
features. Full+Cmp refers to a model with all transition-based features and all graph-based features.
Sel+Cmp refers to a model with selected transition-based features and all graph-based features. Sel+Sel
refers to a model with selected transition-based features and selected graph-based features. The English
and Chinese accuracy scores exclude punctuation marks.
More about parsing time, training time and memory requirements is depicted in Section 6. A compar-
ison with state of the art results as shown in the Tables 5a to 5d reveal that the parser with the selected
features of the transition-based, and graph-based model are on an equal level for Chinese, Russian and
Hungarian with state-of-the-art results. With the selected transition-based and the full graph-based fea-
ture templates, the results for these languages surpass current state-of-the-art results.
6 Time and Memory Requirements
The number of feature templates has a serious impact on training time, parsing time and the amount of
main memory required. The feature selection may have huge impact on the speed of a parser. Therefore,
we measure the actual time and memory usage by applying the parser on the English test set of the Penn
Treebank. This was done with different parsing models, and for each model, test runs were performed
with an increasing number of CPU cores. Figure 6 shows an overview of the results.
The parsing model with all transition- and graph-based features takes on one CPU core 0.085 seconds
per sentence (cf. Figure 6, line with rhombus). In contrast, the parser with selected transition-based
features parses a sentence in less than half of the time (0.042 seconds, line with crosses). The parsing
accuracy is only 0.42 percentage points worse (93.35 vs. 92.93 UAS). When we compare the first parsing
model with the model with selected transition-based and graph-based features, we observe a parsing time
of 0.066 seconds per sentence and a small accuracy difference of only 0.27.
If we use six CPU cores then parsing time decreases drastically to 0.016 seconds per sentence for the
selected transition-based feature model, 0.023 for the selected transition- and graph-based feature model
and to 0.05 seconds per sentence for the model with all features (which is much slower). Our experiments
801
Parser UAS LAS POS
McDonald et al. (2005a) 90.9
McDonald and Pereira (2006) 91.5
Huang and Sagae (2010) 92.1
Koo and Collins (2010) 93.04
Zhang and Nivre (2011) 92.9
Martins et al. (2010) 93.26
Bohnet and Nivre (2012) 93.38 92.44 97.33
this work (sel. trans.& sel. cmpl.) 93.05 92.08 97.44
this work (P&M cf. Table 3) 93.49 92.53 ?
Koo et al. (2008) ? 93.16
Carreras et al. (2008) ? 93.5
Suzuki et al. (2009) ? 93.79
(a) Accuracy scores for WSJ-PTB. Results marked with ? use
additional information sources and are not directly comparable
to the others.
Parser UAS POS
MSTParser1 75.56 93.51
MSTParser2 77.73 93.51
Li et al. (2011) 3rd-order 80.60 92.80
Hatori et al. (2011) HS 79.60 94.01
Hatori et al. (2011) ZN 81.20 93.94
this work (sel. trans.) 81.20 94.17
this work (sel. trans.+ sel. cmp.) 81.77 94.28
(b) Accuracy scores for the Chinese treebank converted with
the head rules of Zhang and Clark (2008). MSTParser results
from Li et al. (2011). UAS scores from Li et al. (2011) and Ha-
tori et al. (2011) recalculated from the separate accuracy scores
for root words and non-root words.
Parser UAS LAS POS
Farkas et al. (2012) 90.1 87.2
Bohnet et al. (2013) 91.3 88.9 98.1
this work (sel. trans. & sel. cmpl.) 90.50 88.40 97.83
this work (sel. trans. & full cmpl.) 91.16 88.67 97.86
(c) State of the art comparison for Hungarian. The table shows
that we can reach state of the art performance with less features.
Parser UAS LAS POS
Boguslavsky et al. (2011) 90.0 86.0
Bohnet et al. (2013) 92.8 87.6 98.5
this work (sel. trans. & sel. cmp.) 92.76 87.57 98.89
this work (sel. trans. & full cmp.) 93.01 87.93 98.88
(d) State of the art comparison for Russian.
Figure 5: Comparison with state of the art results.
0	 ?
0,025	 ?
0,05	 ?
0,075	 ?
0,1	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ?
seco
nds	 ? 	 ?
CPU	 ?Cores	 ?
(1)	 ?all	 ?graph	 ?&	 ?tr.	 ?features	 ?	 ?
(2)	 ?selected	 ?tr.	 ?&	 ?all.	 ?graph	 ?features	 ?
(3)	 ?selected	 ?graph	 ?&	 ?tr.	 ?features	 ?
(4	 ?)selected	 ?tr.	 ?features	 ?
id trans. features graph features # features UAS
(1) all all 75.8 M 93.35
(2) selected all 43.5 M 93.22
(3) selected selected 22.1 M 93.08
(4) selected none 17.8 M 92.93
Figure 6: Parsing Time in relation to CPU cores and number of features in the hash kernel in millions.
demonstrate that we can double the parsing speed and maintain a very high parsing accuracy.
7 Conclusions
In this paper, we have presented the first feature selection algorithm for agenda-based dependency pars-
ing. Our algorithm could be directly used out of the box,
10
and applied to a new data set or language to
get an optimized feature model for a agenda-based parser such as the Mate tools.
11
Our feature selection algorithm provides models with even higher accuracy for Chinese and Russian,
cf.Table 4. For the remaining languages the models provide accuracy scores that are comparable to
the ones obtained by models including a larger set of feature templates. For all languages, the feature
models gained via feature selection are faster and require less memory, which make them very useful
for practical applications. We conclude that feature models obtained with the feature selection algorithm
10
The source code and the feature models found for each language are available at https://code.google.com/p/
mate-tools/
11
https://code.google.com/p/mate-tools/wiki/ParserAndModels
802
often provide a comparable accuracy level while they are considerable faster. Finally, our model for
English with the separated morphology tag-set provides one of the best results reported with 93.49 UAS.
Additionally, the feature selection algorithms for this setting shows competitive results with a largely
reduced number of feature templates, and thus less parsing time and lower memory requirements. The
parser is faster (almost double) and provides 93.05 UAS which is also among the best results.
Acknowledgments
This research project was supported by funding of the European Union (PCIG13-GA-2013-618143).
References
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and Joakim Nivre. 2011. Improving Dependency Parsing
with Semantic Classes. In The 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL), pages 699?703, Portland, USA.
Miguel Ballesteros and Joakim Nivre. 2014. MaltOptimizer: Fast and Effective Parser Optimization. Natural
Language Engineering.
Miguel Ballesteros. 2013. Effective morphological feature selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,
pages 53?60.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grigoriev, Leonid Kreidlin, and Nadezhda Frid. 2000. Depen-
dency treebank for Russian: Concept, tools, types of information. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING), pages 987?991.
Igor Boguslavsky, Ivan Chardin, Svetlana Grigorieva, Nikolai Grigoriev, Leonid Iomdin, Leonid Kreidlin, and
Nadezhda Frid. 2002. Development of a dependency treebank for Russian and its possible applications in NLP.
In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages
852?856.
Igor Boguslavsky, Leonid Iomdin, Victor Sizov, Leonid Tsinman, and Vadim Petrochenkov. 2011. Rule-based
dependency parser refined by empirical and corpus statistics. In Proceedings of the International Conference
on Dependency Linguistics, pages 318?327.
Bernd Bohnet and Jonas Kuhn. 2012. The best of bothworlds - a graph-based completion model for transition-
based parsers. In Proceedings of the 15th Conference of the European Chapter of the Association for Computa-
tional Linguistics (EACL), pages 77?87.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1455?1465, Jeju Island,
Korea, July. Association for Computational Linguistics.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich?ard Farkas, Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Association for Computational
Linguistics(TACL), 1:415?428.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. TIGER treebank. In
Proceedings of the 1st Workshop on Treebanks and Linguistic Theories (TLT), pages 24?42.
Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language
Learning (CoNLL), pages 9?16.
Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the
CoNLL Shared Task at the 2007 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-CONLL), pages 957?961.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the
42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 112?119.
803
Rich?ard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 15th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 55?65.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ulsen Eryi?git, Be?ata Megyesi, Mattias Nilsson, and Markus Saers. 2007.
Single Malt or Blended? A Study in Multilingual Parser Optimization. In Proceedings of the CoNLL Shared
Task at the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CONLL), pages 933?939.
Johan Hall. 2008. Transition-Based Natural Language Parsing with Dependency and Constituency Representa-
tions. Ph.D. thesis, V?axj?o University.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint pos tagging and
dependency parsing in chinese. pages 1216?1224.
He He, Hal Daum?e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In EMNLP,
pages 1455?1464.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1077?1086.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics (ACL), pages 1?11.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 595?603.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for
chinese POS tagging and dependency parsing. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1180?1191.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo parsers: Dependency
parsing by approximate variational inference. In Proceedings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 34?44.
Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL), pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),
pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji?c. 2005b. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the Human Language Technology Conference and the Conference
on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 523?530.
Peter Nilsson and Pierre Nugues. 2010. Automatic Discovery of Feature Sets for Dependency Parsing. In Pro-
ceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 824?832.
Joakim Nivre and Johan Hall. 2010. A quick guide to MaltParser optimization. Technical report, maltparser.org.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsen Eryi?git, and Svetoslav Marinov. 2006. Labeled pseudo-projective
dependency parsing with support vector machines. In Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 221?225.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13:95?135.
Joakim Nivre. 2006. Inductive Dependency Parsing. Springer.
Wolfgang Seeker and Jonas Kuhn. 2012. Making ellipses explicit in dependency conversion for a german treebank.
pages 3132?3139.
Noah A. Smith. 2011. Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies.
Morgan and Claypool, May.
804
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 551?560.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 562?571.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 188?193,
Portland, Oregon, USA.
805
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1402?1413, Dublin, Ireland, August 23-29 2014.
Deep-Syntactic Parsing
Miguel Ballesteros
1
, Bernd Bohnet
2
, Simon Mille
1
, Leo Wanner
1,3
1
Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain
2
School of Computer Science, University of Birmingham, United Kingdom
3
Catalan Institute for Research and Advanced Studies (ICREA)
1,3
{name.lastname}@upf.edu
2
bohnetb@cs.bham.ac.uk
Abstract
?Deep-syntactic? dependency structures that capture the argumentative, attributive and coordi-
native relations between full words of a sentence have a great potential for a number of NLP-
applications. The abstraction degree of these structures is in-between the output of a syntactic
dependency parser (connected trees defined over all words of a sentence and language-specific
grammatical functions) and the output of a semantic parser (forests of trees defined over indi-
vidual lexemes or phrasal chunks and abstract semantic role labels which capture the argument
structure of predicative elements, dropping all attributive and coordinative dependencies). We
propose a parser that delivers deep syntactic structures as output.
1 Introduction
Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per
force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions
and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and
Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simpli-
fication, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the
source and the target structures. Therefore, the use of more abstract ?syntactico-semantic? structures
seems more appropriate. Following Mel??cuk (1988), we call these structures deep-syntactic structures
(DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic
Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to ab-
stract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame
stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attribu-
tive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank
and Semantic Frame structures are not always connected, may contain either individual lexical items or
phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or
sentential). In other words, they constitute incomplete structures that drop not only idiosyncratic, func-
tional but also meaningful elements of a given sentence and often contain dependencies between chunks
rather than individual tokens. Therefore, we propose to put on the research agenda the task of deep-
syntactic parsing and show how a DSyntS is obtained from a SSynt dependency parse using data-driven
tree transduction in a pipeline with a syntactic parser.
1
In Section 2, we introduce SSyntSs and DSyntSs
and discuss the fundamentals of SSyntS?DSyntS transduction. Section 3 describes the experiments that
we carried out on Spanish material, and Section 4 discusses their outcome. Section 5 summarizes the
related work, before in Section 6 some conclusions and plans for future work are presented.
2 Fundamentals of SSyntS?DSyntS transduction
Before we set out to discuss the principles of the SSyntS?DSynt transduction, we must specify the
DSyntSs and SSyntSs as used in our experiments.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The term ?tree transduction? is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension
of finite state transduction (Aho, 1972) to trees.
1402
2.1 Defining SSyntS and DSyntS
SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value
structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels.
The features of the node labels in SSyntSs are lex
ssynt
, and ?syntactic grammemes? of the value of
lex
ssynt
, i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for
verbs. The value of lex
ssynt
can be any (either full or functional) lexical item; in graphical representations
of SSyntSs, usually only the value of lex
ssynt
is shown. The edge labels of a SSyntS are grammatical
functions ?subj?, ?dobj?, ?det?, ?modif?, etc. In other words, SSyntSs are syntactic structures of the kind
as encountered in the standard dependency treebanks; cf., e.g., dependency version of the Penn TreeBank
(Johansson and Nugues, 2007) for English, Prague Dependency Treebank for Czech (Haji?c et al., 2006),
Ancora for Spanish (Taul?e et al., 2008), Copenhagen Dependency Treebank for Danish (Buch-Kromann,
2003), etc. In formal terms that we need for the outline of the transduction below, a SSyntS is defined as
follows:
Definition 1 (SSyntS) An SSyntS of a language L is a quintuple T
SS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over all lexical items L of L, the set of syntactic grammemes G
synt
, and the set of grammatical
functions R
gr
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L,
? ?
r
s
?a
assigns to each a ? A an r ? R
gr
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
synt
.
The features of the node labels in DSyntSs as worked with in this paper are lex
dsynt
and ?seman-
tic grammemes? of the value of lex
dsynt
, i.e., number and determination for nouns and tense, aspect,
mood and voice for verbs.
2
In contrast to lex
ssynt
in SSyntS, DSyntS?s lex
dsynt
can be any full, but
not a functional lexeme. In accordance with this restriction, in the case of look after a person, AFTER
will not appear in the corresponding DSyntS; it is a functional (or governed) preposition (so are TO or
BY, in Figure 1).
3
In contrast, AFTER in leave after the meeting is a full lexeme; it will remain in the
DSyntS because there it has its own meaning of ?succession in time?. The edge labels of a DSyntS are
language-independent ?deep-syntactic? relations I,. . . ,VI, ATTR, COORD, APPEND. ?I?,. . . ,?VI? are
argument relations, analogous to A0, A1, etc. in the PropBank annotation. ?ATTR? subsumes all (cir-
cumstantial) ARGM-x PropBank relations as well as the modifier relations not captured by the PropBank
and FrameNet annotations. ?COORD? is the coordinative relation as in: John-COORD?and-II?Mary,
publish-COORD?or-II?perish, and so on. APPEND subsumes all parentheticals, interjections, direct
addresses, etc., as, e.g., in Listen, John!: listen-APPEND?John. DSyntSs thus show a strong similarity
with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated;
(ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique
ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv)
they are connected.
4
A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al.,
2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows:
Definition 2 (DSyntS) An DSyntS of a language L is a quintuple T
DS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over the full lexical items L
d
of L, the set of semantic grammemes G
sem
, and the set of deep-
syntactic relations R
dsynt
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L
d
,
? ?
r
s
?a
assigns to each a ? A an r ? R
dsynt
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
sem
.
Consider in Figure 1 an example for an SSyntS and its corresponding DSyntS.
2
Most of the grammemes have a semantic and a surface interpretation; see (Mel??cuk, 2013).
3
Functional lexemes also include auxiliaries (e.g. HAVE, or BE when it is not a copula), and definite and indefinite deter-
miners (THE, A); see Figure 1).
4
Our DSyntSs are thus DSyntSs as used in the Meaning-Text Theory (Mel??cuk, 1988), only that our DSyntSs do not
disambiguate lexical items and do not use lexical functions (Mel??cuk, 1996).
1403
(a) almost 1.2 million jobs have been created by the state thanks to their endeavours
restr
quant
quant
subj
analyt perf
analyt pass
agent
adv
prepos
det
obl obj
prepos
det
(b) almost 1.2 million job create state thanks their endeavour
ATTR
ATTR
ATTR
II
I
ATTR
II
I
Figure 1: An SSyntS (a) and its corresponding DSyntS (b)
2.2 Fleshing out the SSyntS?DSyntS transduction
It is clear that the SSyntS and DSyntS of the same sentence are not isomorphic. The following corre-
spondences between the SSyntS S
ss
and DSyntS S
ds
of a sentence need to be taken into account during
SSyntS?DSyntS transduction:
(i) a node in S
ss
is a node in S
ds
;
(ii) a relation in S
ss
corresponds to a relation in S
ds
;
(iii) a fragment of the S
ss
tree corresponds to a single node in S
ds
;
(iv) a relation with a dependent node in S
ss
is a grammeme in S
ds
;
(v) a grammeme in S
ss
is a grammeme in S
ds
;
(vi) a node in S
ss
is conflated with another node in S
ds
; and
(vii) a node in S
ds
has no correspondence in S
ss
.
The grammeme correspondences (iv) and (v) and the ?pseudo? correspondences in (vi) and (vii)
5
are
few or idiosyncratic and are best handled in a rule-based post-processing stage. The main task of the
SSyntS?DSyntS transducer is thus to cope with the correspondences (i)?(iii). For this purpose, we can
view both SSyntS and DSyntS as vectors indexed in terms of two-dimensional matrices I = N ?N (N
being the set of nodes of a given tree 1, . . . ,m), with I(i, j) = ?(n
i
, n
j
), if n
i
, n
j
? N and (n
i
, n
j
) ? A
and I(i, j) = 0 otherwise (where ??(n
i
, n
j
)? is the function that assigns to an edge a relation label and
i, j = 1, . . . ,m; i 6= j are nodes of the tree). That is, for a given SSyntS, the matrix I(i, j) contains in
the cells (i, j), i, j = 1, . . . ,m, the names of the SSynt-relations between the nodes n
i
and n
j
, and ?0?
otherwise, while for a given DSyntS, the cells of its matrix I
D
contain DSyntS-relations.
Starting from the matrix I
S
of a given SSyntS, the task is therefore to obtain the matrix I
D
of the
corresponding DSyntS, that is, to identify correspondences between i/j, (i, j) and groups of (i, j) of
I
S
with i
?
/j
?
and (i
?
, j
?
) of I
D
; see (i)?(iii) above. In other words, the task consists in identifying and
removing all functional lexemes, and attach correctly the remaining nodes between them.
6
As a ?token chain?surface-syntactic tree? projection, this task can be viewed as a classification task.
However, while the former is isomorphic, we know that the SSyntS?DSyntS projection is not. In order
to approach the task to an isomorphic projection (and thus simplify its modelling), it is convenient to
interpret SSyntS and the targeted DSyntS as collections of hypernodes:
Definition 3 (Hypernode) Given a SSyntS S
s
with its index matrix I
S
(a DSyntS S
d
with its index matrix
I
D
), a node partition p (with |p |? 1) of I
S
(I
D
) is a hypernode h
s
i
(h
d
i
) iff p corresponds to a partition
p
?
(with |p
?
|? 1) of S
d
(S
s
).
In this way, the SSyntS?DSyntS correspondence boils down to a correspondence between individual
hypernodes and between individual arcs, and the transduction embraces the following three (classifica-
tion) subtasks: 1. Hypernode identification, 2. DSynt tree construction, and 3. DSynt arc labeling, which
are completed by a post-processing stage.
5
(vi) covers, e.g., reflexive verb particles such as se in Spanish, which are conflated in the DSyntS with the verb:
se?aux refl dir-conocer vs. CONOCERSE ?know each other?; (vii) covers, e.g., the zero subject in pro-drop languages (which
is absent in the SSyntS and present in the DSyntS).
6
What is particularly challenging is the identification of functional prepositions: based on the information found in the
corpus only, our system must decide if a given preposition is a full or a functional lexeme. That is, we do not resort to any
external lexical resources.
1404
1. Hypernode identification. The hypernode identification consists of a binary classification of the
nodes of a given SSyntS as nodes that form a hypernode of cardinality 1 (i.e., nodes that have a one-
to-one correspondence to a node in the DSyntS) vs. nodes that form part of a hypernode of cardinality
> 1. In practice, hypernodes of type one will be formed by: 1) noun nodes that do not govern determiner
or functional preposition nodes, 2) full verb nodes that are not governed by any auxiliary verb nodes
and that do not govern any functional preposition node, adjective nodes, adverbial nodes, and semantic
preposition nodes. Hypernodes of type two will be formed by: 1) noun nodes + determiner / func-
tional preposition nodes they govern, 2) verb nodes + auxiliary nodes they are governed by + functional
preposition nodes they govern.
2. DSynt tree reconstruction. The outcome of the hypernode identification stage is thus the set H
s
=
H
s
|p|=1
?H
s
|p|>1
of hypernodes of two types. With this set at hand, we can define an isomorphy function
? : H
s
? H
d
|p|=1
(with h
d
? H
d
|p|=1
consisting of n
d
? N
ds
, i.e., the set of nodes of the target DSyntS).
? is the identity function for h
s
? H
s
|p|=1
. For h
s
? H
s
|p|>1
, ? maps the functional nodes in h
s
onto
grammemes (attribute-value pairs) of the lexically meaningful node in h
d
and identifies the lexically
meaningful node as head. Some of the dependencies of the obtained nodes n
d
? N
ds
can be recovered
from the dependencies of their sources. Due to the projection of functional nodes to grammemes (which
can be also seen as node removal), some dependencies will be also missing and must be introduced.
Algorithm 1 recalculates the dependencies for the target DSyntS S
d
, starting from the index matrix I
S
of
SSyntS S
s
to obtain a connected tree.
Algorithm 1: DSyntS tree reconstruction
for ?n
i
? N
d
do
if ?n
j
: (n
j
, n
i
) ? S
s
? ?(n
j
) ? N
d
then
(n
j
, n
i
)? S
d
// the equivalent of the head node of n
i
is included in DSyntS
else if ?n
j
, n
a
: (n
j
, n
i
) ? S
s
? ?(n
j
) 6? N
d
?
?(n
a
) ? N
d
then
//n
a
is the first ancestor of n
j
that has an equivalent in DSyntS
//the equivalent of the head node of n
i
is not included in DSyntS, but the ancestor n
a
is
(n
a
, n
i
)? S
d
else
//the equivalent of the head node of n
i
is not included in DSyntS, but several ancestors of it are
n
b
:= BestHead(n
i
, S
s
, S
d
)
(n
b
, n
i
)? S
d
endfor
BestHead recursively ascends S
s
from a given node n
i
until it encounters one or several head nodes
n
d
? N
ds
. In case of several encountered head nodes, the one which governs the highest frequency
dependency is returned.
3. Label Classification. The tree reconstruction stage produces a ?hybrid? connected dependency tree
S
s?d
with DSynt nodes N
ds
, and arcs A
s
labelled by SSynt relation labels, i.e., an index matrix we
can denote as I
?
, whose cells (i, j) contain SSynt labels for all n
i
, n
j
? N
ds
: (n
i
, n
j
) ? A
s
and
?0? otherwise. The next and last stage of SSynt-to-DSyntS transduction is thus the projection of SSynt
relation labels of S
s?d
to their corresponding DSynt labels, or, in other words, the mapping of I
?
to I
D
of the target DSyntS.
4. Postprocessing. As mentioned in Section 2, there is a limited number of idiosyncratic correspon-
dences between elements of SSyntS and DSyntS (the correspondences (iv?vii) which can be straight-
forwardly handled by a rule-based postprocessor because (a) they are non-ambiguous, i.e., a ? b, c ?
d ? a = b ? c = d, and (b) they are few. Thus, only determiners and auxiliaries in SSyntS map onto a
grammeme in DSyntS, both SSyntS and DSyntS count with less than a dozen grammemes, etc.
3 Experiments
In order to validate the outlined SSyntS?DSyntS transduction and to assess its performance in combi-
nation with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of
1405
experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2.
JointPoS TaggerSSynt parser
SSynt?DSyntTransducerPlainSentences
DSyntTreebankSSyntTreebank
SSyntS DSynS
Figure 2: Setup of a deep-syntactic parser
For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et
al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS
treebank the semantically and information structure influenced relation tags to obtain an annotation gran-
ularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)).
Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and
86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641
tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank).
To obtain the SSyntS, we use Bohnet and Nivre (2012)?s transition-based parser, which combines
lemmatization, PoS tagging, and syntactic dependency parsing?tuned and trained on the respective sets
of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set.
POS LEMMA LAS UAS
96.14 91.10 78.64 86.49
Table 1: Results of Bohnet and Nivre?s surface-syntactic parser on the development set
In what follows, we first present the realization of the SSyntS?DSyntS transducer and then the real-
ization of the baseline.
3.1 SSyntS?DSyntS transducer
As outlined in Section 2.2, the SSyntS?DSyntS transducer is composed of three submodules and a post-
processing stage:
1. Hypernode identification. For the hypernode identification, we trained a binary polynomial (degree
2) SVM from LIBSVM (Chang and Lin, 2001). The SVM allows both features related to the processed
node and higher-order features, which can be related to the head node of the processed node or to its
sibling nodes. After several feature selection trials, we chose the following features for each node n:
? lemma or stem of the label of n,
? label of the relation between n and its head,
? surface PoS of n?s label (the SSynt and DSyntS treebanks distinguish between surface and deep
PoS),
? label of the relation between n?s head to its own head,
? surface PoS of the label of n?s head node.
After an optimization round of the parameters available in the SVM implementation, the hypernode
identification achieved over the gold development set 99.78% precision and 99.02% recall (and thus
99.4% F1). That is, only very few hypernodes are not identified correctly. The main error source are
governed prepositions: the classifier has to learn when to assign a preposition an own hypernode (i.e.,
when it is lexically meaningful) and when it should be included into the hypernode of the governor (i.e.,
when it is functional). Our interpretation is that the features we use for this task are appropriate, but
that the training data set is too small. As a result, some prepositions are erroneously left out from or
introduced into the DSyntS.
1406
2. Tree reconstruction. The implementation of the tree reconstruction module shows an unlabelled
dependency attachment precision of 98.18% and an unlabelled dependency attachment recall of 97.43%
over the gold development set. Most of the errors produced by this module have their origin in the
previous module, i.e., hypernode identification. When a node has been incorrectly removed, the module
errs in the attachment because it cannot use the node in question as the destination or the origin of a
dependency, as it is the case in the gold-standard annotation:
Gold-standard: ser como e?ne
be like letter-n
II
II
Predicted: ser e?ne
II
When a node has erroneously not been removed, no dependencies between its governor and its depen-
dent can be established since DSyntS must remain a tree (which gives the same LAS and UAS errors as
when a node has been erroneously removed):
Gold-standard: y Michael Jackson
II
Predicted: y a Michael Jackson
and to Michael Jackson
II
II
3. Relation label classification. For relation label classification, we use a multiclass linear SVM. The
label classification depends on the concrete annotation schemata of the SSyntS and DSyntS treebanks
on which the parser is trained. Depending on the schemata, some DSynt relation labels may be easier to
derive from the original SSyntS relation labels than others. Table 2 lists all SSynt relation labels that have
a straightforward mapping to DSyntS relation labels in the used treebanks, i.e., neither their dependent
nor their governor are removed, and the SSyntS label always maps to the same DSynt label.
SSynt DSynt
abbrev ATTR
abs pred ATTR
adv ATTR
adv mod ATTR
agent I
appos ATTR
attr ATTR
aux phras ?
aux refl dir II
SSynt DSynt
aux refl indir III
bin junct ATTR
compl1 II
compl2 III
compl adnom ATTR
coord COORD
copul II
copul clitic II
copul quot II
SSynt DSynt
dobj clitic II
dobj quot II
elect ATTR
juxtapos APPEND
modal II
modif ATTR
num junct COORD
obj copred ATTR
prepos II
SSynt DSynt
prepos quot II
prolep APPEND
quant ATTR
quasi coord COORD
quasi subj I
relat ATTR
restr ATTR
sequent ATTR
subj I
subj copred ATTR
Table 2: Straightforward SSynt to DSyntS mappings
Table 3 shows SSyntS relation?DSyntS relation label correspondences that are not straightforward.
SSynt DepRel
A
Mapping to DSynt
analyt fut remove Gov and Dep; add tense=FUT
analyt pass remove Gov; invert I and II; add voice=PASS
analyt perf remove Gov; add tense=PAST
analyt progr remove Gov; add tem constituency=PROGR
aux refl lex remove Dep; add se at the end of Gov?s lemma
aux refl pass remove Dep; invert I and II; add voice=PASS
compar remove Dep if conjunction
compar /coord /sub conj remove Dep if governed preposition
det
IF Dep=el?un THEN remove Dep; add definiteness=DEF/INDEF
IF Dep=possessive THEN DepRel ATTR?I?II?III
IF Dep=other THEN DepRel ATTR
dobj remove Dep if governed preposition
iobj remove Dep if governed preposition; DepRel II?III?IV?V?VI
iobj clitic DepRel II?III?IV?V?VI
obl compl remove Dep if governed preposition; DepRel I?II?III?IV?V?VI
obl obj remove Dep if governed preposition; DepRel II?III?IV?V?VI
punc ?
punc init ?
Table 3: Complex SSynt to DSynt mappings
1407
The final set of features selected for label classification includes: (i) lemma of the dependent node, (ii)
dependency relation to the head of the dependent node, (iii) dependency relation label of the head node
to its own head, (iv) dependency relation to the head of the sibling nodes of the dependent node, if any.
After an optimization round of the parameter set of the SVM-model, relation labelling achieved
94.00% label precision and 93.28% label recall on the development set. The recall is calculated con-
sidering all the nodes that are included in the gold standard. The error sources for relation labelling
were mostly the dependencies that involved possessives and the various types of objects (see Table
3) due to their differing valency. For instance, the relation det in su?det?coche ?his/her car? and
su?det?llamada ?his/her phone call? have different correspondences in DSyntS: su?ATTR?coche
vs. su?I?llamada. That is, the DSyntS relation depends on the lexical properties of the governor.
7
Once again, more training data is needed in order to classify better those cases.
4. Postprocessing In the postprocessing stage for Spanish, the following rules capture non-ambiguous
correspondences between elements of the SSynt-index matrix I
S
= N
s
?N
s
and DSyntS index matrix
I
D
= N
d
?N
d
, with n
s
? N
s
and n
d
? N
d
, and n
s
and n
d
corresponding to each other (we do not list
here identity correspondences such as between the number grammemes of n
s
and n
d
):
? if n
s
is dependent of analyt pass or analyt refl pass relation, then the voice grammeme in n
d
is
PASS;
? if n
s
is dependent of analyt progr, then the voice grammeme in n
d
is PROGR;
? if n
s
is dependent of analyt refl lex, then add the particle -SE as suffix of node label (word) of d
d
;
? if any of the children of n
s
is labelled by one of the tokens UN ?a
masc
?, UNA ?a
fem
?, UNOS
?some
masc
? or UNAS ?some
fem
?, then the definiteness grammeme in n
d
INDEF, otherwise it is
DEF;
? if the n
s
label is a finite verb and n
s
does not govern a subject relation, then add to I
?
the relation
n
d
? I?n
?
d
, with n
?
d
being a newly introduced node.
3.2 Baseline
As point of reference for the evaluation of the performance of our SSyntS?DSyntS transducer, we use a
rule-based baseline that carries out the most direct transformations extracted from Tables 2 and 3. The
baseline detects hypernodes by directly removing all the nodes that we are sure need to be removed, i.e.
punctuation and auxiliaries. The nodes that are only potentially to be removed, i.e., all dependents of
DepRels that have a possibly governed preposition or conjunction in Table 3, are left in the DSyntS. The
new relation labels in the DSyntS are obtained by selecting the label that is most likely to substitute the
SSyntS relation label according to classical grammar studies. The rules of the rule-based baseline look
as follows:
1 if (deprel==abbrev) then deep deprel=ATTR
2 if (deprel==obl obj) then deep deprel=II
. . .
n if (deprel==punc) then remove(current node)
4 Results and Discussion
Let us look in this section at the performance figures of the SSyntS parser, the SSyntS?DSyntS trans-
ducer, and the sentence?DSyntS pipeline obtained in the experiments.
4.1 SSyntS?DSyntS transducer results
In Table 4, the performance of the subtasks of the SSyntS?DSyntS transducer is contrasted to the per-
formance of the baselines; the evaluation of the postprocessing subtask is not included because the one-
to-one projection of SSyntS elements to DSyntS guarantees an accuracy of 100% of the operations
performed. The transducer has been applied to the gold standard test set, which is the held-out test set,
with gold standard PoS tags, lemmas and dependency trees. It outputs in total 5610 nodes; the rule-based
baseline outputs 8653 nodes. As mentioned in Section 3, our gold standard includes 5641 nodes.
7
Note that lexemes are not generalized: a verb and its corresponding noun (e.g., construct/construction) are considered
distinct lexemes.
1408
Hyper-Node Detection
Measure Rule-based Baseline Tree Transducer
p 64.31 (5565/8653) 99.79 (5598/5610)
r 98.65 (5565/5641) 99.24 (5598/5641)
F1 77.86 99.51
Attachment and Labelling
Measure Rule-based Baseline Tree Transducer
LAP 50.02 (4328/8653) 91.07 (5109/5610)
UAP 53.05 (4590/8653) 98.32 (5516/5610)
LA-P 57.66 (4989/8653) 92.37 (5182/5610)
LAR 76.72 (4328/5641) 90.57 (5109/5641)
UAR 81.37 (4590/5641) 97.78 (5516/5641)
LA-R 88.44 (4989/5641) 91.86 (5182/5641)
Table 4: Performance of the SSyntS?DSyntS transducer and of the rule-based baseline over the gold-
standard held-out test set (LAP: labelled attachment precision, UAP: unlabelled attachment precision, LA-P: label assign-
ment precision, LAR: labelled attachment recall, UAR: Unlabelled attachment recall and LA-R: Label assignment recall)
Our data-driven SSyntS?DSyntS transducer is much better than the baseline with respect to all eval-
uation measures.
8
The transducer relies on distributional patterns identified in the training data set, and
makes thus use of information that is not available for the rule-based baseline, which studies one node
at a time. However, the rule-based baseline results also show that transduction that would remove a few
nodes would provide results close to a 100% recall for the hypernode detection because a DSynt tree is a
subtree of the SSynt tree (if we ignore the nodes introduced by post-processing). This is also evidenced
by the labeled and attachment recall scores. The results of the transducer on the test and development
sets are quite comparable. The hypernode detection is even better on the test set. The label accuracy
suffers most from using unseen data during the development of the system. The attachment figures are
approximately equivalent on both sets.
4.2 Results of deep-syntactic parsing
Let us consider now the performance of the complete DSynt parsing pipeline (PoS-tagger+surface-
dependency parser? SSyntS?DSyntS transducer) on the held-out test set. Table 5 displays the figures
of the Bohnet and Nivre parser. The figures are in line with the performance of state-of-the-art parsers
for Spanish (Mille et al., 2012).
POS LEMMA LAS UAS
96.05 92.10 81.45 88.09
Table 5: Performance of Bohnet and Nivre?s joint PoS-tagger+dependency parser trained on Ancora-UPF
Table 6 shows the performance of the pipeline when we feed the output of the syntactic parser to the
rule-based baseline SSyntS?DSyntS module and the tree transducer. We observe a clear error propaga-
tion from the dependency parser (which provides 81.45% LAS) to the SSyntS?DSyntS transducer, which
loses in tree quality more than 18%.
Hyper-Node Detection
Measure Baseline Tree Transducer
p 63.87 (5528/8655) 97.07 (5391/5554)
r 98.00 (5528/5641) 95.57 (5391/5641)
F1 77.33 96.31
Labelling and Attachment
Measure Baseline Tree Transducer
LAP 38.75 (3354/8655) 68.31 (3794/5554)
UAP 44.69 (3868/8655) 77.31 (4294/5554)
LA-P 49.66 (4298/8655) 80.47 (4469/5554)
LAR 59.46 (3354/5641) 67.26 (3794/5641)
UAR 68.57 (3868/5641) 76.12 (4294/5641)
LA-R 76.19 (4298/5641) 79.22 (4469/5641)
Table 6: Performance of the deep-syntactic parsing pipeline
5 Related Work
To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As
semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented
structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received
considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007
8
We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too
weak to be used as baseline.
1409
(Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and
semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Haji?c et al., 2009).
The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued
with predicate identification, argument identification, argument labeling, and word sense disambigua-
tion; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly
all arguments to select the best combination was applied. Some of the systems were based on integrated
syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu??s et al.,
2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform
structural changes?as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS.
Klime?s (2006)?s parser removes nodes (producing tectogrammatical structures as in the Prague Depen-
dency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier
works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997).
However, this is not to say that the idea of the surface?surface syntax?deep syntax pipeline is new.
It goes back at least to Curry (1961) and is implemented in a number of more recent works; see, e.g., (de
Groote, 2001; Klime?s, 2006; Bojar et al., 2008).
6 Conclusions and Future Work
We have presented a deep-syntactic parsing pipeline which consists of a state-of-the-art dependency
parser and a novel SSyntS?DSyntS transducer. The obtained DSyntSs can be used in different applica-
tions since they abstract from language-specific grammatical idiosyncrasies of the SSynt structures as
produced by state-of-the art dependency parsers, but still avoid the complexities of genuine semantic
analysis.
9
DSyntS-treebanks needed for data-driven applications can be bootstrapped by the pipeline.
If required, a SSyntS?DSyntS structure pair can be also mapped to a pure predicate-argument graph
such as the DELPH-IN structure (Oepen, 2002) or to an approximation thereof (as the Enju conversion
(Miyao, 2006), which keeps functional nodes), to an DRS (Kamp and Reyle, 1993), or to a PropBank
structure. On the other hand, DSyntS-treebanks can be used for automatic extraction of deep grammars.
As shown by Cahill et al. (2008), automatically obtained resources can be of an even better quality than
manually-crafted resources. In this context, especially research in the context of CCGs (Hockenmeier,
2003; Clark and Curran, 2007) and TAGs (Xia, 1999) should be also mentioned.
To validate our approach with languages other than Spanish, we carried out an experiment on a Chi-
nese SSyntS-DSyntS Treebank (training the DSynt-transducer on the outcome of the SSynt-parser). The
results over predicted input showed an accuracy of about 75%, i.e., an accuracy comparable to the accu-
racy achieved for Spanish. We are also investigating multilingual approaches, such as the one proposed
by McDonald et al. (2013).
In the future, we will carry out further in-depth feature engineering for the task of DSynt-parsing. It
proved to be crucial in semantic role labelling and dependency parsing (Che et al., 2009; Ballesteros and
Nivre, 2012); we expect it be essential for our task as well. Furthermore, we will join surface syntactic
and deep-syntactic parsing we kept so far separate; see, e.g., (Zhang and Clark, 2008; Llu??s et al., 2013;
Bohnet and Nivre, 2012) for analogous proposals. Further research is required here since although joint
models avoid error propagation from the first stage to the second, overall, pipelined models still proved
to be competitive; cf. the outcome of CoNLL shared tasks.
The deep-syntactic parser described in this paper is available for downloading at https://code.
google.com/p/deepsyntacticparsing/.
Acknowledgements
This work has been supported by the European Commission under the contract number FP7-ICT-610411.
Many thanks to the three anonymous COLING reviewers for their very helpful comments and sugges-
tions.
9
The motivation to work with DSyntS instead of SSyntS is thus similiar to the motivation of the authors of the Abstract
Meaning Representation (AMR) for Machine Translation (Banarescu et al., 2013), only that AMRs are considerably more
semantic than DSyntSs.
1410
References
Alfred V. Aho. 1972. The theory of parsing, translation and, compiling. Prentice Hall, Upper Saddle River, NJ.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOptimizer: A System for MaltParser Optimization. In Proceed-
ings of the Eighth International Conference on Language Resources and Evaluation (LREC 12).
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop & Interoperability with Discourse, pages 178?186, Sofia, Bulgaria.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In EMNLP-CoNLL.
O. Bojar, S. Cinkov?a, and J. Pt?a?cek. 2008. Towards English-to-Czech MT via Tectogrammatical Layer. The
Prague Bulletin of Mathematical Linguistics, 90:57?68.
Mathias Buch-Kromann. 2003. The Danish dependency treebank and the dtag treebank tool. In 2nd Workshop on
Treebanks and Linguistic Theories (TLT), Sweden, pages 217?220.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Genabith, and Andy Way. 2008. Wide-
coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics,
34(1):81?124.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: A Library for Support Vector Machines. Software available
at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL 2009): Shared Task, pages 49?54, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
R. Curry. 1961. Some logical aspects of grammatical structure. In R. Jakobson, editor, Structure of Language and
Its Mathematical Aspects, pages 56?68. American Mathematical Society, Providence, RI.
Ph. de Groote. 2001. Towards abstract categorial grammar. In Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL).
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato. 2002. The FrameNet database and software tools. In
Proceedings of the Third International Conference on Language Resources and Evaluation, volume IV, Las
Palmas. LREC, LREC.
A. Gesmundo, J. Henderson, P. Merlo, and I.Titov. 2009. Latent variable model of synchronous syntactic-semantic
parsing for multiple languages. In CoNLL 2009 Shared Task., Conf. on Computational Natural Language
Learning, pages 37?42, Boulder, Colorado, USA.
Jan Haji?c, Jarmila Panevov?a, Eva Haji?cov?a, Petr Sgall, Petr Pajas, Jan
?
St?ep?anek, Ji?r?? Havelka, Marie Mikulov?a,
and Zden
?
k
?
Zabokrtsk?y. 2006. Prague Dependency Treebank 2.0. Linguistic Data Consortium, Philadelphia.
Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s M`arquez,
Adam Meyers, Joakim Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue,
and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared
Task, pages 1?18.
J. Hockenmeier. 2003. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics (ACL), pages 359?366, Sapporo, Japan.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and Dan Flickinger. 2012. Who did what to whom? a contrastive
study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop, pages
2?11, Jeju, Republic of Korea, July. Association for Computational Linguistics.
1411
R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for english. In J. Nivre, H.-J.
Kaalep, K. Muischnek, and M. Koit, editors, Proceedings of NODALIDA 2007, pages 105?112, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic?semantic analysis with PropBank and
NomBank. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages
183?187, Manchester, United Kingdom.
H. Kamp and U. Reyle. 1993. From Discourse to Logic. Kluwer Academic Publishers, Dordrecht, NL.
R.T. Kasper and W.C. Rounds. 1986. A logical semantics for feature structures. In Proceedings of the 24th annual
meeting on Association for Computational Linguistics, pages 257?266.
V?aclav Klime?s. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, UFAL,
MFF UK, Prague, Czech Republic.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic
dependencies. Transactions of the Association for Computational Linguistics, pages 219?230.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92?97.
Igor Mel??cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.
Igor Mel??cuk. 1996. Lexical functions: A tool for the description of lexical relations in the lexicon. In L. Wanner,
editor, Lexical functions in lexicography and natural language processing, pages 37?102. Benjamins Academic
Publishers, Amsterdam.
Igor Mel??cuk. 2013. Semantics: From meaning to text, Volume 2. Benjamins Academic Publishers, Amsterdam.
Simon Mille, Alicia Burga, Gabriela Ferraro, and Leo Wanner. 2012. How does the granularity of an annotation
scheme influence dependency parsing performance? In Conference on Computational Linguistics, COLING
2012.
Simon Mille, Alicia Burga, and Leo Wanner. 2013. AnCora-UPF: A Multi-Level Annotation of Spanish . In
Proceedings of the Second International Conference on Dependency Linguistics (DEPLING 2013).
Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development
and Feature Forest Model. Ph.D. thesis, University of Tokyo.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 915?932.
Stephan Oepen. 2002. Collaborative Language Engineering: A Case Study in Efficient Grammar-based Process-
ing. Stanford Univ Center for the Study.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank. Computational Linguistics,
31:71?106.
Owen Rambow and Aravind Joshi. 1997. A formal look at dependency grammar and phrase structure grammars,
with special consideration of word-order phenomena. In L. Wanner, editor, Recent Trends in Meaning-Text
Theory, pages 167?190. Benjamins Academic Publishers, Amsterdam.
W.C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4(3):257?287.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s M`arquez, and Joakim Nivre. 2008. The conll 2008
shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Language Learning, pages 159?177.
M. Taul?e, M. Ant`onia Mart??, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for Catalan and
Spanish. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08), Marrakech,
Morocco, may. European Language Resources Association (ELRA).
J.W. Thatcher. 1970. Generalized sequential machine maps. Journal of Computer and System Sciences, 4(4):339?
367.
1412
F. Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural
Language Processing Pacific Rim Symposium, pages 398?403, Beijing, China.
Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Pro-
ceedings of ACL-08: HLT, pages 888?896, Columbus, Ohio, June. Association for Computational Linguistics.
1413
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 928?939, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generating Non-Projective Word Order in Statistical Linearization
Bernd Bohnet Anders Bjo?rkelund Jonas Kuhn Wolfgang Seeker Sina Zarrie?
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
{bohnetbd,anders,jonas,seeker,zarriesa}@ims.uni-stuttgart.de
Abstract
We propose a technique to generate non-
projective word orders in an efficient statisti-
cal linearization system. Our approach pre-
dicts liftings of edges in an unordered syntac-
tic tree by means of a classifier, and uses a
projective algorithm for tree linearization. We
obtain statistically significant improvements
on six typologically different languages: En-
glish, German, Dutch, Danish, Hungarian, and
Czech.
1 Introduction
There is a growing interest in language-independent
data-driven approaches to natural language genera-
tion (NLG). An important subtask of NLG is sur-
face realization, which was recently addressed in the
2011 Shared Task on Surface Realisation (Belz et
al., 2011). Here, the input is a linguistic representa-
tion, such as a syntactic dependency tree lacking all
precedence information, and the task is to determine
a natural, coherent linearization of the words.
The standard data-driven approach is to traverse
the dependency tree deciding locally at each node on
the relative order of the head and its children. The
shared task results have proven this approach to be
both effective and efficient when applied to English.
It is what federal support should try to achieve
SBJ
ROOT OBJ
NMOD SBJ
PRD
VC OPRD IM
Figure 1: A non-projective example from the CoNLL
2009 Shared Task data set for parsing (Hajic? et al 2009).
However, the approach can only generate pro-
jective word orders (which can be drawn with-
out any crossing edges). Figure 1 shows a non-
projective word order: the edge connecting the ex-
tracted wh-pronoun with its head crosses another
edge. Once what has been ordered relative to
achieve, there are no ways of inserting intervening
material. In this case, only ungrammatical lineariza-
tions can be produced from the unordered input tree:
(1) a. *It is federal support should try to what achieve
b. *It is federal support should try to achieve what
c. *It is try to achieve what federal support should
Although rather infrequent in English, non-
projective word orders are quite common in lan-
guages with a less restrictive word order. In these
languages, it is often possible to find a grammati-
cally correct projective linearization for a given in-
put tree, but discourse coherence, information struc-
ture, and stylistic factors will often make speak-
ers prefer some non-projective word order.1 Figure
2 shows an object fronting example from German
where the edge between the subject and the finite
verb crosses the edge between the object and the full
verb. Various other constructions, such as extraposi-
tion of (relative) clauses or scrambling, can lead to
non-projectivity. In languages where word order is
driven to an even larger degree by information struc-
ture, such as Czech and Hungarian, non-projectivity
can likewise result from various ordering decisions.
These phenomena have been studied extensively in
1A categorization of non-projective edges in the Prague
Dependency Treebank (Bo?hmova? et al 2000) is presented in
Hajic?ova? et al(2004).
928
the linguistic literature, and for certain languages,
work on rule-based generation has addressed certain
aspects of the problem.
Das Mandat will er zuru?ckgeben .
the.ACC mandate.ACC want.3SG he.NOM return.INF .
NK
OA#?
SB OC
?
?He wants to return the mandate.?
Figure 2: German object fronting with complex verb in-
troducing a non-projective edge.
In this paper, we aim for a general data-driven ap-
proach that can deal with various causes for non-
projectivity and will work for typologically dif-
ferent languages. Our technique is inspired by
work in data-driven multilingual parsing, where
non-projectivity has received considerable attention.
In pseudo-projective parsing (Kahane et al 1998;
Nivre and Nilsson, 2005), the parsing algorithm is
restricted to projective structures, but the issue is
side-stepped by converting non-projective structures
to projective ones prior to training and application,
and then restoring the original structure afterwards.
Similarly, we split the linearization task in two
stages: initially, the input tree is modified by lifting
certain edges in such a way that new orderings be-
come possible even under a projectivity constraint;
the second stage is the original, projective lineariza-
tion step. In parsing, projectivization is a determin-
istic process that lifts edges based on the linear or-
der of a sentence. Since the linear order is exactly
what we aim to produce, this deterministic conver-
sion cannot be applied before linearization. There-
fore, we use a statistical classifier as our initial lift-
ing component. This classifier has to be trained on
suitable data, and it is an empirical question whether
the projective linearizer can take advantage of this
preceding lifting step.
We present experiments on six languages with
varying degrees of non-projective structures: En-
glish, German, Dutch, Danish, Czech and Hungar-
ian, which exhibit substantially different word order
properties. Our approach achieves significant im-
provements on all six languages. On German, we
also report results of a pilot human evaluation.
2 Related Work
An important concept for tree linearization are word
order domains (Reape, 1989). The domains are bags
of words (constituents) that are not allowed to be dis-
continuous. A straightforward method to obtain the
word order domains from dependency trees and to
order the words in the tree is to use each word and
its children as domain and then to order the domains
and contained words recursively. As outlined in the
introduction, the direct mapping of syntactic trees to
domains does not provide the possibility to obtain
all possible correct word orders.
Linearization systems can be roughly distin-
guished as either rule-based or statistical systems. In
the 2011 Shared Task on Surface Realisation (Belz
et al 2011), the top performing systems were all
statistical dependency realizers (Bohnet et al 2011;
Guo et al 2011; Stent, 2011).
Grammar-based approaches map dependency
structures or phrase structures to a tree that repre-
sents the linear precedence. These approaches are
mostly able to generate non-projective word orders.
Early work was nearly exclusively applied to phrase
structure grammars (e.g. (Kathol and Pollard, 1995;
Rambow and Joshi, 1994; Langkilde and Knight,
1998)). Concerning dependency-based frameworks,
Bro?ker (1998) used the concept of word order do-
mains to separate surface realization from linear
precedence trees. Similarly, Duchier and Debus-
mann (2001) differentiate Immediate Dominance
trees (ID-trees) from Linear Precedence trees (LP-
trees). Gerdes and Kahane (2001) apply a hierarchi-
cal topological model for generating German word
order. Bohnet (2004) employs graph grammars to
map between dependency trees and linear prece-
dence trees represented as hierarchical graphs. In the
frameworks of HPSG, LFG, and CCG, a grammar-
based generator produces word order candidates that
might be non-projective, and a ranker is used to se-
lect the best surface realization (Cahill et al 2007;
White and Rajkumar, 2009).
Statistical methods for linearization have recently
become more popular (Langkilde and Knight, 1998;
Ringger et al 2004; Filippova and Strube, 2009;
Wan et al 2009; He et al 2009; Bohnet et al 2010;
Guo et al 2011). They typically work by travers-
ing the syntactic structure either bottom-up (Filip-
929
pova and Strube, 2007; Bohnet et al 2010) or top-
down (Guo et al 2011; Bohnet et al 2011). These
linearizers are mostly applied to English and do not
deal with non-projective word orders. An excep-
tion is Filippova and Strube (2007), who contribute
a study on the treatment of preverbal and postver-
bal constituents for German focusing on constituent
order at the sentence level. The work most similar
to ours is that of Gamon et al(2002). They use
machine-learning techniques to lift edges in a pre-
processing step to a surface realizer. Their objec-
tive is the same as ours: by lifting, they avoid cross-
ing edges. However, contrary to our work, they use
phrase-structure syntax and focus on a limited num-
ber of cases of crossing branches in German only.
3 Lifting Dependency Edges
In this section, we describe the first of the two stages
in our approach, namely the classifier that lifts edges
in dependency trees. The classifier we aim to train
is meant to predict liftings on a given unordered de-
pendency tree, yielding a tree that, with a perfect lin-
earization, would not have any non-projective edges.
3.1 Preliminaries
The dependency trees we consider are of the form
displayed in Figure 1. More precisely, all words (or
nodes) form a rooted tree, where every node has ex-
actly one parent (or head). Edges point from head
to dependent, denoted in the text by h? d, where h
is the head and d the dependent. All nodes directly
or transitively depend on an artificial root node (de-
picted in Figure 1 as the incoming edge to is).
We say that a node a dominates a node d if a is
an ancestor of d. An edge h ? d is projective iff
h dominates all nodes in the linear span between h
and d. Otherwise it is non-projective. Moreover,
a dependency tree is projective iff all its edges are
projective. Otherwise it is non-projective.
A lifting of an edge h? d (or simply of the node
d) is an operation that replaces h ? d with g ? d,
given that there exists an edge g ? h in the tree, and
undefined otherwise (i.e. the dependent d is reat-
tached to the head of its head).2 When the lifting
2The undefined case occurs only when d depends on the
root, and hence cannot be lifted further; but these edges are by
definition projective, since the root dominates the entire tree.
operation is applied n successive times to the same
node, we say the node was lifted n steps.
3.2 Training
During training we make use of the projectivization
algorithm described by Nivre and Nilsson (2005).
It works by iteratively lifting the shortest non-
projective edges until the tree is projective. Here,
shortest edge refers to the edge spanning over the
fewest number of words. Since finding the shortest
edge relies on the linear order, instead of lifting the
shortest edge, we lift non-projective edges ordered
by depth in the tree, starting with the deepest nested
edge. A lifted version of the tree from Figure 1 is
shown in Figure 3. The edge of what has been lifted
three steps (the original edge is dotted), and the tree
is no longer non-projective.
It is what federal support should try to achieve
SBJ
ROOT OBJ
OBJNMOD SBJ
PRD
VC OPRD IM
Figure 3: The sentence from Figure 1, where what has
been assigned a new head (solid line). The original edge
is dotted.
We model the edge lifting problem as a multi-
class classification problem and consider nodes one
at a time and ask the question ?How far should this
edge be lifted??, where classes correspond to lifting
0, 1, 2, ..., n steps. To create training instances we
use the projectivization algorithm mentioned above.
We traverse the nodes of the tree sorted by depth.
For multiple nodes at the same depth, ties are broken
by linear order, i.e. for multiple nodes at the same
depth, the leftmost is visited first. When a node is
visited, we create a training instance out of it. Its
class is determined by the number of steps it would
be lifted by the projectivization algorithm given the
linear order (in most cases the class corresponds to
no lifting, since most edges are projective). As we
traverse the nodes, we also execute the liftings (if
any) and update the tree on the fly.
The training instances derived are used to train a
logistic regression classifier using the LIBLINEAR
package (Fan et al 2008). The features used for
the lifting classifier are described in Table 1. Since
we use linear classifiers, our feature set al con-
tains conjunctions of atomic features. The features
930
Atomic features
?x ? {w,wp, wgp, wch, ws, wun} morph(x), label(x), lemma(x), PoS(x)
?x ? {wgc, wne, wco} label(x), lemma(x), PoS(x)
Complex features
?x ? {w,wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x)
?x ? {wch, ws, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x)
?x ? {w,wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y)
?x ? {w,wp, wgp}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
?x ? {wch, ws, wun}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
Non-binary features
?x ? {w,wp, wgp} SUBTREESIZE(x), RELSUBTREESIZE(x)
Table 1: Features used for lifting. w refers to the word (dependent) in question. And with respect to w, wp is the
parent; wgp is the grandparent; wch are children; ws are siblings; wun are uncles (i.e. children of the grandparent,
excluding the parent); wgc are grandchildren; wne are nephews (i.e. grandchildren of the parent that are not children
of w); wco are cousins (i.e. grandchildren of the grandparent that are not w or siblings of w). The non-binary feature
functions refer to: SUBTREESIZE ? the absolute number of nodes below x, RELSUBTREESIZE ? the relative size of
the subtree rooted at x with respect to the whole tree.
involve the lemma, dependency edge label, part-of-
speech tag, and morphological features of the node
in question, and of several neighboring nodes in the
dependency tree. We also have a few non-binary fea-
tures that encode the size of the subtree headed by
the node and its ancestors.
We ran preliminary experiments to determine the
optimal architecture. First, other ways of modeling
the liftings are conceivable. To find new reattach-
ment points, Gamon et al(2002) propose two other
ways, both using a binary classifier: applying the
classifier to each node x along the path to the root
asking ?Should d be reattached to x??; or lifting one
step at a time and applying the classifier iteratively
until it says stop. They found that the latter outper-
formed the former. We tried this method, but found
that it was inferior to the multi-class model and more
frequently over- or underlifted.
Second, to avoid data sparseness for infrequent
lifting distances, we introduce a maximum number
of liftings. We found that a maximum of 3 gave the
best performance. In the pseudocode below, we re-
fer to this number as maxsteps.3 This means that we
are able to predict the correct lifting for most (but
not all) of the non-projective edges in our data sets
(cf. Table 3).
Third, as Nivre and Nilsson (2005) do for pars-
3During training, nodes that are lifted further than maxsteps
are assigned to the class corresponding to maxsteps. This ap-
proach worked better than ignoring the training instance or
treating it as a non-lifting (i.e. a lifting of 0 steps).
ing, we experimented with marking edges that were
lifted by indicating this on the edge labels. In the
case of parsing, this step is necessary in order to re-
verse the liftings in the parser output. In our case,
it could potentially be beneficial for both the lifting
classifier, and for the linearizer. However, we found
that marking liftings at best gave similar results as
not marking, so we kept the original labels without
marking.
3.3 Decoding
In the decoding stage, an unordered tree is given and
the goal is to lift edges that would be non-projective
with respect to the gold linear order. Similarly to
how training instances are derived, the decoding al-
gorithm traverses the tree bottom-up and visits every
node once. Ties between nodes at the same depth are
broken in an arbitrary but deterministic way. When
a node is visited, the classifier is applied and the cor-
responding lifting is executed. Pseudocode is given
in Algorithm 1.4
Different orderings of nodes at the same depth
can lead to different lifts. The reason is that lift-
ings are applied immediately and this influences the
features when subsequent nodes are considered. For
instance, consider two sibling nodes ni and nj . If
ni is visited before nj , and ni is lifted, this means
4The MIN function is used to guarantee that the edge is not
lifted beyond the root node of the tree. This does not happen
in practice though, since the feature set of the classifier include
features that implicitly encode the proximity to the root node.
931
that at the time we visit nj , ni is no longer a sibling
of nj , but rather an uncle. An obvious extension of
the decoding algorithm presented above is to apply
beam search. This allows us to consider nj both in
the context where ni has been lifted and when it has
not been lifted.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 foreach node ? N do
4 feats? EXTRACT-FEATURES(node, T )
5 steps? CLASSIFY(feats)
6 steps? MIN(steps,ROOT-DIST(node))
7 LIFT(node, T, steps)
8 return T
Algorithm 1: Greedy decoding for lifting.
Pseudocode for the beam search decoder is given
in Algorithm 2. The algorithm keeps an agenda of
trees to explore as each node is visited. For every
node, it clones the current tree and applies every pos-
sible lifting. Every tree also has an associated score,
which is the sum of the scores of each lifting so far.
The score of a lifting is defined to be the log proba-
bility returned from the logistic classifier. After ex-
ploring all trees in the agenda, the k-best new trees
from the beam are extracted and put back into the
agenda. When all nodes have been visited, the best
tree in the agenda is returned. For the experiments
the beam size (k in Algorithm 2) was set to 20.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 Tscore ? 0
4 Agenda? {T}
5 foreach node ? N do
6 Beam? ?
7 foreach tree ? Agenda do
8 feats? EXTRACT-FEATURES(node, tree)
9 m? MIN(maxsteps,ROOT-DIST(node))
10 foreach s ? 0 .. maxsteps do
11 t? CLONE(tree)
12 score? GET-LIFT-SCORE(feats, s)
13 tscore = tscore + score
14 LIFT(node, t, s)
15 Beam? Beam ? {t}
16 Agenda? EXTRACTKBEST(Beam, k)
17 return EXTRACTKBEST(Agenda, 1)
Algorithm 2: Beam decoding for lifting.
While beam search allows us to explore the search
space somewhat more thoroughly, a large number of
possibilities remain unaccounted for. Again, con-
sider the sibling nodes ni and nj when ni is visited
before nj . The beam allows us to consider nj both
when ni is lifted and when it is not. However, the
situation where nj is visited before ni is still never
considered. Ideally, all permutations of nodes at the
same depth should be explored before moving on.
Unfortunately this leads to a combinatorial explo-
sion of permutations, and exhaustive search is not
tractable. As an approximation, we create two or-
derings and run the beam search twice. The dif-
ference between the orderings is that in the second
one all ties are reversed. As this bibeam consistently
improved over the beam in Algorithm 2, we only
present these results in Section 5 (there denoted sim-
ply Beam).
4 Linearization
A linearizer searches for the optimal word order
given an unordered dependency tree, where the op-
timal word order is defined as the single reference
order of the dependency tree in the gold standard.
We employ a statistical linearizer that is trained on a
corpus of pairs consisting of unordered dependency
trees and their corresponding sentences. The lin-
earization method consists of the following steps:
Creating word order domains. In the first step,
we build the word order domains dh for all nodes
h ? y of a dependency tree y. A domain is defined
as a node and all of its direct dependents. For ex-
ample, the tree shown in Figure 3 has the following
domains: {it, be, should}, {what, support, should, try},
{federal, support}, {try, to}, {to, achieve}
If an edge was lifted before the linearization, the
lifted node will end up in the word order domain of
its new head rather than in the domain of its original
head. This way, the linearizer can deduce word or-
ders that would result in non-projective structures in
the non-lifted tree.
Ordering the words of the domains. In the sec-
ond step, the linearizer orders the words of each do-
main. The position of a subtree is determined by the
position of the head of the subtree in the enclosing
domain. Algorithm 3 shows the tree linearization
algorithm. In our implementation, the linearizer tra-
verses the tree either top-down or bottom-up.
932
1 // T is the dependency tree with lifted nodes
2 beam-size? 1000
3 for h ? T do
4 domainh? GET-DOMAIN(T ,h)
5 // initialize the beam with a empty word list
6 Agendah? ()
7 foreach w ? domainh do
8 // beam for extending word order lists
9 Beam? ()
10 foreach l ? Agendah do
11 // clone list l and append the word w
12 if w 6? l then
13 l? ? APPEND(l,m)
14 Beam? Beam ? l?
15 score[l?]? COMPUTE-SCORE(l?)
16 if | Beam | > beam-size then
17 SORT-LISTS-DESCENDING-TO-
SCORE(Beam,score)
18 Agendah? SUBLIST(0,beam-size,Beam)
19 else
20 Agendah? Beam
21 foreach l ? Beam do
22 SCOREg[l]? SCORE[l] +
GLOBAL-SCORE(l)
23 Agendah? Beam
24 return Beam
Algorithm 3: Dependency Tree Linearization.
The linearization algorithm initializes the word
order beam (agendah) with an empty order () (line
6). It then iterates over the words of a domain (lines
7-20). In the first iteration, the algorithm clones and
extends the empty word order list () by each word
of the sentence (line 12-15). If the beam (beam)
exceeds a certain size (beam-size), it is sorted by
score and pruned to maximum beam size (beam-
size) (lines 16-20). The following example illus-
trates the extensions of the beam for the top domain
shown in Figure 3.
Iter. agendabe
0: ()
1: ((it) (be) (should))
2: ((it be) (it should) (be it) (be should) ...)
The beam enables us to apply features that encode
information about the first tokens and the last token,
which are important for generating, e.g. the word
order of questions, i. e. if the last token is a question
mark then the sentence should probably be a ques-
tion (cf. feature set shown in Table 2). Furthermore,
the beam enables us to generate alternative lineariza-
tions. For this, the algorithm iterates over the alter-
native word orders of the domains in order to as-
semble different word orders on the sentence level.5
Finally, when traversing the tree bottom-up, the al-
gorithm has to use the different orders of the already
ordered subtrees as context, which also requires a
search over alternative word orders of the domains.
Training of the Linearizer. We use MIRA
(Crammer et al 2006) for the training of the lin-
earizer. The classifier provides a score that we use to
rank the alternative word orders. Algorithm 3 calls
two functions to compute the score: compute-score
(line 15) for features based on pairs of words and tri-
grams and compute-global-score for features based
on word patterns of a domain. Table 2 shows the
feature set for the two functions. In the case that the
linearization of a word order domain is incorrect the
algorithm updates its weight vector w. The follow-
ing equation shows the update function of the weight
vector:
w = w + ?h(?(dh, T, xg)? ?(dh, T, xp))
We update the weight vector w by adding the dif-
ference of the feature vector representation of the
correct linearization xg and the wrongly predicted
linearization xp, multiplied by ? . ? is the passive-
aggressive update factor as defined below. The suf-
fered lossh is ?(dh, T, xp)? ?(dh, T, xg).
? = lossh||?(dh,T,xg)??(dh,T,xp)||2
Creating the word order of a sentence. The lin-
earizer traverses the tree either top-down or bottom-
up and assembles the results in the surface order.
The bottom-up linearization algorithm can take into
account features drawn from the already ordered
subtrees while the top-down algorithm can employ
as context only the unordered nodes. However, the
bottom-up algorithm additionally has to carry out a
search over the alternative linearization of the sub-
domains, as different orders of the subdomain pro-
vide different context features. This leads to a higher
linearization time. We implemented both, but could
only find a rather small accuracy difference. In the
following, we therefore present results only for the
top-down method.
5The beam also makes it possible to employ a generative
language model to rerank alternative linearizations.
933
Atomic features
For nodes w ?
domainh
lemma(w), label(w), PoS(w), num-children(w), num-grandchildren(w), label-children(w),
PoS-children(w)
For domain
domainh
head(w1,w2), head(w1,w2,w3), label(head), PoS(head), PoS(w1), label(wn), label(wn?1),
contains-?(domainh)
Complex features
For bigrams
(w1, w2) ?
domainh
feat2: label(w1)+label(w2), label(w1)+lemma(w2), lemma(w1)+lemma(w2), PoSw1+PoSw2
feat3: label(w1)+num-children(w2)+num-children(w1),PoS-child(w1)+label(w1)+label(w2)
feat4: label(w1)+label(w2)+lemma(w2)+PoS(w1), label(w1)+label(w2)+PoS(head)+head(w1,w2)
feat5: label(w1)+label(w2)+PoS(head)+label(head)+head(w1,w2)
For trigrams
(w1, w2, w3) ?
domainh
feat3: lemma(w1)+lemma(w2)+lemma(w3)
feat4: PoS(w1)+PoS(w2)+PoS(w3)+head(w1,w2,w3)
feat5: label(w1)+label(w2)+label(w3)+PoS(w1)+head(w1,w2,w3)
For sentence s feat6: label(w1)+label(wn?1)+lemma(head)+lemma(w1)+lemma(wn?1)
feat7: PoS(w1)+PoS(w2)+PoS(w3)+PoS(wn?1)+PoS(wn?2)+PoS(wn?3)+contains-?(s)
Table 2: Exemplified features used for scoring linearizations of a word order domain (see Algorithm 3). Atomic
features which represent properties of a node or a domain are conjoined into feature vectors of different lengths.
Linearizations are scored based on bigrams, trigrams, and global sentence-level features.
5 Experiments
We conduct experiments on six European languages
with varying degrees of word order restrictions:
While English word order is very restrictive, Czech
and Hungarian exhibit few word order constraints.
Danish, Dutch, and German (so-called V2, i. e.
verb-second, languages) show a relatively free word
order that is however more restrictive than in Hun-
garian or Czech. The English and the Czech data
are from the CoNLL 2009 Shared Task data sets
(Hajic? et al 2009). The Danish and the Dutch data
are from the CoNLL 2006 Shared Task data sets
(Buchholz and Marsi, 2006). For Hungarian, we use
the Hungarian Dependency Treebank (Vincze et al
2010), and for German, we use a dependency con-
version by Seeker and Kuhn (2012).
# sent?s np sent?s np edges np ? 3 lifts
English 39,279 7.63 % 0.39% 98.39%
German 36,000 28.71% 2.34% 94.98%
Dutch 13,349 36.44% 5.42% 99.80%
Danish 5,190 15.62 % 1.00% 96.72%
Hungarian 61,034 15.81% 1.45% 99.82%
Czech 38,727 22.42% 1.86% 99.84%
Table 3: Size of training sets, percentage of non-
projective (np) sentences and edges, percentage of np
edges covered by 3 lifting steps.
Table 3 shows the sizes of the training corpora
and the percentage of non-projective sentences and
edges in the data. Note that the data sets for Dan-
ish and Dutch are quite small. English has the least
percentage of non-projective edges. Czech, Ger-
man, and Dutch show the highest percentage of non-
projective edges. The last column shows the per-
centage of non-projective edges that can be made
projective by at most 3 lifting steps.
5.1 Setup
In our two-stage approach, we first train the lifting
classifier. The results for this classifier are reported
in Section 5.2.
Second, we train the linearizer on the output of
the lifting classifier. To assess the impact of the
lifting technique on linearization, we built four sys-
tems on each language: (a) a linearizer trained on
the original, non-lifted dependency structures (No-
lift), two trained on the automatically lifted edges
(comparing (b) the beam and (c) greedy decoding),
(d) one trained on the oracle, i. e. gold-lifted struc-
tures, which gives us an upper bound for the lifting
technique. The linearization results are reported in
Section 5.3.
In this two-stage setup, we have the problem that,
if we re-apply the lifting classifier on the data it was
trained on, the input for the linearizer will be better
during training than during testing. To provide real-
istic training data for the linearizer, we make a 10-
fold cross-validation of the lifting classifier on the
training set, and use this as training data for the lin-
earizer. The lifting classifier that is applied to the
test set is trained on the entire training set.
934
5.2 Lifting results
To evaluate the performance of the lifting classifier,
we present precision, recall, and F-measure results
for each language. We also compute the percentage
of sentences that were handled perfectly by the lift-
ing classifier. Precision and recall are defined the
usual way in terms of true positives, false positives,
and false negatives, where true positives are edges
that should be lifted and were lifted correctly; false
positives are edges that should not be lifted but were
and edges that should be lifted and were lifted, but
were reattached in the wrong place; false negatives
are edges that should be lifted but were not.
The performance of both the greedy decoder and
the bibeam decoder are shown in Table 4. The scores
are taken on the cross-validation on the training set,
as this provides more reliable figures. The scores
are micro-averaged, i.e. all folds are concatenated
and compared to the entire training set.
Although the major evaluation of the lifting is
given by the performance of the linearizer, Table 4
gives us some clues about the lifting. We see that
precision is generally much higher than recall. We
believe this is related to the fact that some phenom-
ena encoded by non-projective edges are more sys-
tematic and thus easier to learn than others (e. g. wh-
extraction vs. relative clause extraposition). We also
find that beam search consistently yields modest in-
creases in performance.
Greedy Beam
P R F1 Perfect P R F1 Perfect
Eng 77.31 50.45 61.05 95.76 78.85 50.63 61.66 95.83
Ger 72.33 63.59 67.68 81.91 72.05 64.41 68.02 81.97
Dut 76.66 74.89 75.77 79.28 78.07 76.49 77.27 80.34
Dan 85.90 58.55 69.64 92.76 85.90 58.55 69.64 92.74
Hun 72.60 61.61 66.66 88.46 73.06 64.77 68.67 88.73
Cze 77.79 55.00 64.44 86.28 77.31 55.68 64.74 86.33
Table 4: Precision, recall, F-measure and perfect projec-
tivization results for the lifting classifier.
5.3 Linearization Results and Discussion
We evaluate the linearizer with standard metrics: n-
gram overlap measures (BLEU, NIST), edit distance
(Edit), and the proportion of exactly linearized sen-
tences (Exact). As a means to assess the impact of
lifting more precisely, we propose the word-based
measure Exactlift which only looks at the words
with an incoming lifted edge. The Exactlift score
then corresponds to the percentage of these words
that has been realized in the exact same position as
in the original sentence.
LangLift BLEU NIST Edit Exact Exactlift Nlift
EngNolift 0.911 15.09 0.922 56.40 0.00 0
EngGreedy 0.914 15.10 0.923 57.27 59.87 152
EngBeam 0.916 15.11 0.925 58.48 62.82 156
EngOracle 0.923 15.14 0.928 60.73 70.42 240
GerNolift 0.792 13.76 0.844 40.4 0.00 0
GerGreedy 0.811 13.86 0.864 42.9 55.21 480
GerBeam 0.813 13.86 0.866 43.3 56.47 487
GerOracle 0.843 13.97 0.889 49.95 72.87 634
DutNolift 0.743 11.31 0.796 30.05 0.00 0
DutGreedy 0.784 11.47 0.797 37.56 41.02 256
DutBeam 0.778 11.46 0.8 37.05 47.45 255
DutOracle 0.825 11.63 0.848 44.82 70.55 292
DanNolift 0.836 11.80 0.886 44.41 0.00 0
DanGreedy 0.852 11.88 0.90 45.96 67.65 34
DanBeam 0.858 11.90 0.90 48.76 67.65 34
DanOracle 0.865 11.92 0.90 50.93 74.42 43
HunNolift 0.755 15.70 0.839 30.71 0.00 0
HunGreedy 0.764 15.71 0.844 31.98 41,81 1,538
HunBeam 0.764 15.71 0.844 31.98 41.37 1,581
HunOracle 0.777 15.79 0.849 34.30 57.53 1,933
CzeNolift 0.693 14.32 0.789 25.14 0.00 0
CzeGreedy 0.711 14.45 0.797 26.85 42.04 923
CzeBeam 0.712 14.45 0.795 26.37 41.34 941
CzeOracle 0.729 14.52 0.806 28.79 53.12 1,282
Table 5: Performance of linearizers using different lift-
ings, Exactlift is the exact match for words with an in-
coming lifted edge, Nlift is the total number of lifted
edges.
The results are presented in Table 5. On each
language, the predicted liftings significantly im-
prove on the non-lifted baseline (except the greedy
decoding in English).6 The differences between
the beam and the greedy decoding are not signif-
icant. The scores on the oracle liftings suggest
that the impact of lifting on linearization is heav-
ily language-dependent: It is highest on the V2-
languages, and somewhat smaller on English, Hun-
garian, and Czech. This is not surprising since the
V2-languages (especially German and Dutch) have
the highest proportion of non-projective edges and
sentences (see Table 3). On the other hand, En-
glish has a very small number of non-projective
edges, such that the BLEU score (which captures
the n-gram level) reflects the improvement by only
6We used a t-test, with ? = 0.01.
935
a small increase. However, note that, on the sen-
tence level, the percentage of exactly regenerated
sentences increases by 2 points which suggests that
a non-negligible amount of non-projective sentences
can now be generated more fluently.
50556065707580
Eng
Ger
Dut
Dan
Hun
Cze
langua
ge
accuracy
periph
ery left right
Figure 4: Accuracy for the linearization of the sentences?
left and right periphery, the bars are upper and lower
bounds of the non-lifted and the gold-lifted baseline.
The Exactlift measure refines this picture: The
linearization of the non-projective edges is relatively
exact in English, and much less precise in Hungarian
and Czech where Exactlift is even low on the gold-
lifted edges. The linearization quality is also quite
moderate on Dutch where the lifting leads to con-
siderable improvements. These tendencies point to
some important underlying distinctions in the non-
projective word order phenomena over which we
are generalizing: In certain cases, the linearization
seems to systematically follow from the fact that the
edge has to be lifted, such as wh-extraction in En-
glish (Figure 1). In other cases, the non-projective
linearization is just an alternative to other grammati-
cal, but maybe less appropriate, realizations, such as
the prefield-occupation in German (Figure 2).
Since a lot of non-projective word orders affect
the clause-initial or clause-final position, we evalu-
ate the exact match of the left periphery (first three
words) and the right periphery (last three words) of
the sentence. The accuracies obtained are plotted
in Figure 4, where the lower and upper bars corre-
spond to the lower and upper bound from the non-
lifted and the gold-lifted baseline. It clearly emerges
from this figure that the range of improvements ob-
tainable from lifting is closely tied to the general
linearization quality, and also to word order prop-
erties of the languages. Thus, the range of sentences
affected by the lifting is clearly largest for the V2-
languages. The accuracies are high, but the ranges
are small for English, whereas the accuracies are low
and the ranges quite small for Czech and Hungarian.
System BLEU NIST
(Bohnet et al 2011) (ranked 1st) 0.896 13.93
(Guo et al 2011) (ranked 2nd) 0.862 13.68
Baseline-Non-Lifted + LM 0.896 13.94
Beam-Lifted + LM 0.901 13.96
Table 6: Results on the development set of the 2011
Shared Task on Surface Realisation data, (the test set was
not officially released).
We also evaluated our linearizer on the data of
2011 Shared Task on Surface Realisation, which is
based on the English CoNLL 2009 data (like our
previous evaluations) but excludes information on
morphological realization. For training and evalu-
ation, we used the exact set up of the Shared Task.
For the morphological realization, we used the mor-
phological realizer of Bohnet et al(2010) that pre-
dicts the word form using shortest edit scripts. For
the language model (LM), we use a 5-gram model
with Kneser-Ney (Kneser and Ney, 1995) smoothing
derived from 11 million sentences of the Wikipedia.
In Table 6, we compare our two linearizers (with
and without lifting) to the two top systems of the
2011 Shared Task on Surface Realisation, (Bohnet et
al., 2011) and (Guo et al 2011). Without the lifting,
our system reaches a score comparable to the top-
ranked system in the Shared Task. With the lifting,
we get a small7 but statistically significant improve-
ment in BLEU such that our system reaches a higher
score than the top ranked systems. This shows that
the improvements we obtain from the lifting carry
over to more complex generation tasks which in-
clude morphological realization.
5.4 Human Evaluation
We have carried out a pilot human evaluation on the
German data in order to see whether human judges
prefer word orders obtained from the lifting-based
7Remember that English has the least percentage of non-
projective edges in our data sets, which are however important
to linearize correctly (see Figure 1).
936
linearizer. In particular, we wanted to check whether
the lifting-based linearizer produces more natural
word orders for sentences that had a non-projective
tree in the corpus, and maybe less natural word or-
ders on originally projective sentences. Therefore,
we divided the evaluated items into originally pro-
jective and non-projective sentences.
We asked four annotators to judge 60 sentence
pairs comparing the lifting-based against the non-
lifted linearizer using the toolkit by Kow and Belz
(2012). All annotators are students, two of them
have a background in linguistics. The items were
randomly sampled from the subset of the develop-
ment set containing those sentences where the lin-
earizers produced different surface realizations. The
items are subdivided into 30 originally projective
and 30 originally non-projective sentences.
For each item, we presented the original context
sentence from the corpus and the pair of automat-
ically produced linearizations for the current sen-
tence. The annotators had to decide on two crite-
ria: (i) which sentence do they prefer? (ii) how flu-
ent is that sentence? In both cases, we used con-
tinuous sliders as rating tools, since humans seem
to prefer them (Belz and Kow, 2011). For the first
criterion, the slider positions were mapped to values
from -50 (preference for left sentence) to 50 (pref-
erence for right sentence). If the slider position is
zero, both sentences are equally preferred. For the
second criterion, the slider positions were mapped
to values from 0 (absolutely broken sentence) to 100
(perfectly fluent sentence).
Sentences Scores Equal Lifted Non-lifted
All
% selected 44.58% 35.0% 20.42%
Fluency 56.14 75.77 72.78
Preference 0 34.75 31.06
Non-
Proj.
% selected 29.63% 58.33% 12.04%
Fluency 43.06 76.27 68.85
Preference 0 37.52 24.46
Proj.
% selected 56.82% 15.91% 27.27%
Fluency 61.72 74.29 74.19
Preference 0 26.43 33.44
Table 7: Results from human evaluation.
Table 7 presents the results averaged over all sen-
tences, as well as for the subsets of non-projective
and projective sentences. We report the percentage
of items where the judges selected both, the lifted, or
non-lifted sentence, alongside with the average flu-
ency score (0-100) and preference strength (0-50).
On the entire set of items, the judges selected both
sentences in almost half of the cases. However, on
the subset of non-projective sentences, the lifted ver-
sion is clearly preferred and has a higher average
fluency and preference strength. The percentage of
zero preference items is much higher on the sub-
set of projective sentences. Moreover, the average
fluency of the zero preference items is remarkably
higher on the projective sentences than on the non-
projective subset. We conclude that humans have
a strong preference for lifting-based linearizations
on non-projective sentences. We attribute the low
fluency score on the non-projective zero preference
items to cases where the linearizer did not get a cor-
rect lifting or could not linearize the lifting correctly
such that the lifted and the non-lifted version were
not appropriate. On the other hand, incorrect lift-
ings on projective sentences do not necessarily seem
to result in deprecated linearizations, which leads to
the high percentage of zero preferences with a good
average fluency on this subset.
6 Conclusion
We have presented a novel technique to linearize
sentences for a range of languages that exhibit non-
projective word order. Our approach deals with non-
projectivity by lifting edges in an unordered input
tree which can then be linearized by a standard pro-
jective linearization algorithm.
We obtain significant improvements for the
lifting-based linearization on English, German,
Dutch, Danish, Czech and Hungarian, and show that
lifting has the largest impact on the V2-languages.
In a human evaluation carried out on German we
also show that human judges clearly prefer lifting-
based linearizations on originally non-projective
sentences, and, on the other hand, that incorrect lift-
ings do not necessarily result in bad realizations of
the sentence.
Acknowledgments
This work was funded by the Deutsche Forschungs-
gemeinschaft (DFG) via the SFB 732 ?Incremental
Specification in Context?. We would also like to
thank Anna Ha?tty and our four annotators for their
contribution to the human evaluation.
937
References
A. Belz and E. Kow. 2011. Discrete vs. Continuous Rat-
ing Scales for Language Evaluation in NLP. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230?235, Portland, Oregon, USA,
June. Association for Computational Linguistics.
A. Belz, M. White, D. Espinosa, D. Hogan, E. Kow, and
A. Stent. 2011. The First Surface Realisation Shared
Task: Overview and Evaluation Results. In ENLG?11.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2000.
The Prague Dependency Treebank: A Three-level an-
notation scenario. In A. Abeille?, editor, Treebanks:
Building and using syntactically annotated corpora.,
chapter 1, pages 103?127. Kluwer Academic Publish-
ers, Amsterdam.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Coling 2010,
pages 98?106.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011.
<stumaba>: From deep representation to surface. In
Proceedings of the Generation Challenges Session at
the 13th European Workshop on NLG, pages 232?235,
Nancy, France.
B. Bohnet. 2004. A Graph Grammar Approach to Map
Between Dependency Trees and Topological Models.
In IJCNLP, pages 636?645.
N. Bro?ker. 1998. Separating Surface Order and Syntactic
Relations in a Dependency Grammar. In COLING-
ACL 98.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 149?164, Morristown,
NJ, USA. Association for Computational Linguistics.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic real-
isation ranking for a free word order language. ENLG
?07, pages 17?24.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Jour-
nal of Machine Learning Research, 7:551?585.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In Proceedings of the ACL.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?1874.
K. Filippova and M. Strube. 2007. Generating con-
stituent order in german clauses. In ACL, pages 320?
327.
K. Filippova and M. Strube. 2009. Tree linearization in
English: improving language model based approaches.
In NAACL, pages 225?228, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
M. Gamon, E. Ringger, R. Moore, S. Corston-Olivier,
and Z. Zhang. 2002. Extraposition: A case study in
German sentence realization. In Proceedings of Col-
ing 2002. Association for Computational Linguistics.
K. Gerdes and S. Kahane. 2001. Word order in german:
A formal dependency grammar using a topological hi-
erarchy. In Proceedings of the ACL.
Y. Guo, D. Hogan, and J. van Genabith. 2011. Dcu at
generation challenges 2011 surface realisation track.
In Proceedings of the Generation Challenges Session
at the 13th European Workshop on NLG, pages 227?
229.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.-
A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. Stepa?nek, P. Strana?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and Semantic dependencies in multiple lan-
guages. In Proceedings of the 13th CoNLL Shared
Task, pages 1?18, Boulder, Colorado.
E. Hajic?ova?, J. Havelka, P. Sgall, K. Vesela?, and D. Ze-
man. 2004. Issues of projectivity in the prague de-
pendency treebank. Prague Bulletin of Mathematical
Linguistics, 81.
W. He, H. Wang, Y. Guo, and T. Liu. 2009. Dependency
Based Chinese Sentence Realization. In Proceedings
of the ACL and of the IJCNLP, pages 809?816.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In COLING-ACL, pages 646?
652.
A. Kathol and C. Pollard. 1995. Extraposition via com-
plex domain formation. In Meeting of the Association
for Computational Linguistics, pages 174?180.
R. Kneser and H. Ney. 1995. In In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, pages 181?184.
E. Kow and A. Belz. 2012. LGRT-Eval: A Toolkit for
Creating Online Language Evaluation Experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 99?106, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
O. Rambow and A. K. Joshi. 1994. A formal look at
dependency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
938
In Leo Wanner, editor, Current Issues in Meaning-Text
Theory. Pinter, London, UK.
M. Reape. 1989. A logical treatment of semi-free word
order and bounded discontinuous constituency. In
Proceedings of the EACL, EACL ?89, pages 103?110.
E. Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In COLING ?04, pages 673?
679.
W. Seeker and J. Kuhn. 2012. Making Ellipses Explicit
in Dependency Conversion for a German Treebank. In
Proceedings of LREC 2012, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
A. Stent. 2011. Att-0: Submission to generation chal-
lenges 2011 surface realization shared task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 230?231, Nancy, France, September. As-
sociation for Computational Linguistics.
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian Dependency Tree-
bank. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC 2010), pages 1855?1862, Valletta, Malta.
S. Wan, M. Dras, R. Dale, and C. Paris. 2009. Improving
grammaticality in statistical sentence generation: In-
troducing a dependency spanning tree algorithm with
an argument satisfaction model. In EACL, pages 852?
860.
M. White and R. Rajkumar. 2009. Perceptron reranking
for CCG realization. In EMNLP?09, pages 410?419,
Singapore, August.
939
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1455?1465, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Transition-Based System for Joint Part-of-Speech Tagging
and Labeled Non-Projective Dependency Parsing
Bernd Bohnet
Institute for Natural Language Processing
University Stuttgart
bohnet@ims.uni-stuttgart.de
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Most current dependency parsers presuppose
that input words have been morphologically
disambiguated using a part-of-speech tagger
before parsing begins. We present a transition-
based system for joint part-of-speech tagging
and labeled dependency parsing with non-
projective trees. Experimental evaluation on
Chinese, Czech, English and German shows
consistent improvements in both tagging and
parsing accuracy when compared to a pipeline
system, which lead to improved state-of-the-
art results for all languages.
1 Introduction
Dependency-based syntactic parsing has been the
focus of intense research efforts during the last
decade, and the state of the art today is represent-
ed by globally normalized discriminative models
that are induced using structured learning. Graph-
based models parameterize the parsing problem by
the structure of the dependency graph and normally
use dynamic programming for inference (McDonald
et al 2005; McDonald and Pereira, 2006; Carreras,
2007; Koo and Collins, 2010; Bohnet, 2010), but
other inference methods have been explored espe-
cially for non-projective parsing (Riedel and Clarke,
2006; Smith and Eisner, 2008; Martins et al 2009;
Martins et al 2010; Koo et al 2010). Transition-
based models parameterize the problem by elemen-
tary parsing actions and typically use incremental
beam search (Titov and Henderson, 2007; Zhang
and Clark, 2008; Zhang and Clark, 2011). Despite
notable differences in model structure, graph-based
and transition-based parsers both give state-of-the-
art accuracy with proper feature selection and opti-
mization (Koo and Collins, 2010; Zhang and Nivre,
2011; Bohnet, 2011).
It is noteworthy, however, that almost all depen-
dency parsers presuppose that the words of an input
sentence have been morphologically disambiguated
using (at least) a part-of-speech tagger. This is in s-
tark contrast to the best parsers based on PCFG mod-
els, such as the Brown parser (Charniak and John-
son, 2005) and the Berkeley parser (Petrov et al
2006; Petrov and Klein, 2007), which not only can
perform their own part-of-speech tagging but nor-
mally give better parsing accuracy when they are al-
lowed to do so. This suggests that joint models for
tagging and parsing might improve accuracy also in
the case of dependency parsing.
It has been argued that joint morphological and
syntactic disambiguation is especially important for
richly inflected languages, where there is consid-
erable interaction between morphology and syntax
such that neither can be fully disambiguated with-
out considering the other. Thus, Lee et al(2011)
show that a discriminative model for joint morpho-
logical disambiguation and dependency parsing out-
performs a pipeline model in experiments on Latin,
Ancient Greek, Czech and Hungarian. However, Li
et al(2011) and Hatori et al(2011) report improve-
ments with a joint model also for Chinese, which
is not a richly inflected language but is nevertheless
rich in part-of-speech ambiguities.
In this paper, we present a transition-based mod-
el for joint part-of-speech tagging and labeled de-
pendency parsing with non-projective trees. Exper-
1455
iments show that joint modeling improves both tag-
ging and parsing accuracy, leading to state-of-the-art
accuracy for richly inflected languages like Czech
and German as well as more configurational lan-
guages like Chinese and English. To our knowledge,
this is the first joint system that performs labeled de-
pendency parsing. It is also the first joint system that
achieves state-of-the-art accuracy for non-projective
dependency parsing.
2 Transition-Based Tagging and Parsing
Transition-based dependency parsing was pioneered
by Yamada and Matsumoto (2003) and Nivre et al
(2004), who used classifiers trained to predict indi-
vidual actions of a deterministic shift-reduce parser.
Recent research has shown that better accuracy can
be achieved by using beam search and optimizing
models on the entire sequence of decisions needed
to parse a sentence instead of single actions (Zhang
and Clark, 2008; Huang and Sagae, 2010; Zhang
and Clark, 2011; Zhang and Nivre, 2011; Bohnet,
2011). In addition, a number of different transition
systems have been proposed, in particular for deal-
ing with non-projective dependencies, which were
beyond the scope of early systems (Attardi, 2006;
Nivre, 2007; Nivre, 2009; Titov et al 2009).
In this section, we start by defining a transition
system for joint tagging and parsing based on the
non-projective transition system proposed in Nivre
(2009). We then show how to perform beam search
and structured online learning with this model, and
conclude by discussing feature representations.
2.1 Transition System
Given a set P of part-of-speech tags and a set D
of dependency labels, a tagged dependency tree for
a sentence x = w1, . . . , wn is a directed tree T =
(Vx, A) with labeling functions pi and ? such that:
1. Vx = {0, 1, . . . , n} is a set of nodes,
2. A ? Vx ? Vx is a set of arcs,
3. pi : Vx ? P is a labeling function for nodes,
4. ? : A? D is a labeling function for arcs,
5. 0 is the root of the tree.
The set Vx of nodes is the set of positive integers up
to and including n, each corresponding to the lin-
ear position of a word in the sentence, plus an extra
artificial root node 0. The set A of arcs is a set of
pairs (i, j), where i is the head node and j is the
dependent node. The functions pi and ? assign a u-
nique part-of-speech label to each node/word and a
unique dependency label to each arc, respectively.
This notion of dependency tree differs from the s-
tandard definition only by including part-of-speech
labels as well as dependency labels (Ku?bler et al
2009).
Following Nivre (2008), we define a transition
system for dependency parsing as a quadruple S =
(C, T, cs, Ct), where
1. C is a set of configurations,
2. T is a set of transitions, each of which is a (par-
tial) function t : C ? C,
3. cs is an initialization function, mapping a sen-
tence x to a configuration c ? C,
4. Ct ? C is a set of terminal configurations.
A transition sequence for a sentence x in S is a
sequence of configuration-transition pairs C0,m =
[(c0, t0), (c1, t1), . . . , (cm, tm)] where c0 = cs(x),
tm(cm) ? Ct and ti(ci) = ci+1 (0 ? i < m).1
In this paper, we take the set C of configurations
to be the set of all 5-tuples c = (?, B,A, pi, ?) such
that ? (the stack) and B (the buffer) are disjoin-
t sublists of the nodes Vx of some sentence x, A
is a set of dependency arcs over Vx, and pi and ?
are labeling functions as defined above. We take the
initial configuration for a sentence x = w1, . . . , wn
to be cs(x) = ([0], [1, . . . , n], { },?,?), where ?
is the function that is undefined for all arguments,
and we take the set Ct of terminal configurations
to be the set of all configurations of the form c =
([0], [ ], A, pi, ?) (for anyA, pi and ?). The tagged de-
pendency tree defined for x by c = (?, B,A, pi, ?)
is the tree (Vx, A) with labeling functions pi and ?,
which we write TREE(x, c).
The set T of transitions is shown in Figure 1. The
LEFT-ARCd and RIGHT-ARCd transitions both add
an arc (with dependency label d) between the two
nodes on top of the stack and replaces these nodes
by the head node of the new arc (which is the right-
most node for LEFT-ARCd and the leftmost node for
RIGHT-ARCd). The SHIFTp transition extracts the
1This definition of transition sequence differs from that of
Nivre (2008) but is equivalent and suits our presentation better.
1456
Transition Condition
LEFT-ARCd ([?|i, j], B,A, pi, ?)? ([?|j], B,A?{(j, i)}, pi, ?[(j, i)? d]) i 6= 0
RIGHT-ARCd ([?|i, j], B,A, pi, ?)? ([?|i], B,A?{(i, j)}, pi, ?[(i, j)? d])
SHIFTp (?, [i|?], A, pi, ?)? ([?|i], ?, A, pi[i? p], ?)
SWAP ([?|i, j], ?, A, pi, ?)? ([?|j], [i|?], A, pi, ?) 0 < i < j
Figure 1: Transitions for joint tagging and dependency parsing extending the system of Nivre (2009). The stack ? is
represented as a list with its head to the right (and tail ?) and the buffer B as a list with its head to the left (and tail ?).
The notation f [a? b] is used to denote the function that is exactly like f except that it maps a to b.
first node in the buffer, pushes it onto the stack and
labels it with the part-of-speech tag p. The SWAP
transition extracts the second topmost node from the
stack and moves it back to the buffer, subject to the
condition that the two top nodes on the stack are still
in the order given by the sentence.
Except for the addition of a tag parameter p to
the SHIFT transition, this is equivalent to the sys-
tem described in Nivre (2009), which thanks to the
SWAP transition can handle arbitrary non-projective
trees. The soundness and completeness results giv-
en in that paper trivially carry over to the new sys-
tem. The only thing to note is that, before a terminal
configuration can be reached, every word has to be
pushed onto the stack in a SHIFTp transition, which
ensures that every node/word in the output tree will
be tagged.
2.2 Inference and Learning
While early transition-based parsers generally used
greedy best-first inference and locally trained clas-
sifiers, recent work has shown that higher accura-
cy can be obtained using beam search and global
structure learning to mitigate error propagation. In
particular, it seems that the globally learned models
can exploit a much richer feature space than local-
ly trained classifiers, as shown by Zhang and Nivre
(2011). Since joint tagging and parsing increases the
size of the search space and is likely to require nov-
el features, we use beam search in combination with
structured perceptron learning.
The beam search algorithm used to derive the best
parse y for a sentence x is outlined in Figure 2. In
addition to the sentence x, it takes as input a weight
vector w corresponding to a linear model for scor-
ing transitions out of configurations and two prun-
PARSE(x,w, b1, b2)
1 h0.c? cs(x)
2 h0.s? 0.0
3 h0.f? {0.0}dim(w)
4 BEAM ? [h0]
5 while ?h ? BEAM : h.c 6? Ct
6 TMP ? [ ]
7 foreach h ? BEAM
8 foreach t ? T : PERMISSIBLE(h.c, t)
9 h.f? h.f + f(h.c, t)
10 h.s? h.s+ f(h.c, t) ? w
11 h.c? t(h.c)
12 TMP ? INSERT(h, TMP)
13 BEAM? PRUNE(TMP, b1, b2)
14 h? TOP(BEAM)
15 y ? TREE(x, h.c)
16 return y
Figure 2: Beam search algorithm for joint tagging and de-
pendency parsing of input sentence x with weight vector
w and beam parameters b1 and b2. The symbols h.c, h.s
and h.f denote, respectively, the configuration, score and
feature representation of a hypothesis h; h.c.A denotes
the arc set of h.c.
ing parameters b1 and b2. A parse hypothesis h is
represented by a configuration h.c, a score h.s and
a feature vector h.f for the transition sequence up to
h.c. Hypotheses are stored in the list BEAM, which
is sorted by descending scores and initialized to hold
the hypothesis h0 corresponding to the initial con-
figuration cs(x) with score 0.0 and all features set
to 0.0 (lines 1?4). In the main loop (lines 5?13), a
set of new hypotheses is derived and stored in the
list TMP, which is finally pruned and assigned as
the new value of BEAM. The main loop terminates
1457
when all hypotheses in BEAM contain terminal con-
figurations, and the dependency tree extracted from
the top scoring hypothesis is returned (lines 14?16).
The set of new hypotheses is created in two nest-
ed loops (lines 7?12), where every hypothesis h in
BEAM is updated using every permissible transition
t for the configuration h.c. The feature representa-
tion of the new hypothesis is obtained by adding the
feature vector f(t, h.c) for the current configuration-
transition pair to the feature vector of the old hy-
pothesis (line 9). Similarly, the score of the new
hypothesis is the sum of the score f(t, h.c) ? w of
the current configuration-transition pair and the s-
core of the old hypothesis (line 10). The feature
representation/score of a complete parse y for x
with transition sequence C0,m is thus the sum of the
feature representations/scores of the configuration-
transition pairs in C0,m:
f(x, y) =
?
(c,t)?C0,m
f(c, t)
s(x, y) =
?
(c,t)?C0,m
f(c, t) ? w
Finally, the configuration of the new hypothesis is
obtained by evaluating t(h.c) (line 11). The new hy-
pothesis is then inserted into TMP in score-sorted or-
der (line 12).
The pruning parameters b1 and b2 determine the
number of hypotheses allowed in the beam and at
the same time control the tradeoff between syntactic
and morphological ambiguity. First, we extract the
b1 highest scoring hypotheses with distinct depen-
dency trees. Then we extract the b2 highest scoring
remaining hypotheses, which will typically be tag-
ging variants of dependency trees that are already in
the beam. In this way, we prevent the beam from
getting filled up with too many tagging variants of
the same dependency tree, which was found to be
harmful in preliminary experiments.
One final thing to note about the inference algo-
rithm is that the notion of permissibility for a transi-
tion t out of a configuration c can be used to capture
not only formal constraints on transitions ? such as
the fact that it is impossible to perform a SHIFTp
transition with an empty buffer or illegal to perform
a LEFT-ARCd transition with the special root node
on top of the stack ? but also to filter out unlike-
ly dependency labels or tags. Thus, in the experi-
ments later on, we will typically constrain the parser
so that SHIFTp is permissible only if p is one of the
k best part-of-speech tags with a score no more than
? below the score of the 1-best tag, as determined by
a preprocessing tagger. We also filter out instances
of LEFT-ARCd and RIGHT-ARCd, where d does not
occur in the training data for the predicted part-of-
speech tag combination of the head and dependent.
This procedure leads to a significant speed up.
In order to learn a weight vector w from a training
set {(xj , yj)}Tj=1 of sentences with their tagged de-
pendency trees, we use a variant of the structured
perceptron, introduced by Collins (2002), which
makes N iterations over the training data and up-
dates the weight vector for every sentence xj where
the highest scoring parse y? is different from yj .
More precisely, we use the passive-aggressive up-
date of Crammer et al(2006):
wi+1 = wi + ?(f(xj , yj)? f(xj , y?))
where
? =
f(xj , yj)? f(xj , y?)
||f(xj , yj)? f(xj , y?)||2
We also use the early update strategy found benefi-
cial for parsing in several previous studies (Collins
and Roark, 2004; Zhang and Clark, 2008; Huang
and Sagae, 2010), which means that, during learn-
ing, we terminate the beam search as soon as the
hypothesis corresponding to the gold parse yj falls
out of the beam and update with respect to the par-
tial transition sequence constructed up to that point.
Finally, we use the standard technique of averaging
over all weight vectors, as originally proposed by
Collins (2002).
2.3 Feature Representations
As already noted, the feature representation f(x, y)
of an input sentence x with parse y decomposes into
feature representations f(c, t) for the transitions t(c)
needed to derive y from cs(x). Features may refer to
any aspect of a configuration, as encoded in the stack
?, the bufferB, the arc setA and the labelings pi and
?. In addition, we assume that each word w in the
input is assigned up to k candidate part-of-speech
tags pii(w) with corresponding scores s(pii(w)).
1458
Features involving word prefixes and suffixes
pii(B0)p2(B0), pii(B0)s2(B0), pii(B0)p1(B0)p1(?0)
pii(?0)p1(?0)p1(?1), pii(?0)s1(?0)s1(?0)
pii(?0)p2(?0)s3(?1),pii(?0)s3(?0)p2(?1)
pii(?0)w(B0)s1(?0), pii(?0)w(B0)s2(?0)
Features involving tag score differences and ranks
pii(B0)[s(pi1(B0))? s(pii(B0))]
pii(B0)pii(?0)[s(pi1(B0))? s(pii(B0))] i
pii(B0)[s(pi1(B0))? s(pii(B0))]pi(?0)
w(B0)[s(pi1(B0))? s(pii(B0))]pi(?0)
Figure 3: Specialized feature templates for tagging. We
use ?i and Bi to denote the ith token in the stack ? and
bufferB, respectively, with indexing starting at 0, and we
use the following functors to extract properties of a token:
pii() = ith best tag; s(pii()) = score of ith best tag; pi() =
finally predicted tag; w() = word form; pi() = word prefix
of i characters; si() = word suffix of i characters. Score
differences are binned in discrete steps of 0.05.
The bulk of features used in our system are tak-
en from Zhang and Nivre (2011), although with t-
wo important differences. First of all, like Hatori et
al. (2011), we have omitted all features that presup-
pose an arc-eager parsing order, since our transition
system defines an arc-standard order. Secondly, any
feature that refers to the part-of-speech tag of a word
w in the buffer B will in our system refer to the top-
scoring tag pi1(w), rather than the finally predicted
tag. By contrast, for a word in the stack ?, part-of-
speech features refer to the tag pi(w) chosen when
shifting w onto the stack (which may or may not be
the same as pi1(w)).
In addition to the standard features for transition-
based dependency parsing, we have added features
specifically to improve the tagging step in the joint
model. The templates for these features, which are
specified in Figure 3, all involve the ith best tag as-
signed to the first word of the buffer B (the next
word to be shifted in a SHIFTp transition) in combi-
nation with neighboring words, word prefixes, word
suffixes, score differences and tag rank.
Finally, in some experiments, we make use of two
additional feature sets, which we call graph features
(G) and cluster features (C), respectively. Graph fea-
tures are defined over the factors of a graph-based
dependency parser, which was shown to improve the
accuracy of a transition-based parser by Zhang and
Clark (2008). However, while their features were
limited to certain first- and second-order factors, we
use features over second- and third-order factors as
found in the parsers of Bohnet and Kuhn (2012).
These features are scored as soon as the factors are
completed, using a technique that is similar to what
Hatori et al(2011) call delayed features, although
they use it for part-of-speech tags in the lookahead
while we use it for subgraphs of the dependency tree.
Cluster features, finally, are features over word clus-
ters, as first used by Koo et al(2008), which replace
part-of-speech tag features.2
We use a hash kernel to map features to weights.
It has been observed that most of the computing time
in feature-rich parsers is spent retrieving the index
of each feature in the weight vector (Bohnet, 2010).
This is usually done via a hash table, but significan-
t speedups can be achieved by using a hash kernel,
which simply replaces table lookup by a hash func-
tion (Bloom, 1970; Shi et al 2009; Bohnet, 2010).
The price to pay for these speedups is that there may
be collisions, so that different features are mapped to
the same index, but this is often compensated by the
fact that the lower time and memory requirements of
the hash kernel enables the use of negative features,
that is, features that are never seen in the training set
but occur in erroneous hypotheses at training time
and can therefore be helpful also at inference time.
As a result, the hash kernel often improves accuracy
as well as efficiency compared to traditional tech-
niques that only make use of features that occur in
gold standard parses (Bohnet, 2010).
3 Experiments
We have evaluated the model for joint tagging and
dependency parsing on four typologically diverse
languages: Chinese, Czech, English, and German.
3.1 Setup
Most of the experiments use the CoNLL 2009 da-
ta sets with the training, development and test s-
plit used in the Shared Task (Hajic? et al 2009),
but for better comparison with previous work we
also report results for the standard benchmark data
sets for Chinese and English. For Chinese, this is
the Penn Chinese Treebank 5.1 (CTB5), converted
2For replicability, a complete description of all features can
be found at http://stp.lingfil.uu.se/?nivre/exp/emnlp12.html.
1459
Parser Chinese Czech English German
k ? TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS
1 0.0 73.85 76.12 80.01 92.78 82.36 82.65 88.03 93.26 85.82 87.17 90.41 97.32 85.08 86.60 89.17 97.24
2 0.1 74.39 76.52 80.41 93.37 82.74 83.01 88.34 99.39 86.43 87.79 91.02 97.49 86.12 87.22 89.69 97.85
3 0.1 74.47 76.63 80.50 93.38 82.76 82.97 88.33 99.40 86.40 87.78 90.99 97.43 86.03 87.27 89.60 97.74
3 0.2 74.35 76.48 80.38 93.43 82.85 83.11 88.44 99.32 86.35 87.79 91.01 97.52 86.24 87.37 89.72 97.90
3 0.3 74.18 76.33 80.28 93.48 82.78 83.05 88.38 99.33 85.94 87.57 90.87 96.97 86.35 87.46 89.86 97.90
3 0.4 86.14 87.23 89.66 97.79
Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and
the score threshold ?. Beam parameters fixed at b1 = 40, b2 = 4.
with the head-finding rules and conversion tools of
Zhang and Clark (2008), and with the same split as
in Zhang and Clark (2008) and Li et al(2011).3 For
English, this is the WSJ section of the Penn Tree-
bank, converted with the head-finding rules of Ya-
mada and Matsumoto (2003) and the labeling rules
of Nivre (2006).4
In order to assign k-best part-of-speech tags and
scores to words in the training set, we used a per-
ceptron tagger with 10-fold jack-knifing. The same
type of tagger was trained on the entire training set
in order to supply tags for the development and test
sets. The feature set of the tagger was optimized
for English and German and provides state-of-the-
art accuracy for these two languages. The 1-best
tagging accuracy for section 23 of the Penn Tree-
bank is 97.28, which is on a par with Toutanova et
al. (2003). For German, we obtain a tagging accura-
cy of 97.24, which is close to the 97.39 achieved by
the RF-Tagger (Schmid and Laws, 2008), which to
our knowledge is the best tagger for German.5 The
results are not directly comparable to the RF-Tagger
as it was evaluated on a different part of the Tiger
Treebank and trained on a larger part of the Tree-
bank. We could not use the larger training set as
it contains the test set of the CoNLL 2009 data that
we use to evaluate the joint model. For Czech, the 1-
best tagging accuracy is 99.11 and for Chinese 92.65
on the CoNLL 2009 test set.
We trained parsers with 25 iterations and report
3Training: 001?815, 1001?1136. Development: 886?931,
1148?1151. Test: 816?885, 1137?1147.
4Training: 02-21. Development: 24. Test: 23.
5The RF-Tagger can take advantage of an additional lexicon
and then reaches 97.97. The lexicon supplies entries for addi-
tional words that are not found in the training corpus and addi-
tional tags for words that do occur in the training data (Schmid
and Laws, 2008).
results for the model obtained after the last iteration.
For cluster features, available only for English and
German, we used standard Brown clusters based on
the English and German Gigaword Corpus. We re-
stricted the vocabulary to words that occur at least
10 times, used 800 clusters, and took cluster prefix-
es of length 6 to define features.
We report the following evaluation metrics: part-
of-speech accuracy (POS), unlabeled attachment s-
core (UAS), labeled attachment score (LAS), and
tagged labeled attachment score (TLAS). TLAS is
a new metric defined as the percentage of words that
are assigned the correct part-of-speech tag, the cor-
rect head and the correct dependency label. In line
with previous work, punctuation is included in the
evaluation for the CoNLL data sets but excluded for
the two benchmark data sets.
3.2 Results
Table 1 presents results on the development sets of
the CoNLL 2009 shared task with varying values
of the two tag parameters k (number of candidates)
and ? (maximum score difference to 1-best tag) and
beam parameters fixed at b1 = 40 and b2 = 4. We
use the combined TLAS score on the development
set to select the optimal settings for each language.
For Chinese, we obtain the best result with 3 tags
and a threshold of 0.1.6 Compared to the baseline,
we observe a POS improvement of 0.60 and a LAS
improvement of 0.51. For Czech, we get the best T-
LAS with k = 3 and ? = 0.2, where POS improves
by 0.06 and LAS by 0.46. For English, the best set-
ting is k = 2 and ? = 0.1 with a POS improvement of
0.17 and a LAS improvement of 0.62. For German,
finally, we see the greatest improvement with k = 3
6While tagging accuracy (POS) increases with larger values
of ?, TLAS decreases because of a drop in LAS.
1460
Parser Chinese Czech English German
TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS TLAS LAS UAS POS
Gesmundo et al(2009) 76.11 92.37 80.38 99.33 88.79 97.48 87.28 95.46
Bohnet (2010) 76.99 92.37 80.96 99.33 90.33 97.48 88.06 95.46
Baseline (k = 1), b1 = 40 73.66 76.55 80.77 92.65 82.07 82.44 87.83 99.11 87.89 89.19 91.74 97.57 86.11 87.78 90.13 97.24
Best dev setting, b1 = 40 74.72 77.00 81.18 93.06 82.56 82.70 88.07 99.32 88.26 89.54 92.06 97.77 86.91 88.23 90.43 97.63
Adding G, b1 = 80 75.84 78.51 82.52 93.19 83.38 83.73 88.82 99.33 88.92 90.20 92.60 97.77 87.86 89.05 91.16 97.78
Adding G+C, b1 = 80 89.22 90.60 92.87 97.84 88.31 89.38 91.37 98.05
Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1?2: Top performing systems in the shared
CoNLL Shared Task 2009; Gesmundo et al(2009) was placed first in the shared task; for Bohnet (2010), we include
the updated scores later reported due to some improvements of the parser. Rows 3?4: Baseline (k = 1) and best settings
for k and ? on development set. Rows 5?6: Wider beam (b1 = 80) and added graph features (G) and cluster features
(C). Second beam parameter b2 fixed at 4 in all cases.
and ? = 0.3, where POS improves by 0.66 and LAS
by 0.86.
Table 2 shows the results on the CoNLL 2009 test
sets. For all languages except English, we obtain
state-of-the-art results already with b1 = 40 (row 4),
and for all languages both tagging and parsing ac-
curacy improve compared to the baseline (row 3).
The improvement in TLAS is statistically significant
with p < 0.01 for all languages (paired t-test). Row
5 shows the scores with a beam of 80 and the addi-
tional graph features. Here the LAS scores for Chi-
nese, Czech and German are higher than the best re-
sults on the CoNLL 2009 data sets, and the score
for English is highly competitive. For Chinese, we
achieve 78.51 LAS, which is 1.5 percentage points
higher than the reference score, while the POS s-
core is 0.54 higher than our baseline. For Czech, we
get 83.73 LAS, which is by far the highest score re-
ported for this data set, together with state-of-the-art
POS accuracy. For German, we obtain 89.05 LAS
and 97.78 POS, which in both cases is substantially
better than in the CoNLL shared task. We believe
it is also the highest POS accuracy ever reported for
a tagger/parser trained only on the Tiger Treebank.
Row 6, finally, presents results with added cluster
features for English and German, which results in
additional improvements in all metrics.
Table 3 gives the results for the Penn Treebank
converted with the head-finding rules of Yamada and
Matsumoto (2003) and the labeling rules of Nivre
(2006). We use k = 3 and ? = 0.4, which gave the
best results on the development set. The UAS im-
proves by 0.24 when we do joint tagging and pars-
ing. The POS accuracy improves slightly by 0.12
Parser TLAS UAS LAS POS
McDonald et al(2005) 90.9
McDonald and Pereira (2006) 91.5
Zhang and Clark (2008) 92.1
Huang and Sagae (2010) 92.1
Koo and Collins (2010) 93.04
Zhang and Nivre (2011) 92.9
Martins et al(2010) 93.26
Koo et al(2008) ? 93.16
Carreras et al(2008) ? 93.5
Suzuki et al(2009) ? 93.79
Baseline (k = 1), b1 = 40 89.42 92.79 91.71 97.28
Best dev setting, b1 = 40 89.75 93.03 91.92 97.40
Adding G, b1 = 40 90.12 93.38 92.44 97.33
Adding G+C, b1 = 80 ? 90.41 93.67 92.68 97.42
Table 3: Accuracy scores for WSJ-PTB converted with
head rules of Yamada and Matsumoto (2003) and labeling
rules of Nivre (2006). Best dev setting: k = 3, ? = 0.4.
Results marked with ? use additional information sources
and are not directly comparable to the others.
but to a lower degree than for the English CoNL-
L data where we observed an improvement of 0.20.
Nonetheless, the improvement in the joint TLAS s-
core is statistically significant at p < 0.01 (paired
t-test). Our joint tagger and dependency parser with
graph features gives very competitive unlabeled de-
pendency scores for English with 93.38 UAS. To
the best of our knowledge, this is the highest score
reported for a (transition-based) dependency parser
that does not use additional information sources. By
adding cluster features and widening the beam to
b1 = 80, we achieve 93.67 UAS. We also obtain
a POS accuracy of 97.42, which is on a par with the
best results obtained using semi-supervised taggers
1461
Parser TLAS UAS LAS POS
MSTParser1 75.56 93.51
MSTParser2 77.73 93.51
Li et al(2011) 3rd-order 80.60 92.80
Li et al(2011) 2nd-order 80.55 93.08
Hatori et al(2011) HS 79.60 94.01
Hatori et al(2011) ZN 81.20 93.94
Baseline (k = 1), b1 = 40 61.95 80.33 76.79 92.81
Best dev setting, b1 = 40 62.54 80.59 77.06 93.11
Adding G, b1 = 80 63.20 81.42 77.91 93.24
Table 4: Accuracy scores for Penn Chinese Treebank
converted with the head rules of Zhang and Clark (2008).
Best dev setting: k = 3, ? = 0.1. MSTParser results from
Li et al(2011). UAS scores from Li et al(2011) and Ha-
tori et al(2011) recalculated from the separate accuracy
scores for root words and non-root words reported in the
original papers.
(S?gaard, 2011).
Table 4 shows the results for the Chinese Penn
Treebank CTB 5.1 together with related work. In ex-
periments with the development set, we could con-
firm the results from the Chinese CoNLL data set
and obtained the best results with the same settings
(k = 3, ? = 0.1). With b1 = 40, UAS improves by
0.25 and POS by 0.30, and the TLAS improvement
is again highly significant (p < 0.01, paired t-test).
We get the highest UAS, 81.42, with a beam of 80
and added graph features, in which case POS accu-
racy increases from 92.81 to 93.24. Since our tagger
was not optimized for Chinese, we have lower base-
line results for the tagger than both Li et al(2011)
and Hatori et al(2011) but still manage to achieve
the highest reported UAS.
The speed of the joint tagger and dependency
parser is quite reasonable with about 0.4 seconds
per sentence on the WSJ-PTB test set, given that we
perform tagging and labeled parsing with a beam of
80 while incorporating the features of a third-order
graph-based model. Experiments were performed
on a computer with an Intel i7-3960X CPU (3.3 GHz
and 6 cores). These performance values are prelim-
inary since we are still working on the speed-up of
the parser.
3.3 Analysis
In order to better understand the benefits of the joint
model, we performed an error analysis for German
Confusion Baseline Joint
Freq F-score Freq F-score
VVINF? VVFIN 28
91.1
2
97.7
VVINF? VVPP|ADJ*|NN 5 9
VVFIN? VVINF 43
94.2
5
98.5
VVFIN? VVPP 20 2
VAINF? VAFIN 10 99.1 1 99.9
NE? NN 184
90.7
128
92.4NE? ADJ*|ADV|FM 24 18
NE? XY 12 21
NN? NE 85
97.5
67
98.1
NN? ADJ*|XY|ADV|VV* 39 29
PRELS? ART 13
92.9
5
95.4
PRELS? PWS 0 2
Table 5: Selected entries from the confusion matrix for
parts of speech in German with F-scores for the left-hand-
side category. ADJ* (ADJD or ADJA) = adjective; ADV
= adverb; ART = determiner; APPR = preposition; NE
= proper noun; NN = common noun; PRELS = relative
pronoun; VVFIN = finite verb; VVINF = non-finite verb;
VAFIN = finite auxiliary verb; VAINF = non-finite auxil-
iary verb; VVPP = participle; XY = not a word. We use
?* to denote the set of categories with ? as a prefix.
and English, where we compared the baseline and
the joint model with respect to F-scores for individu-
al part-of-speech categories and dependency labels.
For the part-of-speech categories, we found an im-
provement across the board for both languages, with
no category having a significant decrease in F-score,
but we also found some interesting patterns for cat-
egories that improved more than the average.
Table 5 shows selected entries from the confu-
sion matrix for German, where we see substantial
improvements for finite and non-finite verbs, which
are often morphologically ambiguous but which can
be disambiguated using syntactic context. We al-
so see improved accuracies for common and proper
nouns, which are both capitalized in standard Ger-
man orthography and therefore often mistagged, and
for relative pronouns, which are less often confused
for determiners in the joint model.
Table 6 gives a similar snapshot for English, and
we again see improvements for verb categories that
are often morphologically ambiguous, such as past
participles, which can be confused for past tense
verbs, and present tense verbs in third person sin-
gular, which can be confused for nouns. We also
see some improvement for the singular noun catego-
1462
Confusion Baseline Joint
Freq F-score Freq F-score
VBN? VBD 40
90.5
19
91.5
VBN? JJ|VB|VBP|NN 13 18
VBZ? NN|NNS 19
97.8
13
98.3
VBZ? POS|JJ|RB 6 6
NN? VBG|VB|VBN|VBD 72
96.8
58
97.2NN? JJ|JJR 79 69
NN? NN*|RB|IN|DT 58 57
RB? IN 126
92.4
93
92.9
RB? JJ*|RP|NN*|RBR|UH 86 89
Table 6: Selected entries from the confusion matrix for
parts of speech in English with F-scores for the left-hand-
side category. DT = determiner; IN = preposition or sub-
ordinating conjunction; JJ = adjective; JJR = compara-
tive adjective; NN = singular or mass noun; NNS = plural
noun; POS = possessive clitic; RB = adverb; RBR = com-
parative adverb; RP = particle; UH = interjection; VB =
base form verb; VBD = past tense verb; VBG = gerund or
present participle; VBN = past participle; VBP = present
tense verb, not 3rd person singular; VBZ = present tense
verb, 3rd person singular. We use ?* to denote the set of
categories with ? as a prefix.
ry and for adverbs, which are less often confused for
prepositions or subordinating conjunctions thanks to
the syntactic information in the joint model.
For dependency labels, it is hard to extract any
striking patterns and it seems that we mainly see an
improvement in overall parsing accuracy thanks to
less severe tagging errors. However, it is worth ob-
serving that, for both English and German, we see
significant F-score improvements for the core gram-
matical functions subject (91.3? 92.1 for German,
95.6 ? 96.1 for English) and object (86.9 ? 87.9
for German, 90.2? 91.9 for English).
4 Related Work
Our work is most closely related to Lee et al(2011),
Li et al(2011) and Hatori et al(2011), who al-
l present discriminative models for joint tagging and
dependency parsing. However, all three models only
perform unlabeled parsing, while our model incor-
porates dependency labels into the parsing process.
Whereas Lee et al(2011) and Li et al(2011) take
a graph-based approach to dependency parsing, Ha-
tori et al(2011) use a transition-based model similar
to ours but limited to projective dependency trees.
Both Li et al(2011) and Hatori et al(2011) only
evaluate their model on Chinese, and of these only
Hatori et al(2011) report consistent improvements
in both tagging and parsing accuracy. Like our sys-
tem, the parser of Lee et al(2011) can handle non-
projective trees and experimental results are present-
ed for four languages, but their graph-based model
is relatively simple and the baselines therefore well
below the state of the art. We are thus the first to
show consistent improvements in both tagging and
(labeled) parsing accuracy across typologically di-
verse languages at the state-of-the-art level. More-
over, the capacity to handle non-projective depen-
dencies, which is crucial to attain good performance
on Czech and German, does not seem to hurt per-
formance on English and Chinese, where the bench-
mark sets contain only projective trees.
The use of beam search in transition-based depen-
dency parsing in order to mitigate the problem of
error propagation was first proposed by Johansson
and Nugues (2006), although they still used a local-
ly trained model. Globally normalized models were
first explored by Titov and Henderson (2007), who
were also the first to use a parameterized SHIFT tran-
sition like the one found in both Hatori et al(2011)
and our own work, although Titov and Henderson
(2007) used it to define a generative model by pa-
rameterizing the SHIFT transition by an input word.
Zhang and Clark (2008) was the first to combine
beam search with a globally normalized discrimi-
native model, using structured perceptron learning
and the early update strategy of Collins and Roark
(2004), and also explored the addition of graph-
based features to a transition-based parser. This
approach was further pursued in Zhang and Clark
(2011) and was used by Zhang and Nivre (2011) to
achieve state-of-the-art results in dependency pars-
ing for both Chinese and English through the ad-
dition of rich non-local features. Huang and Sagae
(2010) combined structured perceptron learning and
beam search with the use of a graph-structured stack
to allow ambiguity packing in the beam, a technique
that was reused by Hatori et al(2011).
Finally, as noted in the introduction, although
joint tagging and parsing is rare in dependency pars-
ing, most state-of-the-art parsers based on PCFG
models naturally incorporate part-of-speech tagging
and usually achieve better parsing accuracy (albeit
not always tagging accuracy) with a joint model than
1463
with a pipeline approach (Collins, 1997; Charniak,
2000; Charniak and Johnson, 2005; Petrov et al
2006). Models that in addition incorporate mor-
phological analysis and segmentation have been ex-
plored by Tsarfaty (2006), Cohen and Smith (2007),
and Goldberg and Tsarfaty (2008) with special ref-
erence to Hebrew parsing.
5 Conclusion
We have presented the first system for joint part-
of-speech tagging and labeled dependency parsing
with non-projective dependency trees. Evaluation
on four languages shows consistent improvements
in both tagging and parsing accuracy over a pipeline
system with state-of-the-art results across the board.
The error analysis reveals improvements in tagging
accuracy for syntactically central categories, mainly
verbs, with improvement in syntactic accuracy for
core grammatical functions as a result. In future
work we intend to explore joint models that incorpo-
rate not only basic part-of-speech tags but also more
fine-grained morphological features.
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of CoNLL, pages 166?170.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13:422?426.
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds ? a graph-based completion model for
transition-based parsers. In Proceedings of EACL,
pages 77?87.
Bernd Bohnet. 2010. Top accuracy and fast dependen-
cy parsing is not a contradiction. In Proceedings of
COLING, pages 89?97.
Bernd Bohnet. 2011. Comparing advanced graph-based
and transition-based dependency parsers. In Proceed-
ings of the International Conference on Dependency
Linguistics (Depling), pages 282?289.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNL-
L, pages 9?16.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007, pages
957?961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL, pages 208?217.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 112?119.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL-
EACL, pages 16?23.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 2009 CoNLL Shared
Task, pages 37?42.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL, pages 371?
379.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, L-
lu??s Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 2009 CoN-
LL Shared Task, pages 1?18.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings of
IJCNLP, pages 1216?1224.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL, pages 1077?1086.
Richard Johansson and Pierre Nugues. 2006. Investigat-
ing multilingual dependency parsing. In Proceedings
of CoNLL, pages 206?210.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL,
pages 1?11.
1464
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL, pages 595?603.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288?1298.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
ACL, pages 885?894.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese POS tagging and dependency parsing. In Pro-
ceedings of EMNLP, pages 1180?1191.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL-IJCNLP,
pages 342?350.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proceedings of EMNLP, pages 34?44.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2007. Incremental non-projective depen-
dency parsing. In Proceedings of NAACL HLT, pages
396?403.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34:513?553.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP, pages 351?359.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT, pages 404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of COL-
ING/ACL, pages 433?440.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP, pages 129?
137.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an ap-
plication to fine-grained POS tagging. In Proceedings
of COLING, pages 777?784.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, Alex Strehl, and S V N Vish-
wanathan. 2009. Hash Kernels for Structured Data.
In Journal of Machine Learning.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proceedings of EMNLP,
pages 145?156.
Anders S?gaard. 2011. Semi-supervised condensed n-
earest neighbor for part-of-speech tagging. In Pro-
ceedings of ACL, pages 48?52.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMNLP, pages 551?560.
Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proceed-
ings of IWPT, pages 144?155.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarization for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL, pages 252?259.
Reut Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In Pro-
ceedings of the COLING/ACL 2006 Student Research
Workshop, pages 49?54.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of EMNLP, pages 562?571.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37:105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
parsing with rich non-local features. In Proceedings
of ACL.
1465
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 77?87,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
The Best of Both Worlds ? A Graph-based Completion Model for
Transition-based Parsers
Bernd Bohnet and Jonas Kuhn
University of Stuttgart
Institute for Natural Language Processing
{bohnet,jonas}@ims.uni-stuttgart.de
Abstract
Transition-based dependency parsers are
often forced to make attachment deci-
sions at a point when only partial infor-
mation about the relevant graph configu-
ration is available. In this paper, we de-
scribe a model that takes into account com-
plete structures as they become available
to rescore the elements of a beam, com-
bining the advantages of transition-based
and graph-based approaches. We also pro-
pose an efficient implementation that al-
lows for the use of sophisticated features
and show that the completion model leads
to a substantial increase in accuracy. We
apply the new transition-based parser on ty-
pologically different languages such as En-
glish, Chinese, Czech, and German and re-
port competitive labeled and unlabeled at-
tachment scores.
1 Introduction
Background. A considerable amount of recent
research has gone into data-driven dependency
parsing, and interestingly throughout the continu-
ous process of improvements, two classes of pars-
ing algorithms have stayed at the centre of at-
tention, the transition-based (Nivre, 2003) vs. the
graph-based approach (Eisner, 1996; McDonald
et al 2005).1 The two approaches apply funda-
mentally different strategies to solve the task of
finding the optimal labeled dependency tree over
the words of an input sentence (where supervised
machine learning is used to estimate the scoring
parameters on a treebank).
The transition-based approach is based on the
conceptually (and cognitively) compelling idea
1More references will be provided in sec. 2.
that machine learning, i.e., a model of linguis-
tic experience, is used in exactly those situations
when there is an attachment choice in an other-
wise deterministic incremental left-to-right pars-
ing process. As a new word is processed, the
parser has to decide on one out of a small num-
ber of possible transitions (adding a dependency
arc pointing to the left or right and/or pushing or
popping a word on/from a stack representation).
Obviously, the learning can be based on the fea-
ture information available at a particular snapshot
in incremental processing, i.e., only surface in-
formation for the unparsed material to the right,
but full structural information for the parts of the
string already processed. For the completely pro-
cessed parts, there are no principled limitations as
regards the types of structural configurations that
can be checked in feature functions.
The graph-based approach in contrast empha-
sizes the objective of exhaustive search over all
possible trees spanning the input words. Com-
monly, dynamic programming techniques are
used to decide on the optimal tree for each par-
ticular word span, considering all candidate splits
into subspans, successively building longer spans
in a bottom-up fashion (similar to chart-based
constituent parsing). Machine learning drives
the process of deciding among alternative can-
didate splits, i.e., feature information can draw
on full structural information for the entire ma-
terial in the span under consideration. However,
due to the dynamic programming approach, the
features cannot use arbitrarily complex structural
configurations: otherwise the dynamic program-
ming chart would have to be split into exponen-
tially many special states. The typical feature
models are based on combinations of edges (so-
77
called second-order factors) that closely follow
the bottom-up combination of subspans in the
parsing algorithm, i.e., the feature functions de-
pend on the presence of two specific dependency
edges. Configurations not directly supported by
the bottom-up building of larger spans are more
cumbersome to integrate into the model (since the
combination algorithm has to be adjusted), in par-
ticular for third-order factors or higher.
Empirically, i.e., when applied in supervised
machine learning experiments based on existing
treebanks for various languages, both strategies
(and further refinements of them not mentioned
here) turn out roughly equal in their capability
of picking up most of the relevant patterns well;
some subtle strengths and weaknesses are com-
plementary, such that stacking of two parsers rep-
resenting both strategies yields the best results
(Nivre and McDonald, 2008): in training and ap-
plication, one of the parsers is run on each sen-
tence prior to the other, providing additional fea-
ture information for the other parser. Another suc-
cessful technique to combine parsers is voting as
carried out by Sagae and Lavie (2006).
The present paper addresses the question if
and how a more integrated combination of the
strengths of the two strategies can be achieved
and implemented efficiently to warrant competi-
tive results.
The main issue and solution strategy. In or-
der to preserve the conceptual (and complexity)
advantages of the transition-based strategy, the
integrated algorithm we are looking for has to
be transition-based at the top level. The advan-
tages of the graph-based approach ? a more glob-
ally informed basis for the decision among dif-
ferent attachment options ? have to be included
as part of the scoring procedure. As a prerequi-
site, our algorithm will require a memory for stor-
ing alternative analyses among which to choose.
This has been previously introduced in transition-
based approaches in the form of a beam (Johans-
son and Nugues, 2006): rather than representing
only the best-scoring history of transitions, the k
best-scoring alternative histories are kept around.
As we will indicate in the following, the mere
addition of beam search does not help overcome
a representational key issue of transition-based
parsing: in many situations, a transition-based
parser is forced to make an attachment decision
for a given input word at a point where no or only
partial information about the word?s own depen-
dents (and further decendents) is available. Fig-
ure 1 illustrates such a case.
Figure 1: The left set of brackets indicates material
that has been processed or is under consideration; on
the right is the input, still to be processed. Access to in-
formation that is yet unavailable would help the parser
to decide on the correct transition.
Here, the parser has to decide whether to create an
edge between house and with or between bought
and with (which is technically achieved by first
popping house from the stack and then adding the
edge). At this time, no information about the ob-
ject of with is available; with fails to provide what
we call a complete factor for the calculation of the
scores of the alternative transitions under consid-
eration. In other words, the model cannot make
use of any evidence to distinguish between the
two examples in Figure 1, and it is bound to get
one of the two cases wrong.
Figure 2 illustrates the same case from the per-
spective of a graph-based parser.
Figure 2: A second order model as used in graph-based
parsers has access to the crucial information to build
the correct tree. In this case, the parser condsiders the
word friend (as opposed to garden, for instance) as it
introduces the bold-face edge.
Here, the combination of subspans is performed
at a point when their internal structure has been
finalized, i.e., the attachment of with (to bought
or house) is not decided until it is clear that friend
is the object of with; hence, the semantically im-
portant lexicalization of with?s object informs the
higher-level attachment decision through a so-
called second order factor in the feature model.
78
Given a suitable amount of training data, the
model can thus learn to make the correct deci-
sion. The dynamic-programming based graph-
based parser is designed in such a way that any
score calculation is based on complete factors for
the subspans that are combined at this point.
Note that the problem for the transition-based
parser cannot be remedied by beam search alone.
If we were to keep the two options for attach-
ing with around in a beam (say, with a slightly
higher score for attachment to house, but with
bought following narrowly behind), there would
be no point in the further processing of the sen-
tence at which the choice could be corrected: the
transition-based parser still needs to make the de-
cision that friend is attached to with, but this will
not lead the parser to reconsider the decision made
earlier on.
The strategy we describe in this paper applies
in this very type of situation: whenever infor-
mation is added in the transition-based parsing
process, the scores of all the histories stored in
the beam are recalculated based on a scoring
model inspired by the graph-based parsing ap-
proach, i.e., taking complete factors into account
as they become incrementally available. As a con-
sequence the beam is reordered, and hence, the
incorrect preference of an attachment of with to
house (based on incomplete factors) can later be
corrected as friend is processed and the complete
second-order factor becomes available.2
The integrated transition-based parsing strategy
has a number of advantages:
(1) We can integrate and investigate a number of
third order factors, without the need to implement
a more complex parsing model each time anew to
explore the properties of such distinct model.
(2) The parser with completion model main-
tains the favorable complexity of transition-based
parsers.
(3) The completion model compensates for the
lower accuracy of cases when only incomplete in-
formation is available.
(4) The parser combines the two leading pars-
ing paradigms in a single efficient parser with-
out stacking the two approaches. Therefore the
2Since search is not exhaustive, there is of course a slight
danger that the correct history drops out of the beam before
complete information becomes available. But as our experi-
ments show, this does not seem to be a serious issue empiri-
cally.
parser requires only one training phase (without
jackknifing) and it uses only a single transition-
based decoder.
The structure of this paper is as follows. In Sec-
tion 2, we discuss related work. In Section 3, we
introduce our transition-based parser and in Sec-
tion 4 the completion model as well as the im-
plementation of third order models. In Section 5,
we describe experiments and provide evaluation
results on selected data sets.
2 Related Work
Kudo and Matsumoto (2002) and Yamada and
Matsumoto (2003) carried over the idea for de-
terministic parsing by chunks from Abney (1991)
to dependency parsing. Nivre (2003) describes
in a more strict sense the first incremental parser
that tries to find the most appropriate dependency
tree by a sequence of local transitions. In order
to optimize the results towards a more globally
optimal solution, Johansson and Nugues (2006)
first applied beam search, which leads to a sub-
stantial improvment of the results (cf. also (Titov
and Henderson, 2007)). Zhang and Clark (2008)
augment the beam-search algorithm, adapting the
early update strategy of Collins and Roark (2004)
to dependency parsing. In this approach, the
parser stops and updates the model when the or-
acle transition sequence drops out of the beam.
In contrast to most other approaches, the training
procedure of Zhang and Clark (2008) takes the
complete transition sequence into account as it is
calculating the update. Zhang and Clark compare
aspects of transition-based and graph-based pars-
ing, and end up using a transition-based parser
with a combined transition-based/second-order
graph-based scoring model (Zhang and Clark,
2008, 567), which is similar to the approach we
describe in this paper. However, their approach
does not involve beam rescoring as the partial
structures built by the transition-based parser are
subsequently augmented; hence, there are cases in
which our approach is able to differentiate based
on higher-order factors that go unnoticed by the
combined model of (Zhang and Clark, 2008, 567).
One step beyond the use of a beam is a dynamic
programming approach to carry out a full search
in the state space, cf. (Huang and Sagae, 2010;
Kuhlmann et al 2011). However, in this case
one has to restrict the employed features to a set
which fits to the elements composed by the dy-
79
namic programming approach. This is a trade-off
between an exhaustive search and a unrestricted
(rich) feature set and the question which provides
a higher accuracy is still an open research ques-
tion, cf. (Kuhlmann et al 2011).
Parsing of non-projective dependency trees is
an important feature for many languages. At
first most algorithms were restricted to projec-
tive dependency trees and used pseudo-projective
parsing (Kahane et al 1998; Nivre and Nilsson,
2005). Later, additional transitions were intro-
duced to handle non-projectivity (Attardi, 2006;
Nivre, 2009). The most common strategy uses
the swap transition (Nivre, 2009; Nivre et al
2009), an alternative solution uses two planes
and a switch transition to switch between the two
planes (Go?mez-Rodr??guez and Nivre, 2010).
Since we use the scoring model of a graph-
based parser, we briefly review releated work
on graph-based parsing. The most well known
graph-based parser is the MST (maximum span-
ning tree) parser, cf. (McDonald et al 2005; Mc-
Donald and Pereira, 2006). The idea of the MST
parser is to find the highest scoring tree in a graph
that contains all possible edges. Eisner (1996)
introduced a dynamic programming algorithm to
solve this problem efficiently. Carreras (2007) in-
troduced the left-most and right-most grandchild
as factors. We use the factor model of Carreras
(2007) as starting point for our experiments, cf.
Section 4. We extend Carreras (2007) graph-
based model with factors involving three edges
similar to that of Koo and Collins (2010).
3 Transition-based Parser with a Beam
This section specifies the transition-based beam-
search parser underlying the combined approach
more formally. Sec. 4 will discuss the graph-
based scoring model that we are adding.
The input to the parser is a word string x,
the goal is to find the optimal set y of labeled
edges xi?l xj forming a dependency tree over x
?{root}. We characterize the state of a transition-
based parser as pii=??i, ?i, yi, hi?, pii ? ?, the set
of possible states. ?i is a stack of words from x
that are still under consideration; ?i is the input
buffer, the suffix of x yet to be processed; yi the
set of labeled edges already assigned (a partial la-
beled dependency tree); hi is a sequence record-
ing the history of transitions (from the set of op-
erations ? = {shift, left-arcl, right-arcl, reduce,
swap}) taken up to this point.
(1) The initial state pi0 has an empty stack, the
input buffer is the full input string x, and the edge
set is empty. (2) The (partial) transition function
?(pii, t) : ? x ? ? ? maps a state and an opera-
tion t to a new state pii+1. (3) Final states pif are
characterized by an empty input buffer and stack;
no further transitions can be taken.
The transition function is informally defined as
follows: The shift transition removes the first ele-
ment of the input buffer and pushes it to the stack.
The left-arcl transition adds an edge with label l
from the first word in the buffer to the word on
top of the stack, removes the top element from
the stack and pushes the first element of the input
buffer to the stack.
The right-arcl transition adds an edge from word
on top of the stack to the first word in the input
buffer and removes the top element of the input
buffer and pushes that element onto the stack.
The reduce transition pops the top word from the
stack.
The swap changes the order of the two top el-
ements on the stack (possibly generating non-
projkective trees).
When more than one operation is applicable, a
scoring function assigns a numerical value (based
on a feature vector and a weight vector trained
by supervised machine learning) to each possi-
ble continuation. When using a beam search ap-
proach with beam size k, the highest-scoring k al-
ternative states with the same length n of transi-
tion history h are kept in a set ?beamn?.
In the beam-based parsing algorithm (cf. the
pseudo code in Algorithm 1), all candidate states
for the next set ?beamn+1? are determined using
the transition function ? , but based on the scor-
ing function, only the best k are preserved. (Fi-
nal) states to which no more transitions apply are
copied to the next state set. This means that once
all transition paths have reached a final state, the
overall best-scoring states can be read off the fi-
nal ?beamn?. The y of the top-scoring state is the
predicted parse.
Under the plain transition-based scoring
regime scoreT , the score for a state pi is the sum
of the ?local? scores for the transitions ti in the
state?s history sequence:
scoreT (pi) =
?|h|
i=0 w ? f(pii, ti)
80
Algorithm 1: Transition-based parser
// x is the input sentence, k is the beam size
?0 = ?, ?0 = x, y0 = ?, h = ?
pi0 ? ??0, ?0, y0, h0? // initial parts of a state
beam0? {pi0} // create initial state
n? 0 // iteration
repeat
n? n+ 1
for all pij ? beamn?1 do
transitions? possible-applicable-transition (pij)
// if no transition is applicable keep state pij :
if transitions = ? then beamn ? beamn ? {pij}
else for all ti ? transitions do
// apply the transition i to state j
pi ? ?(pij , ti)
beamn ? beamn ? {pi}
// end for
// end for
sort beamn due to the score(pij)
beamn ? sublist (beamn, 0, k)
until beamn?1 = beamn // beam changed?
w is the weight vector. Note that the features
f(pii, ti) can take into account all structural and
labeling information available prior to taking tran-
sition ti, i.e., the graph built so far, the words (and
their part of speech etc.) on the stack and in the
input buffer, etc. But if a larger graph configu-
ration involving the next word evolves only later,
as in Figure 1, this information is not taken into
account in scoring. For instance, if the feature
extraction uses the subcategorization frame of a
word under consideration to compute a score, it is
quite possible that some dependents are still miss-
ing and will only be attached in a future transition.
4 Completion Model
We define an augmented scoring function which
can be used in the same beam-search algorithm in
order to ensure that in the scoring of alternative
transition paths, larger configurations can be ex-
ploited as they are completed in the incremental
process. The feature configurations can be largely
taken from graph-based approaches. Here, spans
from the string are assembled in a bottom-up fash-
ion, and the scoring for an edge can be based on
structurally completed subspans (?factors?).
Our completion model for scoring a state pin
incorporates factors for all configurations (match-
ing the extraction scheme that is applied) that are
present in the partial dependency graph yn built
up to this point, which is continuously augmented.
This means if at a given point n in the transition
path, complete information for a particular config-
uration (e.g., a third-order factor involving a head,
its dependent and its grand-child dependent) is
unavailable, scoring will ignore this factor at time
n, but the configuration will inform the scoring
later on, maybe at point n+ 4, when the complete
information for this factor has entered the partial
graph yn+4.
We present results for a number of different
second-order and third-order feature models.
Second Order Factors. We start with the
model introduced by Carreras (2007). Figure 3
illustrates the factors used.
Figure 3: Model 2a. Second order factors of Carreras
(2007). We omit the right-headed cases, which are
mirror images. The model comprises a factoring into
one first order part and three second order factors (2-
4): 1) The head (h) and the dependent (c); 2) the head,
the dependent and the left-most (or right-most) grand-
child in between (cmi); 3) the head, the dependent and
the right-most (or left-most) grandchild away from the
head (cmo). 4) the head, the dependent and between
those words the right-most (or left-most) sibling (ci).
Figure 4: 2b. The left-most dependent of the head or
the right-most dependent in the right-headed case.
Figure 4 illustrates a new type of factor we use,
which includes the left-most dependent in the left-
headed case and symmetricaly the right-most sib-
ling in the right-head case.
Third Order Factors. In addition to the second
order factors, we investigate combinations of third
order factors. Figure 5 and 6 illustrate the third
order factors, which are similar to the factors of
Koo and Collins (2010). They restrict the factor
to the innermost sibling pair for the tri-siblings
81
and the outermost pair for the grand-siblings. We
use the first two siblings of the dependent from
the left side of the head for the tri-siblings and
the first two dependents of the child for the grand-
siblings. With these factors, we aim to capture
non-projective edges and subcategorization infor-
mation. Figure 7 illustrates a factor of a sequence
of four nodes. All the right headed variants are
symmetrically and left out for brevity.
Figure 5: 3a. The first two children of the head, which
do not include the edge between the head and the de-
pendent.
Figure 6: 3b. The first two children of the dependent.
Figure 7: 3c. The right-most dependent of the right-
most dependent.
Integrated approach. To obtain an integrated
system for the various feature models, the scoring
function of the transition-based parser from Sec-
tion 3 is augmented by a family of scoring func-
tions scoreGm for the completion model, wherem
is from 2a, 2b, 3a etc., x is the input string, and y
is the (partial) dependency tree built so far:
scoreTm(pi) = scoreT (pi) + scoreGm(x, y)
The scoring function of the completion model
depends on the selected factor model Gm. The
model G2a comprises the edge factoring of Fig-
ure 3. With this model, we obtain the following
scoring function.
scoreG2a(x, y) =
?
(h,c)?y w ? ffirst(x,h,c)
+
?
(h,c,ci)?y w ? fsib(x,h,c,ci)
+
?
(h,c,cmo)?y w ? fgra(x,h,c,cmo)
+
?
(h,c,cmi)?y w ? fgra(x,h,c,cmi)
The function f maps the input sentence x, and
a subtree y defined by the indexes to a feature-
vector. Again, w is the corresponding weight vec-
tor. In order to add the factor of Figure 4 to our
model, we have to add the scoring function (2a)
the sum:
(2b) scoreG2b(x, y) = scoreG2a(x, y)
+
?
(h,c,cmi)?y w ? fgra(x,h,c,cmi)
In order to build a scoring function for combi-
nation of the factors shown in Figure 5 to 7, we
have to add to the equation 2b one or more of the
following sums:
(3a)
?
(h,c,ch1,ch2)?y w ? fgra(x,h,c,ch1,ch2)
(3b)
?
(h,c,cm1,cm2)?y w ? fgra(x,h,c,cm1,cm2)
(3c)
?
(h,c,cmo,tmo)?y w ? fgra(x,h,c,cmo,tmo)
Feature Set. The feature set of the transition
model is similar to that of Zhang and Nivre
(2011). In addition, we use the cross product of
morphologic features between the head and the
dependent since we apply also the parser on mor-
phologic rich languages.
The feature sets of the completion model de-
scribed above are mostly based on previous work
(McDonald et al 2005; McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010).
The models denoted with + use all combinations
of words before and after the head, dependent,
sibling, grandchilrden, etc. These are respectively
three-, and four-grams for the first order and sec-
ond order. The algorithm includes these features
only the words left and right do not overlap with
the factor (e.g. the head, dependent, etc.). We use
feature extraction procedure for second order, and
third order factors. Each feature extracted in this
procedure includes information about the position
of the nodes relative to the other nodes of the part
and a factor identifier.
Training. For the training of our parser, we use
a variant of the perceptron algorithm that uses the
Passive-Aggressive update function, cf. (Freund
and Schapire, 1998; Collins, 2002; Crammer et
al., 2006). The Passive-Aggressive perceptron
uses an aggressive update strategy by modifying
the weight vector by as much as needed to clas-
sify correctly the current example, cf. (Crammer
et al 2006). We apply a random function (hash
function) to retrieve the weights from the weight
vector instead of a table. Bohnet (2010) showed
that the Hash Kernel improves parsing speed and
accuracy since the parser uses additionaly nega-
tive features. Ganchev and Dredze (2008) used
82
this technique for structured prediction in NLP to
reduce the needed space, cf. (Shi et al 2009).
We use as weight vector size 800 million. After
the training, we counted 65 millions non zero
weights for English (penn2malt), 83 for Czech
and 87 millions for German. The feature vectors
are the union of features originating from the
transition sequence of a sentence and the features
of the factors over all edges of a dependency tree
(e.g. G2a, etc.). To prevent over-fitting, we use
averaging to cope with this problem, cf. (Freund
and Schapire, 1998; Collins, 2002). We calculate
the error e as the sum of all attachment errors and
label errors both weighted by 0.5. We use the
following equations to compute the update.
loss: lt = e-(scoreT (x
g
t , y
g
t )-scoreT (xt, yt))
PA-update: ?t =
lt
||fg?fp||2
We train the model to select the transitions and
the completion model together and therefore, we
use one parameter space. In order to compute the
weight vector, we employ standard online learn-
ing with 25 training iterations, and carry out early
updates, cf. Collins and Roark (2004; Zhang and
Clark (2008).
Efficient Implementation. Keeping the scoring
with the completion model tractable with millions
of feature weights and for second- and third-order
factors requires careful bookkeeping and a num-
ber of specialized techniques from recent work on
dependency parsing.
We use two variables to store the scores (a)
for complete factors and (b) for incomplete fac-
tors. The complete factors (first-order factors and
higher-order factors for which further augmenta-
tion is structurally excluded) need to be calculated
only once and can then be stored with the tree fac-
tors. The incomplete factors (higher-order factors
whose node elements may still receive additional
descendants) need to be dynamically recomputed
while the tree is built.
The parsing algorithm only has to compute the
scores of the factored model when the transition-
based parser selects a left-arc or right-arc transi-
tion and the beam has to be sorted. The parser
sorts the beam when it exceeds the maximal beam
size, in order to discard superfluous parses or
when the parsing algorithm terminates in order to
select the best parse tree. The complexity of the
transition-based parser is quadratic due to swap
operation in the worse case, which is rare, and
O(n) in the best case, cf. (Nivre, 2009). The
beam size B is constant. Hence, the complexity
is in the worst case O(n2).
The parsing time is to a large degree deter-
mined by the feature extraction, the score calcu-
lation and the implementation, cf. also (Goldberg
and Elhadad, 2010). The transition-based parser
is able to parse 30 sentences per second. The
parser with completion model processes about 5
sentences per second with a beam size of 80.
Note, we use a rich feature set, a completion
model with third order factors, negative features,
and a large beam. 3
We implemented the following optimizations:
(1) We use a parallel feature extraction for the
beam elements. Each process extracts the fea-
tures, scores the possible transitions and computes
the score of the completion model. After the ex-
tension step, the beam is sorted and the best ele-
ments are selected according to the beam size.
(2) The calculation of each score is optimized (be-
yond the distinction of a static and a dynamic
component): We calculate for each location de-
termined by the last element sl ? ?i and the first
element of b0 ? ?i a numeric feature representa-
tion. This is kept fix and we add only the numeric
value for each of the edge labels plus a value for
the transition left-arc or right-arc. In this way, we
create the features incrementally. This has some
similarity to Goldberg and Elhadad (2010).
(3) We apply edge filtering as it is used in graph-
based dependency parsing, cf. (Johansson and
Nugues, 2008), i.e., we calculate the edge weights
only for the labels that were found for the part-of-
speech combination of the head and dependent in
the training data.
5 Parsing Experiments and Discussion
The results of different parsing systems are of-
ten hard to compare due to differences in phrase
structure to dependency conversions, corpus ver-
sion, and experimental settings. For better com-
parison, we provide results on English for two
commonly used data sets, based on two differ-
ent conversions of the Penn Treebank. The first
uses the Penn2Malt conversion based on the head-
36 core, 3.33 Ghz Intel Nehalem
83
Section Sentences PoS Acc.
Training 2-21 39.832 97.08
Dev 24 1.394 97.18
Test 23 2.416 97.30
Table 1: Overview of the training, development and
test data split converted to dependency graphs with
head-finding rules of (Yamada and Matsumoto, 2003).
The last column shows the accuracy of Part-of-Speech
tags.
finding rules of Yamada and Matsumoto (2003).
Table 1 gives an overview of the properties of the
corpus. The annotation of the corpus does not
contain non-projective links. The training data
was 10-fold jackknifed with our own tagger.4. Ta-
ble 1 shows the tagging accuracy.
Table 2 lists the accuracy of our transition-
based parser with completion model together with
results from related work. All results use pre-
dicted PoS tags. As a baseline, we present in ad-
dition results without the completion model and
a graph-based parser with second order features
(G2a). For the Graph-based parser, we used 10
training iterations. The following rows denoted
with Ta, T2a, T2ab, T2ab3a, T2ab3b, T2ab3bc, and
T2a3abc present the result for the parser with com-
pletion model. The subscript letters denote the
used factors of the completion model as shown
in Figure 3 to 7. The parsers with subscribed plus
(e.g. G2a+) in addition use feature templates that
contain one word left or right of the head, depen-
dent, siblings, and grandchildren. We left those
feature in our previous models out as they may in-
terfere with the second and third order factors. As
in previous work, we exclude punctuation marks
for the English data converted with Penn2Malt in
the evaluation, cf. (McDonald et al 2005; Koo
and Collins, 2010; Zhang and Nivre, 2011).5 We
optimized the feature model of our parser on sec-
tion 24 and used section 23 for evaluation. We use
a beam size of 80 for our transition-based parser
and 25 training iterations.
The second English data set was obtained by
using the LTH conversion schema as used in the
CoNLL Shared Task 2009, cf. (Hajic? et al 2009).
This corpus preserves the non-projectivity of the
phrase structure annotation, it has a rich edge
label set, and provides automatic assigned PoS
4http://code.google.com/p/mate-tools/
5We follow Koo and Collins (2010) and ignore any token
whose POS tag is one of the following tokens ?? ??:,.
Parser UAS LAS
(McDonald et al 2005) 90.9
(McDonald and Pereira, 2006) 91.5
(Huang and Sagae, 2010) 92.1
(Zhang and Nivre, 2011) 92.9
(Koo and Collins, 2010) 93.04
(Martins et al 2010) 93.26
T (baseline) 92.7
G2a (baseline) 92.89
T2a 92.94 91.87
T2ab 93.16 92.08
T2ab3a 93.20 92.10
T2ab3b 93.23 92.15
T2ab3c 93.17 92.10
T2ab3abc+ 93.39 92.38
G2a+ 93.1
(Koo et al 2008) ? 93.16
(Carreras et al 2008) ? 93.5
(Suzuki et al 2009) ? 93.79
Table 2: English Attachment Scores for the
Penn2Malt conversion of the Penn Treebank for the
test set. Punctuation is excluded from the evaluation.
The results marked with ? are not directly comparable
to our work as they depend on additional sources of
information (Brown Clusters).
tags. From the same data set, we selected the
corpora for Czech and German. In all cases, we
used the provided training, development, and test
data split, cf. (Hajic? et al 2009). In contrast
to the evaluation of the Penn2Malt conversion,
we include punctuation marks for these corpora
and follow in that the evaluation schema of the
CoNLL Shared Task 2009. Table 3 presents the
results as obtained for these data set.
The transition-based parser obtains higher ac-
curacy scores for Czech but still lower scores for
English and German. For Czech, the result of T
is 1.59 percentage points higher than the top la-
beled score in the CoNLL shared task 2009. The
reason is that T includes already third order fea-
tures that are needed to determine some edge la-
bels. The transition-based parser with completion
model T2a has even 2.62 percentage points higher
accuracy and it could improve the results of the
parser T by additional 1.03 percentage points.
The results of the parser T are lower for English
and German compared to the results of the graph-
based parser G2a. The completion model T2a can
reach a similar accuracy level for these two lan-
guages. The third order features let the transition-
based parser reach higher scores than the graph-
based parser. The third order features contribute
for each language a relatively small improvement
84
Parser Eng. Czech German
(Gesmundo
et al 2009)? 88.79/- 80.38 87.29
(Bohnet, 2009) 89.88/- 80.11 87.48
T (Baseline) 89.52/92.10 81.97/87.26 87.53/89.86
G2a (Baseline) 90.14/92.36 81.13/87.65 87.79/90.12
T2a 90.20/92.55 83.01/88.12 88.22/90.36
T2ab 90.26/92.56 83.22/88.34 88.31/90.24
T2ab3a 90.20/90.51 83.21.88.30 88.14/90.23
T2ab3b 90.26/92.57 83.22/88.35 88.50/90.59
T2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45
G2a+ 90.39/92.8 81.43/88.0 88.26/90.50
T2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62
Table 3: Labeled Attachment Scores of parsers that
use the data sets of the CoNLL shared task 2009. In
line with previous work, punctuation is included. The
parsers marked with ? used a joint model for syntactic
parsing and semantic role labelling. We provide more
parsing results for the languages of CoNLL-X Shared
Task at http://code.google.com/p/mate-tools/.
Parser UAS LAS
(Zhang and Clark, 2008) 84.3
(Huang and Sagae, 2010) 85.2
(Zhang and Nivre, 2011) 86.0 84.4
T2ab3abc+ 87.5 85.9
Table 4: Chinese Attachment Scores for the conver-
sion of CTB 5 with head rules of Zhang and Clark
(2008). We take the standard split of CTB 5 and use
in line with previous work gold segmentation, POS-
tags and exclude punctuation marks for the evaluation.
of the score. Small and statistically significant im-
provements provides the additional second order
factor (2b).6 We tried to determine the best third
order factors or set of factors but we cannot denote
such a factor which is the best for all languages.
For German, we obtained a significant improve-
ment with the factor (3b). We believe that this is
due to the flat annotation of PPs in the German
corpus. If we combine all third order factors we
obtain for the Penn2Malt conversion a small im-
provement of 0.2 percentage points over the re-
sults of (2ab). We think that a more deep feature
selection for third order factors may help to im-
prove the actuary further.
In Table 4, we present results on the Chinese
Treebank. To our knowledge, we obtain the best
published results so far.
6The results of the baseline T compared to T2ab3abc are
statistically significant (p < 0.01).
6 Conclusion and Future Work
The parser introduced in this paper combines
advantageous properties from the two major
paradigms in data-driven dependency parsing,
in particular worst case quadratic complexity of
transition-based parsing with a swap operation
and the consideration of complete second and
third order factors in the scoring of alternatives.
While previous work using third order factors, cf.
Koo and Collins (2010), was restricted to unla-
beled and projective trees, our parser can produce
labeled and non-projective dependency trees.
In contrast to parser stacking, which involves
running two parsers in training and application,
we use only the feature model of a graph-based
parser but not the graph-based parsing algorithm.
This is not only conceptually superior, but makes
training much simpler, since no jackknifing has
to be carried out. Zhang and Clark (2008) pro-
posed a similar combination, without the rescor-
ing procedure. Our implementation allows for the
use of rich feature sets in the combined scoring
functions, and our experimental results show that
the ?graph-based? completion model leads to an
increase of between 0.4 (for English) and about
1 percentage points (for Czech). The scores go
beyond the current state of the art results for ty-
pologically different languages such as Chinese,
Czech, English, and German. For Czech, English
(Penn2Malt) and German, these are to our knowl-
ege the highest reported scores of a dependency
parser that does not use additional sources of in-
formation (such as extra unlabeled training data
for clustering). Note that the efficient techniques
and implementation such as the Hash Kernel, the
incremental calculation of the scores of the com-
pletion model, and the parallel feature extraction
as well as the parallelized transition-based pars-
ing strategy play an important role in carrying out
this idea in practice.
References
S. Abney. 1991. Parsing by chunks. In Principle-
Based Parsing, pages 257?278. Kluwer Academic
Publishers.
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Tenth
Conference on Computational Natural Language
Learning (CoNLL-X).
B. Bohnet. 2009. Efficient Parsing of Syntactic and
85
Semantic Dependency Structures. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009).
B. Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 89?97, Beijing,
China, August. Coling 2010 Organizing Commit-
tee.
X. Carreras, M. Collins, and T. Koo. 2008. Tag,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ?08, pages 9?16, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
X. Carreras. 2007. Experiments with a Higher-order
Projective Dependency Parser. In EMNLP/CoNLL.
M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In ACL, pages 111?
118.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?
345, Copenhaen.
Y. Freund and R. E. Schapire. 1998. Large margin
classification using the perceptron algorithm. In
11th Annual Conference on Computational Learn-
ing Theory, pages 209?217, New York, NY. ACM
Press.
K. Ganchev and M. Dredze. 2008. Small statisti-
cal models by random feature mixing. In Proceed-
ings of the ACL-2008 Workshop on Mobile Lan-
guage Processing. Association for Computational
Linguistics.
A. Gesmundo, J. Henderson, P. Merlo, and I. Titov.
2009. A Latent Variable Model of Syn-
chronous Syntactic-Semantic Parsing for Multiple
Languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), Boulder, Colorado, USA., June 4-5.
Y. Goldberg and M. Elhadad. 2010. An efficient al-
gorithm for easy-first non-directional dependency
parsing. In HLT-NAACL, pages 742?750.
C. Go?mez-Rodr??guez and J. Nivre. 2010. A
Transition-Based Parser for 2-Planar Dependency
Structures. In ACL, pages 1492?1501.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, United States, June.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086, Up-
psala, Sweden, July. Association for Computational
Linguistics.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings
of the Shared Task Session of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 206?210, New York City, United
States, June 8-9.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proceedings of the Shared Task
Session of CoNLL-2008, Manchester, UK.
S. Kahane, A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-ACL,
pages 646?652.
T. Koo and M. Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
pages 595?603.
T. Kudo and Y. Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In
proceedings of the 6th conference on Natural lan-
guage learning - Volume 20, COLING-02, pages 1?
7, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Kuhlmann, C. Go?mez-Rodr??guez, and G. Satta.
2011. Dynamic programming algorithms for
transition-based dependency parsers. In ACL, pages
673?682.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: De-
pendency parsing by approximate variational infer-
ence. pages 34?44.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms.
In In Proc. of EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-margin Training of Dependency Parsers.
In Proc. ACL, pages 91?98.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers.
In ACL-08, pages 950?958, Columbus, Ohio.
86
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In ACL.
J. Nivre, M. Kuhlmann, and J. Hall. 2009. An im-
proved oracle for dependency parsing with online
reordering. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, IWPT
?09, pages 73?76, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In 8th International
Workshop on Parsing Technologies, pages 149?160,
Nancy, France.
J. Nivre. 2009. Non-Projective Dependency Parsing
in Expected Linear Time. In Proceedings of the
47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP, pages 351?359, Suntec, Singa-
pore.
K. Sagae and A. Lavie. 2006. Parser combina-
tion by reparsing. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 129?132, Morristown, NJ, USA. Association
for Computational Linguistics.
Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola,
and S.V.N. Vishwanathan. 2009. Hash Kernels for
Structured Data. In Journal of Machine Learning.
J. Suzuki, H. Isozaki, X. Carreras, and M Collins.
2009. An empirical study of semi-supervised struc-
tured conditional models for dependency parsing.
In EMNLP, pages 551?560.
I. Titov and J. Henderson. 2007. A Latent Variable
Model for Generative Dependency Parsing. In Pro-
ceedings of IWPT, pages 144?155.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of IWPT, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two
parsers: investigating and combining graph-based
and transition-based dependency parsing using
beam-search. In Proceedings of EMNLP, Hawaii,
USA.
Y. Zhang and J. Nivre. 2011. Transition-based de-
pendency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 188?193, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
87
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 683?687,
Dublin, Ireland, August 23-24, 2014.
UBham: Lexical Resources and Dependency Parsing for Aspect-Based
Sentiment Analysis
Viktor Pekar
School of Computer Science
University of Birmingham
Birmingham, UK
v.pekar@cs.bham.ac.uk
Naveed Afzal
FCIT, North Branch
King Abdulaziz University
Jeddah, KSA
nafzal@kau.edu.sa
Bernd Bohnet
School of Computer Science
University of Birmingham
Birmingham, UK
b.bohnet@cs.bham.ac.uk
Abstract
This paper describes the system devel-
oped by the UBham team for the SemEval-
2014 Aspect-Based Sentiment Analysis
task (Task 4). We present an approach
based on deep linguistic processing tech-
niques and resources, and explore the pa-
rameter space of these techniques applied
to the different stages in this task and ex-
amine possibilities to exploit interdepen-
dencies between them.
1 Introduction
Aspect-Based Sentiment Analysis (ASBA) is con-
cerned with detection of the author?s sentiment to-
wards different issues discussed in a document,
such as aspects or features of a product in a cus-
tomer review. The specific ASBA scenario we ad-
dress in this paper is as follows. Given a sentence
from a review, identify (1) aspect terms, specific
words or multiword expressions denoting aspects
of the product; (2) aspect categories, categories of
issues being commented on; (3) aspect term po-
larity, the polarity of the sentiment associated with
each aspect term; and (4) aspect category polarity,
the polarity associated with each aspect category
found in the sentence. For example, in:
I liked the service and the staff, but not the food.
aspect terms are service, staff and food, where the
first two are evaluated positively and the last one
negatively; and aspect categories are SERVICE and
FOOD, where the former is associated with pos-
itive sentiment and the latter with negative. It
should be noted that a given sentence may contain
This work is licenced under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The research was partially supported by FP7 ICT project
?Workbench for Interactive Contrastive Analysis of Patent
Documentation? under grant no. FP7-SME-606163.
one, several, or no aspect terms, one, several, or no
aspect categories, and may express either positive,
negative, neutral, or conflicted sentiment.
While the ASBA task is usually studied in the
context of documents (e.g., online reviews), pecu-
liarities of this scenario are short input texts, com-
plex categorization schemas, and a limited amount
of annotated data. Therefore we focused on ways
to exploit deep linguistic processing techniques,
which we use for both creating complex classifi-
cation features and rule-based processing.
2 Related Work
2.1 Aspect Term Extraction
To recognize terms that express key notions in a
product or service review, a common general ap-
proach has been to extract nouns and noun phrases
as potential terms and then apply a certain filtering
technique to ensure only the most relevant terms
remain. These techniques include statistical asso-
ciation tests (Yi et al., 2003), associative mining
rules with additional rule-based post-processing
steps (Hu and Liu, 2004), and measures of asso-
ciation with certain pre-defined classes of words,
such as part-whole relation indicators (Popescu
and Etzioni, 2005).
2.2 Aspect Category Recognition
Aspect category recognition is often addressed as
a text classification problem, where a classifier
is learned from reviews manually tagged for as-
pects (e.g., Snyder and Barzilay, 2007, Ganu et al.,
2009). Titov and McDonald (2008) present an ap-
proach which jointly detects aspect categories and
their sentiment using a classifier trained on top-
ics discovered via Multi-Grain LDA and star rat-
ings available in training data. Zhai et al. (2010)
presented an approach based on Expectation-
Maximization to group aspect expressions into
user-defined aspect categories.
683
2.3 Sentence Sentiment
Lexicon-based approaches to detecting sentiment
in a sentence rely on a lexicon where words and
phrases are provided with sentiment labels as well
as on techniques to recognize ?polarity shifters?,
phrases causing the polarity of a lexical item
to reverse. Early work on detection of polarity
shifters used surface-level patterns (Yu and Hatzi-
vassilouglu, 2003; Hu and Liu, 2004). Moila-
nen and Pulman (2007) provide a logic-oriented
framework to compute the polarity of grammatical
structures, that is capable of dealing with phenom-
ena such as sentiment propagation, polarity rever-
sal, and polarity conflict. Several papers looked at
different ways to use syntactic dependency infor-
mation in a machine learning framework, to better
account for negations and their scope (Nakagawa
et al., 2010; Socher et al., 2013).
To adapt a generic sentiment lexicon to a new
application domain, previous work exploited se-
mantic relations encoded in WordNet (Kim and
Hovy, 2006), unannotated data (Li et al, 2012), or
queries to a search engine (Taboada et al., 2006).
3 Our Approach
In the following sections, we will describe our ap-
proach to each stage of the Shared Task, reporting
experiments on the provided training data using a
10-fold cross-validation.
3.1 Aspect Term Extraction
During pre-processing training data was parsed
using a dependency parser (Bohnet and Nivre,
2012), and sentiment words were recognized in it
using a sentiment lexicon (see Section 6.1). Can-
didate terms were extracted as single nouns, noun
phrases, adjectives and verbs, enforcing certain
exceptions as detailed in the annotation guidelines
for the Shared Task (Pontiki et al., 2014), namely:
? Sentiment words were not allowed as part of
terms;
? Noun phrases with all elements capitalized
and acronyms were excluded, under the as-
sumption they refer to brands rather than
product aspects;
? Nouns referring to the product class as a
whole (?restaurant?, ?laptop?, etc) were ex-
cluded.
Candidate terms that exactly overlapped with
manually annotated terms were discarded, while
those that did not were used as negative examples
of aspect terms.
In order to provide the term extraction process
with additional lexical knowledge, from the train-
ing data we extracted those manually annotated
terms that corresponded to a single aspect cate-
gory. Then the set of terms belonging to each
category was augmented using WordNet: first we
determined the 5 most prominent hyperonyms of
these terms in the WordNet hierarchy using Resnik
(1992)?s algorithm for learning a class in a seman-
tic hierarchy that best represents selectional pref-
erences of a verb, additionally requiring that each
hypernym is at least 7 nodes away from the root, to
make them sufficiently specific. Then we obtained
all lexical items that belong to children synsets of
these hypernyms, and further extended these lexi-
cal items with their meronyms and morphological
derivatives. The resulting set of lexical items was
later used as an extended aspect term lexicon. We
additionally created a list of all individual lemmas
of content words found in this lexicon.
For each term, we extracted the following fea-
tures to be used for automatic classification:
? Normalized form: the surface form of the
term after normalization;
? Term lemmas: lemmas of content words
found in the term;
? Lexicon term: if the term is in the lexicon;
? Lexicon lemmas ratio: the ratio of lexicon
lemmas in the term;
? Unigram: 3 unigrams on either side of the
term;
? Bigrams: The two bigrams around the term;
? Adj+term: If an adjective depends on the
term
1
or related to it via a link verb (?be?,
?get?, ?become?, etc);
? Sentiment+term: If a sentiment word de-
pends on the term or related via a link verb;
? Be+term: If the term depends on a link verb;
? Subject term: If the term is a subject;
1
In case the term was a multi-word expression, the rela-
tion to the head of the phrase was used.
684
? Object term: If the term is an object.
We first look at how well the manually designed
patterns extracted potential terms. We are primar-
ily interested in recall at this stage, since after that
potential terms are classified into terms and non-
terms with an automatic classifier. The recall on
the restaurants was 70.5, and on the laptops ?
56.9. These are upper limits on recall for the over-
all task of aspect term recognition.
Table 1 and Table 2 compare the performance of
several learning algorithms on the restaurants and
the laptops dataset, respectively
2
.
P R F
Linear SVM 94.42 95.51 94.96
Decision Tree 94.24 92.90 93.56
Na??ve Bayes 84.97 95.67 89.99
kNN (k=5) 82.71 93.50 87.76
Table 1: Learning algorithms on the aspect term
extraction task, restaurants dataset.
P R F
Linear SVM 88.14 94.07 91.00
Na??ve Bayes 93.61 79.46 85.92
Decision Tree 83.87 82.99 83.39
kNN (k=5) 82.83 83.31 83.03
Table 2: Learning algorithms on the aspect term
extraction task, laptops dataset.
On both datasets, linear SVMs performed best,
and so they were used in the subsequent experi-
ments on term recognition. To examine the qual-
ity of each feature used for term classification, we
ran experiments where a classifier was built and
tested without that feature, see Tables 3 and 4, for
the restaurants and laptops datasets respectively,
where a greater drop in performance compared to
the entire feature set, indicates a more informative
feature.
The results show the three most useful features
are the same in both datasets: the occurrence of the
candidate term in the constructed sentiment lexi-
con, the lemmas found in the term, and the nor-
malized form of the term account.
We ran further experiments manually selecting
several top-performing features, but none of the
2
This and the following experiments were run on the train
data supplied by the shared task organizers using 10-fold
cross-validation.
P R F
Lexicon term 91.74 95.01 93.33
Term lemmas 92.43 95.00 93.69
Normalized form 93.45 95.36 94.39
Be+term 93.99 95.28 94.63
Left bigram 94.21 95.09 94.64
All features 94.42 95.51 94.96
Table 3: Top 5 most informative features for the
term extraction subtask, restaurants dataset.
P R F
Lexicon term 88.82 88.61 88.69
Term lemmas 85.02 95.16 89.79
Normalized form 87.79 92.13 89.89
Left bigram 87.83 93.62 90.62
Term is obj 87.79 94.43 90.97
All features 88.14 94.07 91.00
Table 4: Top 5 most informative features for the
term extraction subtask, laptops dataset.
configurations produced significant improvements
on the use of the whole feature set.
Table 5 shows the results of evaluation of the as-
pect term extraction on the test data of the Shared
Task (baseline algorithms were provided by the or-
ganizers). The results correspond to what can be
expected based on the upper limits on recall for
the pattern-based extraction of candidate terms as
well as precision and recall for the classifier.
P R F
Restaurants 77.9 61.1 68.5
Restaurants, baseline 53.9 51.4 52.6
Laptops 60.3 39.1 47.5
Laptops, baseline 40.1 38.1 39.1
Table 5: Aspect term extraction on the test data of
the Shared Task.
3.2 Aspect Category Recognition
To recognize aspect categories in a sentence, we
classified individual clauses found in it, assuming
that each aspect category would be discussed in
a separate clause. Features used for classification
were lemmas of content words; to account for the
fact that aspect terms are more indicative of aspect
categories than other words, we additionally used
entire terms as features, weighting them twice as
much as other features. Table 6 compares the per-
685
formance of several learning algorithms when au-
tomatically recognized aspect terms were not used
as an additional feature; Table 7 shows results
when terms were used as features.
P R F
Linear SVM 66.37 58.07 60.69
Decision Tree 58.07 51.22 53.05
Na??ve Bayes 74.34 46.07 48.63
kNN (k=5) 58.65 43.77 46.57
Table 6: Learning algorithms on the aspect cate-
gory recognition task, aspect terms not weighted.
P R F
Linear SVM 67.23 59.43 61.90
Decision Tree 64.41 55.84 58.36
Na??ve Bayes 78.02 49.57 52.87
kNN (k=5) 67.92 47.91 51.94
Table 7: Learning algorithms on the aspect cate-
gory recognition task, aspect terms weighted.
The addition of aspect terms as separate features
increased F-scores for all the learning methods,
sometimes by as much as 5%. Based on these re-
sults, we used the linear SVM method for the task
submission. Table 8 reports results achieved on
the test data of the Shared Task.
P R F
Restaurants 81.8 67.9 74.2
Baseline 64.8 52.5 58.0
Table 8: Aspect category extraction on the test
data of the Shared Task.
3.3 Aspect Term Sentiment
To recognize sentiment in a sentence, we take a
lexicon-based approach. The sentiment lexicon
we used encodes the lemma, the part-of-speech
tag, and the polarity of the sentiment word. It was
built by combining three resources: lemmas from
SentiWordNet (Baccianella et al., 2010), which do
not belong to more than 3 synsets; the General
Inquirer lexicon (Stone et al., 1966), and a sub-
section of the Roget thesaurus annotated for sen-
timent (Heng, 2004). In addition, we added sen-
timent expressions that are characteristic of the
restaurants and laptop domains, obtained based on
manual analysis of the restaurants corpus used in
(Snyder and Barzilay (2007) and the laptop re-
views corpus used in (Jindal and Liu, 2008).
To detect negated sentiment, we used a list of
negating phrases such as ?not?, ?never?, etc., and
two types of patterns to determine the scope of a
negation. The first type detected negations on the
sentence level, checking for negative phrases at
the start of the sentence; negations detected on the
sentence level were propagated to the clause level.
The second type of patterns detected negated sen-
timent within a clause, using patterns specific to
the part-of-speech of the sentiment word (e.g.,
?AUXV + negation + VB + MAINV?, where
MAINV is a sentiment verb). The output of this
algorithm is the sentence split into clauses, with
each clause being assigned one of four sentiment
labels: ?positive?, ?negative?, ?neutral?, ?con-
flict?. Thus, each term was associated with the
sentiment of the clause it appeared in.
On the test data of the Shared Task, the algo-
rithm achieved the accuracy scores of 76.0 (the
restaurants data, for the baseline of 64.3) and 63.6
(the laptops data, for the baseline of 51.1).
3.4 Category Sentiment
Recall that aspect categories were recognized in a
sentence by classifying its individual clauses. Cat-
egory sentiment was determined from the senti-
ment of the clauses where the category was found.
In case more than one clause was assigned to the
same category and at least one clause expressed
positive sentiment and at least one ? negative,
such cases were classified as conflicted sentiment.
This method achieved the accuracy of 72.8 (on the
restaurants data), with the baseline being 65.65.
4 Conclusion
Our study has shown that aspect terms can be de-
tected with a high accuracy using a domain lexicon
derived from WordNet, and a set of classification
features created with the help of deep linguistic
processing techniques. However, the overall accu-
racy of aspect term recognition is greatly affected
by the extraction patterns that are used to extract
initial candidate terms. We also found that au-
tomatically extracted aspect terms are useful fea-
tures in the aspect category recognition task. With
regards to sentiment detection, our results suggest
that reasonable performance can be achieved with
a lexicon-based approach coupled with carefully
designed rules for the detection of polarity shifts.
686
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SENTIWORDNET 3.0: An Enhanced
Lexical Resource for Sentiment Analysis and Opin-
ion Mining. Proceedings of LREC-2010.
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. Pro-
ceedings of EMNLP-CoNLL.
Gayatree Ganu, No?emie Elhadad, and Ameli?e Mar-
ian. 2009. Beyond the Stars: Improving Rating
Predictions using Review Text Content. Proceedings
of Twelfth International Workshop on the Web and
Databases (WebDB 2009).
Adrian Heng. 2004. An exploratory study into the use
of faceted classification for emotional words. Mas-
ter Thesis. Nanyang Technological University, Sin-
gapore.
Minqing Hu and Bing Liu. 2004. Mining opinion
features in customer reviews. Proceedings of the
9th National Conference on Artificial Intelligence
(AAAI-2004).
Nitin Jindal and Bing Liu. 2008. Opinion Spam and
Analysis Proceedings of WWW-2008.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. Proceedings of
HLT/NAACL-2006.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang and
Xiaoyan Zhu. 2012. Cross-Domain Co-Extraction
of Sentiment and Topic Lexicons. Proceedings of
ACL-2012.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. Proceedings
of NAACL/HLT-2010.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. Proceedings of the Recent Advances
in Natural Language Processing (RANLP 2007).
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014).
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. Pro-
ceedings HLT/EMNLP-2005.
Philip Resnik. 1992. A class-based approach to lexi-
cal discovery Proceedings of the Proceedings of the
30th Annual Meeting of the Association for Compu-
tational Linguists.
Benjamin Snyder and Regina Barzilay 2007. Multi-
ple Aspect Ranking using the Good Grief Algorithm.
Proceedings of NAACL-2007.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng
and Christopher Potts 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. Proceedings of EMNLP-2013.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
Cambridge, MA: The MIT Press.
Maite Taboada, Caroline Anthony, and Kimberly Voll.
2006. Creating semantic orientation dictionaries
Proceedings of 5th International Conference on Lan-
guage Resources and Evaluation (LREC).
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. Proceedings of ACL-2008.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun
Zhao. 2013. Mining Opinion Words and Opinion
Targets in a Two-Stage Framework. Proceedings of
ACL-2013.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. Proceedings of the
3rd IEEE International Conference on Data Mining
(ICDM-2003), pp. 423-434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating
Facts from Opinions and Identifying the Polarity of
Opinion Sentences. Proceedings of EMNLP-03.
Zhongwu Zhai, Bing Liu, Hua Xu and Peifa Jia. 2011.
Clustering product features for opinion mining. Pro-
ceedings of the 4th ACM International Conference
on Web Search and Data Mining, ACM, pp 347354.
687
IS-G: The Comparison of Different Learning Techniques for the Selection of
the Main Subject References
Bernd Bohnet
University of Stuttgart
Visualization and Interactive Systems Group
Universita?tstr. 58, 70569 Stuttgart, Germany
bohnet@informatik.uni-stuttgart.de
Abstract
The GREC task of the Referring Expression
Generation Challenge 2008 is to select appro-
priate references to the main subject in given
texts. This means to select the correct type of
the referring expressions such as name, pro-
noun, common, or elision (empty). We employ
for the selection different learning techniques
with the aim to find the most appropriate one
for the task and the used attributes. As train-
ing data, we use the syntactic category of the
searched referring expressions and addition-
ally gathered data from the text itself.
1 Introduction
The training data of the GREC task consists of
Wikipedia articles from five domains. The arti-
cles are about People, Cities, Countries, Rivers, and
Mountains. An XML annotation replaces the orig-
inal referring expressions in the articles with a set
of alternative experssions which could be inserted in
the empty space in order to complete the text. The
training data additionally consists of the original re-
ferring expressions. From the annotations, we use
for the training the syntactic category (SYNCAT) for
the searched referring expression.
The annotation allows us easily to access addi-
tional data. One of the values that we calculate is
the distance (DIST) to the last referring expression.
The idea behind this value is that the references be-
comes with increasing distance unclear because of
other content in the text and therefore, to provide a
short form such as a pronoun is not enough or would
be even misleading. Of cause there might be other
reasons to use the name too. The next information,
we use is the count (POS) of the referring expres-
sion in the text. Because we expect that the first re-
ferring expression is in most cases the name of the
main subject and at the end of a text, it might be
used less frequent. Then we use the type of the last
(LAST) used referring expression in the text. This
might be a good candidate because people want to
avoid consecutive repetitions of the same word. The
disadvantage of this attribute is that it is based itself
on the classification of its predecessor and therefore,
on insecure information. Finally, we use a second
distance measure which provides the information if
the last referring expression was in the same sen-
tence (SENT).
2 Comparison of Learning Techniques
We tried several machine learning techniques and
selected among them the three bests that are
Bayesian Networks with the attribute selection
method K2 (Cooper and Herskovits, 1992), decision
trees C4.5 (Quinlan, 1993), and Multi Layer Percep-
trons with a sigmoid function (Minsky and Papert,
1969). For the comparison with the three machine
learning techniques, we provide in Table 1 a base
line where we chose the type with the most occur-
rences in the training data of the domain.
Table 2 shows the results for the Bayesian Net-
work. The results are significant better then the base
line results. Table 3 shows the results of C4.5. The
results are close to the results of the Bayesian Net-
work. An advantage of decision trees is that they
provide some explanation. A part of the decision
tree is shown in Figure 1. The part selects a refer-
192
Set Most frequent type Accuracy
Cities name 0.47
Countries name 0.45
Mountains name 0.39
People pronoun 0.61
Rivers name 0.43
Total ? 0.47
Table 1: Base Line
Set Cities Cou. Mount. Peo. Ri. Total
Acc 0.48 0.62 0.63 0.673 0.7 0.62
Table 2: Results for Bayesian Networks (K2)
ring expression in the case that the last expression
was already within the same sentences.
Set Cities Cou. Mount. Peo. Ri. Total
Acc 0.545 0.63 0.641 0.673 0.7 0.638
Table 3: Results for C4.5
The uppercase words are the attributes followed
by the value for the branch. If the value of a distinct
instance is in the range of the value then the algo-
rithms chooses the branch until it reaches a leaf. The
leafs are labelled with the result of the decision and
with information of an evaluation that provides the
information how many training instances (cases) are
classified correct / wrong. Interesting for the case
are the following observations:
? The text writers chose nearly always (>99%)
an other referring expression than the name.
? They select more frequent pronouns and an eli-
sions (empty) compared to common names.
? The writers select common names in case of a
high distance to the last referring expression.
Table 4 shows the results for the Multi Layer
Perceptron, which performed best compared to the
other learning techniques.
3 Conclusion
We calculated the information gain of each attribute
to get an overview of the relevance of the attributes.
The most releveant attribute is DIST (0.32) followed
by POS (0.24), LAST (0.239), SENT (0.227), and
SYNCAT (0.19).
SENT = true
? SYNCAT = subj-det
? ? DIST <= 158: pronoun (103.0/8.0)
? ? DIST > 158: common (4.0/1.0)
? SYNCAT = np-subj
? ? DIST <= 33: pronoun (32.0/15.0)
? ? DIST > 33
? ? ? LAST = common
? ? ? ? DIST <= 123: empty (25.0/5.0)
? ? ? ? DIST > 123: common (2.0/1.0)
? ? ? LAST = pronoun: empty (69.0/22.0)
? ? ? LAST = name
? ? ? ? POS <= 2: empty (17.0/2.0)
? ? ? ? POS > 2
? ? ? ? ? POS <= 15
? ? ? ? ? ? DIST <= 79
? ? ? ? ? ? ? DIST <= 39: pronoun (3.0)
? ? ? ? ? ? ? DIST > 39: empty (46.0/15.0)
? ? ? ? ? ? DIST > 79
? ? ? ? ? ? ? POS <= 5: pronoun (3.0)
? ? ? ? ? ? ? POS > 5
? ? ? ? ? ? ? ? POS <= 11
? ? ? ? ? ? ? ? ? DIST <= 113: common (4.0/1.0)
? ? ? ? ? ? ? ? ? DIST > 113: name (2.0/1.0)
? ? ? ? ? ? ? ? POS > 11: pronoun (2.0)
? ? ? ? ? POS > 15: common (2.0)
? ? ? LAST = empty: pronoun (5.0/1.0)
? SYNCAT = np-obj
? ? DIST <= 109
? ? ? POS <= 9: pronoun (23.0/12.0)
? ? ? POS > 9: common (10.0/4.0)
? ? DIST > 109: common (17.0/3.0)
SENT = false
...
Figure 1: Part of the decision tree
The results of all three learning techniques are
significant better than the base line which has in av-
erage an accuracy of 0.47. The multi layer percep-
tron provides the best results with an average accu-
racy of 0.66.
References
G. F. Cooper and E. Herskovits. 1992. A Bayesian
Method for the Induction of Probabilistic Networks
from Data. In Machine Learning 9.
M. Minsky and S. Papert. 1969. Perceptrons. MIT Press.
J. R. Quinlan. 1993. C4.5 Programs for Machine Learn-
ing. Morgan Kaufmann, California.
Set Cities Cou. Mount. Peo. Ri. Total
Acc 0.545 0.64 0.65 0.668 0.8 0.66
Table 4: IS-G: Multi Layer Perceptron
193
The Fingerprint of Human Referring Expressions and their Surface
Realization with Graph Transducers
Bernd Bohnet
University of Stuttgart
Visualization and Interactive Systems Group
Universita?tstr. 58, 70569 Stuttgart, Germany
bohnet@informatik.uni-stuttgart.de
Abstract
The algorithm IS-FP takes up the idea from
the IS-FBN algorithm developed for the
shared task 2007. Both algorithms learn the
individual attribute selection style for each hu-
man that provided referring expressions to the
corpus. The IS-FP algorithm was developed
with two additional goals (1) to improve the
indentification time that was poor for the FBN
algorithm and (2) to push the dice score even
higher. In order to generate a word string for
the selected attributes, we build based on indi-
vidual preferences a surface syntactic depen-
dency tree as input. We derive the individual
preferences from the training set. Finally, a
graph transducer maps the input strucutre to a
deep morphologic structure.
1 IS-FP: Generating Referring Expression
with a Human Imprint
A review of the referring expressions shows that
humans prefer frequently distinct attributes and at-
tribute combination such as in the following exam-
ples.
grey desk (30t1), a red chair (30t2), red sofa (30t3), blue
chair (30t5), a small green desk (30t6)
the one in the top left corner (31t1), the one to the left in
the middle row (31t2), the bottom right most one (31t3),
the blue chair at the bottom center (31t5), etc.
The first individual (#30) seems to prefer colour
and size while the second one (#31) seems to prefer
the relative position (to the left) and places (the top
left corner). Because of the review, we checked, if
the Incremental Algorithm (Dale and Reiter, 1995)
using the order for the attributes due to the frequency
calculated for each individual can outperform the al-
gorithm using the order for the attributes due to the
frequency of the complete training set. This was the
case. Table 1 shows the results. Using the individual
attribute order, the IA performed as good as the FBN
algorithm, cf. Table 1.
Algorithm Furniture People Avg.
IA (complete) 0.796 0.710 0.753
IA (individual) 0.835 0.736 0.7855
FBN 0.810 0.762 0.786
Table 1: Incremental algorithm and FBN
However, the FBN algorithm generates all pos-
sible referring expressions and selects based on the
dice metric the most similar expressions of the same
human. Since there is usually a set of equal good re-
ferring expressions, it is possible to select a referring
expression among these results due to another met-
ric. That this would improve the results shows the
experiment to selected among these results the refer-
ring expression that is closest to the correct result.
The outcome was that the FBN algorithm has still
about 9% room for improvements. The following
sections investigates possibilities to use this chance.
1.1 Identification Time
An important metric is the identification time that is
the time which is used by a human to identify an en-
tity due to a given referring expression. The identi-
fication time is very loosely related with the number
of minimal referring expressions and therefore likely
with the length of a referring expression. The best
identification times had a system with 74% minimal
referring expressions and the second and third best
207
systems had about 41%. Good identification times
had nearly all systems with only a maximum differ-
ence of 0.38 seconds except FBN and FBS which
are about 1.05 and 1.49 seconds behind the best one.
This is a huge difference compared to all other sys-
tems. What could be the reason for that? We know
of two differences of FBN to all other systems: (1)
the lowest portion of minimal referring expressions
of all systems and (2) the nearest neighbour learn-
ing technique. The number of minimal referring ex-
pressions is also different to the number of expres-
sions found in the training set. Table 2 shows in the
columns human the average length and portion of
minimal human referring expressions. Because of
the different length of the human and the generated
expressions, we conducted the experiment to chose
always the shortest. Table 2 shows the change be-
tween the random selection (FBN) and the selection
of the shortest (FP). The experiment leads to a result
that have a length and percentage of minimal refer-
ring expressions in average similar to the humans
ones, cf. columns of human and shortest.
selection human random (FBM) shortest (FP)
RE Len. Min. Len. Min. Len. Min.
Furniture 3.1 26.3 3.5 9.4 3.1 15.9
People 3.0 30.9 2.8 28.8 2.8 30.8
Table 2: Length and portion of min. RE
The second difference is the use of the nearest
neighbour technique. Could the poor identification
time be caused by the nearest neighbour technique?
How does it influence referring expressions? ? The
referring expressions are generated in all the dif-
ferent styles like the human expressions of the cor-
pus. Do humans learn the style of referring expres-
sions and expect then the next expression in the same
style? And are we confused when we don?t get what
we expact? Or does FBN look too much on the ex-
pressions of the humans and too less on the domain?
We hope to get answers for these questions from the
shared task evaluation of IS-FP.
1.2 The IS-FP Algorithm
The basis for the IS-FP algorithm is an extended full
brevity implementation in terms of problem solving
by search which computes all referring expression,
cf. (Bohnet and Dale, 2005). IS-FP uses also the
nearest neighbour technique like the IS-FBN algo-
rithm that was introduced by Bohnet (2007). With
the nearest neighbour technique, IS-FP selects the
expressions which are most similar to the referring
expressions of the same human and a human that
builds referring expressions similar. The similarity
is computed as the average of all dice values be-
tween all combinations of the available trails for two
humans. From the result of the nearest neighbour
evaluation, FP selects the shortest and if still more
than one expressions remain then it computes the
similarity among them and chooses the most typi-
cal and finally, if still alternatives remain, it selects
one with the attributes having the highest frequency.
Table 3 shows the results for IS-FP trained on the
training set and applied to the development set.
Set Dice MASI Accuracy Uniq. Min.
Furniture 0.881 0.691 51.25% 100% 1.25%
People 0.790 0.558 36.8% 100% 0%
Total 0.836 0.625 44% 100% 0.62%
Table 3: Results for the IS-FP algorithm
2 IS-GT: Realization with Graph
Transducers
We build the input depedency tree for the text gener-
ator due to the statistical information that we collect
from the training data for each person. This pro-
cedure is consistent with our referring expression
generator IS-FP that reproduces the individual im-
print in a referring expression for the target person.
We start with the realization of the referring expres-
sions from a surface syntactic dependency tree, cf.
(Mel?c?uk, 1988). For the realization of the text, we
use the Text Generator and Linguistic Environment
MATE, cf. (Bohnet, 2006). We reportet the first
time about MATE on the first International Natu-
ral Language Generation Conference, cf. (Bohnet et
al., 2000). It was since then continuously enhanced
and in the last years, large grammars for several lan-
guages such as Catalan, English, Finnish, French,
German, Polish, Portougees have been developed
within the European Project MARQUIS and PatEx-
pert, cf. (Wanner et al, 2007), (Lareau and Wanner,
2007) and (Mille and Wanner, 2008).
2.1 The Referring Expression Models
A learning program builds a Referring Expression
Model for each person that contributed referring ex-
pression to the corpus. The model contains the fol-
lowing information: (1) The lexicalization for the
208
values of a attribute such as couch for the value sofa,
man for value person, etc. (2) The prefered usage of
determiners for the type that can be definite (the), in-
definite (a), no article. (3) The syntactic preferences
such as the top left chair, the chair at the bottom to
the left, etc.
The information about the determiner and the
lexicalization is collected from the annotated word
string and the word string itself. We collect the most
frequent usage for each person in the coprpus. In
order to collect the prefered syntax, we annotated
the word strings with syntactic dependency trees.
Each of the dependency tress contains additional at-
tributes, which describe the information content of a
branch outgoing from the root as well as the possi-
ble value of the attriube at the nodes which carry the
information. The learning program cuts the syntac-
tic tree at edges starting at the root node and stores
the branches in the referring expression model for
the person. For instance, the complete referring ex-
pression model of a person would contain due to the
training data the following information:
article: definite
lexicalization: person ? man, light ? white
syntax:
t21a: wearing glasses {t:hasGlasses a1:1 v1:glasses}
t21b: with compl ? beard {t:hasBeard a1:1 v1:beard} det ? a
beard compl ? white {t:hairColour a1:light v1:white
a2:dark v2:dark}
t22: with compl ? beard {t:hasBeard a1:1 v1:beard}
beard det ? a
t23: wearing obj ?glasses {t:hasGlasses a1:1 v1:glasses}
t26: with compl ? glasses {t:hasGlasses a1:1 v1:glasses}
glasses coord ? and compl ? heair mod ?
dark{t:hairColour a1:dark v1:dark}
2.2 Setting up the Input for the Generator
One of the input attribute sets of the people domain
looks like the following one:
<TRIAL CONDITION=?-LOC? ID=?s81t25?>
...
<ATTRIBUTE-SET>
<ATTRIBUTE ID=?a4? NAME=?hasBeard? VALUE=?1?/ >
<ATTRIBUTE ID=?a3? NAME=?hairColour? VALUE=?light?/ >
<ATTRIBUTE ID=?a2? NAME=?hasGlasses? VALUE=?1?/ >
<ATTRIBUTE ID=?a1? NAME=?type? VALUE=?person?/ >
< /ATTRIBUTE-SET>
< /TRIAL>
We start to set up the input structure with the top
node which is labeled with the lexicalization of the
type or in seldom cases with elision, when the type
is not in the attribute set. Then we look up in the re-
ferring expression model which determiner the per-
man
the wearing
glasses
with
beard
a dark
mod post_mod post_mod
dobj compl
det mod
Figure 1: The input to the graph transducer
NP
PP
NP
beard
a
dark
withVP
NP
glasses
wearing
the man
b
b
b
b
b
b
bb
b
Figure 2: The output of the graph transducer
son prefers. If she prefers any then a node is build,
labeled with the determiner and connected with an
edge to the type node. After that we add the lexi-
calized values of that attributes which are nearly al-
ways directly attached to the type node such as age
in the people domain or colour and size in the fur-
niture domain. Then the program searches in the
model the syntactic annotations of attribute combi-
nations. If IS-FP has build the referring expression
then it starts to search in the trail selected by the
nearest neighbour algorithm otherwise it calculates
the closest due to the dice metric. In our example
IS-FP might have build as well the given combi-
nation since it is equal to the attribute set of trail
s81t21. Then the program would select the syntactic
part t21b first and adapt the value of the node label
white to dark. After that the the syntactic part t21a
would be selected since the attribute hasGlasses is
still not covered in the structure. This part does not
need any adaption. Figure 1 shows the result of the
process.
2.3 Realization of the Word String
For the realization, we use a handcrafted grammar
that generates out of the dependency trees roughly
209
deep morphologic structure / topologic graph. The
main task of the grammar is to determine the word
order. The grammar contains four main rule groups.
The vertical rules order the parent in relation to
one of its dependent. The horizontal rules oder
two childs. The constituent creation rules build
constituents and the constituent adjoin rules adjoins
constituents with constituents. Special consideration
needed the order of prepositional constituents after
the type and the adjective before the type. The pre-
postional constituents are order because of the order
of the prepostions in the corpus. In order to be able
to derive the order of the adjective, we used the func-
tional class of the adjectives. Halliday (1994) pro-
poses for English, the classes deictic (this, those, ...),
numerative (many, second, , ...), epithet (old, blue,
...), and classifier (vegetarian, Spanish, ...). The or-
der of the adjectives in a noun phrase is in the given
order of the classes. In the lexicon entry of the ad-
jectives, we store only a number between one and
four which refers to the adjective class.
Table 5 shows the result for the TUNA-R task.
The system was developed only by looking on the
training data without any consideration of the devel-
opment data as well without any annotation of the
syntax of the development data. We used as guide
for the optimization cross validation of training data.
Set Accuracy String-Edit Distance
Furniture 35 % 3,163
People 22,06 % 3,647
Total 28,53 3,405
Table 4: Results for the TUNA-R Task
3 IS-FP-GT: The Combination of
Attribute Selection and Realization
The only change, we made in compare to IS-FP is
that we switched off the feature to add the most
similar referring expressions of another human from
the training set for the nearest neighbour evaluation
since the results have been lower. The reason for this
is that other human preferes similar attributes but the
individual preferences such as the chosen words and
syntax of the other human is different. Table 5 shows
the results.
4 Conclusion
The IS-FP algorithm reproduces the imprint of hu-
man referring expressions. It generates combina-
Set Accuracy String-Edit Distance
Furniture 15 % 3,8625
People 8,82 % 4,764
Total 11,91 4,313
Table 5: Results for the TUNA-REG Task
tions such as the x-dimension and y-dimension. El-
ements of a combination have not to occur always
together, however they tent to occur together. This
is an advantage over incremental algorithms which
might have to include other attributes ordered be-
tween elements of a combination. FP has the advan-
tage over its predecessor FBN to generate expres-
sions which are additionally mostly equal in respect
to the length to human referring expressions, it en-
larges automatically the training set for an individ-
ual human and it takes into account properties of the
domain like the frequency of the attributes.
References
B. Bohnet and R. Dale. 2005. Viewing referring expres-
sion generation as search. In IJCAI, pages 1004?1009.
B. Bohnet, A. Langjahr, and L. Wanner. 2000. A De-
velopment Environment for an MTT-Based Sentence
Generator. In Proceedings of the First International
Natural Language Generation Conference.
B. Bohnet. 2006. Textgenerierung durch Transduk-
tion linguistischer Strukturen. Ph.D. thesis, University
Stuttgart.
B. Bohnet. 2007. IS-FBN, IS-FBS, IS-IAC: The Adapta-
tion of Two Classic Algorithms for the Generation of
Referring Expressions in order to Produce Expressions
like Humans Do. In MT Summit XI, UCNLG+MT,
pages 84?86.
R. Dale and E. Reiter. 1995. Computational Interpreta-
tions of the Gricean Maxims in the Generation of Re-
ferring Expressions. Cognitive Science, 19:233?263.
M. A. K. Halliday. 1994. An Introduction to Functional
Grammar. Edward Arnold, London.
F. Lareau and L. Wanner. 2007. Towards a Generic Mul-
tilingual Dependency Grammar for Text Generation.
In GEAF-07, Palo Alto.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press, Albany.
S. Mille and L. Wanner. 2008. Making Text Resources
Available to the Reader: The Case of Patent Claims.
In LREC, Morocco, Marrakech.
L. Wanner, B. Bohnet, N. Bouayad-Agha, F. Lareau,
A. Lohmeyer, and D. Nickla. 2007. On the Challenge
of Creating and Communicating Air Quality Informa-
tion. In In: Swayne A., Hrebicek J.(Eds.): Environ-
mental Software Systems Dimensions of Environmen-
tal Informatics. Vol.7.
210
The UMUS system for named entity generation at GREC 2010
Benoit Favre
LIUM, Universite? du Maine
72000 Le Mans, France
benoit.favre@gmail.com
Bernd Bohnet
Universita?t Stuttgart
Stuttgart, Germany
bohnet@informatik.uni-stuttgart.de
Abstract
We present the UMUS (Universite? du
Maine/Universita?t Stuttgart) submission
for the NEG task at GREC?10. We re-
fined and tuned our 2009 system but we
still rely on predicting generic labels and
then choosing from the list of expressions
that match those labels. We handled recur-
sive expressions with care by generating
specific labels for all the possible embed-
dings. The resulting system performs at a
type accuracy of 0.84 an a string accuracy
of 0.81 on the development set.
1 Introduction
The Named Entity Generation (NEG) task con-
sists in choosing a referential expression (com-
plete name, last name, pronoun, possessive pro-
noun, elision...) for all person entities in a text.
Texts are biographies of chefs, composers and in-
ventors from Wikipedia. For each reference, a list
of expressions is given from which the system has
to choose. This task is challenging because of the
following aspects:
1. The data is imperfect as it is a patchwork of
multiple authors? writing.
2. The problem is hard to handle with a classi-
fier because text is predicted, not classes.
3. The problem has a complex graph structure.
4. Some decisions are recursive for embedded
references, i.e. ?his father?.
5. Syntactic/semantic features cannot be ex-
tracted with a classical parser because the
word sequence is latent.
We do not deal with all of these challenges but
we try to mitigate their impact. Our system ex-
tends our approach for GREC?09 (Favre and Bon-
het, 2009). We use a sequence classifier to predict
generic labels for the possible expressions.
2 Labels for classification
Each referential expression (REFEX) is given a la-
bel consisting of sub-elements:
? The REG08 TYPE as given in the REFEX
(name, common, pronoun, empty...)
? The CASE as given in the REFEX (plain,
genitive, accusative...)
? If the expression is a pronoun, then one of
?he, him, his, who, whom, whose, that?, after
gender and number normalization.
? ?self? if the expression contains ?self?.
? ?short? if the expression is a one-word long
name or common name.
? ?nesting? if the expression is recursive.
For recursive expressions, a special handling is ap-
plied: All possible assignments of the embedded
entities are generated with labels corresponding
to the concatenation of the involved entities? la-
bels. If the embedding is on the right (left) side
of the expression, ?right? (?left?) is added to the
label. Non-sensical labels (i.e. ?he father?) are not
seen in the training data, and therefore not hypoth-
esized.
3 Features
Each reference is characterized with the following
features:
? SYNFUNC, SEMCAT, SYNCAT: syntactic
function, semantic category, syntactic cate-
gory, as given in REF node.
? CHANGE, CHANGE+SYNFUNC: previous
reference is for a different entity, possibly
with syntactic function.
? PREV GENDER NUMBER: if the refer-
ence is from a different entity, can be ?same?
or ?different?. The attribute is being com-
pared is ?male?, ?female? or ?plural?, deter-
mined by looking at the possible expressions.
? FIRST TIME: denotes if it?s the first time
that the entity is seen. For plural entities, the
entity is considered new if at least one of the
involved entities is new.
? BEG PARAGRAPH: the first entity of a
paragraph.
? {PREV,NEXT} PUNCT: the punctuation
immediately before (after) the entity. Can be
?sentence? if the punctuation is one of ?.?!?,
?comma? for ?,;?, ?parenthesis? for ?()[]?
and ?quote?.
? {PREV,NEXT} SENT: whether or not a sen-
tence boundary occurs after (before) the pre-
vious (next) reference.
? {PREV,NEXT} WORD {1,2}GRAM: cor-
responding word n-gram. Words are ex-
tracted up to the previous/next reference or
the start/end of a sentence, with parenthe-
sized content removed. Words are lower-
cased tokens made of letters and numbers.
? {PREV,NEXT} TAG: most likely part-of-
speech tag for the previous/next word, skip-
ping adverbs.
? {PREV,NEXT} BE: any form of the verb ?to
be? is used after (before) the previous (next)
reference.
? EMBEDS PREV: the entity being embedded
was referred to just before.
? EMBEDS ALL KNOWN: all the entities be-
ing embedded have been seen before.
4 Sequence classifier
We rely on Conditional Random Fields1 (Lafferty
et al, 2001) for predicting one label (as defined
previously) per reference. We lay the problem as
one sequence of decisions per entity to prevent, for
instance, the use of the same name twice in a row.
Last year, we generated one sequence per docu-
ment with all entities, but it was less intuitive. To
the features extracted for each reference, we add
the features of the previous and next reference, ac-
cording to label unigrams and label bigrams. The
c hyperparameter and the frequency cutoff of the
classifier are optimized on the dev set. Note that
1CRF++, http://crfpp.sourceforge.net
for processing the test set, we added the develop-
ment data to the training set.
5 Text generation
For each reference, the given expressions are
ranked by classifier-estimated posterior probabil-
ity and the best one is used for output. In case
multiple expressions have the same labeling (and
the same score), we use the longest one and iter-
ate through the list for each subsequent use (useful
for repeated common names). If an expression is
more than 4 words, it?s flagged for not being used
a second time (only ad-hoc rule in the system).
6 Results
Evaluation scores for the output are presented in
Table 1. The source code of our systems is made
available to the community at http://code.google
.com/p/icsicrf-grecneg.
Sys. T.acc Prec. Rec. S.acc Bleu Nist
Old 0.826 0.830 0.830 0.786 0.811 5.758
New 0.844 0.829 0.816 0.813 0.817 6.021
Table 1: Results on the dev set comparing our sys-
tem from last year (old) to the refined one (new),
according to REG08 TYPE accuracy (T.acc), pre-
cision and recall, String accuracy (S.acc), BLEU1
an NIST.
About 50% of the errors are caused by the se-
lection of pronouns instead of a name. The selec-
tion of the pronoun or name seems to depend on
the writing style since a few authors prefer nearly
always the name. The misuse of names instead
of pronouns is second most error with about 15%.
The complex structured named entities are respon-
sible for about 9% of the errors. The selection of
the right name such as given name, family name or
both seems to be more difficult. The next frequent
errors are confusions between pronouns, elisions,
common names, and names.
References
Benoit Favre and Bernd Bonhet. 2009. ICSI-CRF: The
Generation of References to the Main Subject and
Named Entities Using Conditional Random Fields.
In ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. Machine
Learning, pages 282?289.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 22?30,
Utica, May 2012. c?2012 Association for Computational Linguistics
Towards a Surface Realization-Oriented Corpus Annotation
Leo Wanner
ICREA and
Universitat Pompeu Fabra
Roc Boronat 138
Barcelona, 08018, Spain
leo.wanner@upf.edu
Simon Mille
Universitat Pompeu Fabra
Roc Boronat 138
Barcelona, 08018, Spain
simon.mille@upf.edu
Bernd Bohnet
Universita?t Stuttgart
IMS, Pfaffenwaldring 5b
Stuttgart, 70569, Germany
bohnet@ims.uni-
stuttgart.de
Abstract
Until recently, deep stochastic surface realiza-
tion has been hindered by the lack of seman-
tically annotated corpora. This is about to
change. Such corpora are increasingly avail-
able, e.g., in the context of CoNLL shared
tasks. However, recent experiments with
CoNLL 2009 corpora show that these popu-
lar resources, which serve well for other ap-
plications, may not do so for generation. The
attempts to adapt them for generation resulted
so far in a better performance of the realizers,
but not yet in a genuinely semantic generation-
oriented annotation schema. Our goal is to
initiate a debate on how a generation suit-
able annotation schema should be defined. We
define some general principles of a semantic
generation-oriented annotation and propose an
annotation schema that is based on these prin-
ciples. Experiments shows that making the
semantic corpora comply with the suggested
principles does not need to have a negative im-
pact on the quality of the stochastic generators
trained on them.
1 Introduction
With the increasing interest in data-driven surface
realization, the question on the adequate annota-
tion of corpora for generation also becomes increas-
ingly important. While in the early days of stochas-
tic generation, annotations produced for other ap-
plications were used (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Oh and Rudnicky, 2000; Langkilde-
Geary, 2002), the poor results obtained, e.g., by
(Bohnet et al, 2010) with the original CoNLL 2009
corpora, show that annotations that serve well for
other applications, may not do so for generation and
thus need at least to be adjusted.1 This has also
been acknowledged in the run-up to the surface re-
alization challenge 2011 (Belz et al, 2011), where
a considerable amount of work has been invested
into the conversion of the annotations of the CoNLL
2008 corpora (Surdeanu et al, 2008), i.e., PropBank
(Palmer et al, 2005), which served as the reference
treebank, into a more ?generation friendly? annota-
tion. However, all of the available annotations are to
a certain extent still syntactic. Even PropBank and
its generation-oriented variants contain a significant
number of syntactic features (Bohnet et al, 2011b).
Some previous approaches to data-driven genera-
tion avoid the problem related to the lack of seman-
tic resources in that they use hybrid models that im-
ply a symbolic submodule which derives the syntac-
tic representation that is then used by the stochas-
tic submodule (Knight and Hatzivassiloglou, 1995;
Langkilde and Knight, 1998). (Walker et al, 2002),
(Stent et al, 2004), (Wong and Mooney, 2007), and
(Mairesse et al, 2010) start from deeper structures:
Walker et al and Stent et al from deep-syntactic
structures (Mel?c?uk, 1988), and Wong and Mooney
and Mairesse et al from higher order predicate logic
structures. However, to the best of our knowledge,
1Trained on the original ConLL 2009 corpora, (Bohnet et al,
2010)?s SVM-based generator reached a BLEU score of 0.12 for
Chinese, 0.18 for English, 0.11 for German and 0.14 for Span-
ish. Joining the unconnected parts of the sentence annotations to
connected trees (as required by a stochastic realizer) improved
the performance to a BLEU score of 0.69 for Chinese, 0.66 for
English, 0.61 for German and 0.68 for Spanish.
22
none of them uses corpora annotated with the struc-
tures from which they start.
To deep stochastic generation, the use of hybrid
models is not an option and training a realizer on
syntactically-biased annotations is highly problem-
atic in the case of data-to-text NLG, which starts
from numeric time series or conceptual or seman-
tic structures: the syntactic features will be simply
not available in the input structures at the moment
of application.2. Therefore, it is crucial to define a
theoretically sound semantic annotation that is still
good in practical terms.
Our goal is thus to discuss some general prin-
ciples of a semantic generation-oriented annotation
schema and offer a first evaluation of its possible
impact on stochastic generation. Section2 details
what kind of information is available respectively
not available during data-to-text generation. Sec-
tion 3 states some general principles that constrain
an adequate semantic representation, while Section
4 formally defines their well-formedness. Section 5
reports then on the experiments made with the pro-
posed annotation, and Section6 offers some conclu-
sions.
2 What can we and what we cannot count
on?
In data-to-text or ontology-to-text generation, with
the standard content selection?discourse structur-
ing?surface generation pipeline in place, and no
hard-wired linguistic realization of the individual
chunks of the data or ontology structure, the input
to the surface realization module can only be an ab-
stract structure that does not contain any syntactic
(and even lexical) information. Conceptual graphs
in the sense of Sowa (Sowa, 2000) are structures of
this kind;3 see Figure 1 for illustration (?Cmpl? =
?Completion?, ?Rcpt? = ?Recipient?, ?Strt? = ?Start?,
?Attr? = Attribute, ?Chrc? = ?Characteristic?, and
?Amt? = ?Amount?). Content selection accounts for
the determination of the content units that are to be
communicated and Discourse Structuring for the de-
limitation of Elementary Discourse Units (EDUs)
2Even though in this article we are particularly interested in
data-to-text generation, we are convinced that clean semantic
and syntactic annotations also facilitate text-to-text generation.
3But note that this can be any other content structure.
and their organization and for the discursive rela-
tions between them (e.g., Bcas (Because) in the Fig-
ure).
In particular, such a structure cannot contain:
? non-meaningful nodes: governed prepositions
(BECAUSE of, CONCENTRATION of), auxil-
iaries (passive be), determiners (a, the);
? syntactic connectors (between A and B), rela-
tive pronouns, etc.
? syntactic structure information: A modifies B,
A is the subject of B, etc.
In other words, a deep stochastic generator has
to be able to produce all syntactic phenomena from
generic structures that guarantee a certain flexibil-
ity when it comes to their surface form (i.e., without
encoding directly this type of syntactic information).
For instance, a concentration of NO2 can be realized
as a NO2 concentration, between 23h00 and 00h00
as from 23h00 until 00h00, etc. This implies that
deep annotations as, for instance, have been derived
so far from PennTreeBank/PropBank, in which ei-
ther all syntactic nodes of the annotation are kept
(as in (Bohnet et al, 2010)) or only certain syntac-
tic nodes are removed (as THAT complementizers
and TO infinitives in the shared task 2011 on sur-
face realization (Belz et al, 2011)) still fall short
of a genuine semantic annotation. Both retain a
lot of syntactic information which is not accessible
in genuine data-to-text generation: nodes (relative
pronouns, governed prepositions and conjunctions,
determiners, auxiliaries, etc.) and edges (relative
clause edges, control edges, modifier vs. argumen-
tal edges, etc.).
This lets us raise the question how the annotation
policies should look like to serve generation well
and to what extent existing resources such as Prop-
Bank comply with them already. We believe that
the answer is critical for the future research agenda
in generation and will certainly play an outstanding
role in the shared tasks to come.
In the next section, we assess the minimal princi-
ples which the annotation suitable for (at least) data-
to-text generation must follow in order to lead to
a core semantic structure. This core structure still
ignores such important information as co-reference,
23
Figure 1: Sample conceptual structure as could be produced by text planning (Because of a concentration of NO2 of
13?g/m3, the NO2 threshold value was exceeded between 23h00 and 00h00)
scope, presupposition, etc.: this information is ob-
viously necessary, but it is not absolutely vital for
a sufficient restriction of the possible choices faced
during surface generation. Further efforts will be re-
quired to address its annotation in appropriate depth.
3 The principles of generation-suitable
semantic annotation
Before talking about generation-suitable annotation,
we must make some general assumptions concern-
ing NLG as such. These assumptions are necessary
(but might not always be sufficient) to cover deep
generation in all its subtleties: (i) data-to-text gener-
ation starts from an abstract conceptual or semantic
representation of the content that is to be rendered
into a well-formed sentence; (ii) data-to-text gener-
ation is a series of equivalence mappings from more
abstract to more concrete structures, with a chain of
inflected words as the final structure; (iii) the equiva-
lence between the source structure Ss and the target
structure St is explicit and self-contained, i.e., for
the mapping from Ss to St, only features contained
in Ss and St are used. The first assumption is in
the very nature of the generation task in general; the
second and the third are owed to requirements of sta-
tistical generation (although a number of rule-based
generators show these characteristics as well).
The three basic assumptions give rise to the fol-
lowing four principles.
1. Semanticity: The semantic annotation must cap-
ture the meaning and only the meaning of a given
sentence. Functional nodes (auxiliaries, determin-
ers, governed conjunctions and prepositions), node
duplicates and syntactically-motivated arcs should
not appear in the semantic structure: they re-
flect grammatical and lexical features, and thus al-
ready anticipate how the meaning will be worded.
For example, meet-AGENT?the (directors), meet-
LOCATION?in (Spain), meet-TIME?in (2002)
cited in (Buch-Kromann et al, 2011) as semantic
annotation of the phrase meeting between the direc-
tors in Spain in 2002 in the Copenhagen Depen-
dency Treebank does not meet this criterion: the,
and both ins are functional nodes. Node dupli-
cates such as the relative pronoun that in the Prop-
Bank annotation (But Panama illustrates that their
their substitute is a system) that?R-A0-produces
(an absurd gridlock) equally reflect syntactic fea-
tures, as do syntactically-motivated arc labels of the
kind ?R(elative)-A0?.
The PropBank annotation of the sentence cited
above also intermingles predicate-argument rela-
tions (?Ai?) with syntactico-functional relations
(?AM-MNR?): gridlock?AM-MNR?absurd. The
predicate-argument analysis of modifiers suggests
namely that they are predicative semantemes that
take as argument the node that governs them
in the syntactic structure; in the above struc-
ture: absurd?A1?gridlock. This applies also
to locatives, temporals and other ?circumstan-
tials?, which are most conveniently represented
as two-place semantemes: house?A1?location?
A2?Barcelona, party?A1?time?A2?yes-
terday, and so on. Although not realized at the sur-
face, location, time, etc. are crucial.
24
2. Informativity: A propositional semantic annota-
tion must be enriched by information structure fea-
tures that predetermine the overall syntactic struc-
ture (paratactic, hypotactic, parenthetical, . . . ), the
internal syntactic structure (subject/object, clefted or
not, any element fronted or not, etc.), determiner dis-
tribution, etc. in the sentence. Otherwise, it will be
always underspecified with respect to its syntactic
equivalence in that, as a rule, a single semantic struc-
ture will correspond to a number of syntactic struc-
tures. This is not to say that with the information
structure in place we will always achieve a 1:1 cor-
respondence between the semantic and syntactic an-
notations; further criteria may be needed?including
prosody, style, presupposedness, etc. However, in-
formation structure is crucial.
The most relevant information structure features
are those of Thematicity, Foregroundedness and
Givenness.4
Thematicity specifies what the utterance states
(marked as rheme) and about what it states it
(marked as theme).5 Theme/rheme determines, in
the majority of cases, the subject-object structure
and the topology of the sentence. For instance,6
[John]theme?A1?[see?A2?Maria]rheme may
be said to correspond to John?subject?see?
dir.obj?Maria and [John?A1?see]rheme?A2
?[Maria]theme to John ?obj?seepass?subject
?Maria. For the generation of relative sentence
structures such as John bought a car which was old
and ugly, we need to accommodate for a recursive
definition of thematicity: [John]theme?A1?[buy?
A2?[c1 : car]theme?A1?[old]rheme; c1?A1
?[ugly]rheme]rheme.7 With no recursive (or sec-
ondary in Mel?c?uk?s terms) thematicity, we would
4We use mainly the terminology and definitions (although in
some places significantly simplified) of (Mel?c?uk, 2001), who,
to the best of our knowledge, establishes the most detailed cor-
relation between information structure and syntactic features.
5Similar notions are topic/focus (Sgall et al, 1986) and
topic/comment (Gundel, 1988).
6As in PropBank, we use ?Ai? as argument labels of predica-
tive lexemes, but for us, ?A1? stands for the first argument, ?A2?
for the second argument, etc. That is, in contrast to PropBank,
we do not support the use of ?A0? to refer to a lexeme?s exter-
nal argument since the distinction between external and internal
arguments is syntactic.
7c1 is a ?handle? in the sense of Minimal Recursion Seman-
tics (Copestake et al, 1997).
get John bought an old and ugly car.8
It is quite easy to find some counter-examples
to the default theme/rheme?syntactic feature cor-
relation, in particular in the case of questions
and answers. For instance, the neutral answer
to the question What will John bake tomorrow?,
John will bake a cake, would be split as follows:
[John?A1?bake]theme ?A2?[cake]rheme. In
this case, the main verb at the surface, bake, is in-
cluded in the theme and not in the rheme. Consider
also the sentence In a cross-border transaction, the
buyer is in a different region of the globe from the
target, where the main theme is in a cross-border
transaction, i.e., not the subject of the sentence (with
the subject the buyer being the embedded theme of
the main rheme). In these cases, the correlation is
more complex, but it undoubtedly exists and needs
to be distilled during the training phase.
Foregroundedness captures the ?prominence?
of the individual elements of the utterance for
the speaker or hearer. An element is ?fore-
grounded? if it is prominent and ?backgrounded?
if it is of lesser prominence; elements that are
neither foregrounded nor backgrounded are ?neu-
tral?. A number of correlations can be iden-
tified: (i) a ?foregrounded? A1 argument of a
verb will trigger a clefting construction; e.g.,
[John]foregr;theme?A1?[see?A2?Maria]rheme
will lead to It was John who saw Maria; similarly,
[John?A1?bake]foregr;theme ?A2?[cake]rheme
will lead to What John will bake is a cake; (ii) a
?foregrounded? A2 argument of a verb will corre-
spond to a clefting construction or a dislocation: It
was Maria, whom John saw; (iii) a ?foregrounded?
A1 or A2 argument of a noun will result in an argu-
ment promotion, as, e.g., John?s arrival (instead of
arrival of John); (iv) a ?foregrounded? circumstan-
tial will be fronted: Under this tree he used to rest;
(v) marking a part of the semantic structure as ?back-
grounded? will lead to a parenthetical construction:
John (well known among the students and professors
alike) was invited as guest speaker. If no elements
8We believe that operator scopes (e.g., negations and quan-
tifiers) can, to a large extent, be encoded within the thematic
structure; see (Cook and Payne, 2006) for work in the LFG-
framework on German, which provides some evidence for this.
However, it must be stated that very little work has been done
on the subject until now.
25
are marked as foregrounded/backgrounded, the de-
fault syntactic structure and the default word order
are realized.
Givenness captures to what extent an information
element is present to the hearer. The elementary
givenness markers ?given? and ?new? correlate in
syntax with determiner distribution. Thus, the ?new?
marker of an object node will often correspond to
an indefinite or zero determiner of the correspond-
ing noun: A masked man was seen to enter the
bank (man is newly introduced into the discourse).
The ?given? marker will often correlate with a defi-
nite determiner: The masked man (whom a passer-
by noticed before) was seen to enter the bank. To
distinguish between demonstratives and definite de-
terminers, a gradation of givenness markers as sug-
gested by Gundel et al (Gundel et al, 1989) is nec-
essary: ?given1/2/3?.
As already for Thematicity, numerous examples
can be found where the giveness-syntactic feature
correlation deviates from the default correlation. For
instance, in I have heard a cat, the cat of my neigh-
bour, there would be only one single (given) node
cat in the semantic structure, which does not pre-
vent the first appearance of cat in the sentence to be
indefinite. In A warrant permits a holder that he ac-
quire one share of common stock for $17.50 a share,
warrant is given, even if it is marked by an indefinite
determiner. Again, this only shows the complexity
of the annotation of the information structure, but it
does not call into question the relevance of the infor-
mation structure to NLG.
As one of the few treebanks, the Prague Depen-
dency Treebank (PDT) (Hajic? et al, 2006) accounts
for aspects of the information structure in that it an-
notates Topic-Focus Articulation in terms of various
degrees of contextual boundness, which are corre-
lated with word order and intonation (Mikulova? et
al., 2006, p.152).
3. Connectivity: The semantic annotation must
ensure that the annotation of an utterance forms
a connected structure: without a connected struc-
ture, generation algorithms that imply a traver-
sal of the input structure will fail to generate a
grammatical sentence. For instance, the Prop-
Bank annotation of the sentence But Panama il-
lustrates that their substitute is a system that pro-
duces an absurd gridlock (here shown partially)
does not comply with this principle since it con-
sists of four unconnected meaning-bearing sub-
structures (the single node ?but? and the subtrees
governed by ?illustrate?, ?produce? and ?substi-
tute?): but | Panama?A0?illustrate?A1?that |
system?A0?produce?A1?gridlock?AM-
MNR?absurd | substitute?A0?their.
4 Outline of a Generation-Oriented
Annotation
The definitions below specify the syntactic well-
formedness of the semantic annotation. They do not
intend to and cannot substitute a detailed annotation
manual, which is indispensable to achieve a seman-
tically accurate annotation.
Definition 1: [Semantic Annotation of a sentence
S, SA]
SA of S in the text T in language L is a pair
?Ssem, Sinf ?, where Ssem is the semantic structure
of S (ensuring Semanticity and Connectivity), and
Sinf is the information structure of S (ensuring In-
formativity).
Let us define each of the two structures of the se-
mantic annotation in turn.
Definition 2: [Semantic Structure of a sentence S,
Ssem]
Ssem of S is a labeled acyclic directed connected
graph (V,E, ?, ?) defined over the vertex label al-
phabet L := LS ?MC ?MT ?Mt ?Ma (such that
LS ? (MC ?MT ?Mt ?Ma) = ?) and the edge
label alphabet Rsem ? {A1, A2, A3, A4, A5, A6},
with
? V as the set of vertices;
? E as the set of directed edges;
? ? as the function that assigns each v ? V an ele-
ment l ? L;
? ? as the function that assigns each e ? E an ele-
ment a ? Rsem;
? LS as the meaning bearing lexical units (LUs) of
S;
? MC ? {LOC, TMP, EXT, MNR, CAU, DIR,
SPEC, ELAB, ADDR} as the ?circumstantial meta
semantemes? (with the labels standing for ?locative?,
?temporal?, ?temporal/spatial extension?, ?manner?,
?cause?, ?direction?, ?specification?, ?elaboration?,
and ?addressee?);
? MT ? {TIME, TCST} as the ?temporal meta se-
mantemes? (with the labels standing for ?time? and
26
?time constituency?);
? Mt ? {past?, present?, future?} as the ?time
value semantemes?;
? Ma ? {imperfective?, durative?,
semelfactive?, iterative?, telic?, atelic?,
nil?} as the ?aspectual value semantemes?9
such that the following conditions hold:
(a) the edges in Ssem are in accordance with the va-
lency structure of the lexical units (LUs) in S: If
lp?Ai?lr ? Ssem (lp, lr ? LS , i ? {1, 2, 3, . . .}),
then the semantic valency of lp possesses at least i
slots and lr fulfils the semantic restrictions of the i-
th slot
(b) the edges in Ssem are exhaustive: If ?(nr) =
lr ? L instantiates in S the i-th semantic argument
of ?(np) = lp, then lp?Ai?lr ? Ssem
(c) Ssem does not contain any duplicated argument
edges: If ?(np)?Ai??(nr), ?(np) ?Aj? ?(nq) ?
Ssem (with np, nr, nq ? N ) then Ai 6= Aj and nr 6=
nq
(d) circumstantial LUs in S are represented in Ssem
by two-place meta-semantemes: If lr ? Lsem is
a locative/temporal/ manner/cause/direction/specifi-
cation/elaboration/addressee LU and in the syntac-
tic dependency structure of S, lr modifies lp, then
lr?A2-?-A1?lp ? Ssem (with ? ? LOC, TMP,
MNR, CAU, DIR, SPEC, ELAB, ADDR)
(e) verbal tense is captured by the two-place predi-
cate TIME: If lp ? Lsem is a verbal LU then lr?A2-
TIME-A1?lp ? Ssem, with lr ? Mt
(f) verbal aspect is captured by the two-place predi-
cate TCST: If lp ? Lsem is a verbal LU then lr?A2-
TCST-A1?lp ? Ssem, with lr ? Ma.
(a) implies that no functional node is target of an ar-
gument arc: this would contradict the semantic va-
lency conditions of any lexeme in S. (b) ensures that
no edge in Ssem is missing: if a given LU is an argu-
ment of another LU in the sentence, then there is an
edge from the governor LU to the argument LU. (c)
means that no predicate in Ssem possesses in S two
different instances of the same argument slot. The
circumstantial meta-semantemes in (d) either cap-
ture the semantic role of a circumstantial that would
otherwise get lost or introduce a predicate type for a
name. Most of the circumstantial meta-semantemes
9The aspectual feature names are mainly from (Comrie,
1976).
reflect PropBank?s modifier relations ?AM-X? (but in
semantic, not in syntactico-functional terms), such
that their names are taken from PropBank or are in-
spired by PropBank. LOC takes as A1 a name of a
location of its A2: Barcelona?A1-LOC-A2?live-
A1?John; TMP a temporal expression: yesterday
?A1-TMP-A2?arrive-A1?John; MNR a man-
ner attribute: player?A1-MNR-A2?solo; CAU
the cause: accept?A1-CAU-A2?reason in This
is the reason why they accepted it; DIR a spa-
tial direction: run around?A2-DIR-A1?circles in
I?m running around in circles; SPEC a ?context
specifier?: should?A2-SPEC-A1?thought in You
should leave now, just a thought; ELAB an appos-
itive attribute company?A1-ELAB-A2 ?bank in
This company, a bank, closed; and ADDR direct ad-
dress: come?A1-ADDR-A2?John in John, come
here!
Definition 3: [Information Structure of a sen-
tence S, Sinf ]
Let Ssem of S be defined as above. Sinf of S is
an undirected labeled hypergraph (V, I) with V as
the set of vertices of S and I the set of hyperedges,
with I := {themei (i = 1, 2, . . . ), rhemei (i = 1, 2,
. . . ), givenj (j = 1,. . . ,3), new, foregrounded, back-
grounded}. The following conditions apply:
(a) thematicity is recursive, i.e., a thematic hyper-
edge contains under specific conditions embedded
theme/rheme hyperedges: If ?nk ? themei such that
?(nk) = lp is a verbal lexeme and lp -A1?lr ?
Ssem, then ? themei+1, rhemei+1 ? themei
(b) theme and rheme hyperedges of the same re-
cursion level, given and new hyperedges, and fore-
grounded and backgrounded hyperedges are dis-
joint: themei ? rhemei = ? (i = 1, 2, . . . ), givenj
? new = ? (j = 1,. . . ,3), foregr. ? backgr. = ?
(c) any node in Ssem forms part of either theme or
rheme: ?np ? Ssem : np ? theme1 ? rheme1.
Consider in Figure 2 an example of SA with its
two structures.10 All syntactic nodes have been re-
moved, and all the remaining nodes are connected
in terms of a predicate?argument structure, with no
use of any syntactically motivated edge, so as to en-
sure that the structure complies with the Semantic-
ity and Connectivity principles. Figure 2 illustrates
the three main aspects of Informativity: (i) thematic-
10The meta-semanteme TCST is not shown in the figure.
27
ity, with the two theme/rheme oppositions; (ii) fore-
groundedness, with the backgrounded part of the
primary rheme; and (iii) givenness, with the attribute
givenness and the value 2 on the node program. The
information structure constrains the superficial real-
ization of the sentence in that the primary theme will
be the subject of the sentence, and the main node of
the primary rheme pointing to it will be the main
verb of the same sentence. The secondary theme
and rheme will be realized as an embedded sen-
tence in which you will be the subject, that is, forc-
ing the realization of a relative clause. However, it
does not constrain the appearance of a relative pro-
noun. For instance: we obtained technologies you
do not see anywhere else and we obtained technolo-
gies that you do not see anywhere else are possible
realizations of this structure. Leaving the relative
pronoun in the semantic structure would force one
realization to occur when it does not have to (both
outputs are equally correct and meaning-equivalent
to the other). Similarly, marking the Soviet space
program as backgrounded leaves some doors open
when it comes to surface realization: Cosmos, the
Soviet space program vs. Cosmos (the Soviet space
program) vs. the Soviet space program Cosmos (if
Cosmos is backgrounded too) are possible realiza-
tions of this substructure.
ELABORATION is an example of a meta-node
needed to connect the semantic structure: Cosmos
and program have a semantic relation, but neither is
actually in the semantic frame of the other?which
is why the introduction of an extra node cannot be
avoided. In this case, we could have a node NAME,
but ELABORATION is much more generic and can
actually be automatically introduced without any ad-
ditional information.
5 Experiments
Obviously, the removal of syntactic features from
a given standard annotation, with the goal to ob-
tain an increasingly more semantic annotation, can
only be accepted if the quality of (deep) stochas-
tic generation does not unacceptably decrease. To
assess this aspect, we converted automatically the
PropBank annotation of the WSJ journal as used in
the CoNLL shared task 2009 into an annotation that
complies with all of the principles sketched above
for deep statistical generation and trained (Bohnet
et al, 2010)?s generator on this new annotation.11
For our experiments, we used the usual training,
development and test data split of the WSJ cor-
pus (Langkilde-Geary, 2002; Ringger et al, 2004;
Bohnet et al, 2010); Table 1 provides an overview
of the used data.
set section # sentences
training 2 - 21 39218
development 24 1334
test 23 2400
Table 1: Data split of the used data in the WSJ Corpus
The resulting BLEU score of our experiment was
0.64, which is comparable with the accuracy re-
ported in (Bohnet et al, 2010) (namely, 0.659), who
used an annotation that still contained all functional
nodes (such that their generation task was consider-
ably more syntactic and thus more straightforward).
To assess furthermore whether the automatically
converted PropBank already offers some advantages
to other applications than generation, we used it in a
semantic role labeling (SRL) experiment with (Bjo?
rkelund et al, 2010)?s parser. The achieved overall
accuracy is 0.818, with all analysis stages (including
the predicate identification stage) being automatic,
which is a rather competitive figure. In the original
CoNLL SRL setting with Oracle reading, an accu-
racy of 0.856 is achieved.
Another telling comparison can be made between
the outcomes of the First Surface Realization Shared
Task (Belz et al, 2011), in which two different
input representations were given to the competing
teams: a shallow representation and a deep repre-
sentation. The shallow structures were unordered
syntactic dependency trees, with all the tokens of
the sentence, and the deep structures were predicate-
argument graphs with some nodes removed (see
Section 2). Although the performance of shallow
generators was higher than the performance of the
deep generators (the StuMaBa shallow generator
(Bohnet et al, 2011a) obtained a BLEU score of
0.89, as opposed to 0.79 of the StuMaBa deep gen-
11Obviously, our conversion can be viewed only preliminary.
It does not take into account all the subtleties that need to be
taken account?for instance, with respect to the information
structure; see also Section 6.
28
Figure 2: Illustration of the semantic annotation of the sentence Through the development of Cosmos, the Soviet space
program, we obtained technologies you do not see anywhere else.
erator), the difference is not as striking as one would
expect.12
6 Conclusions
Our experiments and the Surface Realization Shared
Task 2011 suggest that making the deep annotation
more semantic does not necessarily imply an unsur-
mountable problem for stochastic generation. We
can thus conclude that deriving automatically a deep
semantic annotation from PropBank allowed us to
obtain very promising results, both for NLG and
SRL. By sticking to universal predicate-argument
structures, as PropBank does, we maintain the po-
tential of the corpus to be mapped to other, more id-
iosyncratic, annotations. Still, automatic conversion
will always remain deficient. Thus, a flawless iden-
tification of semantic predication cannot be guaran-
teed. For instance, when an actancial arc points to a
preposition, it is not clear how to deduce whether
this preposition is semantic or lexical. Also, the
treatment of phraseological nodes is problematic,
as is the annotation of a comprehensive informa-
12Note that our results mentioned above cannot be directly
compared with the StuMaBa results during the Generation
Challenges 2011 because the realizers are different.
tion structure: the criteria for the automatic deriva-
tion of the information structure from the syntactic
structure and the topology of a sentence can only
be superficial and likely to be even less efficient in
longer and complex sentences. The annotation of
intersentential coreferences and the identification of
gapped elements are further major hurdles for an au-
tomatic derivation of a truly semantic resource. As
a consequence, we believe that new annotation poli-
cies are needed to obtain a high quality semantic re-
source. The best strategy is to start with a conver-
sion of an existing semantically annotated treebank
such as PropBank, revising and extending the result
of this conversion in a manual concerted action?
always following truly semantic annotation policies.
Acknowledgments
We would like to thank the reviewers for their valu-
able comments and suggestions and the Penn Tree-
bank/PropBank/NomBank team, without whom our
experiments would not be possible. Many thanks
also to Mike White for the useful discussions on
some of the topics discussed in the paper. Although
we might still not agree on all of the details, he
made us see the task of generation-oriented annota-
29
tion from another angle and revise some of our ini-
tial assumptions.
References
S. Bangalore and O. Rambow. 2000. Exploiting a Proba-
bilistic Hierarchical Model for Generation. In Proc. of
COLING ?00.
A. Belz, M. White, D. Espinosa, D. Hogan, and A. Stent.
2011. The First Surface Realization Shared Task:
Overview and Evaluation Results. In ENLG11.
A. Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic de-
pendency parser. In Proc. of COLING ?10: Demon-
stration Volume.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Proc. of COL-
ING ?10.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011a.
<STUMABA>: From Deep Representation to Sur-
face. In ENLG11.
B. Bohnet, S. Mille, and L. Wanner. 2011b. Statisti-
cal language generation from semantic structures. In
Proc. of International Conference on Dependency Lin-
guistics.
M. Buch-Kromann, M. Gylling-J?rgensen, L. Jelbech-
Knudsen, I. Korzen, and H. Mu?ller. 2011. The
inventory of linguistic relations used in the Copen-
hagen Dependency Treebanks. www.cbs.dk/ con-
tent/download/149771/1973272/file.
B. Comrie. 1976. Aspect. Cambridge University Press,
Cambridge.
P. Cook and J. Payne. 2006. Information Structure and
Scope in German. In LFG06.
A. Copestake, D. Flickinger, and I. Sag. 1997. Minimal
recursion semantics. Technical report, CSLI, Stanford
University, Stanford.
J. Gundel, N. Hedberg, and R. Zacharski. 1989. Give-
ness, Implicature and Demonstrative Expressions in
English Discourse. In CLS-25, Part II (Parasession
on Language in Context), pages 89?103. Chicago Lin-
guistics Society.
J.K. Gundel. 1988. ?Universals of Topic-Comment
Structure?. In M. Hammond, E. Moravc?ik, and
J. Wirth, editors, Studies in Syntactic Typology. John
Benjamins, Amsterdam & Philadelphia.
J. Hajic?, J. Panevova?, E. Hajic?ova?, P. Sgall, P. Pa-
jas, J. S?te?pa?nek, J. Havelka, M. Mikulova?, and
Z. Z?abokrtsky?. 2006. Prague Dependency Treebank
2.0.
K. Knight and V. Hatzivassiloglou. 1995. Two-level,
many paths generation. In Proc. of ACL ?95.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proc. of
COLING/ACL ?98.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proc. of 2nd INLG Conference.
F. Mairesse, M Gas?ic?, F. Juric???c?, S Keizer, B. Thomson,
K. Yu, and S. Young. 2010. Phrase-based statistical
language generation using graphical models and active
learning. In Proc. of ACL ?10.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY Press, Albany.
I.A. Mel?c?uk. 2001. Communicative Organization
in Natural Language (The Semantic-Communicative
Structure of Sentences). Benjamins Academic Pub-
lishers, Amsterdam.
M. Mikulova? et al 2006. Annotation on
the tectogrammatical level in the Prague
Dependency Treebank: Reference book.
www.cbs.dk/content/download/149771/ 1973272/file.
A.H. Oh and A.I. Rudnicky. 2000. Stochastic language
generation for spoken dialogue systems. In Proc. of
ANL/NAACL Workshop on Conversational Systems.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
E. Ringger, M. Gamon, R.C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proceedings of COLING,
pages 673?679.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in its Semantic and Pragmatic As-
pects. Reidel Publishing Company, Dordrecht.
J. F. Sowa. 2000. Knowledge Representation: Logi-
cal, Philosophical, and Computational Foundations.
Brooks Cole Publishing Co., Pacific Grove, CA, USA.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In Proc. of ACL ?04.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
CoNLL-2008.
M.A. Walker, O.C. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken dialogue using
boosting. Computer Speech and Language, 16:409?
433.
Y.W. Wong and R.J. Mooney. 2007. Generation by in-
verting a semantic parser that uses statistical machine
translation. In Proc. of the HLT Conference.
30
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 136?140,
Utica, May 2012. c?2012 Association for Computational Linguistics
The Surface Realisation Task: Recent Developments and Future Plans
Anja Belz
Computing, Engineering and Maths
University of Brighton
Brighton BN1 4GJ, UK
a.s.belz@brighton.ac.uk
Bernd Bohnet
Institute for Natural Language Processing
University of Stuttgart
70174 Stuttgart
bohnet@ims.uni-stuttgart.de
Simon Mille, Leo Wanner
Information and Communication Technologies
Pompeu Fabra University
08018 Barcelona
<firstname>.<lastname>@upf.edu
Michael White
Department of Linguistics
Ohio State University
Columbus, OH, 43210, US
mwhite@ling.osu.edu
Abstract
The Surface Realisation Shared Task was first
run in 2011. Two common-ground input rep-
resentations were developed and for the first
time several independently developed surface
realisers produced realisations from the same
shared inputs. However, the input representa-
tions had several shortcomings which we have
been aiming to address in the time since. This
paper reports on our work to date on improv-
ing the input representations and on our plans
for the next edition of the SR Task. We also
briefly summarise other related developments
in NLG shared tasks and outline how the dif-
ferent ideas may be usefully brought together
in the future.
1 Introduction
The Surface Realisation (SR) Task was introduced
as a new shared task at Generation Challenges 2011
(Belz et al, 2011). Our aim in developing the SR
Task was to make it possible, for the first time, to
directly compare different, independently developed
surface realisers by developing a ?common-ground?
representation that could be used by all participat-
ing systems as input. In fact, we created two dif-
ferent input representations, one shallow, one deep,
in order to enable more teams to participate. Corre-
spondingly, there were two tracks in SR?11: In the
Shallow Track, the task was to map from shallow
syntax-level input representations to realisations; in
the Deep Track, the task was to map from deep
semantics-level input representations to realisations.
By the time teams submitted their system outputs,
it had become clear that the inputs required by some
types of surface realisers were more easily derived
from the common-ground representation than the in-
puts required by other types. There were other re-
spects in which the representations were not ideal,
e.g. the deep representations retained too many syn-
tactic elements as stopgaps where no deeper infor-
mation had been available. It was clear that the in-
put representations had to be improved for the next
edition of the SR Task. In this paper, we report on
our work in this direction so far and relate it to some
new shared task proposals which have been devel-
oped in part as a response to the above difficulties.
We discuss how these developments might usefully
be integrated, and outline plans for SR?13, the next
edition of the SR Task.
2 SR?11
The SR?11 input representations were created by
post-processing the CoNLL 2008 Shared Task
data (Surdeanu et al, 2008), for the preparation of
which selected sections of the WSJ Treebank were
converted to syntactic dependencies with the Pen-
nconverter (Johansson and Nugues, 2007). The
resulting dependency bank was then merged with
Nombank (Meyers et al, 2004) and Propbank
(Palmer et al, 2005). Named entity information
from the BBN Entity Type corpus was also incorpo-
rated. The SR?11 shallow representation was based
on the Pennconverter dependencies, while the deep
representation was derived from the merged Nom-
bank, Propbank and syntactic dependencies in a pro-
136
cess similar to the graph completion algorithm out-
lined by Bohnet (2010).
Five teams submitted a total of six systems to
SR?11 which we evaluated automatically using a
range of intrinsic metrics. In addition, systems were
assessed by human judges in terms of Clarity, Read-
ability and Meaning Similarity.
The four top-performing systems were all statis-
tical dependency realisers that do not make use of
an explicit, pre-existing grammar. By design, statis-
tical dependency realisers are robust and relatively
easy to adapt to new kinds of dependency inputs
which made them well suited to the SR?11 Task. In
contrast, there were only two systems that employed
a grammar, either hand-crafted or treebank-derived,
and these did not produce competitive results. Both
teams reported substantial difficulties in converting
the common ground inputs into the ?native? inputs
required by their systems.
The SR?11 results report pointed towards two
kinds of possible improvements: (i) introducing (ad-
ditional) tasks where performance would not depend
to the same extent on the relation between common-
ground and native inputs, e.g. a text-to-text shared
task on sentential paraphrasing; and (ii) improving
the representations themselves. In the remainder of
this paper we report on developments in both these
directions.
3 Towards SR?13
As outlined above, the first SR Shared Task turned
up some interesting representational issues that re-
quired some in-depth investigation. In the end, it
was this fact that led to the decision to postpone
the 2nd SR Shared Task until 2013 in order to al-
low enough time to address these issues properly. In
this section, we describe our plans for SR?13 to the
extent to which they have progressed.
3.1 Task definition
As in the first SR task, the participating teams will
be provided with annotated corpora consisting of
common-ground input representations and their cor-
responding outputs. Two kinds of input will be of-
fered: deep representations and surface representa-
tions. The deep input representations will be se-
mantic graphs; the surface representations syntactic
trees. Both will be derived from the Penn Treebank.
The task will consist in the generation of a text start-
ing from either of the input representations.
3.2 Changes to the input representations
During the working group discussions which fol-
lowed SR?11, it became apparent that the CoNLL
syntactic dependency trees overlaid with Prop-
bank/Nombank relations had turned out to be inade-
quate in various respects for the purpose of deriving
a suitable semantic representation. For instance:
? Governed prepositions are not distinguished
from semantically loaded prepositions in the
CoNLL annotation. In SR?11, only strongly
governed prepositions such as give something
TO someone were removed, but in many cases
the meaning of a preposition which introduces
an argument (of a verb, a noun, an adjective
or an adverb) clearly depends on the predicate:
believe IN something, account FOR some-
thing, etc. In those cases, too, the preposition
should be removed from the semantic annota-
tion, since the relisers have to be able to intro-
duce non-semantic features un-aided. On the
contrary, semantically loaded governed prepo-
sitions such as live IN a flat/ON a roof/NEXT
TO the main street etc. should be retained in
the annotation. These prepositions all receive
argumental arcs in PropBank/NomBank, so it
is not easy to distinguish between them. One
possibility would be to target a restricted list of
prepositions which are void of meaning most of
the time, and remove those prepositions when
they introduce arguments.
? The annotation of relative pronouns did not
survive the conversion of the original Penn
Treebank to the CoNLL format unscathed: the
antecedent of the relative pronoun is sometimes
lost or the relative pronoun is not annotated,
predominantly because the predicate which the
relative pronoun is an argument of was not con-
sidered to be a predicate by annotators, as in
the degree TO WHICH companies are irritated.
However, in the original constituency annota-
tion, the traces allow for retrieving antecedents
and semantic governors, hence using this orig-
137
inal annotation could be useful in order to get a
clean annotation of such phenomena.
Agreement has been reached on a range of other is-
sues, although the feasibility of implementing the
corresponding changes might have to be further
evaluated:
? Coordinations should be annotated in the se-
mantic representation with the conjunction as
the head of all the conjuncts. This treatment
would allow e.g. an adequate representation of
sharing of dependents among the conjuncts.
? The inversion of ?modifier? arcs and the intro-
duction of meta-semantemes would avoid an-
ticipating syntactic decisions such as the direc-
tion of non-argumental syntactic edges, and al-
low for connecting unconnected parts of the se-
mantic structures.
? In order to keep the scope of various phenom-
ena intact after inverting non-argumental edges,
we should explicitly mark the scope of e.g.
negations, quantifiers, quotation marks etc. as
attribute values on the nodes.
? Control arcs should be removed from the se-
mantic representation since they do not provide
information relevant at that level.
? Named entities will be further specified adding
a reduced set of named entity types from the
BBN annotations.
Finally, we will perform automatic and manual qual-
ity checks in order to ensure that the proposed
changes are adequately introduced in the annotation.
3.3 Evaluation
We will once again follow the main data set divi-
sions of the CoNLL?08 data (training set = WSJ Sec-
tions 02?21; development set = Section 24; test set =
Section 23), with the proviso that we have removed
300 randomly selected sentences from the develop-
ment set for use in human evaluations. Of these, we
used 100 sentences in SR?11 and will use a different
100 in SR?13.
Evaluation criteria identified as important for
evaluation of surface realisation output in previous
work include Adequacy (preservation of meaning),
Fluency (grammaticality/idiomaticity), Clarity, Hu-
manlikeness and Task Effectiveness. We will aim to
evaluate system outputs submitted by SR?13 partic-
ipants in terms of most of these criteria, using both
automatic and human-assessed methods.
As in SR?11, the automatic evaluation metrics (as-
sessing Humanlikeness) will be BLEU, NIST, TER
and possibly METEOR. We will apply text normal-
isation to system outputs before scoring them with
the automatic metrics. For n-best ranked system
outputs, we will again compute a single score for all
outputs by computing their weighted sum of their
individual scores, where a weight is assigned to a
system output in inverse proportion to its rank. For
a subset of the test data we may obtain additional al-
ternative realisations via Mechanical Turk for use in
the automatic evaluations.
We are planning to expand the range of human-
assessed evaluation experiments (assessing Ade-
quacy, Fluency and Clarity) to the following meth-
ods:
1. Preference Judgement Experiment (C2, C3):
Collect preference judgements using an exist-
ing evaluation interface (Kow and Belz, 2012)
and directly recruited evaluators. We will
present sentences in the context of a chunk of
5 consecutive sentences to the evaluators, and
ask for separate judgements for Clarity, Flu-
ency and Meaning Similarity.
2. HTER (Snover et al, 2006): In this evaluation
method, human evaluators are asked to post-
edit the output of a system, and the edits are
then categorised and counted. Crucial to this
evaluation method is the construction of clear
instructions for evaluators and the categorisa-
tion of edits. We will categorise edits as relat-
ing to Meaning Similarity, Fluency and/or Clar-
ity; we will also consider further subcategorisa-
tions.
We will once again provide evaluation scripts to par-
ticipants so they can perform automatic evaluations
on the development data. These scores serve two
purposes. Firstly, development data scores must be
included in participants? reports. Secondly, partici-
138
pants may wish to use the evaluation scripts in de-
veloping and tuning their systems.
We will report per-system results separately for
the automatic metrics (4 sets of results), and for the
human-assessed measures (2 sets of results). For
each set of results, we will report single-best and
n-best results. For single-best results, we may fur-
thermore report results both with and without miss-
ing outputs. We will rank systems, and report sig-
nificance of pairwise differences using bootstrap re-
sampling where necessary (Koehn, 2004; Zhang and
Vogel, 2010). We will separately report correlation
between human and automatic metrics, and between
different automatic metrics.
3.4 Assessing different aspects of realisation
separately
In addition, we will consider measuring different as-
pects of the realisation performance of participating
systems (syntax, word order, morphology) since a
system can perform well on one and badly on an-
other. For instance, a system might perform well
on morphological realisation while it has poor re-
sults on linearisation. We would like to capture this
fact. This may involve asking participating teams to
submit intermediate representations or identifiers to
identify the reference words. This more fine-grained
approach should help us to obtain a more precise
picture of the state of affairs in the field and could
help to reveal the respective strengths of different
surface realisers more clearly.
4 Related Developments
4.1 Syntactic Paraphrase Ranking
The new shared task on syntactic paraphrase ranking
described elsewhere in this volume (White, 2012) is
intended to run as a follow-on to the main surface
realisation shared task. Taking advantage of the hu-
man judgements collected to evaluate the surface re-
alisations produced by competing systems, the task
is to automatically rank the realisations that differ
from the reference sentence in a way that agrees with
the human judgements as often as possible. The task
is designed to appeal to developers of surface real-
isation systems as well as machine translation eval-
uation metrics. For surface realisation systems, the
task sidesteps the thorny issue of converting inputs
to a common representation. Developers of reali-
sation systems that can generate and optionally rank
multiple outputs for a given input will be encouraged
to participate in the task, which will test the system?s
ability to produce acceptable paraphrases and/or to
rank competing realisations. For MT evaluation
metrics, the task provides a challenging framework
for advancing automatic evaluation, as many of the
paraphrases are expected to be of high quality, dif-
fering only in subtle syntactic choices.
4.2 Content Selection Challenge
The new shared task on content selection has been
put forward (Bouayad-Agha et al, 2012) to initi-
ate work on content selection from a common, stan-
dardised semantic-web format input, and thus pro-
vide the context for an objective assessment of dif-
ferent content selection strategies. The task con-
sists in selecting the contents communicated in ref-
erence biographies of celebrities from a large vol-
ume of RDF-triples. The selected triples will be
evaluated against a gold triple selection set using
standard quality assessment metrics.
The task can be considered complementary to the
surface realisation shared task in that it contributes
to the medium-term goal of setting up a task that
covers all stages of the generation pipeline. In fu-
ture challenges, it can be explored to what extent and
how the output content plans can be mapped onto
semantic representations that serve as input to the
surface realisers.
5 Plans
We are currently working on the new improved
common-ground input representation scheme and
converting the data to the new scheme.
The provisional schedule for SR?13 looks as
follows:
Announcement and call for expres-
sions of interest:
6 July 2012
Preliminary registration and release
of description of new representations:
27 July 2012
Release of data and documentation: 2 Nov 2012
System Submission Deadline: 10 May 2013
Evaluation Period: 10 May?
10 Jul 2013
Provisional dates for results session: 8?9 Aug 2013
139
6 Conclusion
For a large number of NLP applications (among
them, e.g., text generation proper, summarisation,
question answering, and dialogue), surface realisa-
tion (SR) is a key technology. Unfortunately, so
far in nearly all of these applications, idiosyncratic,
custom-made SR implementations prevail. How-
ever, a look over the fence at the language analy-
sis side shows that the broad use of standard de-
pendency treebanks and semantically annotated re-
sources such as PropBank and NomBank that were
created especially with parsing in mind led to stan-
dardised high-quality off-the-shelf parser implemen-
tations. It seems clear that in order to advance the
field of surface realisation, the generation commu-
nity also needs adequate resources on which large-
scale experiments can be run in search of the surface
realiser with the best performance, a surface realiser
which is commonly accepted, follows general trans-
parent principles and is thus usable as plug-in in the
majority of applications.
The SR Shared Task aims to contribute to this
goal. On the one hand, it will lead to the creation
of NLG-suitable resources in that it will convert
the PropBank into a more semantic and more com-
pletely annotated resource. On the other hand, it will
offer a forum for the presentation and evaluation of
various approaches to SR and thus help us to search
for the best solution to the SR task with the greatest
potential to become a widely accepted off-the-shelf
tool.
Acknowledgments
We gratefully acknowledge the contributions to dis-
cussions and development of ideas made by the
other members of the SR working group: Miguel
Ballesteros, Johan Bos, Aoife Cahill, Josef van Gen-
abith, Pablo Gerva?s, Deirdre Hogan and Amanda
Stent.
References
Anja Belz, Michael White, Dominic Espinosa, Deidre
Hogan, Eric Kow, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG?11), pages 217?226. Association for Compu-
tational Linguistics.
Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner,
and Chris Mellish. 2012. Content selection from
semantic web data. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112, Tartu, Estonia.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Eric Kow and Anja Belz. 2012. LG-Eval: A toolkit
for creating online language evaluation experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In NAACL/HLT Workshop Frontiers in Corpus
Annotation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: A corpus annotated with
semantic roles. In Computational Linguistics Journal,
pages 71?105.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL?08), Manchester, UK.
Michael White. 2012. Shared task proposal: Syntac-
tic paraphrase ranking. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Ying Zhang and Stephan Vogel. 2010. Significance tests
of automatic machine translation evaluation metrics.
Machine Translation, 24:51?65.
140
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 54?65 Dublin, Ireland, August 23-29 2014.
Exploring Options for Fast Domain Adaptation of Dependency Parsers
Viktor Pekar, Juntao Yu, Mohab El-karef, Bernd Bohnet
School of Computer Science
University of Birmingham
Birmingham, UK
{v.pekar,jxy362,mxe346,b.bohnet}@cs.bham.ac.uk
Abstract
The paper explores different domain-independent techniques to adapt a dependency parser
trained on a general-language corpus to parse web texts (online reviews, newsgroup posts, we-
blogs): co-training, word clusters, and a crowd-sourced dictionary. We examine the relative
utility of these techniques as well as different ways to put them together to achieve maximum
parsing accuracy. While we find that co-training and word clusters produce the most promising
results, there is little additive improvement when combining the two techniques, which suggests
that in the absence of large grammatical discrepancies between the training and test domains,
they address largely the same problem, that of unknown vocabulary, with word clusters being
a somewhat more effective solution for it. Our highest results were achieved by a combination
of word clusters and co-training, significantly improving on the baseline, by up to 1.67%. Eval-
uation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012)
showed that they outperform all the shared task submissions that used a single parser to parse
test data, averaging the results across all the test sets.
1 Introduction
Domain adaptation of a statistical dependency parser is a problem that is of much importance for many
practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops
when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomed-
ical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al.,
2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-of-
the-art parsers ? Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et
al., 2013) ? trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes
v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending
on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.
Domain MST MALT Mate
Newswire 84.8 81.7 87.1
Pivot Texts 84.9 83.0 86.6
Broadcast News 79.4 78.1 81.2
Magazines 77.1 74.7 79.3
Broadcast Conversation 73.4 70.5 74.4
CoNLL09 test 86.9 84.7 90.1
Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate parsers trained on CoNLL09
data and tested on different specialist domains.
In a typical domain adaptation scenario, there are in-domain texts that are manually annotated and
that are used to train a general-language parser, and out-of-domain or target domain texts that are
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
54
parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be
available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem
of domain adaption, previous work focused on weakly supervised methods to re-train parsers on auto-
matically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman
et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et
al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target
domain (Plank and Van Noord, 2011; S?gaard and Plank, 2012; Khan et al., 2013b). Another line of
research aims specifically to overcome the lexical gap between the training data and the target domain
texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010),
the use of external lexica and morphological clues to predict PoS tags of unknown target domain words
(Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled
target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from
word co-occurrences obtained from unannotated texts (Zhou et al., 2011).
The goal of this paper is to investigate a combination of such techniques to adapt a general-language
parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual
annotation. In our study we include several techniques that have been shown to be reasonably effective
for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon,
as well as automatically annotated texts produced with the help of co-training. All these techniques
are domain-independent and can be applied to new target domains given unlabelled texts form these
domains. We explore the relative utility of these methods and ways to combine them for maximum
parser accuracy.
2 Related work
2.1 Text normalization
User-generated content on the web is notoriously low-quality, containing slang, abbreviations, inconsis-
tent grammar and spelling. Foster (2010) investigated lexical phenomena that appear on online discus-
sion forums that present common problems for parsing and compiled a list of such phenomena along
with their transformations. Applying the transformations to test sentences helped to bring the F-score up
by 2.7%. A similar approach was taken by Khan et al. (2013a) who found that it performed better than
spelling correction based on the Levenshtein distance. Gadde et al. (2011) use a word clustering method
and language modelling in order to align misspelled words with their regular spelling. Their method of
cleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%.
2.2 External lexica
To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from
the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting
the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10%
reduction in the error rate of parsing. External lexica have also been used to improve out-of-domain PoS
tagging (Li et al., 2012).
2.3 Word clusters
In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used
word clusters computed from unlabelled data as features for training a parser. The same approach has
proved to be effective for out-of-domain parsing, where there are many words in the test data unseen
during training, and word clusters computed from in-domain data similarly help to deal with the vocab-
ulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et
al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts
(Candito et al., 2011) and web texts (?vrelid and Skj?rholt, 2012). Word clusters created with Brown
clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013).
Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing
accuracy both on the Penn Treebank and on web data.
55
2.4 Co-training
Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification
problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or
more views on the data, i.e. feature subsets, or two or more different learning algorithms are employed
that complement each other to bootstrap additional training data from the unlabelled dataset. Co-training
algorithms have been successfully used in NLP tasks, and specifically for parsing. Sarkar (2001) showed
the both precision and recall of a phrase structure parser can be increased using a co-training procedure
that iteratively adds the most confidently parsed sentences from two different views to the training set.
Steedman et al.(2003) used two different parsers that supplied training data to each other in a bootstrap-
ping manner.
A number of studies specifically aimed to use co-training for domain adaptation of a dependency
parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a
one iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-of-
domain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was
that the agreement between the two classifiers during testing was a very good predictor of accuracy. More
recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm
uses three learners and each learner was designed to learn from those automatically classified unlabelled
data where the other two learners agreed on the classification label.
3 Experimental set-up
3.1 Parsers
In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira,
2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins
et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all
the parsers were the same and came from the Mate parser.
3.2 Baseline
As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation
techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain
test data.
3.3 Data
The experiments were conducted on annotated data on web-related domains available in the Ontonotes
v.5 and SANCL datasets, since a large amount of unlabelled data required for most domain adaptation
techniques is widely available.
OntoNotes. In experiments with weblog texts, we used the CoNLL09 training dataset (Hajic? et al.,
2009) as the general-language training data. The CoNLL09 test dataset was used to evaluate in-domain
parsing. To create an out-of-domain test set, we selected the last 10% of the weblogs section of the
OntoNotes v5.0 corpus1, in order to make the size of the out-of-domain test data comparable to that of
the in-domain test data, i.e. of CoNLL09 test. The OntoNotes corpus was converted to the CoNLL09
format using the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007).
SANCL. In order to compare our results with the results achieved by participants in the SANCL-2012
shared task, we also ran experiments on the Stanford dependences of three SANCL test sets (answers,
newsgroups and reviews). In these experiments we used the training set, test sets, unlabelled data,
as well as the evaluation script provided by SANCL-2012 organizers (Petrov and McDonald, 2012).
Tables 2 and 3 show the sizes of the OntoNotes and SANCL datasets as well as several measures
of lexical and grammatical characteristics of the data. The average sentence length (in tokens) and the
average number of subjects, roughly corresponding to the number of clauses in the sentence, aim to
characterize the syntactic complexity of the sentences: the higher these values, the more complex the
1https://catalog.ldc.upenn.edu/LDC2013T19
56
structure of the sentences is likely to be. The ratio of word forms absent from training data describes
how different the train and test data are in terms of vocabulary.
We see that in the OntoNotes test set the average sentence length and the number of subjects per
sentence is very similar to those in the train data. In SANCL test sets, these measures are more different,
but the values indicate a smaller syntactic complexity than in the train data. The amount of unknown
vocabulary in all the four test sets is between 5% and 8%.
CoNLL09 train CoNLL09 test OntoNotes test
Sentences 39,279 2,399 2,150
Tokens 958,167 57,676 42,144
Sentence length 24.61 24.59 23.4
Subjects 1.8 1.83 1.89
Unk. wordforms ratio 0.0 0.011 0.05
Table 2: The size of OntoNotes train and test datasets.
SANCL train Answers test Newsgroups test Reviews test
Sentences 30,060 1,744 1,195 1,906
Tokens 731,678 28,823 20,651 28,086
Sentence length 24.56 18.44 22.79 16.35
Subjects 1.69 1.78 1.62 1.5
Unk. wordforms ratio 0.0 0.064 0.084 0.051
Table 3: The size of SANCL train and test datasets.
Unlabelled Data. As unlabelled target domain data we used the unlabelled dataset from the SANCL-
2012 shared task. In experiments with word clusters, the entire dataset was used without any pre-
processing. In the co-training experiments, we pre-processed the data by removing sentences that are
longer than 500 tokens, or contained non-English words (this reduced the test set by 2%). Table 4 de-
scribes the size of the subsets of the unlabelled data.
Emails Weblogs Answers Newsgroups Reviews
Sentences 1,194,173 524,834 27,274 1,000,000 1,965,350
Tokens 17,047,731 10,356,284 424,299 18,424,657 29,289,169
Table 4: The size of unlabelled datasets.
3.4 Evaluation method
As a measure of parser accuracy, we report labeled attachment scores (LAS), the percentage of depen-
dencies which are attached and labeled correctly. Significance testing was performed using paired t-test.
4 Results and Discussion
4.1 Text normalization
We used a manually compiled lexicon containing Internet-specific spellings of certain words aligned
with their traditional spellings, e.g. u? you, gr8? great, don,t? don?t, as well as a number of regular
expressions to deal with extra symbols usually added for emphasis (This is sooooo good., This *is*
great.). After the original word forms were read by the parser, the lexicon and the regular expressions
were applied to normalize the spelling of the words. This produced only a very insignificant gain on the
baseline. A manual examination of the test data in both OntoNotes and SANCL has shown that in fact
although it comes from the web it contains very few examples of ?Internet speak?.
57
4.2 Word clusters
We used Liang?s (2005) implementation of the Brown clustering algorithm to create clusters of words
found in unlabelled domain texts. The output of the algorithm are word types assigned to discrete hi-
erarchical clusters, with clusters assigned ids in the form of bit strings of varying length corresponding
to clusters of different granularity. We experimentally set the maximum length of the bit string to 6,
collapsing more fine-grained clusters. Instead of replacing the original word forms and/or PoS tags with
cluster ids as was done in some previous studies (Koo et al., 2008; Candito et al., 2011; Ta?ckstro?m et
al., 2013), the ids of clusters were used to generate additional features in the representations of the word
forms, as this also produced better results in the preliminary runs. Below we describe experiments with
several other parameters of the clustering algorithm.
Number of clusters. As an input parameter, the Brown clustering algorithm requires a desired number
of clusters. Initially discarding all word types with a count of less than 3, we experimented with different
numbers of clusters and found that an optimal settings lies around 600 and 800 clusters, which gives an
improvement on the baseline of 0.9% for out-of-domain texts; but there does not seem to be noticeable
differences between specific numbers of clusters (see Table 5, statistically significant differences to the
baseline are indicated by stars2).
Number of clusters CoNLL09 OntoNotes
50 90.46** 78.10*
100 90.28* 78.40**
200 90.27 78.39**
400 90.37** 78.20**
600 90.40** 78.43**
800 90.30* 78.14**
Baseline 90.07 77.54
Table 5: The effect of the number of word clusters on in- and out-of-domain parsing, using the reviews
and weblogs subsets of the SANCL-2012 unlabelled data.
Filtering rare words. Due to the inevitable data sparseness, the algorithm is likely to mis-cluster
infrequent words. At the same time, it is rare words that are not seen during parser training and are
potentially of greatest value if included into word clusters. We examined several thresholds on word
frequency and their impact on parsing accuracy (see Table 6; statistically significant differences to the
baseline are indicated by stars). We found very slight differences between these three thresholds, al-
though the cut-off point of 3 showed the best results. Hence in further experiments with word clusters
we used this cut-off point.
Min. freq. CoNLL09 OntoNotes
1 90.36** 78.12*
3 90.40** 78.43**
5 90.22 78.24**
Table 6: The effect of filtering out rare words on word clusters, using the reviews and weblogs subsets
of the SANCL-2012 unlabelled data.
Amount of unlabelled data. To examine the effect that the size of unlabelled data from which word
clusters are computed, has on parser accuracy, we compared parser accuracy achieved when using only
the reviews and weblogs subsets of the SANCL corpus (39.6 mln word tokens), and when using the
entire SANCL dataset (75.2 mln tokens). These results are shown in Table 7, significant improvements
on the smaller set are indicated by stars. As expected, a larger amount of data does improve the parsing
accuracy, and the improvement is greater for out-of-domain parsing (+0.55% vs. +0.32%).
2In this and the following tables, one star indicates significance at the p < 0.05 level, two stars at the p < 0.01 level.
58
CoNLL09 OntoNotes
Reviews and Weblogs 90.30 78.14
Entire SANCL dataset 90.62* 78.69*
Table 7: The effect of the size of unlabelled data on word clusters, discarding word types with count less
than 3.
Relevant domain data. Furthermore, we were interested if simply adding more unlabelled data, not
necessarily from the relevant domain, produced the same increase in accuracy. We obtained the plain-
text claims and description parts of 13,600 patents freely available in the Global IP Database which
is based on the Espacenet3, creating a corpus with 42.5 mln tokens, i.e. which was similar in size to
the reviews and weblogs sections of the SANCL unlabelled dataset. Table 8 compares results achieved
when building clusters from the patents corpus and when using the reviews and weblogs texts from the
SANCL unlabelled dataset. Despite the fact that the size of the two datasets is comparable, we find that
while creating clusters from an irrelevant domain does gain on the baseline (+0.25%), the improvement
for clusters built from the relevant domain texts is noticeably higher (+0.6%). The difference between
the accuracy on the legal texts and the accuracy on the reviews and weblogs texts is significant at the
p < 0.05 level.
CoNLL09 OntoNotes
Legal texts 90.19 77.77
Reviews and Weblogs 90.30 78.14*
Table 8: The effect of the domain of unlabelled data on word clusters, discarding word types with count
less than 3.
4.3 External lexicon
It is possible to supply to the dependency parser an external lexicon, where word forms are provided
with PoS tags. Wiktionary, a companion project for Wikipedia that aims to produce a free, large-scale
multilingual dictionary, is a large and constantly growing crowd-sourced resource that appears attrac-
tive for NLP research. Wiktionary encodes word definitions, pronunciation, translations, etymology,
word forms and part-of-speech information. PoS tag dictionaries derived from Wiktionary have been
previously used for out-of-domain PoS tagging (Li et al., 2012) and for PoS tagging of resource-poor
languages (Ta?ckstro?m et al., 2013).
To create a lexicon for the parser, we extracted 753,970 English word forms and their PoS tags from
a dump of Wiktionary4. Wiktionary uses a rather informal set of PoS labels; to convert them to the
CoNLL09 tag set, we manually aligned all unique PoS tags found in Wiktionary with those of the
CoNLL09 tag set. We compared the accuracy achieved by the parser when the lexicon was supplied,
as well as when the lexicon was supplied together with the best configuration word clusters (800 clusters
built from the entire SANCL dataset after filtering words with the count less than 3). Table 9 shows
results achieved with these settings in comparison to the baseline (improvements on the baseline are in-
dicated with stars). When the lexicon is used on its own, we observe only slight gains on the baseline,
on both in-domain and out-domain data, and neither are statistically significant. When combining the
lexicon and word clusters, the accuracy actually decreases compared to using word clusters on their own.
Thus the best combination of domain adaptation techniques so far included the use of 800 word clusters
built from the entire SANCL unlabelled dataset, after filtering out word forms with the count less than 3,
with text normalization, but without the Wiktionary lexicon (+1.15% on the baseline).
3http://www.epo.org/searching/free/espacenet.html
4http://wiki.dbpedia.org/Wiktionary
59
CoNLL09 OntoNotes
Wiktionary 90.22 77.73
Clusters 90.62** 78.69**
Wiktionary+Clusters 90.44 78.49**
Baseline 90.07 77.54
Table 9: The effect of the Wiktionary lexicon on parsing accuracy.
4.4 Co-Training
Following Sagae (2007), the overall approach to parser co-training we adopted was as follows. First,
several parsers were combined to generate additional training data from unlabelled data, i.e. were used
as source learners for co-training. Then, the Mate parser was re-trained on the augmented training set
and tested on a test set, i.e. used as the evaluation learner. The reason Mate was selected the evaluation
learner was that it achieved the best results on the test data in its default settings (see Table 10).
CoNLL09 OntoNotes
Mate 90.07 77.54
MST 86.9 75.35
Turbo 85.94 74.85
Malt 84.72 72.63
Table 10: The baselines of parsers used in co-training experiments.
Agreement-based co-training. We first experimented with three pairwise parser combinations: using
Mate as one source learner and each of the other three parsers as the other source learner in order to obtain
additional training data. If two learners agreed on the parse of an unlabelled sentence, i.e. assigned each
word form the same dependency label and attached it to the same head, this was taken as an indication of
a correct parse, and the sentence was added to the training set. We experimented with different amounts
of the additional training sentences added to the main training set in such a manner: 10k, 20k, and 30k
sentences. The results of these experiments are shown in Table 11 (significant differences to the baseline
results are indicated by stars). The best result is obtained by Mate Malt pair, which outperforms the
baseline by just above 1%.
+10k +20k +30k
Mate+Malt 78.22** 78.61** 78.61**
Mate+MST 78.10** 78.23** 78.31**
Mate+Turbo 77.94** 77.84* 77.99**
Baseline 77.54
Table 11: Agreement-based co-training using two parsers.
Removing short sentences from unlabelled data. We noticed that among those sentences where
two parsers agreed, many tended to be very short: the average number of tokens in generated additional
training data was 8 per sentence, while both the training and test set contain much longer sentences
on average: the OntoNotes test set had 19.6 tokens/sentence and the CoNLL09 training set had 24.4
tokens/sentence. Such short sentences in the additional training data may be less useful or even harmful
for learning an accurate model of the target domain, than those that approximate both training and test
data. We experimented with several thresholds (4, 5, and 6 tokens) on the sentence length below which
sentences were removed from the additional training data. Table 12 shows that discarding short sentences
did improve accuracy by up to 0.25%, though none of the improvements were significant.
Three learners co-training. In the previous experiments, the Mate parser was used both as a source
learner and as the evaluation learner. Therefore it was likely that the additional training data did not
60
Mate+Malt, +30k Avg. Length
>6 tokens 78.88 13.1
>5 tokens 78.61 12.67
>4 tokens 78.67 11.94
All sentences 78.61 8.35
Table 12: The effect of removing short sentences from generated training data.
contain sufficiently novel examples based on which the evaluation parser could adapt better to the new
domain. Thus we next tried the tri-training algorithm (Zhou and Li, 2005), where two parsers are used
as source learners and a third as the evalaution learner. We used Malt and MST as source learners,
identifying sentences which they parsed in the same manner, and using these sentences to retrain the
Mate parser. We find that the tri-training algorithm performs better than the set-up with two parsers:
on 10k and 20k additional sentences, it achieves an accuracy increase on Mate+Malt, significant at the
p < 0.05 level (see Table 13).
+10k +20k +30k
Mate+Malt+MST 78.70* 79.12* 78.95
Mate+Malt 78.43 78.70 78.88
Table 13: Accuracy scores for tri-training (Mate+Malt+MST) and the best two-parser co-training algo-
rithm (Mate+Malt).
5 Combining co-training with clusters and an external lexicon
5.1 OntoNotes test set
We explored several possibilities to combine co-training with word clusters and an external lexicon, each
time supplying word clusters and/or the lexicon to the Mate parser when it is being retrained on additional
training data and applied to the test data. The following configurations of each of the techniques were
used:
? Word clusters: 800 clusters generated from the entire SANCL unlabelled dataset, after discarding
word types with the count less than 3.
? Lexicon: Wiktionary
? Co-training: Retraining the Mate parser on the combination of initial training set and 20k automat-
ically parsed sentences (agreed by Malt and MST) which contained more than 6 tokens.
The results showed that all three combinations failed to obtain significant improvements over co-
training alone. The best result is achieved by combining co-training and clusters, which obtains an
increase of only 0.09% on co-training; this is however, the greatest overall improvement on the baseline
(+1.67%). The combination of co-training and a Wiktionary lexicon in fact harms accuracy (see Table
14).
5.2 SANCL test set
In order to compare different technique combinations with the results achieved by participants of the
SANCL-2012 shared task, we evaluated them on the SANCL test set5.
As the results in Table 15 indicate, similarly to the results on OntoNotes, word clusters usually fare
much better than the Wiktionary-based lexicon, while the latter fails to produce statistically significant
5Note that the data was annotated in the Stanford format.
61
OntoNotes
Co-training 79.12**
Clusters 78.69**
Wiktionary 77.73
Co-training+Clusters 79.21**
Co-training+Wiktionary 78.89*
Co-training+Clusters+Wiktionary 79.19**
Baseline 77.54
Table 14: Combination of co-training with word clusters and an external lexicon, OntoNotes test set.
improvements on the baseline. The best accuracy overall was achieved by combinations of techniques,
in all the three subdomains, improving on the baseline by up to 1.3%.
Comparing the results achieved by our best configurations with the results of the shared task, we see
that our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31
vs. 80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012). Although our
results are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four results
were all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al.,
2012); our highest results only produced by the Mate parser, hence our best configuration achieved the
best performance of a single parser.
Answers Newsgroups Reviews Average
Co-training 77.18 82.72** 78.21 79.37
Clusters 78.04** 83.06* 79.03** 80.04
Wiktionary 77.61 82.8 78.32 79.57
Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31
Co-training+Clusters 78.05* 83.29** 78.8** 80.04
Co-training+Clusters+Wiktionary 78.33** 83.35** 78.84* 80.17
Baseline 77.03 82.4 78.12 79.18
SANCL Stanford-2 77.5 83.56 79.7 80.25
SANCL Best (DCU-Paris13) 81.15 85.38 83.86 83.46
Table 15: Combination of co-training with word clusters and an external lexicon, SANCL test set.
The results on both the OntoNotes and SANCL datasets show that on their own, word clusters and co-
training often improve significantly on the baseline, but their combination results only in minor further
improvements (only up to 0.32%). Word clusters aim specifically to deal with the unknown vocabulary
problem, and, since there seem to be no major grammatical differences between the train and test do-
mains (see Section 3.3), it is likely that the main benefit derived from co-training is the compensation
for unknown domain vocabulary. Word clusters also seem a better way to approach this problem: they
perform better than co-training on three out of four subdomains. The explanation that unknown vocab-
ulary is the main issue for domain adaptation in this domain pair is further supported by the fact that
combinations of word clusters with a Wiktionary lexicon sometimes performed better than combinations
involving co-training (on newsgroups and reviews).
6 Conclusion
In this paper we described experiments with several domain adaptation techniques, in order to quickly
adapt a general-language parser to parse web data. We find that the best combination of the techniques
improves significantly on the baseline (up to 1.67%), and achieves very promising results on the SANCL-
2012 shared task data, outperforming all submissions that used a single parser, in terms of labelled
accuracy score averaged across three test sets.
62
Our experiments with word clusters showed that word clusters derived from unlabelled domain texts
consistently contribute to a greater parsing accuracy, and that both the domain relevance of the unlabelled
data and its quantity are major factors for successful exploitation of word clusters. Experiments with a
crowd-sourced PoS lexicon however were not as conclusive: whereas supplying the lexicon to the parser
often resulted in certain accuracy gains, they were not as large as those for word clusters. This suggests
word clusters created automatically from relevant domain texts are a better tool to deal with unknown
vocabulary than a generic hand-crafted and wide-coverage lexicon. Another interesting finding was
that co-training was most effective when the evaluation parser was not used for creating extra training
data (the so-called tri-training technique), and when removing very short sentences from automatically
labelled data before re-training the evaluation parser.
With respect to combining co-training with word clusters, we could not find clear evidence for additive
improvement. This suggests that co-training solves largely the same problem as word clusters, i.e.,
unknown target domain vocabulary, and that for the web texts under study unknown vocabulary is a much
more significant impediment for domain adaptation than grammatical differences between domains.
Acknowledgements
The research was supported by FP7 ICT project ?Workbench for Interactive Contrastive Analysis of
Patent Documentation? under grant no. FP7-SME-606163.
References
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency
parsing. In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, COLT? 98, pages 92?100, New York, NY,
USA. ACM.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richa?rd Farkas Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Associtation for Computational
Linguistics, 1.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467?479.
Alicia Burga, Joan Codina, Gabriella Ferraro, Horacio Saggion, and Leo Wanner. 2013. The challenge of syntactic
dependency parsing adaptation for the patent domain. In ESSLLI-13 Workshop on Extrinsic Parse Improvement.
Marie Candito, Enrique Henestroza Anguiano, and Djam Seddah. 2011. A word clustering approach to domain
adaptation: Effective parsing of biomedical texts. In IWPT, pages 37?42. The Association for Computational
Linguistics.
Jennifer Foster. 2010. ?cba to check the spelling?: Investigating parser performance on discussion forum posts. In
HLT-NAACL, pages 381?384. The Association for Computational Linguistics.
Phani Gadde, L. V. Subramaniam, and Tanveer A. Faruquie. 2011. Adapting a WSJ trained part-of-speech tag-
ger to noisy text: Preliminary results. In Proceedings of the 2011 Joint Workshop on Multilingual OCR and
Analytics for Noisy Unstructured Text Data, MOCRAND11, pages 51?58, New York, NY, USA. ACM.
Daniel Gildea. 2001. Corpus variation and parser performance. In Lillian Lee and Donna Harman, editors,
Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ?01,
pages 167?202, Stroudsburg. Association for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In 16th
Nordic Conference of Computational Linguistics, pages 105?112. University of Tartu.
63
Mohammad Khan, Markus Dickinson, and Sandra Ku?bler. 2013a. Does size matter? text and grammar revision
for parsing social media data. In Proceedings of the Workshop on Language Analysis in Social Media, pages
1?10, Atlanta, Georgia, June. Association for Computational Linguistics.
Mohammad Khan, Markus Dickinson, and Sandra Ku?bler. 2013b. Towards domain adaptation for parsing web
data. In Galia Angelova, Kalina Bontcheva, and Ruslan Mitkov, editors, RANLP, pages 357?364. RANLP 2011
Organising Committee / ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In In Proc.
ACL/HLT.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. Dcu-
paris13 systems for the sancl 2012 shared task.
Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In Robert Dale, Kam-Fai Wong, Jian
Su, and Oi Yee Kwong, editors, IJCNLP, volume 3651 of Lecture Notes in Computer Science, pages 58?69.
Springer.
Shen Li, Joo Graa, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP-CoNLL, pages
1389?1398. ACL.
Percy Liang. 2005. Semi-supervised learning for natural language. In MASTERS THESIS, MIT.
Andre? FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and Ma?rio AT Figueiredo. 2010. Turbo parsers:
Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?44. Association for Computational Linguistics.
David McClosky and Eugene Charniak. 2008. Self-training for biomedical parsing. In ACL (Short Papers), pages
101?104. The Association for Computer Linguistics.
David McClosky, Wanxiang Che, Marta Recasens, Mengqiu Wang, Richard Socher, and Christopher Manning.
2012. Stanfords system for parsing the english web. In Workshop on the Syntactic Analysis of Non-Canonical
Language (SANCL 2012). Montreal, Canada.
Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algo-
rithms. In EACL.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932. sn.
Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages 351?359. Association for Computational Linguistics.
Lilja ?vrelid and Arne Skj?rholt. 2012. Lexical categories for improved parsing of web data. In Proceedings of
COLING 2012: Posters, pages 903?912, Mumbai, India, December. The COLING 2012 Organizing Committee.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380?
390. The Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the
First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate determin-
istic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 705?713, Stroudsburg, PA, USA. Association for Computational Linguistics.
Barbara Plank and Gertjan Van Noord. 2011. Effective measures of domain similarity for parsing. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-
Volume 1, pages 1566?1576. Association for Computational Linguistics.
Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko. 2006. Lexical adaptation of link
grammar to the biomedical sublanguage: a comparative evaluation of three approaches. BMC Bioinformat-
ics, 7(Suppl 3).
64
Ines Rehbein. 2011. Data point selection for self-training. In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, SPMRL ?11, pages 62?67, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kenji Sagae. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In In
Proceedings of the Eleventh Conference on Computational Natural Language Learning.
Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting
of the North American Chapter of the Association for Computational Linguistics on Language Technologies,
NAACL ?01, pages 1?8, Stroudsburg, PA, USA. Association for Computational Linguistics.
Anders S?gaard and Barbara Plank. 2012. Parsing the web as covariate shift. In Workshop on the Syntactic
Analysis of Non-Canonical Language (SANCL2012), Montreal, Canada.
Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In EACL, pages
331?338. The Association for Computer Linguistics.
Peter Szolovits. 2003. Adding a medical lexicon to an English parser. In AMIA Annual Symposium Proceedings,
volume 2003, page 639. American Medical Informatics Association.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan T. McDonald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging. TACL, 1:1?12.
Meishan Zhang, Wanxiang Che, Yijia Liu, Zhenghua Li, and Ting Liu. 2012. Hit dependency parsing: Bootstrap
aggregating heterogeneous parsers. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. Knowledge and
Data Engineering, IEEE Transactions on, 17(11):1529?1541.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve
statistical dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1, HLT ?11, pages 1556?1565, Stroudsburg, PA, USA.
Association for Computational Linguistics.
65
