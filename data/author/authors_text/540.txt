11
12
13
14
15
16
17
18
Statistical Approaches to
Computer-Assisted Translation
Sergio Barrachina?
Universitat Jaume I
Oliver Bender??
RWTH Aachen
Francisco Casacuberta?
Universitat Polite`cnica de Vale`ncia
Jorge Civera?
Universitat Polite`cnica de Vale`ncia
Elsa Cubel?
Universitat Polite`cnica de Vale`ncia
Shahram Khadivi??
RWTH Aachen
Antonio Lagarda?
Universitat Polite`cnica de Vale`ncia
Hermann Ney??
RWTH Aachen
Jesu?s Toma?s?
Universitat Polite`cnica de Vale`ncia
Enrique Vidal?
Universitat Polite`cnica de Vale`ncia
Juan-Miguel Vilar?
Universitat Jaume I
Current machine translation (MT) systems are still not perfect. In practice, the output
from these systems needs to be edited to correct errors. A way of increasing the productivity of
the whole translation process (MT plus human work) is to incorporate the human correction
activities within the translation process itself, thereby shifting the MT paradigm to that of
computer-assisted translation. This model entails an iterative process in which the human
translator activity is included in the loop: In each iteration, a prefix of the translation is validated
(accepted or amended) by the human and the system computes its best (or n-best) translation
suffix hypothesis to complete this prefix. A successful framework for MT is the so-called statis-
tical (or pattern recognition) framework. Interestingly, within this framework, the adaptation
of MT systems to the interactive scenario affects mainly the search process, allowing a great
reuse of successful techniques and models. In this article, alignment templates, phrase-based
models, and stochastic finite-state transducers are used to develop computer-assisted translation
systems. These systems were assessed in a European project (TransType2) in two real tasks: The
translation of printer manuals; manuals and the translation of the Bulletin of the European
Union. In each task, the following three pairs of languages were involved (in both translation
directions): English?Spanish, English?German, and English?French.
? Departament d?Enginyeria i Cie`ncies dels Computadors, Universitat Jaume I, 12071 Castello? de la Plana,
Spain.
?? Lehrstuhl fu?r Informatik VI, RWTH Aachen University of Technology, D-52056 Aachen, Germany.
? Institut Tecnolo`gic d?Informa`tica, Departament de Sistemes Informa`tics i Computacio?, Universitat
Polite`cnica de Vale`ncia, 46071 Vale`ncia, Spain.
? Institut Tecnolo`gic d?Informa`tica, Departament de Comunicacions, Universitat Polite`cnica de Vale`ncia,
46071 Vale`ncia, Spain.
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Jaume I, 12071 Castello? de la Plana,
Spain.
Submission received: 1 June 2006; revised submission received: 20 September 2007; accepted for publication:
19 December 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
1. Introduction to Computer-Assisted Translation
Research in the field of machine translation (MT) aims to develop computer systems
which are able to translate text or speech without human intervention. However,
present translation technology has not been able to deliver fully automated high-quality
translations. Typical solutions to improving the quality of the translations supplied by
an MT system require manual post-editing. This serial process prevents the MT system
from taking advantage of the knowledge of the human translator, and the human
translator cannot take advantage of the adaptive ability of the MT system.
An alternative way to take advantage of the existing MT technologies is to use
them in collaboration with human translators within a computer-assisted translation
(CAT) or interactive framework (Isabelle and Church 1997). Historically, CAT and MT
have been considered different but close technologies (Kay 1997) and more so for one
of the most popular CAT technologies, namely, translation memories (Bowker 2002;
Somers 2003). Interactivity in CAT has been explored for a long time. Systems have
been designed to interact with human translators in order to solve different types
of (lexical, syntactic, or semantic) ambiguities (Slocum 1985; Whitelock et al 1986).
Other interaction strategies have been considered for updating user dictionaries or for
searching through dictionaries (Slocum 1985; Whitelock et al 1986). Specific proposals
can be found in Tomita (1985), Zajac (1988), Yamron et al (1993), and Sen, Zhaoxiong,
and Heyan (1997), among others.
An important contribution to CAT technology, carried out within the TransType
project, is worth mentioning (Foster, Isabelle, and Plamondon 1997; Langlais, Foster,
and Lapalme 2000; Foster 2002; Langlais, Lapalme, and Loranger 2002). It entailed an
interesting focus shift in which interaction is directly aimed at the production of the
target text, rather than at the disambiguation of the source text, as in earlier interactive
systems. The idea proposed in that work was to embed data-driven MT techniques
within the interactive translation environment. The hope was to combine the best of
both paradigms: CAT, in which the human translator ensures high-quality output, and
MT, in which the machine ensures a significant gain in productivity.
Following these TransType ideas, the innovative embedding proposed here con-
sists in using a complete MT system to produce full target sentence hypotheses, or
portions thereof, which can be accepted or amended by a human translator. Each cor-
rect text segment is then used by the MT system as additional information to achieve
further, hopefully improved, suggestions. More specifically, in each iteration, a prefix
of the target sentence is somehow fixed by the human translator and, in the next itera-
tion, the system predicts a best (or n-best) translation suffix(es)1 to complete this prefix.
We will refer to this process as interactive-predictive machine translation (IPMT).
This approach introduces two important requirements: First, the models have to
provide adequate completions and, second, this has to happen efficiently. Taking these
requirements into account, stochastic finite-state transducers (SFSTs), alignment tem-
plates (ATs), and phrase-based models (PBMs) are compared in this work. In previous
works these models have proven adequate for conventional MT (Vidal 1997; Amengual
et al 2000; Ney et al 2000; Toma?s and Casacuberta 2001; Och and Ney 2003; Casacuberta
and Vidal 2004; Och and Ney 2004; Vidal and Casacuberta 2004). This article shows that
1 The terms prefix and suffix are used here to denote any substring at the beginning and end (respectively)
of a string of characters (including spaces and punctuation), with no implication of morphological
significance as is usually implied by these terms in linguistics.
4
Barrachina et al Statistical Computer-Assisted Translation
existing efficient searching algorithms can be adapted in order to provide completions
(rather than full translations) also in a very efficient way.
The work presented here has been carried out in the TransType2 (TT2) project
(SchlumbergerSema S.A. et al 2001), which is considered as a follow-up to the inter-
active MT concepts introduced in the precursory TransType project cited previously.
We should emphasize the novel contributions of the present work with respect
to TransType. First, we show how fully fledged statistical MT (SMT) systems can be
extended to handle IPMT. In particular, the TT2 systems always produce complete
sentence hypotheses on which the human translator can work. This is an important
difference to previous work, in which the use of basic MT techniques only allowed the
prediction of single tokens (c.f., Section 2.2). Second, using fully fledged SMT systems,
we have performed systematic offline experiments to simulate the specific conditions of
interactive translation and we report and study the results of these experiments. Thirdly,
the IPMT systems presented in this article were successfully used in several field trials
with professional translators (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
We should finally mention that the work developed in TT2 has gone beyond con-
ventional keyboard-and-mouse interaction, leading to the development of advanced
multi-modal interfaces. Speech is the most natural form of human communication and
its use as feedback in the IPMT framework has been explored by Vidal et al (2006).
On the other hand, human translators can be faster dictating the translation text rather
than typing it, thus it has also been investigated how to improve system performance
and usability when the user dictates the translation first and then edits the recognized
text (Khadivi, Zolnay, and Ney 2005; Khadivi, Zens, and Ney 2006).
The rest of the article is structured as follows. The next section introduces the
general setting for SMT and IPMT. In Section 3, AT, PBM, and SFST are briefly surveyed
along with the corresponding learning procedures. In Section 4, general search proce-
dures for the previous models are outlined and a detailed description of the extension
of these procedures to IPMT scenarios is presented. Section 5 is devoted to introducing
the tasks used for the assessment of the proposal presented in the previous sections:
the pairs of languages, corpora, and assessment procedures. The results are reported in
Section 6. A discussion of these results and the conclusions which can be drawn from
this work are presented in the final section.
2. Statistical Framework
The statistical or pattern recognition framework constitutes a very successful frame-
work for MT. As we will see here, this framework also proves adequate for IPMT.
2.1 Statistical Machine Translation
Assuming that we are given a sentence s in a source language, the text-to-text translation
problem can be stated as finding its translation t in a target language. Using statistical
decision theory, the best translation is given by the equation2
t? = argmax
t
Pr(t|s) (1)
2 We follow the common notation of Pr(x) for Pr(X = x) and Pr(x|y) for Pr(X = x|Y = y), for any random
variables X and Y. Similarly, Pr() will be used to denote ?true? probability functions, and p() or q() will
denote model approximations.
5
Computational Linguistics Volume 35, Number 1
Using Bayes?s Theorem, we arrive at
t? = argmax
t
Pr(t) ? Pr(s|t) (2)
This equation is generally interpreted as follows. The best translation must be a correct
sentence in the target language that conveys the meaning of the source sentence. The
probability Pr(t) represents the well-formedness of t and it is generally called the
language model probability (n-gram models are usually adopted [Jelinek 1998]). On
the other hand, Pr(s|t) represents the relationship between the two sentences (the source
and its translation). It should be of a high value if the source is a good translation of
the target and of a low value otherwise. Note that the translation direction is inverted
from what would be normally expected; correspondingly the models built around this
equation are often called inverted translation models (Brown et al 1990, 1993). As we
will see in Section 3, these models are based on the notion of alignment. It is interesting to
note that if we had perfect models, the use of Equation (1) would suffice. Given that we
have only approximations, the use of Equation (2) allows the language model to correct
deficiencies in the translation model.
In practice all of these models (and possibly others) are often combined into a log-
linear model for Pr(t | s) (Och and Ney 2004):
t? = argmax
t
{
N
?
i=1
?i ? log fi(t, s)
}
(3)
where fi(t, s) can be a model for Pr(s|t), a model for Pr(t|s), a target language model
for Pr(t), or any model that represents an important feature for the translation. N is the
number of models (or features) and ?i are the weights of the log-linear combination.
When using SFSTs, a different transformation can be used. These transducers
have an implicit target language model (which can be obtained from the finite-state
transducer by dropping the source symbols of each transition (Vidal et al 2005)). There-
fore, this separation is no longer needed. SFSTs model joint probability distributions;
therefore, Equation (1) has to be rewritten as
t? = argmax
t
Pr(s, t) (4)
This is the approach followed in GIATI (Casacuberta et al 2004a; Casacuberta and Vidal
2004), but other models for the joint probability can be adopted.
If the input is a spoken sentence, instead of a written one, the problem becomes
more complex; we will not deal with this here. The interested reader may consult
Amengual et al (2000), Ney et al (2000), or Casacuberta et al (2004a, 2004b), for
instance.
2.2 Statistical Interactive-Predictive Machine Translation
Unfortunately, current models and therefore the systems which can be built from them
are still far from perfect. This implies that, in order to achieve good, or even acceptable,
translations, manual post-editing is needed. An alternative to this serial approach (first
MT, then manual correction) is given by the IPMT paradigm. Under this paradigm,
translation is considered as an iterative process where human and computer activity
6
Barrachina et al Statistical Computer-Assisted Translation
Figure 1
Typical example of IPMT with keyboard interaction. The aim is to translate the English sentence
Click OK to close the print dialog into Spanish. Each step starts with a previously fixed target
language prefix tp, from which the system suggests a suffix t?s. Then the user accepts a part of this
suffix (a) and types some keystrokes (k), possibly in order to amend the remaining part of ts.
This produces a new prefix, composed by the prefix from the previous iteration and the accepted
and typed text, (a) (k), to be used as tp in the next step. The process ends when the user enters
the special keystroke ?#?. System suggestions are printed in italics and user input in boldface
typewriter font. In the final translation t, text that has been typed by the user is underlined.
are interwoven. This way, the models take into account both the input sentence and the
corrections of the user.
As previously mentioned, this idea was originally proposed in the TransType
project (Foster, Isabelle, and Plamondon 1997; Langlais, Foster, and Lapalme 2000;
Langlais, Lapalme, and Loranger 2002). In that project, the parts proposed by the sys-
tems were produced using a linear combination of a target language model (trigrams)
and a lexicon model (so-called IBM-1 or -2) (Langlais, Lapalme, and Loranger 2002). As
a result, TransType allowed only single-token completions, where a token could be either
a word or a short sequence of words from a predefined set of sequences. This proposal
was extended to complete full target sentences in the TT2 project, as discussed hereafter.
The approach taken in TT2 is exemplified in Figure 1. Initially, the system provides
a possible translation. From this translation, the user marks a prefix as correct and
provides, as a hint, the beginning of the rest of the translation. Depending on the system
or the user preferences, the hint can be the next word or some letters from it (in the
figure, hints are assumed to be words and are referred to as k). Let us use tp for the prefix
validated by the user together with the hint. The system now has to produce (predict)
a suffix ts to complete the translation. The cycle continues with a new validation and
hint from the user until the translation is completed. This justifies our choice of the term
?interactive-predictive machine translation? for this approach.
The crucial step of the process is the production of the suffix. Again, decision theory
tells us to maximize the probability of the suffix given the available information. That
is, the best suffix will be
t?s = argmax
ts
Pr(ts|s, tp) (5)
which can be straightforwardly rewritten as
t?s = argmax
ts
Pr(tp, ts|s) (6)
7
Computational Linguistics Volume 35, Number 1
Note that, because tpts = t, this equation is very similar to Equation (1). The main
difference is that the argmax search now is performed over the set of suffixes ts that
complete tp instead of complete sentences (t in Equation (1)). This implies that we can
use the same models if the search procedures are adequately modified (Och, Zens, and
Ney 2003).
The situation with respect to finite-state models is similar. Now, Equation (5) is
rewritten as
t?s = argmax
ts
Pr(tp, ts, s) (7)
which allows the use of the same models as in Equation (4) as long as the search
procedure is changed appropriately (Cubel et al 2003, 2004; Civera et al 2004a,
2004b).
3. Statistical and Finite-State Models
The models used are presented in the following subsections: Section 3.1 for the condi-
tional distribution Pr(s|t) in Equation (2) and Section 3.2 for the joint distribution Pr(s, t)
in Equation (4).
3.1 Statistical Alignment Models
The translation models which Brown et al (1993) introduced to deal with Pr(s|t) in
Equation (2) are based on the concept of alignment between the components of a pair
(s, t) (thus they are called statistical alignment models). Formally, if the number of
the source words in s is J and the number of target words in t is I, an alignment is a
function a : {1, ..., J} ? {0, ..., I}. The image of j by a will be denoted as aj, in which the
particular case aj = 0 means that the position j in s is not aligned with any position of t.
By introducing the alignment as a hidden variable in Pr(s|t),
Pr(s|t) =
?
a
Pr(s, a|t) (8)
The alignment that maximizes Pr(s, a|t) is shown to be very useful in practice for
training and for searching.
Different approaches have been proposed for modeling Pr(s, a|t) in Equation (8):
Zero-order models such as model 1, model 2, and model 3 (Brown et al 1993) and the first-
order models such as model 4, model 5 (Brown et al 1993), hidden Markov model (Ney
et al 2000), and model 6 (Och and Ney 2003).
In all these models, single words are taken into account. Moreover, in practice the
summation operator is replaced with the maximization operator, which in turn reduces
the contribution of each individual source word in generating a target word. On the
other hand, modeling word sequences rather than single words in both the alignment
and lexicon models cause significant improvement in translation quality (Och and Ney
8
Barrachina et al Statistical Computer-Assisted Translation
2004). In this work, we use two closely related models: ATs (Och and Ney 2004) and
PBMs (Toma?s and Casacuberta 2001; Koehn, Och, and Marcu 2003; Zens and Ney 2004).
Both models are based on bilingual phrases3 (pairs of segments or word sequences)
in which all words within the source-language phrase are aligned only to words of
the target-language phrase and vice versa. Note that at least one word in the source-
language phrase must be aligned to one word of the target-language phrase, that is,
there are no empty phrases similar to the empty word of the word-based models. In
addition, no gaps and no overlaps between phrases are allowed.
We introduce some notation to deal with phrases. As before, s denotes a source-
language sentence; ?s denotes a generic phrase in s, and ?sk the kth phrase in s. sj denotes
the jth source word in s; s
j?
j denotes the contiguous sequence of words in s beginning
at position j and ending at position j? (inclusive); obviously, if s has J words, s
J
1 denotes
the whole sentence s. An analogous notation is used for target words, phrases, and
sequences in target sentence t.
3.1.1 Alignment Templates. The ATs are based on the bilingual phrases but they are
generalized by replacing words with word classes and by storing the alignment in-
formation for each phrase pair. Formally, an AT Z is a triple (S,T,?a), where S and
T are a source class sequence and a target class sequence, respectively, and ?a is an
alignment from the set of positions in S to the set of positions in T.4 Mapping of source
and target words to bilingual word classes is automatically trained using the method
described by Och (1999). The method is actually an unsupervised clustering method
which partitions the source and target vocabularies, so that assigning words to classes
is a deterministic operation. It is also possible to employ parts-of-speech or semantic
categories instead of the unsupervised clustering method used here. More details can
be found in Och (1999) and Och and Ney (2004). However, it should be mentioned
that the whole AT approach (and similar PBM approaches as they are now called) is
independent of the word clustering concept. In particular, for large training corpora,
omitting the word clustering in the AT system does not much affect the translation
accuracy.
To arrive at our translation model, we first perform a segmentation of the source
and target sentences into K ?blocks? dk ? (ik; bk, jk) (ik ? {1, . . . , I} and jk, bk ? {1, . . . , J}
for 1 ? k ? K). For a given sentence pair (sJ1, t
I
1), the kth bilingual segment (?sk,
?tk)
is (s
jk
bk?1+1
, t
ik
ik?1+1
) (Och and Ney 2003). The AT Zk = (Sk,Tk,?ak) associated with the kth
bilingual segment is: Sk the sequence of word classes in ?sk; Tk the sequence of word
classes in ?tk, and ?ak the alignment between positions in a source class sequence S and
positions in a target class sequence T.
For translating a given source sentence s we use the following decision rule as an
approximation to Equation (1):
(I?, t?I?1) = argmax
I,tI1
{
max
K,dK1 ,?a
K
1
log PAT(s
J
1, t
I
1; d
K
1 ,?a
K
1 )
}
(9)
3 Although the term ?phrase? has a more restricted meaning, in this article it refers to a word sequence.
4 Note that the phrases in an AT are sequences of word classes rather than words, which motivates the use
of a different notation.
9
Computational Linguistics Volume 35, Number 1
We use a log-linear model combination:
log PAT(s
J
1, t
I
1; d
K
1 ,?a
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[ ?4 + ?5 ? log q(bk|jk?1)+ ?6 ? log p(Tk,?ak|Sk)+
ik
?
i=ik?1+1
?7 ? log p(ti|?sk,?ak) ] (10)
with weights ?i, i = 1, ? ? ? , 7. The weights ?1 and ?4 play a special role and are used
to control the number I of words and number K of segments for the target sentence
to be generated, respectively. The log-linear combination uses the following set of
models:
 p(ti|t
i?1
i?2): Word-based trigram language model
 p(Ti|T
i?1
i?4 ): Class-based five-gram language model
 p(Tk,?ak|Sk): AT at class level, model parameters are estimated directly
from frequency counts in a training corpus
 p(ti|?sk,?ak): Single word model based on a statistical dictionary and ?ak. As
in the preceding model, the model parameters are estimated by using
frequency counts
 q(bk|jk?1) = e|bk?jk?1+1|: Re-ordering model using absolute j distance of
the phrases.
As can be observed, all models are implemented as feature functions which depend on
the source and the target language sentences, as well as on the two hidden variables
(?aK1 , b
K
1 ). Other feature functions can be added to this sort of model as needed. For a
more detailed description the reader is referred to Och and Ney (2004).
Learning alignment templates. To learn the probability of applying an AT, p(Z =
(S,T,?a)|?s ), all bilingual phrases that are consistent with the segmentation are extracted
from the training corpus together with the alignment within these phrases. Thus, we
obtain a count N(Z) of how often an AT occurred in the aligned training corpus. Using
the relative frequency
p(Z) = (S,T,?a)|?s) =
N(Z) ? ?(S,C(?s))
N(C(?s))
(11)
we estimate the probability of applying an AT Z to translate the source language phrase
?s, in which ? is Kronecker?s delta function. The class function C maps words onto their
10
Barrachina et al Statistical Computer-Assisted Translation
classes. To reduce the memory requirements, only probabilities for phrases up to a
maximal length are estimated, and phrases with a probability estimate below a certain
threshold are discarded.
The weights ?i in Equation (10) are usually estimated using held-out data with
respect to the automatic evaluation metric employed using the downhill simplex al-
gorithm from Press et al (2002).
3.1.2 Phrase-Based Models. A simple alternative to AT has been introduced in recent
works: The PBM approach (Toma?s and Casacuberta 2001; Marcu and Wong 2002; Zens,
Och, and Ney 2002; Toma?s and Casacuberta 2003; Zens and Ney 2004). These methods
learn the probability that a sequence of contiguous words?the source phrase?(as a
whole unit) in a source sentence is a translation of another sequence of contiguous
words?the target phrase?(as a whole unit) in the target sentence. In this case, the
statistical dictionaries of single word pairs are substituted by statistical dictionaries of
bilingual phrases or bilingual segments. These models are simpler than ATs, because no
alignments are assumed between word positions inside a bilingual segment and word
classes are not used in the definition of a bilingual phrase.
The simplest formulation is for monotone PBMs (Toma?s and Casacuberta 2007),
assuming a uniform distribution of the possible segmentations of the source and of the
target sentences. In this case, the approximation to Equation (1) is:
(I?, t?I?1) = argmax
I,tI1
{
max
K,dK1
log PPBM(s
J
1, t
I
1; d
K
1 )
}
(12)
In our implementation of this approach, we have also adopted a log-linear model
log PPBM(s
J
1, t
I
1; d
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[
?4 + ?5 ? log p(?tk|?sk)
]
(13)
with weights ?i, i = 1, ? ? ? , 5. The weights ?1 and ?4 play a special role and are used
to control the number I of words and number K of segments for the target sentence
to be generated, respectively. The log-linear combination uses the following set of
models:
 p(ti|t
i?1
i?2): Word-based trigram language model
 p(Ti|T
i?1
i?4 ): Class-based five-gram language model
 p(?tk|?sk): Statistical dictionary of bilingual phrases.
11
Computational Linguistics Volume 35, Number 1
If segment re-ordering is desired (non-monotone models), the probability of phrase-
alignment q can be introduced (a first-order distortion model is assumed):
log PPBM(s
J
1, t
I
1; d
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[
?4 + ?5 ? log p(?tk|?sk)+ ?6 ? log q(bk|jk?1)
]
(14)
with the additional model q, similar to the one used for AT.
Learning phrase-based alignment models. The parameters of each model and the weights
?i in Equations (13) and (14) have to be estimated. There are different approaches to
estimating the parameters of each model (Toma?s and Casacuberta 2007). Some of these
techniques correspond to a direct learning of the parameters from a sentence-aligned
corpus using a maximum likelihood approach (Toma?s and Casacuberta 2001; Marcu
and Wong 2002). Other techniques are heuristics based on the previous computation
of word alignments in the training corpus (Zens, Och, and Ney 2002; Koehn, Och, and
Marcu 2003). On the other hand, as for AT, the weights ?i in Equation (13) are usually
optimized using held-out data.
3.2 Stochastic Finite-State Transducers
SFSTs constitute an important framework in syntactic pattern recognition and nat-
ural language processing. The simplicity of finite-state models has given rise to some
concerns about their applicability to real tasks. Specifically in the field of language
translation, it is often argued that natural languages are so complex that these simple
models are never able to cope with the required source-target mappings. However, one
should take into account that the complexity of the mapping between the source and
target domains of a transducer is not always directly related to the complexity of the
domains themselves. Instead, a key factor is the degree of monotonicity or sequentiality
between source and target subsequences of these domains (Casacuberta, Vidal, and
Pico? 2005). Finite-state transducers have been shown to be adequate to handle complex
mappings efficiently (Berstel 1979) and SFSTs are closely related to monotone PBMs.
In Equation (4), Pr(s, t) can be modeled by an SFST T, which is defined as a tuple
??,?,Q, q0, p, f ?, where ? is a finite set of source symbols,? is a finite set of target symbols
(? ?? = ?), Q is a finite set of states, q0 is the initial state, p and f are two functions
p : Q ? ??? ? Q ? [0, 1] (for the probabilities of transitions) and f : Q ? [0, 1] (for the
probabilities of final states) that satisfy ?q ? Q:
f (q) +
?
(s,?t,q? )?????Q
p(q, s,?t, q?) = 1 (15)
Given T, a path with J transitions associated with the translation pair (s, t) ?
?? ??? is a sequence of transitions ? = (q0, s1 , t?1, q1) (q1, s2 , t?2, q2) (q2, s3 , t?3, q3) . . .
(qJ?1, sJ , t?J, qJ ), such that s1 s2 . . . sJ = s and t?1 t?2 . . . t?J = t. The probability of a path is
12
Barrachina et al Statistical Computer-Assisted Translation
the product of its transition probabilities, times the final-state probability of the last
state in the path:
PT(?) =
J
?
j=1
p(qj?1, sj , t?j, qj) ? f (qJ ) (16)
The probability of a translation pair (s, t) according to T is then defined as the sum of
the probabilities of all the paths associated with (s, t):
PT(s, t) =
?
?
PT(?) (17)
Learning finite-state transducers. There are different families of techniques to train an
SFST from a parallel corpus of source?target sentences (Casacuberta and Vidal 2007).
One of the techniques that has been adopted in this work is the grammatical inference
and alignments for transducer inference (GIATI) technique. This technique is in the
category of hybrid methods which use statistical techniques to guide the SFST structure
learning and simultaneously train the associated probabilities.
Given a finite sample of string pairs, the inference of SFSTs using the GIATI tech-
nique is performed as follows (Casacuberta and Vidal 2004; Casacuberta, Vidal, and
Pico? 2005): i) Building training strings: Each training pair is transformed into a single
string from an extended alphabet to obtain a new sample of strings. ii) Inferring a
(stochastic) regular grammar. Typically, a smoothed n-gram is inferred from the sample
of strings obtained in the previous step. iii) Transforming the inferred regular grammar
into a transducer: The symbols associated with the grammar rules are converted back
into input/output symbols, thereby transforming the grammar inferred in the previous
step into a transducer. The transformation of a parallel corpus into a string corpus
is performed using statistical alignments. These alignments are obtained using the
GIZA++ software (Och and Ney 2003).
4. Searching
Searching is an important computational problem in SMT. Algorithmic solutions de-
veloped for SMT can be adapted to the IPMT framework. The main general search
procedures for each model in Section 3 are presented in the following subsections,
each followed by a detailed description of the necessary adaptations to the interactive
framework.
4.1 Searching with Alignment Templates
In offline MT, the generation of the best translation for a given source sentence s is
carried out by producing the target sentence in left-to-right order using the model of
Equation (10). At each step of the generation algorithm we maintain a set of active
hypotheses and choose one of them for extension. A word of the target language is
then added to the chosen hypothesis and its costs get updated. This kind of generation
fits nicely into a dynamic programming (DP) framework, as hypotheses which are
indistinguishable by both language and translation models (and that have covered
the same source positions) can be recombined. Because the DP search space grows
13
Computational Linguistics Volume 35, Number 1
Figure 2
Example of a word graph for the source German sentence was hast du gesagt? (English reference
translation: ?what did you say??).
exponentially with the size of the input, standard DP search is prohibitive, and we resort
to a beam-search heuristic.
4.1.1 Adaptation to the Interactive-Predictive Scenario. The most important modification
is to rely on a word graph that represents possible translations of the given source
sentence. This word graph is generated once for each source sentence. During the
process of human?machine interaction the system makes use of this word graph in
order to complete the prefixes accepted by the human translator. In other words, after
the human translator has accepted a prefix string, the system finds the best path in the
word graph associated with this prefix string so that it is able to complete the target
sentence. Using the word graph in such a way, the system is able to interact with the
human translator in a time efficient way. In Och, Zens, and Ney (2003), an efficient
algorithm for interactive generation using word graphs was presented. A word graph
is a weighted directed acyclic graph, in which each node represents a partial translation
hypothesis and each edge is labeled with a word of the target sentence and is weighted
according to the language and translation model scores. In Ueffing, Och, and Ney (2002),
the authors give a more detailed description of word graphs and show how they can be
easily produced as a by-product of the search process. An example of a word graph is
shown in Figure 2.
The computational cost of this approach is much lower, as the whole search for the
translation must be carried out only once, and the generated word graph can be reused
for further completion requests.
For a fixed source sentence, if no pruning is applied in the production of the word
graph, it represents all possible sequences of target words for which the posterior
probability is greater than zero, according to the models used. However, because of
the pruning generally needed to render the problem computationally feasible, the
resulting word graph only represents a subset of the possible translations. Therefore,
it may happen that the user sets prefixes which cannot be found in the word graph. To
circumvent this problem some heuristics need to be implemented.
First, we look for the node with minimum edit distance to the prefix except for
its last (partial) word.5 Then we select the completion path which starts with the last
5 The edit distance concept for finding the prefix string in a word graph could be refined by casting the edit
distance operations into a suitable probabilistic model.
14
Barrachina et al Statistical Computer-Assisted Translation
(partial) word of the prefix and has the best backward score?this is the score associated
with a path going from the node to the final node. Now, because the original word graph
may not be compatible with the new information provided by the prefix, it might be
impossible to find a completion in this word graph due to incompatibility with the
last (partial) word in the prefix. This problem can be solved to a certain degree by
searching for a completion of the last word with the highest probability using only the
language model. This supplementary heuristic to the usual search increases the perfor-
mance of the system, because some of the rejected words in the pruning process can
be recovered.
A desirable feature of an IPMT system is the possibility of producing a list of
alternative target suffixes, instead of only one. This feature can be easily added by
computing the n-best hypotheses. Of course, these n-best hypotheses do not refer to
the whole target sentence, but only to the suffixes. However, the problem is that in
many cases the sentence hypotheses in the n-best list differ in only one or two words.
Therefore, we introduce the additional requirement that the first four words of the n-
best hypotheses must be different.
4.2 Searching with Phrase-Based Models
The generation of the best translation with PBMs is similar to the one described in the
previous section. Each hypothesis is composed of a prefix of the target sentence, a subset
of source positions that are aligned with the positions of the prefix of the target sentence,
and a score. In this case, we adopted an extension of the best-first strategy where the
hypotheses are stored in several sorted lists, depending on which words in the source
sentence have been translated. This strategy is related to the well-known multi-stack-
decoding algorithm (Berger et al 1996; Toma?s and Casacuberta 2004). In each iteration,
the algorithm extends the best hypothesis from each available list.
While target words are always generated from left to right, there are two alter-
natives in the source word extraction: Monotone search, which takes the source words
from left to right, and non-monotone search, which can take source words in any
order.
4.2.1 Adaptation to the Interactive-Predictive Scenario. Only a simple modification of this
search algorithm is necessary: If the new extended hypothesis is not compatible with
the fixed target prefix, tp, then this hypothesis is not considered. This compatibility is
verified at the character level; therefore the user does not need to type the whole target
word at the end of the target prefix.
In the interactive scenario, speed is a critical aspect. In the PBM approach, monotone
search is much faster than non-monotone search in the tasks which are considered in this
work (Toma?s and Casacuberta 2006). However, monotone search presents a problem for
interactive operation: If a user introduces a prefix that cannot be obtained in a monotone
way from the source, the search algorithm is not able to complete this prefix. In order
to solve this problem without losing computational efficiency, we use the following ap-
proach: Non-monotone search is used for target prefixes, whereas completions (suffixes)
are generated using monotone search.
As for AT models, a list of target suffixes can also be produced. This list can be
obtained easily by keeping the n-best hypotheses in each sorted list. To avoid generating
very similar hypotheses in the n-best list, we apply the following procedure: Starting
from the n-best list resulting from the normal search, we first add hypotheses obtained
15
Computational Linguistics Volume 35, Number 1
by translating a single untranslated word from the source, along with hypotheses
consisting of a single high-probability word according to the target language model; we
then re-order the hypotheses, maximizing the diversity at the beginning of the suffixes,
and keep only the n first hypotheses in the re-ordered list.
4.3 Searching with Stochastic Finite-State Transducers
As discussed by Pico? and Casacuberta (2001), the computation of Equation (4) for SFSTs
under a maximum approximation (i.e., using maximization in Equation (17) instead
of the sum) amounts to a conventional Viterbi search. The algorithm finds the most
probable path among those paths in the SFST which are compatible with the source
sentence s. The corresponding translation, t?, is simply obtained by concatenating the
target strings of the edges of this path.
4.3.1 Adaptation to the Interactive-Predictive Scenario. Here, Equation (7) is used wherein
the optimization is performed over the set of target suffixes (completions) rather than
the set of complete target sentences. To solve this maximization problem, an approach
similar to that proposed for AT in Section 4.1 has been adopted.
First, given the source sentence, a word graph is extracted from the SFST. In this
case, the word graph is just (a pruned version of) the Viterbi search trellis obtained when
translating the whole source sentence. The main difference between the word graphs
generated with ATs and SFSTs is how the nodes and edges are defined in each case. On
the one hand, the nodes are defined as partial hypotheses of the search procedure in
the AT approach, whereas the nodes in the case of SFSTs can be directly mapped into
states in the SFST representing a joint (source word/target string) language model. On
the other hand, the scores associated with the edges in the AT approach are computed
from a combination of the language and translation models, whereas in the case of
SFSTs these scores simply come from the joint language model estimated by the GIATI
technique.
Once the word graph has been generated, the search for the most probable com-
pletion as stated in Equation (6) is carried out in two steps, in a similar way to that
explained for the AT approach. In this case, the computation entailed by both the edit-
distance (prefix error-correcting) and the remaining search is significantly accelerated
by visiting the nodes in topological order and by the incorporation of the beam-search
technique (Amengual and Vidal 1998). Moreover, the error-correcting algorithm takes
advantage of the incremental way in which the user prefix is generated, parsing only
the new suffix appended by the user in the last interaction.
It may be the case that a user prefix ends in an incomplete word during the inter-
active translation process. Therefore, it is necessary to start the translation completion
with a word whose prefix matches this unfinished word. The proposed algorithm thus
searches for such a word. First, it considers the target words of the edges leaving
the nodes returned by the error-correcting algorithm. If this initial search fails, then
a matching word is looked up in the word-graph vocabulary. Finally, as a last resort,
the whole transducer vocabulary is taken into consideration to find a matching word;
otherwise this incomplete word is treated as an entire word.
This error-correcting algorithm returns a set of nodes from which the best comple-
tion would be selected according to the best backward score. Moreover, n-best com-
pletions can also be produced. Among many weighted-graph n-best path algorithms
which are available, the recursive enumeration algorithm presented in Jime?nez and
16
Barrachina et al Statistical Computer-Assisted Translation
Marzal (1999) was adopted for its simplicity in calculating best paths on demand and its
smooth integration with the error-correcting algorithm.
5. Experimental Framework
The models and search procedures introduced in the previous sections were assessed
through a series of IPMT experiments with different corpora. These corpora, along with
the corresponding pre- and post-processing and assessment procedures, are presented
in this section.
5.1 Pre- and Post-Processing
Usually, MT models are trained on a pre-processed version of an original corpus. Pre-
processing provides a simpler representation of the training corpus which makes token
or word forms more homogeneous. In this way automatic training of the MT models is
boosted, and the amount of computation decreases.
The pre-processing steps are: tokenization, removing unnecessary case information,
and tagging some special tokens like numerical sequences, e-mail addresses, and URLs
(?categorization?). In translation from a source language to a target language, there are
some words which are translated identically (because they have the same spelling in
both languages). Therefore, we identify them in the corpus and replace them with some
generic tags to help the translation system.
Post-processing takes place after the translation in order to hide the internal repre-
sentation of the text from the user. Thus, the user will only work with an output which
is very similar to human-generated texts. In detail, the post-processing steps are: de-
tokenization, true-casing, and replacing the tags with their corresponding words.
In an IPMT scenario, the pre-/post-processing must run in real-time and should be
reversible as much as possible. In each human?machine interaction, the current prefix
has to be pre-processed for the interactive-predictive engine and then the generated
completion has to be post-processed for the user. It is crucial that the pre-processing of
prefixes is fully compatible with the training corpus.
5.2 Xerox and EU Corpora
Six bilingual corpora were used for two different tasks and three different language
pairs in the framework of the TT2 project (SchlumbergerSema S.A. et al 2001).
The language pairs involved were English?Spanish, English?French, and English?
German (Khadivi and Goutte 2003), and the tasks were Xerox (Xerox printer manuals)
and EU (Bulletin of the European Union).
The three Xerox corpora were obtained from different user manuals for Xerox print-
ers (SchlumbergerSema S.A. et al 2001). The main features of these corpora are shown
in Table 1. Dividing the corpora into training and test sets was performed by randomly
selecting (without replacement) a specified amount of test sentences and leaving the
remaining ones for training. It is worth noting that the manuals were not the same in
each pair of languages. Even though all training and test sets have similar size, this
probably explains why the perplexity varies considerably over the different language
pairs. The vocabulary size was computed using the tokenized and true-case corpus.
The three bilingual EU corpora were extracted from the Bulletin of the European
Union, which exists in all official languages of the European Union (Khadivi and Goutte
17
Computational Linguistics Volume 35, Number 1
Table 1
The Xerox corpora. For all the languages, the training/test full-sentence overlap and the rate of
out-of-vocabulary test-set words were less than 10% and 1%, respectively. Trigram models were
used to compute the test word perplexity. (K and M denote thousands and millions,
respectively.)
English/Spanish English/German English/French
T
ra
in Sent. pairs (K) 56 49 53
Running words (M) 0.7/0.7 0.6/0.5 0.6/0.7
Vocabulary (K) 15/17 14/25 14/16
T
e
st
Sentences (K) 1.1 1.0 1.0
Running words (K) 8/10 12/12 11/12
Running chars. (K) 46/59 63/73 56/65
Perplexity 99/58 57/93 109/70
2003) and is publicly available on the Internet. The corpora used in the experiments
which are described subsequently were again acquired and processed in the framework
of the TT2 project. The main features of these corpora are shown in Table 2. The
vocabulary size and the training and test set partitions were obtained in a similar way
as with the Xerox corpora.
5.3 Assessment
In all the experiments reported in this article, system performance is assessed by
comparing test sentence translations produced by the translation systems with the
corresponding target language references of the test set. Some of the computed assess-
ment figures measure the quality of the translation engines without any system?user
interactivity:
 Word error rate (WER): The minimum number of substitution, insertion,
and deletion operations needed to convert the word strings produced by
the translation system into the corresponding single-reference word
strings. WER is normalized by the overall number of words in the
reference sentences (Och and Ney 2003).
Table 2
The EU corpora. For all the languages, the training/test full-sentence overlap and the rate of
out-of-vocabulary test-set words were less than 3% and 0.2%, respectively. Trigram models were
used to compute the test word perplexity. (K and M denote thousands and millions,
respectively.)
English/Spanish English/German English/French
T
ra
in Sent. pairs (K) 214 223 215
Running words (M) 5.2/5.9 5.7/5.4 5.3/6.0
Vocabulary (K) 84/97 86/153 84/91
T
e
st
Sentences (K) 0.8 0.8 0.8
Running words (K) 20/23 20/19 20/23
Running chars. (K) 119/135 120/134 119/134
Perplexity 58/46 57/87 58/45
18
Barrachina et al Statistical Computer-Assisted Translation
 Bilingual evaluation understudy (BLEU): This is based on the coverage of
n-grams of the hypothesized translation which occur in the reference
translations (Papineni et al 2001).
Other assessment figures are aimed at estimating the effort needed by a human
translator to produce correct translations using the interactive system. To this end, the
target translations which a real user would have in mind are simulated by the given
references. The first translation hypothesis for each given source sentence is compared
with a single reference translation and the longest common character prefix (LCP) is
obtained. The first non-matching character is replaced by the corresponding reference
character and then a new system hypothesis is produced. This process is iterated until
a full match with the reference is obtained.
Each computation of the LCP would correspond to the user looking for the next
error and moving the pointer to the corresponding position of the translation hypothesis.
Each character replacement, on the other hand, would correspond to a keystroke of
the user. If the first non-matching character is the first character of the new system
hypothesis in a given iteration, no LCP computation is needed; that is, no pointer
movement would be made by the user. Bearing this in mind, we define the following
interactive-predictive performance measures:
 Keystroke ratio (KSR): Number of keystrokes divided by the total number
of reference characters.
 Mouse-action ratio (MAR): Number of pointer movements plus one more
count per sentence (aimed at simulating the user action
needed to accept the final translation), divided by the total number of
reference characters.
 Keystroke and mouse-action ratio (KSMR): KSR plus MAR.
Note that KSR estimates only the user?s actions on the keyboard whereas MAR
estimates actions for which the user would typically use the mouse. From a user
point of view the two types of actions are different and require different types of
effort (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006). In any case, as an
approximation, KSMR accounts for both KSR and MAR, assuming that both actions
require a similar effort.
In the case of SMT systems, it is well known that an automatically computed
quality measure like BLEU correlates quite well with human judgment (Callison-Burch,
Osborne, and Koehn 2006). In the case of IPMT, we should keep in mind that the
main goal of (automatic) assessment is to estimate the effort of the human translator.
Moreover, translation quality is not an issue here, because the (simulated) human
intervention ensures ?perfect? translation results. The important question is whether
the (estimated) productivity of the human translator can really be increased or not by
the IPMT approach. In order to answer this question, the KSR and KSMR measures will
be used in the IPMT experiments to be reported in the next section.
In order to show the statistical significance of the results, all the assessment figures
reported in the next section are accompanied by the corresponding 95% confidence
intervals. These intervals have been computed using bootstrap sampling techniques, as
proposed by Bisani and Ney (2004), Koehn (2004), and Zhang and Vogel (2004).
19
Computational Linguistics Volume 35, Number 1
6. Results
Two types of results are reported for each corpus and for each translation approach.
The first are conventional MT results, obtained as a reference to give an idea of the
?classical? MT difficulty of the selected tasks. The second aim is to assess the interactive
MT (IPMT) approach proposed in this article.
The results are presented in different subsections. The first two subsections present
the MT and IPMT results for the 1-best translation obtained by the different techniques
in the Xerox and EU tasks, respectively. The third subsection presents further IPMT
results for the 5-best translations on a single pair of languages.
Some of these results may differ from results presented in previous works (Cubel
et al 2003; Och, Zens, and Ney 2003; Civera et al 2004a; Cubel et al 2004; Bender
et al 2005). The differences are due to variations in the pre-/post-processing procedures
and/or recent improvements of the search techniques used by the different systems.
6.1 Experiments with the Xerox Corpora
In this section, the translation results obtained using ATs, PBMs, and SFSTs for all six
language pairs of the Xerox corpus are reported. Word-based trigram and class-based
five-gram target-language models were used for the AT models (the parameters of the
log-linear model are tuned so as to minimize WER on a development corpus); word-
based trigram target-language models were used for PBMs and trigrams were used to
infer GIATI SFSTs.
Off-line MT Results. MT results with ATs, PBMs, and SFSTs are presented in Figure 3.
Results obtained using the PBMs are slightly but consistently better that those achieved
using the other models. In general, the different techniques perform similarly for the
various translation directions. However, the English?Spanish language pair is the one
for which the best translations can be produced.
IPMT Results. Performance has been measured in terms of KSRs and MARs (KSR and
MAR are represented as the lower and upper portions of each bar, respectively, and
KSMR is the whole bar length). The results are shown in Figure 4.
Figure 3
Off-line MT results (BLEU and WER) for the Xerox corpus. Segments above the bars show the
95% confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
20
Barrachina et al Statistical Computer-Assisted Translation
Figure 4
IPMT results for the Xerox corpus. In each bar, KSR is represented by the lower portion, MAR by
the upper portion, and KSMR is the whole bar. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
According to these results, a human translator assisted by an AT-based or a SFST-
based interactive system would only need an effort equivalent to typing about 20% of
the characters in order to produce the correct translations for the Spanish to English
task; or even less than 20% if a PBM-based system is used.
For the Xerox task, off-line MT performance and IPMT results show similar tenden-
cies. The PBMs show better performance for both the off-line MT and for the IPMT
assessment figures. The AT and SFST models perform more or less equivalently. In
both scenarios, the best results were achieved for the Spanish?English language pair
followed by French?English and German?English.
The computing times needed by all the systems involved in these experiments were
well within the range of the on-line operational requirements. The average initial time
for each source test sentence was very low (less than 50 msec) for PBMs and SFSTs
and adequate for ATs (772 msec). In the case of ATs and SFSTs, this included the time
required for the generation of the initial word-graph of each sentence. Moreover, the
most critical times incurred in the successive IPMT iterations were very low in all
the cases: 18 msec for ATs, 99 msec for PBMs, and 9 msec for SFSTs. Note, however,
that these average times are not exactly comparable because of the differences in the
computer hardware used by each system (2 Ghz AMD, 1.5 Ghz Pentium, and 2.4 Ghz
Pentium for ATs, PBMs, and SFSTs, respectively).
6.2 Experiments with the EU Corpora
The translation results using the AT, PBM, and SFST approaches for all six language
pairs of the EU corpus are reported in this section. As for the Xerox corpora, in the AT
experiments, word-based trigram and class-based five-gram target-language models
were used; in the PBM experiments, word-based trigram and class-based five-gram
target-language models were also used and five-grams were used to infer GIATI SFSTs.
Off-line MT Results. Figure 5 presents the results obtained using ATs, PBMs, and SFSTs.
Generally speaking, the results are comparable to those obtained on the Xerox corpus
with the exception of the English?Spanish language pair, which were better. With these
corpora, the best results were obtained with the ATs and PBMs for all the pairs and the
best translation direction was French-to-English with all the models used.
21
Computational Linguistics Volume 35, Number 1
Figure 5
Off-line MT results (BLEU and WER) for the EU corpus. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
IPMT Results. Figure 6 shows the performance of the AT, PBM, and SFST systems in
terms of KSRs and MARs in a similar way as for the Xerox corpora.
As in the MT experiments, the results are comparable to those obtained on the Xerox
corpus, with the exception of the English?Spanish pair. Similarly, as in MT, the best
results were obtained for the French-to-English translation direction.
Although EU is a more open-domain task, the results demonstrate again the poten-
tial benefit of computer-assisted translation systems. Using PBMs, a human translator
would only need an effort equivalent to typing about 20% of the characters in order
to produce the correct translations for French-to-English translation direction, whereas
for ATs and SFSTs the effort would be about 30%. For the other language pairs, the
efforts would be about 20?30% and 35% of the characters for PBMs and ATs/SFSTs,
respectively.
The systemwise correlation between MT and IPMT results on this corpus is not
as clear as in the Xerox case. One possible cause is the much larger size of the EU
corpus compared to the Xerox corpus. In order to run the EU experiments within rea-
sonable time limits, all the systems have required the use of beam search and/or other
Figure 6
IPMT results for the EU corpus. In each bar, KSR is represented by the lower portion, MAR by
the upper portion and KSMR is the whole bar. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
22
Barrachina et al Statistical Computer-Assisted Translation
Table 3
IPMT results (%) for the Xerox corpus (English?Spanish) using ATs, PBMs, and SFSTs for the
1-best hypothesis and 5-best hypotheses. 95% confidence intervals are shown.
1-best 5-best
Technique KSR KSMR KSR KSMR
AT 12.9?0.9 23.2?1.3 11.1?0.8 20.3?1.2
PBM 8.9?0.8 16.7?1.2 7.3?0.6 15.4?1.1
SFST 13.0?1.0 21.8?1.4 11.2?1.0 19.2?1.3
suboptimal pruning techniques, although this was largely unnecessary for the Xerox
corpus. Clearly, the pruning effects are different in the off-line (MT) and the on-line
(IPMT) search processes and the differences may lead to wide performance variations
for the AT, PBM, and SFST approaches.
Nevertheless, as can be seen in Bender et al (2005), the degradation in system
performance due to pruning is generally not too substantial and sufficiently accurate
real-time interactive operation could also be achieved in the EU task with the three
systems tested.
6.3 Results with n-Best Hypotheses
Further experiments were carried out to study the usefulness of n-best hypotheses in
the interactive framework. In this scenario, the user can choose one out of n proposed
translation suffixes and then proceed as in the usual IPMT paradigm. As with the
previous experiments, the automated evaluation is based on a selected target sentence
that best matches a prefix of the reference translation in each IPMT iteration (therefore
KSR is minimized).
Here, only IPMT results for the English-to-Spanish translation direction are re-
ported for both Xerox and EU tasks, using a list of the five best translations. These results
are shown in Tables 3 and 4.
In all the cases there is a clear and significant accuracy improvement when moving
from single-best to 5-best translations. This gain in translation quality diminishes in a
log-wise fashion as we increase the number of best translations. From a practical point
of view, the improvements provided by using n-best completions would come at the
cost of the user having to ponder which of these completions is more suitable. In a
real operational environment, this additional user effort may or may not outweigh the
Table 4
IPMT results (%) for the EU corpus (English?Spanish) using ATs, PBMs, and SFSTs for the 1-best
hypothesis and 5-best hypotheses. 95% confidence intervals are shown.
1-best 5-best
Technique KSR KSMR KSR KSMR
AT 20.2?0.9 32.6?1.3 18.5?0.8 29.9?1.2
PBM 16.3?0.7 27.8?1.1 13.2?0.6 25.0?1.1
SFST 21.3?0.9 33.0?1.3 19.3?0.9 29.9?1.3
23
Computational Linguistics Volume 35, Number 1
benefits of the n-best increased accuracy. Consequently, this feature should be offered to
the users as an option.
7. Practical Issues
IPMT results reported in the previous section provide reasonable estimations of potential
savings of human translator effort, assuming that the goal is to obtain high quality
translations. In real work, however, several practical issues not discussed in this article
may significantly affect the actual system usability and overall user productivity.
One of the most obvious issues is that a carefully designed graphical user interface
(GUI) is needed to let the users actually be in command of the translation process, so
that they really feel the system is assisting them rather than the other way around. In
addition, an adequate GUI has to provide adequate means for the users to easily and
intuitively change at will IPMT engine parameters that may have an impact on their
way of working with the system. To name just a few: The maximum length of system
hypotheses, the value of n for n-best suggestions, or the ?interaction step granularity?;
that is, whether the system should react at each user keystroke, or at the end of each
complete typed word, or after a sufficiently long typing pause, and so on.
Clearly, all these important issues are beyond the scope of the present article. But
we can comment that, in the TT2 project, complete prototypes of some of the systems
presented in this article, including the necessary GUI, were actually implemented and
thoroughly evaluated by professional human translators in their working environ-
ment (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
The results of these field tests showed that the actual productivity depended not
only on the individual translators, but also on the given test texts. In cases where these
texts were quite unrelated to the training data, the system did not significantly help
the human translators to increase their productivity. However, when the test texts were
reasonably well related to the training data, high productivity gains were registered?
close to what could be expected according to the KSR/MAR empirical results.
8. Concluding Remarks
The IPMT paradigm proposed in this article allows for a close collaboration between a
human translator and a machine translation system. This paradigm entails an iterative
process where, in each iteration, a data-driven machine translation engine suggests a
completion for the current prefix of a target sentence which a human translator can
accept, modify, or ignore.
This idea was originally proposed in the TransType project (Langlais, Foster, and
Lapalme 2000), where a simple engine was used which only supported single-token
suggestions. Furthering these ideas, in the TransType2 project (SchlumbergerSema S.A.
et al 2001), state-of-the-art statistical machine translation systems have been developed
and integrated in the IPMT framework.
In a laboratory environment, results on two different tasks suggest that the pro-
posed techniques can reduce the typing effort needed to produce a high-quality transla-
tion of a given source text by as much as 80% with respect to the effort needed to simply
type the whole translation. In real conditions, a high productivity gain was achieved in
many cases.
We have studied here IPMT from the point of view of a standalone CAT tool.
Nevertheless, IPMT can of course be easily and conveniently combined with other
popular translator workbench tools. More specifically, IPMT lends itself particularly
24
Barrachina et al Statistical Computer-Assisted Translation
well to addressing the typical lack of generalization capabilities of translation memories.
When used as a CAT tool, translation memories allow the human translator to keep
producing increasingly long segments of correct target text. Clearly, these segments can
be used by an IPMT engine to suggest to the translator possible translations for source
text segments that are not found in the translation memories as exact matches.
Acknowledgments
This work has been partially supported by
the ST Programme of European Union under
grant IST-2001-32091, by the Spanish project
TIC?2003-08681-C02-02, and the Spanish
research programme Consolider
Ingenio-2010 CSD2007-00018. The authors
wish to thank the anonymous reviewers for
their criticisms and suggestions.
References
Amengual, J. C., J. M. Bened??, A. Castan?o,
A. Castellanos, V. M. Jime?nez, D. Llorens,
A. Marzal, M. Pastor, F. Prat, E. Vidal, and
J. M. Vilar. 2000. The EuTrans-I speech
translation system. Machine Translation,
15:75?103.
Amengual, J. C. and E. Vidal. 1998. Efficient
error-correcting Viterbi parsing. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 20(10):1109?1116.
Bender, O., S. Hasan, D. Vilar, R. Zens, and
H. Ney. 2005. Comparison of generation
strategies for interactive machine
translation. In Proceedings of the 10th
Annual Conference of the European
Association for Machine Translation (EAMT
05), pages 33?40, Budapest.
Berger, A. L., P. F. Brown, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, A. S. Kehler,
and R. L. Mercer. 1996. Language
translation apparatus and method of using
context-based translation models. United
States Patent No. 5510981, April.
Berstel, J. 1979. Transductions and Context-Free
Languages. B. G. Teubner, Stuttgart.
Bisani, M. and H. Ney. 2004. Bootstrap
estimates for confidence intervals in ASR
performance evaluation. In Proceedings of
the International Conference on Acoustic,
Speech and Signal Processing (ICASSP 04),
volume 1, pages 409?412, Montreal.
Bowker, L. 2002. Computer-Aided Translation
Technology: A Practical Introduction,
chapter 5: Translation-memory systems.
Didactics of Translation. University of
Ottawa Press, pages 92?127.
Brown, P. F., J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, F. Jelinek, J. D. Lafferty,
R. L. Mercer, and P. S. Roosin. 1990.
A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Brown, P. F., S. A. Della Pietra, V. J.
Della Pietra, and R. L. Mercer. 1993. The
mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?310.
Callison-Burch, C., M. Osborne, and
P. Koehn. 2006. Re-evaluating the role of
BLEU in machine translation research. In
Proceedings of the 10th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL 06),
pages 249?256, Trento.
Casacuberta, F., H. Ney, F. J. Och, E. Vidal,
J. M. Vilar, S. Barrachina, I. Garc??a-Varea,
D. Llorens, C. Mart??nez, S. Molau,
F. Nevado, M. Pastor, D. Pico?, A. Sanchis,
and C. Tillmann. 2004a. Some approaches
to statistical and finite-state
speech-to-speech translation. Computer
Speech and Language, 18:25?47.
Casacuberta, F. and E. Vidal. 2004. Machine
translation with inferred stochastic
finite-state transducers. Computational
Linguistics, 30(2):205?225.
Casacuberta, F. and E. Vidal. 2007. Learning
finite-state models for machine translation.
Machine Learning, 66(1):69?91.
Casacuberta, F., E. Vidal, and D. Pico?. 2005.
Inference of finite-state transducers from
regular languages. Pattern Recognition,
38:1431?1443.
Casacuberta, F., E. Vidal, A. Sanchis, and
J. M. Vilar. 2004b. Pattern recognition
approaches for speech-to-speech
translation. Cybernetic and Systems: an
International Journal, 35(1):3?17.
Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,
S. Barrachina, E. Vidal, F. Casacuberta,
D. Pico?, and J. Gonza?lez. 2004a. From
machine translation to computer assisted
translation using finite-state models. In
Proceedings of the Conference on Empirical
Methods for Natural Language Processing
(EMNLP 04), pages 349?356, Barcelona.
Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,
S. Barrachina, F. Casacuberta, E. Vidal,
D. Pico?, and J. Gonza?lez. 2004b. A syntactic
pattern recognition approach to computer
assisted translation. In Advances in
25
Computational Linguistics Volume 35, Number 1
Statistical, Structural and Syntactical Pattern
Recognition, Proceedings of the Joint IAPR
International Workshops on Syntactical and
Structural Pattern Recognition (SSPR 04)
and Statistical Pattern Recognition
(SPR 04)), Lisbon, Portugal, August 18?20,
volume 3138 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 207?215.
Cubel, E., J. Civera, J. M. Vilar, A. L. Lagarda,
S. Barrachina, E. Vidal, F. Casacuberta,
D. Pico?, J. Gonza?lez, and L. Rodr??guez.
2004. Finite-state models for computer
assisted translation. In Proceedings of the
16th European Conference on Artificial
Intelligence (ECAI 04), pages 586?590,
Valencia.
Cubel, E., J. Gonza?lez, A. Lagarda,
F. Casacuberta, A. Juan, and E. Vidal. 2003.
Adapting finite-state translation to the
TransType2 project. In Proceedings of the
Joint Conference Combining the 8th
International Workshop of the European
Association for Machine Translation and the
4th Controlled Language Applications Workshop
(EAMT-CLAW 03), pages 54?60, Dublin.
Foster, G. 2002. Text Prediction for Translators.
Ph.D. thesis, Universite? de Montre?al,
Canada.
Foster, G., P. Isabelle, and P. Plamondon.
1997. Target-text mediated interactive
machine translation. Machine Translation,
12(1?2):175?194.
Isabelle, P. and K. Church. 1997. Special issue
on new tools for human translators.
Machine Translation, 12(1?2).
Jelinek, F. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge,
MA.
Jime?nez, V. M. and A. Marzal. 1999.
Computing the k shortest paths: a new
algorithm and an experimental
comparison. In Algorithm Engineering:
Proceedings of the 3rd International
Workshop (WAE 99), London, UK, July 19?21,
volume 1668 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 15?29.
Kay, M. 1997. The proper place of men and
machines in language translation. Machine
Translation, 12:3?23. [This article first
appeared as a Xerox PARC Working Paper
in 1980].
Khadivi, S. and C. Goutte. 2003. Tools for
corpus alignment and evaluation of the
alignments (deliverable d4.9). Technical
report, TransType2 (IST-2001-32091).
Khadivi, S., R. Zens, and H. Ney. 2006.
Integration of speech to computer-assisted
translation using finite-state automata.
In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics
and 21th International Conference on
Computational Linguistics (COLING/ACL
06), pages 467?474, Sydney.
Khadivi, S., A. Zolnay, and H. Ney. 2005.
Automatic text dictation in
computer-assisted translation. In
Proceedings of the European Conference on
Speech Communication and Technology,
(INTERSPEECH 05-EUROSPEECH),
pages 2265?2268, Lisbon.
Koehn, P. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the Conference on
Empirical Methods for Natural Language
Processing (EMNLP 04), pages 388?395,
Barcelona.
Koehn, P., F. J. Och, and D. Marcu. 2003.
Statistical phrase-based translation. In
Proceedings of the 2003 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 03),
pages 127?133, Edmonton.
Langlais, P., G. Foster, and G. Lapalme. 2000.
TransType: a computer-aided translation
typing system. In Proceedings of the
NAACL/ANLP Workshop on Embedded
Machine Translation Systems, pages 46?52,
Seattle, WA.
Langlais, P., G. Lapalme, and M. Loranger.
2002. Transtype: Development-evaluation
cycles to boost translator?s productivity.
Machine Translation, 15(4):77?98.
Macklovitch, E. 2006. TransType2: The last
word. In Proceedings of the 5th International
Conference on Languages Resources and
Evaluation (LREC 06), pages 167?172,
Genoa.
Macklovitch, E., N. T. Nguyen, and R. Silva.
2005. User evaluation report. Technical
report, TransType2 (IST-2001-32091).
Marcu, D. and W. Wong. 2002. A
phrase-based, joint probability model
for statistical machine translation.
In Proceedings of the Conference on
Empirical Methods for Natural Language
Processing (EMNLP 02), pages 133?139,
Philadelphia, PA.
Ney, H., S. Nie?en, F. Och, H. Sawaf,
C. Tillmann, and S. Vogel. 2000.
Algorithms for statistical translation of
spoken language. IEEE Transactions on
Speech and Audio Processing, 8(1):24?36.
Och, F. J. 1999. An efficient method for
determining bilingual word classes. In
Proceedings of the 9th Conference of the
European Chapter of the Association for
26
Barrachina et al Statistical Computer-Assisted Translation
Computational Linguistics (EACL 99),
pages 71?76, Bergen.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Och, F. J. and H. Ney. 2004. The alignment
template approach to statistical machine
translation. Computational Linguistics,
30(4):417?450.
Och, F. J., R. Zens, and H. Ney. 2003.
Efficient search for interactive statistical
machine translation. In Proceedings of
the 10th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL 03), pages 387?393,
Budapest.
Papineni, K., S. Roukos, T. Ward, and
W. Zhu. 2001. BLEU: a method for
automatic evaluation of machine
translation. Technical Report RC22176,
Thomas J. Watson Research Center.
Pico?, D. and F. Casacuberta. 2001. Some
statistical-estimation methods for
stochastic finite-state transducers. Machine
Learning, 44:121?142.
Press, W. H., S. A. Teukolsky, W. T.
Vetterling, and B. P. Flannery. 2002.
Numerical Recipes in C++: The Art of
Scientific Computing. Cambridge University
Press, Cambridge, UK.
SchlumbergerSema S.A., Intituto Tecnolo?gico
de Informa?tica, Rheinisch Westfa?lische
Technische Hochschule Aachen Lehrstul
fu?r Informatik VI, Recherche Applique?e
en Linguistique Informatique Laboratory
University of Montreal, Celer Soluciones,
Socie?te? Gamma, and Xerox
Research Centre Europe. 2001. TT2.
TransType2?computer-assisted
translation. Project technical annex.
Information Society Technologies (IST)
Programme, IST-2001-32091.
Sen, Z., Ch. Zhaoxiong, and H. Heyan. 1997.
Interactive approach in machine translation
systems. In Proceedings of IEEE International
Conference on Intelligent Processing Systems
(ICIPS 97), pages 1814?1819, Beijing.
Slocum, J. 1985. A survey of machine
translation: Its history, current status and
future prospects. Computational Linguistics,
11(1):1?17.
Somers, H., 2003. Computers and Translation: a
Translator?s Guide, chapter 3: Translation
memory systems. John Benjamins,
Amsterdam, pages 31?48.
Toma?s, J. and F. Casacuberta. 2001.
Monotone statistical translation using
word groups. In Proceedings of the Machine
Translation Summit VIII (MT SUMMIT
VIII), pages 357?361, Santiago de
Compostela.
Toma?s, J. and F. Casacuberta. 2003.
Combining phrase-based and
template-based alignment models in
statistical translation. In Pattern Recognition
and Image Analysis, Proceedings of the First
Iberian Conference (IbPRIA 03), Puerto
de Andratx, Mallorca, Spain, June 4-6,
volume 2652 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 1020?1031.
Toma?s, J. and F. Casacuberta. 2004. Statistical
machine translation decoding using
target word reordering. In Advances in
Statistical, Structural and Syntactical Pattern
Recognition, Proceedings of the Joint IAPR
International Workshops on Syntactical
and Structural Pattern Recognition
(SSPR 04) and Statistical Pattern Recognition
(SPR 04), Lisbon, Portugal, August 18?20,
volume 3138 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 734?743.
Toma?s, J. and F. Casacuberta. 2006. Statistical
phrase-based models for interactive
computer-assisted translation. In
Proceedings of the 44th Annual Meeting
of the Association for Computational
Linguistics and 21th International
Conference on Computational Linguistics
(COLING/ACL 06), pages 835?841, Sydney.
Toma?s, J. and F. Casacuberta. 2007. A
pattern recognition approach to
machine translation: Monotone and
non-monotone phrase-based statistical
models. Technical Report DSIC-II/18/07,
Departamento de Sistemas Informa?ticos y
Computacio?n, Universidad Polite?cnica
de Valencia.
Tomita, M. 1985. Feasibility study of
personal/interactive machine translation
systems. In Proceedings of the First
International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI 85), pages 289?297,
New York, NY.
Ueffing, N., F. J. Och, and H. Ney. 2002.
Generation of word graphs in statistical
machine translation. In Proceedings of
the Conference on Empirical Methods for
Natural Language Processing (EMNLP 02),
pages 156?163, Philadelphia, PA.
Vidal, E. 1997. Finite-state speech-to-speech
translation. In Proceedings of the
International Conference on Acoustic,
Speech and Signal Processing (ICASSP 97),
volume 1, pages 111?114, Munich.
27
Computational Linguistics Volume 35, Number 1
Vidal, E. and F. Casacuberta. 2004. Learning
finite-state models for machine translation.
In Grammatical Inference: Algorithms and
Applications, Proceedings of the 7th
International Coloquium on Grammatical
Inference (ICGI 04), Athens, Greece,
October 11?13, volume 3264 of Lecture
Notes in Artificial Intelligence. Springer,
Heidelberg, pages 16?27.
Vidal, E., F. Casacuberta, L. Rodr??guez,
J. Civera, and C. Mart??nez. 2006.
Computer-assisted translation using
speech recognition. IEEE Transactions
on Speech and Audio Processing,
14(3):941?951.
Vidal, E., F. Thollard, F. Casacuberta
C. de la Higuera, and R. Carrasco. 2005.
Probabilistic finite-state machines?
part II. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
27(7):1025?1039.
Whitelock, P. J., M. McGee Wood, B. J.
Chandler, N. Holden, and H. J. Horsfall.
1986. Strategies for interactive machine
translation: The experience and
implications of the UMIST Japanese
project. In Proceedings of the 11th
International Conference on Computational
Linguistics (COLING 86), pages 329?334,
Bonn.
Yamron, J., J. Baker, P. Bamberg,
H. Chevalier, T. Dietzel, J. Elder,
F. Kampmann, M. Mandel, L. Manganaro,
T. Margolis, and E. Steele. 1993.
LINGSTAT: an interactive, machine-aided
translation system. In Proceedings of the
Workshop on Human Language Technology,
pages 191?195, Princeton, NJ.
Zajac, R. 1988. Interactive translation: A new
approach. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING 88), pages 785?790,
Budapest.
Zens, R. and H. Ney. 2004. Improvements
in phrase-based statistical machine
translation. In Proceedings of the Human
Language Technology Conference / North
American Chapter of the Association for
Computational Linguistics Annual Meeting
(HLT-NAACL 04), pages 257?264,
Boston, MA.
Zens, R., F. J. Och, and H. Ney. 2002.
Phrase-based statistical machine
translation. In Advances in Artificial
Intelligence. 25th Annual German Conference
on Artificial Intelligence (KI 02), Aachen,
Germany, September 16?22, Proceedings,
volume 2479 of Lecture Notes on Artificial
Intelligence. Springer Verlag, Heidelberg,
pages 18?32.
Zhang, Y. and S. Vogel. 2004. Measuring
confidence intervals for the machine
translation evaluation metrics. In
Proceedings of the Tenth International
Conference on Theoretical and
Methodological Issues in Machine
Translation (TMI 04), pages 294?301,
Baltimore, MD.
28
Maximum Entropy Models for Named Entity Recognition
Oliver Bender
 
and Franz Josef Och

and Hermann Ney
 
 
Lehrstuhl fu?r Informatik VI

Information Sciences Institute
Computer Science Department University of Southern California
RWTH Aachen - University of Technology Marina del Rey, CA 90292
D-52056 Aachen, Germany och@isi.edu

bender,ney  @cs.rwth-aachen.de
Abstract
In this paper, we describe a system that applies
maximum entropy (ME) models to the task of
named entity recognition (NER). Starting with
an annotated corpus and a set of features which
are easily obtainable for almost any language,
we first build a baseline NE recognizer which
is then used to extract the named entities and
their context information from additional non-
annotated data. In turn, these lists are incor-
porated into the final recognizer to further im-
prove the recognition accuracy.
1 Introduction
In this paper, we present an approach for extracting the
named entities (NE) of natural language inputs which
uses the maximum entropy (ME) framework (Berger et
al., 1996). The objective can be described as follows.
Given a natural input sequence 	
    

we
choose the NE tag sequence 
 






with the
highest probability among all possible tag sequences:



  ffReranking Translation Hypotheses Using Structural Properties
Sas?a Hasan, Oliver Bender, Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
D-52056 Aachen, Germany
{hasan,bender,ney}@cs.rwth-aachen.de
Abstract
We investigate methods that add syntac-
tically motivated features to a statistical
machine translation system in a reranking
framework. The goal is to analyze whether
shallow parsing techniques help in iden-
tifying ungrammatical hypotheses. We
show that improvements are possible by
utilizing supertagging, lightweight depen-
dency analysis, a link grammar parser and
a maximum-entropy based chunk parser.
Adding features to n-best lists and dis-
criminatively training the system on a de-
velopment set increases the BLEU score
up to 0.7% on the test set.
1 Introduction
Statistically driven machine translation systems
are currently the dominant type of system in the
MT community. Though much better than tradi-
tional rule-based approaches, these systems still
make a lot of errors that seem, at least from a hu-
man point of view, illogical.
The main purpose of this paper is to investigate
a means of identifying ungrammatical hypotheses
from the output of a machine translation system
by using grammatical knowledge that expresses
syntactic dependencies of words or word groups.
We introduce several methods that try to establish
this kind of linkage between the words of a hy-
pothesis and, thus, determine its well-formedness,
or ?fluency?. We perform rescoring experiments
that rerank n-best lists according to the presented
framework.
As methodologies deriving well-formedness of
a sentence we use supertagging (Bangalore and
Joshi, 1999) with lightweight dependency anal-
ysis (LDA)1 (Bangalore, 2000), link grammars
(Sleator and Temperley, 1993) and a maximum-
entropy (ME) based chunk parser (Bender et al,
2003). The former two approaches explicitly
model the syntactic dependencies between words.
Each hypothesis that contains irregularities, such
as broken linkages or non-satisfied dependencies,
should be penalized or rejected accordingly. For
the ME chunker, the idea is to train n-gram mod-
els on the chunk or POS sequences and directly
use the log-probability as feature score.
In general, these concepts and the underlying
programs should be robust and fast in order to be
able to cope with large amounts of data (as it is the
case for n-best lists). The experiments presented
show a small though consistent improvement in
terms of automatic evaluation measures chosen for
evaluation. BLEU score improvements, for in-
stance, lie in the range from 0.3 to 0.7% on the
test set.
In the following, Section 2 gives an overview
on related work in this domain. In Section 3
we review our general approach to statistical ma-
chine translation (SMT) and introduce the main
methodologies used for deriving syntactic depen-
dencies on words or word groups, namely su-
pertagging/LDA, link grammars and ME chunk-
ing. The corpora and the experiments are dis-
cussed in Section 4. The paper is concluded in
Section 5.
2 Related work
In (Och et al, 2004), the effects of integrating
syntactic structure into a state-of-the-art statistical
machine translation system are investigated. The
approach is similar to the approach presented here:
1In the context of this work, the term LDA is not to be
confused with linear discriminant analysis.
41
firstly, a word graph is generated using the base-
line SMT system and n-best lists are extracted ac-
cordingly, then additional feature functions repre-
senting syntactic knowledge are added and the cor-
responding scaling factors are trained discrimina-
tively on a development n-best list.
Och and colleagues investigated a large amount
of different feature functions. The field of appli-
cation varies from simple syntactic features, such
as IBM model 1 score, over shallow parsing tech-
niques to more complex methods using grammars
and intricate parsing procedures. The results were
rather disappointing. Only one of the simplest
models, i.e. the implicit syntactic feature derived
from IBM model 1 score, yielded consistent and
significant improvements. All other methods had
only a very small effect on the overall perfor-
mance.
3 Framework
In the following sections, the theoretical frame-
work of statistical machine translation using a di-
rect approach is reviewed. We introduce the su-
pertagging and lightweight dependency analysis
approach, link grammars and maximum-entropy
based chunking technique.
3.1 Direct approach to SMT
In statistical machine translation, the best trans-
lation e?I?1 = e?1 . . . e?i . . . e?I? of source words fJ1 =
f1 . . . fj . . . fJ is obtained by maximizing the con-
ditional probability
e?I?1 = argmax
I,eI1
{Pr(eI1|fJ1 )}
= argmax
I,eI1
{Pr(fJ1 |eI1) ? Pr(eI1)}
(1)
using Bayes decision rule. The first probability
on the right-hand side of the equation denotes the
translation model whereas the second is the target
language model.
An alternative to this classical source-channel
approach is the direct modeling of the posterior
probability Pr(eI1|fJ1 ) which is utilized here. Us-
ing a log-linear model (Och and Ney, 2002), we
obtain
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
) ,
(2)
where ?m are the scaling factors of the models de-
noted by feature functions hm(?). The denomina-
tor represents a normalization factor that depends
only on the source sentence fJ1 . Therefore, we can
omit it during the search process, leading to the
following decision rule:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach. It has the advantage that ad-
ditional models h(?) can be easily integrated into
the overall system. The model scaling factors
?M1 are trained according to the maximum en-
tropy principle, e.g., using the GIS algorithm. Al-
ternatively, one can train them with respect to
the final translation quality measured by an error
criterion (Och, 2003). For the results reported
in this paper, we optimized the scaling factors
with respect to a linear interpolation of word error
rate (WER), position-independent word error rate
(PER), BLEU and NIST score using the Downhill
Simplex algorithm (Press et al, 2002).
3.2 Supertagging/LDA
Supertagging (Bangalore and Joshi, 1999) uses the
Lexicalized Tree Adjoining Grammar formalism
(LTAG) (XTAG Research Group, 2001). Tree Ad-
joining Grammars incorporate a tree-rewriting for-
malism using elementary trees that can be com-
bined by two operations, namely substitution and
adjunction, to derive more complex tree structures
of the sentence considered. Lexicalization allows
us to associate each elementary tree with a lexical
item called the anchor. In LTAGs, every elemen-
tary tree has such a lexical anchor, also called head
word. It is possible that there is more than one el-
ementary structure associated with a lexical item,
as e.g. for the case of verbs with different subcat-
egorization frames.
The elementary structures, called initial and
auxiliary trees, hold all dependent elements within
the same structure, thus imposing constraints on
the lexical anchors in a local context. Basically,
supertagging is very similar to part-of-speech tag-
ging. Instead of POS tags, richer descriptions,
namely the elementary structures of LTAGs, are
annotated to the words of a sentence. For this pur-
pose, they are called supertags in order to distin-
guish them from ordinary POS tags. The result
is an ?almost parse? because of the dependencies
42
very[?2]
food[?1] delicious[?3]
the[?1]
was[?2]
Figure 1: LDA: example of a derivation tree, ?
nodes are the result of the adjunction operation on
auxiliary trees, ? nodes of substitution on initial
trees.
coded within the supertags. Usually, a lexical item
can have many supertags, depending on the vari-
ous contexts it appears in. Therefore, the local am-
biguity is larger than for the case of POS tags. An
LTAG parser for this scenario can be very slow, i.e.
its computational complexity is in O(n6), because
of the large number of supertags, i.e. elementary
trees, that have to be examined during a parse. In
order to speed up the parsing process, we can ap-
ply n-gram models on a supertag basis in order to
filter out incompatible descriptions and thus im-
prove the performance of the parser. In (Banga-
lore and Joshi, 1999), a trigram supertagger with
smoothing and back-off is reported that achieves
an accuracy of 92.2% when trained on one million
running words.
There is another aspect to the dependencies
coded in the elementary structures. We can use
them to actually derive a shallow parse of the sen-
tence in linear time. The procedure is presented
in (Bangalore, 2000) and is called lightweight de-
pendency analysis. The concept is comparable to
chunking. The lightweight dependency analyzer
(LDA) finds the arguments for the encoded depen-
dency requirements. There exist two types of slots
that can be filled. On the one hand, nodes marked
for substitution (in ?-trees) have to be filled by the
complements of the lexical anchor. On the other
hand, the foot nodes (i.e. nodes marked for adjunc-
tion in ?-trees) take words that are being modified
by the supertag. Figure 1 shows a tree derived by
LDA on the sentence the food was very delicious
from the C-Star?03 corpus (cf. Section 4.1).
The supertagging and LDA tools are available
from the XTAG research group website.2
As features considered for the reranking exper-
iments we choose:
2http://www.cis.upenn.edu/?xtag/
D D EA EA
P P
SS
the food very deliciouswas
Figure 2: Link grammar: example of a valid link-
age satisfying all constraints.
? Supertagger output: directly use the log-
likelihoods as feature score. This did not im-
prove performance significantly, so the model
was discarded from the final system.
? LDA output:
? dependency coverage: determine the
number of covered elements, i.e. where
the dependency slots are filled to the left
and right
? separate features for the number of mod-
ifiers and complements determined by
the LDA
3.3 Link grammar
Similar to the ideas presented in the previous sec-
tion, link grammars also explicitly code depen-
dencies between words (Sleator and Temperley,
1993). These dependencies are called links which
reflect the local requirements of each word. Sev-
eral constraints have to be satisfied within the link
grammar formalism to derive correct linkages, i.e.
sets of links, of a sequence of words:
1. Planarity: links are not allowed to cross each
other
2. Connectivity: links suffice to connect all
words of a sentence
3. Satisfaction: linking requirements of each
word are satisfied
An example of a valid linkage is shown in Fig-
ure 2. The link grammar parser that we use is
freely available from the authors? website.3 Sim-
ilar to LTAG, the link grammar formalism is lex-
icalized which allows for enhancing the methods
with probabilistic n-gram models (as is also the
case for supertagging). In (Lafferty et al, 1992),
the link grammar is used to derive a new class of
3http://www.link.cs.cmu.edu/link/
43
[NP the food ] [VP was] [ADJP very delicious]
the/DT food/NN was/VBD very/RB delicious/JJ
Figure 3: Chunking and POS tagging: a tag next
to the opening bracket denotes the type of chunk,
whereas the corresponding POS tag is given after
the word.
language models that, in comparison to traditional
n-gram LMs, incorporate capabilities for express-
ing long-range dependencies between words.
The link grammar dictionary that specifies the
words and their corresponding valid links cur-
rently holds approximately 60 000 entries and han-
dles a wide variety of phenomena in English. It is
derived from newspaper texts.
Within our reranking framework, we use link
grammar features that express a possible well-
formedness of the translation hypothesis. The sim-
plest feature is a binary one stating whether the
link grammar parser could derive a complete link-
age or not, which should be a strong indicator of
a syntactically correct sentence. Additionally, we
added a normalized cost of the matching process
which turned out not to be very helpful for rescor-
ing, so it was discarded.
3.4 ME chunking
Like the methods described in the two preced-
ing sections, text chunking consists of dividing a
text into syntactically correlated non-overlapping
groups of words. Figure 3 shows again our ex-
ample sentence illustrating this task. Chunks are
represented as groups of words between square
brackets. We employ the 11 chunk types as de-
fined for the CoNLL-2000 shared task (Tjong Kim
Sang and Buchholz, 2000).
For the experiments, we apply a maximum-
entropy based tagger which has been successfully
evaluated on natural language understanding and
named entity recognition (Bender et al, 2003).
Within this tool, we directly factorize the poste-
rior probability and determine the corresponding
chunk tag for each word of an input sequence. We
assume that the decisions depend only on a lim-
ited window ei+2i?2 = ei?2...ei+2 around the current
word ei and on the two predecessor chunk tags
ci?1i?2. In addition, part-of-speech (POS) tags gI1
are assigned and incorporated into the model (cf.
Figure 3). Thus, we obtain the following second-
order model:
Pr(cI1|eI1, gI1) =
=
I
?
i=1
Pr(ci|ci?11 , eI1, gI1) (4)
=
I
?
i=1
p(ci|ci?1i?2, ei+2i?2, gi+2i?2), (5)
where the step from Eq. 4 to 5 reflects our model
assumptions.
Furthermore, we have implemented a set of bi-
nary valued feature functions for our system, in-
cluding lexical, word and transition features, prior
features, and compound features, cf. (Bender et
al., 2003). We run simple count-based feature
reduction and train the model parameters using
the Generalized Iterative Scaling (GIS) algorithm
(Darroch and Ratcliff, 1972). In practice, the
training procedure tends to result in an overfitted
model. To avoid this, a smoothing method is ap-
plied where a Gaussian prior on the parameters is
assumed (Chen and Rosenfeld, 1999).
Within our reranking framework, we firstly use
the ME based tagger to produce the POS and
chunk sequences for the different n-best list hy-
potheses. Given several n-gram models trained on
the WSJ corpus for both POS and chunk models,
we then rescore the n-best hypotheses and simply
use the log-probabilities as additional features. In
order to adapt our system to the characteristics of
the data used, we build POS and chunk n-gram
models on the training corpus part. These domain-
specific models are also added to the n-best lists.
The ME chunking approach does not model ex-
plicit syntactic linkages of words. Instead, it in-
corporates a statistical framework to exploit valid
and syntactically coherent groups of words by ad-
ditionally looking at the word classes.
4 Experiments
For the experiments, we use the translation sys-
tem described in (Zens et al, 2005). Our phrase-
based decoder uses several models during search
that are interpolated in a log-linear way (as ex-
pressed in Eq. 3), such as phrase-based translation
models, word-based lexicon models, a language,
deletion and simple reordering model and word
and phrase penalties. A word graph containing
the most likely translation hypotheses is generated
during the search process. Out of this compact
44
Supplied Data Track
Arabic Chinese Japanese English
Train Sentences 20 000
Running Words 180 075 176 199 198 453 189 927
Vocabulary 15 371 8 687 9 277 6 870
Singletons 8 319 4 006 4 431 2 888
C-Star?03 Sentences 506
Running Words 3 552 3 630 4 130 3 823
OOVs (Running Words) 133 114 61 65
IWSLT?04 Sentences 500
Running Words 3 597 3 681 4 131 3 837
OOVs (Running Words) 142 83 71 58
Table 1: Corpus statistics after preprocessing.
representation, we extract n-best lists as described
in (Zens and Ney, 2005). These n-best lists serve
as a starting point for our experiments. The meth-
ods presented in Section 3 produce scores that are
used as additional features for the n-best lists.
4.1 Corpora
The experiments are carried out on a subset
of the Basic Travel Expression Corpus (BTEC)
(Takezawa et al, 2002), as it is used for the sup-
plied data track condition of the IWSLT evaluation
campaign. BTEC is a multilingual speech corpus
which contains tourism-related sentences similar
to those that are found in phrase books. For the
supplied data track, the training corpus contains
20 000 sentences. Two test sets, C-Star?03 and
IWSLT?04, are available for the language pairs
Arabic-English, Chinese-English and Japanese-
English.
The corpus statistics are shown in Table 1. The
average source sentence length is between seven
and eight words for all languages. So the task is
rather limited and very domain-specific. The ad-
vantage is that many different reranking experi-
ments with varying feature function settings can
be carried out easily and quickly in order to ana-
lyze the effects of the different models.
In the following, we use the C-Star?03 set for
development and tuning of the system?s parame-
ters. After that, the IWSLT?04 set is used as a
blind test set in order to measure the performance
of the models.
4.2 Rescoring experiments
The use of n-best lists in machine translation has
several advantages. It alleviates the effects of the
huge search space which is represented in word
graphs by using a compact excerpt of the n best
hypotheses generated by the system. Especially
for limited domain tasks, the size of the n-best list
can be rather small but still yield good oracle er-
ror rates. Empirically, n-best lists should have an
appropriate size such that the oracle error rate, i.e.
the error rate of the best hypothesis with respect to
an error measure (such asWER or PER) is approx-
imately half the baseline error rate of the system.
N -best lists are suitable for easily applying several
rescoring techniques since the hypotheses are al-
ready fully generated. In comparison, word graph
rescoring techniques need specialized tools which
can traverse the graph accordingly. Since a node
within a word graph allows for many histories, one
can only apply local rescoring techniques, whereas
for n-best lists, techniques can be used that con-
sider properties of the whole sentence.
For the Chinese-English and Arabic-English
task, we set the n-best list size to n = 1500. For
Japanese-English, n = 1000 produces oracle er-
ror rates that are deemed to be sufficiently low,
namely 17.7% and 14.8% for WER and PER, re-
spectively. The single-best output for Japanese-
English has a word error rate of 33.3% and
position-independent word error rate of 25.9%.
For the experiments, we add additional fea-
tures to the initial models of our decoder that have
shown to be particularly useful in the past, such as
IBM model 1 score, a clustered language model
score and a word penalty that prevents the hy-
potheses to become too short. A detailed defini-
tion of these additional features is given in (Zens
et al, 2005). Thus, the baseline we start with is
45
Chinese ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 8.17 46.2 48.6 41.4
with supertagging/LDA 8.29 46.5 48.4 41.0
with link grammar 8.43 45.6 47.9 41.1
with supertagging/LDA + link grammar 8.22 47.5 47.7 40.8
with ME chunker 8.65 47.3 47.4 40.4
with all models 8.42 47.0 47.4 40.5
Chinese ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 8.67 45.5 49.1 39.8
with supertagging/LDA 8.68 45.4 49.8 40.3
with link grammar 8.81 45.0 49.0 40.2
with supertagging/LDA+link grammar 8.56 46.0 49.1 40.6
with ME chunker 9.00 44.6 49.3 40.6
with all models 8.89 46.2 48.1 39.6
Table 2: Effect of successively adding syntactic features to the Chinese-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
BASE Any messages for me?
RESC Do you have any messages for me?
REFE Do you have any messages for me?
BASE She, not yet?
RESC She has not come yet?
REFE Lenny, she has not come in?
BASE How much is it to the?
RESC How much is it to the local call?
REFE How much is it to the city centre?
BASE This blot or.
RESC This is not clean.
REFE This still is not clean.
Table 3: Translation examples for the Chinese-
English test set (IWSLT?04): baseline system
(BASE) vs. rescored hypotheses (RESC) and refer-
ence translation (REFE).
already a very strong one. The log-linear inter-
polation weights ?m from Eq. 3 are directly opti-
mized using the Downhill Simplex algorithm on a
linear combination of WER (word error rate), PER
(position-independent word error rate), NIST and
BLEU score.
In Table 2, we show the effect of adding the
presented features successively to the baseline.
Separate entries for experiments using supertag-
ging/LDA and link grammars show that a combi-
nation of these syntactic approaches always yields
some gain in translation quality (regarding BLEU
score). The performance of the maximum-entropy
based chunking is comparable. A combination of
all three models still yields a small improvement.
Table 3 shows some examples for the Chinese-
English test set. The rescored translations are syn-
tactically coherent, though semantical correctness
cannot be guaranteed. On the test data, we achieve
an overall improvement of 0.7%, 0.5% and 0.3%
in BLEU score for Chinese-English, Japanese-
English and Arabic-English, respectively (cf. Ta-
bles 4 and 5).
4.3 Discussion
From the tables, it can be seen that the use of
syntactically motivated feature functions within
a reranking concept helps to slightly reduce the
number of translation errors of the overall trans-
lation system. Although the improvement on the
IWSLT?04 set is only moderate, the results are
nevertheless comparable or better to the ones from
(Och et al, 2004), where, starting from IBM
model 1 baseline, an additional improvement of
only 0.4% BLEU was achieved using more com-
plex methods.
For the maximum-entropy based chunking ap-
proach, n-grams with n = 4 work best for the
chunker that is trained on WSJ data. The domain-
specific rescoring model which results from the
chunker being trained on the BTEC corpora turns
out to prefer higher order n-grams, with n = 6 or
more. This might be an indicator of the domain-
specific rescoring model successfully capturing
more local context.
The training of the other models, i.e. supertag-
ging/LDA and link grammar, is also performed on
46
Japanese ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.09 57.8 31.3 25.0
with supertagging/LDA 9.13 57.8 31.3 24.8
with link grammar 9.46 57.6 31.9 25.3
with supertagging/LDA + link grammar 9.24 58.2 31.0 24.8
with ME chunker 9.31 58.7 30.9 24.4
with all models 9.21 58.9 30.5 24.3
Japanese ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.22 54.7 34.1 25.5
with supertagging/LDA 9.27 54.8 34.2 25.6
with link grammar 9.37 54.9 34.3 25.9
with supertagging/LDA + link grammar 9.30 55.0 34.0 25.6
with ME chunker 9.27 55.0 34.2 25.5
with all models 9.27 55.2 33.9 25.5
Table 4: Effect of successively adding syntactic features to the Japanese-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
Arabic ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 10.18 64.3 23.9 20.6
with supertagging/LDA 10.13 64.6 23.4 20.1
with link grammar 10.06 64.7 23.4 20.3
with supertagging/LDA + link grammar 10.20 65.0 23.2 20.2
with ME chunker 10.11 65.1 23.0 19.9
with all models 10.23 65.2 23.0 19.9
Arabic ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.75 59.8 26.1 21.9
with supertagging/LDA 9.77 60.5 25.6 21.5
with link grammar 9.74 60.5 25.9 21.7
with supertagging/LDA + link grammar 9.86 60.8 26.0 21.6
with ME chunker 9.71 59.9 25.9 21.8
with all models 9.84 60.1 26.4 21.9
Table 5: Effect of successively adding syntactic features to the Arabic-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
out-of-domain data. Thus, further improvements
should be possible if the models were adapted to
the BTEC domain. This would require the prepa-
ration of an annotated corpus for the supertagger
and a specialized link grammar, which are both
time-consuming tasks.
The syntactically motivated methods (supertag-
ging/LDA and link grammars) perform similarly
to the maximum-entropy based chunker. It seems
that both approaches successfully exploit struc-
tural properties of language. However, one outlier
is ME chunking on the Chinese-English test data,
where we observe a lower BLEU but a larger NIST
score. For Arabic-English, the combination of all
methods does not seem to generalize well on the
test set. In that case, supertagging/LDA and link
grammar outperforms the ME chunker: the over-
all improvement is 1% absolute in terms of BLEU
score.
5 Conclusion
We added syntactically motivated features to a sta-
tistical machine translation system in a rerank-
ing framework. The goal was to analyze whether
shallow parsing techniques help in identifying un-
grammatical hypotheses. We showed that some
improvements are possible by utilizing supertag-
ging, lightweight dependency analysis, a link
47
grammar parser and a maximum-entropy based
chunk parser. Adding features to n-best lists and
discriminatively training the system on a develop-
ment set helped to gain up to 0.7% in BLEU score
on the test set.
Future work could include developing an
adapted LTAG for the BTEC domain or incor-
porating n-gram models into the link grammar
concept in order to derive a long-range language
model (Lafferty et al, 1992). However, we feel
that the current improvements are not significant
enough to justify these efforts. Additionally, we
will apply these reranking methods to larger cor-
pora in order to study the effects on longer sen-
tences from more complex domains.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-Star (Technology and Corpora for Speech
to Speech Translation, IST-2002-FP6-506738,
http://www.tc-star.org), and by the R&D project
TRAMES managed by Bertin Technologies as
prime contractor and operated by the french DGA
(De?le?gation Ge?ne?rale pour l?Armement).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore. 2000. A lightweight dependency
analyzer for partial parsing. Computational Linguis-
tics, 6(2):113?138.
Oliver Bender, Klaus Macherey, Franz Josef Och, and
Hermann Ney. 2003. Comparison of alignment
templates and maximum entropy models for natural
language understanding. In EACL03: 10th Conf. of
the Europ. Chapter of the Association for Computa-
tional Linguistics, pages 11?18, Budapest, Hungary,
April.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon
University, Pittsburgh, PA.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematical Statistics, 43:1470?1480.
John Lafferty, Daniel Sleator, and Davy Temperley.
1992. Grammatical trigrams: A probabilistic model
of link grammar. In Proc. of the AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 89?97, Cambridge, MA.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 295?302, Philadelphia, PA,
July.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proc. 2004 Meeting of the North
American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), pages 161?168,
Boston, MA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Workshop on Parsing Technologies, Tilburg/Durbuy,
The Netherlands/Belgium, August.
Toshiyuki Takezawa, Eiichiro Sumita, F. Sugaya,
H. Yamamoto, and S. Yamamoto. 2002. Toward
a broad-coverage bilingual corpus for speech trans-
lation of travel conversations in the real world. In
Proc. of the Third Int. Conf. on Language Resources
and Evaluation (LREC), pages 147?152, Las Pal-
mas, Spain, May.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, pages 127?132, Lisbon, Portugal,
September.
XTAG Research Group. 2001. A Lexicalized Tree
Adjoining Grammar for English. Technical Re-
port IRCS-01-03, IRCS, University of Pennsylvania,
Philadelphia, PA, USA.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguis-
tics: Proc. Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 191?198, Ann Arbor, MI, June.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based
statistical machine translation system. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
48
Proceedings of the Workshop on Statistical Machine Translation, pages 15?22,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Arabic Preprocessing for Arabic-to-English Statistical
Machine Translation
Anas El Isbihani Shahram Khadivi Oliver Bender
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{isbihani,khadivi,bender,ney}@informatik.rwth-aachen.de
Hermann Ney
Abstract
The Arabic language has far richer sys-
tems of inflection and derivation than En-
glish which has very little morphology.
This morphology difference causes a large
gap between the vocabulary sizes in any
given parallel training corpus. Segmen-
tation of inflected Arabic words is a way
to smooth its highly morphological na-
ture. In this paper, we describe some
statistically and linguistically motivated
methods for Arabic word segmentation.
Then, we show the efficiency of proposed
methods on the Arabic-English BTEC and
NIST tasks.
1 Introduction
Arabic is a highly inflected language compared to
English which has very little morphology. This mor-
phological richness makes statistical machine trans-
lation from Arabic to English a challenging task. A
usual phenomenon in Arabic is the attachment of a
group of words which are semantically dependent on
each other. For instance, prepositions like ?and? and
?then? are usually attached to the next word. This
applies also to the definite article ?the?. In addi-
tion, personal pronouns are attached to the end of
verbs, whereas possessive pronouns are attached to
the end of the previous word, which constitutes the
possessed object. Hence, an Arabic word can be de-
composed into ?prefixes, stem and suffixes?. We re-
strict the set of prefixes and suffixes to those showed
in Table 1 and 2, where each of the prefixes and suf-
fixes has at least one meaning which can be repre-
sented by a single word in the target language. Some
prefixes can be combined. For example the word
wbAlqlm (????AK. ? which means ?and with the pen?)
has a prefix which is a combination of three pre-
fixes, namely w, b and Al. The suffixes we handle
in this paper can not be combined with each other.
Thus, the compound word pattern handled here is
?prefixes-stem-suffix?.
All possible prefix combinations that do not con-
tain Al allow the stem to have a suffix. Note that
there are other suffixes that are not handled here,
such as At ( H@), An ( 	?@) and wn ( 	??) which make
the plural form of a word. The reason why we omit
them is that they do not have their own meaning. The
impact of Arabic morphology is that the vocabulary
size and the number of singletons can be dramati-
cally high, i.e. the Arabic words are not seen often
enough to be learned by statistical machine transla-
tion models. This can lead to an inefficient align-
ment.
In order to deal with this problem and to improve
the performance of statistical machine translation,
each word must be decomposed into its parts. In
(Larkey et al, 2002) it was already shown that word
segmentation for Arabic improves information re-
trieval. In (Lee et al, 2003) a statistical approach
for Arabic word segmentation was presented. It de-
composes each word into a sequence of morphemes
(prefixes-stem-suffixes), where all possible prefixes
and suffixes (not only those we described in Table 1
and 2) are split from the original word. A compa-
rable work was done by (Diab et al, 2004), where
a POS tagging method for Arabic is also discussed.
As we have access to this tool, we test its impact
on the performance of our translation system. In
15
Table 1: Prefixes handled in this work and their meanings.
Prefix ? 	? ? ? H. ?@
Transliteration w f k l b Al
Meaning and and then as, like in order to with, in the
(Habash and Rambow, 2005) a morphology analyzer
was used for the segementation and POS tagging. In
contrast to the methods mentioned above, our seg-
mentation method is unsupervised and rule based.
In this paper we first explain our statistical ma-
chine translation (SMT) system used for testing the
impact of the different segmentation methods, then
we introduce some preprocessing and normalization
tools for Arabic and explain the linguistic motiva-
tion beyond them. Afterwards, we present three
word segmentation methods, a supervised learning
approach, a finite state automaton-based segmenta-
tion, and a frequency-based method. In Section 5,
the experimental results are presented. Finally, the
paper is summarized in Section 6 .
2 Baseline SMT System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(2)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system including the following models: an n-gram
language model, a phrase translation model and a
word-based lexicon model. The latter two mod-
els are used for both directions: p(f |e) and p(e|f).
Additionally, we use a word penalty and a phrase
penalty. More details about the baseline system can
be found in (Zens and Ney, 2004; Zens et al, 2005).
3 Preprocessing and Normalization Tools
3.1 Tokenizer
As for other languages, the corpora must be first to-
kenized. Here words and punctuations (except ab-
breviation) must be separated. Another criterion is
that Arabic has some characters that appear only at
the end of a word. We use this criterion to separate
words that are wrongly attached to each other.
3.2 Normalization and Simplification
The Arabic written language does not contain vow-
els, instead diacritics are used to define the pronun-
ciation of a word, where a diacritic is written under
or above each character in the word. Usually these
diacritics are omitted, which increases the ambigu-
ity of a word. In this case, resolving the ambiguity
of a word is only dependent on the context. Some-
times, the authors write a diacritic on a word to help
the reader and give him a hint which word is really
meant. As a result, a single word with the same
meaning can be written in different ways. For exam-
ple $Eb (I. ? ?) can be read1 as sha?ab (Eng. nation)
or sho?ab (Eng. options). If the author wants to give
the reader a hint that the second word is meant, he
1There are other possible pronunciations for the word $Eb
than the two mentioned.
16
Table 2: Suffixes handled in this work and their meanings.
Suffix ?


?


	
G ? 	?? , ?? , A??
Transliteration y ny k kmA, km, kn
Meaning my me you, your (sing.) you, your (pl.)
Suffix A 	K ? A? 	?? , ?? , A??
Transliteration nA h hA hmA, hm, hn
Meaning us, our his, him her them, their
can write $uEb (I. ?

?) or $uEab (I. ?

?). To avoid
this problem we normalize the text by removing all
diacritics.
After segmenting the text, the size of the sen-
tences increases rapidly, where the number of the
stripped article Al is very high. Not every article in
an Arabic sentence matches to an article in the target
language. One of the reasons is that the adjective in
Arabic gets an article if the word it describes is def-
inite. So, if a word has the prefix Al, then its adjec-
tive will also have Al as a prefix. In order to reduce
the sentence size we decide to remove all these arti-
cles that are supposed to be attached to an adjective.
Another way for determiner deletion is described in
(Lee, 2004).
4 Word Segmentation
One way to simplify inflected Arabic text for a SMT
system is to split the words in prefixes, stem and
suffixes. In (Lee et al, 2003), (Diab et al, 2004)
and (Habash and Rambow, 2005) three supervised
segmentation methods are introduced. However, in
these works the impact of the segmentation on the
translation quality is not studied. In the next subsec-
tions we will shortly describe the method of (Diab et
al., 2004). Then we present our unsupervised meth-
ods.
4.1 Supervised Learning Approach (SL)
(Diab et al, 2004) propose solutions to word seg-
mentation and POS Tagging of Arabic text. For the
purpose of training the Arabic TreeBank is used,
which is an Arabic corpus containing news articles
of the newswire agency AFP. In the first step the text
must be transliterated to the Buckwalter translitera-
tion, which is a one-to-one mapping to ASCII char-
acters. In the second step it will be segmented and
tokenized. In the third step a partial lemmatization is
done. Finally a POS tagging is performed. We will
test the impact of the step 3 (segmentation + lemma-
tization) on the translation quality using our phrase
based system described in Section 2.
4.2 Frequency-Based Approach (FB)
We provide a set of all prefixes and suffixes and
their possible combinations. Based on this set, we
may have different splitting points for a given com-
pound word. We decide whether and where to split
the composite word based on the frequency of dif-
ferent resulting stems and on the frequency of the
compound word, e.g. if the compound word has a
higher frequency than all possible stems, it will not
be split. This simple heuristic harmonizes the cor-
pus by reducing the size of vocabulary, singletons
and also unseen words from the test corpus. This
method is very similar to the method used for split-
ting German compound words (Koehn and Knight,
2003).
4.3 Finite State Automaton-Based Approach
(FSA)
To segment Arabic words into prefixes, stem and one
suffix, we implemented two finite state automata.
One for stripping the prefixes and the other for the
suffixes. Then, we append the suffix automaton to
the other one for stripping prefixes. Figure 1 shows
the finite state automaton for stripping all possible
prefix combinations. We add the prefix s (?), which
changes the verb tense to the future, to the set of
prefixes which must be stripped (see table 1). This
prefix can only be combined with w and f. Our mo-
tivation is that the future tense in English is built by
adding the separate word ?will?.
The automaton showed in Figure 1 consists of the
following states:
? S: the starting point of the automaton.
? E: tne end state, which can only be achieved if
17
S K
AL
B
L
WF
C
E
Figure 1: Finite state automaton for stripping pre-
fixes off Arabic words.
the resulting stem exists already in the text.
? WF: is achieved if the word begins with w or f.
? And the states , K, L, B and AL are achieved if
the word begins with s, k, l, b and Al, respec-
tively.
To minimize the number of wrong segmentations,
we restricted the transition from one state to the
other to the condition that the produced stem occurs
at least one time in the corpus. To ensure that most
compound words are recognized and segmented, we
run the segmenter itteratively, where after each it-
eration the newly generated words are added to the
vocabulary. This will enable recognizing new com-
pound words in the next iteration. Experiments
showed that running the segmenter twice is suffi-
cient and in higher iterations most of the added seg-
mentations are wrong.
4.4 Improved Finite State Automaton-Based
Approach (IFSA)
Although we restricted the finite state segmenter in
such a way that words will be segmented only if the
yielded stem already exists in the corpus, we still get
some wrongly segmented words. Thus, some new
stems, which do not make sense in Arabic, occur
in the segmented text. Another problem is that the
finite state segmenter does not care about ambigui-
ties and splits everything it recognizes. For example
let us examine the word frd (XQ 	?). In one case, the
character f is an original one and therefore can not
be segmented. In this case the word means ?per-
son?. In the other case, the word can be segmented
to ?f rd? (which means ?and then he answers? or
?and then an answer?). If the words Alfrd, frd and
rd(XQ 	? , XQ 	?? @ and XP) occur in the corpus, then the fi-
nite state segmenter will transform the Alfrd (which
means ?the person?) to Al f rd (which can be trans-
lated to ?the and then he answers?). Thus the mean-
ing of the original word is distorted. To solve all
these problems, we improved the last approach in a
way that prefixes and suffixes are recognized simul-
taneously. The segmentation of the ambiguous word
will be avoided. In doing that, we intend to postpone
resolving such ambiguities to our SMT system.
The question now is how can we avoid the seg-
mentation of ambiguous words. To do this, it is suf-
ficient to find a word that contains the prefix as an
original character. In the last example the word Al-
frd contains the prefix f as an original character and
therefore only Al can be stripped off the word. The
next question we can ask is, how can we decide if a
character belongs to the word or is a prefix. We can
extract this information using the invalid prefix com-
binations. For example Al is always the last prefix
that can occur. Therefore all characters that occur in
a word after Al are original characters. This method
can be applied for all invalid combinations to extract
new rules to decide whether a character in a word is
an original one or not.
On the other side, all suffixes we handle in this
work are pronouns. Therefore it is not possible to
combine them as a suffix. We use this fact to make
a decision whether the end characters in a word are
original or can be stripped. For example the word
trkhm (???QK) means ?he lets them?. If we suppose
that hm is a suffix and therefore must be stripped,
then we can conclude that k is an original character
and not a suffix. In this way we are able to extract
from the corpus itself decisions whether and how a
word can be segmented.
In order to implement these changes the original
automaton was modified. Instead of splitting a word
we mark it with some properties which corespond
to the states traversed untill the end state. On the
18
other side, we use the technique described above to
generate negative properties which avoid the corre-
sponding kind of splitting. If a property and its nega-
tion belong to the same word then the property is re-
moved and only the negation is considered. At the
end each word is split corresponding to the proper-
ties it is marked with.
5 Experimental Results
5.1 Corpus Statistics
The experiments were carried out on two tasks: the
corpora of the Arabic-English NIST task, which
contain news articles and UN reports, and the
Arabic-English corpus of the Basic Travel Expres-
sion Corpus (BTEC) task, which consists of typi-
cal travel domain phrases (Takezawa et al, 2002).
The corpus statistics of the NIST and BTEC corpora
are shown in Table 3 and 5. The statistics of the
news part of NIST corpus, consisting of the Ummah,
ATB, ANEWS1 and eTIRR corpora, is shown in Ta-
ble 4. In the NIST task, we make use of the NIST
2002 evaluation set as a development set and NIST
2004 evaluation set as a test set. Because the test
set contains four references for each senence we de-
cided to use only the first four references of the de-
velopment set for the optimization and evaluation.
In the BTEC task, C-Star?03 and IWSLT?04 copora
are considered as development and test sets, respec-
tively.
5.2 Evaluation Metrics
The commonly used criteria to evaluate the trans-
lation results in the machine translation commu-
nity are: WER (word error rate), PER (position-
independent word error rate), BLEU (Papineni et
al., 2002), and NIST (Doddington, 2002). The four
criteria are computed with respect to multiple ref-
erences. The number of reference translations per
source sentence varies from 4 to 16 references. The
evaluation is case-insensitive for BTEC and case-
sensitive for NIST task. As the BLEU and NIST
scores measure accuracy, higher scores are better.
5.3 Translation Results
To study the impact of different segmentation meth-
ods on the translation quality, we apply different
word segmentation methods to the Arabic part of the
BTEC and NIST corpora. Then, we make use of the
phrase-based machine translation system to translate
the development and test sets for each task.
First, we discuss the experimental results on the
BTEC task. In Table 6, the translation results on the
BTEC corpus are shown. The first row of the table is
the baseline system where none of the segmentation
methods is used. All segmentation methods improve
the baseline system, except the SL segmentation
method on the development corpus. The best per-
forming segmentation method is IFSA which gener-
ates the best translation results based on all evalua-
tion criteria, and it is consistent over both develop-
ment and evaluation sets. As we see, the segmen-
tation of Arabic words has a noticeable impact in
improving the translation quality on a small corpus.
To study the impact of word segmentation meth-
ods on a large task, we conduct two sets of experi-
ments on the NIST task using two different amounts
of the training corpus: only news corpora, and full
corpus. In Table 7, the translation results on the
NIST task are shown when just the news corpora
were used to train the machine translation models.
As the results show, except for the FB method, all
segmentation methods improve the baseline system.
For the NIST task, the SL method outperforms the
other segmentation methods, while it did not achieve
good results when comparing to the other methods
in the BTEC task.
We see that the SL, FSA and IFSA segmentation
methods consistently improve the translation results
in the BTEC and NIST tasks, but the FB method
failed on the NIST task, which has a larger training
corpus . The next step is to study the impact of the
segmentation methods on a very large task, the NIST
full corpus. Unfortunately, the SL method failed on
segmenting the large UN corpus, due to the large
processing time that it needs. Due to the negative
results of the FB method on the NIST news corpora,
and very similar results for FSA and IFSA, we were
interested to test the impact of IFSA on the NIST
full corpus. In Table 8, the translation results of the
baseline system and IFSA segmentation method for
the NIST full corpus are depicted. As it is shown in
table, the IFSA method slightly improves the trans-
lation results in the development and test sets.
The IFSA segmentation method generates the
best results among our proposed methods. It
acheives consistent improvements in all three tasks
over the baseline system. It also outperforms the SL
19
Table 3: BTEC corpus statistics, where the Arabic part is tokenized and segmented with the SL, FB, FSA
and the IFSA methods.
ARABIC ENGLISH
TOKENIZED SL FB FSA IFSA
Train: Sentences 20K
Running Words 159K 176.2K 185.5K 190.3K 189.1K 189K
Vocabulary 18,149 14,321 11,235 11,736 12,874 7,162
Dev: Sentences 506
Running Words 3,161 3,421 3,549 3,759 3,715 5,005
OOVs (Running Words) 163 129 149 98 118 NA
Test: Sentences 500
Running Words 3,240 3,578 3,675 3,813 3,778 4,986
OOVs (Running Words) 186 120 156 92 115 NA
Table 4: Corpus statistics for the news part of the NIST task, where the Arabic part is tokenized and seg-
mented with SL, FB, FSA and IFSA methods.
ARABIC ENGLISH
TOKENIZED SL FB FSA IFSA
Train: Sentences 284.9K
Running Words 8.9M 9.7M 12.2M 10.9M 10.9M 10.2M
Vocabulary 118.7K 90.5K 43.1K 68.4K 62.2K 56.1K
Dev: Sentences 1,043
Running Words 27.7K 29.1K 37.3K 34.4K 33.5K 33K
OOVs (Running Words) 714 558 396 515 486 NA
Test: Sentences 1,353
Running Words 37.9K 41.7K 52.6K 48.6K 48.3K 48.3K
OOVs (Running Words) 1,298 1,027 612 806 660 NA
segmentation on the BTEC task.
Although the SL method outperforms the IFSA
method on the NIST tasks, the IFSA segmentation
method has a few notable advantages over the SL
system. First, it is consistent in improving the base-
line system over the three tasks. But, the SL method
failed in improving the BTEC development corpus.
Second, it is fast and robust, and capable of being
applied to the large corpora. Finally, it employs an
unsupervised learning method, therefore can easily
cope with a new task or corpus.
We observe that the relative improvement over
the baseline system is decreased by increasing the
size of the training corpus. This is a natural effect
of increasing the size of the training corpus. As
the larger corpus provides higher probability to have
more samples per word, this means higher chance
to learn the translation of a word in different con-
texts. Therefore, larger training corpus makes a bet-
ter translation system, i.e. a better baseline, then it
would be harder to outperform this better system.
Using the same reasoning, we can realize why the
FB method achieves good results on the BTEC task,
but not on the NIST task. By increasing the size
of the training corpus, the FB method tends to seg-
ment words more than the IFSA method. This over-
segmentation can be compensated by using longer
phrases during the translation, in order to consider
the same context compared to the non-segmented
corpus. Then, it would be harder for a phrase-based
machine translation system to learn the translation
of a word (stem) in different contexts.
6 Conclusion
We presented three methods to segment Arabic
words: a supervised learning approach, a frequency-
20
Table 5: NIST task corpus statistics, where the Arabic part is tokenized and segmented with the IFSA
method.
ARABIC ENGLISH
TOKENIZED IFSA
Train: Sentences 8.5M
Running Words 260.5M 316.8M 279.2M
Vocabulary 510.3K 411.2K 301.2K
Dev: Sentences 1043
Running Words 30.2K 33.3K 33K
OOVs (Running Words) 809 399 NA
Test: Sentences 1353
Running Words 40K 47.9K 48.3K
OOVs (Running Words) 871 505 NA
Table 6: Case insensitive evaluation results for translating the development and test data of BTEC task after
performing divers preprocessing.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 21.4 24.6 63.9 10.0 23.5 27.2 58.1 9.6
SL Segmenter 21.2 24.4 62.5 9.7 23.4 27.4 59.2 9.7
FB Segmenter 20.9 24.4 65.3 10.1 22.1 25.8 59.8 9.7
FSA Segmenter 20.1 23.4 64.8 10.2 21.1 25.2 61.3 10.2
IFSA Segmenter 20.0 23.3 65.0 10.4 21.2 25.3 61.3 10.2
based approach and a finite state automaton-based
approach. We explained that the best of our pro-
posed methods, the improved finite state automaton,
has three advantages over the state-of-the-art Arabic
word segmentation method (Diab, 2000), supervised
learning. They are: consistency in improving the
baselines system over different tasks, its capability
to be efficiently applied on the large corpora, and its
ability to cope with different tasks.
7 Acknowledgment
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic
tagging of arabic text: From raw text to base phrase
chunks. In D. M. Susan Dumais and S. Roukos, edi-
tors, HLT-NAACL 2004: Short Papers, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
M. Diab. 2000. An unsupervised method for multi-
lingual word sense tagging using parallel corpora: A
preliminary investigation. In ACL-2000 Workshop on
Word Senses and Multilinguality, pages 1?9, Hong
Kong, October.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
21
Table 7: Case sensitive evaluation results for translating the development and test data of the news part of
the NIST task after performing divers preprocessing.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 43.7 56.4 43.6 9.9 46.1 58.0 37.4 9.1
SL Segmenter 42.0 54.7 45.1 10.2 44.3 56.3 39.9 9.6
FB Segmenter 43.4 56.1 43.2 9.8 45.6 57.8 37.2 9.2
FSA Segmenter 42.9 55.7 43.7 9.9 44.8 56.9 38.7 9.4
IFSA Segmenter 42.6 55.0 44.6 9.9 44.5 56.6 38.8 9.4
Table 8: Case-sensitive evaluation results for translating development and test data of NIST task.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 41.5 53.5 46.4 10.3 42.5 53.9 42.6 10.0
IFSA Segmenter 41.1 53.2 46.7 10.2 42.1 53.6 43.4 10.1
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological dis-
ambiguation in one fell swoop. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 573?580, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proc. 10th Conf. of the Europ.
Chapter of the Assoc. for Computational Linguistics
(EACL), pages 347?354, Budapest, Hungary, April.
L. S. Larkey, L. Ballesteros, and M. E. Connell. 2002.
Improving stemming for arabic information retrieval:
light stemming and co-occurrence analysis. In Proc.
of the 25th annual of the international Association
for Computing Machinery Special Interest Group on
Information Retrieval (ACM SIGIR), pages 275?282,
New York, NY, USA. ACM Press.
Y. S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In E. Hinrichs and D. Roth, editors, Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Y. S. Lee. 2004. Morphological analysis for statisti-
cal machine translation. In D. M. Susan Dumais and
S. Roukos, editors, HLT-NAACL 2004: Short Papers,
pages 57?60, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, Philadelphia, PA, July.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA, July.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conver-
sations in the real world. In Proc. of the Third Int.
Conf. on Language Resources and Evaluation (LREC),
pages 147?152, Las Palmas, Spain, May.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL),
pages 257?264, Boston, MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
22
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 233?241,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Deep Learning Approach to Machine Transliteration
Thomas Deselaers and Sas?a Hasan and Oliver Bender and Hermann Ney
Human Language Technology and Pattern Recognition Group ? RWTH Aachen University
<surname>@cs.rwth-aachen.de
Abstract
In this paper we present a novel translit-
eration technique which is based on deep
belief networks. Common approaches
use finite state machines or other meth-
ods similar to conventional machine trans-
lation. Instead of using conventional NLP
techniques, the approach presented here
builds on deep belief networks, a tech-
nique which was shown to work well for
other machine learning problems. We
show that deep belief networks have cer-
tain properties which are very interesting
for transliteration and possibly also for
translation and that a combination with
conventional techniques leads to an im-
provement over both components on an
Arabic-English transliteration task.
1 Introduction
Transliteration, i.e. the transcription of words such
as proper nouns from one language into another or,
more commonly from one alphabet into another, is
an important subtask of machine translation (MT)
in order to obtain high quality output.
We present a new technique for transliteration
which is based on deep belief networks (DBNs),
a well studied approach in machine learning.
Transliteration can in principle be considered to be
a small-scale translation problem and, thus, some
ideas presented here can be transferred to the ma-
chine translation domain as well.
Transliteration has been in use in machine trans-
lation systems, e.g. Russian-English, since the ex-
istence of the field of machine translation. How-
ever, to our knowledge it was first studied as a
machine learning problem by Knight and Graehl
(1998) using probabilistic finite-state transducers.
Subsequently, the performance of this system was
greatly improved by combining different spelling
and phonetic models (Al-Onaizan and Knight,
2002). Huang et al (2004) construct a proba-
bilistic Chinese-English edit model as part of a
larger alignment solution using a heuristic boot-
strapped procedure. Freitag and Khadivi (2007)
propose a technique which combines conventional
MT methods with a single layer perceptron.
In contrast to these methods which strongly
build on top of well-established natural language
processing (NLP) techniques, we propose an al-
ternative model. Our new model is based on deep
belief networks which have been shown to work
well in other machine learning and pattern recog-
nition areas (cf. Section 2). Since translation and
transliteration are closely related and translitera-
tion can be considered a translation problem on the
character level, we discuss various methods from
both domains which are related to the proposed
approach in the following.
Neural networks have been used in NLP in
the past, e.g. for machine translation (Asuncio?n
Castan?o et al, 1997) and constituent parsing
(Titov and Henderson, 2007). However, it might
not be straight-forward to obtain good results us-
ing neural networks in this domain. In general,
when training a neural network, one has to choose
the structure of the neural network which involves
certain trade-offs. If a small network with no hid-
den layer is chosen, it can be efficiently trained
but has very limited representational power, and
may be unable to learn the relationships between
the source and the target language. The DBN ap-
proach alleviates some of the problems that com-
monly occur when working with neural networks:
1. they allow for efficient training due to a good
initialisation of the individual layers. 2. Overfit-
ting problems are addressed by creating generative
models which are later refined discriminatively. 3.
The network structure is clearly defined and only a
few structure parameters have to be set. 4. DBNs
can be interpreted as Bayesian probabilistic gener-
ative models.
Recently, Collobert and Weston (2008) pro-
posed a technique which applies a convolutional
DBN to a multi-task learning NLP problem. Their
approach is able to address POS tagging, chunk-
ing, named entity tagging, semantic role and simi-
lar word identification in one model. Our model is
similar to this approach in that it uses the same ma-
chine learning techniques but the encoding and the
233
processing is done differently. First, we learn two
independent generative models, one for the source
input and one for the target output. Then, these
two models are combined into a source-to-target
encoding/decoding system (cf. Section 2).
Regarding that the target is generated and not
searched in a space of hypotheses (e.g. in a word
graph), our approach is similar to the approach
presented by Bangalore et al (2007) who present
an MT system where the set of words of the tar-
get sentence is generated based on the full source
sentence and then a finite-state approach is used to
reorder the words. Opposed to this approach we
do not only generate the letters/words in the target
sentence but we generate the full sentence with or-
dering.
We evaluate the proposed methods on an
Arabic-English transliteration task where Arabic
city names have to be transcribed into the equiva-
lent English spelling.
2 Deep Belief Networks for
Transliteration
Although DBNs are thoroughly described in the
literature, e.g. (Hinton et al, 2006), we give a short
overview on the ideas and techniques and intro-
duce our notation.
Deep architectures in machine learning and ar-
tificial intelligence are becoming more and more
popular after an efficient training algorithm has
been proposed (Hinton et al, 2006), although the
idea is known for some years (Ackley et al, 1985).
Deep belief networks consist of multiple layers of
restricted Boltzmann machines (RBMs). It was
shown that DBNs can be used for dimensionality
reduction of images and text documents (Hinton
and Salakhutdinow, 2006) and for language mod-
elling (Mnih and Hinton, 2007). Recently, DBNs
were also used successfully in image retrieval to
create very compact but meaningful representa-
tions of a huge set of images (nearly 13 million)
for retrieval (Torralba et al, 2008).
DBNs are built from RBMs by first training an
RBM on the input data. A second RBM is built
on the output of the first one and so on until a
sufficiently deep architecture is created. RBMs
are stochastic generative artificial neural networks
with restricted connectivity. From a theoretical
viewpoint, RBMs are interesting because they are
able to discover complex regularities and find no-
table features in data (Ackley et al, 1985).
Figure 1: A schematic representation of our DBN
for transliteration.
Hinton and Salakhutdinow (2006) present a
deep belief network to learn a tiny representation
of its inputs and to reconstruct the input with high
accuracy which is demonstrated for images and
textual documents. Here, we use DBNs similarly:
first, we learn encoders for the source and tar-
get words respectively and then connect these two
through a joint layer to map between the two lan-
guages. This joint layer is trained in the same way
as the top-level neurons in the deep belief classi-
fier from (Hinton et al, 2006).
In Figure 1, a schematic view of our DBN for
transliteration is shown. On the left and on the
right are encoders for the source and target words
respectively. To transliterate a source word, it is
passed through the layers of the network. First, it
traverses through the source encoder on the left,
then it passes into the joint layer, finally travers-
ing down through the target encoder. Each layer
consists of a set of neurons receiving the output
of the preceding layer as input. The first layers in
the source and target encoders consist of S1 and
T1 neurons, respectively; the second layers have
S2 and T2 nodes, and the third layers have S3 and
T3 nodes, respectively. A joint layer with J nodes
connects the source and the target encoders.
Here, the number of nodes in the individual lay-
ers are the most important parameters. The more
234
nodes a layer has, the more information can be
conveyed through it, but the harder the training:
the amount of data needed for training and thus
the computation time required is exponential in the
size of the network (Ackley et al, 1985).
To transliterate a source word, it is first encoded
as a DF -dimensional binary vector SF (cf. Sec-
tion 2.1) and then fed into the first layer of the
source encoder. The S1-dimensional output vec-
tor OS1 of the first layer is computed as
OS1 ? 1/ exp (1 + wS1SF + bS1) , (1)
where wS1 is a S1 ?DF -dimensional weight ma-
trix and bS1 is an S1-dimensional bias vector.
The output of each layer is used as input to the
next layer as follows:
OS2 ? 1/ exp (1 + wS2OS1 + bS2) , (2)
OS3 ? 1/ exp (1 + wS3OS2 + bS3) . (3)
After the source encoder has been traversed, the
joint layer is reached which processes the data
twice: once using the input from the source en-
coder to get a state of the hidden neurons OSJ and
then to infer an output state OJT as input to the
topmost level of the output encoder
OSJ ? 1/ exp (1 + wSJOS3 + bSJ) , (4)
OJT ? 1/ exp (1 + wJTOSJ + bJT ) . (5)
This output vector is decoded by traversing down-
wards through the output encoder:
OT3 ? 1/ exp (1 + wT3OJT + bT3) , (6)
OT2 ? 1/ exp (1 + wT2OT3 + bT2) , (7)
OT1 ? wT1OT2 + bT1, (8)
where OT1 is a vector encoding a word in the tar-
get language.
Note that this model is intrinsically bidirec-
tional since the individual RBMs are bidirectional
models and thus it is possible to transliterate from
source to target and vice versa.
2.1 Source and Target Encoding
A problem with DBNs and transliteration is the
data representation. The input and output data are
commonly sequences of varying length but a DBN
expects input data of constant length. To repre-
sent a source or target language word, it is con-
verted into a sparse binary vector of dimensional-
ity DF = |F | ? J or DE = |E| ? I , respectively,
where |F | and |E| are the sizes of the alphabets
and I and J are the lengths of the longest words.
If a word is shorter than this, a padding letter w0
is used to fill the spaces. This encoding is depicted
in the bottom part of Figure 1.
Since the output vector of the DBN is not bi-
nary, we infer the maximum a posterior hypothe-
sis by selecting the letter with the highest output
value for each position.
2.2 Training Method
For the training, we follow the method proposed
in (Hinton et al, 2006). To find a good starting
point for backpropagation on the whole network,
each of the RBMs is trained individually. First, we
learn the generative encoders for the source and
target words, i.e. the weights wS1 and wT1, respec-
tively. Therefore, each of the layers is trained as a
restricted Boltzmann machine, such that it learns
to generate the input vectors with high probability,
i.e. the weights are learned such that the data val-
ues have low values of the trained cost function.
After learning a layer, the activity vectors of the
hidden units, as obtained from the real training
data, are used as input data for the next layer. This
can be repeated to learn as many hidden layers as
desired. After learning multiple hidden layers in
this way, the whole network can be viewed as a
single, multi-layer generative model and each ad-
ditional hidden layer improves a lower bound on
the probability that the multi-layer model would
generate the training data (Hinton et al, 2006).
For each language, the output of the first layer is
used as input to learn the weights of the next lay-
ers wS2 and wT2. The same procedure is repeated
to learn wS3 and wT3. Note that so far no con-
nection between the individual letters in the two
alphabets is created but each encoder only learns
feature functions to represent the space of possi-
ble source and target words. Then, the weights
for the joined layer are learned using concatenated
outputs of the top layers of the source and target
encoders to find an initial set of weights wSJ and
wJT .
After each of the layers has been trained in-
dividually, backpropagation is performed on the
whole network to tune the weights and to learn the
connections between both languages. We use the
average squared error over the output vectors be-
tween reference and inferred words as the training
criterion. For the training, we split the training
235
data into batches of 100 randomly selected words
and allow for 10 training iterations of the individ-
ual layers and up to 200 training iterations for the
backpropagation. Currently, we only optimise the
parameters for the source to target direction and
thus do not retain the bidirectionality1.
Thus, the whole training procedure consists of
4 phases. First, an autoencoder for the source
words is learnt. Second, an autoencoder for
the target words is learnt. Third, these autoen-
coders are connected by a top connecting layer,
and finally backpropagation is performed over the
whole network for fine-tuning of the weights.
2.3 Creation of n-Best Lists
N -best lists are a common means for combination
of several systems in natural language processing
and for rescoring. In this section, we describe how
a set of hypotheses can be created for a given in-
put. Although these hypotheses are not n-best lists
because they have not been obtained from a search
process, they can be used similarly and can bet-
ter be compared to randomly sampled ?good? hy-
potheses from a full word-graph.
Since the values of the nodes in the individ-
ual layers are probabilities for this particular node
to be activated, it is possible to sample a set of
states from the distribution for the individual lay-
ers, which is called Gibbs sampling (Geman and
Geman, 1984). This sampling can be used to cre-
ate several hypotheses for a given input sentence,
and this set of hypotheses can be used similar to
an n-best list.
The layer in which the Gibbs sampling is done
can in principle be chosen arbitrarily. However,
we believe it is natural to sample in either the first
layer, the joint layer, or the last layer. Sampling in
the first layer leads to different features traversing
the full network. Sampling in the joint layer only
affects the generation of the target sentence, and
sampling in the last layer is equal to directly sam-
pling from the distribution of target hypotheses.
Conventional Gibbs sampling has a very strong
impact on the outcome of the network because the
smoothness of the distributions and the encoding
of similar matches is entirely lost. Therefore, we
use a weak variant of Gibbs sampling. Instead of
replacing the states? probabilities with fully dis-
cretely sampled states, we keep the probabilities
1Note that it is easily possible to extend the backpropaga-
tion to include both directions, but to keep the computational
demands lower we decided to start with only one direction.
and add a fraction of a sampled state, effectively
modifying the probabilities to give a slightly bet-
ter score to the last sampled state. Let p be the
D-dimensional vector of probabilities for D nodes
in an RBM to be on. Normal Gibbs sampling
would sample a D-dimensional vector S contain-
ing a state for each node from this distribution.
Instead of replacing the vector p with S, we use
p? ? p + ?S, leading to smoother changes than
conventional Gibbs sampling. This process can
easily be repeated to obtain multiple hypotheses.
3 Experimental Evaluation
In this section we present experimental results for
an Arabic-English transliteration task. For evalu-
ation we use the character error rate (CER) which
is the commonly used word error rate (WER) on
character level.
We use a corpus of 10,084 personal names in
Arabic and their transliterated English ASCII rep-
resentation (LDC corpus LDC2005G02). The
Arabic names are written in the usual way, i.e.
lacking vowels and diacritics. 1,000 names were
randomly sampled for development and evalua-
tion, respectively (Freitag and Khadivi, 2007).
The vocabulary of the source language is 33 and
the target language has 30 different characters
(including the padding character). The longest
word on both sides consists of 14 characters,
thus the feature vector on the source side is 462-
dimensional and the feature vector on the target
side is 420-dimensional.
3.1 Network Structure
First, we evaluate how the structure of the network
should be chosen. For these experiments, we fixed
the numbers of layers and the size of the bottom
layers in the target and source encoder and evalu-
ate different network structures and the size of the
joint layer.
The experiments we performed are described in
Table 1. The top part of the table gives the results
for different network structures. We compare net-
works with increasing layer sizes, identical layer
sizes, and decreasing layer sizes. It can be seen
that decreasing layer sizes leads to the best results.
In these experiments, we choose the number of
nodes in the joint layer to be three times as large
as the topmost encoder layers.
In the bottom part, we kept most of the network
structure fixed and only vary the number of nodes
236
Table 1: Transliteration experiments using differ-
ent network structures.
number of nodes CER [%]
S1,T1 S2,T2 S3,T3 J train dev eval
400 500 600 1800 0.3 27.2 28.1
400 400 400 1200 0.7 26.1 25.2
400 350 300 900 1.8 25.1 24.3
400 350 300 1000 1.7 24.8 24.0
400 350 300 1500 1.3 24.1 22.7
400 350 300 2000 0.2 24.2 23.5
in the joint layer. Here, a small number of nodes
leads to suboptimal performance and a very high
number of nodes leads to overfitting which can be
seen in nearly perfect performance on the training
data and an increasing CER on the development
and eval data.
3.2 Network Size
Next, we evaluate systems with different numbers
of nodes. Therefore, we start from the best param-
eters (400-350-300-1500) from the previous sec-
tion and scale the number of nodes in the individ-
ual layers by a certain factor, i.e. factor 1.5 leads
to (600-525-450-2250).
In Figure 2 and Table 2, the results from the
experimental evaluation on the transliteration task
are given. The network size denotes the number
of nodes in the bottom layers of the source and the
target encoder (i.e. S1 and T1) and the other layers
are chosen according to the results from the exper-
iments presented in the previous section.
The results show that small networks perform
badly, the optimal performance is reached with
medium sized networks of 400-600 nodes in the
bottom layers, and larger networks perform worse,
which is probably due to overfitting.
For comparison, we give results for a state-of-
the-art phrase-based MT system applied on the
character level with default system parameters (la-
belled as ?PBT untuned?), and the same system,
where all scaling factors were tuned on dev data
(labelled as ?PBT tuned?). The tuned phrase-based
MT system clearly outperforms our approach.
Additionally, we perform an experiment with
a standard multi-layer perceptron. Therefore, we
choose the network structure with 400-350-300-
1500 nodes, initialised these randomly and trained
the entire network with backpropagation training.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 50  200  300  400  500  600  1000
CE
R [%
]
network size
traindevtest
Figure 2: Results for the Arabic-English translit-
eration task depending on the network size.
The results (line ?MLP-400? in Table 2) of this ex-
periment are far worse than any of the other re-
sults, which shows that, apart from the convenient
theoretical interpretation, the creation of the DBN
as described is a suitable method to train the sys-
tem. The reason for the large difference is likely
the bad initialisation of the network and the fact
that the backpropagation algorithm gets stuck in a
local optimum at this point.
3.3 Reordering capabilities
Although reordering is not an issue in transliter-
ation, the proposed model has certain properties
which we investigated and where interesting prop-
erties can be observed.
To investigate the performance under adverse
reordering conditions, we also perform an exper-
iment with reversed ordering of the target letters
(i.e. a word w = c1, c2, . . . , cJ is now written
cJ , cJ?1, . . . , c1). Since the DBN is fully sym-
metric, i.e. each input node is connected with each
output node in the same way and vice versa, the
DBN result is not changed except for some minor
numerical differences due to random initialisation.
Indeed, the DBN obtained is nearly identical ex-
cept for a changed ordering of the weights in the
joint layer, and if desired it is possible to construct
a DBN for reverse-order target language from a
fully trained DBN by permuting the weights.
On the same setup an experiment with our
phrase-based decoder has been performed and
here the performance is strongly decreased (bot-
tom line of Table 2). The phrase-based MT sys-
tem for this experiment used a reordering with
IBM block-limit constraints with distortion lim-
its and all default parameters were reasonably
tuned. We observed that the position-independent
237
Table 2: Results for the Arabic-English translit-
eration task depending on the network size and a
comparison with state of the art results using con-
ventional phrase-based machine translation tech-
niques
network CER [%]
size train dev eval
50 35.8 43.7 43.6
100 26.4 36.3 35.8
200 5.8 25.2 24.3
300 3.9 24.3 24.4
400 1.3 24.1 22.7
500 1.2 22.9 22.8
600 1.0 24.1 22.6
1000 0.2 26.6 24.4
MLP-400 22.0 64.1 63.2
untuned PBT 4.9 23.3 23.6
tuned PBT 2.2 12.9 13.3
(Freitag and Khadivi, 2007) n/a 11.1 11.1
reversed task: PBT 13.0 35.2 35.7
error rate of the phrase-based MT system is hardly
changed which also underlines that, in principle,
the phrase-based MT system is currently better but
that under adverse reordering conditions the DBN
system has some advantages.
3.4 N-Best Lists
As described above, different possibilities to cre-
ate n-best lists exists. Starting from the system
with 400-350-300-1500 nodes, we evaluate the
creation of n-best lists in the first source layer, the
joint layer, and the last target layer. Therefore,
we create n best lists with up to 10 hypotheses
(sometimes, we have less due to duplicates after
sampling, on the average we have 8.3 hypotheses
per sequence), and evaluate the oracle error rate.
In Table 3 it can be observed that sampling in the
first layer leads to the best oracle error rates. The
baseline performance (first best) for this system is
24.1% CER on the development data, and 22.7%
CER on the eval data, which can be improved by
nearly 10% absolute using the oracle from a 10-
best list.
3.5 Rescoring
Using the n-best list sampled in the first source
layer, we also perform rescoring experiments.
Therefore, we rescore the transliteration hypothe-
Table 3: Oracle character error rates on 10-best
lists.
sampling layer oracle CER [%]
dev eval
S1 15.8 14.8
joint layer 17.5 16.4
T1 18.7 18.2
CER [%]
System dev eval
DBN w/o rescoring 24.1 22.7
w/ rescoring 21.3 20.1
Table 4: Results from the rescoring experiments
and fusion with the phrase-based MT system.
ses (after truncating the padding letters w0) with
additional models, which are commonly used in
MT, and which we have trained on the training
data:
? IBM model 1 lexical probabilities modelling
the probability for a target sequence given a
source sequence
hIBM1(f
J
1 , e
I
1)=? log
?
?
1
(I + 1)J
J?
j=1
I?
i=0
p(fj |ei)
?
?
? m-gram language model over the letter se-
quences
hLM(e
I
1) = ? log
I?
i=1
p(ei|e
i?1
i?m+1),
with m being the size of the m-gram, we
choose m = 9.
? sequence length model (commonly referred
to as word penalty).
Then, these models are fused in a log-linear model
(Och and Ney, 2002), and we tune the model scal-
ing factors discriminatively on the development n-
best list using the downhill simplex algorithm. Re-
sults from the rescoring experiments are given in
Table 4.
The performance of the DBN system is im-
proved on the dev data from 24.1% to 21.3% CER
and on the eval data from 22.7% to 20.1% CER.
238
3.6 Application Within a System
Combination Framework
Although being clearly outperformed by the
phrase-based MT system, we applied the translit-
eration candidates generated by the DBN ap-
proach within a system combination framework.
Motivated by the fact that the DBN approach
differs decisively from the other statistical ap-
proaches we applied to the machine transliteration
task, we wanted to investigate the potential ben-
efit of the diverse nature of the DBN transliter-
ations. Taking the transliteration candidates ob-
tained from another study which was intended to
perform a comparison of various statistical ap-
proaches to the transliteration task, we performed
the system combination as is customary in speech
recognition, i.e. following the Recognizer Output
Voting Error Reduction (ROVER) approach (Fis-
cus, 1997).
The following methods were investigated:
(Monotone) Phrase-based MT on character level:
A state-of-the-art phrase-based SMT system
(Zens and Ney, 2004) was used for name
transliteration, i.e. translation of characters
instead of words. No reordering model
was employed due to the monotonicity
of the transliteration task, and the model
scaling factors were tuned on maximum
transliteration accuracy.
Data-driven grapheme-to-phoneme conversion:
In Grapheme-to-Phoneme conversion (G2P),
or phonetic transcription, we seek the most
likely pronunciation (phoneme sequence)
for a given orthographic form (sequence of
letters). Then, a grapheme-phoneme joint
multi-gram, or graphone for short, is a pair
of a letter sequence and a phoneme sequence
of possibly different length (Bisani and Ney,
2008). The model training is done in two
steps: First, maximum likelihood is used to
infer the graphones. Second, the input is
segmented into a stream of graphones and
absolute discounting with leaving-one-out
is applied to estimate the actual M -gram
model. Interpreting the characters of the
English target names as phonemes, we used
the G2P toolkit of (Bisani and Ney, 2008) to
transliterate the Arabic names.
Position-wise maximum entropy models / CRFs:
The segmentation as provided by the G2P
model is used and ?null words? are inserted
such that the transliteration task can be
interpreted as a classical tagging task (e.g.
POS, conceptual tagging, etc.). This means
that we seek for a one-to-one mapping and
define feature functions to model the pos-
terior probability. Maximum entropy (ME)
models are defined position-wise, whereas
conditional random fields (CRFs) consider
full sequences. Both models were trained
according to the maximum class posterior
criterion. We used an ME tagger (Bender et
al., 2003) and the freely available CRF++
toolkit.2
Results for each of the individual systems and
different combinations are given in Table 5. As
expected, the DBN transliterations cannot keep up
with the other approaches. The additional models
(G2P, CRF and ME) perform slightly better than
the PBT method. If we look at combinations of
systems without the DBN approach, we observe
only marginal improvements of around 0.1-0.2%
CER. Interestingly, a combination of all 4 models
(PBT, G2P, ME, CRF) works as good as individual
3-way combinations (the same 11.9% on dev are
obtained). This can be interpreted as a potential
?similarity? of the approaches. Adding e.g. ME to
a combination of PBT, G2P and CRF does not im-
prove results because the transliteration hypothe-
ses are too similar. If we simply put together all
5 systems including DBN with equal weights, we
have a similar trend. Since all systems are equally
weighted and at least 3 of the systems are similar
in individual performance (G2P, ME, CRF have all
around 12% CER on the tested data sets), the DBN
approach does not get a large impact on overall
performance.
If we drop similar systems and tune for 3-way
combinations, we observe a large reduction in
CER if DBN comes into play. Compared to the
best individual system of 12% CER, we now ar-
rive at a CER of 10.9% for a combination of PBT,
CRF and DBN which is significantly better than
each of the individual methods. Our interpreta-
tion of this is that the DBN system has different
hypotheses compared to all other systems and that
the hypotheses from the other systems are too sim-
ilar to be apt for combination. So, although DBN
is much worse than the other approaches, it obvi-
ously helps in the system combination. Using the
rescored variant of the DBN transliterations from
2http://crfpp.sourceforge.net/
239
CER [%]
System dev eval
DBN 24.1 22.7
PBT 12.9 13.3
G2P 12.2 12.1
ME 12.3 12.4
CRF 12.0 12.0
ROVER
best setting w/o DBN 11.9 11.8
5-way equal weights 11.7 11.9
best setting w/ DBN 10.9 10.9
Table 5: Results from the individual methods in-
vestigated versus ROVER combination.
Section 3.5, performance is similar to the one ob-
tained for the DBN baseline.
4 Discussion and Conclusion
We have presented a novel method for machine
transliteration based on DBNs, which despite not
having competitive results can be an important ad-
ditional cue for system combination setups. The
DBN model has some immediate advantages: the
model is in principle fully bidirectional and is
based on sound and valid theories from machine
learning. Instead of common techniques which
are based on finite-state machines or phrase-based
machine translation, the proposed system does not
rely on word alignments and beam-search decod-
ing and has interesting properties regarding the re-
ordering of sequences. We have experimentally
evaluated the network structure and size, reorder-
ing capabilities, the creation of multiple hypothe-
ses, and rescoring and combination with other
transliteration approaches. It was shown that, al-
beit the approach cannot compete with the cur-
rent state of the art, deep belief networks might
be a learning framework with some potential for
transliteration. It was also shown that the pro-
posed method is suited for combination with dif-
ferent state-of-the-art systems and that improve-
ments over the single models can be obtained
in a ROVER-like setting. Furthermore, adding
DBN-based transliterations, although individually
far behind the other approaches, significantly im-
proves the overall results by 1% absolute.
Outlook
In the future we plan to investigate several details
of the proposed model: we will exploit the inher-
ent bidirectionality, further investigate the struc-
ture of the model, such as the number of layers
and the numbers of nodes in the individual lay-
ers. Also, it is important to improve the efficiency
of our implementation to allow for working on
larger datasets and obtain more competitive re-
sults. Furthermore, we are planning to investigate
convolutional input layers for transliteration and
use a translation approach analogous to the one
proposed by Collobert and Weston (2008) in or-
der to allow for the incorporation of reorderings,
language models, and to be able to work on larger
tasks.
Acknowledgement. We would like to thank Ge-
offrey Hinton for providing the Matlab Code ac-
companying (Hinton and Salakhutdinow, 2006).
References
D. Ackley, G. Hinton, and T. Sejnowski. 1985. A
learning algorithm for Boltzmann machines. Cog-
nitive Science, 9(1):147?169.
Y. Al-Onaizan and K. Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
2002 Workshop on Computationaal Approaches to
Semitic Languages.
M. Asuncio?n Castan?o, F. Casacuberta, and E. Vidal.
1997. Machine translation using neural networks
and finite-state models. In Theoretical and Method-
ological Issues in Machine Translation (TMI), pages
160?167, Santa Fe, NM, USA, July.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Sta-
tistical machine translation through global lexical
selection and sentence reconstruction. In Annual
Meeting of the Association for Computational Lin-
gustic (ACL), pages 152?159, Prague, Czech Repub-
lic.
O. Bender, F. J. Och, and H. Ney. 2003. Maxi-
mum entropy models for named entity recognition.
In Proc. 7th Conf. on Computational Natural Lan-
guage Learning (CoNLL), pages 148?151, Edmon-
ton, Canada, May.
M. Bisani and H. Ney. 2008. Joint-sequence models
for grapheme-to-phoneme conversion. Speech Com-
munication, 50(5):434?451, May.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, Helsinki, Finn-
land, July.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 347?354, Santa Barbara, CA, USA, Decem-
ber.
240
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247, Prague, Czech
Republic, June.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence, 6(6):721?741, November.
G. Hinton and R. R. Salakhutdinow. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313:504?507, July.
G. Hinton, S. Osindero, and Y.-W. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527?1554.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In HLT-NAACL.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(2).
A. Mnih and G. Hinton. 2007. Three new graphical
models for statistical language modelling. In ICML
?07: International Conference on Machine Learn-
ing, pages 641?648, New York, NY, USA. ACM.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Annual Meeting of the As-
soc. for Computational Linguistics, pages 295?302,
Philadelphia, PA, USA, July.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 632?639,
Prague, Czech Republic, June.
A. Torralba, R. Fergus, and Y. Weiss. 2008. Small
codes and large image databases for recognition. In
IEEE Conference on Computer Vision and Pattern
Recognition, Anchorage, AK, USA, June.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: HLT-NAACL
2004, pages 257?264, Boston, MA, May.
241
