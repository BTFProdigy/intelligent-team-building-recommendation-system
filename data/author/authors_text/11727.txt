Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 56?60,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Machine Translation System Combination with Flexible Word Ordering
Kenneth Heafield, Greg Hanneman, Alon Lavie
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
{kheafiel,ghannema,alavie}@cs.cmu.edu
Abstract
We describe a synthetic method for com-
bining machine translations produced by
different systems given the same input.
One-best outputs are explicitly aligned
to remove duplicate words. Hypotheses
follow system outputs in sentence order,
switching between systems mid-sentence
to produce a combined output. Experi-
ments with the WMT 2009 tuning data
showed improvement of 2 BLEU and 1
METEOR point over the best Hungarian-
English system. Constrained to data pro-
vided by the contest, our system was sub-
mitted to the WMT 2009 shared system
combination task.
1 Introduction
Many systems for machine translation, with dif-
ferent underlying approaches, are of competitive
quality. Nonetheless these approaches and sys-
tems have different strengths and weaknesses. By
offsetting weaknesses with strengths of other sys-
tems, combination can produce higher quality than
does any component system.
One approach to system combination uses con-
fusion networks (Rosti et al, 2008; Karakos et
al., 2008). In the most common form, a skele-
ton sentence is chosen from among the one-best
system outputs. This skeleton determines the or-
dering of the final combined sentence. The re-
maining outputs are aligned with the skeleton, pro-
ducing a list of alternatives for each word in the
skeleton, which comprises a confusion network. A
decoder chooses from the original skeleton word
and its alternatives to produce a final output sen-
tence. While there are a number of variations on
this theme, our approach differs fundamentally in
that the effective skeleton changes on a per-phrase
basis.
Our system is an enhancement of our previous
work (Jayaraman and Lavie, 2005). A hypothesis
uses words from systems in order, switching be-
tween systems at phrase boundaries. Alignments
and a synchronization method merge meaning-
equivalent output from different systems. Hy-
potheses are scored based on system confidence,
alignment support, and a language model.
We contribute a few enhancements to this pro-
cess. First, we introduce an alignment-sensitive
method for synchronizing available hypothesis ex-
tensions across systems. Second, we pack similar
partial hypotheses, which allows greater diversity
in our beam search while maintaining the accuracy
of n-best output. Finally, we describe an improved
model selection process that determined our sub-
missions to the WMT 2009 shared system combi-
nation task.
The remainder of this paper is organized as fol-
lows. Section 2 describes the system with empha-
sis on our modifications. Tuning, our experimen-
tal setup, and submitted systems are described in
Section 3. Section 4 concludes.
2 System
The system consists of alignment (Section 2.1)
and phrase detection (Section 2.2) followed by de-
coding. The decoder constructs hypothesis sen-
tences one word at a time, starting from the left. A
partially constructed hypothesis comprises:
Word The most recently decoded word. Initially,
this is the beginning of sentence marker.
Used The set of used words from each system.
Initially empty.
Phrase The current phrase constraint from Sec-
tion 2.2, if any. The initial hypothesis is not
in a phrase.
Features Four feature values defined in Section
2.4 and used in Section 2.5 for beam search
56
and hypothesis ranking. Initially, all features
are 1.
Previous A set of preceding hypothesis pointers
described in Section 2.5. Initially empty.
The leftmost unused word from each system
corresponds to a continuation of the partial hy-
pothesis. Therefore, for each system, we extend a
partial hypothesis by appending that system?s left-
most unused word, yielding several new hypothe-
ses. The appended word, and those aligned with it,
are marked as used in the new hypothesis. Since
systems do not align perfectly, too few words may
be marked as used, a problem addressed in Sec-
tion 2.3. As described in Section 2.4, hypotheses
are scored using four features based on alignment,
system confidence, and a language model. Since
the search space is quite large, we use these partial
scores for a beam search, where the beam contains
hypotheses of equal length. This space contains
hypotheses that extend in precisely the same way,
which we exploit in Section 2.5 to increase diver-
sity. Finally, a hypothesis is complete when the
end of sentence marker is appended.
2.1 Alignment
Sentences from different systems are aligned in
pairs using a modified version of the METEOR
(Banerjee and Lavie, 2005) matcher. This iden-
tifies alignments in three phases: exact matches
up to case, WordNet (Fellbaum, 1998) morphol-
ogy matches, and shared WordNet synsets. These
sources of alignments are quite precise and unable
to pick up on looser matches such as ?mentioned?
and ?said? that legitimately appear in output from
different systems. Artificial alignments are in-
tended to fill gaps by using surrounding align-
ments as clues. If a word is not aligned to any
word in some other sentence, we search left and
right for words that are aligned into that sentence.
If these alignments are sufficiently close to each
other in the other sentence, words between them
are considered for artificial alignment. An arti-
ficial alignment is added if a matching part of
speech is found. The algorithm is described fully
by Jayaraman and Lavie (2005).
2.2 Phrases
Switching between systems is permitted outside
phrases or at phrase boundaries. We find phrases
in two ways. Alignment phrases are maximally
long sequences of words which align, in the same
order and without interruption, to a word se-
quence from at least one other system. Punctua-
tion phrases place punctuation in a phrase with the
preceding word, if any. When the decoder extends
a hypothesis, it considers the longest phrase in
which no word is used. If a punctuation phrase is
partially used, the decoder marks the entire phrase
as used to avoid extraneous punctuation.
2.3 Synchronization
While phrases address near-equal pieces of trans-
lation output, we must also deal with equally
meaningful output that does not align. The im-
mediate effect of this issue is that too few words
are marked as used by the decoder, leading to du-
plication in the combined output. In addition, par-
tially aligned system output results in lingering un-
used words between used words. Often these are
function words that, with language model scoring,
make output unnecessarily verbose. To deal with
this problem, we expire lingering words by mark-
ing them as used. Specifically, we consider the
frontier of each system, which is the leftmost un-
used word. If a frontier lags behind, words as used
to advance the frontier. Our two methods for syn-
chronization differ in how frontiers are compared
across systems and the tightness of the constraint.
Previously, we measured frontiers from the be-
ginning of sentence. Based on this measurement,
the synchronization constraint requires that the
frontiers of each system differ by at most s. Equiv-
alently, a frontier is lagging if it is more than s
words behind the rightmost frontier. Lagging fron-
tiers are advanced until the synchronization con-
straint becomes satisfied. We found this method
can cause problems in the presence of variable
length output. When the variability in output
length exceeds s, proper synchronization requires
distances between frontiers greater than s, which
this constraint disallows.
Alignments indicate where words are syn-
chronous. Words near an alignment are also likely
to be synchronous even without an explicit align-
ment. For example, in the fragments ?even more
serious, you? and ?even worse, you? from WMT
2008, ?serious? and ?worse? do not align but
do share relative position from other alignments,
suggesting these are synchronous. We formalize
this by measuring the relative position of fron-
tiers from alignments on each side. For example,
57
if the frontier itself is aligned then relative posi-
tion is zero. For each pair of systems, we check
if these relative positions differ by at most s un-
der an alignment on either side. Confidence in a
system?s frontier is the sum of the system?s own
confidence plus confidence in systems for which
the pair-wise constraint is satisfied. If confidence
in any frontier falls below 0.8, the least confident
lagging frontier is advanced. The process repeats
until the constraint becomes satisfied.
2.4 Scores
We score partial and complete hypotheses using
system confidence, alignments, and a language
model. Specifically, we have four features which
operate at the word level:
Alignment Confidence in the system from which
the word came plus confidence in systems to
which the word aligns.
Language Model Score from a suffix array lan-
guage model (Zhang and Vogel, 2006)
trained on English from monolingual and
French-English data provided by the contest.
N -Gram
(
1
3
)order?ngram
using language model
order and length of ngram found.
Overlap overlaporder?1 where overlap is the length of
intersection between the preceding and cur-
rent n-grams.
The N -Gram and Overlap features are intended to
improve fluency across phrase boundaries. Fea-
tures are combined using a log-linear model
trained as discussed in Section 3. Hypotheses are
scored using the geometric average score of each
word in the hypothesis.
2.5 Search
Of note is that a word?s score is impacted only by
its alignments and the n-gram found by the lan-
guage model. Therefore two partial hypotheses
that differ only in words preceding the n-gram and
in their average score are in some sense duplicates.
With the same set of used words and same phrase
constraint, they extend in precisely the same way.
In particular, the highest scoring hypothesis will
never use a lower scoring duplicate.
We use duplicate detecting beam search to ex-
plore our hypothesis space. A beam contains par-
tial hypotheses of the same length. Duplicate
hypotheses are detected on insertion and packed,
with the combined hypothesis given the highest
score of those packed. Once a beam contains the
top scoring partial hypotheses of length l, these
hypotheses are extended to length l+1 and placed
in another beam. Those hypotheses reaching end
of sentence are placed in a separate beam, which is
equivalent to packing them into one final hypoth-
esis. Once we remove partial hypothesis that did
not extend to the final hypothesis, the hypotheses
are a lattice connected by parent pointers.
While we submitted only one-best hypotheses,
accurate n-best hypotheses are important for train-
ing as explained in Section 3. Unpacking the hy-
pothesis lattice into n-best hypotheses is guided
by scores stored in each hypothesis. For this task,
we use an n-best beam of paths from the end of
sentence hypothesis to a partial hypothesis. Paths
are built by induction, starting with a zero-length
path from the end of sentence hypothesis to itself.
The top scoring path is removed and its terminal
hypothesis is examined. If it is the beginning of
sentence, the path is output as a complete hypoth-
esis. Otherwise, we extend the path to each parent
hypothesis, adjusting each path score as necessary,
and insert into the beam. This process terminates
with n complete hypotheses or an empty beam.
3 Tuning
Given the 502 sentences made available for tun-
ing by WMT 2009, we selected feature weights for
scoring, a set of systems to combine, confidence in
each selected system, and the type and distance s
of synchronization. Of these, only feature weights
can be trained, for which we used minimum error
rate training with version 1.04 of IBM-style BLEU
(Papineni et al, 2002) in case-insensitive mode.
We treated the remaining parameters as a model
selection problem, using 402 randomly sampled
sentences for training and 100 sentences for eval-
uation. This is clearly a small sample on which
to evaluate, so we performed two folds of cross-
validation to obtain average scores over 200 un-
trained sentences. We chose to do only two folds
due to limited computational time and a desire to
test many models.
We scored systems and our own output using
case-insensitive IBM-style BLEU 1.04 (Papineni
et al, 2002), METEOR 0.6 (Lavie and Agarwal,
2007) with all modules, and TER 5 (Snover et
al., 2006). For each source language, we ex-
58
In Sync s BLEU METE TER Systems and Confidences
cz length 8 .236 .507 59.1 google .46 cu-bojar .27 uedin .27
cz align 5 .226 .499 57.8 google .50 cu-bojar .25 uedin .25
cz align 7 .211 .508 65.9 cu-bojar .60 google .20 uedin .20
cz .231 .504 57.8 google
de length 7 .255 .531 54.2 google .40 uka .30 stuttgart .15 umd .15
de length 6 .260 .532 55.2 google .50 systran .25 umd .25
de align 9 .256 .533 55.5 google .40 uka .30 stuttgart .15 umd .15
de align 6 .200 .514 54.2 google .31 uedin .22 systran .18 umd .16 uka .14
de .244 .523 57.5 google
es align 8 .297 .560 52.7 google .75 uedin .25
es length 5 .289 .548 52.1 google .50 talp-upc .17 uedin .17 rwth .17
es .297 .558 52.7 google
fr align 6 .329 .574 49.9 google .70 lium1 .30
fr align 8 .314 .596 48.6 google .50 lium1 .30 limsi1 .20
fr length 8 .323 .570 48.5 google .50 lium1 .25 limsi1 .25
fr .324 .576 48.7 google
hu length 5 .162 .403 69.2 umd .50 morpho .40 uedin .10
hu length 8 .158 .407 69.5 umd .50 morpho .40 uedin .10
hu align 7 .153 .392 68.0 umd .33 morpho .33 uedin .33
hu .141 .391 66.1 umd
xx length 5 .326 .584 49.6 google-fr .61 google-es .39
xx align 4 .328 .580 49.5 google-fr .80 google-es .20
xx align 5 .324 .576 48.6 google-fr .61 google-es .39
xx align 7 .319 .587 51.1 google-fr .50 google-es .50
xx .324 .576 48.7 google-fr
Table 1: Combination models used for submission to WMT 2009. For each language, we list our pri-
mary combination, contrastive combinations, and a high-scoring system for comparison in italic. All
translations are into English. The xx source language combines translations from different languages,
in our case French and Spanish. Scores from BLEU, METEOR, and TER are the average of two cross-
validation folds with 100 evaluation sentences each. Numbers following system names indicate con-
trastive systems. More evaluation, including human scores, will be published by WMT.
perimented with various sets of high-scoring sys-
tems to combine. We also tried confidence val-
ues proportional to various powers of BLEU and
METEOR scores, as well as hand-picked values.
Finally we tried both variants of synchronization
with values of s ranging from 2 to 9. In total, 405
distinct models were evaluated. For each source
source language, our primary system was chosen
by performing well on all three metrics. Models
that scored well on individual metrics were sub-
mitted as contrastive systems. In Table 1 we report
the models underlying each submitted system.
4 Conclusion
We found our combinations are quite sensitive to
presence of and confidence in the underlying sys-
tems. Further, we show the most improvement
when these systems are close in quality, as is the
case with our Hungarian-English system. The
two methods of synchronization were surprisingly
competitive, a factor we attribute to short sentence
length compared with WMT 2008 Europarl sen-
tences. Opportunities for further work include per-
sentence system confidence, automatic training of
more parameters, and different alignment models.
We look forward to evaluation results from WMT
2009.
Acknowledgments
The authors wish to thank Jonathan Clark for
training the language model and other assistance.
This work was supported in part by the DARPA
GALE program and by a NSF Graduate Research
Fellowship.
59
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Proc.
ACLWorkshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65?72.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Shyamsundar Jayaraman and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proc. EAMT, pages 143?152.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL-08: HLT, Short Papers (Companion Vol-
ume), pages 81?84.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An automatic metric for MT evaluation with
high levels of correlation with human judgments.
In Proc. Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proc. 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proc. Seventh Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Language Technologies Institute, School of Com-
puter Science, Carnegie Mellon University, Pitts-
burgh, PA, Dec.
60
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1169?1178, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Language Model Rest Costs and Space-Efficient Storage
Kenneth Heafield?,? Philipp Koehn? Alon Lavie?
? Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{heafield,alavie}@cs.cmu.edu
? School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
pkoehn@inf.ed.ac.uk
Abstract
Approximate search algorithms, such as cube
pruning in syntactic machine translation, rely
on the language model to estimate probabili-
ties of sentence fragments. We contribute two
changes that trade between accuracy of these
estimates and memory, holding sentence-level
scores constant. Common practice uses lower-
order entries in an N -gram model to score
the first few words of a fragment; this vio-
lates assumptions made by common smooth-
ing strategies, including Kneser-Ney. Instead,
we use a unigram model to score the first
word, a bigram for the second, etc. This im-
proves search at the expense of memory. Con-
versely, we show how to save memory by col-
lapsing probability and backoff into a single
value without changing sentence-level scores,
at the expense of less accurate estimates for
sentence fragments. These changes can be
stacked, achieving better estimates with un-
changed memory usage. In order to interpret
changes in search accuracy, we adjust the pop
limit so that accuracy is unchanged and re-
port the change in CPU time. In a German-
English Moses system with target-side syntax,
improved estimates yielded a 63% reduction
in CPU time; for a Hiero-style version, the
reduction is 21%. The compressed language
model uses 26% less RAM while equivalent
search quality takes 27% more CPU. Source
code is released as part of KenLM.
1 Introduction
Language model storage is typically evaluated in
terms of speed, space, and accuracy. We introduce
a fourth dimension, rest cost quality, that captures
how well the model scores sentence fragments for
purposes of approximate search. Rest cost quality is
distinct from accuracy in the sense that the score of
a complete sentence is held constant. We first show
how to improve rest cost quality over standard prac-
tice by using additional space. Then, conversely, we
show how to compress the language model by mak-
ing a pessimistic rest cost assumption1.
Language models are designed to assign probabil-
ity to sentences. However, approximate search algo-
rithms use estimates for sentence fragments. If the
language model has order N (an N -gram model),
then the first N ? 1 words of the fragment have in-
complete context and the last N ? 1 words have not
been completely used as context. Our baseline is
common practice (Koehn et al 2007; Dyer et al
2010; Li et al 2009) that uses lower-order entries
from the language model for the first words in the
fragment and no rest cost adjustment for the last few
words. Formally, the baseline estimate for sentence
fragment wk1 is
(
N?1?
n=1
pN (wn|w
n?1
1 )
)(
k?
n=N
pN (wn|w
n?1
n?N+1)
)
where each wn is a word and pN is an N -gram lan-
guage model.
The problem with the baseline estimate lies in
lower order entries pN (wn|w
n?1
1 ). Commonly used
Kneser-Ney (Kneser and Ney, 1995) smoothing,
1Here, the term rest cost means an adjustment to the score of
a sentence fragment but not to whole sentences. The adjustment
may be good or bad for approximate search.
1169
including the modified version (Chen and Good-
man, 1998), assumes that a lower-order entry will
only be used because a longer match could not
be found2. Formally, these entries actually eval-
uate pN (wn|w
n?1
1 , did not find w
n
0 ). For purposes
of scoring sentence fragments, additional context is
simply indeterminate, and the assumption may not
hold.
As an example, we built 5-gram and unigram lan-
guage models with Kneser-Ney smoothing on the
same data. Sentence fragments frequently begin
with ?the?. Using a lower-order entry from the 5-
gram model, log10 p5(the) = ?2.49417. The uni-
gram model does not condition on backing off, as-
signing log10 p1(the) = ?1.28504. Intuitively, the
5-gram model is surprised, by more than an order of
magnitude, to see ?the? without matching words that
precede it.
To remedy the situation, we train N language
models on the same data. Each model pn is an n-
gram model (it has order n). We then use pn to
score the nth word of a sentence fragment. Thus,
a unigram model scores the first word of a sentence
fragment, a bigram model scores the second word,
and so on until either the n-gram is not present in
the model or the first N?1 words have been scored.
Storing probabilities from these models requires one
additional value per n-gram in the model, except for
N -grams where this probability is already stored.
Conversely, we can lower memory consumption
relative to the baseline at the expense of poorer rest
costs. Baseline models store two entries per n-gram:
probability and backoff. We will show that the prob-
ability and backoff values in a language model can
be collapsed into a single value for each n-gram
without changing sentence probability. This trans-
formation saves memory by halving the number of
values stored per entry, but it makes rest cost esti-
mates worse. Specifically, the rest cost pessimisti-
cally assumes that the model will back off to uni-
grams immediately following the sentence fragment.
The two modifications can be used independently
or simultaneously. To measure the impact of their
different rest costs, we experiment with cube prun-
ing (Chiang, 2007) in syntactic machine transla-
2Other smoothing techniques, including Witten-Bell (Witten
and Bell, 1991), do not make this assumption.
tion. Cube pruning?s goal is to find high-scoring
sentence fragments for the root non-terminal in the
parse tree. It does so by going bottom-up in the parse
tree, searching for high-scoring sentence fragments
for each non-terminal. Within each non-terminal, it
generates a fixed number of high-scoring sentence
fragments; this is known as the pop limit. Increasing
the pop limit therefore makes search more accurate
but costs more time. By moderating the pop limit,
improved accuracy can be interpreted as a reduction
in CPU time and vice-versa.
2 Related Work
Vilar and Ney (2011) study several modifications to
cube pruning and cube growing (Huang and Chiang,
2007). Most relevant is their use of a class-based
language model for the first of two decoding passes.
This first pass is cheaper because translation alter-
natives are likely to fall into the same class. Entries
are scored with the maximum probability over class
members (thereby making them no longer normal-
ized). Thus, paths that score highly in this first pass
may contain high-scoring paths under the lexicalized
language model, so the second pass more fully ex-
plores these options. The rest cost estimates we de-
scribe here could be applied in both passes, so our
work is largely orthogonal.
Zens and Ney (2008) present rest costs for phrase-
based translation. These rest costs are based on fac-
tors external to the sentence fragment, namely out-
put that the decoder may generate in the future. Our
rest costs examine words internal to the sentence
fragment, namely the first and last few words. We
also differ by focusing on syntactic translation.
A wide variety of work has been done on language
model compression. While data structure compres-
sion (Raj and Whittaker, 2003; Heafield, 2011) and
randomized data structures (Talbot and Osborne,
2007; Guthrie and Hepple, 2010) are useful, here
we are concerned solely with the values stored by
these data structures. Quantization (Whittaker and
Raj, 2001; Federico and Bertoldi, 2006) uses less
bits to store each numerical value at the expense
of model quality, including scores of full sentences,
and is compatible with our approach. In fact, the
lower-order probabilities might be quantized further
than normal since these are used solely for rest cost
1170
purposes. Our compression technique reduces stor-
age from two values, probability and backoff, to one
value, theoretically halving the bits per value (ex-
cept N -grams which all have backoff 1). This makes
the storage requirement for higher-quality modified
Kneser-Ney smoothing comparable to stupid back-
off (Brants et al 2007). Whether to use one smooth-
ing technique or the other then becomes largely an
issue of training costs and quality after quantization.
3 Contribution
3.1 Better Rest Costs
As alluded to in the introduction, the first few words
of a sentence fragment are typically scored us-
ing lower-order entries from an N -gram language
model. However, Kneser-Ney smoothing (Kneser
and Ney, 1995) conditions lower-order probabilities
on backing off. Specifically, lower-order counts are
adjusted to represent the number of unique exten-
sions an n-gram has:
a(wn1 ) =
{
|{w0 : c(wn0 ) > 0}| if n < N
c(wn1 ) if n = N
where c(wn1 ) is the number of times w
n
1 appears in
the training data3. This adjustment is also performed
for modified Kneser-Ney smoothing. The intuition
is based on the fact that the language model will
base its probability on the longest possible match. If
an N -gram was seen in the training data, the model
will match it fully and use the smoothed count. Oth-
erwise, the full N -gram was not seen in the train-
ing data and the model resorts to a shorter n-gram
match. Probability of this shorter match is based on
how often the n-gram is seen in different contexts.
Thus, these shorter n-gram probabilities are not rep-
resentative of cases where context is short simply
because additional context is unknown at the time of
scoring.
In some cases, we are able to determine that
the model will back off and therefore the lower-
order probability makes the appropriate assumption.
Specifically, if vwn1 does not appear in the model for
any word v, then computing p(wn|vw
n?1
1 ) will al-
3Counts are not modified for n-grams bound to the begin-
ning of sentence, namely those with w1 = <s>.
ways back off to wn?11 or fewer words
4. This crite-
rion is the same as used to minimize the length of left
language model state (Li and Khudanpur, 2008) and
can be retrieved for each n-gram without using addi-
tional memory in common data structures (Heafield
et al 2011).
Where it is unknown if the model will back off,
we use a language model of the same order to pro-
duce a rest cost. Specifically, there are N language
models, one of each order from 1 to N . The mod-
els are trained on the same corpus with the same
smoothing parameters to the extent that they apply.
We then compile these into one data structure where
each n-gram record has three values:
1. Probability pn from the n-gram language
model
2. Probability pN from the N -gram language
model
3. Backoff b from the N -gram language model
For N -grams, the two probabilities are the same and
backoff is always 1, so only one value is stored.
Without pruning, the n-gram model contains the
same n-grams as the N -gram model. With prun-
ing, the two sets may be different, so we query the
n-gram model in the normal way to score every n-
gram in the N -gram model. The idea is that pn is the
average conditional probability that will be encoun-
tered once additional context becomes known. We
also tried more complicated estimates by addition-
ally interpolating upper bound, lower bound, and pN
with weights trained on cube pruning logs; none of
these improved results in any meaningful way.
Formalizing the above, let wk1 be a sentence frag-
ment. Choose the largest s so that vws1 appears in
the model for some v; equivalently ws1 is the left
state described in Li and Khudanpur (2008). The
4Usually, this happens because wn1 does not appear, though
it can also happen that wn1 appears but all vw
n
1 were removed
by pruning or filtering.
1171
baseline estimate is
pb(w
k
1) =
(
s?
n=1
pN (wn|w
n?1
1 )
)
?
(
N+1?
n=s+1
pN (wn|w
n?1
1 )
)
? (1)
(
k?
n=N
pN (wn|w
n?1
n?N+1)
)
while our improved estimate is
pr(w
k
1) =
(
s?
n=1
pn(wn|w
n?1
1 )
)
?
(
N+1?
n=s+1
pN (wn|w
n?1
1 )
)
? (2)
(
k?
n=N
pN (wn|w
n?1
n?N+1)
)
The difference between these equations is that pn is
used for words in the left state i.e. 1 ? n ? s.
We have also abused notation by using pN to denote
both probabilities stored explicitly in the model and
the model?s backoff-smoothed probabilities when
not present. It is not necessary to store backoffs for
pn because s was chosen such that all queried n-
grams appear in the model.
This modification to the language model improves
rest costs (and therefore quality or CPU time) at the
expense of using more memory to store pn. In the
next section, we do the opposite: make rest costs
worse to reduce storage size.
3.2 Less Memory
Many language model smoothing strategies, includ-
ing modified Kneser-Ney smoothing, use the back-
off algorithm shown in Figure 1. Given an n-gram
wn1 , the backoff algorithm bases probability on as
much context as possible. Equivalently, it finds
the minimum f so that wnf is in the model then
uses p(wn|w
n?1
f ) as a basis. Backoff penalties b
are charged because a longer match was not found,
forming the product
p(wn|w
n?1
1 ) = p(wn|w
n?1
f )
f?1?
j=1
b(wn?1j ) (3)
Notably, the backoff penalties {b(wn?1j )}
n?1
j=1 are in-
dependent of wn, though which backoff penalties are
charged depends on f and therefore wn.
backoff? 1
for f = 1? n do
if wnf is in the model then
return p(wn|wn?1f ) ? backoff
else
if wn?1f is in the model then
backoff? backoff ? b(wn?1f )
end if
end if
end for
Figure 1: The baseline backoff algorithm to com-
pute p(wn|w
n?1
1 ). It always terminates with a prob-
ability because even unknown words are treated as a
unigram.
for f = 1? n do
if wnf is in the model then
return q(wn|wn?1f )
end if
end for
Figure 2: The run-time pessimistic backoff algo-
rithm to find q(wn|w
n?1
1 ). It assumes that q has been
computed at model building time.
In order to save memory, we propose to account
for backoff in a different way, defining q
q(wn|w
n?1
1 ) =
p(wn|w
n?1
f )
?n
j=f b(w
n
j )
?n?1
j=f b(w
n?1
j )
where again wnf is the longest matching entry in the
model. The idea is that q is a term in the telescop-
ing series that scores a sentence fragment, shown
in equation (1) or (2). The numerator pessimisti-
cally charges all backoff penalties, as if the next
word wn+1 will only match a unigram. When wn+1
is scored, the denominator of q(wn+1|wn1 ) cancels
out backoff terms that were wrongly charged. Once
these terms are canceled, all that is left is p, the cor-
rect backoff penalties, and terms on the edge of the
series.
1172
Proposition 1. The terms of q telescope. Formally,
let wk1 be a sentence fragment and f take the mini-
mum value so that wkf is in the model. Then,
q(wk1) = p(w
k
1)
k?
j=f
b(wkj )
Proof. By induction on k. When k = 1, f = 1 since
the word w1 is either in the vocabulary or mapped to
<unk> and treated like a unigram.
q(w1) =
p(w1)
?1
j=1 b(w
1
j )
?0
j=1 b(w
0
j )
= p(w1)b(w1)
For k > 1,
q(wk1) = q(w
k?1
1 )q(wk|w
k?1
1 )
=
q(wk?11 )p(wk|w
k?1
f )
?k
j=f b(w
k
j )
?k?1
j=f b(w
k?1
j )
where f has the lowest value such that wkf is in the
model. Applying the inductive hypothesis to expand
q(wk?11 ), we obtain
p(wk?11 )
(?k?1
j=e b(w
k?1
j )
)
p(wk|w
k?1
f )
?k
j=f b(w
k
j )
?k?1
j=f b(w
k?1
j )
where e has the lowest value such that wk?1e is in the
model. The backoff terms cancel to yield
p(wk?11 )
?
?
f?1?
j=e
b(wk?1j )
?
? p(wk|w
k?1
f )
k?
j=f
b(wkj )
By construction of e, wk?1j is not in the model for all
j < e. Hence, b(wk?1j ) = 1 implicitly for all j < e.
Multiplying by 1,
p(wk?11 )
?
?
f?1?
j=1
b(wk?1j )
?
? p(wk|w
k?1
f )
k?
j=f
b(wkj )
Recognizing the backoff equation (3) to simplify,
p(wk?11 )p(wk|w
k?1
1 )
k?
j=f
b(wkj )
Finally, the conditional probability folds as desired
q(wk1) = p(w
k
1)
k?
j=f
b(wkj )
We note that entries ending in </s> have back-
off 1, so it follows from Proposition 1 that sentence-
level scores are unchanged.
q(<s> wk1 </s>) = p(<s> w
k
1 </s>)
Proposition 1 characterizes q as a pessimistic rest
cost on sentence fragments that scores sentences in
exactly the same way as the baseline using p and
b. To save memory, we simply store q in lieu of
p and b. Compared with the baseline, this halves
number of values from two to one float per n-gram,
except N -grams that already have one value. The
impact of this reduction is substantial, as seen in
Section 4.3. Run-time scoring is also simplified
as shown in Figure 2 since the language model lo-
cates the longest match wnf then returns the value
q(wn|w
n?1
1 ) = q(wn|w
n?1
f ) without any calcula-
tion or additional lookup. Baseline language mod-
els either retrieve backoffs values with additional
lookups (Stolcke, 2002; Federico et al 2008) or
modify the decoder to annotate sentence fragments
with backoff information (Heafield, 2011); we have
effectively moved this step to preprocessing. The
disadvantage is that q is not a proper probability and
it produces worse rest costs than does the baseline.
Language models are actually applied at two
points in syntactic machine translation: scoring lexi-
cal items in grammar rules and during cube pruning.
Grammar scoring is an offline and embarrassingly
parallel process where memory is not as tight (since
the phrase table is streamed) and fewer queries
are made, so slow non-lossy compression and even
network-based sharding can be used. We there-
fore use an ordinary language model for grammar
scoring and only apply the compressed model dur-
ing cube pruning. Grammar scoring impacts gram-
mar pruning (by selecting only top-scoring grammar
rules) and the order in which rules are tried during
cube pruning.
1173
3.3 Combined Scheme
Our two language model modifications can be triv-
ially combined by using lower-order probabilities on
the left of a fragment and by charging all backoff
penalties on the right of a fragment. The net result is
a language model that uses the same memory as the
baseline but has better rest cost estimates.
4 Experiments
To measure the impact of different rest costs, we
use the Moses chart decoder (Koehn et al 2007)
for the WMT 2011 German-English translation task
(Callison-Burch et al 2011). Using the Moses
pipeline, we trained two syntactic German-English
systems, one with target-side syntax and the other
hierarchical with unlabeled grammar rules (Chiang,
2007). Grammar rules were extracted from Europarl
(Koehn, 2005) using the Collins parser (Collins,
1999) for syntax on the English side. The language
model interpolates, on the WMT 2010 test set, sep-
arate models built on Europarl, news commentary,
and the WMT news data for each year. Models were
built and interpolated using SRILM (Stolcke, 2002)
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1998) and the de-
fault pruning settings. In all scenarios, the primary
language model has order 5. For lower-order rest
costs, we also built models with orders 1 through 4
then used the n-gram model to score n-grams in the
5-gram model. Feature weights were trained with
MERT (Och, 2003) on the baseline using a pop limit
of 1000 and 100-best output. Since final feature val-
ues are unchanged, we did not re-run MERT in each
condition. Measurements were collected by running
the decoder on the 3003-sentence test set.
4.1 Rest Costs as Prediction
Scoring the first few words of a sentence fragment
is a prediction task. The goal is to predict what
the probability will be when more context becomes
known. In order to measure performance on this
task, we ran the decoder on the hierarchical system
with a pop limit of 1000. Every time more context
became known, we logged5 the prediction error (es-
timated log probability minus updated log probabil-
5Logging was only enabled for this experiment.
Lower Baseline
n Mean Bias MSE Var Bias MSE Var
1 -3.21 .10 .84 .83 -.12 .87 .86
2 -2.27 .04 .18 .17 -.14 .23 .24
3 -1.80 .02 .07 .07 -.09 .10 .09
4 -1.29 .01 .04 .04 -.10 .09 .08
Table 1: Bias (mean error), mean squared error, and
variance (of the error) for the lower-order rest cost
and the baseline. Error is the estimated log prob-
ability minus the final probability. Statistics were
computed separately for the first word of a fragment
(n = 1), the second word (n = 2), etc. The lower-
order estimates are better across the board, reducing
error in cube pruning. All numbers are in log base
ten, as is standard for ARPA-format language mod-
els. Statistics were only collected for words with
incomplete context.
ity) for both lower-order rest costs and the baseline.
Table 1 shows the results.
Cube pruning uses relative scores, so bias mat-
ters less, though positive bias will favor rules with
more arity. Variance matters the most because lower
variance means cube pruning?s relative rankings are
more accurate. Our lower-order rest costs are bet-
ter across the board in terms of absolute bias, mean
squared error, and variance.
4.2 Pop Limit Trade-Offs
The cube pruning pop limit is a trade-off between
search accuracy and CPU time. Here, we mea-
sure how our rest costs improve (or degrade) that
trade-off. Search accuracy is measured by the aver-
age model score of single-best translations. Model
scores are scale-invariant and include a large con-
stant factor; higher is better. We also measure over-
all performance with uncased BLEU (Papineni et al
2002). CPU time is the sum of user and system time
used by Moses divided by the number of sentences
(3003). Timing includes time to load, though files
were forced into the disk cache in advance. Our test
machine has 64 GB of RAM and 32 cores. Results
are shown in Figures 3 and 4.
Lower-order rest costs perform better in both sys-
tems, reaching plateau model scores and BLEU with
less CPU time. The gain is much larger for tar-
1174
Baseline Lower Order Pessimistic Combined
Pop CPU Model BLEU CPU Model BLEU CPU Model BLEU CPU Model BLEU
2 3.29 -105.56 20.45 3.68 -105.44 20.79 3.74 -105.62 20.01 3.18 -105.49 20.43
10 5.21 -104.74 21.13 5.50 -104.72 21.26 5.43 -104.77 20.85 5.67 -104.75 21.10
50 23.30 -104.31 21.36 23.51 -104.24 21.38 23.68 -104.33 21.25 24.29 -104.22 21.34
500 54.61 -104.25 21.33 55.92 -104.15 21.38 54.23 -104.26 21.31 55.74 -104.15 21.40
700 64.08 -104.25 21.34 87.02 -104.14 21.42 68.74 -104.25 21.29 78.84 -104.15 21.41
(a) Numerical results for select pop limits.
-104.6
-104.5
-104.4
-104.3
-104.2
-104.1
0 10 20 30 40 50 60 70 80 90
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
21
21.05
21.1
21.15
21.2
21.25
21.3
21.35
21.4
0 10 20 30 40 50 60 70 80 90
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
(b) Model and BLEU scores near the plateau.
Figure 3: Target-syntax performance. CPU time and model score are averaged over 3003 sentences.
get syntax, where a pop limit of 50 outperforms the
baseline with pop limit 700. CPU time per sen-
tence is reduced to 23.5 seconds from 64.0 seconds,
a 63.3% reduction. The combined setting, using the
same memory as the baseline, shows a similar 62.1%
reduction in CPU time. We attribute this differ-
ence to improved grammar rule scoring that impacts
pruning and sorting. In the target syntax model,
the grammar is not saturated (i.e. less pruning will
still improve scores) but we nonetheless prune for
tractability reasons. The lower-order rest costs are
particularly useful for grammar pruning because lex-
ical items are typically less than five words long (and
frequently only word).
The hierarchical grammar is nearly saturated with
respect to grammar pruning, so improvement there is
due mostly to better search. In the hierarchical sys-
tem, peak BLEU 22.34 is achieved under the lower-
order condition with pop limits 50 and 200, while
other scenarios are still climbing to the plateau. With
a pop limit of 1000, the baseline?s average model
score is -101.3867. Better average models scores
are obtained from the lower-order model with pop
limit 690 using 79% of baseline CPU, the combined
model with pop limit 900 using 97% CPU, and the
pessimistic model with pop limit 1350 using 127%
CPU.
Pessimistic compression does worsen search, re-
quiring 27% more CPU in the hierarchical system to
achieve the same quality. This is worthwhile to fit
large-scale language models in memory, especially
if the alternative is a remote language model.
4.3 Memory Usage
Our rest costs add a value (for lower-order prob-
abilities) or remove a value (pessimistic compres-
sion) for each n-gram except those of highest order
(n = N ). The combined condition adds one value
1175
Baseline Lower Order Pessimistic Combined
Pop CPU Model BLEU CPU Model BLEU CPU Model BLEU CPU Model BLEU
2 2.96 -101.85 21.19 2.44 -101.80 21.63 2.71 -101.90 20.85 3.05 -101.84 21.37
10 2.80 -101.60 21.90 2.42 -101.58 22.20 2.95 -101.63 21.74 2.69 -101.60 21.98
50 3.02 -101.47 22.18 3.11 -101.46 22.34 3.46 -101.48 22.08 2.67 -101.47 22.14
690 10.83 -101.39 22.28 11.45 -101.39 22.25 10.88 -101.40 22.25 11.19 -101.39 22.23
900 13.41 -101.39 22.27 14.00 -101.38 22.24 13.38 -101.39 22.25 14.09 -101.39 22.22
1000 14.50 -101.39 22.27 15.17 -101.38 22.25 15.09 -101.39 22.26 15.23 -101.39 22.23
1350 18.52 -101.38 22.27 19.16 -101.38 22.23 18.46 -101.39 22.25 18.61 -101.38 22.23
5000 59.67 -101.38 22.24 61.41 -101.38 22.22 59.76 -101.38 22.27 61.38 -101.38 22.22
(a) Numerical results for select pop limits.
-101.42
-101.415
-101.41
-101.405
-101.4
-101.395
-101.39
-101.385
-101.38
0 5 10 15 20 25
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
21.95
22
22.05
22.1
22.15
22.2
22.25
22.3
22.35
0 5 10 15 20 25
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
(b) Model and BLEU scores near the plateau.
Figure 4: Hierarchical system performance. All values are averaged over 3003 sentences.
and removes another, so it uses the same memory
as the baseline. The memory footprint of adding or
removing a value depends on the number of such n-
grams, the underlying data structure, and the extent
of quantization. Our test language model has 135
million n-grams for n < 5 and 56 million 5-grams.
Memory usage was measured for KenLM data struc-
tures (Heafield, 2011) and minimal perfect hashing
(Guthrie and Hepple, 2010). For minimal perfect
hashing, we assume the Compress, Hash and Dis-
place algorithm (Belazzougui et al 2008) with 8-bit
signatures and 8-bit quantization. Table 2 shows the
results. Storage size of the smallest model is reduced
by 26%, bringing higher-quality smoothed models
in line with stupid backoff models that also store one
value per n-gram.
Structure Baseline Change %
Probing 4,072 517 13%
Trie 2,647 506 19%
8-bit quantized trie 1,236 140 11%
8-bit minimal perfect hash 540 140 26%
Table 2: Size in megabytes of our language model,
excluding operating system overhead. Change is the
cost of adding an additional value to store lower-
order probabilities. Equivalently, it is the savings
from pessimistic compression.
1176
5 Conclusion
Our techniques reach plateau-level BLEU scores
with less time or less memory. Efficiently stor-
ing lower-order probabilities and using them as rest
costs improves both cube pruning (21% CPU reduc-
tion in a hierarchical system) and model filtering
(net 63% CPU time reduction with target syntax) at
the expense of 13-26% more RAM for the language
model. This model filtering improvement is surpris-
ing both in the impact relative to changing the pop
limit and simplicity of implementation, since it can
be done offline. Compressing the language model to
halve the number of values per n-gram (except N -
grams) results in a 13-26% reduction in RAM with
26% over the smallest model, costing 27% more
CPU and leaving overall sentence scores unchanged.
This compression technique is likely to have more
general application outside of machine translation,
especially where only sentence-level scores are re-
quired. Source code is being released6 under the
LGPL as part of KenLM (Heafield, 2011).
Acknowledgements
This work was supported by the National Sci-
ence Foundation under grants DGE-0750271, IIS-
0713402, and IIS-0915327; by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme), and by the DARPA GALE
program. Benchmarks were run on Trestles at the
San Diego Supercomputer Center under allocation
TG-CCR110017. Trestles is part of the Extreme
Science and Engineering Discovery Environment
(XSEDE), which is supported by National Science
Foundation grant number OCI-1053575.
References
Djamal Belazzougui, Fabiano C. Botelho, and Martin Di-
etzfelbinger. 2008. Hash, displace, and compress. In
Proceedings of the 35th international colloquium on
Automata, Languages and Programming (ICALP ?08),
pages 385?396.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
6http://kheafield.com/code/kenlm/
Language Processing and Computational Language
Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228, June.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ?10, pages 7?12.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of the Workshop on
Statistical Machine Translation, pages 94?101, New
York City, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Proceedings of Inter-
speech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the web
in memory: Space efficient language models with con-
stant time retrieval. In Proceedings of EMNLP 2010,
Los Angeles, CA.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, San Francisco, CA, USA, De-
cember.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
1177
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second ACL Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
10?18, Columbus, Ohio, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135?139, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Bhiksha Raj and Ed Whittaker. 2003. Lossless compres-
sion of language model structure and word identifiers.
In Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 388?
391.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL, pages 512?519, Prague, Czech
Republic.
David Vilar and Hermann Ney. 2011. Cardinality prun-
ing and language model heuristics for hierarchical
phrase-based translation. Machine Translation, pages
1?38, November. DOI 10.1007/s10590-011-9119-4.
Ed Whittaker and Bhiksha Raj. 2001. Quantization-
based language model compression. In Proceedings
of EUROSPEECH, pages 33?36, September.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Richard Zens and Hermann Ney. 2008. Improvements in
dynamic programming beam search for phrase-based
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), Honolulu, Hawaii, October.
1178
Proceedings of NAACL-HLT 2013, pages 958?968,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Grouping Language Model Boundary Words to Speed K?Best Extraction
from Hypergraphs
Kenneth Heafield?,? Philipp Koehn? Alon Lavie?
? School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
pkoehn@inf.ed.ac.uk
? Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{heafield,alavie}@cs.cmu.edu
Abstract
We propose a new algorithm to approximately
extract top-scoring hypotheses from a hyper-
graph when the score includes an N?gram
language model. In the popular cube prun-
ing algorithm, every hypothesis is annotated
with boundary words and permitted to recom-
bine only if all boundary words are equal.
However, many hypotheses share some, but
not all, boundary words. We use these com-
mon boundary words to group hypotheses and
do so recursively, resulting in a tree of hy-
potheses. This tree forms the basis for our
new search algorithm that iteratively refines
groups of boundary words on demand. Ma-
chine translation experiments show our algo-
rithm makes translation 1.50 to 3.51 times as
fast as with cube pruning in common cases.
1 Introduction
This work presents a new algorithm to search a
packed data structure for high-scoring hypothe-
ses when the score includes an N?gram language
model. Many natural language processing systems
have this sort of problem e.g. hypergraph search
in hierarchical and syntactic machine translation
(Mi et al, 2008; Klein and Manning, 2001), lat-
tice rescoring in speech recognition, and confusion
network decoding in optical character recognition
(Tong and Evans, 1996). Large language models
have been shown to improve quality, especially in
machine translation (Brants et al, 2007; Koehn and
Haddow, 2012). However, language models make
search computationally expensive because they ex-
amine surface words without regard to the structure
at North Korea
in North Korea
with North Korea
with the DPRK
at
?
?
?
North Koreain
with
{
the DPRK
Figure 1: Hypotheses are grouped by common prefixes
and suffixes.
of the packed search space. Prior work, including
cube pruning (Chiang, 2007), has largely treated the
language model as a black box. Our new search
algorithm groups hypotheses by common prefixes
and suffixes, exploiting the tendency of the language
model to score these hypotheses similarly. An exam-
ple is shown in Figure 1. The result is a substantial
improvement over the time-accuracy trade-off pre-
sented by cube pruning.
The search spaces mentioned in the previous para-
graph are special cases of a directed acyclic hyper-
graph. As used here, the difference from a nor-
mal graph is that an edge can go from one vertex
to any number of vertices; this number is the arity
of the edge. Lattices and confusion networks are
hypergraphs in which every edge happens to have
arity one. We experiment with parsing-based ma-
chine translation, where edges represent grammar
rules that may have any number of non-terminals,
including zero.
Hypotheses are paths in the hypergraph scored by
a linear combination of features. Many features are
additive: they can be expressed as weights on edges
that sum to form hypothesis features. However, log
probability from anN?gram language model is non-
958
additive because it examines surface strings across
edge and vertex boundaries. Non-additivity makes
search difficult because locally optimal hypotheses
may not be globally optimal.
In order to properly compute the language model
score, each hypothesis is annotated with its bound-
ary words, collectively referred to as its state (Li
and Khudanpur, 2008). Hypotheses with equal state
may be recombined, so a straightforward dynamic
programming approach (Bar-Hillel et al, 1964) sim-
ply treats state as an additional dimension in the dy-
namic programming table. However, this approach
quickly becomes intractable for large language mod-
els where the number of states is too large.
Beam search (Chiang, 2005; Lowerre, 1976) ap-
proximates the straightforward algorithm by remem-
bering a beam of up to k hypotheses1 in each vertex.
It visits each vertex in bottom-up order, each time
calling a beam filling algorithm to select k hypothe-
ses. The parameter k is a time-accuracy trade-off:
larger k increases both CPU time and accuracy.
We contribute a new beam filling algorithm that
improves the time-accuracy trade-off over the popu-
lar cube pruning algorithm (Chiang, 2007) discussed
in ?2.3. The algorithm is based on the observation
that competing hypotheses come from the same im-
put, so their language model states are often similar.
Grouping hypotheses by these similar words enables
our algorithm to reason over multiple hypotheses at
once. The algorithm is fully described in ?3.
2 Related Work
2.1 Alternatives to Bottom-Up Search
Beam search visits each vertex in the hypergraph
in bottom-up (topological) order. The hypergraph
can also be searched in left-to-right order (Watanabe
et al, 2006; Huang and Mi, 2010). Alternatively,
hypotheses can be generated on demand with cube
growing (Huang and Chiang, 2007), though we note
that it showed little improvement in Moses (Xu and
Koehn, 2012). All of these options are compatible
with our algorithm. However, we only experiment
with bottom-up beam search.
1We use K to denote the number of fully-formed hypotheses
requested by the user and k to denote beam size.
2.2 Exhaustive Beam Filling
Originally, beam search was used with an exhaustive
beam filling algorithm (Chiang, 2005). It generates
every possible hypothesis (subject to the beams in
previous vertices), selects the top k by score, and
discards the remaining hypotheses. This is expen-
sive: just one edge of arity a encodes O(1 + ak)
hypotheses and each edge is evaluated exhaustively.
In the worst case, our algorithm is exhaustive and
generates the same number of hypotheses as beam
search; in practice, we are concerned with the aver-
age case.
2.3 Baseline: Cube Pruning
Cube pruning (Chiang, 2007) is a fast approximate
beam filling algorithm and our baseline. It chooses
k hypotheses by popping them off the top of a prior-
ity queue. Initially, the queue is populated with hy-
potheses made from the best (highest-scoring) parts.
These parts are an edge and a hypothesis from each
vertex referenced by the edge. When a hypothesis
is popped, several next-best alternatives are pushed.
These alternatives substitute the next-best edge or a
next-best hypothesis from one of the vertices.
Our work follows a similar pattern of popping one
queue entry then pushing multiple entries. However,
our queue entries are a group of hypotheses while
cube pruning?s entries are a single hypothesis.
Hypotheses are usually fully scored before being
placed in the priority queue. An alternative priori-
tizes hypotheses by their additive score. The addi-
tive score is the edge?s score plus the score of each
component hypothesis, ignoring the non-additive as-
pect of the language model. When the additive score
is used, the language model is only called k times,
once for each hypothesis popped from the queue.
Cube pruning can produce duplicate queue en-
tries. Gesmundo and Henderson (2010) modified the
algorithm prevent duplicates instead of using a hash
table. We include their work in the experiments.
Hopkins and Langmead (2009) characterized
cube pruning as A* search (Hart et al, 1968) with an
inadmissible heuristic. Their analysis showed deep
and unbalanced search trees. Our work can be inter-
preted as a partial rebalancing of these search trees.
959
2.4 Exact Algorithms
A number of exact search algorithms have been de-
veloped. We are not aware of an exact algorithm that
tractably scales to the size of hypergraphs and lan-
guage models used in many modern machine trans-
lation systems (Callison-Burch et al, 2012).
The hypergraph and language model can be com-
piled into an integer linear program. The best hy-
pothesis can then be recovered by taking the dual
and solving by Lagrangian relaxation (Rush and
Collins, 2011). However, that work only dealt with
language models up to order three.
Iglesias et al (2011) represent the search space
as a recursive transition network and the language
model as a weighted finite state transducer. Using
standard finite state algorithms, they intersect the
two automatons then exactly search for the highest-
scoring paths. However, the intersected automaton
is too large. The authors suggested removing low
probability entries from the language model, but this
form of pruning negatively impacts translation qual-
ity (Moore and Quirk, 2009; Chelba et al, 2010).
Their work bears some similarity to our algorithm
in that partially overlapping state will be collapsed
and efficiently handled together. However, the key
advatage to our approach is that groups have a score
that can be used for pruning before the group is ex-
panded, enabling pruning without first constructing
the intersected automaton.
2.5 Coarse-to-Fine
Coarse-to-fine (Petrov et al, 2008) performs mul-
tiple pruning passes, each time with more detail.
Search is a subroutine of coarse-to-fine and our work
is inside search, so the two are compatible. There are
several forms of coarse-to-fine search; the closest to
our work increases the language model order each
iteration. However, by operating inside search, our
algorithm is able to handle hypotheses at different
levels of refinement and use scores to choose where
to further refine hypotheses. Coarse-to-fine decod-
ing cannot do this because it determines the level of
refinement before calling search.
3 Our New Beam Filling Algorithm
In our algorithm, the primary idea is to group hy-
potheses with similar language model state. The
following sections formalize what these groups are
(partial state), that the groups have a recursive struc-
ture (state tree), how groups are split (bread crumbs),
using groups with hypergraph edges (partial edge),
prioritizing search (scoring) and best-first search
(priority queue).
3.1 Partial State
An N?gram language model (with order N ) com-
putes the probability of a word given the N ? 1 pre-
ceding words. The left state of a hypothesis is the
first N ? 1 words, which have insufficient context
to be scored. Right state is the last N ? 1 words;
these might become context for another hypothesis.
Collectively, they are known as state. State mini-
mization (Li and Khudanpur, 2008) may reduce the
size of state due to backoff in the language model.
For example, the hypothesis ?the few nations that
have diplomatic relations with North Korea? might
have left state ?the few? and right state ?Korea?
after state minimization determined that ?North?
could be elided. Collectively, the state is denoted
(the few a  ` Korea). The diamond  is a stand-in
for elided words. Terminators a and ` indicate when
left and right state are exhausted, respectively2.
Our algorithm is based on partial state. Par-
tial state is simply state with more inner words
elided. For example, (the  Korea) is a partial state
for (the few a  ` Korea). Terminators a and ` can
be elided just like words. Empty state is denoted
using the customary symbol for empty string, . For
example, (  ) is the empty partial state. The termi-
nators serve to distinguish a completed state (which
may be short due to state minimization) from an in-
complete partial state.
3.2 State Tree
States (the few a  ` Korea) and (the a  ` Korea)
have words in common, so the partial state
(the  Korea) can be used to reason over both of
them. Generalizing this notion to the set of hypothe-
ses in a beam, we build a state tree. The root of
the tree is the empty partial state (  ) that reasons
2A corner case arises for hypotheses with less than N ? 1
words. For these hypotheses, we still attempt state minimiza-
tion and, if successful, the state is treated normally. If state
minimization fails, a flag is set in the state. For purposes of the
state tree, the flag acts like a different terminator symbol.
960
(  )
(a  ) (a  Korea) (a a  Korea)
(a a  ` Korea)
(a a  in Korea) (a a  ` in Korea)
(some  ) (some  DPRK) (some a  DPRK) (some a  ` DPRK)
(the  ) (the  Korea)
(the a  Korea) (the a  ` Korea)
(the few  Korea) (the few  ` Korea) (the few a  ` Korea)
Figure 2: A state tree containing five states: (the few a  ` Korea), (the a  ` Korea), (some a  ` DPRK),
(a a  ` in Korea), and (a a  ` Korea). Nodes of the tree are partial states. The branching order is the first word,
the last word, the second word, and so on. If the left or right state is exhausted, then branching continues with the
remaining state. For purposes of branching, termination symbols a and ` act like normal words.
(  )
(a a  Korea)
(a a  ` Korea)
(a a  ` in Korea)
(some a  ` DPRK)
(the  Korea)
(the a  ` Korea)
(the few a  ` Korea)
Figure 3: The optimized version of Figure 2. Nodes
immediately reveal the longest shared prefix and suffix
among hypotheses below them.
over all hypotheses. From the root, the tree branches
by the first word of state, the last word, the second
word, the second-to-last word, and so on. If left or
right state is exhausted, then branching continues us-
ing the remaining state. The branching order priori-
tizes the outermost words because these can be used
to update the language model probability. The deci-
sion to start with left state is arbitrary. An example
tree is shown in Figure 2.
As an optimization, each node determines the
longest shared prefix and suffix of the hypotheses
below it. The node reports these words immedi-
ately, rendering some other nodes redundant. This
makes our algorithm faster because it will then only
encounter nodes when there is a branching decision
to be made. The original tree is shown in Figure 2
and the optimized version is shown in Figure 3. As
a side effect of branching by left state first, the al-
gorithm did not notice that states (the  Korea) and
(  )[1+]
(a a  Korea)
(a a  ` Korea)
(a a  ` in Korea)
(some a  ` DPRK)
(the  Korea)
(the a  ` Korea)
(the few a  ` Korea)
(the  Korea)[0+]
(the a  ` Korea)
(the few a  ` Korea)
Figure 4: Visiting the root node partitions the tree into
best child (the  Korea)[0+] and bread crumb (  )[1+].
The data structure remains intact for use elsewhere.
(a a  Korea) both end with Korea. We designed the
tree building algorithm for speed and plan to exper-
iment with alternatives as future work.
The state tree is built lazily. A node initially holds
a flat array of all the hypotheses below it. When its
children are first needed, the hypotheses are grouped
by the branching word and an array of child nodes
is built. In turn, these newly created children each
initially hold an array of hypotheses. CPU time is
saved because nodes containing low-scoring nodes
may never construct their children.
Each node has a score. For leaves, this score is
copied from the underlying hypothesis (or best hy-
pothesis if some other feature prevented recombina-
tion). The score of an internal node is the maximum
score of its children. As an example, the root node?s
score is the same as the highest-scoring hypothesis
in the tree. Children are sorted by score.
961
3.3 Bread Crumbs
The state tree is explored in a best-first manner.
Specifically, when the algorithm visits a node, it
considers that node?s best child. The best child re-
veals more words, so the score may go up or down
when the language model is consulted. Therefore,
simply following best children may lead to a poor
hypothesis. Some backtracking mechanism is re-
quired, for which we use bread crumbs. Visiting a
node results in two items: the best child and a bread
crumb. The bread crumb encodes the node that was
visited and how many children have already been
considered. Figure 4 shows an example.
More formally, each node has an array of chil-
dren sorted by score, so it suffices for the bread
crumb to keep an index in this array. An in-
dex of zero denotes that no child has been vis-
ited. Continuing the example from Figure 3,
(  )[0+] denotes the root partial state with chil-
dren starting at index 0 (i.e. all of them). Visit-
ing (  )[0+] yields best child (the  Korea)[0+]
and bread crumb (  )[1+]. Later, the search al-
gorithm may return to (  )[1+], yielding best
child (some a  ` DPRK)[0+] and bread crumb
(  )[2+]. If there is no remaining sibling, visit-
ing yields only the best child.
The index serves to restrict the array of children
to those with that index or above. Formally, let d
map from a node or bread crumb to the set of leaves
descended from it. The descendants of a node n are
those of its children
d(n) =
|n|?1?
i=0
d(n[i])
where unionsq takes the union of disjoint sets and n[i] is
the ith child. In a bread crumb with index c, only de-
scendents by the remaining children are considered
d(n[c+]) =
|n|?1?
i=c
d(n[i])
It follows that the set of descendants is partitioned
into two disjoint sets
d(n[c+]) = d(n[c])
?
d(n[c+ 1+])
3.4 Partial Edge
The beam filling algorithm is tasked with selecting
hypotheses given a number of hypergraph edges.
Hypergraph edges are strings comprised of words
and references to vertices (in parsing, terminals and
non-terminals). A hypergraph edge is converted to a
partial edge by replacing each vertex reference with
the root node from that vertex. For example, the hy-
pergraph edge ?is v .? referencing vertex v becomes
partial edge ?is (  )[0+] .?
Partial edges allow our algorithm to reason over
a large set of hypotheses at once. Visiting a
partial edge divides that set into two as follows.
A heuristic chooses one of the non-leaf nodes to
visit. Currently, this heuristic picks the node with
the fewest words revealed. As a tie breaker, it
chooses the leftmost node. The chosen node is
visited (partitioned), yielding the best child and
bread crumb as described in the previous section.
These are substituted into separate copies of the par-
tial edge. Continuing our example with the vertex
shown in Figure 3, ?is (  )[0+] .? partitions into
?is (the  Korea)[0+] .? and ?is (  )[1+] .?
3.5 Scoring
Every partial edge has a score that determines its
search priority. Initially, this score is the sum of the
edge?s score and the scores of each bread crumb (de-
fined below). As words are revealed, the score is
updated to account for new language model context.
Each edge score includes a log language model
probability and possibly additive features. When-
ever there is insufficient context to compute the lan-
guage model probability of a word, an estimate r is
used. For example, edge ?is v .? incorporates esti-
mate
log r(is)r(.)
into its score. The same applies to hypotheses:
(the few a  ` Korea) includes estimate
log r(the)r(few | the)
because the words in left state are those with insuf-
ficient context.
In common practice (Chiang, 2007; Hoang et al,
2009; Dyer et al, 2010), the estimate is taken from
the language model: r = p. However, querying
the language model with incomplete context leads
962
Kneser-Ney smoothing (Kneser and Ney, 1995) to
assume that backoff has occurred. An alternative is
to use average-case rest costs explicitly stored in the
language model (Heafield et al, 2012). Both options
are used in the experiments3.
The score of a bread crumb is the maximum score
of its descendants as defined in ?3.3. For example,
the bread crumb (  )[1+] has a lower score than
(  )[0+] because the best child (the  Korea)[0+]
and its descendants no longer contribute to the max-
imum.
The score of partial edge ?is (  )[0+] .? is
the sum of scores from its two parts: edge
?is v .? and bread crumb (  )[0+]. The
edge?s score includes estimated log probability
log r(is)r(.) as explained earlier. The bread crumb?s
score comes from its highest-scoring descendent
(the few a  ` Korea) and therefore includes esti-
mate log r(the)r(few | the).
Estimates are updated as words are revealed.
Continuing the example, ?is (  )[0+] .? has best
child ?is (the  Korea)[0+] .? In this best child, the
estimate r(.) is updated to r(. | Korea). Similarly,
r(the) is replaced with r(the | is). Updates exam-
ine only words that have been revealed: r(few | the)
remains unrevised.
Updates are computed efficiently by using point-
ers (Heafield et al, 2011) with KenLM. To summa-
rize, the language model computes
r(wn|w
n?1
1 )
r(wn|w
n?1
i )
in a single call. In the popular reverse trie data struc-
ture, the language model visits wni while retrieving
wn1 , so the cost is the same as a single query. More-
over, when the language model earlier provided es-
timate r(wn|w
n?1
i ), it also returned a data-structure
pointer t(wni ). Pointers are retained in hypotheses,
edges, and partial edges for each word with an esti-
mated probability. When context is revealed, our al-
gorithm queries the language model with new con-
text wi?11 and pointer t(w
n
i ). The language model
uses this pointer to immediately retrieve denomina-
tor r(wn|w
n?1
i ) and as a starting point to retrieve nu-
merator r(wn|w
n?1
1 ). It can therefore avoid looking
3We also tested upper bounds (Huang et al, 2012; Carter et
al., 2012) but the result is still approximate due to beam pruning
and initial experiments showed degraded performance.
up r(wn), r(wn|wn?1), . . . , r(wn|w
n?1
i+1 ) as would
normally be required with a reverse trie.
3.6 Priority Queue
Our beam filling algorithm is controlled by a priority
queue containing partial edges. The queue is popu-
lated by converting all outgoing hypergraph edges
into partial edges and pushing them onto the queue.
After this initialization, the algorithm loops. Each
iteration begins by popping the top-scoring partial
edge off the queue. If all nodes are leaves, then the
partial edge is converted to a hypothesis and placed
in the beam. Otherwise, the partial edge is parti-
tioned as described in ?3.3. The two resulting partial
edges are pushed onto the queue. Looping continues
with the next iteration until the queue is empty or the
beam is full. After the loop terminates, the beam is
given to the root node of the state tree; other nodes
will be built lazily as described in ?3.2.
Overall, the algorithm visits hypergraph vertices
in bottom-up order. Our beam filling algorithm runs
in each vertex, making use of state trees in vertices
below. The top of the tree contains full hypotheses.
If a K-best list is desired, packing and extraction
works the same way as with cube pruning.
4 Experiments
Performance is measured by translating the 3003-
sentence German-English test set from the 2011
Workshop on Machine Translation (Callison-Burch
et al, 2011). Two translation models were built, one
hierarchical (Chiang, 2007) and one with target syn-
tax. The target-syntax system is based on English
parses from the Collins (1999) parser. Both were
trained on Europarl (Koehn, 2005). The language
model interpolates models built on Europarl, news
commentary, and news data provided by the evalua-
tion. Interpolation weights were tuned on the 2010
test set. Language models were built with SRILM
(Stolcke, 2002), modified Kneser-Ney smoothing
(Kneser and Ney, 1995; Chen and Goodman, 1998),
default pruning, and order 5. Feature weights were
tuned with MERT (Och, 2003), beam size 1000,
100-best output, and cube pruning. Systems were
built with the Moses (Hoang et al, 2009) pipeline.
Measurements were collected by running the de-
coder on all 3003 sentences. For consistency, all
963
-101.6
-101.5
-101.4
0 1 2
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
This work
Additive cube pruning
Cube pruning
Figure 5: Hierarchial system in Moses with our algo-
rithm, cube pruning with additive scores, and cube prun-
ing with full scores (?2.3). The two baselines overlap.
relevant files were forced into the operating system
disk cache before each run. CPU time is the to-
tal user and system time taken by the decoder mi-
nus loading time. Loading time was measured by
running the decoder with empty input. In partic-
ular, CPU time includes the cost of parsing. Our
test system has 32 cores and 64 GB of RAM; no
run came close to running out of memory. While
multi-threaded experiments showed improvements
as well, we only report single-threaded results to re-
duce noise and to compare with cdec (Dyer et al,
2010). Decoders were compiled with the optimiza-
tion settings suggested in their documentation.
Search accuracy is measured by average model
score; higher is better. Only relative comparisons
are meaningful because model scores have arbitrary
scale and include constant factors. Beam sizes start
at 5 and rise until a time limit determined by running
the slowest algorithm with beam size 1000.
4.1 Comparison Inside Moses
Figure 5 shows Moses performance with this work
and with cube pruning. These results used the hi-
erarchical system with common-practice estimates
(?3.5). The two cube pruning variants are explained
in ?2.3. Briefly, the queue can be prioritized using
-101.6
-101.5
-101.4
0 1 2
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
This work
Gesmundo 1
Gesmundo 2
Cube pruning
Figure 6: Hierarchical system in cdec with our algorithm,
similarly-performing variants of cube pruning defined in
Gesmundo and Henderson (2010), and the default.
additive or full scores. Performance with additive
scores is roughly the same as using full scores with
half the beam size.
Our algorithm is faster for every beam size tested.
It is also more accurate than additive cube pruning
with the same beam size. However, when compared
with full scores cube pruning, it is less accurate for
beam sizes below 300. This makes sense because
our algorithm starts with additive estimates and iter-
atively refines them by calling the language model.
Moreover, when beams are small, there are fewer
chances to group hypotheses. With beams larger
than 300, our algorithm can group more hypotheses,
overtaking both forms of cube pruning.
Accuracy improvements can be interpreted as
speed improvements by asking how much time each
algorithm takes to achieve a set level of accuracy.
By this metric, our algorithm is 2.04 to 3.37 times as
fast as both baselines.
4.2 Comparison Inside cdec
We also implemented our algorithm in cdec (Dyer
et al, 2010). Figure 6 compares with two enhanced
versions of cube pruning (Gesmundo and Hender-
son, 2010) and the cdec baseline. The model scores
964
-101.6
-101.5
-101.4
0 1 2
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Rest+This work
This work
Rest+Cube pruning
Cube pruning
21.4
21.6
21.8
22
0 1 2
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Rest+This work
This work
Rest+Cube pruning
Cube pruning
Figure 7: Effect of rest costs on our algorithm and on cube pruning in Moses. Noisy BLEU scores reflect model errors.
are comparable with Moses4.
Measuring at equal accuracy, our algorithm
makes cdec 1.56 to 2.24 times as fast as the best
baseline. At first, this seems to suggest that cdec is
faster. In fact, the opposite is true: comparing Fig-
ures 5 and 6 reveals that cdec has a higher parsing
cost than Moses5, thereby biasing the speed ratio to-
wards 1. In subsequent experiments, we use Moses
because it more accurately reflects search costs.
4.3 Average-Case Rest Costs
Previous experiments used the common-practice
probability estimate described in ?3.5. Figure 7
shows the impact of average-case rest costs on our
algorithm and on cube pruning in Moses. We also
looked at uncased BLEU (Papineni et al, 2002)
scores, finding that our algorithm attains near-peak
BLEU in less time. The relationship between model
score and BLEU is noisy due to model errors.
4The glue rule builds hypotheses left-to-right. In Moses,
glued hypotheses start with <s> and thus have empty left state.
In cdec, sentence boundary tokens are normally added last, so
intermediate hypotheses have spurious left state. Running cdec
with the Moses glue rule led to improved time-accuracy perfor-
mance. The improved version is used in all results reported. We
accounted for constant-factor differences in feature definition
i.e. whether <s> is part of the word count.
5In-memory phrase tables were used with both decoders.
The on-disk phrase table makes Moses slower than cdec.
Average-case rest costs impact our algorithm
more than they impact cube pruning. For small beam
sizes, our algorithm becomes more accurate, mostly
eliminating the disadvantage reported in ?4.1. Com-
pared to the common-practice estimate with beam
size 1000, rest costs made our algorithm 1.62 times
as fast and cube pruning 1.22 times as fast.
Table 1 compares our best result with the best
baseline: our algorithm and cube pruning, both with
rest costs inside Moses. In this scenario, our algo-
rithm is 2.59 to 3.51 times as fast as cube pruning.
4.4 Target-Syntax
We took the best baseline and best result from previ-
ous experiments (Moses with rest costs) and ran the
target-syntax system. Results are shown in Figure
8. Parsing and search are far more expensive. For
beam size 5, our algorithm attains equivalent accu-
racy 1.16 times as fast. Above 5, our algorithm is
1.50 to 2.00 times as fast as cube pruning. More-
over, our algorithm took less time with beam size
6900 than cube pruning took with beam size 1000.
A small bump in model score occurs around 15
seconds. This is due to translating ?durchzoge-
nen? as ?criss-crossed? instead of passing it through,
which incurs a severe penalty (-100). The only rule
capable of doing so translates ?X durchzogenen? as
?criss-crossed PP?; a direct translation rule was not
965
-105
-104.8
-104.6
-104.4
-104.2
0 10 20
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Rest+This work
Rest+Cube pruning
21
21.2
21.4
0 10 20
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Rest+This work
Rest+Cube pruning
Figure 8: Performance of Moses with the target-syntax system.
extracted due to reordering. An appropriate prepo-
sitional phrase (PP) was pruned with smaller beam
sizes because it is disfluent.
4.5 Memory
Peak virtual memory usage was measured before
each process terminated. Compared with cube prun-
ing at a beam size of 1000, our algorithm uses 160
MB more RAM in Moses and 298 MB less RAM in
cdec. The differences are smaller with lower beam
sizes and minor relative to 12-13 GB total size, most
of which is the phrase table and language model.
Rest+This work Rest+Cube pruning
k CPU Model BLEU CPU Model BLEU
5 0.068 -1.698 21.59 0.243 -1.667 21.75
10 0.076 -1.593 21.89 0.255 -1.592 21.97
50 0.125 -1.463 22.07 0.353 -1.480 22.04
75 0.157 -1.446 22.06 0.408 -1.462 22.05
100 0.176 -1.436 22.03 0.496 -1.451 22.05
500 0.589 -1.408 22.00 1.356 -1.415 22.00
750 0.861 -1.405 21.96 1.937 -1.409 21.98
1000 1.099 -1.403 21.97 2.502 -1.407 21.98
Table 1: Numerical results from the hierarchical system
for select beam sizes k comparing our best result with the
best baseline, both in Moses with rest costs enabled. To
conserve space, model scores are shown with 100 added.
5 Conclusion
We have described a new search algorithm that
achieves equivalent accuracy 1.16 to 3.51 times as
fast as cube pruning, including two implementations
and four variants. The algorithm is based on group-
ing similar language model feature states together
and dynamically expanding these groups. In do-
ing so, it exploits the language model?s ability to
estimate with incomplete information. Our imple-
mentation is available under the LGPL as a stand-
alone from http://kheafield.com/code/
and distributed with Moses and cdec.
Acknowledgements
This research work was supported in part by the Na-
tional Science Foundation under grant IIS-0713402,
by a NPRP grant (NPRP 09-1140-1-177) from the
Qatar National Research Fund (a member of the
Qatar Foundation), and by computing resources pro-
vided by the NSF-sponsored XSEDE program under
grant TG-CCR110017. The statements made herein
are solely the responsibility of the authors. The re-
search leading to these results has received funding
from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreements
287576 (CASMACAT), 287658 (EU BRIDGE),
287688 (MateCat), and 288769 (ACCEPT).
966
References
Yehoshua Bar-Hillel, Micha Perles, and Eli Shamir.
1964. On Formal Properties of Simple Phrase Struc-
ture Grammars. Hebrew University Students? Press.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Language
Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact sampling and decoding in
high-order hidden Markov models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125?1134, Jeju
Island, Korea, July.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and Kneser-Ney smoothing. In Proceedings of In-
terspeech, pages 2242?2245.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270, Ann Arbor, Michi-
gan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228, June.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ?10, pages 7?12.
Andrea Gesmundo and James Henderson. 2010. Faster
cube pruning. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT),
pages 267?274.
Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. A
formal basis for the heuristic determination of mini-
mum cost paths. IEEE Transactions on Systems Sci-
ence and Cybernetics, 4(2):100?107, July.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, San Francisco, CA, USA, De-
cember.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, Jeju Is-
land, Korea.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 152?159, Tokyo, Japan.
Mark Hopkins and Greg Langmead. 2009. Cube pruning
as heuristic search. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 62?71, Singapore, August.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273?283, Cambridge,
MA, October.
Zhiheng Huang, Yi Chang, Bo Long, Jean-Francois Cre-
spo, Anlei Dong, Sathiya Keerthi, and Su-Lin Wu.
2012. Iterative Viterbi A* algorithm for k-best se-
quential decoding. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1125?1134, Jeju Island, Korea,
July.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchical
phrase-based translation representations. In Proceed-
ings of the 2011 Conference on Empirical Methods in
967
Natural Language Processing, pages 1373?1383, Ed-
inburgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the Seventh Inter-
national Workshop on Parsing Technologies, Beijing,
China, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second ACL Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
10?18, Columbus, Ohio, June.
Bruce Lowerre. 1976. The Harpy Speech Recognition
System. Ph.D. thesis, Carnegie Mellon University.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June.
Robert C. Moore and Chris Quirk. 2009. Less is more:
Significance-based n-gram selection for smaller, better
language models. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 746?755, August.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 108?116, Honolulu, HI, USA, October.
Alexander Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 72?82, Portland, Oregon, USA,
June.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Xiang Tong and David A. Evans. 1996. A statistical
approach to automatic OCR error correction in con-
text. In Proceedings of the Fourth Workshop on Very
Large Corpora, pages 88?100, Copenhagen, Den-
mark, April.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages 777?
784, Sydney, Australia, July.
Wenduan Xu and Philipp Koehn. 2012. Extending hiero
decoding in Moses with cube growing. The Prague
Bulletin of Mathematical Linguistics, 98:133?142.
968
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Scalable Modified Kneser-Ney Language Model Estimation
Kenneth Heafield?,? Ivan Pouzyrevsky? Jonathan H. Clark? Philipp Koehn?
?University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
?Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
?Yandex
Zelenograd, bld. 455 fl. 128
Moscow 124498, Russia
heafield@cs.cmu.edu ivan.pouzyrevsky@gmail.com jhclark@cs.cmu.edu pkoehn@inf.ed.ac.uk
Abstract
We present an efficient algorithm to es-
timate large modified Kneser-Ney mod-
els including interpolation. Streaming
and sorting enables the algorithm to scale
to much larger models by using a fixed
amount of RAM and variable amount of
disk. Using one machine with 140 GB
RAM for 2.8 days, we built an unpruned
model on 126 billion tokens. Machine
translation experiments with this model
show improvement of 0.8 BLEU point
over constrained systems for the 2013
Workshop on Machine Translation task in
three language pairs. Our algorithm is also
faster for small models: we estimated a
model on 302 million tokens using 7.7%
of the RAM and 14.0% of the wall time
taken by SRILM. The code is open source
as part of KenLM.
1 Introduction
Relatively low perplexity has made modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998) a popular choice for
language modeling. However, existing estima-
tion methods require either large amounts of RAM
(Stolcke, 2002) or machines (Brants et al, 2007).
As a result, practitioners have chosen to use
less data (Callison-Burch et al, 2012) or simpler
smoothing methods (Brants et al, 2007).
Backoff-smoothed n-gram language models
(Katz, 1987) assign probability to a word wn in
context wn?11 according to the recursive equation
p(wn|wn?11 ) =
{
p(wn|wn?11 ), if wn1 was seen
b(wn?11 )p(wn|wn2 ), otherwise
The task is to estimate probability p and backoff
b from text for each seen entry wn1 . This paper
Filesystem
Map
Reduce 1
Filesystem
Identity Map
Reduce 2
Filesystem ...
MapReduce Steps
Filesystem
Map
Reduce 1
Reduce 2...
Optimized
Figure 1: Each MapReduce performs three copies
over the network when only one is required. Ar-
rows denote copies over the network (i.e. to and
from a distributed filesystem). Both options use
local disk within each reducer for merge sort.
contributes an efficient multi-pass streaming algo-
rithm using disk and a user-specified amount of
RAM.
2 Related Work
Brants et al (2007) showed how to estimate
Kneser-Ney models with a series of five MapRe-
duces (Dean and Ghemawat, 2004). On 31 billion
words, estimation took 400 machines for two days.
Recently, Google estimated a pruned Kneser-Ney
model on 230 billion words (Chelba and Schalk-
wyk, 2013), though no cost was provided.
Each MapReduce consists of one layer of map-
pers and an optional layer of reducers. Mappers
read from a network filesystem, perform optional
processing, and route data to reducers. Reducers
process input and write to a network filesystem.
Ideally, reducers would send data directly to an-
other layer of reducers, but this is not supported.
Their workaround, a series of MapReduces, per-
forms unnecessary copies over the network (Fig-
ure 1). In both cases, reducers use local disk.
690
Writing and reading from the distributed filesys-
tem improves fault tolerance. However, the same
level of fault tolerance could be achieved by
checkpointing to the network filesystem then only
reading in the case of failures. Doing so would en-
able reducers to start processing without waiting
for the network filesystem to write all the data.
Our code currently runs on a single machine
while MapReduce targets clusters. Appuswamy et
al. (2013) identify several problems with the scale-
out approach of distributed computation and put
forward several scenarios in which a single ma-
chine scale-up approach is more cost effective in
terms of both raw performance and performance
per dollar.
Brants et al (2007) contributed Stupid Backoff,
a simpler form of smoothing calculated at runtime
from counts. With Stupid Backoff, they scaled to
1.8 trillion tokens. We agree that Stupid Backoff
is cheaper to estimate, but contend that this work
makes Kneser-Ney smoothing cheap enough.
Another advantage of Stupid Backoff has been
that it stores one value, a count, per n-gram in-
stead of probability and backoff. In previous work
(Heafield et al, 2012), we showed how to collapse
probability and backoff into a single value without
changing sentence-level probabilities. However,
local scores do change and, like Stupid Backoff,
are no longer probabilities.
MSRLM (Nguyen et al, 2007) aims to scal-
ably estimate language models on a single ma-
chine. Counting is performed with streaming algo-
rithms similarly to this work. Their parallel merge
sort also has the potential to be faster than ours.
The biggest difference is that their pipeline de-
lays some computation (part of normalization and
all of interpolation) until query time. This means
that it cannot produce a standard ARPA file and
that more time and memory are required at query
time. Moreover, they use memory mapping on en-
tire files and these files may be larger than physi-
cal RAM. We have found that, even with mostly-
sequential access, memory mapping is slower be-
cause the kernel does not explicitly know where
to read ahead or write behind. In contrast, we use
dedicated threads for reading and writing. Perfor-
mance comparisons are omitted because we were
unable to compile and run MSRLM on recent ver-
sions of Linux.
SRILM (Stolcke, 2002) estimates modified
Kneser-Ney models by storing n-grams in RAM.
Corpus
Counting
Adjusting Counts
DivisionSumming
Interpolation
Model
Figure 2: Data flow in the estimation pipeline.
Normalization has two threads per order: sum-
ming and division. Thick arrows indicate sorting.
It also offers a disk-based pipeline for initial steps
(i.e. counting). However, the later steps store
all n-grams that survived count pruning in RAM.
Without pruning, both options use the same RAM.
IRSTLM (Federico et al, 2008) does not imple-
ment modified Kneser-Ney but rather an approxi-
mation dubbed ?improved Kneser-Ney? (or ?mod-
ified shift-beta? depending on the version). Esti-
mation is done in RAM. It can also split the corpus
into pieces and separately build each piece, intro-
ducing further approximation.
3 Estimation Pipeline
Estimation has four streaming passes: counting,
adjusting counts, normalization, and interpolation.
Data is sorted between passes, three times in total.
Figure 2 shows the flow of data.
3.1 Counting
For a language model of order N , this step counts
all N -grams (with length exactly N ) by streaming
through the corpus. Words near the beginning of
sentence also formN -grams padded by the marker
<s> (possibly repeated multiple times). The end
of sentence marker </s> is appended to each sen-
tence and acts like a normal token.
Unpruned N -gram counts are sufficient, so
lower-order n-grams (n < N ) are not counted.
Even pruned models require unpruned N -gram
counts to compute smoothing statistics.
Vocabulary mapping is done with a hash table.1
Token strings are written to disk and a 64-bit Mur-
1This hash table is the only part of the pipeline that can
grow. Users can specify an estimated vocabulary size for
memory budgeting. In future work, we plan to support lo-
cal vocabularies with renumbering.
691
Suffix
3 2 1
Z B A
Z A B
B B B
Context
2 1 3
Z A B
B B B
Z B A
Figure 3: In suffix order, the last word is primary.
In context order, the penultimate word is primary.
murHash2 token identifier is retained in RAM.
Counts are combined in a hash table and spilled
to disk when a fixed amount of memory is full.
Merge sort also combines identical N -grams (Bit-
ton and DeWitt, 1983).
3.2 Adjusting Counts
The counts c are replaced with adjusted counts a.
a(wn1 ) =
{
c(wn1 ), if n = N or w1 = <s>
|v : c(vwn1 ) > 0|, otherwise
Adjusted counts are computed by streaming
through N -grams sorted in suffix order (Figure 3).
The algorithm keeps a running total a(wNi ) for
each i and compares consecutive N -grams to de-
cide which adjusted counts to output or increment.
Smoothing statistics are also collected. For each
length n, it collects the number tn,k of n-grams
with adjusted count k ? [1, 4].
tn,k = |{wn1 : a(wn1 ) = k}|
These are used to compute closed-form estimates
(Chen and Goodman, 1998) of discounts Dn(k)
Dn(k) = k ?
(k + 1)tn,1tn,k+1
(tn,1 + 2tn,2)tn,k
for k ? [1, 3]. Other cases are Dn(0) = 0 and
Dn(k) = Dn(3) for k ? 3. Less formally, counts
0 (unknown) through 2 have special discounts.
3.3 Normalization
Normalization computes pseudo probability u
u(wn|wn?11 ) =
a(wn1 )?Dn(a(wn1 ))?
x a(wn?11 x)
and backoff b
b(wn?11 ) =
?3
i=1Dn(i)|{x : a(wn?11 x) = i}|?
x a(wn?11 x)
2https://code.google.com/p/smhasher/
The difficulty lies in computing denominator?
x a(wn?11 x) for all wn?11 . For this, we sort in
context order (Figure 3) so that, for every wn?11 ,
the entries wn?11 x are consecutive. One pass col-
lects both the denominator and backoff3 terms
|{x : a(wn?11 x) = i}| for i ? [1, 3].
A problem arises in that denominator?
x a(wn?11 x) is known only after streaming
through all wn?11 x, but is needed immediately
to compute each u(wn|wn?11 ). One option is to
buffer in memory, taking O(N |vocabulary|) space
since each order is run independently in parallel.
Instead, we use two threads for each order. The
sum thread reads ahead to compute?x a(wn?11 x)
and b(wn?11 ) then places these in a secondary
stream. The divide thread reads the input and the
secondary stream then writes records of the form
(wn1 , u(wn|wn?11 ), b(wn?11 )) (1)
The secondary stream is short so that data read by
the sum thread will likely be cached when read by
the divide thread. This sort of optimization is not
possible with most MapReduce implementations.
Because normalization streams through wn?11 x
in context order, the backoffs b(wn?11 ) are com-
puted in suffix order. This will be useful later
(?3.5), so backoffs are written to secondary files
(one for each order) as bare values without keys.
3.4 Interpolation
Chen and Goodman (1998) found that perplex-
ity improves when the various orders within the
same model are interpolated. The interpolation
step computes final probability p according to the
recursive equation
p(wn|wn?11 ) = u(wn|wn?11 )+b(wn?11 )p(wn|wn?12 )
(2)
Recursion terminates when unigrams are interpo-
lated with the uniform distribution
p(wn) = u(wn) + b()
1
|vocabulary|
where  denotes the empty string. The unknown
word counts as part of the vocabulary and has
count zero,4 so its probability is b()/|vocabulary|.
3Sums and counts are done with exact integer arithmetic.
Thus, every floating-point value generated by our toolkit is
the result of O(N) floating-point operations. SRILM has nu-
merical precision issues because it uses O(N |vocabulary|)
floating-point operations to compute backoff.
4SRILM implements ?another hack? that computes
pSRILM(wn) = u(wn) and pSRILM(<unk>) = b() when-
ever p(<unk>) < 3? 10?6, as it usually is. We implement
both and suspect their motivation was numerical precision.
692
Probabilities are computed by streaming in suf-
fix lexicographic order: wn appears before wnn?1,
which in turn appears before wnn?2. In this way,
p(wn) is computed before it is needed to compute
p(wn|wn?1), and so on. This is implemented by
jointly iterating through N streams, one for each
length of n-gram. The relevant pseudo probability
u(wn|wn?11 ) and backoff b(wn?11 ) appear in the
input records (Equation 1).
3.5 Joining
The last task is to unite b(wn1 ) computed in ?3.3
with p(wn|wn?11 ) computed in ?3.4 for storage in
the model. We note that interpolation (Equation 2)
used the different backoff b(wn?11 ) and so b(wn1 )
is not immediately available. However, the back-
off values were saved in suffix order (?3.3) and in-
terpolation produces probabilities in suffix order.
During the same streaming pass as interpolation,
we merge the two streams.5 Suffix order is also
convenient because the popular reverse trie data
structure can be built in the same pass.6
4 Sorting
Much work has been done on efficient disk-based
merge sort. Particularly important is arity, the
number of blocks that are merged at once. Low
arity leads to more passes while high arity in-
curs more disk seeks. Abello and Vitter (1999)
modeled these costs and derived an optimal strat-
egy: use fixed-size read buffers (one for each
block being merged) and set arity to the number of
buffers that fit in RAM. The optimal buffer size is
hardware-dependent; we use 64 MB by default. To
overcome the operating system limit on file han-
dles, multiple blocks are stored in the same file.
To further reduce the costs of merge sort, we
implemented pipelining (Dementiev et al, 2008).
If there is enough RAM, input is lazily merged
and streamed to the algorithm. Output is cut into
blocks, sorted in the next step?s desired order, and
then written to disk. These optimizations elim-
inate up to two copies to disk if enough RAM
is available. Input, the algorithm, block sorting,
and output are all threads on a chain of producer-
consumer queues. Therefore, computation and
disk operations happen simultaneously.
5Backoffs only exist if the n-gram is the context of some
n+ 1-gram, so merging skips n-grams that are not contexts.
6With quantization (Whittaker and Raj, 2001), the quan-
tizer is trained in a first pass and applied in a second pass.
0
10
20
30
40
50
0 200 400 600 800 1000
RA
M
(G
B)
Tokens (millions)
SRI
SRI compact
IRST
This work
Figure 4: Peak virtual memory usage.
0
2
4
6
8
10
12
14
0 200 400 600 800 1000
CP
U
tim
e(
ho
urs
)
Tokens (millions)
SRI
SRI compact
IRST
This work
Figure 5: CPU usage (system plus user).
Each n-gram record is an array of n vocabu-
lary identifiers (4 bytes each) and an 8-byte count
or probability and backoff. At peak, records are
stored twice on disk because lazy merge sort is
not easily amenable to overwriting the input file.
Additional costs are the secondary backoff file (4
bytes per backoff) and the vocabulary in plaintext.
5 Experiments
Experiments use ClueWeb09.7 After spam filter-
ing (Cormack et al, 2011), removing markup, se-
lecting English, splitting sentences (Koehn, 2005),
deduplicating, tokenizing (Koehn et al, 2007),
and truecasing, 126 billion tokens remained.
7http://lemurproject.org/clueweb09/
693
1 2 3 4 5
393 3,775 17,629 39,919 59,794
Table 1: Counts of unique n-grams (in millions)
for the 5 orders in the large LM.
5.1 Estimation Comparison
We estimated unpruned language models in bi-
nary format on sentences randomly sampled from
ClueWeb09. SRILM and IRSTLM were run un-
til the test machine ran out of RAM (64 GB).
For our code, the memory limit was set to 3.5
GB because larger limits did not improve perfor-
mance on this small data. Results are in Figures
4 and 5. Our code used an average of 1.34?1.87
CPUs, so wall time is better than suggested in Fig-
ure 5 despite using disk. Other toolkits are single-
threaded. SRILM?s partial disk pipeline is not
shown; it used the same RAM and took more time.
IRSTLM?s splitting approximation took 2.5 times
as much CPU and about one-third the memory (for
a 3-way split) compared with normal IRSTLM.
For 302 million tokens, our toolkit used 25.4%
of SRILM?s CPU time, 14.0% of the wall time,
and 7.7% of the RAM. Compared with IRSTLM,
our toolkit used 16.4% of the CPU time, 9.0% of
the wall time, and 16.6% of the RAM.
5.2 Scaling
We built an unpruned model (Table 1) on 126 bil-
lion tokens. Estimation used a machine with 140
GB RAM and six hard drives in a RAID5 configu-
ration (sustained read: 405 MB/s). It took 123 GB
RAM, 2.8 days wall time, and 5.4 CPU days. A
summary of Google?s results from 2007 on differ-
ent data and hardware appears in ?2.
We then used this language model as an ad-
ditional feature in unconstrained Czech-English,
French-English, and Spanish-English submissions
to the 2013 Workshop on Machine Translation.8
Our baseline is the University of Edinburgh?s
phrase-based Moses (Koehn et al, 2007) submis-
sion (Durrani et al, 2013), which used all con-
strained data specified by the evaluation (7 billion
tokens of English). It placed first by BLEU (Pap-
ineni et al, 2002) among constrained submissions
in each language pair we consider.
In order to translate, the large model was quan-
tized (Whittaker and Raj, 2001) to 10 bits and
compressed to 643 GB with KenLM (Heafield,
8http://statmt.org/wmt13/
Source Baseline Large
Czech 27.4 28.2
French 32.6 33.4
Spanish 31.8 32.6
Table 2: Uncased BLEU results from the 2013
Workshop on Machine Translation.
2011) then copied to a machine with 1 TB RAM.
Better compression methods (Guthrie and Hepple,
2010; Talbot and Osborne, 2007) and distributed
language models (Brants et al, 2007) could reduce
hardware requirements. Feature weights were re-
tuned with PRO (Hopkins and May, 2011) for
Czech-English and batch MIRA (Cherry and Fos-
ter, 2012) for French-English and Spanish-English
because these worked best for the baseline. Un-
cased BLEU scores on the 2013 test set are shown
in Table 2. The improvement is remarkably con-
sistent at 0.8 BLEU point in each language pair.
6 Conclusion
Our open-source (LGPL) estimation code is avail-
able from kheafield.com/code/kenlm/
and should prove useful to the community. Sort-
ing makes it scalable; efficient merge sort makes
it fast. In future work, we plan to extend to the
Common Crawl corpus and improve parallelism.
Acknowledgements
Miles Osborne preprocessed ClueWeb09. Mo-
hammed Mediani contributed to early designs.
Jianfeng Gao clarified how MSRLM operates.
This work used the Extreme Science and Engi-
neering Discovery Environment (XSEDE), which
is supported by National Science Foundation grant
number OCI-1053575. We used Stampede and
Trestles under allocation TG-CCR110017. Sys-
tem administrators from the Texas Advanced
Computing Center (TACC) at The University of
Texas at Austin made configuration changes on
our request. This work made use of the resources
provided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT ini-
tiative (http://www.edikt.org.uk/). The
research leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment 287658 (EU BRIDGE).
694
References
James M. Abello and Jeffrey Scott Vitter, editors.
1999. External memory algorithms. American
Mathematical Society, Boston, MA, USA.
Raja Appuswamy, Christos Gkantsidis, Dushyanth
Narayanan, Orion Hodson, and Antony Rowstron.
2013. Nobody ever got fired for buying a cluster.
Technical Report MSR-TR-2013-2, Microsoft Re-
search.
Dina Bitton and David J DeWitt. 1983. Duplicate
record elimination in large data files. ACM Trans-
actions on database systems (TODS), 8(2):255?265.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Ciprian Chelba and Johan Schalkwyk, 2013. Em-
pirical Exploration of Language Modeling for the
google.com Query Stream as Applied to Mobile
Voice Search, pages 197?229. Springer, New York.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, August.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Gordon V Cormack, Mark D Smucker, and Charles LA
Clarke. 2011. Efficient and effective spam filtering
and re-ranking for large web datasets. Information
retrieval, 14(5):441?465.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In OSDI?04: Sixth Symposium on Operating Sys-
tem Design and Implementation, San Francisco, CA,
USA, 12.
Roman Dementiev, Lutz Kettner, and Peter Sanders.
2008. STXXL: standard template library for XXL
data sets. Software: Practice and Experience,
38(6):589?637.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the ACL 2013 Eighth Workshop on Sta-
tistical Machine Translation, Sofia, Bulgaria, Au-
gust.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the
web in memory: Space efficient language mod-
els with constant time retrieval. In Proceedings of
EMNLP 2010, Los Angeles, CA.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-efficient
storage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, July.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, ASSP-35(3):400?
401, March.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181?184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.
2007. MSRLM: a scalable language modeling
toolkit. Technical Report MSR-TR-2007-144, Mi-
crosoft Research.
695
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine trans-
lation. In Proceedings of ACL, pages 512?519,
Prague, Czech Republic.
Edward Whittaker and Bhiksha Raj. 2001.
Quantization-based language model compres-
sion. In Proceedings of Eurospeech, pages 33?36,
Aalborg, Denmark, December.
696
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130?135,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Faster Phrase-Based Decoding by Refining Feature State
Kenneth Heafield Michael Kayser Christopher D. Manning
Computer Science Department Stanford University, Stanford, CA, 94305
{heafield,mkayser,manning}@stanford.edu
Abstract
We contribute a faster decoding algo-
rithm for phrase-based machine transla-
tion. Translation hypotheses keep track
of state, such as context for the language
model and coverage of words in the source
sentence. Most features depend upon only
part of the state, but traditional algorithms,
including cube pruning, handle state atom-
ically. For example, cube pruning will re-
peatedly query the language model with
hypotheses that differ only in source cov-
erage, despite the fact that source cover-
age is irrelevant to the language model.
Our key contribution avoids this behav-
ior by placing hypotheses into equivalence
classes, masking the parts of state that
matter least to the score. Moreover, we ex-
ploit shared words in hypotheses to itera-
tively refine language model scores rather
than handling language model state atom-
ically. Since our algorithm and cube prun-
ing are both approximate, improvement
can be used to increase speed or accuracy.
When tuned to attain the same accuracy,
our algorithm is 4.0?7.7 times as fast as
the Moses decoder with cube pruning.
1 Introduction
Translation speed is critical to making suggestions
as translators type, mining for parallel data by
translating the web, and running on mobile de-
vices without Internet connectivity. We contribute
a fast decoding algorithm for phrase-based ma-
chine translation along with an implementation in
a new open-source (LGPL) decoder available at
http://kheafield.com/code/.
Phrase-based decoders (Koehn et al, 2007; Cer
et al, 2010; Wuebker et al, 2012) keep track
of several types of state with translation hypothe-
ses: coverage of the source sentence thus far, con-
text for the language model, the last position for
the distortion model, and anything else features
need. Existing decoders handle state atomically:
hypotheses that have exactly the same state can be
recombined and efficiently handled via dynamic
programming, but there is no special handling for
partial agreement. Therefore, features are repeat-
edly consulted regarding hypotheses that differ
only in ways irrelevant to their score, such as cov-
erage of the source sentence. Our decoder bun-
dles hypotheses into equivalence classes so that
features can focus on the relevant parts of state.
We pay particular attention to the language
model because it is responsible for much of the hy-
pothesis state. As the decoder builds translations
from left to right (Koehn, 2004), it records the last
N ? 1 words of each hypothesis so that they can
be used as context to score the first N ? 1 words
of a phrase, where N is the order of the language
model. Traditional decoders (Huang and Chiang,
2007) try thousands of combinations of hypothe-
ses and phrases, hoping to find ones that the lan-
guage model likes. Our algorithm instead discov-
ers good combinations in a coarse-to-fine manner.
The algorithm exploits the fact that hypotheses of-
ten share the same suffix and phrases often share
the same prefix. These shared suffixes and prefixes
allow the algorithm to coarsely reason over many
combinations at once.
Our primary contribution is a new search algo-
rithm that exploits the above observations, namely
that state can be divided into pieces relevant to
each feature and that language model state can be
further subdivided. The primary claim is that our
algorithm is faster and more accurate than the pop-
ular cube pruning algorithm.
2 Related Work
Our previous work (Heafield et al, 2013) devel-
oped language model state refinement for bottom-
130
up decoding in syntatic machine translation. In
bottom-up decoding, hypotheses can be extended
to the left or right, so hypotheses keep track of
both their prefix and suffix. The present phrase-
based setting is simpler because sentences are
constructed from left to right, so prefix infor-
mation is unnecessary. However, phrase-based
translation implements reordering by allowing hy-
potheses that translate discontiguous words in the
source sentence. There are exponentially many
ways to cover the source sentence and hypotheses
carry this information as additional state. A main
contribution in this paper is efficiently ignoring
coverage when evaluating the language model. In
contrast, syntactic machine translation hypotheses
correspond to contiguous spans in the source sen-
tence, so in prior work we simply ran the search
algorithm in every span.
Another improvement upon Heafield et al
(2013) is that we previously made no effort to
exploit common words that appear in translation
rules, which are analogous to phrases. In this
work, we explicitly group target phrases by com-
mon prefixes, doing so directly in the phrase table.
Coarse-to-fine approaches (Petrov et al, 2008;
Zhang and Gildea, 2008) invoke the decoder
multiple times with increasingly detailed models,
pruning after each pass. The key difference in our
work is that, rather than refining models in lock
step, we effectively refine the language model on
demand for hypotheses that score well. More-
over, their work was performed in syntactic ma-
chine translation while we address issues specific
to phrase-based translation.
Our baseline is cube pruning (Chiang, 2007;
Huang and Chiang, 2007), which is both a way
to organize search and an algorithm to search
through cross products of sets. We adopt the same
search organization (Section 3.1) but change how
cross products are searched.
Chang and Collins (2011) developed an exact
decoding algorithm based on Lagrangian relax-
ation. However, it has not been shown to tractably
scale to 5-gram language models used by many
modern translation systems.
3 Decoding
We begin by summarizing the high-level organiza-
tion of phrase-based cube pruning (Koehn, 2004;
Koehn et al, 2007; Huang and Chiang, 2007).
Sections 3.2 and later show our contribution.
0 word 1 word
the
cat
.
2 words
cat
the cat
cat the
. the
3 words
cat .
the cat .
cat the .
. the cat
Figure 1: Stacks to translate the French ?le chat .?
into English. Filled circles indicate that the source
word has been translated. A phrase translates ?le
chat? as simply ?cat?, emphasizing that stacks are
organized by the number of source words rather
than the number of target words.
3.1 Search Organization
Phrase-based decoders construct hypotheses from
left to right by appending phrases in the target lan-
guage. The decoder organizes this search process
using stacks (Figure 1). Stacks contain hypothe-
ses that have translated the same number of source
words. The zeroth stack contains one hypothe-
sis with nothing translated. Subsequent stacks are
built by extending hypotheses in preceding stacks.
For example, the second stack contains hypothe-
ses that translated two source words either sepa-
rately or as a phrasal unit. Returning to Figure 1,
the decoder can apply a phrase pair to translate ?le
chat? as ?cat? or it can derive ?the cat? by translat-
ing one word at a time; both appear in the second
stack because they translate two source words. To
generalize, the decoder populates the ith stack by
pairing hypotheses in the i ? jth stack with tar-
get phrases that translate source phrases of length
j. Hypotheses remember which source word they
translated, as indicated by the filled circles.
The reordering limit prevents hypotheses from
jumping around the source sentence too much and
dramatically reduces the search space. Formally,
the decoder cannot propose translations that would
require jumping back more than R words in the
source sentence, including multiple small jumps.
In practice, stacks are limited to k hypothe-
ses, where k is set by the user. Small k is faster
but may prune good hypotheses, while large k is
slower but more thorough, thereby comprising a
time-accuracy trade-off. The central question in
this paper is how to select these k hypotheses.
Populating a stack boils down to two steps.
First, the decoder matches hypotheses with source
phrases subject to three constraints: the total
source length matches the stack being populated,
none of the source words has already been trans-
131
country
a
nations
few
countries
Figure 2: Hypothesis suffixes arranged into a trie.
The leaves indicate source coverage and any other
hypothesis state.
lated, and the reordering limit. Second, the de-
coder searches through these matches to select
k high-scoring hypotheses for placement in the
stack. We improve this second step.
The decoder provides our algorithm with pairs
consisting of a hypothesis and a compatible source
phrase. Each source phrase translates to multiple
target phrases. The task is to grow these hypothe-
ses by appending a target phrase, yielding new hy-
potheses. These new hypotheses will be placed
into a stack of size k, so we are interested in se-
lecting k new hypotheses that score highly.
Beam search (Lowerre, 1976; Koehn, 2004)
tries every hypothesis with every compatible tar-
get phrase then selects the top k new hypotheses
by score. This is wasteful because most hypothe-
ses are discarded. Instead, we follow cube pruning
(Chiang, 2007) in using a priority queue to gen-
erate k hypotheses. A key difference is that we
generate these hypotheses iteratively.
3.2 Tries
For each source phrase, we collect the set of com-
patible hypotheses. We then place these hypothe-
ses in a trie that emphasizes the suffix words be-
cause these matter most when appending a target
phrase. Figure 2 shows an example. While it suf-
fices to build this trie on the last N ? 1 words
that matter to the language model, Li and Khu-
danpur (2008) have identified cases where fewer
words are necessary because the language model
will back off. The leaves of the trie are complete
hypotheses and reveal information irrelevant to the
language model, such as coverage of the source
sentence and the state of other features.
Each source phrase translates to a set of tar-
get phrases. Because these phrases will be ap-
pended to a hypothesis, the first few words mat-
ter the most to the language model. We therefore

which
have diplomatic
are
that have
diplomatic
Figure 3: Target phrases arranged into a trie. Set
in italic, leaves reveal parts of the phrase that are
irrelevant to the language model.
arrange the target phrases into a prefix trie. An
example is shown in Figure 3. Similar to the hy-
pothesis trie, the depth may be shorter than N ? 1
in cases where the language model will provably
back off (Li and Khudanpur, 2008). The trie can
also be short because the target phrase has fewer
than N ? 1 words. We currently store this trie
data structure directly in the phrase table, though
it could also be computed on demand to save mem-
ory. Empirically, our phrase table uses less RAM
than Moses?s memory-based phrase table.
As an optimization, a trie reveals multiple
words when there would otherwise be no branch-
ing. This allows the search algorithm to make de-
cisions only when needed.
Following Heafield et al (2013), leaves in the
trie take the score of the underlying hypothesis or
target phrase. Non-leaf nodes take the maximum
score of their descendants. Children of a node are
sorted by score.
3.3 Boundary Pairs
The idea is that the decoder reasons over pairs of
nodes in the hypothesis and phrase tries before de-
vling into detail. In this way, it can determine what
the language model likes and, conversely, quickly
discard combinations that the model does not like.
A boundary pair consists of a node in the hy-
pothesis trie and a node in the target phrase trie.
For example, the decoder starts at the root of each
trie with the boundary pair (, ). The score of a
boundary pair is the sum of the scores of the un-
derlying trie nodes. However, once some words
have been revealed, the decoder calls the language
model to compute a score adjustment. For exam-
ple, the boundary pair (country, that) has score ad-
justment
log
p(that | country)
p(that)
times the weight of the language model. This
has the effect of cancelling out the estimate made
132
when the phrase was scored in isolation, replacing
it with a more accurate estimate based on avail-
able context. These score adjustments are efficient
to compute because the decoder retained a pointer
to ?that? in the language model?s data structure
(Heafield et al, 2011).
3.4 Splitting
Refinement is the notion that the boundary pair
(, ) divides into several boundary pairs that re-
veal specific words from hypotheses or target
phrases. The most straightforward way to do this
is simply to split into all children of a trie node.
Continuing the example from Figure 2, we could
split (, ) into three boundary pairs: (country, ),
(nations, ), and (countries, ). However, it is
somewhat inefficient to separately consider the
low-scoring child (countries, ). Instead, we con-
tinue to split off the best child (country, ) and
leave a note that the zeroth child has been split off,
denoted ([1
+
], ). The index increases each time
a child is split off.
The the boundary pair ([1
+
], ) no longer
counts (country, ) as a child, so its score is lower.
Splitting alternates sides. For example,
(country, ) splits into (country, that) and
(country, [1
+
]). If one side has completely
revealed words that matter to the language model,
then splitting continues with the other side.
This procedure ensures that the language model
score is completely resolved before considering
irrelevant differences, such as coverage of the
source sentence.
3.5 Priority Queue
Search proceeds in a best-first fashion controlled
by a priority queue. For each source phrase,
we convert the compatible hypotheses into a trie.
The target phrases were already converted into a
trie when the phrase table was loaded. We then
push the root (, ) boundary pair into the prior-
ity queue. We do this for all source phrases under
consideration, putting their root boundary pairs
into the same priority queue. The algorithm then
loops by popping the top boundary pair. It the top
boundary pair uniquely describes a hypothesis and
target phrase, then remaining features are evalu-
ated and the new hypothesis is output to the de-
coder?s stack. Otherwise, the algorithm splits the
boundary pair and pushes both split versions. Iter-
ation continues until k new hypotheses have been
found.
3.6 Overall Algorithm
We build hypotheses from left-to-right and man-
age stacks just like cube pruning. The only dif-
ference is how the k elements of these stacks are
selected.
When the decoder matches a hypothesis with a
compatible source phrase, we immediately evalu-
ate the distortion feature and update future costs,
both of which are independent of the target phrase.
Our future costs are exactly the same as those used
in Moses (Koehn et al, 2007): the highest-scoring
way to cover the rest of the source sentence. This
includes the language model score within target
phrases but ignores the change in language model
score that would occur were these phrases to be
appended together. The hypotheses compatible
with each source phrase are arranged into a trie.
Finally, the priority queue algorithm from the pre-
ceding section searches for options that the lan-
guage model likes.
4 Experiments
The primary claim is that our algorithm performs
better than cube pruning in terms of the trade-off
between time and accuracy. We compare our new
decoder implementation with Moses (Koehn et al,
2007) by translating 1677 sentences from Chinese
to English. These sentences are a deduplicated
subset of the NIST Open MT 2012 test set and
were drawn from Chinese online text sources, such
as discussion forums. We trained our phrase table
using a bitext of 10.8 million sentence pairs, which
after tokenization amounts to approximately 290
million words on the English side. The bitext con-
tains data from several sources, including news ar-
ticles, UN proceedings, Hong Kong government
documents, online forum data, and specialized
sources such as an idiom translation table. We also
trained our language model on the English half of
this bitext using unpruned interpolated modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998).
The system has standard phrase table, length,
distortion, and language model features. We
plan to implement lexicalized reordering in future
work; without this, the test system is 0.53 BLEU
(Papineni et al, 2002) point behind a state-of-the-
art system. We set the reordering limit to R = 15.
The phrase table was pre-pruned by applying the
same heuristic as Moses: select the top 20 target
phrases by score, including the language model.
133
-29.5
-29.0
-28.5
-28.0
-27.5
0 1 2 3 4
A
v
e
r
a
g
e
m
o
d
e
l
s
c
o
r
e
CPU seconds/sentence
This Work
Moses
13
14
15
0 1 2 3 4
U
n
c
a
s
e
d
B
L
E
U
CPU seconds/sentence
This Work
Moses
Figure 4: Performance of our decoder and Moses for various stack sizes k.
Moses (Koehn et al, 2007) revision d6df825
was compiled with all optimizations recom-
mended in the documentation. We use the in-
memory phrase table for speed. Tests were run
on otherwise-idle identical machines with 32 GB
RAM; the processes did not come close to running
out of memory. The language model was com-
piled into KenLM probing format (Heafield, 2011)
and placed in RAM while text phrase tables were
forced into the disk cache before each run. Timing
is based on CPU usage (user plus system) minus
loading time, as measured by running on empty
input; our decoder is also faster at loading. All re-
sults are single-threaded. Model score is compa-
rable across decoders and averaged over all 1677
sentences; higher is better. The relationship be-
tween model score and uncased BLEU (Papineni
et al, 2002) is noisy, so peak BLEU is not attained
by the highest search accuracy.
Figure 4 shows the results for pop limits k rang-
ing from 5 to 10000 while Table 1 shows select
results. For Moses, we also set the stack size to
k to disable a second pruning pass, as is common.
Because Moses is slower, we also ran our decoder
with higher beam sizes to fill in the graph. Our
decoder is more accurate, but mostly faster. We
can interpret accuracy improvments as speed im-
provements by asking how much time is required
to attain the same accuracy as the baseline. By
this metric, our decoder is 4.0 to 7.7 times as fast
as Moses, depending on k.
Model CPU BLEU
Stack Moses This Moses This Moses This
10 -29.96 -29.70 0.019 0.004 12.92 13.46
100 -28.68 -28.54 0.057 0.016 14.19 14.40
1000 -27.87 -27.80 0.463 0.116 14.91 14.95
10000 -27.46 -27.39 4.773 1.256 15.32 15.28
Table 1: Results for select stack sizes k.
5 Conclusion
We have contributed a new phrase-based search al-
gorithm based on the principle that the language
model cares the most about boundary words. This
leads to two contributions: hiding irrelevant state
from features and an incremental refinement algo-
rithm to find high-scoring combinations. This al-
gorithm is implemented in a new fast phrase-based
decoder, which we release as open-source under
the LGPL at kheafield.com/code/.
Acknowledgements
This work was supported by the Defense Ad-
vanced Research Projects Agency (DARPA)
Broad Operational Language Translation (BOLT)
program through IBM. This work used Stam-
pede provided by the Texas Advanced Comput-
ing Center (TACC) at The University of Texas at
Austin under XSEDE allocation TG-CCR140009.
XSEDE is supported by NSF grant number OCI-
1053575. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
134
References
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A statis-
tical machine translation toolkit for exploring new
model features. In Proceedings of the NAACL HLT
2010 Demonstration Session, pages 9?12, Los An-
geles, California, June. Association for Computa-
tional Linguistics.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, Scotland, UK, July.
Association for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, August.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201?228,
June.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tet-
suo Kiso, and Marcello Federico. 2011. Left lan-
guage model state for syntactic machine translation.
In Proceedings of the International Workshop on
Spoken Language Translation, San Francisco, CA,
USA, December.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words
to speed k-best extraction from hypergraphs. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, Georgia, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, Prague, Czech Repub-
lic, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181?184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic, June.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Machine translation: From real
users to research, pages 115?124. Springer, Septem-
ber.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings of the Second ACL Workshop on Syn-
tax and Structure in Statistical Translation (SSST-2),
pages 10?18, Columbus, Ohio, June.
Bruce T. Lowerre. 1976. The Harpy speech recogni-
tion system. Ph.D. thesis, Carnegie Mellon Univer-
sity, Pittsburgh, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 108?116, Honolulu, HI,
USA, October.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In Proceedings of COLING 2012:
Demonstration Papers, pages 483?492, Mumbai, In-
dia, December.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of ACL-08: HLT, pages 209?
217, Columbus, Ohio.
135
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 301?306,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CMU Multi-Engine Machine Translation for WMT 2010
Kenneth Heafield
Carnegie Mellon University
Pittsburgh, PA, USA.
heafield@cs.cmu.edu
Alon Lavie
Carnegie Mellon University
Pittsburgh, PA, USA.
alavie@cs.cmu.edu
Abstract
This paper describes our submission,
cmu-heafield-combo, to the WMT
2010 machine translation system combi-
nation task. Using constrained resources,
we participated in all nine language pairs,
namely translating English to and from
Czech, French, German, and Spanish as
well as combining English translations
from multiple languages. Combination
proceeds by aligning all pairs of system
outputs then navigating the aligned out-
puts from left to right where each path is
a candidate combination. Candidate com-
binations are scored by their length, agree-
ment with the underlying systems, and a
language model. On tuning data, improve-
ment in BLEU over the best system de-
pends on the language pair and ranges
from 0.89% to 5.57% with mean 2.37%.
1 Introduction
System combination merges the output of sev-
eral machine translation systems into a sin-
gle improved output. Our system combina-
tion scheme, submitted to the Workshop on Sta-
tistical Machine Translation (WMT) 2010 as
cmu-heafield-combo, is an improvement
over our previous system (Heafield et al, 2009),
called cmu-combo in WMT 2009. The scheme
consists of aligning 1-best outputs from each sys-
tem using the METEOR (Denkowski and Lavie,
2010) aligner, identifying candidate combinations
by forming left-to-right paths through the aligned
system outputs, and scoring these candidates us-
ing a battery of features. Improvements this year
include unigram paraphrase alignment, support for
all target languages, new features, language mod-
eling without pruning, and more parameter opti-
mization. This paper describes our scheme with
emphasis on improved areas.
2 Related Work
Confusion networks (Rosti et al, 2008) are the
most popular form of system combination. In this
approach, a single system output acts as a back-
bone to which the other outputs are aligned. This
backbone determines word order while other out-
puts vote for substitution, deletion, and insertion
operations. Essentially, the backbone is edited
to produce a combined output which largely pre-
serves word order. Our approach differs in that
we allow paths to switch between sentences, effec-
tively permitting the backbone to switch at every
word.
Other system combination techniques typically
use TER (Snover et al, 2006) or ITGs (Karakos
et al, 2008) to align system outputs, meaning
they depend solely on positional information to
find approximate matches; we explicitly use stem,
synonym, and paraphrase data to find alignments.
Our use of paraphrases is similar to Leusch et al
(2009), though they learn a monolingual phrase
table while we apply cross-lingual pivoting (Ban-
nard and Callison-Burch, 2005).
3 Alignment
System outputs are aligned at the token level using
a variant of the METEOR (Denkowski and Lavie,
2010) aligner. This identifies, in decreasing order
of priority: exact, stem, synonym, and unigram
paraphrase matches. Stems (Porter, 2001) are
available for all languages except Czech, though
this is planned for future work and expected
to produce significant improvement. Synonyms
come from WordNet (Fellbaum, 1998) and are
only available in English. Unigram paraphrases
are automatically generated using phrase table piv-
oting (Bannard and Callison-Burch, 2005). The
phrase tables are trained using parallel data from
Europarl (fr-en, es-en, and de-en), news commen-
tary (fr-en, es-en, de-en, and cz-en), United Na-
301
tions (fr-en and es-en), and CzEng (cz-en) (Bojar
and Z?abokrtsky?, 2009) sections 0?8. The German
and Spanish tables also use the German-Spanish
Europarl corpus released for WMT08 (Callison-
Burch et al, 2008). Currently, the generated para-
phrases are filtered to solely unigram matches;
full use of this table is planned for future work.
When alignment is ambiguous (i.e. ?that? appears
twice in a system output), an alignment is chosen
to minimize crossing with other alignments. Fig-
ure 1 shows an example alignment. Compared to
our previous system, this replaces heuristic ?arti-
ficial? alignments with automatically learned uni-
gram paraphrases.
Twice that produced by nuclear plants
Double that that produce nuclear power stations
Figure 1: Alignment generated by METEOR
showing exact (that?that and nuclear?nuclear),
stem (produced?produce), synonym (twice?
double), and unigram paraphrase (plants?stations)
alignments.
4 Search Space
A candidate combination consists of a string of to-
kens (words and punctuation) output by the under-
lying systems. Unconstrained, the string could re-
peat tokens and assemble them in any order. We
therefore have several constraints:
Sentence The string starts with the beginning of
sentence token and finishes with the end of
sentence token. These tokens implicitly ap-
pear in each system?s output.
Repetition A token may be used at most once.
Tokens that METEOR aligned are alterna-
tives and cannot both be used.
Weak Monotonicity This prevents the scheme
from reordering too much. Specifically, the
path cannot jump backwards more than r to-
kens, where positions are measured relative
to the beginning of sentence. It cannot make
a series of smaller jumps that add up to more
than r either. Equivalently, once a token
in the ith position of some system output is
used, all tokens before the i? rth position in
their respective system outputs become un-
usable. The value of r is a hyperparameter
considered in Section 6.
Completeness Tokens may not be skipped unless
the sentence ends or another constraint would
be violated. Specifically, when a token from
some system is used, it must be the first (left-
most in the system output) available token
from that system. For example, the first de-
coded token must be the first token output by
some system.
Together, these define the search space. The candi-
date starts at the beginning of sentence by choos-
ing the first token from any system. Then it can
either continue with the next token from the same
system or switch to another one. When it switches
to another system, it does so to the first available
token from the new system. The repetition con-
straint requires that the token does not repeat con-
tent. The weak monotonicity constraint ensures
that the jump to the new system goes at most r
words back. The process repeats until the end of
sentence token is encountered.
The previous version (Heafield et al, 2009) also
had a hard phrase constraint and heuristics to de-
fine a phrase; this has been replaced with new
match features.
Search is performed using beam search where
the beam contains partial candidates of the same
length, each of which starts with the beginning of
sentence token. In our experiments, the beam size
is 500. When two partial candidates will extend
in the same way (namely, the set of available to-
kens is the same) and have the same feature state
(i.e. language model history), they are recom-
bined. The recombined partial candidate subse-
quently acts like its highest scoring element, until
k-best list extraction when it is lazily unpacked.
5 Scoring Features
Candidates are scored using three feature classes:
Length Number of tokens in the candidate. This
compensates, to first order, for the impact of
length on other features.
Match For each system s and small n, feature
ms,n is the number of n-grams in the candi-
date matching the sentence output by system
s. This is detailed in Section 5.1.
302
Language Model Log probability from a n-gram
language model and backoff statistics. Sec-
tion 5.2 details our training data and backoff
features.
Features are combined into a score using a linear
model. Equivalently, the score is the dot product
of a weight vector with the vector of our feature
values. The weight vector is a parameter opti-
mized in Section 6.
5.1 Match Features
The n-gram match features reward agreement be-
tween the candidate combination and underlying
system outputs. For example, feature m1,1 counts
tokens in the candidate that also appear in sys-
tem 1?s output for the sentence being combined.
Featurem1,2 counts bigrams appearing in both the
candidate and the translation suggested by system
1. Figure 2 shows example feature values.
System 1: Supported Proposal of France
System 2: Support for the Proposal of France
Candidate: Support for Proposal of France
Unigram Bigram Trigram
System 1 4 2 1
System 2 5 3 1
Figure 2: Example match feature values with two
systems and matches up to length three. Here,
?Supported? counts because it aligns with ?Sup-
port?.
The match features count n-gram matches be-
tween the candidate and each system. These
matches are defined in terms of alignments. A to-
ken matches the system that supplied it as well as
the systems to which it aligns. This can be seen in
Figure 2 where System 1?s unigram match count
includes ?Supported? even though the candidate
chose ?Support?. Longer matches are defined sim-
ilarly: a bigram match consists of two consecutive
alignments without reordering. Since METEOR
generates several types of alignments as shown in
Figure 1, we wonder whether all alignment types
should count as matches. If we count all types
of alignment, then the match features are blind to
lexical choice, leaving only the language model to
discriminate. If only exact alignments count, then
less systems are able to vote on a word order deci-
sion mediated by the bigram and trigram features.
We find that both versions have their advantages,
and therefore include two sets of match features:
one that counts only exact alignments and another
that counts all alignments. We also tried copies of
the match features at the stem and synonym level
but found these impose additional tuning cost with
no measurable improvement in quality.
Since systems have different strengths and
weaknesses, we avoid assigning a single system
confidence (Rosti et al, 2008) or counting n-gram
matches with uniform system confidence (Hilde-
brand and Vogel, 2009). The weight on match
feature ms,n corresponds to our confidence in n-
grams from system s. These weights are fully tun-
able. However, there is another hyperparameter:
the maximum length of n-gram considered; we
typically use 2 or 3 with little gain seen above this.
5.2 Language Model
We built language models for each of the five tar-
get languages with the aim of using all constrained
data. For each language, we used the provided
Europarl (Koehn, 2005) except for Czech, News
Commentary, and News monolingual corpora. In
addition, we used:
Czech CzEng (Bojar and Z?abokrtsky?, 2009) sec-
tions 0?7
English Gigaword Fourth Edition (Parker et al,
2009), Giga-FrEn, and CzEng (Bojar and
Z?abokrtsky?, 2009) sections 0?7
French Gigaword Second Edition (Mendonca et
al., 2009a), Giga-FrEn
Spanish Gigaword Second Edition (Mendonca et
al., 2009b)
Paragraphs in the Gigaword corpora were split
into sentences using the script provided with
Europarl (Koehn, 2005); parenthesized format-
ting notes were removed from the NYT portion.
We discarded Giga-FrEn lines containing invalid
UTF8, control characters, or less than 90% Latin
characters or punctuation. Czech training data
and system outputs were preprocessed using Tec-
toMT (Z?abokrtsky? and Bojar, 2008) following the
CzEng 0.9 pipeline (Bojar and Z?abokrtsky?, 2009).
English training data and system outputs were to-
kenized with the IBM tokenizer. French, Ger-
man, and Spanish used the provided tokenizer.
303
Czech words were truecased based on automati-
cally identified lemmas marking names; for other
languages, training data was lowercased and sys-
tems voted, with uniform weight, on capitalization
of each character in the final output.
With the exception of Czech (for which we used
an existing model), all models were built with no
lossy pruning whatsoever, including our English
model with 5.8 billion tokens (i.e. after IBM to-
kenization). Using the stock SRILM (Stolcke,
2002) toolkit with modified Kneser-Ney smooth-
ing, the only step that takes unbounded memory is
final model estimation from n-gram counts. Since
key parameters have already been estimated at this
stage, this final step requires only counts for the
desired n-grams and all of their single token ex-
tensions. We can therefore filter the n-grams on
all but the last token. Our scheme will only query
an n-gram if all of the tokens appear in the union
of system outputs for some sentence; this strict fil-
tering criterion is further described and released
as open source in Heafield and Lavie (2010). The
same technique applies to machine translation sys-
tems, with phrase table expansion taking the place
of system outputs.
For each language, we built one model by ap-
pending all data. Another model interpolates
smaller models built on the individual sources
where each Gigaword provider counts as a distinct
source. Interpolation weights were learned on the
WMT 2009 references. For English, we also tried
an existing model built solely on Gigaword using
interpolation. The choice of model is a hyperpa-
rameter we consider in Section 6.
In the combination scheme, we use the log lan-
guage model probability as a feature. Another
feature reports the length of the n-gram matched
by the model; this exposes limited tunable con-
trol over backoff behavior. For Czech, the model
was built with a closed vocabulary; when an out-
of-vocabulary (OOV) word is encountered, it is
skipped for purposes of log probability and a
third feature counts how often this happens. This
amounts to making the OOV probability a tunable
parameter.
6 Parameter Optimization
6.1 Feature Weights
Feature weights are tuned using Minimum Error
Rate Training (MERT) (Och, 2003) on the 455
provided references. Our largest submission, xx-
en primary, combines 17 systems with five match
features each plus three other features for a total of
88 features. This immediately raises two concerns.
First, there is overfitting and we expect to see a
loss in the test results, although our experience in
the NIST Open MT evaluation is that the amount
of overfitting does not significantly increase at this
number of parameters. Second, MERT is poor at
fitting this many feature weights. We present one
modification to MERT that addresses part of this
problem, leaving other tuning methods as future
work.
MERT is prone to local maxima, so we apply
a simple form of simulated annealing. As usual,
the zeroth iteration decodes with some initial fea-
ture weights. Afterward, the weights {?f} learned
from iteration 0 ? j < 10 are perturbed to pro-
duce new feature weights
?f ? U
[
j
10
?f ,
(
2?
j
10
)
?f
]
where U is the uniform distribution. This sam-
pling is done on a per-sentence basis, so the first
sentence is decoded with different weights than
the second sentence. The amount of random per-
turbation decreases linearly each iteration until
the 10th and subsequent iterations whose learned
weights are not perturbed. We emphasize that
the point is to introduce randomness in sentences
decoded during MERT, and therefore considered
during parameter tuning, and not on the spe-
cific formula presented in this system description.
In practice, this technique increases the number
of iterations and decreases the difference in tun-
ing scores following MERT. In our experiments,
weights are tuned towards uncased BLEU (Pap-
ineni et al, 2002) or the combined metric TER-
BLEU (Snover et al, 2006).
6.2 Hyperparameters
In total, we tried 1167 hyperparameter configura-
tions, limited by CPU time during the evaluation
period. For each of these configurations, the fea-
ture weights were fully trained with MERT and
scored on the same tuning set, which we used to
select the submitted combinations. Because these
configurations represent a small fraction of the
hyperparameter space, we focused on values that
work well based on prior experience and tuning
scores as they became available:
Set of systems Top systems by BLEU. The num-
ber of top systems included ranged from 3 to
304
Pair Entry #Sys r Match LM Objective ?BLEU ?TER ?METE
cz-en main 5 4 2 Append BLEU 2.38 0.99 1.50
de-en
main 6 4 2 Append TER-BLEU 2.63 -2.38 1.36
contrast 7 3 2 Append BLEU 2.60 -2.62 1.09
es-en
main 7 5 3 Append BLEU 1.22 -0.74 0.70
contrast 5 6 2 Gigaword BLEU 1.08 -0.80 0.97
fr-en
main 9 5 3 Append BLEU 2.28 -2.26 0.78
contrast 8 5 3 Append BLEU 2.19 -1.81 0.63
xx-en
main 17 5 3 Append BLEU 5.57 -5.60 4.33
contrast 16 5 3 Append BLEU 5.45 -5.38 4.22
en-cz main 7 5 3 Append TER-BLEU 0.74 -0.26 0.68
en-de
main 6 6 2 Interpolate BLEU 1.26 0.16 1.14
contrast 5 4 2 Interpolate BLEU 1.26 0.30 1.00
en-es
main 8 5 3 Interpolate BLEU 2.38 -2.20 0.96
contrast 6 7 2 Append BLEU 2.40 -1.85 1.02
en-fr main 6 7 2 Append BLEU 2.64 -0.50 1.55
Table 1: Submitted combinations chosen from among 1167 hyperparameter settings by tuning data
scores. Uncased BLEU, uncased TER, and METEOR 1.0 with adequacy-fluency parameters are shown
relative to top system by BLEU. Improvement is seen in all pairs on all metrics except for TER on cz-en
and en-de where the top systems are 5% and 2% shorter than the references, respectively. TER has a well
known preference for shorter hypotheses. The #Sys column indicates the number of systems combined,
using the top scoring systems by BLEU. The Match column indicates the maximum n-gram length con-
sidered for matching on all alignments; we separately counted unigram and bigram exact matches. In
some cases, we made a contrastive submission where metrics disagreed or length behavior differed near
the top; contrastive submissions are not our 2009 scheme.
all of them, except on xx-en where we com-
bined up to 17.
Jump limit Mostly r = 5, with some experi-
ments ranging from 3 to 7.
Match features Usually unigram and bigram fea-
tures, sometimes trigrams as well.
Language model Balanced between the ap-
pended and interpolated models, with the
occasional baseline Gigaword model for
English.
Tuning objective Usually BLEU for speed rea-
sons; occasional TER-BLEU with typical
values for other hyperparameters.
7 Conclusion
Table 1 shows the submitted combinations and
their performance. Our submissions this year im-
prove over last year (Heafield et al, 2009) in
overall performance and support for multiple lan-
guages. The improvement in performance we pri-
marily attribute to the new match features, which
account for most of the gain and allowed us to in-
clude lower quality systems. We also trained lan-
guage models without pruning, replaced heuristic
alignments with unigram paraphrases, tweaked the
other features, and improved the parameter opti-
mization process. We hope that the improvements
seen on tuning scores generalize to significantly
improved test scores, especially human evaluation.
Acknowledgments
Ondr?ej Bojar made the Czech language model
and preprocessed Czech system outputs. Michael
Denkowski provided the paraphrase tables and
wrote the version of METEOR used. This work
was supported in part by the DARPA GALE pro-
gram and by a NSF Graduate Research Fellow-
ship.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings ACL.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
305
0.9, building a large Czech-English automatic paral-
lel treebank. The Prague Bulletin of Mathematical
Linguistics, (92):63?83.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Denkowski and Alon Lavie. 2010. Extend-
ing the METEOR machine translation metric to the
phrase level. In Proceedings NAACL 2010, Los An-
geles, CA, June.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. In The Prague Bulletin of Mathematical
Linguistics, number 93, pages 27?36, Dublin.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination
with flexible word ordering. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 56?60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU system combination for WMT?09. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 47?50, Athens, Greece,
March. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proceedings ACL-08: HLT, Short Papers (Compan-
ion Volume), pages 81?84.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2009. The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 51?
55, Athens, Greece, March. Association for Compu-
tational Linguistics.
Angelo Mendonca, David Graff, and Denise DiPer-
sio. 2009a. French gigaword second edition.
LDC2009T28.
Angelo Mendonca, David Graff, and Denise DiPer-
sio. 2009b. Spanish gigaword second edition.
LDC2009T21.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proceed-
ings of 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English gigaword fourth
edition. LDC2009T13.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proceedings Third Workshop on Statistical
Machine Translation, pages 183?186.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings Seventh Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904.
Zdene?k Z?abokrtsky? and Ondr?ej Bojar. 2008. TectoMT,
Developer?s Guide. Technical Report TR-2008-39,
Institute of Formal and Applied Linguistics, Faculty
of Mathematics and Physics, Charles University in
Prague, December.
306
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 145?151,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU System Combination in WMT 2011
Kenneth Heafield and Alon Lavie
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA, USA
{heafield,alavie}@cs.cmu.edu
Abstract
This paper describes our submissions,
cmu-heafield-combo, to the ten tracks
of the 2011 Workshop on Machine Transla-
tion?s system combination task. We show how
the combination scheme operates by flexibly
aligning system outputs then searching a
space constructed from the alignments.
Humans judged our combination the best on
eight of ten tracks.
1 Introduction
We participated in all ten tracks of the 2011 Work-
shop on Machine Translation system combination
task as cmu-heafield-combo. This uses a sys-
tem combination scheme that builds on our prior
work (Heafield and Lavie, 2010), especially with
respect to language modeling and handling non-
English languages. We present a summary of
the system, describe improvements, list the data
used (all of the constrained monolingual data), and
present automatic results in anticipation of human
evaluation by the workshop.
2 Our Combination Scheme
Given single-best outputs from each system, the
scheme aligns system outputs then searches a space
based on these alignments. The scheme is a contin-
uation of our previous system (Heafield and Lavie,
2010) so we describe unchanged parts of the sys-
tem in less detail, preferring instead to focus on new
components.
2.1 Alignment
We run the METEOR matcher (Denkowski and
Lavie, 2010) on every pair of system outputs for a
given sentence. It identifies exact matches, identi-
cal stems (Porter, 2001) except for Czech, WordNet
synonym matches for English (Fellbaum, 1998), and
automatically extracted matches for all five target
languages. The automatic matches come from piv-
oting (Bannard and Callison-Burch, 2005) on con-
strained data. An example METEOR alignment is
shown in Figure 1, though it need not be monotone.
Twice that produced by nuclear plants
Double that that produce nuclear power stations
Figure 1: Alignment generated by METEOR showing
exact (that?that and nuclear?nuclear), stem (produced?
produce), synonym (twice?double), and unigram para-
phrase (plants?stations) alignments.
2.2 Search
The search space is unchanged from Heafield and
Lavie (2010), so we give a summary here. The gen-
eral idea is to generate a combined sentence one
word at a time, going from left to right. As the
scheme creates an output, it also steps through the
system outputs from left to right. Stepping through
systems is synchronized with the partial output, so
that words to the left are already captured in the hy-
pothesis and the next word from any of the systems
represents a meaningful extension of the partial out-
put. All of these options are considered by hypothe-
sis branching.
145
Thus far, we have assumed that system outputs are
monotone: they agree on word order, so it is possi-
ble to step through all of them simultaneously. On
the left are words captured in the partial output and
on the right are the words whose meaning remains
to be captured in the output. When systems disagree
on word order, the partial output corresponds to dis-
joint pieces of a system?s output. We still retain that
notion that a word is either captured in the partial
output or not captured, but do not have a single di-
viding line between them. In this case, we still pro-
ceed from left to right, considering the first uncap-
tured word for extension. Then, we skip over parts
of a system?s output that have already been captured.
Here, we have used the informal notion of words
whose meaning is ?captured? or ?uncaptured? by the
partial output. The system interprets words aligned
to the partial output as captured while those not
aligned to the hypothesis are considered uncaptured.
A heuristic also cleans up excess words in order
to keep the stepping process loosely synchronized
across system outputs.
2.3 Features
We use three feature categories to guide search:
Length The length of the hypothesis in tokens.
Language Model Log probability and OOV count
from an N -gram language model. Details are
in Section 4.1.
Match Counts Counts of n-gram matches between
systems outputs and the hypothesis.
The match count features report n-gram matches
between each system and the hypothesis. Specifi-
cally, feature ms,n reports n-gram overlap between
the hypothesis and system s. We track n-gram
counts up to length N , typically 2 or 3, finding that
tracking longer lengths adds little. An example is
shown in Figure 2.
These match counts may be exact, in which case
every word of the n-gram must be the same (up
to case) or approximate, in which case any aligned
word found by METEOR may be substituted. Be-
cause exact matches handle lexical choice and in-
exact matches collect more votes that better handle
word order, we use both sets of features. However,
the limit N may be different i.e. Ne = 2 counts
exact matches up to length 2 and Na = 3 counts
inexact matches up to length 3.
System 1: Supported Proposal of France
System 2: Support for the Proposal of France
Candidate: Support for Proposal of France
Unigram Bigram Trigram
System 1 4 2 1
System 2 5 3 1
Figure 2: Example match feature values with two systems
and matches up to length three. Here, ?Supported? counts
because it aligns with ?Support?.
3 Related Work
Hypothesis selection (Hildebrand and Vogel, 2009)
selects an entire sentence at a time instead of picking
and merging words. This makes the approach less
flexible, in that it cannot synthesize new sentences,
but also less risky by avoiding matching and related
problems entirely.
While our alignment is based on METEOR, other
techniques are based on TER (Snover et al, 2006),
Inversion Transduction Grammars (Narsale, 2010),
and other alignment methods. These use exact
alignments and positional information to infer align-
ments, ignoring the content-based method used by
METEOR. This means they might align content
words to function words, while we never do. In prac-
tice, using both signals would likely work better.
Confusion networks (Rosti et al, 2010; Narsale,
2010) are the dominant method for system combi-
nation. These base their word order on one system,
dubbed the backbone, and have all systems vote on
editing the backbone. Word order is largely fixed to
that of one system; by contrast, ours can piece to-
gether word orders taken from multiple systems. In
a loose sense, our approach is a confusion network
where the backbone is permitted to switch after each
word.
Interestingly, BBN (Rosti et al, 2010) this year
added a novel-bigram penalty that penalizes bigrams
in the output if they do not appear in one of the sys-
146
tem outputs. This is the complement of our bigram
match count features (and, since, we have a length
feature, the same up to rearranging weights). How-
ever, they threshold it to indicate whether the bigram
appears at all instead of how many systems support
the bigram.
4 Resources
The resources we use are constrained to those pro-
vided for the shared task.
For the paraphrase matches described in Sec-
tion 2.1, METEOR (Denkowski and Lavie, 2010)
trains its paraphrase tables via pivoting (Bannard
and Callison-Burch, 2005). The phrase tables are
trained using parallel data from Europarl v6 (Koehn,
2005) (fr-en, es-en, de-en, and es-de), news com-
mentary (fr-en, es-en, de-en, and cz-en), United Na-
tions (fr-en and es-en), and CzEng (cz-en) (Bojar
and Z?abokrtsky?, 2009) sections 0?8.
4.1 Language Modeling
As with previous versions of the system, we use
language model log probability as a feature to bias
translations towards fluency. We add a second fea-
ture per language model that counts OOVs, allow-
ing MERT to independently tune the OOV penalty.
Language models often have poor OOV estimates
for translation because they come not from new text
in the same language but from new text in a differ-
ent language. The distribution is even more biased
in system combination, where most systems have al-
ready applied a language model. The new OOV fea-
ture replaces a previous feature that reported the av-
erage n-gram length matched by the model.
We added support for multiple language mod-
els so that their probabilities, OOV penalties, and
all other features are dynamically interpolated using
MERT. This we use for the Haitian Creole-English
tasks, where the first language model is a large
model built on the monolingual data except SMS
messages and the second small language model is
built on the SMS messages. The OOV features play
an important role here because frequent anonymiza-
tion markers such as ?[firstname]? do not appear in
the large language model.
To scale to larger language models, we use
BigFatLM1, an open-source builder of large un-
pruned models with modified Kneser-Ney smooth-
ing. Then, we filter the models to the system out-
puts. In order for an n-gram to be queried, all of the
words must appear in system outputs for the same
sentence. This enables a filtering constraint stronger
than normal vocabulary filtering, which permits n-
grams supported only by words in different sen-
tences. Finally, we use KenLM (Heafield, 2011) for
inference at runtime.
Our primary use of data is for language model-
ing. We used essientially every constrained resource
available and appended them together to build one
large model. For every language, we used the pro-
vided Europarl v6 (Koehn, 2005), News Crawl, and
News Commentary corpora. In addition, we used:
English Gigaword Fourth Edition (Parker et al,
2009) and the English parts of United Na-
tions documents, Giga-FrEn, and CzEng (Bojar
and Z?abokrtsky?, 2009) sections 0?7. For the
Haitian Creole-English tasks, we built a sepa-
rate language model on the SMS messages and
used it alongside the large English model.
Czech CzEng (Bojar and Z?abokrtsky?, 2009) sec-
tions 0?7
French Gigaword Second Edition (Mendonc?a et
al., 2009a) and the French parts of Giga-FrEn
and United Nations documents.
German There were no additional corpora avail-
able.
Spanish Gigaword Second Edition (Mendonc?a et
al., 2009b) and the Spanish parts of United Na-
tions documents.
4.2 Preprocessing
Many corpora contained excessive duplicate text.
We wrote a deduplicator that removes all but the
first instance of each line. Clean corpora generally
reduced line count by 10-25% when deduplicated,
resulting from naturally-occuring duplicates such as
?yes .? We left the duplicate lines in these corpora.
The News Crawl corpus showed a 72.6% reduction
in line count due mainly to boilerplace, such as the
1https://github.com/jhclark/bigfatlm
147
Reuters comment section header and Fark headlines
that appear in a box on many pages. We dedupli-
cated the News Crawl corpus, United Nations docu-
ments, and New York Times and LA Times portions
of English Gigaword.
The Giga-FrEn corpus is noisy. We removed lines
from Giga-FrEn if any of the following conditions
held:
? Invalid UTF8 or control characters.
? Less than 90% of characters are in the Latin
alphabet (including diacritics) or punctuation.
We did not count ?<? and ?>? as punctuation
to limit the amount of HTML code.
? Less than half the characters are Latin letters.
System outputs and language model training data
were normalized using the provided punctuation
normalization script, Unicode codepoint collaps-
ing, the provided Moses (Koehn et al, 2007) to-
kenizer, and several custom rules. These remove
formatting-related tokens from Gigaword, rejoin
some French words with internal apostrophes, and
threshold repetitive punctuation. In addition, Ger-
man words were segmented as explained in Section
4.3. Text normalization is more difficult for system
combination because the system outputs, while theo-
retically detokenized, contain errors that result from
different preprocessing at each site.
4.3 German Segmentation
German makes extensive use of compounding, cre-
ating words that do not cleanly align to English and
have less reliable statistics. German-English trans-
lation systems therefore typically segment German
compounds as a preprocessing step. In our case,
we are concerned with combining translations into
German that may be segmented differently. These
can be due to stylistic choices; for example both
?jahrzehnte lang? and ?jahrzehntelang? appear with
approximately equal frequency as shown in Table 1.
Translation systems add additional biases due to the
various preprocessing approaches taken by individ-
ual sites and inherent biases in models such as word
alignment.
In order to properly align differently segmented
words, we normalize by segmenting all system out-
puts and our language model training data using
Words Separate Compounded
jahrzehnte lang 554 542
klar gemacht 840 802
unter anderem 49538 4
wieder herzustellen 513 1532
Table 1: Counts of separate or compounded versions of
select words in the lowercased German monolingual data.
Compounding can be optional or biased in either way.
the single-best segmentation from cdec (Dyer et
al., 2010). Running our system therefore produces
segmented German output. Internally, we tuned
towards segmented references but for final output
it is desirable to rejoin compound words. Since
the cdec segmentation was designed for German-
English translation, no corresponding desegmenter
was provided.
We created a German desegmenter in the natural
way: segment German words then invert the map-
ping to identify words that should be rejoined. To do
so, we ran every word from the German monolingual
data and system outputs through the cdec segmenter,
counted both the compounded and segmented ver-
sions in the monolingual data, and removed those
that appear segmented more often. Desegmenting is
a mildly ambiguous process because n-grams to re-
join may overlap. When an n-gram compounded to
one word, we gave that a score of n2. The total score
is a sum of these squares, favoring compounds that
cover more words. Maximizing the score is a fast
and exact dynamic programming algorithm. Casing
of unchanged words comes from equally-weighted
system votes at the character level while casing of
rejoined words is based on the majority appearance
in the corpus; this is almost always initial capital.
We ran our desegmenter followed by the workshop?s
provided detokenizer to produce the submitted out-
put.
5 Results
We tried many variations on the scheme, such as se-
lecting different systems, tuning to BLEU (Papineni
et al, 2002) or METEOR (Denkowski and Lavie,
2010), and changing the structure of the match count
features from Section 2.3. To try these, we ran
MERT 242 times, or about 24 times for each of the
ten tasks in which we participated. Then we selected
148
the best performing systems on the tuning set and
submitted them, with the secondary system chosen
to meaningfully differ from the primary while still
scoring well. Once the evaluation released refer-
ences, we scored against them to generate Table 2.
On the featured Haitian Creole task, we show no
and sometimes even negative improvement. This we
attribute to the gap between the top system, bm-i2r,
and the second place system. For htraw-en, where
training data is noisy, the bm-i2r is 3.65 BLEU
higher than the second place system at 28.53 BLEU.
On htclean-en, the gap is 4.44 points to the second
place cmu-denkowski-contrastive.
The main tasks were quite competitive and many
systems were within a BLEU point of the top. This
is an ideal scenario for system combination, and we
show corresponding improvements. The English-
Czech task is difficult for our scheme because we do
not properly handle Czech morpology in alignment.
On Czech-English, online-B beat other systems by
a substantial (6.21 BLEU) margin, so we see little
gain. On English-German, the gain is small but this
is consistent with a general observation that more
improvement is seen on higher-quality systems. Fur-
ther, strength in this year?s submission comes from
language modeling, but only limited German data
was available; segmenting German improved our
scores. Translations into Spanish and French show
the impact of Gigaword in those languages.
The evaluation?s official metric is human rank-
ing judgments. On this metric, our submissions
score highest on eight of ten tracks: Czech-English,
German-English, English-Czech, English-German,
English-Spanish, English-French, the clean Haitian
Creole-English task, and the raw Haitian Creole-
English task. For Spanish-English, humans pre-
ferred RWTH?s submission. For French-English,
humans preferred RWTH and BBN. However, sys-
tem combinations were ranked against other system
combinations, but not against underlying systems,
so we suspect that the bm-i2r submission still per-
forms better than combinations on the Haitian Cre-
ole tasks. The human judges also preferred our
translations more than BLEU (where we lead on
three language pairs: English to German, Span-
ish, and French). We attribute this to the tendency
of confusion networks to drop words supported by
many systems due to position-based alignment er-
Track Entry BLEU TER MET
htraw-en
primary 32.30 56.57 61.05
contrast 31.76 56.69 60.81
bm-i2r 32.18 57.01 60.85
htclean-en
primary 36.39 51.16 63.72
contrast 36.49 51.15 63.78
bm-i2r 36.97 51.06 64.01
cz-en
primary 29.85 53.20 62.50
contrast 29.88 53.19 62.40
online-B 29.59 52.15 61.77
de-en
primary 26.21 56.19 60.56
contrast 26.11 56.42 60.54
online-B 24.30 57.95 59.63
es-en
primary 33.90 48.88 65.72
contrast 33.47 49.41 66.41
online-A 30.26 51.56 63.83
fr-en
primary 32.41 48.93 65.72
contrast 32.15 49.12 65.71
kit 30.36 50.74 64.32
en-cz
primary 20.80 61.17 41.68
contrast 20.74 61.29 41.69
online-B 20.37 61.38 41.40
en-de
primary 18.45 64.15 22.91
contrast 18.27 64.48 22.75
online-B 17.92 64.01 22.95
en-es
primary 36.47 47.08 34.96
contrast 35.82 47.52 34.64
online-B 33.85 50.09 33.96
en-fr
primary 36.42 48.28 24.29
contrast 36.31 48.56 24.12
online-B 35.34 48.68 23.53
Table 2: Automatic scores for our submissions. For com-
parison, the top individual system by BLEU is shown
in the third row of each track. Test data and references
were preprocessed prior to scoring. Metrics are uncased
and METEOR 1.0 uses adequacy-fluency parameters. We
show improvement on all tasks except Haitian Creole-
English.
149
rors; our content-based alignment method avoids
many of these errors. BLEU penalizes the missing
word the same as missing punctuation while human
judges will penalize heavily for missing content. For
full results, we refer to the simultaneously published
Workshop on Machine Translation findings paper.
6 Conclusion
We participated in the all ten tracks of the sys-
tem combination, prioritizing participation and lan-
guage support over optimizing for one particular
language pair. Nonetheless, we show improvement
on several tasks, including wins by BLEU on three
tracks. The Haitian Creole and Czech-English tasks
proved challenging due to the gap between top sys-
tems. However, other tracks show a variety of
high-performing systems that make our scheme per-
form well. Unlike most other system combination
schemes, our code is open source2 so that these re-
sults may be replicated and brought to bear on simi-
lar problems.
Acknowledgements
Jon Clark assisted with language model construction
and wrote BigFatLM. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship under Grant No.
0750271 and by the DARPA GALE program.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings ACL.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng 0.9,
building a large Czech-English automatic parallel tree-
bank. The Prague Bulletin of Mathematical Linguis-
tics, (92):63?83.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evalua-
tion support for five target languages. In Proceed-
ings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 339?342,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
2http://kheafield.com/code/mt
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ?10, pages 7?12.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Kenneth Heafield and Alon Lavie. 2010. CMU multi-
engine machine translation for WMT 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR. Association for
Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 47?50, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
A?ngelo Mendonc?a, David Graff, and Denise DiPer-
sio. 2009a. French gigaword second edition.
LDC2009T28.
A?ngelo Mendonc?a, David Graff, and Denise DiPer-
sio. 2009b. Spanish gigaword second edition.
LDC2009T21.
Sushant Narsale. 2010. JHU system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 311?314, Uppsala, Sweden,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English gigaword fourth edi-
tion. LDC2009T13.
150
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2010. BBN system description for
wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 321?326, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), pages 223?231, Cambridge, MA, Au-
gust.
151
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187?197,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
KenLM: Faster and Smaller Language Model Queries
Kenneth Heafield
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15213 USA
heafield@cs.cmu.edu
Abstract
We present KenLM, a library that imple-
ments two data structures for efficient lan-
guage model queries, reducing both time and
memory costs. The PROBING data structure
uses linear probing hash tables and is de-
signed for speed. Compared with the widely-
used SRILM, our PROBING model is 2.4
times as fast while using 57% of the mem-
ory. The TRIE data structure is a trie with
bit-level packing, sorted records, interpola-
tion search, and optional quantization aimed
at lower memory consumption. TRIE simul-
taneously uses less memory than the small-
est lossless baseline and less CPU than the
fastest baseline. Our code is open-source1,
thread-safe, and integrated into the Moses,
cdec, and Joshua translation systems. This
paper describes the several performance tech-
niques used and presents benchmarks against
alternative implementations.
1 Introduction
Language models are widely applied in natural lan-
guage processing, and applications such as machine
translation make very frequent queries. This pa-
per presents methods to query N -gram language
models, minimizing time and space costs. Queries
take the form p(wn|w
n?1
1 ) where w
n
1 is an n-gram.
Backoff-smoothed models estimate this probability
based on the observed entry with longest matching
1http://kheafield.com/code/kenlm
history wnf , returning
p(wn|w
n?1
1 ) = p(wn|w
n?1
f )
f?1?
i=1
b(wn?1i ). (1)
where the probability p(wn|w
n?1
f ) and backoff
penalties b(wn?1i ) are given by an already-estimated
model. The problem is to store these two values for a
large and sparse set of n-grams in a way that makes
queries efficient.
Many packages perform language model queries.
Throughout this paper we compare with several
packages:
SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit
based on tries used in several decoders.
IRSTLM 5.60.02 (Federico et al, 2008) is a sorted
trie implementation designed for lower mem-
ory consumption.
MITLM 0.4 (Hsu and Glass, 2008) is mostly de-
signed for accurate model estimation, but can
also compute perplexity.
RandLM 0.2 (Talbot and Osborne, 2007) stores
large-scale models in less memory using ran-
domized data structures.
BerkeleyLM revision 152 (Pauls and Klein, 2011)
implements tries based on hash tables and
sorted arrays in Java with lossy quantization.
Sheffield Guthrie and Hepple (2010) explore sev-
eral randomized compression techniques, but
did not release code.
TPT Germann et al (2009) describe tries with bet-
ter locality properties, but did not release code.
These packages are further described in Section 3.
We substantially outperform all of them on query
187
speed and offer lower memory consumption than
lossless alternatives. Performance improvements
transfer to the Moses (Koehn et al, 2007), cdec
(Dyer et al, 2010), and Joshua (Li et al, 2009)
translation systems where our code has been inte-
grated. Our open-source (LGPL) implementation is
also available for download as a standalone package
with minimal (POSIX and g++) dependencies.
2 Data Structures
We implement two data structures: PROBING, de-
signed for speed, and TRIE, optimized for mem-
ory. The set of n-grams appearing in a model is
sparse, and we want to efficiently find their associ-
ated probabilities and backoff penalties. An impor-
tant subproblem of language model storage is there-
fore sparse mapping: storing values for sparse keys
using little memory then retrieving values given keys
using little time. We use two common techniques,
hash tables and sorted arrays, describing each before
the model that uses the technique.
2.1 Hash Tables and PROBING
Hash tables are a common sparse mapping technique
used by SRILM?s default and BerkeleyLM?s hashed
variant. Keys to the table are hashed, using for ex-
ample Austin Appleby?s MurmurHash2, to integers
evenly distributed over a large range. This range is
collapsed to a number of buckets, typically by tak-
ing the hash modulo the number of buckets. Entries
landing in the same bucket are said to collide.
Several methods exist to handle collisions; we use
linear probing because it has less memory overhead
when entries are small. Linear probing places at
most one entry in each bucket. When a collision oc-
curs, linear probing places the entry to be inserted
in the next (higher index) empty bucket, wrapping
around as necessary. Therefore, a populated probing
hash table consists of an array of buckets that contain
either one entry or are empty. Non-empty buckets
contain an entry belonging to them or to a preceding
bucket where a conflict occurred. Searching a prob-
ing hash table consists of hashing the key, indexing
the corresponding bucket, and scanning buckets un-
til a matching key is found or an empty bucket is
2http://sites.google.com/site/murmurhash/
encountered, in which case the key does not exist in
the table.
Linear probing hash tables must have more buck-
ets than entries, or else an empty bucket will never
be found. The ratio of buckets to entries is controlled
by space multiplier m > 1. As the name implies,
space is O(m) and linear in the number of entries.
The fraction of buckets that are empty is m?1m , so av-
erage lookup time is O
(
m
m?1
)
and, crucially, con-
stant in the number of entries.
When keys are longer than 64 bits, we conserve
space by replacing the keys with their 64-bit hashes.
With a good hash function, collisions of the full 64-
bit hash are exceedingly rare: one in 266 billion
queries for our baseline model will falsely find a key
not present. Collisions between two keys in the table
can be identified at model building time. Further, the
special hash 0 suffices to flag empty buckets.
The PROBING data structure is a rather straight-
forward application of these hash tables to store N -
gram language models. Unigram lookup is dense so
we use an array of probability and backoff values.
For 2 ? n ? N , we use a hash table mapping from
the n-gram to the probability and backoff3. Vocab-
ulary lookup is a hash table mapping from word to
vocabulary index. In all cases, the key is collapsed
to its 64-bit hash. Given counts cn1 where e.g. c1 is
the vocabulary size, total memory consumption, in
bits, is
(96m+ 64)c1 + 128m
N?1?
n=2
cn + 96mcN .
Our PROBING data structure places all n-grams
of the same order into a single giant hash table.
This differs from other implementations (Stolcke,
2002; Pauls and Klein, 2011) that use hash tables
as nodes in a trie, as explained in the next section.
Our implementation permits jumping to any n-gram
of any length with a single lookup; this appears to
be unique among language model implementations.
2.2 Sorted Arrays and TRIE
Sorted arrays store key-value pairs in an array sorted
by key, incurring no space overhead. SRILM?s com-
pact variant, IRSTLM, MITLM, and BerkeleyLM?s
3N -grams do not have backoff so none is stored.
188
sorted variant are all based on this technique. Given
a sorted array A, these other packages use binary
search to find keys in O(log |A|) time. We re-
duce this to O(log log |A|) time by evenly distribut-
ing keys over their range then using interpolation
search4 (Perl et al, 1978). Interpolation search for-
malizes the notion that one opens a dictionary near
the end to find the word ?zebra.? Initially, the algo-
rithm knows the array begins at b ? 0 and ends at
e? |A|?1. Given a key k, it estimates the position
pivot?
k ?A[b]
A[e]?A[b]
(e? b).
If the estimate is exact (A[pivot] = k), then the al-
gorithm terminates succesfully. If e < b then the
key is not found. Otherwise, the scope of the search
problem shrinks recursively: if A[pivot] < k then
this becomes the new lower bound: l ? pivot; if
A[pivot] > k then u ? pivot. Interpolation search
is therefore a form of binary search with better esti-
mates informed by the uniform key distribution.
If the key distribution?s range is also known (i.e.
vocabulary identifiers range from 0 to the number
of words), then interpolation search can use this in-
formation instead of reading A[0] and A[|A| ? 1] to
estimate pivots; this optimization alone led to a 24%
speed improvement. The improvement is due to the
cost of bit-level reads and avoiding reads that may
fall in different virtual memory pages.
Vocabulary lookup is a sorted array of 64-bit word
hashes. The index in this array is the vocabulary
identifier. This has the effect of randomly permuting
vocabulary identifiers, meeting the requirements of
interpolation search when vocabulary identifiers are
used as keys.
While sorted arrays could be used to implement
the same data structure as PROBING, effectively
making m = 1, we abandoned this implementation
because it is slower and larger than a trie implemen-
tation. The trie data structure is commonly used for
language modeling. Our TRIE implements the pop-
ular reverse trie, in which the last word of an n-gram
is looked up first, as do SRILM, IRSTLM?s inverted
variant, and BerkeleyLM except for the scrolling
variant. Figure 1 shows an example. Nodes in the
4Not to be confused with interpolating probabilities, which
is outside the scope of this paper.
Australia <s>
are
one
are
is Australia
is Australia <s>
<s>
of one
are
is
Figure 1: Lookup of ?is one of? in a reverse trie. Children
of each node are sorted by vocabulary identifier so order
is consistent but not alphabetical: ?is? always appears be-
fore ?are?. Nodes are stored in column-major order. For
example, nodes corresponding to these n-grams appear in
this order: ?are one?, ?<s> Australia?, ?is one of?, ?are
one of?, ?<s> Australia is?, and ?Australia is one?.
trie are based on arrays sorted by vocabulary identi-
fier.
We maintain a separate array for each length n
containing all n-gram entries sorted in suffix order.
Therefore, for n-gram wn1 , all leftward extensions
wn0 are an adjacent block in the n + 1-gram array.
The record for wn1 stores the offset at which its ex-
tensions begin. Reading the following record?s off-
set indicates where the block ends. This technique
was introduced by Clarkson and Rosenfeld (1997)
and is also implemented by IRSTLM and Berke-
leyLM?s compressed option. SRILM inefficiently
stores 64-bit pointers.
Unigram records store probability, backoff, and
an index in the bigram table. Entries for 2 ? n < N
store a vocabulary identifier, probability, backoff,
and an index into the n+1-gram table. The highest-
order N -gram array omits backoff and the index,
since these are not applicable. Values in the trie are
minimally sized at the bit level, improving memory
consumption over trie implementations in SRILM,
IRSTLM, and BerkeleyLM. Given n-gram counts
{cn}Nn=1, we use dlog2 c1e bits per vocabulary iden-
tifier and dlog2 cne per index into the table of n-
grams.
When SRILM estimates a model, it sometimes re-
moves n-grams but not n + 1-grams that extend it
to the left. In a model we built with default set-
tings, 1.2% of n + 1-grams were missing their n-
189
gram suffix. This causes a problem for reverse trie
implementations, including SRILM itself, because it
leaves n+1-grams without an n-gram node pointing
to them. We resolve this problem by inserting an en-
try with probability set to an otherwise-invalid value
(??). Queries detect the invalid probability, using
the node only if it leads to a longer match. By con-
trast, BerkeleyLM?s hash and compressed variants
will return incorrect results based on an n? 1-gram.
2.2.1 Quantization
Floating point values may be stored in the trie ex-
actly, using 31 bits for non-positive log probability
and 32 bits for backoff5. To conserve memory at
the expense of accuracy, values may be quantized
using q bits per probability and r bits per backoff6.
We allow any number of bits from 2 to 25, unlike
IRSTLM (8 bits) and BerkeleyLM (17?20 bits). To
quantize, we use the binning method (Federico and
Bertoldi, 2006) that sorts values, divides into equally
sized bins, and averages within each bin. The cost
of storing these averages, in bits, is
[32(N ? 1)2q + 32(N ? 2)2r
Because there are comparatively few unigrams,
we elected to store them byte-aligned and unquan-
tized, making every query faster. Unigrams also
have 64-bit overhead for vocabulary lookup. Using
cn to denote the number of n-grams, total memory
consumption of TRIE, in bits, is
(32 + 32 + 64 + 64)c1+
N?1?
n=2
(dlog2 c1e+ q + r + dlog2 cn+1e)cn+
(dlog2 c1e+ q)cN
plus quantization tables, if used. The size of TRIE
is particularly sensitive to dlog2 c1e, so vocabulary
filtering is quite effective at reducing model size.
3 Related Work
SRILM (Stolcke, 2002) is widely used within
academia. It is generally considered to be fast (Pauls
5Backoff ?penalties? are occasionally positive in log space.
6One probability is reserved to mark entries that SRILM
pruned. Two backoffs are reserved for Section 4.1. That leaves
2q ? 1 probabilities and 2r ? 2 non-zero backoffs.
and Klein, 2011), with a default implementation
based on hash tables within each trie node. Each trie
node is individually allocated and full 64-bit point-
ers are used to find them, wasting memory. The
compact variant uses sorted arrays instead of hash
tables within each node, saving some memory, but
still stores full 64-bit pointers. With some minor API
changes, namely returning the length of the n-gram
matched, it could also be faster?though this would
be at the expense of an optimization we explain in
Section 4.1. The PROBING model was designed
to improve upon SRILM by using linear probing
hash tables (though not arranged in a trie), allocat-
ing memory all at once (eliminating the need for full
pointers), and being easy to compile.
IRSTLM (Federico et al, 2008) is an open-source
toolkit for building and querying language models.
The developers aimed to reduce memory consump-
tion at the expense of time. Their default variant im-
plements a forward trie, in which words are looked
up in their natural left-to-right order. However, their
inverted variant implements a reverse trie using less
CPU and the same amount of memory7. Each trie
node contains a sorted array of entries and they use
binary search. Compared with SRILM, IRSTLM
adds several features: lower memory consumption,
a binary file format with memory mapping, caching
to increase speed, and quantization. Our TRIE im-
plementation is designed to improve upon IRSTLM
using a reverse trie with improved search, bit level
packing, and stateful queries. IRSTLM?s quantized
variant is the inspiration for our quantized variant.
Unfortunately, we were unable to correctly run the
IRSTLM quantized variant. The developers sug-
gested some changes, such as building the model
from scratch with IRSTLM, but these did not resolve
the problem.
Our code has been publicly available and inter-
grated into Moses since October 2010. Later, Berke-
leyLM (Pauls and Klein, 2011) described ideas sim-
ilar to ours. Most similar is scrolling queries,
wherein left-to-right queries that add one word at
a time are optimized. Both implementations em-
ploy a state object, opaque to the application, that
carries information from one query to the next; we
7Forward tries are faster to build with IRSTLM and can effi-
ciently return a list of rightward extensions, but this is not used
by the decoders we consider.
190
discuss both further in Section 4.2. State is imple-
mented in their scrolling variant, which is a trie an-
notated with forward and backward pointers. The
hash variant is a reverse trie with hash tables, a
more memory-efficient version of SRILM?s default.
While the paper mentioned a sorted variant, code
was never released. The compressed variant uses
block compression and is rather slow as a result. A
direct-mapped cache makes BerkeleyLM faster on
repeated queries, but their fastest (scrolling) cached
version is still slower than uncached PROBING, even
on cache-friendly queries. For all variants, we found
that BerkeleyLM always rounds the floating-point
mantissa to 12 bits then stores indices to unique
rounded floats. The 1-bit sign is almost always neg-
ative and the 8-bit exponent is not fully used on the
range of values, so in practice this corresponds to
quantization ranging from 17 to 20 total bits.
Lossy compressed models RandLM (Talbot and
Osborne, 2007) and Sheffield (Guthrie and Hepple,
2010) offer better memory consumption at the ex-
pense of CPU and accuracy. These enable much
larger models in memory, compensating for lost
accuracy. Typical data structures are generalized
Bloom filters that guarantee a customizable prob-
ability of returning the correct answer. Minimal
perfect hashing is used to find the index at which
a quantized probability and possibly backoff are
stored. These models generally outperform our
memory consumption but are much slower, even
when cached.
4 Optimizations
In addition to the optimizations specific to each data-
structure described in Section 2, we implement sev-
eral general optimizations for language modeling.
4.1 Minimizing State
Applications such as machine translation use lan-
guage model probability as a feature to assist in
choosing between hypotheses. Dynamic program-
ming efficiently scores many hypotheses by exploit-
ing the fact that an N -gram language model condi-
tions on at most N ? 1 preceding words. We call
these N ? 1 words state. When two partial hy-
potheses have equal state (including that of other
features), they can be recombined and thereafter ef-
ficiently handled as a single packed hypothesis. If
there are too many distinct states, the decoder prunes
low-scoring partial hypotheses, possibly leading to a
search error. Therefore, we want state to encode the
minimum amount of information necessary to prop-
erly compute language model scores, so that the de-
coder will be faster and make fewer search errors.
We offer a state function s(wn1 ) = w
n
m where
substring wnm is guaranteed to extend (to the right)
in the same way that wn1 does for purposes of
language modeling. The state function is inte-
grated into the query process so that, in lieu of
the query p(wn|w
n?1
1 ), the application issues query
p(wn|s(w
n?1
1 )) which also returns s(w
n
1 ). The re-
turned state s(wn1 ) may then be used in a follow-
on query p(wn+1|s(wn1 )) that extends the previous
query by one word. These make left-to-right query
patterns convenient, as the application need only
provide a state and the word to append, then use the
returned state to append another word, etc. We have
modified Moses (Koehn et al, 2007) to keep our
state with hypotheses; to conserve memory, phrases
do not keep state. Syntactic decoders, such as cdec
(Dyer et al, 2010), build state from null context then
store it in the hypergraph node for later extension.
Language models that contain wk1 must also con-
tain prefixes wi1 for 1 ? i ? k. Therefore, when
the model is queried for p(wn|w
n?1
1 ) but the longest
matching suffix is wnf , it may return state s(w
n
1 ) =
wnf since no longer context will be found. IRSTLM
and BerkeleyLM use this state function (and a limit
of N ? 1 words), but it is more strict than necessary,
so decoders using these packages will miss some re-
combination opportunities.
State will ultimately be used as context in a sub-
sequent query. If the context wnf will never extend to
the right (i.e. wnf v is not present in the model for all
words v) then no subsequent query will match the
full context. If the log backoff of wnf is also zero
(it may not be in filtered models), then wf should
be omitted from the state. This logic applies recur-
sively: if wnf+1 similarly does not extend and has
zero log backoff, it too should be omitted, termi-
nating with a possibly empty context. We indicate
whether a context with zero log backoff will extend
using the sign bit: +0.0 for contexts that extend and
?0.0 for contexts that do not extend. RandLM and
SRILM also remove context that will not extend, but
191
SRILM performs a second lookup in its trie whereas
our approach has minimal additional cost.
4.2 Storing Backoff in State
Section 4.1 explained that state s is stored by appli-
cations with partial hypotheses to determine when
they can be recombined. In this section, we ex-
tend state to optimize left-to-right queries. All lan-
guage model queries issued by machine translation
decoders follow a left-to-right pattern, starting with
either the begin of sentence token or null context for
mid-sentence fragments. Storing state therefore be-
comes a time-space tradeoff; for example, we store
state with partial hypotheses in Moses but not with
each phrase.
To optimize left-to-right queries, we extend state
to store backoff information:
s(wn?11 ) =
(
wn?1m ,
{
b(wn?1i )
}n?1
i=m
)
where m is the minimal context from Section 4.1
and b is the backoff penalty. Because b is a function,
no additional hypothesis splitting happens.
As noted in Section 1, our code finds the longest
matching entry wnf for query p(wn|s(w
n?1
1 )) then
computes
p(wn|w
n?1
1 ) = p(wn|w
n?1
f )
f?1?
i=1
b(wn?1i ).
The probability p(wn|w
n?1
f ) is stored with w
n
f and
the backoffs are immediately accessible in the pro-
vided state s(wn?11 ).
When our code walks the data structure to find
wnf , it visits w
n
n, w
n
n?1, . . . , w
n
f . Each visited entry
wni stores backoff b(w
n
i ). These are written to the
state s(wn1 ) and returned so that they can be used for
the following query.
Saving state allows our code to walk the data
structure exactly once per query. Other packages
walk their respective data structures once to find wnf
and again to find {b(wn?1i )}
f?1
i=1 if necessary. In
both cases, SRILM walks its trie an additional time
to minimize context as mentioned in Section 4.1.
BerkeleyLM uses states to optimistically search
for longer n-gram matches first and must perform
twice as many random accesses to retrieve back-
off information. Further, it needs extra pointers
in the trie, increasing model size by 40%. This
makes memory usage comparable to our PROBING
model. The PROBING model can perform optimistic
searches by jumping to any n-gram without needing
state and without any additional memory. However,
this optimistic search would not visit the entries nec-
essary to store backoff information in the outgoing
state. Though we do not directly compare state im-
plementations, performance metrics in Table 1 indi-
cate our overall method is faster.
4.3 Threading
Only IRSTLM does not support threading. In our
case multi-threading is trivial because our data struc-
tures are read-only and uncached. Memory mapping
also allows the same model to be shared across pro-
cesses on the same machine.
4.4 Memory Mapping
Along with IRSTLM and TPT, our binary format is
memory mapped, meaning the file and in-memory
representation are the same. This is especially effec-
tive at reducing load time, since raw bytes are read
directly to memory?or, as happens with repeatedly
used models, are already in the disk cache.
Lazy mapping reduces memory requirements by
loading pages from disk only as necessary. How-
ever, lazy mapping is generally slow because queries
against uncached pages must wait for the disk. This
is especially bad with PROBING because it is based
on hashing and performs random lookups, but it
is not intended to be used in low-memory scenar-
ios. TRIE uses less memory and has better locality.
However, TRIE partitions storage by n-gram length,
so walking the trie reads N disjoint pages. TPT
has theoretically better locality because it stores n-
grams near their suffixes, thereby placing reads for a
single query in the same or adjacent pages.
We do not experiment with models larger than
physical memory in this paper because TPT is un-
released, factors such as disk speed are hard to repli-
cate, and in such situations we recommend switch-
ing to a more compact representation, such as Ran-
dLM. In all of our experiments, the binary file
(whether mapped or, in the case of most other pack-
ages, interpreted) is loaded into the disk cache in ad-
vance so that lazy mapping will never fault to disk.
This is similar to using the Linux MAP POPULATE
192
110
100
10 1000 100000 107
L
oo
ku
ps
/?
s
Entries
probing
hash set
unordered
interpolation
binary search
set
Figure 2: Speed in lookups per microsecond by data
structure and number of 64-bit entries. Performance dips
as each data structure outgrows the processor?s 12 MB L2
cache. Among hash tables, indicated by shapes, probing
is initially slower but converges to 43% faster than un-
ordered or hash set. Interpolation search has a more ex-
pensive pivot function but does less reads and iterations,
so it is initially slower than binary search and set, but be-
comes faster above 4096 entries.
flag that is our default loading mechanism.
5 Benchmarks
This section measures performance on shared tasks
in order of increasing complexity: sparse lookups,
evaluating perplexity of a large file, and translation
with Moses. Our test machine has two Intel Xeon
E5410 processors totaling eight cores, 32 GB RAM,
and four Seagate Barracuda disks in software RAID
0 running Linux 2.6.18.
5.1 Sparse Lookup
Sparse lookup is a key subproblem of language
model queries. We compare three hash tables:
our probing implementation, GCC?s hash set, and
Boost?s8 unordered. For sorted lookup, we compare
interpolation search, standard C++ binary search,
and standard C++ set based on red-black trees.
The data structure was populated with 64-bit inte-
gers sampled uniformly without replacement. For
queries, we uniformly sampled 10 million hits and
8http://boost.org
10 million misses. The same numbers were used for
each data structure. Time includes all queries but ex-
cludes random number generation and data structure
population. Figure 2 shows timing results.
For the PROBING implementation, hash table
sizes are in the millions, so the most relevant val-
ues are on the right size of the graph, where linear
probing wins. It also uses less memory, with 8 bytes
of overhead per entry (we store 16-byte entries with
m = 1.5); linked list implementations hash set and
unordered require at least 8 bytes per entry for point-
ers. Further, the probing hash table does only one
random lookup per query, explaining why it is faster
on large data.
Interpolation search has a more expensive pivot
but performs less pivoting and reads, so it is slow on
small data and faster on large data. This suggests
a strategy: run interpolation search until the range
narrows to 4096 or fewer entries, then switch to bi-
nary search. However, reads in the TRIE data struc-
ture are more expensive due to bit-level packing, so
we found that it is faster to use interpolation search
the entire time. Memory usage is the same as with
binary search and lower than with set.
5.2 Perplexity
For the perplexity and translation tasks, we used
SRILM to build a 5-gram English language model
on 834 million tokens from Europarl v6 (Koehn,
2005) and the 2011 Workshop on Machine Trans-
lation News Crawl corpus with duplicate lines re-
moved. The model was built with open vocabulary,
modified Kneser-Ney smoothing, and default prun-
ing settings that remove singletons of order 3 and
higher. Unlike Germann et al (2009), we chose a
model size so that all benchmarks fit comfortably in
main memory. Benchmarks use the package?s bi-
nary format; our code is also the fastest at building a
binary file. As noted in Section 4.4, disk cache state
is controlled by reading the entire binary file before
each test begins. For RandLM, we used the settings
in the documentation: 8 bits per value and false pos-
itive probability 1256 .
We evaluate the time and memory consumption
of each data structure by computing perplexity on
4 billion tokens from the English Gigaword corpus
(Parker et al, 2009). Tokens were converted to vo-
cabulary identifiers in advance and state was carried
193
from each query to the next. Table 1 shows results
of the benchmark. Compared to decoding, this task
is cache-unfriendly in that repeated queries happen
only as they naturally occur in text. Therefore, per-
formance is more closely tied to the underlying data
structure than to the cache. In fact, we found that
enabling IRSTLM?s cache made it slightly slower,
so results in Table 1 use IRSTLM without caching.
Moses sets the cache size parameter to 50 so we did
as well; the resulting cache size is 2.82 GB.
The results in Table 1 show PROBING is 81%
faster than TRIE, which is in turn 31% faster than the
fastest baseline. Memory usage in PROBING is high,
though SRILM is even larger, so where memory is of
concern we recommend using TRIE, if it fits in mem-
ory. For even larger models, we recommend Ran-
dLM; the memory consumption of the cache is not
expected to grow with model size, and it has been
reported to scale well. Another option is the closed-
source data structures from Sheffield (Guthrie and
Hepple, 2010). Though we are not able to calculate
their memory usage on our model, results reported
in their paper suggest lower memory consumption
than TRIE on large-scale models, at the expense of
CPU time.
5.3 Translation
This task measures how well each package performs
in machine translation. We run the baseline Moses
system for the French-English track of the 2011
Workshop on Machine Translation,9 translating the
3003-sentence test set. Based on revision 4041, we
modified Moses to print process statistics before ter-
minating. Process statistics are already collected
by the kernel (and printing them has no meaning-
ful impact on performance). SRILM?s compact vari-
ant has an incredibly expensive destructor, dwarfing
the time it takes to perform translation, and so we
also modified Moses to avoiding the destructor by
calling exit instead of returning normally. Since
our destructor is an efficient call to munmap, by-
passing the destructor favors only other packages.
The binary language model from Section 5.2 and
text phrase table were forced into disk cache before
each run. Time starts when Moses is launched and
therefore includes model loading time. These con-
9http://statmt.org/wmt11/baseline.html
Package Variant Queries/ms RAM (GB)
Ken
PROBING 1818 5.28
TRIE 1139 2.72
TRIE 8 bitsa 1127 1.59
SRI
Default 750 9.19
Compact 238 7.27
IRSTb
Invert 426 2.91
Default 368 2.91
MIT Default 410 7.72+1.34c
Rand Backoff 8 bitsa 56 1.30+2.82c
Berkeley
Hash+Scrolla 913 5.28+2.32d
Hasha 767 3.71+1.72d
Compresseda 126 1.73+0.71d
Estimates for unreleased packages
Sheffield C-MPHRa 607e
TPT Default 357f
Table 1: Single-threaded speed and memory use on the
perplexity task. The PROBING model is fastest by a sub-
stantial margin but generally uses more memory. TRIE is
faster than competing packages and uses less memory than
non-lossy competitors. The timing basis for Queries/ms in-
cludes kernel and user time but excludes loading time; we
also subtracted time to run a program that just reads the
query file. Peak virtual memory is reported; final resident
memory is similar except for BerkeleyLM. We tried both
aggressive reading and lazy memory mapping where appli-
cable, but results were much the same.
aUses lossy compression.
bThe 8-bit quantized variant returned incorrect probabilities as
explained in Section 3. It did 402 queries/ms using 1.80 GB.
cMemory use increased during scoring due to batch processing
(MIT) or caching (Rand). The first value reports use immediately
after loading while the second reports the increase during scoring.
dBerkeleyLM is written in Java which requires memory be
specified in advance. Timing is based on plentiful memory. Then
we ran binary search to determine the least amount of memory
with which it would run. The first value reports resident size af-
ter loading; the second is the gap between post-loading resident
memory and peak virtual memory. The developer explained that
the loading process requires extra memory that it then frees.
eBased on the ratio to SRI?s speed reported in Guthrie and
Hepple (2010) under different conditions. Memory usage is likely
much lower than ours.
fThe original paper (Germann et al, 2009) provided only 2s of
query timing and compared with SRI when it exceeded available
RAM. The authors provided us with a ratio between TPT and SRI
under different conditions.
194
Time (m) RAM (GB)
Package Variant CPU Wall Res Virt
Ken
PROBING-L 72.3 72.4 7.83 7.92
PROBING-P 73.6 74.7 7.83 7.92
TRIE-L 80.4 80.6 4.95 5.24
TRIE-P 80.1 80.1 4.95 5.24
TRIE-L 8a 79.5 79.5 3.97 4.10
TRIE-P 8a 79.9 79.9 3.97 4.10
SRI
Default 85.9 86.1 11.90 11.94
Compact 155.5 155.7 9.98 10.02
IRST
Cache-Invert-L 106.4 106.5 5.36 5.84
Cache-Invert-R 106.7 106.9 5.73 5.84
Invert-L 117.2 117.3 5.27 5.67
Invert-R 117.7 118.0 5.64 5.67
Default-L 126.3 126.4 5.26 5.67
Default-R 127.1 127.3 5.64 5.67
Rand
Backoffa 277.9 278.0 4.05 4.18
Backoffb 247.6 247.8 4.06 4.18
Table 2: Single-threaded time and memory consumption
of Moses translating 3003 sentences. Where applicable,
models were loaded with lazy memory mapping (-L),
prefaulting (-P), and normal reading (-R); results differ
by at most than 0.6 minute.
aLossy compression with the same weights.
bLossy compression with retuned weights.
ditions make the value appropriate for estimating re-
peated run times, such as in parameter tuning. Table
2 shows single-threaded results, mostly for compar-
ison to IRSTLM, and Table 3 shows multi-threaded
results.
Part of the gap between resident and virtual mem-
ory is due to the time at which data was collected.
Statistics are printed before Moses exits and after
parts of the decoder have been destroyed. Moses
keeps language models and many other resources in
static variables, so these are still resident in mem-
ory. Further, we report current resident memory and
peak virtual memory because these are the most ap-
plicable statistics provided by the kernel.
Overall, language modeling significantly impacts
decoder performance. In line with perplexity results
from Table 1, the PROBING model is the fastest fol-
lowed by TRIE, and subsequently other packages.
We incur some additional memory cost due to stor-
ing state in each hypothesis, though this is minimal
compared with the size of the model itself. The
TRIE model continues to use the least memory of
Time (m) RAM (GB)
Package Variant CPU Wall Res Virt
Ken
PROBING-L 130.4 20.2 7.91 8.53
PROBING-P 132.6 21.7 7.91 8.41
TRIE-L 132.1 20.6 5.03 5.85
TRIE-P 132.2 20.5 5.02 5.84
TRIE-L 8a 137.1 21.2 4.03 4.60
TRIE-P 8a 134.6 20.8 4.03 4.72
SRI
Default 153.2 26.0 11.97 12.56
Compact 243.3 36.9 10.05 10.55
Rand
Backoffa 346.8 49.4 5.41 6.78
Backoffb 308.7 44.4 5.26 6.81
Table 3: Multi-threaded time and memory consumption
of Moses translating 3003 sentences on eight cores. Our
code supports lazy memory mapping (-L) and prefault-
ing (-P) with MAP POPULATE, the default. IRST is not
threadsafe. Time for Moses itself to load, including load-
ing the language model and phrase table, is included.
Along with locking and background kernel operations
such as prefaulting, this explains why wall time is not
one-eighth that of the single-threaded case.
aLossy compression with the same weights.
bLossy compression with retuned weights.
the non-lossy options. For RandLM and IRSTLM,
the effect of caching can be seen on speed and mem-
ory usage. This is most severe with RandLM in
the multi-threaded case, where each thread keeps a
separate cache, exceeding the original model size.
As noted for the perplexity task, we do not ex-
pect cache to grow substantially with model size, so
RandLM remains a low-memory option. Caching
for IRSTLM is smaller at 0.09 GB resident mem-
ory, though it supports only a single thread. The
BerkeleyLM direct-mapped cache is in principle
faster than caches implemented by RandLM and by
IRSTLM, so we may write a C++ equivalent imple-
mentation as future work.
5.4 Comparison with RandLM
RandLM?s stupid backoff variant stores counts in-
stead of probabilities and backoffs. It also does not
prune, so comparing to our pruned model would
be unfair. Using RandLM and the documented
settings (8-bit values and 1256 false-positive prob-
ability), we built a stupid backoff model on the
same data as in Section 5.2. We used this data
to build an unpruned ARPA file with IRSTLM?s
195
RAM (GB)
Pack Variant Time (m) Res Virt BLEU
Ken
TRIE 82.9 12.16 14.39 27.24
TRIE 8 bits 82.7 8.41 9.41 27.22
TRIE 4 bits 83.2 7.74 8.55 27.09
Rand
Stupid 8 bits 218.7 5.07 5.18 25.54
Backoff 8 bits 337.4 7.17 7.28 25.45
Table 4: CPU time, memory usage, and uncased BLEU
(Papineni et al, 2002) score for single-threaded Moses
translating the same test set. We ran each lossy model
twice: once with specially-tuned weights and once with
weights tuned using an exact model. The difference in
BLEU was minor and we report the better result.
improved-kneser-ney option and the default
three pieces. Table 4 shows the results. We elected
run Moses single-threaded to minimize the impact
of RandLM?s cache on memory use. RandLM is the
clear winner in RAM utilization, but is also slower
and lower quality. However, the point of RandLM
is to scale to even larger data, compensating for this
loss in quality.
6 Future Work
There any many techniques for improving language
model speed and reducing memory consumption.
For speed, we plan to implement the direct-mapped
cache from BerkeleyLM. Much could be done to fur-
ther reduce memory consumption. Raj and Whit-
taker (2003) show that integers in a trie implemen-
tation can be compressed substantially. Quantiza-
tion can be improved by jointly encoding probability
and backoff. For even larger models, storing counts
(Talbot and Osborne, 2007; Pauls and Klein, 2011;
Guthrie and Hepple, 2010) is a possibility. Beyond
optimizing the memory size of TRIE, there are alter-
native data structures such as those in Guthrie and
Hepple (2010). Finally, other packages implement
language model estimation while we are currently
dependent on them to generate an ARPA file.
While we have minimized forward-looking state
in Section 4.1, machine translation systems could
also benefit by minimizing backward-looking state.
For example, syntactic decoders (Koehn et al, 2007;
Dyer et al, 2010; Li et al, 2009) perform dynamic
programming parametrized by both backward- and
forward-looking state. If they knew that the first four
words in a hypergraph node would never extend to
the left and form a 5-gram, then three or even fewer
words could be kept in the backward state. This in-
formation is readily available in TRIE where adja-
cent records with equal pointers indicate no further
extension of context is possible. Exposing this in-
formation to the decoder will lead to better hypoth-
esis recombination. Generalizing state minimiza-
tion, the model could also provide explicit bounds
on probability for both backward and forward ex-
tension. This would result in better rest cost esti-
mation and better pruning.10 In general, tighter, but
well factored, integration between the decoder and
language model should produce a significant speed
improvement.
7 Conclusion
We have described two data structures for language
modeling that achieve substantial reductions in time
and memory cost. The PROBING model is 2.4
times as fast as the fastest alternative, SRILM, and
uses less memory too. The TRIE model uses less
memory than the smallest lossless alternative and is
still faster than SRILM. These performance gains
transfer to improved system runtime performance;
though we focused on Moses, our code is the best
lossless option with cdec and Joshua. We attain
these results using several optimizations: hashing,
custom lookup tables, bit-level packing, and state
for left-to-right query patterns. The code is open-
source, has minimal dependencies, and offers both
C++ and Java interfaces for integration.
Acknowledgments
Alon Lavie advised on this work. Hieu Hoang
named the code ?KenLM? and assisted with Moses
along with Barry Haddow. Adam Pauls provided a
pre-release comparison to BerkeleyLM and an initial
Java interface. Nicola Bertoldi and Marcello Fed-
erico assisted with IRSTLM. Chris Dyer integrated
the code into cdec. Juri Ganitkevitch answered ques-
tions about Joshua. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship under Grant No.
0750271 and by the DARPA GALE program.
10One issue is efficient retrieval of bounds, though these
could be quantized, rounded in the safe direction, and stored
with each record.
196
References
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge toolkit.
In Proceedings of Eurospeech.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of the Workshop on
Statistical Machine Translation, pages 94?101, New
York City, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Proceedings of Inter-
speech, Brisbane, Australia.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly packed tries: How to fit large models into
memory, and make them load fast, too. In Proceedings
of the NAACL HLT Workshop on Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, pages 31?39, Boulder, Colorado.
David Guthrie and Mark Hepple. 2010. Storing the web
in memory: Space efficient language models with con-
stant time retrieval. In Proceedings of EMNLP 2010,
Los Angeles, CA.
Bo-June Hsu and James Glass. 2008. Iterative lan-
guage model estimation: Efficient data structure & al-
gorithms. In Proceedings of Interspeech, Brisbane,
Australia.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135?139, Athens, Greece,
March. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English gigaword fourth edi-
tion. LDC2009T13.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL, Port-
land, Oregon.
Yehoshua Perl, Alon Itai, and Haim Avni. 1978. Inter-
polation search?a log log N search. Commun. ACM,
21:550?553, July.
Bhiksha Raj and Ed Whittaker. 2003. Lossless compres-
sion of language model structure and word identifiers.
In Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 388?
391.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL, pages 512?519, Prague, Czech
Republic.
197
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114?121,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Edinburgh?s Machine Translation Systems for European Language Pairs
Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn
School of Informatics
University of Edinburgh
Scotland, United Kingdom
{dnadir,bhaddow,kheafiel,pkoehn}@inf.ed.ac.uk
Abstract
We validated various novel and recently
proposed methods for statistical machine
translation on 10 language pairs, using
large data resources. We saw gains
from optimizing parameters, training with
sparse features, the operation sequence
model, and domain adaptation techniques.
We also report on utilizing a huge lan-
guage model trained on 126 billion tokens.
The annual machine translation evaluation cam-
paign for European languages organized around
the ACL Workshop on Statistical Machine Trans-
lation offers the opportunity to test recent advance-
ments in machine translation in large data condi-
tion across several diverse language pairs.
Building on our own developments and external
contributions to the Moses open source toolkit, we
carried out extensive experiments that, by early in-
dications, led to a strong showing in the evaluation
campaign.
We would like to stress especially two contri-
butions: the use of the new operation sequence
model (Section 3) within Moses, and ? in a sepa-
rate unconstraint track submission ? the use of a
huge language model trained on 126 billion tokens
with a new training tool (Section 4).
1 Initial System Development
We start with systems (Haddow and Koehn, 2012)
that we developed for the 2012 Workshop on
Statistical Machine Translation (Callison-Burch
et al, 2012). The notable features of these systems
are:
? Moses phrase-based models with mostly de-
fault settings
? training on all available parallel data, includ-
ing the large UN parallel data, the French-
English 109 parallel data and the LDC Giga-
word data
? very large tuning set consisting of the test sets
from 2008-2010, with a total of 7,567 sen-
tences per language
? German?English with syntactic pre-
reordering (Collins et al, 2005), compound
splitting (Koehn and Knight, 2003) and use
of factored representation for a POS target
sequence model (Koehn and Hoang, 2007)
? English?German with morphological target
sequence model
Note that while our final 2012 systems in-
cluded subsampling of training data with modified
Moore-Lewis filtering (Axelrod et al, 2011), we
did not use such filtering at the starting point of
our development. We will report on such filtering
in Section 2.
Moreover, our system development initially
used the WMT 2012 data condition, since it took
place throughout 2012, and we switched to WMT
2013 training data at a later stage. In this sec-
tion, we report cased BLEU scores (Papineni et al,
2001) on newstest2011.
1.1 Factored Backoff (German?English)
We have consistently used factored models in past
WMT systems for the German?English language
pairs to include POS and morphological target se-
quence models. But we did not use the factored
decomposition of translation options into multi-
ple mapping steps, since this usually lead to much
slower systems with usually worse results.
A good place, however, for factored decompo-
sition is the handling of rare and unknown source
words which have more frequent morphological
variants (Koehn and Haddow, 2012a). Here, we
used only factored backoff for unknown words,
giving gains in BLEU of +.12 for German?English.
1.2 Tuning with k-best MIRA
In preparation for training with sparse features, we
moved away from MERT which is known to fall
114
apart with many more than a couple of dozen fea-
tures. Instead, we used k-best MIRA (Cherry and
Foster, 2012). For the different language pairs, we
saw improvements in BLEU of -.05 to +.39, with an
average of +.09. There was only a minimal change
in the length ratio (Table 1)
MERT k-best MIRA ?
de-en 22.11 (1.010) 22.10 (1.008) ?.01 (+.002)
fr-en 30.00 (1.023) 30.11 (1.026) +.11 (?.003)
es-en 30.42 (1.021) 30.63 (1.020) +.21 (?.001)
cs-en 25.54 (1.022) 25.49 (1.024) ?.05 (?.002)
en-de 16.08 (0.995) 16.04 (1.001) ?.04 (?.006)
en-fr 29.26 (0.980) 29.65 (0.982) +.39 (?.002)
en-es 31.92 (0.985) 31.95 (0.985) +.03 (?.000)
en-cs 17.38 (0.967) 17.42 (0.974) +.04 (?.007)
avg ? ? +.09
Table 1: Tuning with k-best MIRA instead of MERT
(cased BLEU scores with length ratio)
1.3 Translation Table Smoothing with
Kneser-Ney Discounting
Previously, we smoothed counts for the phrasal
conditional probability distributions in the trans-
lation model with Good Turing discounting. We
explored the use of Kneser-Ney discounting, but
results are mixed (no difference on average, see
Table 2), so we did not pursue this further.
Good Turing Kneser Ney ?
de-en 22.10 22.15 +.05
fr-en 30.11 30.13 +.02
es-en 30.63 30.64 +.01
cs-en 25.49 25.56 +.07
en-de 16.04 15.93 ?.11
en-fr 29.65 29.75 +.10
en-es 31.95 31.98 +.03
en-cs 17.42 17.26 ?.16
avg ? ? ?.00
Table 2: Translation model smoothing with Kneser-Ney
1.4 Sparse Features
A significant extension of the Moses system over
the last couple of years was the support for large
numbers of sparse features. This year, we tested
this capability on our big WMT systems. First, we
used features proposed by Chiang et al (2009):
? phrase pair count bin features (bins 1, 2, 3,
4?5, 6?9, 10+)
? target word insertion features
? source word deletion features
? word translation features
? phrase length feature (source, target, both)
The lexical features were restricted to the 50 most
frequent words. All these features together only
gave minor improvements (Table 3).
baseline sparse ?
de-en 22.10 22.02 ?.08
fr-en 30.11 30.24 +.13
es-en 30.63 30.61 ?.02
cs-en 25.49 25.49 ?.00
en-de 16.04 15.93 ?.09
en-fr 29.65 29.81 +.16
en-es 31.95 32.02 +.07
en-cs 17.42 17.28 ?.14
avg ? ? +.04
Table 3: Sparse features
We also explored domain features in the sparse
feature framework, in three different variations.
Assume that we have three domains, and a phrase
pair occurs in domain A 15 times, in domain B 5
times, and in domain C never.
We compute three types of domain features:
? binary indicator, if phrase-pairs occurs in do-
main (example: indA = 1, indB = 1, indC = 0)
? ratio how frequent the phrase pairs occurs in
domain (example: ratioA = 1515+5 = .75, ratioB =
5
15+5 = .25, ratioC = 0)
? subset of domains in which phrase pair oc-
curs (example: subsetAB = 1, other subsets 0)
We tested all three feature types, and found
the biggest gain with the domain indicator feature
(+.11, Table 4). Note that we define as domain the
different corpora (Europarl, etc.). The number of
domains ranges from 2 to 9 (see column #d).1
#d base. indicator ratio subset
de-en 2 22.10 22.14 +.04 22.07 ?.03 22.12 +.02
fr-en 4 30.11 30.34 +.23 30.29 +.18 30.15 +.04
es-en 3 30.63 30.88 +.25 30.64 +.01 30.82 +.19
cs-en 9 25.49 25.58 +.09 25.58 +.09 25.46 ?.03
en-de 2 16.122 16.14 +.02 15.96 ?.16 16.01 ?.11
en-fr 4 29.65 29.75 +.10 29.71 +.05 29.70 +.05
en-es 3 31.95 32.06 +.11 32.13 +.18 32.02 +.07
en-cs 9 17.42 17.45 +.03 17.35 ?.07 17.44 +.02
avg. - ? +.11 +.03 +.03
Table 4: Sparse domain features
When combining the domain features and the
other sparse features, we see roughly additive
gains (Table 5). We use the domain indicator fea-
ture and the other sparse features in subsequent ex-
periments.
1In the final experiments on the 2013 data condition, one
domain (commoncrawl) was added for all language pairs.
115
baseline indicator ratio subset
de-en 22.10 22.18 +.08 22.10 ?.00 22.16 +.06
fr-en 30.11 30.41 +.30 30.49 +.38 30.36 +.25
es-en 30.63 30.75 +.12 30.56 ?.07 30.85 +.22
cs-en 25.49 25.56 +.07 25.63 +.14 25.43 ?.06
en-de 16.12 15.95 ?.17 15.96 ?.16 16.05 ?.07
en-fr 29.65 29.96 +.31 29.88 +.23 29.92 +.27
en-es 31.95 32.12 +.17 32.16 +.21 32.08 +.23
en-cs 17.42 17.38 ?.04 17.35 ?.07 17.40 ?.02
avg. ? +.11 +.09 +.11
Table 5: Combining domain and other sparse features
1.5 Tuning Settings
Given the opportunity to explore the parameter
tuning of models with sparse features across many
language pairs, we investigated a number of set-
tings. We expect tuning to work better with more
iterations, longer n-best lists and bigger cube prun-
ing pop limits. Our baseline settings are 10 itera-
tions with 100-best lists (accumulating) and a pop
limit of 1000 for tuning and 5000 for testing.
base 25 it. 25it+1k-best 25it+pop5k
de-en 22.18 22.16 ?.02 22.14 ?.04 22.17 ?.01
fr-en 30.41 30.40 ?.01 30.44 +.03 30.49 +.08
es-en 30.75 30.91 +.16 30.86 +.11 30.81 +.06
cs-en 25.56 25.60 +.04 25.64 +.08 25.56 ?.00
en-de 15.96 15.99 +.03 16.05 +.09 15.96 ?.00
en-fr 29.96 29.90 ?.06 29.95 ?.01 29.92 ?.04
en-es 32.12 32.17 +.05 32.11 ?.01 32.19 +.07
en-cs 17.38 17.43 +.05 17.50 +.12 17.38 ?.00
avg ? +.03 +.05 +.02
Table 6: Tuning settings (number of iterations, size of n-best
list, and cube pruning pop limit)
Results support running tuning for 25 iterations
but we see no gains for 5000 pops. There is ev-
idence that an n-best list size of 1000 is better in
tuning but we did not adopt this since these large
lists take up a lot of disk space and slow down the
MIRA optimization step (Table 6).
1.6 Smaller Phrases
Given the very large corpus sizes (up to a billion
words of parallel data for French?English), the
size of translation model and lexicalized reorder-
ing model becomes a challenge. Hence, we want
to examine if restriction to smaller phrases is fea-
sible without loss in translation quality. Results
in Table 7 suggest that a maximum phrase length
of 5 gives almost identical results, and only with
a phrase length limit of 4 significant losses occur.
We adopted the limit of 5.
max 7 max 6 max 5 max 4
de-en 22.16 22.03 ?.13 22.05 ?.11 22.17 +.01
fr-en 30.40 30.30 ?.10 30.39 ?.01 30.23 ?.17
es-en 30.91 30.80 ?.09 30.86 ?.05 30.81 ?.10
cs-en 25.60 25.55 ?.05 25.53 ?.07 25.48 ?.12
en-de 15.99 15.94 ?.05 15.97 ?.02 16.03 +.04
en-fr 29.90 29.97 +.07 29.89 ?.01 29.77 ?.13
en-es 32.17 32.13 ?.04 32.27 +.10 31.93 ?.24
en-cs 17.43 17.46 +.03 17.41 ?.02 17.41 ?.02
avg ? ?.05 ?.03 ?.09
Table 7: Maximum phrase length, reduced from baseline
1.7 Unpruned Language Models
Previously, we trained 5-gram language models
using the default settings of the SRILM toolkit in
terms of singleton pruning. Thus, training throws
out all singletons n-grams of order 3 and higher.
We explored whether unpruned language models
could give better performance, even if we are only
able to train 4-gram models due to memory con-
straints. At the time, we were not able to build un-
pruned 4-gram language models for English, but
for the other language pairs we did see improve-
ments of -.07 to +.13 (Table 8). We adopted such
models for these language pairs.
5g pruned 4g unpruned ?
en-fr 29.89 29.83 ?.07
en-es 32.27 32.34 +.07
en-cs 17.41 17.54 +.13
Table 8: Language models without singleton pruning
1.8 Translations per Input Phrase
Finally, we explored one more parameter: the limit
on how many translation options are considered
per input phrase. The default for this setting is 20.
However, our experiments (Table 9) show that we
can get better results with a translation table limit
of 100, so we adopted this.
ttl 20 ttl 30 ttl 50 ttl 100
de-en 21.05 +.06 +.09 +.01
fr-en 30.39 ?.02 +.05 +.07
es-en 30.86 ?.00 ?.03 ?.07
cs-en 25.53 +.24 +.13 +.20
en-de 15.97 +.03 +.07 +.11
en-fr 29.83 +.14 +.19 +.13
en-es 32.34 +.08 +.10 +.07
en-cs 17.54 ?.05 ?.02 +.01
avg ? +.06 +.07 +.07
Table 9: Maximal number translations per input phrase
1.9 Other Experiments
We explored a number of other settings and fea-
tures, but did not observe any gains.
116
? Using HMM alignment instead of IBM
Model 4 leads to losses of ?.01 to ?.27.
? An earlier check of modified Moore?Lewis
filtering (see also below in Section 3) gave
very inconsistent results.
? Filtering the phrase table with significance
filtering (Johnson et al, 2007) leads to losses
of ?.19 to ?.63.
? Throwing out phrase pairs with direct transla-
tion probability ?(e?|f?) of less than 10?5 has
almost no effect.
? Double-checking the contribution of the
sparse lexical features in the final setup, we
observe an average losses of ?.07 when drop-
ping these features.
? For the German?English language pairs we
saw some benefits to using sparse lexical fea-
tures over POS tags instead of words, so we
used this in the final system.
1.10 Summary
We adopted a number of changes that improved
our baseline system by an average of +.30, see Ta-
ble 10 for a breakdown.
avg. method
+.01 factored backoff
+.09 kbest MIRA
+.11 sparse features and domain indicator
+.03 tuning with 25 iterations
?.03 maximum phrase length 5
+.02 unpruned 4-gram LM
+.07 translation table limit 100
+.30 total
Table 10: Summary of impact of changes
Minor improvements that we did not adopt was
avoiding reducing maximum phrase length to 5
(average +.03) and tuning with 1000-best lists
(+.02).
The improvements differed significantly by lan-
guage pair, as detailed in Table 11, with the
biggest gains for English?French (+.70), no gain
for English?German and no gain for English?
German.
1.11 New Data
The final experiment of the initial system devel-
opment phase was to train the systems on the new
data, adding newstest2011 to the tuning set (now
10,068 sentences). Table 12 reports the gains on
newstest2012 due to added data, indicating very
clearly that valuable new data resources became
available this year.
baseline improved ?
de-en 21.99 22.09 +.10
fr-en 30.00 30.46 +.46
es-en 30.42 30.79 +.37
cs-en 25.54 25.73 +.19
en-de 16.08 16.08 ?.00
en-fr 29.26 29.96 +.70
en-es 31.92 32.41 +.49
en-cs 17.38 17.55 +.17
Table 11: Overall improvements per language pair
WMT 2012 WMT 2013 ?
de-en 23.11 24.01 +0.90
fr-en 29.25 30.77 +1.52
es-en 32.80 33.99 +1.19
cs-en 22.53 22.86 +0.33
ru-en ? 31.67 ?
en-de 16.78 17.95 +1.17
en-fr 27.92 28.76 +0.84
en-es 33.41 34.00 +0.59
en-cs 15.51 15.78 +0.27
en-ru ? 23.78 ?
Table 12: Training with new data (newstest2012 scores)
2 Domain Adaptation Techniques
We explored two additional domain adaptation
techniques: phrase table interpolation and modi-
fied Moore-Lewis filtering.
2.1 Phrase Table Interpolation
We experimented with phrase-table interpolation
using perplexity minimisation (Foster et al, 2010;
Sennrich, 2012). In particular, we used the im-
plementation released with Sennrich (2012) and
available in Moses, comparing both the naive and
modified interpolation methods from that paper.
For each language pair, we took the alignments
created from all the data concatenated, built sepa-
rate phrase tables from each of the individual cor-
pora, and interpolated using each method. The re-
sults are shown in Table 13
baseline naive modified
fr-en 30.77 30.63 ?.14 ?
es-en? 33.98 33.83 ?.15 34.03 +.05
cs-en? 23.19 22.77 ?.42 23.03 ?.17
ru-en 31.67 31.42 ?.25 31.59 ?.08
en-fr 28.76 28.88 +.12 ?
en-es 34.00 34.07 +.07 34.31 +.31
en-cs 15.78 15.88 +.10 15.87 +.09
en-ru 23.78 23.84 +.06 23.68 ?.10
Table 13: Comparison of phrase-table interpolation (two
methods) with baseline (on newstest2012). The baselines are
as Table 12 except for the starred rows where tuning with
PRO was found to be better. The modified interpolation was
not possible in fr?en as it uses to much RAM.
The results from the phrase-table interpolation
are quite mixed, and we only used the technique
117
for the final system in en-es. An interpolation
based on PRO has recently been shown (Haddow,
2013) to improve on perplexity minimisation is
some cases, but the current implementation of this
method is limited to 2 phrase-tables, so we did not
use it in this evaluation.
2.2 Modified Moore-Lewis Filtering
In last year?s evaluation (Koehn and Haddow,
2012b) we had some success with modified
Moore-Lewis filtering (Moore and Lewis, 2010;
Axelrod et al, 2011) of the training data. This
year we conducted experiments in most of the lan-
guage pairs using MML filtering, and also exper-
imented using instance weighting (Mansour and
Ney, 2012) using the (exponential of) the MML
weights. The results are show in Table 14
base MML Inst. Wt Inst. Wt
line 20% (scale)
fr-en 30.77 ? ? ?
es-en? 33.98 34.26 +.28 33.85 ?.13 33.98 ?.00
cs-en? 23.19 22.62 ?.57 23.17 ?.02 23.13 ?.06
ru-en 31.67 31.58 ?.09 31.57 ?.10 31.62 ?.05
en-fr 28.67 28.74 +.07 28.81 +.17 28.63 ?.04
en-es 34.00 34.07 +.07 34.27 +.27 34.03 +.03
en-cs 15.78 15.37 ?.41 15.87 +.09 15.89 +.11
en-ru 23.78 22.90 ?.88 23.82 +.05 23.72 ?.06
Table 14: Comparison of MML filtering and weighting with
baseline. The MML uses monolingual news as in-domain,
and selects from all training data after alignment.The weight-
ing uses the MML weights, optionally downscaled by 10,
then exponentiated. Baselines are as Table 13.
As with phrase-table interpolation, MML filter-
ing and weighting shows a very mixed picture, and
not the consistent improvements these techniques
offer on IWSLT data. In the final systems, we used
MML filtering only for es-en.
3 Operation Sequence Model (OSM)
We enhanced the phrase segmentation and re-
ordering mechanism by integrating OSM: an op-
eration sequence N-gram-based translation and re-
ordering model (Durrani et al, 2011) into the
Moses phrase-based decoder. The model is based
on minimal translation units (MTUs) and Markov
chains over sequences of operations. An opera-
tion can be (a) to jointly generate a bi-language
MTU, composed from source and target words, or
(b) to perform reordering by inserting gaps and do-
ing jumps.
Model: Given a bilingual sentence pair <
F,E > and its alignment A, we transform it to
Figure 1: Bilingual Sentence with Alignments
sequence of operations (o1, o2, . . . , oJ ) and learn
a Markov model over this sequence as:
posm(F,E,A) = p(oJ1 ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision condi-
tions on n ? 1 previous (translation and reorder-
ing) decisions spanning across phrasal boundaries
thus overcoming the problematic phrasal indepen-
dence assumption in the phrase-based model. In
the OSM model, the reordering decisions influ-
ence lexical selection and vice versa. Lexical gen-
eration is strongly coupled with reordering thus
improving the overall reordering mechanism.
We used the modified version of the OSM
model (Durrani et al, 2013b) that addition-
ally handles discontinuous and unaligned target
MTUs3. We borrow 4 count-based supportive fea-
tures, the Gap, Open Gap, Gap-width and Dele-
tion penalties from Durrani et al (2011).
Training: During training, each bilingual sen-
tence pair is deterministically converted to a
unique sequence of operations. Please refer to
Durrani et al (2011) for a list of operations and
the conversion algorithm and see Figure 1 and Ta-
ble 15 for a sample bilingual sentence pair and
its step-wise conversion into a sequence of oper-
ation. A 9-gram Kneser-Ney smoothed operation
sequence model is trained with SRILM.
Search: Although the OSM model is based on
minimal units, phrase-based search on top of OSM
model was found to be superior to the MTU-based
decoding in Durrani et al (2013a). Following this
framework allows us to use OSM model in tandem
with phrase-based models. We integrated the gen-
erative story of the OSM model into the hypothe-
sis extension of the phrase-based Moses decoder.
Please refer to (Durrani et al, 2013b) for details.
Results: Table 16 shows case-sensitive BLEU
scores on newstest2012 and newstest2013 for fi-
3In the original OSM model these are removed from the
alignments through a post-processing heuristic which hurts in
some language pairs. See Durrani et al (2013b) for detailed
experiments.
118
Operation Sequence Generation
Generate(Ich, I) Ich ?
I
Generate Target Only (do) Ich ?
I do
Insert Gap Ich nicht ?
Generate (nicht, not) I do not
Jump Back (1) Ich gehe ? nicht
Generate (gehe, go) I do not go
Generate Source Only (ja) Ich gehe ja ? nicht
I do not go
Jump Forward Ich gehe ja nicht ?
I do not go
Generate (zum, to the) . . . gehe ja nicht zum ?
. . . not go to the
Generate (haus, house) . . . ja nicht zum haus ?
. . . go to the house
Table 15: Step-wise Generation of Figure 1
LP Baseline +OSM
newstest 2012 2013 2012 2013
de-en 23.85 26.54 24.11 +.26 26.83 +.29
fr-en 30.77 31.09 30.96 +.19 31.46 +.37
es-en 34.02 30.04 34.51 +.49 30.94 +.90
cs-en 22.70 25.70 23.03 +.33 25.79 +.09
ru-en 31.87 24.00 32.33 +.46 24.33 +.33
en-de 17.95 20.06 18.02 +.07 20.26 +.20
en-fr 28.76 30.03 29.36 +.60 30.39 +.36
en-es 33.87 29.66 34.44 +.57 30.10 +.44
en-cs 15.81 18.35 16.16 +.35 18.62 +.27
en-ru 23.75 18.44 24.05 +.30 18.84 +.40
Table 16: Results using the OSM Feature
nal systems from Section 1 and these systems aug-
mented with the operation sequence model. The
model gives gains for all language pairs (BLEU
+.09 to +.90, average +.37, on newstest2013).
4 Huge Language Models
To overcome the memory limitations of SRILM,
we implemented modified Kneser-Ney (Kneser
and Ney, 1995; Chen and Goodman, 1998)
smoothing from scratch using disk-based stream-
ing algorithms. This open-source4 tool is de-
scribed fully by Heafield et al (2013). We used it
to estimate an unpruned 5?gram language model
on web pages from ClueWeb09.5 The corpus was
preprocessed by removing spam (Cormack et al,
2011), selecting English documents, splitting sen-
tences, deduplicating, tokenizing, and truecasing.
Estimation on the remaining 126 billion tokens
took 2.8 days on a single machine with 140 GB
RAM (of which 123 GB was used at peak) and six
hard drives in a RAID5 configuration. Statistics
about the resulting model are shown in Table 17.
4http://kheafield.com/code/
5http://lemurproject.org/clueweb09/
1 2 3 4 5
393m 3,775m 17,629m 39,919m 59,794m
Table 17: Counts of unique n-grams (m for millions) for the
5 orders in the unconstrained language model
The large language model was then quantized
to 10 bits and compressed to 643 GB with KenLM
(Heafield, 2011), loaded onto a machine with 1
TB RAM, and used as an additional feature in
unconstrained French?English, Spanish?English,
and Czech?English submissions. This additional
language model is the only difference between our
final constrained and unconstrained submissions;
no additional parallel data was used. Results are
shown in Table 18. Improvement from large lan-
guage models is not a new result (Brants et al,
2007); the primary contribution is estimating on a
single machine.
Constrained Unconstrained ?
fr-en 31.46 32.24 +.78
es-en 30.59 31.37 +.78
cs-en 27.38 28.16 +.78
ru-en 24.33 25.14 +.81
Table 18: Gain on newstest2013 from the unconstrained lan-
guage model. Our time on shared machines with 1 TB is
limited so Russian?English was run after the deadline and
German?English was not ready in time.
5 Summary
Table 19 breaks down the gains over the final sys-
tem from Section 1 from using the operation se-
quence models (OSM), modified Moore-Lewis fil-
tering (MML), fixing a bug with the sparse lex-
ical features (Sparse-Lex Bugfix), and instance
weighting (Instance Wt.), translation model com-
bination (TM-Combine), and use of the huge lan-
guage model (ClueWeb09 LM).
Acknowledgments
Thanks to Miles Osborne for preprocessing the ClueWeb09
corpus. The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE) and grant agreement
288487(MosesCore).This work made use of the resources
provided by the Edinburgh Compute and Data Facility6.
The ECDF is partially supported by the eDIKT initia-
tive7. This work also used the Extreme Science and
Engineering Discovery Environment (XSEDE), which is
supported by National Science Foundation grant number
OCI-1053575. Specifically, Stampede was used under
allocation TG-CCR110017.
6http://www.ecdf.ed.ac.uk/
7http://www.edikt.org.uk/
119
System 2012 2013
Spanish-English
1. Baseline 34.02 30.04
2. 1+OSM 34.51 +.49 30.94 +.90
3. 1+MML (20%) 34.38 +.36 30.38 +.34
4. 1+Sparse-Lex Bugfix 34.17 +.15 30.33 +.29
5. 1+2+3: OSM+MML 34.65 +.63 30.51 +.47
6. 1+2+3+4 34.68 +.66 30.59 +.55
7. 6+ClueWeb09 LM 31.37 +1.33
English-Spanish
1. Baseline 33.87 29.66
2. 1+OSM 34.44 +.57 30.10 +.44
3. 1+TM-Combine 34.31 +44 29.76 +.10
4. 1+Instance Wt. 34.27 +.40 29.63 ?.03
5. 1+Sparse-Lex Bugfix 34.20 +.33 29.86 +.20
6. 1+2+3: OSM+TM-Cmb. 34.63 +.76 30.21 +.55
7. 1+2+4: OSM+Inst. Wt. 34.58 +.71 30.11 +.45
8. 1+2+3+5 34.78 +.91 30.43 +.77
Czech-English
1. Baseline 22.70 25.70
2. 1+OSM 23.03 +.33 25.79 +.09
3. 1+with PRO 23.19 +.49 26.08 +.38
4. 1+Sparse-Lex Bugfix 22.86 +.16 25.74 +.04
5. 1+OSM+PRO 23.42 +.72 26.23 +.53
6. 1+2+3+4 23.16 +.46 25.94 +.24
7. 5+ClueWeb09 LM 27.06 +.36
English-Czech
1. Baseline 15.85 18.35
2. 1+OSM 16.16 +.31 18.62 +.27
French-English
1. Baseline 30.77 31.09
2. 1+OSM 30.96 +.19 31.46 +.37
3. 2+ClueWeb09 LM 32.24 +1.15
English-French
1. Baseline 28.76 30.03
2. 1+OSM 29.36 +.60 30.39 +.36
3. 1+Sparse-Lex Bugfix 28.97 +.21 30.08 +.05
4. 1+2+3 29.37 +.61 30.58 +.55
German-English
1. Baseline 23.85 26.54
2. 1+OSM 24.11 +.26 26.83 +.29
English-German
1. Baseline 17.95 20.06
2. 1+OSM 18.02 +.07 20.26 +.20
Russian-English
1. Baseline 31.87 24.00
2. 1+OSM 32.33 +.46 24.33 +.33
English-Russian
1. Baseline 23.75 18.44
2. 1+OSM 24.05 +.40 18.84 +.40
Table 19: Summary of methods with BLEU scores on news-
test2012 and newstest2013. Bold systems were submitted,
with the ClueWeb09 LM systems submitted in the uncon-
straint track. The German?English and English?German
OSM systems did not complete in time for the official sub-
mission.
References
Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation
via pseudo in-domain data selection. In Proceedings of the
2011 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 355?362, Edinburgh, Scotland,
UK. Association for Computational Linguistics.
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.
(2007). Large language models in machine translation.
In Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-CoNLL),
pages 858?867.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut,
R., and Specia, L. (2012). Findings of the 2012 work-
shop on statistical machine translation. In Proceedings of
the Seventh Workshop on Statistical Machine Translation,
pages 10?48, Montreal, Canada. Association for Compu-
tational Linguistics.
Chen, S. and Goodman, J. (1998). An empirical study of
smoothing techniques for language modeling. Technical
Report TR-10-98, Harvard University.
Cherry, C. and Foster, G. (2012). Batch tuning strategies for
statistical machine translation. In Proceedings of the 2012
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language
Technologies, pages 427?436, Montre?al, Canada. Associ-
ation for Computational Linguistics.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001 new
features for statistical machine translation. In Proceedings
of Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 218?226, Boulder,
Colorado. Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause
restructuring for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 531?540,
Ann Arbor, Michigan. Association for Computational Lin-
guistics.
Cormack, G. V., Smucker, M. D., and Clarke, C. L. (2011).
Efficient and effective spam filtering and re-ranking for
large web datasets. Information retrieval, 14(5):441?465.
Durrani, N., Fraser, A., and Schmid, H. (2013a). Model With
Minimal Translation Units, But Decode With Phrases. In
The 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Atlanta, Georgia, USA. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Hoang, H., and Koehn,
P. (2013b). Can Markov Models Over Minimal Transla-
tion Units Help Phrase-Based SMT? In Proceedings of
the 51st Annual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Schmid, H., and Fraser, A. (2011). A Joint Se-
quence Translation Model with Integrated Reordering. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 1045?1054, Portland, Oregon, USA.
Foster, G., Goutte, C., and Kuhn, R. (2010). Discriminative
instance weighting for domain adaptation in statistical ma-
chine translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing,
pages 451?459, Cambridge, MA. Association for Compu-
tational Linguistics.
120
Haddow, B. (2013). Applying pairwise ranked optimisation
to improve the interpolation of translation models. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 342?347, Atlanta,
Georgia. Association for Computational Linguistics.
Haddow, B. and Koehn, P. (2012). Analysing the effect of
out-of-domain data on smt systems. In Proceedings of
the Seventh Workshop on Statistical Machine Translation,
pages 175?185, Montreal, Canada. Association for Com-
putational Linguistics.
Heafield, K. (2011). KenLM: Faster and smaller language
model queries. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 187?197, Edin-
burgh, Scotland. Association for Computational Linguis-
tics.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.
(2013). Scalable modified Kneser-Ney language model
estimation. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia, Bul-
garia.
Johnson, H., Martin, J., Foster, G., and Kuhn, R. (2007).
Improving translation quality by discarding most of the
phrasetable. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 967?975.
Kneser, R. and Ney, H. (1995). Improved backing-off for
m-gram language modeling. In Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal
Processing, pages 181?184.
Koehn, P. and Haddow, B. (2012a). Interpolated backoff for
factored translation models. In Proceedings of the Tenth
Conference of the Association for Machine Translation in
the Americas (AMTA).
Koehn, P. and Haddow, B. (2012b). Towards Effective Use of
Training Data in Statistical Machine Translation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine
Translation, pages 317?321, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored Translation
Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical methods for com-
pound splitting. In Proceedings of Meeting of the Euro-
pean Chapter of the Association of Computational Lin-
guistics (EACL).
Mansour, S. and Ney, H. (2012). A Simple and Effec-
tive Weighted Phrase Extraction for Machine Translation
Adaptation. In Proceedings of IWSLT.
Moore, R. C. and Lewis, W. (2010). Intelligent selection of
language model training data. In Proceedings of the ACL
2010 Conference Short Papers, pages 220?224, Uppsala,
Sweden. Association for Computational Linguistics.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001).
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176(W0109-022),
IBM Research Report.
Sennrich, R. (2012). Perplexity minimization for translation
model domain adaptation in statistical machine transla-
tion. In Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics, pages 539?549, Avignon, France. Association for
Computational Linguistics.
121
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97?104,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Phrase-based Machine Translation Systems for WMT-14
Nadir Durrani Barry Haddow Philipp Koehn
School of Informatics
University of Edinburgh
{dnadir,bhaddow,pkoehn}@inf.ed.ac.uk
Kenneth Heafield
Computer Science Department
Stanford University
heafield@cs.stanford.edu
Abstract
This paper describes the University of Ed-
inburgh?s (UEDIN) phrase-based submis-
sions to the translation and medical trans-
lation shared tasks of the 2014 Work-
shop on Statistical Machine Translation
(WMT). We participated in all language
pairs. We have improved upon our 2013
system by i) using generalized represen-
tations, specifically automatic word clus-
ters for translations out of English, ii) us-
ing unsupervised character-based models
to translate unknown words in Russian-
English and Hindi-English pairs, iii) syn-
thesizing Hindi data from closely-related
Urdu data, and iv) building huge language
on the common crawl corpus.
1 Translation Task
Our baseline systems are based on the setup de-
scribed in (Durrani et al., 2013b) that we used
for the Eighth Workshop on Statistical Machine
Translation (Bojar et al., 2013). The notable fea-
tures of these systems are described in the follow-
ing section. The experiments that we carried out
for this year?s translation task are described in the
following sections.
1.1 Baseline
We trained our systems with the following set-
tings: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, hierarchical lexicalized re-
ordering (Galley and Manning, 2008), a lexically-
driven 5-gram operation sequence model (OSM)
(Durrani et al., 2013a) with 4 count-based sup-
portive features, sparse domain indicator, phrase
length, and count bin features (Blunsom and Os-
borne, 2008; Chiang et al., 2009), a distortion limit
of 6, maximum phrase-length of 5, 100-best trans-
lation options, Minimum Bayes Risk decoding
(Kumar and Byrne, 2004), Cube Pruning (Huang
and Chiang, 2007), with a stack-size of 1000
during tuning and 5000 during test and the no-
reordering-over-punctuation heuristic (Koehn and
Haddow, 2009). We used POS and morphologi-
cal tags as additional factors in phrase translation
models (Koehn and Hoang, 2007) for German-
English language pairs. We also trained target se-
quence models on the in-domain subset of the par-
allel corpus using Kneser-Ney smoothed 7-gram
models. We used syntactic-preordering (Collins
et al., 2005) and compound splitting (Koehn and
Knight, 2003) for German-to-English systems.
We used trivia tokenizer for tokenizing Hindi.
The systems were tuned on a very large tun-
ing set consisting of the test sets from 2008-2012,
with a total of 13,071 sentences. We used news-
test 2013 for the dev experiments. For Russian-
English pairs news-test 2012 was used for tuning
and for Hindi-English pairs, we divided the news-
dev 2014 into two halves, used the first half for
tuning and second for dev experiments.
1.2 Using Generalized Word Representations
We explored the use of automatic word clusters
in phrase-based models (Durrani et al., 2014a).
We computed the clusters with GIZA++?s mkcls
(Och, 1999) on the source and target side of the
parallel training corpus. Clusters are word classes
that are optimized to reduce n-gram perplexity.
By generating a cluster identifier for each out-
put word, we are able to add an n-gram model
97
over these identifiers as an additional scoring func-
tion. The inclusion of such an additional factor
is trivial given the factored model implementation
(Koehn and Hoang, 2007) of Moses (Koehn et al.,
2007). The n-gram model is trained in the similar
way as the regular language model. We trained
domain-specific language models separately and
then linearly interpolated them using SRILM with
weights optimized on the tuning set (Schwenk and
Koehn, 2008).
We also trained OSM models over cluster-ids
(?). The lexically driven OSM model falls back to
very small context sizes of two to three operations
due to data sparsity. Learning operation sequences
over cluster-ids enables us to learn richer trans-
lation and reordering patterns that can generalize
better in sparse data conditions. Table 1 shows
gains from adding target LM and OSM models
over cluster-ids. Using word clusters was found
more useful translating from English-to-*.
from English into English
Lang B
0
+Cid ? B
0
+Cid ?
de 20.60 20.85 +0.25 27.44 27.34 -0.10
cs 18.84 19.39 +0.55 26.42 26.42 ?0.00
fr 30.73 30.82 +0.09 31.64 31.76 +0.12
ru 18.78 19.67 +0.89 24.45 24.63 +0.18
hi 10.39 10.52 +0.13 15.48 15.26 -0.22
Table 1: Using Word Clusters in Phrase-based and
OSM models ? B
0
= System without Clusters,
+Cid = with Cluster
We also trained OSM models over POS and
morph tags. For the English-to-German sys-
tem we added an OSM model over [pos, morph]
(source:pos, target:morph) and for the German-
to-English system we added an OSM model over
[morph,pos] (source:morph, target:pos), a config-
uration that was found to work best in our previous
experiments (Birch et al., 2013). Table 2 shows
gains from additionally using OSM models over
POS/morph tags.
Lang B
0
+OSM
p,m
?
en-de 20.44 20.60 +0.16
de-en 27.24 27.44 +0.20
Table 2: Using POS and Morph Tags in
OSM models ? B
0
= Baseline, +OSM
p,m
=
POS/Morph-based OSM
1.3 Unsupervised Transliteration Model
Last year, our Russian-English systems performed
badly on the human evaluation. In comparison
other participants that used transliteration did well.
We could not train a transliteration system due
to unavailability of a transliteration training data.
This year we used an EM-based method to in-
duce unsupervised transliteration models (Durrani
et al., 2014b). We extracted transliteration pairs
automatically from the word-aligned parallel data
and used it to learn a transliteration system. We
then built transliteration phrase-tables for trans-
lating OOV words and used the post-decoding
method (Method 2 as described in the paper) to
translate these.
Pair Training OOV B
0
+T
r
?
ru-en 232K 1356 24.63 25.06 +0.41
en-ru 232K 681 19.67 19.91 +0.24
hi-en 38K 503 14.67 15.48 +0.81
en-hi 38K 394 11.76 12.83 +1.07
Table 3: Using Unsupervised Transliteration
Model ? Training = Extracted Transliteration Cor-
pus (types), OOV = Out-of-vocabulary words (to-
kens) B
0
= System without Transliteration, +T
r
= Transliterating OOVs
Table 3 shows the number (types) of translit-
eration pairs extracted using unsupervised min-
ing, number of OOV words (tokens) in each pair
and the gains achieved by transliterating unknown
words.
1.4 Synthesizing Hindi Data from Urdu
Hindi and Urdu are closely related language pairs
that share grammatical structure and have a large
overlap in vocabulary. This provides a strong
motivation to transform any Urdu-English paral-
lel data into Hindi-English by translating the Urdu
part into Hindi. We made use of the Urdu-English
segment of the Indic multi-parallel corpus (Post
et al., 2012) which contains roughly 87K sentence
pairs. The Hindi-English segment of this corpus
is a subset of parallel data made available for the
translation task but is completely disjoint from the
Urdu-English segment.
We initially trained a Urdu-to-Hindi SMT sys-
tem using a very tiny EMILLE
1
corpus (Baker
1
EMILLE corpus contains roughly 12000 sentences of
Hindi and Urdu comparable data. From these we were able
to sentence align 7000 sentences to build an Urdu-to-Hindi
system.
98
et al., 2002). But we found this system to be use-
less for translating the Urdu part of Indic data due
to domain mismatch and huge number of OOV
words (approximately 310K tokens). To reduce
sparsity we synthesized additional phrase-tables
using interpolation and transliteration.
Interpolation: We trained two phrase transla-
tion tables p(u?
i
|e?
i
) and p(e?
i
|
?
h
i
), from Urdu-
English (Indic corpus) and Hindi-English (Hin-
dEnCorp (Bojar et al., 2014)) bilingual cor-
pora. Given the phrase-table for Urdu-English
p(u?
i
|e?
i
) and the phrase-table for English-Hindi
p(e?
i
|
?
h
i
), we estimated a Urdu-Hindi phrase-table
p(u?
i
|
?
h
i
) using the well-known convolution model
(Utiyama and Isahara, 2007; Wu and Wang, 2007):
p(u?
i
|
?
h
i
) =
?
e?
i
p(u?
i
|e?
i
)p(e?
i
|
?
h
i
)
The number of entries in the baseline Urdu-to-
Hindi phrase-table were approximately 254K. Us-
ing interpolation we were able to build a phrase-
table containing roughly 10M phrases. This re-
duced the number of OOV tokens from 310K to
approximately 50K.
Transliteration: Urdu and Hindi are written in
different scripts (Arabic and Devanagri respec-
tively). We added a transliteration component
to our Urdu-to-Hindi system. An unsupervised
transliteration model is learned from the word-
alignments of Urdu-Hindi parallel data. We were
able to extract around 2800 transliteration pairs.
To learn a richer transliteration model, we addi-
tionally fed the interpolated phrase-table, as de-
scribed above, to the transliteration miner. We
were able to mine additional 21000 translitera-
tion pairs and built a Urdu-Hindi character-based
model from it. The transliteration module can
be used to translate the 50K OOV words but
previous research (Durrani et al., 2010; Nakov
and Tiedemann, 2012) has shown that translit-
eration is useful for more than just translating
OOV words when translating closely related lan-
guage pairs. To fully capitalize on the large over-
lap in Hindi?Urdu vocabulary, we transliterated
each word in the Urdu test-data into Hindi and
produced a phrase-table with 100-best transliter-
ations. The two synthesized (triangulated and
transliterated) phrase-tables are then used along
with the baseline Urdu-to-Hindi phrase-table in
a log-linear model. Detailed results on Urdu-to-
Hindi baseline and improvements obtained from
using transliteration and triangulated phrase-tables
are presented in Durrani and Koehn (2014). Using
our best Urdu-to-Hindi system, we translated the
Urdu part of the multi-indic corpus to form Hindi-
English parallel data. Table 4 shows results from
using the synthesized Hindi-English corpus in iso-
lation (Syn) and on top of the baseline system
(B
0
+ Syn).
Pair B
0
Syn ? B
0
+ Syn ?
hi-en 14.28 10.49 -3.79 14.72 +0.44
en-hi 10.59 9.01 -1.58 11.76 +1.17
Table 4: Evaluating Synthesized (Syn) Hindi-
English Parallel Data, B
0
= System without Syn-
thesized Data
1.5 Huge Language Models
Our unconstrained submissions use an additional
language model trained on web pages from the
2012, 2013, and winter 2013 CommonCrawl.
2
The additional language model is the only differ-
ence between the constrained and unconstrained
submissions; we did not use additional parallel
data. These language models were trained on text
provided by the CommonCrawl foundation, which
they converted to UTF-8 after stripping HTML.
Languages were detected using the Compact Lan-
guage Detection 2
3
and, except for Hindi where
we lack tools, sentences were split with the Eu-
roparl sentence splitter (Koehn, 2005). All text
was then deduplicated, minimizing the impact of
boilerplate, such as social media sharing buttons.
We then tokenized and truecased the text as usual.
Statistics are shown in Table 5. A full description
of the pipeline, including a public data release, ap-
pears in Buck et al. (2014).
Lang Lines (B) Tokens (B) Bytes
en 59.13 975.63 5.14 TiB
de 3.87 51.93 317.46 GiB
fr 3.04 49.31 273.96 GiB
ru 1.79 21.41 220.62 GiB
cs 0.47 5.79 34.67 GiB
hi 0.01 0.28 3.39 GiB
Table 5: Size of huge language model training data
We built unpruned modified Kneser-Ney lan-
guage models using lmplz (Heafield et al., 2013).
2
http://commoncrawl.org
3
https://code.google.com/p/cld2/
99
Pair B
0
+L
newstest 2013 2014 2013 2014
en-de 20.85 20.10 ? 20.61 +0.51
en-cs 19.39 21.00 20.03 +0.64 21.60 +0.60
en-ru 19.90 28.70 20.80 +0.90 29.90 +1.20
en-hi 11.43 11.10 12.83 +1.40 12.50 +1.40
hi-en 15.48 13.90 ? 14.80 +0.90
Table 6: Gains obtained by using huge language
models ? B
0
= Baseline, +L = Adding Huge LM
While the Hindi and Czech models are small
enough to run directly, models for other languages
are quite large.We therefore created a filter that op-
erates directly on files in KenLM trie binary for-
mat, preserving only n-grams whose words all ap-
pear in the target side vocabulary of at least one
source sentence. For example, an English lan-
guage model trained on just the 2012 and 2013
crawls takes 3.5 TB without any quantization. Af-
ter filtering to the Hindi-English tuning set, the
model fit in 908 GB, again without quantization.
We were then able to tune the system on a machine
with 1 TB RAM. Results are shown in Table 6; we
did not submit to English-French because the sys-
tem takes too long to tune.
1.6 Miscellaneous
Hindi-English: 1) A large number of Hindi sen-
tences in the Hindi-English parallel corpus were
ending with a full-stop ?.?, although the end-of-
the-sentence marker in Hindi is ?Danda? (|). Re-
placing full-stops with Danda gave improvement
of +0.20 for hi-en and +0.40 in en-hi. 2) Using
Wiki subtitles did not give any improvement in
BLEU and were in fact harmful for the en-hi di-
rection.
Russian-English: We tried to improve word-
alignments by integrating a transliteration sub-
model into GIZA++ word aligner. The probabil-
ity of a word pair is calculated as an interpola-
tion of the transliteration probability and transla-
tion probability stored in the t-table of the differ-
ent alignment models used by the GIZA++ aligner.
This interpolation is done for all iterations of all
alignment models (See Sajjad et al. (2013) for de-
tails). Due to shortage of time we could only run it
for Russian-to-English. The improved alignments
gave a gain of +0.21 on news-test 2013 and +0.40
on news-test 2014.
Pair GIZA++ Fast Align ?
de-en 24.02 23.89 ?.13
fr-en 30.78 30.66 ?.12
es-en 34.07 34.24 +.17
cs-en 22.63 22.44 ?.19
ru-en 31.68 32.03 +.35
en-de 18.04 17.88 ?.16
en-fr 28.96 28.83 ?.13
en-es 34.15 34.32 +.17
en-cs 15.70 16.02 +.32
avg +.03
Table 7: Comparison of fast word alignment
method (Dyer et al., 2013) against GIZA++
(WMT 2013 data condition, test on new-
stest2012). The method was not used in the official
submission.
Pair Baseline MSD Hier. MSD Hier. MSLR
de-en 27.04 27.10 +.06 27.17 +.13
fr-en 31.63 - 31.65 +.02
es-en 31.20 31.14 ?.06 31.25 +.05
cs-en 26.11 26.32 +.21 26.26 +.15
ru-en 24.09 24.01 ?.08 24.19 +.11
en-de 20.43 20.34 ?.09 20.32 -.11
en-fr 30.54 - 30.52 ?.02
en-es 30.36 30.44 +.08 30.51 +.15
en-cs 18.53 18.59 +.06 18.66 +.13
en-ru 18.37 18.47 +.10 18.19 ?.18
avg + .035 +.045
Table 8: Hierarchical lexicalized reordering model
(Galley and Manning, 2008).
Fast align: In preliminary experiments, we
compared the fast word alignment method by
Dyer et al. (2013) against our traditional use of
GIZA++. Results are quite mixed (Table 7), rang-
ing from a gain of +.35 for Russian-English to a
loss of ?.19 for Czech-English. We stayed with
GIZA++ for all of our other experiments.
Hierarchical lexicalized reordering model:
We explored the use of the hierarchical lexicalized
reordering model (Galley and Manning, 2008)
in two variants: using the same orientations as
our traditional model (monotone, discontinuous,
swap), and one that distinguishes the discontin-
uous orientations to the left and right. Table 8
shows slight improvements with these models, so
we used them in our baseline.
Threshold filtering of phrase table: We exper-
imented with discarding some phrase table entry
due to their low probability. We found that phrase
translations with the phrase translation probability
100
?(f |e)<10
?4
can be safely discarded with almost
no change in translations. However, discarding
phrase translations with the inverse phrase transla-
tion probability ?(e|f)<10
?4
is more risky, espe-
cially with morphologically rich target languages,
so we kept those.
1.7 Summary
Table 9 shows cumulative gains obtained from us-
ing word classes, transliteration and big language
models
4
over the baseline system. Our German-
English constrained systems were used for EU-
Bridge system combination, a collaborative effort
to improve the state-of-the-art in machine transla-
tion (See Freitag et al. (2014) for details).
from English into English
Lang B
0
B
1
? B
0
B
1
?
de 20.44 20.85 +0.41 27.24 27.44 +0.20
cs 18.84 20.03 +1.19 26.42 26.42 ?0.00
fr 30.73 30.82 +0.09 31.64 31.76 +0.12
ru 18.78 20.81 +2.03 24.45 25.21 +0.76
hi 9.27 12.83 +3.56 14.08 15.48 +1.40
Table 9: Cumulative gains obtained for each lan-
guage ? B
0
= Baseline, B
1
= Best System
2 Medical Translation Task
For the medical translation task, the organisers
supplied several medical domain corpora (detailed
on the task website), as well some out-of-domain
patent data, and also all the data available for the
constrained track of the news translation task was
permitted. In general, we attempted to use all of
this data, except for the LDC Gigaword language
model data (for reasons of time) and we divided
the data into ?in-domain? and ?out-of-domain?
corpora. The data sets are summarised in Tables
10 and 11.
In order to create systems for the medical trans-
lation tasks, we used phrase-based Moses with ex-
actly the same settings as for the news translation
task, including the OSM (Durrani et al., 2011),
and compound splitting Koehn and Knight (2003)
for German source. We did not use word clusters
(Section 1.2), as they did not give good results on
this task, but we have yet to find a reason for this.
For language model training, we decided not to
build separate models on each corpus as there was
4
Cumulative gains do not include gains obtain from big
language models for hi-en and en-de.
Data Set cs-en de-en fr-en
coppa-in n n y
PatTR-in-claims n y y
PatTR-in-abstract n y y
PatTR-in-titles n y y
UMLS y y y
MuchMore n y n
EMEA y y y
WikiTitles y y y
PatTR-out n y y
coppa-out n n y
MultiUN n n y
czeng y n n
europarl y y y
news-comm y y y
commoncrawl y y y
FrEnGiga n n y
Table 10: Parallel data sets used in the medical
translation task. The sets above the line were clas-
sified as ?in-domain? and those below as ?out-of-
domain?.
Data Set cs de en fr
PIL n n y n
DrugBank n n y n
WikiArticles y y y y
PatTR-in-description n y y y
GENIA n n y n
FMA n n y n
AACT n n y n
PatTR-out-description n y y y
Table 11: Additional monolingual data used in
the medical translation task. Those above the line
were classified as ?in-domain? and the one below
as ?out-of-domain?. We also used the target sides
of all the parallel corpora for language modelling.
a large variation in corpus sizes. Instead we con-
catenated the in-domain target sides with the in-
domain extra monolingual data to create training
data for an in-domain language model, and simi-
larly for the out-of-domain data. The two language
models were interpolated using SRILM, minimis-
ing perplexity on the Khresmoi summary develop-
ment data.
During system development, we only had 500
sentences of development data (SUMMARY-DEV)
from the Khresmoi project, so we decided to se-
lect further development and devtest data from the
EMEA corpus, reasoning that it was fairly close
in domain to SUMMARY-DEV. We selected a tun-
ing set (5000 sentence pairs, which were added to
SUMMARY-DEV) and a devtest set (3000 sentence
pairs) from EMEA after first de-duplicating it, and
ignoring sentence pairs which were too short, or
101
contained too many capital letters or numbers. The
EMEA contains many duplicated sentences, and
we removed all sentence pairs where either side
was a duplicate, reducing the size of the corpus
to about 25% of the original. We also removed
EMEA from Czeng, since otherwise it would over-
lap with our selected development sets.
We also experimented with modified Moore-
Lewis (Moore and Lewis, 2010; Axelrod et al.,
2011) data selection, using the EMEA corpus as
the in-domain corpus (for the language model re-
quired in MML) and selecting from all the out-of-
domain data.
When running on the final test set (SUMMARY-
TEST) we found that it was better to tune just on
SUMMARY-DEV, even though it was much smaller
than the EMEA dev set we had selected. All but
two (cs-en, de-en) of our submitted systems used
the MML selection, because it worked better on
our EMEA devtest set. However, as can be seen
from Table 12, systems built with all the data gen-
erally perform better. We concluded that EMEA
was not a good representative of the Khresmoi
data, perhaps because of domain differences, or
perhaps just because of the alignment noise that
appears (from informal inspection) to be present
in EMEA.
from English into English
in in+20 in+out in in+20 in+out
de 18.59 20.88 ? 36.17 ? 38.57
cs 18.78 23.45 23.77 30.12 ? 36.32
fr 35.24 40.74 41.04 45.15 46.44 46.58
Table 12: Results (cased BLEU) on the khresmoi
summary test set. The ?in? systems include all
in-domain data, the ?in+20? systems also include
20% of the out-of-domain data and the ?out? sys-
tems include all data. The submitted systems are
shown in italics, except for de-en and cs-en where
we submitted a ?in+out? systems. For de-en, this
was tuned on SUMMARY-DEV plus the EMEA dev
set and scored 37.31, whilst for cs-en we included
LDC Giga in the LM, and scored 36.65.
For translating the Khresmoi queries, we used
the same systems as for the summaries, except that
generally we did not retune on the SUMMARY-DEV
data. We added a post-processing script to strip
out extraneous stop words, which improved BLEU,
but we would not expect it to matter in a real CLIR
system as it would do its own stop-word removal.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements n
?
287658 (EU-BRIDGE),
n
?
287688 (MateCat) and n
?
288769 (ACCEPT).
Huge language model experiments made use of
the Stampede supercomputer provided by the
Texas Advanced Computing Center (TACC) at
The University of Texas at Austin under NSF
XSEDE allocation TG-CCR140009. We also ac-
knowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Op-
erational Language Translation (BOLT) program
through IBM. This publication only reflects the
authors? views.
References
Axelrod, A., He, X., and Gao, J. (2011). Domain
adaptation via pseudo in-domain data selection.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK.
Association for Computational Linguistics.
Baker, P., Hardie, A., McEnery, T., Cunningham,
H., and Gaizauskas, R. J. (2002). EMILLE,
a 67-million word corpus of indic languages:
Data collection, mark-up and harmonisation. In
LREC.
Birch, A., Durrani, N., and Koehn, P. (2013). Ed-
inburgh SLT and MT system description for the
IWSLT 2013 evaluation. In Proceedings of the
10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg,
Germany.
Blunsom, P. and Osborne, M. (2008). Probabilis-
tic inference for machine translation. In Pro-
ceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 215?223, Honolulu, Hawaii. Association
for Computational Linguistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 workshop on statistical
machine translation. In Eighth Workshop on
Statistical Machine Translation, WMT-2013,
pages 1?44, Sofia, Bulgaria.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
102
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth In-
ternational Language Resources and Evalua-
tion Conference (LREC?14), Reykjavik, Ice-
land. ELRA, European Language Resources
Association. in prep.
Buck, C., Heafield, K., and van Ooyen, B. (2014).
N-gram counts and language models from the
common crawl. In Proceedings of the Language
Resources and Evaluation Conference, Reyk-
jav??k, Iceland.
Chiang, D., Knight, K., and Wang, W. (2009).
11,001 New Features for Statistical Machine
Translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics, pages
218?226, Boulder, Colorado. Association for
Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Durrani, N., Fraser, A., Schmid, H., Hoang, H.,
and Koehn, P. (2013a). Can markov mod-
els over minimal translation units help phrase-
based SMT? In Proceedings of the 51st An-
nual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria. Association
for Computational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Durrani, N. and Koehn, P. (2014). Improving ma-
chine translation via triangulation and transliter-
ation. In Proceedings of the 17th Annual Con-
ference of the European Association for Ma-
chine Translation (EAMT), Dubrovnik, Croatia.
Durrani, N., Koehn, P., Schmid, H., and Fraser,
A. (2014a). Investigating the usefulness of
generalized word representations in SMT. In
Proceedings of the 25th Annual Conference on
Computational Linguistics (COLING), Dublin,
Ireland. To Appear.
Durrani, N., Sajjad, H., Fraser, A., and Schmid,
H. (2010). Hindi-to-urdu machine translation
through transliteration. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, pages 465?474, Up-
psala, Sweden. Association for Computational
Linguistics.
Durrani, N., Sajjad, H., Hoang, H., and Koehn, P.
(2014b). Integrating an unsupervised translit-
eration model into statistical machine transla-
tion. In Proceedings of the 15th Conference of
the European Chapter of the ACL (EACL 2014),
Gothenburg, Sweden. Association for Compu-
tational Linguistics.
Durrani, N., Schmid, H., and Fraser, A. (2011).
A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Tech-
nologies, pages 1045?1054, Portland, Oregon,
USA.
Dyer, C., Chahuneau, V., and Smith, N. A. (2013).
A simple, fast, and effective reparameterization
of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, pages 644?
648, Atlanta, Georgia. Association for Compu-
tational Linguistics.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). EU-BRIDGE MT:
combined machine translation. In Proceedings
of the ACL 2014 Ninth Workshop on Statistical
Machine Translation, Baltimore, MD, USA.
Galley, M. and Manning, C. D. (2008). A sim-
ple and effective hierarchical phrase reorder-
ing model. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 848?856, Honolulu,
Hawaii.
Heafield, K. (2011). Kenlm: Faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Trans-
lation, pages 187?197, Edinburgh, Scotland,
United Kingdom.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and
Koehn, P. (2013). Scalable modified Kneser-
Ney language model estimation. In Proceedings
103
of the 51st Annual Meeting of the Association
for Computational Linguistics, Sofia, Bulgaria.
Huang, L. and Chiang, D. (2007). Forest rescor-
ing: Faster decoding with integrated language
models. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 144?151, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings
of MT Summit.
Koehn, P. and Haddow, B. (2009). Edinburgh?s
Submission to all Tracks of the WMT 2009
Shared Task with Reordering and Speed Im-
provements to Moses. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation, pages 160?164, Athens, Greece. Associ-
ation for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868?876, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In ACL 2007 Demonstrations,
Prague, Czech Republic.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Kumar, S. and Byrne, W. J. (2004). Mini-
mum bayes-risk decoding for statistical ma-
chine translation. In HLT-NAACL, pages 169?
176.
Moore, R. C. and Lewis, W. (2010). Intelligent
selection of language model training data. In
Proceedings of the ACL 2010 Conference Short
Papers, pages 220?224, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Nakov, P. and Tiedemann, J. (2012). Combining
word-level and character-level models for ma-
chine translation between closely-related lan-
guages. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages
301?305, Jeju Island, Korea. Association for
Computational Linguistics.
Och, F. J. (1999). An efficient method for deter-
mining bilingual word classes. In Ninth Confer-
ence the European Chapter of the Association
for Computational Linguistics (EACL), pages
71?76.
Post, M., Callison-Burch, C., and Osborne, M.
(2012). Constructing parallel corpora for six in-
dian languages via crowdsourcing. In Proceed-
ings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 401?409, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at wmt13: Using transliteration mining to
improve statistical machine translation. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Schwenk, H. and Koehn, P. (2008). Large and di-
verse language models for statistical machine
translation. In International Joint Conference
on Natural Language Processing, pages 661?
666.
Utiyama, M. and Isahara, H. (2007). A compar-
ison of pivot methods for phrase-based statis-
tical machine translation. In 2007 Meeting of
the North American Chapter of the Association
for Computational Linguistics (NAACL), pages
484?491.
Wu, H. and Wang, H. (2007). Pivot language
approach for phrase-based statistical machine
translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 856?863, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
104
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150?156,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Stanford University?s Submissions to the WMT 2014 Translation Task
Julia Neidert
?
, Sebastian Schuster
?
, Spence Green,
Kenneth Heafield, and Christopher D. Manning
Computer Science Department, Stanford University
{jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu
Abstract
We describe Stanford?s participation in
the French-English and English-German
tracks of the 2014 Workshop on Statisti-
cal Machine Translation (WMT). Our sys-
tems used large feature sets, word classes,
and an optional unconstrained language
model. Among constrained systems, ours
performed the best according to uncased
BLEU: 36.0% for French-English and
20.9% for English-German.
1 Introduction
Phrasal (Green et al., 2014b) is a phrase-based ma-
chine translation system (Och and Ney, 2004) with
an online, adaptive tuning algorithm (Green et al.,
2013c) which allows efficient tuning of feature-
rich translation models. We improved upon the
basic Phrasal system with sparse features over word
classes, class-based language models, and a web-
scale language model.
We submitted one constrained French-English
(Fr-En) system, one unconstrained English-German
(En-De) system with a huge language model, and
one constrained English-German system without it.
Each system was built using over 100,000 features
and was tuned on over 10,000 sentences. This paper
describes our submitted systems and discusses how
the improvements affect translation quality.
2 Data Preparation & Post-Processing
We used all relevant data allowed by the con-
strained condition, with the exception of HindEn-
Corp and Wiki Headlines, which we deemed too
noisy. Specifically, our parallel data consists of the
Europarl version 7 (Koehn, 2005), parallel Com-
monCrawl (Smith et al., 2013), French-English UN,
Giga-FrEn, and News Commentary corpora pro-
vided by the evaluation. For monolingual data, we
?
These authors contributed equally.
Sentences Tokens
En-De 4.5M 222M
Fr-En 36.3M 2.1B
Table 1: Gross parallel corpus statistics after pre-
processing.
Constrained LM Unconstrained LM
German 1.7B 38.9 B
English 7.2B -
Table 2: Number of tokens in pre-processed mono-
lingual corpora used to estimate the language mod-
els. We split the constrained English data into two
models: 3.7 billion tokens from Gigaword and 3.5
billion tokens from all other sources.
used the provided news crawl data from all years,
English Gigaword version 5 (Parker et al., 2011),
and target sides of the parallel data. This includes
English from the Yandex, CzEng, and parallel Com-
monCrawl corpora. For parallel CommonCrawl,
we concatenated the English halves for various lan-
guage pairs and then deduplicated at the sentence
level.
In addition, our unconstrained English-German
system used German text extracted from the en-
tire 2012, 2013, and winter 2013 CommonCrawl
1
corpora by Buck et al. (2014).
Tables 1 and 2 show the sizes of the pre-
processed corpora of parallel text and monolingual
text from which our systems were built.
2.1 Pre-Processing
We used Stanford CoreNLP to tokenize the English
and German data according to the Penn Treebank
standard (Marcus et al., 1993). The French source
data was tokenized similarly to the French Treebank
1http://commoncrawl.org
150
(Abeill? et al., 2003) using the Stanford French
tokenizer (Green et al., 2013b).
We also lowercased the data and removed any
control characters. Further, we filtered out all lines
that consisted mainly of punctuation marks, re-
moved characters that are frequently used as bullet
points and standardized white spaces and newlines.
We additionally filtered out sentences longer than
100 tokens from the parallel corpora in order to
speed up model learning.
2.2 Alignment
For both systems, we used the Berkeley Aligner
(Liang et al., 2006) with default settings to align
the parallel data. We symmetrized the alignments
using the grow-diag heuristic.
2.3 Language Models
Our systems used up to three language models.
2.3.1 Constrained Language Models
For En-De, we used lmplz (Heafield et al., 2013)
to estimate a 5-gram language model on all WMT
German monolingual data and the German side of
the parallel Common Crawl corpus. To query the
model, we used KenLM (Heafield, 2011).
For the Fr-En system, we also estimated a 5-gram
language model from all the monolingual English
data and the English side of the parallel Common
Crawl, UN, Giga-FrEn, CzEng and Yandex corpora
using the same procedure as above. Additionally,
we estimated a second language model from the
English Gigaword corpus.
All of these language models used interpolated
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998).
2.3.2 Unconstrained Language Model
Our unconstrained En-De submission used an ad-
ditional language model trained on German web
text gathered by the Common Crawl Foundation
and processed by Buck et al. (2014). This cor-
pus was formed from the 2012, 2013, and winter
2013 CommonCrawl releases, which consist of web
pages converted to UTF-8 encoding with HTML
stripped. Applying the Compact Language Detec-
tor 2,
2
2.89% of the data was identified as German,
amounting to 1 TB of uncompressed text. After
splitting sentences with the Europarl sentence split-
ter (Koehn, 2005), the text was deduplicated at the
sentence level to reduce the impact of boilerplate
2https://code.google.com/p/cld2/
Order 1 2 3 4 5
Count 226 1,916 6,883 13,292 17,576
Table 3: Number of unique n-grams, in millions,
appearing in the Common Crawl German language
model.
and pages that appeared in multiple crawls, discard-
ing 78% of the data. We treated the resulting data
as normal text, pre-processing it as described in
Section 2.1 to yield 38.9 billion tokens. We built
an unpruned interpolated modified Kneser-Ney lan-
guage model with this corpus (Table 3) and added
it as an additional feature alongside the constrained
language models. At 38.9 billion tokens after dedu-
plication, this monolingual data is almost 23 times
as large as the rest of the German monolingual cor-
pus. Since the test data was also collected from the
web, we cannot be sure that the test sentences were
not in the language model. However, substantial
portions of the test set are translations from other
languages, which were not posted online until after
2013.
2.3.3 Word-Class Language Model
We also built a word-class language model for the
En-De system. We trained 512 word classes on
the constrained German data using the predictive
one-sided class model of Whittaker and Woodland
(2001) with the parallelized clustering algorithm of
Uszkoreit and Brants (2008) by Green et al. (2014a).
All tokens were mapped to their word class; infre-
quent tokens appearing fewer than 5 times were
mapped to a special cluster for unknown tokens.
Finally, we estimated a 7-gram language model on
the mapped corpus with SRILM (Stolcke, 2002)
using Witten-Bell smoothing (Bell et al., 1990).
2.4 Tuning and Test Data
For development, we tuned our systems on all
13,573 sentences contained in the newstest2008-
2012 data sets and tested on the 3,000 sentences of
the newstest2013 data set. The final system weights
were chosen among all tuning iterations using per-
formance on the newstest2013 data set.
2.5 Post-Processing
Our post-processor recases and detokenizes sys-
tem output. For the English-German system, we
combined both tasks by using a Conditional Ran-
dom Field (CRF) model (Lafferty et al., 2001) to
151
learn transformations between the raw output char-
acters and the post-processed versions. For each
test dataset, we trained a separate model on 500,000
sentences selected using the Feature Decay Algo-
rithm for bitext selection (Bi?ici and Yuret, 2011).
Features used include the character type of the cur-
rent and surrounding characters, the token type of
the current and surrounding tokens, and the position
of the character within its token.
The English output was recased using a language
model based recaser (Lita et al., 2003). The lan-
guage model was trained on the English side of the
Fr-En parallel data using lmplz.
3 Translation System
We built our translation systems using Phrasal.
3.1 Features
Our translation model has 19 dense features that
were computed for all translation hypotheses: the
nine Moses (Koehn et al., 2007) baseline features,
the eight hierarchical lexicalized reordering model
features by Galley and Manning (2008), the log
count of each rule, and an indicator for unique rules.
On top of that, the model uses the following addi-
tional features of Green et al. (2014a).
Rule indicator features: An indicator feature for
each translation rule. To combat overfitting, this
feature fires only for rules that occur more than
50 times in the parallel data. Additional indicator
features were constructed by mapping the words in
each rule to their corresponding word classes.
Target unigram class: An indicator feature for
the class of each target word.
Alignments: An indicator feature for each align-
ment in a translation rule, including multi-word
alignments. Again, class-based translation rules
were used to extract additional indicator features.
Source class deletion: An indicator feature for
the class of each unaligned source word in a trans-
lation rule.
Punctuation count ratio: The ratio of target
punctuation tokens to source punctuation tokens
for each derivation.
Functionword ratio: The ratio of target function
words to source functionwords. The functionwords
for each language are the 35 most frequent words
on each side of the parallel data. Numbers and
punctuation marks are not included in this list.
Target-class bigram boundary: An indicator
feature for the concatenation of the word class of
the rightmost word in the left rule and the word
class of the leftmost word in the right rule in each
adjacent rule pair in a derivation.
Length features: Indicator features for the length
of the source side and for the length of the target
side of the translation rule and an indicator feature
for the concatenation of the two lengths.
Rule orientation features: An indicator feature
for each translation rule combined with its orienta-
tion class (monotone, swap, or discontinuous). This
feature also fires only for rules that occur more than
50 times in the parallel data. Again, class-based
translation rules were used to extract additional fea-
tures.
Signed linear distortion: The signed linear dis-
tortion ? for two rules a and b is ? = r(a)?l(b)+1,
where r(x) is the rightmost source index of rule x
and l(x) is the leftmost source index of rule x. Each
adjacent rule pair in a derivation has an indicator
feature for the signed linear distortion of this pair.
Many of these features consider word classes
instead of the actual tokens. For the target side, we
used the same word classes as we used to train the
class-based language model. For the source side,
we trained word classes on all available data using
the same method.
3.2 Tuning
We used an online, adaptive tuning algorithm
(Green et al., 2013c) to learn the feature weights.
The loss function is an online variant of expected
BLEU (Green et al., 2014a). As a sentence-level
metric, we used the extended BLEU+1 metric that
smooths the unigram precision as well as the refer-
ence length (Nakov et al., 2012). For feature selec-
tion, we used L
1
regularization. Each tuning epoch
produces a different set of weights; we tried all of
them on newstest2013, which was held out from the
tuning set, then picked the weights that produced
the best uncased BLEU score.
3.3 System Parameters
We started off with the parameters of our systems
for the WMT 2013 Translation Task (Green et
al., 2013a) and optimized the L
1
-regularization
strength. Both systems used the following tuning
parameters: a 200-best list, a learning rate of 0.02
and a mini-batch size of 20. The En-De system
152
Track Stanford Best Rank
En-De constrained 19.9 20.1 3
En-De unconstrained 20.0 20.6 5
Fr-En constrained 34.5 35.0 3
(a) cased BLEU (%)
Track Stanford Best Rank
En-De constrained 20.7 20.7 1
En-De unconstrained 20.9 21.0 3
Fr-En constrained 36.0 36.0 1
(b) uncased BLEU (%)
Table 4: Official results in terms of cased and uncased BLEU of our submitted systems compared to the
best systems for each track. The ranks for the unconstrained system are calculated relative to all primary
submissions for the language pair, whereas the ranks for the constrained systems are relative to only the
constrained systems submitted.
used a phrase length limit of 8, a distortion limit of
6 and a L
1
-regularization strength of 0.0002. The
Fr-En system used a phrase length limit of 9, a dis-
tortion limit of 5 and a L
1
-regularization strength
of 0.0001.
During tuning, we set the stack size for cube prun-
ing to Phrasal?s default value of 1200. To decode
the test set, we increased the stack size to 3000.
4 Results
Table 4 shows the official results of our systems
compared to other submissions to the WMT shared
task. Both our En-De and Fr-En systems achieved
the highest uncased BLEU scores among all con-
strained submissions. However, our recaser evi-
dently performed quite poorly compared to other
systems, so our constrained systems ranked third by
cased BLEU score. Our unconstrained En-De sub-
mission ranked third among all systems by uncased
BLEU and fifth by cased BLEU.
To demonstrate the effectiveness of the individ-
ual improvements, we show results for four differ-
ent En-De systems: (1) A baseline that contains
only the 19 dense features, (2) a feature-rich trans-
lation system with the additional rich features, (3)
a feature-rich translation system with an additional
word class LM, and (4) a feature-rich translation
system with an additional wordclass LM and a huge
language model. For Fr-En we only built systems
(1)-(3). Results for all systems can be seen in Table
5 and Table 6. From these results, we can see that
both language pairs benefitted from adding rich fea-
tures (+0.4 BLEU for En-De and +0.5 BLEU for
Fr-En). However, we only see improvements from
the class-based language model in the case of the
En-De system (+0.4 BLEU). For this reason our Fr-
En submission did not use a class-based language
model. Using additional data in the form of a huge
language model further improved our En-De sys-
tem by almost 1% BLEU on the newstest2013 data
set. However, we only saw 0.2 BLEU improvement
on the newstest2014 data set.
4.1 Analysis
Gains from rich features are in line with the gains
we saw in the WMT 2013 translation task (Green
et al., 2013a). We suspect that rich features would
improve the translation quality a lot more if we had
several reference translations to tune on.
The word class language model seemed to im-
prove only translations in our En-De system while
it had no effect on BLEU in our Fr-En system. One
of the main reasons seems to be that the 7-gram
word class language model helped particularly with
long range reordering, which happens far more fre-
quently in the En-De language pair compared to the
Fr-En pair. For example, in the following transla-
tion, we can see that the system with the class-based
language model successfully translated the verb in
the second clause (set in italic) while the system
without the class-based language model did not
translate the verb.
Source: It became clear to me that this is my path.
Feature-rich: Es wurde mir klar, dass das mein
Weg.
Word class LM: Es wurde mir klar, dass das mein
Weg ist.
We can also see that the long range of the word
class language model improved grammaticality as
shown in the following example:
Source: Meanwhile, more than 40 percent of the
population are HIV positive.
Feature-rich: Inzwischen sind mehr als 40
Prozent der Bev?lkerung sind HIV positiv.
153
#iterations tune 2013 2013 cased 2014 2014 cased
Dense 8 16.9 19.6 18.7 20.0 19.2
Feature-rich 10 20.1 20.0 19.0 20.0 19.2
+ Word class LM 15 21.1 20.4 19.5 20.7 19.9
+ Huge LM 9 21.0 21.3 20.3 20.9 20.1
Table 5: En-De BLEU results. The tuning set is newstest2008?2012. Scores on newstest2014 were
computed after the system submission deadline using the released references.
#iterations tune 2013 2013 cased 2014 2014 cased
Dense 1 29.1 32.0 30.4 35.6 34.0
Feature-rich 12 37.2 32.5 30.9 36.0 34.5
+ Word class LM 14 35.7 32.3 30.7 ? ?
Table 6: Fr-En BLEU results. The tuning set is newstest2008?2012. Scores on newstest2014 were
computed after the system submission deadline using the released references.
Word class LM: Unterdessen mehr als 40 Prozent
der Bev?lkerung sind HIV positiv.
In this example, the system without the class-
based language model translated the verb twice. In
the second translation, the class-based language
model prevented this long range disagreement. An
analysis of the differences in the translation output
of our Fr-En systems showed that the word class
languagemodelmainly led to different word choices
but does not seem to help grammatically.
4.2 Casing
Our system performed comparatively poorly at cas-
ing, as shown in Table 4. In analysis after the eval-
uation, we found many of these errors related to
words with internal capitals, such as ?McCaskill?,
because the limited recaser we used, which is based
on a language model, considered only all lowercase,
an initial capital, or all uppercase words. We ad-
dressed this issue by allowing any casing seen in the
monolingual data. Some words were not seen at all
in the monolingual data but, since the target side of
the parallel data was included in monolingual data,
these words must have come from the source sen-
tence. In such situations, we preserved the word?s
original case. Table 7 shows the results with the re-
vised casing model. We gained about 0.24% BLEU
for German recasing and 0.15% BLEU for English
recasing over our submitted systems. In future work,
we plan to compare with a truecased system.
En-De Fr-En
Uncased Oracle 20.71 36.05
Conditional Random Field 19.85 ?
Limited Recaser 19.82 34.51
Revised Recaser 20.09 34.66
Table 7: Casing results on newstest2014 performed
after the evaluation. The oracle scores are uncased
BLEU (%) while all other scores are cased. Sub-
mitted systems are shown in italic.
5 Negative Results
We experimented with several additions that did not
make it into the final submissions.
5.1 Preordering
One of the key challenges when translating from
English to German is the long-range reordering of
verbs. For this reason, we implemented a depen-
dency tree based reordering system (Lerner and
Petrov, 2013). We parsed all source side sentences
using the Stanford Dependency Parser (De Marn-
effe et al., 2006) and trained the preordering system
on the entire bitext. Then we preordered the source
side of the bitext and the tuning and development
data sets using our preordering system, realigned
the bitext and tuned a machine translation system
using the preordered data. While preordering im-
proved verb reordering in many cases, many other
parts of the sentences were often also reordered
which led to an overall decrease in translation qual-
154
ity. Therefore, we concluded that this systemwill re-
quire further development before it is useful within
our translation system.
5.2 Minimum Bayes Risk Decoding
We further attempted to improve our output by re-
ordering the best 1000 translations for each sentence
using Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) with BLEU as the distance measure.
This in effect increases the score of candidates that
are ?closer? to the other likely translations, where
?closeness? is measured by the BLEU score for the
candidate when the other translations are used as the
reference. Choosing the best translation following
this reordering improved overall performance when
tuned on the first half of the newstest2013 test set by
only 0.03 BLEU points for the English-German sys-
tem and 0.005 BLEU points for the French-English
system, so we abandoned this approach.
6 Conclusion
We submitted three systems: one constrained Fr-En
system, one constrained En-De system, and one un-
constrained En-De system. Among all constrained
systems, ours performed the best according to un-
cased BLEU. The key differentiating components
of our systems are class-based features, word class
language models, and a huge web-scale language
model. In ongoing work, we are investigating pre-
ordering for En-De translation as well as improved
recasing.
Acknowledgements
We thank Michael Kayser and Thang Luong for
help with experiments. This work was supported
by the Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Translation
(BOLT) program through IBM. This work used the
Extreme Science and Engineering Discovery Envi-
ronment (XSEDE), which is supported by National
Science Foundation grant number OCI-1053575.
The authors acknowledge the Texas Advanced Com-
puting Center (TACC) at The University of Texas
at Austin for providing HPC resources that have
contributed to the research results reported within
this paper. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
References
Anne Abeill?, Lionel Cl?ment, and Alexandra Kinyon,
2003. Building a treebank for French, chapter 10.
Kluwer.
Timothy C. Bell, John G. Cleary, and Ian H. Witten.
1990. Text compression. Prentice-Hall.
Ergun Bi?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In WMT.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC.
Stanley Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard Univer-
sity, August.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt,
John Bauer, Sida Wang, et al. 2013a. Feature-rich
phrase-based translation: Stanford University?s sub-
mission to the WMT 2013 translation task. In WMT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013b. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D. Manning. 2013c. Fast and adaptive online
training of feature-rich translation models. In ACL.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014a. An empirical comparison of features
and tuning for phrase-based machine translation. In
WMT.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014b. Phrasal: A toolkit for new directions
in statistical machine translation. In WMT.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
Kneser-Ney language model estimation. In ACL.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In WMT.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
ICASSP.
155
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
et al. 2007. Moses: Open source toolkit for statisti-
cal machine translation. In ACL, Demonstration Ses-
sion.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InProceedings ofMT
Summit.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML.
Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In ACL.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword
fifth edition, june. Linguistic Data Consortium,
LDC2011T07.
Jason Smith, Herv? Saint-Amand, Magdalena Plamada,
Philipp Koehn, Chris Callison-Burch, and Adam
Lopez. 2013. Dirt cheap web-scale parallel text
from the common crawl. In ACL. Association for
Computational Linguistics, August.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In ICLSP.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In ACL.
Ed W. D. Whittaker and Philip C. Woodland. 2001. Ef-
ficient class-based language modelling for very large
vocabularies. In ICASSP.
156
