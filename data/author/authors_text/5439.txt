Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121?124,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Reducing SMT Rule Table with Monolingual Key Phrase
Zhongjun He? Yao Meng? Yajuan Lj ? Hao Yu? Qun Liu?
? Fujitsu R&D Center CO., LTD, Beijing, China
{hezhongjun, mengyao, yu}@cn.fujitsu.com
? Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
{lvyajuan, liuqun}@ict.ac.cn
Abstract
This paper presents an effective approach
to discard most entries of the rule table for
statistical machine translation. The rule ta-
ble is filtered by monolingual key phrases,
which are extracted from source text us-
ing a technique based on term extraction.
Experiments show that 78% of the rule ta-
ble is reduced without worsening trans-
lation performance. In most cases, our
approach results in measurable improve-
ments in BLEU score.
1 Introduction
In statistical machine translation (SMT) commu-
nity, the state-of-the-art method is to use rules that
contain hierarchical structures to model transla-
tion, such as the hierarchical phrase-based model
(Chiang, 2005). Rules are more powerful than
conventional phrase pairs because they contain
structural information for capturing long distance
reorderings. However, hierarchical translation
systems often suffer from a large rule table (the
collection of rules), which makes decoding slow
and memory-consuming.
In the training procedure of SMT systems, nu-
merous rules are extracted from the bilingual cor-
pus. During decoding, however, many of them are
rarely used. One of the reasons is that these rules
have low quality. The rule quality are usually eval-
uated by the conditional translation probabilities,
which focus on the correspondence between the
source and target phrases, while ignore the quality
of phrases in a monolingual corpus.
In this paper, we address the problem of reduc-
ing the rule table with the information of mono-
lingual corpus. We use C-value, a measurement
of automatic term recognition, to score source
phrases. A source phrase is regarded as a key
phrase if its score greater than a threshold. Note
that a source phrase is either a flat phrase consists
of words, or a hierarchical phrase consists of both
words and variables. For rule table reduction, the
rule whose source-side is not key phrase is dis-
carded.
Our approach is different from the previous re-
search. Johnson et al (2007) reduced the phrase
table based on the significance testing of phrase
pair co-occurrence in bilingual corpus. The ba-
sic difference is that they used statistical infor-
mation of bilingual corpus while we use that of
monolingual corpus. Shen et al (2008) pro-
posed a string-to-dependency model, which re-
stricted the target-side of a rule by dependency
structures. Their approach greatly reduced the rule
table, however, caused a slight decrease of trans-
lation quality. They obtained improvements by
incorporating an additional dependency language
model. Different from their research, we restrict
rules on the source-side. Furthermore, the system
complexity is not increased because no additional
model is introduced.
The hierarchical phrase-based model (Chiang,
2005) is used to build a translation system. Exper-
iments show that our approach discards 78% of the
rule table without worsening the translation qual-
ity.
2 Monolingual Phrase Scoring
2.1 Frequency
The basic metrics for phrase scoring is the fre-
quency that a phrase appears in a monolingual cor-
pus. The more frequent a source phrase appears in
a corpus, the greater possibility the rule that con-
tains the source phrase may be used.
However, one limitation of this metrics is that if
we filter the rule table by the source phrase with
lower frequency, most long phrase pairs will be
discarded. Because the longer the phrase is, the
less possibility it appears. However, long phrases
121
are very helpful for reducing ambiguity since they
contains more information than short phrases.
Another limitation is that the frequency metrics
focuses on a phrase appearing by itself while ig-
nores it appears as a substring of longer phrases.
It is therefore inadequate for hierarchical phrases.
We use an example for illustration. Considering
the following three rules (the subscripts indicate
word alignments):
R
1
:
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 542 ? 552, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Lexicon-Constrained Character Model for Chinese 
Morphological Analysis 
Yao Meng, Hao Yu, and Fumihito Nishino 
Fujitsu R&D Center Co., Ltd, Room B1003, Eagle Run Plaza, No. 26 Xiaoyun Road,  
Chaoyang District, Bejing, 100016, P. R. China 
{Mengyao, Yu, Nishino}@frdc.fujitsu.com 
Abstract. This paper proposes a lexicon-constrained character model that com-
bines both word and character features to solve complicated issues in Chinese 
morphological analysis. A Chinese character-based model constrained by a 
lexicon is built to acquire word building rules. Each character in a Chinese sen-
tence is assigned a tag by the proposed model. The word segmentation and part-
of-speech tagging results are then generated based on the character tags. The 
proposed method solves such problems as unknown word identification, data 
sparseness, and estimation bias in an integrated, unified framework. Preliminary 
experiments indicate that the proposed method outperforms the best SIGHAN 
word segmentation systems in the open track on 3 out of the 4 test corpora. Ad-
ditionally, our method can be conveniently integrated with any other Chinese 
morphological systems as a post-processing module leading to significant im-
provement in performance. 
1   Introduction 
Chinese morphological analysis is a fundamental problem that has been studied ex-
tensively [1], [2], [3], [4], [5], [6], [7], [8]. Researchers make use of word or character 
features to cope with this problem. However, neither of them seems completely satis-
factory. 
In general, a simple word-based approach can achieve about 90% accuracy for 
segmentation with a medium-size dictionary. However, since no dictionary includes 
every Chinese word, the unknown word (or Out Of Vocabulary, OOV) problem [9], 
[10] can severely affect the performance of word-based approaches. Furthermore, 
word-based models have an estimation bias when faced with segmentation candidates 
with different numbers of words. For example, in the standard hidden Markov model, 
the best result, ?
=
?
==
n
i
iiii
TT
tttptwpWTpT
1
11
* )...|()|(maxarg)|( maxarg , is related to the number of 
the words in the segmentation candidates. As such, a candidate with fewer words is 
preferred over those with more words in the selection process. Therefore, most word-
based models are likely to fail when a combinational ambiguity1 sequence is separated 
into multiple words.  
                                                          
1
  A typical segmentation ambiguity, it refers to a situation in which the same Chinese sequence 
may be one word or several words in different contexts. 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 543 
Compared with Chinese words, Chinese characters are relatively less unambigu-
ous. The Chinese character set is very limited. Therefore, unknown characters occur 
rarely in a sentence. The grammatical advantages of characters have inspired re-
searchers to adopt character features in Chinese morphology and parsing [5], [6], [11], 
[12]. However, it is difficult to incorporate necessary word features, such as the form 
of a Chinese word and its fixed part-of-speech tags, in most character-based ap-
proaches. For this reason, character-based approaches have not achieved satisfactory 
performance in large-scale open tests.  
In this paper, we propose a lexicon-constrained character model to combine the 
merits of both approaches. We explore how to capture the Chinese word building 
rules using a statistical method, which reflects the regularities in the word formation 
process. First, a character hidden Markov method assigns the candidate tags to each 
character. Next, a large-size word list combined with linguistic information is used to 
filter out erroneous candidates. Finally, segmentation and part-of-speech tagging for 
the sentence are provided based on the character tags.  
The proposed model solves the problems of unknown word detection, word seg-
mentation and part-of-speech tagging using both word and character features. Addi-
tionally, our module is a post-processing module, which can be coupled to any exist-
ing Chinese morphological system; and it can readily recall some of the unknown 
words omitted by the system, and as a result, significantly improves the overall per-
formance. Evaluations of the proposed system on SIGHAN open test sets indicate that 
our method outperforms the best bakeoff results on 3 test sets, and ranks 2nd in the 4th 
test set [9].  
2   A Lexicon-Constrained Character Model for Chinese 
Morphology 
2.1   An Elementary Model to Describe Chinese Word Building Rules  
It is recognized that there are some regularities in the process of forming words from 
Chinese characters. This in general can be captured by word building rules. In this 
paper, we explore a statistical model to acquire such rules. The following are some 
definitions used in the proposed model. 
[Def. 1] character position feature 
We use four notations to denote the position of a character in a Chinese word. ?F? 
means the first character of the word, ?L? the last character, ?M? is a character within 
it and ?S? the word itself.  
[Def. 2] character tag set 
It is the product of the set of character position features and the set of part-of-
speech tags.  
Character tag set ={xy| setwordx  POS ? , },,,{ LMFSy ? }, where, x denotes one 
part-of-speech (POS) tag and y a character position feature. Together they are used to 
define the rules of Chinese word formation.  
[Def. 3] character tagging 
Given a Chinese sentence; character tagging is the process for assigning a character 
tag to each character in the sentence.  
544 Y. Meng, H. Yu, and F. Nishino 
Word building rules are acquired based on the relation between the character and 
the corresponding character tag. Word segmentation and part-of-speech tagging can 
be achieved easily based on the result of character tagging. For example, a character 
with ?xS? is a single character word with the part-of-speech tag ?x?; a character se-
quence starting with ?xF? and ending with ?xL? is a multiple character word with the 
part-of-speech tag ?x?.   
The elementary model adopts the character bi-gram hidden Markov model. In hid-
den Markov model, given the sentence,
nn ccccs 121 ...: ? , and character tagging result 
nn xyxyxyxyt 121 ...: ? , the probability of result t of s is estimated as: 
?
=
??
?=
ni
iiiii xycpxyxyxypstp
,1
12 )|() |()|(  (1) 
The best character tagging result for the sentences is given by equation (2):  
?
=
??
?=
ni
iiiii
t
xycpxyxyxypt
,1
12
* )|()|(maxarg  (2) 
We used the People's Daily Corpus of 1998 [13] to train this model. Also we 
adopted a 100,000-word dictionary listing all valid part-of-speech tags for each Chi-
nese word in the training phase to solve the data sparseness problem. The training data 
are converted into character tagging data through the following steps: a single charac-
ter word with ?x? is converted into the character marked with tag ?xS?; a two-character 
word with ?x? is converted into a first character with ?xF? and a second character with 
?xL?; a word with more than two characters with ?x? are converted into a first character 
with ?xF?, middle characters with ?xM? and last character with ?xL?. We adopt the POS 
tag set from the People's Daily Corpus, which consists of 46 tags. Taking into account 
of the four position features, the final character tag set is comprised of 184 tags. 
The emitted probability and transition probability of the model are estimated by the 
maximum likelihood method. The emitted probability is counted by the training Cor-
pus and the dictionary, where the Chinese words in the dictionary are counted one 
time. The transition probability is trained from the training Corpus only. 
2.2   An Improved Character-Based Model Using Lexicon Constraints 
We tested the above model based on the SIGHAN open test set [9]. The average pre-
cision for word segmentation was more than 88%. This means that most of the word 
building rules in Chinese have been obtained by the elementary model. However, the 
performance was relatively inferior to other word segmentation systems. It indicated 
that the model needed more features to learn word building rules. In error analysis, we 
found that the elementary model was so flexible that it produced many pseudo-words 
and invalid part-of-speech tags. In practice, a Chinese word is a stable sequence of 
Chinese characters, whose formation and part-of-speech tags are fixed by long-term 
usage. It seemed that only character position and meaning cannot describe a word 
building rule effectively.  
We also observed that word segmentation systems based on a simple dictionary 
matching algorithm and a few linguistic rules could achieve about 90% accuracy [14]. 
This suggested that a lexicon may have contribution to word building rules. Thus, we 
tried to incorporate a lexicon to the model to improve the performance. 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 545 
The major errors in the elementary model were pseudo words and invalid part-of-
speech (POS) tags. We proposed two constraints based on the lexicon to deal with 
these errors: 
1. If a possible word produced from the elementary model is in the word-
dictionary, the character tag of the characters forming this word should be 
consistent with the part-of-speech tag of the word in the dictionary.  
2. If a possible word produced is not in the dictionary, it must include one or 
more single characters, and none of which may be subsumed by any word in 
the dictionary in the current context. 
The first constraint eliminates invalid character tags. For example, the character  
?? ? has six character tags: ?aF? (first in adjective) , ?dF? (first in adverb), ?nF? (first 
in noun), ?nrF? (first in person name), ?tF? (first in time), and ?vF? (first in verb). The 
character ??? has five character tags: ?dL?, ?nL?, ?nrL?, ?tL?, and ?vL?. The combina-
tion of the two characters produces the possible word ????, which includes five 
possible word part-of-speech tags: ?d?, ?n?, ?nr?, ?t?, and ?v? based on these character 
tags. But ???? is a word in the dictionary, which only has two valid part-of-speech 
tags, namely, ?time? and ?person name?. Obviously, the part-of-speech tags: ?d?, ?n? 
and ?v? of ???? are invalid. Accordingly, the tags ?aF?, ?dF?, ?nF? , ?vF? on ??? and 
the tags ?dL?, ?nL?, ?vL? on ??? are also invalid. So they should be pruned from the 
candidates of the character tagging.  
The second constraint prunes pseudo words in the elementary model. Many studies 
in dictionary-based segmentation treat unknown words as sequences of single charac-
ters [1], [14]. The second constraint ensures that the new word produced by the ele-
mentary model must have one or more ?unattached? single characters (not subsumed 
by any other words). For example, the sequence ?????? (program error) will 
combine the pseudo word ???? because of the tag ?nF? on ??? and the tag ?nL? on 
???. The second constraint will prune ???? since ???? (program) and ???? 
(error) are already in the dictionary and there is no ?unattached? single character in it. 
Accordingly, the tag ?nF? on ??? and the tag ?nL? on ??? will be deleted from the 
candidates of character tagging.  
The following experiments show the lexicon-based constraints are very effective in 
eliminating error cases. The elementary model faces an average of 9.3 character tags 
for each character. The constraints will prune 70% of these error tags from it. As a 
result, the performance of character tagging is improved.  
It is worth noting that the lexicon in the elementary model cannot distort the prob-
ability of the character tagging results in the model. The pruned cases are invalid 
cases which cannot occur in the training data because all the words and POS tags in 
the training data are valid. Thus, the model built from the training data is not affected 
by the pruning process.  
2.3   Case Study 
In this subsection, we illustrate the advantages of the proposed method for Chinese 
morphology with an example.  
546 Y. Meng, H. Yu, and F. Nishino 
Example: ?????????????? 
(Xiaoming will analyze the program errors tomorrow).  
Where, ???? is an unknown word (person name), and the sequence ???? is a 
combinational ambiguity (either ???? (put up with) or ???+ ??? (will)). Here is 
how our approach works. 
Step 1: List all the character tags for each character. Figure 1 shows the character 
tags in the sequence ?????? . 
?
aF dF nF nrF nM nrM nsM qM vM aL dL vL aS 
?
aF dF nF nrF vF tF nM lM tM aL dL nrL aS 
?
aF dF nF nrF vF tF nM lM tM aL dL nrL aS 
?
nF tF nrM dL nL nrL tL vL
 
Fig. 1. Candidates for the sequence ?????? 
In this step we are able to find possible unknown words based on character position 
features. For example, the character tags in ?????? combine four possible un-
known words: ????, ?????, ????? , and ??????.   
Step 2: Prune the invalid candidates using constraints. 
The first constraint prunes some invalid character tags. For example, ???? can be 
either an adverb (d) or a personal name (nr); ???? is a time (t) word. The other part-
of-speech tags of these two words will be deleted. With the second constraint, we can 
delete ????? because ???? and ???? are words in the dictionary. However, ??
?? , ?????, and ?????? will be kept because ??? is a ?unattached? single 
character. The remaining candidates are shown in figure 2.  
?
aF dF nF nrF nM nrM nsM qM vM aL dL vL aS
?
 dF  nrF   nM lM tM    aS
?
     tF nM lM tM  dL nrL aS
?
nF tF nrM   nrL tL  
 
Fig. 2. Remaining Candidates for the sequence ?????? 
Step 3: Choose the best character tagging result based on the proposed character 
hidden Markov model.  
The best character tagging result is chosen using equation 2 in Section 2.1. The 
ambiguities in segmentation and word POS tagging are solved in the character tag-
ging process.  
Consider the combinational ambiguity ???? in the following 2 candidates: 
Candidate 1: ???/nr ??/t ?/d ?/d ??/n ??/n ??/v ??/v? 
Candidate 2: ???/nr ??/t ??/v ??/n ??/n ??/v ??/v? 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 547 
In word-based linear model, the erroneous candidate 2 will be prior to the correct 
candidate 1 since the model counts 9 nodes in candidate 1 but 8 nodes in candidate 2. 
However, there is no such bias in the character model because the number of charac-
ters does not change. The combinational ambiguity ???? will be denoted as ??/dS 
?/dS? or ??/vF ?/vL?. The number of nodes in all candidates of character tagging is 
the same.  
At last, the correct result ??/nrF ?/nrL ?/tF ?/tL ?/dS ?/dS?/nF ?/nL ?/nF 
?/nL ?/vF ?/vL ?/vF ?/vL? is selected, and the corresponding morphological result 
is: ???/nr ??/t ?/d ?/d ??/n ??/n ?? /v ??/v ?.  
The above steps show the proposed approach solves the various issues related to 
Chinese morphology by a concise character tagging process where word building is 
revealed. 
3   Experiments and Discussion 
We evaluated the proposed character method using the SIGHAN Backoff data, i.e. the 
one-month People's Daily Corpus of 1998, and the first version of Penn Chinese Tree-
bank [15]. We compared our approach against two state-of-the-art systems: one is 
based on a bi-gram word segmentation model [7], and the other based on a word-
based hidden Markov model [3]. For simplicity, we only considered three kinds of 
unknown words (personal name, location name, and organization name) in the all 
methods.  
The same corpus and word-dictionary were used to train the above three systems. 
The training data set was the 5-month People's Daily Corpus of 1998, which con-
tained approximately 6,300,000 words and 46 word part-of-speech tags. The system 
dictionary contained 100,000 words and the valid part-of-speech tag(s) of each word.  
On average, there were 1.3 part-of-speech tags for a word in the dictionary.  
In the following, chr-HMM refers to the proposed elementary model; chr-
HMM+Dic refers to the character model improved by integrating linguistic informa-
tion. W-Bigram is the word-based bi-gram system, and W-HMM is the word-based 
hidden Markov system. 
3.1   Morphological Experimental Results 
We examined the performance of our model in comparison against W-Bigram and W-
HMM. Table 1 compares the segmentation performance of our model against that of 
other models. Table 2 shows the accuracy in unknown word identification. Table 3 
illustrates the performance of the part-of-speech tagging. The experiments in Table 1 
and Table 2 were examined using the SIGHAN open test corpora. The experiments in 
Table 3 were performed again on the one-month People's Daily Corpus (PD corpus) 
and 4,000 sentences in the Penn Chinese Treebank (Penn CTB). We only examined 4 
major word categories in the Penn Chinese Treebank due to inconsistency in the part-
of-speech tag sets between the two corpora. The 4 major word categories were: noun 
(shown as NN, NR in Penn CTB; n, nr, ns, nz in PD corpus), verb (VV in Penn CTB; 
v, vd, vn in PD corpus), adjective (JJ in Penn CTB; a, ad, an in PD corpus) and adverb 
(AD in Penn CTB; d in PD corpus). 
548 Y. Meng, H. Yu, and F. Nishino 
Segmentation and word POS tagging performance is measured in precision (P%), 
recall (R%) and F-score (F). Unknown words (NW) are those words not found in our 
word-dictionary, which include named entities and other new words. The unknown 
word rate (NW-Rate), the precision on unknown words (NW-Precision) and recall on 
total unknown words (NW-Recall) are given by:   
NW-Rate= identifiedNW  of  #  total
rdsunknown wo of  #
 NW-Precision = identifiedNW  of # total
rdsunknown wo  validof #
 
NW-Recall = data in testingNW  of # total
rdunknown wo  validof #
 
Table 1 shows that the above three systems achieve similar performances on the 
PK testing corpus. All of them were trained by the People's Daily corpus. For this 
reason, their performances were similar when the testing data had similar styles. But 
for other texts, the proposed character model performed much better than the word-
based models in both recall and precision. This indicated that our approach performed 
better for unseen data.  
Table 2 shows that our method for unknown word identification also outperforms 
the word-based method. We notice that word-based approaches and character-based 
approaches have similar precision on unknown word identification, however word-
based approaches have much lower recall than character-based ones. The main reason 
for this is that word-based systems focus only on unknown words with proper word 
structures, but cannot recognize newly generated words, rare words, and other new 
words unlisted in the dictionary. A very high proportion of these types of unknown 
word in the SIGHAN testing data affects the recall of the word-based methods on 
unknown words. The experiments reveal that our method could effectively identify all 
kinds of new words. This is because our model has defined word building rules for all 
kinds of words. 
Without a widely recognized testing standard, it is very hard to evaluate the per-
formance on part-of-speech tagging. The results in Penn Chinese Treebank was better 
than that in the People's Daily Corpus since we examined all 42 POS tags in the Peo-
ple's Daily Corpus, but we only tested four major POS tags in Penn Chinese Tree-
bank. Our approach is better than the word-based method for two test data sets. How-
ever, we could not conclude that our method was superior to the word-based method 
because of the limited testing approaches and testing data. A thorough empirical com-
parison among different approaches should be investigated in the future.  
Table 1. Comparison of word segmentation based on SIGHAN open test sets 
 PK CTB HK AS 
 R%/ P% F R%/ P% F R%/ P% F R%/ P% F 
Chr-HMM 91.9/91.8 91.8 86.9/87.3 87.1 87.7/86.7 87.2 89.9/89.1 89.5 
Chr-HMM+Dic 95.9/96.7 96.3 92.7/93.5 93.1 91.1/91.9 91.5 92.3/93.9 93.1 
W-Bigram 94.7/95.4 95.1 87.4/86.8 87.1 88.7/83.7 86.3 87.9/85.1 86.5 
W-HMM 94.6/95.1 94.9 88.6/89.2 88.9 90.7/89.1 89.9 90.7/87.2 89.0 
Rank 1 in SIG 96.3/95.6 96.0 91.6/90.7 91.2 95.8/95.4 95.6 91.5/89.4 90.5 
Rank 2 in SIG 96.3/94.3 95.3 91.1/89.1 90.1 90.9/86.3 88.6 89.2/85.3 87.3 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 549 
Table 2. Accuracy of unknown word identification for SIGHAN open test sets 
 PK CTB HK AS 
Chr-HMM UWR% P% R% UWR% P% R% UWR% P% R% UWR% P% R% 
Chr-HMM+Dic 2.3 56.2 54.8 10.4 68.8 64.4 9.7 61.4 58.4 8 65.4 62.9 
W-Bigram 2.3 54.7 53.6 10.4 53.9 23.8 9.7 53.0 29.6 8 64.6 35.3 
W-HMM 2.3 58.1 51.3 10.4 68.3 37.2 9.7 62.3 40.7 8 68.4 41.1 
Table 3. Comparison of word part-of-speech tagging 
 People Daily Penn CTB 
 P% R% F-score P% R% F-score 
Chr-HMM 82.4% 82.5% 82.5 89.7% 88.5% 89.1 
Chr-HMM+Dic 89.3 87.8 88.6 92.5 91.5 92.0 
W-HMM 86.2% 85.4% 85.7 91.1% 90.8% 91.0 
From Table 1 and Table 3, we notice that chr-HMM achieved 88% accuracy in 
word segmentation and 80% in part-of-speech tagging without a word-dictionary. 
Chr-HMM is a state-of-the-art Chinese morphology system without a word-
dictionary. Its performance is comparable to some dictionary-based approaches (e.g., 
forward-maximum). This result indicates that our model has effectively captured most 
of the Chinese word building rules.  
The results also show that chr-HMM+Dic outperformed the best SIGHAN word 
segmentation system on 3 out of the 4 SIGHAN open track test corpora, and achieved 
top 2 in the case of HK testing corpus.  
3.2   Incorporation with Other Systems  
The advantage of the proposed model is proficiency in describing word building rules 
and since many existing NLP application systems are weak in identifying new words, 
it is intuitive to integrate our model to existing systems and serves as a post-
processing subsystem. In this subsection, we show how existing word segmentation 
systems could be improved using chr-HMM.  
Given a segmentation result, we assume that unidentified new words may be a se-
quence of unattached characters. That is, all multiple-character words in the given 
result are considered correct, while single words, which might include unidentified 
new words will be rechecked by the chr-HMM. The entire process involves 3 steps: 
1. Only character tags that are consistent with the position of the character in the 
word are listed for multi-character words.  
2. The unattached characters are tagged with all possible character tags. In this 
way, the original segmentation result is converted into a group of character 
tagging candidates.  
3. We then input these character tagging candidates into the chr-HMM to select 
the best one.  
550 Y. Meng, H. Yu, and F. Nishino 
Consider an original result:  
?? [?  ?  ?  ?  ?] ??  [?  ? ] (Jordan bounced back strongly from the 
bottom yesterday) 
The parts in brackets are the sequence of single characters where the new words 
may appear. The chr-HMM will list all possible character tags for these ?unattached? 
characters. The parts outside the brackets are multiple-character words identified by 
the original system. They are assumed correct and maintain also positional informa-
tion. Only the character tags, which are consistent with the positions of the character 
in the word are listed. The character tagging candidates for the above sample is given 
in Figure 3: 
?  ? ?  
vL ? nL nsL ? 
 
tL vL nM nrL vM
  
? nL lM nsF nL 
 
nM ?
nrF nrL tL nM vF nrF nM vF vL vF vL
nF nL nL aM pS nF vF nF nL aF nL
aF aL tS tS dS nS nS aF aL aS vS
? ? ? ? ? ? ? ? ? ? ?
 
Fig. 3. Character tagging candidates for rechecking 
Chr-HMM is then applied to the character tagging candidates and the best charac-
ter tagging selected based on the probability of the candidates is output as the result. 
In this example, the result is: ??? (Jordan) ?? (yesterday) ? (from) ?? (earth) 
?? (strongly) ?? (bound)?. The three missing new words in the original system 
are identified by this post-processing subsystem.  
We re-assigned the word segmentation results for all participants who have given 
permission to release data from the SIGHAN site (available for download from 
http://www.sighan.org/bakeoff2003 ). Table 4 enlists the performance of SIGHAN 
open test with and without chr-HMM. The participant numbers correspond to the sites 
listed in [9].  
Table 4. Comparison of results with and without chr-HMM 
Corpus Site  R% P% F 
AS 03 Before After 
89.2 
90.8 
85.3 
92.0 
87.2 
91.4 
01 Before After 
88.7 
90.1 
87.6 
91.8 
88.1 
90.9 
03 Before After 
85.3 
86.4 
80.6 
87.8 
82.9 
87.1 CTB 
10 Before After 
91.1 
91.0 
89.1 
93.5 
90.1 
92.3 
HK 03 Before After 
90.9 
89.4 
86.3 
91.0 
88.6 
90.2 
03 Before After 
94.1 
94.4 
91.1 
95.3 
92.5 
94.9 PK 
10 Before After 
96.3 
95.6 
95.6 
97.7 
95.9 
96.7 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 551 
From Table 4, it is obvious that word segmentation precision increases signifi-
cantly, and at the same time, the corresponding recall remains the same or slightly 
declined. This implies that the chr-HMM retains the correct words by the original 
system and concurrently decreases significantly its errors.  
4   Related Work  
Although character features are very important in Chinese morphology, research in 
character-based approach is unpopular.  Chooi-Ling Goh et al [16], Jianfeng Gao et 
al. [8] and Huaping Zhang [3] adopted character information to handle unknown 
words; X. Luo [11], Yao Meng [12] and Shengfen Luo [17] each presented character-
based parsing models for Chinese parsing or new-word extraction. T. Nakagawa used 
word-level information and character-level information for word segmentation [6]. 
Hwee Tou Ng et al [5] investigated word-based and character-based approaches and 
proposed a maximum entropy character-based POS analyzer. Although the character 
tags proposed in this paper are essentially similar to some of the previous work men-
tioned above, here our focus is to integrate various word features with the character-
based model in such a way that the probability of the model is undistorted. The pro-
posed model is effective in acquiring word building rules. To our knowledge, our 
work is the first character-based approach, which outperforms the word-based ap-
proaches for SIGHAN open test. Also, our approach is versatile and can be easily 
integrated with existing morphological systems to achieve improved performance.  
5   Conclusion and Future Works  
A lexicon-constrained character model is proposed to capture word building rules 
using word features and character features. The combination of word and character 
features improves the performance of word segmentation and part-of-speech tagging. 
The proposed model can solve complicated issues in Chinese morphological analysis. 
The Chinese morphological analysis is generalized into a process of specific character 
tagging and word filtering. A lexicon supervises the character-based model to elimi-
nate invalid character tagging candidates.  
Our system outperformed the best SIGHAN word segmentation system in 3 out of 
the 4 SIGHAN open test sets. To our knowledge, our work is the first character-based 
approach, which performs better than word-based approaches for SIGHAN open test. 
In addition, the proposed method is versatile and can be easily integrated to any exist-
ing Chinese morphological system as a post-processing subsystem leading to en-
hanced performance.  
In this paper, we focused on word features in character-based mode, and adopted 
HMM as the statistical model to identify the rules. Other statistical models, such as 
maximum entropy, boosting, support vector machine, etc., may also be suitable for 
this application. They are worth investigating. The data sparseness problem is practi-
cally non-existent in the character-based model for the Chinese character set is lim-
ited. However, odd characters are occasionally found in Chinese personal or place 
names. Some rules using named entity identification technique may help smoothen 
552 Y. Meng, H. Yu, and F. Nishino 
this. In a broader view, the word building rules proposed in our model is simple 
enough for linguistic studies to better understand for example formation of Chinese 
words or even the Chinese language itself. 
References 
1. Andi Wu. Chinese Word Segmentation in MSR-NLP. In Proc. of SIGHAN Workshop, 
Sapporo, Japan, (2003) 127-175 
2. GuoDong Zhou and Jian Su. A Chinese Efficient Analyzer Integrating Word Segmenta-
tion, Part-Of-Speech Tagging, Partial Parsing and Full Parsing. In Proc. Of SIGHAN 
Workshop, Sapporo, Japan, (2003) 78-83 
3. Huaping Zhang, Hong-Kui Yu et al. HHMM-based Chinese Lexical Analyzer ICTCLAS. 
In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 184-187 
4. Nianwen Xue and Libin Shen. Chinese Word Segmentation as LMR Tagging. In Proc. Of 
SIGHAN Workshop, Sapporo, Japan, (2003) 176-179 
5. Hwee Tou Ng, Low, Jin Kiat. Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-
Once? Word-Based or Character-Based? In Proc. of EMNLP, Barcelona, Spain, (2004) 
277-284 
6. Tetsuji Nakagawa. Chinese and Japanese Word Segmentation Using Word-level and 
Character-level Information, In Proc. of the 20th COLING, Geneva, Switzerland, (2004) 
466-472 
7. Guohong Fu and Kang-Kwong Luke. A Two-stage Statistical Word Segmentation System 
for Chinese. In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 156-157 
8. Jianfeng Gao, Andi Wu, Chang-Ning Huang et al Adaptive Chinese Word Segmentation. 
In Proc. of 42nd ACL. Barcelona, Spain, (2004) 462-469  
9. Richard Sproat and Thomas Emerson. The First International Chinese Word Segmentation 
Bakeoff. In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 133-143 
10. X. Luo. A Maximum Entropy Chinese Character-based Parser. In Proc. of EMNLP. Sap-
poro, Japan, (2003) 192-199 
11. Honglan Jin, Kam-Fai Wong, ?A Chinese Dictionary Construction Algorithm for Informa-
tion Retrieval?, ACM Transactions on Asian Language Information Processing, 1(4):281-
296, Dec. 2002. 
12. Yao Meng, Hao Yu and Fumihito Nishino. 2004. Chinese New Word Identification Based 
on Character Parsing Model. In Proc. of 1st  IJCNLP, Hainan, China, (2004) 489-496 
13. Shiwen Yu, Huiming Duan, etal. ?????????????????. ?????
?v(5), (2002) 49-64, 58-65 
14. Maosong Sun and Benjamin K. T? Sou. Ambiguity Resolution in Chinese Word Segmenta-
tion. In Proc. of 10th Pacific Asia Conference on Language, Information & Computation, 
(1995) 121-126 
15. Nianwen Xue, Fu-Dong Chiou and Martha Palmer. Building a Large-scale Annotated 
Chinese Corpus. In Proc. of the 19th COLING. Taibei, Taiwan, (2002)  
16. Chooi-Ling GOH, Masayuki Asahara, Yuji Matsumoto. Chinese Unknown Word Identifi-
cation Using Character-based Tagging and Chunking. In Proc. of the 41st ACL, Interac-
tive Poster/Demo Sessions, Sapporo, Japan, (2003) 197-200 
17. Shengfen Luo, Maosong Sun. 2003, Two-character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual Measure, In Proc. of the 2nd SIGHAN Workshop, Sap-
poro, Japan, (2003) 20-30 
NAACL HLT Demonstration Program, pages 23?24,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
A Conversational In-car Dialog System
Baoshi Yan1 Fuliang Weng1 Zhe Feng1 Florin Ratiu2 Madhuri Raya1 Yao Meng1
Sebastian Varges2 Matthew Purver2 Annie Lien1 Tobias Scheideck1 Badri Raghunathan1
Feng Lin1 Rohit Mishra4 Brian Lathrop4 Zhaoxia Zhang4 Harry Bratt3 Stanley Peters2
Research and Technology Center, Robert Bosch LLC, Palo Alto, California1
Center for the Study of Language and Information, Stanford University, Stanford, California2
Speech Technology and Research Lab, SRI International, Menlo Park, California3
Electronics Research Lab, Volkswagen of America, Palo Alto, California4
Abstract
In this demonstration we present a con-
versational dialog system for automobile
drivers. The system provides a voice-
based interface to playing music, finding
restaurants, and navigating while driving.
The design of the system as well as the
new technologies developed will be pre-
sented. Our evaluation showed that the
system is promising, achieving high task
completion rate and good user satisfation.
1 Introduction
As a constant stream of electronic gadgets such as
navigation systems and digital music players en-
ters cars, it threatens driving safety by increasing
driver distraction. According to a 2005 report by
the National Highway Traffic Safety Administration
(NHTSA) (NHTSA, 2005), driver distraction and
inattention from all sources contributed to 20-25%
of police reported crashes. It is therefore impor-
tant to design user interfaces to devices that mini-
mize driver distraction, to which voice-based inter-
faces have been a promising approach as they keep
a driver?s hands on the wheel and eyes on the road.
In this demonstration we present a conversational
dialog system, CHAT, that supports music selection,
restaurant selection, and driving navigation (Weng
et al, 2006). The system is a joint research effort
from Bosch RTC, VWERL, Stanford CSLI, and SRI
STAR Lab funded by NIST ATP. It has reached a
promising level, achieving a task completion rate of
98%, 94%, 97% on playing music, finding restau-
rants, and driving navigation respectively.
Specifically, we plan to present a number of fea-
tures in the CHAT system, including end-pointing
with prosodic cues, robust natural language under-
standing, error identification and recovery strate-
gies, content optimization, full-fledged reponse gen-
eration, flexible multi-threaded, multi-device dialog
management, and support for random events, dy-
namic information, and domain switching.
2 System Descriptions
The spoken dialog system consists of a number of
components (see the figure on the next page). In-
stead of the hub architecture employed by Commu-
nicator projects (Seneff et al, 1998), it is devel-
oped in Java and uses flexible event-based, message-
oriented middleware. This allows for dynamic regis-
tration of new components. Among the component
modules in the figure, we use the Nuance speech
recognition engine with class-based n-grams and
dynamic grammars, and the Nuance Vocalizer as the
TTS engine. The Speech Enhancer removes noises
and echo. The Prosody module will provide addi-
tional features to the Natural Language Understand-
ing (NLU) and Dialog Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation. Par-
allel to the deep analysis, a topic classifier assigns
n-best topics to the utterance, which are used in the
cases where the dialog manager cannot make any
sense of the parsed structure. The NLU module also
supports dynamic updates of the knowledge base.
The DM module mediates and manages interac-
23
tion. It uses an information-state-update approach to
maintain dialog context, which is then used to inter-
pret incoming utterances (including fragments and
revisions), resolve NPs, construct salient responses,
track issues, etc. Dialog states can also be used to
bias SR expectation and improve SR performance,
as has been performed in previous applications of
the DM. Detailed descriptions of the DM can be
found in (Lemon et al, 2002) (Mirkovic and Cave-
don, 2005).
The Knowledge Manager (KM) controls access
to knowledge base sources (such as domain knowl-
edge and device information) and their updates. Do-
main knowledge is structured according to domain-
dependent ontologies. The current KMmakes use of
OWL, a W3C standard, to represent the ontological
relationships between domain entities.
The Content Optimization module acts as an in-
termediary between the dialog management module
and the knowledge management module and con-
trols the amount of content and provides recommen-
dations to user. It receives queries in the form of se-
mantic frames from the DM, resolves possible ambi-
guities, and queries the KM. Depending on the items
in the query result as well as configurable properties,
the module selects and performs an appropriate op-
timization strategy (Pon-Barry et al, 2006).
The Response Generation module takes query re-
sults from the KM or Content Optimizer and gener-
ates natural language sentences as system responses
to user utterances. The query results are converted
into natural language sentences via a bottom-up ap-
proach using a production system. An alignment-
based ranking algorithm is used to select the best
generated sentence.
The system supports random events and dy-
namic external information, for example, the system
prompts users for the next turn when they drive close
to an intersection and dialogs can be carried out in
terms of the current dynamic situation. The user can
also switch among the three different applications
easily by explicitly instructing the system which do-
main to operate in.
3 Acknowledgement
This work is partially supported by the NIST Ad-
vanced Technology Program.
References
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in
dialogue systems. In Traitement Automatique des
Langues (TAL), page 43(2).
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING), page
43(2), Tokyo, Japan.
National Highway Traffic Safety Administration
NHTSA. 2005. NHTSA Vehicle Safety Rulemaking
and Supporting Research Priorities: Calendar Years
2005-2009. January.
Heather Pon-Barry, Fuliang Weng, and Sebastian Varges.
2006. Evaluation of content presentation strategies
for an in-car spoken dialogue system. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (Interspeech/ICSLP), pages 1930?
1933, Pittsburgh, PA, September.
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: A Reference Architecture for Conversa-
tional System Development. In International Confer-
ence on Spoken Language Processing (ICSLP), page
43(2), Sydney, Australia, December.
Fuliang Weng, Sebastian Varges, Badri Raghunathan,
Florin Ratiu, Heather Pon-Barry, Brian Lathrop,
Qi Zhang, Tobias Scheideck, Harry Bratt, Kui Xu,
Matthew Purver, Rohit Mishra, Annie Lien, Mad-
huri Raya, Stanley Peters, Yao Meng, Jeff Russel,
Lawrence Cavedon, Liz Shriberg, and Hauke Schmidt.
2006. CHAT: A conversational helper for automo-
tive tasks. In Proceedings of the 9th International
Conference on Spoken Language Processing (Inter-
speech/ICSLP), pages 1061?1064, Pittsburgh, PA,
September.
24
Coling 2010: Poster Volume, pages 383?390,
Beijing, August 2010
Learning Phrase Boundaries
for Hierarchical Phrase-based Translation
Zhongjun HE Yao MENG Hao YU
Fujitsu R&D Center CO., LTD.
{hezhongjun, mengyao, yu}@cn.fujitsu.com
Abstract
Hierarchical phrase-based models pro-
vide a powerful mechanism to capture
non-local phrase reorderings for statis-
tical machine translation (SMT). How-
ever, many phrase reorderings are arbi-
trary because the models are weak on de-
termining phrase boundaries for pattern-
matching. This paper presents a novel
approach to learn phrase boundaries di-
rectly from word-aligned corpus without
using any syntactical information. We use
phrase boundaries, which indicate the be-
ginning/ending of phrase reordering, as
soft constraints for decoding. Experi-
mental results and analysis show that the
approach yields significant improvements
over the baseline on large-scale Chinese-
to-English translation.
1 Introduction
The hierarchial phrase-based (HPB) model (Chi-
ang, 2005) outperformed previous phrase-based
models (Koehn et al, 2003; Och and Ney, 2004)
by utilizing hierarchical phrases consisting of both
words and variables. Thus the HPB model has
generalization ability: a translation rule learned
from a phrase pair can be used for other phrase
pairs with the same pattern, e.g. reordering infor-
mation of a short span can be applied for a large
span during decoding. Therefore, the model cap-
tures both short and long distance phrase reorder-
ings.
However, one shortcoming of the HPB model is
that it is difficult to determine phrase boundaries
for pattern-matching. Therefore, during decod-
ing, a rule may be applied for all possible source
phrases with the same pattern. However, incorrect
pattern-matching will cause wrong translation.
Consider the following rule that is used to trans-
late the Chinese sentence in Figure 1 into English:
X ? ?XL de XR, XR in XL? (1)
The rule translates the Chinese word ?de? into
English word ?in?, and swaps the left sub-phrase
covered by XL and the right sub-phrase covered
by XR on the target side. However, XL may
pattern-match 5 spans on the left side of ?de? and
XR may pattern-match 3 spans on the right side.
Therefore, the rule produces 15 different deriva-
tions. However, 14 of them are incorrect.
The correct derivation Sc is shown in Figure 2,
while one of the wrong derivations Si is shown in
Figure 3. We observe that the basic difference be-
tween Sc and Si is the phrase boundary matched
by ?XR?. In Sc, XR matches the span [7, 9] and
moves it as a whole unit. While in Si, XR matches
the span [7, 8] and left the last word [9, 9] be trans-
lated separately. Similarly, other incorrect deriva-
tions are caused by inadequate pattern-matching
of XL and/or XR.
Previous research showed that phrases should
be constrained to some extent for improving trans-
lation quality. Most of the existing approaches uti-
lized syntactic information to constrain phrases to
respect syntactic boundaries. Chiang (2005) in-
troduced a constituent feature to reward phrases
that match a syntactic tree but did not yield signif-
icant improvement. Marton and Resnik (2008) re-
vised this method by distinguishing different con-
stituent syntactic types, and defined features for
each type to count whether a phrase matches or
crosses the syntactic boundary. This led to a sub-
stantial improvements. Gimpel and Smith (2008)
presented rich contextual features on the source
side including constituent syntactical features for
phrase-based translation. Cherry (2008) utilized
a dependency tree as a soft constraint to detect
syntactic cohesion violations for a phrase-based
383
?1
ta
?2
jiang
??3
chengwei
??4
yindu
????5
youshiyilai
?6
de
??7
shouwei
?8
n?
??9
zongtong
She1 will2 become3 the4 first5 female6 president7 in8 India?s9 history10
X[5,5]
X[4,5]
X[3,5]
X[2,5]
X[1,5]
X[7,7]
X[7,8]
X[7,9]
Figure 1: An example of Chinese-English translation. The rule X ? ?XL de XR, XR in XL?
pattern-matches 5 and 3 spans on the left and right of the Chinese word ?de?, respectively.
Sc ? ????? X, She will become X?
? ????? X[4,5]? X[7,9], She will become X[7,9] in X[4,5]?
? ????? ??????? ?? ??????,
She will become the first female president in India?s history?
Figure 2: The correct derivation with adequate pattern-matching of XR.
Si ? ????? X ??, She will become X president?
? ????? X[4,5]? X[7,8]??, She will become X[7,8] in X[4,5] president?
? ????? ??????? ?? ???? ???,
She will become the first female in India?s history president?
Figure 3: A wrong derivation with inadequate pattern-matching of XR.
system. Xiong et al (2009) presented a syntax-
driven bracketing model to predict whether two
phrases are translated together or not, using syn-
tactic features learned from training corpus. Al-
though these approaches differ from each other,
the main basic idea is the utilization of syntactic
information.
In this paper, we present a novel approach to
learn phrase boundaries for hierarchical phrase-
based translation. A phrase boundary indicates the
beginning or ending of a phrase reordering. Moti-
vated by Ng and Low (2004) that built a classifier
to predict word boundaries for word segmenta-
tion, we build a classifier to predict phrase bound-
aries. We classify each source word into one of the
4 boundary tags: ?b? indicates the beginning of a
phrase, ?m? indicates a word appears in the mid-
dle of a phrase, ?e? indicates the end of a phrase,
?s? indicates a single-word phrase.
We use phrase boundaries as soft constraints for
decoding. To do this, we incorporate our classifier
as a feature into the HPB model and propose an
efficient decoding algorithm.
Compared to the previous work, out approach
has the following advantages:
? Our approach maintains the strength of the
phrase-based models since it does not re-
quire any syntactical information. There-
fore, phrases do not need to respect syntactic
boundaries.
? The training instances are directly learned
from a word-aligned bilingual corpus, rather
than from manually annotated corpus.
384
? The decoder outputs phrase segmentation in-
formation as a byproduct, in addition to
translation result.
We evaluate our approach on large-scale
Chinese-to-English translation. Experimental re-
sults and analysis show that using phrase bound-
aries as soft constraints achieves significant im-
provements over the baseline system.
2 Previous Work
2.1 Learning Word Boundaries
In some languages, such as Chinese, words are not
demarcated. Therefore, it is a preliminary task to
determine word boundaries for a sentence, which
is the so-called word segmentation.
Ng and Low (2004) regarded word segmen-
tation as a classification problem. They labelled
each Chinese character with one of 4 possible
boundary tags: ?b?, ?m?, ?e? respectively indi-
cates the begin, the middle and the end of a word,
and ?s? indicates a single-character word. Their
segmenter was built within a maximum entropy
framework and trained on manually segmented
sentences.
Learning phrase boundaries is analogous to
word boundaries. The basic difference is that
the unit for learning word boundaries is charac-
ter while the unit for learning phrase boundaries
is word. In this paper, we adopt the boundary
tags presented by Ng and Low (2004) and build a
classifier to predict phrase boundaries within max-
imum entropy framework. We train it directly on a
word-aligned bilingual corpus, without any man-
ually annotation and syntactical information.
2.2 The Hierarchical Phrase-based Model
We built a hierarchical phrase-based MT system
(Chiang, 2007) based on weighted SCFG. The
translation knowledge is represented by rewriting
rules:
X ? ??, ?,?? (2)
where X is a non-terminal, ? and ? are source and
target strings, respectively. Both of them contain
words and possibly co-indexed non-terminals. ?
describes a one-to-one correspondence between
non-terminals in ? and ?.
Chiang (2007) used the standard log-linear
framework (Och and Ney, 2002) to combine var-
ious features:
Pr(e|f) ?
?
i
?ihi(?, ?) (3)
where hi(?, ?) is a feature function and ?i is
the weight of hi. Analogous to the previous
phrase-based model, Chiang defined the follow-
ing features: translation probabilities p(?|?) and
p(?|?), lexical weights pw(?|?) and pw(?|?),
word penalty, rule penalty, and a target n-gram
language model.
In this paper, we integrate a phrase boundary
classifier as an additional feature into the log-
linear model to provide soft constraint for pattern-
matching during decoding. The feature weights
are optimized by MERT algorithm (Och, 2003).
3 Learning Phrase Boundaries
We build a phrase boundary classifier (PBC)
within a maximum entropy framework. The PBC
predicts a boundary tag for each source word, con-
sidering contextual features:
Ptag(t|fj , F J1 ) =
exp(
?
i ?ihi(t, fj , F J1 ))?
t exp(
?
i ?ihi(t, fj , F J1 )
(4)
where, t ? {b, m, e, s}, fj is the jth word in
source sentence F J1 , hi is a feature function and
?i is the weight of hi.
To build PBC, we first present a method to rec-
ognize phrase boundaries and extract training ex-
amples from word-aligned bilingual corpus, then
we define contextual feature functions.
3.1 Phrase Boundary
During decoding, intuitively, words within a
phrase should be translated or moved together.
Therefore, a phrase boundary should indicate re-
ordering information. We assign one of the
boundary tags (b,m, e, s) to each word in source
sentences. Thus the word with tag b, e or s is a
phrase boundary. One question is that how to as-
sign boundary tag to a word? In this paper, we
recognize the largest source span which has the
monotone translation. Then we assign boundary
385
?? ?? ?
jointly held by
(a)
?? ??
a short visit
(b)
Figure 4: Illustration for monotone span (a) and
PM span (b).
tags to each word in the source span, according to
their position.
To do this, we first introduce some notations.
Given a bilingual sentence (F J1 , EI1) together with
word alignment matrix A, we use L(Aj) and
H(Aj) to represent the lowest and highest tar-
get word position which links to the source word
fj , respectively. Since the word alignment for fj
maybe ?one-to-many?, all the corresponding tar-
get words will appear in the span [L(Aj),H(Aj)].
we define a source span [j1, j2] (1 ? j1 ? j2 ?
J) a monotone span, iff:
1. ?(j, i) ? A, j1 ? j ? j2 ? L(Aj1) ? i ?
H(Aj2)
2. ?k1, k2 ? [j1, j2], k1 ? k2 ? H(Ak1) ?
L(Ak2)
The first condition indicates that
(F j2j1 , E
H(Aj2 )
L(Aj1 )
) is a phrase pair as described
previously in phrase-based SMT models. While
the second condition indicates that the lower
target bound linked to a source word cannot be
lower than any target word position linked to the
previous source word. Therefore, a monotone
span does not contain crossed links or internal
reorderings.
Considering that word alignments could be
very noisy and complex in real-world data, we de-
fine pseudo-monotone (PM) span by loosening the
second condition:
?k1, k2 ? [j1, j2], k1 ? k2 ? L(Ak1) ? L(Ak2)
(5)
This condition allows crossed links to some ex-
tent by loosening the bound of Ak1 from upper
to lower. Figure 4 (a) shows an example of
monotone span, in which the translation is mono-
tone. While Figure 4 (b) is not a monotone span
because there is a cross link between the upper
bound of ???? and the lower bound of ????
on the target side. However, it is a PM span ac-
cording to the definition. Note that in some cases,
a source word may not be contained in any phrase
pair, therefore we consider a single word span as
a PM span, specificly.
An interesting feature of PM span is that if two
PM spans are consecutive on both source side and
their corresponding target side, the two PM spans
can be combined as a larger PM span. Formally,
(F jj1 , E
i
i1)
?
(F j2j+1, Ei2i+1) = (F
j2
j1 , E
i2
i1 ) (6)
where [j1, j] and [j+1, j2] are PM spans, [i1, i]
and [i + 1, i2] are the target spans corresponding
to [j1, j] and [j+1, j2], respectively. For example,
Figure 4 (a) shows a PM phrase pair that consists
of two small PM pairs ???, jointly? and ???
?, held by?.
In this paper, we are interested in phrase re-
ordering boundaries for a source sentence. We de-
fine translation span (TS) the largest possible PM
span. A TS may consist of one or more PM spans.
According to our definition, cross links may ap-
pear within PM spans but do not appear between
PM spans within a TS. Therefore, TS is the largest
possible span that will be translated as a unit and
phrase reorderings may occur between TSs during
decoding.
To obtain phrase boundary examples from
word-aligned bilingual sentences, we first find all
possible TSs and then assign boundary tags to
each word. For a TS [j1, j2] (j1 < j2) that contain
more than two words, we assign ?b? to the first
word fj1 and ?e? to the last word fj2 , and ?m? to
the middle words fj (j1 < j < j2). For a single
word span TS [j, j], we assign ?s? to the word fj .
Figure 5 shows an example of labelling source
words with boundary tags. The source sentence is
segmented into 4 TSs. Using the phrase boundary
information to guide decoding, the decoder will
produce the correct derivation and translation as
shown in Figure 2.
386
??
?
?
?
?
?
?
?
??
?
??
?
?
TAG b m e b e s b m e
She
will
become
the first
female
president
in
India?s
history
Figure 5: Illustration for labelling the source
words with boundary tags. The solid boxes
present word alignments. The bordered boxes are
TSs.
3.2 Feature Definition
The features we used for the PS model are anal-
ogous to (Ng and Low, 2004). For a word W0,
we define the following contextual features with a
window of ?n?:
? The word feature Wn, which denotes the left
(right) n words of the current word W0;
? The part-of-speech (POS) feature Pn, which
denotes the POS tag of the word Wn.
For example, the tag of the word ??? (be-
come)? in Figure 5 is ?e?, indicating that it is
the end of a phrase. If we set the context window
n = 2, the features of the word ??? (become)?
are:
? W?2=? W?1=? W0=? ? W1=? ?
W2=????
? P?2=r P?1=d P0=v P1=ns P2=l
We collect TSs from bilingual sentences to-
gether with the contextual features and used a
MaxEnt toolkit (Zhang, 2004) to train a PBC.
? ? ??
b 0.78 0.10 1.2e-5
m 6.4e-8 0.75 5.4e-5
e 2.1e-8 0.11 0.87
s 0.22 0.04 0.13
Table 1: The TPM for a source sentence. The
highest probability of each word is in bold.
4 Phrase Boundary Constrained
Decoding
Give a source sentence, we can assign boundary
tags to each word by running the PBC. During
decoding, a rule is prohibited to pattern-match
across phrase boundaries. By doing this, the PBC
is integrated as a hard constraint. However, this
method will invalidate a large number of rules and
the decoder suffers from a risk that there are not
enough rules to cover the source sentence.
Alternatively, inspired by previous approaches,
we integrate the phrase boundary classifier as a
soft constraint by incorporating it as a feature into
the HPB model:
hpbc(F J1 ) = log(
J?
j=1
Ptag(t|fj , F J1 )) (7)
To perform translation, for each word fj in
a source sentence F J1 , we first compute all tag
probabilities Ptag(t|fj), where t ? (b,m, e, s),
j ? [1, J ], according to Equation 4. Therefore, we
build a 4? J tag-word probability matrix (TPM).
TPM [i, j] indicates the probability of the word
fj labelled with the tag ti. Table 1 shows the
TPM for a source text ??????.
Then we select rule options from the rule ta-
ble that can be used for translating the source text.
Since each rule option (f? , e?, a) 1 can be regarded
as a bilingual sentence with word alignments, thus
we find all TS in f? and assign an initial tag (IT)
for each source word. This procedure is analogous
to label phrase boundary tags for a word-aligned
bilingual sentence. For example, the following
rules are used for translating the Chinese sentence
in Table 1:
1We keep word alignments of a rule when it is extracted
from bilingual sentence.
387
X ? ??bX?1 , She X1? (8)
X1 ? ??b??e, will become? (9)
Since both the source sides of these two rules
are PM spans according to the word alignments,
the IT sequences for rule (8) and (9) are ?b *?2
and ?b e?, respectively. According to Table 1,
the initial hpbc score for these two rules can be
computed as follows:
h(7)pbc = log(Ptag(b|?)) = log(TPM [1, 1]) (10)
h(8)pbc = log(Ptag(b|?)) + log(Ptag(e|??))
= log(TPM [1, 2]) + log(TPM [3, 3]) (11)
Note that to keep the tag sequence valid, e.g.
?m? follows ?b? rather than ?s?, the ITs maybe
updated during decoding. The tag-updating
should be consistent with the definition of TS as
described in Section 3.1. Specifically, when the
non-terminal symbol X is derived from its cov-
ered span f(X), the boundary tags should be up-
dated.
When a tag of word fj is updated from tk1 to
tk2 , the PBC score should also be updated accord-
ing to TPM:
?PBC = log(TPM [k2, j])? log(TPM [k1, j])
(12)
The following is a derivation of the source sen-
tence in Table 1:
S ? ??bX?1 , She X1?
? ??b?b?m??e, She will become?
When X1 is derived, the tag of its left boundary
word ??? is updated from ?b? to ?m?. The reason
is that after derivation, the combined span forms
a larger PM span and the left boundary of f(X1)
should be updated.
As a result, the hpbc score is recomputed:
hpbc(F 31 ) = h
(7)
pbc + h
(8)
pbc +?PBC (13)
where,
?PBC = log(TPM [2, 2])? log(TPM [1, 2])
(14)
2We use ?*? as a tag of the non-terminal symbol ?X1?
since it has not been derived.
The decoding algorithm is efficient since the
computing of the PBC score is a procedure of
table-lookup.
5 Experiments
5.1 Experimental Setup
Our experiments were on Chinese-to-English
translation. The training corpus (77M+81M) we
used are from LDC 3. The evaluation metric is
BLEU (Papineni et al, 2002), as calculated by
mteval-v11b.pl with case-insensitive matching of
n-grams, where n = 4.
To obtain word alignments, we first ran
GIZA++ (Och and Ney, 2002) in both translation
directions and then refined it by ?grow-diag-final?
method (Koehn et al, 2003).
For the language model, we used the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
two 4-gram models on xinhua portion of Giga-
Word corpus and the English side of the training
corpus.
The NIST MT03 test set is used to tune the fea-
ture weights of the log-linear model by MERT
(Och, 2003). We tested our system on the NIST
MT06 and MT08 test sets.
5.2 Results
The results are shown in Table 2. We tested vari-
ous settings of the context window. It is observed
that the small values of n (n = 1, 2) drop the
BLEU score, suggesting that perhaps there are not
enough contextual information. With more con-
textual information is used, the BLEU scores are
improved over all test sets. When n = 3, the most
significant improvements are obtained on MT06G
and MT08. The improvements over the baseline
are statistically significant at p < 0.01 by using
the significant test method described in (Koehn,
2004). While for MT06N, the optimized context
window size is n = 4 but the improvement is
not statistically significant. In most cases, with
n larger than 3, we do not obtain further improve-
ments because of the data sparseness for training
3LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards).
388
System MT06G MT06N MT08
baseline 14.66 34.42 26.29
+PBC (n=1) 13.78 33.20 24.58
+PBC (n=2) 14.34 34.21 25.87
+PBC (n=3) 15.19* 34.63 27.25*
+PBC (n=4) 14.76 34.73 26.70
Table 2: Results on the test sets with different con-
text window (n) of the phrase boundary classifier.
The largest BLEU score on each test set is in bold.
MT06G: MT06 GALE set. MT06N: MT06 NIST
set. *: significantly better than the baseline at
p < 0.01.
the classifier.
6 Discussion
The experimental results show that the phrase
boundary constrained method improves the BLEU
score over the baseline system. Furthermore, we
are interested in how the PBC affects the transla-
tion results? We compared the outputs generated
by the baseline and ?+PBC (n = 3)? system and
found some interesting translations. For example,
the translations of a source sentence of NIST08
are as follows 4:
? Src: ?b1 ??m2 ?m3 ??m4 ??e5 ???b6
?m7 ??e8 ??b9??m10??e11
? Ref: US1 Treasury-Secretary2 Arrives-in3
China4 for-a-Visit-with5 Environment6 and7
Exchange-Rate8 as9 Focus10,11
? HPB: US1 Treasury2 in-environmental-
protection6 and7 visit5 China4 is9 key11
to-the-concern-of10 the-exchange-rate8
? +PBC: US1 Treasury2 arrived-in3 China4
for-a-visit5 environmental-protection6 and7
exchange-rate8 is9 concerned-about10 the-
key11
In the example, both ???? and ???? in the
source sentence are the concern of the ?visit?.
Therefore, the source span [6, 8] indicates a co-
hesive phrase, which should be translated as a
4The co-indexes of the words on the source and target
sentence indicate word alignments.
whole unit. However, the baseline translates the
spans [6, 7] and [8, 8] separately. It moves [6, 7]
before ?visit China? and [8, 8] after ?concern?.
This makes an mistake on phrase reordering. We
observe that the ?+PBC? system produces a bet-
ter translation. After incorporating the PBC as
a soft constraint, the system assigns a boundary
tag to each source word and segments the source
sentence into three TSs. According to our defi-
nition, TSs are encouraged as pseudo-monotone
translation unit during decoding. As a result, the
?+PBC? system discourages some arbitrary re-
ordering rules and produces more fluent transla-
tion.
7 Conclusion and Future Work
This paper presented a phrase boundary con-
strained method for hierarchical phrase-based
translation. A phrase boundary indicates begin
or end of a phrase reordering. We built a phrase
boundary classifier within a maximum entropy
framework and learned phrase boundary exam-
ples directly from word-aligned bilingual corpus.
We proposed an efficient decoding method to in-
tegrate the PBC into the decoder as a soft con-
straint. Experiments and analysis show that the
phrase boundary constrained method achieves sig-
nificant improvements over the baseline system.
The most advantage of the PBC is that it han-
dles both syntactic and non-syntactic phrases. In
the future, We would like to try different meth-
ods to determine more informative phrase bound-
aries, e.g. Xiong et al (2010) proposed a method
to learn translation boundaries from a hierarchical
tree that decomposed from word alignments using
a shift-reduce algorithm. In addition, we will try
more features as described in (Chiang et al, 2008;
Chiang et al, 2009), e.g. the length of the phrases
that covered by non-terminals.
References
Cherry, Colin. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings
of the 46rd Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, page 72?80.
Chiang, David, Yuval Marton, and Philip Resnik.
389
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, page 224?233.
Chiang, David, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, page 218?226.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages
33(2):201?228.
Gimpel, Kevin and Noah A. Smith. 2008. Rich
source-side context for statistical machine transla-
tion. In In Proceedings of the ACL-2008 Workshop
on Statistical Machine Translation (WMT-2008),
pages 9?17.
Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 388?
395.
Marton, Yuval and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of the 46rd Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1003?1011.
Ng, Hweetou and Jinkiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2004), pages
277?284.
Och, Franz Josef and Hermann Ney. 2002. Dis-
criminative training and maximum entropy models
for statistical machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 295?302.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. 30:417?449.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu.
2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318.
Stolcke, Andreas. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Process-
ing, volume 2, pages 901?904.
Xiong, Deyi, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In ACL-IJCNLP 2009, page
315?323.
Xiong, Deyi, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the ACL, page 136?144.
Zhang, Le. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
390
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 555?563,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Maximum Entropy Based Phrase Reordering
for Hierarchical Phrase-based Translation
Zhongjun He Yao Meng Hao Yu
Fujitsu R&D Center CO., LTD.
15/F, Tower A, Ocean International Center, 56 Dongsihuan Zhong Rd.
Chaoyang District, Beijing, 100025, China
{hezhongjun, mengyao, yu}@cn.fujitsu.com
Abstract
Hierarchical phrase-based (HPB) translation
provides a powerful mechanism to capture
both short and long distance phrase reorder-
ings. However, the phrase reorderings lack of
contextual information in conventional HPB
systems. This paper proposes a context-
dependent phrase reordering approach that
uses the maximum entropy (MaxEnt) model
to help the HPB decoder select appropriate re-
ordering patterns. We classify translation rules
into several reordering patterns, and build a
MaxEnt model for each pattern based on var-
ious contextual features. We integrate the
MaxEnt models into the HPB model. Ex-
perimental results show that our approach
achieves significant improvements over a stan-
dard HPB system on large-scale translation
tasks. On Chinese-to-English translation,
the absolute improvements in BLEU (case-
insensitive) range from 1.2 to 2.1.
1 Introduction
The hierarchical phrase-based (HPB) model (Chi-
ang, 2005; Chiang, 2007) has been widely adopted
in statistical machine translation (SMT). It utilizes
synchronous context free grammar (SCFG) rules
to perform translation. Typically, there are three
types of rules (see Table 1): phrasal rule, a phrase
pair consisting of consecutive words; hierarchical
rule, a hierarchical phrase pair consisting of both
words and variables; and glue rule, which is used to
merge phrases serially. Phrasal rule captures short
distance reorderings within phrases, while hierar-
chical rule captures long distance reorderings be-
Type Constituent ExamplesWord Variable
PR
?
- X ? ???, one of?
HR
? ?
X ? ?X, ofX?
GR -
?
S ? ?SX, SX?
Table 1: A classification of grammar rules for the HPB
model. PR = phrasal rule, HR = hierarchical rule, GR =
glue rule.
tween phrases. Therefore, the HPB model outper-
forms conventional phrase-based models on phrase
reorderings.
However, HPB translation suffers from a limita-
tion, in that the phrase reorderings lack of contex-
tual information, such as the surrounding words of
a phrase and the content of sub-phrases that rep-
resented by variables. Consider the following two
hierarchical rules in translating a Chinese sentence
into English:
X ? ?X1  X2, X1 ?s X2? (1)
X ? ?X1  X2, X2X1? (2)
? ?d  !
with Russia ?s talks
talks with Russia
Both pattern-match the source sentence, but pro-
duce quite different phrase reorderings. The first
rule generates a monotone translation, while the sec-
ond rule swaps the source phrases covered by X1
and X2 on the target side. During decoding, the first
555
rule is more likely to be used, as it occurs more fre-
quently in a training corpus. However, the exam-
ple is not a noun possessive case because the sub-
phrase covered by X1 is not a noun but a preposi-
tional phrase. Thus, without considering informa-
tion of sub-phrases, the decoder may make errors on
phrase reordering.
Contextual information has been widely used to
improve translation performance. It is helpful to re-
duce ambiguity, thus guide the decoder to choose
correct translation for a source text. Several re-
searchers observed that word sense disambiguation
improves translation quality on lexical translation
(Carpuat and Wu, 2007; Chan et al, 2007). These
methods utilized contextual features to determine
the correct meaning of a source word, thus help an
SMT system choose an appropriate target transla-
tion.
Zens and Ney (2006) and Xiong et al (2006)
utilized contextual information to improve phrase
reordering. They addressed phrase reordering as
a two-class classification problem that translating
neighboring phrases serially or inversely. They built
a maximum entropy (MaxEnt) classifier based on
boundary words to predict the order of neighboring
phrases.
He et al (2008) presented a lexicalized rule selec-
tion model to improve both lexical translation and
phrase reordering for HPB translation. They built
a MaxEnt model for each ambiguous source side
based on contextual features. The method was also
successfully applied to improve syntax-based SMT
translation (Liu et al, 2008), using more sophisti-
cated syntactical features. Shen et al (2008) inte-
grated various contextual and linguistic features into
an HPB system, using surrounding words and de-
pendency information for building context and de-
pendency language models, respectively.
In this paper, we focus on improving phrase re-
ordering for HPB translation. We classify SCFG
rules into several reordering patterns consisting of
two variables X and F (or E) 1, such as X1FX2
and X2EX1. We treat phrase reordering as a classi-
fication problem and build a MaxEnt model for each
source reordering pattern based on various contex-
1We use F and E to represent source and target words, re-
spectively.
tual features. We propose a method to integrate the
MaxEnt models into an HPB system. Specifically:
? For hierarchical rules, we classify the source-
side and the target-side into 7 and 17 reordering
patterns, respectively. Target reordering pat-
terns are treated as possible labels. We then
build a classifier for each source pattern to pre-
dict phrase reorderings. This is different from
He et al (2008), in which they built a clas-
sifier for each ambiguous hierarchical source-
side. Therefore, the training examples for each
MaxEnt model is small and the model maybe
unstable. Here, we classify source hierarchical
phrases into 7 reordering patterns according to
the arrangement of words and variables. We
can obtain sufficient samples for each MaxEnt
model from large-scale bilingual corpus.
? For glue rules, we extend the HPB model by
using bracketing transduction grammar (BTG)
(Wu, 1996) instead of the monotone glue rule.
By doing this, there are two options for the de-
coder to merge phrases: serial or inverse. We
then build a classifier for glue rules to predict
reorderings of neighboring phrases, analogous
to Xiong et al (2006).
? We integrate the MaxEnt based phrase reorder-
ing models as features into the HPB model
(Chiang, 2005). The feature weights can be
tuned together with other feature functions by
MERT algorithm (Och, 2003).
Experimental results show that the presented method
achieves significant improvement over the baseline.
On Chinese-to-English translation tasks of NIST
evluation, improvements in BLEU (case-insensitive)
are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set,
and 2.1 on MT08.
The rest of the paper is structured as follows: Sec-
tion 2 describes the MaxEnt based phrase reorder-
ing method. Section 3 integrates the MaxEnt mod-
els into the translation model. In Section 4, we re-
port experimental results. We analyze the presented
method and experimental results in Section 5 and
conclude in Section 6.
556
Source phrase Target phrase
X and
X ? with X
between X and
Figure 1: A source hierarchical phrase and its corre-
sponding target translation.
2 MaxEnt based Phrase Reordering
We regard phrase reordering as a pattern classifica-
tion problem. A reordering pattern indicates an ar-
rangement of words and variables. Although there
are a large amount of hierarchical rules may be ex-
tracted from bilingual corpus, these rules can be
classified into several reordering patterns (Section
2.1). In addition, we extend the HPB model with
BTG, that adding an inverted glue rule to merge
phrases inversely (Section 2.2). Therefore, the glue
rules are classified into two patterns: serial or in-
verse. We then build a MaxEnt phrase reordering
(MEPR) classifier for each source reordering pattern
(Section 2.3). In Section 2.4, we describe contextual
features.
2.1 Reordering Pattern Classification for
Hierarchical Rule
Hierarchical rule, consisting of both words and vari-
ables, is of great importance for the HPB model.
During decoding, words are used for lexical trans-
lation, and variables capture phrase reordering. We
may learn millions of hierarchical rules from a bilin-
gual corpus. Although these rules are different from
each other, they can be classified into several re-
ordering patterns according to the arrangement of
variables and words.
In this paper, we follow the constraint as de-
scribed in (Chiang, 2005) that a hierarchical rule
can have at most two variables and they cannot be
adjacent on the source side. We use ?X? to rep-
resent the variable, and ?F ? and ?E? to represent
word strings in source and target language, respec-
tively. Therefore, in a hierarchical rule, E is the lex-
ical translation of F , while the order of X and E
contains phrase reordering information.
For the hierarchical rule that contains one vari-
able (see Figure 1 for example), both the source and
the target phrases can be classified into three pat-
Source pattern Target pattern
XF XE
FX EX
FXF EXE
Table 2: A classification of the source side and the target
side for the hierarchical rule that contains one variable.
Source pattern Target pattern
X1EX2
X2EX1
X1X2E
X2X1E
EX1X2
X1FX2 EX2X1
X1FX2F X1EX2E
FX1FX2 X2EX1E
FX1FX2F EX1X2E
EX2X1E
EX1EX2
EX2EX1
EX1EX2E
EX2EX1E
Table 3: A classification of the source side and the target
side for the hierarchical rule that contains two variables.
terns (Table 2). To reduce the complexity of clas-
sification, we do not distinguish the order of word
strings. For example, we consider ?e1Xe2? and
?e2Xe1? as the same pattern ?EXE?, because the
target words are determined by lexical translation of
source words. Our focus is the order between X and
E. During decoding the phrases covered by X are
dynamically changed and the contextual information
of these phrases is ignored for pattern-matching of
hierarchical rules.
Analogously, for the hierarchical rule that con-
tains two variables, the source phrases are classified
into 4 patterns, while the target phrases are classified
into 14 patterns, as shown in Table 3. The pattern
number on the source side is less than that on the
target side, because on the source side, ?X1? always
appears before ?X2?, and they cannot be adjacent.
557
2.2 Reordering Pattern Classification for Glue
Rule
The HPB model used glue rule to combine phrases
serially. The reason is that in some cases, there are
no valid translation rules that cover a source span.
Therefore, the glue rule provides a default monotone
combination of phrases in order to complete a trans-
lation. This is not sufficient because in certain cases,
the order of phrases may be inverted on the target-
side.
In this paper, we extend the glue rule with BTG
(Wu, 1996), which consists of three types of rules:
X ? ?f? , e?? (3)
X ? ?X1X2, X1X2? (4)
X ? ?X1X2, X2X1? (5)
Rule 3 is a phrasal rule that translates a source
phrase f? into a target phrase e?. Rule 4 merges two
consecutive phrases in monotone order, while Rule
5 merges them in inverted order. During decod-
ing, the decoder first uses Rule 3 to produce phrase
translation, and then iteratively uses Rule 4 and 5 to
merge two neighboring phrases into a larger phrase
until the whole sentence is covered.
We replace the original glue rules in the HPB
model with BTG rules (see Table 4). We believe
that the extended HPB model can benefit from BTG
in the following aspects:
? In the HPB model, as we mentioned, hierarchi-
cal rules are constrained in that nonterminals
cannot be adjacent on the source side, i.e., the
source side cannot contain ?X1X2?. One rea-
son is that it will heavily increase the rule table
size. The other reason is that it can cause a spu-
rious ambiguity problem (Chiang, 2005). The
inverted glue rule in BTG, however, can solve
this problem.
? In the HPB model, only a monotone glue rule
is provided to merge phrases serially. In the ex-
tended HPB model, the combination of phrases
is classified into two types: monotone and in-
verse.
Analogous to Xiong et al (2006), to perform
context-dependent phrase reordering, we build a
Glue Rule Extended Glue Rule
S ? ?X,X? S ? ?X,X?
S ? ?SX, SX? X ? ?X1X2, X1X2?
- X ? ?X1X2, X2X1?
Table 4: Extending the glue rules in the HPB model with
BTG.
MaxEnt based classifier for glue rules to predict the
order of two neighboring phrases. In this paper, we
utilize more contextual features.
2.3 The MaxEnt based Phrase Reordering
Classifier
As described above, we classified phrase reorderings
into several patterns. Therefore, phrase reordering
can be regarded as a classification problem: for each
source reordering pattern, we treat the correspond-
ing target reordering patterns as labels.
We build a general classification model within the
MaxEnt framework:
Pme(T? |T?, ?, ?) =
exp(?i ?ihi(?, ?, f(X), e(X))
?
T? exp(
?
i ?ihi(?, ?, f(X), e(X))
(6)
where, ? and ? are the source and target side, re-
spectively. T?/T? is the reordering pattern of ?/?.
f(X) and e(X) are the phrases that covered by X
one the source and target side, respectively. Given
a source phrase, the model predicts a target reorder-
ing pattern, considering various contextual features
(Section 2.4).
According to the classification of reordering pat-
terns, there are 3 kinds of classifiers:
? P hr1me includes 3 classifiers for the hierarchical
rules that contain 1 variable. Each of the clas-
sifier has 3 labels;
? P hr2me includes 4 classifiers for the hierarchical
rules that contain 2 variables. Each of the clas-
sifier has 14 labels;
? P grme includes 1 classifier for the glue rules. The
classifier has 2 labels that predict a monotone
or inverse order for two neighboring phrases.
This classifier is analogous to (Xiong et al,
2006).
558
There are 8 classifiers in total. This is much fewer
than the classifiers in He et al (2008), in which a
classifier was built for each ambiguous hierarchical
source side. In this way, a classifier may face the
risk that there are not enough samples for training a
stable MaxEnt model. While our approach is more
generic, rather than training a MaxEnt model for a
specific hierarchical source side, we train a model
for a source reordering pattern. Thus, we reduce the
number of classifiers and can extract large training
examples for each classifier.
2.4 Feature definition
For a reordering pattern pair ?T?, T??, we design
three feature functions for phrase reordering classi-
fiers:
? Source lexical feature, including boundary
words and neighboring words. Boundary
words are the left and right word of the source
phrases covered by f(X), while neighboring
words are the words that immediately to the left
and right of a source phrase f(?);
? Part-of-Speech (POS) feature, POS tags of the
boundary and neighboring words on the source
side.
? Target lexical feature, the boundary words of
the target phrases covered by e(X).
These features can be extracted together with
translation rules from bilingual corpus. However,
since the hierarchical rule does not allow for adja-
cent variables on the source side, we extract features
for P grme by using the method described in Xiong et
al. (2006). We train the classifiers with a MaxEnt
trainer (Zhang, 2004).
3 Integrating the MEPR Classifier into the
HPB Model
The HPB model is built within the standard log-
linear framework (Och and Ney, 2002):
Pr(e|f) ?
?
i
?ihi(?, ?) (7)
where hi(?, ?) is a feature function and ?i is the
weight of hi. The HPB model has the following fea-
tures: translation probabilities p(?|?) and p(?|?),
lexical weights pw(?|?) and pw(?|?), word penalty,
phrase penalty, glue rule penalty, and a target n-
gram language model.
To integrate the MEPR classifiers into the transla-
tion model, the features of the log-linear model are
changed as follows:
? We add the MEPR classifier as a feature func-
tion to predict reordering pattern:
hme(T? |T?) =
?
Pme(T? |T?, ?, ?) (8)
During decoding, we first classify each source
phrase into one of the 8 source reordering pat-
terns and then use the corresponding MEPR
classifier to predict the possible target reorder-
ing pattern. Therefore, the contextual informa-
tion guides the decoder to perform phrase re-
ordering.
? We split the ?glue rule penalty? into two fea-
tures: monotone glue rule number and inverted
glue rule number. These features reflect pref-
erence of the decoder for using monotone or
inverted glue rules.
The advantage of our extension method is that the
weights of the new features can be tuned together
with the other features by MERT algorithm (Och,
2003).
We utilize a standard CKY algorithm for decod-
ing. Given a source sentence, the decoder searches
the best derivation from the bottom to top. For a
source span [j1, j2], the decoder uses three kinds of
rules: translation rules produce lexical translation
and phrase reordering (for hierarchical rules), mono-
tone rule merges any neighboring sub-spans [j1, k]
and [k + 1, j2] serially, and inverted rule swap them.
Note that when the decoder uses the monotone and
inverted glue rule to combine sub-spans, it merges
phrases that do not contain variables. Because the
CKY algorithm guarantees that the sub spans [j1, k]
and [k + 1, j2] have been translated before [j1, j2].
559
4 Experiments
We carried out experiments on four systems:
? HPB: replication of the Hiero system (Chiang,
2005);
? HPB+MEHR: HPB with MaxEnt based classi-
fier for hierarchical rules, as described in Sec-
tion 2.1;
? HPB+MEGR: HPB with MaxEnt based classi-
fier for glue rules, as described in Section 2.2;
? HPB+MER: HPB with MaxEnt based classifier
for both hierarchical and glue rules.
All systems were tuned on NIST MT03 and tested
on MT06 and MT08. The evaluation metric was
BLEU (Papineni et al, 2002) with case-insensitive
matching of n-grams, where n = 4.
We evaluated our approach on Chinese-to-
English translation. The training data contained
77M Chinese words and 81M English words.
These data come from 17 corpora: LDC2002E18,
LDC2002L27, LDC2002T01, LDC2003E07,
LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34,
LDC2006E24, LDC2006E26, LDC2006E34,
LDC2006E86, LDC2006E92, LDC2006E93,
LDC2004T08 (HK News, HK Hansards).
To obtain word alignments, we first ran GIZA++
(Och and Ney, 2000) in both translation directions
and then refined the results using the ?grow-diag-
final? method (Koehn et al, 2003). For the lan-
guage model, we used the SRI Language Modeling
Toolkit (Stolcke, 2002) to train two 4-gram models
on the Xinhua portion of the GigaWord corpus and
the English side of the training corpus.
4.1 Statistical Information of Rules
Hierarchical Rules
We extracted 162M translation rules from the train-
ing corpus. Among them, there were 127M hi-
erarchical rules, which contained 85M hierarchical
source phrases. We classified these source phrases
into 7 patterns as described in Section 2.1. Table
5 shows the statistical information. We observed
that the most frequent source pattern is ?FXF ?,
Source Pattern Percentage (%)
XF 9.7
FX 9.7
FXF 46.1
X1FX2 3.7
X1FX2F 11.9
FX1FX2 11.8
FX1FX2F 7.1
Table 5: Statistical information of reordering pattern clas-
sification for hierarchical source phrases.
# Source
Target (%) FX XF FXF
EX 82.8 7 4.6
XE 6.4 82.4 2.9
EXE 10.8 10.6 92.5
Table 6: Percentage of target reordering pattern for each
source pattern containing one variable.
which accounted for 46.1% of the total. Interest-
ingly, ?X1FX2?, accounting for 3.7%, was the least
frequent pattern. Table 6 and Table 7 show the
distributions of reordering patterns for hierarchical
source phrases that contain one and two variables,
respectively. From both the tables, we observed
that for Chinese-to-English translation, the most fre-
quent ?reordering? pattern for a source phrase is
monotone translation (bold font in the tables).
Glue Rules
To train a MaxEnt classifier for glue rules, we ex-
tracted 65.8M reordering (monotone and inverse)
instances from the training data, using the algo-
rithm described in Xiong et al (2006). There were
63M monotone instances, accounting for 95.7%. Al-
though instances of inverse reordering accounted for
4.3%, they are important for phrase reordering.
4.2 Results
Table 8 shows the BLEU scores and decoding speed
of the four systems on MT06 (GALE set and NIST
set) and MT08. From the table, we made the follow-
ing observations:
560
# Source
Target (%) FX1FX2 FX1FX2F X1FX2 X1FX2F
EX1EX2 78.1 3.6 4.6 1.2
EX1EX2E 2.1 75.9 0.1 1.6
EX1X2 6.8 0.1 2.8 0.1
EX1X2E 1.8 11.2 0.1 2
EX2EX1 2.8 1.4 2 1.2
EX2EX1E 1.4 2.3 0.7 1.1
EX2X1 0.9 0.1 2.2 0.2
EX2X1E 1 1.1 0.9 1.0
X1EX2 1.9 0.1 71.2 3.3
X1EX2E 0.7 2.1 6 78.4
X1X2E 0.1 0.1 2.8 5.9
X2EX1 0.9 0.4 1.6 0.7
X2EX1E 1.5 1.5 2.6 2.4
X2X1E 0.1 0.04 2.2 0.8
Table 7: Percentage of target reordering pattern for each source pattern containing two variables.
System Test Data Speed06G 06N 08
HPB 14.19 33.93 25.85 8.7
HPB+MEHR 14.76 34.95 26.56 3.2
HPB+MEGR 15.09 35.72 27.34 2.7
HPB+MER 15.42 35.80 27.94 1.7
Table 8: BLEU percentage scores and translation speed (words/second) on test data. G=GALE set, N=NIST set. All
improvements are statistically significant (p < 0.01). Note that MT06G has one reference for each source sentence,
while the MT06N and MT08 have four references.
? The HPB+MEHR system achieved significant
improvements on all test sets compared to the
HPB system. The absolute increases in BLEU
scores ranging from 0.6 (on 06G) to 1.0 (on
06N) percentage points. This indicates that the
ME based reordering for hierarchical rules im-
proves translation performance.
? The HPB+MEGR system achieved significant
improvements over the HPB system. The ab-
solute increases in BLEU scores ranging from
0.9 (on 06G) to 1.8 (on 06N) percentage points.
The HPB+MEGR system overcomes the short-
coming of the HPB system by using both
monotone glue rule and inverted glue rule,
which merging phrases serially and inversely,
respectively. Furthermore, the HPB+MEGR
system outperformed the HPB+MEHR system.
? The HPB+MER system achieved the best per-
formances on all test sets, with absolute in-
creases of BLEU scores ranging from 1.2 (on
06G) to 2.1 (on 08). The system combin-
ing with ME based reordering for both hier-
archical and glue rules, outperformed both the
HPB+MEHR and HPB+MEGR systems.
? In addition, we found that the decoder takes
more time after adding the MEPR models (the
speed column of Table 8). The average transla-
tion speed of HPB+MER (1.7 words/second) is
about 5 times slower than the HPB system (8.7
words/second). One reason is that the MEPR
models utilized contextual information to com-
pute classification scores. Another reason is
that adding inverted glue rules increases search
space.
561
5 Analysis
Experiments showed that the presented approach
achieved significant gains on BLEU scores. Further-
more, we sought to explore what would happen af-
ter integrating the MEPR classifiers into the transla-
tion model. We compared the outputs of HPB and
HPB+MER and observed that the translation perfor-
mance are improved on phrase reordering. For ex-
ample, the translations of a source sentence in MT08
are as follows 2:
? Src: ?I1 ?2 ??3 .4 m?5 ??6
?7 ?m8 J?9 4010 ?11 ??12 13 
?14 Oy15
? Ref: At the end4 of last3 month3, the
South1 Korean1 government2 began5 a plan15
to provide9 400,00010 tonnes11 of rice12 as
aid14 to North8 Korea8
? HPB: South Korean government late last
month to start with 400,000 tons of rice aid to
the DPRK
? HPB+MER: Start at the end of last month,
South Korean government plans to provide
400,000 tons of rice in aid to the DPRK
The most obvious error that the baseline system
makes is the order of the time expression ???
., the end of last month?, which should be either
at the beginning or the end on target side. However,
the baseline produced a monotone translation by us-
ing the rule ??I ? X1, South Korean govern-
ment X1?. The HPB+MER system, however, moved
the time expression to the beginning of the sentence
by using the rule ??I ? X1, X1 South Ko-
rean government?. The reason is that the MaxEnt
phrase reordering classifier uses the contextual fea-
tures (e.g. the boundary words) of the phrase cov-
ered by X1 to predict the phrase reordering as X1E
for the source phrase FX1.
2The co-indexes of the words in the source and reference
sentence indicate word alignments.
6 Conclusions and Future Work
In this paper, we have proposed a MaxEnt based
phrase reordering approach to help the HPB decoder
select reordering patterns. We classified hierarchical
rules into 7 reordering patterns on the source side
and 17 reordering patterns on the target side. In ad-
dition, we introduced BTG to enhance the reorder-
ing of neighboring phrases and classified the glue
rules into two patterns. We trained a MaxEnt clas-
sifier for each reordering pattern and integrated it
into a standard HPB system. Experimental results
showed that the proposed approach achieved signif-
icant improvements over the baseline. The absolute
improvements in BLEU range from 1.2 to 2.1.
MaxEnt based phrase reordering provides a mech-
anism to incorporate various features into the trans-
lation model. In this paper, we only use a few fea-
ture sets based on standard contextual word and POS
tags. We believe that additional features will fur-
ther improve translation performance. Such features
could include syntactical features (Chiang et al,
2009). In the future, we will carry out experiments
on deeper features and evaluate the effects of differ-
ent feature sets.
References
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of EMNLP-CoNLL 2007, pages
61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 33?40.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics - Hu-
man Language Technologies 2009 Conference, page
218?226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 33(2):201?
228.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
562
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 321?328.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, page
89?97.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2008. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 72?80.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901?904.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of the
Thirty-Fourth Annual Meeting of the Association for
Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, pages 521?528.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
563
