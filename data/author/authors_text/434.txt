Wordform- and class-based prediction of the components
of German nominal compounds in an AAC system
Marco Baroni Johannes Matiasek
Austrian Research Institute for
Artificial Intelligence
Schottengasse 3,
A-1010 Vienna, Austria
{marco,john}@oefai.at
Harald Trost
Department of Medical Cybernetics and
Artificial Intelligence, University of Vienna
Freyung 6/2
A-1010 Vienna, Austria
harald@ai.univie.ac.at
Abstract
In word prediction systems for augmentative and al-
ternative communication (AAC), productive word-
formation processes such as compounding pose a
serious problem. We present a model that predicts
German nominal compounds by splitting them into
their modifier and head components, instead of try-
ing to predict them as a whole. The model is im-
proved further by the use of class-based modifier-
head bigrams constructed using semantic classes
automatically extracted from a corpus. The eval-
uation shows that the split compound model with
class bigrams leads to an improvement in keystroke
savings of more than 15% over a no split compound
baseline model. We also present preliminary results
obtained with a word prediction model integrating
compound and simple word prediction.
1 Introduction
N-gram language modeling techniques have been
successfully embedded in a number of natural lan-
guage processing applications, including word pre-
dictors for augmentative and alternative communi-
cation (AAC). N-gram based techniques rely cru-
cially on the assumption that the large majority of
words to be predicted have also occurred in the cor-
pus used to train the models.
Productive word-formation by compounding in
languages such as German, Dutch, the Scandina-
vian languages and Greek, where compounds are
commonly written as single orthographic words, is
problematic for this assumption.
Productive compounding implies that a sizeable
number of new words will constantly be added to
the language. Such words cannot, in principle, be
contained in any already existing training corpus,
no matter how large. Moreover, the training cor-
pus itself is likely to contain a sizeable number of
newly formed compounds that, as such, will have
an extremely low frequency, causing data sparse-
ness problems.
New compounds, however, differ from other
types of new/rare words in that, while they are rare,
they can typically be decomposed into more com-
mon smaller units (the words that were put together
to form them). For example, in the corpus we an-
alyzed, Abend ?evening? and Sitzung ?session?, the
two components of the German compound Abend-
sitzung ?evening session?, are much more frequent
words than the latter. Thus, a natural way to handle
productively formed compounds is to treat them not
as primitive units, but as the concatenation of their
components.
A model of this sort will be able to predict newly
formed compounds that never occurred in the train-
ing corpus, as long as they can be analyzed as the
concatenation of constituents that did occur in the
training corpus. Moreover, a model of this sort
avoids the specific type of data sparseness problems
caused by newly formed compounds in the training
corpus, since it collects statistics based on their (typ-
ically more frequent) components.
Building upon previous work (Spies, 1995;
Carter et al, 1996; Fetter, 1998; Larson et al,
2000), Baroni et al (2002) reported encouraging
results obtained with a model in which two-element
nominal German compounds are predicted by treat-
ing them as the concatenation of a modifier (left el-
ement) and a head (right element).
Here, we report of further improvements to this
model that we obtained by adding a class-based bi-
gram term to head prediction. As far as we know,
this it the first time that semantic classes auto-
matically extracted from the training corpus have
been used to enhance compound prediction, inde-
pendently of the domain of application of the pre-
diction model.
Moreover, we present the results of preliminary
experiments we conducted in the integration of
compound predictions and simple word predictions
within the AAC word prediction task.
The remainder of this paper is organized as fol-
lows. In section 2, we describe the AAC word pre-
diction task. In section 3, we describe the basic
properties of German compounds. In section 4, we
present our split compound prediction model, focus-
ing on the new class-based head prediction compo-
nent. In section 5, we report the results of simu-
lations run with the enhanced compound prediction
model. In section 6, we report about our prelimi-
nary experiments with the integration of compound
and simple word prediction. Finally, in section 7,
we summarize the main results we obtained and in-
dicate directions for further work.
2 Word prediction for AAC
Word prediction systems based on n-gram statistics
are an important component of AAC devices, i.e.,
software and possibly hardware typing aids for dis-
abled users (Copestake, 1997; Carlberger, 1998).
Word predictors provide the user with a predic-
tion window, i.e. a menu that, at any time, lists the
most likely next word candidates, given the input
that the user has typed until the current character.
If the word that the user intends to type next is in
the prediction window, the user can select it from
there. Otherwise, the user will keep typing letters,
until the target word appears in the prediction win-
dow (or until she finishes typing the word).
The (percentage) keystroke savings rate (ksr) is a
standard measure used in AAC research to evaluate
word predictors. The ksr can be thought of as the
percentage of keystrokes that a ?perfect? user would
save by employing the relevant word predictor to
type the test set, over the total number of keystrokes
that are needed to type the test set without using the
word predictor.
Usually, the ksr is defined by
ksr = (1? ki + ks
kn
) ? 100 (1)
where: ki is the number of input characters actually
typed, ks is the number of keystrokes needed to se-
lect among the predictions presented by the model
and kn is the number of keystrokes that would be
needed if the whole text was typed without any
prediction aid. Typically, the user will need one
keystroke to select among the predictions , and thus
we assume that ks equals 1.1
1In the split compound model, the user needs one keystroke
to select the modifier and one keystroke to select the head.
The ksr is influenced not only by the quality of
the prediction model but also by the size of the pre-
diction window. In our simulations, we use a 7 word
prediction window.
Ksr is not a function of perplexity, but it is gener-
ally true that there is an inverse correlation between
ksr and perplexity (Carlberger, 1998).
3 Compounding in German
Compounding is an extremely common and produc-
tive mean to form words in German.
In an analysis of the APA newswire corpus (a
corpus of over 28 million words), we found that al-
most half (47%) of the word types were compounds.
However, the compounds accounted for a small por-
tion of the overall token count (7%). This suggests
that, as expected, many of them are productively
formed hapax legomena or very rare words (83%
of the compounds had a corpus frequency of 5 or
lower).
By far the most common type of German com-
pound is the N+N type, i.e., a sequence of two
nouns (62% of the compounds in our corpus have
this shape). Thus, we decided to limit ourselves, for
now, to handling compounds of this shape.
In German, nominal compounds, including the
N+N type, are right-headed, i.e., the rightmost ele-
ment of the compound determines its basic semantic
and morphosyntactic properties.
Thus, the context of a compound is often more
informative about its right element (the head) than
about its left element (the modifier).
In modifier context, nouns are sometimes fol-
lowed by a linking suffix (Krott, 2001; Dressler et
al., 2001), or they take other special inflectional
shapes.
As a consequence of the presence of linking suf-
fixes and related patterns, the forms that nouns take
in modifier position are sometimes specific to this
position only, i.e., they are bound forms that do not
occur as independent words.
We did not parse special modifier forms in or-
der to reconstruct their independent nominal forms.
Thus, we treat all inflected modifier forms, includ-
ing bound forms, as unanalyzed primitive nominal
wordforms.
4 The split compound prediction model
In Baroni et al (2002), we present and evaluate a
split compound model in which N+N compounds
are predicted by treating them as the sequence of a
modifier and a head.
Modifiers are predicted on the basis of weighed
probabilities deriving from the following three
terms: the unigram and bigram training corpus fre-
quency of nominal wordforms as modifiers or in-
dependent words, and the training corpus type fre-
quency of nominal wordforms as modifiers:2
Pmod(w) = ?1P (w) + ?2P (w|c) + ?3Pismod(w) (2)
The type frequency of nouns as modifiers is de-
termined by the number of distinct compounds in
which a noun form occurs as modifier.
Heads are predicted on the basis of weighted
probabilities deriving from three terms analogous to
the ones used for modifiers: the unigram and bigram
frequency of nouns as heads or independent words,
and the type frequency of nouns as heads:
Phead(w) = ?1P (w) +?2P (w|c) +?3Pishead(w) (3)
The type frequency of nouns as heads is de-
termined by the number of distinct compounds in
which a noun form occurs as head.
Given that compound heads determine the syn-
tactic properties of compounds, bigrams for head
prediction are collected by considering not the im-
mediate left context of heads (i.e., their modifiers),
but the word preceding the compound (e.g., die
Abendsitzung is counted as an instance of the bi-
gram die Sitzung).
For reasons of size and efficiency, single uni- and
bigram count lists are used for predicting modifiers
and heads.3 For the same reasons, and to minimize
the chances of over-fitting to the training corpus, all
n-gram/frequency tables are trimmed by removing
elements that occur only once in the training corpus.
We currently use a simple interpolation model, in
which all terms are assigned equal weight.
4.1 Improving head prediction
While we obtained encouraging results with it (Ba-
roni et al, 2002), we feel that a particularly unsat-
isfactory aspect of the model described in the previ-
ous section is that information on the modifier is not
2Here and below, c stands for the last word in the left con-
text of w; w is the suffix of the word to be predicted minus
the (possibly empty) prefix typed by the user up to the current
point.
3This has a distorting effect on the bigram counts (words
occurring before compounds are counted twice, once as the left
context of the modifier and once as the left context of the head).
However, preliminary experiments indicated that the empirical
effect of this distortion is minimal.
exploited when trying to predict the head of a com-
pound. Intuitively, knowing what the modifier is
should help us in guessing the head of a compound.
However, constructing a plausible head-prediction
term based on modifier-head dependencies is not
straightforward.
The word-form-based compound-bigram fre-
quency of a head, i.e., the number of times a specific
head occurs after a specific modifier, is not a very
useful measure: Counting how often a modifier-
head pair occurs in the training corpus is equiv-
alent to collecting statistics on unanalyzed com-
pounds, and it will not help us to generalize beyond
the compounds encountered in the training corpus.
Moreover, if a specific modifier-head bigram is fre-
quent, i.e., the corresponding compound is a fre-
quent word, it is probably better to treat the whole
compound as an unanalyzed lexical unit anyway.
POS-based head-modifier bigrams are not going
to be of any help either, since we are considering
only N+N compounds, and thus we would collect a
single POS bigram (N N) with probability 1.4
We decided instead to try to exploit a
semantically-driven route. It seems plausible
that modifiers that are semantically related will
tend to co-occur with heads that are, in turn,
semantically related. Consider for example the
relationship between the class of fruits and the
class of sweets in English compounds. It is easy
to think of compounds in which a member of
the class of fruits (bananas, cherries, apricots...)
modifies a member of the class of sweets (pies,
cakes, muffins...). Thus, if you have to predict the
head of a compound given a fruit modifier, it would
be reasonable, all else being equal, to guess some
kind of sweet.
4.1.1 Class-based modifier-head bigrams
While semantically-driven prediction makes sense
in principle, clustering nouns into semantic classes
is certainly not a trivial job, and, if a large input lex-
icon must be partitioned, it is not a task that could
be accomplished by a human expert. Drawing inspi-
ration from Brown et al (1990), we constructed in-
stead semantic classes using a clustering algorithm
extracting them from a corpus, on the basis of the
average mutual information (MI) between pairs of
words (Rosenfeld, 1996).5
4Even if the model handled other compound types, very few
POS combinations are attested within compounds.
5We are aware of the fact that other measures of lexical as-
sociation have been proposed (Evert and Krenn, 2001, and
MI values were computed using Adam Berger?s
trigger toolkit (Berger, 1997).6 The same training
corpus of about 25.5M words (and with N+N com-
pounds split) that we describe below was used to
collect MI values for noun pairs. All modifiers and
heads of N+N compounds and all corpus words that
were parsed as nouns by the Xerox morphological
analyzer (Karttunen et al, 1997) were counted as
nouns for this purpose.
MI was computed only for pairs that co-occurred
at least three times in the corpus (thus, only a subset
of the input nouns appears in the output list). Valid
co-occurrences were bound by a maximal distance
between elements of 500 words, and a minimal dis-
tance of 2 words (to avoid lexicalized phrases, such
as proper names or phrasal loanwords).
Having obtained a list of pairs from the toolkit,
the next step was to cluster them into classes, by
grouping together nouns with a high MI. For space
reasons, we do not discuss our clustering algorithm
in detail here (we motivate and analyze the algo-
rithm in a paper currently in preparation).
In short, the algorithm starts by building classes
out of nouns that occur with very few other nouns in
the MI pair list, and thus their assignment to classes
is relatively unambiguous, and it then adds progres-
sively more ambiguous nouns (ambiguous in the
sense that they occur in a progressively larger num-
ber of MI pairs, and thus it becomes harder to deter-
mine with which other nouns they should be clus-
tered). Each input word is assigned to a single class
(thus, we do not try to capture polysemy). More-
over, not all words in the input are clustered (see
step 5 below).7
Schematically, the algorithm works as follows
(the input vocabulary of step 1 is simply a list of
all the words that occur at least once in the MI pair
references quoted there) and are sometimes claimed to be more
reliable than MI, and we are planning to run our clustering al-
gorithm using alternative measures.
6The trigger toolkit returns directional MI values (i.e., sepa-
rate MI values for the pairs N1 N2 and N2 N1). Since we were
not interested in directional information, we merged pairs con-
taining identical nouns by summing their MI. We realize that
this is not mathematically equivalent to computing symmetric
MI values, but it is a practical approximation that allowed us to
use the trigger toolkit for our purposes.
7We also experimented with an iterative version of the al-
gorithm that tried to cluster all words, through multiple passes.
The classes generated by the non-iterative procedure described
in the text, however, gave better results, when integrated in the
head prediction task, than those generated with the iterative ver-
sion.
list):
? step 1: Rank words in input vocabulary on the
basis of how often they occur in the MI pair list
(from least to most frequent);
? step 2: Shift top word from ranked list and de-
termine with the members of which existing
class it has the highest average mutual infor-
mation;
? step 3: If highest value found in step 2 is 0,
assign current word to new class; else, assign
it to class corresponding to highest value;
? step 4: If ranked list is not empty, go back to
step 2;
? step 5: Discard all classes that have only one
member.
This is a heuristic clustering procedure and there
is no guarantee that it will construct classes that
maximize MI. A cursory inspection of the output
list indicates that most classes constructed by our
algorithm are intuitively reasonable, while there are
also, undoubtedly, classes that contain heteroge-
neous elements, and missed generalizations. Table
1 reports a list of ten randomly selected classes that
were constructed using this procedure.
Alleinstehende, Singles, Alben, Platten, Platte, Sound,
Hits, Hit, Live, Songs, Single, Album, Pop, Studio,
Rock, Fans, Band
Atrophie, Hartung, Neurologe
Magische, Magie
Bilgen, Tivoli, Baur, Scharrer, Streiter, Winkel, Pfeffer,
Schmid, M
Effizienz, Transparenz
Harm, Radar, Jets, Flugzeugen, Typs, Abwehr, Raketen,
Maschinen, Angriffen, Flugzeuge, Kampf
Relegation, Birmingham, Stephen
Partnerschafts, Partnerschaft, Kooperation,
Bereichen, Aktivita?ten
Importeure, Zo?lle
Labyrinths, Labyrinth
Table 1: Randomly selected noun classes
The algorithm generated 3744 classes, containing
a total of 14059 nouns (about one third of the nouns
in the training corpus).
Class-based modifier-head bigrams were then
collected by labeling all the modifiers and heads in
the training corpus with their semantic classes, and
counting how often each combination of modifier
and head class occurred.
Like the other tables, class-based bigrams were
trimmed by removing elements with a frequency of
1.
4.1.2 The class-based head prediction model
We compute the class-based probability of a com-
pound head given its modifier in the following way:
Pclass(h|m) = P (Cl(h)|Cl(m))P (h|Cl(h)) (4)
where
P (Cl(h)|Cl(m)) =
count(Cl(m), Cl(h))
count(Cl(m))
(5)
and P (h|Cl(h)) = 1
|Cl(h)|
(6)
The latter term assigns equal probability to all
members of a class, but lower probability to mem-
bers of larger classes.
Class-based probability is added to the
wordform-based terms of equation 3 obtaining
the following formula to compute head probability:
Phead(w) = (7)
?1P (w) +?2P (w|c) +?3Pishead(w) +?4Pclass(w|m)
5 Evaluation
The new split compound model and a baseline
model with no compound processing were eval-
uated in a series of simulations, using the APA
newswire articles from January to September 1999
(containing 25,466,500 words) as the training cor-
pus, and all the 90,643 compounds found in the
Frankfurter Rundschau newspaper articles from
June 29 to July 12 of 1992 (in bigram context) as
the testing targets.8
In order to train and test the split compound
model, all words in both sets were run though the
morphological analyzer, and all N+N compounds
were split into their modifier and head surface
forms.
We first ran simulations in which compound
heads were predicted using each of the terms in
equation 7 separately. The results are reported in
table 2.
As an independent predictor, the class-based term
performs slightly worse than wordform-based bi-
gram prediction.
We then simulated head and compound predic-
tion using the head prediction model of equation 7.
8In other experiments, including those reported in Baroni
et al (2002), we tested on another section of the APA corpus
from the same year. Not surprisingly, ksr?s in the experiments
with the APA corpus were overall higher, and the difference
between the split compound and baseline models was less dra-
matic (because many compounds in the test set were already in
the training corpus).
model P (w) P (w|c) Pishead Pclass(w|m)
head ksr 42.2 30.0 47.1 29.4
Table 2: Predicting heads with single term models
The results of this simulation are reported in table
3, together with the results of a simulation in which
class-based prediction was not used, and the re-
sults obtained with the baseline no-split-compound
model.
Model split split no split
w/ classes no classes
head ksr 51.2 48.8 N/A
compound ksr 50.1 48.8 34.9
Table 3: Predicting heads and compounds
When used in conjunction with the other terms,
class bigrams lead to an improvement in head pre-
diction of more than 2% over the split compound
model without class-based prediction. This trans-
lates into an improvement of 1.3% in the prediction
of whole compounds. Overall, the split compound
model with class bigrams leads to an improvement
of more than 15% over the baseline model.
The results of these experiments confirm the use-
fulness of the split compound model, and they
also show that the addition of class-based predic-
tion improves the performance of the model, even
if this improvement is not dramatic. Clearly, fu-
ture research should concentrate on whether alterna-
tive measures of association, clustering techniques
and/or integration strategies can make class-based
prediction more effective.
6 Preliminary experiments in integration
In a working word prediction system, compounds
are obviously not the only type of words that the
user needs to type. Thus, the predictions provided
by the compound model must be integrated with
predictions of simple words. In this section, we re-
port preliminary results we obtained with a model
limited to the integration of N+N compound predic-
tion with simple noun prediction.
In our approach to compound/simple prediction
integration, candidate modifiers are presented to-
gether and in competition with simple word so-
lutions as soon as the user starts typing a new
word. The user can distinguish modifiers from sim-
ple words in the prediction window because the for-
mer are suffixed with a special symbol (for exam-
ple an underscore). If the user selects a modifier,
the head prediction model is activated, and the user
can start typing the prefix of the desired compound
head, while the system suggests completions based
on the head prediction model.
For example, if the user has just typed Abe,
the prediction window could contain, among other
things, the candidates Abend and Abend . If the
user selects the latter, possible head completions for
a compound having Abend as its modifier are pre-
sented.
Modifier candidates are proposed on the basis of
Pmod(w) computed as in equation 2 above. Simple
noun candidates are proposed on the basis of their
unigram and bigram probabilities (interpolated with
equal weights).
We experimented with two versions of the inte-
grated model.
In one, modifier and simple noun candidates are
ranked directly on the basis of their probabilities.
This risks to lead to over-prediction of modifier can-
didates (recall that, from the point of view of token
frequency, compounds are much rarer than simple
words; the prediction window should not be clut-
tered by too many modifier candidates when, most
of the time, users will want to type simple words).
Thus, we constructed a second version of the in-
tegrated model in which Pmod(w) is multiplied by a
penalty term. This term discounts the probability of
modifier candidates built from nominal wordforms
that occur more frequently in the training corpus as
independent nouns than as modifiers (forms that are
equally or more frequent in modifier position are not
affected by the penalty).
The same training corpus and procedures de-
scribed in section 5 above were used to train the two
versions of the integrated model, and the baseline
model that does not use compound prediction.
These models were tested by treating all the
nouns in the test corpus as prediction targets. The
integrated test set contained 90,643 N+N tokens and
395,731 more nouns. The results of the simulations
are reported in table 4.
Model integrated integrated simple pred
no penalty w/ penalty only
compound ksr 47.6 45.9 34.9
simple n ksr 40.5 42.5 45.6
combined ksr 42.5 43.5 42.6
Table 4: Integrated prediction
Because of the simple noun predictions getting in
the way, the integrated models perform compound
prediction worse than the non-integrated split com-
pound model of table 3. However the integrated
models still perform compound prediction consid-
erably better than the baseline model.
The integrated model with modifier penalties per-
forms worse than the model without penalties when
predicting compounds. This is expected, since the
modifier penalties make this model more conserva-
tive in proposing modifier candidates.
However, the model with penalties outperforms
the model without penalties in simple noun predic-
tion. Given that in our test set (and, we expect, in
most German texts) simple noun tokens greatly out-
number compound tokens, this results in an overall
better performance of the model with penalties.
The integrated model with penalties achieves
an overall ksr that is about 1% higher than that
achieved by the baseline model.
Thus, these preliminary experiments indicate that
an approach to integrating compound and simple
word predictions along the lines sketched at the be-
ginning of this section, and in particular the version
of the model in which modifier predictions are pe-
nalized, is feasible. However, the model is clearly in
need of further refinement, given that the improve-
ment over the baseline model is currently minimal.
7 Conclusion
The main result concerning German compound pre-
diction that was reported in this paper pertains to the
introduction of class-based modifier-head bigrams
to enhance head prediction.
We presented a procedure to cluster nominal
wordforms into semantic classes and to extract
class-based modifier-head bigrams, and then a
model to calculate the class-based probability of
candidate heads using these bigrams.
While we evaluated our system in the context
of the AAC word prediction task, we believe that
the class-based prediction model we proposed could
be extended to any other domain in which n-gram-
based compound prediction must be performed.
The addition of class-based head prediction to the
split compound model of Baroni et al (2002) leads
to an improvement in head prediction (from a ksr of
48.8% to a ksr of 51.2%). This translates into an
improvement of 1.3% in whole compound predic-
tion (from 48.8% to 50.1%). Overall, the split com-
pound model with class bigrams led to an improve-
ment of more than 15% over a no split compound
baseline model.
This result was presented in the context of the
AAC word prediction task, but we believe that the
class-based prediction model we proposed could be
extended to any other domain in which n-gram-
based compound prediction must be performed.
While the results we report are encouraging,
the improvement obtained with the addition of the
class-based model is hardly dramatic. It is clear that
further work in this area is required.
In particular, we plan to experiment with different
measures of association to determine the degree of
relatedness of words, and with alternative clustering
techniques.
Moreover, we hope to improve the overall perfor-
mance of the compound predictor by resorting to a
better interpolation strategy than the uniform weight
assignment model we are currently using.
We also reported results obtained with a prelim-
inary model in which split compound prediction is
integrated with simple noun prediction. This model
outperforms the baseline model without compound
prediction, but only of about 1% ksr. Clearly, fur-
ther work in this area is also necessary. In partic-
ular, as suggested by a reviewer, we will try to ex-
ploit morpho-syntactic differences between simple
nouns and modifiers to help distinguishing between
the two types.
Acknowledgements
We would like to thank an anonymous reviewer for
helpful comments and the Austria Presse Agentur
for kindly making the APA corpus available to us.
This work was supported by the European Union
in the framework of the IST programme, project
FASTY (IST-2000-25420). Financial support for
?OFAI is provided by the Austrian Federal Ministry
of Education, Science and Culture.
References
M. Baroni, J. Matiasek, and H. Trost, ?Predict-
ing the Components of German Nominal Com-
pounds?, to appear in Proc. ECAI 2002.
A. Berger: Trigger Toolkit, publicly available soft-
ware, 1997.
http://www-2.cs.cmu.edu/ aberger/software.html
P. Brown, V. Della Pietra, P. DeSouza, J. Lai, and
R. Mercer, ?Class-based n-gram models of nat-
ural language?, Computational Linguistics 18(4),
pp.467-479, 1990.
J. Carlberger, Design and Implementation of a Prob-
abilistic Word Prediction Program, Royal Insti-
tute of Technology (KTH), 1998.
D. Carter, J. Kaja, L. Neumeyer, M. Rayner, F.
Weng, and M. Wire`n, ?Handling Compounds in
a Swedish Speech-Understanding System?, Proc.
ICSLP-96.
A Copestake, ?Augmented and alternative NLP
techniques for augmentative and alternative com-
munication?, Proceedings of the ACL workshop
on Natural Language Processing for Communi-
cation Aids, 1997.
W. Dressler, G. Libben, J. Stark, C. Pons, and G.
Jarema, ?The processing of interfixed German
compounds?, Yearbook of Morphology 1999, pp.
185-220, 2001.
S. Evert and B. Krenn, ?Methods for the Qual-
itative Evaluation of Lexical Association Mea-
sures?, Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
Toulouse, France, 2001.
P. Fetter, Detection and Transcription of OOV
Words, Verbmobil Report 231, 1998.
L. Karttunen, K. Gal, and A. Kempe, Xerox
Finite-State Tool, Xerox Research Centre Europe,
Grenoble, 1997.
A. Krott, Analogy in Morphology, Max Planck In-
stitute for Psycholinguistics, Nijmegen, 2001.
M. Larson, D. Willett, J. Kohler, and G. Rigoll,
?Compound splitting and lexical unit recombi-
nation for improved performance of a speech
recognition system for German parliamentary
speeches?, Proceedings of the 6th Interna-
tional Conference of Spoken Language Pro-
cessing (ICSLP-2000), October 16-20., Peking,
China, 2000.
R. Rosenfeld, ?A Maximum Entropy Approach to
Adaptive Statistical Language Modeling?, Com-
puter Speech and Language 10, 187?228, 1996.
M. Spies, ?A Language Model for Compound
Words?, Proc. Eurospeech ?95, pp.1767-1779,
1995.
Unsupervised discovery of morphologically related words based on
orthographic and semantic similarity
Marco Baroni
?OFAI
Schottengasse 3
A-1010 Vienna, Austria
marco@oefai.at
Johannes Matiasek
?OFAI
Schottengasse 3
A-1010 Vienna, Austria
john@oefai.at
Harald Trost
IMKAI
Freyung 6
A-1010 Vienna, Austria
harald@ai.univie.ac.at
Abstract
We present an algorithm that takes an
unannotated corpus as its input, and re-
turns a ranked list of probable morpho-
logically related pairs as its output. The
algorithm tries to discover morphologi-
cally related pairs by looking for pairs
that are both orthographically and seman-
tically similar, where orthographic simi-
larity is measured in terms of minimum
edit distance, and semantic similarity is
measured in terms of mutual information.
The procedure does not rely on a mor-
pheme concatenation model, nor on dis-
tributional properties of word substrings
(such as affix frequency). Experiments
with German and English input give en-
couraging results, both in terms of pre-
cision (proportion of good pairs found at
various cutoff points of the ranked list),
and in terms of a qualitative analysis of
the types of morphological patterns dis-
covered by the algorithm.
1 Introduction
In recent years, there has been much interest in com-
putational models that learn aspects of the morphol-
ogy of a natural language from raw or structured
data. Such models are of great practical interest as
tools for descriptive linguistic analysis and for mini-
mizing the expert resources needed to develop mor-
phological analyzers and stemmers. From a theo-
retical point of view, morphological learning algo-
rithms can help answer questions related to human
language acquisition.
In this study, we present a system that, given a
corpus of raw text from a language, returns a ranked
list of probable morphologically related word pairs.
For example, when run with the Brown corpus as
its input, our system returned a list with pairs such
as pencil/pencils and structured/unstructured at the
top.
Our algorithm is completely knowledge-free, in
the sense that it processes raw corpus data, and it
does not require any form of a priori information
about the language it is applied to. The algorithm
performs unsupervised learning, in the sense that it
does not require a correctly-coded standard to (iter-
atively) compare its output against.
The algorithm is based on the simple idea that a
combination of formal and semantic cues should be
exploited to identify morphologically related pairs.
In particular, we use minimum edit distance to mea-
sure orthographic similarity,1 and mutual informa-
tion to measure semantic similarity. The algo-
rithm does not rely on the notion of affix, and it
does not depend on global distributional properties
of substrings (such as affix frequency). Thus, at
least in principle, the algorithm is well-suited to
discover pairs that are related by rare and/or non-
concatenative morphological processes.
The algorithm returns a list of related pairs, but
it does not attempt to extract the patterns that relate
the pairs. As such, it can be used as a tool to pre-
1Given phonetically transcribed input, our model would
compute phonetic similarity instead of orthographic similarity.
                     July 2002, pp. 48-57.  Association for Computational Linguistics.
        ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
       Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
process corpus data for an analysis to be performed
by a human morphologist, or as the first step of a
fully automated morphological learning program, to
be followed, for example, by a rule induction pro-
cedure that extracts correspondence patterns from
paired forms. See the last section of this paper for
further discussion of possible applications.
We tested our model with German and English
input. Our results indicate that the algorithm is
able to identify a number of pairs related by a va-
riety of derivational and inflectional processes with
a remarkably high precision rate. The algorithm is
also discovering morphological relationships (such
as German plural formation with umlaut) that would
probably be harder to discover using affix-based ap-
proaches.
The remainder of the paper is organized as fol-
lows: In section 2, we shortly review related work.
In section 3, we present our model. In section 4, we
discuss the results of experiments with German and
English input. Finally, in section 5 we summarize
our main results, we sketch possible directions that
our current work could take, and we discuss some
potential uses for the output of our algorithm.
2 Related work
For space reason, we discuss here only three ap-
proaches that are closely related to ours. See, for
example, Goldsmith (2001) for a very different (pos-
sibly complementary) approach, and for a review of
other relevant work.
2.1 Jacquemin (1997)
Jacquemin (1997) presents a model that automati-
cally extracts morphologically related forms from a
list of English two-word medical terms and a corpus
from the medical domain.
The algorithm looks for correspondences between
two-word terms and orthographically similar pairs
of words that are adjacent in the corpus. For exam-
ple, the list contains the term artificial ventilation,
and the corpus contains the phrase artificially ven-
tilated. Jacquemin?s algorithm thus postulates the
(paired) morphological analyses artificial ventilat-
ion and artificial-ly ventilat-ed.
Similar words, for the purposes of this pairing
procedure, are simply words that share a common
left substring (with constraints that we do not dis-
cuss here).
Jacquemin?s procedure then builds upon these
early steps by clustering together sets that follow the
same patterns, and using these larger classes to look
for spurious analyses. Finally, the algorithm tries to
cluster classes that are related by similar, rather than
identical, suffixation patterns. Again, we will not
describe here how this is accomplished.
Our basic idea is related to that of Jacquemin, but
we propose an approach that is more general both
in terms of orthography and in terms of semantics.
In terms of orthography, we do not require that two
strings share the left (or right) substring in order to
constitute a candidate pair. Thus, we are not limited
to affixal morphological patterns. Moreover, our al-
gorithm extracts semantic information directly from
the input corpus, and thus it does not require a pre-
compiled list of semantically related pairs.
2.2 Schone and Jurafsky (2000)
Schone and Jurafsky (2000) present a knowledge-
free unsupervised model in which orthography-
based distributional cues are combined with seman-
tic information automatically extracted from word
co-occurrence patterns in the input corpus.
They first look for potential suffixes by search-
ing for frequent word-final substrings. Then, they
look for potentially morphologically related pairs,
i.e., pairs that end in potential suffixes and share the
left substring preceding those suffixes. Finally, they
look, among those pairs, for those whose semantic
vectors (computed using latent semantic analysis)
are significantly correlated. In short, the idea behind
the semantic component of their model is that words
that tend to co-occur with the same set of words,
within a certain window of text, are likely to be se-
mantically correlated words.
While we follow Schone and Jurafsky?s idea of
combining orthographic and semantic cues, our al-
gorithm differs from them in both respects. From the
point of view of orthography, we rely on the com-
parison between individual word pairs, without re-
quiring that the two pairs share a frequent affix, and
indeed without requiring that they share an affix at
all.
From the point of view of semantics, we compute
scores based on mutual information instead of latent
semantic analysis. Thus, we only look at the co-
occurrence patterns of target words, rather than at
the similarity of their contexts.
Future research should try to assess to what extent
these two approaches produce significantly different
results, and/or to what extent they are complemen-
tary.
2.3 Yarowsky and Wicentowski (2000)
Yarowsky and Wicentowski (2000) propose an algo-
rithm that extracts morphological rules relating roots
and inflected forms of verbs (but the algorithm can
be extended to other morphological relations).
Their algorithm performs unsupervised, but not
completely knowledge-free, learning. It requires
a table of canonical suffixes for the relevant parts
of speech of the target language, a list of the con-
tent word roots with their POS (and some informa-
tion about the possible POS/inflectional features of
other words), a list of the consonants and vowels of
the language, information about some characteristic
syntactic patterns and, if available, a list of function
words.
The algorithm uses a combination of different
probabilistic models to find pairs that are likely to be
morphologically related. One model matches root
+ inflected form pairs that have a similar frequency
profile. Another model matches root + inflected
form pairs that tend to co-occur with the same sub-
jects and objects (identified using simple regular ex-
pressions). Yet another model looks for words that
are orthographically similar, in terms of a minimum
edit distance score that penalizes consonant changes
more than vowel changes. Finally, the rules relating
stems and inflected forms that the algorithm extracts
from the pairs it finds in an iteration are used as a
fourth probabilistic model in the subsequent itera-
tions.
Yarowsky and Wicentowski show that the al-
gorithm is extremely accurate in identifying En-
glish root + past tense form pairs, including those
pairs that are related by non-affixal patterns (e.g.,
think/thought.)
The main issue with this model is, of course, that
it cannot be applied to a new target language with-
out having some a priori knowledge about some of
its linguistic properties. Thus, the algorithm can-
not be applied in cases in which the grammar of
the target language has not been properly described
yet, or when the relevant information is not available
for other reasons. Moreover, even when such infor-
mation is in principle available, trying to determine
to what extent morphology could be learned with-
out relying on any other knowledge source remains
an interesting theoretical pursuit, and one whose an-
swer could shed some light on the problem of human
language acquisition.
3 The current approach: Morphological
relatedness as a function of orthographic
and semantic similarity
The basic intuition behind the model presented here
is extremely simple: Morphologically related words
tend to be both orthographically and semantically
similar. Obviously, there are many words that are or-
thographically similar, but are not morphologically
related; for example, blue and glue. At the same
time, many semantically related words are not mor-
phologically related (for example, blue and green).
However, if two words have a similar shape and a
related meaning (e.g., green and greenish), they are
very likely to be also morphologically related.
In order to make this idea concrete, we use min-
imum edit distance to identify words that are ortho-
graphically similar, and mutual information between
words to identify semantically related words.
3.1 Outline of the procedure
Given an unannotated input corpus, the algorithm
(after some elementary tokenization) extracts a list
of candidate content words. This is simply a list
of all the alphabetic space- or punctuation-delimited
strings in the corpus that have a corpus frequency
below .01% of the total token count.2
Preliminary experiments indicated that our proce-
dure does not perform as well without this trimming.
Notice in any case that function words tend to be of
little morphological interest, as they display highly
lexicalized, often suppletive morphological patterns.
The word list extracted as described above and the
input corpus are used to compute two lists of word
pairs: An orthographic similarity list, in which the
2In future versions of the algorithm, we plan to make this
high frequency threshold dependent on the size of the input cor-
pus.
pairs are scored on the basis of their minimum edit
distance, and a semantic similarity list, based on mu-
tual information. Because of minimum thresholds
that are enforced during the computation of the two
measures, neither list contains all the pairs that can
in principle be constructed from the input list.
Before computing the combined score, we get rid
of the pairs that do not occur in both lists (the ra-
tionale being that we do not want to guess the mor-
phological status of a pair on the sole basis of ortho-
graphic or semantic evidence).
We then compute a weighted sum of the ortho-
graphic and semantic similarity scores of each re-
maining pair. In the experiments reported below, the
weights are chosen so that the maximum weighted
scores for the two measures are in the same order
of magnitude (we prefer to align maxima rather than
means because both lists are trimmed at the bottom,
making means and other measures of central ten-
dency less meaningful).
The pairs are finally ranked on the basis of the
resulting combined scores.
In the next subsections, we describe how the or-
thographic and semantic similarity lists are con-
structed, and some properties of the measures we
adopted.
3.2 Scoring the orthographic similarity of
word pairs
Like Yarowsky and Wicentowski, we use mini-
mum edit distance to measure orthographic simi-
larity. The minimum edit distance between two
strings is the minimum number of editing oper-
ations (insertion, deletion, substitution) needed to
transform one string into the other (see section 5.6
of Jurafsky and Martin (2000) and the references
quoted there).
Unlike Yarowsky and Wicentowski, we do not at-
tempt to define a phonologically sensible edit dis-
tance scoring function, as this would require making
assumptions about how the phonology of the target
language maps onto its orthography, thus falling out-
side the domain of knowledge-free induction. In-
stead, we assign a cost of 1 to all editing operations,
independently of the nature of the source and target
segments. Thus, in our system, the pairs dog/Dog,
man/men, bat/mat and day/dry are all assigned a
minimum edit distance of 1.3
Rather than computing absolute minimum edit
distance, we normalize this measure by dividing
it by the length of the longest string (this corre-
sponds to the intuition that, say, two substitutions
are less significant if we are comparing two eight-
letter words than if we are comparing two three-
letter words). Moreover, since we want to rank pairs
on the basis of orthographic similarity, rather than
dissimilarity, we compute (1 - normalized minimum
edit distance), obtaining a measure that ranges from
1 for identical forms to 0 for forms that do not share
any character.
This measure is computed for all pairs of words in
the potential content word list. However, for reasons
of size, only pairs that have a score of .5 or higher
(i.e., where the two members share at least half of
their characters) are recorded in the output list.
Notice that orthographic similarity does not favor
concatenative affixal morphology over other types
of morphological processes. For example, the pairs
woman/women and park/parks both have an ortho-
graphic similarity score of .8.
Moreover, orthographic similarity depends only
on the two words being compared, and not on global
distributional properties of these words and their
substrings. Thus, words related by a rare morpho-
logical pattern can have the same score as words
related by a very frequent pattern, as long as the
minimum edit distance is the same. For example,
both nucleus/nuclei and bench/benches have an or-
thographic similarity score of .714, despite the fact
that the latter pair reflects a much more common plu-
ralization pattern.
Of course, this emancipation from edge-anchored
concatenation and global distributional salience also
implies that orthographic similarity will assign high
3Following a suggestion by two reviewers, we are currently
experimenting with an iterative version of our algorithm, along
the lines of the one described by Yarowsky and Wicentowski.
We start with the cost matrix described in the text, but we re-
estimate the editing costs on the basis of the empirical character-
to-character (or character-to-zero/zero-to-character) probabili-
ties observed in the output of the previous run of the algorithm.
Surprisingly, the revised version of the algorithm leads to (mod-
erately) worse results than the single-run version described in
this paper. Further experimentation with edit cost re-estimation
is needed, in order to understand which aspects of our iterative
procedure make it worse than the single-run model, and how it
could be improved.
scores to many pairs that are not morphologically
related ? for example, the pair friends/trends also has
an orthographic similarity score of .714.
Furthermore, since in most languages the range of
possible word lengths is narrow, orthographic simi-
larity as a ranking measure tends to suffer of a ?mas-
sive tying? problem. For example, when pairs from
the German corpus described below are ranked on
the sole basis of orthographic similarity, the result-
ing list is headed by a block of 19,597 pairs that all
have the same score. These are all pairs where one
word has 9 characters, the other 9 or 8 characters,
and the two differ in only one character.4
For the above reasons, it is crucial that ortho-
graphic similarity is combined with an independent
measure that allows us to distinguish between simi-
larity due to morphological relatedness vs. similar-
ity due to chance or other reasons.
3.3 Scoring the semantic similarity of word
pairs
Measuring the semantic similarity of words on the
basis of raw corpus data is obviously a much harder
task than measuring the orthographic similarity of
words.
Mutual information (first introduced to compu-
tational linguistics by Church and Hanks (1989)) is
one of many measures that seems to be roughly
correlated to the degree of semantic relatedness be-
tween words. The mutual information between two
words A and B is given by:
I(A,B) = log
Pr(A,B)
Pr(A)Pr(B)
(1)
Intuitively, the larger the deviation between the
empirical frequency of co-occurrence of two words
and the expected frequency of co-occurrence if they
were independent, the more likely it is that the oc-
currence of one of the two words is not independent
from the occurrence of the other.
Brown et ali (1990) observed that when mutual
information is computed in a bi-directional fashion,
and by counting co-occurrences of words within a
4Most of the pairs in this block ? 78% ? are actually morpho-
logically related. However, given that all pairs contain words
of length 9 and 8/9 that differ in one character only, they are
bound to reflect only a very small subset of the morphological
processes present in German.
relatively large window, but excluding ?close? co-
occurrences (which would tend to capture colloca-
tions and lexicalized phrases), the measure identifies
semantically related pairs.
It is particularly interesting for our purposes that
most of the examples of English word clusters con-
structed on the basis of this interpretation of mutual
information by Brown and colleagues (reported in
their table 6) include morphologically related words.
A similar pattern emerges among the examples of
German words clustered in a similar manner by
Baroni et ali (2002). Rosenfeld (1996) reports that
morphologically related pairs are common among
words with a high (average) mutual information.
We computed mutual information by considering,
for each pair, only co-occurrences within a maxi-
mal window of 500 words and outside a minimal
window of 3 words. Given that mutual informa-
tion is notoriously unreliable at low frequencies (see,
for example, Manning and Schu?tze (1999), section
5.4), we only collected mutual information scores
for pairs that co-occurred at least three times (within
the relevant window) in the input corpus. Obvi-
ously, occurrences across article boundaries were
not counted. Notice however that the version of the
Brown corpus we used does not mark article bound-
aries. Thus, in this case the whole corpus was treated
as a single article.
Our ?semantic? similarity measure is based on the
notion that related words will tend to often occur in
the nears of each other. This differs from the (more
general) approach of Schone and Jurafsky (2000),
who look for words that tend to occur in the same
context. It remains an open question whether the
two approaches produce complementary or redun-
dant results.5
Taken by itself, mutual information is a worse
predictor of morphological relatedness than mini-
mum edit distance. For example, among the top one
hundred pairs ranked by mutual information in each
language, only one German pair and five English
pairs are morphologically motivated. This poor per-
formance is not too surprising, given that there are
5We are currently experimenting with a measure based on
semantic context similarity (determined on the basis of class-
based left-to-right and right-to-left bigrams), but the current im-
plementation of this requires ad hoc corpus-specific settings to
produce interesting results with both our test corpora.
plenty of words that often co-occur together without
being morphologically related. Consider for exam-
ple (from our English list) the pairs index/operand
and orthodontist/teeth.
4 Empirical evaluation
4.1 Materials
We tested our procedure on the German APA corpus,
a corpus of newswire containing over twenty-eight
million word tokens, and on the English Brown cor-
pus (Kuc?era and Francis, 1967), a balanced corpus
containing less than one million two hundred thou-
sand word tokens. Of course, the most important dif-
ference between these two corpora is that they rep-
resent different languages. However, observe also
that they have very different sizes, and that they are
different in terms of the types of texts constituting
them.
Besides the high frequency trimming procedure
described above, for both languages we removed
from the potential content word lists those words
that were not recognized by the XEROX morpholog-
ical analyzer for the relevant language. The reason
for this is that, as we describe below, we use this tool
to build the reference sets for evaluation purposes.
Thus, morphologically related pairs composed of
words not recognized by the analyzer would unfairly
lower the precision of our algorithm.
Moreover, after some preliminary experimenta-
tion, we also decided to remove words longer than 9
characters from the German list (this corresponds to
trimming words whose length is one standard devi-
ation or more above the average token length). This
actually lowers the performance of our system, but
makes the results easier to analyze ? otherwise, the
top of the German list would be cluttered by a high
number of rather uninteresting morphological pairs
formed by inflected forms from the paradigm of
very long nominal compounds (such as Wirtschafts-
forschungsinstitut ?institute for economic research?).
Unlike high frequency trimming, the two opera-
tions we just described are meant to facilitate empir-
ical evaluation, and they do not constitute necessary
steps of the core algorithm.
4.2 Precision
In order to evaluate the precision obtained by our
procedure, we constructed a list of all the pairs that,
according to the analysis provided by the XEROX
analyzer for the relevant language, are morpholog-
ically related (i.e., share one of their stems).6 We
refer to the lists constructed in the way we just de-
scribed as reference sets.
The XEROX tools we used do not provide deriva-
tional analysis for English, and a limited form of
derivational analysis for German. Our algorithm,
however, finds both inflectionally and derivationally
related pairs. Thus, basing our evaluation on a com-
parison with the XEROX parses leads to an underes-
timation of the precision of the algorithm. We found
that this problem is particularly evident in English,
since English, unlike German, has a rather poor in-
flectional morphology, and thus the discrepancies
between our output and the analyzer parses in terms
of derivational morphology have a more visible im-
pact on the results of the comparison. For example,
the English analyzer does not treat pairs related by
the adverbial suffix -ly or by the prefix un- as mor-
phologically related, whereas our algorithm found
pairs such as soft/softly and load/unload.
In order to obtain a more fair assessment of the
algorithm, we went manually through the first 2,000
English pairs found by our algorithm but not parsed
as related by the analyzer, looking for items to be
added to the reference set. We were extremely
conservative, and we added to the reference set
only those pairs that are related by a transparent
and synchronically productive morphological pat-
tern. When in doubt, we did not correct the analyzer-
based analysis. Thus, for example, we did not count
pairs such as machine/machinery, variables/varies
or electric/electronic as related.
We did not perform any manual post-processing
on the German reference set.
Tables 1 and 2 report percentage precision (i.e.,
the percentage of pairs that are in the reference set
over the total number of ranked pairs up to the rele-
vant threshold) at various cutoff points, for German
and English respectively.
6The XEROX morphological analyzers are state-of-the-art,
knowledge-driven morphological analysis tools (see for exam-
ple Karttunen et ali (1997)).
# of pairs precision
500 97%
1000 96%
1500 96%
2000 94%
3000 81%
4000 65%
5000 53%
5279 50%
Table 1: German precision at various cutoff points
(5279 = total number of pairs)
# of pairs precision
500 98%
1000 95%
1500 91%
2000 83%
3000 72%
4000 58%
5000 48%
8902 29%
Table 2: English precision at various cutoff points
(8902 = total number of pairs)
For both languages we notice a remarkably high
precision rate (> 90%) up to the 1500-pair cutoff
point.
After that, there is a sharper drop in the English
precision, whereas the decline in German is more
gradual. This is perhaps due in part to the problems
with the English reference set we discussed above,
but notice also that English has an overall poorer
morphological system and that the English corpus is
considerably smaller than the German one. Indeed,
our reference set for German contains more than ten
times the forms in the English reference set.
Notice anyway that, for both languages, the preci-
sion rate is still around 50% at the 5000-pair cutoff.7
7Yarowsky and Wicentowski (2000) report an accuracy of
over 99% for their best model and a test set of 3888 pairs. Our
precision rate at a comparable cutoff point is much lower (58%
at the 4000-pair cutoff). However, Yarowksy and Wicentowski
restricted the possible matchings to pairs in which one member
is an inflected verb form, and the other member is a potential
verbal root, whereas in our experiments any word in the corpus
(as long as it was below a certain frequency threshold, and it was
recognized by the XEROX analyzer) could be matched with any
other word in the corpus. Thus, on the one hand, Yarowsky and
Wicentowski forced the algorithm to produce a matching for a
certain set of words (their set of inflected forms), whereas our
algorithm was not subject to an analogous constraint. On the
other hand, though, our algorithm had to explore a much larger
possible matching space, and it could (and did) make a high
number of mistakes on pairs (such as, e.g., sorry and worry) that
Of course, what counts as a ?good? precision rate
depends on what we want to do with the output of
our procedure. We show below that even a very
naive morphological rule extraction algorithm can
extract sensible rules by taking whole output lists as
its input, since, although the number of false pos-
itives is high, they are mostly related by patterns
that are not attested as frequently in the list as the
patterns relating true morphological pairs. In other
words, true morphological pairs tend to be related
by patterns that are distributionally more robust than
those displayed by false positives. Thus, rule ex-
tractors and other procedures processing the output
of our algorithm can probably tolerate a high false
positive rate if they take frequency and other distri-
butional properties of patterns into account.
Notice that we discussed only precision, and not
recall. This is because we believe that the goal of a
morphological discovery procedure is not to find the
exhaustive list of all morphologically related forms
in a language (indeed, because of morphological
productivity, such list is infinite), but rather to dis-
cover all the possible (synchronically active and/or
common) morphological processes present in a lan-
guage. It is much harder to measure how good our
algorithm performed in this respect, but the qualita-
tive analysis we present in the next subsection indi-
cates that, at least, the algorithm discovers a varied
and interesting set of morphological processes.
4.3 Morphological patterns discovered by the
algorithm
The precision tables confirm that the algorithm
found a good number of morphologically related
pairs. However, if it turned out that all of these
pairs were examples of the same morphological pat-
tern (say, nominal plural formation in -s), the al-
gorithm would not be of much use. Moreover, we
stated at the beginning that, since our algorithm does
not assume an edge-based stem+affix concatenation
model of morphology, it should be well suited to dis-
cover relations that cannot be characterized in these
Yarowksy and Wicentowski?s algorithm did not have to con-
sider. Schone and Jurafsky (2000) report a maximum precision
of 92%. It is hard to compare this with our results, since they
use a more sophisticated scoring method (based on paradigms
rather than pairs) and a different type of gold standard. More-
over, they do not specify what was the size of the input they
used for evaluation.
terms (e.g., pairs related by circumfixation, stem
changes, etc.). It is interesting to check whether the
algorithm was indeed able to find relations of this
sort.
Thus, we performed a qualitative analysis of the
output of the algorithm, trying to understand what
kind of morphological processes were captured by
it.
In order to look for morphological processes in
the algorithm output, we wrote a program that ex-
tracts ?correspondence rules? in the following sim-
ple way: For each pair, the program looks for the
longest shared (case-insensitive) left- and right-edge
substrings (i.e., for a stem + suffix parse and for a
prefix + stem parse). The program then chooses the
parse with the longest stem (assuming that one of the
two parses has a non-zero stem), and extracts the rel-
evant edge-bound correspondence rule. If there is a
tie, the stem + suffix parse is preferred. The program
then ranks the correspondence rules on the basis of
their frequency of occurrence in the original output
list.8
We want to stress that we are adopting this proce-
dure as a method to explore the results, and we are
by no means proposing it as a serious rule induction
algorithm. One of the most obvious drawbacks of
the current rule extraction procedure is that it is only
able to extract linear, concatenative, edge-bound suf-
fixation and prefixation patterns, and thus it misses
or fails to correctly generalize some of the most in-
teresting patterns in the output. Indeed, looking at
the patterns missed by the algorithm (as we do in
part below) is as instructive as looking at the rules it
found.
Tables 3 and 4 report the top five suffixation and
prefixation patterns found by the rule extractor by
taking the entire German and English output lists as
its input.
These tables show that our morphological pair
scoring procedure found many instances of various
common morphological patterns. With the excep-
tion of the German ?prefixation? rule ers?drit (ac-
tually relating the roots of the ordinals ?first? and
?second?), and of the compounding pattern  ? ?Ol
(?Oil?), all the rules in these lists correspond to re-
alistic affixation patterns. Not surprisingly, in both
8Ranking by cumulative score yields analogous results.
rule example fq
?s Jelzin?Jelzins 921
?n lautete?lauteten 670
?en digital?digitalen 225
?e rot?rote 201
?es Papst?Papstes 113
?ge stiegen?gestiegen 9
? ?Ol Embargo? ?Olembargo 6
?vor Mittag?Vormittag 5
aus?ein ausfuhren?einfuhren 4
ers?drit Erstens?Drittens 4
Table 3: The most common German suffixation and
prefixation patterns
rule example fq
?s allotment?allotments 860
?ed accomplish?accomplished 98
ed?ing established?establishing 87
?ing experiment?experimenting 85
?d conjugate?conjugated 58
?un structured?unstructured 17
?re organization?reorganization 12
?in organic?inorganic 7
?non specifically?nonspecifically 6
?dis satisfied?dissatisfied 5
Table 4: The most common English suffixation and
prefixation patterns
languages many of the most frequent rules (such as,
e.g., ?s) are poly-functional, corresponding to a
number of different morphological relations within
and across categories.
The results reported in these tables confirm that
the algorithm is capturing common affixation pro-
cesses, but they are based on patterns that are so
frequent that even a very naive procedure could un-
cover them9
More interesting observations emerge from fur-
ther inspection of the ranked rule files. For exam-
ple, among the 70 most frequent German suffixation
rules extracted by the procedure, we encounter those
in table 5.10
The patterns in this table show that our algorithm
is capturing the non-concatenative plural formation
9For example, as shown by a reviewer, a procedure that pairs
words that share the same first five letters, and extracts the di-
verging substrings following the common prefix from each pair.
10In order to find the set of rules presented in table 5 using the
naive algorithm described in the previous footnote, we would
have to consider the 2672 most frequent rules. Most of these
2672 rules, of course, do not correspond to true morphological
patterns ? thus, the interesting rules would be buried in noise.
rule example fq
ag?a?ge Anschlag?Anschla?ge 10
ang?a?nge Ru?ckgang?Ru?ckga?nge 6
all?a?lle ?Uberfall? ?Uberfa?lle 6
ug?u?ge Tiefflug?Tiefflu?ge 5
and?a?nde Vorstand?Vorsta?nde 5
uch?u?che Einbruch?Einbru?che 3
auf?a?ufe Verkauf?Verka?ufe 3
ag?a?gen Vertrag?Vertra?gen 3
Table 5: Some German rules involving stem vowel
changes found by the rule extractor
process involving fronting of the stem vowel plus
addition of a suffix (-e/-en). A smarter rule extractor
should be able to generalize from patterns like these
to a smaller number of more general rules capturing
the discontinuous change. Other umlaut-based pat-
terns that do not involve concomitant suffixation ?
such as in Mutter/Mu?tter ? were also found by our
core algorithm, but they were wrongly parsed as in-
volving prefixes (e.g., Mu?Mu?) by the rule extrac-
tor.
Finally, it is very interesting to look at those pairs
that are morphologically related according to the
XEROX analyzer, and that were discovered by our
algorithm, but where the rule extractor could not
posit a rule, since they do not share a substring at
either edge. These are listed, for German, in table 6.
Alter a?lteren fordern gefordert
Arzt ?Arzte forderten gefordert
Arztes ?Arzte fo?rdern gefo?rdert
Fesseln gefesselt genannt nannte
Folter gefoltert genannten nannte
Putsch geputscht geprallt prallte
Spende gespendet gesetzt setzte
Spenden gespendet gestu?rzt stu?rzte
Streik gestreikt
Table 6: Morphologically related German pairs that
do not share an edge found by the basic algorithm
We notice in this table, besides three further in-
stances of non-affixal morphology, a majority of
pairs involving circumfixation of one of the mem-
bers.
While a more in-depth qualitative analysis of our
results should be conducted, the examples we dis-
cussed here confirm that our algorithm is able to cap-
ture a number of different morphological patterns,
including some that do not fit into a strictly concate-
native edge-bound stem+affix model.
5 Conclusion and Future Directions
We presented an algorithm that, by taking a raw cor-
pus as its input, produces a ranked list of morpho-
logically related pairs at its output. The algorithm
finds morphologically related pairs by looking at the
degree of orthographic similarity (measured by min-
imum edit distance) and semantic similarity (mea-
sured by mutual information) between words from
the input corpus.
Experiments with German and English inputs
gave encouraging results, both in terms of precision,
and in terms of the nature of the morphological pat-
terns found within the output set.
In work in progress, we are exploring various pos-
sible improvements to our basic algorithm, includ-
ing iterative re-estimation of edit costs, addition of a
context-similarity-based measure, and extension of
the output set by morphological transitivity, i.e. the
idea that if word a is related to word b, and word b
is related to word c, then word a and word c should
also form a morphological pair.
Moreover, we plan to explore ways to relax the re-
quirement that all pairs must have a certain degree of
semantic similarity to be treated as morphologically
related (there is evidence that humans treat certain
kinds of semantically opaque forms as morpholog-
ically complex ? see Baroni (2000) and the refer-
ences quoted there). This will probably involve tak-
ing distributional properties of word substrings into
account.
From the point of view of the evaluation
of the algorithm, we should design an as-
sessment scheme that would make our exper-
imental results more directly comparable to
those of Yarowsky and Wicentowski (2000),
Schone and Jurafsky (2000) and others. Moreover,
a more in depth qualitative analysis of the results
should concentrate on identifying specific classes of
morphological processes that our algorithm can or
cannot identify correctly.
We envisage a number of possible uses for the
ranked list that constitutes the output of our model.
First, the model could provide the input for a
more sophisticated rule extractor, along the lines of
those proposed by Albright and Hayes (1999) and
Neuvel (2002). Such models extract morphologi-
cal generalizations in terms of correspondence pat-
terns between whole words, rather than in terms of
affixation rules, and are thus well suited to iden-
tify patterns involving non-concatenative morphol-
ogy and/or morphophonological changes. A list
of related words constitutes a more suitable input
for them than a list of words segmented into mor-
phemes.
Rules extracted in this way would have a number
of practical uses ? for example, they could be used
to construct stemmers for information retrieval ap-
plications, or they could be integrated into morpho-
logical analyzers.
Our procedure could also be used to re-
place the first step of algorithms, such as those
of Goldsmith (2001) and Snover and Brent (2001),
where heuristic methods are employed to generate
morphological hypotheses, and then an information-
theoretically/probabilistically motivated measure is
used to evaluate or improve such hypotheses. More
in general, our algorithm can help reduce the size
of the search space that all morphological discovery
procedures must explore.
Last but not least, the ranked output of (an im-
proved version of) our algorithm can be of use to
the linguist analyzing the morphology of a language,
who can treat it as a way to pre-process her/his
data, while still relying on her/his analytical skills
to extract the relevant morphological generalizations
from the ranked pairs.
Acknowledgements
We would like to thank Adam Albright, Bruce
Hayes and the anonymous reviewers for helpful
comments, and the Austria Presse Agentur for
kindly making the APA corpus available to us. This
work was supported by the European Union in the
framework of the IST programme, project FASTY
(IST-2000-25420). Financial support for ?OFAI is
provided by the Austrian Federal Ministry of Edu-
cation, Science and Culture.
References
A. Albright and B. Hayes. 1999. An automated learner
for phonology and morphology. UCLA manuscript.
M. Baroni. 2000. Distributional cues in morpheme
discovery: A computational model and empirical ev-
idence. Ph.D. dissertation, UCLA.
M. Baroni, J. Matiasek and H. Trost. 2002. Wordform-
and class-based prediction of the components of Ger-
man nominal compounds in an AAC system. To ap-
pear in Proceedings of COLING 2002.
P. Brown, P. Della Pietra, P. DeSouza, J. Lai, and R. Mer-
cer. 1990. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467-479.
K. Church and P. Hanks. 1989. Word association norms,
mutual information, and lexicography. Proceedings of
ACL 27, 76?83.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27:153-198.
C. Jacquemin. 1997. Guessing morphology from terms
and corpora. Proceedings of SIGIR 97, 156?265.
D. Jurafsky and J. Martin. 2000. Speech and Language
Processing. Prentice-Hall, Upper Saddle River, NJ.
L. Karttunen, K. Gaa?l, and A. Kempe. 1997. Xe-
rox Finite-State Tool Xerox Research Centre Europe,
Grenoble.
H. Kuc?era and N. Francis. 1967. Computational analysis
of present-day American English. Brown University
Press, Providence, RI.
C. Manning and H. Schu?tze. 1999. Foundations of sta-
tistical natural language processing. MIT Press, Cam-
bridge, MASS.
S. Neuvel. 2002. Whole word morphologizer. Expand-
ing the word-based lexicon: A non-stochastic compu-
tational approach. Brain and Language, in press.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187?228.
P. Schone and D. Jurafsky. 2000. Knowldedge-free in-
duction of morphology using latent semantic analysis.
Proceedings of the Conference on Computational Nat-
ural Language Learning.
M. Snover and M. Brent. 2001. A Bayesian model for
morpheme and paradigm identification. Proceedings
of ACL 39, 482-490.
D. Yarowksy and R. Wicentowski. 2000. Minimally su-
pervised morphological analysis by multimodal align-
ment. Proceedings of ACL 38, 207?216.
A Figure of Merit for the Evaluation of Web-Corpus Randomness
Massimiliano Ciaramita
Institute of Cognitive Science and Technology
National Research Council
Roma, Italy
m.ciaramita@istc.cnr.it
Marco Baroni
SSLMIT
Universita` di Bologna
Forl?`, Italy
baroni@sslmit.unibo.it
Abstract
In this paper, we present an automated,
quantitative, knowledge-poor method to
evaluate the randomness of a collection
of documents (corpus), with respect to a
number of biased partitions. The method
is based on the comparison of the word
frequency distribution of the target corpus
to word frequency distributions from cor-
pora built in deliberately biased ways. We
apply the method to the task of building a
corpus via queries to Google. Our results
indicate that this approach can be used,
reliably, to discriminate biased and unbi-
ased document collections and to choose
the most appropriate query terms.
1 Introduction
The Web is a very rich source of linguistic data,
and in the last few years it has been used in-
tensively by linguists and language technologists
for many tasks (Kilgarriff and Grefenstette, 2003).
Among other uses, the Web allows fast and in-
expensive construction of ?general purpose? cor-
pora, i.e., corpora that are not meant to repre-
sent a specific sub-language, but a language as a
whole. There are several recent studies on the
extent to which Web-derived corpora are com-
parable, in terms of variety of topics and styles,
to traditional ?balanced? corpora (Fletcher, 2004;
Sharoff, 2006). Our contribution, in this paper, is
to present an automated, quantitative method to
evaluate the ?variety? or ?randomness? (with re-
spect to a number of non-random partitions) of
a Web corpus. The more random/less-biased to-
wards specific partitions a corpus is, the more it
should be suitable as a general purpose corpus.
We are not proposing a method to evaluate
whether a sample of Web pages is a random sam-
ple of the Web, although this is a related issue
(Bharat and Broder, 1998; Henzinger et al, 2000).
Instead, we propose a method, based on simple
distributional properties, to evaluate if a sample
of Web pages in a certain language is reasonably
varied in terms of the topics (and, perhaps, tex-
tual types) it contains. This is independent from
whether they are actually proportionally represent-
ing what is out there on the Web or not. For exam-
ple, although computer-related technical language
is probably much more common on the Web than,
say, the language of literary criticism, one might
prefer a biased retrieval method that fetches docu-
ments representing these and other sub-languages
in comparable amounts, to an unbiased method
that leads to a corpus composed mostly of com-
puter jargon. This is a new area of investigation ?
with traditional corpora, one knows a priori their
composition. As the Web plays an increasingly
central role as data source in NLP, we believe that
methods to efficiently characterize the nature of
automatically retrieved data are becoming of cen-
tral importance to the discipline.
In the empirical evaluation of the method, we
focus on general purpose corpora built issuing au-
tomated queries to a search engine and retrieving
the corresponding pages, which has been shown to
be an easy and effective way to build Web-based
corpora (Ghani et al, 2001; Ueyama and Baroni,
2005; Sharoff, 2006). It is natural to ask which
kinds of query terms, henceforth seeds, are more
appropriate to build a corpus comparable, in terms
of variety, to traditional balanced corpora such as
the British National Corpus, henceforth BNC (As-
ton and Burnard, 1998). We test our procedure
to assess Web-corpus randomness on corpora built
217
using seeds chosen following different strategies.
However, the method per se can also be used to as-
sess the randomness of corpora built in other ways;
e.g., by crawling the Web.
Our method is based on the comparison of the
word frequency distribution of the target corpus
to word frequency distributions constructed using
queries to a search engine for deliberately biased
seeds. As such, it is nearly resource-free, as it
only requires lists of words belonging to specific
domains that can be used as biased seeds. In our
experiments we used Google as the search engine
of choice, but different search engines could be
used as well, or other ways to obtain collections
of biased documents, e.g., via a directory of pre-
categorized Web-pages.
2 Relevant work
Our work is related to the recent literature on
building linguistic corpora from the Web using au-
tomated queries to search engines (Ghani et al,
2001; Fletcher, 2004; Ueyama and Baroni, 2005;
Sharoff, 2006). Different criteria are used to se-
lect the seeds. Ghani and colleagues iteratively
bootstrapped queries to AltaVista from retrieved
documents in the target language and in other lan-
guages. They seeded the bootstrap procedure with
manually selected documents, or with small sets
of words provided by native speakers of the lan-
guage. They showed that the procedure produces
a corpus that contains, mostly, pages in the rele-
vant language, but they did not evaluate the results
in terms of quality or variety. Fletcher (2004) con-
structed a corpus of English by querying AltaVista
for the 10 top frequency words from the BNC.
He then conducted a qualitative analysis of fre-
quent n-grams in the Web corpus and in the BNC,
highlighting the differences between the two cor-
pora. Sharoff (2006) built corpora of English, Rus-
sian and German via queries to Google seeded
with manually cleaned lists of words that are fre-
quent in a reference corpus in the relevant lan-
guage, excluding function words, while Ueyama
and Baroni (2005) built corpora of Japanese using
seed words from a basic Japanese vocabulary list.
Both Sharoff and Ueyama and Baroni evaluated
the results through a manual classification of the
retrieved pages and by qualitative analysis of the
words that are most typical of the Web corpora.
We are also interested in evaluating the effect
that different seed selection (or, more in general,
corpus building) strategies have on the nature of
the resulting Web corpus. However, rather than
performing a qualitative investigation, we develop
a quantitative measure that could be used to evalu-
ate and compare a large number of different corpus
building methods, as it does not require manual in-
tervention. Moreover, our emphasis is not on the
corpus building methodology, nor on classifying
the retrieved pages, but on assessing whether they
appear to be reasonably unbiased with respect to a
range of topics or other criteria.
3 Measuring distributional properties of
biased and unbiased collections
Our goal is to create a ?balanced? corpus of Web
pages in a given language; e.g., the portion com-
posed of all Spanish Web pages. As we observed
in the introduction, obtaining a sample of unbi-
ased documents is not the same as obtaining an
unbiased sample of documents. Thus, we will not
motivate our method in terms of whether it favors
unbiased samples from the Web, but in terms of
whether the documents that are sampled appear to
be balanced with respect to a set of deliberately
biased samples. We leave it to further research to
investigate how the choice of the biased sampling
method affects the performance of our procedure
and its relations to uniform sampling.
3.1 Corpora as unigram distributions
A compact way of representing a collection of
documents is by means of frequency lists, where
each word is associated with the number of times
it occurs in the collection. This representation de-
fines a simple ?language model?, a stochastic ap-
proximation to the language of the collection; i.e.,
a ?0th order? word model or a ?unigram? model.
Language models of varying complexity can be
defined. As the model?s complexity increases, its
approximation to the target language improves ?
cf. the classic example of Shannon (1948) on the
entropy of English. In this paper we focus on un-
igram models, as a natural starting point, however
the approach extends naturally to more complex
language models.
3.2 Corpus similarity measure
We start by making the assumption that similar
collections will determine similar language mod-
els, hence that the similarity of collections of doc-
uments is closely related to the similarity of the
218
derived unigram distributions. The similarity of
two unigram distributions P and Q is estimated as
the relative entropy, or Kullback Leibler distance,
or KL (Cover and Thomas, 1991) D(P ||Q):
D(P ||Q) =
?
x?W
P (x) log P (x)Q(x) (1)
KL is a measure of the cost, in terms of aver-
age number of additional bits needed to describe
the random variable, of assuming that the distribu-
tion is Q when instead the true distribution is P .
Since D(P ||Q) ? 0, with equality only if P = Q,
unigram distributions generated by similar collec-
tions should have low relative entropy. To guaran-
tee that KL is always finite we make the assump-
tion that the random variables are defined over the
same finite alphabet W , the set of all word types
occurring in the observed data. To avoid further
infinite cases a smoothing value ? is added when
estimating probabilities; i.e.,
P (x) = cP (x) + ?|W |? +
?
x?W cP (x)
(2)
where cP (x) is the frequency of x in distribution
P, and |W | is the number of word types in W .
3.3 A scoring function for sampled unigram
distributions
What properties distinguish unigram distributions
drawn from the whole of a document collection
such as the BNC or the Web (or, rather, from the
space of the Web we are interested in sampling
from) from distributions drawn from biased sub-
sets of it? This is an important question because,
if identified, such properties might help discrimi-
nating between sampling methods which produce
more random collections of documents from more
biased ones. We suggest the following hypothesis.
Unigrams sampled from the full set of documents
have distances from biased samples which tend
to be lower than the distances of biased samples
to other samples based on different biases. Sam-
ples from the whole corpus, or Web, should pro-
duce lower KL distances because they draw words
across the whole vocabulary, while biased samples
have mostly access to a single specialized vocab-
ulary. If this hypothesis is true then, on average,
the distance between the unbiased sample and all
other samples should be lower than the distance
between a biased sample and all other samples.
2
1
m
b
a2
1
b2
a
b
m
m
l
c
c
c
1a
h
g
A
C
B
Figure 1. Distances (continuous lines with arrows) be-
tween points representing unigram distributions, sam-
pled from biased partitions A and B and from the full
collection of documents C = A ?B.
Figure 1 depicts a geometric interpretation of
the intuition behind this hypothesis. Suppose that
the two squares A and B represent two parti-
tions of the space of documents C. Additionally,
m pairs of unigram distributions, represented as
points, are produced by sampling documents uni-
formly at random from these partitions; e.g. a1
and b1. The mean Euclidean distance between
(ai, bi) pairs is a value between 0 and h, the length
of the diagonal of the rectangle which is the union
of A and B. Instead of drawing pairs we can draw
triples of points, one point from A, one from B,
and another point from C = A ? B. Approxi-
mately half of the points drawn from C will lie in
the A square, while the other half will lie in the B
square. The distance of the points drawn from C
from the points drawn from B will be between 0
and g, for approximately half of the points (those
laying in the B region), while the distance is be-
tween 0 and h for the other half of the points (those
inA). Therefore, ifm is large enough, the average
distance between C and B (or A) must be smaller
than the average distance between A and B, be-
cause h > g.
To summarize, then, we suggest the hypothe-
sis that samples from the full distribution have
a smaller mean distance than all other samples.
More precisely, let Ui,k be the kth of N unigram
distributions sampled with method yi, yi ? Y ,
where Y is the set of sampling categories. Ad-
ditionally, for clarity, we will always denote with
y1 the predicted unbiased sample, while yj , j =
2..|Y |, denote the biased samples. Let M be a
matrix of measurements, M ? IR|Y |?|Y |, such
that Mi.j =
PN
k=1 D(Ui,k,Uj,k)
N , where D(., .) is the
relative entropy. In other words, the matrix con-
tains the average distances between pairs of sam-
219
Mode Domain Genre
1 BNC BNC BNC
2 W S education W miscellaneous
3 S W leisure W pop lore
4 W arts W nonacad soc sci
5 W belief thought W nonacad hum art
.. .. ..
C-4 S spont conv C1 S sportslive
C-3 S spont conv C2 S consultation
C-2 S spont conv DE W fict drama
C-1 S spont conv UN S lect commerce
C no cat no cat
Table 1. Rankings based on ?, as the mean distance
between samples from the BNC partitions plus samples
from the whole corpus (BNC). C is the total number of
categories. W stands for Written, S for Spoken. C1, C2,
DE, UN are demographic classes for the spontaneous
conversations, no cat is the BNC undefined category.
ples (biased or unbiased). Each row Mi ? IR|Y |
contains the average distances between yi and all
other ys, including yi. A score ?i is assigned to
each yi which is equal to the mean of the vector
Mi (excluding Mi,j , j = i, which is always equal
to 0):
?i =
1
|Y | ? 1
|Y |
?
j=1,j 6=i
Mi,j (3)
We propose this function as a figure of merit1
for assigning a score to sampling methods. The
smaller the ? value the closer the sampling method
is to a uniform sampling method, with respect to
the pre-defined set of biased sampling categories.
3.4 Randomness of BNC samples
Later we will show how this hypothesis is consis-
tent with empirical evidence gathered from Web
data. Here we illustrate a proof-of-concept exper-
iment conducted on the BNC. In the BNC docu-
ments come classified along different dimensions
thus providing a controlled environment to test our
hypothesis. We adopt here David Lee?s revised
classification (Lee, 2001) and we partition the doc-
uments in terms of ?mode? (spoken/written), ?do-
main? (19 labels; e.g., imaginative, leisure, etc.)
and ?genre? (71 labels; e.g., interview, advertise-
ment, email, etc.). For each of the three main
partitions we sampled with replacement (from a
distribution determined by relative frequency in
the relevant set) 1,000 words from the BNC and
from each of the labels belonging to the specific
1A function which measures the quality of the sampling
method with the convention that smaller values are better as
with merit functions in statistics.
partitions.2 Then we measured the distance be-
tween each label in a partition, plus the sample
from the whole BNC. We repeated this experiment
100 times, built a matrix of average distances, and
ranked each label yi, within each partition type,
using ?i. Table 1 summarizes the results (only par-
tial results are shown for domain and genre). In all
three experiments the unbiased sample ?BNC? is
ranked higher than all other categories. At the top
of the rankings we also find other less narrowly
topic/genre-dependent categories such as ?W? for
mode, or ?W miscellaneous? and ?W pop lore?
for genre. Thus the hypothesis seems supported by
these experiments. Unbiased sampled unigrams
tend to be closer, on average, to biased samples.
4 Evaluating the randomness of
Google-derived corpora
When downloading documents from the Web via a
search engine (or sample them in other ways), one
cannot choose to sample randomly, nor select doc-
uments belonging to a certain category. One can
try to control the typology of documents returned
by using specific query terms. At this point a mea-
sure such as the one we proposed can be used to
choose the least biased retrieved collection among
a set of retrieved collections.
4.1 Biased and unbiased query categories
To construct a ?balanced? corpus via a search
engine one reasonable strategy is to use appro-
priately balanced query terms, e.g., using ran-
dom terms extracted from an available balanced
corpus (Sharoff, 2006). We will evaluate sev-
eral such strategies by comparing the derived
collections with those obtained with openly bi-
ased/specialized Web corpora. In order to build
specialized domain corpora, we use biased query
terms from the appropriate domain following the
approach of Baroni and Bernardini (2004). We
compiled several lists of words that define likely
biased and unbiased categories. We extracted the
less biased terms from the balanced 1M-words
Brown corpus of American English (Kuc?era and
Francis, 1967), from the 100M-words BNC, and
from a list of English ?basic? terms. From these
resources we defined the following categories of
query terms:
2We filtered out words in a stop list containing 1,430
types, which were either labeled with one of the BNC func-
tion word tags (such as ?article? or ?coordinating conjunc-
tion?), or occurred more than 50,000 times.
220
1. Brown.hf: the top 200 most frequent words
from the Brown corpus;
2. Brown.mf: 200 random terms with fre-
quency between 100 and 50 inclusive from
Brown;
3. Brown.af: 200 random terms with minimum
frequency 10 from Brown;
4. BNC.mf: 200 random terms with frequency
between 506 and 104 inclusive from BNC;
5. BNC.af: 200 random terms from BNC;
6. BNC.demog: 200 random terms with fre-
quency between 1000 and 50 inclusive from
the BNC spontaneous conversation sections;
7. 3esl: 200 random terms from an ESL ?core
vocabulary? list.3
Some of these lists implement plausible strate-
gies to get an unbiased sample from the search
engine: high frequency words and basic vocab-
ulary words should not be linked to any specific
domain; while medium frequency words, such as
the words in the Brown.mf/af and BNC.mf lists,
should be spread across a variety of domains and
styles. The BNC.af list is sampled randomly from
the whole BNC and, because of the Zipfian prop-
erties of word types, coupled with the large size
of the BNC, it is mostly characterized by very low
frequency words. In this case, we might expect
data sparseness problems. Finally, we expect the
spoken demographic sample to be a ?mildly bi-
ased? set, as it samples only words used in spoken
conversational English.
In order to build biased queries, hopefully lead-
ing to the retrieval of topically related documents,
we defined a set of specialized categories us-
ing the WordNet (Fellbaum, 1998) ?domain? lists
(Magnini and Cavaglia, 2000). We selected 200
words at random from each of the following do-
mains: administration, commerce, computer sci-
ence, fashion, gastronomy, geography, law, mili-
tary, music, sociology. These domains were cho-
sen since they look ?general? enough that they
should be very well-represented on the Web, but
not so general as to be virtually unbiased (cf. the
WordNet domain person). We selected words only
among those that did not belong to more than
3http://wordlist.sourceforge.net/
12dicts-readme.html
one WordNet domain, and we avoided multi-word
terms.
It is important to realize that a balanced corpus
is not necessary to produce unbiased seeds, nor a
topic-annotated lexical resource for biased seeds.
Here we focus on these sources to test plausible
candidate seeds. However, biased seeds can be ob-
tained following the method of Baroni and Bernar-
dini (2004) for building specialized corpora, while
unbiased seeds could be selected, for example,
from word lists extracted from all corpora ob-
tained using the biased seeds.
4.2 Experimental setting
From each source list we randomly select 20 pairs
of words without replacement. Each pair is used
as a query to Google, asking for pages in En-
glish only. Pairs are used instead of single words
to maximize our chances to find documents that
contain running text (Sharoff, 2006). For each
query, we retrieve a maximum of 20 documents.
The whole procedure is repeated 20 times with all
lists, so that we can compute the mean distances
to fill the distance matrices. Our unit of analysis
is the corpus of all the non-duplicated documents
retrieved with a set of 20 paired word queries.
The documents retrieved from the Web undergo
post-processing, including filtering by minimum
and maximum size, removal of HTML code and
?boilerplate? (navigational information and simi-
lar) and heuristic filtering of documents that do
not contain connected text. A corpus can con-
tain maximally 400 documents (20 queries times
20 documents retrieved per query), although typi-
cally the documents retrieved are less, because of
duplicates, or because some query pairs are found
in less than 20 documents. Table 2 summarizes
the average size in terms of word types, tokens
and number of documents of the resulting cor-
pora. Queries for the unbiased seeds tend to re-
trieve more documents except for the BNC.af set,
which, as expected, found considerably less data
than the other unbiased sets. Most of the differ-
ences are not statistically significant and, as the ta-
ble shows, the difference in number of documents
is often counterbalanced by the fact that special-
ized queries tend to retrieve longer documents.
4.3 Distance matrices and bootstrap error
estimation
After collecting the data each sample was repre-
sented as a frequency list as we did before with
221
Search category Types Tokens Docs
Brown.hf 39.3 477.2 277.2
Brown.mf 32.8 385.3 261.1
Brown.af 35.9 441.5 262.5
BNC.mf 45.6 614.7 253.6
BNC.af 23.0 241.7 59.7
BNC.demog 32.6 367.1 232.2
3esl 47.1 653.2 261.9
Admin 39.8 545.1 220.5
Commerce 38.9 464.5 184.7
Comp sci 25.8 311.5 185.3
Fashion 44.5 533.7 166.2
Gastronomy 36.5 421.7 159.0
Geography 42.7 498.0 167;6
Law 49.2 745.4 211.4
Military 47.1 667.8 223.0
Music 45.5 558.7 201.3
Sociology 56.0 959.5 258.8
Table 2. Average number of types, tokens and docu-
ments of corpora constructed with Google queries (type
and token sizes in thousands).
the BNC partitions (cf. section 3.4). Unigram dis-
tributions resulting from different search strate-
gies were compared by building a matrix of mean
distances between pairs of unigram distributions.
Rows and columns of the matrices are indexed by
the query category, the first category corresponds
to one unbiased query, while the remaining in-
dexes correspond to the biased query categories;
i.e., M ? IR11?11, Mi,j =
P20
k=1 D(Ui,k,Uj,k)
20 ,
where Us,k is the kth unigram distribution pro-
duced with query category ys.
These Web-corpora can be seen as a dataset D
of n = 20 data-points each consisting of a series
of unigram word distributions, one for each search
category. If all n data-points are used once to build
the distance matrix we obtain one such matrix for
each unbiased category and rank each search strat-
egy yi using ?i, as before (cf. section 3.3). Instead
of using all n data-points once, we createB ?boot-
strap? datasets (Duda et al, 2001) by randomly se-
lecting n data-points fromD with replacement (we
used a value of B=10). The B bootstrap datasets
are treated as independent sets and used to produce
B individual matricesMb from which we compute
the score ?i,b, i.e., the mean distance of a category
yi with respect to all other query categories in that
specific bootstrap dataset. The bootstrap estimate
of ?i, called ??i is the mean of the B estimates on
the individual datasets:
??i =
1
B
B
?
b=1
?i,b (4)
Bootstrap estimation can be used to compute the
standard error of ?i:
?boot[?i] =
?
?
?
?
1
B
B
?
b=1
[??i ? ?i,b]2 (5)
Instead of building one matrix of average dis-
tances over N trials, we could build N matri-
ces and compute the variance from there rather
than with bootstrap methods. However this sec-
ond methodology produces noisier results. The
reason for this is that our hypothesis rests on the
assumption that the estimated average distance is
reliable. Otherwise, the distance of two arbitrary
biased distributions can very well be smaller than
the distance of one unbiased and a biased one, pro-
ducing noisier measurements.
As we did before for the BNC data, we
smoothed the word counts by adding a count of 1
to all words in the overall dictionary. This dictio-
nary is approximated with the set of all words oc-
curring in the unigrams involved in a given exper-
iment, overall on average approximately 1.8 mil-
lion types (notice that numbers and other special
tokens are boosting up this total). Words with an
overall frequency greater than 50,000 are treated
as stop words and excluded from consideration
(188 types).
5 Results
Table 3 summarizes the results of the experiments
with Google. Each column represents one experi-
ment involving a specific ? supposedly ? unbiased
category. The category with the best (lowest) ?
score is highlighted in bold. The unbiased sample
is always ranked higher than all biased samples.
The results show that the best results are achieved
with Brown corpus seeds. The bootstrapped er-
ror estimate shows that the unbiased Brown sam-
ples are significantly more random than the biased
samples and, orthogonally, of the BNC and 3esl
samples. In particular medium frequency terms
seem to produce the best results, although the dif-
ference among the three Brown categories are not
significant. Thus, while more testing is needed,
our data provide some support for the choice of
medium frequency words as best seeds.
Terms extracted from the BNC are less effec-
tive than terms from the Brown corpus. One pos-
sible explanation is that the Web is likely to con-
tain much larger portions of American than British
English, and thus the BNC queries are overall
222
? scores with bootstrap error estimates
Category Brown.mf Brown.af Brown.hf BNC.mf BNC.demog BNC.all 3esl
Unbiased .1248/.0015 .1307/.0019 .1314/.0010 .1569/.0025 .1616/.0026 .1635/.0026 .1668/.0030
Commerce .1500/.0074 .1500/.0074 .1500/.0073 .1708/.0088 .1756/.0090 .1771/.0091 .1829/.0093
Geography .1702/.0084 .1702/.0084 .1707/.0083 .1925/.0089 .1977/.0091 .1994/.0092 .2059/.0094
Fashion .1732/.0060 .1732/.0060 .1733/.0059 .1949/.0069 .2002/.0070 .2019/.0071 .2087/.0073
Admin .1738/.0034 .1738/.0034 .1738/.0033 .2023/.0037 .2079/.0038 .2096/.0038 .2163/.0039
Comp sci .1749/.0037 .1749/.0037 .1746/.0038 .1858/.0041 .1912/.0042 .1929/.0042 .1995/.0043
Military .1899/.0070 .1899/.0070 .1901/.0067 .2233/.0079 .2291/.0081 .2311/.0082 .2384/.0084
Music .1959/.0067 .1959/.0067 .1962/.0067 .2196/.0077 .2255/.0078 .2274/.0079 .2347/.0081
Gastronomy .1973/.0122 .1973/.0122 .1981/.0120 .2116/.0133 .2116/.0133 .2193/.0138 .2266/.0142
Law .1997/.0060 .1997/.0060 .1990/.0061 .2373/.0067 .2435/.0068 .2193/.0138 .2533/.0070
Sociology .2393/.0063 .2393/.0063 .2389/.0062 .2885/.0069 .2956/.0070 .2980/.0071 .3071/.0073
Table 3. Mean scores based on ? with bootstrap standard error (B=10). In bold the lowest (best) score in each
column, always the unbiased category.
more biased than the Brown queries. Alterna-
tively, this might be due to the smaller, more con-
trolled nature of the Brown corpus, where even
medium- and low-frequency words tend to be rel-
atively common terms. The internal ranking of the
BNC categories, although not statistically signifi-
cant, seems also to suggest that medium frequency
words (BNC.mf) are better than low frequency
words. In this case, the all/low frequency set
(BNC.af) tends to contain very infrequent words;
thus, the poor performance is likely due to data
sparseness issues, as also indicated by the rela-
tively smaller quantity of data retrieved (Table 2
above). We take the comparatively lower rank
of BNC.demog to constitute further support for
the validity of our method, given that the corre-
sponding set, being entirely composed of words
from spoken English, should be more biased than
other unbiased sets. This latter finding is partic-
ularly encouraging because the way in which this
set is biased, i.e., in terms of mode of communica-
tion, is completely different from the topic-based
bias of the WordNet sets. Finally, the queries
extracted from the 3esl set are the most biased.
This unexpected result might relate to the fact
that, on a quick inspection, many words in this
set, far from being what we would intuitively con-
sider ?core? vocabulary, are rather cultivated, of-
ten technical terms (aesthetics, octopi, misjudg-
ment, hydroplane), and thus they might show a
register-based bias that we do not find in lists
extracted from balanced corpora. We randomly
selected 100 documents from the corpora con-
structed with the ?best? unbiased set (Brown.mf)
and 100 documents from this set, and we classi-
fied them in terms of genre, topic and other cat-
egories (in random order, so that the source of
the rated documents was not known). This pre-
liminary analysis did not highlight dramatic dif-
ferences between the two corpora, except for the
fact that 6 over 100 documents in the 3esl sub-
corpus pertained to the rather narrow domain of
aviation and space travel, while no comparably
narrow topic had such a large share of the distri-
bution in the Brown.mf sub-corpus. More research
is needed into the qualitative differences that cor-
relate with our figure of merit. Finally, although
different query sets retrieve different amounts of
documents, and lead to the construction of corpora
of different lengths, there is no sign that these dif-
ferences are affecting our figure of merit in a sys-
tematic way; e.g., some of the larger collections,
in terms of number of documents and token size,
are both at the top (most unbiased samples) and at
the bottom of the ranks (law, sociology).
On Web data we observed the same effect we
saw with the BNC data, where we could directly
sample from the whole collection and from its bi-
ased partitions. This provides support for the hy-
pothesis that our measure can be used to evaluate
how unbiased a corpus is, and that issuing unbi-
ased/biased queries to a search engine is a viable,
nearly knowledge-free way to create unbiased cor-
pora, and biased corpora to compare them against.
6 Conclusion
As research based on the Web as corpus becomes
more prominent within computational and corpus-
based linguistics, many fundamental issues have
to be tackled in a systematic way. Among these,
the problem of assessing the quality and nature
of automatically created corpora, where we do
not know a priori the composition of the cor-
pus. In this paper, we considered an approach to
automated corpus construction, via search engine
queries for combinations of a set of seed words.
223
We proposed an automated, quantitative, nearly
knowledge-free way to evaluate how biased a cor-
pus constructed in this way is. Our method is
based on the idea that the more a collection is un-
biased the closer its distribution of words will be,
on average, to reference distributions derived from
biased partitions (we showed that this is indeed the
case using a fully available balanced collection;
i.e., the BNC), and on the idea that biased collec-
tions of Web documents can be created by issu-
ing biased queries to a search engine. The results
of our experiments with Google support our hy-
pothesis, and suggest that seeds to build unbiased
corpora should be selected among mid-frequency
words rather than high or low frequency words.
We realize that our study opens many ques-
tions. The most crucial issue is probably what it
means for a corpus to be unbiased. As we already
stressed, we do not necessarily want our corpus
to be an unbiased sample of what is out there on
the Net ? we want it to be composed of content-
rich pages, and reasonably balanced in terms of
topics and genres, despite the fact that the Web
itself is unlikely to be ?balanced?. For our pur-
poses, we implicitly define balance in terms of the
set of biased corpora that we compare the target
corpus against. Assuming that our measure is ap-
propriate, what it tells us is that a certain corpus is
more/less biased than another corpus with respect
to the biased corpora they are compared against. It
remains to be seen how well the results generalize
across different typologies of biased corpora.
The method is not limited to the evaluation of
corpora built via search engine queries; e.g., it
would be interesting to compare the latter to cor-
pora built by Web crawling. The method could
be also applied to the analysis of corpora in gen-
eral (Web-derived or not), both for the purpose of
evaluating biased-ness, and as a general purpose
corpus comparison technique (Kilgarriff, 2001).
Acknowledgments
We would like to thank Ioannis Kontoyiannis,
Adam Kilgarriff and Silvia Bernardini for useful
comments on this work.
References
G. Aston and L. Burnard. 1998. The BNC Handbook:
Exploring the British National Corpus with SARA.
Edinburgh University Press, Edinburgh.
M. Baroni and S. Bernardini. 2004. BootCaT: Boot-
strapping Corpora and Terms from the Web. In Pro-
ceedings of LREC 2004, pages 1313?1316.
K. Bharat and A. Broder. 1998. A Technique for Mea-
suring the Relative Size and Overlap of the Public
Web Search Engines. In Proceedings of WWW7,
pages 379?388.
T.M. Cover and J.A. Thomas. 1991. Elements of In-
formation Theory. Wiley, New York.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
Classification 2nd ed. Wiley Interscience, Wiley In-
terscience.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
B. Fletcher. 2004. Making the Web more Useful as
a Source for Linguistic Corpora. In U. Conor and
T. Upton, editors, Corpus Linguistics in North Amer-
ica 2002. Rodopi, Amsterdam.
R. Ghani, R. Jones, and D. Mladenic. 2001. Using
the Web to Create Minority Language Corpora. In
Proceedings of the 10th International Conference on
Information and Knowledge Management.
M. Henzinger, A. Heydon, and M. Najork. 2000. On
Near-Uniform URL Sampling. In Proceedings of
WWW9.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the Special Issue on the Web as Corpus. Compu-
tational Linguistics, 29:333?347.
A. Kilgarriff. 2001. Comparing Corpora. Interna-
tional Journal of Corpus Linguistics, 6:1?37.
H. Kuc?era and W. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown Uni-
versity Press, Providence, RI.
D. Lee. 2001. Genres, Registers, Text, Types, Do-
mains and Styles: Clarifying the Concepts and Nav-
igating a Path through the BNC Jungle. Language
Learning & Technology, 5(3):37?72.
B. Magnini and G. Cavaglia. 2000. Integrating Subject
Field Codes into WordNet. In Proceedings of LREC
2000, Athens, pages 1413?1418.
C.E. Shannon. 1948. A Mathematical Theory of Com-
munication. Bell System Technical Journal, 27:379?
423 and 623?656.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, WaCky! Working pa-
pers on the Web as Corpus. Gedit, Bologna.
M. Ueyama and M. Baroni. 2005. Automated Con-
struction and Evaluation of a Japanese Web-Based
Reference Corpus. In Proceedings of Corpus Lin-
guistics 2005.
224
Large linguistically-processed Web corpora for multiple languages
Marco Baroni
SSLMIT
University of Bologna
Italy
baroni@sslmit.unibo.it
Adam Kilgarriff
Lexical Computing Ltd. and
University of Sussex
Brighton, UK
adam@lexmasterclass.com
Abstract
The Web contains vast amounts of linguis-
tic data. One key issue for linguists and
language technologists is how to access
it. Commercial search engines give highly
compromised access. An alternative is to
crawl the Web ourselves, which also al-
lows us to remove duplicates and near-
duplicates, navigational material, and a
range of other kinds of non-linguistic mat-
ter. We can also tokenize, lemmatise and
part-of-speech tag the corpus, and load the
data into a corpus query tool which sup-
ports sophisticated linguistic queries. We
have now done this for German and Ital-
ian, with corpus sizes of over 1 billion
words in each case. We provide Web ac-
cess to the corpora in our query tool, the
Sketch Engine.
1 Introduction
The Web contains vast amounts of linguistic data
for many languages (Kilgarriff and Grefenstette,
2003). One key issue for linguists and language
technologists is how to access it. The drawbacks
of using commercial search engines are presented
in Kilgarriff (2003). An alternative is to crawl the
Web ourselves.1 We have done this for two lan-
guages, German and Italian, and here we report on
the pipeline of processes which give us reasonably
well-behaved, ?clean? corpora for each language.
1Another Web access option is Alexa (http://pages.
alexa.com/company/index.html), who allow the
user (for a modest fee) to access their cached Web directly.
Using Alexa would mean one did not need to crawl; however
in our experience, crawling, given free software like Heritrix,
is not the bottleneck. The point at which input is required is
the filtering out of non-linguistic material.
We use the German corpus (which was developed
first) as our example throughout. The procedure
was carried on a server running RH Fedora Core 3
with 4 GB RAM, Dual Xeon 4.3 GHz CPUs and
about 2.5 TB hard disk space. We are making the
tools we develop as part of the project freely avail-
able,2 in the hope of stimulating public sharing of
resources and know-how.
2 Crawl seeding and crawling
We would like a ?balanced? resource, containing
a range of types of text corresponding, to some
degree, to the mix of texts we find in designed lin-
guistic corpora (Atkins et al, 1992), though also
including text types found on the Web which were
not anticipated in linguists? corpus design discus-
sions. We do not want a ?blind? sample dominated
by product listings, catalogues and computer sci-
entists? bulletin boards. Our pragmatic solution is
to query Google through its API service for ran-
dom pairs of randomly selected content words in
the target language. In preliminary experimenta-
tion, we found that single word queries yielded
many inappropriate pages (dictionary definitions
of the word, top pages of companies with the word
in their name), whereas combining more than two
words retrieved pages with lists of words, rather
than collected text.
Ueyama (2006) showed how queries for words
sampled from traditional written sources such as
newspaper text and published essays tend to yield
?public sphere? pages (online newspaper, govern-
ment and academic sites), whereas basic vocabu-
lary/everyday life words tend to yield ?personal?
pages (blogs, bulletin boards). Since we wanted
both types, we obtained seed URLs with queries
2http://sslmitdev-online.sslmit.unibo.
it/wac/wac.php
87
for words from both kinds of sources. For Ger-
man, we sampled 2000 mid-frequency words from
a corpus of the Su?ddeutsche Zeitung newspaper
and paired them randomly. Then, we found a ba-
sic vocabulary list for German learners,3 removed
function words and particles and built 653 random
pairs. We queried Google via its API retrieving
maximally 10 pages for each pair. We then col-
lapsed the URL list, insuring maximal sparseness
by keeping only one (randomly selected) URL for
each domain, leaving a list of 8626 seed URLs.
They were fed to the crawler.
The crawls are performed using the Her-
itrix crawler,4 with a multi-threaded breadth-first
crawling strategy. The crawl is limited to pages
whose URL does not end in one of several suffixes
that cue non-html data (.pdf, .jpeg, etc.)5 For
German, the crawl is limited to sites from the .de
and .at domains. Heritrix default crawling op-
tions are not modified in any other respect. We
let the German crawl run for ten days, retrieving
gzipped archives (the Heritrix output format) of
about 85GB.
3 Filtering
We undertake some post-processing on the ba-
sis of the Heritrix logs. We identify documents
of mime type text/html and size between 5
and 200KB. As observed by Fletcher (2004) very
small documents tend to contain little genuine text
(5KB counts as ?very small? because of the html
code overhead) and very large documents tend to
be lists of various sorts, such as library indices,
store catalogues, etc. The logs also contain sha-
1 fingerprints, allowing us to identify perfect du-
plicates. After inspecting some of the duplicated
documents (about 50 pairs), we decided for a dras-
tic policy: if a document has at least one dupli-
cate, we discard not only the duplicate(s) but also
the document itself. We observed that, typically,
such documents came from the same site and were
warning messages, copyright statements and sim-
ilar, of limited or no linguistic interest. While the
strategy may lose some content, one of our gen-
eral principles is that, given how vast the Web is,
we can afford to privilege precision over recall.
All the documents that passed the pre-filtering
3http://mypage.bluewin.ch/a-z/
cusipage/
4http://crawler.archive.org
5Further work should evaluate pros and cons of retrieving
documents in other formats, e.g., Adobe pdf.
stage are run through a perl program that performs
1) boilerplate stripping 2) function word filtering
3) porn filtering.
Boilerplate stripping
By ?boilerplate? we mean all those components
of Web pages which are the same across many
pages. We include stripping out HTML markup,
javascript and other non-linguistic material in this
phase. We aimed to identify and remove sections
of a document that contain link lists, navigational
information, fixed notices, and other sections poor
in human-produced connected text. For purposes
of corpus construction, boilerplate removal is crit-
ical as it will distort statistics collected from the
corpus.6 We adopted the heuristic used in the Hyp-
pia project BTE tool,7: content-rich sections of a
page will have a low html tag density, whereas
boilerplate is accompanied by a wealth of html
(because of special formatting, newlines, links,
etc.) The method is based on general properties
of Web documents, so is relatively independent of
language and crawling strategy.
Function word and pornography filtering
Connected text in sentences reliably contains a
high proportion of function words (Baroni, to ap-
pear), so, if a page does not meet this criterion
we reject it. The German function word list con-
tains 124 terms. We require that a minimum of 10
types and 30 tokens appear in a page, with a ra-
tio of function words to total words of at least one
quarter. The filter also works as a simple language
identifier.8
Finally, we use a stop list of words likely to oc-
cur in pornographic Web pages, not out of prudery,
but because they tend to contain randomly gener-
ated text, long keyword lists and other linguisti-
cally problematic elements. We filter out docu-
ments that have at least three types or ten tokens
from a list of words highly used in pornography.
The list was derived from the analysis of porno-
graphic pages harvested in a previous crawl. This
is not entirely satisfactory, since some of the words
6We note that this phase currently removes the links from
the text, so we can no longer explore the graph structure of
the dataset. In future we may retain link structure, to support
research into the relation between it and linguistic character-
istics.
7http://www.smi.ucd.ie/hyppia/
8Of course, these simple methods will not filter out all
machine-generated text (typically produced as part of search
engine ranking scams or for other shady purposes); some-
times this appears to have been generated with a bigram lan-
guage model, and thus identifying it with automated tech-
niques is far from trivial.
88
in the list, taken in isolation, are wholly innocent
(fat, girls, tongue, etc.) We shall revisit the strat-
egy in due course.
This filtering took 5 days and resulted in a ver-
sion of the corpus containing 4.86M documents
for a total of 20GB of uncompressed data.
4 Near-duplicate detection
We use a simplified version of the ?shingling? al-
gorithm (Broder et al, 1997). For each document,
after removing all function words, we take finger-
prints of a fixed number s of randomly selected n-
grams; then, for each pair of documents, we count
the number of shared n-grams, which can be seen
as an unbiased estimate of the overlap between the
two documents (Broder et al, 1997; Chakrabarti,
2002). We look for pairs of documents sharing
more than t n-grams, and we discard one of the
two.
After preliminary experimentation, we chose to
extract 25 5-grams from each document, and to
treat as near-duplicates documents that shared at
least two of these 5-grams. Near-duplicate spot-
ting on the German corpus took about 4 days.
2,466,271 near-duplicates were removed. The cor-
pus size decreased to 13GB. Most of the process-
ing time was spent in extracting the n-grams and
adding the corresponding fingerprints to the data-
base (which could be parallelized).
5 Part-of-speech tagging/lemmatization
and post-annotation cleaning
We performed German part-of-speech tagging and
lemmatization with TreeTagger.9 Annotation took
5 days. The resulting corpus contains 2.13B
words, or 34GB of data including annotation.
After inspecting various documents from the
annotated corpus, we decided to perform a further
round of cleaning. There are two reasons for this:
first, we can exploit the annotation to find other
anomalous documents, through observing where
the distribution of parts-of-speech tags is very un-
usual and thus not likely to contain connected text.
Second, the TreeTagger was not trained on Web
data, and thus its performance on texts that are
heavy on Web-like usage (e.g., texts all in lower-
case, colloquial forms of inflected verbs, etc.) is
dismal. While a better solution to this second
problem would be to re-train the tagger on Web
9http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger
data (ultimately, the documents displaying the sec-
ond problem might be among the most interest-
ing ones to have in the corpus!), for now we try to
identify the most problematic documents through
automated criteria and discard them. The cues we
used included the number of words not recognised
by the lemmatizer; the proportion of words with
upper-case initial letters; proportion of nouns, and
proportion of sentence markers.
After this further processing step, the corpus
contains 1,870,259 documents from 10818 differ-
ent domains, and its final size is 1.71 billion to-
kens (26GB of data, with annotation). The final
size of the Italian corpus is 1,875,337 documents
and about 1.9 billion tokens.
6 Indexing and Web user interface
We believe that matters of efficient indexing and
user friendly interfacing will be crucial to the suc-
cess of our initiative, both because many linguists
will lack the relevant technical skills to write their
own corpus-access routines, and because we shall
not publicly distribute the corpora for copyright
reasons; an advanced interface that allows lin-
guists to do actual research on the corpus (includ-
ing the possibility of saving settings and results
across sessions) will allow us to make the corpus
widely available while keeping it on our servers.10
We are using the Sketch Engine,11 a corpus query
tool which has been widely used in lexicography
and which supports queries combining regular ex-
pressions and boolean operators over words, lem-
mas and part-of-speech tags.
7 Comparison with other corpora
We would like to compare the German Web cor-
pus to an existing ?balanced? corpus of German
attempting to represent a broad range of genres
and topics. Unfortunately, as far as we know no
resource of this sort is publicly available (which
is one of the reasons why we are interested in de-
veloping the German Web corpus in the first in-
stance.) Instead, we use a corpus of newswire
articles from the Austria Presse Agentur (APA,
kindly provided to us by ?OFAI) as our reference
10The legal situation is of course complex. We consider
that our case is equivalent to that of other search engines,
and that offering linguistically-encoded snippets of pages to
researchers does not go beyond the ?fair use? terms routinely
invoked by search engine companies in relation to Web page
caching.
11http://www.sketchengine.co.uk/
89
WEB APA
ich hier APA NATO
dass wir Schlu? EU
und man Prozent Forts
sie nicht Mill AFP
ist das MRD Dollar
oder sind Wien Reuters
kann so Kosovo Dienstag
du mir DPA Mittwoch
wenn ein US Donnerstag
was da am sei
Table 1: Typical Web and APA words
point. This corpus contains 28M tokens, and,
despite its uniformity in terms of genre and re-
stricted thematic range, it has been successfully
employed as a general-purpose German corpus in
many projects. After basic regular-expression-
based normalization and filtering, the APA con-
tains about 500K word types, the Web corpus
about 7.4M. There is a large overlap among the 30
most frequent words in both corpora: 24 out of 30
words are shared. The non-overlapping words oc-
curring in the Web top 30 only are function words:
sie ?she?, ich ?I?, werden ?become/be?, oder ?or?,
sind ?are?, er ?he?. The words only in the APA
list show a bias towards newswire-specific vocab-
ulary (APA, Prozent ?percent?, Schlu? ?closure?)
and temporal expressions that are also typical of
newswires (am ?at?, um ?on the?, nach ?after?).
Of the 232,322 hapaxes (words occurring only
once) in the APA corpus, 170,328 (73%) occur in
the Web corpus as well.12 89% of these APA ha-
paxes occur more than once in the Web corpus,
suggesting how the Web data will help address
data sparseness issues.
Adopting the methodology of Sharoff (2006),
we then extracted the 20 words most characteris-
tics of the Web corpus vs. APA and vice versa,
based on the log-likelihood ratio association mea-
sure. Results are presented in Table 1. The APA
corpus has a strong bias towards newswire par-
lance (acronyms and named entities, temporal ex-
pressions, financial terms, toponyms), whereas the
terms that come out as most typical of the Web
corpus are function words that are not strongly
connected with any particular topic or genre. Sev-
eral of these top-ranked function words mark first
and second person forms (ich, du, wir, mir).
This preliminary comparison both functioned as
a ?sanity check?, showing that there is consider-
12Less than 1% of the Web corpus hapaxes are attested in
the APA corpus.
able overlap between our corpus and a smaller cor-
pus used in previous research, and suggested that
the Web corpus has more a higher proportion of
interpersonal material.
8 Conclusion
We have developed very large corpora from the
Web for German and Italian (with other languages
to follow). We have filtered and cleaned the text so
that the obvious problems with using the Web as a
corpus for linguistic research do not hold. Prelim-
inary evidence suggests the ?balance? of our Ger-
man corpus compares favourably with that of a
newswire corpus (though of course any such claim
begs a number of open research questions about
corpus comparability). We have lemmatised and
part-of-speech-tagged the data and loaded it into
a corpus query tool supporting sophisticated lin-
guistic queries, and made it available to all.
References
B. Atkins, J. Clear, and N. Ostler. 1992. Corpus design
criteria. Literary and Linguistic Computing, 7:1?16.
M. Baroni. to appear. Distributions in text. In
A. Lu?deling and M. Kyto?, editors, Corpus lin-
guistics: An international handbook. Mouton de
Gruyter, Berlin.
A. Broder, S. Glassman, M. Manasse, and G. Zweig.
1997. Syntactic clustering of the Web. In Proc.
Sixth International World-Wide Web Conference.
S. Chakrabarti. 2002. Mining the Web: Discovering
knowledge from hypertext data. Morgan Kaufmann,
San Francisco.
W. Fletcher. 2004. Making the web more useful as
a source for linguistic corpora. In U. Connor and
T. Upton, editors, Corpus Linguistics in North Amer-
ica 2002.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the Web as corpus. Compu-
tational Linguistics, 29(3):333?347.
A. Kilgarriff. 2003. Linguistic search engine. In
K. Simov, editor, Proc. SPROLAC Workshop, Lan-
caster.
S. Sharoff. 2006. Creating general-purpose corpora
using automated search engine queries. In M. Ba-
roni and S. Bernardini, editors, WaCky! Working pa-
pers on the Web as Corpus. Gedit, Bologna.
M. Ueyama. 2006. Creation of general-purpose
Japanese Web corpora with different search engine
query strategies. In M. Baroni and S. Bernardini,
editors, WaCky! Working papers on the Web as Cor-
pus. Gedit, Bologna.
90
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 619?627,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
EEG responds to conceptual stimuli and corpus semantics
Brian Murphy
CIMeC, University of Trento
Rovereto 38068, Italy
brian.murphy@unitn.it
Marco Baroni
CIMeC, University of Trento
Rovereto 38068, Italy
marco.baroni@unitn.it
Massimo Poesio
CIMeC, University of Trento
Rovereto 38068, Italy
massimo.poesio@unitn.it
Abstract
Mitchell et al (2008) demonstrated that
corpus-extracted models of semantic
knowledge can predict neural activation
patterns recorded using fMRI. This
could be a very powerful technique for
evaluating conceptual models extracted
from corpora; however, fMRI is expensive
and imposes strong constraints on data
collection. Following on experiments
that demonstrated that EEG activation
patterns encode enough information to
discriminate broad conceptual categories,
we show that corpus-based semantic rep-
resentations can predict EEG activation
patterns with significant accuracy, and
we evaluate the relative performance of
different corpus-models on this task.
1 Introduction
Models of semantic relatedness induced from cor-
pus data have proven effective in a number of em-
pirical tasks (Sahlgren, 2006) and there is increas-
ing interest in whether distributional information
extracted from corpora correlates with aspects
of speakers? semantic knowledge: see Lund and
Burgess (1996), Landauer and Dumais (1997), Al-
muhareb (2006), Pad?o and Lapata (2007), Schulte
im Walde (2008), among many others. For this
purpose, corpus models have been tested on data-
sets that are based on semantic judgements (met-
alinguistic or meta-cognitive intuitions about syn-
onymy, semantic distance, category-membership)
or behavioural experiments (semantic priming,
property generation, free association). While all
these data are valuable, they are indirect reflec-
tions of semantic knowledge, and when the pre-
dictions they make diverge from those of corpora,
interpretation is problematic: is the corpus model
missing essential aspects of semantics, or are non-
semantic factors biasing the data elicited from in-
formants?
Reading semantic processes and representations
directly from the brain would be an ideal way to
get around these limitations. Until recently, anal-
ysis of linguistic quantities using neural data col-
lected with EEG (measurement at the scalp of volt-
ages induced by neuronal firing) or fMRI (mea-
surement of changes of oxygen concentrations in
the brain tied to cognitive processes) had neither
the advantages of corpora (scale) nor of infor-
mants (finer grained judgements).
However, some clear patterns of differential ac-
tivity have been found for broad semantic classes.
Viewing images of natural (typically animals and
plants) and non-natural (typically artefacts like
tools or vehicles) objects elicits different loci of
activity in fMRI (Martin and Chao, 2001) and
EEG (Kiefer, 2001), that persist across partici-
pants. Differences have also been found in re-
sponse to auditorily or visually presented words of
different lexical classes, such as abstract/concrete,
and verb/noun (Pulverm?uller, 2002). But interpre-
tation of such group results remains somewhat dif-
ficult, as they may be consistent with more than
one distinction: the natural/artefactual division
just mentioned, may rather be between living/non-
living entities, dynamic/static entities, or be based
on embodied experience (e.g. manipulable or not).
More recently, however, machine learning and
other numerical techniques have been successfully
applied to extract semantic information from neu-
ral data in a more discriminative fashion, down
to the level of individual concepts. The work
presented here builds on two strands of previ-
ous work: Murphy et al (2008) use EEG data
to perform semantic categorisation on single stim-
uli; and Mitchell et al (2008) introduce an fMRI-
based method that detects word level distinctions
by learning associations between features of neu-
ral activity and semantic features derived from a
619
corpus. We combine these innovations by intro-
ducing a method that extracts featural represen-
tations from the EEG signal, and uses corpus-
based models to predict word level distinctions in
patterns of EEG activity. The proposed method
achieves a performance level significantly above
chance (also when distinguishing between con-
cepts from the same semantic category, e.g., dog
and cat), and approaching that achieved with
fMRI.
The paper proceeds as follows. The next section
describes a simple behavioural experiment where
Italian-speaking participants had to name photo-
graphic images of mammals and tools while their
EEG activity was being recorded, and continues
to detail how the rich and multidimensional sig-
nals collected were reduced to a small set of op-
timally informative features using a new method.
Section 3 describes a series of corpus-based se-
mantic models derived from both a raw-text web
corpus, and from various parsings of a conven-
tional corpus. In Section 4 we describe the train-
ing of a series of linear models, that each learn
the associations between a set of corpus semantic
features and an individual EEG activity feature.
By combining these models it is possible to pre-
dict the EEG activity pattern for a single unseen
word, and compare this to the observed pattern
for the corresponding concept. Results (Section
5) show that these predictions succeed at a level
significantly above chance, both for coarser dis-
tinctions between words in different superordinate
categories (e.g., differentiating between drill and
gorilla), and, at least for the model based on the
larger web corpus, for those within the same cate-
gory (e.g., drill vs spanner, koala vs gorilla).
2 Neural Activation Data
2.1 Data collection
EEG data was gathered from native speakers of
Italian during a simple behavioural experiment at
the CIMeC/DiSCoF laboratories at Trento Univer-
sity. Seven participants (five male and two fe-
male; age range 25-33; all with college educa-
tion) performed a silent naming task. Each of them
was presented
1
on screen with a series of contrast-
normalised greyscale photographs of tools (gar-
den and work tools) and land mammals (exclud-
ing emotionally valent domesticated animals and
1
Using the E-Prime software package: http://www.
pstnet.com/e-prime/.
~1.5s0.5s 0.5s 2s
Figure 1: Presentation of image stimuli
predators), for which they had to think of the most
appropriate name (see figure 1). They were not ex-
plicitly asked to group the entities into superordi-
nate categories, or to concentrate on their seman-
tic properties, but completing the task involved re-
solving each picture to its corresponding concept.
Images remained on screen until a keyboard re-
sponse was received from the participant to indi-
cate a suitable label had been found, and presenta-
tions were interleaved with three second rest peri-
ods. Thirty stimuli in each of the two classes were
each presented six times, in random order, to give
a total of 360 image presentations in the session.
Response rates were over 95%, and a post-session
questionnaire determined that participants agreed
on image labels in approximately 90% of cases.
English terms for the concepts used are listed be-
low.
Mammals anteater, armadillo, badger, beaver, bi-
son, boar, camel, chamois, chimpanzee, deer,
elephant, fox, giraffe, gorilla, hare, hedge-
hog, hippopotamus, ibex, kangaroo, koala,
llama, mole, monkey, mouse, otter, panda,
rhinoceros, skunk, squirrel, zebra
Tools Allen key, axe, chainsaw, craft knife, crow-
bar, file, garden fork, garden trowel, hack-
saw, hammer, mallet, nail, paint brush, paint
roller, pen knife, pick axe, plaster trowel,
pliers, plunger, pneumatic drill, power drill,
rake, saw, scissors, scraper, screw, screw-
driver, sickle, spanner, tape measure
The EEG signals were recorded at 500Hz from
64 scalp locations based on the 10-20 standard
620
montage.
2
The EEG recording computer and stim-
ulus presentation computer were synchronised by
means of parallel port transmitted triggers. Af-
ter the experiment, pre-processing of the recorded
signals was carried out using the EEGLAB pack-
age (Delorme and Makeig, 2003): signals were
band-pass filtered at 1-50Hz to remove slow drifts
and high-frequency noise, and then down-sampled
to 120Hz. An ICA decomposition was subse-
quently applied (Makeig et al, 1996), and signal
components due to eye-movements were manually
identified and removed.
As a preliminary test to verify that the recorded
signals included category specific patterns, we
applied a discriminative classification technique
based on source-separation, similar to that de-
scribed in Murphy et al (2008). This found that
the categories of mammals and tools could be dis-
tinguished with an accuracy ranging from 57% to
80% (mean of 72% over the seven participants).
2.2 Feature extraction
The features extracted are metrics of signal power
at a particular scalp location, in a particular fre-
quency band, and at a particular time latency rel-
ative to the presentation of each image stimulus.
Termed Event Related Synchronisation (ERS) or
Event Related Spectral Perturbation (ERSP), such
frequency-specific changes in signal amplitude are
known to correlate with a wide range of cogni-
tive functions (Pfurtscheller and Lopes da Silva,
1999), and have specifically been shown to be sen-
sitive to category distinctions during the process-
ing of linguistic and visual stimuli (Murphy et al,
2008; Gilbert et al, 2009).
Feature extraction and selection is performed
individually on a per-participant basis. As a first
step all signal channels are z-score normalised
to control for varying conductivity at each elec-
trode site, and a Laplacian sharpening is applied
to counteract the spatial blurring of signals caused
by the skull, and so minimise redundancy of infor-
mation between channels.
For each stimulus presentation, 14,400 signal
power features are extracted: 64 electrode chan-
nels by 15 frequency bands (of width 3.3Hz, be-
tween 1 and 50Hz) by 15 time intervals (of length
67ms, in the first second after image presentation).
A z-score normalisation is carried out across all
2
Using a Brain Vision BrainAmp system: http://
www.brainproducts.com/.
Figure 2: Mean rank of selected features in the
time/frequency space (left panel) and on the scalp
(right panel) for participant E
stimulus presentations to equalise variance across
frequencies and times: to control both for the low-
pass filtering action of the skull, and for the re-
duced synchronicity of activity at increasing laten-
cies. For each stimulus a mean is then taken over
each of six presentations to arrive at a more reli-
able power estimate for each feature.
3
The feature ranking method used in Mitchell et
al. (2008) evaluates the extent to which the rela-
tionship among stimuli is stable across across pre-
sentations, using a correlational measure,
4
but pre-
liminary analyses with this selection method on
EEG features proved disappointing. Here, two ad-
ditional ranking criteria are used: each feature is
evaluated for its noisiness (the amount of power
variation seen across presentations of the same
stimulus), and for its distinctiveness (the amount
of variation in power estimates across different
stimuli). A combination of these three strategies
is used to rank the features by their informative-
ness, and the top 50 features are then selected for
each participant.
5
A qualitative evaluation of the feature selec-
tion strategy can be carried out by examining
the distribution of features selected. Figure 2
shows the distribution of selected features over the
time/frequency spectrum (left panel), and over the
scalp (right panel - viewed from above, with the
nose pointing upwards). The distribution seen is
3
Stimulus power features are isolated by band-pass filter-
ing for the required frequencies, cropping following the rel-
evant time interval relative to each image presentation, and
then taking the variance of the resulting signal, which is pro-
portional to power.
4
See the associated supplementary materials of Mitchell
et al (2008) for details: http://www.sciencemag.
org/cgi/content/full/320/5880/1191/DC1.
5
Several combinations of these parameters (selection
thresholds of 5, 20, 50, 100, 200 features; ranking criteria in
isolation and in combination) were investigated - the one cho-
sen gave highest overall performance with the web-derived
corpus model: 50 features, combined ranking criteria.
621
Figure 3: First two components of principal com-
ponents analysis of selected features for partici-
pant E (crosses: mammals; circles: tools)
plausible in reference to previous work: lower fre-
quencies (Pfurtscheller and Lopes da Silva, 1999),
latencies principally in the first few hundred mil-
liseconds (Kiefer, 2001), and activity in the visual
centres at the rear of the head, as well as parietal
areas (Pulverm?uller, 2005). A principal compo-
nents analysis can also be performed on the se-
lected features to see if they reflect any plausi-
ble semantic space. As figure 3 shows, the fea-
ture selection stage has captured quite faithfully
the mammal/tool distinction in a totally unsuper-
vised fashion.
3 Corpus-based semantic models
Data from linguistics (Pustejovsky, 1995; Fill-
more, 1982) and neuroscience (Barsalou, 1999;
Barsalou, 2003; Pulverm?uller, 2005) underline
how certain verbs, by emphasising typical ways in
which we interact with entities and how they be-
have, are pivotal in the representation of concrete
nominal concepts. Following these traditions,
Mitchell et al (2008) use 25 manually picked
verbs as their corpus-based features.
Here that approach is replicated by translating
these verbs into Italian. Mitchell et al (2008) se-
lected verbs that denote our interaction with ob-
jects and living things, such as smell and ride.
While the translations are not completely faithful
(because frequent verbs of this sort tend to span
different sets of senses in the two languages), the
aim was to respect the same principle when build-
ing the Italian list. The full list, with our back
translations into English, is presented in Table 1.
We refer to this set as the ?Mitchell? verbs.
alzare ?raise? annusare ?smell/sniff?
aprire ?open? ascoltare ?listen?
assaggiare ?taste? avvicinare ?near?
cavalcare ?ride? correre ?run/flow?
dire ?say/tell? entrare ?enter?
guidare ?drive? indossare ?wear?
maneggiare ?handle? mangiare ?eat?
muovere ?move? pulire ?clean?
puzzare ?stink? riempire ?fill?
rompere ?break? sentire ?feel/hear?
sfregare ?rub? spingere ?push?
temere ?fear? toccare ?touch?
vedere ?see?
Table 1: The ?Mitchell? verbs, with English trans-
lations
As in Mitchell et al (2008), in order to find
a corpus large enough to provide reliable co-
occurrence statistics for our target concepts and
the 25 verbs, we resorted to the Web, queried us-
ing the Yahoo! API.
6
In particular, we represent
each concept by a vector that records how many
times it co-occurred with each target verb within
a span of 5 words left and right, according to Ya-
hoo! counts. We refer to this corpus-based model
as the yahoo-mitchell model below.
While manual verb picking has proved effec-
tive for Mitchell and colleagues (and for us, as we
will see in a moment), ultimately what we are in-
terested in is discovering the most distinctive fea-
tures of each conceptual category. We are there-
fore interested in more systematic approaches to
inducing corpus-based concept descriptions, and
in which of these approaches works best for this
task. The alternative models we consider were
not extracted from the Web, but from an existing
corpus, so that we could rely on pre-existing lin-
guistic annotation (POS tagging, lemmatization,
dependency paths), and perform more flexible,
annotation-aware queries to collect co-occurrence
statistics.
More specifically, we used the la Repub-
blica/SSLMIT corpus
7
, that contains about 400
million tokens of newspaper text. From this, we
extracted four models where nominal concepts are
represented in terms of patterns of co-occurrence
with verbs (we collected statistics for the top
20,000 most common nouns in the corpus, includ-
ing the concepts used as stimuli in the silent nam-
6
http://developer.yahoo.com/search/
7
http:://sslmit.unibo.it/repubblica/
622
ing experiment, and the top 5,000 verbs). We first
re-implemented a ?classic? window-based word
space model (Sahlgren, 2006), referred to below
as repubblica-window, where each noun lemma is
represented by its co-occurrence with verb lem-
mas within the maximum span of a sentence, with
no more than one other intervening noun. The
repubblica-position model is similar, but it also
records the position of the verb with respect to
the noun (so that X-usare ?X-use? and usare-X
?use-X? count as different features), analogously
to the seminal HAL model (Lund and Burgess,
1996). It has been shown that models that take
the syntactic relation between a target word and
a collocate feature into account can outperform
?flat? models in some tasks (Pad?o and Lapata,
2007). The next two models are based on the de-
pendency parse of the la Repubblica corpus docu-
mented by Lenci (2009). We only counted as col-
locates those verbs that were linked to nouns by
a direct path (such as subject and object) or via
preposition-mediated paths (e.g., tagliare con for-
bici ?to cut with scissors?), and where the paths
were among the top 30 most frequent in the cor-
pus. In the repubblica-depfilter model, we record
co-occurrence with verbs that are linked to the
nouns by one of the top 30 paths, but we do
not preserve the paths themselves in the features.
This is analogous to the model proposed by Pad?o
and Lapata (2007). In the repubblica-deppath
model, we preserve the paths as part of the fea-
tures (so that subj-uccidere ?subj-kill? and obj-
uccidere count as different features), analogously
to Lin (1998), Curran and Moens (2002) and oth-
ers. For all models, following standard practice in
computational linguistics (Evert, 2005), we trans-
form raw co-occurrence counts into log-likelihood
ratios.
Following the evaluation paradigm of Mitchell
et al (2008), linear models trained on corpus-
based features are used to predict the pattern of
EEG activity for unseen concepts. This only
works if we have a very limited number of fea-
tures (or else we would have more parameters to
estimate than data-points to estimate them). The
Repubblica-based models have thousands of fea-
tures (one per verb collocate, or verb+path collo-
cate). We adopt two strategies to select a reduced
number of features. In the topfeat versions, we
first pick the 50 features that have the highest asso-
ciation with each of the target concepts. We then
count in how many of these concept-specific top
lists a feature occurs, and we pick the 25 features
that occur in the largest number of them. The intu-
ition is that this should give us a good trade-off be-
tween how characteristic the features are (we only
use features that are highly associated with some
of our concepts), and their generalization capabili-
ties (we pick features that are associated with mul-
tiple concepts). Randomly selected examples of
the features extracted in this way for the various
Repubblica models are reported in Table 2.
repubblica-window repubblica-position
abbattere ?demolish? X-ferire ?X-wound?
afferrare ?seize? X-usare ?X-use?
impugnare ?grasp? dipingere-X ?paint-X?
tagliare ?cut? munire-X ?supply-X?
trovare ?find? tagliare-X ?cut-X?
repubblica-depfilter repubblica-deppath
abbattere ?demolish? con+tagliare ?with+cut?
correre ?run? obj+abbattere ?obj+demolish?
parlare ?speak? obj+uccidere ?obj+kill?
saltare ?jump? intr-subj+vivere ?intr-subj+live?
tagliare ?cut? tr-subj+aprire ?tr-subj+open?
Table 2: Examples of top features from the la Re-
pubblica models
Alternatively, instead of feature selection we
perform feature reduction by means of a Singular
Value Decomposition (SVD) of the noun-by-verb
matrix. We apply the SVD to matrices that include
the top 20,000 most frequent nouns in the cor-
pus (including our target concepts) since the qual-
ity of the resulting reduced model should improve
if we can exploit richer patterns of correlations
among the columns ? verbs ? across rows ? nouns
(Landauer and Dumais, 1997; Sch?utze, 1997). In
the svd versions of our models, we pick as fea-
tures the top 25 left singular vectors, weighted
by the corresponding singular values. These fea-
tures do not have a straightforward interpretation,
but they tend to group verb meanings that belong
to broad semantic domains. For example, among
the original verbs that are most strongly correlated
with one of the top singular vectors of repubblica-
window we find giocare ?play?, vincere ?win? and
perdere ?lose?. Another singular vector is asso-
ciated with ammontare ?amount?, costare ?cost?,
pagare ?pay?, etc. One of the top singular vec-
tors of repubblica-deppath is strongly correlated
with in+scendere ?descend into?, in+mettere ?put
into?, in+entrare ?enter into?, though not all sin-
gular vectors are so clearly characterized by the
verbs they correlate with.
623
None of the la Repubblica models had full cov-
erage of our concept stimulus set (see the second
column of Table 3 below), because our extraction
method missed some multi-word units, and fea-
ture selection led to losing some more items due
to data sparseness (e.g., some target words had no
collocates connected by the dependency paths we
selected). The experiments reported in the next
section used all the target concepts available in
each model, but a replication using the 50 concepts
that were common to all models obtained results
that are comparable. For a direct comparison be-
tween Yahoo! and la Repubblica derived features,
we tried collecting statistics for the Mitchell verbs
from Repubblica as well, but the resulting model
was extremely sparse, and we do not report its per-
formance here.
Finally, it is important to note that any repre-
sentation yielded by a corpus semantic model does
not characterise a concept directly, but is rather an
aggregate of the various senses and usages of the
noun chosen to represent it. This obvious limita-
tion will persist until comprehensive, robust and
computationally efficient word-sense disambigua-
tion techniques become available. However these
models are designed to extract semantic (as op-
posed to syntactic or phonological) properties of
words, and as noted in the introduction, have been
demonstrated to correlate with behavioural effects
of conceptual processing.
4 Predicting EEG patterns using
corpus-based models
In Section 2.2 above we showed how we extracted
features summarizing the spatial, temporal and
frequency distribution of the EEG signal collected
while participants were processing each of the tar-
get concepts. In Section 3, we described various
ways to obtain a compact representation of the
same concepts in terms of corpus-derived features.
We will now discuss the method we employed to
verify whether the corpus-derived features can be
used to predict the EEG patterns ? that is whether
semantics can be used to predict neural activity.
Our hope is that a good corpus-based model will
provide a decomposition of concepts into mean-
ingful properties, corresponding to coherent sub-
patterns of activation in the brain, and thus capture
generalizations across concepts. For example, if
a concept is particularly visually evocative (e.g.,
zebra), we might expect it to be strongly associ-
ated with the verb see, while also causing partic-
ular activation of the vision centres of the brain.
Similarly, concepts with strong associations with
a particular sound (e.g., cuckoo) might be seman-
tically associated with hear while also dispropor-
tionately activating auditory areas of the brain. It
should thus be possible to learn a model of corpus-
to-EEG-pattern correspondences on training data,
and use it to predict the EEG activation patterns of
unseen concepts.
We follow the paradigm proposed by Mitchell et
al. (2008) for fMRI data. For each participant and
selected EEG feature, we train a model where the
level of activation of the latter in response to dif-
ferent concepts is approximated by a linear com-
bination of the corpus features:
~
f = C
~
? + ~
where
~
f is the vector of activations of a specific
EEG feature for different concepts, the matrix C
contains the values of the corpus features for the
same concepts (row-normalised to z-scores),
~
? is
the weight we must learn for each corpus feature,
and~ is a vector of error terms. We use the method
of least squared errors to learn the weights that
maximize the fit of the model. We can then predict
the activation of an EEG feature in response to a
new concept that was not in the training data by a
~
?-weighted sum of the values of each corpus fea-
ture for the new concept. In some cases collinear-
ity in the corpus data (regular linear relationships
among the corpus-feature columns) prevented the
estimation procedure from finding a solution. In
such cases (due to the small number of data, rel-
ative to the number of unknowns), the least in-
formative corpus-features (those that correlated on
average most highly with other features) were iter-
atively removed until a solution was reached. All
models were trained with between 23 and 25 cor-
pus features.
Again following Mitchell and colleagues, we
adopt a leave-2-out paradigm in which a linear
model for each EEG feature is trained in turn on
all concepts minus 2. For each of the 2 left out
concepts, we predict the EEG activation pattern
using the trained linear model and their corpus
features, as just described. We then try to cor-
rectly match the predicted and observed activa-
tions, by measuring the Euclidean distance be-
tween the model-generated EEG activity (a vec-
tor of estimated power levels for the n EEG fea-
624
tures selected) and the corresponding EEG activ-
ity recorded in the experiment (other distance met-
rics gave similar results to the ones reported here).
Given the 2 left-out concepts a and b, we com-
pute 2 matched distances (i.e., distance between
predicted and observed pattern for a, and the same
for b) and 2 mismatched distances (predicted a and
observed b; predicted b and observed a). If the av-
erage of the matched distances is lower than the
average of the mismatched distances, we consider
the prediction successful ? otherwise we count is
as a failed prediction. At chance levels, expected
matching accuracy is 50%.
5 Results
Table 3 shows the comparative results for all the
corpus models introduced in Section 3. The third
column (all) shows the overall accuracy in cor-
rectly matching predicted to observed EEG ac-
tivity patterns, and so successfully distinguishing
word meanings. The significance of the figures is
indicated with the conventional annotation (calcu-
lated using a one-way two-sided t-test across the
individual participant accuracy figures against an
expected population mean of 50%).
8
The second
column shows the coverage of each model of the
60 mammal and tool concepts used, which ranged
from full (for the yahoo-mitchell model) to 51 con-
cepts (for the depfilter-topfeat model). The corre-
sponding number of matching comparisons over
which accuracy was calculated ranged from 1770
down to 1225.
As suggested by previous work (Murphy et al,
2008), and illustrated by figure 3, coarse distinc-
tions between words in different superordinate cat-
egories (e.g., hammer vs armadillo; giraffe vs
nail) may be easier to detect than those among
concepts within the same category (e.g., ham-
mer vs nail; giraffe vs armadillo). The fourth
and fifth columns give these accuracies, and while
between-category discriminations do prove more
reliable, they indicate that, for the top rated model
at least, finer within-category distinctions are also
being captured. Figures from the top two perform-
ing models are given for individual participants in
tables 4 and 5.
8
On average, the difference seen between matched and
mismatched pairs was small, at about 3% of the distance
between observed and predicted representations, and was
marginally bigger for correct than for incorrect predictions
(p < 0.01).
part. overall within between
A 54 53 55
B 54 47 60
C 62 56 67
D 61 56 67
E 68 58 78
F 52 54 51
G 57 51 63
Table 4: Accuracy levels for individual participant
sessions, yahoo-mitchell web corpus
part. overall within between
A 49 52 46
B 59 57 60
C 60 60 59
D 50 45 55
E 56 53 58
F 64 64 65
G 52 49 55
Table 5: Accuracy levels for individual participant
sessions, repubblica-window-svd
6 Discussion
Our results show that corpus-extracted conceptual
models can be used to distinguish between the
EEG activation levels associated with conceptual
categories to a degree that is significantly above
chance. Though category specific patterns are de-
tectable in the EEG signal alone (as illustrated by
the PCA analysis in figure 3), on that basis we can-
not be sure that semantics is being detected. Some
other property of the stimuli that co-varies with the
semantic classes of interest could be responsible,
such as visual complexity, conceptual familiarity,
lexical frequency, or phonological form. Only by
cross-training with individual corpus features and
showing that these hold a predictive relationship to
neural activity have we been able to establish that
EEG patterns encode semantics.
Present evidence indicates that fMRI may pro-
vide richer data for training such models than EEG
(Mitchell and colleagues obtain an average accu-
racy of 77%, and 65% for the within category set-
ting). However, fMRI has several clear disadvan-
tages as a tool for language researchers. First of
all, the fine spatial resolution it provides (down
to 2-3mm), while of great interest to neuroscien-
tists, is not in itself linguistically informative. Its
coarse temporal resolution (of the order of several
seconds), makes it ill-suited to analysing on-line
linguistic processes. EEG on the other hand, de-
spite its low spatial resolution (several centime-
tres), gives millisecond-level temporal resolution,
625
model coverage all within cat between cat
yahoo-mitchell 100 58.3** (5.7) 53.6* (3.7) 63.0** (8.9)
repubblica-window-svd 96.7 55.7* (5.6) 54.3 (6.5) 56.9* (5.9)
repubblica-window-topfeat 93.3 52.1 (4.3) 48.7 (3.6) 55.4 (7.0)
repubblica-deppath-svd 93.3 51.4 (8.7) 49.0 (8.0) 54.0 (10.0)
repubblica-depfilter-topfeat 85.0 51.1 (9.6) 49.3 (9.6) 53.1 (10.0)
repubblica-position-topfeat 93.3 50.0 (5.2) 46.0 (4.7) 53.6 (8.0)
repubblica-deppath-topfeat 86.7 49.9 (9.0) 47.0 (9.3) 52.4 (9.6)
repubblica-position-svd 96.7 49.4 (10.2) 46.6 (9.8) 52.3 (11.3)
repubblica-depfilter-svd 93.3 48.9 (11.1) 47.1 (8.9) 50.6 (12.9)
Table 3: Comparison across corpus models, with percentage concept coverage, mean cross-subject per-
centage prediction accuracy and standard deviation; ?p < 0.05, ? ? p < 0.01
enabling the separate analysis of sequential cogni-
tive processes and states (e.g., auditory process-
ing, word comprehension, semantic representa-
tion). fMRI is also prohibitively expensive for
most researchers (ca. 300 euros per hour at cost
price), compared to EEG (ca. 30 euros per hour).
Finally, there is no prospect of fMRI being minia-
turised, while wearable EEG systems are already
becoming commercially available, making exper-
imentation in more ecological settings a possibil-
ity (e.g., playing with a child, meeting at a desk,
walking around). In short, while EEG can be used
to carry out systematic investigations of categori-
cal distinctions, doing so with fMRI would be pro-
hibitively expensive.
Present results indicate that distinctions be-
tween categories are easier than distinctions be-
tween category elements; and that selecting the
conceptual features by hand gives better results
than discovering them automatically. Both of
these results however may be due to limitations
of the current method. One limitation is that we
have been using the same set of features for all
concepts, which is likely to blur the distinctions
between members of a category more than those
between categories. A second limitation of our
present methodology is that it is constrained to use
very small numbers of semantic features, which
limits its applicability. For example it is hard to
conceive of a small set of verbs, or other parts-of-
speech, whose co-occurrence patterns could suc-
cessfully characterise the full range of meaning
found in the human lexicon. Even the more eco-
nomical corpus-extracted conceptual models tend
to run in the hundreds of features (Almuhareb,
2006). We are currently working on variations in
the method that will address these shortcomings.
The web-based model with manually picked
features outperformed all la Repubblica-based
models. However, the results attained with
repubblica-window-svd are encouraging, espe-
cially considering that we are reporting results for
an EEG feature configuration optimised for the
web data (see footnote 5), and that la Repubblica
is several orders of magnitude smaller than the
web. That data sparseness might be the main is-
sue with la Repubblica models is suggested by
the fact that repubblica-window-svd is the least
sparse of them, since it does not filter data by posi-
tion or dependency path, and compresses informa-
tion from many verbs via SVD. In future research,
we plan to extract richer models from larger cor-
pora. And as the discriminative accuracy of cross-
training techniques improves, further insights into
the relative validity of corpus representations will
be attainable. One research aim is to see if individ-
ual corpus semantic properties are encoded neu-
rally, so providing strong evidence for a particular
model. These techniques may also prove more ob-
jective and reliable in evaluating representations of
abstract concepts, for which it is more difficult to
collect reliable judgements from informants.
References
A. Almuhareb. 2006. Attributes in lexical acquisition.
Dissertation, University of Essex.
L. Barsalou. 1999. Perceptual symbol systems. Be-
havioural and Brain Sciences, 22:577?660.
L. Barsalou. 2003. Situated simulation in the human
conceptual system. Language and Cognitive Pro-
cesses, 18:513?562.
J.R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Proceedings of
SIGLEX, pages 59?66.
A. Delorme and S. Makeig. 2003. Eeglab: an open
source toolbox for analysis of single-trial dynamics
includingindependent component analysis. Journal
of Neuroscience Methods, 134:9?21.
626
S. Evert. 2005. The statistics of word cooccurrences.
Dissertation, Stuttgart University.
Ch. J. Fillmore. 1982. Frame semantics. In Linguis-
tic Society of Korea, editor, Linguistics in the Morn-
ing Calm, pages 111?138. Hanshin, Seoul.
J. Gilbert, L. Shapiro, and G. Barnes. 2009. Processing
of living and nonliving objects diverges in the visual
processing system: evidence from meg. In Proceed-
ings of the Cognitive Neuroscience Society Annual
Meeting.
M. Kiefer. 2001. Perceptual and seman-
tic sources of category-specific effects in object
categorization:event-related potentials during pic-
ture and word categorization. Memory and Cogni-
tion, 29(1):100?116.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
A. Lenci. 2009. Argument alternations in italian verbs:
a computational study. In Atti del XLII Congresso
Internazionale di Studi della Societ`a di Linguistica
Italiana.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL98, Montreal,
Canada.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
S. Makeig, A.J. Bell, T. Jung, and T.J. Sejnowski.
1996. Independent component analysis of elec-
troencephalographic data. In in Advances in Neu-
ral Information Processing Systems, pages 145?151.
MIT Press.
A. Martin and L. Chao. 2001. Semantic memory and
the brain: structure and processes. Current Opinions
in Neurobiology, 11:194?201.
T. Mitchell, S. Shinkareva, A. Carlson, K. Chang,
V. Malave, R. Mason, and M. Just. 2008. Predicting
human brain activity associated with the meanings
of nouns. Science, 320:1191?1195.
B. Murphy, M. Dalponte, M. Poesio, and L. Bruz-
zone. 2008. Distinguishing concept categories from
single-trial electrophysiological activity. In Pro-
ceedings of the Annual Meeting of the Cognitive Sci-
ence Society.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
G. Pfurtscheller and F. Lopes da Silva. 1999. Event-
related EEG/MEG synchronization and desynchro-
nization: Basic principles. Clinical Neurophysiol-
ogy, 110:1842?1857.
F. Pulverm?uller. 2002. The neuroscience of language:
on brain circuits of words and serial order. Cam-
bridge University Press, Cambridge.
F. Pulverm?uller. 2005. Brain mechanisms linking lan-
guage and action. Nature Reviews Neuroscience,
6:576?582.
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
M. Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stockholm
University.
S. Schulte im Walde. 2008. Theoretical adequacy, hu-
man data and classification approaches in modelling
word properties, word relatedness and word classes.
Habilitation, Saarland University.
H. Sch?utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford.
627
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 904?911,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Words and Echoes: Assessing and Mitigating
the Non-Randomness Problem in Word Frequency Distribution Modeling
Marco Baroni
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
marco.baroni@unitn.it
Stefan Evert
IKW (University of Osnabru?ck)
Albrechtstr. 28
49069 Osnabru?ck, Germany
stefan.evert@uos.de
Abstract
Frequency distribution models tuned to
words and other linguistic events can pre-
dict the number of distinct types and their
frequency distribution in samples of arbi-
trary sizes. We conduct, for the first time,
a rigorous evaluation of these models based
on cross-validation and separation of train-
ing and test data. Our experiments reveal
that the prediction accuracy of the models
is marred by serious overfitting problems,
due to violations of the random sampling as-
sumption in corpus data. We then propose
a simple pre-processing method to allevi-
ate such non-randomness problems. Further
evaluation confirms the effectiveness of the
method, which compares favourably to more
complex correction techniques.
1 Introduction
Large-Number-of-Rare-Events (LNRE) models
(Baayen, 2001) are a class of specialized statistical
models that allow us to estimate the characteristics
of the distribution of type probabilities in type-rich
linguistic populations (such as words) from limited
samples (our corpora). They also allow us to
extrapolate quantities such as vocabulary size (the
number of distinct types) and the number of hapaxes
(types occurring just once) beyond a given corpus or
make predictions for completely unseen data from
the same underlying population.
LNREmodels have applications in theoretical lin-
guistics, e.g. for comparing the type richness of mor-
phological or syntactic processes that are attested to
different degrees in the data (Baayen, 1992). Con-
sider for example a very common prefix such as re-
and a rather rare prefix such as meta-. With LNRE
models we can answer questions such as: If we
could obtain as many tokens of meta- as we have
of re-, would we also see as many distinct types?
In other words, is the prefix meta- as productive as
the prefix re-? Practical NLP applications, on the
other hand, include estimating how many out-of-
vocabulary words we will encounter given a lexicon
of a certain size, or making informed guesses about
type counts in very large data sets (e.g., how many
typos are there on the Internet?)
In this paper, after introducing LNRE models
(Section 2), we present an evaluation of their per-
formance based on separate training and test data
as well as cross-validation (Section 3). As far as
we know, this is the first time that such a rigorous
evaluation has been conducted. The results show
how evaluating on the training set, a common strat-
egy in LNRE research, favours models that overfit
the training data and perform poorly on unseen data.
They also confirm the observation by Evert and Ba-
roni (2006) that current LNRE models achieve only
unsatisfactory prediction accuracy, and this is the is-
sue we turn to in the second part of the paper (Sec-
tion 4). Having identified the violation of the ran-
dom sampling assumption by real-world data as one
of the main factors affecting the quality of the mod-
els, we present a new approach to alleviating non-
randomness problems. Further evaluation shows our
solution to outperform Baayen?s (2001) partition-
adjustment method, the former state-of-the-art in
non-randomness correction. Section 5 concludes by
904
pointing out directions for future work.
2 LNRE models
Baayen (2001) introduces a family of models for
Zipf-like frequency distributions of linguistic pop-
ulations, referred to as LNRE models. Such a lin-
guistic population is formally described by a finite
or countably infinite set of types ?i and their occur-
rence probabilities pii. Word frequency models are
not concerned with the probabilities (i.e., relative
frequencies) of specific individual types, but rather
the overall distribution of these probabilities.
Numbering the types in order of decreasing prob-
ability (pi1 ? pi2 ? pi3 ? . . ., called a popula-
tion Zipf ranking), we can specify a LNRE model
for their distribution as a function that computes pii
from the Zipf rank i of ?i. For instance, the Zipf-
Mandelbrot law1 is defined by the equation
pii =
C
(i + b)a
(1)
with parameters a > 1 and b > 0. It is mathemati-
cally more convenient to formulate LNRE models in
terms of a type density function g(pi) on the interval
pi ? [0, 1], such that
? B
A
g(pi) dpi (2)
is the (approximate) number of types ?i with A ?
pii ? B. Evert (2004) shows that Zipf-Mandelbrot
corresponds to a type density of the form
g(pi) :=
{
C ? pi???1 A ? pi ? B
0 otherwise
(3)
with parameters 0 < ? < 1 and 0 ? A < B.2
Models that are formulated in terms of such a type
density g have many direct applications (e.g. using g
as a Bayesian prior), and we refer to them as proper
LNRE models.
Assuming that a corpus of N tokens is a random
sample from such a population, we can make pre-
dictions about lexical statistics such as the number
1The Zipf-Mandelbrot law is an extension of Zipf?s law
(which has a = 1 and b = 0). While the latter originally refers
to type frequencies in a given sample, the Zipf-Mandelbrot law
is formulated for type probabilities in a population.
2In this equation, C is a normalizing constant required in
order to ensure
R 1
0 pig(pi) dpi = 1, the equivalent of
P
i pii = 1.
V (N) of different types in the corpus (the vocab-
ulary size), the number V1(N) of hapax legomena
(types occurring just once), as well as the further dis-
tribution of type frequencies Vm(N). Since the pre-
cise values would be different from sample to sam-
ple, the model predictions are given by expectations
E[V (N)] and E[Vm(N)], which can be computed
with relative ease from the type density function g.
By comparing expected and observed values of V
and Vm (for the lowest frequency ranks, usually up
to m = 15), the parameters of a LNRE model can
be estimated (we refer to this as training the model),
allowing inferences about the population (such as
the total number of types in the population) as well
as further applications of the estimated type density
(e.g. for Good-Turing smoothing). Since we can cal-
culate expected values for samples of arbitrary size
N , we can use the trained model to predict how
many new types would be seen in a larger corpus,
how many hapaxes there would be, etc. This kind of
vocabulary growth extrapolation has become one of
the most important applications of LNRE models in
linguistics and NLP.
A detailed account of the mathematics of LNRE
models can be found in Baayen (2001, Ch. 2).
Baayen describes two LNRE models, lognormal
and GIGP, as well as several other approaches (in-
cluding a version of Zipf?s law and the Yule-Simon
model) that are not based on a type density and
hence do not qualify as proper LNRE models. Two
LNRE models based on Zipf?s law, ZM and fZM, are
introduced by Evert (2004).
In the following, we will only consider proper
LNRE models because of their considerably greater
utility, and because their performance in extrapo-
lation tasks appears to be better than, or at least
comparable to, the other models (Evert and Baroni,
2006). In addition, we exclude the lognormal model
because of its computational complexity and numer-
ical instability.3 In initial evaluation experiments,
the performance of lognormal was also inferior to
the remaining three models (ZM, fZM and GIGP).
Note that ZM is the most simplistic model, with only
2 parameters and assuming an infinite population
vocabulary, while fZM and GIGP have 3 parameters
3There are no closed form equations for the expectations of
the lognormal model, which have to be calculated by numerical
integration.
905
and can model populations of different sizes.
3 Evaluation of LNRE models
LNRE models are traditionally evaluated by look-
ing at how well expected values generated by them
fit empirical counts extracted from the same data-
set used for parameter estimation, often by visual
inspection of differences between observed and pre-
dicted data in plots. More rigorously, Baayen (2001)
and Evert (2004) compare the frequency distribu-
tion observed in the training set to the one predicted
by the model with a multivariate chi-squared test.
As we will show below, evaluating standard LNRE
models on the same data that were used to estimate
their parameters favours overfitting, which results in
poor performance on unseen data.
Evert and Baroni (2006) attempt, for the first time,
to evaluate LNRE models on unseen data. However,
rather than splitting the data into separate training
and test sets, they evaluate the models in an extra-
polation setting, where the parameters of the model
are estimated on a subset of the data used for testing.
Evert and Baroni do not attempt to cross-validate the
results, and they do not provide a quantitative evalu-
ation, relying instead on visual inspection of empir-
ical and observed vocabulary growth curves.
3.1 Data and procedure
We ran our experiments with three corpora in differ-
ent languages and representing different textual ty-
pologies: the British National Corpus (BNC), a ?bal-
anced? corpus of British English of about 100 mil-
lion tokens illustrating different communicative set-
tings, genres and topics; the deWaC corpus, a Web-
crawled corpus of about 1.5 billion German words;
and the la Repubblica corpus, an Italian newspaper
corpus of about 380 million words.4
From each corpus, we extracted 20 non-
overlapping samples of randomly selected docu-
ments, amounting to a total of 4 million tokens each
(punctuation marks and entirely non-alphabetical to-
kens were removed before sampling, and all words
were converted to lowercase). Each of these sam-
ples was then split into a training set of 1 million to-
kens (the training size N0) and a test set of 3 million
4See www.natcorp.ox.ac.uk, http://wacky.
sslmit.unibo.it and http://sslmit.unibo.it/
repubblica
tokens. The documents in the la Repubblica sam-
ples were ordered chronologically before splitting,
to simulate a typical scenario arising when working
with newspaper data, where the data available for
training precede, chronologically, the data one wants
to generalize to.
We estimate parameters of the ZM, fZM and
GIGP models on each training set, using the zipfR
toolkit.5 The models are then used to predict the
expected number of distinct types, i.e., vocabulary
size V , at sample sizes of 1, 2 and 3 million tokens,
equivalent to 1, 2 and 3 times the size of the training
set (we refer to these as the prediction sizesN0, 2N0
and 3N0, respectively). Finally, the expected vo-
cabulary size E[V (N)] is compared to the observed
value V (N) in the test set for N = N0, N = 2N0
and N = 3N0. We also look at V1(N), the number
of hapax legomena, in the same way.
Our main focus is V prediction, since this is by
far the most useful measure in practical applica-
tions, where we are typically interested in knowing
how many types (or how many types belonging to
a certain category) we will see as our sample size
increases (How many typos are there on the Web?
How many types with prefix meta- would we see
if we had as many types of meta- as we have of
re-?) Hapax legomena counts, on the other hand,
play a central role in quantifying morphological pro-
ductivity (Baayen, 1992) and they give us a first in-
sight into how good the models are at predicting fre-
quency distributions, besides vocabulary size (as we
will see, a model?s success in predicting V does not
necessary imply that the model is also capturing the
right frequency distribution).
For all models, corpora and prediction sizes,
goodness-of-fit of the model on the training set
is measured with a multivariate chi-squared test
(Baayen, 2001, 118-122). Performance of the mod-
els in prediction of V is assessed via relative error,
computed for each of the 20 samples from a corpus
and the 3 prediction sizes as follows:
e =
E[V (N)]? V (N)
V (N)
where N = k ? N0 is the prediction size (for k =
1, 2, 3), V (N) is the observed V in the relevant test
5http://purl.org/stefan.evert/zipfR
906
set at size N , and E[V (N)] is the corresponding ex-
pected V predicted by a model.6
For each corpus and prediction size we obtain 20
values ei (viz., e1, . . . , e20). As a summary measure,
we report the square root of the mean square relative
error (rMSE) calculated according to
?
rMSE =
?
?
?
? 1
20
?
20?
i=1
(ei)2
This gives us an overall assessment of prediction ac-
curacy (we take the square root to obtain values on
the same scale as relative errors, and thus easier to
interpret). We complement rMSEs with reports on
the average relative error (indicating whether there
is a systematic under- or overestimation bias) and its
asymptotic 95% confidence intervals, based on the
empirical standard deviation of the ei across the 20
trials (the confidence intervals are usually somewhat
larger than the actual range of values found in the
experiments, so they should be seen as ?pessimistic
estimates? of the actual variance).
3.2 Results
The panels of Figure 1 report rMSE values for the
3 corpora and for each prediction size. For now,
we focus on the first 3 histograms of each panel,
that present rMSEs for the 3 LNRE models intro-
duced above: ZM, fZM and GIGP (the remaining
histograms will be discussed later).7
For all corpora and all extrapolation sizes beyond
N0, the simple ZM model outperforms the more so-
phisticated fZM and GIGP models (which seem to
be very similar to each other). Even at the largest
prediction size of 3N0, ZM?s rMSE is well below
10%, whereas the other models have, in the worst
case (BNC 3N0), a rMSE above 15%. Figure 2
presents plots of average relative error and its em-
pirical confidence intervals (again, focus for now on
the ZM, fZM and GIGP results; the rest of the figure
is discussed later). We see that the poor performance
6We normalize by V (N) rather than (a function of)
E[V (N)] because in the latter case we would favour models
that overestimate V , compared to ones that are equally ?close?
to the correct value but underestimate V .
7A table with the full numerical results is available upon
request; we find, however, that graphical summaries such as
those presented in this paper make the results easier to interpret.
of fZM and GIGP is due to their tendency to under-
estimate the true vocabulary size V , while variance
is comparable across models.
The rMSEs of V1 prediction are reported in Fig-
ure 3. V1 prediction performance is poorer across
the board, and ZM is no longer outperforming the
other models. For space reasons, we do not present
relative error and variance plots for V1, but the gen-
eral trends are the same observed for V , except that
the bias of ZM towards V1 overestimation is much
clearer than for V .
Interestingly, goodness-of-fit on the training data
is not a good predictor of V and V1 prediction per-
formance on unseen data. This is shown in Figure
4, which plots rMSE for prediction of V against
goodness-of-fit (quantified by multivariate X2 on
the training set, as discussed above) for all corpora
and LNREmodels at the 3N0 prediction size (but the
same patterns emerge at other prediction sizes and
with V1). The larger X2, the poorer the training set
fit; the larger rMSE, the worse the prediction. Thus,
ideally, we should see a positive correlation between
X2 and rMSE. Focusing for now on the circles (pin-
pointing the ZM, fZM and GIGP models), we see
that there is instead a negative correlation between
goodness of fit on the training set and quality of pre-
diction on unseen data.8
First, these results indicate that, if we take good-
ness of fit on the training set as a criterion for choos-
ing the best model (as done by Baayen and Evert),
we end up selecting the worst model for actual pre-
diction tasks. This is, we believe, a very strong
case for applying the split train-test cross-validation
method used in other areas of statistical NLP to fre-
quency distribution modeling. Second, the data sug-
gest that the more sophisticated models are overfit-
ting the training set, leading to poorer performance
than the simpler ZM on unseen data. We turn now to
what we think is the main cause for this overfitting.
4 Non-randomness and echoes
The results in the previous section indicate that the
V s predicted by LNRE models are at best ?ballpark
estimates? (and V1 predictions, with a relative error
that is often above 20%, do not even qualify as plau-
8With correlation coefficients of r < ?.8, significant at the
0.01 level despite the small sample size.
907
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V] vs. V on test set (BNC)
rMSE  
  
(%)
0
5
10
15
20
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V] vs. V on test set (DEWAC)
rMSE  
  
(%)
0
5
10
15
20
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V] vs. V on test set (REPUBBLICA)
rMSE  
  
(%)
0
5
10
15
20
Figure 1: rMSEs of predicted V on the BNC, deWaC and la Repubblica data-sets
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
Relative error: E[V] vs. V on test set (BNC)
relativ
e error
 (%)
?
40
?
20
0
20
40
l
l l
l l l
l N02N03N0
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
Relative error: E[V] vs. V on test set (DEWAC)
relativ
e error
 (%)
?
20
?
10
0
10
20
l
l l
l l
l
l N02N03N0
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
Relative error: E[V] vs. V on test set (REPUBBLICA)
relativ
e error
 (%)
?
20
?
10
0
10
20
l
l l
l l
l
l N02N03N0
Figure 2: Average relative errors and asymptotic 95% confidence intervals of V prediction on BNC, deWaC
and la Repubblica data-sets
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V1] vs. V1 on test set (BNC)
rMSE  
  
(%)
0
10
20
30
40
50
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V1] vs. V1 on test set (DEWAC)
rMSE  
  
(%)
0
10
20
30
40
50
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V1] vs. V1 on test set (REPUBBLICA)
rMSE  
  
(%)
0
10
20
30
40
50
Figure 3: rMSEs of predicted V1 on the BNC, deWaC and la Repubblica data-sets
908
0 5000 10000 15000
0
5
10
15
Accuracy for V on test set (3N0)
X2
rMSE
  
  
(%)
l
ll
l
ll
l
ll
l standard
echomodelpartition?adjusted
Figure 4: Correlation between X2 and V prediction
rMSE across corpora and models
sible ballpark estimates). Although such rough esti-
mates might be more than adequate for many practi-
cal applications, is it possible to further improve the
quality of LNRE predictions?
A major factor hampering prediction quality is
that real texts massively violate the randomness as-
sumption made in LNRE modeling: words, rather
obviously, are not picked at random on the basis
of their population probability (Evert and Baroni,
2006; Baayen, 2001). The topic-driven ?clumpi-
ness? of low frequency content words reduces the
number of hapax legomena and other rare events
used to estimate the parameters of LNRE models,
leading the models to underestimate the type rich-
ness of the population. Interestingly (but unsurpris-
ingly), ZM with its assumption of an infinite pop-
ulation, is less prone to this effect, and thus it has
a better prediction performance than the more so-
phisticated fZM and GIGP models, despite its poor
goodness-of-fit.
The effect of non-randomness is illustrated very
clearly for the BNC (but the same could be shown
for the other corpora) by Figure 5, a comparison
of rMSE for prediction of V from our experiments
above to results obtained on versions of the BNC
samples with words scrambled in random order,
thus forcibly removing non-randomness effects. We
see from this figure that the performance of both
fZM and GIGP improves dramatically when they
are trained and tested on randomized sequences of
words. Interestingly, randomization has instead a
negative effect on ZM performance.
ZM fZM GIGP ZMrandom fZMrandom GIGPrandom
N02N03N0
rMSE for E[V] vs. V on test set (BNC)
rMSE
  
  
(%)
0
5
10
15
20
Figure 5: rMSEs of predicted V on unmodified
vs. randomized versions of the BNC sets
4.1 Previous approaches to non-randomness
While non-randomness is widely acknowledged as
a serious problem for the statistical analysis of cor-
pus data, very few authors have suggested correc-
tion strategies. The key problem of non-random data
seems to be that the occurrence frequencies of a type
in different documents do not follow the binomial
distribution assumed by random sampling models.
One approach is therefore to model this distribu-
tion explicitly, replacing the binomial with its sin-
gle parameter pi by a more complex distribution that
has additional parameters (Church and Gale, 1995;
Katz, 1996). However, these distributions are cur-
rently not applicable to LNRE modeling, which is
based on the overall frequencies of types in a cor-
pus rather than their frequencies in individual doc-
uments. The overall frequencies can only be calcu-
lated by summation over all documents in the cor-
pus, resulting in a mathematically and numerically
intractable model. In addition, the type density g(pi)
would have to be extended to a multi-dimensional
function, requiring a large number of parameters to
be estimated from the data.
Baayen (2001) suggests a different approach,
which partitions the population into ?normal? types
that satisfy the random sampling assumption, and
?totally underdispersed? types, which are assumed
to concentrate all occurrences in the corpus into a
909
single ?burst?. Using a standard LNRE model for
the normal part of the population and a simple lin-
ear growth model for the underdispersed part, ad-
justed values for E[V ] and E[Vm] can easily be cal-
culated. These so-called partition-adjusted models
(which introduce one additional parameter) are thus
the only viable models for non-randomness correc-
tion in LNRE modeling and have to be considered
the state of the art.
4.2 Echo adjustment
Rather than making more complex assumptions
about the population distribution or the sampling
model, we propose that non-randomness should be
tackled as a pre-processing problem. The issue, we
argue, is really with the way we count occurrences
of types. The fact that a rare topic-specific word oc-
curs, say, four times in a single document does not
make it any less a hapax legomenon for our purposes
than if the word occurred once (this is the case, for
example, of the word chondritic in the BNC, which
occurs 4 times, all in the same scientific document).
We operationalize our intuition by proposing that,
for our purposes, each content word (at least each
rare, topic-specific content word) occurs maximally
once in a document, and all other instances of that
word in the document are really instances of a spe-
cial ?anaphoric? type, whose function is that of
?echoing? the content words in the document. Thus,
in the BNC document mentioned above, the word
chondritic is counted only once, whereas the other
three occurrences are considered as tokens of the
echo type. Thus, we are counting what in the in-
formation retrieval literature is known as document
frequencies. Intuitively, these are less susceptible to
topical clumpiness effects than plain token frequen-
cies. However, by replacing repeated words with
echo tokens, we can stick to a sampling model based
on random word token sampling (rather than docu-
ment sampling), so that the LNRE models can be
applied ?as is? to echo-adjusted corpora.
Echo-adjustment does not affect the sample size
N nor the vocabulary size V , making the interpre-
tation of results obtained with echo-adjusted mod-
els entirely straightforward. N does not change be-
cause repeated types are replaced with echo tokens,
not deleted. V does not change because only re-
peated types are replaced. Thus, no type present in
the original corpus disappears (more precisely, V in-
creases by 1 because of the addition of the echo type,
but given the large size of V this can be ignored for
all practical purposes). Thus, the expected V com-
puted for a specified sample size N with a model
trained on an echo-adjusted corpus can be directly
compared to observed values at N , and to predic-
tions made for the same N by models trained on an
unprocessed corpus. The same is not true for the pre-
diction of the frequency distribution, where, for the
same N , echo-based models predict the distribution
of document frequencies.
We are proposing echoes as a model for the us-
age of (rare) content words. It would be diffi-
cult to decide where the boundary is between top-
ical words that are inserted once in a discourse
and then anaphorically modulated and ?general-
purpose? words that constitute the frame of the dis-
course and can occur multiple times. Luckily, we
do not have to make this decision when estimating
a LNRE model, since model fitting is based on the
distribution of the lowest frequencies. For example,
with the default zipfR model fitting setting, only the
lowest 15 spectrum elements are used to fit the mod-
els. For any reasonably sized corpus, it is unlikely
that function words and common content words will
occur in less than 16 documents, and thus their dis-
tribution will be irrelevant for model fitting. Thus,
we can ignore the issue of what is the boundary be-
tween topical words to be echo-adjusted and general
words, as long as we can be confident that the set
of lowest frequency words used for model fitting be-
long to the topical set.9 This makes practical echo-
adjustment extremely simple, since all we have to
do is to replace all repetitions of a word in the same
document with echo tokens, and estimate the param-
eters of a plain LNRE model with the resulting ver-
sion of the training corpus.
4.3 Experiments with echo adjustment
Using the same training and test sets as in Sec-
tion 3.1, we train the partition-adjusted GIGP model
9The issue becomes more delicate if we want to predict
the frequency spectrum rather than V , since a model trained
on echo-adjusted data will predict echo-adjusted frequencies
across the board. However, in many theoretical and practical
settings only the lowest frequency spectrum elements are of in-
terest, where, again, it is safe to assume that words are highly
topic-dependent, and echo-adjustment is appropriate.
910
implemented in the LEXSTATS toolkit (Baayen,
2001). We estimate the parameters of echo-adjusted
ZM, fZM and GIGP models on versions of the train-
ing corpora that have been pre-processed as de-
scribed above. The performance of the models is
evaluated with the same measures as in Section 3.1
(for prediction of V1, echo-adjusted versions of the
test data are used).
Figure 1 reports the performance of the echo-
adjusted fZM and GIGP models and of partition-
adjusted GIGP (echo-adjusted ZM performed sys-
tematically much worse than the other echo-adjusted
models and typically worse than uncorrected ZM,
and it is not reported in the figure). Both correction
methods lead to a dramatic improvement, bringing
the prediction performance of fZM and GIGP to lev-
els comparable to ZM (with the latter outperforming
the corrected models on the BNC, but being outper-
formed on la Repubblica). Moreover, echo-adjusted
GIGP is as good as partitioned GIGP on la Repub-
blica, and better on both BNC and deWaC, suggest-
ing that the much simpler echo-adjustment method
is at least as good and probably better than Baayen?s
partitioning. The mean error and confidence interval
plots in Figure 2 show that the echo-adjusted models
have a much weaker underestimation bias than the
corresponding unadjusted models, and are compara-
ble to, if not better than, ZM (although they might
have a tendency to display more variance, as clearly
illustrated by the performance of echo-adjusted fZM
on la Repubblica at 3N0 prediction size). Finally,
the echo-adjusted models clearly stand out with re-
spect to ZM when it comes to V1 prediction (Fig-
ure 3), indicating that echo-adjusted versions of the
more sophisticated fZM and GIGP models should
be the focus of future work on improving predic-
tion of the full frequency distribution, rather than
plain ZM. Moreover, echo-adjusted GIGP is outper-
forming partitioned GIGP, and emerging as the best
model overall.10 Reassuringly, for the echoed mod-
els there is a very strong positive correlation between
goodness-of-fit on the training set and quality of pre-
diction, as illustrated for V prediction at 3N0 by
the triangles in Figure 4 (again, the patterns in this
10In looking at the V1 data, it must be kept in mind, how-
ever, that V1 has a different interpretation when predicted by
echo-adjusted models, i.e., it is the number of document-based
hapaxes, the number of types that occur in one document only.
figure represent the general trend for echo-adjusted
models found in all settings).11 This indicates that
the over-fitting problem has been resolved, and for
echo-adjusted models goodness-of-fit on the train-
ing set is a reliable indicator of prediction accuracy.
5 Conclusion
Despite the encouraging results we reported, much
work, of course, remains to be done. Even with
the echo-adjusted models, prediction of V1 suffers
from large errors and prediction of V quickly deteri-
orates with increasing prediction sizeN . If the mod-
els? estimates for 3 times the size of the training set
have acceptable errors of around 5%, for many ap-
plications we might want to extrapolate to 100N0 or
more (recall the example of estimating type counts
for the entire Web). Moreover, echo-adjusted mod-
els make predictions pertaining to the distribution of
document frequencies, rather than plain token fre-
quencies. The full implications of this remain to
be investigated. Finally, future work should system-
atically explore to what extent different textual ty-
pologies are affected by the non-randomness prob-
lem (notice, e.g., that non-randomness seems to be a
greater problem for the BNC than for the more uni-
form la Repubblica corpus).
References
Baayen, Harald. 1992. Quantitative aspects of morpho-
logical productivity. Yearbook of Morphology 1991,
109-150.
Baayen, Harald. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Church, KennethW. andWilliam A. Gale. 1995. Poisson
mixtures. Journal of Natural Language Engineering
1, 163-190.
Evert, Stefan. 2004. A simple LNRE model for random
character sequences. Proceedings of JADT 2004, 411-
422.
Evert, Stefan and Marco Baroni. 2006. Testing the ex-
trapolation quality of word frequency models. Pro-
ceedings of Corpus Linguistics 2005.
Katz, Slava M. 1996. Distribution of content words and
phrases in text and language modeling. Natural Lan-
guage Engineering, 2(2) 15-59.
11With significant correlation coefficients of r = .76 for 2N0
(p < 0.05) and r = .94 for 3N0 (p 0.01).
911
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 29?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
zipfR: Word Frequency Distributions in R
Stefan Evert
IKW (University of Osnabru?ck)
Albrechtstr. 28
49069 Osnabru?ck, Germany
stefan.evert@uos.de
Marco Baroni
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
marco.baroni@unitn.it
Abstract
We introduce the zipfR package, a power-
ful and user-friendly open-source tool for
LNRE modeling of word frequency distribu-
tions in the R statistical environment. We
give some background on LNRE models,
discuss related software and the motivation
for the toolkit, describe the implementation,
and conclude with a complete sample ses-
sion showing a typical LNRE analysis.
1 Introduction
As has been known at least since the seminal work
of Zipf (1949), words and other type-rich linguis-
tic populations are characterized by the fact that
even the largest samples (corpora) do not contain in-
stances of all types in the population. Consequently,
the number and distribution of types in the avail-
able sample are not reliable estimators of the number
and distribution of types in the population. Large-
Number-of-Rare-Events (LNRE) models (Baayen,
2001) are a class of specialized statistical models
that estimate the distribution of occurrence proba-
bilities in such type-rich linguistic populations from
our limited samples.
LNRE models have applications in many
branches of linguistics and NLP. A typical use
case is to predict the number of different types (the
vocabulary size) in a larger sample or the whole
population, based on the smaller sample available to
the researcher. For example, one could use LNRE
models to infer how many words a 5-year-old child
knows in total, given a sample of her writing. LNRE
models can also be used to quantify the relative
productivity of two morphological processes (as
illustrated below) or of two rival syntactic construc-
tions by looking at their vocabulary growth rate as
sample size increases. Practical NLP applications
include making informed guesses about type counts
in very large data sets (e.g., How many typos are
there on the Internet?) and determining the ?lexical
richness? of texts belonging to different genres. Last
but not least, LNRE models play an important role
as a population model for Bayesian inference and
Good-Turing frequency smoothing (Good, 1953).
However, with a few notable exceptions (such as
the work by Baayen on morphological productivity),
LNRE models are rarely if ever employed in linguis-
tic research and NLP applications. We believe that
this has to be attributed, at least in part, to the lack of
easy-to-use but sophisticated LNRE modeling tools
that are reliable and robust, scale up to large data
sets, and can easily be integrated into the workflow
of an experiment or application. We have developed
the zipfR toolkit in order to remedy this situation.
2 LNRE models
In the field of LNRE modeling, we are not interested
in the frequencies or probabilities of individual word
types (or types of other linguistic units), but rather
in the distribution of such frequencies (in a sam-
ple) and probabilities (in the population). Conse-
quently, the most important observations (in mathe-
matical terminology, the statistics of interest) are the
total number V (N) of different types in a sample of
N tokens (also called the vocabulary size) and the
number Vm(N) of types that occur exactly m times
29
in the sample. The set of values Vm(N) for all fre-
quency ranks m = 1, 2, 3, . . . is called a frequency
spectrum and constitutes a sufficient statistic for the
purpose of LNRE modeling.
A LNRE model M is a population model that
specifies a certain distribution for the type proba-
bilities in the population. This distribution can be
linked to the observable values V (N) and Vm(N)
by the standard assumption that the observed data
are a random sample of size N from this popula-
tion. It is most convenient mathematically to formu-
late a LNRE model in terms of a type density func-
tion g(pi), defined over the range of possible type
probabilities 0 < pi < 1, such that
? b
a g(pi) dpi is
the number of types with occurrence probabilities
in the range a ? pi ? b.1 From the type density
function, expected values E
[
V (N)
]
and E
[
Vm(N)
]
can be calculated with relative ease (Baayen, 2001),
especially for the most widely-used LNRE models,
which are based on Zipf?s law and stipulate a power
law function for g(pi). These models are known as
GIGP (Sichel, 1975), ZM and fZM (Evert, 2004).
For example, the type density of the ZM and fZM
models is given by
g(pi) :=
{
C ? pi???1 A ? pi ? B
0 otherwise
with parameters 0 < ? < 1 and 0 ? A < B.
Baayen (2001) also presents approximate equations
for the variances Var
[
V (N)
]
and Var
[
Vm(N)
]
. In
addition to such predictions for random samples, the
type density g(pi) can also be used as a Bayesian
prior, where it is especially useful for probability es-
timation from low-frequency data.
Baayen (2001) suggests a number of models that
calculate the expected frequency spectrum directly
without an underlying population model. While
these models can sometimes be fitted very well to
an observed frequency spectrum, they do not inter-
pret the corpus data as a random sample from a pop-
ulation and hence do not allow for generalizations.
They also cannot be used as a prior distribution for
Bayesian inference. For these reasons, we do not see
1Since type probabilities are necessarily discrete, such a
type density function can only give an approximation to the true
distribution. However, the approximation is usually excellent
for the low-probability types that are the center of interest for
most applications of LNRE models.
them as proper LNRE models and do not consider
them useful for practical application.
3 Requirements and related software
As pointed out in the previous section, most appli-
cations of LNRE models rely on equations for the
expected values and variances of V (N) and Vm(N)
in a sample of arbitrary size N . The required ba-
sic operations are: (i) parameter estimation, where
the parameters of a LNRE model M are determined
from a training sample of size N0 by comparing
the expected frequency spectrum E
[
Vm(N0)
]
with
the observed spectrum Vm(N0); (ii) goodness-of-fit
evaluation based on the covariance matrix of V and
Vm; (iii) interpolation and extrapolation of vocabu-
lary growth, using the expectations E
[
V (N)
]
; and
(iv) prediction of the expected frequency spectrum
for arbitrary sample size N . In addition, Bayesian
inference requires access to the type density g(pi)
and distribution function G(a) =
? 1
a g(pi) dpi, while
random sampling from the population described by
a LNRE model M is a prerequisite for Monte Carlo
methods and simulation experiments.
Up to now, the only publicly available implemen-
tation of LNRE models has been the lexstats toolkit
of Baayen (2001), which offers a wide range of
models including advanced partition-adjusted ver-
sions and mixture models. While the toolkit sup-
ports the basic operations (i)?(iv) above, it does
not give access to distribution functions or random
samples (from the model distribution). It has not
found widespread use among (computational) lin-
guists, which we attribute to a number of limitations
of the software: lexstats is a collection of command-
line programs that can only be mastered with expert
knowledge; an ad-hoc Tk-based graphical user in-
terfaces simplifies basic operations, but is fully sup-
ported on the Linux platform only; the GUI also has
only minimal functionality for visualization and data
analysis; it has restrictive input options (making its
use with languages other than English very cumber-
some) and works reliably only for rather small data
sets, well below the sizes now routinely encountered
in linguistic research (cf. the problems reported in
Evert and Baroni 2006); the standard parameter es-
timation methods are not very robust without exten-
sive manual intervention, so lexstats cannot be used
30
as an off-the-shelf solution; and nearly all programs
in the suite require interactive input, making it diffi-
cult to automate LNRE analyses.
4 Implementation
First and foremost, zipfR was conceived and de-
veloped to overcome the limitations of the lexstats
toolkit. We implemented zipfR as an add-on library
for the popular statistical computing environment R
(R Development Core Team, 2003). It can easily
be installed (from the CRAN archive) and used off-
the-shelf for standard LNRE modeling applications.
It fully supports the basic operations (i)?(iv), cal-
culation of distribution functions and random sam-
pling, as discussed in the previous section. We have
taken great care to offer robust parameter estimation,
while allowing advanced users full control over the
estimation procedure by selecting from a wide range
of optimization techniques and cost functions. In
addition, a broad range of data manipulation tech-
niques for word frequency data are provided. The
integration of zipfR within the R environment makes
the full power of R available for visualization and
further statistical analyses.
For the reasons outlined above, our software
package only implements proper LNRE models.
Currently, the GIGP, ZM and fZM models are sup-
ported. We decided not to implement another LNRE
model available in lexstats, the lognormal model, be-
cause of its numerical instability and poor perfor-
mance in previous evaluation studies (Evert and Ba-
roni, 2006).
More information about zipfR can be found on its
homepage at http://purl.org/stefan.evert/zipfR/.
5 A sample session
In this section, we use a typical application example
to give a brief overview of the basic functionality of
the zipfR toolkit. zipfR accepts a variety of input for-
mats, the most common ones being type frequency
lists (which, in the simplest case, can be newline-
delimited lists of frequency values) and tokenized
(sub-)corpora (one word per line). Thus, as long as
users can extract frequency data or at least tokenize
the corpus of interest with other tools, they can per-
form all further analysis with zipfR.
Suppose that we want to compare the relative pro-
ductivity of the Italian prefix ri- with that of the
rarer prefix ultra- (roughly equivalent to English re-
and ultra-, respectively), and that we have frequency
lists of the word types containing the two prefixes.2
In our R session, we import the data, create fre-
quency spectra for the two classes, and we plot the
spectra to look at their frequency distribution (the
output graph is shown in the left panel of Figure 1):
ItaRi.tfl <- read.tfl("ri.txt")
ItaUltra.tfl <- read.tfl("ultra.txt")
ItaRi.spc <- tfl2spc(ItaRi.tfl)
ItaUltra.spc <- tfl2spc(ItaUltra.tfl)
> plot(ItaRi.spc,ItaUltra.spc,
+ legend=c("ri-","ultra-"))
We can then look at summary information about
the distributions:
> summary(ItaRi.spc)
zipfR object for frequency spectrum
Sample size: N = 1399898
Vocabulary size: V = 1098
Class sizes: Vm = 346 105 74 43 ...
> summary(ItaUltra.spc)
zipfR object for frequency spectrum
Sample size: N = 3467
Vocabulary size: V = 523
Class sizes: Vm = 333 68 37 15 ...
We see that the ultra- sample is much smaller than
the ri- sample, making a direct comparison of their
vocabulary sizes problematic. Thus, we will use the
fZM model (Evert, 2004) to estimate the parameters
of the ultra- population (notice that the summary of
an estimated model includes the parameters of the
relevant distribution as well as goodness-of-fit infor-
mation):
> ItaUltra.fzm <- lnre("fzm",ItaUltra.spc)
> summary(ItaUltra.fzm)
finite Zipf-Mandelbrot LNRE model.
Parameters:
Shape: alpha = 0.6625218
Lower cutoff: A = 1.152626e-06
Upper cutoff: B = 0.1368204
[ Normalization: C = 0.673407 ]
Population size: S = 8732.724
...
Goodness-of-fit (multivariate chi-squared):
X2 df p
19.66858 5 0.001441900
Now, we can use the model to predict the fre-
quency distribution of ultra- types at arbitrary sam-
ple sizes, including the size of our ri- sample. This
allows us to compare the productivity of the two pre-
fixes by using Baayen?sP , obtained by dividing the
2The data used for illustration are taken from an Italian
newspaper corpus and are distributed with the toolkit.
31
ri?ultra?
Frequency Spectrum
m
V m
0
50
100
150
200
250
300
350
0 200000 600000 1000000
0
200
0
400
0
600
0
800
0
Vocabulary Growth
N
E[[V((
N))]]
ri?ultra?
Figure 1: Left: Comparison of the observed ri- and ultra- frequency spectra. Right: Interpolated ri- vs. ex-
trapolated ultra- vocabulary growth curves.
number of hapax legomena by the overall sample
size (Baayen, 1992):
> ItaUltra.ext.spc<-lnre.spc(ItaUltra.fzm,
+ N(ItaRi.spc))
> Vm(ItaUltra.ext.spc,1)/N(ItaRi.spc)
[1] 0.0006349639
> Vm(ItaRi.spc,1)/N(ItaRi.spc)
[1] 0.0002471609
The rarer ultra- prefix appears to be more produc-
tive than the more frequent ri-. This is confirmed by
a visual comparison of vocabulary growth curves,
that report changes in vocabulary size as sample size
increases. For ri-, we generate the growth curve
by binomial interpolation from the observed spec-
trum, whereas for ultra- we extrapolate using the
estimated LNRE model (Baayen 2001 discuss both
techniques).
> sample.sizes <- floor(N(ItaRi.spc)/100)
+ *(1:100)
> ItaRi.vgc <- vgc.interp(ItaRi.spc,
+ sample.sizes)
> ItaUltra.vgc <- lnre.vgc(ItaUltra.fzm,
+ sample.sizes)
> plot(ItaRi.vgc,ItaUltra.vgc,
+ legend=c("ri-","ultra-"))
The plot (right panel of Figure 1) confirms the
higher (potential) type richness of ultra-, a ?fancier?
prefix that is rarely used, but, when it does get used,
is employed very productively (see discussion of
similar prefixes in Gaeta and Ricca 2003).
References
Baayen, Harald. 1992. Quantitative aspects of morpho-
logical productivity. Yearbook of Morphology 1991,
109?150.
Baayen, Harald. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Evert, Stefan. 2004. A simple LNRE model for random
character sequences. Proceedings of JADT 2004, 411?
422.
Evert, Stefan and Marco Baroni. 2006. Testing the ex-
trapolation quality of word frequency models. Pro-
ceedings of Corpus Linguistics 2005.
Gaeta, Livio and Davide Ricca. 2003. Italian prefixes
and productivity: a quantitative approach. Acta Lin-
guistica Hungarica, 50 89?108.
Good, I. J. (1953). The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3/4), 237?264.
R Development Core Team (2003). R: A lan-
guage and environment for statistical computing. R
Foundation for Statistical Computing, Vienna, Aus-
tria. ISBN 3-900051-00-3. See also http://www.
r-project.org/.
Sichel, H. S. (1975). On a distribution law for word fre-
quencies. Journal of the American Statistical Associ-
ation, 70, 542?547.
Zipf, George K. 1949. Human behavior and the princi-
ple of least effort. Cambridge (MA): Addison-Wesley.
32
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 49?56,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
ISA meets Lara:
An incremental word space model
for cognitively plausible simulations of semantic learning
Marco Baroni
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
marco.baroni@unitn.it
Alessandro Lenci
Department of Linguistics
University of Pisa
Via Santa Maria 36
56126 Pisa, Italy
alessandro.lenci@ilc.cnr.it
Luca Onnis
Department of Psychology
Cornell University
Ithaca, NY 14853
lo35@cornell.edu
Abstract
We introduce Incremental Semantic Analy-
sis, a fully incremental word space model,
and we test it on longitudinal child-directed
speech data. On this task, ISA outperforms
the related Random Indexing algorithm, as
well as a SVD-based technique. In addi-
tion, the model has interesting properties
that might also be characteristic of the se-
mantic space of children.
1 Introduction
Word space models induce a semantic space from
raw textual input by keeping track of patterns of
co-occurrence of words with other words through a
vectorial representation. Proponents of word space
models such as HAL (Burgess and Lund, 1997) and
LSA (Landauer and Dumais, 1997) have argued that
such models can capture a variety of facts about hu-
man semantic learning, processing, and representa-
tion. As such, word space methods are not only
increasingly useful as engineering applications, but
they are also potentially promising for modeling
cognitive processes of lexical semantics.
However, to the extent that current word space
models are largely non-incremental, they can hardly
accommodate how young children develop a seman-
tic space by moving from virtually no knowledge
of the language to reach an adult-like state. The
family of models based on singular value decom-
position (SVD) and similar dimensionality reduc-
tion techniques (e.g., LSA) first construct a full co-
occurrence matrix based on statistics extracted from
the whole input corpus, and then build a model at
once via matrix algebra operations. Admittedly,
this is hardly a plausible simulation of how chil-
dren learn word meanings incrementally by being
exposed to short sentences containing a relatively
small number of different words. The lack of incre-
mentality of several models appears conspicuous es-
pecially given their explicit claim to solve old theo-
retical issues about the acquisition of language (e.g.,
(Landauer and Dumais, 1997)). Other extant models
display some degree if incrementality. For instance,
HAL and Random Indexing (Karlgren and Sahlgren,
2001) can generate well-formed vector representa-
tions at intermediate stages of learning. However,
they lack incrementality when they make use of stop
word lists or weigthing techniques that are based on
whole corpus statistics. For instance, consistently
with the HAL approach, Li et al (2000) first build
a word co-occurrence matrix, and then compute the
variance of each column to reduce the vector dimen-
sions by discarding those with the least contextual
diversity.
Farkas and Li (2000) and Li et al (2004) pro-
pose an incremental version of HAL by using a a re-
current neural network trained with Hebbian learn-
ing. The networks incrementally build distributional
vectors that are then used to induce word semantic
clusters with a Self-Organizing Map.Farkas and Li
(2000) does not contain any evaluation of the struc-
ture of the semantic categories emerged in the SOM.
A more precise evaluation is instead performed by
Li et al (2004), revealing the model?s ability to sim-
ulate interesting aspects of early vocabulary dynam-
ics. However, this is achieved by using hybrid word
49
representations, in which the distributional vectors
are enriched with semantic features derived from
WordNet.
Borovsky and Elman (2006) also model word
learning in a fairly incremental fashion, by using the
hidden layer vectors of a Simple Recurrent Network
as word representations. The network is probed at
different training epochs and its internal represen-
tations are evaluated against a gold standard ontol-
ogy of semantic categories to monitor the progress in
word learning. Borovsky and Elman (2006)?s claim
that their model simulates relevant aspects of child
word learning should probably be moderated by the
fact that they used a simplified set of artificial sen-
tences as training corpus. From their simulations it
is thus difficult to evaluate whether the model would
scale up to large naturalistic samples of language.
In this paper, we introduce Incremental Semantic
Indexing (ISA), a model that strives to be more de-
velopmentally plausible by achieving full incremen-
tality. We test the model and some of its less incre-
mental rivals on Lara, a longitudinal corpus of child-
directed speech based on samples of child-adult lin-
guistic interactions collected regularly from 1 to 3
years of age of a single English child. ISA achieves
the best performance on these data, and it learns
a semantic space that has interesting properties for
our understanding of how children learn and struc-
ture word meaning. Thus, the desirability of incre-
mentality increases as the model promises to cap-
ture specific developmental trajectories in semantic
learning.
The plan of the paper is as follows. First, we
introduce ISA together with its main predecessor,
Random Indexing. Then, we present the learning
experiments in which several versions of ISA and
other models are trained to induce and organize lexi-
cal semantic information from child-directed speech
transcripts. Lastly, we discuss further work in devel-
opmental computational modeling using word space
models.
2 Models
2.1 Random Indexing
Since the model we are proposing can be seen as
a fully incremental variation on Random Indexing
(RI), we start by introducing the basic features of
RI (Karlgren and Sahlgren, 2001). Initially, each
context word is assigned an arbitrary vector repre-
sentation of fixed dimensionality d made of a small
number of randomly distributed +1 and -1, with all
other dimensions assigned a 0 value (d is typically
much smaller than the dimensionality of the full co-
occurrence matrix). This vector representation is
called signature. The context-dependent represen-
tation for a given target word is then obtained by
adding the signatures of the words it co-occurs with
to its history vector. Multiplying the history by a
small constant called impact typically improves RI
performance. Thus, at each encounter of target word
t with a context word c, the history of t is updated as
follows:
ht += i? sc (1)
where i is the impact constant, ht is the history vec-
tor of t and sc is the signature vector of c. In this
way, the history of a word keeps track of the con-
texts in which it occurred. Similarity among words
is then measured by comparing their history vectors,
e.g., measuring their cosine.
RI is an extremely efficient technique, since it di-
rectly builds and updates a matrix of reduced di-
mensionality (typically, a few thousands elements),
instead of constructing a full high-dimensional co-
occurrence matrix and then reducing it through SVD
or similar procedures. The model is incremental
to the extent that at each stage of corpus process-
ing the vector representations are well-formed and
could be used to compute similarity among words.
However, RI misses the ?second order? effects that
are claimed to account, at least in part, for the ef-
fectiveness of SVD-based techniques (Manning and
Schu?tze, 1999, 15.4). Thus, for example, since dif-
ferent random signatures are assigned to the words
cat, dog and train, the model does not capture the
fact that the first two words, but not the third, should
count as similar contexts. Moreover, RI is not fully
incremental in several respects. First, on each en-
counter of two words, the same fixed random sig-
nature of one of them is added to the history of the
other, i.e., the way in which a word affects another
does not evolve with the changes in the model?s
knowledge about the words. Second, RI makes use
of filtering and weighting procedures that rely on
50
global statistics, i.e., statistics based on whole cor-
pus counts. These procedures include: a) treating
the most frequent words as stop words; b) cutting
off the lowest frequency words as potential contexts;
and c) using mutual information or entropy mea-
sures to weight the effect of a word on the other).
In addition, although procedures b) and c) may have
some psychological grounding, procedure a) would
implausibly entail that to build semantic represen-
tations the child actively filters out high frequency
words as noise from her linguistic experience. Thus,
as it stands RI has some noticeable limitations as a
developmental model.
2.2 Incremental Semantic Analysis
Incremental Semantic Analysis (ISA) differs from
RI in two main respects. First and most importantly,
when a word encounters another word, the history
vector of the former is updated with a weighted sum
of the signature and the history of the latter. This
corresponds to the idea that a target word is affected
not only by its context words, but also by the se-
mantic information encoded by that their distribu-
tional histories. In this way, ISA can capture SVD-
like second order effects: cat and dog might work
like similar contexts because they are likely to have
similar histories. More generally, this idea relies on
two intuitively plausible assumptions about contex-
tual effects in word learning, i.e., that the informa-
tion carried by a context word will change as our
knowledge about the word increases, and that know-
ing about the history of co-occurrence of a context
word is an important part of the information being
contributed by the word to the targets it affects.
Second, ISA does not rely on global statistics for
filtering and weighting purposes. Instead, it uses a
weighting scheme that changes as a function of the
frequency of the context word at each update. This
makes the model fully incremental and (together
with the previous innovation) sensitive not only to
the overall frequency of words in the corpus, but to
the order in which they appear.
More explicitly, at each encounter of a target word
t with a context word c, the history vector of t is
updated as follows:
ht += i? (mchc + (1?mc)sc) (2)
The constant i is the impact rate, as in the RI for-
mula (1) above. The valuemc determines how much
the history of a word will influence the history of an-
other word. The intuition here is that frequent words
tend to co-occur with a lot of other words by chance.
Thus, the more frequently a word is seen, the less
informative its history will be, since it will reflect
uninteresting co-occurrences with all sorts of words.
ISA implements this by reducing the influence that
the history of a context word c has on the target word
t as a function of the token frequency of c (notice
that the model still keeps track of the encounter with
c, by adding its signature to the history of t; it is just
the history of c that is weighted down). More pre-
cisely, the m weight associated with a context word
c decreases as follows:
mc =
1
exp
(
Count(c)
km
)
where km is a parameter determining how fast the
decay will be.
3 Experimental setting
3.1 The Lara corpus
The input for our experiments is provided by the
Child-Directed-Speech (CDS) section of the Lara
corpus (Rowland et al, 2005), a longitudinal cor-
pus of natural conversation transcripts of a single
child, Lara, between the ages of 1;9 and 3;3. Lara
was the firstborn monolingual English daughter of
two White university graduates and was born and
brought up in Nottinghamshire, England. The cor-
pus consists of transcripts from 122 separate record-
ing sessions in which the child interacted with adult
caretakers in spontaneous conversations. The total
recording time of the corpus is of about 120 hours,
representing one of the densest longitudinal corpora
available. The adult CDS section we used contains
about 400K tokens and about 6K types.
We are aware that the use of a single-child corpus
may have a negative impact on the generalizations
on semantic development that we can draw from the
experiments. On the other hand, this choice has the
important advantage of providing a fairly homoge-
neous data environment for our computational sim-
ulations. In fact, we can abstract from the intrin-
sic variability characterizing any multi-child corpus,
51
and stemming from differences in the conversation
settings, in the adults? grammar and lexicon, etc.
Moreover, whereas we can take our experiments to
constitute a (very rough) simulation of how a par-
ticular child acquires semantic representations from
her specific linguistic input, it is not clear what simu-
lations based on an ?averages? of different linguistic
experiences would represent.
The corpus was part-of-speech-tagged and lem-
matized using the CLAN toolkit (MacWhinney,
2000). The automated output was subsequently
checked and disambiguated manually, resulting in
very accurate annotation. In our experiments, we
use lemma-POS pairs as input to the word space
models (e.g., go-v rather than going, goes, etc.)
Thus, we make the unrealistic assumptions that the
learner already solved the problem of syntactic cate-
gorization and figured out the inflectional morphol-
ogy of her language. While a multi-level bootstrap-
ping process in which the morphosyntactic and lex-
ical properties of words are learned in parallel is
probably cognitively more likely, it seems reason-
able at the current stage of experimentation to fix
morphosyntax and focus on semantic learning.
3.2 Model training
We experimented with three word space models:
ISA, RI (our implementations in both cases) and the
SVD-based technique implemented by the Infomap
package.1
Parameter settings may considerably impact the
performance of word space models (Sahlgren,
2006). In a stage of preliminary investigations (not
reported here, and involving also other corpora) we
identified a relatively small range of values for each
parameter of each model that produced promising
results, and we focused on it in the subsequent, more
systematic exploration of the parameter space.
For all models, we used a context window of five
words to the left and five words to the right of the
target. For both RI and ISA, we set signature initial-
ization parameters (determining the random assign-
ment of 0s, +1s and -1s to signature vectors) similar
to those described by Karlgren and Sahlgren (2001).
For RI and SVD, we used two stop word filtering
lists (removing all function words, and removing the
1http://infomap-nlp.sourceforge.net/
top 30 most frequent words), as well as simulations
with no stop word filtering. For RI and ISA, we used
signature and history vectors of 1,800 and 2,400 di-
mensions (the first value, again, inspired by Karl-
gren and Sahlgren?s work). Preliminary experiments
with 300 and 900 dimensions produced poor results,
especially with RI. For SVD, we used 300 dimen-
sions only. This was in part due to technical lim-
itations of the implementation, but 300 dimensions
is also a fairly typical choice for SVD-based mod-
els such as LSA, and a value reported to produce
excellent results in the literature. More importantly,
in unrelated experiments SVD with 300 dimensions
and function word filtering achieved state-of-the-art
performance (accuracy above 90%) in the by now
standard TOEFL synonym detection task (Landauer
and Dumais, 1997).
After preliminary experiments showed that both
models (especially ISA) benefited from a very low
impact rate, the impact parameter i of RI and ISA
was set to 0.003 and 0.009. Finally, km (the ISA pa-
rameter determining the steepness of decay of the
influence of history as the token frequency of the
context word increases) was set to 20 and 100 (recall
that a higher km correspond to a less steep decay).
The parameter settings we explored were system-
atically crossed in a series of experiments. More-
over, for RI and ISA, given that different random
initializations will lead to (slightly) different results,
each experiment was repeated 10 times.
Below, we will report results for the best perform-
ing models of each type: ISA with 1,800 dimen-
sions, i set to 0.003 and km set to 100; RI with 2,400
dimensions, i set to 0.003 and no stop words; SVD
with 300-dimensional vectors and function words
removed. However, it must be stressed that 6 out
of the 8 ISA models we experimented with outper-
formed the best RI model (and they all outperformed
the best SVD model) in the Noun AP task discussed
in section 4.1. This suggests that the results we re-
port are not overly dependent on specific parameter
choices.
3.3 Evaluation method
The test set was composed of 100 nouns and 70
verbs (henceforth, Ns and Vs), selected from the
most frequent words in Lara?s CDS section (word
frequency ranges from 684 to 33 for Ns, and from
52
3501 to 89 for Vs). This asymmetry in the test
set mirrors the different number of V and N types
that occur in the input (2828 Ns vs. 944 Vs). As
a further constraint, we verified that all the words
in the test set alo appeared among the child?s pro-
ductions in the corpus. The test words were un-
ambiguously assigned to semantic categories pre-
viously used to model early lexical development
and represent plausible early semantic groupings.
Semantic categories for nouns and verbs were de-
rived by combining two methods. For nouns, we
used the ontologies from the Macarthur-Bates Com-
municative Development Inventories (CDI).2 All
the Ns in the test set alo appear in the Tod-
dler?s List in CDI. The noun semantic categories are
the following (in parenthesis, we report the num-
ber of words per class and an example): ANI-
MALS REAL OR TOY (19; dog), BODY PARTS (16;
nose), CLOTHING (5; hat), FOOD AND DRINK (13;
pizza), FURNITURE AND ROOMS (8; table), OUT-
SIDE THINGS AND PLACES TO GO (10; house),
PEOPLE (10; baby), SMALL HOUSEHOLD ITEMS
(13; bottle), TOYS (6; pen). Since categories for
verbs were underspecified in the CDI, we used
12 broad verb semantic categories for event types,
partly extending those in Borovsky and Elman
(2006): ACTION (11; play), ACTION BODY (6;
eat), ACTION FORCE (5; pull), ASPECTUAL (6;
start), CHANGE (12; open), COMMUNICATION (4;
talk), MOTION (5; run), PERCEPTION (6; hear),
PSYCH (7; remember), SPACE (3; stand), TRANS-
FER (6; buy).
It is worth emphasizing that this experimental set-
ting is much more challenging than those that are
usually adopted by state-of-the-art computational
simulations of word learning, as the ones reported
above. For instance, the number of words in our
test set is larger than the one in Borovsky and Elman
(2006), and so is the number of semantic categories,
both for Ns and for Vs. Conversely, the Lara corpus
is much smaller than the data-sets normally used to
train word space models. For instance, the best re-
sults reported by Li et al (2000) are obtained with
an input corpus which is 10 times bigger than ours.
As an evaluation measure of the model perfor-
mance in the word learning task, we adopted Aver-
2http://www.sci.sdsu.edu/cdi/
age Precision (AP), recently used by Borovsky and
Elman (2006). AP evaluates how close all members
of a certain category are to each other in the seman-
tic space built by the model.
To calculate AP, for each wi in the test set we first
extracted the corresponding distributional vector vi
produced by the model. Vectors were used to cal-
culate the pair-wise cosine between each test word,
as a measure of their distance in the semantic space.
Then, for each target word wi, we built the list ri of
the other test words ranked by their decreasing co-
sine values with respect to wi. The ranking ri was
used to calculate AP (wi), the Word Average Preci-
sion for wi, with the following formula:
AP (wi) =
1
|Cwi |
?
wj?Cwi
nwj (Cwi)
nwj
where Cwi is the semantic category assigned to wi,
nwj is the set of words appearing in ri up to the rank
occupied bywj , and nwj (Cwi) is the subset of words
in nwj that belong to category Cwi .
AP (wi) calculates the proportion of words that
belong to the same category of wi at each rank in
ri, and then divides this proportion by the number
of words that appear in the category. AP ranges
from 0 to 1: AP (wi) = 1 would correspond to the
ideal case in which all the closest words to wi in ri
belonged to the same category as wi; conversely, if
all the words belonging to categories other than Cwi
were closer to wi than the words in Cwi , AP (wi)
would approach 0. We also defined the Class AP
for a certain semantic category by simply averaging
over the Word AP (wi) for each word in that cate-
gory:
AP (Ci) =
?j=|Ci|
j=1 AP (wj)
|Ci|
We adopted AP as a measure of the purity and co-
hesiveness of the semantic representations produced
by the model. Words and categories for which the
model is able to converge on well-formed represen-
tations should therefore have higher AP values. If
we define Recall as the number of words in nwj be-
longing to Cwi divided by the total number of words
in Cwi , then all the AP scores reported in our exper-
iments correspond to 100% Recall, since the neigh-
bourhood we used to compute AP (wi) always in-
cluded all the words in Cwi . This represents a very
53
Nouns
Tokens ISA RI SVD
100k 0.321 0.317 0.243
200k 0.343 0.337 0.284
300k 0.374 0.367 0.292
400k 0.400 0.393 0.306
Verbs
100k 0.242 0.247 0.183
200k 0.260 0.266 0.205
300k 0.261 0.266 0.218
400k 0.270 0.272 0.224
Table 1: Word AP scores for Nouns (top) and Verbs
(bottom). For ISA and RI, scores are averaged
across 10 iterations
stringent evaluation condition for our models, far be-
yond what is commonly used in the evaluation of
classification and clustering algorithms.
4 Experiments and results
4.1 Word learning
Since we intended to monitor the incremental path
of word learning given increasing amounts of lin-
guistic input, AP scores were computed at four
?training checkpoints? established at 100K, 200K,
300K and 400K word tokens (the final point corre-
sponding to the whole corpus).3 Scores were calcu-
lated independently for Ns and Vs. In Table 1, we
report the AP scores obtained by the best perform-
ing models of each type , as described in section 3.2.
The reported AP values refer to Word AP averaged
respectively over the number of Ns and Vs in the test
set. Moreover, for ISA and RI we report mean AP
values across 10 repetitions of the experiment.
For Ns, both ISA and RI outperformed SVD at all
learning stages. Moreover, ISA also performed sig-
nificantly better than RI in the full-size input condi-
tion (400k checkpoint), as well as at the 300k check-
point (Welch t-test; df = 17, p < .05).
One of the most striking results of these experi-
ments was the strongN-V asymmetry in theWord AP
scores, with the Vs performing significantly worse
than the Ns. For Vs, RI appeared to have a small
advantage over ISA, although it was never signifi-
cant at any stage. The asymmetry is suggestive of
the widely attested N-V asymmetry in child word
3The checkpoint results for SVD were obtained by training
different models on increasing samples from the corpus, given
the non-incremental nature of this method.
learning. A consensus has gathered in the early
word learning literature that children from several
languages acquire Ns earlier and more rapidly than
Vs (Gentner, 1982). An influential account explains
this noun-bias as a product of language-external fac-
tors such as the different complexity of the world
referents for Ns and Vs. Recently, Christiansen and
Monaghan (2006) found that distributional informa-
tion in English CDS was more reliable for identi-
fying Ns than Vs. This suggests that the category-
bias may also be partly driven by how good cer-
tain language-internal cues for Ns and Vs are in a
given language. Likewise, distributional cues to se-
mantics may be stronger for English Ns than for
Vs. The noun-bias shown by ISA (and by the other
models) could be taken to complement the results
of Christiansen and Monaghan in showing that En-
glish Ns are more easily discriminable than Vs on
distributionally-grounded semantic terms.
4.2 Category learning
In Table 2, we have reported the Class AP scores
achieved by ISA, RI and SVD (best models) under
the full-corpus training regime for the nine nominal
semantic categories. Although even in this case ISA
and RI generally perform better than SVD (with the
only exceptions of FURNITURE AND ROOMS
and SMALL HOUSEHOLD ITEMS), results
show a more complex and articulated sit-
uation. With BODY PARTS, PEOPLE, and
SMALL HOUSEHOLD ITEMS, ISA significantly
outperforms its best rival RI (Welch t-test; p < .05).
For the other classes, the differences among the two
models are not significant, except for CLOTHING
in which RI performs significantly better than ISA.
For verb semantic classes (whose analytical data are
not reported here for lack of space), no significant
differences exist among the three models.
Some of the lower scores in Table 2 can be ex-
plained either by the small number of class mem-
bers (e.g. TOYS has only 6 items), or by the class
highly heterogeneous composition (e.g. in OUT-
SIDE THINGS AND PLACES TO GO we find nouns
like garden, flower and zoo). The case of PEOPLE,
for which the performance of all the three models
is far below their average Class AP score (ISA =
0.35; RI = 0.35; SVD = 0.27), is instead much more
surprising. In fact, PEOPLE is one of the classes
54
Semantic class ISA RI SVD
ANIMALS REAL OR TOY 0.616 0.619 0.438
BODY PARTS 0.671 0.640 0.406
CLOTHING 0.301 0.349 0.328
FOOD AND DRINK 0.382 0.387 0.336
FURNITURE AND ROOMS 0.213 0.207 0.242
OUTSIDE THINGS PLACES 0.199 0.208 0.198
PEOPLE 0.221 0.213 0.201
SMALL HOUSEHOLD ITEMS 0.208 0.199 0.244
TOYS 0.362 0.368 0.111
Table 2: Class AP scores for Nouns. For ISA and
RI, scores are averaged across 10 iterations
with the highest degree of internal coherence, be-
ing composed only of nouns unambiguously denot-
ing human beings, such as girl, man, grandma, etc.
The token frequency of the members in this class is
also fairly high, ranging between 684 and 55 occur-
rences. Last but not least, in unrelated experiments
we found that a SVD model trained on the British
National Corpus with the same parameters as those
used with Lara was able to achieve very good per-
formances with human denoting nouns, similar to
the members of our PEOPLE class.
These facts have prompted us to better investi-
gate the reasons why with Lara none of the three
models was able to converge on a satisfactory rep-
resentation for the nouns belonging to the PEO-
PLE class. We zoomed in on this semantic class
by carrying out another experiment with ISA. This
model underwent 8 cycles of evaluation, in each of
which the 10 words originally assigned to PEOPLE
have been reclassified into one of the other nom-
inal classes. For each cycle, AP scores were re-
computed for the 10 test words. The results are re-
ported in Figure 1 (where AP refers to the average
Word AP achieved by the 10 words originally be-
longing to the class PEOPLE). The highest score is
reached when the PEOPLE nouns are re-labeled as
ANIMALS REAL OR TOY (we obtained similar re-
sults in a parallel experiment with SVD). This sug-
gests that the low score for the class PEOPLE in the
original experiment was due to ISA mistaking peo-
ple names for animals. What prima facie appeared
as an error could actually turn out to be an interesting
feature of the semantic space acquired by the model.
The experiments show that ISA (as well as the other
models) groups together animals and people Ns, as
Figure 1: AP scores for Ns in PEOPLE reclassified
in the other classes
it has formed a general and more underspecified se-
mantic category that we might refer to as ANIMATE.
This hypothesis is also supported by qualitative ev-
idence. A detailed inspection of the CDS in the
Lara corpus reveals that the animal nouns in the
test set are mostly used by adults to refer either to
toy-animals with which Lara plays or to characters
in stories. In the transcripts, both types of entities
display a very human-like behavior (i.e., they talk,
play, etc.), as it happens to animal characters in most
children?s stories. Therefore, the difference between
model performance and the gold standard ontology
can well be taken as an interesting clue to a genuine
peculiarity in children?s semantic space with respect
to adult-like categorization. Starting from an input
in which animal and human nouns are used in sim-
ilar contexts, ISA builds a semantic space in which
these nouns belong to a common underspecified cat-
egory, much like the world of a child in which cats
and mice behave and feel like human beings.
5 Conclusion
Our main experiments show that ISA significantly
outperforms state-of-the-art word space models in
a learning task carried out under fairly challenging
training and testing conditions. Both the incremen-
tal nature and the particular shape of the semantic
representations built by ISA make it a (relatively)
realistic computational model to simulate the emer-
55
gence of a semantic space in early childhood.
Of course, many issues remain open. First of all,
although the Lara corpus presents many attractive
characteristics, it still contains data pertaining to a
single child, whose linguistic experience may be un-
usual. The evaluation of the model should be ex-
tended to more CDS corpora. It will be especially
interesting to run experiments in languages such as
as Korean (Choi and Gopnik, 1995), where no noun-
bias is attested. There, we would predict that the dis-
tributional information to semantics be less skewed
in favor of nouns. All CDS corpora we are aware of
are rather small, compared to the amount of linguis-
tic input a child hears. Thus, we also plan to test the
model on ?artificially enlarged? corpora, composed
of CDS from more than one child, plus other texts
that might be plausible sources of early linguistic in-
put, such as children?s stories.
In addition, the target of the model?s evaluation
should not be to produce as high a performance as
possible, but rather to produce performance match-
ing that of human learners.4 In this respect, the
output of the model should be compared to what is
known about human semantic knowledge at various
stages, either by looking at experimental results in
the acquisition literature or, more directly, by com-
paring the output of the model to what we can in-
fer about the semantic generalizations made by the
child from her/his linguistic production recorded in
the corpus.
Finally, further studies should explore how the
space constructed by ISA depends on the order in
which sentences are presented to it. This could shed
some light on the issue of how different experien-
tial paths might lead to different semantic general-
izations.
While these and many other experiments must be
run to help clarifying the properties and effective-
ness of ISA, we believe that the data presented here
constitute a very promising beginning for this new
line of research.
References
Borovsky, A. and J. Elman. 2006. Language input and
semantic categories: a relation between cognition and
4We thank an anonymous reviewer for this note
early word learning. Journal of Child Language, 33:
759-790.
Burgess, C. and K. Lund. 1997. Modelling parsing
constraints with high-dimensional context space. Lan-
guage and Cognitive Processes, 12: 1-34.
Choi, S. and A. Gopnik, A. 1995. Early acquisition of
verbs in Korean: a cross-linguistic study. Journal of
Child Language 22: 497-529.
Christiansen, M.H. and P. Monaghan. 2006. Dis-
covering verbs through multiple-cue integration. In
K. Hirsh-Pasek and R.M. Golinkoff (eds.), Action
meets word: How children learn verbs. OUP, Oxford.
Farkas, I. and P. Li. 2001. A self-organizing neural net-
work model of the acquisition of word meaning. Pro-
ceedings of the 4th International Conference on Cog-
nitive Modeling.
Gentner, D. 1982. Why nouns are learned before verbs:
Linguistic relativity versus natural partitioning. In
S.A. Kuczaj (ed.), Language development, vol. 2: Lan-
guage, thought and culture. Erlbaum, Hillsdale, NJ.
Karlgren, J. and M. Sahlgren. 2001. From words to un-
derstanding. In Uesaka, Y., P. Kanerva and H. Asoh
(eds.), Foundations of real-world intelligence, CSLI,
Stanford: 294-308,
Landauer, T.K. and S.T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis theory
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104(2): 211-240.
Li, P., C. Burgess and K. Lund. 2000. The acquisition of
word meaning through global lexical co-occurrences.
Proceedings of the 31st Child Language Research Fo-
rum: 167-178.
Li, P., I. Farkas and B. MacWhinney. 2004. Early lexical
acquisition in a self-organizing neural network. Neu-
ral Networks, 17(8-9): 1345-1362.
Manning Ch. and H. Schu?tze. 1999. Foundations of sta-
tistical natural language processing The MIT Press,
Cambridge, MASS.
MacWhinney, B. 2000. The CHILDES project: Tools for
analyzing talk (3d edition). Erlbaum, Mahwah, NJ.
Rowland, C., J. Pine, E. Lieven and A. Theakston.
2005. The incidence of error in young children?s wh-
questions. Journal of Speech, Language and Hearing
Research, 48(2): 384-404.
Sahlgren, M. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, Depart-
ment of Linguistics, Stockholm University.
56
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 1?8,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
One distributional memory, many semantic spaces
Marco Baroni
University of Trento
Trento, Italy
marco.baroni@unitn.it
Alessandro Lenci
University of Pisa
Pisa, Italy
alessandro.lenci@ilc.cnr.it
Abstract
We propose an approach to corpus-based
semantics, inspired by cognitive science,
in which different semantic tasks are tack-
led using the same underlying reposi-
tory of distributional information, col-
lected once and for all from the source
corpus. Task-specific semantic spaces are
then built on demand from the repository.
A straightforward implementation of our
proposal achieves state-of-the-art perfor-
mance on a number of unrelated tasks.
1 Introduction
Corpus-derived distributional semantic spaces
have proved valuable in tackling a variety of tasks,
ranging from concept categorization to relation ex-
traction to many others (Sahlgren, 2006; Turney,
2006; Pado? and Lapata, 2007). The typical ap-
proach in the field has been a ?local? one, in which
each semantic task (or set of closely related tasks)
is treated as a separate problem, that requires its
own corpus-derived model and algorithms. Its
successes notwithstanding, the ?one task ? one
model? approach has also some drawbacks.
From a cognitive angle, corpus-based models
hold promise as simulations of how humans ac-
quire and use conceptual and linguistic informa-
tion from their environment (Landauer and Du-
mais, 1997). However, the common view in cog-
nitive (neuro)science is that humans resort to a
multipurpose semantic memory, i.e., a database
of interconnected concepts and properties (Rogers
and McClelland, 2004), adapting the information
stored there to the task at hand. From an engineer-
ing perspective, going back to the corpus to train a
different model for each application is inefficient
and it runs the risk of overfitting the model to a
specific task, while losing sight of its adaptivity ? a
highly desirable feature for any intelligent system.
Think, by contrast, of WordNet, a single network
of semantic information that has been adapted to
all sorts of tasks, many of them certainly not en-
visaged by the resource creators.
In this paper, we explore a different approach
to corpus-based semantics. Our model consists
of a distributional semantic memory ? a graph of
weighted links between concepts - built once and
for all from our source corpus. Starting from the
tuples that can be extracted from this graph, we
derive multiple semantic spaces to solve a wide
range of tasks that exemplify various strands of
corpus-based semantic research: measuring se-
mantic similarity between concepts, concept cate-
gorization, selectional preferences, analogy of re-
lations between concept pairs, finding pairs that
instantiate a target relation and spotting an alterna-
tion in verb argument structure. Given a graph like
the one in Figure 1 below, adaptation to all these
tasks (and many others) can be reduced to two ba-
sic operations: 1) building semantic spaces, as co-
occurrence matrices defined by choosing different
units of the graph as row and column elements;
2) measuring similarity in the resulting matrix ei-
ther between specific rows or between a row and
an average of rows whose elements share a certain
property.
After reviewing some of the most closely re-
lated work (Section 2), we introduce our approach
(Section 3) and, in Section 4, we proceed to test
it in various tasks, showing that its performance is
always comparable to that of task-specific meth-
ods. Section 5 draws the current conclusions and
discusses future directions.
2 Related work
Turney (2008) recently advocated the need for a
uniform approach to corpus-based semantic tasks.
Turney recasts a number of semantic challenges in
terms of relational or analogical similarity. Thus,
if an algorithm is able to tackle the latter, it can
1
also be used to address the former. Turney tests his
system in a variety of tasks, obtaining good results
across the board. His approach amounts to pick-
ing a task (analogy recognition) and reinterpreting
other tasks as its particular instances. Conversely,
we assume that each task may keep its speci-
ficity, and unification is achieved by designing a
sufficiently general distributional structure, from
which semantic spaces can be generated on de-
mand. Currently, the only task we share with Tur-
ney is finding SAT analogies, where his method
outperforms ours by a large margin (cf. Section
4.2.1). However, Turney uses a corpus that is
25 times larger than ours, and introduces nega-
tive training examples, whereas we dependency-
parse our corpus ? thus, performance is not di-
rectly comparable. Besides the fact that our ap-
proach does not require labeled training data like
Turney?s one, it provides, we believe, a more intu-
itive measure of taxonomic similarity (taxonomic
neighbours are concepts that share similar con-
texts, rather than concepts that co-occur with pat-
terns indicating a taxonomic relation), and it is
better suited to model productive semantic phe-
nomena, such as the selectional preferences of
verbs with respect to unseen arguments (eating
topinambur vs. eating ideas). Such tasks will re-
quire an extension of the current framework of
Turney (2008) beyond evidence from the direct co-
occurrence of target word pairs.
While our unified framework is, as far as we
know, novel, the specific ways in which we tackle
the different tasks are standard. Concept similar-
ity is often measured by vectors of co-occurrence
with context words that are typed with dependency
information (Lin, 1998; Curran and Moens, 2002).
Our approach to selectional preference is nearly
identical to the one of Pado? et al (2007). We
solve SAT analogies with a simplified version of
the method of Turney (2006). Detecting whether
a pair expresses a target relation by looking at
shared connector patterns with model pairs is a
common strategy in relation extraction (Pantel and
Pennacchiotti, 2008). Finally, our method to de-
tect verb slot similarity is analogous to the ?slot
overlap? of Joanis et al (2008) and others. Since
we aim at a unified approach, the lack of origi-
nality of our task-specific methods should be re-
garded as a positive fact: our general framework
can naturally reproduce, locally, well-tried ad-hoc
solutions.
3 Distributional semantic memory
Many different, apparently unrelated, semantic
tasks resort to the same underlying information,
a ?distributional semantic memory? consisting of
weighted concept+link+concept tuples extracted
from the corpus. The concepts in the tuples are
typically content words. The link contains corpus-
derived information about how the two words are
connected in context: it could be for example a
dependency path or a shallow lexico-syntactic pat-
tern. Finally, the weight typically derives from co-
occurrence counts for the elements in a tuple, re-
scaled via entropy, mutual information or similar
measures. The way in which the tuples are iden-
tified and weighted when populating the memory
is, of course, of fundamental importance to the
quality of the resulting models. However, once
the memory has been populated, it can be used to
tackle many different tasks, without ever having to
go back to the source corpus.
Our approach can be compared with the typical
organization of databases, in which multiple alter-
native ?views? can be obtained from the same un-
derlying data structure, to answer different infor-
mation needs. The data structure is virtually inde-
pendent from the way in which it is accessed. Sim-
ilarly, the structure of our repository only obeys
to the distributional constraints extracted from the
corpus, and it is independent from the ways it will
be ?queried? to address a specific semantic task.
Different tasks can simply be defined by how we
split the tuples from the repository into row and
column elements of a matrix whose cells are filled
by the corresponding weights. Each of these de-
rived matrices represents a particular view of dis-
tributional memory: we will discuss some of these
views, and the tasks they are appropriate for, in
Section 4.
Concretely, we used here the web-derived, 2-
billion word ukWaC corpus,1 dependency-parsed
with MINIPAR.2 Focusing for now on modeling
noun-to-noun and noun-to-verb connections, we
selected the 20,000 most frequent nouns and 5,000
most frequent verbs as target concepts (minus stop
lists of very frequent items). We selected as tar-
get links the top 30 most frequent direct verb-
noun dependency paths (e.g., kill+obj+victim),
the top 30 preposition-mediated noun-to-noun or
1http://wacky.sslmit.unibo.it
2http://www.cs.ualberta.ca/?lindek/
minipar.htm
2
die
victim
subj_in
1335.2
teacher
subj_tr
109.4
soldier
subj_in
4547.5
policeman
subj_in
68.6
school
in
2.5
kill
       subj_tr
22.4
obj
915.4     obj
9.9
subj_tr
1306.9
obj
8948.3
subj_tr
38.2
obj
538.1
at
7020.1
with
28.9
in
11894.4
handbook
with
3.2
use
10.1
gun
with
105.9
use
41.0
in
2.8
at
10.3
in
2.5
with
30.5
use
7.4
Figure 1: A fragment of distributional memory
verb-to-noun paths (e.g., soldier+with+gun) and
the top 50 transitive-verb-mediated noun-to-noun
paths (e.g., soldier+use+gun). We extracted all
tuples in which a target link connected two target
concepts. We computed the weight (strength of
association) for all the tuples extracted in this way
using the local MI measure (Evert, 2005), that is
theoretically justified, easy to compute for triples
and robust against overestimation of rare events.
Tuples with local MI ? 0 were discarded. For
each preserved tuple c1+ l+c2, we added a same-
weight c1 + l?1 + c2 tuple. In graph-theoretical
terms (treating concepts as nodes and labeling the
weighted edges with links), this means that, for
each edge directed from c1 to c2, there is an edge
from c2 to c1 with the same weight and inverse
label, and that such inverse edges constitute the
full set of links directed from c2 to c1. The re-
sulting database (DM, for Distributional Memory)
contains about 69 million tuples. Figure 1 de-
picts a fragment of DM represented as a graph (as-
sume, for what we just said, that for each edge
from x to y there is a same-weight edge from y
to x with inverse label: e.g., the obj link from
kill to victim stands for the tuples kill+obj+victim
and victim+obj?1+kill, both with weight 915.4;
subj in identifies the subjects of intransitive con-
structions, as in The victim died; subj tr refers to
the subjects of transitive sentences, as in The po-
liceman killed the victim).
We also trained 3 closely comparable models
that use the same source corpus, the same tar-
get concepts (in one case, also the same target
links) and local MI as weighting method, with the
same filtering threshold. The myPlain model im-
plements a classic ?flat? co-occurrence approach
(Sahlgren, 2006) in which we keep track of verb-
to-noun co-occurrence within a window that can
include, maximally, one intervening noun, and
noun-to-noun co-occurrence with no more than
2 intervening nouns. The myHAL model uses
the same co-occurrence window, but, like HAL
(Lund and Burgess, 1996), treats left and right co-
occurrences as distinct features. Finally, myDV
uses the same dependency-based target links of
DM as filters. Like in the DV model of Pado?
and Lapata (2007), only pairs connected by target
links are preserved, but the links themselves are
not part of the model. Since none of these alter-
native models stores information about the links,
they are only appropriate for the concept similar-
ity tasks, where links are not necessary.
4 Semantic views and experiments
We now look at three views of the DM
graph: concept-by-link+concept (CxLC),
concept+concept-by-link (CCxL), and
concept+link-by-concept (CLxC). Each view
will be tested on one or more semantic tasks and
compared with alternative models. There is a
fourth possible view, links-by-concept+concept
(LxCC), that is not explored here, but would lead
to meaningful semantic tasks (finding links that
express similar semantic relations).
4.1 The CxLC semantic space
Much work in computational linguistics and re-
lated fields relies on measuring similarity among
words/concepts in terms of their patterns of co-
occurrence with other words/concepts (Sahlgren,
2006). For this purpose, we arrange the informa-
tion from the graph in a matrix where the concepts
(nodes) of interest are rows, and the nodes they
are connected to by outgoing edges are columns,
typed with the corresponding edge label. We re-
fer to this view as the concept-by-link+concept
3
(CxLC) semantic space. From the graph in Fig-
ure 1, we can for example construct the matrix
in Table 1 (here and below, showing only some
rows and columns of interest). By comparing the
row vectors of such matrix using standard geo-
metrical techniques (e.g., measuring the normal-
ized cosine distance), we can find out about con-
cepts that tend to share similar properties, i.e., are
taxonomically similar (synonyms, antonyms, co-
hyponyms), e.g., soldiers and policemen, that both
kill, are killed and use guns.
subj in?1subj tr?1 obj?1 with use
die kill kill gun gun
teacher 109.4 0.0 9.9 0.0 0.0
victim 1335.2 22.4 915.4 0.0 0.0
soldier 4547.5 1306.9 8948.3 105.9 41.0
policeman 68.6 38.2 538.1 30.5 7.4
Table 1: A fragment of the CxLC space
We use the CxLC space in three taxonomic sim-
ilarity tasks: modeling semantic similarity judg-
ments, noun categorization and verb selectional
restrictions.
4.1.1 Human similarity ratings
We use the dataset of Rubenstein and Goode-
nough (1965), consisting of 65 noun pairs rated
by 51 subjects on a 0-4 similarity scale (e.g. car-
automobile 3.9, cord-smile 0.0). The average rat-
ing for each pair is taken as an estimate of the
perceived similarity between the two words. Fol-
lowing Pado? and Lapata (2007), we use Pearson?s
r to evaluate how the distances (cosines) in the
CxLC space between the nouns in each pair cor-
relate with the ratings. Percentage correlations for
DM, our other models and the best absolute re-
sult obtained by Pado? and Lapata (DV+), as well
as their best cosine-based performance (cosDV+),
are reported in Table 2.
model r model r
myDV 70 DV+ 62
DM 64 myHAL 61
myPlain 63 cosDV+ 47
Table 2: Correlation with similarity ratings
DM is the second-best model, outperformed
only by DV when the latter is trained on compara-
ble data (myDV in Table 2). Notice that, here and
below, we did not try any parameter tuning (e.g.,
using a similarity measure different than cosine,
feature selection, etc.) to improve the performance
of DM.
4.1.2 Noun categorization
We use the concrete noun dataset of the ESSLLI
2008 Distributional Semantics shared task,3 in-
cluding 44 concrete nouns to be clustered into cog-
nitively justified categories of increasing general-
ity: 6-way (birds, ground animals, fruits, greens,
tools and vehicles), 3-way (animals, plants and
artifacts) and 2-way (natural and artificial enti-
ties). Following the task guidelines, we clustered
the target row vectors in the CxLX matrix with
CLUTO,4 using its default settings, and evalu-
ated the resulting clusters in terms of cluster-size-
weighted averages of purity and entropy (see the
CLUTO documentation). An ideal solution would
have 100% purity and 0% entropy. Table 3 pro-
vides percentage results for our models as well as
for the ESSLLI systems that reported all the rel-
evant performance measures, indexed by first au-
thor. Models are ranked by a global score given by
summing the 3 purity values and subtracting the 3
entropies.
model 6-way 3-way 2-way global
P E P E P E
Katrenko 89 13 100 0 80 59 197
Peirsman+ 82 23 84 34 86 55 140
DM 77 24 79 38 59 97 56
myDV 80 28 75 51 61 95 42
myHAL 75 27 68 51 68 89 44
Peirsman? 73 28 71 54 61 96 27
myPlain 70 31 68 60 59 97 9
Shaoul 41 77 52 84 55 93 -106
Table 3: Concrete noun categorization
DM outperforms our models trained on com-
parable resources. Katrenko?s system queries
Google for patterns that cue the category of a con-
cept, and thus its performance should rather be
seen as an upper bound for distributional models.
Peirsman and colleagues report results based on
different parameter settings: DM?s performance
? not tuned to the task ? is worse than their top
model, but better than their worse.
4.1.3 Selectional restrictions
In this task we test the ability of the CxLC space to
predict verbal selectional restrictions. We use the
CxLC matrix to compare a concept to a ?proto-
type? constructed by averaging a set of other con-
cepts, that in this case represent typical fillers of
3http://wordspace.collocations.de/
doku.php/esslli:start
4http://glaros.dtc.umn.edu/gkhome/
cluto/cluto/overview
4
a verbal slot ? for example, by averaging the vec-
tors of the nouns that are, according to the underly-
ing graph, objects of killing, we can build a vector
for the typical ?killee?, and model selectional re-
strictions by measuring the similarity of other con-
cepts (including concepts that have not been seen
as objects of killing in the corpus) to this proto-
type. Note that the DM graph is used both to find
the concepts to enter in the prototype (the set of
nouns that are connected to a verb by the relevant
edge) and to compute similarity. Thus, the method
is fully unsupervised.
We test on the two datasets of human judgments
about the plausibility of nouns as arguments (ei-
ther subjects or objects) of verbs used in Pado? et
al. (2007), one (McRae) consisting of 100 noun-
verb pairs rated by 36 subjects, the second (Pado?)
with 211 pairs rated by 20 subjects. For each verb
in these datasets, we built its prototypical sub-
ject/object argument vector by summing the nor-
malized vectors of the 50 nouns with the highest
weight on the appropriate dependency link to the
verb (e.g., the top 50 nouns connected to kill by an
obj link). The cosine distance of a noun to a proto-
type is taken as the model ?plausibility judgment?
about the noun occurring as the relevant verb ar-
gument. Since we are interested in generalization,
if the target noun is in the prototype set we sub-
tract its vector from the prototype before calculat-
ing the cosine. For our comparison models, there
is no way to determine which nouns would form
the prototype, and thus we train them using the
same top noun lists we employ for DM. Following
Pado? and colleagues, performance is measured by
the Spearman ? correlation coefficient between the
average human ratings and the model predictions.
Table 4 reports percentage coverage and correla-
tions for our models as well as those in Pado? et
al. (2007) (ParCos is the best among their purely
corpus-based systems).
model McRae Pado?
coverage ? coverage ?
Pado? 56 41 97 51
DM 96 28 98 50
ParCos 91 21 98 48
myDV 96 21 98 39
myHAL 96 12 98 29
myPlain 96 12 98 27
Resnik 94 3 98 24
Table 4: Correlation with verb-argument plausibil-
ity judgments
DM does very well on this task: its performance
on the Pado? dataset is comparable to that of the
Pado? system, that relies on FrameNet. DM has
nearly identical performance to the latter on the
Pado? dataset. On the McRae data, DM has a lower
correlation, but much higher coverage. Since we
are using a larger corpus than Pado? et al (2007),
who train on the BNC, a fairer comparison might
be the one with our alternative models, that are all
outperformed by DM by a large margin.
4.2 The CCxL semantic space
Another view of the DM graph is exemplified in
Table 5, where concept pairs are represented in
terms of the edge labels (links) connecting them.
Importantly, this matrix contains the same infor-
mation that was used to build the CxLC space
of Table 1, with a different arrangement of what
goes in the rows and in the columns, but the same
weights in the cells ? compare, for example, the
soldier+gun-by-with cell in Table 5 to the soldier-
by-with+gun cell in Table 1.
in at with use
teacher school 11894.47020.1 28.9 0.0
teacher handbook 2.5 0.0 3.2 10.1
soldier gun 2.8 10.3 105.9 41.0
Table 5: A fragment of the CCxL space
We use this space to measure ?relational? sim-
ilarity (Turney, 2006) of concept pairs, e.g., find-
ing that the relation between teachers and hand-
books is more similar to the one between soldiers
and guns, than to the one between teachers and
schools. We also extend relational similarity to
prototypes. Given some example pairs instantiat-
ing a relation, we can harvest new pairs linked by
the same relation by computing the average CCxL
vector of the examples, and finding the nearest
neighbours to this average. In the case at hand,
the link profile of pairs such as soldier+gun and
teacher+handbook could be used to build an ?in-
strument relation? prototype.
We test the CCxL semantic space on recogniz-
ing SAT analogies (relational similarity between
pairs) and semantic relation classification (rela-
tional similarity to prototypes).
4.2.1 Recognizing SAT analogies
We used the set of 374 multiple-choice ques-
tions from the SAT college entrance exam. Each
question includes one target pair, usually called
5
the stem (ostrich-bird) , and 5 other pairs (lion-
cat, goose-flock, ewe-sheep, cub-bear, primate-
monkey). The task is to choose the pair most anal-
ogous to the stem. Each SAT pair can be rep-
resented by the corresponding row vector in the
CCxL matrix, and we select the pair with the high-
est cosine to the stem. In Table 6 we report our
results, together with the state-of-the-art from the
ACL wiki5 and the scores of Turney (2008) (Pair-
Class) and from Amac? Herdag?delen?s PairSpace
system, that was trained on ukWaC. The Attr cells
summarize the performance of the 6 models on the
wiki table that are based on ?attributional similar-
ity? only (Turney, 2006). For the other systems,
see the references on the wiki. Since our coverage
is very low (44% of the stems), in order to make a
meaningful comparison with the other models, we
calculated a corrected score (DM?). Having full
access to the results of the ukWaC-trained, simi-
larly performing PairSpace system, we calculated
the adjusted score by assuming that the DM-to-
PairSpace error ratio (estimated on the items we
cover) is constant on the whole dataset, and thus
the DM hit count on the unseen items is approx-
imated by multiplying the PairSpace hit count on
the same items by the error ratio (DM+ is DM?s
accuracy on the covered test items only).
model % correct model % correct
LRA 56.1 KnowBest 43.0
PERT 53.3 DM? 42.3
PairClass 52.1 LSA 42.0
VSM 47.1 AttrMax 35.0
DM+ 45.3 AttrAvg 31.0
PairSpace 44.9 AttrMin 27.3
k-means 44.0 Random 20.0
Table 6: Accuracy with SAT analogies
DM does not excel in this task, but its corrected
performance is well above chance and that of all
the attributional models, and comparable to that of
a WordNet-based system (KnowBest) and a sys-
tem that uses manually crafted information about
analogy domains (LSA). All systems with perfor-
mance above DM+ (and k-means) use corpora that
are orders of magnitude larger than ukWaC.
4.2.2 Classifying semantic relations
We also tested the CCxL space on the 7
semantic relations between nominals adopted
in Task 4 of SEMEVAL 2007 (Girju et
5http://www.aclweb.org/aclwiki/index.
php?title=SAT_Analogy_Questions
al., 2007): Cause-Effect, Instrument-Agency,
Product-Producer, Origin-Entity, Theme-Tool,
Part-Whole, Content-Container. For each rela-
tion, the dataset includes 140 training examples
and about 80 test cases. Each example consists
of a small context retrieved from the Web, con-
taining word pairs connected by a certain pattern
(e..g., ?* contains *?). The retrieved contexts were
manually classified by the SEMEVAL organizers
as positive (e.g., wrist-arm) or negative (e.g., ef-
fectiveness-magnesium) instances of a certain re-
lation (e.g., Part-Whole). About 50% training and
test cases are positive instances. For each rela-
tion, we built ?hit? and ?miss? prototype vectors,
by averaging across the vectors of the positive and
negative training pairs attested in our CCxL model
(we use only the word pairs, not the surround-
ing contexts). A test pair is classified as a hit
for a certain relation if it is closer to the hit pro-
totype vector for that relation than to the corre-
sponding miss prototype. We used the SEMEVAL
2007 evaluation method, i.e., precision, recall, F-
measure and accuracy, macroaveraged over all re-
lations, as reported in Table 7. The DM+ scores
ignore the 32% pairs not in our CCxL space; the
DM? scores assume random performance on such
pairs. These scores give the range within which
our performance will lie once we introduce tech-
niques to deal with unseen pairs. We also report
results of the SEMEVAL systems that did not use
the organizer-provided WordNet sense labels nor
information about the query used to retrieve the
examples, as well as performance of several trivial
classifiers, also from the SEMEVAL task descrip-
tion.
model precision recall F accuracy
UCD-FC 66.1 66.7 64.8 66.0
UCB 62.7 63.0 62.7 65.4
ILK 60.5 69.5 63.8 63.5
DM+ 60.3 62.6 61.1 63.3
UMELB-B 61.5 55.7 57.8 62.7
SemeEval avg 59.2 58.7 58.0 61.1
DM? 56.7 58.2 57.1 59.0
UTH 56.1 57.1 55.9 58.8
majority 81.3 42.9 30.8 57.0
probmatch 48.5 48.5 48.5 51.7
UC3M 48.2 40.3 43.1 49.9
alltrue 48.5 100.0 64.8 48.5
Table 7: SEMEVAL relation classification
The DM accuracy is higher than the three SE-
MEVAL baselines (majority, probmatch and all-
true), DM+ is above the average performance of
6
the comparable SEMEVAL models. Differently
from DM, the models that outperform it use fea-
tures extracted from the training contexts and/or
specific additional resources: an annotated com-
pound database for UCD-FC, machine learning
algorithms to train the relation classifiers (ILK,
UCD-FC), Web counts (UCB), etc. The less than
optimal performance by DM is thus counterbal-
anced by its higher ?parsimony? and generality.
4.3 The CLxC semantic space
A third view of the information in the DM graph
is the concept+link-by-concept (CLxC) semantic
space exemplified by the matrix in Table 8.
teacher victim soldier policeman
kill subj tr 0.0 22.4 1306.9 38.2
kill obj 9.9 915.4 8948.3 538.1
die subj in 109.4 1335.2 4547.5 68.6
Table 8: A fragment of the CLxC space
This view captures patterns of similarity be-
tween (surface approximations to) argument slots
of predicative words. We can thus use the CLxC
space to extract generalizations about the inner
structure of lexico-semantic representations of the
sort formal semanticists have traditionally being
interested in. In the example, the patterns of
co-occurrence suggest that objects of killing are
rather similar to subjects of dying, hinting at the
classic cause(subj,die(obj)) analysis of killing by
Dowty (1977) and many others. Again, no new in-
formation has been introduced ? the matrix in Ta-
ble 8 is yet another re-organization of the data in
our graph (compare, for example, the die+subj in-
by-teacher cell of this matrix with the teacher-by-
subj in+die cell in Table 1).
4.3.1 The causative/inchoative alternation
Syntactic alterations (Levin, 1993) represent
a key aspect of the complex constraints that
shape the syntax-semantics interface. One of
the most important cases of alternation is the
causative/inchoative, in which the object argu-
ment (e.g., John broke the vase) can also be re-
alized as an intransitive subject (e.g., The vase
broke). Verbs differ with respect to the possi-
ble syntactic alternations they can participate in,
and this variation is strongly dependent on their
semantic properties (e.g. semantic roles, event
type, etc.). For instance, while break can undergo
the causative/inchoative alternation, mince cannot:
cf. John minced the meat and *The meat minced.
We test our CLxC semantic space on the
discrimination between transitive verbs un-
dergoing the causative-inchoative alterna-
tions and non-alternating ones. We took
232 causative/inchoative verbs and 170 non-
alternating transitive verbs from Levin (1993).
For each verb vi, we extracted from the CLxC
matrix the row vectors corresponding to its tran-
sitive subject (vi + subj tr), intransitive subject
(vi + subj in), and direct object (vi + obj) slots.
Given the definition of the causative/inchoative
alternation, we predict that with alternating verbs
vi + subj in should be similar to vi + obj
(the things that are broken also break), while
this should not hold for non-alternating verbs
(mincees are very different from mincers).
Our model is completely successful in detect-
ing the distinction. The cosine similarity between
transitive subject and object slots is fairly low for
both classes, as one would expect (medians of 0.16
for alternating verbs and 0.11 for non-alternating
verbs). On the other hand, while for the non-
alternating verbs the median cosine similarity be-
tween the intransitive subject and object slots is
a similarly low 0.09, for the alternating verbs the
median similarity between these slots jump up
to 0.31. Paired t-tests confirm that the per-verb
difference between transitive subject vs. object
cosines and intransitive subject vs. object cosines
is highly statistically significant for the alternating
verbs, but not for the non-alternating ones.
5 Conclusion
We proposed an approach to semantic tasks where
statistics are collected only once from the source
corpus and stored as a set of weighted con-
cept+link+concept tuples (naturally represented
as a graph). Different semantic spaces are con-
structed on demand from this underlying ?distri-
butional memory?, to tackle different tasks with-
out going back to the corpus. We have shown that
a straightforward implementation of this approach
leads to excellent performance in various taxo-
nomic similarity tasks, and to performance that,
while not outstanding, is at least reasonable on re-
lational similarity. We also obtained good results
in a task (detecting the causative/inchoative alter-
nation) that goes beyond classic NLP applications
and more in the direction of theoretical semantics.
The most pressing issue we plan to address is
how to improve performance in the relational sim-
7
ilarity tasks. Fortunately, some shortcomings of
our current model are obvious and easy to fix.
The low coverage is in part due to the fact that
our set of target concepts does not contain, by de-
sign, some words present in the task sets. More-
over, while our framework does not allow ad-hoc
optimization of corpus-collection methods for dif-
ferent tasks, the way in which the information in
the memory graph is adapted to tasks should of
course go beyond the nearly baseline approaches
we adopted here. In particular, we need to de-
velop a backoff strategy for unseen pairs in the
relational similarity tasks, that, following Turney
(2006), could be based on constructing surrogate
pairs of taxonomically similar words found in the
CxLC space.
Other tasks should also be explored. Here, we
viewed our distributional memory in line with how
cognitive scientists look at the semantic memory
of healthy adults, i.e., as an essentially stable long
term knowledge repository. However, much in-
teresting semantic action takes place when under-
lying knowledge is adapted to context. We plan
to explore how contextual effects can be modeled
in our framework, focusing in particular on how
composition affects word meaning (Erk and Pado?,
2008). Similarity could be measured directly on
the underlying graph, by relying on graph-based
similarity algorithms ? an elegant approach that
would lead us to an even more unitary view of
what distributional semantic memory is and what
it does. Alternatively, DM could be represented as
a three-mode tensor in the framework of Turney
(2007), enabling smoothing operations analogous
to singular value decomposition.
Acknowledgments
We thank Ken McRae and Peter Turney for pro-
viding data-sets, Amac? Herdag?delen for access to
his results, Katrin Erk for making us look at DM as
a graph, and the reviewers for helpful comments.
References
J. Curran and M. Moens. 2002. Improvements in auto-
matic thesaurus extraction. Proceedings of the ACL
Workshop on Unsupervised Lexical Acquisition, 59?
66.
D. Dowty. 1977. Word meaning and Montague Gram-
mar. Kluwer, Dordrecht.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP 2008.
S. Evert. 2005. The statistics of word cooccurrences.
Ph.D. dissertation, Stuttgart University, Stuttgart.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney and Y. Deniz. 2007. SemEval-2007 task 04:
Classification of semantic relations between nomi-
nals. Proceedings of SemEval-2007, 13?18.
E. Joanis, S. Stevenson and D. James. 2008. A gen-
eral feature space for automatic verb classification.
Natural Language Engineering, 14(3): 337?367.
T.K. Landauer and S.T. Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2): 211?
240.
B. Levin. 1993. English Verb Classes and Alterna-
tions. A Preliminary Investigation. Chicago, Uni-
versity of Chicago Press.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proceedings of ACL 1998, 768?774.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behaviour Research Methods, 28: 203?
208.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2): 161?199.
S. Pado?, S. Pado? and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
Proceedings EMNLP 2007, 400?409.
P. Pantel and M. Pennacchiotti. 2008. Automatically
harvesting and ontologizing semantic relations. In
P. Buitelaar and Ph. Cimiano (eds.), Ontology learn-
ing and population. IOS Press, Amsterdam.
T. Rogers and J. McClelland. 2004. Semantic cog-
nition: A parallel distributed processing approach.
The MIT Press, Cambridge.
H. Rubenstein and J.B. Goodenough. 1965. ?Contex-
tual correlates of synonymy?. Communications of
the ACM, 8(10):627-633.
M. Sahlgren. 2006. The Word-space model. Ph.D. dis-
sertation, Stockholm University, Stockholm.
P. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379?416.
P. Turney. 2007. Empirical evaluation of four ten-
sor decomposition algorithms. IIT Technical Report
ERB-1152, National Research Council of Canada,
Ottawa.
P. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. Proceedings
of COLING 2008, 905?912.
8
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 33?40,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
BagPack: A general framework to represent semantic relations
Ama? Herdag?delen
CIMEC, University of Trento
Rovereto, Italy
amac@herdagdelen.com
Marco Baroni
CIMEC, University of Trento
Rovereto, Italy
marco.baroni@unitn.it
Abstract
We introduce a way to represent word pairs
instantiating arbitrary semantic relations that
keeps track of the contexts in which the words
in the pair occur both together and indepen-
dently. The resulting features are of sufficient
generality to allow us, with the help of a stan-
dard supervised machine learning algorithm,
to tackle a variety of unrelated semantic tasks
with good results and almost no task-specific
tailoring.
1 Introduction
Co-occurrence statistics extracted from corpora lead
to good performance on a wide range of tasks that
involve the identification of the semantic relation be-
tween two words or concepts (Sahlgren, 2006; Turney,
2006). However, the difficulty of such tasks and the
fact that they are apparently unrelated has led to the
development of largely ad-hoc solutions, tuned to spe-
cific challenges. For many practical applications, this is
a drawback: Given the large number of semantic rela-
tions that might be relevant to one or the other task, we
need a multi-purpose approach that, given an appropri-
ate representation and training examples instantiating
an arbitrary target relation, can automatically mine new
pairs characterized by the same relation. Building on a
recent proposal in this direction by Turney (2008), we
propose a generic method of this sort, and we test it
on a set of unrelated tasks, reporting good performance
across the board with very little task-specific tweaking.
There has been much previous work on corpus-based
models to extract broad classes of related words. The
literature on word space models (Sahlgren, 2006) has
focused on taxonomic similarity (synonyms, antonyms,
co-hyponyms. . . ) and general association (e.g., find-
ing topically related words), exploiting the idea that
taxonomically or associated words will tend to occur
in similar contexts, and thus share a vector of co-
occurring words. The literature on relational similar-
ity, on the other hand, has focused on pairs of words,
devising various methods to compare how similar the
contexts in which target pairs appear are to the contexts
of other pairs that instantiate a relation of interest (Tur-
ney, 2006; Pantel and Pennacchiotti, 2006). Beyond
these domains, purely corpus-based methods play an
increasingly important role in modeling constraints on
composition of words, in particular verbal selectional
preferences ? finding out that, say, children are more
likely to eat than apples, whereas the latter are more
likely to be eaten (Erk, 2007; Pad? et al, 2007). Tasks
of this sort differ from relation extraction in that we
need to capture productive patterns: we want to find
out that shabu shabu (a Japanese meat dish) is eaten
whereas ink is not, even if in our corpus neither noun is
attested in proximity to forms of the verb to eat.
Turney (2008) is the first, to the best of our knowl-
edge, to raise the issue of a unified approach. In par-
ticular, he treats synonymy and association as special
cases of relational similarity: in the same way in which
we might be able to tell that hands and arms are in
a part-of relation by comparing the contexts in which
they co-occur to the contexts of known part-of pairs,
we can guess that cars and automobiles are synonyms
by comparing the contexts in which they co-occur to
the contexts linking known synonym pairs.
Here, we build on Turney?s work, adding two main
methodological innovations that allow us further gen-
eralization. First, merging classic approaches to taxo-
nomic and relational similarity, we represent concept
pairs by a vector that concatenates information about
the contexts in which the two words occur indepen-
dently, and the contexts in which they co-occur (Mirkin
et al 2006 also integrate information from the lexi-
cal patterns in which two words co-occur and simi-
larity of the contexts in which each word occurs on
its own, to improve performance in lexical entailment
acquisition). Second, we represent contexts as bag of
words and bigrams, rather than strings of words (?pat-
terns?) of arbitrary length: we leave it to the machine
learning algorithm to zero in on the most interesting
words/bigrams.
Thanks to the concatenated vector, we can tackle
tasks in which the two words are not expected to
co-occur even in very large corpora (such as selec-
tional preference). Concatenation, together with un-
igram/bigram representation of context, allows us to
scale down the approach to smaller training corpora
(Turney used a corpus of more than 50 billion words),
since we do not need to see the words directly co-
occurring, and the unigram/bigram dimensions of the
33
vectors are less sparse than dimensions based on longer
strings of words. We show that our method produces
reasonable results also on a corpus of 2 billion words,
with many unseen pairs. Moreover, our bigram and
unigram representation is general enough that we do
not need to extract separate statistics nor perform ad-
hoc feature selection for each task: we build the co-
occurrence matrix once, and use the same matrix in all
experiments. The bag-of-words assumption also makes
for faster and more compact model building, since the
number of features we extract from a context is linear
in the number of words in the context, whereas it is ex-
ponential for Turney. On the other hand, our method
is currently lagging behind Turney?s in terms of perfor-
mance, suggesting that at least some task-specific tun-
ing will be necessary.
Following Turney, we focus on devising a suitably
general featural representation, and we see the spe-
cific machine learning algorithm employed to perform
the various tasks as a parameter. Here, we use Sup-
port Vector Machines since they are a particularly ef-
fective general-purpose method. In terms of empirical
evaluation of the model, besides experimenting with
the ?classic? SAT and TOEFL datasets, we show how
our algorithm can tackle the selectional preference task
proposed in Pad? (2007) ? a regression task ? and we
introduce to the corpus-based semantics community a
challenge from the ConceptNet repository of common-
sense knowledge (extending such repository by auto-
mated means is the original motivation of our project).
In the next section, we will present our proposed
method along with the corpora and model parameter
choices used in the implementation. In Section 3, we
describe the tasks that we use to evaluate the model.
Results are reported in Section 4 and we conclude in
Section 5, with a brief overview of the contributions of
this paper.
2 Methodology
2.1 Model
The central idea in BagPack (Bag-of-words represen-
tation of Paired concept knowledge) is to construct a
vector-based representation of a pair of words in such a
way that the vector represents both the contexts where
the two words co-occur and the contexts where the sin-
gle words occur on their own. A straightforward ap-
proach is to construct three different sub-vectors, one
for the first word, one for the second word, and one for
the co-occurring pair. The concatenation of these three
sub-vectors is the final vector that represents the pair.
This approach provides us a graceful fall back mech-
anism in case of data scarcity. Even if the two words are
not observed co-occurring in the corpus ? no syntag-
maic information about the pair ?, the corresponding
vector will still represent the individual contexts where
the words are observed on their own. Our hypothesis
(and hope) is that this information will be representa-
tive of the semantic relation between the pair, in the
sense that, given pairs characterized by same relation,
there should be paradigmatic similarity across the first,
resp. second elements of the pairs (e.g., if the relation
is between professionals and the typical tool of their
trade, it is reasonable to expect that that both profes-
sionals and tools will tend to share similar contexts).
Before going into further details, we need to describe
what a ?co-occurrence? precisely means, define the no-
tion of context, and determine how to structure our vec-
tor. For a single word W , the following pseudo regular
expression identifies an observation of occurrence:
?C W D? (1)
where C and D can be empty strings or concatena-
tions of up to 4 words separated by whitespace (i.e.
C1, . . . , Ci and D1, . . . , Dj where i, j ? 4). Each ob-
servation of this pattern constitutes a single context of
W . The pattern is matched with the longest possible
substring without crossing sentence boundaries.
Let (W1,W2) denote an ordered pair of words W1
and W2. We say the two words occur as a pair when-
ever one of the following pseudo regular expressions is
observed in the corpus:
?C W1 DW2 E? (2)
?C W2 D W1 E? (3)
where C and E can be empty strings or concatena-
tions of up to 2 words and similarly, D can be ei-
ther an empty string or concatenation of up to 5 words
(i.e. C1, . . . , Ci, D1, . . . , Dj , and E1, . . . , Ek where
i, j ? 2 and k ? 5). Together, patterns 2 and 3 con-
stitute the pair context for W1 and W2. The pattern is
matched with the longest possible substring while mak-
ing sure that D does not contain neither W1 nor W2.
The number of context words allowed before, after,
and between the targets are actually model parameters
but for the experiments reported in this study, we used
the aforementioned values with no attempt at tuning.
The vector representing (W1,W2) is a concatenation
v1v2v1,2, where, the sub-vectors v1 and v2 are con-
structed by using the single contexts of W1 and W2
correspondingly (i.e. by pattern 1) and the sub-vector
v1,2 is built by using the pair contexts identified by
the patterns 2 and 3. We refer to the components as
single-occurrence vectors and pair-occurrence vector
respectively.
The population of BagPack starts by identifying the
b most frequent unigrams and the b most frequent bi-
grams as basis terms. Let T denote a basis term. For
the construction of v1, we create two features for each
term T : tpre corresponds to the number of observations
of T in the single contexts of W1 occurring before W1
and tpost corresponds to the number of observations of
T in the single occurrence of W1 where T occurs after
W1 (i.e. number of observations of the pattern 1 where
T ? C and T ? D correspondingly). The construc-
tion of v2 is identical except that this time the features
34
correspond to the number of times the basis term is ob-
served before and after the target word W2 in single
contexts. The construction of the pair-occurrence sub-
vector v1,2 proceeds in a similar fashion but in addi-
tion, we incorporate also the order of W1 and W2 as
they co-occur in the pair context: The number of ob-
servations of the pair contexts where W1 occurs before
W2 and T precedes (follows) the pair, are represented
by feature t+pre (t+post). The number of cases where
the basis term is in between the target words is repre-
sented by t+betw. The number of cases where W2 oc-
curs before W1 and T precedes the pair is represented
by the feature t?pre. Similarly the number of cases
where T follows (is in between) the pair is represented
by the feature t?post (t?betw).
Assume that the words "only" and "that" are our ba-
sis terms and consider the following context for the
word pair ("cat", "lion"): "Lion is the only cat that
lives in large social groups." The observation of the ba-
sis terms should contribute to the pair-occurrence sub-
vector v1,2 and since the target words occur in reverse
order, this context results in the incrementation of the
features only?betw and that?post by one.
To sum up, we have 2b basis terms (b unigrams and
b bigrams). Each of the single-occurrence sub-vectors
v1 and v2 consists of 4b features: Each basis term
gives rise to 2 features incorporating the relative posi-
tion of basis term with respect to the single word. The
pair-occurrence sub-vector, v1,2, consists of 12b fea-
tures: Each basis term gives rise to 6 new features; ?3
for possible relative positions of the basis term with re-
spect to the pair and ?2 for the order of the words.
Importantly, the 2b basis terms are picked only once,
and the overall co-occurrence matrix is built once and
for all for all the tasks: unlike Turney, we do not need
to go back to the corpus to pick basis terms and collect
separate statistics for different tasks.
The specifics of the adaptation to each task will be
detailed in Section 3. For the moment, it should suffice
to note that the vectors v1 and v2 represent the con-
texts in which the two words occur on their own, thus
encode paradigmatic information. However, v1,2 rep-
resents the contexts in which the two words co-occur,
thus encode sytagmatic information.
The model training and evaluation is done in a 10-
fold cross-validation setting whenever applicable. The
reported performance measures are the averages over
all folds and the confidence intervals are calculated by
using the distribution of fold-specific results. The only
exception to this setting is the SAT analogy questions
task simply because we consider each question as a
separate mini dataset as described in Section 3.
2.2 Source Corpora
We carried out our tests on two different corpora:
ukWaC, a Web-derived, POS-tagged and lemmatized
collection of about 2 billion tokens,1 and the Yahoo!
1http://wacky.sslmit.unibo.it
database queried via the BOSS service.2 We will refer
to these corpora as ukWaC and Yahoo from now on.
In ukWaC, we limited the number of occurrence and
co-occurrence queries to the first 5000 observations
for computational efficiency. Since we collect cor-
pus statistics at the lemma level, we construct Yahoo!
queries using disjunctions of inflected forms that were
automatically generated with the NodeBox Linguistics
library.3 For example, the query to look for ?lion? and
?cat? with 4 words in the middle is: ?(lion OR lions) *
* * * (cat OR cats OR catting OR catted)?. Each pair
requires 14 Yahoo! queries (one for W1, one for W2,
6 for (W1,W2), in that order, with 0-to-5 intervening
words, 6 analogous queries for (W2,W1)). Yahoo! re-
turns maximally 1,000 snippets per query, and the latter
are lemmatized with the TreeTagger4 before feature ex-
traction.
2.3 Model implementation
We did not carry out a search for ?good? parameter val-
ues. Instead, the model parameters are generally picked
at convenience to ease memory requirements and com-
putational efficiency. For instance, in all experiments,
b is set to 1500 unless noted otherwise in order to fit
the vectors of all pairs at our hand into the computer
memory.
Once we construct the vectors for a set of word pairs,
we get a co-occurrence matrix with pairs on the rows
and the features on the columns. In all of our exper-
iments, the same normalization method and classifi-
cation algorithm is used with the default parameters:
First, a TF-IDF feature weighting is applied to the co-
occurrence matrix (Salton and Buckley, 1988). Then
following the suggestion of Hsu and Chang (2003),
each feature t?s [??t?2??t, ??t+2??t] interval is scaled to
[0, 1], trimming the exceeding values from upper and
lower bounds (the symbols ??t and ??t denote the av-
erage and standard deviation of the feature values re-
spectively). For the classification algorithm, we use the
C-SVM classifier and for regression the -SVM regres-
sor, both implemented in the Matlab toolbox of Canu
et al (2005). We employed a linear kernel. The cost
parameter C is set to 1 for all experiments; for the re-
gressor,  = 0.2. For other pattern recognition related
coding (e.g., cross validation, scaling, etc.) we made
use of the Matlab PRTools (Duin, 2001).
For each task that will be defined in the next section,
we evaluated our algorithm on the following represen-
tations: 1) Single-occurrence vectors (v1v2 condition)
2) Pair-occurrence vectors (v1,2 condition) 3) Entire
co-occurrence matrix (v1v2v1,2 condition).
2http://developer.yahoo.com/search/
boss/
3http://nodebox.net/code/index.php/
Linguistics
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
35
3 Tasks
3.1 SAT Analogy Questions
The first task we evaluated our algorithm on is the
SAT analogy questions task introduced by Turney et al
(2003). In this task, there are 374 multiple choice ques-
tions with a pair of related words like (lion,cat) as the
stem and 5 other pairs as the choices. The correct an-
swer is the choice pair which has the relationship most
similar to that in the stem pair.
We adopt a similar approach to the one used in Tur-
ney (2008) and consider each question as a separate bi-
nary classification problem with one positive training
instance and 5 unknown pairs. For a question, we pick
a pair at random from the stems of other questions as a
pseudo negative instance and train our classifier on this
two-instance training set. Then the trained classifier is
evaluated on the choice pairs and the pair with the high-
est posterior probability for the positive class is called
the winner. The procedure is repeated 10 times pick-
ing a different pseudo-negative instance each time and
the choice pair which is selected as the winner most of-
ten is taken as the answer to that question. The perfor-
mance measure on this task is defined as the percent-
age of correctly answered questions. The mean score
and confidence intervals are calculated over the perfor-
mance scores obtained for all folds.
3.2 TOEFL Synonym Questions
This task, introduced by Landauer and Dumais (1997),
consists of 80 multiple choice questions in which a
word is given as the stem and the correct choice is the
word which has the closest meaning to that of the stem,
among 4 candidates. To fit the task into our frame-
work, we pair each choice with the stem word and ob-
tain 4 word pairs for each question. The word pair
constructed with the stem and the correct choice is la-
beled as positive and the other pairs are labeled as neg-
ative. We consider all 320 pairs constructed for all 80
questions as our dataset. Thus, the problem is turned
into a binary classification problem where the task is
to discriminate the synonymous word pairs (i.e. pos-
itive class) from the other pairs (i.e. negative class).
We made sure that the pairs constructed for the same
question were never split between training and test set,
so that no question-specific learning is performed. The
reason for this precaution is that the evaluation is done
on a per-question basis. The estimated posterior class
probabilities of the pairs constructed for the same ques-
tion are compared to each other and the pair with the
highest probability for the positive class is selected as
the answer for the question. By keeping the pairs of
a question in the same set we make sure their posteri-
ors are calculated by the same trained classifier. The
performance measure is the percentage of correctly an-
swered questions and we report the mean performance
over all 10 folds.
3.3 Selectional Preference Judgments
Linguists have long been interested in the semantic
constraints that verbs impose on their arguments, a
broad area that has also attracted computational mod-
eling, with increasing interest in purely corpus-based
methods (Erk, 2007; Pad? et al, 2007). This task is
of particular interest to us as an example of a broader
class of linguistic problems that involve productive
constraints on composition. As has been stressed at
least since Chomsky?s early work (Chomsky, 1957), no
matter how large a corpus is, if a phenomenon is pro-
ductive there will always be new well-formed instances
that are not in the corpus. In the domain of selectional
restrictions this is particularly obvious: we would not
say that an algorithm learned the constraints on the pos-
sible objects/patients of eating simply by producing the
list of all the attested objects of this verb in a very large
corpus; the interesting issue is whether the algorithm
can detect if an unseen object is or is not a plausible
?eatee?, like humans do without problems. Specifi-
cally, we test selectional preferences on the dataset con-
structed by Pad? (2007), that collects average plausi-
bility judgments (from 20 speakers) for nouns as either
subjects or objects of verbs (211 noun-verb pairs).
We formulate this task as a regression problem. We
train the -SVM regressor with 18-fold cross valida-
tion: Since the pair instances are not independent but
grouped according to the verbs, one fold is constructed
for each of the 18 verbs used in the dataset. In each
fold, all instances sharing the corresponding verb are
left out as the test set. The performance measure for
this task is the Spearman correlation between the hu-
man judgments and our algorithm?s estimates. There
are two possible ways to calculate this measure. One is
to get the overall correlation between the human judg-
ments and our estimates obtained by concatenating the
output of each cross-validation fold. That measure al-
lows us to compare our method with the previously re-
ported results. However, it cannot control for a possi-
ble verb-effect on the human judgment values: If the
average judgment values of the pairs associated with a
specific verb is significantly higher (or lower) than the
average of the pairs associated with another verb, then
any regressor which simply learns to assign the aver-
age value to all pairs associated with that verb (regard-
less of whether there is a patient or agent relation be-
tween the pairs) will still get a reasonably high correla-
tion because of the variation of judgment scores across
the verbs. To control for this effect, we also calculated
the correlation between the human judgments and our
estimates for each verb?s plausibility values separately,
and we report averages across these separate correla-
tions (the ?mean? results reported below).
3.4 Common-sense Relations from ConceptNet
Open Mind Common Sense5 is an ongoing project of
acquisition of common-sense knowledge from ordinary
5http://commons.media.mit.edu/en/
36
Relation Pairs Relation Pairs
IsA 316 PartOf 139
UsedFor 198 LocationOf 1379
CapableOf 228 Total 1943
Table 1: ConceptNet relations after filtering.
people by letting them carry out simple semantic and
linguistics tasks. An end result of the project is Con-
ceptNet 3, a large scale semantic network consisting of
relations between concept pairs (Havasi et al, 2007). It
is possible to view this network as a collection of se-
mantic assertions, each of which can be represented by
a triple involving two concepts and a relation between
them, e.g. UsedFor(piccolo, make music). One moti-
vation for this project is the fact that common-sense
knowledge is assumed to be known by both parties in
a communication setting and usually is not expressed
explicitly. Thus, corpus-based approaches may have
serious difficulties in capturing these relations (Havasi
et al, 2007), but there are reasons to believe that they
could still be useful: Eslick (2006) uses the assertions
of ConceptNet as seeds to parse Web search results and
augment ConceptNet by new candidate relations.
We use the ConceptNet snapshot released in June
2008, containing more than 200.000 assertions with
around 20 semantic relations like UsedFor, Desirious-
EffectOf, or SubEventOf. Each assertion has a confi-
dence rating based on the number of people who ex-
pressed or confirmed that assertion. For simplicity we
limited ourselves to single word concepts and the re-
lations between them. Furthermore, we eliminated the
assertions with a confidence score lower than 3 in an
attempt to increase the "quality" of the assertions and
focused on the most populated 5 relations of the re-
maining set, as given in Table 3.4. There may be more
than one relation between a pair of concepts, so the to-
tal number is less than the sum of the size of the indi-
vidual relation sets.
4 Results
For the multiple choice question tasks (i.e. SAT and
TOEFL), we say a question is complete when all of the
related pairs (stem and choice) are represented by vec-
tors with at least one non-zero component. If a ques-
tion has at least one pair represented by a zero-vector
(missing pairs), then we say that the question is partial.
For these tasks, we report the worst-case performance
scores where we assume that a random guessing per-
formance is obtained on the partial questions. This is
a strict lower bound because it discards all information
we have about a partial question even if it has only one
missing pair. We define coverage as the percentage of
complete questions.
4.1 SAT
In Yahoo, the coverage is quite high. In the v1,2 only
condition, 4 questions had at least some choice/stem
pairs with all zero components. In all other cases, all of
the pairs were represented by vectors with at least one
non-zero component. The highest score is obtained for
the v1v2v1,2 condition with a 44.1% of correct ques-
tions, that is not significantly above the 42.5% perfor-
mance of v1,2 (paired t-test, ? = 0.05). The v1v2 only
condition results in a poorer performance of 33.9% cor-
rect questions, statistically lower than the former two
conditions.
For ukWaC, the v1,2 only condition provides a rel-
atively low coverage. Only 238 questions out of 374
were complete. For the other conditions, we get a com-
plete coverage. The performances are statistically in-
distinguishable from each other and are 38.0%, 38.2%,
and 39.6% for v1,2, v1v2, and v1v2v1,2 respectively.
Condition Yahoo ukWaC
v1,2 42.5% 38.0%
v1v2 33.9% 38.2%
v1v2v1,2 44.1% 39.6%
Table 2: Percentage of correctly answered questions in
SAT analogy task, worst-case scenario.
In Fig. 1, the best performances we get for Yahoo
and ukWaC are compared to previous studies with 95%
binomial confidence intervals plotted. The reported
values are taken from the ACL wiki page on the state of
the art for SAT analogy questions6. The algorithm pro-
posed by Turney (2008) is labeled as Turney-PairClass.
35
40
45
50
55
60
65
Perc
enta
ge of
 corr
ect a
nswe
rs
BagPack
?ukWaCMangalat
h et al
Veale?K
NOW?BE
ST
Bicici an
d Yuret
BagPack
?Yahoo
Turney a
nd Littma
n
Turney?P
airClassTurney?P
ERTTurney?L
RA
Figure 1: Comparison with previous algorithms on
SAT analogy questions.
Overall, the performance of BagPack is not at the
level of the state of the art but still provides a reasonable
level even in the v1v2 only condition for which we do
not utilize the contexts where the two words co-occur.
This aspect is most striking for ukWaC where the cov-
erage is low and by only utilizing the single-occurrence
sub-vectors we obtain a performance of 38.2% cor-
rect answers (the comparable ?attributional? models re-
6See http://aclweb.org/aclwiki/ for further
information and references
37
ported in Turney, 2006, have an average performance of
31%).
4.2 TOEFL
For the v1,2 sub-vector calculated for Yahoo, we have
two partial questions out of 80 and the system answers
80.0% of the questions correctly. The single occur-
rence case v1v2 instead provides a correct percentage
of 41.2% which is significantly above the random per-
formance of 25% but still very poor. The combined
case v1v2v1,2 provides a score of 75.0% with no sta-
tistically significant difference from the v1,2 case. The
reason of the low performance for v1v2 is an open
question.
For ukWaC, the coverage for the v1v2 case is pretty
low. Out of 320 pairs, 70 were represented by zero-
vectors, resulting in 34 partial questions out of 80.
The performance is at 33.8%. The v1v2 case on its
own does not lead to a performance better than random
guessing (27.5%) but the combined case v1v2v1,2
provides the highest ukWaC score of 42.5%.
Condition Yahoo ukWaC
v1,2 80.0% 33.8%
v1v2 41.2% 27.5%
v1v2v1,2 75.0% 42.5%
Table 3: Percentage of correctly answered questions in
TOEFL synonym task, worst-case scenario.
To our knowledge, the best performance with a
purely corpus-based approach is that of Rapp (2003)
who obtained a score of 92.5% with SVD. Fig. 2 re-
ports our results and a list of other corpus-based sys-
tems which achieve scores higher than 70%, along with
95% confidence interval values. The results are taken
from the ACL wiki page on the state of the art for
TOEFL synonym questions.
30
40
50
60
70
80
90
100
Perc
enta
ge of
 corr
ect a
nswe
rs
BagPack
?ukWaC
Pado and
 LapataTurney?P
MI?IR
Turney?P
airClass
BagPack
?Yahoo
Terra an
d Clarke
Bullinaria
 and Lev
y
Matveev
a et al Rapp
Figure 2: Comparison with previous algorithms on
TOEFL synonym questions with 95% confidence in-
tervals.
We note that our results obtained for Yahoo are com-
parable to the results of Turney but even the best re-
sults obtained for ukWaC and the Yahoo?s results for
v1v2 only condition are very poor. Whether this is
because of the inability of the sub-vectors to capture
synonymity or because the default parameter values of
SVM are not adequate is an open question. Notice that
our concatenated v1v2 vector does not exploit infor-
mation about the similarity of v1 to v2, that, presum-
ably, should be of great help in solving the synonym
task.
4.3 Selectional Preference
The coverage for this dataset is quite high. All pairs
were represented by non-zero vectors for Yahoo while
only two pairs had zero-vectors for ukWaC. The two
pairs are discarded in our experiments. For Yahoo, the
best results are obtained for the v1,2 case. The single-
occurrence case, v1v2, provides an overall correlation
of 0.36 and mean correlation of 0.26. However low, in
case of rarely co-occurring word pairs this data could
be the only data we have in our hands and it is impor-
tant that it provides reasonable judgment estimates.
For the ukWaC corpus, the best results we get are
an overall correlation of 0.60 and a mean correlation of
0.52 for the combined case v1v2v1,2. The results for
v1,2 and v1v2v1,2 are statistically indistinguishable.
Yahoo ukWaC
Condition Overall Mean Overall Mean
v1,2 0.60 0.45 0.58 0.48
v1v2 0.36 0.26 0.33 0.22
v1v2v1,2 0.55 0.42 0.60 0.52
Table 4: Spearman correlations between the targets and
estimations for selectional preference task.
In Fig. 3, we present a comparison of our results with
some previous studies reported in Pad? et al (2007).
The best result reported so far is a correlation of 0.52.
Our results for Yahoo and ukWaC are currently the
highest correlation values reported. Even the verb-
effect-controlled correlations achieve competitive per-
formance.
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
Spea
rman
 corr
elatio
n
Resnik
BagPack
?Yahoo (m
ean)
Pado, Pa
do, & Erk
 (parsed co
sine)
BagPack
?ukWaC 
(mean)
Pado, Ke
ller, & Cr
ocker
BagPack
?Yahoo (ov
erall)
BagPack
?ukWaC 
(overall)
Figure 3: Comparison of algorithms on selectional
preference task.
38
4.4 ConceptNet
Only for this task, (because of practical memory limita-
tions) we reduced the model parameter b to 500, which
means we used the 500 most frequent unigrams and
500 most frequent bigrams as our basis terms. For each
of the 5 relations at our hand, we trained a different
SVM classifier by labeling the pairs with the corre-
sponding relation as positive and the rest as negative.
To eliminate the issue of unbalanced number of nega-
tive and positive instances we randomly down-sampled
the positive or negative instances set (whichever is
larger). For the IsA, UsedFor, CapableOf, and PartOf
relations, the down-sampling procedure means keep-
ing some of the negative instances out of the training
and test sets while for the LocationOf relation it means
keeping a subset of the positive instances out. We per-
formed 5 iterations of the down-sampling procedure
and for each iteration we carried out a 10-fold cross-
validation to train and test our classifier. The results are
test set averages over all iterations and folds. The per-
formance measure we use is the area under the receiver
operating characteristic (AUC in short for area under
the curve). The AUC of a classifier is the area under the
curve defined by the corresponding true positive rate
and false positive rate values obtained for varying the
threshold of the classifier to accept an instance as posi-
tive. Intuitively, AUC is the probability that a randomly
picked positive instance?s estimated posterior probabil-
ity is higher than a randomly picked negative instance?s
estimated posterior probability (Fawcett, 2006).
The coverage is quite high for both corpora: Out of
1943 pairs,only 3 were represented by a zero-vector in
Yahoo while in ukWaC this number is 68. For sim-
plicity, we discarded missing pairs from our analysis.
We report only the results obtained for the entire co-
occurrence matrix. The results are virtually identi-
cal for the other conditions too: Both for Yahoo and
ukWaC, almost all of the AUC values obtained for all
relations and for all conditions are above 95%. Only
the PartOf relation has AUC values above 90% (which
is still a very good result).
Relation Yahoo ukWaC
IsA 99.0% 98.0%
UsedFor 98.2% 98.5%
CapableOf 98.9% 99.1%
PartOf 97.6% 95.0%
LocationOf 99.0% 98.8%
Table 5: AUC scores for 5 relations of ConceptNet,
classifier trained for v1v2v1,2 condition.
The very high performance we observe for the Con-
ceptNet task is surprising when compared to the mod-
erate performance we observe for other tasks. Our ex-
tensive filtering of the assertions could have resulted
in a biased dataset which might have made the job of
the classifier easy while reducing its generalization ca-
pacity. To investigate this, we decided to use the pairs
coming from the SAT task as a validation set.
Again, we trained an SVM classifier on the Concept-
Net data for each of the 5 relations like we did previ-
ously, but this time without cross-validation (i.e. after
the down-sampling, we used the entire set as the train-
ing dataset in each iteration). Then we evaluated the
classifiers on the 2224 pairs of the SAT analogy task
(removing pairs that were in the training data) and av-
eraged the posterior probability reported by each SVM
over each down-sampling iteration. The 5 pairs which
are assigned the highest posterior probability for each
relation are reported in Table 6. We have not yet quan-
tified the performance of BagPack in this task but the
preliminary results in this table are, qualitatively, ex-
ceptionally good.
5 Conclusions
We presented a general way to build a vector-based
space to represent the semantic relations between word
pairs and showed how that representation can be used
to solve various tasks involving semantic similarity.
For SAT and TOEFL, we obtained reasonable perfor-
mances comparable to the state of the art. For the es-
timation of selective preference judgments about verb-
noun pairs, we achieved state of the art performance.
Perhaps more importantly, our representation format
allows us to provide meaningful estimates even when
the verb and noun are not observed co-occurring in the
corpus ? which is an obvious advantage over the mod-
els which rely on sytagmatic contexts alone and cannot
provide estimates for word pairs that are not seen di-
rectly co-occurring. We also obtained very promising
results for the automated augmentation of ConceptNet.
The generality of the proposed method is also re-
flected in the fact that we built a single feature space
based on frequent basis terms and used the same fea-
tures for all pairs coming from different tasks. The
use of the same feature set for all pairs makes it pos-
sible to build a single database of word-pair vectors.
For example, we were able to re-use the vectors con-
structed for SAT pairs as a validation set in the Con-
ceptNet task. Furthermore, the results reported here are
obtained for the same machine learning model (SVM)
without any parameter tweaking, which renders them
very strict lower bounds.
Another contribution is that the proposed method
provides a way to represent the relations between
words even if they are not observed co-occurring in the
corpus. Employing a larger corpus can be an alternative
solution for some cases but this is not always possible
and some tasks, like estimating selectional preference
judgments, inherently call for a method that does not
exclusively depends on paired co-occurrence observa-
tions.
Finally, we introduced ConceptNet, a common-sense
semantic network, to the corpus-based semantics com-
munity, both as a new challenge and as a repository we
39
Rank IsA UsedFor PartOf CapableOf LocationOf
1 watch,timepiece pencil,draw vehicle,wheel motorist,drive spectator,arena
2 emerald,gem blueprint,build spider,leg volatile,vaporize water,riverbed
3 cherry,fruit detergent,clean keyboard,finger concrete,harden bovine,pasture
4 dinosaur,reptile guard,protect train,caboose parasite,contribute benediction,church
5 ostrich,bird buttress,support hub,wheel immature,develop byline,newspaper
Table 6: Top 5 SAT pairs classified as positive for ConceptNet relations, classifier trained for v1v2v1,2 condition.
can benefit from.
In future work, one of the most pressing issue we
want to explore is how to better exploit the informa-
tion in the single occurrence vectors: currently, we do
not make any use of the overlap between v1 and v2.
In this way, we are missing the classic intuition that
taxonomically similar words tend to occur in similar
contexts, and it is thus not surprising that v1v2 flunks
the TOEFL. We are currently looking at ways to aug-
ment our concatenated vector with ?meta-information?
about vector overlap.
References
S. Canu, Y. Grandvalet, V. Guigue and A. Rakotoma-
monjy. 2005. SVM and Kernel Methods Matlab
Toolbox, Perception Syst?mes et Information, INSA
de Rouen, Rouen, France
N. Chomsky. 1957. Syntactic structures. Mouton, The
Hague.
R. P. W. Duin. 2001. PRTOOLD (Version 3.1.7),
A Matlab toolbox for pattern recognition. Pattern
Recognition Group. Delft University of Technology.
K. Erk. 2007. A simple, similarity-based model for
selectional preferences. Proceedings of ACL 2007.
K. Erk and S. Pad?. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP 2008.
I. Eslick. 2006. Searching for commonsense. Master?s
thesis, Massachusetts Institute of Technology.
T. Fawcett. 2006. An introduction to roc analysis.
Pattern Recogn. Lett., 27(8):861?874.
C. Havasi, R. Speer and J. Alonso. 2007. Concept-
net 3: a flexible, multilingual semantic network for
common sense knowledge. In Recent Advances in
Natural Language Processing, Borovets, Bulgaria,
September.
C.-W. Hsu, C.-C Chang. 2003. A practical guide
to support vector classification. Technical report,
Department of Computer Science, National Taiwan
University.
T.K. Landauer and S.T. Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2): 211?
240.
H. Liu and P. Singh. 2004. ConceptNet ? A practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4) 211?226.
S. Mirkin, I. Dagan and M. Geffet. 2006. Integrat-
ing pattern-based and distributional similarity meth-
ods for lexical entailment acquisition. Proceedings
of COLING/ACL 2006, 579?586.
S. Pad?, S. Pad? and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
Proceedings EMNLP 2007, 400?409.
U. Pad?. 2007. The Integration of Syntax and Semantic
Plausibility in a Wide-Coverage Model of Sentence
Processing. Ph.D. thesis, Saarland University.
P. Pantel and M. Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. Proceedings of COL-
ING/ACL 2006, 113?120.
R. Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. Proceedings of MT Summit
IX: 315?322.
M. Sahlgren. 2006. The Word-space model. Ph.D. dis-
sertation, Stockholm University, Stockholm.
G. Salton and C. Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information
Processing and Management, 24(5): 513?523.
R. Speer, C. Havasi and H. Lieberman. 2008. Anal-
ogyspace: Reducing the dimensionality of common
sense knowledge. In Dieter Fox and Carla P. Gomes,
editors, AAAI, pages 548?553. AAAI Press.
P. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379?416.
P. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. Proceedings
of COLING 2008, 905?912.
40
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 50?53,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Measuring semantic relatedness with vector space models and random
walks
Amac? Herda
?
gdelen
Center for Mind/Brain Sciences
University of Trento
amac@herdagdelen.com
Katrin Erk
Linguistics Department
University of Texas at Austin
katrin.erk@mail.utexas.edu
Marco Baroni
Center for Mind/Brain Sciences
University of Trento
marco.baroni@unitn.it
Abstract
Both vector space models and graph ran-
domwalk models can be used to determine
similarity between concepts. Noting that
vectors can be regarded as local views of
a graph, we directly compare vector space
models and graph random walk models on
standard tasks of predicting human simi-
larity ratings, concept categorization, and
semantic priming, varying the size of the
dataset from which vector space and graph
are extracted.
1 Introduction
Vector space models, representing word mean-
ings as points in high-dimensional space, have
been used in a variety of semantic relatedness
tasks (Sahlgren, 2006; Pad?o and Lapata, 2007).
Graphs are another way of representing relations
between linguistic entities, and they have been
used to capture semantic relatedness by using both
corpus-based evidence and the graph structure of
WordNet and Wikipedia (Pedersen et al, 2004;
Widdows and Dorow, 2002; Minkov and Cohen,
2008). We study the relationship between vec-
tor space models and graph random walk mod-
els by embedding vector space models in graphs.
The flexibility offered by graph randomwalk mod-
els allows us to compare the vector space-based
similarity measures to extended notions of relat-
edness and similarity. In particular, a random
walk model can be viewed as smoothing direct
similarity between two vectors using second-order
and even higher-order vectors. This view leads
to the second focal point of this paper: We in-
vestigate whether random walk models can sim-
ulate the smoothing effects obtained by methods
like Singular Value Decomposition (SVD). To an-
swer this question, we compute models on reduced
(downsampled) versions of our dataset and evalu-
ate the robustness of random walk models, a clas-
sic vector-based model, and SVD-based models
against data sparseness.
2 Model definition and implementation
We use directed graphs with weighted edges, G =
(V,E,w) where V is a set of nodes, E = V ? V
is a set of edges and w : E ? R is the weight-
ing function on edges. For simplicity, we assume
that G is fully connected, edges with zero weights
can be considered as non-existing in the graph. On
these graphs, we perform random walks with an
initial probability distribution q over the nodes (a
1 ? |V | vector). We then follow edges with prob-
ability proportional to their weights, so that the
probability of walking from node v
1
to node v
2
is w(v
1
, v
2
)/
?
v
w(v
1
, v). A fixed length random
walk ends after a predetermined number of steps.
In flexible walks, there is a constant probability ?
of stopping at each step. Thus, walk length fol-
lows a geometric distribution with parameter ?,
the probability of a walk of length k is ?(1??)
k?1
and the expected walk length is 1/?. For example,
a flexible walk with ? = 1/2 will produce 1-step,
2-step, and higher-step walks while the expected
average length is 2.
Relating vectors and graphs. Corpus co-
occurrence (e
1
, e
2
, a
12
) of two entities e
1
and e
2
that co-occur with (potentially transformed) count
a
12
can be represented in either a vector or a graph.
In a vector, it corresponds to a dimension value of
a
12
for the dimension e
2
of entity e
1
. In a graph,
it corresponds to two nodes labeled e
1
and e
2
con-
nected by an edge with weight a
12
.
Similarity measures. Let R(q) = p denote a
specific random walk process which transforms an
50
initial probability distribution q to a final prob-
ability distribution p over the nodes. We write
q(m) for the probability assigned to the node m
under q. If the initial distribution q concentrates
all probability on a single node n, i.e., q(n) = 1
and q(x) = 0 for all nodes x 6= n, we write
pr(n ? m) for the probability p(m) of ending
up at node m.
The simplest way of measuring relatedness
through random walks is to consider the probabil-
ity p(m) of a single node m as an endpoint for a
walk starting with start probability distribution q,
that is, p = R(q). We call this a direct, one-
direction measure of relatedness between q and
m. Direct, one-direction measures are typically
asymmetric. In case all start probability is con-
centrated on a single node n, we can also consider
direct, two-direction measures, which will be a
combination of pr(m ? n) and pr(n ? m). The
point of using two-direction measures is that these
can be made symmetric, which is an advantage
when we are modeling undirected semantic sim-
ilarity. In the experiments below we focus on the
average of the two probabilities.
In addition to direct measures, we will use in-
direct measures, in which we compute the relat-
edness of endpoint probability distributions p
1
=
R(q
1
) and p
2
= R(q
2
). As endpoint distribu-
tions can be viewed both as probability distribu-
tions and as vectors, we used three indirect mea-
sures: 1) Jensen/Shannon divergence, a symmet-
ric variant of the Kullback/Leibler divergence be-
tween probability distributions. 2) cosine similar-
ity, and 3) dot product. Dot product is a natural
choice in a graph setting because we can view it as
the probability of a pair of walks, one starting at a
node determined by q
1
and the other starting at a
node governed by q
2
, ending at the same node.
Discussion. Direct and indirect relatedness mea-
sures together with variation in walk length give us
a simple, powerful and flexible way to capture dif-
ferent kinds of similarity (with traditional vector-
based approach as a special case). Longer walks
or flexible walks will capture higher order effects
that may help coping with data sparseness, similar
to the use of second-order vectors. Dimensionality
reduction techniques like Singular Value Decom-
position (SVD) also capture these higher-order ef-
fects, and it has been argued that that makes them
more resistant against sparseness (Sch?utze, 1997).
To our knowledge, no systematic comparison of
SVD and classical vector-based methods has been
done on different corpus sizes. In our experiments,
we will compare the performance of SVD and
flexible-walk smoothing at different corpus sizes
and for a variety of tasks.
Implementation: We extract tuples from the 2-
billion word ukWaC corpus,
1
dependency-parsed
with MINIPAR.
2
Following Pad?o and Lapata
(2007), we only consider co-occurrences where
two target words are connected by certain de-
pendency paths, namely: the top 30 most fre-
quent preposition-mediated noun-to-noun paths
(soldier+with+gun), the top 50 transitive-verb-
mediated noun-to-noun paths (soldier+use+gun),
the top 30 direct or preposition-mediated verb-
noun paths (kill+obj+victim, kill+in+school), and
the modifying and predicative adjective-to-noun
paths. Pairs (w
1
, w
2
) that account for 0.01%
or less of the marginal frequency of w
1
were
trimmed. The resulting tuple list, with raw counts
converted to mutual information scores, contains
about 25 million tuples.
To test how well graph-based and alternative
methods ?scale down? to smaller corpora, we sam-
pled random subsets of tuples corresponding to
0.1%, 1%, 10%, and 100% of the full list. To put
things into perspective, the full list was extracted
from a corpus of about 2 billion words; so, the
10% list is on the order of magnitude of the BNC,
and the 0.1% list is on the order of magnitude of
the Brown corpus. From each of the 4 resulting
datasets, we built one graph and two vector space
models: one space with full dimensionality, and
one space reduced to 300 dimensions using singu-
lar value decomposition.
3 Experiments
First, we report the results for all tasks obtained on
the full data-set and then proceed with the compar-
ison of different models on differing graph sizes
to see the robustness of the models against data
sparseness.
Human similarity ratings: We use the dataset
of Rubenstein and Goodenough (1965), consist-
ing of averages of subject similarity ratings for
65 noun pairs. We use the Pearson?s coefficient
between estimates and human judgments as our
performance measure. The results obtained for
1
http://wacky.sslmit.unibo.it
2
http://www.cs.ualberta.ca/
?
lindek/
minipar.htm
51
Direct (average) Vector (cosine) Indirect (dot product) Previous
0.5 1 2 svd vector 0.5 1 2
RG 0.409 0.326 0.571 0.798 0.689 0.634 0.673 0.400 BL: 0.70
CLW: 0.849
AAMP Purity 0.480 0.418 0.669 0.701 0.704 0.664 0.667 0.612 AP: 0.709
RS: 0.791
Hodgson
synonym 2, 563 1.289 5, 408
??
10.015
??
6, 623
??
5, 462
??
5, 954
??
5, 537
??
coord 4, 275
??
3, 969
??
6, 319
??
11.157
??
7, 593
??
8, 466
??
8, 477
??
4, 854
??
antonym 2, 853? 2, 237 5, 319
??
7, 724
??
5, 455
??
4, 589
??
4, 859
??
6, 810
??
conass 9, 209
??
10.016
??
5, 889
??
9, 299
??
6, 950
??
5, 993
??
5, 455
??
4, 994
??
supersub 4, 038
??
4, 113
??
6, 773
??
10.422
??
7, 901
??
6, 792
??
7, 165
??
4, 828
??
phrasacc 4, 577
??
4, 718
??
2, 911
?
3, 532
?
3, 023
?
3, 506
?
3, 612
?
1.038
Table 1: All datasets. * (**) indicates significance level p < 0.01 (p < 0.001). BL: (Baroni and Lenci,
2009), CLW: (Chen et al, 2006), AP: (Almuhareb, 2006), RS: (Rothenh?ausler and Sch?utze, 2009)
0.1% 1% 10%
cos svd cos vector dot 2 cos svd cos vector dot 2 cos svd cos vector dot 2
RG 0.219 0.244 0.669 0.676 0.700 1.159 0.911 0.829 1.068
AAMP 0.379 0.339 0.366 0.723 0.622 0.634 0.923 0.886 0.948
Synonym 0.369 0.464 0.610 0.493 0.590 0.833 0.857 0.770 1.081
Antonym 0.449 0.493 0.231 0.768 0.585 0.730 1.044 0.849 0.977
Conass 0.187 0.260 0.261 0.451 0.498 0.942 0.857 0.704 1.062
Coord 0.282 0.362 0.456 0.527 0.570 1.050 0.927 0.810 1.187
Phrasacc 0.268 0.132 0.761 0.849 0.610 1.215 0.920 0.868 1.049
Supersub 0.313 0.353 0.285 0.645 0.601 1.029 0.936 0.752 1.060
Table 2: Each cell contains the ratio of the performance of the corresponding model for the corresponding
downsampling ratio to the performance of the same model on the full graph. The higher ratio means the
less deterioration due to data sparseness.
the full graph are in Table 1, line 1. The SVD
model clearly outperforms the pure-vector based
approach and the graph-based approaches. Its per-
formance is above that of previous models trained
on the same corpus (Baroni and Lenci, 2009). The
best model that we report is based on web search
engine results (Chen et al, 2006). Among the
graph-based random walk models, flexible walk
with parameter 0.5 and fixed 1-step walk with in-
direct relatedness measures using dot product sim-
ilarity achieve the highest performance.
Concept categorization: Almuhareb (2006) pro-
posed a set of 402 nouns to be categorized into
21 classes of both concrete (animals, fruit. . . ) and
abstract (feelings, times. . . ) concepts. Our results
on this clustering task are given in Table 1 (line
2). The difference between SVD and pure-vector
models is negligible and they both obtain the best
performance in terms of both cluster entropy (not
shown in the table) and purity. Both models? per-
formances are comparable with the previously re-
ported studies, and above that of random walks.
Semantic priming: The next dataset comes
from Hodgson (1991) and it is of interest since
it requires capturing different forms of seman-
tic relatedness between prime-target pairs: syn-
onyms (synonym), coordinates (coord), antonyms
(antonym), free association pairs (conass), super-
and subordinate pairs (supersub) and phrasal as-
sociates (phrasacc). Following previous simula-
tions of this data-set (Pad?o and Lapata, 2007), we
measure the similarity of each related target-prime
pair, and we compare it to the average similar-
ity of the target to all the other primes instanti-
ating the same relation, treating the latter quan-
tity as our surrogate of an unrelated target-prime
pair. We report results in terms of differences be-
tween unrelated and related pairs, normalized to
t-scores, marking significance according to two-
tailed paired t-tests for the relevant degrees of free-
dom. Even though the SVD-based and pure-vector
models are among the top achievers in general, we
see that in different tasks different random walk
models achieve comparable or even better perfor-
mances. In particular, for phrasal associates and
conceptual associates, the best results are obtained
by random walks based on direct measures.
3.1 Robustness against data sparseness
So far, we reported only the results obtained on
the full graph. However, in order to see the re-
sponse of the models to using smaller corpora
52
we ran another set of experiments on artificially
down-sampled graphs as explained above. In this
case, we are not interested in the absolute perfor-
mance of the models per se but the relative per-
formance. Thus, for ease of comparison we fixed
each model?s performance on the full graph to 1
for each task and linearly scaled its performance
on smaller graphs. For example saying that the
SVD-based model achieves a score of 0.911 on
10% graph for the Rubenstein and Goodenough
dataset means that the ratio of the performance
of SVD-based model on 10% graph to the per-
formance of the same model on the full graph is
0.911. The results are given in Table 2, where the
only random walk model we report is dot 2, i.e., a
2-step random walk coupled with the dot product-
based indirect measure. This is by far the random
walk model most robust to downsampling. In the
10% graph, we see that on all tasks but one, dot 2
is the model least affected by the data reduction.
On the contrary, down-sampling has a positive ef-
fect on this model because on 6 tasks, it actually
performs better than it does on the full graph! The
same behavior is also observed on the 1% graph
- as an example, for phrasal associates relations,
dot 2 performance increases by a factor of around
1.2 when we use one hundredth of the graph in-
stead of the full one. For the smallest graph we
used, 0.1%, still dot 2 provides the highest relative
performance in 5 out of the 8 tasks.
4 Conclusion
We compared graph-based random walk models
and vector models. For this purpose, we showed
how corpus co-occurrences could be represented
both as a graph and a vector, and we identified
two different ways to calculate relatedness based
on the outcomes of random walks, by direct and
indirect measures. The experiments carried out
on 8 different tasks by using the full graph re-
vealed that SVD-based model performs very well
across all types of semantic relatedness. How-
ever, there is also evidence that -depending on
the particular relation- some random walk models
can achieve results as good as or even better than
those of SVD-based models. Our second ques-
tion was whether the random walk models would
be able to simulate the smoothing effects obtained
by SVD. While answering this question, we also
carried out a systematic comparison of plain and
SVD-based models on different tasks with differ-
ent sizes of data. One interesting result is that an
SVD-based model is not necessarily more robust
to data sparseness than the plain vector model.
The more interesting result is that a 2-step ran-
dom walk model, based on indirect measures with
dot product, consistently outperforms both SVD-
based and plain vector models in terms of relative
performance, thus it is able to achieve compara-
ble results on very small datasets. Actually, the
improvement on absolute performance measures
of this random walk by making the dataset even
smaller calls for future research.
References
A. Almuhareb. 2006. Attributes in lexical acquisition.
Dissertation, University of Essex.
M. Baroni and A. Lenci. 2009. One distributional
memory, many semantic spaces. In Proceedings of
GEMS, Athens, Greece.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In Proceedings of ACL, pages
1009?16.
J. Hodgson. 1991. Informational constraints on pre-
lexical priming. Language and Cognitive Processes,
6:169?205.
Einat Minkov and William W. Cohen. 2008. Learn-
ing graph walk based similarity measures for parsed
text. In Proceedings of EMNLP?08.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity: Measuring the relatedness of
concepts. In Proceedings of NAACL, pages 38?41.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of GEMS, pages 17?
24.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
M. Sahlgren. 2006. The Word-Space Model. Disserta-
tion, Stockholm University.
H. Sch?utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford.
Dominic Widdows and Beate Dorow. 2002. A
graph model for unsupervised lexical acquisition.
In 19th International Conference on Computational
Linguistics, pages 1093?1099.
53
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183?1193,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Nouns are vectors, adjectives are matrices:
Representing adjective-noun constructions in semantic space
Marco Baroni and Roberto Zamparelli
Center for Mind/Brain Sciences, University of Trento
Rovereto (TN), Italy
{marco.baroni,roberto.zamparelli}@unitn.it
Abstract
We propose an approach to adjective-noun
composition (AN) for corpus-based distribu-
tional semantics that, building on insights
from theoretical linguistics, represents nouns
as vectors and adjectives as data-induced (lin-
ear) functions (encoded as matrices) over
nominal vectors. Our model significantly out-
performs the rivals on the task of reconstruct-
ing AN vectors not seen in training. A small
post-hoc analysis further suggests that, when
the model-generated AN vector is not simi-
lar to the corpus-observed AN vector, this is
due to anomalies in the latter. We show more-
over that our approach provides two novel
ways to represent adjective meanings, alter-
native to its representation via corpus-based
co-occurrence vectors, both outperforming the
latter in an adjective clustering task.
1 Introduction
An influential approach for representing the mean-
ing of a word in NLP is to treat it as a vector
that codes the pattern of co-occurrence of that word
with other expressions in a large corpus of language
(Sahlgren, 2006; Turney and Pantel, 2010). This
approach to semantics (sometimes called distribu-
tional semantics) naturally captures word cluster-
ing, scales well to large lexicons and doesn?t re-
quire words to be manually disambiguated (Schu?tze,
1997). However, until recently it has been limited to
the level of content words (nouns, adjectives, verbs),
and it hasn?t tackled in a general way compositional-
ity (Frege, 1892; Partee, 2004), that crucial property
of natural language which allows speakers to de-
rive the meaning of a complex linguistic constituent
from the meaning of its immediate syntactic subcon-
stituents.
Formal semantics (FS), the research program
stemming from Montague (1970b; 1973), has oppo-
site strengths and weaknesses. Its core semantic no-
tion is the sentence, not the word; at the lexical level,
it focuses on the meaning of function words; one
of its main goals is to formulate recursive composi-
tional rules that derive the quantificational properties
of complex sentences and their antecedent-pronoun
dependencies.
Given its focus on quantification, FS treats the
meanings of nouns and verbs as pure extensions:
nouns and (intransitive) verbs are properties, and
thus denote sets of individuals. Adjectives are also
often assumed to denote properties: in this view
redadj would be the set of ?entities which are red?,
plasticadj , the set of ?objects made of plastic?, and
so forth. In the simplest case, the meaning of an at-
tributive adjective-noun (AN) constituent can be ob-
tained as the intersection of the adjective and noun
extensions A?N:
[ red car ] = {. . . red objects. . . } ? {. . . cars. . . }
However, the intersective method of combination
is well-known to fail in many cases (Kamp, 1975;
Montague, 1970a; Siegel, 1976): for instance, a
fake gun is not a gun. Even for red, the manner in
which the color combines with a noun will be dif-
ferent in red Ferrari (the outside), red watermelon
(the inside), red traffic light (the signal). These prob-
lems have prompted a more flexible FS representa-
tion for attributive adjectives ? functions from the
meaning of a noun onto the meaning of a modified
noun (Montague, 1970a). This mapping could now
be sensitive to the particular noun the adjective re-
ceives, and it does not need to return a subset of the
1183
original noun denotation (as in the case of fake N).
However, FS has nothing to say on how these func-
tions should be constructed.
In the last few years there have been attempts to
build compositional models that use distributional
semantic representations as inputs (see Section 2 be-
low), most of them focusing on the combination of a
verb and its arguments. This paper addresses instead
the combination of nouns and attributive adjectives.
This case was chosen as an interesting testbed be-
cause it has the property of recursivity (it applies in
black dog, but also in large black dog, etc.), and be-
cause very frequent adjectives such as different are
at the border between content and function words.
Following the insight of FS, we treat attributive ad-
jectives as functions over noun meanings; however,
noun meanings are vectors, not sets, and the func-
tions are learnt from corpus-based noun-AN vector
pairs.
Original contribution We propose and evaluate a
new method to derive distributional representations
for ANs, where an adjective is a linear function from
a vector (the noun representation) to another vector
(the AN representation). The linear map for a spe-
cific adjective is learnt, using linear regression, from
pairs of noun and AN vectors extracted from a cor-
pus.
Outline Distributional approaches to composi-
tionality are shortly reviewed in Section 2. In Sec-
tion 3, we introduce our proposal. The experimen-
tal setting is described in Section 4. Section 5 pro-
vides some empirical justification for using corpus-
harvested AN vectors as the target of our function
learning and evaluation benchmark. In Section 6, we
show that our model outperforms other approaches
at the task of approximating such vectors for unseen
ANs. In Section 7, we discuss how adjectival mean-
ing can be represented in our model and evaluate this
representation in an adjective clustering task. Sec-
tion 8 concludes by sketching directions for further
work.
2 Related work
The literature on compositionality in vector-based
semantics encompasses various related topics, some
of them not of direct interest here, such as how to
encode word order information in context vectors
(Jones and Mewhort, 2007; Sahlgren et al, 2008)
or sophisticated composition methods based on ten-
sor products, quantum logic, etc., that have not yet
been empirically tested on large-scale corpus-based
semantic space tasks (Clark and Pulman, 2007;
Rudolph and Giesbrecht, 2010; Smolensky, 1990;
Widdows, 2008). Closer to our current purposes is
the general framework for vector composition pro-
posed by Mitchell and Lapata (2008), subsuming
various earlier proposals. Given two vectors u and
v, they identify two general classes of composition
models, (linear) additive models:
p = Au+Bv (1)
where A and B are weight matrices, and multiplica-
tive models:
p = Cuv
where C is a weight tensor projecting the uv tensor
product onto the space of p. Mitchell and Lapata de-
rive two simplified models from these general forms.
Their simplified additive model p = ?u+?v was a
common approach to composition in the earlier liter-
ature, typically with the scalar weights set to 1 or to
normalizing constants (Foltz et al, 1998; Kintsch,
2001; Landauer and Dumais, 1997). Mitchell and
Lapata also consider a constrained version of the
multiplicative approach that reduces to component-
wise multiplication, where the i-th component of
the composed vector is given by: pi = uivi. The
simplified additive model produces a sort of (sta-
tistical) union of features, whereas component-wise
multiplication has an intersective effect. They also
evaluate a weighted combination of the simplified
additive and multiplicative functions. The best re-
sults on the task of paraphrasing noun-verb combi-
nations with ambiguous verbs (sales slump is more
like declining than slouching) are obtained using the
multiplicative approach, and by weighted combina-
tion of addition and multiplication (we do not test
model combinations in our current experiments).
The multiplicative approach also performs best (but
only by a small margin) in a later application to lan-
guage modeling (Mitchell and Lapata, 2009). Erk
and Pado? (2008; 2009) adopt the same formalism
but focus on the nature of input vectors, suggest-
ing that when a verb is composed with a noun, the
noun component is given by an average of verbs that
the noun is typically object of (along similar lines,
1184
Kintsch (2001) also focused on composite input vec-
tors, within an additive framework). Again, the mul-
tiplicative model works best in Erk and Pado??s ex-
periments.
The above-mentioned researchers do not exploit
corpus evidence about the p vectors that result from
composition, despite the fact that it is straightfor-
ward (at least for short constructions) to extract
direct distributional evidence about the composite
items from the corpus (just collect co-occurrence
information for the composite item from windows
around the contexts in which it occurs). The
main innovation of Guevara (2010), who focuses on
adjective-noun combinations (AN), is to use the co-
occurrence vectors of observed ANs to train a su-
pervised composition model (we became aware of
Guevara?s approach after we had developed our own
model, that also exploits observed ANs for training).
Guevara adopts the full additive composition form
from Equation (1) and he estimates the A and B
weights using partial least squares regression. The
training data are pairs of adjective-noun vector con-
catenations, as input, and corpus-derived AN vec-
tors, as output. Guevara compares his model to
the simplified additive and multiplicative models of
Mitchell and Lapata. Observed ANs are nearer, in
the space of observed and predicted test set ANs, to
the ANs generated by his model than to those from
the alternative approaches. The additive model, on
the other hand, is best in terms of shared neighbor
count between observed and predicted ANs.
In our empirical tests, we compare our approach
to the simplified additive and multiplicative models
of Mitchell and Lapata (the former with normaliza-
tion constants as scalar weights) as well as to Gue-
vara?s approach.
3 Adjectives as linear maps
As discussed in the introduction, we will take ad-
jectives in attributive position to be functions from
one noun meaning to another. To start simple, we
assume here that adjectives in the attributive posi-
tion (AN) are linear functions from n-dimensional
(noun) vectors onto n-dimensional vectors, an oper-
ation that can be expressed as multiplication of the
input noun column vector by a n ? n matrix, that
is our representation for the adjective (in the lan-
guage of linear algebra, an adjective is an endomor-
phic linear map in noun space). In the framework of
Mitchell and Lapata, our approach derives from the
additive form in Equation (1) with the matrix multi-
plying the adjective vector (say, A) set to 0:
p = Bv
where p is the observed AN vector, B the weight
matrix representing the adjective at hand, and v a
noun vector. In our approach, the weight matrix B is
specific to a single adjective ? as we will see in Sec-
tion 7 below, it is our representation of the meaning
of the adjective.
Like Guevara, we estimate the values in the
weight matrix by partial least squares regression.
In our case, the independent variables for the re-
gression equations are the dimensions of the corpus-
based vectors of the component nouns, whereas the
AN vectors provide the dependent variables. Unlike
Guevara, (i) we train separate models for each adjec-
tive (we learn adjective-specific functions, whereas
Guevara learns a generic ?AN-slot? function) and,
consequently, (ii) corpus-harvested adjective vectors
play no role for us (their values would be constant
across the training input vectors).
A few considerations are in order. First, although
we use a supervised learning method (least squares
regression), we do not need hand-annotated data,
since the target AN vectors are automatically col-
lected from the corpus just like vectors for single
words are. Thus, there is no extra ?external knowl-
edge? cost with respect to unsupervised approaches.
Second, our approach rests on the assumption that
the corpus-derived AN vectors are interesting ob-
jects that should constitute the target of what a com-
position process tries to approximate. We provide
preliminary empirical support for this assumption in
Section 5 below. Third, we have some reasonable
hope that our functions can capture to a certain ex-
tent the polysemous nature of adjectives: we could
learn, for example, a green matrix with large posi-
tive weights mapping from noun features that per-
tain to concrete objects to color dimensions of the
output vector (green chair), as well as large positive
weights from features characterizing certain classes
of abstract concepts to political/social dimensions in
the output (green initiative). Somewhat optimisti-
cally, we hope that chair will have near-0 values
1185
on the relevant abstract dimensions, like initiative
on the concrete features, and thus the weights will
not interfere. We do not evaluate this claim specif-
ically, but our quantitative evaluation in Section 6
shows that our approach does best with high fre-
quency, highly ambiguous adjectives. Fourth, the
approach is naturally syntax-sensitive, since we train
it on observed data for a specific syntactic position:
we would train separate linear models for, say, the
same adjective in attributive (AN) and predicative
(N is A) position. As a matter of fact, the current
model is too syntax-sensitive and does not capture
similarities across different constructions. Finally,
although adjective representations are not directly
harvested from corpora, we can still meaningfully
compare adjectives to each other or other words by
using their estimated matrix, or an average vector for
the ANs that contain them: both options are tested
in Section 7 below.
4 Experimental setup
4.1 Corpus
We built a large corpus by concatenating the
Web-derived ukWaC corpus (http://wacky.
sslmit.unibo.it/), a mid-2009 dump of the
English Wikipedia (http://en.wikipedia.
org) and the British National Corpus (http:
//www.natcorp.ox.ac.uk/). This concate-
nated corpus, tokenized, POS-tagged and lemma-
tized with the TreeTagger (Schmid, 1995), contains
about 2.83 billion tokens (excluding punctuation,
digits, etc.). The ukWaC and Wikipedia sections can
be freely downloaded, with full annotation, from the
ukWaC site.
We performed some of the list extraction and
checking operations we are about to describe on a
more manageable data-set obtained by selecting the
first 100M tokens of ukWaC; we refer to this subset
as the sample corpus below.
4.2 Vocabulary
We could in principle limit ourselves to collecting
vectors for the ANs to be analyzed (the AN test set)
and their components. However, to make the anal-
ysis more challenging and interesting, we populate
the semantic space where we will look at the be-
haviour of the ANs with a large number of adjectives
and nouns, as well as further ANs not in the test set.
We refer to the overall list of items we build seman-
tic vectors for as the extended vocabulary. We use
a subset of the extended vocabulary containing only
nouns and adjectives (the core vocabulary) for fea-
ture selection and dimensionality reduction, so that
we do not implicitly bias the structure of the seman-
tic space by our choice of ANs.
To construct the AN test set, we first selected 36
adjectives across various classes: size (big, great,
huge, large, major, small, little), denominal (Amer-
ican, European, national, mental, historical, elec-
tronic), colors (white, black, red, green) positive
evaluation (nice, excellent, important, appropriate),
temporal (old, recent, new, young, current), modal
(necessary, possible), plus some common abstract
antonymous pairs (difficult, easy, good, bad, spe-
cial, general, different, common). We were care-
ful to include intersective cases such as electronic
as well as non-intersective adjectives that are almost
function words (the modals, different, etc.). We ex-
tracted all nouns that occurred at least 300 times
in post-adjectival position in the sample corpus, ex-
cluding some extremely frequent temporal and mea-
sure expressions such as time and range, for a to-
tal of 1,420 distinct nouns. By crossing the selected
adjectives and nouns, we constructed a test set con-
taining 26,440 ANs, all attested in the sample cor-
pus (734 ANs per adjective on average, ranging from
1,337 for new to 202 for mental).
The core vocabulary contains the top 8K most
frequent noun lemmas and top 4K adjective lemmas
from the concatenated corpus (excluding the top 50
most frequent nouns and adjectives). The extended
vocabulary contains this core plus (i) the 26,440
test ANs, (ii) the 16 adjectives and 43 nouns that
are components of these ANs and that are not in the
core set, and (iii) 2,500 more ANs randomly sam-
pled from those that are attested in the sample cor-
pus, have a noun from the same list used for the test
set ANs, and an adjective that occurred at least 5K
times in the sample corpus. In total, the extended
vocabulary contains 40,999 entries: 8,043 nouns,
4,016 adjectives and 28,940 ANs.
4.3 Semantic space construction
Full co-occurrence matrix The 10K lemmas
(nouns, adjectives or verbs) that co-occur with
1186
the largest number of items in the core vocabu-
lary constitute the dimensions (columns) of our co-
occurrence matrix. Using the concatenated corpus,
we extract sentence-internal co-occurrence counts of
all the items in the extended vocabulary with the
10K dimension words. We then transform the raw
counts into Local Mutual Information (LMI) scores
(LMI is an association measure that closely approx-
imates the Log-Likelihood Ratio, see Evert (2005)).
Dimensionality reduction Since, for each test set
adjective, we need to estimate a regression model
for each dimension, we want a compact space with
relatively few, dense dimensions. A natural way to
do this is to apply the Singular Value Decomposi-
tion (SVD) to the co-occurrence matrix, and repre-
sent the items of interest with their coordinates in
the space spanned by the first n right singular vec-
tors. Applying SVD is independently justified be-
cause, besides mitigating the dimensionality prob-
lem, it often improves the quality of the semantic
space (Landauer and Dumais, 1997; Rapp, 2003;
Schu?tze, 1997). To avoid bias in favour of dimen-
sions that capture variance in the test set ANs, we
applied SVD to the core vocabulary subset of the
co-occurrence matrix (containing only adjective and
noun rows). The core 12K?10K matrix was re-
duced using SVD to a 12K?300 matrix. The other
row vectors of the full co-occurrence matrix (in-
cluding the ANs) were projected onto the same re-
duced space by multiplying them by a matrix con-
taining the first n right singular vectors as columns.
Merging the items used to compute the SVD and
those projected onto the resulting space, we obtain a
40,999?300 matrix representing 8,043 nouns, 4,016
adjectives and 28,940 ANs. This reduced matrix
constitutes a realistically sized semantic space, that
also contains many items that are not part of our test
set, but will be potential neighbors of the observed
and predicted test ANs in the experiments to follow.
The quality of the SVD reduction itself was indepen-
dently validated on a standard similarity judgment
data-set (Rubenstein and Goodenough, 1965), ob-
taining similar (and state-of-the-art-range) Pearson
correlations of vector cosines and human judgments
in both the original (r = .70) and reduced (r = .72)
spaces.
There are several parameters involved in con-
structing a semantic space (choice of full and re-
duced dimensions, co-occurrence span, weighting
method). Since our current focus is on alterna-
tive composition methods evaluated on a shared se-
mantic space, exploring parameters pertaining to the
construction of the semantic space is not one of our
priorities, although we cannot of course exclude that
the nature of the underlying semantic space affects
different composition methods differently.
4.4 Composition methods
In the proposed adjective-specific linear map (alm)
method, an AN is generated by multiplying an adjec-
tive weight matrix with a noun (column) vector. The
j weights in the i-th row of the matrix are the coeffi-
cients of a linear regression predicting the values of
the i-th dimension of the AN vector as a linear com-
bination of the j dimensions of the component noun.
The linear regression coefficients are estimated sep-
arately for each of the 36 tested adjectives from
the corpus-observed noun-AN pairs containing that
adjective (observed adjective vectors are not used).
Since we are working in the 300-dimensional right
singular vector space, for each adjective we have
300 regression problems with 300 independent vari-
ables, and the training data (the noun-AN pairs avail-
able for each test set adjective) range from about
200 to more than 1K items. We estimate the coef-
ficients using (multivariate) partial least squares re-
gression (PLSR) as implemented in the R pls pack-
age (Mevik and Wehrens, 2007). With respect to
standard least squares estimation, this technique is
more robust against over-training by effectively us-
ing a smaller number of orthogonal ?latent? vari-
ables as predictors (Hastie et al, 2009, Section 3.4),
and it exploits the multivariate nature of the prob-
lem (different regressions for each AN vector di-
mension to be predicted) when determining the la-
tent dimensions. The number of latent variables to
be used in the core regression are a free parameter of
PLSR. For efficiency reasons, we did not optimize it.
We picked instead 50 latent variables, by the rule-
of-thumb reasoning that for any adjective we can
use at least 200 noun-AN pairs for training, and the
independent-variable-to-training-item ratio will thus
never be above 1/4. We adopt a leave-one-out train-
ing regime, so that each target AN is generated by
an adjective matrix that was estimated from all the
1187
other ANs with the same adjective, minus the target.
We use PLSR with 50 latent variables also for
our re-implementation of Guevara?s (2010) single
linear map (slm) approach, in which a single re-
gression matrix is estimated for all ANs across ad-
jectives. The training data in this case are given
by the concatenation of the observed adjective and
noun vectors (600 independent variables) coupled
with the corresponding AN vectors (300 dependent
variables). For each target AN, we randomly sam-
ple 2,000 other adjective-noun-AN tuples for train-
ing (with larger training sets we run into memory
problems), and use the resulting coefficient matrix to
generate the AN vector from the concatenated target
adjective and noun vectors.
Additive AN vectors (add method) are obtained
by summing the corresponding adjective and noun
vectors after normalizing them (non-normalized ad-
dition was also tried, but it did not work nearly as
well as the normalized variant). Multiplicative vec-
tors (mult method) were obtained by component-
wise multiplication of the adjective and noun vec-
tors (normalization does not matter here since it
amounts to multiplying the composite vector by a
scalar, and the cosine similarity measure we use is
scale-invariant). Finally, the adj and noun baselines
use the adjective and noun vectors, respectively, as
surrogates of the AN vector.
For the add, mult, adj and noun methods, we ran
the tests of Section 6 not only in the SVD-reduced
space, but also in the original 10K-dimensional co-
occurrence space. Only the mult method achieved
better performance in the original space. We con-
jecture that this is because the SVD dimensions can
have negative values, leading to counter-intuitive re-
sults with component-wise multiplication (multiply-
ing large opposite-sign values results in large nega-
tive values). We tried to alleviate this problem by as-
signing a 0 to composite dimensions where the two
input vectors had different signs. The resulting per-
formance was better but still below that of mult in
original space. Thus, in Section 6 we report mult
results from the full co-occurrence matrix; reduced
space results for all other methods.
5 Study 1: ANs in semantic space
The actual distribution of ANs in the corpus, as
recorded by their co-occurrence vectors, is funda-
mental to what we are doing. Our method relies on
the hypothesis that the semantics of AN composi-
tion does not depend on the independent distribu-
tion of adjectives themselves, but on how adjectives
transform the distribution of nouns, as evidenced by
observed pairs of noun-AN vectors. Moreover, co-
herently with this view, our evaluation below will be
based on how closely the models approximate the
observed vectors of unseen ANs.
That our goal in modeling composition should be
to approximate the vectors of observed ANs is in
a sense almost trivial. Whether we synthesize an
AN for generation or decoding purposes, we would
want the synthetic AN to look as much as possible
like a real AN in its natural usage contexts, and co-
occurrence vectors of observed ANs are a summary
of their usage in actual linguistic contexts. However,
it might be the case that the specific resources we
used for our vector construction procedure are not
appropriate, so that the specific observed AN vectors
we extract are not reliable (e.g., they are so sparse in
the original space as to be uninformative, or they are
strictly tied to the domains of the input corpora). We
provide here some preliminary qualitative evidence
that this is in general not the case, by tapping into
our own intuitions on where ANs should be located
in semantic space, and thus on how sensible their
neighbors are.
First, we computed centroids from normalized
SVD space vectors of all the ANs that share the same
adjective (e.g., the normalized vectors of American
adult, American menu, etc., summed to construct
the American N centroid). We looked at the near-
est neighbors of these centroids in semantic space
among the 41K items (adjectives, nouns and ANs)
in our extended vocabulary (here and in all experi-
ments below, similarity is quantified by the cosine of
the angle between two vectors). As illustrated for a
random sample of 9 centroids in Table 1 (but apply-
ing to the remaining 27 adjectives as well), centroids
are positioned in intuitively reasonable areas of the
space, typically near the adjective itself or the corre-
sponding noun (the noun green near green N), proto-
typical ANs for that adjective (black face), elements
1188
related to the definition of the adjective (mental ac-
tivity, historical event, green colour, quick and little
cost for easy N), and so on.
American N black N easy N
Am. representative black face easy start
Am. territory black hand quick
Am. source black (n) little cost
green N historical N mental N
green (n) historical mental activity
red road hist. event mental experience
green colour hist. content mental energy
necessary N nice N young N
necessary nice youthful
necessary degree good bit young doctor
sufficient nice break young staff
Table 1: Nearest 3 neighbors of centroids of ANs that
share the same adjective.
How about the neighbors of specific ANs? Ta-
ble 2 reports the nearest 3 neighbors of 9 randomly
selected ANs involving different adjectives (we in-
spected a larger random set, coming to similar con-
clusions to the ones emerging from this table).
bad electronic historical
luck communication map
bad elec. storage topographical
bad weekend elec. transmission atlas
good spirit purpose hist. material
important route nice girl little war
important transport good girl great war
important road big girl major war
major road guy small war
red cover special collection young husband
black cover general collection small son
hardback small collection small daughter
red label archives mistress
Table 2: Nearest 3 neighbors of specific ANs.
The nearest neighbors of the corpus-based AN
vectors in Table 2 make in general intuitive sense.
Importantly, the neighbors pick up the composite
meaning rather than that of the adjective or noun
alone. For example, cover is an ambiguous word,
but the hardback neighbor relates to its ?front of a
book? meaning that is the most natural one in com-
bination with red. Similarly, it makes more sense
that a young husband (rather than an old one) would
have small sons and daughters (not to mention the
mistress!).
We realize that the evidence presented here is
of a very preliminary and intuitive nature. Indeed,
we will argue in the next section that there are
cases in which the corpus-derived AN vector might
not be a good approximation to our semantic in-
tuitions about the AN, and a model-composed AN
vector is a better semantic surrogate. One of the
most important avenues for further work will be to
come to a better characterization of the behaviour of
corpus-observed ANs, where they work and where
the don?t. Still, the neighbors of average and AN-
specific vectors of Tables 1 and 2 suggest that, for
the bulk of ANs, such corpus-based co-occurrence
vectors are semantically reasonable.
6 Study 2: Predicting AN vectors
Having tentatively established that the sort of vec-
tors we can harvest for ANs by directly collecting
their corpus co-occurrences are reasonable represen-
tations of their composite meaning, we move on to
the core question of whether it is possible to recon-
struct the vector for an unobserved AN from infor-
mation about its components. We use nearness to
the corpus-observed vectors of held-out ANs as a
very direct way to evaluate the quality of model-
generated ANs, since we just saw that the observed
ANs look reasonable (but see the caveats at the end
of this section). We leave it to further work to as-
sess the quality of the generated ANs in an applied
setting, for example adapting Mitchell and Lapata?s
paraphrasing task to ANs. Since the observed vec-
tors look like plausible representations of compos-
ite meaning, we expect that the closer the model-
generated vectors are to the observed ones, the better
they should also perform in any task that requires ac-
cess to the composite meaning, and thus that the re-
sults of the current evaluation should correlate with
applied performance.
More in detail, we evaluate here the composition
methods (and the adjective and noun baselines) by
computing, for each of them, the cosine of the test
set AN vectors they generate (the ?predicted? ANs)
with the 41K vectors representing our extended vo-
cabulary in semantic space, and looking at the posi-
tion of the corresponding observed ANs (that were
not used for training, in the supervised approaches)
1189
in the cosine-ranked lists. The lower the rank, the
better the approximation. For efficiency reasons, we
flatten out the ranks after the top 1,000 neighbors.
The results are summarized in Table 3 by the me-
dian and the other quartiles, calculated across all
26,440 ANs in the test set. These measures (unlike
mean and variance) are not affected by the cut-off
after 1K neighbors. To put the reported results into
perspective, a model with a first quartile rank of 999
does very significantly better than chance (the bino-
mial probability of 1/4 or more of 26,440 trials be-
ing successful with pi = 0.024 is virtually 0, where
the latter quantity is the probability of an observed
AN being at rank 999 or lower according to a geo-
metric distribution with pi=1/40999).
method 25% median 75%
alm 17 170 ?1K
add 27 257 ?1K
noun 72 448 ?1K
mult 279 ?1K ?1K
slm 629 ?1K ?1K
adj ?1K ?1K ?1K
Table 3: Quartile ranks of observed ANs in cosine-ranked
lists of predicted AN neighbors.
Our proposed method, alm, emerges as the best
approach. The difference with the second best
model, add (the only other model that does better
than the non-trivial baseline of using the compo-
nent noun vector as a surrogate for AN), is highly
statistically significant (Wilcoxon signed rank test,
p< 0.00001). If we randomly downsample the AN
set to keep an equal number of ANs per adjective
(200), the difference is still significant with p below
the same threshold, indicating that the general result
is not due to a better performance of alm on a few
common adjectives.1
Among the alternative models, the fact that the
performance of add is decidedly better than that of
mult is remarkable, since earlier studies found that
1The semantic space in which we rank the observed ANs
with respect to their predicted counterparts also contain the ob-
served vectors of nouns and ANs that were used to train alm.
We do not see how this should affect performance, but we nev-
ertheless repeated the evaluation leaving out, for each AN, the
observed items used in training, and we obtained the same re-
sults reported in the main text (same ordering of method perfor-
mance, and very significant difference between alm and add).
multiplicative models are, in general, better than ad-
ditive ones in compositionality tasks (see Section 2
above). This might depend on the nature of AN
composition, but there are also more technical is-
sues at hand: (i) we are not sure that previous stud-
ies normalized before summing like we did, and
(ii) the multiplicative model, as discussed in Section
4, does not benefit from SVD reduction. The sin-
gle linear mapping model (slm) proposed by Gue-
vara (2010) is doing even worse than the multiplica-
tive method, suggesting that a single set of weights
does not provide enough flexibility to model a vari-
ety of adjective transformations successfully. This
is at odds with Guevara?s experiment in which slm
outperformed mult and add on the task of ranking
predicted ANs with respect to a target observed AN.
Besides various differences in task definition and
model implementation, Guevara trained his model
on ANs that include a wide variety of adjectives,
whereas our training data were limited to ANs con-
taining one of our 36 test set adjectives. Future work
should re-evalute the performance of Guevara?s ap-
proach in our task, but under his training regime.
Looking now at the alm results in more detail, the
best median ranks are obtained for very frequent ad-
jectives. The top ones are new (median rank: 34),
great (79), American (82), large (82) and different
(97). There is a high inverse correlation between
median rank and adjective frequency (Spearman?s
? =?0.56). Although from a statistical perspec-
tive it is expected that we get better results where
we have more data, from a linguistic point of view it
is interesting that alm works best with extremely fre-
quent, highly polysemous adjectives like new, large
and different, that border on function words ? a do-
main where distributional semantics has generally
not been tested.
Although, in relative terms and considering the
difficulty of the task, alm performs well, it is still far
from perfect ? for 27% alm-predicted ANs, the ob-
served vector is not even in the top 1K neighbor set!
A qualitative look at some of the most problematic
examples indicates however that a good proportion
of them might actually not be instances where our
model got the AN vector wrong, but cases of anoma-
lous observed ANs. The left side of Table 4 com-
pares the nearest neighbors (excluding each other)
of the observed and alm-predicted vectors in 10 ran-
1190
SIMILAR DISSIMILAR
adj N obs. neighbor pred. neighbor adj N obs. neighbor pred. neighbor
common understanding common approach common vision American affair Am. development Am. policy
different authority diff. objective diff. description current dimension left (a) current element
different partner diff. organisation diff. department good complaint current complaint good beginning
general question general issue same great field excellent field gr. distribution
historical introduction hist. background same historical thing different today hist. reality
necessary qualification nec. experience same important summer summer big holiday
new actor new cast same large pass historical region large dimension
recent request recent enquiry same special something little animal special thing
small drop droplet drop white profile chrome (n) white show
young engineer young designer y. engineering young photo important song young image
Table 4: Left: nearest neighbors of observed and alm-predicted ANs (excluding each other) for a random set of ANs
where rank of observed w.r.t. predicted is 1. Right: nearest neighbors of predicted and observed ANs for random set
where rank of observed w.r.t. predicted is ? 1K.
domly selected cases where the observed AN is the
nearest neighbor of the predicted one. Here, the
ANs themselves make sense, and the (often shared)
neighbors are also sensible (recent enquiry for re-
cent request, common approach and common vision
for common understanding, etc.). Moving to the
right, we see 10 random examples of ANs where the
observed AN was at least 999 neighbors apart from
the alm prediction. First, we notice some ANs that
are difficult to interpret out-of-context (important
summer, white profile, young photo, large pass, . . . ).
Second, at least subjectively, we find that in many
cases the nearest neighbor of predicted AN is actu-
ally more sensible than that of observed AN: cur-
rent element (vs. left) for current dimension, histori-
cal reality (vs. different today) for historical thing,
special thing (vs. little animal) for special some-
thing, young image (vs. important song) for young
photo. In the other cases, the predicted AN neighbor
is at least not obviously worse than the observed AN
neighbor.
There is a high inverse correlation between the
frequency of occurrence of an AN and the rank of
the observed AN with respect to the predicted one
(? =?0.48), suggesting that our model is worse at
approximating the observed vectors of rare forms,
that might, in turn, be those for which the corpus-
based representation is less reliable. In these cases,
dissimilarities between observed and expected vec-
tors, rather than signaling problems with the model,
might indicate that the predicted vector, based on a
composition function learned from many examples,
is better than the one directly extracted from the cor-
pus. The examples in the right panel of Table 4 bring
some preliminary support to this hypothesis, to be
systematically explored in future work.
7 Study 3: Comparing adjectives
If adjectives are functions, and not corpus-derived
vectors, is it still possible to compare them mean-
ingfully? We explore two ways to accomplish this
in our framework: one is to represent adjectives by
the average of the AN vectors that contain them
(the centroid vectors whose neighbors are illustrated
in Table 1 above), and the other to compare them
based on the 300?300 weight matrices we esti-
mate from noun-AN pairs (we unfold these matri-
ces into 90K-dimensional vectors). We compare the
quality of these representations to that of the stan-
dard approach in distributional semantics, i.e., rep-
resenting the adjectives directly with their corpus
co-occurrence profile vectors (in our case, projected
onto the SVD-reduced space).
We evaluate performance on the task of cluster-
ing those 19 adjectives in our set that can be rel-
atively straightforwardly categorized into general
classes comprising a minimum of 4 items. The
test set built according to these criteria contains 4
classes: color (white, black, red, green), positive
evaluation (nice, excellent, important, major, ap-
propriate), time (recent, new, current, old, young),
and size (big, huge, little, small, large). We clus-
ter with the CLUTO toolkit (Karypis, 2003), us-
ing the repeated bisections with global optimization
1191
method, accepting all of CLUTO?s default values
for this choice. Cluster quality is evaluated by per-
centage purity (Zhao and Karypis, 2003). If nir is
the number of items from the i-th true (gold stan-
dard) class assigned to the r-th cluster, n is the to-
tal number of items and k the number of clusters,
then: Purity = 1n
?k
r=1 maxi
(nir). We calculate
empirical 95% confidence intervals around purity by
a heuristic bootstrap procedure based on 10K resam-
plings of the data set (Efron and Tibshirani, 1994).
The random baseline distribution is obtained by 10K
random assignments of adjectives to the clusters, un-
der the constraint that no cluster is empty.
Table 5 shows that all methods are significantly
better than chance. Our two ?indirect? represen-
tations achieve similar performance, and they are
(slightly) better than the traditional method based on
adjective co-occurrence vectors. We conclude that,
although our approach does not provide a direct en-
coding of adjective meaning in terms of such inde-
pendently collected vectors, it does have meaningful
ways to represent their semantic properties.
input purity
matrix 73.7 (68.4-94.7)
centroid 73.7 (63.2-94.7)
vector 68.4 (63.2-89.5)
random 45.9 (36.8-57.9)
Table 5: Percentage purity in adjective clustering with
bootstrapped 95% confidence intervals.
8 Conclusion
The work we reported constitutes an encouraging
start for our approach to modeling (AN) composi-
tion. We suggested, along the way, various direc-
tions for further studies. We consider the following
issues to be the most pressing ones.
We currently train each adjective-specific model
separately: We should explore hierarchical model-
ing approaches that exploit similarities across adjec-
tives (and possibly syntactic constructions) to esti-
mate better models.
Evaluation-wise, the differences between ob-
served and predicted ANs must be analyzed more
extensively, to support the claim that, when their
vectors differ, model-based prediction improves on
the observed vector. Evaluation in a more applied
task should also be pursued ? in particular, we will
design a paraphrasing task similar to the one pro-
posed by Mitchell and Lapata to evaluate noun-verb
constructions.
Since we do not collect vectors for the ?functor?
component of a composition process (for AN con-
structions, the adjective), our approach naturally ex-
tends to processes that involve bound morphemes,
such as affixation, where we would not need to col-
lect independent co-occurrence information for the
affixes. For example, to account for re- prefixation
we do not need to collect a re- vector (required by all
other approaches to composition), but simply vec-
tors for a set of V/reV pairs, where both members of
the pairs are words (e.g., consider/reconsider).
Our approach can also deal, out-of-the-box, with
recursive constructions (sad little red hat), and can
be easily extended to more abstract constructions,
such as determiner N (mapping dog to the/a/one
dog). Still, we need to design a good testing scenario
to evaluate the quality of such model-generated con-
structions.
Ultimately, we want to compose larger and larger
constituents, up to full sentences. It remains to be
seen if the approach we proposed will scale up to
such challenges.
Acknowledgments
We thank Gemma Boleda, Emilano Guevara,
Alessandro Lenci, Louise McNally and the anony-
mous reviewers for useful information, advice and
comments.
References
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the First Symposium on Quantum Interaction, pages
52?55.
B. Efron and R. Tibshirani. 1994. An Introduction to the
Bootstrap. Chapman and Hall, Boca Raton, FL.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proceedings of
EMNLP, pages 897?906.
K. Erk and S. Pado?. 2009. Paraphrase assessment
in structured vector space: Exploring parameters and
datasets. In Proceedings of the EACL GEMS Work-
shop, pages 57?65.
1192
S. Evert. 2005. The Statistics of Word Cooccurrences.
Dissertation, Stuttgart University.
P. Foltz, W. Kintsch, and Th. Landauer. 1998. The mea-
surement of textual coherence with Latent Semantic
Analysis. Discourse Processes, 25:285?307.
G. Frege. 1892. U?ber sinn und bedeutung. Zeitschrift
fuer Philosophie un philosophische Kritik, 100.
E. Guevara. 2010. A regression model of adjective-noun
compositionality in distributional semantics. In Pro-
ceedings of the ACL GEMS Workshop, pages 33?37.
T. Hastie, R. Tibshirani, and J. Friedman. 2009. The El-
ements of Statistical Learning, 2nd ed. Springer, New
York.
M. Jones and D. Mewhort. 2007. Representing word
meaning and order information in a composite holo-
graphic lexicon. Psychological Review, 114:1?37.
H. Kamp. 1975. Two theories about adjectives. In
E. Keenan, editor, Formal Semantics of Natural Lan-
guage, pages 123?155. Cambridge University Press.
G. Karypis. 2003. CLUTO: A clustering toolkit. Tech-
nical Report 02-017, University of Minnesota Depart-
ment of Computer Science.
W. Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Th. Landauer and S. Dumais. 1997. A solution to Plato?s
problem: The Latent Semantic Analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240.
B. Mevik and R. Wehrens. 2007. The pls package: Prin-
cipal component and partial least squares regression in
R. Journal of Statistical Software, 18(2).
J. Mitchell and M. Lapata. 2008. Vector-based models of
semantic composition. In Proceedings of ACL, pages
236?244.
J. Mitchell and M. Lapata. 2009. Language models
based on semantic composition. In Proceedings of
EMNLP, pages 430?439.
R. Montague. 1970a. English as a formal language. In
B. Visentini, editor, Linguaggi nella Societa` e nella
Tecnica, pages 189?224. Edizioni di Comunita`, Milan.
Reprinted in Thomason (1974).
R. Montague. 1970b. Universal grammar. Theoria,
36:373?398. Reprinted in Thomason (1974).
R. Montague. 1973. The proper treatment of quantifica-
tion in English. In K.J.J. Hintikka, editor, Approaches
to Natural Language, pages 221?242. Reidel, Dor-
drecht. Reprinted in Thomason (1974).
B. Partee. 2004. Compositionality. In Compositionality
in Formal Semantics: Selected Papers by Barbara H.
Partee. Blackwell, Oxford.
R. Rapp. 2003. Word sense discovery based on sense de-
scriptor dissimilarity. In Proceedings of the MT Sum-
mit, pages 315?322.
H. Rubenstein and J. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
S. Rudolph and E. Giesbrecht. 2010. Compositional
matrix-space models of language. In Proceedings of
ACL.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of CogSci, pages 1300?1305.
M. Sahlgren. 2006. The Word-Space Model. Disserta-
tion, Stockholm University.
H. Schmid. 1995. Improvements in part-of-speech tag-
ging with an application to German. In Proceedings of
the EACL-SIGDAT Workshop.
H. Schu?tze. 1997. Ambiguity Resolution in Natural Lan-
guage Learning. CSLI, Stanford, CA.
M. Siegel. 1976. Capturing the Adjective. Ph.D. thesis,
University of Massachusetts at Amherst.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connec-
tionist networks. Artificial Intelligence, 46:159?216.
R. H. Thomason, editor. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New York.
P. Turney and P. Pantel. 2010. From frequency to mean-
ing: Vector space models of semantics. Journal of Ar-
tificial Intelligence Research, 37:141?188.
D. Widdows. 2008. Semantic vector products: Some ini-
tial investigations. In Proceedings of the Second Sym-
posium on Quantum Interaction, Oxford.
Y. Zhao and G. Karypis. 2003. Criterion functions for
document clustering: Experiments and analysis. Tech-
nical Report 01-40, University of Minnesota Depart-
ment of Computer Science.
1193
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141?151,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Studying the recursive behaviour of adjectival modification
with compositional distributional semantics
Eva Maria Vecchi and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.it
Abstract
In this study, we use compositional distribu-
tional semantic methods to investigate restric-
tions in adjective ordering. Specifically, we
focus on properties distinguishing Adjective-
Adjective-Noun phrases in which there is flex-
ibility in the adjective ordering from those
bound to a rigid order. We explore a number
of measures extracted from the distributional
representation of AAN phrases which may in-
dicate a word order restriction. We find that
we are able to distinguish the relevant classes
and the correct order based primarily on the
degree of modification of the adjectives. Our
results offer fresh insight into the semantic
properties that determine adjective ordering,
building a bridge between syntax and distri-
butional semantics.
1 Introduction
A prominent approach for representing the meaning
of a word in Natural Language Processing (NLP) is
to treat it as a numerical vector that codes the pat-
tern of co-occurrence of that word with other ex-
pressions in a large corpus of language (Sahlgren,
2006; Turney and Pantel, 2010). This approach to
semantics (sometimes called distributional seman-
tics) scales well to large lexicons and does not re-
quire words to be manually disambiguated (Schu?tze,
1997). Until recently, however, this method had
been almost exclusively limited to the level of sin-
gle content words (nouns, adjectives, verbs), and had
not directly addressed the problem of composition-
ality (Frege, 1892; Montague, 1970; Partee, 2004),
the crucial property of natural language which al-
lows speakers to derive the meaning of a complex
linguistic constituent from the meaning of its imme-
diate syntactic subconstituents.
Several recent proposals have strived to ex-
tend distributional semantics with a component that
also generates vectors for complex linguistic con-
stituents, using compositional operations in the vec-
tor space (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al, 2012). All of
these approaches construct distributional represen-
tations for novel phrases starting from the corpus-
derived vectors for their lexical constituents and
exploiting the geometric quality of the representa-
tion. Such methods are able to capture complex se-
mantic information of adjective-noun (AN) phrases,
such as characterizing modification (Boleda et al,
2012; Boleda et al, 2013), and can detect seman-
tic deviance in novel phrases (Vecchi et al, 2011).
Furthermore, these methods are naturally recursive:
they can derive a representation not only for, e.g.,
red car, but also for new red car, fast new red car,
etc. This aspect is appealing since trying to extract
meaningful representations for all recursive phrases
directly from a corpus will result in a problem of
sparsity, since most large phrases will never occur in
any finite sample.
Once we start seriously looking into recursive
modification, however, the issue of modifier order-
ing restrictions naturally arises. Such restrictions
have often been discussed in the theoretical linguis-
tic literature (Sproat and Shih, 1990; Crisma, 1991;
Scott, 2002), and have become one of the key in-
141
gredients of the ?cartographic? approach to syntax
(Cinque, 2002). In this paradigm, the ordering is
derived by assigning semantically different classes
of modifiers to the specifiers of distinct functional
projections, whose sequence is hard-wired. While
it is accepted that in different languages movement
can lead to a principled rearrangement of the linear
order of the modifiers (Cinque, 2010; Steddy and
Samek-Lodovici, 2011), one key assumption of the
cartographic literature is that exactly one intonation-
ally unmarked order for stacked adjectives should
be possible in languages like English. The possi-
bility of alternative orders, when discussed at all,
is attributed to the presence of idioms (high Amer-
ican building, but American high officer), to asyn-
detic conjunctive meanings (e.g. new creative idea
parsed as [new & creative] idea, rather than [new
[creative idea]]), or to semantic category ambiguity
for any adjective which appears in different orders
(see Cinque (2004) for discussion).
In this study, we show that the existence of both
rigid and flexible order cases is robustly attested at
least for adjectival modification, and that flexible or-
dering is unlikely to reduce to idioms, coordination
or ambiguity. Moreover, we show that at least for
some recursively constructed adjective-adjective-
noun phrases (AANs) we can extract meaning-
ful representations from the corpus, approximating
them reasonably well by means of compositional
distributional semantic models, and that the seman-
tic information contained in these models character-
izes which AA will have rigid order (as with rapid
social change vs. *social rapid change), or flexible
order (e.g. total estimated population vs. estimated
total population). In the former case, we find that
the same distributional semantic cues discriminate
between correct and wrong orders.
To achieve these goals, we consider various
properties of the distributional representation of
AANs (both corpus-extracted and compositionally-
derived), and explore their correlation with restric-
tions in adjective ordering. We conclude that mea-
sures that quantify the degree to which the modifiers
have an impact on the distributional meaning of the
AAN can be good predictors of ordering restrictions
in AANs.
2 Materials and methods
2.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row represents the meaning of an adjective, noun,
AN or AAN as a distributional vector, each column
a semantic dimension of meaning. We first introduce
the source corpus, then the vocabulary of words and
phrases that we represent in the space, and finally the
procedure adopted to build the vectors representing
the vocabulary items from corpus statistics, and ob-
tain the semantic space matrix. We work here with a
traditional, window-based semantic space, since our
focus is on the effect of different composition meth-
ods given a common semantic space. In addition,
Blacoe and Lapata (2012) found that a vanilla space
of this sort performed best in their composition ex-
periments, when compared to a syntax-aware space
and to neural language model vectors such as those
used for composition by Socher et al (2011).
Source corpus We use as our source corpus the
concatenation of the Web-derived ukWaC corpus, a
mid-2009 dump of the English Wikipedia and the
British National Corpus1. The corpus has been tok-
enized, POS-tagged and lemmatized with the Tree-
Tagger (Schmid, 1995), and it contains about 2.8 bil-
lion tokens. We extract all statistics at the lemma
level, meaning that we consider only the canonical
form of each word ignoring inflectional information,
such as pluralization and verb inflection.
Semantic space vocabulary The words/phrases
in the semantic space must of course include the
items that we need for our experiments (adjectives,
nouns, ANs and AANs used for model training, as
input to composition and for evaluation). Therefore,
we first populate our semantic space with a core vo-
cabulary containing the 8K most frequent nouns and
the 4K most frequent adjectives from the corpus.
The ANs included in the semantic space are com-
posed of adjectives with very high frequency in the
corpus so that they are generally able to combine
with many classes of nouns. They are composed
of the 700 most frequent adjectives and 4K most
frequent nouns in the corpus, which were manually
1http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
142
controlled for problematic cases ? excluding adjec-
tives such as above, less, or very, and nouns such
as cant, mph, or yours ? often due to tagging errors.
We generated the set of ANs by crossing the filtered
663 adjectives and 3,910 nouns. We include those
ANs that occur at least 100 times in the corpus in
our vocabulary, which amounted to a total of 128K
ANs.
Finally, we created a set of AAN phrases com-
posed of the adjectives and nouns used to gener-
ate the ANs. Additional preprocessing of the gen-
erated AxAyNs includes: (i) control that both AxN
and AyN are attested in the corpus; (ii) discard any
AxAyN in which AxN or AyN are among the top
200 most frequent ANs in the source corpus (as in
this case, order will be affected by the fact that such
phrases are almost certainly highly lexicalized); and
(iii) discard AANs seen as part of a conjunction in
the source corpus (i.e., where the two adjectives ap-
pear separated by comma, and, or or; this addresses
the objection that a flexible order AAN might be a
hidden A(&)A conjunction: we would expect that
such a conjunction should also appear overtly else-
where). The set of AANs thus generated is then di-
vided into two types of adjective ordering:
1. Flexible Order (FO): phrases where both or-
ders, AxAyN and AyAxN, are attested (f>10
in both orders).
2. Rigid Order (RO): phrases with one order,
AxAyN, attested (20<f<200)2 and AyAxN
unattested.
All AANs that did not meet either condition were
excluded from our semantic space vocabulary. The
preserved set resulted in 1,438 AANs: 621 flexible
order and 817 rigid order. Note that there are almost
as many flexible as rigid order cases; this speaks
against the idea that free order is a marginal phe-
nomenon, due to occasional ambiguities that reas-
sign the adjective to a different semantic class. The
existence of freely ordered stacked adjectives is a ro-
bust phenomenon, which needs to be addressed.
2The upper threshold was included as an additional filter
against potential multiword expressions. Of course, the bound-
ary between phrases that are at least partially compositional and
those that are fully lexicalized is not sharp, and we leave it to
further work to explore the interplay between the semantic fac-
tors we study here and patterns of lexicalization.
Model ? M&L
CORP 0.41 0.43
W.ADD 0.41 0.44
F.ADD 0.40 ?
MULT 0.33 0.46
LFM 0.40 ?
Table 1: Correlation scores (Spearman?s ?, all signif-
icant at p<0.001) between cosines of corpus-extracted
or model-generated AN vectors and phrase similarity rat-
ings collected in Mitchell and Lapata (2010), as well as
best reported results from Mitchell & Lapata (M&L).
Semantic vector construction For each of
the items in our vocabulary, we first build 10K-
dimensional vectors by recording the item?s
sentence-internal co-occurrence with the top 10K
most frequent content lemmas (nouns, adjectives,
verbs or adverbs) in the corpus. We built a rank
of these co-occurrence counts, and excluded as
stop words from the dimensions any element of
any POS whose rank was from 0 to 300. The raw
co-occurrence counts were then transformed into
(positive) Pointwise Mutual Information (pPMI)
scores (Church and Hanks, 1990). Next, we reduce
the full co-occurrence matrix to 300 dimensions
applying the Non-negative Matrix Factorization
(NMF) operation (Lin, 2007). We did not tune the
semantic vector construction parameters, since we
found them to work best in a number of independent
earlier experiments.
Corpus-extracted vectors (corp) were computed
for the ANs and for the flexible order and attested
rigid order AANs, and then mapped onto the 300-
dimension NMF-reduced semantic space. As a san-
ity check, the first row of Table 1 reports the corre-
lation between the AN phrase similarity ratings col-
lected in Mitchell and Lapata (2010) and the cosines
of corpus-extracted vectors in our space, for the
same ANs. For the AAN vectors, which are sparser,
we used human judgements to build a reliable sub-
set to serve as our gold standard, as detailed in Sec-
tion 2.4.
2.2 Composition models
We focus on four composition functions proposed
in recent literature with high performance in a num-
ber of semantic tasks. We first consider meth-
ods proposed by Mitchell and Lapata (2010) in
143
which the model-generated vectors are simply ob-
tained through component-wise operations on the
constituent vectors. Given input vectors ~u and ~v, the
multiplicative model (MULT) computes a composed
vector by component-wise multiplication () of the
constituent vectors, where the i-th component of the
composed vector is given by pi = uivi.3 Given an
AxAyN phrase, this model extends naturally to the
recursive setting of this experiment, as seen in Equa-
tion (1).
~p = ~ax  ~ay  ~n (1)
This composition method is order-insensitive, the
formula above corresponding to the representation
of both AxAyN and AyAxN.
In the weighted additive model (W.ADD), we ob-
tain the composed vector as a weighted sum of the
two component vectors: ~p = ?~u+ ?~v, where ? and
? are scalars. Again, we can easily apply this func-
tion recursively, as in Equation (2).
~p = ?~ax + ?(?~ay + ?~n) = ?~ax + ??~ay + ?
2~n
(2)
We also consider the full extension of the addi-
tive model (F.ADD), presented in Guevara (2010)
and Zanzotto et al (2010), such that the component
vectors are pre-multiplied by weight matrices before
being added: ~p = W1~u + W2~v. Similarly to the
W.ADD model, Equation (3) describes how we apply
this function recursively.
~p = W1~ax + W2(W1~ay + W2~n) (3)
= W1~ax + W2W1~ay + W22~n
Finally, we consider the lexical function model
(LFM), first introduced in Baroni and Zamparelli
(2010), in which attributive adjectives are treated as
functions from noun meanings to noun meanings.
This is a standard approach in Montague semantics
(Thomason, 1974), except noun meanings here are
distributional vectors, not denotations, and adjec-
tives are (linear) functions learned from a large cor-
pus. In this model, predicted vectors are generated
3We conjecture that the different performance of our multi-
plicative model and M&L?s (cf. Table 1) is due to the fact that
we use log-transformed pPMI scores, making their multiplica-
tive model more akin to our additive approach.
by multiplying a function matrix U with a compo-
nent vector: ~p = U~v. Given a weight matrix, A, for
each adjective in the phrase, we apply the functions
in sequence recursively as shown in Equation (4).
~p = Ax(Ay~n) (4)
Composition model estimation Parameters for
W.ADD, F.ADD and LFM were estimated following
the strategy proposed by Guevara (2010) and Ba-
roni and Zamparelli (2010), recently extended to all
composition models by Dinu et al (2013b). Specif-
ically, we learn parameter values that optimize the
mapping from the noun to the AN as seen in ex-
amples of corpus-extracted N-AN vector pairs, us-
ing least-squares methods. All parameter estima-
tions and phrase compositions were implemented
using the DISSECT toolkit4 (Dinu et al, 2013a),
with a training set of 74,767 corpus-extracted N-
AN vector pairs, ranging from 100 to over 1K items
across the 663 adjectives. Importantly, while below
we report experimental results on capturing various
properties of recursive AAN constructions, no AAN
was seen during training, which was based entirely
on mapping from N to AN. Table 1 reports the re-
sults attained by our model implementations on the
Mitchell and Lapata AN similarity data set.
2.3 Measures of adjective ordering
Our general goal is to determine which
linguistically-motivated factors distinguish the
two types of adjective ordering. We hypothesize
that in cases of flexible order, the two adjectives
will have a similarly strong effect on the noun, thus
transforming the meaning of the noun equivalently
in the direction of both adjectives and component
ANs. For example, in the phrase creative new idea,
the idea is both new and creative, so we would
expect a similar impact of modification by both
adjectives.
On the other hand, we predict that in rigid order
cases, one adjective, the one closer to the noun, will
dominate the meaning of the phrase, distorting the
meaning of the noun by a significant amount. For
example, the phrase different architectural style in-
tuitively describes an architectural style that is dif-
4http://clic.cimec.unitn.it/composes/
toolkit
144
ferent, rather than a style that is to the same extent
architectural and different.
We consider a number of measures that could cap-
ture our intuitions and quantify this difference, ex-
ploring the distance relationship between the AAN
vectors and each of the AAN subparts. First, we
examine how the similarity of an AAN to its com-
ponent adjectives affects the ordering, using the co-
sine between the AxAyN vector and each of the
component A vectors as an expression of similarity
(we abbreviate this as cosAx and cosAy for the first
and second adjective, respectively).5 Our hypothe-
sis predicts that flexible order AANs should remain
similarly close to both component As, while rigid
order AANs should remain systematically closer to
their Ay than to their Ax.
Next, we consider the similarity between the
AxAyN vector and its component N vector (cosN ).
This measure is aimed at verifying if the degree to
which the meaning of the head noun is distorted
could be a property that distinguishes the two types
of adjective ordering. Again, vectors for flexible or-
der AANs should remain closer to their component
nouns in the semantic space, while rigid order AANs
should distort the meaning of the head noun more
notably.
We also inspect how the similarity of the AAN
to its component AN vectors affects the type of ad-
jective ordering (cosAxN and cosAyN ). Consid-
ering the examples above, we predict that the flex-
ible order AAN creative new idea will share many
properties with both creative idea and new idea, as
represented in our semantic space, while rigid or-
der AANs, like different architectural style, should
remain quite similar to the AyN, i.e., architectural
style, and relatively distant from the AxN, i.e., dif-
ferent style.
Finally, we consider a measure that does not ex-
ploit distributional semantic representations, namely
the difference in PMI between AxN and AyN
(?PMI). Based on our hypothesis described for the
other measures, we expect the association in the cor-
pus of AyN to be much greater than AxN for rigid
order AANs, resulting in a large negative ?PMI val-
ues. While flexible order AANs should have similar
5In the case of LFM, we compare the similarity of the AAN
with the AN centroids for each adjective, since the model does
not make use of A vectors (Baroni and Zamparelli, 2010).
association strengths for both AxN and AyN, thus
we expect ?PMI to be closer to 0 than for rigid or-
der AANs.
2.4 Gold standard
To our knowledge, this is the first study to use
distributional representations of recursive modifi-
cation; therefore we must first determine if the
composed AAN vector representations are seman-
tically coherent objects. Thus, for vector analysis,
a gold standard of 320 corpus-extracted AAN vec-
tors were selected and their quality was established
by inspecting their nearest neighbors. In order to
create the gold standard, we ran a crowdsourcing
experiment on CrowdFlower6 (Callison-Burch and
Dredze, 2010; Munro et al, 2010), as follows.
First, we gathered a randomly selected set of 600
corpus-extracted AANs, containing 300 flexible or-
der and 300 attested rigid order AANs. We then
extracted the top 3 nearest neighbors to the corpus-
extracted AAN vectors as represented in the seman-
tic space7. Each AAN was then presented with each
of the nearest neighbors, and participants were asked
to judge ?how strongly related are the two phrases??
on a scale of 1-7. The rationale was that if we
obtained a good distributional representation of the
AAN, its nearest neighbors should be closely related
words and phrases. Each pair was judged 10 times,
and we calculated a relatedness score for the AAN
by taking the average of the 30 judgments (10 for
each of the three neighbors).
The final set for the gold standard contains the 320
AANs (152 flexible order and 168 attested rigid or-
der) which had a relatedness score over the median-
split (3.9). Table 2 shows examples of gold stan-
dard AANs and their nearest neighbors. As these
example indicate, the gold standard AANs reside in
semantic neighborhoods that are populated by in-
tuitively strongly related expressions, which makes
them a sensible target for the compositional models
to approximate.
We also find that the neighbors for the AANs rep-
resent an interesting variety of types of semantic
6http://www.crowdflower.com
7The top 3 neighbors included adjectives, nouns, ANs and
AANs. The preference for ANs and AANs, as seen in Table 2,
is likely a result of the dominance of those elements in the se-
mantic space (c.f. Section 2.1).
145
medieval old town contemp. political issue
fascinating town cultural topic
impressive cathedral contemporary debate
medieval street contemporary politics
rural poor people British naval power
poor rural people naval war
rural infrastructure British navy
rural people naval power
friendly helpful staff last live performance
near hotel final gig
helpful staff live dvd
quick service live release
creative new idea rapid social change
innovative effort social conflict
creative design social transition
dynamic part cultural consequence
national daily newspaper new regional government
national newspaper regional government
major newspaper local reform
daily newspaper regional council
daily national newspaper fresh organic vegetable
national daily newspaper organic vegetable
well-known journalist organic fruit
weekly column organic product
Table 2: Examples of the nearest neighbors of the gold
standard, both flexible order (left column) and rigid order
(right column) AANs.
similarity. For example, the nearest neighbors to the
corpus-extracted vectors for medieval old town and
rapid social change include phrases which describe
quite complex associations, cf. Table 2. In addition,
we find that the nearest neighbors for flexible order
AAN vectors are not necessarily the same for both
adjective orders, as seen in the difference in neigh-
bors of national daily newspaper and daily national
newspaper. We can expect that the change in or-
der, when acceptable and frequent, does not neces-
sarily yield synonymous phrases, and that corpus-
extracted vector representations capture subtle dif-
ferences in meaning.
3 Results
3.1 Quality of model-generated AAN vectors
Our nearest neighbor analysis suggests that the
corpus-extracted AAN vectors in the gold standard
are meaningful, semantically coherent objects. We
can thus assess the quality of AANs recursively gen-
erated by composition models by how closely they
Gold FO RO
W.ADD 0.565 0.572 0.558
F.ADD 0.618 0.622 0.614
MULT 0.424 0.468 0.384
LFM 0.655 0.675 0.637
Table 3: Mean cosine similarities between the corpus-
extracted and model-generated gold AAN vectors. All
pairwise differences between models are significant ac-
cording to Bonferroni-corrected paired t-tests (p<0.001).
For MULT and LFM, the difference between mean flexible
order (FO) and rigid order (RO) cosines is also signifi-
cant.
approximate these vectors. We find that the perfor-
mances of most composition models in approximat-
ing the vectors for the gold AANs is quite satisfac-
tory (cf. Table 3). To put this evaluation into per-
spective, note that 99% of the simulated distribu-
tion of pairwise cosines of corpus-extracted AANs
is below the mean cosine of the worst-performing
model (MULT), that is, a cosine of 0.424 is very sig-
nificantly above what is expected by chance for two
random corpus-extracted AAN vectors. Also, ob-
serve that the two more parameter-rich models are
better than W.ADD, and that LFM also significantly
outperforms F.ADD.
Further, the results show that the models are able
to approximate flexible order AAN vectors better
than rigid order AANs, significantly so for LFM and
MULT. This result is quite interesting because it sug-
gests that flexible order AANs express a more lit-
eral (or intersective) modification by both adjectives,
which is what we would expect to be better captured
by compositional models. Clearly, a more complex
modification process is occurring in the case of rigid
order AANs, as we predicted to be the case.
3.2 Distinguishing flexible vs. rigid order
In the results reported below, we test how both our
baseline ?PMI measure and the distance from the
AAN and its component parts changes depending on
the type of adjective ordering to which the AAN be-
longs. From this point forward, we only use gold
standard items, where we are sure of the quality of
the corpus-extracted vectors. The first block of Ta-
ble 4 reports the t-normalized difference between
flexible order and rigid order mean cosines for the
corpus-extracted vectors.
146
Measure t sig.
CORP
cosAx 2.478
cosAy -4.348 * RO>FO
cosN 4.656 * FO>RO
cosAxN 5.913 * FO>RO
cosAyN 1.970
W.ADD
cosAx 4.805 * FO>RO
cosAy -1.109
cosN 1.140
cosAxN 1.059
cosAyN 0.584
F.ADD
cosAx 2.050
cosAy -1.451
cosN 4.493 * FO>RO
cosAxN -0.445
cosAyN 2.300
MULT
cosAx 3.830 * FO>RO
cosAy -0.503
cosN 5.090 * FO>RO
cosAxN 4.435 * FO>RO
cosAyN 3.900 * FO>RO
LFM
cosAx -1.649
cosAy -1.272
cosN 5.539 * FO>RO
cosAxN 3.336 * FO>RO
cosAyN 4.215 * FO>RO
?PMI 8.701 * FO>RO
Table 4: Flexible vs. Rigid Order AANs. t-normalized
differences between flexible order (FO) and rigid order
(FO) mean cosines (or mean ?PMI values) for corpus-
extracted and model-generated vectors. For significant
differences (p<0.05 after Bonferroni correction), the last
column reports whether mean cosine (or ?PMI) is larger
for flexible order (FO) or rigid order (RO) class.
These results show, in accordance with our con-
siderations in Section 2.3 above: (i) flexible or-
der AxAyNs are closer to AxN and the component
N than rigid order AxAyNs, and (ii) rigid order
AxAyNs are closer to their Ay (flexible order AANs
are also closer to Ax but the effect does not reach
significance).8 The results imply that the degree of
modification of the Ay on the noun is a significant
indicator of the type of ordering present.
8As an aside, the fact that mean cosines are significantly
larger for the flexible order class in two cases but for the rigid or-
der class in another addresses the concern, raised by a reviewer,
that the words and phrases in one of the two classes might sys-
tematically inhabit denser regions of the space than those of the
other class, thus distorting results based on comparing mean
cosines.
In particular, rigid order AxAyNs are heavily
modified by Ay, distorting the meaning of the head
noun in the direction of the closest adjective quite
drastically, and only undergoing a slight modifica-
tion when the Ax is added. In other words, in rigid
order phrases, for example rapid social change, the
AyN expresses a single concept (probably a ?kind?,
in the terminology of formal semantics), strongly re-
lated to social, social change, which is then mod-
ified by the Ax. Thus, the change is not both so-
cial and rapid, rather, the social change is rapid. On
the other hand, flexible order AANs maintain the se-
mantic value of the head noun while being modi-
fied only slightly by both adjectives, almost equiv-
alently. For example, in the phrase friendly help-
ful staff, one is saying that the staff is both friendly
and helpful. Most importantly, the corpus-extracted
distributional representations are able to model this
phenomenon inherently and can significantly distin-
guish the two adjective orders.
The results of the composition models (cf. Ta-
ble 4) show that for all models at least some prop-
erties do distinguish flexible and rigid order AANs,
although only MULT and LFM capture the two prop-
erties that show the largest effect for the corpus-
extracted vectors, namely the asymmetry in similar-
ity to the noun and the AxN (flexible order AANs
being more similar to both).
It is worth remarking that MULT approximated the
patterns observed in the corpus vectors quite well,
despite producing order-insensitive representations
of recursive structures. For flexible order AANs, or-
der is indeed only slightly affecting the meaning, so
it stands to reason that MULT has no problems mod-
eling this class. For rigid order AANs, where we
consider here the attested-order only, evidently the
order-insensitive MULT representation is sufficient
to capture their relations to their constituents.
Finally, we see that the ?PMI measure is the best
at distinguishing between the two classes of AAN
ordering. This confirms our hypothesis that a lot has
to do with how integrated Ay and N are. While it
is somewhat disappointing that ?PMI outperforms
all distributional semantic cues, note that this mea-
sure conflates semantic and lexical factors, as the
high PMI of AyN in at least some rigid order AANs
might be also a cue of the fact that the latter bigram
is a lexicalized phrase (as discussed in footnote 2, it
147
is unlikely that our filtering strategies sifted out all
multiword expressions). Moreover, ?PMI does not
produce a semantic representation of the phrase (see
how composed distributional vectors approximate of
high quality AAN vectors in Table 3). Finally, this
measure will not scale up to cases where the ANs
are not attested, whereas measures based on compo-
sition only need corpus-harvested representations of
adjectives and nouns.
3.3 Properties of the correct adjective order
Having shown that flexible order and rigid order
AANs are significantly distinguished by various
properties, we proceed now to test whether those
same properties also allow us to distinguish between
correct (corpus-attested) and wrong (unattested) ad-
jective ordering in rigid AANs (recall that we are
working with cases where the attested-order occurs
more than 20 times in the corpus, and both adjec-
tives modify the nouns at least 10 times, so we are
confident that there is a true asymmetry).
We expect that the fundamental property that dis-
tinguishes the orders is again found in the degree
of modification of both component adjectives. We
predict that the single concept created by the AyN
in attested-order rigid AANs, such as legal status
in formal legal status, is an effect of the modifica-
tion strength of the Ay on the head noun, and when
seen in the incorrect ordering, i.e., ?legal formal sta-
tus, the strong modification of legal will still domi-
nate the meaning of the AAN. Composition models
should be able to capture this effect based on the dis-
tance from both the component adjectives and ANs.
Clearly, we cannot run these analyses on corpus-
extracted vectors since the unattested order, by def-
inition, is not seen in our corpus, and therefore we
cannot collect co-occurrence statistics for the AAN
phrase. Thus, we test our measures of adjective or-
dering on the model-generated AAN vectors, for all
gold rigid order AANs in both orders.
We also consider the ?PMI measure which was
so effective in distinguishing flexible vs. rigid or-
der AANs. We expect that the greater association
with AyN for attested-order AANs will again lead
to large, negative differences in PMI scores, while
the expectation that unattested-order AANs will be
highly associated with their AxN will correspond to
large, positive differences in PMI.
Measure t sig.
W.ADD
cosAx -7.840 * U>A
cosAy 7.924 * A>U
cosN 2.394
cosAxN -5.462 * U>A
cosAyN 3.627 * A>U
F.ADD
cosAx -8.418 * U>A
cosAy 6.534 * A>U
cosN -1.927
cosAxN -3.583 * U>A
cosAyN -2.185
MULT
cosAx -5.100 * U>A
cosAy 5.100 * A>U
cosN 0.000
cosAxN -0.598
cosAyN 0.598
LFM
cosAx -7.498 * U>A
cosAy 7.227 * A>U
cosN -2.172
cosAxN -5.792 * U>A
cosAyN 0.774
?PMI -11.448 * U>A
Table 5: Attested- vs. unattested-order rigid order
AANs. t-normalized mean paired cosine (or ?PMI) dif-
ferences between attested (A) and unattested (U) AANs
with their components. For significant differences (paired
t-test p<0.05 after Bonferroni correction), last column
reports whether cosines (or ?PMI) are on average larger
for A or U.
Across all composition models, we find that the
distance between the model-generated AAN and its
component adjectives, Ax and Ay, are significant in-
dicators of attested vs. unattested adjective ordering
(cf. Table 5). Specifically, we find that rigid order
AANs in the correct order are closest to their Ay,
while we can detect the unattested order when the
rigid order AAN is closer to its Ax. This finding
is quite interesting, since it shows that the order in
which the composition functions are applied does
not alter the fact that the modification of one ad-
jective in rigid order AANs (the Ay in the case of
attested-order rigid order AANs) is much stronger
than the other. Unlike the measures that differenti-
ated flexible and rigid order AANs, here we see that
the distance from the component N is not an indi-
cator of the correct adjective ordering (trivially so
for MULT, where attested and unattested AANs are
identical).
Next, we find that for W.ADD, F.ADD and LFM,
148
the distance from the component AxN is a strong
indicator of attested- vs. unattested-order rigid order
AANs. Specifically, attested-order AANs are further
from their AxN than unattested-order AANs. This
finding is in line with our predictions and follows
the findings of the impact of the distance from the
component adjectives.
?PMI, as seen in the ability to distinguish flexi-
ble vs. rigid order AANs, is the strongest indicator
of correct vs wrong adjective ordering. This mea-
sure confirms that the association of one adjective
(the Ay in attested-order AANs) with the head noun
is indeed the most significant factor distinguishing
these two classes. However, as we mentioned be-
fore, this measure has its limitations and is likely not
to be entirely sufficient for future steps in modeling
recursive modification.
4 Conclusion
While AN constructions have been extensively stud-
ied within the framework of compositional distri-
butional semantics (Baroni and Zamparelli, 2010;
Boleda et al, 2012; Boleda et al, 2013; Guevara,
2010; Mitchell and Lapata, 2010; Turney, 2012;
Vecchi et al, 2011), for the first time, we extended
the investigation to recursively built AAN phrases.
First, we showed that composition functions ap-
plied recursively can approximate corpus-extracted
AAN vectors that we know to be of high semantic
quality.
Next, we looked at some properties of the same
high-quality corpus-extracted AAN vectors, finding
that the distinction between ?flexible? AANs, where
the adjective order can be flipped, and ?rigid? ones,
where the order is fixed, is reflected in distributional
cues. These results all derive from the intuition that
the most embedded adjective in a rigid AAN has a
very strong effect on the distributional semantic rep-
resentation of the AAN. Most compositional models
were able to capture at least some of the same cues
that emerged in the analysis of the corpus-extracted
vectors.
Finally, similar cues were also shown to distin-
guish (compositional) representations of rigid AANs
in the ?correct? (corpus-attested) and ?wrong?
(unattested) orders, again pointing to the degree to
which the (attested-order) closest adjective affects
the overall AAN meaning as an important factor.
Comparing the composition functions, we find
that the linguistically motivated LFM approach has
the most consistent performance across all our tests.
This model significantly outperformed all others in
approximating high-quality corpus-extracted AAN
vectors, it provided the closest approximation to the
corpus-observed patterns when distinguishing flexi-
ble and rigid AANs, and it was one of the models
with the strongest cues distinguishing attested and
unattested orders of rigid AANs.
From an applied point of view, a natural next step
would be to use the cues we proposed as features to
train a classifier to predict the preferred order of ad-
jectives, to be tested also in cases where neither or-
der is found in the corpus, so direct corpus evidence
cannot help. For a full account of adjectival order-
ing, non-semantic factors should also be taken into
account. As shown by the effectiveness in our ex-
periments of PMI, which is a classic measure used
to harvest idioms and other multiword expressions
(Church and Hanks, 1990), ordering is affected by
arbitrary lexicalization patterns. Metrical effects are
also likely to play a role, like they do in the well-
studied case of ?binomials? such as salt and pep-
per (Benor and Levy, 2006; Copestake and Herbe-
lot, 2011). In a pilot study, we found that indeed
word length (roughly quantified by number of let-
ters) is a significant factor in predicting adjective
ordering (the shorter adjective being more likely to
occur first), but its effect is not nearly as strong as
that of the semantic measures we considered here.
In our future work, we would like to develop an or-
der model that exploits semantic, metrical and lexi-
calization features jointly for maximal classification
accuracy.
Adjectival ordering information could be useful
in parsing: in English, it could tell whether an
AANN sequence should be parsed as A[[AN]N]
or A[A[NN]]; in languages with pre- and post-
N adjectives, like Italian or Spanish, it could tell
whether ANA sequences should be parsed as A[NA]
or [AN]A. The ability to detect ordering restric-
tions could also help Natural Language Generation
tasks (Malouf, 2000), especially for the generation
of unattested combinations of As and Ns.
From a theoretical point of view, we would like to
extend our analysis to adjective coordination (what?s
149
the difference between new and creative idea and
new creative idea?). Additionally, we could go more
granular, looking at whether compositional models
can help us to understand why certain classes of ad-
jectives are more likely to precede or follow others
(why is size more likely to take scope over color,
so that big red car sounds more natural than red big
car?) or studying the behaviour of specific adjectives
(can our approach capture the fact that strong alco-
holic drink is preferable to alcoholic strong drink
because strong pertains to the alcoholic properties
of the drink?).
In the meantime, we hope that the results we re-
ported here provide convincing evidence of the use-
fulness of compositional distributional semantics in
tackling topics, such as recursive adjectival modifi-
cation, that have been of traditional interest to theo-
retical linguists from a new perspective.
Acknowledgments
We would like to thank the anonymous reviewers,
Fabio Massimo Zanzotto, Yao-Zhong Zhang and the
members of the COMPOSES team. This research
was supported by the ERC 2011 Starting Indepen-
dent Research Grant n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Sarah Bunin Benor and Roger Levy. 2006. The chicken
or the egg? A probabilistic analysis of english binomi-
als. Language, pages 233?278.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the 2012 Joint Conference on
EMNLP and CoNLL, pages 546?556, Jeju Island, Ko-
rea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In Pro-
ceedings of the 2012 Joint Conference on EMNLP and
CoNLL, pages 1223?1233, Jeju Island, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46, Pots-
dam, Germany.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
CA.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Guglielmo Cinque, editor. 2002. Functional Structure in
DP and IP - The Carthography of Syntactic Structures,
volume 1. Oxford University Press.
Guglielmo Cinque. 2004. Issues in adverbial syntax.
Lingua, 114:683?710.
Guglielmo Cinque. 2010. The syntax of adjectives: a
comparative study. MIT Press.
Ann Copestake and Aure?lie Herbelot. 2011. Exciting
and interesting: issues in the generation of binomials.
In Proceedings of the UCNLG+ Eval: Language Gen-
eration and Evaluation Workshop, pages 45?53, Edin-
burgh, UK.
Paola Crisma. 1991. Functional categories inside the
noun phrase: A study on the distribution of nominal
modifiers. ?Tesi di Laurea?, University of Venice.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013a. DISSECT: DIStributional SEmantics Compo-
sition Toolkit. In Proceedings of the System Demon-
strations of ACL 2013, East Stroudsburg, PA.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013b. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the ACL 2013 Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC
2013), East Stroudsburg, PA.
Gottlob Frege. 1892. U?ber sinn und bedeutung.
Zeitschrift fuer Philosophie un philosophische Kritik,
100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Computa-
tion, 19(10):2756?2779.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings
of ACL, pages 85?92, East Stroudsburg, PA.
150
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal Grammar. Theoria,
36:373?398.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation of
linguistic data. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 122?130,
Los Angeles, CA.
Barbara Partee. 2004. Compositionality. In Compo-
sitionality in Formal Semantics: Selected Papers by
Barbara H. Partee. Blackwell, Oxford.
Magnus Sahlgren. 2006. The Word-Space Model. Dis-
sertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Gary-John Scott. 2002. Stacked adjectival modification
and the structure of nominal phrases. In Guglielmo
Cinque, editor, Functional Structure in DP and IP. The
Carthography of Syntactic Structures, volume 1. Ox-
ford University Press.
Richard Socher, E.H. Huang, J. Pennington, Andrew Y.
Ng, and C.D. Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for paraphrase de-
tection. Advances in Neural Information Processing
Systems, 24:801?809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Edinburgh, UK.
Richard Sproat and Chilin Shih. 1990. The cross-
linguistics distribution of adjective ordering restric-
tions. In C. Georgopoulos and Ishihara R., editors,
Interdisciplinary approaches to language: essays in
honor of Yuki Kuroda, pages 565?593. Kluver, Dor-
drecht.
Sam Steddy and Vieri Samek-Lodovici. 2011. On the
ungrammaticality of remnant movement in the deriva-
tion of greenberg?s universal 20. Linguistic Inquiry,
42(3):445?469.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533?585.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1?9, Portland,
OR.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca Faluc-
chi, and Suresh Manandhar. 2010. Estimating linear
models for compositional distributional semantics. In
Proceedings of COLING, pages 1263?1271, Beijing,
China.
151
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908?1913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Fish transporters and miracle homes:
How compositional distributional semantics can help NP parsing
Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni
Center for Mind/Brain Sciences
University of Trento, Italy
first.last@unitn.it
Abstract
In this work, we argue that measures that have
been shown to quantify the degree of semantic
plausibility of phrases, as obtained from their
compositionally-derived distributional seman-
tic representations, can resolve syntactic am-
biguities. We exploit this idea to choose the
correct parsing of NPs (e.g., (live fish) trans-
porter rather than live (fish transporter)). We
show that our plausibility cues outperform
a strong baseline and significantly improve
performance when used in combination with
state-of-the-art features.
1 Introduction
Live fish transporter: A transporter of live fish
or rather a fish transporter that is not dead?
While our intuition, based on the meaning of this
phrase, prefers the former interpretation, the Stan-
ford parser, which lacks semantic features, incor-
rectly predicts the latter as the correct parse.1 The
correct syntactic parsing of sentences is clearly
steered by semantic information (as formal syn-
tacticians have pointed out at least since Fillmore
(1968)), and consequently the semantic plausibil-
ity of alternative parses can provide crucial evidence
about their validity.
An emerging line of parsing research capitalizes
on the advances of compositional distributional se-
mantics (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Socher et al,
2012). Information related to compositionally-
derived distributional representations of phrases is
1http://nlp.stanford.edu:8080/parser/
index.jsp
integrated at various stages of the parsing process
to improve overall performance.2 We are aware of
two very recent studies exploiting the semantic in-
formation provided by distributional models to re-
solve syntactic ambiguity: Socher et al (2013) and
Le et al (2013).
Socher et al (2013) present a recursive neural net-
work architecture which jointly learns semantic rep-
resentations and syntactic categories of phrases. By
annotating syntactic categories with their distribu-
tional representation, the method emulates lexical-
ized approaches (Collins, 2003) and captures sim-
ilarity more flexibly than solutions based on hard
clustering (Klein and Manning, 2003; Petrov et al,
2006). Thus, their approach mainly aims at improv-
ing parsing by capturing a richer, data-driven cate-
gorial structure.
On the other hand, Le et al (2013) work with the
output of the parser. Their hypothesis is that parses
that lead to less semantically plausible interpreta-
tions will be penalized by a reranker that looks at
the composed semantic representation of the parse.
Their method achieves an improvement of 0.2% in
F-score. However, as the authors also remark, be-
cause of their experimental setup, they cannot con-
clude that the improvement is truly due to the se-
mantic composition component, a crucial issue that
is deferred to further investigation.
This work aims at corroborating the hypothesis
that the semantic plausibility of a phrase can in-
deed determine its correct parsing. We develop a
system based on simple and intuitive measures, ex-
2Distributional representations approximate word and
phrase meaning by vectors that record the contexts in which
they are likely to appear in corpora; for a review see, e.g., Tur-
ney and Pantel (2010).
1908
Type of NP # Example
A (N N) 1296 local phone company
(A N) N 343 crude oil sector
N (N N) 164 miracle home run
(N N) N 424 blood pressure medicine
Total 2227 -
Table 1: NP dataset
tracted from the compositional distributional repre-
sentations of phrases, that have been shown to corre-
late with semantic plausibility (Vecchi et al, 2011).
We develop a controlled experimental setup, fo-
cusing on a single syntactic category, that is, noun
phrases (NP), where our task can be formalized as
(left or right) bracketing. Unlike previous work,
we compare our compositional semantic component
against features based on n-gram statistics, which
can arguably also capture some semantic informa-
tion in terms of frequent occurrences of meaningful
phrases. Inspired by previous literature demonstrat-
ing the power of metrics based on Pointwise Mu-
tual Information (PMI) in NP bracketing (Nakov and
Hearst, 2005; Pitler et al, 2010; Vadas and Curran,
2011), we test an approach exploiting PMI features,
and show that plausibility features relying on com-
posed representations can significantly boost accu-
racy over PMI.
2 Setup
Noun phrase dataset To construct our dataset,
we used the Penn TreeBank (Marcus et al, 1993),
which we enriched with the annotation provided by
Vadas and Curran (2007a), since the original tree-
bank does not distinguish different structures inside
the NPs and always marks them as right bracketed,
e.g., local (phone company) but also blood (pressure
medicine). We focus on NPs formed by three ele-
ments, where the first can be an adjective (A) or a
noun (N), the other two are nouns. Table 1 summa-
rizes the characteristics of the dataset.3
Distributional semantic space As our source cor-
pus we use the concatenation of ukWaC, the English
Wikipedia (2009 dump) and the BNC, with a total of
3The dataset is available from: http://clic.cimec.
unitn.it/composes
about 2.8 billion tokens.4 We collect co-occurrence
statistics for the top 8K Ns and 4K As, plus any
other word from our NP dataset that was below this
rank. Our context elements are composed of the top
10K content words (adjectives, adverbs, nouns and
verbs). We use a standard bag-of-words approach,
counting within-sentence collocates for every target
word. We apply (non-negative) Pointwise Mutual
Information as weighting scheme and dimensional-
ity reduction using Non-negative Matrix Factoriza-
tion, setting the number of reduced-space dimen-
sions to 300.5
Composition functions We experiment with vari-
ous composition functions, chosen among those sen-
sitive to internal structure (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010),
namely dilation (dil), weighted additive (wadd), lex-
ical function (lexfunc) and full additive (fulladd).6
For model implementation and (unsupervised) es-
timation, we rely on the freely available DISSECT
toolkit (Dinu et al, 2013).7 For all methods, vectors
were normalized before composing, both in training
and in generation. Table 2 presents a summary de-
scription of the composition methods we used.
Following previous literature (Mitchell and Lap-
ata, 2010), and the general intuition that adjectival
modification is quite a different process from noun
combination (Gagne? and Spalding, 2009; McNally,
2013), we learn different parameters for noun-noun
(NN) and adjective-noun (AN) phrases. As an ex-
ample of the learned parameters, for the wadd model
the ratio of parameters w1 and w2 is 1:2 for ANs,
whereas for NNs it is almost 1:1, confirming the in-
tuition that a non-head noun plays a stronger role in
composition than an adjective modifier.
4http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
5For tuning the parameters of the semantic space, we com-
puted the correlation of cosines produced with a variety of pa-
rameter settings (SVD/NMF/no reduction, PMI/Local MI/raw
counts/log transform, 150 to 300 dimensions in steps of 50) with
the word pair similarity ratings in the MEN dataset: http:
//clic.cimec.unitn.it/?elia.bruni/MEN
6We do not consider the popular multiplicative model, as it
produces identical representations for NPs irrespective of their
internal structure.
7http://clic.cimec.unitn.it/composes/
toolkit/
1909
Model Composition function Parameters
wadd w1~u+ w2~v w1, w2
dil ||~u||22~v + (?? 1)?~u,~v?~u ?
fulladd W1~u+W2~v W1,W2 ? Rm?m
lexfunc Au~v Au ? Rm?m
Table 2: Composition functions of inputs (u, v).
Recursive composition In this study we also ex-
periment with recursive composition; to the best
of our knowledge, this is the first time that these
composition functions have been explicitly used in
this manner. For example, given the left brack-
eted NP (blood pressure) medicine, we want to
obtain its compositional semantic representation,
???????????????????
blood pressure medicine. First, basic composition
is applied, in which
????
blood and ???????pressure are com-
bined with one of the composition functions. Fol-
lowing that, we apply recursive composition; the
output of basic composition, i.e.,
???????????
blood pressure,
is fed to the function again to be composed with the
representation of
???????
medicine.
The latter step is straightforward for all com-
position functions except lexfunc applied to left-
bracketed NPs, where the first step should return a
matrix representing the left constituent (blood pres-
sure in the running example). To cope with this nui-
sance, we apply the lexfunc method to basic compo-
sition only, while recursive representations are de-
rived by summing (e.g.,
???????????
blood pressure is obtained
by multiplying the blood matrix by the pressure vec-
tor, and it is then summed to
???????
medicine).
3 Experiments
Semantic plausibility measures We use mea-
sures of semantic plausibility computed on com-
posed semantic representations introduced by Vec-
chi et al (2011). The rationale is that the correct
(wrong) bracketing will lead to semantically more
(less) plausible phrases. Thus, a measure able to dis-
criminate semantically plausible from implausible
phrases should also indicate the most likely parse.
Considering, for example, the alternative parses of
miracle home run, we observe that home run is
a more semantically plausible phrase than miracle
home. Furthermore, we might often refer to a base-
ball player?s miracle home run, but we doubt that
even a miracle home can run! Given the com-
posed representation of an AN (or NN), Vecchi et
al. (2011) define the following measures:
? Density, quantified as the average cosine of a
phrase with its (top 10) nearest neighbors, cap-
tures the intuition that a deviant phrase should
be isolated in the semantic space.
? Cosine of phrase and head N aims to capture
the fact that the meaning of a deviant AN (or
NN) will tend to diverge from the meaning of
the head noun.
? Vector length should capture anomalous vec-
tors.
Since length, as already observed by Vecchi et al,
is strongly affected by independent factors such as
input vector normalization and the estimation pro-
cedure, we introduce entropy as a measure of vec-
tor quality. The intuition is that meaningless vec-
tors, whose dimensions contain mostly noise, should
have high entropy.
NP Parsing as Classification Parsing NPs con-
sisting of three elements can be treated as bi-
nary classification; given blood pressure medicine,
we predict whether it is left- ((blood pres-
sure) medicine) or right-bracketed (blood (pressure
medicine)).
We conduct experiments using an SVM with Ra-
dial Basis Function kernel as implemented in the
scikit-learn toolkit.8 Our dataset is split into 10 folds
in which the ratio between the two classes is kept
constant. We tune the SVM complexity parameter
C on the first fold and we report accuracy results on
the remaining nine folds after cross-validation.
Features Given a composition function f , we de-
fine the following feature sets, illustrated with the
usual blood pressure medicine example, which are
used to build different classifiers:
? fbasic consists of the semantic plausibility
measures described above computed for the
two-word phrases resulting from alternative
bracketings, i.e., 3 measures for each bracket-
ing, evaluated on blood pressure and pressure
medicine respectively, for a total of 6 features.
? frec contains 6 features computed on the vec-
tors resulting from the recursive compositions
8http://scikit-learn.org/
1910
Features Accuracy
right 65.6
pos 77.3
lexfuncbasic 74.6
lexfuncrec 74.0
lexfuncplausibility 76.2
waddbasic 75.9
waddrec 78.2
waddplausibility 78.7
pmi 81.2
pmi+lexfuncplausibility 82.9
pmi+waddplausibility 85.6
Table 3: Evaluation of feature sets from Section 3
(blood pressure) medicine and blood (pressure
medicine).
? fplausibility concatenates fbasic and frec.
? pmi contains the PMI scores extracted from
our corpus for blood pressure and pressure
medicine.9
? pmi + fplausibility concatenates pmi and
fplausibility.
Baseline Model Given the skewed bracketing dis-
tribution in our dataset, we implement the following
majority baselines: a) right classifies all phrases
as right-bracketed; b) pos classifies NNN as left-
bracketed (Lauer, 1995), ANN as right-bracketed.
4 Results and Discussion
Table 3 omits results for dil and fulladd since they
were outperformed by the right baseline. That
wadd- and lexfunc-based plausibility features per-
form well above this baseline is encouraging, since
it represents the typical default behaviour of parsers
for NPs, although note that these features perform
comparably to the pos baseline, which would be
quite simple to embed in a parser (for English, at
least). For both models, using both basic and recur-
sive features leads to a boost in performance over
basic features alone. Note that recursive features
(frec) achieve at least equal or better performance
than basic ones (fbasic). We expect indeed that in
many cases the asymmetry in plausibility will be
9Several approaches to computing PMI for these purposes
have been proposed in the literature including the dependency
model (Lauer, 1995) and the adjacency model (Marcus, 1980).
We implement the latter since it has been shown to perform
better (Vadas and Curran, 2007b) on NPs extracted from Penn
TreeBank.
sharper when considering the whole NP rather than
its sub-parts; a pressure medicine is still a conceiv-
able concept, but blood (pressure medicine) makes
no sense whatsoever. Finally, wadd outperforms
both the more informative baseline pos and lexfunc.
The difference between wadd and lexfunc is signif-
icant (p < 0.05)10 only when they are trained with
recursive composition features, probably due to our
suboptimal adaptation of the latter to recursive com-
position (see Section 2).
The pmi approach outperforms the best
plausibility-based feature set waddplausibility.
However, the two make only a small proportion of
common errors (29% of the total waddplausibility
errors, 32% for pmi), suggesting that they are com-
plementary. Indeed the pmi + waddplausibility
combination significantly outperforms pmi alone
(p < 0.001), indicating that plausibility features
can improve NP bracketing on top of the pow-
erful PMI-based approach. The same effect can
also be observed in the combination of pmi +
lexfuncplausibility, which again significantly
outperforms pmi alone (p < 0.05). This behaviour
further suggests that the different types of errors are
not a result of the parameters or type of composi-
tion applied, but rather highlights fundamental dif-
ferences in the kind of information that PMI and
composition models are able to capture.
One hypothesis is that compositional models are
more robust for low-frequency NPs, for which
PMI estimates will be less accurate; results on
those low-frequency trigrams only (20% of the NP
dataset, operationalized as those consisting of bi-
grams with frequency ? 100) revealed indeed that
waddplausibility performed 8.1% better in terms
of accuracy than pmi.
5 Conclusion
Our pilot study showed that semantic plausibility,
as measured on compositional distributional repre-
sentations, can improve syntactic parsing of NPs.
Our results further suggest that state-of-the-art PMI
features and the ones extracted from compositional
representations are complementary, and thus, when
combined, can lead to significantly better results.
Besides paving the way to a more general integration
10Significance values are based on t-tests.
1911
of compositional distributional semantics in syntac-
tic parsing, the proposed methodology provides a
new way to evaluate composition functions.
The relatively simple-minded wadd approach out-
performed more complex models such as lexfunc.
We plan to experiment next with more linguistically
motivated ways to adapt the latter to recursive com-
position, including hybrid methods where ANs and
NNs are treated differently. We would also like
to consider more sophisticated semantic plausibility
measures (e.g., supervised ones), and apply them to
other ambiguous syntactic constructions.
6 Acknowledgments
We thank Georgiana Dinu and German Kruszewski
for helpful discussions and the reviewers for use-
ful feedback. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational linguis-
tics, 29(4):589?637.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Composi-
tion Toolkit. In Proceedings of the System Demonstra-
tions of ACL 2013, Sofia, Bulgaria.
Charles Fillmore. 1968. The case for case. In Emmon
Bach and Robert Harms, editors, Universals in Lin-
guistic Theory, pages 1?89. Holt, Rinehart and Win-
ston, New York.
Christina Gagne? and Thomas Spalding. 2009. Con-
stituent integration during the processing of compound
words: Does it involve the use of relational structures?
Journal of Memory and Language, 60:20?35.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37, Up-
psala, Sweden.
Dan Klein and Christopher D Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430. Association for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, pages 47?54, Cambridge, MA.
Phong Le, Willem Zuidema, and Remko Scha. 2013.
Learning from errors: Using vector-based composi-
tional semantics for parse reranking. In Proceedings of
the ACL 2013 Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):313?330.
Mitchell P Marcus. 1980. Theory of syntactic recogni-
tion for natural languages. MIT press.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17?24, Stroudsburg, PA, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Stroudsburg, PA, USA.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of the
COLING, pages 886?894, Beijing, China.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Korea.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Proceedings of ACL, Sofia, Bul-
garia.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
David Vadas and James Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL, pages 240?247, Prague, Czech Republic.
David Vadas and James R Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In Pro-
ceedings of the PACLING, pages 104?112.
David Vadas and James R. Curran. 2011. Parsing
noun phrases in the penn treebank. Comput. Linguist.,
37(4):753?809.
1912
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1?9, Portland,
OR.
1913
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960?1970,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Of words, eyes and brains:
Correlating image-based distributional semantic models
with neural representations of concepts
Andrew J. Anderson, Elia Bruni, Ulisse Bordignon, Massimo Poesio and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, C.so Bettini 31, Rovereto, Italy)
first.last@unitn.it
Abstract
Traditional distributional semantic models ex-
tract word meaning representations from co-
occurrence patterns of words in text cor-
pora. Recently, the distributional approach has
been extended to models that record the co-
occurrence of words with visual features in
image collections. These image-based models
should be complementary to text-based ones,
providing a more cognitively plausible view
of meaning grounded in visual perception. In
this study, we test whether image-based mod-
els capture the semantic patterns that emerge
from fMRI recordings of the neural signal.
Our results indicate that, indeed, there is a
significant correlation between image-based
and brain-based semantic similarities, and that
image-based models complement text-based
ones, so that the best correlations are achieved
when the two modalities are combined. De-
spite some unsatisfactory, but explained out-
comes (in particular, failure to detect differ-
ential association of models with brain areas),
the results show, on the one hand, that image-
based distributional semantic models can be a
precious new tool to explore semantic repre-
sentation in the brain, and, on the other, that
neural data can be used as the ultimate test set
to validate artificial semantic models in terms
of their cognitive plausibility.
1 Introduction
Many recent neuroscientific studies have brought
support to the view that concepts are represented
in terms of patterns of neural activation over broad
areas, naturally encoded as vectors in a neural se-
mantic space (Haxby et al, 2001; Huth et al, 2012).
Similar representations are also widely used in com-
putational linguistics, and in particular in distribu-
tional semantics (Clark, 2012; Erk, 2012; Turney
and Pantel, 2010), that captures meaning in terms
of vectors recording the patterns of co-occurrence
of words in large corpora, under the hypothesis that
words that occur in similar contexts are similar in
meaning.
Since the seminal work of Mitchell et al (2008),
there has thus being interest in investigating whether
corpus-harvested semantic representations can con-
tribute to the study of concepts in the brain. The
relation is mutually beneficial: From the point of
view of brain activity decoding, a strong correlation
between corpus-based and brain-derived conceptual
representations would mean that we could use the
former (much easier to construct on a very large
scale) to make inferences about the second: e.g., us-
ing corpus-based representations to reconstruct the
likely neural signal associated to words we have no
direct brain data for. From the point of view of com-
putational linguistics, neural data provide the ulti-
mate testing ground for models that strive to cap-
ture important aspects of human semantic mem-
ory (much more so than the commonly used ex-
plicit semantic rating benchmarks). If we found that
a corpus-based model of meaning can make non-
trivial predictions about the structure of the semantic
space in the brain, that would make a pretty strong
case for the intriguing idea that the model is approx-
imating, in interesting ways, the way in which hu-
mans acquire and represent semantic knowledge.
1960
We take as our starting point the extensive experi-
ments reported in Murphy et al (2012), who showed
that purely corpus-based distributional models are at
least as good at brain signal prediction tasks as ear-
lier models that made use of manually-generated or
controlled knowledge sources (Chang et al, 2011;
Palatucci et al, 2009; Pereira et al, 2011), and we
evaluate a very recent type of distributional model,
namely one that is not extracted from textual data
but from image collections through automated vi-
sual feature extraction techniques. It has been ar-
gued that this new generation of image-based dis-
tributional models (Bruni et al, 2011; Bruni et al,
2012b; Feng and Lapata, 2010; Leong and Mihal-
cea, 2011) provides a more realistic view of mean-
ing, since humans obviously acquire a large propor-
tion of their semantic knowledge from perceptual
data. The first question that we ask, thus, is whether
the more ?grounded? image-based models can help
us in interpreting conceptual representations in the
brain. More specifically, we will compare the per-
formance of different image-based representations,
and we will test whether text- and image-based rep-
resentations are complementary, so that when used
together they can better account for patterns in neu-
ral data. Finally, we will check for differences be-
tween anatomical regions in the degree to which text
and/or image models are effective, as one might ex-
pect given the well-known functional specializations
of different anatomical regions.
2 Brain data
We use the data that were recorded and preprocessed
by Mitchell et al (2008), available for download in
their supporting online material.1 Full details of the
experimental protocol, data acquisition and prepro-
cessing can be found in Mitchell et al (2008) and
the supporting material. Key points are that there
were nine right-handed adult participants (5 female,
age between 18 and 32). The experimental task was
to actively think about the properties of sixty objects
that were presented visually, each as a line drawing
in combination with a text label. The entire set of
objects was presented in a random order in six ses-
sions, each object remained on screen for 3 seconds
with a seven second fixation gap between presenta-
1http://www.cs.cmu.edu/?tom/science2008/
tions.
Mitchell and colleagues examined 12 categories,
five objects per category, for a total of 60 concepts
(words). Due to coverage limitations, we use 51/60
words representing 11/12 categories. Table 1 con-
tains the full list of 51 words organized by category.
fMRI acquisition and preprocessing Mitchell et
al. (2008) acquired functional images on a Siemens
Allegra 3.0T scanner using a gradient echo EPI
pulse sequence with TR=1000 ms, TE=30 ms and
a 60? angle. Seventeen 5-mm thick oblique-axial
slices were imaged with a gap of 1-mm between
slices. The acquisition matrix was 64?64 with
3.125?3.125?5-mm voxels. They subsequently
corrected data for slice timing, motion, linear trend,
and performed temporal smoothing with a high-pass
filter at 190s cutoff. The data were normalized to
the MNI template brain image, spatially normalized
into MNI space and resampled to 3?3?6 mm3 vox-
els. The voxel-wise percent signal change relative to
the fixation condition was computed for each object
presentation. The mean of the four images acquired
4s post stimulus presentation was used for analysis.
To create a single representation per object per
participant, we took the voxel-wise mean of the six
presentations of each word. Likewise to create a sin-
gle representation per category per participant, we
took the voxel-wise mean of all word models per
category, per participant.
Anatomical parcellation Analysis was conducted
on the whole brain, and to address the question of
whether there are differences in models? effective-
ness between anatomical regions, brains were fur-
ther partitioned into frontal, parietal, temporal and
occipital lobes. This partitioning is coarse (each lobe
is large and serves many diverse functions), but, for
an initial test, appropriate, given that each lobe has
specialisms that on face value are amenable to inter-
pretation by our different distributional models and
the exact nature of specialist processing in localised
areas is often subject to debate (so being overly re-
strictive may be risky). Formulation of the distribu-
tional models is described in detail in the Section 3,
but for now it is sufficient to know that the Object
model is derived from image statistics of the object
depicted in images, Context from image statistics of
the background scene, Object&Context is a com-
1961
Animals Bear, Cat, Cow, Dog Horse
Building Apartment, Barn, Church, House
Building parts Arch, Chimney, Closet, Door, Window
Clothing Coat, Dress, Pants, Shirt, Skirt
Furniture Bed, Chair, Desk, Dresser, Table
Insect Ant, Bee, Beetle, Butterfly, Fly
Kitchen utensils Bottle, Cup, Glass, Knife, Spoon
Man made objects Bell, Key, Refrigerator, Telephone, Watch
Tool Chisel, Hammer, Screwdriver
Vegetable Celery, Corn, Lettuce, Tomato
Vehicle Airplane, Bicycle, Car, Train, Truck
Table 1: The 51 words represented by the brain and the distributional models, organized by category.
bination of the two, and Window2 is a text-based
model.
The occipital lobe houses the primary visual pro-
cessing system and consequently it is reasonable
to expect some bias toward image-based semantic
models. Furthermore, given that experimental stim-
uli incorporated line drawings of the object,and the
visual cortex has a well-established role in process-
ing low-level visual statistics including edge detec-
tion (Bruce et al, 2003), we naturally expected a
good performance from Object (formulated from
edge orientation histograms of similar objects).
Following Goodale and Milner (1992)?s influ-
ential perception-action model (see McIntosh and
Schenk (2009) for recent discussion), visual infor-
mation is channeled from the occipital lobe in two
streams: a perceptual stream, serving object identi-
fication and recognition; and an action stream, spe-
cialist in processing egocentric spatial relationships
and ultimately supporting interaction with the world.
The perceptual stream leads to the temporal lobe.
Here the fusiform gyrus (shared with the occipital
lobe) plays a general role in object categorisation
(e.g., animals and tools (Chao et al, 1999), faces
(Kanwisher and Yovel, 2006), body parts (Peelen
and Downing, 2005) and even word form percep-
tion (McCandliss et al, 2003)). As the parahip-
pocampus is strongly associated with scene repre-
sentation (Epstein, 2008), we expect both the Object
and Context models to capture variability in the tem-
poral lobe. Of wider relevance to semantic process-
ing, the medial temporal gyrus, inferior temporal
gyrus and ventral temporal lobe have generally been
implicated to have roles in supramodal integration
and concept retrieval (Binder et al, 2009). Given
this, we expected that incorporating text would also
be valuable and that the Window2&Object&Context
combination would be a good model.
The visual action stream leads from the occipi-
tal lobe to the parietal lobe to support spatial cog-
nition tasks and action control (Sack, 2009). In
that there seems to be an egocentric frame of ref-
erence, placing actor in environment, it is tempt-
ing to speculate that the Context model is more ap-
propriate than the Object model here. As the pari-
etal lobe also contains the angular gyrus, thought
to be involved in complex, supra-modal information
integration and knowledge retrieval (Binder et al,
2009), we might again forecast that integrating text
and image information would boost performance, so
Window2&Context was earmarked as a strong can-
didate.
The frontal lobe, is traditionally associated with
high-level processing and manipulation of abstract
knowledge and rules and controlled behaviour
(Miller et al, 2002). Regarding semantics, the dor-
somedial prefrontal cortex has been implicated in
self-guided retrieval of semantic information (e.g.,
uncued speech production), the ventromedial pre-
frontal cortex in motivation and emotional process-
ing, the inferior frontal gyrus in phonological and
syntactic processing, (Binder et al, 2009) and in-
tegration of lexical information (Hagoort, 2005).
Given the association with linguistic processing we
anticipated a bias in favour of Window2.
The four lobes were identified and partitioned
using Tzourio-Mazoyer et al (2002)?s automatic
anatomical labelling scheme.
1962
Voxel selection The set of 500 most stable voxels,
both within the whole brain and from within each
region of interest were identified for analysis. The
most stable voxels were those showing consistent
variation across the different stimuli between scan-
ning sessions. Specifically, and following a similar
strategy to Mitchell et al (2008), for each voxel, the
set of 51 words from each unique pair of scanning
sessions were correlated using Pearson?s correlation
(6 sessions and therefore 15 unique pairs), and the
mean of the 15 resulting correlation coefficients was
taken as the measure of stability. The 500 voxels
with highest mean correlations were selected.
3 Distributional models
Distributional semantic models approximate word
meaning by keeping track of word co-occurrence
statistics from large textual input, relying on the dis-
tributional hypothesis: The meaning of a word can
be induced by the context in which it occurs (Turney
and Pantel, 2010). Despite their great success, these
models still rely on verbal input only, while humans
base their meaning representation also on perceptual
information (Louwerse, 2011).
Thanks to recent developments in computer vi-
sion, it is nowadays possible to take the visual per-
ceptual channel into account, and build new com-
putational models of semantics enhanced with vi-
sual information (Feng and Lapata, 2010; Bruni et
al., 2011; Leong and Mihalcea, 2011; Bergsma and
Goebel, 2011; Bruni et al, 2012a). Given a set of
target concepts and a collection of images depicting
those concepts, it is indeed possible to first encode
the image content into low-level features, and subse-
quently convert it into a higher-level representation
based on the bag-of-visual-words method (Grauman
and Leibe, 2011). Recently, Bruni et al (2012b)
have shown that better semantic representations can
be extracted if we first localize the concept in the
image, and then extract distinct higher-level features
(visual words) from the box containing the concept
and from the surrounding context. We also follow
this strategy here.
In our experiments we utilize both traditional text-
based models and experimental image-based mod-
els, as well as their combination.
3.1 Textual models
Verb We experiment with the original text-based
semantic model used to predict fMRI patterns by
Mitchell et al (2008). Each object stimulus word
is represented as a 25-dimensional vector, with each
value corresponding to the normalized sentence-
wide co-occurrence of that word with one of 25
manually-picked sensorimotor verbs (such as see,
hear, eat, . . . ) in a trillion word text corpus.
Window2 To create this model, we collect text
co-occurrence statistics from the freely available
ukWaC and Wackypedia corpora combined (about 3
billion words in total).2 As collocates of our distri-
butional model we select a set of 30K words, namely
the top 20K most frequent nouns, 5K most frequent
adjectives and 5K most frequent verbs.
In the tradition of HAL (Lund and Burgess, 1996),
the model is based on co-occurrence statistics with
collocates within a fixed-size window of 2 to the left
and right of each target word. Despite their sim-
plicity, narrow-window-based models have shown
to achieve state-of-the-art results in various stan-
dard semantic tasks (Bullinaria and Levy, 2007)
and to outperform both document-based and syntax-
based models trained on the same corpus (Bruni et
al., 2012a). Moreover, in Murphy et al (2012) a
window-based model very similar to ours was not
significantly worse than their best model for brain
decoding. We tried also a few variations, e.g., us-
ing a larger window or different transformations on
the raw co-occurrences from those presented below,
but with little, insignificant changes in performance.
Given that our focus here is on visual information,
we only report results for Window2 and its combi-
nation with visual models.
3.2 Visual models
Our visual models are inspired by Bruni et al
(2012b), that have explored to what extent extract-
ing features from images where objects are local-
ized results in better semantic representations. They
found that extracting visual features separately from
the object and its surrounding context leads to bet-
ter performance than not using localization, and us-
ing only object- and, more surprisingly, context-
extracted features also results in performant models
2http://wacky.sslmit.unibo.it/
1963
(especially when evaluating inter-object similarity,
the context in which an object is located can signif-
icantly contribute to semantic representation, in cer-
tain cases carrying even more information than the
depicted object itself).
More in detail, with localization the visual fea-
tures (visual words) can be extracted from the ob-
ject bounding box (in our experiments, the Object
model) or from only outside the object box (the
Context model). A combined model is obtained
by concatenating the two feature vectors (the Ob-
ject&Context model).
Visual model construction pipeline To extract
visual co-occurrence statistics, we use images from
ImageNet (Deng et al, 2009),3 a very large im-
age database organized on top of the WordNet hi-
erarchy (Fellbaum, 1998). ImageNet has more than
14 million images, covering 21K WordNet nominal
synsets. ImageNet stands out for the high quality of
its images, both in terms of resolution and concept
annotations. Moreover, for around 3K concepts, an-
notations of object bounding boxes is provided. This
last feature allows us to exploit object localization
within our experiments.
To build visual distributional models, we utilize
the bag-of-visual-words (BoVW) representation of
images (Sivic and Zisserman, 2003; Csurka et al,
2004). Inspired by NLP, BoVW discretizes the im-
age content in terms of a histogram of visual word
counts. Differently from NLP, in vision there is not a
natural notion of visual words, hence a visual vocab-
ulary has to be built from scratch. The process works
as follows. First, a large set of low-level features is
extracted from a corpus of images. The low-level
feature vectors are subsequently clustered into dif-
ferent regions (visual words). Given then a new im-
age, each of the low-level feature vectors extracted
from the patches that compose it is mapped to the
nearest visual word (e.g., in terms of Euclidean dis-
tance from the cluster centroid) such that the image
can be represented with a histogram counting the in-
stances of each visual word in the image.
As low-level features we use SIFT, the Scale In-
variant Feature Transform (Lowe, 2004). SIFT fea-
tures are good at capturing parts of objects and are
designed to be invariant to image transformations
3http://www.image-net.org/
such as change in scale, rotation and illumination.
To construct the visual vocabulary, we cluster the
SIFT features into 25K different clusters.4 We add
also spatial information by dividing the image into
several subregions, representing each of them in
terms of BoVW and then stacking the resulting his-
tograms (Lazebnik et al, 2006). We use in total 8
different regions, obtaining a final vector of 200K
dimensions (25K visual words ? 8 regions). Since
each concept in our dataset is represented by mul-
tiple images, we pool the visual word occurrences
across images by summing them up into a single
vector.
To perform the entire visual pipeline we use
VSEM, an open library for visual semantics (Bruni
et al, 2013).5
3.3 Model transformations and combination
Once both the textual and the visual models are built,
we perform two different transformations on the raw
co-occurrence counts. First, we transform them into
nonnegative Pointwise Mutual Information (PMI)
association scores (Church and Hanks, 1990). As a
second transformation, we apply dimensionality re-
duction to the two matrices. In particular, we adopt
the Singular Value Decomposition (SVD), one of the
most effective methods to approximate the original
data in lower dimensionality space (Schu?tze, 1997),
and reduce the vectors to 50 dimensions.
To combine text- and image-based semantic mod-
els in a joint representation, we separately normalize
their vectors to unit length, and concatenate them,
along the lines of Bruni et al (2011). More sophis-
ticated combination models have been proposed in
the recent literature on multimodal semantics. For
example, Bruni et al (2012a) use SVD as a mix-
ing strategy, given its ability to smooth the matrices
and uncover latent dimensions. Another example is
Silberer and Lapata (2013), where Canonical Corre-
lation Analysis is used. We reserve the exploration
of more advanced combination methods for further
studies.
Finally, to represent the 11 categories we experi-
ment with (see Table 1), we average the vectors of
the concepts they include.
4We use k-means, the most commonly employed clustering
algorithm for this task.
5http://clic.cimec.unitn.it/vsem/
1964
4 Experiments
A question is posed over how to evaluate the rela-
tionship between the different distributional models
and brain data. Comparing each model?s predictive
performance using the same strategy as Mitchell et
al. (2008) (also followed by Murphy et al (2012))
is one possibility: they used multiple regression to
relate distributional codes to individual voxel activa-
tions, thus allowing brain states to be estimated from
previously unseen distributional codes. Regression
models were trained on 58/60 words and in testing
the regression models estimated the brain state as-
sociated with the 2 unseen distributional codes. The
predicted brain states were compared with the actual
fMRI data, and the process repeated for each per-
mutation of left-out words, to build a metric of pre-
diction accuracy. For our purposes, a fair compari-
son of models using this strategy is complicated by
differences in dimensionality between both seman-
tic models and lobes (which we compare to other
lobes) in association with the comparatively small
number of words in the fMRI data set. Large dimen-
sionality models risk overfitting the data, and it is a
nuisance to try to reliably correct for the effects of
overfitting in performance comparisons. Not least,
to thoroughly evaluate all possible cross-validation
permutations is demanding in processing time, and
we have many models to compare.
An alternative approach, and that which we
have adopted, is representational similarity analy-
sis (Kriegeskorte et al, 2008). Representational
similarity analysis circumvents the previous prob-
lems by abstracting each fMRI/distributional data
source to a common structure capturing the inter-
relationships between each pair of data items (e.g.,
words). Specifically, for each model/participant?s
fMRI data/anatomical region, the similarity struc-
ture was evaluated by taking the pairwise correla-
tion (Pearson?s correlation coefficient) between all
unique category or word combinations. This pro-
duced a list of 55 category pair correlations and 121
word pair correlations for each data source. For all
brain data, correlation lists were averaged across the
nine participants to produce a single list of mean
word pair correlations and a single list of mean cat-
egory pair correlations for each anatomical region
and the whole brain. Then to provide a measure of
similarity between models and brain data, the cor-
relation lists for respective data sources were them-
selves correlated using Spearman?s rank correlation.
Statistical significance was tested using a permuta-
tion test: The word-pair (or category-pair) labels
were randomly shuffled 10,000 times to estimate a
null distribution when the two similarity lists are
not correlated. The p-value is calculated as the pro-
portion of random correlation coefficients that are
greater than or equal to the observed coefficient.
5 Results
5.1 Category-level analyses
Do image models correlate with brain data? Ta-
ble 2 displays results of Spearman?s correlations be-
tween the per-category similarity structure of dis-
tributional models and brain data. There is a sig-
nificant correlation between every purely image-
based model and the occipital, parietal and tempo-
ral lobes, and also the whole brain (.38? ? ?.51,
all p?.01). The frontal lobe is less well described.
Still, whilst not significant, correlations are only
marginally above the conventional p = .05 cutoff
(all are less than p = .064). This strongly suggests
that the answer to our first question is yes: distri-
butional models derived from images can be used
to explain concept fMRI data. Otherwise Window2
significantly correlates with the whole brain and all
anatomical regions except for the frontal lobe where
?=.34, p = .07. In contrast Verb (the original, par-
tially hand-crafted model used by Mitchell and col-
leagues) captures inter-relationships poorly and nei-
ther correlates with the whole brain or any lobe.
Do different models correlate with different
anatomical regions? 2-way ANOVA without
replication was used to test for differences in cor-
relation coefficients between the five pure-modality
models (Verb, Window2, Object, Context and Ob-
ject&Context), and the four brain lobes. This re-
vealed a highly significant difference between mod-
els F(4,12)=45.2, p<.001. Post-hoc 2-tailed t-tests
comparing model pairs found that Verb differed sig-
nificantly from all other models (correlations were
lower). There was a clear difference even when Verb
(mean?sd over lobes = .1?.1) was compared to the
second weakest model, Object (mean?sd=.4?.09),
where t =-7.7, p <.01, df=4. There were no
1965
Frontal Parietal Occipital Temporal Whole-Brain
Verb 0.00 (0.51) 0.06 (0.37) 0.24 (0.10) 0.07 (0.35) 0.17 (0.17)
Window2 0.34 (0.06) 0.49 (0.00) 0.47 (0.01) 0.47 (0.00) 0.44 (0.00)
Object 0.27 (0.07) 0.38 (0.02) 0.45 (0.00) 0.47 (0.00) 0.43 (0.01)
Context 0.33 (0.06) 0.50 (0.00) 0.44 (0.00) 0.44 (0.01) 0.44 (0.01)
Object&Context 0.32 (0.05) 0.48 (0.00) 0.51 (0.00) 0.49 (0.00) 0.49 (0.00)
Window2&Object 0.32 (0.06) 0.45 (0.00) 0.52 (0.00) 0.53 (0.00) 0.49 (0.00)
Window2&Context 0.39 (0.04) 0.57 (0.00) 0.53 (0.00) 0.55 (0.00) 0.51 (0.00)
Window2&Object&Context 0.37 (0.04) 0.52 (0.00) 0.55 (0.00) 0.55 (0.00) 0.53 (0.00)
Table 2: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 11 categories. In each column the first value corre-
sponds to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.
Frontal Parietal Occipital Temporal Whole-Brain
Verb -0.04 (0.72) 0.09 (0.06) 0.07 (0.20) 0.03 (0.31) 0.07 (0.18)
Window2 0.07 (0.13) 0.19 (0.00) 0.12 (0.06) 0.21 (0.00) 0.13 (0.04)
Object 0.01 (0.40) 0.08 (0.07) 0.17 (0.01) 0.18 (0.00) 0.17 (0.01)
Context 0.04 (0.24) 0.14 (0.01) 0.01 (0.44) 0.12 (0.02) 0.02 (0.38)
Object&Context 0.03 (0.31) 0.13 (0.01) 0.10 (0.07) 0.17 (0.00) 0.11 (0.06)
Window2&Object 0.04 (0.24) 0.16 (0.00) 0.16 (0.01) 0.23 (0.00) 0.17 (0.00)
Window2&Context 0.07 (0.12) 0.20 (0.00) 0.09 (0.11) 0.22 (0.00) 0.11 (0.07)
Window2&Object&Context 0.05 (0.18) 0.18 (0.00) 0.12 (0.05) 0.23 (0.00) 0.13 (0.02)
Table 3: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 51 words. In each column the first value corresponds
to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.
other significant differences between models. How-
ever there was a highly significant difference be-
tween lobes F(3,12)=13.77, p <.001. Post-hoc 2-
tailed t-tests comparing lobe pairs found that the
frontal lobe yielded significantly different correla-
tions (lower) than each other lobe. When the frontal
lobe (mean?sd over models = .25?.14) was com-
pared to the second weakest anatomical region, the
parietal lobe (mean?sd=.38?.19), the difference
was highly significant, t =-8, df=3, p <.01. This
introduces the question of whether this difference in
correlations is the result of differences in neural cat-
egory organisation and representation, or differences
in the quality of the signal, which we address next.
Category-level inter-correlations between lobes
were all relatively strong and highly significant. The
occipital lobe was found to be the most distinct, be-
ing similar to the temporal lobe (?=.71, p <.001),
but less so to the parietal and frontal lobes (?=.53,
p <.001 and ?=.57, p <.001 respectively). The
temporal lobe shows roughly similar levels of cor-
relation to each other lobe (all .71? ? ?.73, all
p <.001). The frontal and parietal lobes are related
most strongly to each other (?=.77, p <.001), to a
slightly lesser extent to the temporal lobe (in both
cases ?=.73, p <.001) and least so to the occipital
lobe. These strong relationships are consistent with
there being a broadly similar category organisation
across lobes.
To appraise this assertion in the context of the
previously detected difference between the frontal
lobe and all other lobes, we examine the raw cat-
egory pair similarity matrices derived from the oc-
cipital lobe and the frontal lobe (Figure 1). All the
below observations are qualitative. Although it is
difficult to have intuitions about the relative differ-
ences between all category pairs (e.g., whether tools
or furniture should be more similar to animals), we
might reasonably expect some obvious similarities.
For instance, for animals to be visually similar to in-
1966
sects and clothing, because all have legs and arms
and curves (of course we would not expect a strong
relationship between insects and clothes in function
or other modalities such as sound), buildings to be
similar to building parts and vehicles (hard edges
and windows), building parts to be similar to furni-
ture (e.g., from Table 1 we see there is some overlap
in category membership between these categories,
such as closet and door) and tools to be similar to
kitchen utensils. All of these relationships are main-
tained in the occipital lobe, and many are visible in
the frontal lobe (including the similarity between in-
sects and clothes), however there are exceptions that
are difficult to explain e.g., within the frontal lobe,
building parts are not similar to furniture, kitchen
utensils are closer to clothing than to tools and ve-
hicles are more similar to clothing than anything
else. As such we conclude that category-level rep-
resentations were similar across lobes with differ-
ences likely due to variation in signal quality be-
tween lobes.
Are text- and image-based semantic models com-
plementary? Turning to the question of whether
text- and image-derived semantic information can
be complementary, we observe from Table 2 that
there is not a single instance of a joint model with
a weaker correlation than its pure-image counter-
part. The Window2 model showed a stronger cor-
relation than the Window2&Object model for the
frontal and parietal lobes, but was weaker than Win-
dow2&Object&Context and Window2&Context in
all tests and was also weaker than any joint model
in whole-brain comparisons. The mean?sd correla-
tions for all purely image-based results pooled over
lobes (3 models * 4 lobes) was .42?.08 in com-
parison to .49?.08 for the joint models. The rel-
ative performance of Object vs. Context vs. Ob-
ject&Context on the four different lobes is preserved
between image-based and joint models: correlating
the 12 combinations using Spearman?s correlation
gives ?=.85, p <.001. Differences can be statis-
tically quantified by pooling all image related cor-
relation coefficients for each anatomical region (3
models * 4 regions), as for the respective joint mod-
els, and comparing with a 2-tailed Wilcoxon signed
rank test. Differences were highly significant (W=0,
p <.001,n=12). This evidence accumulates to sug-
Figure 1: Similarity (Pearson correlation) between each
category pair in (top) occipital and (bottom) frontal lobes.
gest that text and image-derived semantic informa-
tion can be complementary in interpreting concept
fMRI data.
5.2 Word-level analyses
Do image models capture word pair similari-
ties? Per-word results generally corroborate the
relationships observed in the previous section in
the sense that Spearman?s correlation between per-
word and per-category results for the 40 combina-
tions of models and lobes was ?=.78, p <.001.
There were differences, most obviously a dramatic
drop in the strength of correlation coefficients for
the per-word results, visible in Table 3. Subsets
1967
of per-word image-based models correlated with
three lobes and the whole brain. Correlations corre-
sponding to significance values of p <.05 were ob-
served in the temporal and parietal lobes, for Con-
text, Object&Context and Window2 whereas Ob-
ject was correlated with the occipital and temporal
lobes (p <.05). 2-way ANOVA without replica-
tion was used to test for differences between mod-
els and lobes. This revealed a significant differ-
ence between models (F(4,12)=4.05, p=.027). Post-
hoc t-tests showed that the Window2 model signifi-
cantly differed from (was stronger than) the Context
(t=3.8, p =.03, df=3) and Object&Context models
(t =4.5, p =.02, df=3). There were no other signifi-
cant differences between models. There was again a
significant difference between lobes (F(3,12)=7.89,
p < .01), with the frontal lobe showing the weak-
est correlations. Post-hoc 2-tailed t-tests comparing
lobe-pairs found that the frontal lobe differed signif-
icantly (correlations were weaker) from the parietal
(t =-9, p <.001, df=4) and temporal lobes (t =-6.4,
p <.01, df=4) but not from the occipital lobe (t =-
2.18, p =.09, df=4). No other significant differences
between lobes were observed.
Are there differences between models/lobes?
Word-level inter-correlations between lobes were all
significant and the pattern of differences in correla-
tion strength largely resembled that of the category-
level analyses. The occipital lobe was again most
similar to the temporal lobe (?=.57, p <.001), but
less so to the parietal and frontal lobes (?=.47,
p <.001 and ?=.34, p <.001 respectively). The
temporal lobe this time showed stronger correlation
to the parietal (?=.68, p <.001) and frontal lobes
(?=.61, p <.001) than the occipital lobe. The frontal
and parietal lobes were again strongly related to one
another (?=.67, p <.001). These results echo the
category-level findings, that word-level brain activ-
ity is also organised in a similar way across lobes.
Consequently this diminishes our chances of uncov-
ering neat interactions between models and brain ar-
eas (where for instance the Window2 model corre-
lates with the frontal lobe and Object model matches
the occipital lobe). It is however noteworthy that
we can observe some interpretable selectivity in
lobe*model combinations. In particular the Con-
text model better matches the parietal lobe than the
Object model, which in turn better captures the oc-
cipital and temporal lobes (Observations are quali-
tative). Also as we see next, adding text informa-
tion boosts performance in both parietal and tempo-
ral lobes (see Section 2 on our expectations about
information encoded in the lobes).
Does joining text and image models help word-
level interpretation? As concerns the benefits of
joining Text and Image information, per-word joint
models were generally stronger than the respective
image-based models. There was one exception:
adding text to the Object model weakened corre-
lation with the occipital lobe. Joint models were
exclusively stronger than Window2 for the tempo-
ral and occipital lobes, and were stronger in 1/3 of
cases for the frontal and parietal lobes. In an anal-
ogous comparison to the per-category analysis, a
Wilcoxon signed rank test was used to examine the
difference made by adding text information to image
models (pooling 3 models over 4 anatomical areas
for both image and joint models). The mean?sd of
image models was .1?.06 whereas for Joint models
it was .15?.07. The difference was highly signifi-
cant (W=1, p <.001, n=12).
6 Conclusion
This study brought together, for the first time, two
recent research lines: The exploration of ?seman-
tic spaces? in the brain using distributional semantic
models extracted from corpora, and the extension
of the latter to image-based features. We showed
that image-based distributional semantic measures
significantly correlate with fMRI-based neural sim-
ilarity patterns pertaining to categories of concrete
concepts as well as concrete basic-level concepts ex-
pressed by specific words (although correlations, es-
pecially at the basic-concept level, are rather low,
which might signify the need to develop still more
performant distributional models and/or noise inher-
ent to neural data). Moreover, image-based mod-
els complement a state-of-the-art text-based model,
with the best performance achieved when the two
modalities are combined. This not only presents an
optimistic outlook for the future use of image-based
models as an interpretative tool to explore issues of
cognitive grounding, but also demonstrates that they
are capturing useful additional aspects of meaning to
1968
the text models, which are likely relevant for com-
putational semantic tasks.
The weak comparative performance of the origi-
nal Mitchell et al?s Verb model is perhaps surprising
given its previous success in prediction (Mitchell et
al., 2008), but a useful reminder that a good predic-
tor does not necessarily have to capture the internal
structure of the data it predicts.
The lack of finding organisational differences be-
tween anatomical regions differentially described by
the various models is perhaps disappointing, but not
uncontroversial, given that the dataset was not origi-
nally designed to tease apart visual information from
linguistic context. It is however interesting that
in the more challenging word-level analysis some
meaningful trend was visible. In future experiments
it may prove valuable to configure a fMRI stimulus
set where text-based and image-based interrelation-
ships are maximally different. Collecting our own
fMRI data will also allow us to move beyond ex-
ploratory analysis, to test sharper predictions about
distributional models and their brain area correlates.
There are also many opportunities for focusing anal-
yses on different subsets of brain regions, with the
semantic system identified by Binder et al (2009) in
particular presenting one interesting avenue for in-
vestigation.
Acknowledgments
This research was partially funded by a Google Re-
search Award to the fifth author.
References
Shane Bergsma and Randy Goebel. 2011. Using visual
information to predict lexical preference. In Proceed-
ings of RANLP, pages 399?405, Hissar, Bulgaria.
Jeffrey R. Binder, Rutvik H. Desai, William W. Graves,
and Lisa L. Conant. 2009. Where is the semantic
system? a critical review and meta-analysis of 120
functional neuroimaging studies. Cerebral Cortexl,
12:2767?2796.
Vicki Bruce, Patrick R Green, and Georgeson Mark A.
2003. Visual perception: Physiology, psychology, and
ecology. Psychology Pr.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the EMNLP GEMS Workshop, pages 22?
32, Edinburgh, UK.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012a. Distributional semantics in
Technicolor. In Proceedings of ACL, pages 136?145,
Jeju Island, Korea.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes: Us-
ing image analysis to improve computational represen-
tations of word meaning. In Proceedings of ACM Mul-
timedia, pages 1219?1228, Nara, Japan.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open li-
brary for visual semantics representation. In Proceed-
ings of ACL, Sofia, Bulgaria.
John Bullinaria and Joseph Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39:510?526.
Kai-min Chang, Tom Mitchell, and Marcel Just. 2011.
Quantitative modeling of the neural representation of
objects: How semantic feature norms can account for
fMRI activation. NeuroImage, 56:716?727.
Linda L Chao, James V Haxby, and Alex Martin. 1999.
Attribute-based neural substrates in temporal cortex
for perceiving and knowing about objects. Nature neu-
roscience, 2(10):913?919.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22, Prague, Czech Republic.
Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchi-
cal image database. In Proceedings of CVPR, pages
248?255, Miami Beach, FL.
Russell A Epstein. 2008. Parahippocampal and ret-
rosplenial contributions to human spatial navigation.
Trends in cognitive sciences, 12(10):388?396.
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL, pages 91?99, Los Angeles, CA.
1969
Melvyn A. Goodale and David Milner. 1992. Separate
visual pathways for perception and action. Trends in
Neurosciences, 15:20?25.
Kristen Grauman and Bastian Leibe. 2011. Visual Object
Recognition. Morgan & Claypool, San Francisco.
Peter Hagoort. 2005. On Broca, brain, and bind-
ing: a new framework. Trends in cognitive sciences,
9(9):416?423.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces and
objects in ventral temporal cortex. Science, 293:2425?
2430.
Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gal-
lant. 2012. A continuous semantic space describes the
representation of thousands of object and action cate-
gories across the human brain. Neuron, 76(6):1210?
1224.
Nancy Kanwisher and Galit Yovel. 2006. The fusiform
face area: a cortical region specialized for the percep-
tion of faces. Philosophical Transactions of the Royal
Society B: Biological Sciences, 361(1476):2109?2128.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008. Representational similarity analysis?
connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In
Proceedings of CVPR, pages 2169?2178, Washington,
DC.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David G Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60:91?110.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Bruce D McCandliss, Laurent Cohen, and Stanislas De-
haene. 2003. The visual word form area: expertise
for reading in the fusiform gyrus. Trends in cognitive
sciences, 7(7):293?299.
Robert D McIntosh and Thomas Schenk. 2009. Two vi-
sual streams for perception and action: Current trends.
Neuropsychologia, 47(6):1391?1396.
Earl K Miller, David J Freedman, and Jonathan D Wal-
lis. 2002. The prefrontal cortex: categories, con-
cepts and cognition. Philosophical Transactions of
the Royal Society of London. Series B: Biological Sci-
ences, 357(1424):1123?1136.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason, and
Marcel Just. 2008. Predicting human brain activ-
ity associated with the meanings of nouns. Science,
320:1191?1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012.
Selecting corpus-semantic models for neurolinguistic
decoding. In Proceedings of *SEM, pages 114?123,
Montreal, Canada.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and
Tom Mitchell. 2009. Zero-shot learning with seman-
tic output codes. In Proceedings of NIPS, pages 1410?
1418, Vancouver, Canada.
Marius V Peelen and Paul E Downing. 2005. Selectivity
for the human body in the fusiform gyrus. Journal of
Neurophysiology, 93(1):603?608.
Francisco Pereira, Greg Detre, and Matthew Botvinick.
2011. Generating text from functional brain images.
Frontiers in Human Neuroscience, 5(72). Published
online: http://www.frontiersin.org/
human_neuroscience/10.3389/fnhum.
2011.00072/abstract.
Alexander T Sack. 2009. Parietal cortex and spatial cog-
nition. Behavioural brain research, 202(2):153?161.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Carina Silberer and Mirella Lapata. 2013. Models of
semantic representation with visual attributes. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of ICCV, pages 1470?1477, Nice,
France.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
N Tzourio-Mazoyer, B Landeau, D Papathanassiou,
F Crivello, O Etard, N Delcroix, B Mazoyer, and M Jo-
liot. 2002. Automated anatomical labeling of activa-
tions in SPM using a macroscopic anatomical parcel-
lation of the MNI MRI single-subject brain. Neuroim-
age, 15(1):273?289.
1970
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23?32,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Entailment above the word level in distributional semantics
Marco Baroni
Raffaella Bernardi
University of Trento
name.surname@unitn.it
Ngoc-Quynh Do
Free University of Bozen-Bolzano
quynhdtn.hut@gmail.com
Chung-chieh Shan
Cornell University
University of Tsukuba
ccshan@post.harvard.edu
Abstract
We introduce two ways to detect entail-
ment using distributional semantic repre-
sentations of phrases. Our first experiment
shows that the entailment relation between
adjective-noun constructions and their head
nouns (big cat |= cat), once represented as
semantic vector pairs, generalizes to lexical
entailment among nouns (dog |= animal).
Our second experiment shows that a classi-
fier fed semantic vector pairs can similarly
generalize the entailment relation among
quantifier phrases (many dogs|=some dogs)
to entailment involving unseen quantifiers
(all cats|=several cats). Moreover, nominal
and quantifier phrase entailment appears to
be cued by different distributional corre-
lates, as predicted by the type-based view
of entailment in formal semantics.
1 Introduction
Distributional semantics (DS) approximates lin-
guistic meaning with vectors summarizing the
contexts where expressions occur. The success
of DS in lexical semantics has validated the hy-
pothesis that semantically similar expressions oc-
cur in similar contexts (Landauer and Dumais,
1997; Lund and Burgess, 1996; Sahlgren, 2006;
Schu?tze, 1997; Turney and Pantel, 2010). For-
mal semantics (FS) represents linguistic mean-
ings as symbolic formulas and assemble them via
composition rules. FS has successfully modeled
quantification and captured inferential relations
between phrases and between sentences (Mon-
tague, 1970; Thomason, 1974; Heim and Kratzer,
1998). The strengths of DS and FS have been
complementary to date: On one hand, DS has in-
duced large-scale semantic representations from
corpora, but it has been largely limited to the
lexical domain. On the other hand, FS has pro-
vided sophisticated models of sentence meaning,
but it has been largely limited to hand-coded mod-
els that do not scale up to real-life challenges by
learning from data.
Given these complementary strengths, we nat-
urally ask if DS and FS can address each other?s
limitations. Two recent strands of research are
bringing DS closer to meeting core FS chal-
lenges. One strand attempts to model compo-
sitionality with DS methods, representing both
primitive and composed linguistic expressions
as distributional vectors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011; Gue-
vara, 2010; Mitchell and Lapata, 2010). The
other strand attempts to reformulate FS?s notion
of logical inference in terms that DS can cap-
ture (Erk, 2009; Geffet and Dagan, 2005; Kotler-
man et al 2010; Zhitomirsky-Geffet and Dagan,
2010). In keeping with the lexical emphasis of
DS, this strand has focused on inference at the
word level, or lexical entailment, that is, discover-
ing from distributional vectors of hyponyms (dog)
that they entail their hypernyms (animal).
This paper brings these two strands of research
together by demonstrating two ways in which the
distributional vectors of composite expressions
bear on inference. Here we focus on phrasal vec-
tors harvested directly from the corpus rather than
obtained compositionally. In a first experiment,
we exploit the entailment properties of a class
of composite expressions, namely adjective-noun
constructions (ANs), to harvest training data for
an entailment recognizer. The recognizer is then
successfully applied to detect lexical entailment.
In short, since almost all ANs entail the noun they
contain (red car entails car), the distributional
vectors of AN-N pairs can train a classifier to de-
tect noun pairs that stand in the same relation (dog
23
entails animal). With almost no manual effort,
we achieve performance nearly identical with the
state-of-the-art balAPinc measure that Kotlerman
et al(2010) crafted, which detects feature inclu-
sion between the two nouns? occurrence contexts.
Our second experiment goes beyond lexical in-
ference. We look at phrases built from a quanti-
fying determiner1 and a noun (QNs) and use their
distributional vectors to recognize entailment re-
lations of the form many dogs |= some dogs, be-
tween two QNs sharing the same noun. It turns
out that a classifier trained on a set of Q1N |=Q2N
pairs can recognize entailment in pairs with a new
quantifier configuration. For example, we can
train on many dogs |= some dogs then correctly
predict all cats|=several cats. Interestingly, on the
QN entailment task, neither our classifier trained
on AN-N pairs nor the balAPinc method beat
baseline methods. This suggests that our success-
ful QN classifiers tap into vector properties be-
yond such relations as feature inclusion that those
methods for nominal entailment rely upon.
Together, our experiments show that corpus-
harvested DS representations of composite ex-
pressions such as ANs and QNs contain suffi-
cient information to capture and generalize their
inference patterns. This result brings DS closer
to the central concerns of FS. In particular, the
QN study is the first to our knowledge to show
that DS vectors capture semantic properties not
only of content words, but of an important class of
function words (quantifying determiners) deeply
studied in FS but of little interest until now in DS.
Besides these theoretical implications, our re-
sults are of practical import. First, our AN study
presents a novel, practical method for detect-
ing lexical entailment that reaches state-of-the-
art performance with little or no manual interven-
tion. Lexical entailment is in turn fundamental
for constructing ontologies and other lexical re-
sources (Buitelaar and Cimiano, 2008). Second,
our QN study demonstrates that phrasal entail-
ment can be automatically detected and thus paves
the way to apply DS to advanced NLP tasks such
as recognizing textual entailment (Dagan et al
2009).
1In the sequel we will simply refer to a ?quantifying de-
terminer? as a ?quantifier?.
2 Background
2.1 Distributional semantics above the word
level
DS models such as LSA (Landauer and Dumais,
1997) and HAL (Lund and Burgess, 1996) ap-
proximate the meaning of a word by a vector that
summarizes its distribution in a corpus, for exam-
ple by counting co-occurrences of the word with
other words. Since semantically similar words
tend to share similar contexts, DS has been very
successful in tasks that require quantifying se-
mantic similarity among words, such as synonym
detection and concept clustering (Turney and Pan-
tel, 2010).
Recently, there has been a flurry of interest
in DS to model meaning composition: How can
we derive the DS representation of a composite
phrase from that of its constituents? Although the
general focus in the area is to perform algebraic
operations on word semantic vectors (Mitchell
and Lapata, 2010), some researchers have also di-
rectly examined the corpus contexts of phrases.
For example, Baldwin et al(2003) studied vec-
tor extraction for phrases because they were inter-
ested in the decomposability of multiword expres-
sions. Baroni and Zamparelli (2010) and Gue-
vara (2010) look at corpus-harvested phrase vec-
tors to learn composition functions that should de-
rive such composite vectors automatically. Ba-
roni and Zamparelli, in particular, showed qual-
itatively that directly corpus-harvested vectors for
AN constructions are meaningful; for example,
the vector of young husband has nearest neigh-
bors small son, small daughter and mistress. Fol-
lowing up on this approach, we show here quanti-
tatively that corpus-harvested AN vectors are also
useful for detecting entailment. We find moreover
distributional vectors informative and useful not
only for phrases made of content words (such as
ANs) but also for phrases containing functional
elements, namely quantifying determiners.
2.2 Entailment from formal to distributional
semantics
Entailment in FS To characterize the condi-
tions under which a sentence is true, FS begins
with the lexical meanings of the words in the sen-
tence and builds up the meanings of larger and
larger phrases until it arrives at the meaning of the
whole sentence. The meanings throughout this
24
compositional process inhabit a variety of seman-
tic domains, depending on the syntactic category
of the expressions: typically, a sentence denotes a
truth value (true or false) or truth conditions,
a noun such as cat denotes a set of entities, and a
quantifier phrase (QP) such as all cats denotes a
set of sets of entities.
The entailment relation (|=) is a core notion of
logic: it holds between one or more sentences and
a sentence such that it cannot be that the former
(antecedent) are true and the latter (consequent)
is false. FS extends this notion from formal-logic
sentences to natural-language expressions. By as-
signing meanings to parts of a sentence, FS allows
defining entailment not only among sentences but
also among words and phrases. Each semantic
domain A has its own entailment relation |=A.
The entailment relation |=S among sentences is
the logical notion just described, whereas the en-
tailment relations |=N and |=QP among nouns
and quantifier phrases are the inclusion relations
among sets of entities and sets of sets of entities
respectively. Our results in Section 5 show that
DS needs to treat |=N and |=QP differently as well.
Empirical, corpus-based perspectives on en-
tailment Until recently, the corpus-based re-
search tradition has studied entailment mostly at
the word level, with applied goals such as clas-
sifying lexical relations and building taxonomic
WordNet-like resources automatically. The most
popular approach, first adopted by Hearst (1992),
extracts lexical relations from patterns in large
corpora. For instance, from the pattern N1 such
as N2 one learns that N2 |=N1 (from insects such
as beetles, derive beetles |= insects). Several stud-
ies have refined and extended this approach (Pan-
tel and Ravichandran, 2004; Snow et al 2005;
Snow et al 2006; Turney, 2008).
While empirically very successful, the pattern-
based method is mostly limited to single content
words (or frequent content-word phrases). We are
interested in entailment between phrases, where it
is not obvious how to use lexico-syntactic patterns
and cope with data sparsity. For instance, it seems
hard to find a pattern that frequently connects one
QP to another it entails, as in all beetles PATTERN
many beetles. Hence, we aim to find a more gen-
eral method and investigate whether DS vectors
(whether corpus-harvested or compositionally de-
rived) encode the information needed to account
for phrasal entailment in a way that can be cap-
tured and generalized to unseen phrase pairs.
Rather recently, the study of sentential entail-
ment has taken an empirical turn, thanks to the de-
velopment of benchmarks for entailment systems.
The FS definition of entailment has been modified
by taking common sense into account. Instead of
a relation from the truth of the consequent to the
truth of the antecedent in any circumstance, the
applied view looks at entailment in terms of plau-
sibility: ? |= ? if a human who reads (and trusts)
? would most likely infer that ? is also true. En-
tailment systems have been compared under this
new perspective in various evaluation campaigns,
the best known being the Recognizing Textual En-
tailment (RTE) initiative (Dagan et al 2009).
Most RTE systems are based on advanced NLP
components, machine learning techniques, and/or
syntactic transformations (Zanzotto et al 2007;
Kouleykov and Magnini, 2005). A few systems
exploit deep FS analysis (Bos and Markert, 2006;
Chambers et al 2007). In particular, the FS re-
sults about QP properties that affect entailment
have been exploited by Chambers et alwho com-
plement a core broad-coverage system with a Nat-
ural Logic module to trade lower recall for higher
precision. For instance, they exploit the mono-
tonicity properties of no that cause the follow-
ing reversal in entailment direction: some bee-
tles |= some insects but no insects |= no beetles.
To investigate entailment step by step, we ad-
dress here a much simpler and clearer type of
entailment than the more complex notion taken
up by the RTE community. While RTE is out-
side our present scope, we do focus on QP entail-
ment as Natural Logic does. However, our eval-
uation differs from Chambers et als, since we
rely on general-purpose DS vectors as our only
resource, and we look at phrase pairs with differ-
ent quantifiers but the same noun. For instance,
we aim to predict that all beetles |= many beetles
but few beetles 6|=all beetles. QPs, of course, have
many well-known semantic properties besides en-
tailment; we leave their analysis to future study.
Entailment in DS Erk (2009) suggests that it
may not be possible to induce lexical entailment
directly from a vector space representation, but it
is possible to encode the relation in this space af-
ter it has been derived through other means. On
the other hand, recent studies (Geffet and Dagan,
25
2005; Kotlerman et al 2010; Weeds et al 2004)
have pursued the intuition that entailment is the
asymmetric ability of one term to ?substitute? for
another. For example, baseball contexts are also
sport contexts but not vice versa, hence baseball
is ?narrower? than sport and baseball |=sport. On
this view, entailment between vectors corresponds
to inclusion of contexts or features, and can be
captured by asymmetric measures of distribution
similarity. In particular, Kotlerman et al(2010)
carefully crafted the balAPinc measure (see Sec-
tion 3.5 below). We adopt this measure because
it has been shown to outperform others in several
tasks that require lexical entailment information.
Like Kotlerman et al we want to capture the
entailment relation between vectors of features.
However, we are interested in entailment not only
between words but also between phrases, and we
ask whether the DS view of entailment as fea-
ture inclusion, which captures entailment between
nouns, also captures entailment between QPs. To
this end, we complement balAPinc with a more
flexible supervised classifier.
3 Data and methods
3.1 Semantic space
We construct distributional semantic vectors from
the 2.83-billion-token concatenation of the British
National Corpus (http://www.natcorp.
ox.ac.uk/), WackyPedia and ukWaC (http:
//wacky.sslmit.unibo.it/). We tok-
enize and POS-tag this corpus, then lemmatize
it with TreeTagger (Schmid, 1995) to merge sin-
gular and plural instances of words and phrases
(some dogs is mapped to some dog).
We process the corpus in two steps to compute
semantic vectors representing our phrases of in-
terest. We use phrases of interest as a general
term to refer to both multiword phrases and sin-
gle words, and more precisely to: those AN and
QN sequences that are in the data sets (see next
subsections), the adjectives, quantifiers and nouns
contained in those sequences, and the most fre-
quent (9.8K) nouns and (8.1K) adjectives in the
corpus. The first step is to count the content
words (more precisely, the most frequent 9.8K
nouns, 8.1K adjectives, and 9.6K verbs in the cor-
pus) that occur in the same sentence as phrases
of interest. In the second step, following standard
practice, the co-occurrence counts are converted
into pointwise mutual information (PMI) scores
(Church and Hanks, 1990). The result of this step
is a sparse matrix (with both positive and negative
entries) with 48K rows (one per phrase of interest)
and 27K columns (one per content word).
3.2 The AN |= N data set
To characterize entailment between nouns using
their semantic vectors, we need data exemplifying
which noun entails which. This section introduces
one cheap way to collect such a training data set
exploiting semantic vectors for composed expres-
sions, namely AN sequences. We rely on the lin-
guistic fact that ANs share a syntactic category
and semantic type with plain common nouns (big
cat shares syntactic category and semantic type
with cat). Furthermore, most adjectives are re-
strictive in the sense that, for every noun N, the
AN sequence entails the N alone (every big cat
is a cat). From a distributional point of view, the
vector for an N should by construction include the
information in the vector for an AN, given that the
contexts where the AN occurs are a subset of the
contexts where the N occurs (cat occurs in all the
contexts where big cat occurs). This ideal inclu-
sion suggests that the DS notion of lexical entail-
ment as feature inclusion (see Section 2.2 above)
should be reflected in the AN |= N pattern.
Because most ANs entail their head Ns, we can
create positive examples of AN |= N without any
manual inspection of the corpus: simply pair up
the semantic vectors of ANs and Ns. Furthermore,
because an AN usually does not entail another N,
we can create negative examples (AN1 6|=N2) just
by randomly permuting the Ns. Of course, such
unsupervised data would be slightly noisy, espe-
cially because some of the most frequent adjec-
tives are not restrictive.
To collect cleaner data and to be sure that we
are really examining the phenomenon of entail-
ment, we took a mere few moments of man-
ual effort to select the 256 restrictive adjectives
from the most frequent 300 adjectives in the cor-
pus. We then took the Cartesian product of these
256 adjectives with the 200 concrete nouns in the
BLESS data set (Baroni and Lenci, 2011). Those
nouns were chosen to avoid highly polysemous
words. From the Cartesian product, we obtain a
total of 1246 AN sequences, such as big cat, that
occur more than 100 times in the corpus. These
AN sequences encompass 190 of the 256 adjec-
26
tives and 128 of the 200 nouns.
The process results in 1246 positive instances
of AN |= N entailment, which we use as training
data. To create a comparable amount of negative
data, we randomly permuted the nouns in the pos-
itive instances to obtain pairs of AN1 6|= N2 (e.g.,
big cat 6|=dog). We manually double-checked that
all positive and negative examples are correctly
classified (2 of 1246 negative instances were re-
moved, leaving 1244 negative training examples).
3.3 The lexical entailment N1 |= N2 data set
For testing data, we first listed all WordNet nouns
in our corpus, then extracted hyponym-hypernym
chains linking the first synsets of these nouns. For
example, pope is found to entail leader because
WordNet contains the chain pope ? spiritual
leader ? leader. Eliminating the 20 hypernyms
with more than 180 hyponyms (mostly very ab-
stract nouns such as entity, object, and quality)
yields 9734 hyponym-hypernym pairs, encom-
passing 6402 nouns. Manually double-checking
these pairs leaves us with 1385 positive instances
of N1 |= N2 entailment.
We created the negative instances of again 1385
pairs by inverting 33% of the positive instances
(from pope|=leader to leader 6|=pope), and by ran-
domly shuffling the words across the positive in-
stances. We also manually double-checked these
pairs to make sure that they are not hyponym-
hypernym pairs.
3.4 The Q1N |= Q2N data set
We study 12 quantifiers: all, both, each, either,
every, few, many, most, much, no, several, some.
We took the Cartesian product of these quantifiers
with the 6402 WordNet nouns described in Sec-
tion 3.3. From this Cartesian product, we obtain
a total of 28926 QN sequences, such as every cat,
that occur at least 100 times in the corpus. These
are our QN phrases of interest to which the proce-
dure in Section 3.1 assigns a semantic vector.
Also, from the set of quantifier pairs (Q1,Q2)
where Q1 6= Q2, we identified 13 clear cases
where Q1 |=Q2 and 17 clear cases where Q1 6|=Q2.
These 30 cases are listed in the first column of
Table 1. For each of these 30 quantifier pairs
(Q1,Q2), we enumerate those WordNet nouns N
such that semantic vectors are available for both
Q1N and Q2N (that is, both sequences occur in
at least 100 times). Each such noun then gives
Quantifier pair Instances Correct
all |= some 1054 1044 (99%)
all |= several 557 550 (99%)
each |= some 656 647 (99%)
all |= many 873 772 (88%)
much |= some 248 217 (88%)
every |= many 460 400 (87%)
many |= some 951 822 (86%)
all |= most 465 393 (85%)
several |= some 580 439 (76%)
both |= some 573 322 (56%)
many |= several 594 113 (19%)
most |= many 463 84 (18%)
both |= either 63 1 (2%)
Subtotal 7537 5804 (77%)
some 6|= every 484 481 (99%)
several 6|= all 557 553 (99%)
several 6|= every 378 375 (99%)
some 6|= all 1054 1043 (99%)
many 6|= every 460 452 (98%)
some 6|= each 656 640 (98%)
few 6|= all 157 153 (97%)
many 6|= all 873 843 (97%)
both 6|= most 369 347 (94%)
several 6|= few 143 134 (94%)
both 6|= many 541 397 (73%)
many 6|= most 463 300 (65%)
either 6|= both 63 39 (62%)
many 6|= no 714 369 (52%)
some 6|= many 951 468 (49%)
few 6|= many 161 33 (20%)
both 6|= several 431 63 (15%)
Subtotal 8455 6690 (79%)
Total 15992 12494 (78%)
Table 1: Entailing and non-entailing quantifier pairs
with number of instances per pair (Section 3.4) and
SVMpair-out performance breakdown (Section 5).
rise to an instance of entailment (Q1N |= Q2N if
Q1 |=Q2; example: many dogs |= several dogs) or
non-entailment (Q1N 6|=Q2N if Q1 6|=Q2; example:
many dogs 6|=most dogs). The number of QN pairs
that each quantifier pair gives rise to in this way is
listed in the second column of Table 1. As shown
there, we have a total of 7537 positive instances
and 8455 negative instances of QN entailment.
3.5 Classification methods
We consider two methods to classify candidate
pairs as entailing or non-entailing, the balAPinc
measure of Kotlerman et al(2010) and a standard
Support Vector Machine (SVM) classifier.
27
balAPinc As discussed in Section 2.2, balAP-
inc is optimized to capture a relation of feature
inclusion between the narrower (entailing) and
broader (entailed) terms, while capturing other in-
tuitions about the relative relevance of features.
balAPinc averages two terms, APinc and LIN.
APinc is given by:
APinc(u |= v) =
?|Fu|
r=1
(
P (r) ? rel?(fr)
)
|Fu|
APinc is a version of the Average Precision
measure from Information Retrieval tailored to
lexical inclusion. Given vectors Fu and Fv rep-
resenting the dimensions with positive PMI val-
ues in the semantic vectors of the candidate pair
u |= v, the idea is that we want the features (that
is, vector dimensions) that have larger values in
Fu to also have large values in Fv (the opposite
does not matter because it is u that should be in-
cluded in v, not vice versa). The Fu features are
ranked according to their PMI value so that fr
is the feature in Fu with rank r, i.e., r-th high-
est PMI. Then the sum of the product of the two
terms P (r) and rel?(fr) across the features in Fu
is computed. The first term is the precision at r,
which is higher when highly ranked u features are
present in Fv as well. The relevance term rel?(fr)
is higher when the feature fr in Fu also appears
in Fv with a high rank. (See Kotlerman et alfor
how P (r) and rel?(fr) are computed.) The result-
ing score is normalized by dividing by the entail-
ing vector size |Fu| (in accordance with the idea
that having more v features should not hurt be-
cause the u features should be included in the v
features, not vice versa).
To balance the potentially excessive asymmetry
of APinc towards the features of the antecedent,
Kotlerman et alaverage it with LIN, the widely
used symmetric measure of distributional similar-
ity proposed by Lin (1998):
LIN(u, v) =
?
f?Fu?Fv [wu(f) + wv(f)]?
f?Fu wu(f) +
?
f?Fv wv(f)
LIN essentially measures feature vector overlap.
The positive PMI values wu(f) and wv(f) of a
feature f in Fu and Fv are summed across those
features that are positive in both vectors, normal-
izing by the cumulative positive PMI mass in both
vectors. Finally, balAPinc is the geometric aver-
age of APinc and LIN:
balAPinc(u|=v) =
?
APinc(u |= v) ? LIN(u, v)
To adapt balAPinc to recognize entailment, we
must select a threshold t above which we classify
a pair as entailing. In the experiments below, we
explore two approaches. In balAPincupper, we op-
timize the threshold directly on the test data, by
setting t to maximize the F-measure on the test
set. This gives us an upper bound on how well bal-
APinc could perform on the test set (but note that
optimizing F does not necessarily translate into a
good accuracy performance, as clearly illustrated
by Table 3 below). In balAPincAN |= N, we use the
AN |= N data set as training data and pick the t
that maximizes F on this training set.
We use the balAPinc measure as a refer-
ence point because, on the evidence provided by
Kotlerman et al it is the state of the art in various
tasks related to lexical entailment. We recognize
however that it is somewhat complex and specifi-
cally tuned to capturing the relation of feature in-
clusion. Consequently, we also experiment with
a more flexible classifier, which can detect other
systematic properties of vectors in an entailment
relation. We present this classifier next.
SVM Support vector machines are widely used
high-performance discriminative classifiers that
find the hyperplane providing the best separation
between negative and positive instances (Cristian-
ini and Shawe-Taylor, 2000). Our SVM classifiers
are trained and tested using Weka 3 and LIBSVM
2.8 (Chang and Lin, 2011). We use the default
polynomial kernel ((u ?v/600)3) with  (tolerance
of termination criterion) set to 1.6. This value was
tuned on the AN |=N data set, which we never use
for testing. In the same initial tuning experiments
on the AN |=N data set, SVM outperformed deci-
sion trees, naive Bayes, and k-nearest neighbors.
We feed each potential entailment pair to SVM
by concatenating the two vectors representing the
antecedent and consequent expressions.2 How-
ever, for efficiency and to mitigate data sparse-
ness, we reduce the dimensionality of the seman-
tic vectors to 300 columns using Singular Value
Decomposition (SVD) before feeding them to the
classifier.3 Because the SVD-reduced semantic
2We have tried also to represent a pair by subtracting and
by dividing the two vectors. The concatenation operation
gave more successful results.
3To keep a manageable parameter space, we picked 300
columns without tuning. This is the best value reported in
many earlier studies, including classic LSA. Since SVD
sometimes improves the semantic space (Landauer and Du-
28
vectors occupy a 300-dimensional space, the en-
tailment pairs occupy a 600-dimensional space.
An SVM with a polynomial kernel takes into
account not only individual input features but also
their interactions (Manning et al 2008, chapter
15). Thus, our classifier can capture not just prop-
erties of individual dimensions of the antecedent
and consequent pairs, but also properties of their
combinations (e.g., the product of the first dimen-
sions of the antecedent and the consequent). We
conjecture that this property of SVMs is funda-
mental to their success at detecting entailment,
where relations between the antecedent and the
consequent should matter more than their inde-
pendent characteristics.
4 Predicting lexical entailment from
AN |= N evidence
Since the contexts of AN must be a subset of the
contexts of N, semantic vectors harvested from
AN phrases and their head Ns are by construc-
tion in an inclusion relation. The first experiment
shows that these vectors constitute excellent train-
ing data to discover entailment between nouns.
This suggests that the vector pairs representing
entailment between nouns are also in an inclusion
relation, supporting the conjectures of Kotlerman
et al(2010) and others.
Table 2 reports the results we obtained with
balAPincupper, balAPincAN |= N (Section 3.5) and
SVMAN |= N (the SVM classifier trained on the
AN |= N data). As an upper bound for meth-
ods that generalize from AN |= N, we also re-
port the performance of SVM trained with 10-fold
cross-validation on the N1 |= N2 data themselves
(SVMupper). Finally, we tried two baseline classi-
fiers. The first baseline (fq(N1)< fq(N2)) guesses
entailment if the first word is less frequent than
the second. The second (cos(N1, N2)) applies a
threshold (determined on the test set) to the co-
sine similarity of the pair. The results of these
baselines shown in Table 2 use SVD; those with-
out SVD are similar. Both baselines outperformed
more trivial methods such as random guessing or
fixed response, but they performed significantly
worse than SVM and balAPinc.
Both methods that generalize entailment from
AN |= N to N1 |= N2 perform well, with 70%
mais, 1997; Rapp, 2003; Schu?tze, 1997), we tried balAPinc
on the SVD-reduced vectors as well, but results were consis-
tently worse than with PMI vectors.
P R F Accuracy
(95% C.I.)
SVMupper 88.6 88.6 88.5 88.6 (87.3?89.7)
balAPincAN |= N 65.2 87.5 74.7 70.4 (68.7?72.1)
balAPincupper 64.4 90.0 75.1 70.1 (68.4?71.8)
SVMAN |= N 69.3 69.3 69.3 69.3 (67.6?71.0)
cos(N1, N2) 57.7 57.6 57.5 57.6 (55.8?59.5)
fq(N1)< fq(N2) 52.1 52.1 51.8 53.3 (51.4?55.2)
Table 2: Detecting lexical entailment. Results ranked
by accuracy and expressed as percentages. 95% con-
fidence intervals around accuracy calculated by bino-
mial exact tests.
accuracy on the test set, which is balanced be-
tween positive and negative instances. Interest-
ingly, the balAPinc decision thresholds tuned on
the AN |= N set and on the test data are very
close (0.26 vs. 0.24), resulting in very similar per-
formance for balAPincAN |= N and balAPincupper.
This suggests that the relation captured by bal-
APinc on the phrasal entailment training data is
indeed the same that the measure captures when
applied to lexical entailment data.
The success of this first experiment shows that
the entailment relation present in the distribu-
tional representation of AN phrases and their
head Ns transfers to lexical entailment (entailment
among Ns). Most importantly, this result demon-
strates that the semantic vectors of composite ex-
pressions (such as ANs) are useful for lexical en-
tailment. Moreover, the result is in accordance
with the view of FS, that ANs and Ns have the
same semantic type, and thus they enter entail-
ment relations of the same kind. Finally, the hy-
pothesis that entailment among nouns is reflected
by distributional inclusion among their semantic
vectors (Kotlerman et al 2010) is supported both
by the successful generalization of the SVM clas-
sifier trained on AN |= N pairs and by the good
performance of the balAPinc measure.
5 Generalizing QN entailment
The second study is somewhat more ambitious,
as it aims to capture and generalize the entailment
relation between QPs (of shape QN) using only
the corpus-harvested semantic vectors represent-
ing these phrases as evidence. We are thus first
and foremost interested in testing whether these
vectors encode information that can help a power-
29
P R F Accuracy
(95% C.I.)
SVMpair-out 76.7 77.0 76.8 78.1 (77.5?78.8)
SVMquantifier-out 70.1 65.3 68.0 71.0 (70.3?71.7)
SVMQpair-out 67.9 69.8 68.9 70.2 (69.5?70.9)
SVMQquantifier-out 53.3 52.9 53.1 56.0 (55.2?56.8)
cos(QN1, QN2) 52.9 52.3 52.3 53.1 (52.3?53.9)
balAPincAN |= N 46.7 5.6 10.0 52.5 (51.7?53.3)
SVMAN |= N 2.8 42.9 5.2 52.4 (51.7?53.2)
fq(QN1)<fq(QN2) 51.0 47.4 49.1 50.2 (49.4?51.0)
balAPincupper 47.1 100 64.1 47.2 (46.4?47.9)
Table 3: Detecting quantifier entailment. Results
ranked by accuracy and expressed as percentages.
95% confidence intervals around accuracy calculated
by binomial exact tests.
ful classifier, such as SVM, to detect entailment.
To abstract away from lexical or other effects
linked to a specific quantifier, we consider two
challenging training and testing regimes. In the
first (SVMpair-out), we hold out one quantifier pair
as testing data and use the other 29 pairs in Table 1
as training data. Thus, for example, the classifier
must discover all dogs |= some dogs without see-
ing any all N |= some N instance in the training
data. In the second (SVMquantifier-out), we hold out
one of the 12 quantifiers as testing data (that is,
hold out every pair involving a certain quantifier)
and use the rest as training data. For example,
the quantifier must guess all dogs |= some dogs
without ever seeing all in the training data. We
expect the second training regime to be more dif-
ficult, not just because there is less training data,
but also because the trained classifier is tested on
a quantifier that it has never encountered within
any training QN sequence.4
Table 3 reports the results for SVMpair-out and
SVMquantifier-out, as well as for the methods we
tried in the lexical entailment experiments. (As
in the first study, the frequency- and cosine-based
4In our initial experiments, we added negative entail-
ment instances by blindly permuting the nouns, under the
assumption that Q1N1 typically does not entail Q2N2 when
Q1 6= Q2 and N1 6= N2. These additional instances turned
out to be much easier to classify: adding an equal proportion
of them to the training data and testing data, such that the
number of instances where N1 = N2 and where N1 6= N2
is equal, reduced every error rate roughly by half. The re-
ported results do not involve these additional instances.
baselines are only slightly better overall than more
trivial baselines.) We consider moreover an alter-
native approach that ignores the noun altogether
and uses vectors for the quantifiers only (e.g., the
decision about all dogs |=some dogs considers the
corpus-derived all and some vectors only). The
models resulting from this Q-only strategy are
marked with the superscript Q in the table.
The results confirm clearly that semantic vec-
tors for QNs contain enough information to allow
a classifier to detect entailment: SVMquantifier-out
performs as well as the lexical entailment classi-
fiers of our first study, and SVMpair-out does even
better. This success is especially impressive given
our challenging training and testing regimes.
In contrast to the first study, now SVMAN |= N,
the classifier trained on the AN |= N data set,
and balAPinc perform no better than the base-
lines. (Here balAPincupper and balAPincAN |= N
pick very different thresholds: the first settling
on a very low t = 0.01, whereas for the sec-
ond t = 0.26.) As predicted by FS (see Section
2.2 above), noun-level entailment does not gen-
eralize to quantifier phrase entailment, since the
two structures have different semantic types, cor-
responding to different kinds of entailment rela-
tions. Moreover, the failure of balAPinc suggests
that, whatever evidence the SVMs rely upon, it is
not simple feature inclusion.
Interestingly, even the Q vectors alone encode
enough information to capture entailment above
chance. Still, the huge drop in performance from
SVMQpair-out to SVM
Q
quantifier-out suggests that the Q-
only method learned ad-hoc properties that do not
generalize (e.g., ?all entails every Q2?).
Tables 1 and 4 break down the SVM results by
(pairs of) quantifiers. We highlight the remark-
able dichotomy in Table 4 between the good per-
formance on the universal-like quantifiers (each,
every, all, much) and the poor performance on the
existential-like ones (some, no, both, either).
In sum, the QN experiments show that seman-
tic vectors contain enough information to detect
a logical relation such as entailment not only be-
tween words, but also between phrases contain-
ing quantifiers that determine their entailment re-
lation. While a flexible classifier such as SVM
performs this task well, neither measuring fea-
ture inclusion nor generalizing nominal entail-
ment works. SVMs are evidently tapping into
other properties of the vectors.
30
Quantifier Instances Correct
|= 6|= |= 6|=
each 656 656 649 637 (98%)
every 460 1322 402 1293 (95%)
much 248 0 216 0 (87%)
all 2949 2641 2011 2494 (81%)
several 1731 1509 1302 1267 (79%)
many 3341 4163 2349 3443 (77%)
few 0 461 0 311 (67%)
most 928 832 549 511 (60%)
some 4062 3145 1780 2190 (55%)
no 0 714 0 380 (53%)
both 636 1404 589 303 (44%)
either 63 63 2 41 (34%)
Total 15074 16910 9849 12870 (71%)
Table 4: Breakdown of results with leaving-one-
quantifier-out (SVMquantifier-out) training regime.
6 Conclusion
Our main results are as follows.
1. Corpus-harvested semantic vectors repre-
senting adjective-noun constructions and
their heads encode a relation of entailment
that can be exploited to train a classifier
to detect lexical entailment. In particular,
a relation of feature inclusion between the
narrower antecedent and broader consequent
terms captures both AN |= N and N1 |= N2
entailment.
2. The semantic vectors of quantifier-noun con-
structions also encode information sufficient
to learn an entailment relation that general-
izes to QNs containing quantifiers that were
not seen during training.
3. Neither the entailment information encoded
in AN |= N vectors nor the balAPinc mea-
sure generalizes well to entailment detection
in QNs. This result suggests that QN vectors
encode a different kind of entailment, as also
suggested by type distinctions in Formal Se-
mantics.
In future work, we want first of all to conduct
an analysis of the features in the Q1N |=Q2N vec-
tors that are crucially exploited by our success-
ful entailment recognizers, in order to understand
which characteristics of entailment are encoded in
these vectors.
Very importantly, instead of extracting vectors
representing phrases directly from the corpus, we
intend to derive them by compositional operations
proposed in the literature (see Section 2.1 above).
We will look for composition methods producing
vector representations of composite expressions
that are as good as (or better than) vectors directly
extracted from the corpus at encoding entailment.
Finally, we would like to evaluate our entail-
ment detection strategies for larger phrases and
sentences, possibly containing multiple quanti-
fiers, and eventually embed them as core compo-
nents of an RTE system.
Acknowledgments
We thank the Erasmus Mundus EMLCT Program
for the student and visiting scholar grants to the
third and fourth author, respectively. The first
two authors are partially funded by the ERC 2011
Starting Independent Research Grant supporting
the COMPOSES project (nr. 283554). We are
grateful to Gemma Boleda, Louise McNally, and
the anonymous reviewers for valuable comments,
and to Ido Dagan for important insights into en-
tailment from an empirical point of view.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 Workshop on Multiword
Expressions, pages 89?96.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the Workshop on Geometrical Mod-
els of Natural Language Semantics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Johan Bos and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesn?t. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
Paul Buitelaar and Philipp Cimiano. 2008. Bridging
the Gap between Text and Knowledge. IOS, Ams-
terdam.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
31
and Christopher D. Manning. 2007. Learning
alignments and leveraging natural logic. In ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2(3):27:1?27:27.
Kenneth Church and Peter Hanks. 1990. Word associ-
ation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Nello Cristianini and John Shawe-Taylor. 2000. An
introduction to Support Vector Machines and other
kernel-based learning methods. Cambridge Univer-
sity Press, Cambridge.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15:459?476.
Katrin Erk. 2009. Supporting inferences in semantic
space: representing words as regions. In Proceed-
ings of IWCS, pages 104?115, Tilburg, Netherlands.
Maayan Geffet and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of ACL, pages 107?114, Ann Arbor,
MI.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1395?1404, Edinburgh.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33?37, Uppsala, Sweden.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539?545, Nantes, France.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell, Oxford.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and
Maayan Zhitomirsky-Geffet. 2010. Directional
distributional similarity for lexical inference. Natu-
ral Language Engineering, 16(4):359?389.
Milen Kouleykov and Bernardo Magnini. 2005. Tree
edit sistance for textual entailment. In Proceed-
ings of RALNP-2005, International Conference on
Recent Advances in Natural Language Processing,
pages 271?278.
Thomas Landauer and Susan Dumais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of ICML, pages
296?304, Madison, WI, USA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?
208.
Chris Manning, Prabhakar Raghavan, and Hinrich
Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge.
Jeff Mitchell and Mirella Lapata. 2010. Composi-
tion in distributional models of semantics. Cogni-
tive Science, 34(8):1388?1429.
Richard Montague. 1970. Universal Grammar. Theo-
ria, 36:373?398.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeliing semantic classes. In Proceed-
ings of HLT-NAACL 2004, pages 321?328.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA.
Magnus Sahlgren. 2006. The Word-Space Model.
Dissertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Proceedings of the EACL-SIGDAT Workshop,
Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Nat-
ural Language Learning. CSLI, Stanford, CA.
Rion Snow, Daniel Juravsky, and Andrew Y. Ng.
2005. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS 17.
Rion Snow, Daniel Juravsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from het-
erogenous evidence. In Proceedings of ACL 2006,
pages 801?808.
Richmond H. Thomason, editor. 1974. Formal Phi-
losophy: Selected Papers of Richard Montague.
Yale University Press, New York.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. In Proceed-
ings of COLING, pages 905?912, Manchester, UK.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference of Computational Linguistics,
COLING-2004, pages 1015?1021.
Fabio M. Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2007. Shallow semantics in fast tex-
tual entailment rule learners. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2010.
Bootstrapping distributional feature vector quality.
Computational Linguistics, 35(3):435?461.
32
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 434?442,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Improving the Lexical Function Composition Model
with Pathwise Optimized Elastic-Net Regression
Jiming Li and Marco Baroni and Georgiana Dinu
Center for Mind/Brain Sciences
University of Trento, Italy
(jiming.li|marco.baroni|georgiana.dinu)@unitn.it
Abstract
In this paper, we show that the lexical
function model for composition of dis-
tributional semantic vectors can be im-
proved by adopting a more advanced re-
gression technique. We use the pathwise
coordinate-descent optimized elastic-net
regression method to estimate the compo-
sition parameters, and compare the result-
ing model with several recent alternative
approaches in the task of composing sim-
ple intransitive sentences, adjective-noun
phrases and determiner phrases. Experi-
mental results demonstrate that the lexical
function model estimated by elastic-net re-
gression achieves better performance, and
it provides good qualitative interpretabil-
ity through sparsity constraints on model
parameters.
1 Introduction
Vector-based distributional semantic models of
word meaning have gained increased attention in
recent years (Turney and Pantel, 2010). Differ-
ent from formal semantics, distributional seman-
tics represents word meanings as vectors in a high-
dimensional semantic space, where the dimen-
sions are given by co-occurring contextual fea-
tures. The intuition behind these models lies in
the fact that words which are similar in meaning
often occur in similar contexts, e.g., moon and
star might both occur with sky, night and bright.
This leads to convenient ways to measure similar-
ity between different words using geometric meth-
ods (e.g., the cosine of the angle between two
vectors that summarize their contextual distribu-
tion). Distributional semantic models have been
successfully applied to many tasks in linguistics
and cognitive science (Griffiths et al., 2007; Foltz
et al., 1998; Laham, 1997; McDonald and Brew,
2004). However, most of these tasks only deal
with isolated words, and there is a strong need
to construct representations for longer linguistic
structures such as phrases and sentences. In or-
der to achieve this goal, the principle of com-
positionality of linguistic structures, which states
that complex linguistic structures can be formed
through composition of simple elements, is ap-
plied to distributional vectors. Therefore, in recent
years, the problem of composition within distribu-
tional models has caught many researchers? atten-
tion (Clark, 2013; Erk, 2012).
A number of compositional frameworks have
been proposed and tested. Mitchell and Lapata
(2008) propose a set of simple component-wise
operations, such as multiplication and addition.
Later, Guevara (2010) and Baroni and Zampar-
elli (2010) proposed more elaborate methods, in
which composition is modeled as matrix-vector
multiplication operations. Particularly new to their
approach is the proposal to estimate model param-
eters by minimizing the distance of the composed
vectors to corpus-observed phrase vectors. For ex-
ample, Baroni and Zamparelli (2010) consider the
case of Adjective-Noun composition and model it
as matrix-vector multiplication: adjective matrices
are parameters to be estimated and nouns are co-
occurrence vectors. The model parameter estima-
tion procedure becomes a multiple response mul-
tivariate regression problem. This method, that,
following Dinu et al. (2013) and others, we term
the lexical function composition model, can also
be generalized to more complex structures such
as 3rd order tensors for modeling transitive verbs
(Grefenstette et al., 2013).
Socher et al. (2012) proposed a more complex
and flexible framework based on matrix-vector
representations. Each word or lexical node in a
parsing tree is assigned a vector (representing in-
herent meaning of the constituent) and a matrix
(controlling the behavior to modify the meaning of
434
Model Composition function Parameters
Add w
1
u? + w
2
v? w
1
, w
2
Mult u?
w
1
? v?
w
2
w
1
, w
2
Dil ||u?||
2
2
v? + (?? 1)?u?, v??u? ?
Fulladd W
1
u? + W
2
v? W
1
,W
2
? R
m?m
Lexfunc A
u
v? A
u
? R
m?m
Fulllex tanh([W
1
,W
2
]
[
A
u
v?
A
v
u?
]
) W
1
,W
2
,
A
u
, A
v
? R
m?m
Table 1: Composition functions of inputs (u, v).
neighbor words or phrases) simultaneously. They
use recursive neural networks to learn and con-
struct the entire model and show that it reaches
state-of-the-art performance in various evaluation
experiments.
In this paper, we focus on the simpler, linear
lexical function model proposed by Baroni and
Zamparelli (2010) (see also Coecke et al. (2010))
and show that its performance can be further im-
proved through more advanced regression tech-
niques. We use the recently introduced elastic-
net regularized linear regression method, which
is solved by the pathwise coordinate descent opti-
mization algorithm along a regularization parame-
ter path. This new regression method can rapidly
generate a sequence of solutions along the regular-
ization path. Performing cross-validation on this
parameter path should yield a much more accurate
model for prediction. Besides better prediction ac-
curacy, the elastic-net method also brings inter-
pretability to the composition procedure through
sparsity constraints on the model.
The rest of this paper is organized as follows: In
Section 2, we give details on the above-mentioned
composition models, which will be used for com-
parison in our experiments. In Section 3, we de-
scribe the pathwise optimized elastic-net regres-
sion algorithm. Experimental evaluation on three
composition tasks is provided in Section 4. In Sec-
tion 5 we conclude and suggest directions for fu-
ture work.
2 Composition Models
Mitchell and Lapata (2008; 2010) present a set of
simple but effective models in which each compo-
nent of the output vector is a function of the cor-
responding components of the inputs. Given in-
put vectors u? and v?, the weighted additive model
(Add) returns their weighted sum: p? = w
1
u? +
w
2
v?. In the dilation model (Dil), the output vector
is obtained by decomposing one of the input vec-
tors, say v?, into a vector parallel to u? and an or-
thogonal vector, and then dilating only the parallel
vector by a factor ? before re-combining (formula
in Table 1). Mitchell and Lapata also propose a
simple multiplicative model in which the output
components are obtained by component-wise mul-
tiplication of the corresponding input components.
We use its natural weighted extension (Mult), in-
troduced by Dinu et al. (2013), that takes w
1
and
w
2
powers of the components before multiplying,
such that each phrase component p
i
is given by:
p
i
= u
w
1
i
v
w
2
i
.
Guevara (2010) and Zanzotto et al. (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: p? =
W
1
u? + W
2
v?.
Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a regression function from nouns onto (mod-
ified) nouns. Given that linear functions can be
expressed by matrices and their application by
matrix-by-vector multiplication, a functor (such
as the adjective) is represented by a matrix A
u
to be composed with the argument vector v? (e.g.,
the noun) by multiplication, returning the lexical
function (Lexfunc) representation of the phrase:
p? = A
u
v?.
The method proposed by Socher et al. (2012)
can be seen as a combination and non-linear ex-
tension of Fulladd and Lexfunc (that Dinu and col-
leagues thus called Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (u?, A
u
) and (v?, A
v
), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products A
u
v?
and A
v
u? (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.
Parameter estimation of the above composition
models follows Dinu et al. (2013) by minimizing
the distance to corpus-extracted phrase vectors. In
435
Figure 1: A sketch of the composition model train-
ing and composing procedure.
the case of the Fulladd and Lexfunc models this
amounts to solving a multiple response multivari-
ate regression problem.
The whole composition model training and
phrase composition procedure is described with a
sketch in Figure 1. To illustrate with an example,
given an intransitive verb boom, we want to train
a model for this intransitive verb so that we can
use it for composition with a noun subject (e.g.,
export) to form an intransitive sentence (e.g., ex-
port boom(s)). We treat these steps as a composi-
tion model learning and predicting procedure. The
training dataset is formed with pairs of input (e.g.,
activity) and output (e.g., activity boom) vectors.
All composition models except Lexfunc also use
the functor vector (boom) in the training data. Lex-
func does not use this functor vector, but it would
rather like to encode the learning target?s vector
meaning in a different way (see experimental anal-
ysis in Section 4.3). Then, this dataset is used for
parameter estimation of models. When a model
(boom) is trained and given a new input seman-
tic vector (e.g., export), it will output another vec-
tor representing the concept for export boom. And
the concept export boom should be close to simi-
lar concepts (e.g., export prosper) in meaning un-
der some distance metric in semantic vector space.
The same training and composition scheme is ap-
plied for other types of functors (e.g., adjectives
and determiners). All the above mentioned com-
position models are evaluated within this scheme,
but note that in the case of Add, Dil, Mult and Ful-
ladd, a single set of parameters is obtained across
all functors of a certain syntactic category.
3 Pathwise Optimized Elastic-net
Algorithm
The elastic-net regression method (Zou and
Hastie, 2005) is proposed as a compromise be-
tween lasso (Tibshirani, 1996) and ridge regres-
sion (Hastie et al., 2009). Suppose there are N
observation pairs (x
i
, y
i
), here x
i
? R
p
is the ith
training sample and y
i
? R is the corresponding
response variable in the typical regression setting.
For simplicity, assume the x
ij
are standardized:
?
N
i=1
x
2
ij
= 1, for j = 1, . . . , p. The elastic-net
solves the following problem:
min
(?
0
,?)?R
p+1
[
1
N
N
?
i=1
(y
i
? ?
0
? x
T
i
?)
2
+ ?P
?
(?)
]
(1)
where
P
?
(?) = ?((1? ?)
1
2
? ? ?
2
?
2
+??
?
1
)
=
p
?
j=1
[
1
2
(1? ?)?
2
j
+ ?|?
j
|].
P is the elastic-net penalty, and it is a compro-
mise between the ridge regression penalty and the
lasso penalty. The merit of the elastic-net penalty
depends on two facts: the first is that elastic-net in-
herits lasso?s characteristic to shrink many of the
regression coefficients to zero, a property called
sparsity, which results in better interpretability of
model; the second is that elastic-net inherits ridge
regression?s property of a grouping effect, which
means important correlated features can be con-
tained in the model simultaneously, and not be
omitted as in lasso.
For these linear-type regression problem (ridge,
lasso and elastic-net), the determination of the ?
value is very important for prediction accuracy.
Efron et al. (2004) developed an efficient algo-
rithm to compute the entire regularization path
for the lasso problem in 2004. Later, Friedman
et al. (Friedman et al., 2007; Friedman et al.,
2010) proposed a coordinate descent optimization
436
method for the regularization parameter path, and
they also provided a solution for elastic-net. The
main idea of pathwise coordinate descent is to
solve the penalized regression problem along an
entire path of values for the regularization param-
eters ?, using the current estimates as warm starts.
The idea turns out to be quite efficient for elastic-
net regression. The procedure can be described as
below: firstly establish an 100 ? value sequence
in log scale, and for each of the 100 regulariza-
tion parameters, use the following coordinate-wise
updating rule to cycle around the features for es-
timating the corresponding regression coefficients
until convergence.
?
?
j
?
S
(
1
N
?
N
i=1
x
ij
(y
i
? y?
(j)
i
), ??
)
1 + ?(1? ?)
(2)
where
? y?
(j)
i
=
?
?
0
+
?
? ?=j
x
i?
?
?
?
is the fitted value ex-
cluding the contribution from x
ij
, and hence
y
i
? y?
(j)
i
the partial residual for fitting ?
j
.
? S(z, ?) is the soft-thresholding operator with
value
S(z, ?) = sign(z)(|z| ? ?)
+
=
?
?
?
z ? ? if z > 0 and ? < |z|
z + ? if z < 0 and ? < |z|
0 if ? ? |z|
Then solutions for a decreasing sequence of val-
ues for ? are computed in this way, starting at the
smallest value ?
max
for which the entire coeffi-
cient vector
?
? = 0. Then, 10-fold cross valida-
tion on this regularization path is used to deter-
mine the best model for prediction accuracy. The
? parameter controls the model sparsity (the num-
ber of coefficients equal to zero) and grouping ef-
fect (shrinking highly correlated features simulta-
neously).
In what follows, we call the elastic-net regres-
sion lexical function model EnetLex. In Sec-
tion 4, we will report the experiment results by
EnetLex with ? = 1. It equals to pathwise co-
ordinate descent optimized lasso, which favours
sparser solutions and is often a better estimator
when the number of training samples is far greater
than the number of feature dimensions, as in our
case. We also experimented with intermediate ?
values (e.g., ? = 0.5), that were, consistently, in-
ferior or equal to the lasso setting.
?2 0 2 4
200
400
600
800
log(Lambda)
Mean
?Squ
ared 
Error
50 50 50 50 50 50 50 50 50 48 29 21 12 7 4 2Model selection procedure for ?EnetLex?
Figure 2: Example of model selection procedure
for elastic-net regression (?the? model for deter-
miner phrase experiment, SVD, 50 dimensions).
Figure 2 is an example of the model selection
procedure between different regularization param-
eter ? values for determiner ?the? (experimental
details are described in section 4). When ? is
fixed, EnetLex first generates a ? sequence from
?
max
to ?
min
(?
max
is set to the smallest value
which will shrink all the regression coefficients
to zero, ?
min
= 0.0001) in log scale (rightmost
point in the plot). The red points corresponding
to each ? value in the plot represent mean cross-
validated errors and their standard errors. To esti-
mate a model corresponding to some ? value ex-
cept ?
max
, we use the solution from previous ?
value as the initial coefficients (the warm starts
mentioned before) for iteration with coordinate
descent. This will often generate a stable solu-
tion path for the whole ? sequence very fast. And
we can choose the model with minimum cross-
validation error on this path and use it for more
accurate prediction. In Figure 2, the labels on the
top are numbers of corresponding selected vari-
ables (features), the right vertical dotted line is the
largest value of lambda such that error is within 1
standard error of the minimum, and the left verti-
cal dotted line corresponds to the ? value which
gives minimum cross-validated error. In this case,
the ? value of minimum cross-validated error is
0.106, and its log is -2.244316. In all of our ex-
periments, we will select models corresponding to
minimum training-data cross-validated error.
4 Experiments
4.1 Datasets
We evaluate on the three data sets described below,
that were also used by Dinu et al. (2013), our most
437
direct point of comparison.
Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2010), focuses
on the composition of intransitive verbs and their
noun subjects. It contains a total of 120 sentence
pairs together with human similarity judgments on
a 7-point scale. For example, value slumps/value
declines is scored 7, skin glows/skin burns is
scored 1. On average, each pair is rated by 30
participants. Rather than evaluating against mean
scores, we use each rating as a separate data point,
as done by Mitchell and Lapata. We report Spear-
man correlations between human-assigned scores
and cosines of model-generated vector pairs.
Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We fo-
cus on the latter, and we frame the task as in Dinu
et al. (2013). The dataset contains 620 ANs, each
paired with a single-noun paraphrase. Examples
include: upper side/upside, false belief/fallacy and
electric refrigerator/fridge. We evaluate a model
by computing the cosine of all 20K nouns in our
semantic space with the target AN, and looking at
the rank of the correct paraphrase in this list. The
lower the rank, the better the model. We report
median rank across the test items.
Determiner phrases The third dataset, intro-
duced in Bernardi et al. (2013), focuses on a
class of determiner words. It is a multiple-
choice test where target nouns (e.g., omniscience)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., all knowl-
edge). There are 173 target nouns in total, each
paired with one correct DP response, as well as
5 foils, namely the determiner (all) and noun
(knowledge) from the correct response and three
more DPs, two of which contain the same noun as
the correct phrase (much knowledge, some knowl-
edge), the third the same determiner (all prelimi-
naries). Other examples of targets/related-phrases
are quatrain/four lines and apathy/no emotion.
The models compute cosines between target noun
and responses and are scored based on their accu-
racy at ranking the correct phrase first.
4.2 Setup
We use a concatenation of ukWaC, Wikipedia
(2009 dump) and BNC as source corpus, total-
Model Reduction Dim Correlation
Add NMF 150 0.1349
Dil NMF 300 0.1288
Mult NMF 250 0.2246
Fulladd SVD 300 0.0461
Lexfunc SVD 250 0.2673
Fulllex NMF 300 0.2682
EnetLex SVD 250 0.3239
Table 2: Best performance comparison for intran-
sitive verb sentence composition.
ing 2.8 billion tokens.
1
Word co-occurrences are
collected within sentence boundaries (with a max-
imum of a 50-words window around the target
word). Following Dinu et al. (2013), we use the
top 10K most frequent content lemmas as context
features, Pointwise Mutual Information as weight-
ing method and we reduce the dimensionality of
the data by both Non-negative Matrix Factoriza-
tion (NMF, Lee and Seung (2000)) and Singular
Value Decomposition (SVD). For both data di-
mensionality reduction techniques, we experiment
with different numbers of dimension varying from
50 to 300 with a step of 50. Since the Mult model
works very poorly when the input vectors contain
negative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.
We use the DIStributional SEmantics Compo-
sition Toolkit (DISSECT)
2
which provides imple-
mentations for all models we use for comparison.
Following Dinu and colleagues, we used ordinary
least-squares to estimate Fulladd and ridge for
Lexfunc. The EnetLex model is implemented in R
with support from the glmnet package,
3
which im-
plements pathwise coordinate descent elastic-net
regression.
4.3 Experimental Results and Analysis
The experimental results are shown in Ta-
bles 2, 3, 4 and Figures 3, 4, 5. The best per-
formances from each model on the three compo-
sition tasks are shown in the tables. The over-
all result distributions across reduction techniques
and dimensionalities are displayed in the figure
1
http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk
2
http://clic.cimec.unitn.it/composes/
toolkit/
3
http://cran.r-project.org/web/
packages/glmnet/
438
Model Reduction Dim Rank
Add NMF 300 113
Dil NMF 300 354.5
Mult NMF 300 146.5
Fulladd SVD 300 123
Lexfunc SVD 150 117.5
Fulllex SVD 50 394
EnetLex SVD 300 108.5
Table 3: Best performance comparison for adjec-
tive noun composition (lower ranks mean better
performance).
Model Reduction Dim Rank
Add NMF 100 0.3237
Dil NMF 100 0.3584
Mult NMF 300 0.2023
Fulladd NMF 200 0.3642
Lexfunc SVD 200 0.3699
Fulllex SVD 100 0.3699
EnetLex SVD 250 0.4046
Table 4: Best performance comparison for deter-
miner phrase composition.
boxplots (NMF and SVD results are shown sep-
arately). From Tables 2, 3, 4, we can see that
EnetLex consistently achieves the best composi-
tion performance overall, also outperforming the
standard lexical function model. In the boxplot
display, we can see that SVD is in general more
stable across dimensionalities, yielding smaller
variance in the results than NMF. We also observe,
more specifically, larger variance in EnetLex per-
formance on NMF than in Lexfunc, especially for
determiner phrase composition. The large vari-
ance with EnetLex comes from the NMF low-
dimensionality results, especially the 50 dimen-
sions condition. The main reason for this lies
in the fast-computing tricks of the coordinate de-
scent algorithm when cycling around many fea-
tures with zero values (as resulting from NMF),
which cause fast convergence at the beginning of
the regularization path, generating an inaccurate
model. A subordinate reason might lie in the un-
standardized larger values of the NMF features
(causing large gaps between adjacent parameter
values in the regularization path). Although data
standardization or other feature scaling techniques
are often adopted in statistical analysis, they are
seldom used in semantic composition tasks due to
Add?
nmf Dil?n
mf
Mult?
nmf
Fulla
dd?n
mf
Lexfu
nc?n
mf
Fullle
x?n
mf
Enet
Lex?
nmf
Add?
svd Dil?s
vd
Mult?
svd
Fulla
dd?s
vd
Lexfu
nc?s
vd
Fullle
x?s
vd
Enet
Lex?
svd
0.000.05
0.100.15
0.200.25
0.30
Intransitive sentences
Figure 3: Intransitive verb sentence composition
results.
Add?
nmf Dil?n
mf
Mult?
nmf
Fulla
dd?n
mf
Lexfu
nc?n
mf
Fullle
x?n
mf
Enet
Lex?
nmf
Add?
svd Dil?s
vd
Mult?
svd
Fulla
dd?s
vd
Lexfu
nc?s
vd
Fullle
x?s
vd
Enet
Lex?
svd
800
600
400
200
0 Adjective?noun phrases
Figure 4: Adjective noun phrase composition re-
sults.
the fact that they might negatively affect the se-
mantic vector space. A reasonable way out of this
problem would be to save the mean and standard
deviation parameters used for data standardization
and use them to project the composed phrase vec-
tor outputs back to the original vector space.
On the other hand, EnetLex obtained a stable
good performance in SVD space, with the best re-
sults achieved with dimensions between 200 and
300. A set of Tukey?s Honestly Significant Tests
show that EnetLex significantly outperforms the
other models across SVD settings for determiner
phrases and intransitive sentences. The difference
is not significant for most comparisons in the ad-
jective phrases task.
For the simpler models for which it was com-
putationally feasible, we repeated the experiments
without dimensionality reduction. The results ob-
tained with (unweighted) Add and Mult using full-
space representations are reported in Table 5. Due
to computational limitations, we tuned full-space
weights for Add model only, obtaining similar re-
sults to those reported in the table. The full-space
439
Add?
nmf Dil?n
mf
Mult?
nmf
Fulla
dd?n
mf
Lexfu
nc?n
mf
Fullle
x?n
mf
Enet
Lex?
nmf
Add?
svd Dil?s
vd
Mult?
svd
Fulla
dd?s
vd
Lexfu
nc?s
vd
Fullle
x?s
vd
Enet
Lex?
svd
0.15
0.20
0.25
0.30
0.35
0.40 Determiner phrases
Figure 5: Determiner phrase composition results.
model verb adjective determiner
Add 0.0259 957 0.2832
Mult 0.1796 298.5 0.0405
Table 5: Performance of Add and Mult models
without dimensionality reduction.
results confirm that dimensionality reduction is
not only a computational necessity when work-
ing with more complex models, but it is actually
improving the quality of the underlying semantic
space.
Another benefit that elastic-net has brought to
us is the sparsity in coefficient matrices. Sparsity
here means that many entries in the coefficient ma-
trix are shrunk to 0. For the above three exper-
iments, the mean adjective, verb and determiner
models? sparsity ratios are 0.66, 0.55 and 0.18 re-
spectively. Sparsity can greatly reduce the space
needed to store the lexical function model, espe-
cially when we want to use higher orders of repre-
sentation. Moreover, sparsity in the model is help-
ful to interpret the concept a specific functor word
is conveying. For example, we show how to an-
alyze the coefficient matrices for functor content
words (verbs and adjectives). The verb burst and
adjective poisonous, when estimated in the space
projected to 100 dimensions with NMF, have per-
centages of sparsity 47% and 39% respectively,
which means 47% of the entries in the burst ma-
trix and 39% of the entries in the poisonous ma-
trix are zeros.
4
Most of the (hopefully) irrelevant
dimensions were discarded during model training.
For visualization, we list the 6 most significant
4
We analyze NMF rather than the better-performing SVD
features because the presence of negative values in the latter
makes their interpretation very difficult. And NMF achieves
comparably good performance for interpretation when di-
mension exceeds 100.
columns and rows from verb burst and adjective
poisonous in Table 6. Each reduced NMF di-
mension is represented by the 3 largest original-
context entries in the corresponding row of the
NMF basis matrix. The top columns and rows
are selected by ordering sums of row entries and
sums of column entries (the 10 most common fea-
tures across trained matrices are omitted). In the
matrix-vector multiplication scenario, a larger col-
umn contributes more to all the features of the
composed output phrase vector, while one large
row corresponds to a large composition output fea-
ture. From these tables, we can see that the se-
lected top columns and rows are mostly semanti-
cally relevant to the corresponding functor words
(burst and poisonous, in the displayed examples).
A very interesting aspect of these experiments
is the role of the intercept in our regression model.
The path-wise optimization algorithm starts with
a lambda value (?
max
), which sets all the coef-
ficients exactly to 0, and at that time the inter-
cept is just the expected mean value of the train-
ing phrase vectors, which in turn is of course quite
similar to the co-occurrence vector of the cor-
responding functor word (by averaging the poi-
sonous N context distributions, we obtain a vec-
tor that approximates the poisonous distribution).
And, although the intercept also changes with dif-
ferent lambda values, it still highly correlates with
the co-occurrence vectors of the functor words
in vector space. For adjectives and verbs, we
compared the initial model?s (?
max
) intercept and
the minimum cross-validation error model inter-
cept with corpus-extracted vectors for the corre-
sponding words. That is, we used the word co-
occurrence vector for a verb or an adjective ex-
tracted from the corpus and projected onto the
reduced feature space (e.g., NMF, 100 dimen-
sions), then computed cosine similarity between
this word meaning representation and its corre-
sponding EnetLex matrix initial and minimum-
error intercepts, respectively. Most of the simi-
larities are still quite high after estimation: The
mean cosine values for adjectives are 0.82 for the
initial intercept and 0.72 for the minimum-error
one. For verbs, the corresponding values are 0.75
and 0.69, respectively. Apparently, the sparsity
constraint helps the intercept retaining information
from training phrases.
Qualitatively, often the intercept encodes the
representation of the original word meaning in
440
burst significant columns significant rows
policeman, mob, guard hurricane, earthquake, disaster
Iraqi, Lebanese, Kurdish conquer, Byzantine, conquest
jealousy, anger, guilt policeman, mob, guard
hurricane, earthquake, disaster terminus, traffic, interchange
defender, keeper, striker convict, sentence, imprisonment
volcanic, sediment, geological boost, unveil, campaigner
poisonous significant columns significant rows
bathroom, wc, shower ventilation, fluid, bacterium
ignite, emit, reactor ignite, emit, reactor
reptile, mammal, predator infectious, infect, infected
ventilation, fluid, bacterium slay, pharaoh, tribe
flowering, shrub, perennial park, lorry, pavement
sauce, onion, garlic knife, pierce, brass
Table 6: Interpretability for verbs and adjectives (exemplified by burst and poisonous).
vector space. For example, if we check the inter-
cept for poisonous, the cosine between the origi-
nal vector space representation (from corpus) and
the minimum-error solution intercept (from train-
ing phrases) is at 0.7. The NMF dimensions cor-
responding with the largest intercept entries are
rather intuitive for poisonous: ?ventilation, fluid,
bacterium?, ?racist, racism, outrage?, ?reptile,
mammal, predator?, ?flowering, shrub, perennial?,
?sceptical, accusation, credibility?, ?infectious, in-
fect, infected?.
The mathematical reason for the above facts lies
in the updating rule of the elastic-net?s intercept:
?
0
=
?
y ?
p
?
j=1
?
?
j
?
x
j
(3)
Sparsity in the regression coefficients (
?
?
j
) encour-
ages intercept ?
0
to stay as close to the mean
value of response
?
y as possible. So the elastic-
net lexical function composition model is de facto
also capturing the inherent meaning of the func-
tor word, learning it from the training word-phrase
pairs. In future research, we would like to test if
these lexical meaning representations are as good
or even better than standard co-occurrence vectors
for single-word similarity tasks.
5 Conclusion
In this paper, we have shown that the lexical func-
tion composition model can be improved by ad-
vanced regression techniques. We use pathwise
coordinate descent optimized elastic-net, testing
it on composing intransitive sentences, adjective-
noun phrases and determiner phrases in compari-
son with other composition models, including lex-
ical function estimated with ridge regression. The
elastic-net method leads to performance gains on
all three tasks. Through sparsity constraints on the
model, elastic-net also introduces interpretability
in the lexical function composition model. The
regression coefficient matrices can often be eas-
ily interpreted by looking at large row and column
sums, as many matrix entries are shrunk to zero.
The intercept of elastic-net regression also plays
an interesting role in the model. With the sparsity
constraints, the intercept of the model tends to re-
tain the inherent meaning of the word by averaging
training phrase vectors.
Our approach naturally generalizes to similar
composition tasks, in particular those involving
higher-order tensors (Grefenstette et al., 2013),
where sparseness might be crucial in producing
compact representations of very large objects. Our
results also suggest that the performance of the
lexical function composition model might be fur-
ther improved with even more advanced methods,
such as nonlinear regression. In the future, we
would also like to explore interpretability more in
depth, by looking at grouping and interaction ef-
fects between features.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES), and we
thank the reviewers for helpful feedback.
441
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), pages 53?57, Sofia, Bulgaria.
Stephen Clark. 2013. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Bradley Efron, Trevor Hastie, Iain Johnstone, and
Robert Tibshirani. 2004. Least angle regression.
The Annals of statistics, 32(2):407?499.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
Latent Semantic Analysis. Discourse Processes,
25:285?307.
Jerome Friedman, Trevor Hastie, Holger H?ofling, and
Robert Tibshirani. 2007. Pathwise coordinate
optimization. The Annals of Applied Statistics,
1(2):302?332.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of statisti-
cal software, 33(1):1.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.
Darrell Laham. 1997. Latent Semantic Analysis
approaches to categorization. In Proceedings of
CogSci, page 979.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Scott McDonald and Chris Brew. 2004. A distribu-
tional model of semantic context effects in lexical
processing. In Proceedings of ACL, pages 17?24,
Barcelona, Spain.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Rob Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
J. Artif. Intell. Res.(JAIR), 44:533?585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
Hui Zou and Trevor Hastie. 2005. Regularization
and variable selection via the elastic net. Journal
of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2):301?320.
442
Distributional Memory: A General
Framework for Corpus-Based Semantics
Marco Baroni?
University of Trento
Alessandro Lenci??
University of Pisa
Research into corpus-based semantics has focused on the development of ad hoc models that treat
single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting
different kinds of distributional information from the corpus. As an alternative to this ?one task,
one model? approach, the Distributional Memory framework extracts distributional information
once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged
into a third-order tensor. Different matrices are then generated from the tensor, and their rows
and columns constitute natural spaces to deal with different semantic problems. In this way,
the same distributional information can be shared across tasks such as modeling word similarity
judgments, discovering synonyms, concept categorization, predicting selectional preferences of
verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia
structures with patterns or example pairs, predicting the typical properties of concepts, and
classifying verbs into alternation classes. Extensive empirical testing in all these domains shows
that a Distributional Memory implementation performs competitively against task-specific al-
gorithms recently reported in the literature for the same tasks, and against our implementations
of several state-of-the-art methods. The Distributional Memory approach is thus shown to be
tenable despite the constraints imposed by its multi-purpose nature.
1. Introduction
The last two decades have seen a rising wave of interest among computational linguists
and cognitive scientists in corpus-basedmodels of semantic representation (Grefenstette
1994; Lund and Burgess 1996; Landauer and Dumais 1997; Schu?tze 1997; Sahlgren 2006;
Bullinaria and Levy 2007; Griffiths, Steyvers, and Tenenbaum 2007; Pado? and Lapata
2007; Lenci 2008; Turney and Pantel 2010). These models, variously known as vector
spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term
we will adopt, distributional semantic models (DSMs), all rely on some version of the
distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree
of semantic similarity between two words (or other linguistic units) can be modeled
? Center for Mind/Brain Sciences (CIMeC), University of Trento, C.so Bettini 31, 38068 Rovereto (TN),
Italy. E-mail: marco.baroni@unitn.it.
?? Department of Linguistics T. Bolelli, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy.
E-mail: alessandro.lenci@ling.unipi.it.
Submission received: 11 January 2010; revised submission received: 15 April 2010; accepted for publication:
1 June 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
as a function of the degree of overlap among their linguistic contexts. Conversely, the
format of distributional representations greatly varies depending on the specific aspects
of meaning they are designed to model.
The most straightforward phenomenon tackled by DSMs is what Turney (2006b)
calls attributional similarity, which encompasses standard taxonomic semantic rela-
tions such as synonymy, co-hyponymy, and hypernymy. Words like dog and puppy,
for example, are attributionally similar in the sense that their meanings share a large
number of attributes: They are animals, they bark, and so on. Attributional similarity
is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and
Burgess 1996; Schu?tze 1997; Bullinaria and Levy 2007; Pado? and Lapata 2007). These
collocates are seen as proxies for various attributes of the concepts that the words
denote. Words that share many collocates denote concepts that share many attributes.
Both dog and puppy may occur near owner, leash, and bark, because these words denote
properties that are shared by dogs and puppies. The attributional similarity between
dog and puppy, as approximated by their contextual similarity, will be very high.
DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or
concept categorization (Almuhareb and Poesio 2004) because such tasks require a mea-
sure of attributional similarity that favors concepts that share many properties, such
as synonyms and co-hyponyms. However, many other tasks require detecting different
kinds of semantic similarity. Turney (2006b) defines relational similarity as the property
shared by pairs of words (e.g, dog?animal and car?vehicle) linked by similar semantic
relations (e.g., hypernymy), despite the fact that the words in one pair might not be
attributionally similar to those in the other pair (e.g., dog is not attributionally similar to
car, nor is animal to vehicle). Turney generalizes DSMs to tackle relational similarity and
represents pairs of words in the space of the patterns that connect them in the corpus.
Pairs of words that are connected by similar patterns probably hold similar relations,
that is, they are relationally similar. For example, we can hypothesize that dog?tail is
more similar to car?wheel than to dog?animal, because the patterns connecting dog and
tail (of, have, etc.) are more like those of car?wheel than like those of dog?animal (is a, such
as, etc.). Turney uses the relational space to implement tasks such as solving analogies
and harvesting instances of relations. Although they are not explicitly expressed in
these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and
Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and
focus on learning one relation type at a time (e.g., finding parts).
Although semantic similarity, either attributional or relational, has the lion?s share
in DSMs, similarity is not the only aspect of meaning that is addressed by distributional
approaches. For instance, the notion of property plays a key role in cognitive science and
linguistics, which both typically represent concepts as clusters of properties (Jackendoff
1990; Murphy 2002). In this case, the task is not to find out that dog is similar to puppy
or cat, but that it has a tail, it is used for hunting, and so on. Almuhareb (2006), Baroni
and Lenci (2008), and Baroni et al (2010) use the words co-occurring with a noun to
approximate its most prototypical properties and correlate distributionally derived data
with the properties produced by human subjects. Cimiano and Wenderoth (2007) in-
stead focus on that subset of noun properties known in lexical semantics as qualia roles
(Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive
parts of a concept or its function (this is in turn analogous to the problem of relation
extraction). The distributional semantics methodology also extends to more complex
aspects of word meaning, addressing issues such as verb selectional preferences (Erk
2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James
2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture
674
Baroni and Lenci Distributional Memory
a sort of ?topical? relatedness between words: They might find, for example, a relation
between dog and fidelity. Topical relatedness, addressed by DSMs based on document
distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Griffiths,
Steyvers, and Tenenbaum 2007), is not further discussed in this article.
DSMs have found wide applications in computational lexicography, especially for
automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens
2002; Kilgarriff et al 2004; Rapp 2004). Corpus-based semantic models have also at-
tracted the attention of lexical semanticists as a way to provide the notion of synonymy
with a more robust empirical foundation (Geeraerts 2010; Heylen et al 2008). Moreover,
DSMs for attributional and relational similarity are widely used for the semi-automatic
bootstrapping or extension of terminological repositories, computational lexicons (e.g.,
WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Inno-
vative applications of corpus-based semantics are also being explored in linguistics,
for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical
variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions
(Alishahi and Stevenson 2008).
The wealth and variety of semantic issues that DSMs are able to tackle confirms
the importance of looking at distributional data to explore meaning, as well as the
maturity of this research field. However, if we looked from a distance at the whole field
of DSMs we would see that, besides the general assumption shared by all models that
information about the context of a word is an important key in grasping its meaning, the
elements of difference overcome the commonalities. For instance, DSMs geared towards
attributional similarity represent words in the contexts of other (content) words, thereby
looking very different from models that represent word pairs in terms of patterns
linking them. In turn, both these models differ from those used to explore concept
properties or argument alternations. The typical approach in the field has been a local
one, in which each semantic task (or set of closely related tasks) is treated as a separate
problem, that requires its own corpus-derived model and algorithm, both optimized to
achieve the best performance in a given task, but lacking generality, since they resort
to task-specific distributional representations, often complemented by additional task-
specific resources. As a consequence, the landscape of DSMs looks more like a jigsaw
puzzle in which different parts have been completed and the whole figure starts to
emerge from the fragments, but it is not clear yet how to put everything together and
compose a coherent picture.
We argue that the ?one semantic task, one distributional model? approach repre-
sents a great limit of the current state of the art. From a theoretical perspective, corpus-
based models hold promise as large-scale simulations of how humans acquire and use
conceptual and linguistic information from their environment (Landauer and Dumais
1997). However, existing DSMs lack exactly the multi-purpose nature that is a hallmark
of human semantic competence. The common view in cognitive (neuro)science is that
humans resort to a single semantic memory, a relatively stable long-term knowledge
database, adapting the information stored there to the various tasks at hand (Murphy
2002; Rogers and McClelland 2004). The fact that DSMs need to go back to their
environment (the corpus) to collect ad hoc statistics for each semantic task, and the fact
that different aspects of meaning require highly different distributional representations,
cast many shadows on the plausibility of DSMs as general models of semantic mem-
ory. From a practical perspective, going back to the corpus to train a different model for
each application is inefficient, and it runs the risk of overfitting the model to a specific
task, while losing sight of its adaptivity?a highly desirable feature for any intelligent
system. Think, by contrast, of WordNet (Fellbaum 1998), a single, general purpose
675
Computational Linguistics Volume 36, Number 4
network of semantic information that has been adapted to all sorts of tasks, many of
them certainly not envisaged by the resource creators. We think that it is not by chance
that no comparable resource has emerged from DSM development.
In this article, we want to show that a unified approach is not only a desirable
goal, but it is also a feasible one. With this aim in mind, we introduce Distributional
Memory (DM), a generalized framework for distributional semantics. Differently from
other current proposals that share similar aims, we believe that the lack of generalization
in corpus-based semantics stems from the choice of representing co-occurrence statistics
directly as matrices?geometrical objects that model distributional data in terms of
binary relations between target items (the matrix rows) and their contexts (the matrix
columns). This results in the development of ad hocmodels that lose sight of the fact that
different semantic spaces actually rely on the same kind of underlying distributional
information. DM instead represents corpus-extracted co-occurrences as a third-order
tensor, a ternary geometrical object that models distributional data in terms of word?
link?word tuples. Matrices are then generated from the tensor in order to perform se-
mantic tasks in the spaces they define. Crucially, these on-demand matrices are derived
from the same underlying resource (the tensor) and correspond to different ?views?
of the same data, extracted once and for all from a corpus. DM is tested here on what
we believe to be the most varied array of semantic tasks ever addressed by a single
distributional model. In all cases, we compare the performance of several DM imple-
mentations to state-of-the-art results. While some of the ad hoc models that were devel-
oped to tackle specific tasks do outperform our most successful DM implementation,
the latter is never too far from the top, without any task-specific tuning. We think that
the advantage of having a general model that does not need to be retrained for each new
task outweighs the (often minor) performance advantage of the task-specific models.
The article is structured as follows. After framing our proposal within the general
debate on co-occurrence modeling in distributional semantics (Section 2), we introduce
the DM framework in Section 3 and compare it to other unified approaches in Section 4.
Section 5 pertains to the specific implementations of the DM framework we will test
experimentally. The experiments are reported in Section 6. Section 7 concludes by
summarizing what we have achieved, and discussing the implications of these results
for corpus-based distributional semantics.
2. Modeling Co-occurrence in Distributional Semantics
Corpus-based semantics aims at characterizing the meaning of linguistic expressions in
terms of their distributional properties. The standard view models such properties in
terms of two-way structures, that is, matrices coupling target elements (either single
words or whatever other linguistic constructions we try to capture distributionally)
and contexts. In fact, the formal definition of semantic space provided by Pado? and
Lapata (2007) is built around the notion of a matrix M|B|?|T|, with B the set of basis
elements representing the contexts used to compare the distributional similarity of the
target elements T.
This binary structure is inherently suitable for approaches that represent distribu-
tional data in terms of unstructured co-occurrence relations between an element and
a context. The latter can be either documents (Landauer and Dumais 1997; Griffiths,
Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the
target (Lund and Burgess 1996; Schu?tze 1997; Rapp 2003; Bullinaria and Levy 2007). We
will refer to such models as unstructured DSMs, because they do not use the linguistic
structure of texts to compute co-occurrences, and only record whether the target occurs
676
Baroni and Lenci Distributional Memory
in or close to the context element, without considering the type of this relation. For
instance, an unstructured DSM might derive from a sentence like The teacher eats a red
apple that eat is a feature shared by apple and red, just because they appear in the same
context window, without considering the fact that there is no real linguistic relation
linking eat and red, besides that of linear proximity.
In structured DSMs, co-occurrence statistics are collected instead in the form of
corpus-derived triples: typically, word pairs and the parser-extracted syntactic relation
or lexico-syntactic pattern that links them, under the assumption that the surface con-
nection between two words should cue their semantic relation (Grefenstette 1994; Lin
1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Turney 2006b; Pado? and
Lapata 2007; Erk and Pado? 2008; Rothenha?usler and Schu?tze 2009). Distributional triples
are also used in computational lexicography to identify the grammatical and colloca-
tional behavior of a word and to define its semantic similarity spaces. For instance,
the Sketch Engine1 builds ?word sketches? consisting of triples extracted from parsed
corpora and formed by two words linked by a grammatical relation (Kilgarriff et al
2004). The number of shared triples is then used to measure the attributional similarity
between word pairs.
Structured models take into account the crucial role played by syntactic structures
in shaping the distributional properties of words. To qualify as context of a target item,
a word must be linked to it by some (interesting) lexico-syntactic relation, which is
also typically used to distinguish the type of this co-occurrence. Given the sentence
The teacher eats a red apple, structured DSMs would not consider eat as a legitimate con-
text for red and would distinguish the object relation connecting eat and apple as a
different type of co-occurrence from the modifier relation linking red and apple. On the
other hand, structured models require more preliminary corpus processing (parsing or
extraction of lexico-syntactic patterns), and tend to be more sparse (because there are
more triples than pairs). What little systematic comparison of the two approaches has
been carried out (Pado? and Lapata 2007; Rothenha?usler and Schu?tze 2009) suggests
that structured models have a slight edge. In our experiments in Section 6.1 herein, the
performance of unstructured and structured models trained on the same corpus is in
general comparable. It seems safe to conclude that structured models are at least not
worse than unstructured models?an important conclusion for us, as DM is built upon
the structured DSM idea.
Structured DSMs extract a much richer array of distributional information from
linguistic input, but they still represent it in the same way as unstructured models.
The corpus-derived ternary data are mapped directly onto a two-way matrix, either
by dropping one element from the tuple (Pado? and Lapata 2007) or, more commonly, by
concatenating two elements. The two words can be concatenated, treating the links as
basis elements, in order to model relational similarity (Pantel and Pennacchiotti 2006;
Turney 2006b). Alternatively, pairs formed by the link and one word are concatenated
as basis elements to measure attributional similarity among the other words, treated
as target elements (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareb
and Poesio 2004; Rothenha?usler and Schu?tze 2009). In this way, typed DSMs obtain
finer-grained features to compute distributional similarity, but, by couching distribu-
tional information as two-way matrices, they lose the high expressive power of corpus-
derived triples. We believe that falling short of fully exploiting the potential of ternary
1 http://www.sketchengine.co.uk.
677
Computational Linguistics Volume 36, Number 4
distributional structures is the major reason for the lack of unification in corpus-based
semantics.
The debate in DSMs has so far mostly focused on the context choice?for example,
lexical collocates vs. documents (Sahlgren 2006; Turney and Pantel 2010)?or on the
costs and benefits of having structured contexts (Pado? and Lapata 2007; Rothenha?usler
and Schu?tze 2009). Although we see the importance of these issues, we believe that a
real breakthrough in DSMs can only be achieved by overcoming the limits of current
two-way models of distributional data. We propose here the alternative DM approach,
in which the core geometrical structure of a distributional model is a three-way object,
namely a third-order tensor. As in structured DSMs, we adopt word?link?word tuples
as the most suitable way to capture distributional facts. However, we extend and
generalize this assumption, by proposing that, once they are formalized as a three-
way tensor, tuples can become the backbone of a unified model for distributional
semantics. Different semantic spaces are then generated on demand through the inde-
pendently motivated operation of tensor matricization, mapping the third-order tensor
onto two-way matrices. The matricization of the tuple tensor produces both familiar
spaces, similar to those commonly used for attributional or relational similarity, and
other less known distributional spaces, which will yet prove useful for capturing some
interesting semantic phenomena. The crucial fact is that all these different semantic
spaces are now alternative views of the same underlying distributional object. Appar-
ently unrelated semantic tasks can be addressed in terms of the same distributional
memory, harvested only once from the source corpus. Thus, thanks to the tensor-based
representation, distributional data can be turned into a general purpose resource for
semantic modeling. As a further advantage, the third-order tensor formalization of
corpus-based tuples allows distributional information to be represented in a similar
way to other types of knowledge. In linguistics, cognitive science, and AI, semantic and
conceptual knowledge is represented in terms of symbolic structures built around typed
relations between elements, such as synsets, concepts, properties, and so forth. This is
customary in lexical networks like WordNet (Fellbaum 1998), commonsense resources
like ConceptNet (Liu and Singh 2004), and cognitive models of semantic memory
(Rogers andMcClelland 2004). The tensor representation of corpus-based distributional
data promises to build new bridges between existing approaches to semantic represen-
tation that still appear distant in many respects. This may indeed contribute to the on-
going efforts to combine distributional and symbolic approaches to meaning (Clark and
Pulman 2007).
3. The Distributional Memory Framework
We first introduce the notion of a weighted tuple structure, the format in which DM
expects the distributional data extracted from the corpus to be arranged (and that it
shares with traditional structured DSMs). We then show how aweighted tuple structure
can be represented, in linear algebraic terms, as a labeled third-order tensor. Finally,
we derive different semantic vector spaces from the tensor by the operation of labeled
tensor matricization.
3.1 Weighted Tuple Structures
Relations among entities can be represented by ternary tuples, or triples. Let O1 and
O2 be two sets of objects, and R ? O1 ?O2 a set of relations between these objects.
A triple ?o1, r, o2? expresses the fact that o1 is linked to o2 through the relation r. DM
678
Baroni and Lenci Distributional Memory
(like previous structured DSMs) includes tuples of a particular type, namely, weighted
distributional tuples that encode distributional facts in terms of typed co-occurrence
relations among words. Let W1 and W2 be sets of strings representing content words,
and L a set of strings representing syntagmatic co-occurrence links between words in
a text. T ?W1 ? L?W2 is a set of corpus-derived tuples t = ?w1, l,w2?, such that w1
co-occurs with w2 and l represents the type of this co-occurrence relation. For instance,
the tuple ?marine, use, bomb? in the toy example reported in Table 1 encodes the piece
of distributional information that marine co-occurs with bomb in the corpus, and use
specifies the type of the syntagmatic link between these words. Each tuple t has a
weight, a real-valued score vt, assigned by a scoring function ? :W1 ? L?W2 ? R.
A weighted tuple structure consists of the set TW of weighted distributional tuples
tw = ?t, vt? for all t ? T and ?(t) = vt. The ? function encapsulates all the operations
performed to score the tuples, for example, by processing an input corpus with a
dependency parser, counting the occurrences of tuples, and weighting the raw counts
by mutual information. Because our focus is on how tuples, once they are harvested,
should be represented geometrically, we gloss over the important challenges of choos-
ing the appropriateW1, L andW2 string sets, as well as specifying ?.
In this article, we make the further assumption that W1 =W2. This is a natural
assumption when the tuples represent (link-mediated) co-occurrences of word pairs.
Moreover, we enforce an inverse link constraint such that for any link l in L, there is a
k in L such that for each tuple tw = ??wi, l,wj?, vt? in the weighted tuple structure TW ,
the tuple t?1w = ??wj, k,wi?, vt? is also in TW (we call k the inverse link of l). Again, this
seems reasonable in our context: If we extract a tuple ?marine, use, gun? and assign it a
certain score, we might as well add the tuple ?gun, use?1, marine? with the same score.
The two assumptions, combined, lead the matricization process described in Section 3.3
to generate exactly four distinct vector spaces that, as we discuss there, are needed for
the semantic analyses we conduct. See Section 6.6 of Turney (2006b) for a discussion of
similar assumptions. Still, it is worth emphasizing that the general formalism we are
proposing, where corpus-extracted weighted tuple structures are represented as labeled
tensors, does not strictly require these assumptions. For example,W2 could be a larger
set of ?relata? including not only words, but also documents, morphological features,
or even visual features (with appropriate links, such as, for word-document relations,
occurs-at-the-beginning-of ). The inverse link constraint might not be appropriate, for
example, if we use an asymmetric association measure, or if we are only interested in
one direction of certain grammatical relations. We leave the investigation of all these
possibilities to further studies.
Table 1
A toy weighted tuple structure.
word link word weight word link word weight
marine own bomb 40.0 sergeant use gun 51.9
marine use bomb 82.1 sergeant own book 8.0
marine own gun 85.3 sergeant use book 10.1
marine use gun 44.8 teacher own bomb 5.2
marine own book 3.2 teacher use bomb 7.0
marine use book 3.3 teacher own gun 9.3
sergeant own bomb 16.7 teacher use gun 4.7
sergeant use bomb 69.5 teacher own book 48.4
sergeant own gun 73.4 teacher use book 53.6
679
Computational Linguistics Volume 36, Number 4
3.2 Labeled Tensors
DSMs adopting a binary model of distributional information (either unstructured mod-
els or structured models reduced to binary structures) are represented by matrices
containing corpus-derived co-occurrence statistics, with rows and columns labeled by
the target elements and their contexts. In DM,we formalize the weighted tuple structure
as a labeled third-order tensor, from which semantic spaces are then derived through
the operation of labeled matricization. Tensors are multi-way arrays, conventionally
denoted by boldface Euler script letters: X (Turney 2007; Kolda and Bader 2009). The
order (or n-way) of a tensor is the number of indices needed to identify its elements.
Tensors are a generalization of vectors and matrices. The entries in a vector can be
denoted by a single index. Vectors are thus first-order tensors, often indicated by a bold
lowercase letter: v. The i-th element of a vector v is indicated by vi. Matrices are second-
order tensors, and are indicatedwith bold capital letters:A. The entry (i, j) in the i-th row
and j-th column of a matrix A is denoted by aij. An array with three indices is a third-
order (or three-way) tensor. The element (i, j, k) of a third-order tensor X is denoted
by xijk. A convenient way to display third-order tensors is via nested tables such as
Table 2, where the first index is in the header column, the second index in the first
header row, and the third index in the second header row. The entry x321 of the tensor in
the table is 7.0 and the entry x112 is 85.3. An index has dimensionality I if it ranges over
the integers from 1 to I. The dimensionality of a third-order tensor is the product of the
dimensionalities of its indices I ? J ? K. For example, the third-order tensor in Table 2
has dimensionality 3? 2? 3.
If we fix the integer i as the value of the first index of a matrix A and take the
entries corresponding to the full range of values of the other index j, we obtain a row
vector (that we denote ai?). Similarly, by fixing the second index to j, we obtain the
column vector a?j. Generalizing, a fiber is equivalent to rows and columns in higher
order tensors, and it is obtained by fixing the values of all indices but one. A mode-n
fiber is a fiber where only the n-th index has not been fixed. For example, in the tensor
X of Table 2, x?11 = (40.0, 16.7, 5.2) is a mode-1 fiber, x2?3 = (8.0, 10.1) is a mode-2 fiber,
and x32? = (7.0, 4.7, 53.6) is a mode-3 fiber.
A weighted tuple structure can be represented as a third-order tensor whose entries
contain the tuple scores. As for the two-way matrices of classic DSMs, in order to make
tensors linguistically meaningful we need to assign linguistic labels to the elements of
the tensor indices. We define a labeled tensor X? as a tensor such that for each of its
indices there is a one-to-one mapping of the integers from 1 to I (the dimensionality of
the index) to I distinct strings, that we call the labels of the index. We will refer herein to
the string ? uniquely associated to index element i as the label of i, their correspondence
Table 2
A labeled third-order tensor of dimensionality 3? 2? 3 representing the weighted tuple
structure of Table 1.
j=1:own j=2:use j=1:own j=2:use j=1:own j=2:use
k=1:bomb k=2:gun k=3:book
i=1:marine 40.0 82.1 85.3 44.8 3.2 3.3
i=2:sergeant 16.7 69.5 73.4 51.9 8.0 10.1
i=3:teacher 5.2 7.0 9.3 4.7 48.4 53.6
680
Baroni and Lenci Distributional Memory
being indicated by i : ?. A simple way to perform the mapping?the one we apply in the
running example of this section?is by sorting the I items in the string set alhabetically,
and mapping increasing integers from 1 to I to the sorted strings.
A weighted tuple structure TW built from W1, L, and W2 can be represented by a
labeled third-order tensor X? with its three indices labeled by W1, L, and W2, respec-
tively, and such that for each weighted tuple t ? TW = ??w1, l,w2?, vt? there is a tensor
entry (i : w1, j : l, k : w2)= vt. In other terms, a weighted tuple structure corresponds to
a tensor whose indices are labeled with the string sets forming the triples, and whose
entries are the tuple weights. Given the toy weighted tuple structure in Table 1, the
object in Table 2 is the corresponding labeled third-order tensor.
3.3 Labeled Matricization
Matricization rearranges a higher order tensor into a matrix (Kolda 2006; Kolda and
Bader 2009). The simplest case is mode-n matricization, which arranges the mode-n
fibers to be the columns of the resultingDn ?Dj matrix (whereDn is the dimensionality
of the n-th index, Dj is the product of the dimensionalities of the other indices). Mode-n
matricization of a third-order tensor can be intuitively understood as the process of
making vertical, horizontal, or depth-wise slices of a three-way object like the tensor in
Table 2, and arranging these slices sequentially to obtain a matrix (a two-way object).
Matricization unfolds the tensor into a matrix with the n-th index indexing the rows of
the matrix and a column for each pair of elements from the other two tensor indices. For
example, the mode-1 matricization of the tensor in Table 2 results in a matrix with the
entries vertically arranged as they are in the table, but replacing the second and third
indices with a single index ranging from 1 to 6 (cf. matrix A of Table 3). More explicitly,
in mode-n matricization we map each tensor entry (i1, i2, ..., iN ) to matrix entry (in, j),
where j is computed as in Equation (1), adapted from Kolda and Bader (2009).
j = 1+
N
?
k=1
k =n
((ik ? 1)
k?1
?
m=1
m =n
Dm) (1)
For example, if we apply mode-1 matricization to the tensor of dimensionality 3? 2? 3
in Table 2, we obtain the matrix A3?6 in Table 3 (ignore the labels for now). The tensor
entry x3,1,1 is mapped to the matrix cell a3,1; x3,2,3 is mapped to a3,6; and x1,2,2 is mapped
to a1,4. Observe that each column of the matrix is a mode-1 fiber of the tensor: The first
column is the x?11 fiber; the second column is the x?21 fiber, and so on.
Matricization has various mathematically interesting properties and practical appli-
cations in computations involving tensors (Kolda 2006). In DM, matricization is applied
to labeled tensors and it is the fundamental operation for turning the third-order tensor
representing the weighted tuple structure into matrices whose row and column vector
spaces correspond to the linguistic objects we want to study; that is, the outcome of
matricization must be labeled matrices. Therefore, we must define an operation of
labeled mode-n matricization. Recall from earlier discussion that when mode-n matri-
cization is applied, the n-th index becomes the row index of the resultingmatrix, and the
corresponding labels do not need to be updated. The problem is to determine the labels
of the column index of the resulting matrix. We saw that the columns of the matrix pro-
duced by mode-n matricization are the mode-n fibers of the original tensor. We must
681
Computational Linguistics Volume 36, Number 4
Table 3
Labeled mode-1, mode-2, and mode-3 matricizations of the tensor in Table 2.
Amode-1 1:?own, 2:?use, 3:?own, 4:?use, 5:?own, 6:?use,
bomb? bomb? gun? gun? book? book?
1:marine 40.0 82.1 85.3 44.8 3.2 3.3
2:sergeant 16.7 69.5 73.4 51.9 8.0 10.1
3:teacher 5.2 7.0 9.3 4.7 48.4 53.6
Bmode-2 1:?marine, 2:?serg., 3:?teacher, 4:?marine, 5:?serg., 6:?teacher, 7:?marine, 8:?serg., 9:?teach.,
bomb? bomb? bomb? gun? gun? gun? book? book? book?
1:own 40.0 16.7 5.2 85.3 73.4 9.3 3.2 8.0 48.4
2:use 82.1 69.5 7.0 44.8 51.9 4.7 3.3 10.1 53.6
Cmode-3 1:?marine, 2:?marine, 3:?sergeant, 4:?sergeant, 5:?teacher, 6:?teacher,
own? use? own? use? own? use?
1:bomb 40.0 82.1 16.7 69.5 5.2 7.0
2:gun 85.3 44.8 73.4 51.9 9.3 4.7
3:book 3.2 3.3 8.0 10.1 48.4 53.6
therefore assign a proper label to mode-n tensor fibers. A mode-n fiber is obtained by
fixing the values of two indices, and by taking the tensor entries corresponding to the
full range of values of the third index. Thus, the natural choice for labeling a mode-n
fiber is to use the pair formed by the labels of the two index elements that are fixed.
Specifically, each mode-n fiber of a tensor X? is labeled with the binary tuple whose
elements are the labels of the corresponding fixed index elements. For instance, given
the labeled tensor in Table 2, the mode-1 fiber x?11 = (40, 16.7, 5.2) is labeled with the
pair ?own, bomb?, the mode-2 fiber x2?1 = (16.7, 69.5) is labeled with the pair ?sergeant,
bomb?, and the mode-3 fiber x32? = (7.0, 4.7, 53.6) is labeled with the pair ?teacher, use?.
Because mode-n fibers are the columns of the matrices obtained through mode-n
matricization, we define the operation of labeled mode-n matricization that, given
a labeled third-order tensor X?, maps each entry (i1 : ?1, i2 : ?2, i3 : ?3) to the labeled
entry (in : ?n, j : ?j) such that j is obtained according to Equation (1), and ?j is the bi-
nary tuple obtained from the triple ??1, ?2, ?3? by removing ?n. For instance, in mode-1
matricization, the entry (1:marine, 1:own, 2:gun) in the tensor in Table 2 is mapped onto
the entry (1:marine, 3:?own, gun?). Table 3 reports the matrices A, B, and C, respectively,
obtained by applying labeled mode-1, mode-2, and mode-3 matricization to the labeled
tensor in Table 2. The columns of each matrix are labeled with pairs, according to the
definition of labeled matricization we just gave. From now on, when we refer to mode-n
matricization we always assume we are performing labeledmode-nmatricization.
The rows and columns of the three matrices resulting from n-mode matricization
of a third-order tensor are vectors in spaces whose dimensions are the corresponding
column and row elements. Such vectors can be used to perform all standard linear
algebra operations applied in vector-based semantics: Measuring the cosine of the
angle between vectors, applying singular value decomposition (SVD) to the whole
matrix, and so on. Under the assumption thatW1 =W2 and the inverse link constraint
(see Section 3.1), it follows that for each column of the matrix resulting from mode-1
matricization and labeled by ?l,w2?, there will be a column in the matrix resulting
682
Baroni and Lenci Distributional Memory
from mode-3 matricization that is labeled by ?w1, k? (with k being the inverse link of
l and w1 = w2) and that is identical to the former, except possibly for the order of the
dimensions (which is irrelevant to all operations we perform on matrices and vectors,
however). Similarly, for any row w2 in the matrix resulting from mode-3 matricization,
there will be an identical row w1 in the mode-1 matricization. Therefore, given a
weighted tuple structure TW extracted from a corpus and subject to the constraints we
just mentioned, by matricizing the corresponding labeled third-order tensor X ? we
obtain the following four distinct semantic vector spaces:
word by link?word (W1?LW2): vectors are labeled with words w1, and vector
dimensions are labeled with tuples of type ?l,w2?;
word?word by link (W1W2?L): vectors are labeled with tuples of type ?w1,w2?,
and vector dimensions are labeled with links l;
word?link by word (W1L?W2): vectors are labeled with tuples of type ?w1, l?,
and vector dimensions are labeled with words w2;
link by word?word (L?W1W2): vectors are labeled with links l and vector
dimensions are labeled with tuples of type ?w1,w2?.
Words like marine and teacher are represented in the W1?LW2 space by vectors whose
dimensions correspond to features such as ?use, gun? or ?own, book?. In this space,
we can measure the similarity of words to each other, in order to tackle attributional
similarity tasks such as synonym detection or concept categorization. The W1W2?L
vectors represent instead word pairs in a space whose dimensions are links, and it
can be used to measure relational similarity among different pairs. For example, one
could notice that the link vector of ?sergeant, gun? is highly similar to that of ?teacher,
book?. Crucially, as can be seen in Table 3, the corpus-derived scores that populate the
vectors in these two spaces are exactly the same, just arranged in different ways. In DM,
attributional and relational similarity spaces are different views of the same underlying
tuple structure.
The other two distinct spaces generated by tensor matricization look less familiar,
and yet we argue that they allow us to subsume under the same general DM framework
other interesting semantic phenomena. We will show in Section 6.3 how the W1L?W2
space can be used to capture different verb classes based on the argument alternations
they display. For instance, this space can be used to find out that the object slot of kill
is more similar to the subject slot of die than to the subject slot of kill (and, generalizing
from similar observations, that the subject slot of die is a theme rather than an agent). The
L?W1W2 space displays similarities among links. The usefulness of this will of course
depend on what the links are. We will illustrate in Section 6.4 one function of this space,
namely, to perform feature selection, picking links that can then be used to determine a
meaningful subspace of theW1W2?L space.
Direct matricization is just one of the possible uses we can make of the labeled
tensor. In Section 6.5 we illustrate another use of the tensor formalism by performing
smoothing through tensor decomposition. Other possibilities, such as graph-based algo-
rithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), or
deriving unstructured semantic spaces from the tensor by removing one of the indices,
are left to future work.
Before we move on, it is worth emphasizing that, from a computational point of
view, there is virtually no additional cost in the tensor approach, with respect to tra-
ditional structured DSMs. The labeled tensor is nothing other than a formalization of
683
Computational Linguistics Volume 36, Number 4
distributional data extracted in the word?link?word?score format, which is customary
in many structured DSMs. Labeled matricization can then simply be obtained by con-
catenating two elements in the original triple to build the corresponding matrix?again,
a common step in building a structured DSM. In spite of being cost-free in terms of
implementation, the mathematical formalism of labeled tensors highlights the common
core shared by different views of the semantic space, thereby making distributional
semantics more general.
4. Related Work
As will be clear in the next sections, the ways in which we tackle specific tasks are, by
themselves, mostly not original. The main element of novelty is the fact that methods
originally developed to resort to ad hoc distributional spaces are now adapted to fit into
the unified DM framework. We will point out connections to related research specific to
the various tasks in the sections devoted to describing their reinterpretation in DM.
We omit discussion of our own work that the DM framework is an extension and
generalization of Baroni et al (2010) and Baroni and Lenci (2009). Instead, we briefly
discuss two other studies that explicitly advocate a uniform approach to corpus-based
semantic tasks, and one article that, like us, proposes a tensor-based formalization of
corpus-extracted triples. See Turney and Pantel (2010) for a very recent general survey
of DSMs.
Pado? and Lapata (2007), partly inspired by Lowe (2001), have proposed an interest-
ing general formalization of DSMs. In their approach, a corpus-based semantic model is
characterized by (1) a set of functions to extract statistics from the corpus, (2) construc-
tion of the basis-by-target-elements co-occurrence matrix, and (3) a similarity function
operating on the matrix. Our focus is entirely on the second aspect. A DM, according
to the characterization in Section 3, is a labeled tensor based on a source weighted
tuple structure and coupled with matricization operations. How the tuple structure
was built (corpus extraction methods, association measures, etc.) is not part of the DM
formalization. At the other end, DM provides sets of vectors in different vector spaces,
but it is agnostic about how they are used (measuring similarity via cosines or other
measures, reducing the matrices with SVD, etc.). Of course, much of the interesting
progress in distributional semantics will occur at the two ends of our tensor, with better
tuple extraction and weighting techniques on one side, and better matrix manipulation
and similarity measurement on the other. As long as the former operations result in
data that can be arranged into a weighted tuple structure, and the latter procedures act
on vectors, such innovations fit into the DM framework and can be used to improve
performance on tasks defined on any space derivable from the DM tensor.
Whereas the model proposed by Pado? and Lapata (2007) is designed only to ad-
dress tasks involving the measurement of the attributional similarity between words,
Turney (2008) shares with DM the goal of unifying attributional and relational similarity
under the same distributional model. He observes that tasks that are traditionally solved
with an attributional similarity approach can be recast as relational similarity tasks.
Instead of determining whether two words are, for example, synonymous by looking
at the features they share, we can learn what the typical patterns are that connect syn-
onym pairs when they co-occur (also known as, sometimes called, etc.), and make a de-
cision about a potential synonym pair based on their occurrence in similar contexts.
Given a list of pairs instantiating an arbitrary relation, Turney?s PairClass algorithm
extracts patterns that are correlated with the relation, and can be used to discover new
684
Baroni and Lenci Distributional Memory
pairs instantiating it. Turney tests his system in a variety of tasks (TOEFL synonyms;
SAT analogies; distinguishing synonyms and antonyms; distinguishing pairs that are
semantically similar, associated, or both), obtaining good results across the board.
In the DM approach, we collect one set of statistics from the corpus, and then exploit
different views of the extracted data and different algorithms to tackle different tasks.
Turney, on the contrary, uses a single generic algorithm, but must go back to the corpus
to obtain new training data for each new task. We compare DM with some of Turney?s
results in Section 6 but, independently of performance, we find the DM approach more
appealing. As corpora grow in size and are enriched with further levels of annotation,
extracting ad hoc data from them becomes a very time-consuming operation. Although
we did not carry out any systematic experiments, we observe that the extraction of tuple
counts from (already POS-tagged and parsed) corpora in order to train our sample DM
models took days, whereas even the most time-consuming operations to adapt DM to
a task took on the order of 1 to 2 hours on the same machines (task-specific training is
also needed in PairClass, anyway). Similar considerations apply to space: Compressed,
our source corpora take about 21 GB, our best DM tensor (TypeDM) 1.1 GB (and opti-
mized sparse tensor representations could bring this quantity down drastically, if the
need arises). Perhaps more importantly, extracting features from the corpus requires
a considerable amount of NLP know-how (to pre-process the corpus appropriately, to
navigate a dependency tree, etc.), whereas the DM representation of distributional data
as weighted triples is more akin to other standard knowledge representation formats
based on typed relations, which are familiar to most computer and cognitive scientists.
Thus, a trained DM can become a general-purpose resource and be used by researchers
beyond the realms of the NLP community, whereas applying PairClass requires a good
understanding of various aspects of computational linguistics. This severely limits its
interdisciplinary appeal.
At a more abstract level, DM and PairClass differ in the basic strategy with which
unification in distributional semantics is pursued. Turney?s approach amounts to pick-
ing a task (identifying pairs expressing the same relation) and reinterpreting other tasks
as its particular instances. Thus, attributional and relational similarity are unified by
considering the former as a subtype of the latter. Conversely, DM assumes that each
semantic task may keep its specificity, and unification is achieved by designing a suffi-
ciently general distributional structure, populating a specific instance of the structure,
and generating semantic spaces on demand from the latter. This way, DM is able to
address a wider range of semantic tasks than Turney?s model. For instance, language
is full of productive semantic phenomena, such as the selectional preferences of verbs
with respect to unseen arguments (eating topinambur vs. eating sympathy). Predicting
the plausibility of unseen pairs cannot, by definition, be tackled by the current version
of PairClass, which will have to be expanded to deal with such cases, perhaps adopting
ideas similar to those we present (that are, in turn, inspired by Turney?s own work on
attributional and relational similarity). A first step in this direction, within a framework
similar to Turney?s, was taken by Herdag?delen and Baroni (2009).
Turney (2007) explicitly formalizes the set of corpus-extracted word?link?word
triples as a tensor, and was our primary source of inspiration in formalizing DM in these
terms. The focus of Turney?s article, however, is on dimensionality reduction techniques
applied to tensors, and the application to corpora is only briefly discussed. Moreover,
Turney only derives the W1?LW2 space from the tensor, and does not discuss the pos-
sibility of using the tensor-based formalization to unify different views of semantic data,
which is instead our main point. The higher-order tensor dimensionality reduction
techniques tested on language data by Turney (2007) and Van de Cruys (2009) can be
685
Computational Linguistics Volume 36, Number 4
applied to the DM tensors before matricization. We present a pilot study in this direc-
tion in Section 6.5.
5. Implementing DM
5.1 Extraction of Weighted Tuple Structures from Corpora
In order to make our proposal concrete, we experiment with three different DMmodels,
corresponding to different ways to construct the underlying weighted tuple structure
(Section 3.1). All models are based on the natural idea of extracting word?link?word
tuples from a dependency parse of a corpus, but this is not a requirement for DM: The
links could for example be based on frequent n-grams as in Turney (2006b) and Baroni
et al (2010), or even on very different kinds of relation, such as co-occurring within the
same document.
The current models are trained on the concatenation of (1) the Web-derived ukWaC
corpus,2 about 1.915 billion tokens (here and subsequently, counting only strings
that are entirely made of alphabetic characters); (2) a mid-2009 dump of the English
Wikipedia,3 about 820 million tokens; and (3) the British National Corpus,4 about
95 million tokens. The resulting concatenated corpus was tokenized, POS-tagged, and
lemmatized with the TreeTagger5 and dependency-parsed with the MaltParser.6 It
contains about 2.83 billion tokens. The ukWaC and Wikipedia sections can be freely
downloaded, with full annotation, from the ukWaC corpus site.
For all our models, the label sets W1 =W2 contain 30,693 lemmas (20,410 nouns,
5,026 verbs, and 5,257 adjectives). These terms were selected based on their frequency
in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000
most frequent verbs and adjectives), augmenting the list with lemmas that we found in
various standard test sets, such as the TOEFL and SAT lists. In all models, the words are
stored in POS-suffixed lemma form. The weighted tuple structures differ for the choice
of links in L and/or for the scoring function ?.
DepDM. Our first DM model relies on the classic intuition that dependency paths are
a good approximation to semantic relations between words (Grefenstette 1994; Curran
and Moens 2002; Pado? and Lapata 2007; Rothenha?usler and Schu?tze 2009). DepDM is
also the model with the least degree of link lexicalization among the three DM instances
we have built (its only lexicalized links are prepositions). LDepDM includes the follow-
ing noun?verb, noun?noun, and adjective?noun links (in order to select more reliable
dependencies and filter out possible parsing errors, dependencies between words with
more than five intervening items were discarded):
sbj intr: subject of a verb that has no direct object: The teacher is singing?
?teacher, sbj intr, sing?; The soldier talked with his sergeant? ?soldier, sbj intr, talk?;
sbj tr: subject of a verb that occurs with a direct object: The soldier is reading a
book? ?soldier, sbj tr, read?;
2 http://wacky.sslmit.unibo.it/.
3 http://en.wikipedia.org/wiki/Wikipedia:Database download.
4 http://www.natcorp.ox.ac.uk.
5 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/.
6 http://w3.msi.vxu.se/?nivre/research/MaltParser.html.
686
Baroni and Lenci Distributional Memory
obj: direct object: The soldier is reading a book? ?book, obj, read?;
iobj: indirect object in a double object construction: The soldier gave the woman
a book? ?woman, iobj, give?;
nmod: noun modifier: good teacher? ?good, nmod, teacher?; school teacher? ?school,
nmod, teacher?;
coord: noun coordination: teachers and soldiers? ?teacher, coord, soldier?;
prd: predicate noun: The soldier became sergeant? ?sergeant, prd, become?;
verb: an underspecified link between a subject noun and a complement noun
of the same verb: The soldier talked with his sergeant? ?soldier, verb, sergeant?;
The soldier is reading a book? ?soldier, verb, book?;
preposition: every preposition linking the noun head of a prepositional phrase
to its noun or verb head (a different link for each preposition): I saw a soldier
with the gun? ?gun, with, soldier?; The soldier talked with his sergeant?
?sergeant, with, talk?.
For each link, we also extract its inverse (this holds for all our DMmodels). For example,
there is a sbj intr?1 link between an intransitive verb and its subject: ?talk, sbj intr?1,
soldier?. The cardinality of LDepDM is 796, including direct and inverse links.
The weights assigned to the tuples by the scoring function ? are given by Local
Mutual Information (LMI) computed on the raw corpus-derived word?link?word co-
occurrence counts. Given the co-occurrence count Oijk of three elements of interest (in
our case, the first word, the link, and the second word), and the corresponding expected
count under independence Eijk, LMI = Oijk log
Oijk
Eijk
. LMI is an approximation to the
log-likelihood ratio measure that has been shown to be a very effective weighting
scheme for sparse frequency counts (Dunning 1993; Pado? and Lapata 2007). The mea-
sure can also be interpreted as the dominant term of average MI or as a heuristic
variant of pointwise MI to avoid its bias towards overestimating the significance of
low frequency events, and it is nearly identical to the Poisson?Stirling measure (Evert
2005). LMI has considerable computational advantages in cases like ours, in which we
measure the association of three elements, because it does not require keeping track
of the full 2? 2? 2 contingency table, which is the case for the log-likelihood ratio.
Following standard practice (Bullinaria and Levy 2007), negative weights (cases where
the observed value is lower than the expected value) are raised to 0. The number of
non-zero tuples in the DepDM tensor is about 110M, including tuples with direct links
and their inverses. DepDM is a 30, 693? 796? 30, 693 tensor with density 0.0149% (the
proportion of non-zero entries in the tensor).
LexDM. The second model is inspired by the idea that the lexical material connect-
ing two words is very informative about their relation (Hearst 1992; Pantel and
Pennacchiotti 2006; Turney 2006b). LLexDM contains complex links, each with the struc-
ture pattern+suffix. The suffix is in turn formed by two substrings separated by a+, each
respectively encoding the following features ofw1 andw2: their POS andmorphological
features (number for nouns, number and tense for verbs); the presence of an article
(further specified with its definiteness value) and of adjectives for nouns; the presence
of adverbs for adjectives; and the presence of adverbs, modals, and auxiliaries for verbs,
together with their diatheses (for passive only). If the adjective (adverb) modifying
w1 or w2 belongs to a list of 10 (250) high frequency adjectives (adverbs), the suffix
687
Computational Linguistics Volume 36, Number 4
string contains the adjective (adverb) itself, otherwise only its POS. For instance, from
the sentence The tall soldier has already shot we extract the tuple ?soldier, sbj intr+n-the-
j+vn-aux-already, shoot?. Its complex link contains the pattern sbj intr and the suffix
n-the-j+vn-aux-already. The suffix substring n-the-j encodes the information that w1 is
a singular noun (n), is definite (the), and has an adjective ( j) that does not belong to
the list of high frequency adjectives. The substring vn-aux-already specifies that w2 is a
past-participle (vn), has an auxiliary (aux), and is modified by already, belonging to the
pre-selected list of high frequency adverbs. The patterns in the LexDM links include:
LDepDM : every DepDM link is a potential pattern of a LexDM link: The soldier has
shot? ?soldier, sbj intr+n-the+vn-aux, shoot?;
verb: if the verb link between a subject noun and a complement noun belongs to
a list of 52 high frequency verbs, the underspecified verb link of DepDM is
replaced by the verb itself: The soldier used a gun? ?soldier, use+n-the+n-a,
gun?; The soldier read the yellow book? ?soldier, verb+n-the+n-the-j, book?;
is: copulative structures with an adjectival predicate: The soldier is tall? ?tall,
is+j+n-the, soldier?;
preposition?link noun?preposition: this schema captures connecting expressions
such as of a number of, in a kind of ; link noun is one of 48 semi-manually
selected nouns such as number, variety, or kind; the arrival of a number of
soldiers? ?soldier, of-number-of+ns+n-the, arrival?;
attribute noun: one of 127 nouns extracted fromWordNet and expressing
attributes of concepts, such as size, color, or height. This pattern connects
adjectives and nouns that occur in the templates (the) attribute noun of
(a|the) NOUN is ADJ (Almuhareb and Poesio 2004) and (a|the) ADJ
attribute noun of NOUN (Veale and Hao 2008): the color of strawberries is red?
?red, color+j+ns, strawberry?; the autumnal color of the forest? ?autumnal,
color+j+n-the, forest?;
as adj as: this pattern links an adjective and a noun that match the template as
ADJ as (a|the) NOUN (Veale and Hao 2008): as sharp as a knife? ?sharp,
as adj as+j+n-a, knife?;
such as: links two nouns occurring in the templates NOUN such as NOUN and
such NOUN as NOUN (Hearst 1992, 1998): animals such as cats?
?animal, such as+ns+ns, cat?; such vehicles as cars? ?vehicle, such as+ns+ns, car?.
LexDM links have a double degree of lexicalization. First, the suffixes encode a wide
array of surface features of the tuple elements. Secondly, the link patterns themselves,
besides including standard syntactic relations (such as direct object or coordination),
extend to lexicalized dependency relations (specific verbs) and lexico-syntactic shallow
templates. The latter include patterns adopted in the literature to extract specific pieces
of semantic knowledge. For instance, NOUN such as NOUN and such NOUN as NOUN
were first proposed by Hearst (1992) as highly reliable patterns for hypernym identifi-
cation, whereas (the) attribute noun of (a|the) NOUN is ADJ and (a|the) ADJ attribute noun
of NOUN were successfully used to identify typical values of concept attributes
(Almuhareb and Poesio 2004; Veale and Hao 2008). Therefore, the LexDM distributional
memory is a repository of partially heterogeneous types of corpus-derived information,
differing in their level of abstractness, which ranges from fairly abstract syntactic rela-
tions to shallow lexicalized patterns. LLexDM contains 3,352,148 links, including inverses.
688
Baroni and Lenci Distributional Memory
The scoring function ? is the same as that in DepDM, and the number of non-
zero tuples is about 355M, including direct and inverse links. LexDM is a 30,693?
3,352,148? 30,693 tensor with density 0.00001%.
TypeDM. This model is based on the idea, motivated and tested by Baroni et al (2010)?
but see also Davidov and Rappoport (2008a, 2008b) for a related method?that what
matters is not so much the frequency of a link, but the variety of surface forms that
express it. For example, if we just look at frequency of co-occurrence (or strength of
association), the triple ? fat, of?1, land? (a figurative expression) is much more common
than the semantically more informative ? fat, of?1, animal?. However, if we count the
different surface realizations of the former pattern in our corpus, we find that there are
only three of them (the fat of the land, the fat of the ADJ land, and the ADJ fat of the land),
whereas ? fat, of?1, animal? has nine distinct realizations (a fat of the animal, the fat of the
animal, fats of animal, fats of the animal, fats of the animals, ADJ fats of the animal, and the fats
of the animal). TypeDM formalizes this intuition by adopting as links the patterns inside
the LexDM links, while the suffixes of these patterns are used to count their number
of distinct surface realizations. We call the model TypeDM because it counts types of
realizations, not tokens. For instance, the two LexDM links of?1+n-a+n-the and of?1+ns-
j+n-the are counted as two occurrences of the same TypeDM link of?1, corresponding to
the pattern in the two original links.
The scoring function ? computes LMI not on the raw word?link?word co-
occurrence counts, but on the number of distinct suffix types displayed by a link when it
co-occurs with the relevant words. For instance, a TypeDM link derived from a LexDM
pattern that occurs with nine different suffix types in the corpus is assigned a frequency
of 9 for the purpose of the computation of LMI. The distinct TypeDM links are 25,336.
The number of non-zero tuples in the TypeDM tensor is about 130M, including direct
and inverse links. TypeDM is a 30, 693? 25, 336? 30, 693 tensor with density 0.0005%.
To sum up, the three DM instance models herein differ in the degree of lexicali-
zation of the link set, and/or in the scoring function. LexDM is a heavily lexicalized
model, contrasting with DepDM, which has a minimum degree of lexicalization, and
consequently the smallest set of links. TypeDM represents a sort of middle level both
for the kind and the number of links. These consist of syntactic and lexicalized patterns,
as in LexDM. The lexical information encoded in the LexDM suffixes, however, is not
used to generate different links, but to implement a different counting scheme as part
of a different scoring function.
A weighted tuple structure (equivalently: a labeled DM tensor) is intended as
a long-term semantic resource that can be used in different projects for different
tasks, analogously to traditional hand-coded resources such as WordNet. Coherent
with this approach, we make our best DM model (TypeDM) publicly available from
http://clic.cimec.unitn.it/dm. The site also contains a set of Perl scripts that per-
form the basic operations on the tensor and its derived vectors we are about to describe.
5.2 Semantic Vector Manipulation
TheDM framework provides, viamatricization, a set of matrices with associated labeled
row and column vectors. These labeled matrices can simply be derived from the tuple
tensor by concatenating two elements in the original triples. Any operation that can
be performed on the resulting matrices and that might help in tackling a semantic
task is fair game. However, in the experiments reported in this article we will work
with a limited number of simple operations that are well-motivated in terms of the
689
Computational Linguistics Volume 36, Number 4
geometric framework we adopt, and suffice to face all the tasks we will deal with (the
decomposition techniques explored in Section 6.5 are briefly introduced there).
Vector length and normalization. The length of a vector v with dimensions v1, v2, . . . , vn is:
||v|| =
?
?i=n
i=1
v2i
A vector is normalized to have length 1 by dividing each dimension by the original
vector length.
Cosine.We measure the similarity of two vectors x and y by the cosine of the angle they
form:
cos(x,y) =
?i=n
i=1 xiyi
||x||||y||
The cosine ranges from?1 for vectors pointing in the same direction to 0 for orthogonal
vectors. Other similarity measures, such as Lin?s measure (Lin 1998b), work better than
the cosine in some tasks (Curran and Moens 2002; Pado? and Lapata 2007). However,
the cosine is the most natural similarity measure in the geometric formalism we are
adopting, and we stick to it as the default approach to measuring similarity.
Vector sum. Two or more vectors are summed in the obvious way, by adding their values
on each dimension. We always normalize the vectors before summing. The resulting
vector points in the same direction as the average of the summed normalized vectors.
We refer to it as the centroid of the vectors.
Projection onto a subspace. It is sometimes useful to measure length or compare vectors by
taking only some of their dimensions into account. For example, one way to find nouns
that are typical objects of the verb to sing is to measure the length of nouns in aW1?LW2
subspace in which only dimensions such as ?obj, sing? have non-0 values. We project
a vector onto a subspace of this kind through multiplication of the vector by a square
diagonal matrix with 1s in the diagonal cells corresponding to the dimensions we want
to preserve and 0s elsewhere. A matrix of this sort performs an orthogonal projection of
the vector it multiplies (Meyer 2000, chapter 5).
6. Semantic Experiments with the DM Spaces
As we saw in Section 3, labeled matricization generates four distinct semantic spaces
from the third-order tensor. For each space, we have selected a set of semantic ex-
periments that we model by applying some combination of the vector manipulation
operations of Section 5.2. The experiments correspond to key semantic tasks in compu-
tational linguistics and/or cognitive science, typically addressed by distinct DSMs so
far. We have also aimed at maximizing the variety of aspects of meaning covered by
the experiments, ranging from synonymy detection to argument structure and concept
properties, and encompassing all the major lexical classes. Both these facts support the
view of DM as a generalized model that is able to overtake state-of-the-art DSMs in
the number and types of semantic issues addressed, while being competitive in each
specific task.
690
Baroni and Lenci Distributional Memory
The choice of the DM semantic space to tackle a particular task is essentially based
on the ?naturalness? with which the task can be modeled in that space. However,
alternatives are conceivable, both with respect to space selection, and to the operations
performed on the space. For instance, Turney (2008) models synonymy detection with
a DSM that closely resembles our W1W2?L space, whereas we tackle this task under
the more standard W1?LW2 view. It is an open question whether there are principled
ways to select the optimal space configuration for a given semantic task. In this article,
we limit ourselves to proving that each space derived through tensor matricization is
semantically interesting in the sense that it provides the proper ground to address some
semantic task.
Feature selection/reweighting and dimensionality reduction have been shown to
improve DSM performance. For instance, the feature bootstrapping method proposed
by Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical en-
tailment recognition. Even if these methods can be applied to DM as well, we did not
use them in our experiments. The results presented subsequently should be regarded as
a ?baseline? performance that could be enhanced in future work by exploring various
task-specific parameters (we will come back in the conclusion to the role of parameter
tuning in DM). This is consistent with our current aim of focusing on the generality and
adaptivity of DM, rather than on task-specific optimization. As a first, important step
in this latter direction, however, we conclude the empirical evaluation in Section 6.5
by replicating one experiment using tensor-decomposition-based smoothing, a form of
optimization that can only be performed within the tensor-based approach to DSMs.
In order to maximize coverage of the experimental test sets, they are pre-processed
with a mixture of manual and heuristic procedures to assign a POS to the words they
contain, lemmatize, convert some multiword forms to single words, and turn some ad-
verbs into adjectives (our models do not contain multiwords or adverbs). Nevertheless,
some words (or word pairs) are unrecoverable, and in such cases we make a random
guess (in cases where we do not have full coverage of a data set, the reported results are
averages across repeated experiments, to account for the variability in random guesses).
In many of the experiments herein, DM is not only compared to the results avail-
able in the literature, but also to our implementation of state-of-the-art DSMs. These
alternative models have been trained on the same corpus (with the same linguistic pre-
processing) used to build the DM tuple tensors. This way, we aim at achieving a fairer
comparison with alternative approaches in distributional semantics, abstracting away
from the effects induced by differences in the training data.
Most experiments report global (micro-averaged) test set accuracy (alone, or com-
bined with other measures) to assess the performance of the algorithms. The number of
correctly classified items among all test elements can be seen as a binomially distributed
random variable, and we follow the ACL Wiki state-of-the-art site7 in reporting also
Clopper?Pearson binomial 95% confidence intervals around the accuracies (binomial
intervals and other statistical quantities were computed using the R package;8 where no
further references are given, we used the standard R functions for the relevant analysis).
The binomial confidence intervals give a sense of the spread of plausible population
values around the test-set-based point estimates of accuracy. Where appropriate and
interesting, we compare the accuracy of two specific models statistically with an exact
Fisher test on the contingency table of correct and wrong responses given by the two
7 http://aclweb.org/aclwiki/index.php?title=State Of The Art.
8 http://www.r-project.org/.
691
Computational Linguistics Volume 36, Number 4
models. This approach to significance testing is problematic in many respects, the most
important being that we ignore dependencies in correct and wrong counts due to the
fact that the algorithms are evaluated on the same test set (Dietterich 1998). More
appropriate tests, however, would require access to the fully itemized results from the
compared algorithms, whereas in most cases we only know the point estimate reported
in the earlier literature. For similar reasons, we do not make significance claims regard-
ing other performance measures, such as macro-averaged F. Other forms of statistical
analysis of the results are introduced herein when they are used; they are mostly limited
to the models for which we have full access to the results. Note that we are interested in
whether DM performance is overall within state-of-the-art range, and not on making
precise claims about the models it outperforms. In this respect, we think that our
general results are clear even where they are not supported by statistical inference, or
interpretation of the latter is problematic.
6.1 The W1?LW2 Space
The vectors of this space are labeled with words w1 (rows of matrix Amode-1 in Table 3),
and their dimensions are labeled with binary tuples of type ?l,w2? (columns of the same
matrix). The dimensions represent the attributes of words in terms of lexico-syntactic
relations with lexical collocates, such as ?sbj intr, read?, or ?use, gun?. Consistently, all
the semantic tasks that we address with this space involve the measurement of the
attributional similarity between words.
TheW1?LW2 matrix is a structured semantic space similar to those used by Curran
and Moens (2002), Grefenstette (1994), and Lin (1998a), among others. To test if the
use of links detracts from performance on attributional similarity tasks, we trained on
our concatenated corpus two alternative models?Win and DV?whose features only
include lexical collocates of the target. Win is an unstructured DSM that does not rely on
syntactic structure to select the collocates, but just on their linear proximity to the targets
(Lund and Burgess 1996; Schu?tze 1997; Bullinaria and Levy 2007, and many others). Its
matrix is based on co-occurrences of the same 30K words we used for the other models
within a window of maximally five content words before or after the target. DV is an
implementation of the Dependency Vectors approach of Pado? and Lapata (2007). It is a
structured DSM, but dependency paths are used to pick collocates, without being part of
the attributes. The DV model is obtained from the same co-occurrence data as DepDM
(thus, relying on the dependency paths we picked, not the ones originally selected
by Pado? and Lapata for their tests). Frequencies are summed across dependency path
links for word?link?word triples with the same first and second words. Suppose that
soldier and gun occur in the tuples ?soldier, have, gun? (frequency 3) and ?soldier, use, gun?
(frequency 37). In DepDM, this results in two features for soldier: ?have, gun? and ?use,
gun?. In DV, we would derive a single gun feature with frequency 40. As for the DM
models, theWin and DV counts are converted to LMI weights, and negative LMI values
are raised to 0. Win is a 30,693? 30,693 matrix with about 110 million non-zero entries
(density: 11.5%). DV is a 30,693? 30,693 matrix with about 38 million non-zero values
(density: 4%).
6.1.1 Similarity Judgments. Our first challenge comes from the classic data set of
Rubenstein and Goodenough (1965), consisting of 65 noun pairs rated by 51 subjects
on a 0?4 similarity scale. The average rating for each pair is taken as an estimate of the
perceived similarity between the two words (e.g., car?automobile: 3.9, cord?smile: 0.0).
Following the earlier literature, we use Pearson?s r to evaluate how well the cosines
692
Baroni and Lenci Distributional Memory
in the W1?LW2 space between the nouns in each pair correlate with the ratings. The
results (expressed in terms of percentage correlations) are presented in Table 4, which
also reports state-of-the-art performance levels of corpus-based systems from the litera-
ture (the correlation of all systems with the ratings is very significantly above chance,
according to a two-tailed t-test for Pearson correlation coefficients; df = 63, p < 0.0001
for all systems).
One of the DMmodels, namely TypeDM, does very well on this task, outperformed
only by DoubleCheck, an unstructured system that relies onWeb queries (and thus on a
much larger corpus) and for which we report the best result across parameter settings.
We also report the best results from a range of experiments with different models and
parameter settings from Herdag?delen, Erk, and Baroni (2009) (whose corpus is about
half the size of ours) and Pado? and Lapata (2007) (who use a much smaller corpus). For
the latter, we also report the best result they obtain when using cosine as the similarity
measure (cosDV-07). Overall, the TypeDM result is in line with the state of the art, given
the size of the input corpus, and the fact that we did not perform any tuning. Following
Pado?, Pado?, and Erk (2007) we used the approximate test proposed by Raghunathan
(2003) to compare the correlations with the human ratings of sets of models (this is only
possible for themodels we developed, as the test requires computation of correlation co-
efficients across models). The test suggests that the difference in correlation with human
ratings between TypeDM and our second best model, Win, is significant (Q = 4.55, df =
0.23, p< 0.01). On the other hand, there is no significant difference across Win, DepDM,
DV and LexDM (Q = 1.02, df = 1.80, p = 0.55).
6.1.2 Synonym Detection. The previous experiment assessed how the models can simu-
late quantitative similarity ratings. The classic TOEFL synonym detection task focuses
on the high end of the similarity scale, asking the models to make a discrete decision
about which word is the synonym from a set of candidates. The data set, introduced
to computational linguistics by Landauer and Dumais (1997), consists of 80 multiple-
choice questions, each made of a target word (a noun, verb, adjective, or adverb) and
four candidates. For example, given the target levied, the candidates are imposed, believed,
requested, correlated, the first one being the correct choice. Our algorithms pick the
candidate with the highest cosine to the target item as their guess of the right synonym.
Table 5 reports results (percentage accuracies) on the TOEFL set for our models as
well as the best model of Herdag?delen and Baroni (2009) and the corpus-based models
from the ACL Wiki TOEFL state-of-the-art table (we do not include those models from
the Wiki that resort to other knowledge sources, such as WordNet or a thesaurus). The
claims to follow about the relative performance of the models must be interpreted
cautiously, in light of the spread of the confidence intervals: It suffices to note that,
Table 4
Percentage Pearson correlation with the Rubenstein and Goodenough (1965) similarity ratings.
model r model r model r
DoubleCheck1 85 Win 65 DV 57
TypeDM 82 DV-073 62 LexDM 53
SVD-092 80 DepDM 57 cosDV-073 47
Model sources: 1Chen, Lin, and Wei (2006); 2Herdag?delen, Erk, and Baroni (2009); 3Pado? and
Lapata (2007).
693
Computational Linguistics Volume 36, Number 4
Table 5
Percentage accuracy in TOEFL synonym detection with 95% binomial confidence intervals (CI).
model accuracy 95% CI model accuracy 95% CI
LSA-031 92.50 84.39?97.20 DepDM 75.01 64.06?84.01
GLSA2 86.25 76.73?92.93 LexDM 74.37 63.39?83.49
PPMIC3 85.00 75.26?92.00 PMI-IR-018 73.75 62.72?82.96
CWO4 82.55 72.38?90.09 DV-079 73.00 62.72?82.96
PMI-IR-035 81.25 70.97?89.11 Win 69.37 58.07?79.20
BagPack6 80.00 69.56?88.11 Human10 64.50 53.01?74.88
DV 76.87 66.10?85.57 LSA-9710 64.38 52.90?74.80
TypeDM 76.87 66.10?85.57 Random 25.00 15.99?35.94
PairClass7 76.25 65.42?85.05
Model sources: 1Rapp (2003); 2Matveeva et al (2005); 3Bullinaria and Levy (2007); 4Ruiz-Casado,
Alfonseca, and Castells (2005); 5Terra and Clarke (2003); 6Herdag?delen and Baroni (2009); 7Turney
(2008); 8Turney (2001); 9Pado? and Lapata (2007); 10Landauer and Dumais (1997).
according to a Fisher test, the difference between the second-best model, GLSA, and the
twelfthmodel, PMI-IR-01, is not significant at the? = .05 level (p= 0.07). The difference
between the bottom model, LSA-97, and random guessing is, on the other hand, highly
significant (p < .00001).
The best DM model is again TypeDM, which also outperforms Turney?s unified
PairClass approach (supervised, and relying on a much larger corpus), as well as his
Web-statistics based PMI-IR-01 model. TypeDM does better than the best Pado? and
Lapata model (DV-07), and comparably to our DV implementation. Its accuracy is more
than 10% higher than the average human test taker and the classic LSAmodel (LSA-97).
Among the approaches that outperform TypeDM, BagPack is supervised, and CWO
and PMI-IR-03 rely on much larger corpora. This leaves us with three unsupervised
(and unstructured) models from the literature that outperform TypeDM while being
trained on comparable or smaller corpora: LSA-03, GLSA, and PPMIC. In all three
cases, the authors show that parameter tuning is beneficial in attaining the reported
best performance. Further work should investigate how we could improve TypeDM by
exploring various parameter settings (many of which do not require going back to the
corpus: feature selection and reweighting, SVD, etc.).
6.1.3 Noun Categorization. Humans are able to group words into classes or categories
depending on their meaning similarities. Categorization tasks play a prominent role
in cognitive research on concepts and meaning, as a probe into the semantic organiza-
tion of the lexicon and the ability to arrange concepts hierarchically into taxonomies
(Murphy 2002). Research in corpus-based semantics has always been interested in
investigating whether distributional (attributional) similarity could be used to group
words into semantically coherent categories. From the computational point of view, this
is a particularly crucial issue because it concerns the possibility of using distributional
information to assign a semantic class or type to words. Categorization requires (at least
in current settings) a discrete decision, as in the TOEFL task, but it is based on detecting
not only synonyms but also less strictly related words that stand in a coordinate/co-
hyponym relation. We focus here on noun categorization, which we operationalize as
a clustering task. Distributional categorization has been investigated for other POS as
well, most notably verbs (Merlo and Stevenson 2001; Schulte imWalde 2006). However,
694
Baroni and Lenci Distributional Memory
verb classifications are notoriously more controversial than nominal ones, and deeply
interact with argument structure properties. Some experiments on verb classification
will be carried out in theW1L?W2 space in Section 6.3.
Because the task of clustering concepts/words into superordinates has recently
attracted much attention, we have three relevant data sets from the literature available
for our tests. The Almuhareb?Poesio (AP) set includes 402 concepts from WordNet,
balanced in terms of frequency and ambiguity. The concepts must be clustered into
21 classes, each selected from one of the 21 uniqueWordNet beginners, and represented
by between 13 and 21 nouns. Examples include the vehicle class (helicopter,motorcycle. . . ),
the motivation class (ethics, incitement, . . . ), and the social unit class (platoon, branch). See
Almuhareb (2006) for the full set.
The Battig test set introduced by Baroni et al (2010) is based on the expanded
Battig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004). The
set comprises 83 concepts from 10 common concrete categories (up to 10 concepts per
class), with the concepts selected so that they are rated as highly prototypical of the
class. Class examples include land mammals (dog, elephant. . . ), tools (screwdriver, hammer)
and fruit (orange, plum). See Baroni et al (2010) for the full list.
Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Work-
shop shared tasks (Baroni, Evert, and Lenci 2008). It is also based on concrete nouns,
but it includes fewer prototypical members of categories (rocket as vehicle or snail as
land animal). The 44 target concepts are organized into a hierarchy of classes of in-
creasing abstraction. There are 6 lower level classes, with maximally 13 concepts per
class (birds, land animals, fruit, greens, tools, vehicles). At a middle level, concepts are
grouped into three classes (animals, vegetables, and artifacts). At the most abstract level,
there is a two-way distinction between living beings and objects. See http://wordspace.
collocations.de for the full set.
We cluster the nouns in each set by computing their similarity matrix based on
pairwise cosines, and feeding it to the widely used CLUTO toolkit (Karypis 2003). We
use CLUTO?s built-in repeated bisections with global optimization method, accepting
all of CLUTO?s default values for this method.
Cluster quality is evaluated by percentage purity, one of the standard clustering
quality measures returned by CLUTO (Zhao and Karypis 2003). If nir is the number of
items from the i-th true (gold standard) class that were assigned to the r-th cluster, n the
total number of items, and k the number of clusters, then
Purity = 1n
k
?
r=1
max
i
(nir)
Expressed in words, for each cluster we count the number of items that belong to the
true class that is most represented in the cluster, and then we sum these counts across
clusters. The resulting sum is divided by the total number of items so that, in the best
case (perfect clusters), purity will be 1 (in percentage terms, 100%). As cluster quality de-
teriorates, purity approaches 0. For the models where we have full access to the results,
we use a heuristic bootstrap procedure to obtain confidence intervals around the puri-
ties (Efron and Tibshirani 1994). We resample with replacement 10K data sets (cluster-
assignment+true-label pairs) of the original size. Empirical 95% confidence intervals are
then computed from the distribution of the purities in the bootstrapped data sets (for
the ESSLLI results, we only perform the procedure for 6-way clustering). The confidence
intervals give a rough idea of how stable purity estimates are across small variations of
695
Computational Linguistics Volume 36, Number 4
the items in the data sets. The Random models for this task are baselines assigning
the concepts randomly to the target clusters, with the constraint that each cluster must
contain at least one concept. Random assignment is repeated 10K times, and we obtain
means and confidence intervals from the distribution of these simulations.
Table 6 reports purity results for the three data sets, comparing our models to
those in the literature. Again, the TypeDM model has an excellent performance. On
the ESSLLI 2008 set, it outperforms the best configuration of the best shared task system
among those that did three-level categorization (Katrenko?s), despite the fact that the
latter uses the full Web as a corpus and manually crafted patterns to improve feature
extraction. TypeDM?s performance is equally impressive on the AP set, where it outper-
forms AttrValue-05, the best unsupervised model by the data set proponents, trained
on the full Web. Interestingly, the DepPath model of Rothenha?usler and Schu?tze (2009),
which is the only one outperforming TypeDM on the AP set, is another structured
model with dependency-based link-mediated features, which would fit well into the
Table 6
Purity in noun clustering with bootstrapped 95% confidence intervals (CI).
Almuhareb & Poesio (AP)
model purity 95% CI model purity 95% CI
DepPath1 79 NA DV 65 61?69
TypeDM 76 72?81 DepDM 62 59?67
AttrValue-052 71 NA LexDM 59 56?65
Win 71 67?76 Random 16 14?17
VSM3 70 67?75
Battig
model purity 95% CI model purity 95% CI
Win 96 91?100 DV-104 79 73?89
TypeDM 94 89?99 LexDM 78 72?88
Strudel4 91 85?98 SVD-104 71 67?83
DepDM 90 84?96 AttrValue4 45 44?61
DV 84 79?93 Random 29 24?34
ESSLLI 2008
model 6-way purity 95% CI 3-way purity 2-way purity avg purity
TypeDM 84 77?95 98 100 94.0
Katrenko5 91 NA 100 80 90.3
DV 75 70?89 93 100 89.3
DepDM 75 68?89 93 100 89.3
LexDM 75 70?89 87 100 87.3
Peirsman5 82 NA 84 86 84.0
Win 75 70?89 86 59 73.3
Shaoul5 41 NA 52 55 49.3
Random 38 32?45 49 57 48.0
Model sources: 1Rothenha?usler and Schu?tze (2009); 2Almuhareb and Poesio (2005); 3Herdag?delen,
Erk, and Baroni (2009); 4Baroni et al (2010); 5ESSLLI 2008 shared task.
696
Baroni and Lenci Distributional Memory
DM framework. TypeDM?s purity is extremely high with the Battig set as well, although
here it is outperformed by the unstructured Win model. Our top two performances are
higher than Strudel, the best model by the proponents of the task. The latter was trained
on about half of the data we used, however (moreover, the confidence intervals of these
models largely overlap, suggesting that their difference is not significant).
6.1.4 Selectional Preferences. Our last pair of data sets for the W1?LW2 space illustrate
how the space can be used not only to measure similarity among words, but also to
work with more abstract notions, such as that of a typical filler of an argument slot of a
verb (such as the typical killer and the typical killee). We think that these are especially
important experiments, because they show how the same matrix that has been used for
tasks that were entirely bound to lexical items can also be used to generalize to struc-
tures that go beyond what is directly observed in the corpus. In particular, we model
here selectional preferences (how plausible a noun is as subject/object of a verb), but
our method is generalizable to many other semantic tasks that pertain to composition
constraints; that is, they require measuring the goodness of fit of a word/concept as
argument filler of another word/concept, including assigning semantic roles, logical
metonymy, coercion (Pustejovsky 1995), and many other challenges.
The selectional preference test sets are based on averages of human judgments
on a seven-point scale about the plausibility of nouns as arguments (either subjects
or objects) of verbs. The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus
1998) consists of 100 noun?verb pairs rated by 36 subjects. The Pado? set (Pado? 2007)
has 211 pairs rated by 20 subjects.
For each verb, we first use the W1?LW2 space to select a set of nouns that are
highly associated with the verb via a subject or an object link. In this space, nouns are
represented as vectors with dimensions that are labeled with ?link, word? tuples, where
the word might be a verb, and the link might stand for, among other things, syntactic
relations such as obj (or, in the LexDMmodel, an expansion thereof, such as obj+the-j). To
find nouns that are highly associated with a verb v when linked by the subject relation,
we project theW1?LW2 vectors onto a subspace where all dimensions are mapped to 0
except the dimensions that are labeled with ?lsbj, v?, where lsbj is a link containing either
the string sbj intr or the string sbj tr, and v is the verb. We then measure the length of the
noun vectors in this subspace, and pick the top n longest ones as prototypical subjects
of the verb. The same operation is performed for the object relation. In our experiments,
we set n to 20, but this is of course a parameter that should be explored.
We normalize and sum the vectors (in the fullW1?LW2 space) of the picked nouns,
to obtain a centroid that represents an abstract ?subject prototype? for the verb (and
analogously for objects). The plausibility of an arbitrary noun as the subject (object) of a
verb is then measured by the cosine of the noun vector to the subject (object) centroid in
W1?LW2 space. Crucially, the algorithm can provide plausibility scores for nouns that
do not co-occur with the target verb in the corpus, by looking at how close they are
to the centroid of nouns that do often co-occur with the verb. The corpus may contain
neither eat topinambur nor eat sympathy, but the topinambur vector will likely be closer to
the prototypical eat object vector than the one of sympathy would be.
It is worth stressing that the whole process relies on a single W1?LW2 matrix: This
space is first used to identify typical subjects (or objects) of a verb via subspacing, then to
construct centroid vectors for the verb subject (object) prototypes, and finally tomeasure
the distance of nouns to these centroids. Our method is essentially the same, save for
implementation and parameter choice details, as the one proposed by Pado?, Pado?,
and Erk (2007), in turn inspired by Erk (2007). However, they treat the identification
697
Computational Linguistics Volume 36, Number 4
of typical argument fillers of a verb as an operation to be carried out using different
resources, whereas we reinterpret it as a different way to use the sameW1?LW2 space in
which we measure plausibility.
Following Pado? and colleagues, we measure performance by the Spearman ? corre-
lation coefficient between the average human ratings and the model predictions, con-
sidering only verb?noun pairs that are present in the model. Table 7 reports percentage
coverage and correlations for the DM models (the task requires the links to extract
typical subjects and objects, so we cannot use DV nor Win), results from Pado?, Pado?,
and Erk (2007) (ParCos is the best among their purely corpus-based systems), and the
performance on the Pado? data set of the supervised system of Herdag?delen and Baroni
(2009). Testing for significance of the correlation coefficients with two-tailed tests based
on a Spearman-coefficient derived t statistic, we find that the Resnik?s model correlation
for the McRae data is not significantly different from 0 (t = 0.29, df = 92, p = 0.39),
ParCos on McRae is significant at ? = .05 (t = 2.134, df = 89, p = 0.018), and all other
models on either data set are significant at ? = .01 and below.
TypeDM emerges as an excellent model to tackle selectional preferences, and as the
overall winner on this task. On the Pado? data set, it is as good as Pado??s (2007) FrameNet
based model, and it is outperformed only by the supervised BagPack approach. On the
McRae data set, all three DM models do very well, and TypeDM is slightly worse than
the other two models. On this data set, the DM models are outperformed by Pado??s
FrameNet model in terms of correlation, but the latter has a much lower coverage,
suggesting that for practical purposes the DM models are a better choice. According to
Raghunathan?s test (see Section 6.1.1), the difference in correlation with human ratings
among the three DM models is not significant on the McRae data, where TypeDM is
below the other models (Q = 0.19, df = 0.67, p = 0.50). On the Pado? data set, on the
other hand, where TypeDM outperforms the other DM models, the same difference is
highly significant (Q = 12.70, df = 1.00, p < 0.001).
As a final remark on the W1?LW2 space, we can notice that DM models perform
very well in tasks involving attributional similarity. The performance of unstructured
DSMs (including Win, our own implementation of this type of model) is also high,
sometimes even better than that of structured DSMs. However, our best DMmodel also
achieves brilliant results in capturing selectional preferences, a task that is not directly
addressable by unstructured DSMs. This fact suggests that the real advantage provided
by structured DSMs?particularly when linguistic structure is suitably exploited, as
Table 7
Correlation with verb?argument plausibility judgments.
McRae Pado?
model coverage ? model coverage ?
Pado?1 56 41 BagPack2 100 60
DepDM 97 32 TypeDM 100 51
LexDM 97 29 Pado?1 97 51
TypeDM 97 28 ParCos1 98 48
ParCos1 91 22 DepDM 100 35
Resnik1 94 3 LexDM 100 34
Resnik1 98 24
Model sources: 1Pado?, Pado?, and Erk (2007); 2Herdag?delen and Baroni (2009).
698
Baroni and Lenci Distributional Memory
with the DM third-order tensor?actually resides in their versatility in addressing a
much larger and various range of semantic tasks. This preliminary conclusion will also
be confirmed by the experiments modeled with the other DM spaces.
6.2 The W1W2?L Space
The vectors of this space are labeled with word pair tuples ?w1,w2? (columns of matrix
Bmode-2 in Table 3) and their dimensions are labeled with links l (rows of the same
matrix). This arrangement of our tensor reproduces the ?relational similarity? space of
Turney (2006b), also implicitly assumed in much relation extraction work, where word
pairs are compared based on the patterns that link them in the corpus, in order to mea-
sure the similarity of their relations (Pantel and Pennacchiotti 2006). The links that in
W1?LW2 space provide a form of shallow typing of lexical features (?use, gun?) associ-
ated with single words (soldier) constitute under theW1W2?L view full features (use) as-
sociated with word pairs (?soldier, gun?). Besides exploiting this view of the tensor to
solve classic relational tasks, we will also show how problems that have not been tradi-
tionally defined in terms of a word-pair-by-link matrix, such as qualia harvesting with
patterns or generating lists of characteristic properties, can be elegantly recast in the
W1W2?L space by measuring the length of ?w1,w2? vectors in a link (sub)space, thus
bringing a wider range of semantic operations under the umbrella of the natural DM
spaces.
TheW1W2?L space represents pairs of words that co-occur in the corpus within the
maximum span determined by the scope of the links connecting them (for our models,
this maximum span is never larger than a single sentence). When words do not co-occur
or only co-occur very rarely (and even in large corpora this will often be the case), attri-
butional similarity can come to the rescue. Given a target pair, we can construct other,
probably similar pairs by replacing one of the words with an attributional neighbor. For
example, given the pair ?automobile, wheel?, we might discover inW1?LW2 space that car
is a close neighbor of automobile. We can then look for the pair ?car, wheel?, and use rela-
tional evidence about this pair as if it pertained to ?automobile, wheel?. This is essentially
the way to deal withW1W2?L data sparseness proposed by Turney (2006b), except that
he relies on independently harvested attributional and relational spaces, whereas we
derive both from the same tensor. More precisely, in theW1W2?L tasks where we know
the set of target pairs in advance (Sections 6.2.1 and 6.2.2), we smooth the DMmodels by
combining in turn one of the words of each target pair with the top 20 nearestW1?LW2
neighbors of the other word, obtaining a total of 41 pairs (including the original). The
centroid of the W1W2?L vectors of these pairs is then taken to represent a target pair
(the smoothed ?automobile, wheel? vector is an average of the ?automobile, wheel?, ?car,
wheel?, ?automobile, circle?, etc., vectors). The nearest neighbors are efficiently searched
in the W1?LW2 matrix by compressing it to 5,000 dimensions via random indexing,
using the parameters suggested by Sahlgren (2005). Smoothing consistently improved
performance, and we only report the relevant results for the smoothed versions of the
models (including our implementation of LRA, to be discussed next).
We reimplemented Turney?s Latent Relational Analysis (LRA) model, training it on
our source corpus (LRA is trained separately for each test set, because it relies on a given
list of word pairs to find the patterns that link them). We chose the parameter values of
Turney?s main model (his ?baseline LRA system?). In short (see Turney?s article for de-
tails), for a given set of target pairs we count all the patterns that connect them, in either
order, in the corpus. Patterns are sequences of one to three words occurring between the
targets, with all, none, or any subset of the elements replaced bywildcards (with the,with
699
Computational Linguistics Volume 36, Number 4
Table 8
Percentage accuracy in solving SAT analogies with 95% binomial confidence intervals (CI).
model accuracy 95% CI model accuracy 95% CI
Human1 57.0 52.0?62.3 TypeDM 42.4 37.4?47.7
LRA-062 56.1 51.0?61.2 LSA7 42.0 37.2?47.4
PERT3 53.3 48.5?58.9 LRA 37.8 32.8?42.8
PairClass4 52.1 46.9?57.3 PMI-IR-062 35.0 30.2?40.1
VSM1 47.1 42.2?52.5 DepDM 31.4 26.6?36.2
BagPack5 44.1 39.0?49.3 LexDM 29.3 24.8?34.3
k-means6 44.0 39.0?49.3 Random 20.0 16.1?24.5
Model sources: 1Turney and Littman (2005); 2Turney (2006b); 3Turney (2006a); 4Turney (2008);
5Herdag?delen and Baroni (2009); 6Bicic?i and Yuret (2006); 7Quesada, Mangalath, and Kintsch
(2004).
*, * the, * *). Only the top 4,000 most frequent patterns are preserved, and a target-pair-
by-pattern matrix is constructed (with 8,000 dimensions, to account for directionality).
Values in the matrix are log- and entropy-transformed using Turney?s formula. Finally,
SVD is applied, reducing the columns to the top 300 latent dimensions (here and sub-
sequently, we use SVDLIBC9 to perform SVD). For simplicity and to make LRA more
directly comparable to the DM models, we applied our attributional-neighbor-based
smoothing technique (the neighbors for target pair expansion are taken from the best
attributional DM model, namely, TypeDM) instead of the more sophisticated one used
by Turney. Thus, our LRA implementation differs from Turney?s original in two aspects:
the smoothing method and the source corpus (Turney uses a corpus of more than
50 billion words). Neither variation pertains to inherent differences between LRA and
DM.Given the appropriate resources, a DMmodel could be trained on Turney?s gigantic
corpus, and smoothed with his technique.
6.2.1 Solving Analogy Problems. The SAT test set introduced by Turney and collaborators
contains 374 multiple-choice questions from the SAT college entrance exam. Each ques-
tion includes one target (ostrich?bird) and five candidate analogies (lion?cat, goose?flock,
ewe?sheep, cub?bear, primate?monkey). The data set is dominated by noun?noun pairs,
but all other combinations are also attested (noun?verb, verb?adjective, verb?verb, etc.)
The task is to choose the candidate pair most analogous to the target (lion?cat in the
previous example). This is essentially the same task as the TOEFL, but applied to word
pairs instead of words. As in the TOEFL, we pick the candidate with the highest cosine
with the target as the right analogy.
Table 8 reports our SAT results together with those of other corpus-based methods
from the ACL Wiki and other systems. TypeDM is again emerging as the best among
our models. To put its performance in context statistically, according to a Fisher test its
accuracy is not significantly different from that of VSM (p = 0.239), whereas it is better
than that of PMI-IR-06 (p= 0.043; even the bottommodel, LexDM, is significantly better
than the random guesser, p = 0.004).
TypeDM is at least as good as LRA when the latter is trained on the same data
and smoothed with our method, suggesting that the excellent performance of Turney?s
version of LRA (LRA-06) is due to the fact that he used a much larger corpus, and/or to
9 http://tedlab.mit.edu/?dr/SVDLIBC/.
700
Baroni and Lenci Distributional Memory
his more sophisticated smoothing technique, and not to the specific way in which LRA
collects corpus-based statistics. All the algorithms with higher accuracy than TypeDM
are based onmuch larger input corpora, except BagPack, which is, however, supervised.
The LSA system of Quesada, Mangalath, and Kintsch (2004), which performs similarly
to TypeDM, is based on a smaller corpus, but it relies on hand-coded ?analogy domains?
that are represented by lists of manually selected characteristic words.
6.2.2 Relation Classification. Just as the SAT is the relational equivalent of the TOEFL task,
the test sets we tackle next are a relational analog to attributional concept clustering,
in that they require grouping pairs of words into classes that instantiate the same
relations. Whereas we cast attributional categorization as an unsupervised clustering
problem (following much of the earlier literature), the common approach to classify-
ing word pairs by relation is supervised, and relies on labeled examples for training. In
this article, we exploit training data in a very simple way, via a nearest centroid method.
In the SEMEVAL task we are about to introduce, where both positive and negative
examples are available for each class, we use the positive examples to construct a
centroid that represents a target class, and negative examples to construct a centroid rep-
resenting items outside the class. We then decide if a test pair belongs to the target class
by measuring its distance from the positive and negative centroids, picking the nearest
one. For example, the Cause?Effect relation has positive training examples such as
cycling?happiness and massage?relief and negative examples such as customer?satisfaction
and exposure?protection. We create a positive centroid by summing theW1W2?L vectors
of the first set of pairs, and a negative centroid by summing the latter. We then mea-
sure the cosine of a test item such as smile?wrinkle with the centroids, and decide
if it instantiates the Cause?Effect relation based on whether it is closer to the positive
or negative centroid. For the other tasks (as well as the transitive alternation task of
Section 6.3), we do not have negative examples, but positive examples for different
classes. We create a centroid for each class, and classify test items based on the centroid
they are nearest to.
Our first test pertains to the seven relations between nominals in Task 4 of
SEMEVAL 2007 (Girju et al 2007): Cause?Effect, Instrument?Agency, Product?
Producer, Origin?Entity, Theme?Tool, Part?Whole, Content?Container. For each rela-
tion, the data set includes 140 training and about 80 test items. Each item consists of a
Web snippet, containing word pairs connected by a certain pattern (e.g., ?* causes *?).
The retrieved snippets are manually classified by the SEMEVAL organizers as positive
or negative instances of a certain relation (see the earlier Cause?Effect examples). About
50% training and test cases are positive instances. In our experiments we do not make
use of the contexts of the target word pairs that are provided with the test set.
The second data set (NS) comes from Nastase and Szpakowicz (2003). It pertains to
the classification of 600 modifier?noun pairs and it is of interest because it proposes a
very fine-grained categorization into 30 semantic classes, such as Cause (cloud?storm),
Purpose (album?picture), Location-At (pain?chest), Location-From (visitor?country), Fre-
quency (superstition?occasional), Time-At (snack?midnight), and so on. The modifiers can
be nouns, adjectives, or adverbs. Because the data set is not split into training and test
data we follow Turney (2006b) and perform leave-one-out cross-validation. The data set
also comes with a coarser five-way classification. Our unreported results on it are com-
parable, in terms of relative performance, to the ones for the 30-way classification.
The last data set (OC) contains 1,443 noun?noun compounds classified by O?
Se?aghdha and Copestake (2009) into 6 relations: Be (celebrity?winner), Have (door?latch),
In (air?disaster), Actor (university?scholarship), Instrument ( freight?train), and About
701
Computational Linguistics Volume 36, Number 4
(bank?panic); see O? Se?aghdha and Copestake (2009) and references there. We use the
same five-way cross-validation splits as the data set proponents.
Table 9 reports performance of models from our experiments and from the literature
on the three supervised relation classification tasks. Following the relevant earlier stud-
ies, for SEMEVAL we report macro-averaged accuracy, whereas for the other two data
sets we report global accuracy (with binomial confidence intervals). All other measures
are macro-averaged. Majority is the performance of a classifier that always guesses the
Table 9
Relation classification performance; all measures macro-averaged, except accuracy in the NS and
OC data sets, where we also report the accuracy 95% confidence intervals (CI).
SEMEVAL 2007
model prec recall F acc model prec recall F acc
TypeDM 71.7 62.5 66.4 70.2 DepDM 61.0 57.3 58.9 61.8
UCD-FC1 66.1 66.7 64.8 66.0 UTH1 56.1 57.1 55.9 58.8
UCB1 62.7 63.0 62.7 65.4 Majority 81.3 42.9 30.8 57.0
LexDM 64.7 61.3 62.5 65.4 ProbMatch 48.5 48.5 48.5 51.7
ILK1 60.5 69.5 63.8 63.5 UC3M1 48.2 40.3 43.1 49.9
LRA 63.7 60.0 61.0 63.1 AllTrue 48.5 100.0 64.8 48.5
UMELB-B1 61.5 55.7 57.8 62.7
Nastase & Szpakowicz (NS)
model prec recall F acc acc 95% CI
LRA-062 41.0 35.9 36.6 39.8 35.9?43.9
VSM-AV3 27.9 26.8 26.5 27.8 24.3?31.6
LRA 23.0 23.1 21.1 25.5 22.1?29.2
VSM-WMTS2 24.0 20.9 20.3 24.7 21.3?28.3
TypeDM 19.5 20.2 13.7 15.4 12.5?18-5
LexDM 7.5 14.1 8.1 12.1 9.7?15.0
DepDM 11.6 14.5 8.1 8.7 6.5?11.2
Majority 0.3 3.3 0.5 8.2 6.1?10.6
ProbMatch 3.3 3.3 3.3 4.7 3.1?6.7
AllTrue 3.3 100 6.4 NA NA
O? Se?aghdha & Copestake (OC)
model prec recall F acc acc 95% CI
OC-Comb4 NA NA 61.6 63.1 60.6?65.6
OC-Rel4 NA NA 49.9 52.1 49.5?54.7
TypeDM 33.8 33.5 31.4 32.1 29.7?34.6
LRA 31.5 30.8 30.7 31.3 28.9?33.8
LexDM 29.9 28.9 28.7 29.7 27.4?32.2
DepDM 28.2 28.2 27.0 27.6 25.3?30.0
Majority 3.6 16.7 5.9 21.3 19.2?23.5
ProbMatch 16.7 16.7 16.7 17.1 15.2?19.2
AllTrue 16.7 100 28.5 NA NA
Model sources: 1SEMEVAL Task 4; 2Turney (2006b); 3Turney and Littman (2005); 4O? Se?aghdha
and Copestake (2009).
702
Baroni and Lenci Distributional Memory
majority class in the test set (in SEMEVAL, for each class, it guesses that all or no items
belong to it depending on whether there are more positive or negative examples in the
test data; in the other tasks, it labels all items with the majority class). AllTrue always
assigns an item to the target class (being inherently binary, it does not provide a well-
defined multi-class global accuracy). ProbMatch randomly guesses classes matching
their distribution in the test data (in SEMEVAL, it matches the proportion of positive
and negative examples within each class).
For SEMEVAL, the table reports the results of those models that took part in the
shared task and, like ours, did not use the organizer-provided WordNet sense labels
nor information about the query used to retrieve the examples. All these models are
outperformed by TypeDM, despite the fact that they exploit the training contexts
and/or specific additional resources: an annotated compound database (UCD-FC),
more sophisticated machine learning algorithms to train the relation classifiers (ILK,
UCD-FC), Web counts (UCB), and so on.
For the NS data set, none of the DM models do well, although TypeDM is once
more the best among them. The DM models are outperformed by other models from
the literature, all trained on much larger corpora, and also by our implementation of
LRA. The difference in global accuracy between LRA and TypeDM is significant (Fisher
test, p = 0.00002). TypeDM?s accuracy is nevertheless well above the best (Majority)
baseline accuracy (p = 0.0001).
The OC results confirm that TypeDM is the best of our models, again (slightly)
outperforming our LRA implementation. Still, our best performance is well below
that of OC-Comb, the absolute best, and OC-Rel, the best purely relational model
of O? Se?aghdha and Copestake (2009) (the difference in global accuracy between the
latter and TypeDM is highly significant, p < 0.000001). O? Se?aghdha and Copestake use
sophisticated kernel-based methods and extensive parameter tuning to achieve these
results. We hope that the TypeDM performance would also improve by improving the
machine learning aspects of the procedure.
As an ad interim summary, we observe that TypeDM achieves competitive results in
semantic tasks involving relational similarity. In particular, in both analogy solving and
two out of three relation classification experiments, TypeDM is at least as good as our
LRA implementation. We now move on to show how this same view of the DM tensor
can be successfully applied to aspects of meaning that are not normally addressed by
relational DSMs.
6.2.3 Qualia Extraction. A popular alternative to the supervised approach to relation
extraction is to pick a set of lexico-syntactic patterns that should capture the relation
of interest and to harvest pairs they connect in text, as famously illustrated by Hearst
(1992) for the hyponymy relation. In the DM approach, instead of going back to the
corpus to harvest the patterns, we exploit the information already available in the
W1W2?L space. We select promising links as our equivalent of patterns and we measure
the length of word pair vectors in the W1W2?L subspace defined by these links. We
illustrate this with the data set of Cimiano andWenderoth (2007), which contains qualia
structures (Pustejovsky 1995) for 30 nominal concepts, both concrete (door) and abstract
(imagination). Cimiano and Wenderoth asked 30 subjects to produce qualia for these
words (each word was rated by at least three subjects), obtaining a total of 1,487 word?
quale pairs, instantiating the four roles postulated by Pustejovsky: Formal (the category
of the object: door?barrier), Constitutive (constitutive parts, materials the object is made
of: food?fat), Agentive (what brings the object about: letter?write), and Telic (the func-
tion of the object: novel?entertain).
703
Computational Linguistics Volume 36, Number 4
We approximate the patterns proposed by Cimiano and Wenderoth by manually
selecting links that are already in our DM models, as reported in Table 10 (here and
subsequently when discussing qualia-harvesting links, we use n and q to indicate the
linear position of the noun and the potential quale with respect to the link). All qualia
roles have links pertaining to noun?noun pairs. The Agentive and Telic patterns also
harvest noun?verb pairs. For LexDM, we pick all links that begin with one of the strings
in Table 10. For the DepDM model, the only attested links are n with q (Constitutive),
n sbj intr q, n sbj tr q (Telic), and q obj n (Agentive). Consequently, DepDM does not
harvest Formal qualia, and is penalized accordingly in the evaluation.
We project all W1W2?L vectors that contain a target noun onto each of the four
subspaces determined by the quale-specific link sets, and we compute their subspace
lengths. Given a target noun n and a potential quale q, the length of the ?n, q? vector
in the subspace characterized by the links that represent role r is our measure of how
good q is as a quale of type r for n (for example, the length of ?book, read? in the subspace
defined by the Telic links is our measure of fitness of read as Telic role of book). We use
length in the subspace associated to the qualia role r to rank all ?n, q? pairs relevant to r.
Following Cimiano and Wenderoth?s evaluation method, for each noun we first
compute, separately for each role, the ranked list precision (with respect to themanually
constructed qualia structure) at 11 equally spaced recall levels from 0% to 100%. We
select the precision, recall, and F values at the recall level that results in the highest
F score (i.e., in the best precision?recall trade-off). We then average across the roles, and
then across target nouns. The task, as framed here, cannot be run with the LRA model,
and, because of its open-ended nature (we do not start from a predefined list of pairs),
we do not smooth the models.
Table 11 reports the performance of our models, as well as the F scores reported by
Cimiano and Wenderoth. For our models, where we have access to the itemized data,
we also report the standard deviation of F across the target nouns.
All the DM models perform well (including DepDM, which is disfavored by the
lack of Formal links), and once more TypeDM emerges as the best among them, with
an F value that is also (slightly) above the best Cimiano and Wenderoth models (that
are based on co-occurrence counts from the whole Web). Despite the large standard
deviations, the difference in F across concepts between TypeDM and the second-best
DM model (DepDM) is highly significant (paired t-test, t = 4.02, df = 29, p < 0.001),
suggesting that the large variance is due to different degrees of difficulty of the concepts,
affecting the models in similar ways.
Table 10
Links approximating the patterns proposed in Cimiano and Wenderoth (2007).
FORMAL CONSTITUTIVE
n as-form-of q, q as-form-of n q as-member-of n, q as-part-of n, nwith q
n as-kind-of q, n as-sort-of q, n be q nwith-lot-of q, nwith-majority-of q
q such as n nwith-number-of q, nwith-sort-of q
nwith-variety-of q
AGENTIVE TELIC
n as-result-of q, q obj n n for-use-as q, n for-use-in q, n sbj tr q
n sbj intr q
704
Baroni and Lenci Distributional Memory
Table 11
Average qualia extraction performance.
model precision recall F F s.d.
TypeDM 26.2 22.7 18.4 8.7
P1 NA NA 17.1 NA
WebP1 NA NA 16.7 NA
LexDM 19.9 23.6 16.2 7.1
WebJac1 NA NA 15.2 NA
DepDM 17.8 16.9 12.8 6.4
Verb-PMI1 NA NA 10.7 NA
Base1 NA NA 7.6 NA
Model source: 1Cimiano and Wenderoth (2007).
6.2.4 Predicting Characteristic Properties. Recently, there has been some interest in the
automated generation of commonsense concept descriptions in terms of intuitively
salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb
2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al 2010). Similar
property lists, collected from subjects in elicitation tasks, are widely used in cognitive
science as surrogates of mental features (Garrard et al 2001; McRae et al 2005; Vinson
and Vigliocco 2008). Large-scale collections of property-based concept descriptions are
also carried out in AI, where they are important for commonsense reasoning (Liu and
Singh 2004).
In the qualia task, given a concept we had to extract properties of certain kinds (cor-
responding to the qualia roles). The property-based description task is less constrained,
because the most salient relations of a nominal concept might be in all sorts of relations
with it (parts, typical behaviors, location, etc.). Still, we couch the task of unconstrained
property extraction as a challenge in theW1W2?L space. The approach is similar to the
method adopted for qualia roles, but now the wholeW1W2?L space is used, instead of
selected subspaces. Given all the ?n,w2? pairs that have the target nominal concept as
first element, we rank them by length in theW1W2?L space. The longest ?n,w2? vectors
in this space should correspond to salient properties of the target concept, as we expect
a concept to often co-occur in texts with its important properties (because in the current
DM implementations links are disjoint across POS, we map properties with different
POS onto the same scale by dividing the length of the vector representing a pair by the
length of the longest vector in the harvested concept?property set that has the same POS
pair). For example, among the longestW1W2?L vectors with car as first itemwe find ?car,
drive?, ?car, park?, and ?car, engine?. The first two pairs are normalized by dividing by the
longest ?noun, verb? vector in the harvested set, the third by dividing by the longest
?noun, noun? vector.
We test this approach in the ESSLLI 2008 Distributional Semantic Workshop un-
constrained property generation challenge (Baroni, Evert, and Lenci 2008). The data set
contains, for each of 44 concrete concepts, 10 properties that are those that were most
frequently produced by subjects in the elicitation experiment of McRae et al (2005) (the
?gold standard lists?). Algorithms must generate lists of 10 properties per concept, and
performance is measured by overlap with the subject-produced properties, that is, by
the cross-concept average proportions of properties in the generated lists that are also
in the corresponding gold standard lists. Smoothing would be very costly (we would
need to smooth all pairs that contain a target concept) and probably counterproductive
705
Computational Linguistics Volume 36, Number 4
(as the most typical properties of a concept should be highly specific to it, rather than
shared with neighbors). Because LRA (at least in a reasonably efficient implementation)
requires a priori specification of the target pairs, it is not well suited to this task.
Table 12 reports the percentage overlap with the gold standard properties (averaged
across the 44 concepts) for our models as well as the only ESSLLI 2008 participant that
tried this task, and for the models of Baroni et al (2010). TypeDM is the best DMmodel,
and it also does quite well compared to the state of the art. The difference between
Strudel, the best model from the earlier literature, and TypeDM is not statistically signif-
icant, according to a paired t-test across the target concepts (t = 1.1, df = 43, p = 0.27).
The difference between TypeDM and DV-10, the second best model from the literature,
is highly significant (t = 2.9, df = 43, p < 0.01). If we consider how difficult this sort
of open-ended task is (see the very low performance of the respectable models at the
bottom of the list), matching on average two out of ten speaker-generated properties, as
TypeDM does, is an impressive feat.
6.3 The W1L?W2 Space
The vectors of this space are labeled with binary tuples of type ?w1, l? (columns of
matrix Cmode-3 in Table 3), and their dimensions are labeled with words w2 (rows of the
samematrix). We illustrate this space in the task of discriminating verbs participating in
different argument alternations. However, other uses of the space can also be foreseen.
For example, the rows of W1L?W2 correspond to the columns of the W1?LW2 space
(given the constraints on the tuple structure we adopted in Section 3.1). We could use
the former space for feature smoothing or selection in the latter space, for example, by
merging the features ofW1?LW2 whose corresponding vectors inW1L?W2 have a cosine
similarity over a given threshold. We leave this possibility to further work.
Among the linguistic objects represented by the W1L?W2 vectors, we find the
syntactic slots of verb frames. For instance, the vector labeled with the tuple ?read,
sbj?1? represents the subject slot of the verb read in terms of the distribution of its
noun fillers, which label the dimensions of the space. We can use theW1L?W2 space to
explore the semantic properties of syntactic frames, and to extract generalizations about
the inner structure of lexico-semantic representations of the sort formal semanticists
have traditionally been interested in. For instance, the high similarity between the
object slot of kill and the subject slot of die might provide a distributional correlate to
the classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others.
Measuring the cosine between the vectors of different syntactic slots of the same
verb corresponds to estimating the amount of fillers they share. Measures of ?slot
overlap? have been used by Joanis, Stevenson, and James (2008) as features to classify
verbs on the basis of their argument alternations. Levin and Rappaport-Hovav (2005)
Table 12
Average percentage overlap with subject-generated properties and standard deviation.
model overlap s.d. model overlap s.d. model overlap s.d.
Strudel1 23.9 11.3 LexDM 14.5 12.1 SVD-101 4.1 6.1
TypeDM 19.5 12.4 DV-101 14.1 10.3 Shaoul2 1.8 3.9
DepDM 16.1 12.6 AttrValue1 8.8 9.9
Model sources: 1Baroni et al (2010); 2ESSLLI 2008 shared task.
706
Baroni and Lenci Distributional Memory
define argument alternations as the possibility for verbs to have multiple syntactic
realizations of their semantic argument structure. Alternations involve the expression
of the same semantic argument in two different syntactic slots. We expect that, if a
verb undergoes a particular alternation, then the set of nouns that appear in the two
alternating slots should overlap to a certain degree.
Argument alternations represent a key aspect of the complex constraints that shape
the syntax?semantics interface. Verbs differ with respect to the possible alternations
they can undergo, and this variation is strongly dependent on their semantic proper-
ties (semantic roles, event type, etc.). Levin (1993) has in fact proposed a well-known
classification of verbs based on their range of syntactic alternations. Recognizing the
alternations licensed by a verb is extremely important in capturing its argument struc-
ture properties, and consequently in describing its semantic behavior. We focus here on
a particular class of alternations, namely transitivity alternations, whose verbs allow
both for a transitive NP V NP variant and for an intransitive NP V (PP) variant (Levin
1993). We use the W1L?W2 space to carry out the automatic classification of verbs that
participate in different types of transitivity alternations.
In the causative/inchoative alternation, the object argument (e.g., John broke the vase)
can also be realized as an intransitive subject (e.g., The vase broke). In a first experiment,
we use the W1L?W2 space to discriminate between transitive verbs undergoing the
causative/inchoative alternation (C/I) (e.g., break) and non-alternating ones (e.g.,mince;
cf. John minced the meat vs. *The meat minced). The C/I data set was introduced by
Baroni and Lenci (2009), but not tested in a classification task there. It consists of 232
causative/inchoative verbs and 170 non-alternating transitive verbs from Levin (1993).
In a second experiment, we apply the W1L?W2 space to discriminate verbs that
belong to three different classes, each corresponding to a different type of transitive
alternation. We use the MS data set (Merlo and Stevenson 2001), which includes 19 un-
ergative verbs undergoing the induced action alternation (e.g., race), 19 unaccusative
verbs that undergo the causative/inchoative alternation (e.g., break), and 20 object-drop
verbs participating in the unexpressed object alternation (e.g., play). See Levin (1993)
for details about each of these transitive alternations. The complexity of this task is
due to the fact that the verbs in the three classes have both transitive and intransitive
variants, but with very different semantic roles. For instance, the transitive subject of
unaccusative (The man broke the vase) and unergative verbs (The jockey raced the horse past
the barn) is an agent of causation, whereas the subject of the intransitive variant of un-
accusative verbs has a theme role (i.e., undergoes a change of state: The vase broke), and
the intransitive subject of unergative verbs has instead an agent role (The horse raced past
the barn). Thus, their surface identity notwithstanding, the semantic properties of the
syntactic slots of the verbs in each class are very different. By testing theW1L?W2 space
on such a task we can therefore evaluate its ability to capture non-trivial properties of
the verb?s thematic structure.
We address these tasks by measuring the similarities between theW1L?W2 vectors
of the transitive subject, intransitive subject, and direct object slots of a verb, and using
these inter-slot similarities to classify the verb. For instance, given the definition of
the C/I alternation, we can predict that with alternating verbs the intransitive subject
slot should be similar to the direct object slot (the things that are broken also break),
while this should not hold for non-alternating verbs (mincees are very different from
mincers). For each verb v in a data set, we extract the corresponding W1L?W2 slot
vectors ?v, l? whose links are sbj intr, sbj tr, and obj (for LexDM, we sum the vectors
with links beginning with one of these three patterns). Then, for each v we build a
three-dimensional vector with the cosines between the three slot vectors. These second
707
Computational Linguistics Volume 36, Number 4
order vectors encode the profile of similarity across the slots of a verb, and can be used to
spot verbs that have comparable profiles (e.g., verbs that have a high similarity between
their subj intr and obj slots).
We model both experiments as classification tasks using the nearest centroid
method on the three-dimensional vectors, with leave-one-out cross-validation. We per-
form binary classification of the C/I data set (treating non-alternating verbs as negative
examples), and three-way classification of the MS data. Table 13 reports the results, with
the baselines computed similarly to the ones in Section 6.2.2 (for C/I, Majority is equiv-
alent to AllTrue). The DM performance is also compared with the results of Merlo and
Stevenson (2001) for their classifiers tested with the leave-one-out methodology (macro-
averaged F has been computed on the class-by-class scores reported in that article).
All the DMmodels discriminate the verb classes much more reliably than the base-
lines. The accuracy of DepDM, the worst DM model, is significantly higher than that of
the best baselines, AllTrue in C/I (Fisher test, p= 0.024) andMajority onMS (p= 0.039).
TypeDM is again our best model. Its performance is comparable to the lower range
of the Merlo and Stevenson classifiers (considering the large confidence intervals due to
the small sample size, the accuracy of TypeDM is not significantly below even that of the
top model NoPass; p = 0.43). The TypeDM results were obtained simply by measuring
the verb inter-slot similarities in theW1L?W2 space. Conversely, the classifiers in Merlo
and Stevenson (2001) rely on a much larger range of knowledge-intensive features
selected in an ad hoc fashion for this task (on the other hand, their training corpus
Table 13
Verb classification performance (precision, recall, and F for MS are macro-averaged). Global
accuracy supplemented by 95% binomial confidence intervals (CI).
Causative/Inchoative (C/I)
model prec recall F acc acc 95% CI
LexDM 76.0 69.9 72.8 69.9 65.2?74.3
TypeDM 75.7 68.5 71.9 69.1 64.4?73.6
DepDM 72.8 64.6 68.4 65.7 60.8?70.3
AllTrue 57.7 100 73.2 57.7 52.7?62.6
ProbMatch 57.7 57.7 57.7 51.2 46.2?56.2
Merlo & Stevenson (MS)
model prec recall F acc acc 95% CI
NoPass1 NA NA 71.2 71.2 57.3?81.9
AllFeatures1 NA NA 69.1 69.5 55.5?80.5
NoTrans1 NA NA 63.8 64.4 50.1?76.0
NoCaus1 NA NA 62.6 62.7 48.4?74.5
TypeDM 60.7 61.7 60.8 61.5 47.5?73.7
NoVBN1 NA NA 61.0 61.0 46.6?73.0
NoAnim1 NA NA 59.9 61.0 46.6?73.0
LexDM 55.3 56.7 55.8 56.4 43.2?69.8
DepDM 52.9 55.0 53.2 54.7 41.5?68.3
Majority 11.3 33.3 16.9 33.9 22.5?48.1
ProbMatch 33.3 33.3 33.3 33.3 21.0?46.3
AllTrue 33.3 100 50.0 NA NA
Model source: 1Merlo and Stevenson (2001).
708
Baroni and Lenci Distributional Memory
is not parsed and it is much smaller than ours). Finally, we can notice that in both
experiments the mildly (TypeDM) and heavily (LexDM) lexicalized DM models score
better than their non-lexicalized counterpart (DepDM), although the difference between
the best DM model and DepDM is not significant on either data set (p = 0.23 for the
LexDM/DepDMdifference in C/I; p= 0.57 for the TypeDM/DepDMdifference inMS).
Verb alternations do not typically appear among the standard tasks on which DSMs
are tested. Moreover, they involve non-trivial properties of argument structure. The
good performance of DM in these experiments is therefore particularly significant in
supporting its vocation as a general model for distributional semantics.
6.4 The L?W1W2 Space
The vectors of this space are labeled with links l (rows of matrix Bmode-2 in Table 3)
and their dimensions are labeled with word pair tuples ?w1,w2? (columns of the same
matrix). Links are represented in terms of the word pairs they connect. The L?W1W2
space supports tasks where we are directly interested in the links as an object of
study?for example, characterizing prepositions (Baldwin, Kordoni, and Villavicencio
2009) or measuring the relative similarity of different kinds of verb?noun relations. We
focus here instead on a potentially more common use of L?W1W2 vectors as a ?feature
selection and labeling? space forW1W2?L tasks.
Specifically, we go back to the qualia extraction task of Section 6.2.3. There, we
started with manually identified links. Here, we start with examples of noun?quale
pairs ?n, qr? that instantiate a role r. We project all L?W1W2 vectors in a subspace where
only dimensions corresponding to one of the example pairs are non-zero. We then pick
the most characteristic links in this subspace to represent the target role r, and look for
new pairs ?n, qr? in theW1W2?L subspace defined by these automatically picked links,
instead of the manual ones. Although we stop at this point, the procedure can be seen
as a DM version of popular iterative bootstrapping algorithms such as Espresso (Pantel
and Pennacchiotti 2006): Start with some examples of the target relation, find links that
are typical of these examples, use the links to find new examples, and so on. In DM,
the process does not go back to a corpus to harvest new links and example pairs, but it
iterates between the column and row spaces of a pre-compiled matrix (i.e, the mode-2
matricization in Table 3).
For each of the 30 noun concepts in the Cimiano and Wenderoth gold standard, we
use the noun?quale pairs pertaining to the remaining 29 concepts as training examples
to select a set of 20 links that we then use in the same way as the manually selected links
of Section 6.2.3. Simply picking the longest links in the L?W1W2 subspace defined by the
example ?n, qr? dimensions does not work, because we harvest links that are frequent
in general, rather than characteristic of the qualia roles (noun modification, of, etc.). For
each role r, we construct instead two L?W1W2 subspaces, one positive subspace with the
example pairs ?n, qr? as unique non-zero dimensions, and a negative subspace with non-
zero dimensions corresponding to all ?w1,w2? pairs such that w1 is one of the training
nominal concepts, and w2 is not a quale qr in the example pairs. We then measure the
length of each link in both subspaces. For example, we measure the length of the obj
link in a subspace characterized by ?n, qtelic? example pairs, and the length of obj in a
subspace characterized by ?n, w2? pairs that are probably not Telic examples. We com-
pute the pointwise mutual information (PMI) statistic (Church and Hanks 1990) on
these lengths to find the links that are most typical of the positive subspace corre-
sponding to each qualia role. PMI, with respect to other association measures, finds
more specific links, which is good for our purposes. However, it is also notoriously
709
Computational Linguistics Volume 36, Number 4
prone to over-estimating the importance of rare items (Manning and Schu?tze 1999,
Chapter 5). Thus, before selecting the top 20 links ranked by PMI, we filter out those
links that do not have at least 10 non-zero dimensions in the positive subspace. Many
parameters here should be tuned more systematically (top n links, association measure,
minimum non-zero dimensions), but the current results will nevertheless illustrate our
methodology.
Table 14 reports, for each quale, the TypeDM links that were selected in each of the
30 leave-one-concept-out folds. The links n is q, n in q, and q such as n are a good sketch of
the Formal relation, which essentially subsumes various taxonomic relations. The other
Formal links are less conspicuous. However, note the presence of noun coordination
(n coord q and q coord n), consistently with the common claim that coordinated terms
tend to be related taxonomically (Widdows and Dorow 2002). Constitutive is mostly a
whole?part relation, and the harvested links do a good job at illustrating such a relation.
For the Telic, q by n, q through n, and q via n capture cases in which the quale stands in an
action?instrument relation to the target noun (murder by knife). These links thus encode
the subtype of Telic role that Pustejovsky (1995) calls ?indirect.? The two verb?noun
links (q obj n and n sbj intr q) instead capture ?direct? Telic roles, which are typically
expressed by the theme of a verb (read a book, the book reads well). The least convincing
results are those for the Agentive role, where only q obj n and perhaps q out n are
intuitively plausible canonical links. Interestingly, the manual selections we carried out
in Section 6.2.3 also gave very poor results for the Agentive role, as shown by the fact
that Table 10 reports just one link for such a role. This suggests that the problems with
this qualia role might be due to the number and type of lexicalized links used to build
the DM tensors, rather than to the selection algorithm presented here.
Coming now to the quantitative evaluation of the harvested patterns, the results in
Table 15 (to be compared to Table 11 in Section 6.2.3) are based on W1W2?L subspaces
where the non-zero dimensions correspond to the links that we picked automatically
with the method we just described (different links for each concept, because of the
leave-one-concept-out procedure). TypeDM is the best model in this setting as well.
Its performance is even better than the one (reported in Table 11) obtained with the
manually picked patterns (although the difference is not statistically significant; paired
t-test, t = 0.75, df = 29, p = 0.46), and the automated approach has more room for
improvement via parameter optimization.
We did not get as deeply into L?W1W2 space as we did with the other views, but
our preliminary results on qualia harvesting suggest at least that looking at links as
Table 14
Links selected in all folds of the leave-one-out procedure to extract links typical of each qualia
role.
FORMAL CONSTITUTIVE
n is q, q is n, q become n, n coord q, n have q, n use q, nwith q, nwithout q
q coord n, q have n, n in q, n provide q,
q such as n
AGENTIVE TELIC
q after n, q alongside n, q as n, q before n, q behind n, q by n, q like n, q obj n,
q besides n, q during n, q in n, q obj n, n sbj intr q, q through n, q via n
q out n, q over n, q since n, q unlike n
710
Baroni and Lenci Distributional Memory
Table 15
Average qualia extraction performance with automatically harvested links (compare to Table 11).
model precision recall F F s.d.
TypeDM 24.2 26.7 19.1 7.7
DepDM 18.4 27.0 15.1 4.9
LexDM 22.6 18.1 14.8 7.7
L?W1W2 vectors might be useful for feature selection in W1W2?L or for tasks in which
we are given a set of pairs, and we have to find links that can function as verbal labels
for the relation between the word pairs (Turney 2006a).
6.5 Smoothing by Tensor Decomposition
Dimensionality reduction techniques such as the (truncated) SVD approximate a sparse
co-occurrence matrix with a denser lower-rank matrix of the same size, and they have
been shown to be effective in many semantic tasks, probably because they provide
a beneficial form of smoothing of the dimensions. See Turney and Pantel (2010) for
references and discussion. We can apply SVD (or similar methods) to any of the tensor-
derivedmatrices we used for the tasks herein. An interesting alternative is to smooth the
source tensor directly by a tensor decomposition technique. In this section, we present
(very preliminary) evidence that tensor decomposition can improve performance, and
it is at least as good in this respect as matrix-based SVD. This is the only experiment
in which we operate on the tensor directly, rather than on the matrices derived from it,
paving the way to a more active role for the underlying tensor in the DM approach to
semantics.
The (truncated) Tucker decomposition of a tensor can be seen as a higher-order
generalization of SVD. Given a tensor X of dimensionality I1 ? I2 ? I3, its n-rank Rn
is the rank of the vector space spanned by its mode-n fibers (obviously, for each
mode n of the tensor, Rn ? In). Tucker decomposition approximates the tensorX having
n-ranks R1, . . . ,Rn with X? , a tensor with n-ranks Qn ? Rn for all modes n. Unlike the
case of SVD, there is no analytical procedure to find the best lower-rank approximation
to a tensor, and Tucker decomposition algorithms search for the reduced rank tensor
with the best fit (as measured by least square error) iteratively. Specifically, we use the
memory-efficient MET(1) algorithm of Kolda and Sun (2008) as implemented in the
Matlab Tensor Toolbox.10 Kolda and Bader (2009) provide details on Tucker decompo-
sition, its general properties, as well as applications and alternatives.
SVD is believed to exploit patterns of higher order co-occurrence between the rows
and columns of a matrix (Manning and Schu?tze 1999; Turney and Pantel 2010), making
row elements that co-occur with two synonymic columns more similar than in the
original space. Tucker decomposition applied to the mode-3 tuple tensor could capture
patterns of higher order co-occurrence for each of the modes. For example, it might
capture at the same time similarities between links such as use and hold and w2 elements
such as gun and knife. SVD applied after construction of the W1?LW2 matrix, on the
other hand, would miss the composite nature of columns such as ?use, gun?, ?use, knife?
and ?hold, gun?. Another attractive feature of Tucker decomposition is that it could be
10 http://csmr.ca.sandia.gov/?tgkolda/TensorToolbox/.
711
Computational Linguistics Volume 36, Number 4
Table 16
Purity in Almuhareb?Poesio concept clustering with rank reduction of the APTypeDM tensor;
95% confidence intervals (CI) obtained by bootstrapping.
reduction rank purity 95% CI
Tucker 250?50?500 75 72?80
Tucker 300?50?500 75 71?79
Tucker 300?50?450 74 71?79
SVD 200 74 71?79
SVD 350 74 70?79
Tucker 300?40?500 74 70?78
Tucker 300?60?500 74 70?78
Tucker 350?50?500 73 69?77
Tucker 300?50?550 72 69?77
SVD 250 72 69?77
SVD 150 72 68?77
none ? 402 71 69?77
SVD 300 71 68?76
SVD 100 68 65?73
SVD 50 64 61?70
applied once to smooth the source tensor, whereas with SVD each matricization must
be smoothed separately. However, Tucker decomposition and SVD are computationally
intensive procedures, and, at least with our current computational resources, we are not
able to decompose even the smallest DM tensor (similarly, we cannot apply SVD to a
full matricization). Given the continuous growth in computational power and the fact
that efficient tensor decomposition is a very active area of research (Turney 2007; Kolda
and Sun 2008) full tensor decomposition is nevertheless a realistic near future task.
For the current pilot study, we replicated the AP concept clustering experiment
described in Section 6.1.3. Because for efficiency reasons we must work with just a
portion of the original tensor, we thought that the AP data set, consisting of a relatively
large and balanced collection of nominal concepts, would offer a sensible starting point
to extract the subset. Specifically, we extract from our best tensor TypeDM the values
labeled by tuples ?wAP, l,w2?where wAP is in the AP set, l is one of the 100 most common
links occurring in tuples with a wAP, and w2 is one of the 1,000 most common words
occurring in tuples with a wAP and a l. The resulting (sub-)tensor, APTypeDM, has
dimensionality 402? 100? 1, 000 with 1,318,214 non-zero entries (density: 3%). The
W1?LW2 matricization of APTypeDM results in a 402? 1, 000, 000 matrix with 66,026
non-zero columns and the same number of non-zero entries and density as the tensor.
The possible combinations of target lower n-ranks constitute a large tridimensional
parameter space, and we leave its systematic exploration to further work. Instead, we
pick 300, 50, and 500 as (intuitively reasonable) initial target n-ranks for the threemodes,
and we explore their neighborhood in parameter space by changing one target n-rank at
a time, by a relatively small value (300? 50, 50? 10, and 500? 50, respectively). For the
parameters concerning the reduced tensor fitting process, we accept the default values
of the Tensor Toolbox. For comparison purposes, we also apply SVD to the W1?LW2
matrix derived from APTypeDM. We systematically explore the SVD target lower rank
parameter from 50 to 350 in increments of 50 units.
The results are reported in Table 16. The rank column reports the n-ranks when
reduction is performed on the tensor, and matrix ranks in the other cases. Bootstrapped
confidence intervals are obtained as described in Section 6.1.3. In general, the results
712
Baroni and Lenci Distributional Memory
confirm that smoothing by rank reduction is beneficial to semantic performance, al-
though not spectacularly so, with an improvement of about 4% for the best reduced
model with respect to the raw APTypeDM tensor (consider however also the relatively
wide confidence intervals). As a general trend, tensor-based smoothing (Tucker) does
better than matrix-based smoothing (SVD). As we said, for Tucker we only report re-
sults from a small region of the tridimensional parameter space, whereas the SVD rank
parameter range is explored coarsely but exhaustively. Thus, although other parameter
combinations might lead to dramatic changes in Tucker performance, the best SVD
performance in the table is probably close to the SVD performance upper bound.
The present pilot study suggests an attitude of cautious optimism towards tensor
decomposition as a smoothing technique. At least in the AP task, it helps as compared
to no smoothing at all. The same conclusion is reached by Turney (2007), who uses
essentially the same method (with some differences in implementation) to tackle the
TOEFL task, and obtains more than 10% improvement in accuracy with respect to the
corresponding raw tensor. At least as a trend, tensor decomposition appears to be better
than matrix decomposition, but only marginally so (Turney does not perform this com-
parison). Still, even if the tensor- and matrix-based decompositions turned out to have
comparable effects, tensor-based smoothing is more attractive in the DM framework
because we could perform the decomposition once, and use the smoothed tensor as our
stable underlying DM (modulo, of course, memory problems with computing such a
large tensor decomposition).
Beyond smoothing, tensor decomposition might provide some novel avenues for
distributional semantics, while keeping to the DM program of a single model for many
tasks. Van de Cruys (2009) used tensor decomposition to find commonalities in latent
dimensions across the fiber labels (in the DM formalism, this would amount to finding
commonalities across w1, l, and w2 elements). Another possible use for smoothing
would be to propagate ?link mass? across parts of speech. Our tensors, being based on
POS tagging and dependency parsing, have 0 values for noun-link-noun tuples such as
?city, obj, destruction? and ?city, subj tr, destruction?. In a smoothed tensor, by the influence
of tuples such as ?city, obj, destroy? and ?city, sbj tr, destroy?, these tuples will get some
non-0 weight that, hopefully, will make the object relation between city and destruction
emerge. This is at the moment just a conjecture, but it constitutes an exciting direction
for further work focusing on tensor decomposition within the DM framework.
7. Conclusion
A general framework for distributional semantics should satisfy the following two
requirements: (1) representing corpus-derived data in such a way as to capture aspects
of meaning that have so far been modeled with different, prima facie incompatible data
structures; (2) using this common representation to address a large battery of semantic
experiments, achieving a performance at least comparable to that of state-of-art, task-
specific DSMs. We can now safely claim that DM satisfies both these desiderata, and
thereby represents a genuine step forward in the quest for a general purpose approach
to distributional semantics.
DM addresses point (1) by modeling distributional data as a structure of weighted
tuples that is formalized as a labeled third-order tensor. This is a generalization with
respect to the common approach of many corpus-based semantic models (the structured
DSMs) that rely on distributional information encoded into word?link?word tuples,
associated with weights that are functions of their frequency of co-occurrence in the cor-
pus. Existing structured DSMs still couch this information directly in binary structures,
713
Computational Linguistics Volume 36, Number 4
namely, co-occurrence matrices, thereby giving rise to different semantic spaces and los-
ing sight of the fact that such spaces share the same kind of distributional information.
The third-order tensor formalization of distributional data allows DM to fully exploit
the potential of corpus-derived tuples. The four semantic spaces we analyzed and tested
in Section 6 are generated from the same underlying third-order tensor, by the standard
operation of tensor matricization. This way, we derive a set of semantic spaces that can
be used for measuring attributional similarity (finding synonyms, categorizing concepts
into superordinates, etc.) and relational similarity (finding analogies, grouping concept
pairs into relation classes, etc.). Moreover, the distributional information encoded in the
tensor and unfolded via matricization leads to further arrangements of the data useful
in addressing semantic problems that do not fall straightforwardly into the attributional
or the relational paradigm (grouping verbs by alternations, harvesting patterns that
represent a relation). In some cases, it is obvious how to reformulate a semantic problem
in the new framework. Other tasks can be reframed in terms of our four semantic
spaces using geometric operations such as centroid computations and projection onto
a subspace. This was the case for selectional preferences, pattern- and example-based
relation extraction (illustrated by qualia harvesting), and the task of generating typical
properties of concepts. We consider a further strength of the DM approach that it natu-
rally encourages us to think, as we did in these cases, of ways to tackle apparently
unrelated tasks with the existing resources, rather than devising unrelated approaches
to deal with them.
Regarding point (2), that is, addressing a large battery of semantic experiments with
good performance, in nearly all test sets our best implementation of DM (TypeDM) is
at least as good as other algorithms reported in recently published papers (typically
developed or tuned for the task at hand), often towards (or at) the top of the state-of-
the-art ranking. Where other models outperform TypeDM by a large margin, there are
typically obvious reasons for this: The rivals have been trained on much larger corpora,
or they rely on special knowledge resources, or on sophisticated machine learning
algorithms. Importantly, TypeDM is consistently at least as good (or better than) those
models we reimplemented to be fully comparable to our DMs (i.e., Win, DV, LRA).
Moreover, the best DM implementation does not depend on the semantic space:
TypeDM outperforms (at least in terms of average performance across tasks) the other
two models in all four spaces. This is not surprising (better distributional tuples should
still be better when seen from different views), but it is good to have an empirical con-
firmation of the a priori intuition. The current results suggest that one could, for exam-
ple, compare alternative DMs on a few attributional tasks, and expect the best DM in
these tasks to also be the best in relational tasks and other semantic challenges.
The final experiment of Section 6 briefly explored an interesting aspect of the
tensor-based formalism, namely, the possibility of improving performance on some
tasks by working directly on the tensor (in this case, applying tensor rank reduction
for smoothing purposes) rather than on the matrices derived from it. Besides this pilot
study, we did not carry out any task-specific optimization of TypeDM, which achieves
its very good performance using exactly the same underlying parameter configuration
(e.g., dependency paths, weighting function) across the different spaces and tasks.
Parameter tuning is an important aspect in DSM development, with an often dramatic
impact of parameter variation (Bullinaria and Levy 2007; Erk and Pado? 2009). We
leave the exploration of parameter space in DM for future research. Its importance not-
withstanding, however, we regard this as a rather secondary aspect, if compared with
the good performance of a DM model (even in its current implementation) in the large
and multifarious set of tasks we presented.
714
Baroni and Lenci Distributional Memory
Of course, many issues are still open. It is one thing to claim that the models that
outperform TypeDMdo so because they rely on larger corpora; it is another to show that
TypeDM trained on more data does reach the top of the current heap. The differences
between TypeDM and the other, generally worse-performing DM models remind us
that the idea of a shared distributional memory per se is not enough to obtain good
results, and the extraction of an ideal DM from the corpus certainly demands further
attention. We need to reach a better understanding of which pieces of distributional
information to extract, and whether different semantic tasks require focusing on specific
subsets of distributional data. Another issue we completely ignored but which will be of
fundamental importance in applications is how a DM-based system can deal with out-
of-vocabulary items. Ideally, we would like a seamless way to integrate new terms in
the model incrementally, based on just a few extra data points, but we leave it to further
research to study how this could be accomplished, together with the undoubtedly many
further practical and theoretical problems that will emerge. We will conclude, instead,
by discussing some general advantages that follow from the DM approach of separating
corpus-based model building, the multi-purpose long term distributional memory, and
different views of the memory data to accomplish different semantic tasks, without
resorting to the source corpus again.
First of all, we would like to make a more general point regarding parameter
tuning and task-specific optimization, by going back to the analogy with WordNet as a
semantic multi-purpose resource. If you want to improve performance of a WordNet-
based system, you will probably not wait for its next release, but rather improve the
algorithms that work on the existing WordNet graph. Similarly, in the DM approach we
propose that corpus-based resources for distributional semantics should be relatively
stable, multi-purpose, large-scale databases (in the form of weighted tuple structures),
only occasionally updated (because a better or larger corpus becomes available, a better
parser, etc.). Still, given the same underlying DM and a certain task, much work can be
done to exploit the DM optimally in the task, with no need to go back to corpus-based
resource construction. For example, performance on attributional tasks could be raised
by dimension reweighting techniques such as recently proposed by Zhitomirsky-Geffet
and Dagan (2009). For the problem of data sparseness in the W1W2?L space, we could
treat the tensor as a graph and explore random walks and other graphical approaches
that have been shown to ?scale down? gracefully to capture relations in sparser data
sets (Minkov and Cohen 2007, 2008). As in our simple example of smoothing relational
pairs with attributional neighbors, more complex tasks may be tackled by combining
different views of DM, and/or resorting to different (sub)spaces within the same view,
as in our approach to selectional preferences. One might even foresee an algorithmic
way to mix and match the spaces as most appropriate to a certain task. We propose a
similar split for the role of supervision in DSMs. Construction of the DM tensor from the
corpus is most naturally framed as an unsupervised task, because the model will serve
many different purposes. On the other hand, supervision can be of great help in tuning
the DM data to specific tasks (as we did, in a rather naive way, with the nearest centroid
approach to most non-attributional tasks). A crucial challenge for DSMs is whether
and how corpus-derived vectors can also be used in the construction of meaning for
constituents larger than words. These are the traditional domains of formal semantics,
which is most interested in how the logical representation of a sentence or a discourse
is built compositionally by combining the meanings of its constituents. DSMs have so
far focused on representing lexical meaning, and compositional and logical issues have
either remained out of the picture, or have received still unsatisfactory accounts. A gen-
eral consensus exists on the need to overcome this limitation, and to build new bridges
715
Computational Linguistics Volume 36, Number 4
between corpus-based semantics and symbolic models of meanings (Clark and Pulman
2007; Widdows 2008). Most problems encountered by DSMs in tackling this challenge
are specific instances of more general issues concerning the possibility of representing
symbolic operations with distributed, vector-based data structures (Markman 1999).
Many avenues are currently being explored in corpus-based semantics, and interesting
synergies are emerging with research areas such as neural systems (Smolensky 1990;
Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts
and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole 2005; Hou
and Song 2009), holographic models of memory (Jones and Mewhort 2007), and so
on. A core problem in dealing with compositionality with DSMs is to account for the
role of syntactic information in determining the way semantic representations are built
from lexical items. For instance, the semantic representation assigned to The dog bites
the man must be different from the one assigned to The man bites the dog, even if they
contain exactly the same lexical items. Although it is still unclear which is the best
way to compose the representation of content words in vector spaces, it is nowadays
widely assumed that structured representations like those adopted by DM are in the
right direction towards a solution to this issue, exactly because they allow distributional
representations to become sensitive to syntactic structures (Erk and Pado? 2008). Compo-
sitionality and similar issues in DSMs lie beyond the scope of this paper. However, there
is nothing in DM that prevents it from interacting with any of the research directions we
have mentioned here. Indeed, we believe that the generalized nature of DM represents
a precondition for distributional semantics to be able to satisfactorily address these
more advanced challenges. A multi-purpose, distributional semantic resource like DM
can allow researchers to focus on the next steps of semantic modeling. These include
compositionality, but also modulating word meaning in context (Erk and Pado? 2008;
Mitchell and Lapata 2008) and finding ways to embed the distributional memory in
complex NLP systems (e.g., for question answering or textual entailment) or even
embodied agents and robots.
DM-style triples predicating a relation between two entities are common currency
in many semantic representation models (e.g., semantic networks) and knowledge-
exchange formalisms such as RDF. This might also pave the way to the integration of
corpus-based information with other knowledge sources. It is hard to see how such
integration could be pursued within generalized systems, such as PairClass (Turney
2008), that require keeping a full corpus around and corpus-processing know-how on
behalf of interested researchers from outside the NLP community (see discussion in
Section 4 above). Similarly, the DM triples might help in fostering the dialogue between
computational linguists and the computational neuro-cognitive community, where it is
common to adopt triple-based representations of knowledge, and to use the same set of
tuples to simulate various aspects of cognition. For a recent extended example of this
approach, see Rogers and McClelland (2004). It would be relatively easy to use a DM
model in lieu of their neural network, and use it to simulate the conceptual processes
they reproduce.
DM, unlike classic DSM models that go directly from the corpus data to solving
specific semantic tasks, introduces a clear distinction between an acquisition phase
(corpus-based tuple extraction and weighting), the declarative structure at the core of
semantic modeling (the distributional memory), and the procedural problem-solving
components (possibly supervised procedures to perform different semantic tasks). This
separation is in line with what is commonly assumed in cognitive science and formal
linguistics, and we hope it will contribute to make corpus-based modeling a core part
of the ongoing study of semantic knowledge in humans and machines.
716
Baroni and Lenci Distributional Memory
Acknowledgments
We thank Abdulrahman Almuhareb,
Philipp Cimiano, George Karypis,
Tamara Kolda, Thomas Landauer,
Mirella Lapata, Ken McRae, Brian Murphy,
Vivi Nastase, Diarmuid O? Se?aghdha,
Sebastian and Ulrike Pado?, Suzanne
Stevenson, Peter Turney, their colleagues,
and the SEMEVAL Task 4 organizers for
data and tools. We thank Gemma Boleda,
Phillipp Cimiano, Katrin Erk, Stefan Evert,
Brian Murphy, Massimo Poesio, Magnus
Sahlgren, Tim Van de Cruys, Peter Turney,
and three anonymous reviewers for a
mixture of advice, clarification, and ideas.
References
Aerts, Diederik and Marek Czachor. 2004.
Quantum aspects of semantic analysis and
symbolic artificial intelligence. Journal of
Physics A: Mathematical and General,
37:123?132.
Alishahi, Afra and Suzanne Stevenson. 2008.
A distributional account of the semantics
of multiword expressions. Italian Journal of
Linguistics, 20(1):157?179.
Almuhareb, Abdulrahman. 2006. Attributes
in Lexical Acquisition. Ph.D. thesis,
University of Essex.
Almuhareb, Abdulrahman and Massimo
Poesio. 2004. Attribute-based and
value-based clustering: An evaluation.
In Proceedings of EMNLP, pages 158?165,
Barcelona.
Almuhareb, Abdulrahman and Massimo
Poesio. 2005. Concept learning and
categorization from the web. In Proceedings
of CogSci, pages 103?108, Stresa.
Baldwin, Timothy, Valia Kordoni, and
Aline Villavicencio. 2009. Prepositions in
applications: A survey and introduction to
the special issue. Computational Linguistics,
35(2):119?149.
Baroni, Marco, Eduard Barbu, Brian Murphy,
and Massimo Poesio. 2010. Strudel: A
distributional semantic model based on
properties and types. Cognitive Science,
34(2):222?254.
Baroni, Marco, Stefan Evert, and
Alessandro Lenci, editors. 2008. Bridging
the Gap between Semantic Theory and
Computational Simulations: Proceedings
of the ESSLLI Workshop on Distributional
Lexical Semantic. FOLLI, Hamburg.
Baroni, Marco and Alessandro Lenci. 2008.
Concepts and properties in word spaces.
Italian Journal of Linguistics, 20(1):55?88.
Baroni, Marco and Alessandro Lenci. 2009.
One distributional memory, many
semantic tasks. In Proceedings of the EACL
GEMS Workshop, pages 1?8, Athens.
Bicic?i, Ergun and Deniz Yuret. 2006.
Clustering word pairs to answer analogy
questions. In Proceedings of the Fifteenth
Turkish Symposium on Artificial Intelligence
and Neural Networks, pages 277?284,
Mug?la.
Bruza, Peter and Richard Cole. 2005.
Quantum logic of semantic space:
An exploratory investigation of context
effects in practical reasoning. In
Sergei Artemov, Howard Barringer,
Arthur d?Avila Garcez, Luis C. Lamb,
and John Woods, editors,We Will Show
Them: Essays in Honour of Dov Gabbay,
volume one. College Publications, London,
pages 339?361.
Buitelaar, Paul, Philipp Cimiano, and
Bernardo Magnini. 2005. Ontology
Learning from Text. IOS Press, Amsterdam.
Bullinaria, John and Joseph Levy. 2007.
Extracting semantic representations
from word co-occurrence statistics: A
computational study. Behavior Research
Methods, 39:510?526.
Chen, Hsin-Hsi, Ming-Shun Lin, and
Yu-Chuan Wei. 2006. Novel association
measures using Web search with double
checking. In Proceedings of COLING-ACL,
pages 1009?1016, Sydney.
Church, Kenneth and Peter Hanks. 1990.
Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Cimiano, Philipp and Johanna Wenderoth.
2007. Automatic acquisition of ranked
qualia structures from the Web. In
Proceedings of ACL, pages 888?895,
Prague.
Clark, Stephen and Stephen Pulman. 2007.
Combining symbolic and distributional
models of meaning. In Proceedings of the
AAAI Spring Symposium on Quantum
Interaction, pages 52?55, Stanford, CA.
Curran, James and Marc Moens. 2002.
Improvements in automatic thesaurus
extraction. In Proceedings of the ACL
Workshop on Unsupervised Lexical
Acquisition, pages 59?66, Philadelphia, PA.
Davidov, Dmitry and Ari Rappoport. 2008a.
Classification of semantic relationships
between nominals using pattern clusters.
In Proceedings of ACL, pages 227?235,
Columbus, OH.
Davidov, Dmitry and Ari Rappoport.
2008b. Unsupervised discovery of
717
Computational Linguistics Volume 36, Number 4
generic relationships using pattern
clusters and its evaluation by
automatically generated SAT analogy
questions. In Proceedings of ACL,
pages 692?700, Columbus, OH.
Dietterich, Thomas. 1998. Approximate
statistical tests for comparing supervised
classification learning algorithms. Neural
Computation, 10(7):1895?1924.
Dowty, David. 1977.Word Meaning and
Montague Grammar. Kluwer, Dordrecht.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Efron, Bradley and Robert Tibshirani. 1994.
An Introduction to the Bootstrap. Chapman
and Hall, Boca Raton, FL.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences.
In Proceedings of ACL, pages 216?223,
Prague.
Erk, Katrin and Sebastian Pado?. 2008. A
structured vector space model for word
meaning in context. In Proceedings of
EMNLP, pages 897?906, Honolulu, HI.
Erk, Katrin and Sebastian Pado?. 2009.
Paraphrase assessment in structured
vector space: Exploring parameters and
datasets. In Proceedings of the EACL
GEMS Workshop, pages 57?65, Athens.
Evert, Stefan. 2005. The Statistics of Word
Cooccurrences. Ph.D. dissertation,
Stuttgart University.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Garrard, Peter, Matthew Lambon Ralph,
John Hodges, and Karalyn Patterson.
2001. Prototypicality, distinctiveness,
and intercorrelation: Analyses of the
semantic attributes of living and nonliving
concepts. Cognitive Neuropsychology,
18(2):25?174.
Geeraerts, Dirk. 2010. Theories of Lexical
Semantics. Oxford University Press,
Oxford.
Girju, Roxana, Adriana Badulescu, and
Dan Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Girju, Roxana, Preslav Nakov, Vivi Nastase,
Stan Szpakowicz, Peter Turney, and
Deniz Yuret. 2007. SemEval-2007 task 04:
Classification of semantic relations
between nominals. In Proceedings of
SemEval 2007, pages 13?18, Prague.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer,
Boston, MA.
Griffiths, Tom, Mark Steyvers, and Josh
Tenenbaum. 2007. Topics in semantic
representation. Psychological Review,
114:211?244.
Harris, Zellig. 1954. Distributional structure.
Word, 10(2-3):1456?1162.
Hearst, Marti. 1992. Automatic acquisition
of hyponyms from large text corpora. In
Proceedings of COLING, pages 539?545,
Nantes.
Hearst, Marti. 1998. Automated discovery
of WordNet relations. In Christiane
Fellbaum, editor,WordNet: An Electronic
Lexical Database. MIT Press, Cambridge,
MA, pages 131?151.
Herdag?delen, Amac? and Marco Baroni.
2009. BagPack: A general framework to
represent semantic relations. In Proceedings
of the EACL GEMS Workshop, pages 33?40,
Athens.
Herdag?delen, Amac?, Katrin Erk, and
Marco Baroni. 2009. Measuring semantic
relatedness with vector space models
and random walks. In Proceedings of
TextGraphs-4, pages 50?53, Singapore.
Heylen, Kris, Yves Peirsman, Dirk Geeraerts,
and Dirk Speelman. 2008. Modelling word
similarity: An evaluation of automatic
synonymy extraction algorithms. In
Proceedings of LREC, pages 3243?3249,
Marrakech.
Hou, Yuexian and Dawei Song. 2009.
Characterizing pure high-order
entanglements in lexical semantic
spaces via information geometry.
In Peter Bruza, Donald Sofge, William
Lawless, and C. J. van Rijsbergen, editors,
Quantum Interaction: Third International
Symposium, QI 2009. Springer, Berlin,
pages 237?250.
Jackendoff, Ray. 1990. Semantic Structures.
MIT Press, Cambridge, MA.
Joanis, Eric, Suzanne Stevenson, and David
James. 2008. A general feature space for
automatic verb classification. Natural
Language Engineering, 14(3):337?367.
Jones, Michael and Douglas Mewhort.
2007. Representing word meaning
and order information in a composite
holographic lexicon. Psychological
Review, 114:1?37.
Karypis, George. 2003. CLUTO: A clustering
toolkit. Technical Report 02-017,
University of Minnesota Department
of Computer Science, Minneapolis.
Kilgarriff, Adam, Pavel Rychly, Pavel Smrz,
and David Tugwell. 2004. The Sketch
Engine. In Proceedings of Euralex,
pages 105?116, Lorient.
718
Baroni and Lenci Distributional Memory
Kolda, Tamara. 2006. Multilinear operators
for higher-order decompositions. Technical
Report 2081, SANDIA, Albuquerque, NM.
Kolda, Tamara and Brett Bader. 2009. Tensor
decompositions and applications. SIAM
Review, 51(3):455?500.
Kolda, Tamara and Jimeng Sun. 2008.
Scalable tensor decompositions for
multi-aspect data mining. In Proceedings
of ICDM, pages 94?101, Pisa.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The latent
semantic analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104(2):211?240.
Lenci, Alessandro. 2008. Distributional
approaches in linguistic and cognitive
research. Italian Journal of Linguistics,
20(1):1?31.
Lenci, Alessandro. 2010. The life cycle
of knowledge. In Chu-Ren Huang,
Nicoletta Calzolari, Aldo Gangemi,
Alessandro Lenci, Alessandro Oltramari,
and Laurent Pre?vot, editors, Ontology
and the Lexicon. A Natural Language
Processing Perspective. Cambridge
University Press, Cambridge, UK,
pages 241?257.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago, IL.
Levin, Beth and Malka Rappaport-Hovav.
2005. Argument Realization. Cambridge
University Press, Cambridge, UK.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words. In
Proceedings of COLING-ACL,
pages 768?774, Montreal.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of
ICML, pages 296?304, Madison, WI.
Liu, Hugo and Push Singh. 2004.
ConceptNet: A practical commonsense
reasoning toolkit. BT Technology Journal,
pages 211?226.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of CogSci,
pages 576?581, Edinburgh, UK.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.
Manning, Chris and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language
Processing. MIT Press, Cambridge, MA.
Markman, Arthur B. 1999. Knowledge
Representation. Psychology Press,
New York, NY.
Matveeva, Irina, Gina-Anne Levow,
Ayman Farahat, and Christian Royer. 2005.
Generalized latent semantic analysis for
term representation. In Proceedings of
RANLP, pages 60?68, Borovets.
McRae, Ken, George Cree, Mark Seidenberg,
and Chris McNorgan. 2005. Semantic
feature production norms for a large set
of living and nonliving things. Behavior
Research Methods, 37(4):547?559.
McRae, Ken, Michael Spivey-Knowlton,
and Michael Tanenhaus. 1998. Modeling
the influence of thematic fit (and
other constraints) in on-line sentence
comprehension. Journal of Memory and
Language, 38:283?312.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distributions of argument
structure. Computational Linguistics,
27(3):373?408.
Meyer, Carl. 2000.Matrix Analysis and Applied
Linear Algebra. SIAM, Philadelphia, PA.
Miller, George and Walter Charles. 1991.
Contextual correlates of semantic
similarity. Language and Cognitive
Processes, 6:1?28.
Minkov, Einat and William Cohen. 2007.
Learning to rank typed graph walks:
Local and global approaches. In
Proceedings of WebKDD/SNA-KDD,
pages 1?8, San Jose?, CA.
Minkov, Einat and William Cohen. 2008.
Learning graph walk based similarity
measures for parsed text. In Proceedings
of EMNLP, pages 907?916, Honolulu, HI.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
Murphy, Gregory. 2002. The Big Book of
Concepts. MIT Press, Cambridge, MA.
Nastase, Vivi and Stan Szpakowicz. 2003.
Exploring noun-modifier semantic
relations. In Proceedings of the Fifth
International Workshop on Computational
Semantics, pages 285?301, Tilburg,
The Netherlands.
O? Se?aghdha, Diarmuid and Ann Copestake.
2009. Using lexical and relational
similarity to classify semantic relations.
In Proceedings of EACL, pages 621?629,
Athens.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pado?, Ulrike. 2007. The Integration of
Syntax and Semantic Plausibility in a
719
Computational Linguistics Volume 36, Number 4
Wide-Coverage Model of Sentence Processing.
Ph.D. dissertation, Saarland University,
Saarbru?cken.
Pado?, Ulrike, Sebastian Pado?, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of EMNLP, pages 400?409,
Prague.
Pantel, Patrick and Marco Pennacchiotti.
2006. Espresso: Leveraging generic
patterns for automatically harvesting
semantic relations. In Proceedings of
COLING-ACL, pages 113?120, Sydney.
Peirsman, Yves and Dirk Speelman. 2009.
Word space models of lexical variation.
In Proceedings of the EACL GEMS
Workshop, pages 9?16, Athens.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Quesada, Jose, Praful Mangalath, and
Walter Kintsch. 2004. Analogy-making
as predication using relational information
and LSA vectors. In Proceedings of CogSci,
page 1623, Chicago, IL.
Raghunathan, Trivellore. 2003. An
approximate test for homogeneity of
correlated correlation coefficients.
Quality & Quantity, 37:99?110.
Rapp, Reinhard. 2003. Word sense discovery
based on sense descriptor dissimilarity.
In Proceedings of the 9th MT Summit,
pages 315?322, New Orleans, LA.
Rapp, Reinhard. 2004. A freely available
automatically generated thesaurus of
related words. In Proceedings of LREC,
pages 395?398, Lisbon.
Rogers, Timothy and James McClelland.
2004. Semantic Cognition: A Parallel
Distributed Processing Approach. MIT Press,
Cambridge, MA.
Rothenha?usler, Klaus and Hinrich Schu?tze.
2009. Unsupervised classification with
dependency based word spaces. In
Proceedings of the EACL GEMS Workshop,
pages 17?24, Athens, Greece.
Rubenstein, Herbert and John Goodenough.
1965. Contextual correlates of synonymy.
Communications of the ACM, 8(10):627?633.
Ruiz-Casado, Maria, Enrique Alfonseca,
and Pablo Castells. 2005. Using
context-window overlapping in synonym
discovery and ontology extension. In
Proceedings of RANLP, pages 1?7, Borovets.
Sagi, Eyal, Stefan Kaufmann, and Brady
Clark. 2009. Semantic density analysis:
Comparing word meaning across time
and phonetic space. In Proceedings of the
EACL GEMS Workshop, pages 104?111,
Athens.
Sahlgren, Magnus. 2005. An introduction to
random indexing. http://www.sics.se/
?mange/papers/RI intro.pdf.
Sahlgren, Magnus. 2006. The Word-Space
Model. Ph.D. dissertation, Stockholm
University.
Schulte im Walde, Sabine. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32:159?194.
Schu?tze, Hinrich. 1997. Ambiguity Resolution
in Natural Language Learning. CSLI
Publications, Stanford, CA.
Smolensky, Paul. 1990. Tensor product
variable binding and the representation
of symbolic structures in connectionist
systems. Artificial Intelligence, 46:159?216.
Smolensky, Paul and Geraldine Legendre.
2006. The Harmonic Mind. From Neural
Computation to Optimality-theoretic
Grammar. MIT Press, Cambridge, MA.
Terra, Egidio and Charles Clarke. 2003.
Frequency estimates for statistical word
similarity measures. In Proceedings of
HLT-NAACL, pages 244?251, Edmonton.
Turney, Peter. 2001. Mining the Web for
synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of ECML, pages 491?502,
Freiburg.
Turney, Peter. 2006a. Expressing implicit
semantic relations without supervision.
In Proceedings of COLING-ACL,
pages 313?320, Sydney.
Turney, Peter. 2006b. Similarity of semantic
relations. Computational Linguistics,
32(3):379?416.
Turney, Peter. 2007. Empirical evaluation
of four tensor decomposition algorithms.
Technical Report ERB-1152, NRC,
Ottawa.
Turney, Peter. 2008. A uniform approach to
analogies, synonyms, antonyms and
associations. In Proceedings of COLING,
pages 905?912, Manchester.
Turney, Peter and Michael Littman. 2005.
Corpus-based learning of analogies and
semantic relations.Machine Learning,
60(1-3):251?278.
Turney, Peter and Patrick Pantel. 2010. From
frequency to meaning: Vector space
models of semantics. Journal of Artificial
Intelligence Research, 37:141?188.
Van de Cruys, Tim. 2009. A non-negative
tensor factorization model for selectional
preference induction. In Proceedings of the
EACL GEMS Workshop, pages 83?90,
Athens.
Van Overschelde, James, Katherine Rawson,
and John Dunlosky. 2004. Category
720
Baroni and Lenci Distributional Memory
norms: An updated and expanded
version of the Battig and Montague (1969)
norms. Journal of Memory and Language,
50:289?335.
Van Rijsbergen, C. J. 2004. The Geometry of
Information Retrieval. Cambridge
University Press, Cambridge, UK.
Veale, Tony and Yanfen Hao. 2008.
Acquiring naturalistic concept
descriptions from the Web. In
Proceedings of LREC, pages 1121?1124,
Marrakech.
Vinson, David and Gabriella Vigliocco. 2008.
Semantic feature production norms for a
large set of objects and events. Behavior
Research Methods, 40(1):183?190.
Widdows, Dominic. 2004. Geometry and
Meaning. CSLI Publications, Stanford, CA.
Widdows, Dominic. 2008. Semantic vector
products: Some initial investigations.
In Proceedings of the Second AAAI
Symposium on Quantum Interaction,
pages 1?8, Oxford.
Widdows, Dominic and Beate Dorow.
2002. A graph model for unsupervised
lexical acquisition. In Proceedings of
ICCL, pages 1?7, Taipei.
Widdows, Dominic and Stanley Peters.
2003. Word vectors and quantum logic.
In Proceedings of the Eighth Mathematics
of Language Conference, pages 1?14,
Bloomington, IN.
Zarcone, Alessandra and Alessandro Lenci.
2008. Computational models of event type
classification in context. In Proceedings of
LREC, pages 1232?1238, Marrakech.
Zhao, Ying and George Karypis. 2003.
Criterion functions for document
clustering: Experiments and analysis.
Technical Report 01-40, University of
Minnesota Department of Computer
Science, Minneapolis.
Zhitomirsky-Geffet, Maayan and Ido Dagan.
2009. Bootstrapping distributional feature
vector quality. Computational Linguistics,
35(3):435?461.
721

Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 136?145,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Distributional Semantics in Technicolor
Elia Bruni
University of Trento
elia.bruni@unitn.it
Gemma Boleda
University of Texas at Austin
gemma.boleda@utcompling.com
Marco Baroni
Nam-Khanh Tran
University of Trento
name.surname@unitn.it
Abstract
Our research aims at building computational
models of word meaning that are perceptually
grounded. Using computer vision techniques,
we build visual and multimodal distributional
models and compare them to standard textual
models. Our results show that, while visual
models with state-of-the-art computer vision
techniques perform worse than textual models
in general tasks (accounting for semantic re-
latedness), they are as good or better models
of the meaning of words with visual correlates
such as color terms, even in a nontrivial task
that involves nonliteral uses of such words.
Moreover, we show that visual and textual in-
formation are tapping on different aspects of
meaning, and indeed combining them in mul-
timodal models often improves performance.
1 Introduction
Traditional semantic space models represent mean-
ing on the basis of word co-occurrence statistics in
large text corpora (Turney and Pantel, 2010). These
models (as well as virtually all work in computa-
tional lexical semantics) rely on verbal information
only, while human semantic knowledge also relies
on non-verbal experience and representation (Louw-
erse, 2011), crucially on the information gathered
through perception. Recent developments in com-
puter vision make it possible to computationally
model one vital human perceptual channel: vision
(Mooney, 2008). A few studies have begun to use
visual information extracted from images as part of
distributional semantic models (Bergsma and Van
Durme, 2011; Bergsma and Goebel, 2011; Bruni et
al., 2011; Feng and Lapata, 2010; Leong and Mihal-
cea, 2011). These preliminary studies all focus on
how vision may help text-based models in general
terms, by evaluating performance on, for instance,
word similarity datasets such as WordSim353.
This paper contributes to connecting language and
perception, focusing on how to exploit visual infor-
mation to build better models of word meaning, in
three ways: (1) We carry out a systematic compari-
son of models using textual, visual, and both types of
information. (2) We evaluate the models on general
semantic relatedness tasks and on two specific tasks
where visual information is highly relevant, as they
focus on color terms. (3) Unlike previous work, we
study the impact of using different kinds of visual
information for these semantic tasks.
Our results show that, while visual models with
state-of-the-art computer vision techniques perform
worse than textual models in general semantic tasks,
they are as good or better models of the mean-
ing of words with visual correlates such as color
terms, even in a nontrivial task that involves nonlit-
eral uses of such words. Moreover, we show that vi-
sual and textual information are tapping on different
aspects of meaning, such that they are complemen-
tary sources of information, and indeed combining
them in multimodal models often improves perfor-
mance. We also show that ?hybrid? models exploit-
ing the patterns of co-occurrence of words as tags
of the same images can be a powerful surrogate of
visual information under certain circumstances.
The rest of the paper is structured as follows. Sec-
tion 2 introduces the textual, visual, multimodal,
136
and hybrid models we use for our experiments. We
present our experiments in sections 3 to 5. Section
6 reviews related work, and section 7 finishes with
conclusions and future work.
2 Distributional semantic models
2.1 Textual models
For the current project, we constructed a set of
textual distributional models that implement vari-
ous standard ways to extract them from a corpus,
chosen to be representative of the state of the art.
In all cases, occurrence and co-occurrence statis-
tics are extracted from the freely available ukWaC
and Wackypedia corpora combined (size: 1.9B and
820M tokens, respectively).1 Moreover, in all mod-
els the raw co-occurrence counts are transformed
into nonnegative Local Mutual Information (LMI)
scores.2 Finally, in all models we harvest vector rep-
resentations for the same words (lemmas), namely
the top 20K most frequent nouns, 5K most frequent
adjectives and 5K most frequent verbs in the com-
bined corpora (for coherence with the vision-based
models, that cannot exploit contextual information
to distinguish nouns and adjectives, we merge nom-
inal and adjectival usages of the color adjectives in
the text-based models as well). The same 30K tar-
get nouns, verbs and adjectives are also employed as
contextual elements.
The Window2 and Window20 models are based
on counting co-occurrences with collocates within
a window of fixed width, in the tradition of HAL
(Lund and Burgess, 1996). Window2 records
sentence-internal co-occurrence with the nearest 2
content words to the left and right of each target con-
cept, a narrow context definition expected to capture
taxonomic relations. Window20 considers a larger
window of 20 words to the left and right of the target,
and should capture broader topical relations. The
Document model corresponds to a ?topic-based?
approach in which words are represented as distri-
butions over documents. It is based on a word-by-
document matrix, recording the distribution of the
1http://wacky.sslmit.unibo.it/
2LMI is obtained by multiplying raw counts by Pointwise
Mutual Information, and it is a close approximation to the Log-
Likelihood Ratio (Evert, 2005). It counteracts the tendency of
PMI to favour extremely rare events.
30K target words across the 30K documents in the
concatenated corpus that have the largest cumulative
LMI mass. This model is thus akin to traditional
Latent Semantic Analysis (Landauer and Dumais,
1997), without dimensionality reduction.
We add to the models we constructed the freely
available Distributional Memory (DM) model,3 that
has been shown to reach state-of-the-art perfor-
mance in many semantic tasks (Baroni and Lenci,
2010). DM is an example of a more complex text-
based model that exploits lexico-syntactic and de-
pendency relations between words (see Baroni and
Lenci?s article for details), and we use it as an in-
stance of a grammar-based model. DM is based
on the same corpora we used plus the 100M-word
British National Corpus,4 and it also uses LMI
scores.
2.2 Visual models
The visual models use information extracted from
images instead of textual corpora. We use image
data where each image is associated with one or
more words or tags (we use ?tag? for each word as-
sociated to the image, and ?label? for the set of tags
of an image). We use the ESP-Game dataset,5 con-
taining 100K images labeled through a game with a
purpose in which two people partnered online must
independently and rapidly agree on an appropriate
word to label randomly selected images. Once a
word is entered by both partners in a certain num-
ber of game matches, that word is added to the label
for that image, and it becomes a taboo word for the
following rounds of the game (von Ahn and Dab-
bish, 2004). There are 20,515 distinct tags in the
dataset, with an average of 4 tags per image. We
build one vector with visual features for each tag in
the dataset.
The visual features are extracted with the use of
a standard bag-of-visual-words (BoVW) represen-
tation of images, inspired by NLP (Sivic and Zisser-
man, 2003; Csurka et al, 2004; Nister and Stewe-
nius, 2006; Bosch et al, 2007; Yang et al, 2007).
This approach relies on the notion of a common vo-
cabulary of ?visual words? that can serve as discrete
representations for all images. Contrary to what hap-
3http://clic.cimec.unitn.it/dm
4http://www.natcorp.ox.ac.uk/
5http://www.espgame.org
137
pens in NLP, where words are (mostly) discrete and
easy to identify, in vision the visual words need to
be first defined. The process is completely induc-
tive. In a nutshell, BoVW works as follows. From
every image in a dataset, relevant areas are identified
and a low-level feature vector (called a ?descriptor?)
is built to represent each area. These vectors, living
in what is sometimes called a descriptor space, are
then grouped into a number of clusters. Each cluster
is treated as a discrete visual word, and the clusters
will be the vocabulary of visual words used to rep-
resent all the images in the collection. Now, given
a new image, the nearest visual word is identified
for each descriptor extracted from it, such that the
image can be represented as a BoVW feature vec-
tor, by counting the instances of each visual word
in the image (note that an occurrence of a low-level
descriptor vector in an image, after mapping to the
nearest cluster, will increment the count of a single
dimension of the higher-level BoVW vector). In our
work, the representation of each word (tag) is a also
a BoVW vector. The values of each dimension are
obtained by summing the occurrences of the relevant
visual word in all the images tagged with the word.
Again, raw counts are transformed into Local Mu-
tual Information scores. The process to extract vi-
sual words and use them to create image-based vec-
tors to represent (real) words is illustrated in Figure
1, for a hypothetical example in which there is only
one image in the collection labeled with the word
horse.
! !
!"#$%&'()*&!$(+%#
!! !!!"#$%
,*&$# - . / .!!!!0#%)*&!&#(&#$#1)+)'*1
!!!!!!!!2345!.6.
Figure 1: Procedure to build a visual representation for a
word, exemplified with SIFT features.
We extract descriptor features of two types.6
First, the standard Scale-Invariant Feature Trans-
form (SIFT) feature vectors (Lowe, 1999; Lowe,
2004), good at characterizing parts of objects. Sec-
ond, LAB features (Fairchild, 2005), which encode
only color information. We also experimented with
other visual features, such as those focusing on
edges (Canny, 1986), texture (Zhu et al, 2002), and
shapes (Oliva and Torralba, 2001), but they were
not useful for the color tasks. Moreover, we ex-
perimented also with different color scales, such as
LUV, HSV and RGB, obtaining significantly worse
performance compared to LAB. Further details on
feature extraction follow.
SIFT features are designed to be invariant to im-
age scale and rotation, and have been shown to pro-
vide a robust matching across affine distortion, noise
and change in illumination. The version of SIFT fea-
tures that we use is sensitive to color (RGB scale;
LUV, LAB and OPPONENT gave worse results).
We automatically identified keypoints for each im-
age and extracted SIFT features on a regular grid de-
fined around the keypoint with five pixels spacing,
at four multiple scales (10, 15, 20, 25 pixel radii),
zeroing the low contrast ones. To obtain the visual
word vocabulary, we cluster the SIFT feature vec-
tors with the standardly used k-means clustering al-
gorithm. We varied the number k of visual words
between 500 and 2,500 in steps of 500.
For the SIFT-based representation of images, we
used spatial histograms to introduce weak geometry
(Grauman and Darrell, 2005; Lazebnik et al, 2006),
dividing the image into several (spatial) regions, rep-
resenting each region in terms of BoVW, and then
concatenating the vectors. In our experiments, the
spatial regions were obtained by dividing the image
in 4? 4, for a total of 16 regions (other values and a
global representation did not perform as well). Note
that, following standard practice, descriptor cluster-
ing was performed ignoring the region partition, but
the resulting visual words correspond to different di-
mensions in the concatenated BoVW vectors, de-
pending on the region in which they occur. Con-
sequently, a vocabulary of k visual words results in
BoVW vectors with k ? 16 dimensions.
6We use VLFeat (http://www.vlfeat.org/) for fea-
ture extraction (Vedaldi and Fulkerson, 2008).
138
The LAB color space plots image data in 3 di-
mensions along 3 independent (orthogonal) axes,
one for brightness (luminance) and two for color
(chrominance). Luminance corresponds closely to
brightness as recorded by the brain-eye system;
the chrominance (red-green and yellow-blue) axes
mimic the oppositional color sensations the retina
reports to the brain (Szeliski, 2010). LAB features
are densely sampled for each pixel. Also here we use
the k-means algorithm to build the descriptor space.
We varied the number of k visual words between
128 and 1,024 in steps of 128.
2.3 Multimodal models
To assemble the textual and visual representations in
multimodal semantic spaces, we concatenate the two
vectors after normalizing them. We use the linear
weighted combination function proposed by Bruni
et al (2011): Given a word that is present both in
the textual model and in the visual model, we sepa-
rately normalize the two vectors Ft and Fv and we
combine them as follows:
F = ?? Ft ? (1? ?)? Fv
where ? is the vector concatenate operator. The
weighting parameter ? (0 ? ? ? 1) is tuned on the
MEN development data (2,000 word pairs; details
on the MEN dataset in the next section). We find the
optimal value to be close to ? = 0.5 for most model
combinations, suggesting that textual and visual in-
formation should have similar weight. Our imple-
mentation of the proposed method is open source
and publicly available.7
2.4 Hybrid models
We further introduce hybrid models that exploit the
patterns of co-occurrence of words as tags of the
same images. Like textual models, these mod-
els are based on word co-occurrence; like visual
models, they consider co-occurrence in images (im-
age labels). In one model (ESP-Win, analogous
to window-based models), words tagging an im-
age were represented in terms of co-occurrence with
the other tags in the image label (Baroni and Lenci
(2008) are a precedent for the use of ESP-Win).
The other (ESP-Doc, analogous to document-based
7https://github.com/s2m/FUSE
models) represented words in terms of their co-
occurrence with images, using each image as a dif-
ferent dimension. This information is very easy to
extract, as it does not require the sophisticated tech-
niques used in computer vision. We expected these
models to perform very bad; however, as we will
show, they perform relatively well in all but one of
the tasks tested.
3 Textual and visual models as general
semantic models
We test the models just presented in two different
ways: First, as general models of word meaning,
testing their correlation to human judgements on
word similarity and relatedness (this section). Sec-
ond, as models of the meaning of color terms (sec-
tions 4 and 5).
We use one standard dataset (WordSim353) and
one new dataset (MEN). WordSim353 (Finkelstein
et al, 2002) is a widely used benchmark constructed
by asking 16 subjects to rate a set of 353 word pairs
on a 10-point similarity scale and averaging the rat-
ings (dollar/buck receives a high 9.22 average rat-
ing, professor/cucumber a low 0.31). MEN is a
new evaluation benchmark with a better coverage of
our multimodal semantic models.8 It contains 3,000
pairs of randomly selected words that occur as ESP
tags (pairs sampled to ensure a balanced range of re-
latedness levels according to a text-based semantic
score). Each pair is scored on a [0, 1]-normalized
semantic relatedness scale via ratings obtained by
crowdsourcing on the Amazon Mechanical Turk (re-
fer to the online MEN documentation for more de-
tails). For example, cold/frost has a high 0.9 MEN
score, eat/hair a low 0.1. We evaluate the models
in terms of their Spearman correlation to the human
ratings. Our models have a perfect MEN coverage
and a coverage of 252 WordSim pairs.
We used the development set of MEN to test
the effect of varying the number k of visual words
in SIFT and LAB. We restrict the discussion to
SIFT with the optimal k (2.5K words) and to LAB
with the optimal (256), lowest (128), and highest
k (1024). We report the results of the multimodal
8An updated version of MEN is available from http://
clic.cimec.unitn.it/?elia.bruni/MEN.html.
The version used here contained 10 judgements per word pair.
139
models built with these visual models and the best
textual models (Window2 and Window20).
Columns WS and MEN in Table 1 report corre-
lations with the WordSim and MEN ratings, respec-
tively. As expected, because they are more mature
and capture a broader range of semantic informa-
tion, textual models perform much better than purely
visual models. Also as expected, SIFT features out-
perform the simpler LAB features for this task.
A first indication that visual information helps is
the fact that, for MEN, multimodal models perform
best. Note that all models that are sensitive to vi-
sual information perform better for MEN than for
WordSim, and the reverse is true for textual models.
Because of its design, word pairs in MEN can be
expected to be more imageable than those in Word-
Sim, so the visual information is more relevant for
this dataset. Also recall that we did some parameter
tuning on held-out MEN data.
Surprisingly, hybrid models perform quite well:
They are around 10 points worse than textual and
multimodal models for WordSim, and only slightly
worse than multimodal models for MEN.
4 Experiment 1: Discovering the color of
concrete objects
In Experiment 1, we test the hypothesis that the re-
lation between words denoting concrete things and
words denoting their typical color is reflected by the
distance of the corresponding vectors better when
the models are sensitive to visual information.
4.1 Method
Two authors labeled by consensus a list of concrete
nouns (extracted from the BLESS dataset9 and the
nouns in the BNC occurring with color terms more
than 100 times) with one of the 11 colors from
the basic set proposed by Berlin and Kay (1969):
black, blue, brown, green, grey, orange, pink, pur-
ple, red, white, yellow. Objects that do not have
an obvious characteristic color (computer) and those
with more than one characteristic color (zebra, bear)
were eliminated. Moreover, only nouns covered by
all the models were preserved. The final list con-
9http://sites.google.com/site/
geometricalmodels/shared-evaluation
Model WS MEN E1 E2
DM .44 .42 3 (09) .14
Document .63 .62 3 (07) .06
Window2 .70 .66 5 (13) .49***
Window20 .70 .62 3 (11) .53***
LAB128 .21 .41 1 (27) .25*
LAB256 .21 .41 2 (24) .24*
LAB1024 .19 .41 2 (24) .28**
SIFT2.5K .33 .44 3 (15) .57***
W2-LAB128 .40 .59 1 (27) .40***
W2-LAB256 .41 .60 2 (23) .40***
W2-LAB1024 .39 .61 2 (24) .44***
W20-LAB128 .40 .60 1 (27) .36***
W20-LAB256 .41 .60 2 (23) .36***
W20-LAB1024 .39 .62 2 (24) .40***
W2-SIFT2.5K .64 .69 2.5 (19) .68***
W20-SIFT2.5K .64 .68 2 (17) .73***
ESP-Doc .52 .66 1 (37) .29*
ESP-Win .55 .68 4 (15) .16
Table 1: Results of the textual, visual, multimodal, and
hybrid models on the general semantic tasks (first two
columns, section 3; Pearson ?) and Experiments 1 (E1,
section 4) and 2 (E2, section 5). E1 reports the median
rank of the correct color and the number of top matches
(in parentheses), and E2 the average difference in nor-
malized cosines between literal and nonliteral adjective-
noun phrases, with the significance of a t-test (*** for
p< 0.001, ** < 0.01, * < 0.05).
tains 52 nouns.10 Some random examples are fog?
grey, crow?black, wood?brown, parsley?green, and
grass?green.
For evaluation, we measured the cosine of each
noun with the 11 basic color words in the space pro-
duced by each model, and recorded the rank of the
correct color in the resulting ordered list.
4.2 Results
Column E1 in Table 1 reports the median rank for
each model (the smaller the rank, the better the
model), as well as the number of exact matches (that
is, number of nouns for which the model ranks the
correct color first).
Discovering knowledge such that grass is green
is arguably a simple task but Experiment 1 shows
10Dataset available from the second author?s webpage, under
resources.
140
that textual models fail this simple task, with median
ranks around 3.11 This is consistent with the findings
in Baroni and Lenci (2008) that standard distribu-
tional models do not capture the association between
concrete concepts and their typical attributes. Visual
models, as expected, are better at capturing the as-
sociation between concepts and visual attributes. In
fact, all models that are sensitive to visual informa-
tion achieve median rank 1.
Multimodal models do not increase performance
with respect to visual models: For instance, both
W2-LAB128 and W20-LAB128 have the same me-
dian rank and number of exact matches as LAB128
alone. Textual information in this case is not com-
plementary to visual information, but simply poorer.
Also note that LAB features do better than SIFT
features. This is probably due to the fact that Exper-
iment 1 is basically about identifying a large patch
of color. The SIFT features we are using are also
sensitive to color, but they seem to be misguided by
the other cues that they extract from images. For
example, pigs are pink in LAB space but brown in
SIFT space, perhaps because SIFT focused on the
color of the typical environment of a pig. We can
thus confirm that, by limiting multimodal spaces to
SIFT features, as has been done until now in the lit-
erature, we are missing important semantic informa-
tion, such as the color information that we can mine
with LAB.
Again we find that hybrid models do very well,
in fact in this case they have the top performance,
as they perform better than LAB128 (the differ-
ence, which can be noticed in the number of exact
matches, is highly significant according to a paired
Mann-Whitney test, with p<0.001).
5 Experiment 2
Experiment 2 requires more sophisticated informa-
tion than Experiment 1, as it involves distinguishing
between literal and nonliteral uses of color terms.
11We also experimented with a model based on direct co-
occurrence of adjectives and nouns, obtaining promising results
in a preliminary version of Exp. 1. We abandoned this approach
because such a model inherently lacks scalability, as it will not
generalize behind cases where the training data contain direct
examples of co-occurrences of the target pairs.
5.1 Method
We test the performance of the different models
with a dataset consisting of color adjective-noun
phrases, randomly drawn from the most frequent 8K
nouns and 4K adjectives in the concatenated ukWaC,
Wackypedia, and BNC corpora (four color terms are
not among these, so the dataset includes phrases for
black, blue, brown, green, red, white, and yellow
only). These were tagged by consensus by two hu-
man judges as literal (white towel, black feather)
or nonliteral (white wine, white musician, green fu-
ture). Some phrases had both literal and nonliteral
uses, such as blue book in ?book that is blue? vs.
?automobile price guide?. In these cases, only the
most common sense (according to the judges) was
taken into account for the present experiment. The
dataset consists of 370 phrases, of which our models
cover 342, 227 literal and 115 nonliteral.12
The prediction is that, in good semantic models,
literal uses will in general result in a higher simi-
larity between the noun and color term vectors: A
white towel is white, while wine or musicians are
not white in the same manner. We test this prediction
by comparing the average cosine between the color
term and the nouns across the literal and nonliteral
pairs (similar results were obtained in an evaluation
in terms of prediction accuracy of a simple classi-
fier).
5.2 Results
Column E2 in Table 1 summarizes the results of
the experiment, reporting the mean difference be-
tween the normalized cosines (that is, how large
the difference is between the literal and nonliteral
uses of color terms), as well as the significance of
the differences according to a t-test. Window-based
models perform best among textual models, partic-
ularly Window20, while the rest can?t discriminate
between the two uses. This is particularly striking
for the Document model, which performs quite well
in general semantic tasks but bad in visual tasks.
Visual models are all able to discriminate between
the two uses, suggesting that indeed visual infor-
mation can capture nonliteral aspects of meaning.
However, in this case SIFT features perform much
better than LAB features, as Experiment 2 involves
12Dataset available upon request to the second author.
141
tackling much more sophisticated information than
Experiment 1. This is consistent with the fact that,
for LAB, a lower k (lower granularity of the in-
formation) performs better for Experiment 1 and a
higher k (higher granularity) for Experiment 2.
One crucial question to ask, given the goals of
our research, is whether textual and visual models
are doing essentially the same job, only using dif-
ferent types of information. Note that, in this case,
multimodal models increase performance over the
individual modalities, and are the best models for
this task. This suggests that the information used in
the individual models is complementary, and indeed
there is no correlation between the cosines obtained
with the best textual and visual models (Pearson?s
? = .09, p = .11).
Figure 2 depicts the results broken down by
color.13 Both modalities can capture the differ-
ences for black and green, probably because nonlit-
eral uses of these color terms have also clear textual
correlates (more concretely, topical correlates, as
they are related to race and ecology, respectively).14
Significantly, however, vision can capture nonliteral
uses of blue and red, while text can?t. Note that
these uses (blue note, shark, shield, red meat, dis-
trict, face) do not have a clear topical correlate, and
thus it makes sense that vision does a better job.
Finally, note that for this more sophisticated task,
hybrid models perform quite bad, which shows their
limitations as models of word meaning.15 Overall,
13Yellow and brown are excluded because the dataset contains
only one and two instances of nonliteral cases for these terms,
respectively. The significance of the differences as explained in
the text has been tested via t-tests.
14It?s not entirely clear why neither modality can capture
the differences for white; for text, it may be because the non-
literal cases are not so tied to race as is the cases for black,
but they also contain many other types of nonliteral uses, such
as type-referring (white wine/rice/cell) or metonymical ones
(white smile).
15The hybrid model that performs best in the color tasks is
ESP-Doc. This model can only detect a relation between an ad-
jective and a noun if they directly co-occur in the label of at least
one image (a ?document? in this setting). The more direct co-
occurrences there are, the more related the words will be for the
model. This works for Exp. 1: Since the ESP labels are lists of
what subjects saw in a picture, and the adjectives of Exp. 1 are
typical colors of objects, there is a high co-occurrence, as all but
one adjective-noun pairs co-occur in at least one ESP label. For
the model to perform well in Exp. 2 too, literal phrases should
occur in the same labels and non-literal pairs should not. We
our results suggest that co-occurrence in an image
label can be used as a surrogate of true visual infor-
mation to some extent, but the behavior of hybrid
models depends on ad-hoc aspects of the labeled
dataset, and, from an empirical perspective, they are
more limited than truly multimodal models, because
they require large amounts of rich verbal picture de-
scriptions to reach good coverage.
6 Related work
There is an increasing amount of work in com-
puter vision that exploits text-derived information
for image retrieval and annotation tasks (Farhadi
et al, 2010; Kulkarni et al, 2011). One particu-
lar techinque inspired by NLP that has acted as a
very effective proxy from CV to NLP is precisely
the BoVW. Recently, NLPers have begun exploit-
ing BoVW to enrich distributional models that rep-
resent word meaning with visual features automati-
cally extracted from images (Feng and Lapata, 2010;
Bruni et al, 2011; Leong and Mihalcea, 2011). Pre-
vious work in this area relied on SIFT features only,
whereas we have enriched the visual representation
of words with other kinds of features from computer
vision, namely, color-related features (LAB). More-
over, earlier evaluation of multimodal models has
focused only on standard word similarity tasks (us-
ing mainly WordSim353), whereas we have tested
them on both general semantic tasks and specific
tasks that tap directly into aspects of semantics (such
as color) where we expect visual information to be
crucial.
The most closely related work to ours is that re-
cently presented by O?zbal et al (2011). Like us,
O?zbal and colleagues use both a textual model and a
visual model (as well as Google adjective-noun co-
occurrence counts) to find the typical color of an ob-
ject. However, their visual model works by analyz-
ing pictures associated with an object, and determin-
ing the color of the object directly by image analysis.
We attempt the more ambitious goal of separately
associating a vector to nouns and adjectives, and de-
find no such difference (89% of adjective-noun pairs co-occur
in at least one image in the literal set, 86% in the nonliteral set),
because many of the relevant pairs describe concrete concepts
that, while not necessarily of the ?right? literal colour, are per-
fectly fit to be depicted in images (?blue shark?, ?black boy?,
?white wine?).
142
L N
0.05
0.10
0.15
0.20
0.25
0.30
Vision: black
l
ll
l
ll
l
L N0
.0
0.1
0.2
0.3
0.4
0.5
Text: black
L N
0.10
0.15
0.20
0.25
0.30
0.35
Vision: blue
l
l
L N0.
0
0.1
0.2
0.3
Text: blue
l
l
L N
0.05
0.15
0.25
Vision: green
l
l
l
L N0.0
0
0.04
0.08
0.12
Text: green
L N
0.05
0.10
0.15
0.20
0.25
0.30
Vision: red
l
l
l
L N0.0
0
0.10
0.20
0.30
Text: red
l
L N0
.05
0.10
0.15
0.20
0.25
0.30
Vision: white
l
ll
l
l
ll
L N0.
00
0.05
0.10
0.15
Text: white
Figure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models.
termining the color of an object by the nearness of
the noun denoting the object to the color term. In
other words, we are trying to model the meaning of
color terms and how they relate to other words, and
not to directly extract the color of an object from pic-
tures depicting them. Our second experiment is con-
nected to the literature on the automated detection of
figurative language (Shutova, 2010). There is in par-
ticular some similarity with the tasks studied by Tur-
ney et al (2011). Turney and colleagues try, among
other things, to distinguish literal and metaphorical
usages of adjectives when combined with nouns, in-
cluding the highly visual adjective dark (dark hair
vs. dark humour). Their method, based on automat-
ically quantifying the degree of abstractness of the
noun, is complementary to ours. Future work could
combine our approach and theirs.
7 Conclusion
We have presented evidence that distributional se-
mantic models based on text, while providing a
good general semantic representation of word mean-
ing, can be outperformed by models using visual
information for semantic aspects of words where
vision is relevant. More generally, this suggests
that computer vision is mature enough to signifi-
cantly contribute to perceptually grounded compu-
tational models of language. We have also shown
that different types of visual features (LAB, SIFT)
are appropriate for different tasks. Future research
should investigate automated methods to discover
which (if any) kind of visual information should be
highlighted in which task, more sophisticated mul-
timodal models, visual properties other than color,
and larger color datasets, such as the one recently
introduced by Mohammad (2011).
Acknowledgments
E.B. and M.B. are partially supported by a Google
Research Award. G.B. is partially supported
by the Spanish Ministry of Science and Innova-
tion (FFI2010-15006, TIN2009-14715-C04-04), the
EU PASCAL2 Network of Excellence (FP7-ICT-
216886) and the AGAUR (2010 BP-A 00070). The
E2 evaluation set was created by G.B. with Louise
McNally and Eva Maria Vecchi. Fig. 1 was adapted
from a figure by Jasper Uijlings. G. B. thanks Mar-
garita Torrent for taking care of her children while
she worked hard to meet the Sunday deadline.
References
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
143
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Shane Bergsma and Randy Goebel. 2011. Using visual
information to predict lexical preference. In Proceed-
ings of Recent Advances in Natural Language Process-
ing, pages 399?405, Hissar.
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing bilingual lexicons using the visual similarity of la-
beled web images. In Proc. IJCAI, pages 1764?1769,
Barcelona, Spain, July.
Brent Berlin and Paul Key. 1969. Basic Color Terms:
Their Universality and Evolution. University of Cali-
fornia Press, Berkeley, CA.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image Classification using Random Forests and
Ferns. In Computer Vision, 2007. ICCV 2007. IEEE
11th International Conference on, pages 1?8.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the EMNLP GEMS Workshop, pages 22?
32, Edinburgh.
John Canny. 1986. A computational approach to edge
detection. IEEE Trans. Pattern Anal. Mach. Intell,
36(4):679?698.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Mark D. Fairchild. 2005. Status of cie color appearance
models.
A. Farhadi, M. Hejrati, M. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In Proceedings of ECCV.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL, pages 91?99, Los Angeles, CA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Kristen Grauman and Trevor Darrell. 2005. The pyramid
match kernel: Discriminative classification with sets
of image features. In In ICCV, pages 1458?1465.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. Berg,
and T. Berg. 2011. Baby talk: Understanding and
generating simple image descriptions. In Proceedings
of CVPR.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In
Proceedings of the 2006 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition
- Volume 2, CVPR 2006, pages 2169?2178, Washing-
ton, DC, USA. IEEE Computer Society.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407, Chiang Mai, Thailand.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David Lowe. 1999. Object Recognition from Local
Scale-Invariant Features. Computer Vision, IEEE In-
ternational Conference on, 2:1150?1157 vol.2, Au-
gust.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2), November.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Saif Mohammad. 2011. Colourful language: Measuring
word-colour associations. In Proceedings of the 2nd
Workshop on Cognitive Modeling and Computational
Linguistics, pages 97?106, Portland, Oregon.
Raymond J. Mooney. 2008. Learning to connect lan-
guage and perception.
David Nister and Henrik Stewenius. 2006. Scalable
recognition with a vocabulary tree. In Proceedings
of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition - Volume 2,
CVPR ?06, pages 2161?2168.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. Int. J. Comput. Vision, 42:145?175.
Go?zde O?zbal, Carlo Strapparava, Rada Mihalcea, and
Daniele Pighin. 2011. A comparison of unsupervised
methods to associate colors with words. In Proceed-
ings of ACII, pages 42?51, Memphis, TN.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of ACL, pages 688?697, Uppsala, Swe-
den.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of the International Conference on
Computer Vision, volume 2, pages 1470?1477, Octo-
ber.
144
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer-Verlag New York Inc.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identifi-
cation through concrete and abstract context. In Pro-
ceedings of EMNLP, pages 680?690, Edinburgh, UK.
Andrea Vedaldi and Brian Fulkerson. 2008. VLFeat:
An open and portable library of computer vision algo-
rithms. http://www.vlfeat.org/.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems, pages 319?326, Vienna, Austria.
Jun Yang, Yu-Gang Jiang, Alexander G. Hauptmann, and
Chong-Wah Ngo. 2007. Evaluating bag-of-visual-
words representations in scene classification. In Mul-
timedia Information Retrieval, pages 197?206.
Song Chun Zhu, Cheng en Guo, Ying Nian Wu, and
Yizhou Wang. 2002. What are textons? In Computer
Vision - ECCV 2002, 7th European Conference on
Computer Vision, Copenhagen, Denmark, May 28-31,
2002, Proceedings, Part IV, pages 793?807. Springer.
145
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1517?1526,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Compositional-ly Derived Representations of
Morphologically Complex Words in Distributional Semantics
Angeliki Lazaridou and Marco Marelli and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Speakers of a language can construct an
unlimited number of new words through
morphological derivation. This is a major
cause of data sparseness for corpus-based
approaches to lexical semantics, such as
distributional semantic models of word
meaning. We adapt compositional meth-
ods originally developed for phrases to the
task of deriving the distributional meaning
of morphologically complex words from
their parts. Semantic representations con-
structed in this way beat a strong baseline
and can be of higher quality than represen-
tations directly constructed from corpus
data. Our results constitute a novel evalua-
tion of the proposed composition methods,
in which the full additive model achieves
the best performance, and demonstrate the
usefulness of a compositional morphology
component in distributional semantics.
1 Introduction
Effective ways to represent word meaning are
needed in many branches of natural language pro-
cessing. In the last decades, corpus-based meth-
ods have achieved some degree of success in mod-
eling lexical semantics. Distributional semantic
models (DSMs) in particular represent the mean-
ing of a word by a vector, the dimensions of which
encode corpus-extracted co-occurrence statistics,
under the assumption that words that are semanti-
cally similar will occur in similar contexts (Turney
and Pantel, 2010). Reliable distributional vectors
can only be extracted for words that occur in many
contexts in the corpus. Not surprisingly, there is
a strong correlation between word frequency and
vector quality (Bullinaria and Levy, 2007), and
since most words occur only once even in very
large corpora (Baroni, 2009), DSMs suffer data
sparseness.
While word rarity has many sources, one of the
most common and systematic ones is the high pro-
ductivity of morphological derivation processes,
whereby an unlimited number of new words can
be constructed by adding affixes to existing stems
(Baayen, 2005; Bauer, 2001; Plag, 1999).1 For
example, in the multi-billion-word corpus we in-
troduce below, perfectly reasonable derived forms
such as lexicalizable or affixless never occur. Even
without considering the theoretically infinite num-
ber of possible derived nonce words, and restrict-
ing ourselves instead to words that are already
listed in dictionaries, complex forms cover a high
portion of the lexicon. For example, morphologi-
cally complex forms account for 55% of the lem-
mas in the CELEX English database (see Section
4.1 below). In most of these cases (80% according
to our corpus) the stem is more frequent than the
complex form (e.g., the stem build occurs 15 times
more often than the derived form rebuild, and the
latter is certainly not an unusual derived form).
DSMs ignore derivational morphology alto-
gether. Consequently, they cannot provide mean-
ing representations for new derived forms, nor can
they harness the systematic relation existing be-
tween stems and derivations (any English speaker
can infer that to rebuild is to build again, whether
they are familiar with the prefixed form or not)
in order to mitigate derived-form sparseness prob-
lems. A simple way to handle derivational mor-
1Morphological derivation constructs new words (in
the sense of lemmas) from existing lexical items (re-
source+ful?resourceful). In this work, we do not treat in-
flectional morphology, pertaining to affixes that encode gram-
matical features such as number or tense (dog+s). We use
morpheme for any component of a word (resource and -ful
are both morphemes). We use stem for the lexical item that
constitutes the base of derivation (resource) and affix (pre-
fix or suffix) for the element attached to the stem to derive
the new form (-ful). In English, stems are typically indepen-
dent words, affixes bound morphemes, i.e., they cannot stand
alone. Note that a stem can in turn be morphologically de-
rived, e.g., point+less in pointless+ly. Finally, we use mor-
phologically complex as synonymous with derived.
1517
phology would be to identify the stem of rare de-
rived words and use its distributional vector as a
proxy to derived-form meaning.2 The meaning of
rebuild is not that far from that of build, so the
latter might provide a reasonable surrogate. Still,
something is clearly lost (if the author of a text
felt the need to use the derived form, the stem was
not fully appropriate), and sometimes the jump in
meaning can be quite dramatic (resourceless and
resource mean very different things!).
In the past few years there has been much in-
terest in how DSMs can scale up to represent the
meaning of larger chunks of text such as phrases
or even sentences. Trying to represent the mean-
ing of arbitrarily long constructions by directly
collecting co-occurrence statistics is obviously in-
effective and thus methods have been developed
to derive the meaning of larger constructions as a
function of the meaning of their constituents (Ba-
roni and Zamparelli, 2010; Coecke et al, 2010;
Mitchell and Lapata, 2008; Mitchell and Lapata,
2010; Socher et al, 2012). Compositional distri-
butional semantic models (cDSMs) of word units
aim at handling, compositionally, the high produc-
tivity of phrases and consequent data sparseness.
It is natural to hypothesize that the same methods
can be applied to morphology to derive the mean-
ing of complex words from the meaning of their
parts: For example, instead of harvesting a rebuild
vector directly from the corpus, the latter could be
constructed from the distributional representations
of re- and build. Besides alleviating data sparse-
ness problems, a system of this sort, that automati-
cally induces the semantic contents of morpholog-
ical processes, would also be of tremendous theo-
retical interest, given that the semantics of deriva-
tion is a central and challenging topic in linguistic
morphology (Dowty, 1979; Lieber, 2004).
In this paper, we explore, for the first time (ex-
cept for the proof-of-concept study in Guevara
(2009)), the application of cDSMs to derivational
morphology. We adapt a number of composition
methods from the literature to the morphological
setting, and we show that some of these methods
can provide better distributional representations of
derived forms than either those directly harvested
from a large corpus, or those obtained by using
the stem as a proxy to derived-form meaning. Our
2Of course, spotting and segmenting complex words is a
big research topic unto itself (Beesley and Karttunen, 2000;
Black et al, 1991; Sproat, 1992), and one we completely
sidestep here.
results suggest that exploiting morphology could
improve the quality of DSMs in general, extend
the range of tasks that cDSMs can successfully
model and support the development of new ways
to test their performance.
2 Related work
Morphological induction systems use corpus-
based methods to decide if two words are mor-
phologically related and/or to segment words into
morphemes (Dreyer and Eisner, 2011; Goldsmith,
2001; Goldwater and McClosky, 2005; Goldwater,
2006; Naradowsky and Goldwater, 2009; Wicen-
towski, 2004). Morphological induction has re-
cently received considerable attention since mor-
phological analysis can mitigate data sparseness in
domains such as parsing and machine translation
(Goldberg and Tsarfaty, 2008; Lee, 2004). Among
the cues that have been exploited there is distri-
butional similarity among morphologically related
words (Schone and Jurafsky, 2000; Yarowsky and
Wicentowski, 2000). Our work, however, dif-
fers substantially from this track of research. We
do not aim at segmenting morphological complex
words or identifying paradigms. Our goal is to
automatically construct, given distributional rep-
resentations of stems and affixes, semantic repre-
sentations for the derived words containing those
stems and affixes. A morphological induction sys-
tem, given rebuild, will segment it into re- and
build (possibly using distributional similarity be-
tween the words as a cue). Our system, given
re- and build, predicts the (distributional seman-
tic) meaning of rebuild.
Another emerging line of research uses distribu-
tional semantics to model human intuitions about
the semantic transparency of morphologically de-
rived or compound expressions and how these im-
pact various lexical processing tasks (Kuperman,
2009; Wang et al, 2012). Although these works
exploit vectors representing complex forms, they
do not attempt to generate them compositionally.
The only similar study we are aware of is that
of Guevara (2009). Guevara found a systematic
geometric relation between corpus-based vectors
of derived forms sharing an affix and their stems,
and used this finding to motivate the composition
method we term lexfunc below. However, unlike
us, he did not test alternative models, and he only
presented a qualitative analysis of the trajectories
triggered by composition with various affixes.
1518
3 Composition methods
Distributional semantic models (DSMs), also
known as vector-space models, semantic spaces,
or by the names of famous incarnations such as
Latent Semantic Analysis or Topic Models, ap-
proximate the meaning of words with vectors that
record their patterns of co-occurrence with cor-
pus context features (often, other words). There
is an extensive literature on how to develop such
models and on their evaluation. Recent surveys
include Clark (2012), Erk (2012) and Turney and
Pantel (2010). We focus here on compositional
DSMs (cDSMs). Since the very inception of dis-
tributional semantics, there have been attempts to
compose meanings for sentences and larger pas-
sages (Landauer and Dumais, 1997), but inter-
est in compositional DSMs has skyrocketed in
the last few years, particularly since the influen-
tial work of Mitchell and Lapata (2008; 2009;
2010). For the current study, we have reimple-
mented and adapted to the morphological setting
all cDSMs we are aware of, excluding the tensor-
product-based models that Mitchell and Lapata
(2010) have shown to be empirically disappointing
and the models of Socher and colleagues (Socher
et al, 2011; Socher et al, 2012), that require com-
plex optimization procedures whose adaptation to
morphology we leave to future work.
Mitchell and Lapata proposed a set of simple
and effective models in which the composed vec-
tors are obtained through component-wise opera-
tions on the constituent vectors. Given input vec-
tors u and v, the multiplicative model (mult) re-
turns a composed vector c with: ci = uivi. In the
weighted additive model (wadd), the composed
vector is a weighted sum of the two input vectors:
c = ?u + ?v, where ? and ? are two scalars. In
the dilation model, the output vector is obtained
by first decomposing one of the input vectors, say
v, into a vector parallel to u and an orthogonal
vector. Following this, the parallel vector is dilated
by a factor ? before re-combining. This results in:
c = (?? 1)?u,v?u+ ?u,u?v.
Guevara (2010) and Zanzotto et al (2010) pro-
pose the full additive model (fulladd), where the
two vectors to be added are pre-multiplied by
weight matrices: c = Au+Bv
Since the Mitchell and Lapata and fulladd mod-
els were developed for phrase composition, the
two input vectors were taken to be, very straight-
forwardly, the vectors of the two words to be com-
posed into the phrase of interest. In morphological
derivation, at least one of the items to be composed
(the affix) is a bound morpheme. In our adapta-
tion of these composition models, we build bound
morpheme vectors by accumulating the contexts
in which a set of derived words containing the rel-
evant morphemes occur, e.g., the re- vector aggre-
gates co-occurrences of redo, remake, retry, etc.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application, where the distributional representa-
tion of one element in a composition (the func-
tor) is not a vector but a function. Given that
linear functions can be expressed by matrices and
their application by matrix-by-vector multiplica-
tion, in this lexical function (lexfunc) model, the
functor is represented by a matrix U to be multi-
plied with the argument vector v: c = Uv. In
the case of morphology, it is natural to treat bound
affixes as functions over stems, since affixes en-
code the systematic semantic patterns we intend
to capture. Unlike the other composition meth-
ods, lexfunc does not require the construction of
distributional vectors for affixes. A matrix repre-
sentation for every affix is instead induced directly
from examples of stems and the corresponding de-
rived forms, in line with the intuition that every af-
fix corresponds to a different pattern of change of
the stem meaning.
Finally, as already discussed in the Introduc-
tion, performing no composition at all but using
the stem vector as a surrogate of the derived form
is a reasonable strategy. We saw that morphologi-
cally derived words tend to appear less frequently
than their stems, and in many cases the meanings
are close. Consequently, we expect a stem-only
?composition? method to be a strong baseline in
the morphological setting.
4 Experimental setup
4.1 Morphological data
We obtained a list of stem/derived-form pairs from
the CELEX English Lexical Database, a widely
used 100K-lemma lexicon containing, among
other things, information about the derivational
structure of words (Baayen et al, 1995). For each
derivational affix present in CELEX, we extracted
from the database the full list of stem/derived
pairs matching its most common part-of-speech
signature (e.g., for -er we only considered pairs
1519
Affix Stem/Der. Training HQ/Tot. Avg.
POS Items Test Items SDR
-able verb/adj 177 30/50 5.96
-al noun/adj 245 41/50 5.88
-er verb/noun 824 33/50 5.51
-ful noun/adj 53 42/50 6.11
-ic noun/adj 280 43/50 5.99
-ion verb/noun 637 38/50 6.22
-ist noun/noun 244 38/50 6.16
-ity adj/noun 372 33/50 6.19
-ize noun/verb 105 40/50 5.96
-less noun/adj 122 35/50 3.72
-ly adj/adv 1847 20/50 6.33
-ment verb/noun 165 38/50 6.06
-ness adj/noun 602 33/50 6.29
-ous noun/adj 157 35/50 5.94
-y noun/adj 404 27/50 5.25
in- adj/adj 101 34/50 3.39
re- verb/verb 86 27/50 5.28
un- adj/adj 128 36/50 3.23
tot */* 6549 623/900 5.52
Table 1: Derivational morphology dataset
having a verbal stem and nominal derived form).
Since CELEX was populated by semi-automated
morphological analysis, it includes forms that are
probably not synchronically related to their stems,
such as crypt+ic or re+form. However, we did not
manually intervene on the pairs, since we are in-
terested in training and testing our methods in re-
alistic, noisy conditions. In particular, the need to
pre-process corpora to determine which forms are
?opaque?, and should thus be bypassed by our sys-
tems, would greatly reduce their usefulness. Pairs
in which either word occurred less than 20 times
in our source corpus (described in Section 4.2 be-
low) were filtered out and, in our final dataset, we
only considered the 18 affixes (3 prefixes and 15
suffixes) with at least 100 pairs meeting this con-
dition. We randomly chose 50 stem/derived pairs
(900 in total) as test data. The remaining data were
used as training items to estimate the parameters
of the composition methods. Table 1 summarizes
various characteristics of the dataset3 (the last two
columns of the table are explained in the next para-
graphs).
Annotation of quality of test vectors The qual-
ity of the corpus-based vectors representing de-
rived test items was determined by collecting hu-
man semantic similarity judgments in a crowd-
sourcing survey. In particular, we use the similar-
ity of a vector to its nearest neighbors (NNs) as a
proxy measure of quality. The underlying assump-
3Available from http://clic.cimec.unitn.it/
composes
tion is that a vector, in order to be a good represen-
tation of the meaning of the corresponding word,
should lie in a region of semantic space populated
by intuitively similar meanings, e.g., we are more
likely to have captured the meaning of car if the
NN of its vector is the automobile vector rather
than potato. Therefore, to measure the quality of
a given vector, we can look at the average simi-
larity score provided by humans when comparing
this very vector with its own NNs.
All 900 derived vectors from the test set were
matched with their three closest NNs in our se-
mantic space (see Section 4.2), thus producing a
set of 2, 700 word pairs. These pairs were admin-
istered to CrowdFlower users,4 who were asked
to judge the relatedness of the two meanings on a
7-point scale (higher for more related). In order
to ensure that participants were committed to the
task and exclude non-proficient English speakers,
we used 60 control pairs as gold standard, consist-
ing of either perfect synonyms or completely un-
related words. We obtained 30 judgments for each
derived form (10 judgments for each of 3 neighbor
comparisons), with mean participant agreement of
58%. These ratings were averaged item-wise, re-
sulting in a Gaussian distribution with a mean of
3.79 and a standard deviation of 1.31. Finally,
each test item was marked as high-quality (HQ)
if its derived form received an average score of at
least 3, as low-quality (LQ) otherwise. Table 1 re-
ports the proportion of HQ test items for each af-
fix, and Table 2 reports some examples of HQ and
LQ items with the corresponding NNs. It is worth
observing that the NNs of the LQ items, while not
as relevant as the HQ ones, are hardly random.
Annotation of similarity between stem and de-
rived forms Derived forms differ in terms of
how far their meaning is with respect to that of
their stem. Certain morphological processes have
systematically more impact than others on mean-
ing: For example, the adjectival prefix in- negates
the meaning of the stem, whereas -ly has the sole
function to convert an adjective into an adverb.
But the very same affix can affect different stems
in different ways. For example, remelt means lit-
tle more than to melt again, but rethink has subtler
implications of changing one?s way to look at a
problem, and while one of the senses of cycling is
present in recycle, it takes some effort to see their
relation.
4http://www.crowdflower.com
1520
Affix Type Derived form Neighbors
-ist HQ transcendentalist mythologist, futurist, theosophistLQ florist Harrod, wholesaler, stockist
-ity HQ publicity publicise, press, publicizeLQ sparsity dissimilarity, contiguity, perceptibility
-ment HQ advertisement advert, promotional, advertisingLQ inducement litigant, contractually, voluntarily
in- HQ inaccurate misleading, incorrect, erroneousLQ inoperable metastasis, colorectal, biopsy
re- HQ recapture retake, besiege, captureLQ rename defunct, officially, merge
Table 2: Examples of HQ and LQ derived vectors with their NNs
We conducted a separate crowdsourcing study
where participants were asked to rate the 900
test stem/derived pairs for the strength of their
semantic relationship on a 7-point scale. We
followed a procedure similar to the one de-
scribed for quality measurement; 7 judgments
were collected for each pair. Participants? agree-
ment was at 60%. The last column of Ta-
ble 1 reports the average stem/derived related-
ness (SDR) for the various affixes. Note that
the affixes with systematically lower SDR are
those carrying a negative meaning (in-, un-, -less),
whereas those with highest SDR do little more
than changing the POS of the stem (-ion, -ly, -
ness). Among specific pairs with very low related-
ness we encounter hand/handy, bear/bearable and
active/activist, whereas compulsory/compulsorily,
shameless/shamelessness and chaos/chaotic have
high SDR. Since the distribution of the average
ratings was negatively skewed (mean rating: 5.52,
standard deviation: 1.26),5 we took 5 as the rating
threshold to classify items as having high (HR) or
low (LR) relatedness to their stems.
4.2 Distributional semantic space6
We use as our source corpus the concatenation of
ukWaC, the English Wikipedia (2009 dump) and
the BNC,7 for a total of about 2.8 billion tokens.
We collect co-occurrence statistics for the top 20K
content words (adjectives, adverbs, nouns, verbs)
5The negative skew is not surprising, as derived forms
must have some relation to their stems!
6Most steps of the semantic space construction
and composition pipelines were implemented using
the DISSECT toolkit: https://github.com/
composes-toolkit/dissect.
7http://wacky.sslmit.unibo.it, http:
//en.wikipedia.org, http://www.natcorp.
ox.ac.uk
in lemma format, plus any item from the mor-
phological dataset described above that was below
this rank. The top 20K content words also con-
stitute our context elements. We use a standard
bag-of-words approach, counting collocates in a
narrow 2-word before-and-after window. We ap-
ply (non-negative) Pointwise Mutual Information
as weighting scheme and dimensionality reduc-
tion by Non-negative Matrix Factorization, setting
the number of reduced-space dimensions to 350.
These settings are chosen without tuning, and are
based on previous experiments where they pro-
duced high-quality semantic spaces (Boleda et al,
2013; Bullinaria and Levy, 2007).
4.3 Implementation of composition methods
All composition methods except mult and stem
have weights to be estimated (e.g., the ? parame-
ter of dilation or the affix matrices of lexfunc). We
adopt the estimation strategy proposed by Gue-
vara (2010) and Baroni and Zamparelli (2010),
namely we pick parameter values that optimize
the mapping between stem and derived vectors di-
rectly extracted from the corpus. To learn, say, a
lexfunc matrix representing the prefix re-, we ex-
tract vectors of V/reV pairs that occur with suffi-
cient frequency (visit/revisit, think/rethink. . . ). We
then use least-squares methods to find weights for
the re- matrix that minimize the distance between
each reV vector generated by the model given the
input V and the corresponding corpus-observed
derived vector (e.g., we try to make the model-
predicted re+visit vector as similar as possible
to the corpus-extracted one). This is a general
estimation approach that does not require task-
specific hand-labeled data, and for which simple
analytical solutions of the least-squares error prob-
1521
lem exist for all our composition methods. We use
only the training items from Section 4.1 for esti-
mation. Note that, unlike the test items, these have
not been annotated for quality, so we are adopting
an unsupervised (no manual labeling) but noisy es-
timation method.8
For the lexfunc model, we use the training items
separately to obtain weight matrices represent-
ing each affix, whereas for the other models all
training data are used together to globally de-
rive single sets of affix and stem weights. For
the wadd model, the learning process results in
0.16?affix+0.33? stem, i.e., the affix contributes
only half of its mass to the composition of the
derived form. For dilation, we stretch the stem
(i.e., v of the dilation equation is the stem vector),
since it should provide richer contents than the af-
fix to the derived meaning. We found that, on av-
erage across the training pairs, dilation weighted
the stem 20 times more heavily than the affix
(0.05?affix+1?stem). We then expect that the di-
lation model will have similar performance to the
baseline stem model, as confirmed below.9
For all methods, vectors were normalized be-
fore composing both in training and in generation.
5 Experiment 1: approximating
high-quality corpus-extracted vectors
The first experiment investigates to what extent
composition models can approximate high-quality
(HQ) corpus-extracted vectors representing de-
rived forms. Note that since the test items were
excluded from training, we are simulating a sce-
nario in which composition models must generate
representations for nonce derived forms.
Cosine similarity between model-generated and
corpus-extracted vectors were computed for all
models, including the stem baseline (i.e., co-
sine between stem and derived form). The first
row of Table 3 reports mean similarities. The
stem method sets the level of performance rel-
atively high, confirming its soundness. Indeed,
the parameter-free mult model performs below the
baseline.10 As expected, dilation performs simi-
8More accurately, we relied on semi-manual CELEX in-
formation to identify derived forms. A further step towards a
fully knowledge-free system would be to pre-process the cor-
pus with an unsupervised morphological induction system to
extract stem/derived pairs.
9The other models have thousands of weights to be es-
timated, so we cannot summarize the outcome of parameter
estimation here.
10This result does not necessarily contradict those of
stem mult dil. wadd fulladd lexfunc
All 0.47 0.39 0.48 0.50 0.56 0.54
HR 0.52 0.43 0.53 0.55 0.61 0.58
LR 0.32 0.28 0.33 0.38 0.41 0.42
Table 3: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors, for all as well as high- (HR) and low-
relatedness (LR) test items
larly to the baseline, while wadd outperforms it,
although the effect does not reach significance
(p=.06).11 Both fulladd and lexfunc perform sig-
nificantly better than stem (p < .001). Lexfunc
provides a flexible way to account for affixation,
since it models it directly as a function mapping
from and onto word vectors, without requiring a
vector representation of bound affixes. The rea-
son at the base of its good performance is thus
quite straightforward. On the other hand, it is
surprising that a simple representation of bound
affixes (i.e., as vectors aggregating the contexts
of words containing them) can work so well, at
least when used in conjunction with the granular
dimension-by-dimension weights assigned by the
fulladd method. We hypothesize that these aggre-
gated contexts, by providing information about the
set of stems an affix combines with, capture the
shared semantic features that the affix operates on.
When the meaning of the derived form is far
from that of its stem, the stem baseline should no
longer constitute a suitable surrogate of derived-
form meaning. The LR cases (see Section 4.1
above) are thus crucial to understand how well
composition methods capture not only stem mean-
ing, but also affix-triggered semantics. The HR
and LR rows of Table 3 present the results for
the respective test subsets. As expected, the stem
approach undergoes a strong drop when perfor-
mance is measured on LR items. At the other ex-
treme, fulladd and lexfunc, while also finding the
LR cases more difficult, still clearly outperform
the baseline (p<.001), confirming that they cap-
ture the meaning of derived forms beyond what
their stems contribute to it. The effect of wadd,
again, approaches significance when compared to
the baseline (p= .05). Very encouragingly, both
Mitchell and Lapata and others who found mult to be highly
competitive. Due to differences in co-occurrence weighting
schemes (we use a logarithmically scaled measure, they do
not), their multiplicative model is closer to our additive one.
11Significance assessed by means of Tukey Honestly Sig-
nificant Difference tests (Abdi and Williams, 2010)
1522
stem mult wadd dil. fulladd lexfunc
-less 0.22 0.23 0.30 0.24 0.38 0.44
in- 0.39 0.34 0.45 0.40 0.47 0.45
un- 0.33 0.33 0.41 0.34 0.44 0.46
Table 4: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors with negative affixes
fulladd and lexfunc significantly outperform stem
also in the HR subset (p<.001). That is, the mod-
els provide better approximations of derived forms
even when the stem itself should already be a good
surrogate. The difference between the two models
is not significant.
We noted in Section 4.1 that forms containing
the ?negative? affixes -less, un- and in- received
on average low SDR scores, since negation im-
pacts meaning more drastically than other opera-
tions. Table 4 reports the performance of the mod-
els on these affixes. Indeed, the stem baseline per-
forms quite poorly, whereas fulladd, lexfunc and,
to a lesser extent, wadd are quite effective in this
condition as well, all performing greatly above the
baseline. These results are intriguing in light of
the fact that modeling negation is a challenging
task for DSMs (Mohammad et al, 2013) as well as
cDSMs (Preller and Sadrzadeh, 2011). To the ex-
tent that our best methods have captured the negat-
ing function of a prefix such as in-, they might be
applied to tasks such as recognizing lexical op-
posites, or even simple forms of syntactic nega-
tion (modeling inoperable is just a short step away
from modeling not operable compositionally).
6 Experiment 2: Comparing the quality
of corpus-extracted and
compositionally generated words
The first experiment simulated the scenario in
which derived forms are not in our corpus, so
that directly extracting their representation from
it is not an option. The second experiment tests
if compositionally-derived representations can be
better than those extracted directly from the corpus
when the latter is a possible strategy (i.e., the de-
rived forms are attested in the source corpus). To
this purpose, we focused on those 277 test items
that were judged as low-quality (LQ, see Section
4.1), which are presumably more challenging to
generate, and where the compositional route could
be most useful.
We evaluated the derived forms generated by
corpus stem wadd fulladd lexfunc
All 2.28 3.26 4.12 3.99 3.09
HR 2.29 3.56 4.48 4.31 3.31
LR 2.22 2.48 3.14 3.12 2.52
Table 5: Average quality ratings of derived vectors
Target Model Neighbors
florist
wadd flora, fauna, ecosystem
fulladd flora, fauna, egologist
lexfunc ornithologist, naturalist, botanist
sparsity
wadd sparse, sparsely, dense
fulladd sparse, sparseness, angularity
lexfunc fragility, angularity, smallness
inducement
wadd induce, inhibit, inhibition
fulladd induce, inhibition, mediate
lexfunc impairment, cerebral, ocular
inoperable
wadd operable, palliation, biopsy
fulladd operable, inoperative, ventilator
lexfunc inoperative, unavoidably, flaw
rename
wadd name, later, namesake
fulladd name, namesake, later
lexfunc temporarily, reinstate, thereafter
Table 6: Examples of model-predicted neighbors
for words with LQ corpus-extracted vectors
the models that performed best in the first exper-
iment (fulladd, lexfunc and wadd), as well as the
stem baseline, by means of another crowdsourcing
study. We followed the same procedure used to
assess the quality of corpus-extracted vectors, that
is, we asked judges to rate the relatedness of the
target forms to their NNs (we obtained on average
29 responses per form).
The first line of Table 5 reports the average
quality (on a 7-point scale) of the representations
of the derived forms as produced by the models
and baseline, as well as of the corpus-harvested
ones (corpus column). All compositional models
produce representations that are of significantly
higher quality (p < .001) than the corpus-based
ones. The effect is also evident in qualitative
terms. Table 6 presents the NNs predicted by the
three compositional methods for the same LQ test
items whose corpus-based NNs are presented in
Table 2. These results indicate that morpheme
composition is an effective solution when the qual-
ity of corpus-extracted derived forms is low (and
the previous experiment showed that, when their
quality is high, composition can at least approxi-
mate corpus-based vectors).
With respect to Experiment 1, we obtain a dif-
ferent ranking of the models, with lexfunc being
outperformed by both wadd and fulladd (p<.001),
that are statistically indistinguishable. The wadd
1523
composition is dominated by the stem, and by
looking at the examples in Table 6 we notice that
both this model and fulladd tend to feature the
stem as NN (100% of the cases for wadd, 73%
for fulladd in the complete test set). The question
thus arises as to whether the good performance of
these composition techniques is simply due to the
fact that they produce derived forms that are near
their stems, with no added semantic value from the
affix (a ?stemploitation? strategy).
However, the stemploitation hypothesis is dis-
pelled by the observation that both models signifi-
cantly outperform the stem baseline (p<.001), de-
spite the fact that the latter, again, has good per-
formance, significantly outperforming the corpus-
derived vectors (p < .001). Thus, we confirm
that compositional models provide higher qual-
ity vectors that are capturing the meaning of de-
rived forms beyond the information provided by
the stem.
Indeed, if we focus on the third row of Ta-
ble 5, reporting performance on low stem-derived
relatedness (LR) items (annotated as described in
Section 4.1), fulladd and wadd still significantly
outperform the corpus representations (p<.001),
whereas the quality of the stem representations of
LR items is not significantly different form that of
the corpus-derived ones. Interestingly, lexfunc dis-
plays the smallest drop in performance when re-
stricting evaluation to LR items; however, since it
does not significantly outperform the LQ corpus
representations, this is arguably due to a floor ef-
fect.
7 Conclusion and future work
We investigated to what extent cDSMs can gener-
ate effective meaning representations of complex
words through morpheme composition. Several
state-of-the-art composition models were adapted
and evaluated on this novel task. Our results sug-
gest that morpheme composition can indeed pro-
vide high-quality vectors for complex forms, im-
proving both on vectors directly extracted from the
corpus and on a stem-backoff strategy. This re-
sult is of practical importance for distributional se-
mantics, as it paves the way to address one of the
main causes of data sparseness, and it confirms the
usefulness of the compositional approach in a new
domain. Overall, fulladd emerged as the best per-
forming model, with both lexfunc and the simple
wadd approach constituting strong rivals. The ef-
fectiveness of the best models extended also to the
challenging cases where the meaning of derived
forms is far from that of the stem, including nega-
tive affixes.
The fulladd method requires a vector represen-
tation for bound morphemes. A first direction for
future work will thus be to investigate which as-
pects of the meaning of bound morphemes are
captured by our current simple-minded approach
to populating their vectors, and to explore alterna-
tive ways to construct them, seeing if they further
improve fulladd performance.
A natural extension of our research is to ad-
dress morpheme composition and morphological
induction jointly, trying to model the intuition that
good candidate morphemes should have coherent
semantic representations. Relatedly, in the cur-
rent setting we generate complex forms from their
parts. We want to investigate the inverse route,
namely ?de-composing? complex words to de-
rive representations of their stems, especially for
cases where the complex words are more frequent
(e.g. comfort/comfortable).
We would also like to apply composition to in-
flectional morphology (that currently lies outside
the scope of distributional semantics), to capture
the nuances of meaning that, for example, distin-
guish singular and plural nouns (consider, e.g., the
difference between the mass singular tea and the
plural teas, which coerces the noun into a count
interpretation (Katz and Zamparelli, 2012)).
Finally, in our current setup we focus on a single
composition step, e.g., we derive the meaning of
inoperable by composing the morphemes in- and
operable. But operable is in turn composed of op-
erate and -able. In the future, we will explore re-
cursive morpheme composition, especially since
we would like to apply these methods to more
complex morphological systems (e.g., agglutina-
tive languages) where multiple morphemes are the
norm.
8 Acknowledgments
We thank Georgiana Dinu and Nghia The Pham
for helping out with DISSECT-ion and the review-
ers for helpful feedback. This research was sup-
ported by the ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES).
1524
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database (re-
lease 2). CD-ROM, Linguistic Data Consortium,
Philadelphia, PA.
Harald Baayen. 2005. Morphological productivity. In
Rajmund Piotrowski Reinhard Ko?hler, Gabriel Alt-
mann, editor, Quantitative Linguistics: An Inter-
national Handbook, pages 243?256. Mouton de
Gruyter, Berlin, Germany.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni. 2009. Distributions in text. In Anke
Lu?deling and Merja Kyto?, editors, Corpus Linguis-
tics: An International Handbook, volume 2, pages
803?821. Mouton de Gruyter, Berlin, Germany.
Laurie Bauer. 2001. Morphological Productivity.
Cambridge University Press, Cambridge, UK.
Kenneth Beesley and Lauri Karttunen. 2000. Finite-
State Morphology: Xerox Tools and Techniques.
Cambridge University Press, Cambridge, UK.
Alan Black, Stephen Pulman, Graeme Ritchie, and
Graham Russell. 1991. Computational Morphol-
ogy. MIT Press, Cambrdige, MA.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46,
Potsdam, Germany.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
David Dowty. 1979. Word Meaning and Montague
Grammar. Springer, New York.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of EMNLP, pages 616?627, Edinburgh, UK.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL, pages
371?379, Columbus, OH.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 2(27):153?198.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of EMNLP, pages 676?683,
Vancouver, Canada.
Sharon Goldwater. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Emiliano Guevara. 2009. Compositionality in distribu-
tional semantics: Derivational affixes. In Proceed-
ings of the Words in Action Workshop, Pisa, Italy.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Graham Katz and Roberto Zamparelli. 2012. Quanti-
fying count/mass elasticity. In Proceedings of WC-
CFL, pages 371?379, Tucson, AR.
Victor Kuperman. 2009. Semantic transparency revis-
ited. Presentation at the 6th International Morpho-
logical Processing Conference.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, pages 57?60, Boston, MA.
Rochelle Lieber. 2004. Morphology and Lexical Se-
mantics. Cambridge University Press, Cambridge,
UK.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
1525
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics. In press.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of IJCAI, pages 11?17,
Pasadena, CA.
Ingo Plag. 1999. Morphological Productivity: Struc-
tural Constraints in English Derivation. Mouton de
Gruyter, Berlin, Germany.
Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell
states and negative sentences in the distributed
model of meaning. Electr. Notes Theor. Comput.
Sci., 270(2):141?153.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
ConLL workshop on learning language in logic,
pages 67?72, Lisbon, Portugal.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Sproat. 1992. Morphology and Computation.
MIT Press, Cambrdige, MA.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, and
Marc Pomplun. 2012. Estimating semantic trans-
parency of constituents of English compounds and
two-character Chinese words using Latent Semantic
Analysis. In Proceedings of CogSci, pages 2499?
2504, Sapporo, Japan.
Richard Wicentowski. 2004. Multilingual noise-
robust supervised morphological analysis using the
wordframe model. In Proceedings of SIGPHON,
pages 70?77, Barcelona, Spain.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of ACL,
pages 207?216, Hong Kong.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
1526
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53?57,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A relatedness benchmark to test the role of determiners
in compositional distributional semantics
Raffaella Bernardi and Georgiana Dinu and Marco Marelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Distributional models of semantics cap-
ture word meaning very effectively, and
they have been recently extended to ac-
count for compositionally-obtained rep-
resentations of phrases made of content
words. We explore whether compositional
distributional semantic models can also
handle a construction in which grammat-
ical terms play a crucial role, namely de-
terminer phrases (DPs). We introduce a
new publicly available dataset to test dis-
tributional representations of DPs, and we
evaluate state-of-the-art models on this set.
1 Introduction
Distributional semantics models (DSMs) approx-
imate meaning with vectors that record the dis-
tributional occurrence patterns of words in cor-
pora. DSMs have been effectively applied to in-
creasingly more sophisticated semantic tasks in
linguistics, artificial intelligence and cognitive sci-
ence, and they have been recently extended to
capture the meaning of phrases and sentences via
compositional mechanisms. However, scaling up
to larger constituents poses the issue of how to
handle grammatical words, such as determiners,
prepositions, or auxiliaries, that lack rich concep-
tual content, and operate instead as the logical
?glue? holding sentences together.
In typical DSMs, grammatical words are treated
as ?stop words? to be discarded, or at best used
as context features in the representation of content
words. Similarly, current compositional DSMs
(cDSMs) focus almost entirely on phrases made
of two or more content words (e.g., adjective-noun
or verb-noun combinations) and completely ig-
nore grammatical words, to the point that even
the test set of transitive sentences proposed by
Grefenstette and Sadrzadeh (2011) contains only
Tarzan-style statements with determiner-less sub-
jects and objects: ?table show result?, ?priest say
mass?, etc. As these examples suggest, however,
as soon as we set our sight on modeling phrases
and sentences, grammatical words are hard to
avoid. Stripping off grammatical words has more
serious consequences than making you sound like
the Lord of the Jungle. Even if we accept the
view of, e.g., Garrette et al (2013), that the log-
ical framework of language should be left to other
devices than distributional semantics, and the lat-
ter should be limited to similarity scoring, still ig-
noring grammatical elements is going to dramat-
ically distort the very similarity scores (c)DSMs
should provide. If we want to use a cDSM for
the classic similarity-based paraphrasing task, the
model shouldn?t conclude that ?The table shows
many results? is identical to ?the table shows no
results? since the two sentences contain the same
content words, or that ?to kill many rats? and ?to
kill few rats? are equally good paraphrases of ?to
exterminate rats?.
We focus here on how cDSMs handle determin-
ers and the phrases they form with nouns (deter-
miner phrases, or DPs).1 While determiners are
only a subset of grammatical words, they are a
large and important subset, constituting the natu-
ral stepping stone towards sentential distributional
semantics: Compositional methods have already
been successfully applied to simple noun-verb and
noun-verb-noun structures (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011), and de-
terminers are just what is missing to turn these
skeletal constructions into full-fledged sentences.
Moreover, determiner-noun phrases are, in super-
ficial syntactic terms, similar to the adjective-noun
phrases that have already been extensively studied
from a cDSM perspective by Baroni and Zampar-
1Some linguists refer to what we call DPs as noun phrases
or NPs. We say DPs simply to emphasize our focus on deter-
miners.
53
elli (2010), Guevara (2010) and Mitchell and Lap-
ata (2010). Thus, we can straightforwardly extend
the methods already proposed for adjective-noun
phrases to DPs.
We introduce a new task, a similarity-based
challenge, where we consider nouns that are
strongly conceptually related to certain DPs and
test whether cDSMs can pick the most appropri-
ate related DP (e.g., monarchy is more related to
one ruler than many rulers).2 We make our new
dataset publicly available, and we hope that it will
stimulate further work on the distributional seman-
tics of grammatical elements.3
2 Composition models
Interest in compositional DSMs has skyrocketed
in the last few years, particularly since the influ-
ential work of Mitchell and Lapata (2008; 2009;
2010), who proposed three simple but effective
composition models. In these models, the com-
posed vectors are obtained through component-
wise operations on the constituent vectors. Given
input vectors u and v, the multiplicative model
(mult) returns a composed vector p with: pi =
uivi. In the weighted additive model (wadd), the
composed vector is a weighted sum of the two in-
put vectors: p = ?u+?v, where ? and ? are two
scalars. Finally, in the dilation model, the output
vector is obtained by first decomposing one of the
input vectors, say v, into a vector parallel to u and
an orthogonal vector. Following this, the parallel
vector is dilated by a factor ? before re-combining.
This results in: p = (?? 1)?u,v?u+ ?u,u?v.
A more general form of the additive model
(fulladd) has been proposed by Guevara (2010)
(see also Zanzotto et al (2010)). In this approach,
the two vectors to be added are pre-multiplied by
weight matrices estimated from corpus-extracted
examples: p = Au+Bv.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application. The former model adjective-noun
phrases by treating the adjective as a function from
nouns onto modified nouns. Given that linear
functions can be expressed by matrices and their
application by matrix-by-vector multiplication, a
2Baroni et al (2012), like us, study determiner phrases
with distributional methods, but they do not model them com-
positionally.
3Dataset and code available from clic.cimec.
unitn.it/composes.
functor (such as the adjective) is represented by a
matrix U to be multiplied with the argument vec-
tor v (e.g., the noun vector): p = Uv. Adjective
matrices are estimated from corpus-extracted ex-
amples of noun vectors and corresponding output
adjective-noun phrase vectors, similarly to Gue-
vara?s approach.4
3 The noun-DP relatedness benchmark
Paraphrasing a single word with a phrase is a
natural task for models of compositionality (Tur-
ney, 2012; Zanzotto et al, 2010) and determin-
ers sometimes play a crucial role in defining the
meaning of a noun. For example a trilogy is com-
posed of three works, an assemblage includes sev-
eral things and an orchestra is made of many
musicians. These examples are particularly in-
teresting, since they point to a ?conceptual? use
of determiners, as components of the stable and
generic meaning of a content word (as opposed to
situation-dependent deictic and anaphoric usages):
for these determiners the boundary between con-
tent and grammatical word is somewhat blurred,
and they thus provide a good entry point for testing
DSM representations of DPs on a classic similarity
task. In other words, we can set up an experiment
in which having an effective representation of the
determiner is crucial in order to obtain the correct
result.
Using regular expressions over WordNet
glosses (Fellbaum, 1998) and complementing
them with definitions from various online dic-
tionaries, we constructed a list of more than 200
nouns that are strongly conceptually related to a
specific DP. We created a multiple-choice test set
by matching each noun with its associated DP
(target DP), two ?foil? DPs sharing the same noun
as the target but combined with other determiners
(same-N foils), one DP made of the target deter-
miner combined with a random noun (same-D
foil), the target determiner (D foil), and the target
noun (N foil). A few examples are shown in Table
1. After the materials were checked by all authors,
two native speakers took the multiple-choice test.
We removed the cases (32) where these subjects
provided an unexpected answer. The final set,
4Other approaches to composition in DSMs have been re-
cently proposed by Socher et al (2012) and Turney (2012).
We leave their empirical evaluation on DPs to further work,
in the first case because it is not trivial to adapt their complex
architecture to our setting; in the other because it is not clear
how Turney would extend his approach to represent DPs.
54
noun target DP same-N foil 1 same-N foil 2 same-D foil D foil N foil
duel two opponents various opponents three opponents two engineers two opponents
homeless no home too few homes one home no incision no home
polygamy several wives most wives fewer wives several negotiators several wives
opulence too many goods some goods no goods too many abductions too many goods
Table 1: Examples from the noun-DP relatedness benchmark
characterized by full subject agreement, contains
173 nouns, each matched with 6 possible answers.
The target DPs contain 23 distinct determiners.
4 Setup
Our semantic space provides distributional repre-
sentations of determiners, nouns and DPs. We
considered a set of 50 determiners that include all
those in our benchmark and range from quanti-
fying determiners (every, some. . . ) and low nu-
merals (one to four), to multi-word units analyzed
as single determiners in the literature, such as a
few, all that, too much. We picked the 20K most
frequent nouns in our source corpus considering
singular and plural forms as separate words, since
number clearly plays an important role in DP se-
mantics. Finally, for each of the target determiners
we added to the space the 2K most frequent DPs
containing that determiner and a target noun.
Co-occurrence statistics were collected from the
concatenation of ukWaC, a mid-2009 dump of the
English Wikipedia and the British National Cor-
pus,5 with a total of 2.8 billion tokens. We use
a bag-of-words approach, counting co-occurrence
with all context words in the same sentence with
a target item. We tuned a number of parameters
on the independent MEN word-relatedness bench-
mark (Bruni et al, 2012). This led us to pick the
top 20K most frequent content word lemmas as
context items, Pointwise Mutual Information as
weighting scheme, and dimensionality reduction
by Non-negative Matrix Factorization.
Except for the parameter-free mult method, pa-
rameters of the composition methods are esti-
mated by minimizing the average Euclidean dis-
tance between the model-generated and corpus-
extracted vectors of the 20K DPs we consider.6
For the lexfunc model, we assume that the deter-
miner is the functor and the noun is the argument,
5wacky.sslmit.unibo.it; www.natcorp.ox.
ac.uk
6All vectors are normalized to unit length before compo-
sition. Note that the objective function used in estimation
minimizes the distance between model-generated and corpus-
extracted vectors. We do not use labeled evaluation data to
optimize the model parameters.
method accuracy method accuracy
lexfunc 39.3 noun 17.3
fulladd 34.7 random 16.7
observed 34.1 mult 12.7
dilation 31.8 determiner 4.6
wadd 23.1
Table 2: Percentage accuracy of composition
methods on the relatedness benchmark
and estimate separate matrices representing each
determiner using the 2K DPs in the semantic space
that contain that determiner. For dilation, we treat
direction of stretching as a parameter, finding that
it is better to stretch the noun.
Similarly to the classic TOEFL synonym detec-
tion challenge (Landauer and Dumais, 1997), our
models tackle the relatedness task by measuring
cosines between each target noun and the candi-
date answers and returning the item with the high-
est cosine.
5 Results
Table 2 reports the accuracy results (mean ranks
of correct answers confirm the same trend). All
models except mult and determiner outperform the
trivial random guessing baseline, although they
are all well below the 100% accuracy of the hu-
mans who took our test. For the mult method we
observe a very strong bias for choosing a single
word as answer (>60% of the times), which in
the test set is always incorrect. This leads to its
accuracy being below the chance level. We sus-
pect that the highly ?intersective? nature of this
model (we obtain very sparse composed DP vec-
tors, only ?4% dense) leads to it not being a re-
liable method for comparing sequences of words
of different length: Shorter sequences will be con-
sidered more similar due to their higher density.
The determiner-only baseline (using the vector of
the component determiner as surrogate for the DP)
fails because D vectors tend to be far from N vec-
tors, thus the N foil is often preferred to the correct
response (that is represented, for this baseline, by
its D). In the noun-only baseline (use the vector
of the component noun as surrogate for the DP),
55
the correct response is identical to the same-N and
N foils, thus forcing a random choice between
these. Not surprisingly, this approach performs
quite badly. The observed DP vectors extracted di-
rectly from the corpus compete with the top com-
positional methods, but do not surpass them.7
The lexfunc method is the best compositional
model, indicating that its added flexibility in mod-
eling composition pays off empirically. The ful-
ladd model is not as good, but also performs well.
The wadd and especially dilation models perform
relatively well, but they are penalized by the fact
that they assign more weight to the noun vectors,
making the right answer dangerously similar to the
same-N and N foils.
Taking a closer look at the performance of the
best model (lexfunc), we observe that it is not
equally distributed across determiners. Focusing
on those determiners appearing in at least 4 cor-
rect answers, they range from those where lexfunc
performance was very significantly above chance
(p<0.001 of equal or higher chance performance):
too few, all, four, too much, less, several; to
those on which performance was still significant
but less impressively so (0.001<p< 0.05): sev-
eral, no, various, most, two, too many, many, one;
to those where performance was not significantly
better than chance at the 0.05 level: much, more,
three, another. Given that, on the one hand, per-
formance is not constant across determiners, and
on the other no obvious groupings can account
for their performance difference (compare the ex-
cellent lexfunc performance on four to the lousy
one on three!), future research should explore the
contextual properties of specific determiners that
make them more or less amenable to be captured
by compositional DSMs.
6 Conclusion
DSMs, even when applied to phrases, are typically
seen as models of content word meaning. How-
ever, to scale up compositionally beyond the sim-
plest constructions, cDSMs must deal with gram-
matical terms such as determiners. This paper
started exploring this issue by introducing a new
and publicly available set testing DP semantics in
a similarity-based task and using it to systemati-
cally evaluate, for the first time, cDSMs on a con-
7The observed method is in fact at advantage in our ex-
periment because a considerable number of DP foils are not
found in the corpus and are assigned similarity 0 with the tar-
get.
struction involving grammatical words. The most
important take-home message is that distributional
representations are rich enough to encode infor-
mation about determiners, achieving performance
well above chance on the new benchmark.
Theoretical considerations would lead one to
expect a ?functional? approach to determiner rep-
resentations along the lines of Baroni and Zampar-
elli (2010) and Coecke et al (2010) to outperform
those approaches that combine vectors separately
representing determiners and nouns. This predic-
tion was largely borne out in the results, although
the additive models, and particularly fulladd, were
competitive rivals.
We attempted to capture the distributional se-
mantics of DPs using a fairly standard, ?vanilla?
semantic space characterized by latent dimensions
that summarize patterns of co-occurrence with
content word contexts. By inspecting the con-
text words that are most associated with the var-
ious latent dimensions we obtained through Non-
negative Matrix Factorization, we notice how they
are capturing broad, ?topical? aspects of meaning
(the first dimension is represented by scripture, be-
liever, resurrection, the fourth by fever, infection,
infected, and so on). Considering the sort of se-
mantic space we used (which we took to be a rea-
sonable starting point because of its effectiveness
in a standard lexical task), it is actually surpris-
ing that we obtained the significant results we ob-
tained. Thus, a top priority in future work is to ex-
plore different contextual features, such as adverbs
and grammatical terms, that might carry informa-
tion that is more directly relevant to the semantics
of determiners.
Another important line of research pertains to
improving composition methods: Although the
best model, at 40% accuracy, is well above chance,
we are still far from the 100% performance of hu-
mans. We will try, in particular, to include non-
linear transformations in the spirit of Socher et al
(2012), and look for better ways to automatically
select training data.
Last but not least, in the near future we
would like to test if cDSMs, besides dealing with
similarity-based aspects of determiner meaning,
can also help in capturing those formal properties
of determiners, such as monotonicity or definite-
ness, that theoretical semanticists have been tradi-
tionally interested in.
56
7 Acknowledgments
This research was supported by the ERC 2011
Starting Independent Research Grant n. 283554
(COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23?32, Avignon, France.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Dan Garrette, Katrin Erk, and Ray Mooney. 2013. A
formal approach to linking logical form and vector-
space lexical semantics. In H. Bunt, J. Bos, and
S. Pulman, editors, Computing Meaning, Vol. 4. In
press.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
57
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 31?36,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DISSECT - DIStributional SEmantics Composition Toolkit
Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
Abstract
We introduce DISSECT, a toolkit to
build and explore computational models
of word, phrase and sentence meaning
based on the principles of distributional
semantics. The toolkit focuses in partic-
ular on compositional meaning, and im-
plements a number of composition meth-
ods that have been proposed in the litera-
ture. Furthermore, DISSECT can be use-
ful to researchers and practitioners who
need models of word meaning (without
composition) as well, as it supports var-
ious methods to construct distributional
semantic spaces, assessing similarity and
even evaluating against benchmarks, that
are independent of the composition infras-
tructure.
1 Introduction
Distributional methods for meaning similarity are
based on the observation that similar words oc-
cur in similar contexts and measure similarity
based on patterns of word occurrence in large cor-
pora (Clark, 2012; Erk, 2012; Turney and Pan-
tel, 2010). More precisely, they represent words,
or any other target linguistic elements, as high-
dimensional vectors, where the dimensions repre-
sent context features. Semantic relatedness is as-
sessed by comparing vectors, leading, for exam-
ple, to determine that car and vehicle are very sim-
ilar in meaning, since they have similar contextual
distributions. Despite the appeal of these meth-
ods, modeling words in isolation has limited ap-
plications and ideally we want to model semantics
beyond word level by representing the meaning of
phrases or sentences. These combinations are in-
finite and compositional methods are called for to
derive the meaning of a larger construction from
the meaning of its parts. For this reason, the ques-
tion of compositionality within the distributional
paradigm has received a lot of attention in recent
years and a number of compositional frameworks
have been proposed in the distributional seman-
tic literature, see, e.g., Coecke et al (2010) and
Mitchell and Lapata (2010). For example, in such
frameworks, the distributional representations of
red and car may be combined, through various op-
erations, in order to obtain a vector for red car.
The DISSECT toolkit (http://clic.
cimec.unitn.it/composes/toolkit)
is, to the best of our knowledge, the first to
provide an easy-to-use implementation of many
compositional methods proposed in the literature.
As such, we hope that it will foster further work
on compositional distributional semantics, as well
as making the relevant techniques easily available
to those interested in their many potential applica-
tions, e.g., to context-based polysemy resolution,
recognizing textual entailment or paraphrase
detection. Moreover, the DISSECT tools to
construct distributional semantic spaces from
raw co-occurrence counts, to measure similarity
and to evaluate these spaces might also be of
use to researchers who are not interested in the
compositional framework. DISSECT is freely
available under the GNU General Public License.
2 Building and composing distributional
semantic representations
The pipeline from corpora to compositional mod-
els of meaning can be roughly summarized as con-
sisting of three stages:1
1. Extraction of co-occurrence counts from cor-
pora In this stage, an input corpus is used to ex-
tract counts of target elements co-occurring with
some contextual features. The target elements
can vary from words (for lexical similarity), to
pairs of words (e.g., for relation categorization),
1See Turney and Pantel (2010) for a technical overview of
distributional methods for semantics.
31
to paths in syntactic trees (for unsupervised para-
phrasing). Context features can also vary from
shallow window-based collocates to syntactic de-
pendencies.
2. Transformation of the raw counts This
stage may involve the application of weighting
schemes such as Pointwise Mutual Information,
feature selection, dimensionality reduction meth-
ods such as Singular Value Decomposition, etc.
The goal is to eliminate the biases that typically
affect raw counts and to produce vectors which
better approximate similarity in meaning.
3. Application of composition functions
Once meaningful representations have been
constructed for the atomic target elements of
interest (typically, words), various methods, such
as vector addition or multiplication, can be used
for combining them to derive context-sensitive
representations or for constructing representations
for larger phrases or even entire sentences.
DISSECT can be used for the second and
third stages of this pipeline, as well as to measure
similarity among the resulting word or phrase vec-
tors. The first step is highly language-, task- and
corpus-annotation-dependent. We do not attempt
to implement all the corpus pre-processing and
co-occurrence extraction routines that it would
require to be of general use, and expect instead as
input a matrix of raw target-context co-occurrence
counts.2 DISSECT provides various methods to
re-weight the counts with association measures,
dimensionality reduction methods as well as the
composition functions proposed by Mitchell and
Lapata (2010) (Additive, Multiplicative and Dila-
tion), Baroni and Zamparelli (2010)/Coecke et al
(2010) (Lexfunc) and Guevara (2010)/Zanzotto et
al. (2010) (Fulladd). In DISSECT we define and
implement these in a unified framework and in a
computationally efficient manner. The focus of
DISSECT is to provide an intuitive interface for
researchers and to allow easy extension by adding
other composition methods.
3 DISSECT overview
DISSECT is written in Python. We provide many
standard functionalities through a set of power-
2These counts can be read from a text file containing two
strings (the target and context items) and a number (the corre-
sponding count) on each line (e.g., maggot food 15) or
from a matrix in format word freq1 freq2 ...
#create a semantic space from counts in
#dense format("dm"): word freq1 freq2 ..
ss = Space.build(data="counts.txt",
format="dm")
#apply transformations
ss = ss.apply(PpmiWeighting())
ss = ss.apply(Svd(300))
#retrieve the vector of a target element
print ss.get_row("car")
Figure 1: Creating a semantic space.
ful command-line tools, however users with ba-
sic Python familiarity are encouraged to use the
Python interface that DISSECT provides. This
section focuses on this interface (see the online
documentation on how to perform the same oper-
ations with the command-line tools), that consists
of the following top-level packages:
#DISSECT packages
composes.matrix
composes.semantic_space
composes.transformation
composes.similarity
composes.composition
composes.utils
Semantic spaces and transforma-
tions The concept of a semantic space
(composes.semantic space) is at the
core of the DISSECT toolkit. A semantic
space consists of co-occurrence values, stored
as a matrix, together with strings associated to
the rows of this matrix (by design, the target
linguistic elements) and a (potentially empty)
list of strings associated to the columns (the
context features). A number of transforma-
tions (composes.transformation) can
be applied to semantic spaces. We implement
weighting schemes such as positive Pointwise
Mutual Information (ppmi) and Local Mu-
tual Information, feature selection methods,
dimensionality reduction (Singular Value De-
composition (SVD) and Nonnegative Matrix
Factorization (NMF)), and new methods can
be easily added.3 Going from raw counts to a
transformed space is accomplished in just a few
lines of code (Figure 1).
3The complete list of transformations currently sup-
ported can be found at http://clic.cimec.unitn.
it/composes/toolkit/spacetrans.html#
spacetrans.
32
#load a previously saved space
ss = io_utils.load("ss.pkl")
#compute cosine similarity
print ss.get_sim("car", "book",
CosSimilarity())
#the two nearest neighbours of "car"
print ss.get_neighbours("car", 2,
CosSimilarity())
Figure 2: Similarity queries in a semantic space.
Furthermore DISSECT allows the pos-
sibility of adding new data to a seman-
tic space in an online manner (using the
semantic space.peripheral space
functionality). This can be used as a way to effi-
ciently expand a co-occurrence matrix with new
rows, without re-applying the transformations to
the entire space. In some other cases, the user may
want to represent phrases that are specialization
of words already existing in the space (e.g.,
slimy maggot and maggot), without distorting the
computation of association measures by counting
the same context twice. In this case, adding slimy
maggot as a ?peripheral? row to a semantic space
that already contains maggot implements the
desired behaviour.
Similarity queries Semantic spaces are used for
the computation of similarity scores. DISSECT
provides a series of similarity measures such as co-
sine, inverse Euclidean distance and Lin similarity,
implemented in the composes.similarity
package. Similarity of two elements can be com-
puted within one semantic space or across two
spaces that have the same dimensionality. Figure
2 exemplifies (word) similarity computations with
DISSECT.
Composition functions Composition functions
in DISSECT (composes.composition) take
as arguments a list of element pairs to be com-
posed, and one or two spaces where the elements
to be composed are represented. They return a se-
mantic space containing the distributional repre-
sentations of the composed items, which can be
further transformed, used for similarity queries, or
used as inputs to another round of composition,
thus scaling up beyond binary composition. Fig-
ure 3 shows a Multiplicative composition exam-
ple. See Table 1 for the currently available com-
position models, their definitions and parameters.
Model Composition function Parameters
Add. w1~u+ w2~v w1(= 1), w2(= 1)
Mult. ~u ~v -
Dilation ||~u||22~v + (?? 1)?~u,~v?~u ?(= 2)
Fulladd W1~u+W2~v W1,W2 ? Rm?m
Lexfunc Au~v Au ? Rm?m
Table 1: Currently implemented composition
functions of inputs (u, v) together with parame-
ters and their default values in parenthesis, where
defined. Note that in Lexfunc each functor word
corresponds to a separate matrix or tensor Au (Ba-
roni and Zamparelli, 2010).
Parameter estimation All composition models
except Multiplicative have parameters to be esti-
mated. For simple models with few parameters,
such as as Additive, the parameters can be passed
by hand. However, DISSECT supports automated
parameter estimation from training examples. In
particular, we extend to all composition methods
the idea originally proposed by Baroni and Zam-
parelli (2010) for Lexfunc and Guevara (2010) for
Fulladd, namely to use corpus-extracted example
vectors of both the input (typically, words) and
output elements (typically, phrases) in order to op-
timize the composition operation parameters. The
problem can be generally stated as:
?? = arg min
?
||P ? fcomp?(U, V )||F
where U, V and P are matrices containing input
and output vectors respectively. For example U
may contain adjective vectors such as red, blue,
V noun vectors such as car, sky and P corpus-
extracted vectors for the corresponding phrases
red car, blue sky. fcomp? is a composition func-
tion and ? stands for a list of parameters that this
composition function is associated with.4 We im-
plement standard least-squares estimation meth-
ods as well as Ridge regression with the option
for generalized cross-validation, but other meth-
ods such as partial least-squares regression can be
easily added. Figure 4 exemplifies the Fulladd
model.
Composition output examples DISSECT pro-
vides functions to evaluate (compositional) distri-
butional semantic spaces against benchmarks in
the composes.utils package. However, as a
more qualitatively interesting example of what can
be done with DISSECT, Table 2 shows the nearest
4Details on the extended corpus-extracted vector estima-
tion method in DISSECT can be found in Dinu et al (2013).
33
#instantiate a multiplicative model
mult_model = Multiplicative()
#use the model to compose words from input space input_space
comp_space = mult_model.compose([("red", "book", "my_red_book"),
("red", "car", "my_red_car")],
input_space)
#compute similarity of: 1) two composed phrases and 2) a composed phrase and a word
print comp_space.get_sim("my_red_book", "my_red_car", CosSimilarity())
print comp_space.get_sim("my_red_book", "book", CosSimilarity(), input_space)
Figure 3: Creating and using Multiplicative phrase vectors.
#training data for learning an adjective-noun phrase model
train_data = [("red","book","red_book"), ("blue","car","blue_car")]
#train a fulladd model
fa_model = FullAdditive()
fa_model.train(train_data, input_space, phrase_space)
#use the model to compose a phrase from new words and retrieve its nearest neighb.
comp_space = fa_model.compose([("yellow", "table", "my_yellow_table")], input_space)
print comp_space.get_neighbours("my_yellow_table", 10, CosSimilarity())
Figure 4: Estimating a Fulladd model and using it to create phrase vectors.
Target Method Neighbours
florist Corpus Harrod, wholesaler, stockist
flora + -ist
Fulladd flora, fauna, ecologist
Lexfunc ornithologist, naturalist, botanist
Additive flora, fauna, ecosystem
Table 3: Compositional models for morphol-
ogy. Top 3 neighbours of florist using its (low-
frequency) corpus-extracted vector, and when the
vector is obtained through composition of flora
and -ist with Fulladd, Lexfunc and Additive.
neighbours of false belief obtained through com-
position with the Fulladd, Lexfunc and Additive
models. In Table 3, we exemplify a less typical ap-
plication of compositional models to derivational
morphology, namely obtaining a representation of
florist compositionally from distributional repre-
sentations of flora and -ist (Lazaridou et al, 2013).
4 Main features
Support for dense and sparse representations
Co-occurrence matrices, as extracted from text,
tend to be very sparse structures, especially when
using detailed context features which include syn-
tactic information, for example. On the other
hand, dimensionality reduction operations, which
are often used in distributional models, lead to
smaller, dense structures, for which sparse rep-
resentations are not optimal. This is our motiva-
tion for supporting both dense and sparse repre-
sentations. The choice of dense vs. sparse is ini-
tially determined by the input format, if a space
is created from co-occurrence counts. By default,
DISSECT switches to dense representations af-
ter dimensionality reduction, however the user can
freely switch from one representation to the other,
in order to optimize computations. For this pur-
pose DISSECT provides wrappers around matrix
operations, as well as around common linear alge-
bra operations, in the composes.matrix pack-
age. The underlying Python functionality is pro-
vided by numpy.array and scipy.sparse.
Efficient computations DISSECT is optimized
for speed since most operations are cast as matrix
operations, that are very efficiently implemented
in Python?s numpy and scipy modules5. Ta-
bles 4 and 5 show running times for typical DIS-
SECT operations: application of the ppmi weight-
ing scheme, nearest neighbour queries and estima-
tion of composition function parameters (on a 2.1
5For SVD on sparse structures, we use sparsesvd
(https://pypi.python.org/pypi/sparsesvd/).
For NMF, we adapted http://www.csie.ntu.edu.
tw/?cjlin/nmf/ (Lin, 2007).
34
Target Method Neighbours
belief Corpus moral, dogma, worldview, religion, world-view, morality, theism, tenet, agnosticism, dogmatic
false belief
Fulladd pantheist, belief, agnosticism, religiosity, dogmatism, pantheism, theist, fatalism, deism, mind-set
Lexfunc self-deception, untruth, credulity, obfuscation, misapprehension, deceiver, disservice, falsehood
Additive belief, assertion, falsity, falsehood, truth, credence, dogma, supposition, hearsay, denial
Table 2: Top nearest neighbours of belief and of false belief obtained through composition with the
Fulladd, Lexfunc and Additive models.
Method Fulladd Lexfunc Add. Dilation
Time (s.) 2864 787 46 68
Table 4: Composition model parameter estimation
times (in seconds) for 1 million training points in
300-dimensional space.
Matrix size (nnz) Ppmi Query
100Kx300 (30M) 5.8 0.5
100Kx100K (250M) 52.6 9.5
Table 5: Running times (in seconds) for 1) appli-
cation of ppmi weighting and 2) querying for the
top neighbours of a word (cosine similarity) for
different matrix sizes (nnz: number of non-zero
entries, in millions).
GHz machine). The price to pay for fast computa-
tions is that data must be stored in main memory.
We do not think that this is a major inconvenience.
For example, a typical symmetric co-occurrence
matrix extracted from a corpus of several billion
words, defining context in terms of 5-word win-
dows and considering the top 100K?100K most
frequent words, contains? 250 million entries and
requires only 2GB of memory for (double preci-
sion) storage.
Simple design We have opted for a very simple
and intuitive design as the classes interact in
very natural ways: A semantic space stores
the actual data matrix and structures to index
its rows and columns, and supports similarity
queries and transformations. Transformations
take one semantic space as input to return
another, transformed, space. Composition func-
tions take one or more input spaces and yield
a composed-elements space, which can further
undergo transformations and be used for similarity
queries. In fact, DISSECT semantic spaces also
support higher-order tensor representations, not
just vectors. Higher-order representations are
used, for example, to represent transitive verbs
and other multi-argument functors by Coecke
et al (2010) and Grefenstette et al (2013).
See http://clic.cimec.unitn.it/
composes/toolkit/composing.html for
an example of using DISSECT for estimating
such tensors.
Extensive documentation The DISSECT
documentation can be found at http://clic.
cimec.unitn.it/composes/toolkit.
We provide a tutorial which guides the user
through the creation of some toy semantic spaces,
estimation of the parameters of composition
models and similarity computations in semantic
spaces. We also provide a full-scale example
of intransitive verb-subject composition. We
show how to go from co-occurrence counts to
composed representations and make the data used
in the examples available for download.
Comparison to existing software In terms of
design choices, DISSECT most resembles the
Gensim toolkit (R?ehu?r?ek and Sojka, 2010). How-
ever Gensim is intended for topic modeling, and
therefore diverges considerably from DISSECT in
its functionality. The SSpace package of Jurgens
and Stevens (2010) also overlaps to some degree
with DISSECT in terms of its intended use, how-
ever, like Gensim, it does not support composi-
tional operations that, as far as we know, are an
unique feature of DISSECT.
5 Future extensions
We implemented and are currently testing DIS-
SECT functions supporting other composition
methods, including the one proposed by Socher
et al (2012). Adding further methods is our top-
priority goal. In particular, several distributional
models of word meaning in context share impor-
tant similarities with composition models, and we
plan to add them to DISSECT. Dinu et al (2012)
show, for example, that well-performing, simpli-
fied variants of the method in Thater et al (2010),
Thater et al (2011) and Erk and Pado? (2008) can
be reduced to relatively simple matrix operations,
making them particularly suitable for a DISSECT
implementation.
35
DISSECT is currently optimized for the compo-
sition of many phrases of the same type. This is in
line with most of the current evaluations of com-
positional models, which focus on specific phe-
nomena, such as adjectival modification, noun-
noun compounds or intransitive verbs, to name a
few. In the future we plan to provide a module for
composing entire sentences, taking syntactic trees
as input and returning composed representations
for each node in the input trees.
Finally, we intend to make use of the exist-
ing Python plotting libraries to add a visualization
module to DISSECT.
6 Acknowledgments
We thank Angeliki Lazaridou for helpful dis-
cussions. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Stefan Thater, and So?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL HLT, pages 611?
615, Montreal, Canada.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. A general framework for the estimation of
distributional composition functions. In Proceed-
ings of ACL Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
In press.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In Proceedings of
ACL, Sofia, Bulgaria. In press.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756?2779.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Radim R?ehu?r?ek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP,
pages 1134?1143, Chiang Mai, Thailand.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
36
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, page 1,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Visual Features for Linguists:
Basic image analysis techniques for multimodally-curious NLPers
Elia Bruni
University of Trento
elia.bruni@unitn.it
Marco Baroni
University of Trento
marco.baroni@unitn.it
Description
Features automatically extracted from images con-
stitute a new and rich source of semantic knowl-
edge that can complement information extracted
from text. The convergence between vision- and
text-based information can be exploited in scenar-
ios where the two modalities must be combined
to solve a target task (e.g., generating verbal de-
scriptions of images, or finding the right images
to illustrate a story). However, the potential ap-
plications for integrated visual features go beyond
mixed-media scenarios: Because of their comple-
mentary nature with respect to language, visual
features might provide perceptually grounded se-
mantic information that can be exploited in purely
linguistic domains.
The tutorial will first introduce basic techniques
to encode image contents in terms of low-level fea-
tures, such as the widely adopted SIFT descriptors.
We will then show how these low-level descriptors
are used to induce more abstract features, focus-
ing on the well-established bags-of-visual-words
method to represent images, but also briefly in-
troducing more recent developments, that include
capturing spatial information with pyramid repre-
sentations, soft visual word clustering via Fisher
encoding and attribute-based image representa-
tion. Next, we will discuss some example appli-
cations, and we will conclude with a brief practi-
cal illustration of visual feature extraction using a
software package we developed.
The tutorial is addressed to computational lin-
guists without any background in computer vi-
sion. It provides enough background material to
understand the vision-and-language literature and
the less technical articles on image analysis. After
the tutorial, the participants should also be able to
autonomously incorporate visual features in their
NLP pipelines using off-the-shelf tools.
Outline
1. Why image analysis?
? The grounding problem
? Multimodal datasets (Pascal, SUN, Im-
ageNet and ESP-Game)
2. Extraction of low-level features from images
? Challenges (viewpoint, illumination,
scale, occlusion, etc.)
? Feature detectors
? Feature descriptors
3. Visual words for higher-level representation
of visual information
? Constructing a vocabulary of visual
words
? Classic Bags-of-visual-words represen-
tation
? Recent advances
? Computer vision applications: Object
recognition and emotion analysis
4. Going multimodal: Example applications of
visual features in NLP
? Generating image descriptions
? Semantic relatedness
? Modeling selectional preference
1
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 90?99,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A practical and linguistically-motivated approach
to compositional distributional semantics
Denis Paperno and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(denis.paperno|thenghia.pham|marco.baroni)@unitn.it
Abstract
Distributional semantic methods to ap-
proximate word meaning with context
vectors have been very successful empir-
ically, and the last years have seen a surge
of interest in their compositional exten-
sion to phrases and sentences. We present
here a new model that, like those of Co-
ecke et al (2010) and Baroni and Zam-
parelli (2010), closely mimics the standard
Montagovian semantic treatment of com-
position in distributional terms. However,
our approach avoids a number of issues
that have prevented the application of the
earlier linguistically-motivated models to
full-fledged, real-life sentences. We test
the model on a variety of empirical tasks,
showing that it consistently outperforms a
set of competitive rivals.
1 Compositional distributional semantics
The research of the last two decades has estab-
lished empirically that distributional vectors for
words obtained from corpus statistics can be used
to represent word meaning in a variety of tasks
(Turney and Pantel, 2010). If distributional vec-
tors encode certain aspects of word meaning, it is
natural to expect that similar aspects of sentence
meaning can also receive vector representations,
obtained compositionally from word vectors. De-
veloping a practical model of compositionality is
still an open issue, which we address in this pa-
per. One approach is to use simple, parameter-
free models that perform operations such as point-
wise multiplication or summing (Mitchell and La-
pata, 2008). Such models turn out to be sur-
prisingly effective in practice (Blacoe and Lap-
ata, 2012), but they have obvious limitations. For
instance, symmetric operations like vector addi-
tion are insensitive to syntactic structure, there-
fore meaning differences encoded in word order
are lost in composition: pandas eat bamboo is
identical to bamboo eats pandas. Guevara (2010),
Mitchell and Lapata (2010), Socher et al (2011)
and Zanzotto et al (2010) generalize the simple
additive model by applying structure-encoding op-
erators to the vectors of two sister nodes before
addition, thus breaking the inherent symmetry of
the simple additive model. A related approach
(Socher et al, 2012) assumes richer lexical rep-
resentations where each word is represented with
a vector and a matrix that encodes its interaction
with its syntactic sister. The training proposed in
this model estimates the parameters in a super-
vised setting. Despite positive empirical evalua-
tion, this approach is hardly practical for general-
purpose semantic language processing, since it re-
quires computationally expensive approximate pa-
rameter optimization techniques, and it assumes
task-specific parameter learning whose results are
not meant to generalize across tasks.
1.1 The lexical function model
None of the proposals mentioned above, from sim-
ple to elaborate, incorporates in its architecture the
intuitive idea (standard in theoretical linguistics)
that semantic composition is more than a weighted
combination of words. Generally one of the com-
ponents of a phrase, e.g., an adjective, acts as
a function affecting the other component (e.g., a
noun). This underlying intuition, adopted from
formal semantics of natural language, motivated
the creation of the lexical function model of com-
position (lf ) (Baroni and Zamparelli, 2010; Co-
ecke et al, 2010). The lf model can be seen as a
projection of the symbolic Montagovian approach
to semantic composition in natural language onto
the domain of vector spaces and linear operations
on them (Baroni et al, 2013). In lf, arguments
are vectors and functions taking arguments (e.g.,
adjectives that combine with nouns) are tensors,
with the number of arguments (n) determining the
90
order of tensor (n+1). For example, adjectives, as
unary functors, are modeled with 2-way tensors, or
matrices. Tensor by vector multiplication formal-
izes function application and serves as the general
composition method.
Baroni and Zamparelli (2010) propose a practi-
cal and empirically effective way to estimate ma-
trices representing adjectival modifiers of nouns
by linear regression from corpus-extracted exam-
ples of noun and adjective-noun vectors. Un-
like the neural network approach of Socher et
al. (2011; 2012), the Baroni and Zamparelli
method does not require manually labeled data nor
costly iterative estimation procedures, as it relies
on automatically extracted phrase vectors and on
the analytical solution of the least-squares-error
problem.
The same method was later applied to matrix
representations of intransitive verbs and determin-
ers (Bernardi et al, 2013; Dinu et al, 2013), al-
ways with good empirical results.
The full range of semantic types required for
natural language processing, including those of
adverbs and transitive verbs, has to include, how-
ever, tensors of greater rank. The estimation
method originally proposed by Baroni and Zam-
parelli has been extended to 3-way tensors rep-
resenting transitive verbs by Grefenstette et al
(2013) with preliminary success. Grefenstette et
al.?s method works in two steps. First, one esti-
mates matrices of verb-object phrases from sub-
ject and subject-verb-object vectors; next, transi-
tive verb tensors are estimated from verb-object
matrices and object vectors.
1.2 Problems with the extension of the lexical
function model to sentences
With all the advantages of lf, scaling it up to ar-
bitrary sentences, however, leads to several issues.
In particular, it is desirable for all practical pur-
poses to limit representation size. For example,
if noun meanings are encoded in vectors of 300
dimensions, adjectives become matrices of 300
2
cells, and transitive verbs are represented as ten-
sors with 300
3
=27, 000, 000 dimensions.
Estimating tensors of this size runs into data
sparseness issues already for less common tran-
sitive verbs. Indeed, in order to train a transitive
verb tensor (e.g., eat), the method of Grefenstette
et al (2013) requires a sufficient number of dis-
tinct verb object phrases with that verb (e.g., eat
cake, eat fruits), each attested in combination with
a certain number of subject nouns with sufficient
frequency to extract sensible vectors. It is not fea-
sible to obtain enough data points for all verbs in
such a training design.
Things get even worse for other categories.
Adverbs like quickly that modify intransitive
verbs have to be represented with 300
2
2
=
8, 100, 000, 000 dimensions. Modifiers of transi-
tive verbs would have even greater representation
size, which may not be possible to store and learn
efficiently.
Another issue is that the same or similar items
that occur in different syntactic contexts are as-
signed different semantic types with incompara-
ble representations. For example, verbs like eat
can be used in transitive or intransitive construc-
tions (children eat meat/children eat), or in passive
(meat is eaten). Since predicate arity is encoded
in the order of the corresponding tensor, eat and
the like have to be assigned different representa-
tions (matrix or tensor) depending on the context.
Deverbal nouns like demolition, often used with-
out mention of who demolished what, would have
to get vector representations while the correspond-
ing verbs (demolish) would become tensors, which
makes immediately related verbs and nouns in-
comparable. Nouns in general would oscillate be-
tween vector and matrix representations depend-
ing on argument vs. predicate vs. modifier posi-
tion (an animal runs vs. this is an animal vs. an-
imal shelter). Prepositions are the hardest, as the
syntactic positions in which they occur are most
diverse (park in the dark vs. play in the dark vs.
be in the dark vs. a light glowing in the dark).
In all those cases, the same word has to be
mapped to tensors of different orders. Since each
of these tensors must be learned from examples
individually, their obvious relation is missed. Be-
sides losing the comparability of the semantic con-
tribution of a word across syntactic contexts, we
also worsen the data sparseness issues.
The last, and related, point is that for the ten-
sor calculus to work, one needs to model, for each
word, each of the constructions in the corpus that
the word is attested in. In its pure form lf does
not include an emergency backoff strategy when
unknown words or constructions are encountered.
For example, if we only observe transitive usages
of to eat in the training corpus, and encounter an
intransitive or passive example of it in testing data,
91
the system would not be able to compose a sen-
tence vector at all. This issue is unavoidable since
we don?t expect to find all words in all possible
constructions even in the largest corpus.
2 The practical lexical function model
As follows from section 1.2, it would be desirable
to have a compositional distributional model that
encodes function-argument relations but avoids
the troublesome high-order tensor representations
of the pure lexical function model, with all the
practical problems that come with them. We may
still want to represent word meanings in differ-
ent syntactic contexts differently, but at the same
time we need to incorporate a formal connection
between those representations, e.g., between the
transitive and the intransitive instantiations of the
verb to eat. Last but not least, all items need to
include a common aspect of their representation
(e.g., a vector) to allow comparison across cate-
gories (the case of demolish and demolition).
To this end, we propose a new model of compo-
sition that maintains the idea of function applica-
tion, while avoiding the complications and rigidity
of lf. We call our proposal practical lexical func-
tion model, or plf. In plf, a functional word is not
represented by a single tensor of arity-dependent
order, but by a vector plus an ordered set of matri-
ces, with one matrix for each argument the func-
tion takes. After applying the matrices to the cor-
responding argument vectors, a single representa-
tion is obtained by summing across all resulting
vectors.
2.1 Word meaning representation
In plf, all words are represented by a vector, and
functional words, such as predicates and modi-
fiers, are also assigned one or more matrices. The
general form of a semantic representation for a
linguistic unit is an ordered tuple of a vector and
n ? N matrices:
1
?
~x,
2
1
x , . . . ,
2
n
x
?
The number of matrices in the representation
encodes the arity of a linguistic unit, i.e., the num-
ber of other units to which it applies as a function.
Each matrix corresponds to a function-argument
relation, and words have as many matrices as
many arguments they take: none for (most) nouns,
1
Matrices associated with term x are symbolized
2
x.
dog
~
dog
run ~run,
2
run
chase
~
chase,
2
s
chase,
2
o
chase
give
~
give,
2
s
give,
2
o
give,
2
io
give
big
~
big,
2
big
very ~very,
2
n
very,
2
a
very
quickly
~
quickly,
2
s
quickly,
2
v
quickly
Table 1: Examples of word representations. Sub-
scripts encode, just for mnemonic purposes, the
constituent whose vector the matrix combines
with: subject, object, indirect object, noun,
adjective, verb phrase.
one for adjectives and intransitive verbs, two for
transitives, etc. The matrices formalize argument
slot saturation, operating on an argument vector
representation through matrix by vector multipli-
cation, as described in the next section.
Modifiers of n-ary functors are represented by
n+1-ary structures. For instance, we treat adjec-
tives that modify nouns (0-ary) as unary functions,
encoded in a vector-matrix pair. Adverbs have dif-
ferent semantic types depending on their syntac-
tic role. Sentential adverbs are unary, while ad-
verbs that modify adjectives (very) or verb phrases
(quickly) are encoded as binary functions, repre-
sented by a vector and two matrices. The form of
semantic representations we are using is shown in
Table 1.
2
2.2 Semantic composition
Our system incorporates semantic composition via
two composition rules, one for combining struc-
tures of different arity and the other for symmet-
ric composition of structures with the same ar-
ity. These rules incorporate insights of two em-
pirically successful models, lexical function and
the simple additive approach, used as the default
structure merging strategy.
The first rule is function application, illustrated
in Figure 1. Table 2 illustrates simple cases of
function application. For transitive verbs seman-
tic composition applies iteratively as shown in the
derivation of Figure 2. For ternary predicates such
2
To determine the number and ordering of matrices rep-
resenting the word in the current syntactic context, our plf
implementation relies on the syntactic type assigned to the
word in the categorial grammar parse of the sentence.
92
?~x+
2
n+k
x ? ~y,
2
1
x +
2
1
y , . . . ,
2
n
x +
2
n
y , . . .
?
?
~x,
2
1
x , . . . ,
2
n
x , . . . ,
2
n+k
x
? ?
~y,
2
1
y , . . . ,
2
n
y
?
Figure 1: Function application: If two syntactic
sisters have different arity, treat the higher-arity
sister as the functor. Compose by multiplying the
last matrix in the functor tuple by the argument
vector and summing the result to the functor vec-
tor. Unsaturated matrices are carried up to the
composed node, summing across sisters if needed.
dogs
~
dogs
run ~run,
2
run
dogs run ~run +
2
run?
~
dog
house
~
house
big
~
big,
2
big
big house
~
big +
2
big ?
~
house
Table 2: Examples of function application.
as give in a ditransitive construction, the first step
in the derivation absorbs the innermost argument
by multiplying its vector by the third give matrix,
and then composition proceeds like for transitives.
The second composition rule, symmetric com-
position applies when two syntactic sisters are of
the same arity (e.g., two vectors, or two vector-
matrix pairs). Symmetric composition simply
sums the objects in the two tuples: vector with
vector, n-th matrix with n-th matrix.
Symmetric composition is reserved for struc-
tures in which the function-argument distinction
is problematic. Some candidates for such treat-
ment are coordination and nominal compounds,
although we recognize that the headless analysis is
2
s
chase?
~
dogs+
~
chase+
2
o
chase?
~
cats
~
dogs
?
~
chase+
2
o
chase?
~
cats,
2
s
chase
?
?
~
chase,
2
s
chase,
2
o
chase
?
~
cats
Figure 2: Applying function application twice to
derive the representation of a transitive sentence.
sing:
~
sing,
2
sing dance:
~
dance,
2
dance
sing and dance:
~
sing +
~
dance,
2
sing +
2
dance
rice:
~
rice cake:
~
cake
rice cake
~
rice+
~
cake
Table 3: Examples of symmetric composition.
not the only possible one here. See two examples
of Symmetric Composition application in Table 3.
Note that the sing and dance composition in Ta-
ble 3 skips the conjunction. Our current plf im-
plementation treats most grammatical words, in-
cluding conjunctions, as ?empty? elements, that
do not project into semantics. This choice leads
to some interesting ?serendipitous? treatments of
various constructions. For example, since the cop-
ula is empty, a sentence with a predicative adjec-
tive (cars are red) is treated in the same way as a
phrase with the same adjective in attributive posi-
tion (red cars) ? although the latter, being a phrase
and not a full sentence, will later be embedded as
argument in a larger construction. Similarly, leav-
ing the relative pronoun empty makes cars that
run identical to cars run, although, again, the for-
mer will be embedded in a larger construction later
in the derivation.
We conclude our brief exposition of plf with an
alternative intuition for it: the plf model is also
a more sophisticated version of the additive ap-
proach, where argument words are adapted by ma-
trices that encode the relation to their functors be-
fore the sentence vector is derived by summing.
2.3 Satisfying the desiderata
Let us now outline how plf addresses the short-
comings of lf listed in Section 1.2. First, all is-
sues caused by representation size disappear. An
n-ary predicate is no longer encoded as an n+1-
way tensor; instead we have a sequence of n ma-
trices. The representation size grows linearly, not
exponentially, for higher semantic types, allowing
for simpler and more efficient parameter estima-
tion, storage, and computation.
As a consequence of our architecture, we no
longer need to perform the complicated step-by-
step estimation for elements of higher arity. In-
deed, one can estimate each matrix of a com-
plex representation individually using the simple
method of Baroni and Zamparelli (2010). For in-
stance, for transitive verbs we estimate the verb-
subject combination matrix from subject and verb-
93
boys
~
boys
eat (intrans.)
~
eat,
2
s
eat
boys eat
2
s
eat?
~
boys+
~
eat
meat
~
meat
eat (trans.)
~
eat,
2
s
eat,
2
o
eat
boys eat meat
2
s
eat?
~
boys+
~
eat+
2
o
eat?
~
meat
(is) eaten (pass.)
~
eat,
2
o
eat
meat is eaten
~
eat+
2
o
eat?
~
meat
Table 4: The verb to eat associated to different sets
of matrices in different syntactic contexts.
subject vectors, the verb-object combination ma-
trix from object and verb-object vectors. We ex-
pect a reasonably large corpus to feature many oc-
currences of a verb with a variety of subjects and
a variety of objects (but not necessarily a variety
of subjects with each of the objects as required by
Grefenstette et al?s training), allowing us to avoid
the data sparseness issue.
The semantic representations we propose in-
clude a semantic vector for constituents of any se-
mantic type, thus enabling semantic comparison
for words of different parts of speech (the case of
demolition vs. demolish).
Finally, the fact that we represent the predicate
interaction with each of its arguments in a sepa-
rate matrix allows for a natural and intuitive treat-
ment of argument alternations. For instance, as
shown in Table 4, one can distinguish the transi-
tive and intransitive usages of the verb to eat by
the presence of the object-oriented matrix of the
verb while keeping the rest of the representation
intact. To model passive usages, we insert the ob-
ject matrix of the verb only, which will be multi-
plied by the syntactic subject vector, capturing the
similarity between eat meat and meat is eaten.
So keeping the verb?s interaction with subject
and object encoded in distinct matrices not only
solves the issues of representation size for arbi-
trary semantic types, but also provides a sensible
built-in strategy for handling a word?s occurrence
in multiple constructions. Indeed, if we encounter
a verb used intransitively which was only attested
as transitive in the training corpus, we can simply
omit the object matrix to obtain a type-appropriate
representation. On the other hand, if the verb oc-
curs with more arguments than usual in testing
materials, we can add a default diagonal identity
matrix to its representation, signaling agnosticism
about how the verb relates to the unexpected argu-
ment. This flexibility makes our model suitable to
compute vector representations of sentences with-
out stumbling at unseen syntactic usages of words.
To summarize, plf is an extension of the lexi-
cal function model that inherits its strengths and
overcomes its weaknesses. We still employ a
linguistically-motivated notion of semantic com-
position as function application and use distinct
kinds of representations for different semantic
types. At the same time, we avoid high order ten-
sor representations, produce semantic vectors for
all syntactic constituents, and allow for an elegant
and transparent correspondence between different
syntactic usages of a lexeme, such as the transi-
tive, the intransitive, and the passive usages of the
verb to eat. Last but not least, our implementation
is suitable for realistic language processing since
it allows to produce vectors for sentences of arbi-
trary size, including those containing novel syn-
tactic configurations.
3 Evaluation
3.1 Evaluation materials
We consider 5 different benchmarks that focus on
different aspects of sentence-level semantic com-
position. The first data set, created by Edward
Grefenstette and Mehrnoosh Sadrzadeh and in-
troduced in Kartsaklis et al (2013), features 200
sentence pairs that were rated for similarity by
43 annotators. In this data set, sentences have
fixed adjective-noun-verb-adjective-noun (anvan)
structure, and they were built in order to cru-
cially require context-based verb disambiguation
(e.g., young woman filed long nails is paired with
both young woman smoothed long nails and young
woman registered long nails). We also consider a
similar data set introduced by Grefenstette (2013),
comprising 200 sentence pairs rated by 50 anno-
tators. We will call these benchmarks anvan1 and
anvan2, respectively. Evaluation is carried out by
computing the Spearman correlation between the
annotator similarity ratings for the sentence pairs
and the cosines of the vectors produced by the var-
ious systems for the same sentence pairs.
The benchmark introduced by The Pham et al
(2013) at the TFDS workshop (tfds below) was
specifically designed to test compositional meth-
ods for their sensitivity to word order and the se-
mantic effect of determiners. The tfds benchmark
contains 157 target sentences that are matched
with a set of (approximate) paraphrases (8 on av-
94
erage), and a set of ?foils? (17 on average). The
foils have high lexical overlap with the targets but
very different meanings, due to different determin-
ers and/or word order. For example, the target
A man plays an acoustic guitar is matched with
paraphrases such as A man plays guitar and The
man plays the guitar, and foils such as The man
plays no guitar and A guitar plays a man. A
good system should return higher similarities for
the comparison with the paraphrases with respect
to that with the foils. Performance is assessed
through the t-standardized cross-target average of
the difference between mean cosine with para-
phrases and mean cosine with foils (Pham and col-
leagues, equivalently, reported non-standardized
average and standard deviations).
The two remaining data sets are larger and more
?natural?, as they were not constructed by linguists
under controlled conditions to focus on specific
phenomena. They are aimed at evaluating sys-
tems on the sort of free-form sentences one en-
counters in real-life applications. The msrvid data
set from the SemEval-2012 Semantic Textual Sim-
ilarity (STS) task (Agirre et al, 2012) consists of
750 sentence pairs that describe brief videos. Sen-
tence pairs were scored for similarity by 5 subjects
each. Following standard practice in paraphrase
detection studies (e.g., Blacoe and Lapata (2012)),
we use cosine similarity between sentence pairs as
computed by one of our systems together with two
shallow similarity cues: word overlap between the
two sentences and difference in sentence length.
We obtain a final similarity score by weighted ad-
dition of the 3 cues, with the optimal weights de-
termined by linear regression on separate msrvid
train data that were also provided by the SemEval
task organizers (before combining, we checked
that the collinearity between cues was low). Sys-
tem scores are evaluated by their Pearson correla-
tion with the human ratings.
The final set we use is onwn, from the *SEM-
2013 STS shared task (Agirre et al, 2013). This
set contains 561 pairs of glosses (from the Word-
Net and OntoNotes databases), rated by 5 judges
for similarity. Our main interest in this set stems
from the fact that glosses are rarely well-formed
full sentences (consider, e.g., cause something to
pass or lead somewhere; coerce by violence, fill
with terror). For this reason, they are very chal-
lenging for standard parsers. Indeed, we estimated
from a sample of 40 onwn glosses that the C&C
parser (see below) has only 45% accuracy on this
set. Since plf needs syntactic information to con-
struct sentence vectors compositionally, we test it
on onwn to make sure that it is not overly sensi-
tive to parser noise. Evaluation proceeds as with
msrvid (cue weights are determined by 10-fold
cross-validation).
3
3.2 Semantic space construction and
composition model implementation
Our source corpus was given by the concatena-
tion of ukWaC (wacky.sslmit.unibo.it),
a mid-2009 dump of the English Wikipedia (en.
wikipedia.org) and the British National Cor-
pus (www.natcorp.ox.ac.uk), for a total of
about 2.8 billion words.
We collected a 30K-by-30K matrix by counting
co-occurrence of the 30K most frequent content
lemmas (nouns, adjectives and verbs) within a 3-
word window. The raw count vectors were trans-
formed into positive Pointwise Mutual Informa-
tion scores and reduced to 300 dimensions by the
Singular Value Decomposition. All vectors were
normalized to length 1. This setup was picked
without tuning, as we found it effective in previ-
ous, unrelated experiments.
4
We consider four composition models. The add
(additive) model produces the vector of a sentence
by summing the vectors of all content words in it.
Similarly, mult uses component-wise multiplica-
tion of vectors for composition. While these mod-
els are very simple, a long experimental tradition
has proven their effectiveness (Landauer and Du-
mais, 1997; Mitchell and Lapata, 2008; Mitchell
and Lapata, 2010; Blacoe and Lapata, 2012).
For the lf (lexical function) model, we construct
functional matrix representations of adjectives, de-
terminers and intransitive verbs. These are trained
using Ridge regression with generalized cross-
validation from corpus-extracted vectors of nouns,
3
We did not evaluate on other STS benchmarks since they
have characteristics, such as high density of named entities,
that would require embedding our compositional models into
more complex systems, obfuscating their impact on the over-
all performance.
4
With the multiplicative composition model we also tried
Nonnegative Matrix Factorization instead of Singular Value
Decomposition, because the negative values produced by
SVD are potentially problematic for mult. In addition, we re-
peated the evaluation for the multiplicative and additive mod-
els without any form of dimensionality reduction. The over-
all pattern of results did not change significantly, and thus for
consistency we report all models? performance only for the
SVD-reduced space.
95
as input, and phrases including those nouns as out-
put (e.g., the matrix for red is trained from corpus-
extracted ?noun, red-noun? vector pairs). Transi-
tive verb tensors are estimated using the two-step
regression procedure outlined by Grefenstette et
al. (2013). We did not attempt to train a lf model
for the larger and more varied msrvid and onwn
data sets, as this would have been extremely time
consuming and impractical for all the reasons we
discussed in Section 1.2 above.
Training plf (practical lexical function) pro-
ceeds similarly, but we also build preposition
matrices (from ?noun, preposition-noun? vector
pairs), and for verbs we prepare separate subject
and object matrices.
Since syntax guides lf and plf composition, we
supplied all test sentences with categorial gram-
mar parses. Every sentence in the anvan1 and
anvan2 datasets has the form (subject) Adjective
+ Noun + Transitive Verb + (object) Adjective +
Noun, so parsing them is trivial. All sentences in
tfds have a predictable structure that allows per-
fect parsing with simple finite state rules. In all
these cases, applying a general-purpose parser to
the data would have, at best, had no impact and,
at worst, introduced parsing errors. For msrvid
and onwn, we used the output of the C&C parser
(Clark and Curran, 2007).
3.3 Results
Table 5 summarizes the performance of our mod-
els on the chosen tasks, and compares it to the state
of the art reported in previous work, as well as to
various strong baselines.
The plf model performs very well on both an-
van benchmarks, outperforming not only add and
mult, but also the full-fledged lf model. Given
that these data sets contain, systematically, transi-
tive verbs, the major difference between plf and lf
lies in their representation of the latter. Evidently,
the separately-trained subject and object matrices
of plf, being less affected by data sparseness than
the 3-way tensors of lf, are better able to capture
how verbs interact with their arguments. For an-
van1, plf is just below the state of the art, which
is based on disambiguating the verb vector in con-
text (Kartsaklis and Sadrzadeh, 2013), and lf out-
performs the baseline, which consists in using the
verb vector only as a proxy to sentence similar-
ity.
5
On anvan2, plf outperforms the best model
5
We report state of the art from Kartsaklis and Sadrzadeh
models anvan anvan tfds msr onwn
1 2 vid
add 8 22 -0.2 78 66
mult 8 -4 -2.3 77 55
lf 15 30 5.90 NA NA
plf 20 36 2.7 79 67
soa 22 27 11.4 87 75
baseline 8 22 7.9 77 55
Table 5: Performance of composition models on
all evaluation sets. Figures of merit follow previ-
ous art on each set and are: percentage Spearman
coefficients for anvan1 and anvan2, t-standardized
average difference between mean cosines with
paraphrases and with foils for tfds, percentage
Pearson coefficients for msrvid and onwn. State-
of-the-art (soa) references: anvan1: Kartsaklis and
Sadrzadeh (2013); anvan2: Grefenstette (2013);
tfds: The Pham et al (2013); msrvid: B?ar et
al. (2012); onwn: Han et al (2013). Baselines:
anvan1/anvan2: verb vectors only; tfds: word
overlap; msrvid/onwn: word overlap + sentence
length.
reported by Grefenstette (2013) (an implementa-
tion of the lexical function ideas along the lines of
Grefenstette and Sadrzadeh (2011a; 2011b)). And
lf is, again, the only model, besides plf, that per-
forms better than the baseline.
In the tfds task, not surprisingly the add and
mult models, lacking determiner representations
and being order-insensitive, fail to distinguish be-
tween true paraphrases and foils (indeed, for the
mult model foils are significantly closer to the tar-
gets than the paraphrases, probably because the
latter have lower content word overlap than the
foils, that often differ in word order and determin-
ers only). Our plf approach is able to handle deter-
miners and word order correctly, as demonstrated
by a highly significant (p < 0.01) difference be-
tween paraphrase and foil similarity (average dif-
ference in cosine .017, standard deviation .077). In
this case, however, the traditional lf model (aver-
age difference .044, standard deviation .092) out-
performs plf. Since determiners are handled iden-
tically under the two approaches, the culprit must
be word order. We conjecture that the lf 3-way
tensor representation of transitive verbs leads to
a stronger asymmetry between sentences with in-
(2013) rather than Kartsaklis et al (2013), since only the for-
mer used a source corpus that is comparable to ours.
96
verted arguments, and thus makes this model par-
ticularly sensitive to word order differences. In-
deed, if we limit evaluation to those foils charac-
terized by word order changes only, lf discrim-
inates between paraphrases and foils even more
clearly, whereas the plf difference, while still sig-
nificant, decreases slightly.
The state-of-the-art row for tfds reports the lf
implementation by The Pham et al (2013), which
outperforms ours. The main difference is that
Pham and colleagues do not normalize vectors like
we do. If we don?t normalize, we do get larger dif-
ferences for our models as well, but consistently
lower performance in all other tasks. More wor-
ryingly, the simple word overlap baseline reported
in the table sports a larger difference than our best
model. Clearly, this baseline is exploiting the sys-
tematic determiner differences in the foils and, in-
deed, when it is evaluated on foils where only
word order changes its performance is no longer
significant.
On msrvid, the plf approach outperforms add
and mult, although the difference between the
three is not big. Our result stands in contrast with
Blacoe and Lapata (2012), the only study we are
aware of that compared a sophisticated composi-
tion model (Socher et al?s 2011 model) to add
and mult on realistic sentences, which attained the
top performance with the simple models for both
figures of merit they used.
6
The best 2012 STS
system (B?ar et al, 2012), obtained 0.87 correla-
tion, but with many more and considerably more
complex features than the ones we used here. In-
deed, our simple system would have obtained a re-
spectable 25/89 ranking in the STS 2012 msrvid
task. Still, we must also stress the impressive per-
formance of our baseline, given by the combina-
tion of the word overlap and sentence length cues.
This suggests that the msrvid benchmark lacks the
lexical and syntactic variety we would like to test
our systems on.
Our plf model is again the best on the onwn
set (albeit by a small margin over add). This
is a very positive result, in the light of the fact
that the parser has very low performance on the
onwn glosses, thus suggesting that plf can pro-
duce sensible semantic vectors from noisy syntac-
6
We refer here to the results reported in the er-
ratum available at http://homepages.inf.ed.ac.
uk/s1066731/pdf/emnlp2012erratum.pdf. The
add/mult advantage was even more marked in the original pa-
per.
tic representations. Here the overlap+length base-
line does not perform so well, and again the best
STS 2013 system (Han et al, 2013) uses consider-
ably richer knowledge sources and algorithms than
ours. Our plf-based method would have reached a
respectable 20/90 rank in the STS 2013 onwn task.
As a final remark, in all experiments the running
time of plf was only slightly larger than for the
simpler models, but orders of magnitude smaller
than lf, confirming another practical side of our
approach.
4 Conclusion
We introduced an approach to compositional dis-
tributional semantics based on a linguistically-
motivated syntax-to-semantics type mapping, but
simple and flexible enough that it can produce rep-
resentations of English sentences of arbitrary size
and structure.
We showed that our approach is competitive
against the more complex lexical function model
when evaluated on the simple constructions the
latter can be applied to, and it outperforms the ad-
ditive and multiplicative compositionality models
when tested on more realistic benchmarks (where
the full-fledged lexical function approach is dif-
ficult or impossible to use), even in presence of
strong noise in its syntactic input. While our re-
sults are encouraging, no current benchmark com-
bines large-scale, real-life data with the syntactic
variety on which a syntax-driven approach to se-
mantics such as ours could truly prove its worth.
The recently announced SemEval 2014 Task 1
7
is
filling exactly this gap, and we look forward to ap-
ply our method to this new benchmark, as soon as
it becomes available.
One of the strengths of our framework is that
it allows for incremental improvement focused on
specific constructions. For example, one could add
representations for different conjunctions (and vs.
or), train matrices for verb arguments other than
subject and direct object, or include new types of
modifiers into the model, etc.
While there is potential for local improvements,
our framework, which extends and improves on
existing compositional semantic vector models,
has demonstrated its ability to account for full sen-
tences in a principled and elegant way. Our imple-
mentation of the model relies on simple and effi-
7
http://alt.qcri.org/semeval2014/
task1/
97
cient training, works fast, and shows good empiri-
cal results.
Acknowledgements
We thank Roberto Zamparelli and the COM-
POSES team for helpful discussions. This re-
search was supported by the ERC 2011 Starting
Independent Research Grant n. 283554 (COM-
POSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: a
pilot on semantic textual similarity. In Proceedings
of *SEM, pages 385?393, Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings
of *SEM, pages 32?43, Atlanta, GA.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of *SEM, pages
435?440, Montreal, Canada.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2013. Frege in space: A program for
compositional distributional semantics. Linguistic
Issues in Language Technology. In press; http:
//clic.cimec.unitn.it/composes/
materials/frege-in-space.pdf.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), pages 53?57, Sofia, Bulgaria.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1394?1404, Edinburgh,
UK.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of GEMS, pages 62?66, Ed-
inburgh, UK.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. PhD thesis, Uni-
versity of Oxford Essex.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Lushan Han, Abhay Kashyap, Tim Finin,
James Mayfield, and Jonathan Weese. 2013.
UMBC EBIQUITY-CORE: Semantic textual sim-
ilarity systems. In Proceedings of *SEM, pages
44?52, Atlanta, GA.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of EMNLP,
pages 1590?1601, Seattle, WA.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In Pro-
ceedings of CoNLL, pages 114?123, Sofia, Bulgaria.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
98
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Nghia The Pham, Raffaella Bernardi, Yao-Zhong
Zhang, and Marco Baroni. 2013. Sentence para-
phrase detection: When determiners and word or-
der make the difference. In Proceedings of the To-
wards a Formal Distributional Semantics Workshop
at IWCS 2013, pages 21?29, Potsdam, Germany.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
99
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 238?247,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Don?t count, predict! A systematic comparison of
context-counting vs. context-predicting semantic vectors
Marco Baroni and Georgiana Dinu and Germ
?
an Kruszewski
Center for Mind/Brain Sciences (University of Trento, Italy)
(marco.baroni|georgiana.dinu|german.kruszewski)@unitn.it
Abstract
Context-predicting models (more com-
monly known as embeddings or neural
language models) are the new kids on the
distributional semantics block. Despite the
buzz surrounding these models, the litera-
ture is still lacking a systematic compari-
son of the predictive models with classic,
count-vector-based distributional semantic
approaches. In this paper, we perform
such an extensive evaluation, on a wide
range of lexical semantics tasks and across
many parameter settings. The results, to
our own surprise, show that the buzz is
fully justified, as the context-predicting
models obtain a thorough and resounding
victory against their count-based counter-
parts.
1 Introduction
A long tradition in computational linguistics has
shown that contextual information provides a good
approximation to word meaning, since semanti-
cally similar words tend to have similar contex-
tual distributions (Miller and Charles, 1991). In
concrete, distributional semantic models (DSMs)
use vectors that keep track of the contexts (e.g.,
co-occurring words) in which target terms appear
in a large corpus as proxies for meaning represen-
tations, and apply geometric techniques to these
vectors to measure the similarity in meaning of
the corresponding words (Clark, 2013; Erk, 2012;
Turney and Pantel, 2010).
It has been clear for decades now that raw co-
occurrence counts don?t work that well, and DSMs
achieve much higher performance when various
transformations are applied to the raw vectors,
for example by reweighting the counts for con-
text informativeness and smoothing them with di-
mensionality reduction techniques. This vector
optimization process is generally unsupervised,
and based on independent considerations (for ex-
ample, context reweighting is often justified by
information-theoretic considerations, dimension-
ality reduction optimizes the amount of preserved
variance, etc.). Occasionally, some kind of indi-
rect supervision is used: Several parameter set-
tings are tried, and the best setting is chosen based
on performance on a semantic task that has been
selected for tuning.
The last few years have seen the development
of a new generation of DSMs that frame the vec-
tor estimation problem directly as a supervised
task, where the weights in a word vector are set to
maximize the probability of the contexts in which
the word is observed in the corpus (Bengio et al,
2003; Collobert and Weston, 2008; Collobert et
al., 2011; Huang et al, 2012; Mikolov et al,
2013a; Turian et al, 2010). The traditional con-
struction of context vectors is turned on its head:
Instead of first collecting context vectors and then
reweighting these vectors based on various crite-
ria, the vector weights are directly set to optimally
predict the contexts in which the corresponding
words tend to appear. Since similar words occur
in similar contexts, the system naturally learns to
assign similar vectors to similar words.
This new way to train DSMs is attractive be-
cause it replaces the essentially heuristic stacking
of vector transforms in earlier models with a sin-
gle, well-defined supervised learning step. At the
same time, supervision comes at no manual anno-
tation cost, given that the context windows used
for training can be automatically extracted from
an unannotated corpus (indeed, they are the very
same data used to build traditional DSMs). More-
over, at least some of the relevant methods can ef-
ficiently scale up to process very large amounts of
input data.
1
1
The idea to directly learn a parameter vector based on
an objective optimum function is shared by Latent Dirichlet
238
We will refer to DSMs built in the traditional
way as count models (since they initialize vectors
with co-occurrence counts), and to their training-
based alternative as predict(ive) models.
2
Now,
the most natural question to ask, of course, is
which of the two approaches is best in empirical
terms. Surprisingly, despite the long tradition of
extensive evaluations of alternative count DSMs
on standard benchmarks (Agirre et al, 2009; Ba-
roni and Lenci, 2010; Bullinaria and Levy, 2007;
Bullinaria and Levy, 2012; Sahlgren, 2006; Pad?o
and Lapata, 2007), the existing literature contains
very little in terms of direct comparison of count
vs. predictive DSMs. This is in part due to the fact
that context-predicting vectors were first devel-
oped as an approach to language modeling and/or
as a way to initialize feature vectors in neural-
network-based ?deep learning? NLP architectures,
so their effectiveness as semantic representations
was initially seen as little more than an interest-
ing side effect. Sociological reasons might also be
partly responsible for the lack of systematic com-
parisons: Context-predictive models were devel-
oped within the neural-network community, with
little or no awareness of recent DSM work in com-
putational linguistics.
Whatever the reasons, we know of just three
works reporting direct comparisons, all limited in
their scope. Huang et al (2012) compare, in pass-
ing, one count model and several predict DSMs
on the standard WordSim353 benchmark (Table
3 of their paper). In this experiment, the count
model actually outperforms the best predictive ap-
proach. Instead, in a word-similarity-in-context
task (Table 5), the best predict model outperforms
the count model, albeit not by a large margin.
Blacoe and Lapata (2012) compare count and
predict representations as input to composition
functions. Count vectors make for better inputs
in a phrase similarity task, whereas the two repre-
sentations are comparable in a paraphrase classifi-
cation experiment.
3
Allocation (LDA) models (Blei et al, 2003; Griffiths et al,
2007), where parameters are set to optimize the joint prob-
ability distribution of words and documents. However, the
fully probabilistic LDA models have problems scaling up to
large data sets.
2
We owe the first term to Hinrich Sch?utze (p.c.). Predic-
tive DSMs are also called neural language models, because
their supervised context prediction training is performed with
neural networks, or, more cryptically, ?embeddings?.
3
We refer here to the updated results reported in
the erratum at http://homepages.inf.ed.ac.uk/
s1066731/pdf/emnlp2012erratum.pdf
Finally, Mikolov et al (2013d) compare their
predict models to ?Latent Semantic Analysis?
(LSA) count vectors on syntactic and semantic
analogy tasks, finding that the predict models are
highly superior. However, they provide very little
details about the LSA count vectors they use.
4
In this paper, we overcome the comparison
scarcity problem by providing a direct evaluation
of count and predict DSMs across many parameter
settings and on a large variety of mostly standard
lexical semantics benchmarks. Our title already
gave away what we discovered.
2 Distributional semantic models
Both count and predict models are extracted from
a corpus of about 2.8 billion tokens constructed
by concatenating ukWaC,
5
the English Wikipedia
6
and the British National Corpus.
7
For both model
types, we consider the top 300K most frequent
words in the corpus both as target and context ele-
ments.
2.1 Count models
We prepared the count models using the DISSECT
toolkit.
8
We extracted count vectors from sym-
metric context windows of two and five words to
either side of target. We considered two weight-
ing schemes: positive Pointwise Mutual Informa-
tion and Local Mutual Information (akin to the
widely used Log-Likelihood Ratio scheme) (Ev-
ert, 2005). We used both full and compressed vec-
tors. The latter were obtained by applying the Sin-
gular Value Decomposition (Golub and Van Loan,
1996) or Non-negative Matrix Factorization (Lee
and Seung, 2000), Lin (2007) algorithm, with re-
duced sizes ranging from 200 to 500 in steps of
100. In total, 36 count models were evaluated.
Count models have such a long and rich his-
tory that we can only explore a small subset of
the counting, weighting and compressing meth-
ods proposed in the literature. However, it is
worth pointing out that the evaluated parameter
subset encompasses settings (narrow context win-
dow, positive PMI, SVD reduction) that have been
4
Chen et al (2013) present an extended empirical evalua-
tion, that is however limited to alternative context-predictive
models, and does not include the word2vec variant we use
here.
5
http://wacky.sslmit.unibo.it
6
http://en.wikipedia.org
7
http://www.natcorp.ox.ac.uk
8
http://clic.cimec.unitn.it/composes/
toolkit/
239
found to be most effective in the systematic explo-
rations of the parameter space conducted by Bul-
linaria and Levy (2007; 2012).
2.2 Predict models
We trained our predict models with the word2vec
toolkit.
9
The toolkit implements both the skip-
gram and CBOW approaches of Mikolov et
al. (2013a; 2013c). We experimented only with
the latter, which is also the more computationally-
efficient model of the two, following Mikolov et
al. (2013b) which recommends CBOW as more
suitable for larger datasets.
The CBOW model learns to predict the word in
the middle of a symmetric window based on the
sum of the vector representations of the words in
the window. We considered context windows of
2 and 5 words to either side of the central ele-
ment. We vary vector dimensionality within the
200 to 500 range in steps of 100. The word2vec
toolkit implements two efficient alternatives to the
standard computation of the output word proba-
bility distributions by a softmax classifier. Hi-
erarchical softmax is a computationally efficient
way to estimate the overall probability distribu-
tion using an output layer that is proportional to
log(unigram.perplexity(W )) instead of W (for
W the vocabulary size). As an alternative, nega-
tive sampling estimates the probability of an out-
put word by learning to distinguish it from draws
from a noise distribution. The number of these
draws (number of negative samples) is given by
a parameter k. We test both hierarchical softmax
and negative sampling with k values of 5 and 10.
Very frequent words such as the or a are not very
informative as context features. The word2vec
toolkit implements a method to downsize their ef-
fect (and simultaneously improve speed perfor-
mance). More precisely, words in the training
data are discarded with a probability that is pro-
portional to their frequency (capturing the same
intuition that motivates traditional count vector
weighting measures such as PMI). This is con-
trolled by a parameter t and words that occur with
higher frequency than t are aggressively subsam-
pled. We train models without subsampling and
with subsampling at t = 1e
?5
(the toolkit page
suggests 1e
?3
? 1e
?5
as a useful range based on
empirical observations).
In total, we evaluate 48 predict models, a num-
9
https://code.google.com/p/word2vec/
ber comparable to that of the count models we
consider.
2.3 Out-of-the-box models
Baroni and Lenci (2010) make the vectors of
their best-performing Distributional Memory (dm)
model available.
10
This model, based on the same
input corpus we use, exemplifies a ?linguistically
rich? count-based DSM, that relies on lemmas
instead or raw word forms, and has dimensions
that encode the syntactic relations and/or lexico-
syntactic patterns linking targets and contexts. Ba-
roni and Lenci showed, in a large scale evaluation,
that dm reaches near-state-of-the-art performance
in a variety of semantic tasks.
We also experiment with the popular predict
vectors made available by Ronan Collobert.
11
Fol-
lowing the earlier literature, with refer to them
as Collobert and Weston (cw) vectors. These are
100-dimensional vectors trained for two months
(!) on the Wikipedia. In particular, the vectors
were trained to optimize the task of choosing the
right word over a random alternative in the middle
of an 11-word context window (Collobert et al,
2011).
3 Evaluation materials
We test our models on a variety of benchmarks,
most of them already widely used to test and com-
pare DSMs. The following benchmark descrip-
tions also explain the figures of merit and state-
of-the-art results reported in Table 2.
Semantic relatedness A first set of semantic
benchmarks was constructed by asking human
subjects to rate the degree of semantic similarity
or relatedness between two words on a numeri-
cal scale. The performance of a computational
model is assessed in terms of correlation between
the average scores that subjects assigned to the
pairs and the cosines between the corresponding
vectors in the model space (following the previ-
ous art, we use Pearson correlation for rg, Spear-
man in all other cases). The classic data set of
Rubenstein and Goodenough (1965) (rg) consists
of 65 noun pairs. State of the art performance
on this set has been reported by Hassan and Mi-
halcea (2011) using a technique that exploits the
Wikipedia linking structure and word sense dis-
ambiguation techniques. Finkelstein et al (2002)
10
http://clic.cimec.unitn.it/dm/
11
http://ronan.collobert.com/senna/
240
introduced the widely used WordSim353 set (ws)
that, as the name suggests, consists of 353 pairs.
The current state of the art is reached by Halawi
et al (2012) with a method that is in the spirit
of the predict models, but lets synonymy infor-
mation from WordNet constrain the learning pro-
cess (by favoring solutions in which WordNet syn-
onyms are near in semantic space). Agirre et al
(2009) split the ws set into similarity (wss) and re-
latedness (wsr) subsets. The first contains tighter
taxonomic relations, such as synonymy and co-
hyponymy (king/queen) whereas the second en-
compasses broader, possibly topical or syntag-
matic relations (family/planning). We report state-
of-the-art performance on the two subsets from the
work of Agirre and colleagues, who used different
kinds of count vectors extracted from a very large
corpus (orders of magnitude larger than ours). Fi-
nally, we use (the test section of) MEN (men), that
comprises 1,000 word pairs. Bruni et al (2013),
the developers of this benchmark, achieve state-of-
the-art performance by extensive tuning on ad-hoc
training data, and by using both textual and image-
extracted features to represent word meaning.
Synonym detection The classic TOEFL (toefl)
set was introduced by Landauer and Dumais
(1997). It contains 80 multiple-choice questions
that pair a target term with 4 synonym candidates.
For example, for the target levied one must choose
between imposed (correct), believed, requested
and correlated. The DSMs compute cosines of
each candidate vector with the target, and pick the
candidate with largest cosine as their answer. Per-
formance is evaluated in terms of correct-answer
accuracy. Bullinaria and Levy (2012) achieved
100% accuracy by a very thorough exploration of
the count model parameter space.
Concept categorization Given a set of nominal
concepts, the task is to group them into natural cat-
egories (e.g., helicopters and motorcycles should
go to the vehicle class, dogs and elephants into the
mammal class). Following previous art, we tackle
categorization as an unsupervised clustering task.
The vectors produced by a model are clustered
into n groups (with n determined by the gold stan-
dard partition) using the CLUTO toolkit (Karypis,
2003), with the repeated bisections with global op-
timization method and CLUTO?s default settings
otherwise (these are standard choices in the liter-
ature). Performance is evaluated in terms of pu-
rity, a measure of the extent to which each cluster
contains concepts from a single gold category. If
the gold partition is reproduced perfectly, purity
reaches 100%; it approaches 0 as cluster quality
deteriorates. The Almuhareb-Poesio (ap) bench-
mark contains 402 concepts organized into 21 cat-
egories (Almuhareb, 2006). State-of-the-art purity
was reached by Rothenh?ausler and Sch?utze (2009)
with a count model based on carefully crafted syn-
tactic links. The ESSLLI 2008 Distributional Se-
mantic Workshop shared-task set (esslli) contains
44 concepts to be clustered into 6 categories (Ba-
roni et al, 2008) (we ignore here the 3- and 2-
way higher-level partitions coming with this set).
Katrenko and Adriaans (2008) reached top per-
formance on this set using the full Web as a cor-
pus and manually crafted, linguistically motivated
patterns. Finally, the Battig (battig) test set intro-
duced by Baroni et al (2010) includes 83 concepts
from 10 categories. Current state of the art was
reached by the window-based count model of Ba-
roni and Lenci (2010).
Selectional preferences We experiment with
two data sets that contain verb-noun pairs that
were rated by subjects for the typicality of the
noun as a subject or object of the verb (e.g., peo-
ple received a high average score as subject of
to eat, and a low score as object of the same
verb). We follow the procedure proposed by Ba-
roni and Lenci (2010) to tackle this challenge: For
each verb, we use the corpus-based tuples they
make available to select the 20 nouns that are most
strongly associated to the verb as subjects or ob-
jects, and we average the vectors of these nouns
to obtain a ?prototype? vector for the relevant ar-
gument slot. We then measure the cosine of the
vector for a target noun with the relevant proto-
type vector (e.g., the cosine of people with the eat-
ing subject prototype vector). Systems are eval-
uated by Spearman correlation of these cosines
with the averaged human typicality ratings. Our
first data set was introduced by Ulrike Pad?o (2007)
and includes 211 pairs (up). Top-performance was
reached by the supervised count vector system of
Herda?gdelen and Baroni (2009) (supervised in the
sense that they directly trained a classifier on gold
data, as opposed to the 0-cost supervision of the
context-learning methods). The mcrae set (McRae
et al, 1998) consists of 100 noun?verb pairs, with
top performance reached by the DepDM system of
Baroni and Lenci (2010), a count DSM relying on
241
syntactic information.
Analogy While all the previous data sets are rel-
atively standard in the DSM field to test traditional
count models, our last benchmark was introduced
in Mikolov et al (2013a) specifically to test pre-
dict models. The data-set contains about 9K se-
mantic and 10.5K syntactic analogy questions. A
semantic question gives an example pair (brother-
sister), a test word (grandson) and asks to find
another word that instantiates the relation illus-
trated by the example with respect to the test word
(granddaughter). A syntactic question is similar,
but in this case the relationship is of a grammatical
nature (work?works, speak. . . speaks). Mikolov
and colleagues tackle the challenge by subtract-
ing the second example term vector from the first,
adding the test term, and looking for the nearest
neighbour of the resulting vector (what is the near-
est neighbour of
~
brother?
~
sister+
~
grandson?).
Systems are evaluated in terms of proportion of
questions where the nearest neighbour from the
whole semantic space is the correct answer (the
given example and test vector triples are excluded
from the nearest neighbour search). Mikolov et al
(2013a) reach top accuracy on the syntactic subset
(ansyn) with a CBOW predict model akin to ours
(but trained on a corpus twice as large). Top ac-
curacy on the entire data set (an) and on the se-
mantic subset (ansem) was reached by Mikolov
et al (2013c) using a skip-gram predict model.
Note however that, because of the way the task
is framed, performance also depends on the size
of the vocabulary to be searched: Mikolov et al
(2013a) pick the nearest neighbour among vectors
for 1M words, Mikolov et al (2013c) among 700K
words, and we among 300K words.
Some characteristics of the benchmarks we use
are summarized in Table 1.
4 Results
Table 2 summarizes the evaluation results. The
first block of the table reports the maximum per-
task performance (across all considered parameter
settings) for count and predict vectors. The latter
emerge as clear winners, with a large margin over
count vectors in most tasks. Indeed, the predic-
tive models achieve an impressive overall perfor-
mance, beating the current state of the art in sev-
eral cases, and approaching it in many more. It is
worth stressing that, as reviewed in Section 3, the
state-of-the-art results were obtained in almost all
cases using specialized approaches that rely on ex-
ternal knowledge, manually-crafted rules, parsing,
larger corpora and/or task-specific tuning. Our
predict results were instead achieved by simply
downloading the word2vec toolkit and running it
with a range of parameter choices recommended
by the toolkit developers.
The success of the predict models cannot be
blamed on poor performance of the count mod-
els. Besides the fact that this would not explain
the near-state-of-the-art performance of the pre-
dict vectors, the count model results are actually
quite good in absolute terms. Indeed, in several
cases they are close, or even better than those at-
tained by dm, a linguistically-sophisticated count-
based approach that was shown to reach top per-
formance across a variety of tasks by Baroni and
Lenci (2010).
Interestingly, count vectors achieve perfor-
mance comparable to that of predict vectors only
on the selectional preference tasks. The up task
in particular is also the only benchmark on which
predict models are seriously lagging behind state-
of-the-art and dm performance. Recall from Sec-
tion 3 that we tackle selectional preference by cre-
ating average vectors representing typical verb ar-
guments. We conjecture that this averaging ap-
proach, that worked well for dm vectors, might
be problematic for prediction-trained vectors, and
we plan to explore alternative methods to build the
prototypes in future research.
Are our results robust to parameter choices, or
are they due to very specific and brittle settings?
The next few blocks of Table 2 address this ques-
tion. The second block reports results obtained
with single count and predict models that are best
in terms of average performance rank across tasks
(these are the models on the top rows of tables
3 and 4, respectively). We see that, for both ap-
proaches, performance is not seriously affected by
using the single best setup rather than task-specific
settings, except for a considerable drop in perfor-
mance for the best predict model on esslli (due to
the small size of this data set?), and an even more
dramatic drop of the count model on ansem. A
more cogent and interesting evaluation is reported
in the third block of Table 2, where we see what
happens if we use the single models with worst
performance across tasks (recall from Section 2
above that, in any case, we are exploring a space
of reasonable parameter settings, of the sort that an
242
name task measure source soa
rg relatedness Pearson Rubenstein and Goodenough Hassan and Mihalcea (2011)
(1965)
ws relatedness Spearman Finkelstein et al (2002) Halawi et al (2012)
wss relatedness Spearman Agirre et al (2009) Agirre et al (2009)
wsr relatedness Spearman Agirre et al (2009) Agirre et al (2009)
men relatedness Spearman Bruni et al (2013) Bruni et al (2013)
toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012)
(1997)
ap categorization purity Almuhareb (2006) Rothenh?ausler and Sch?utze
(2009)
esslli categorization purity Baroni et al (2008) Katrenko and Adriaans
(2008)
battig categorization purity Baroni et al (2010) Baroni and Lenci (2010)
up sel pref Spearman Pad?o (2007) Herda?gdelen and Baroni
(2009)
mcrae sel pref Spearman McRae et al (1998) Baroni and Lenci (2010)
an analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)
ansyn analogy accuracy Mikolov et al (2013a) Mikolov et al (2013a)
ansem analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)
Table 1: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference
(source) and reference to current state-of-the-art system (soa).
rg ws wss wsr men toefl ap esslli battig up mcrae an ansyn ansem
best setup on each task
cnt 74 62 70 59 72 76 66 84 98 41 27 49 43 60
pre 84 75 80 70 80 91 75 86 99 41 28 68 71 66
best setup across tasks
cnt 70 62 70 57 72 76 64 84 98 37 27 43 41 44
pre 83 73 78 68 80 86 71 77 98 41 26 67 69 64
worst setup across tasks
cnt 11 16 23 4 21 49 24 43 38 -6 -10 1 0 1
pre 74 60 73 48 68 71 65 82 88 33 20 27 40 10
best setup on rg
cnt (74) 59 66 52 71 64 64 84 98 37 20 35 42 26
pre (84) 71 76 64 79 85 72 84 98 39 25 66 70 61
other models
soa 86 81 77 62 76 100 79 91 96 60 32 61 64 61
dm 82 35 60 13 42 77 76 84 94 51 29 NA NA NA
cw 48 48 61 38 57 56 58 61 70 28 15 11 12 9
Table 2: Performance of count (cnt), predict (pre), dm and cw models on all tasks. See Section 3 and
Table 1 for figures of merit and state-of-the-art results (soa). Since dm has very low coverage of the an*
data sets, we do not report its performance there.
243
experimenter might be tempted to choose without
tuning). The count model performance is severely
affected by this unlucky choice (2-word window,
Local Mutual Information, NMF, 400 dimensions,
mean performance rank: 83), whereas the predict
approach is much more robust: To put its worst in-
stantiation (2-word window, hierarchical softmax,
no subsampling, 200 dimensions, mean rank: 51)
into perspective, its performance is more than 10%
below the best count model only for the an and
ansem tasks, and actually higher than it in 3 cases
(note how on esslli the worst predict models per-
forms much better than the best one, confirming
our suspicion about the brittleness of this small
data set). The fourth block reports performance in
what might be the most realistic scenario, namely
by tuning the parameters on a development task.
Specifically, we pick the models that work best
on the small rg set, and report their performance
on all tasks (we obtained similar results by pick-
ing other tuning sets). The selected count model
is the third best overall model of its class as re-
ported in Table 3. The selected predict model is
the fourth best model in Table 4. The overall count
performance is not greatly affected by this choice.
Again, predict models confirm their robustness,
in that their rg-tuned performance is always close
(and in 3 cases better) than the one achieved by the
best overall setup.
Tables 3 and 4 let us take a closer look at
the most important count and predict parame-
ters, by reporting the characteristics of the best
models (in terms of average performance-based
ranking across tasks) from both classes. For the
count models, PMI is clearly the better weight-
ing scheme, and SVD outperforms NMF as a di-
mensionality reduction technique. However, no
compression at all (using all 300K original dimen-
sions) works best. Compare this to the best over-
all predict vectors, that have 400 dimensions only,
making them much more practical to use. For the
predict models, we observe in Table 4 that nega-
tive sampling, where the task is to distinguish the
target output word from samples drawn from the
noise distribution, outperforms the more costly hi-
erarchical softmax method. Subsampling frequent
words, which downsizes the importance of these
words similarly to PMI weighting in count mod-
els, is also bringing significant improvements.
Finally, we go back to Table 2 to point out the
poor performance of the out-of-the-box cw model.
window weight compress dim. mean
rank
2 PMI no 300K 35
5 PMI no 300K 38
2 PMI SVD 500 42
2 PMI SVD 400 46
5 PMI SVD 500 47
2 PMI SVD 300 50
5 PMI SVD 400 51
2 PMI NMF 300 52
2 PMI NMF 400 53
5 PMI SVD 300 53
Table 3: Top count models in terms of mean
performance-based model ranking across all tasks.
The first row states that the window-2, PMI, 300K
count model was the best count model, and, across
all tasks, its average rank, when ALL models are
decreasingly ordered by performance, was 35. See
Section 2.1 for explanation of the parameters.
We must leave the investigation of the parameters
that make our predict vectors so much better than
cw (more varied training corpus? window size?
objective function being used? subsampling? . . . )
to further work. Still, our results show that it?s
not just training by context prediction that ensures
good performance. The cw approach is very popu-
lar (for example both Huang et al (2012) and Bla-
coe and Lapata (2012) used it in the studies we dis-
cussed in Section 1). Had we also based our sys-
tematic comparison of count and predict vectors
on the cw model, we would have reached opposite
conclusions from the ones we can draw from our
word2vec-trained vectors!
5 Conclusion
This paper has presented the first systematic com-
parative evaluation of count and predict vectors.
As seasoned distributional semanticists with thor-
ough experience in developing and using count
vectors, we set out to conduct this study because
we were annoyed by the triumphalist overtones of-
ten surrounding predict models, despite the almost
complete lack of a proper comparison to count
vectors.
12
Our secret wish was to discover that it is
all hype, and count vectors are far superior to their
predictive counterparts. A more realistic expec-
12
Here is an example, where word2vec is called the crown
jewel of natural language processing: http://bit.ly/
1ipv72M
244
win. hier. neg. subsamp. dim mean
softm. samp. rank
5 no 10 yes 400 10
2 no 10 yes 300 13
5 no 5 yes 400 13
5 no 5 yes 300 13
5 no 10 yes 300 13
2 no 10 yes 400 13
2 no 5 yes 400 15
5 no 10 yes 200 15
2 no 10 yes 500 15
2 no 5 yes 300 16
Table 4: Top predict models in terms of mean
performance-based model ranking across all tasks.
See Section 2.2 for explanation of the parameters.
tation was that a complex picture would emerge,
with predict and count vectors beating each other
on different tasks. Instead, we found that the pre-
dict models are so good that, while the triumphal-
ist overtones still sound excessive, there are very
good reasons to switch to the new architecture.
However, due to space limitations we have only
focused here on quantitative measures: It remains
to be seen whether the two types of models are
complementary in the errors they make, in which
case combined models could be an interesting av-
enue for further work.
The space of possible parameters of count
DSMs is very large, and it?s entirely possible that
some options we did not consider would have im-
proved count vector performance somewhat. Still,
given that the predict vectors also outperformed
the syntax-based dm model, and often approxi-
mated state-of-the-art performance, a more profic-
uous way forward might be to focus on parameters
and extensions of the predict models instead: Af-
ter all, we obtained our already excellent results
by just trying a few variations of the word2vec de-
faults. Add to this that, beyond the standard lex-
ical semantics challenges we tested here, predict
models are currently been successfully applied in
cutting-edge domains such as representing phrases
(Mikolov et al, 2013c; Socher et al, 2012) or fus-
ing language and vision in a common semantic
space (Frome et al, 2013; Socher et al, 2013).
Based on the results reported here and the con-
siderations we just made, we would certainly rec-
ommend anybody interested in using DSMs for
theoretical or practical applications to go for the
predict models, with the important caveat that they
are not all created equal (cf. the big difference be-
tween word2vec and cw models). At the same
time, given the large amount of work that has been
carried out on count DSMs, we would like to ex-
plore, in the near future, how certain questions
and methods that have been considered with re-
spect to traditional DSMs will transfer to predict
models. For example, the developers of Latent
Semantic Analysis (Landauer and Dumais, 1997),
Topic Models (Griffiths et al, 2007) and related
DSMs have shown that the dimensions of these
models can be interpreted as general ?latent? se-
mantic domains, which gives the corresponding
models some a priori cognitive plausibility while
paving the way for interesting applications. An-
other important line of DSM research concerns
?context engineering?: There has been for exam-
ple much work on how to encode syntactic in-
formation into context features (Pad?o and Lapata,
2007), and more recent studies construct and com-
bine feature spaces expressing topical vs. func-
tional information (Turney, 2012). To give just
one last example, distributional semanticists have
looked at whether certain properties of vectors re-
flect semantic relations in the expected way: e.g.,
whether the vectors of hypernyms ?distribution-
ally include? the vectors of hyponyms in some
mathematical precise sense.
Do the dimensions of predict models also en-
code latent semantic domains? Do these models
afford the same flexibility of count vectors in cap-
turing linguistically rich contexts? Does the struc-
ture of predict vectors mimic meaningful seman-
tic relations? Does all of this even matter, or are
we on the cusp of discovering radically new ways
to tackle the same problems that have been ap-
proached as we just sketched in traditional distri-
butional semantics?
Either way, the results of the present investiga-
tion indicate that these are important directions for
future research in computational semantics.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009.
245
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of HLT-NAACL, pages 19?27, Boulder, CO.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-
itors. 2008. Bridging the Gap between Semantic
Theory and Computational Simulations: Proceed-
ings of the ESSLLI Workshop on Distributional Lex-
ical Semantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elia Bruni, Nam Khanh Tran, and Marco Ba-
roni. 2013. Multimodal distributional seman-
tics. Journal of Artificial Intelligence Research.
In press; http://clic.cimec.unitn.it/
marco/publications/mmds-jair.pdf.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
John Bullinaria and Joseph Levy. 2012. Extracting
semantic representations from word co-occurrence
statistics: Stop-lists, stemming and SVD. Behavior
Research Methods, 44:890?907.
Yanqing Chen, Bryan Perozzi, Rami Al-Rfou?, and
Steven Skiena. 2013. The expressive power of
word embeddings. In Proceedings of the ICML
Workshop on Deep Learning for Audio, Speech and
Language Processing, Atlanta, GA. Published on-
line: https://sites.google.com/site/
deeplearningicml2013/accepted_
papers.
Stephen Clark. 2013. Vector space mod-
els of lexical meaning. In Shalom Lappin
and Chris Fox, editors, Handbook of Contem-
porary Semantics, 2nd ed. Blackwell, Malden,
MA. In press; http://www.cl.cam.ac.uk/
?
sc609/pubs/sem_handbook.pdf.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167, Helsinki, Fin-
land.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of
word relatedness with constraints. In Proceedings
of KDD, pages 1406?1414.
Samer Hassan and Rada Mihalcea. 2011. Semantic
relatedness using salient semantic analysis. In Pro-
ceedings of AAAI, pages 884?889, San Francisco,
CA.
Amac? Herda?gdelen and Marco Baroni. 2009. Bag-
Pack: A general framework to represent semantic
relations. In Proceedings of GEMS, pages 33?40,
Athens, Greece.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of ACL, pages 873?882, Jeju
Island, Korea.
George Karypis. 2003. CLUTO: A clustering toolkit.
Technical Report 02-017, University of Minnesota
Department of Computer Science.
246
Sophia Katrenko and Pieter Adriaans. 2008. Qualia
structures and their impact on the concrete noun
categorization task. In Proceedings of the ESS-
LLI Workshop on Distributional Lexical Semantics,
pages 17?24, Hamburg, Germany.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756?2779.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of the-
matic fit (and other constraints) in on-line sentence
comprehension. Journal of Memory and Language,
38:283?312.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for Ma-
chine Translation. http://arxiv.org/abs/
1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013c. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111?3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013d. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746?751, Atlanta, Georgia.
George Miller and Walter Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ulrike Pad?o. 2007. The Integration of Syntax and
Semantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Dissertation, Saarland Univer-
sity, Saarbr?ucken.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of GEMS, pages 17?
24, Athens, Greece.
Herbert Rubenstein and John Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL, pages 384?394, Uppsala, Sweden.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
247
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 624?633,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How to make words with vectors:
Phrase generation in distributional semantics
Georgiana Dinu and Marco Baroni
Center for Mind/Brain Sciences
University of Trento, Italy
(georgiana.dinu|marco.baroni)@unitn.it
Abstract
We introduce the problem of generation
in distributional semantics: Given a distri-
butional vector representing some mean-
ing, how can we generate the phrase that
best expresses that meaning? We mo-
tivate this novel challenge on theoretical
and practical grounds and propose a sim-
ple data-driven approach to the estimation
of generation functions. We test this in
a monolingual scenario (paraphrase gen-
eration) as well as in a cross-lingual set-
ting (translation by synthesizing adjective-
noun phrase vectors in English and gener-
ating the equivalent expressions in Italian).
1 Introduction
Distributional methods for semantics approximate
the meaning of linguistic expressions with vectors
that summarize the contexts in which they occur
in large samples of text. This has been a very suc-
cessful approach to lexical semantics (Erk, 2012),
where semantic relatedness is assessed by compar-
ing vectors. Recently these methods have been
extended to phrases and sentences by means of
composition operations (see Baroni (2013) for an
overview). For example, given the vectors repre-
senting red and car, composition derives a vector
that approximates the meaning of red car.
However, the link between language and mean-
ing is, obviously, bidirectional: As message recip-
ients we are exposed to a linguistic expression and
we must compute its meaning (the synthesis prob-
lem). As message producers we start from the
meaning we want to communicate (a ?thought?)
and we must encode it into a word sequence (the
generation problem). If distributional semantics
is to be considered a proper semantic theory, then
it must deal not only with synthesis (going from
words to vectors), but also with generation (from
vectors to words).
Besides these theoretical considerations, phrase
generation from vectors has many useful applica-
tions. We can, for example, synthesize the vector
representing the meaning of a phrase or sentence,
and then generate alternative phrases or sentences
from this vector to accomplish true paraphrase
generation (as opposed to paraphrase detection or
ranking of candidate paraphrases).
Generation can be even more useful when the
source vector comes from another modality or lan-
guage. Recent work on grounding language in vi-
sion shows that it is possible to represent images
and linguistic expressions in a common vector-
based semantic space (Frome et al, 2013; Socher
et al, 2013). Given a vector representing an im-
age, generation can be used to productively con-
struct phrases or sentences that describe the im-
age (as opposed to simply retrieving an existing
description from a set of candidates). Translation
is another potential application of the generation
framework: Given a semantic space shared be-
tween two or more languages, one can compose a
word sequence in one language and generate trans-
lations in another, with the shared semantic vector
space functioning as interlingua.
Distributional semantics assumes a lexicon of
atomic expressions (that, for simplicity, we take
to be words), each associated to a vector. Thus,
at the single-word level, the problem of genera-
tion is solved by a trivial generation-by-synthesis
approach: Given an arbitrary target vector, ?gener-
ate? the corresponding word by searching through
the lexicon for the word with the closest vector to
the target. This is however unfeasible for larger
expressions: Given n vocabulary elements, this
approach requires checking n
k
phrases of length
k. This becomes prohibitive already for relatively
short phrases, as reasonably-sized vocabularies do
not go below tens of thousands of words. The
search space for 3-word phrases in a 10K-word
vocabulary is already in the order of trillions. In
624
this paper, we introduce a more direct approach to
phrase generation, inspired by the work in com-
positional distributional semantics. In short, we
revert the composition process and we propose
a framework of data-induced, syntax-dependent
functions that decompose a single vector into a
vector sequence. The generated vectors can then
be efficiently matched against those in the lexicon
or fed to the decomposition system again to pro-
duce longer phrases recursively.
2 Related work
To the best of our knowledge, we are the first to
explicitly and systematically pursue the generation
problem in distributional semantics. Kalchbrenner
and Blunsom (2013) use top-level, composed dis-
tributed representations of sentences to guide gen-
eration in a machine translation setting. More pre-
cisely, they condition the target language model
on the composed representation (addition of word
vectors) of the source language sentence.
Andreas and Ghahramani (2013) discuss the
the issue of generating language from vectors and
present a probabilistic generative model for distri-
butional vectors. However, their emphasis is on
reversing the generative story in order to derive
composed meaning representations from word se-
quences. The theoretical generating capabilities of
the methods they propose are briefly exemplified,
but not fully explored or tested.
Socher et al (2011) come closest to our target
problem. They introduce a bidirectional language-
to-meaning model for compositional distributional
semantics that is similar in spirit to ours. How-
ever, we present a clearer decoupling of synthesis
and generation and we use different (and simpler)
training methods and objective functions. More-
over, Socher and colleagues do not train separate
decomposition rules for different syntactic config-
urations, so it is not clear how they would be able
to control the generation of different output struc-
tures. Finally, the potential for generation is only
addressed in passing, by presenting a few cases
where the generated sequence has the same syn-
tactic structure of the input sequence.
3 General framework
We start by presenting the familiar synthesis set-
ting, focusing on two-word phrases. We then in-
troduce generation for the same structures. Fi-
nally, we show how synthesis and generation of
longer phrases is handled by recursive extension
of the two-word case. We assume a lexicon L,
that is, a bi-directional look-up table containing a
list of words L
w
linked to a matrix L
v
of vectors.
Both synthesis and generation involve a trivial lex-
icon look-up step to retrieve vectors associated to
words and vice versa: We ignore it in the exposi-
tion below.
3.1 Synthesis
To construct the vector representing a two-word
phrase, we must compose the vectors associated
to the input words. More formally, similarly to
Mitchell and Lapata (2008), we define a syntax-
dependent composition function yielding a phrase
vector ~p:
~p = f
comp
R
(~u,~v)
where ~u and ~v are the vector representations asso-
ciated to words u and v. f
comp
R
: R
d
? R
d
? R
d
(for d the dimensionality of vectors) is a compo-
sition function specific to the syntactic relation R
holding between the two words.
1
Although we are not bound to a specific com-
position model, throughout this paper we use the
method proposed by Guevara (2010) and Zanzotto
et al (2010) which defines composition as appli-
cation of linear transformations to the two con-
stituents followed by summing the resulting vec-
tors: f
comp
R
(~u,~v) = W
1
~u+W
2
~v. We will further
use the following equivalent formulation:
f
comp
R
(~u,~v) = W
R
[~u;~v]
where W
R
? R
d?2d
and [~u;~v] is the vertical con-
catenation of the two vectors (using Matlab no-
tation). Following Guevara, we learn W
R
using
examples of word and phrase vectors directly ex-
tracted from the corpus (for the rest of the pa-
per, we refer to these phrase vectors extracted
non-compositionally from the corpus as observed
vectors). To estimate, for example, the weights
in the W
AN
(adjective-noun) matrix, we use the
corpus-extracted vectors of the words in tuples
such as ?red, car, red.car?, ?evil, cat, evil.cat?,
etc. Given a set of training examples stacked into
matrices U , V (the constituent vectors) and P (the
corresponding observed vectors), we estimate W
R
by solving the least-squares regression problem:
1
Here we make the simplifying assumption that all vec-
tors have the same dimensionality, however this need not nec-
essarily be the case.
625
min
W
R
?R
d?2d
?P ?W
R
[U ;V ]? (1)
We use the approximation of observed phrase
vectors as objective because these vectors can pro-
vide direct evidence of the polysemous behaviour
of words: For example, the corpus-observed vec-
tors of green jacket and green politician reflect
how the meaning of green is affected by its occur-
rence with different nouns. Moreover, it has been
shown that for two-word phrases, despite their
relatively low frequency, such corpus-observed
representations are still difficult to outperform in
phrase similarity tasks (Dinu et al, 2013; Turney,
2012).
3.2 Generation
Generation of a two-word sequence from a vec-
tor proceeds in two steps: decomposition of the
phrase vectors into two constituent vectors, and
search for the nearest neighbours of each con-
stituent vector in L
v
(the lexical matrix) in order
to retrieve the corresponding words from L
w
.
Decomposition We define a syntax-dependent
decomposition function:
[~u;~v] = f
decomp
R
(~p)
where ~p is a phrase vector, ~u and ~v are vectors as-
sociated to words standing in the syntactic relation
R and f
decomp
R
: R
d
? R
d
? R
d
.
We assume that decomposition is also a linear
transformation, W
?
R
? R
2d?d
, which, given an in-
put phrase vector, returns two constituent vectors:
f
decomp
R
(~p) = W
?
R
~p
Again, we can learn from corpus-observed vectors
associated to tuples of word pairs and the corre-
sponding phrases by solving:
min
W
?
R
?R
2d?d
?[U ;V ]?W
?
R
P? (2)
If a composition function f
comp
R
is available, an
alternative is to learn a function that can best revert
this composition. The decomposition function is
then trained as follows:
min
W
?
R
?R
2d?d
?[U ;V ]?W
?
R
W
R
[U ;V ]? (3)
where the matrix W
R
is a given composition
function for the same relation R. Training with
observed phrases, as in eq. (2), should be better
at capturing the idiosyncrasies of the actual dis-
tribution of phrases in the corpus and it is more
robust by being independent from the availability
and quality of composition functions. On the other
hand, if the goal is to revert as faithfully as possi-
ble the composition process and retrieve the orig-
inal constituents (e.g., in a different modality or a
different language), then the objective in eq. (3) is
more motivated.
Nearest neighbour search We retrieve the near-
est neighbours of each constituent vector ~u ob-
tained by decomposition by applying a search
function s:
NN
~u
= s(~u, L
v
, t)
where NN
~u
is a list containing the t nearest
neighours of ~u from L
v
, the lexical vectors. De-
pending on the task, t might be set to 1 to retrieve
just one word sequence, or to larger values to re-
trieve t alternatives. The similarity measure used
to determine the nearest neighbours is another pa-
rameter of the search function; we omit it here as
we only experiment with the standard cosine mea-
sure (Turney and Pantel, 2010).
2
3.3 Recursive (de)composition
Extension to longer sequences is straightforward
if we assume binary tree representations as syn-
tactic structures. In synthesis, the top-level
vector can be obtained by applying composi-
tion functions recursively. For example, the
vector of big red car would be obtained as:
f
comp
AN
(
~
big, f
comp
AN
(
~
red, ~car)), where f
comp
AN
is the composition function for adjective-noun
phrase combinations. Conversely, for generation,
we decompose the phrase vector with f
decomp
AN
.
The first vector is used for retrieving the nearest
adjective from the lexicon, while the second vec-
tor is further decomposed.
In the experiments in this paper we assume that
the syntactic structure is given. In Section 7, we
discuss ways to eliminate this assumption.
2
Note that in terms of computational efficiency, cosine-
based nearest neighbour searches reduce to vector-matrix
multiplications, for which many efficient implementations
exist. Methods such as locality sensitive hashing can be used
for further speedups when working with particularly large vo-
cabularies (Andoni and Indyk, 2008).
626
4 Evaluation setting
In our empirical part, we focus on noun phrase
generation. A noun phrase can be a single noun or
a noun with one or more modifiers, where a mod-
ifier can be an adjective or a prepositional phrase.
A prepositional phrase is in turn composed of a
preposition and a noun phrase. We learn two com-
position (and corresponding decomposition) func-
tions: one for modifier-noun phrases, trained on
adjective-noun (AN) pairs, and a second one for
prepositional phrases, trained on preposition-noun
(PN) combinations. For the rest of this section we
describe the construction of the vector spaces and
the (de)composition function learning procedure.
Construction of vector spaces We test two
types of vector representations. The cbow model
introduced in Mikolov et al (2013a) learns vec-
tor representations using a neural network archi-
tecture by trying to predict a target word given the
words surrounding it. We use the word2vec soft-
ware
3
to build vectors of size 300 and using a con-
text window of 5 words to either side of the target.
We set the sub-sampling option to 1e-05 and esti-
mate the probability of a target word with the neg-
ative sampling method, drawing 10 samples from
the noise distribution (see Mikolov et al (2013a)
for details). We also implement a standard count-
based bag-of-words distributional space (Turney
and Pantel, 2010) which counts occurrences of a
target word with other words within a symmetric
window of size 5. We build a 300Kx300K sym-
metric co-occurrence matrix using the top most
frequent words in our source corpus, apply posi-
tive PMI weighting and Singular Value Decompo-
sition to reduce the space to 300 dimensions. For
both spaces, the vectors are finally normalized to
unit length.
4
For both types of vectors we use 2.8 billion to-
kens as input (ukWaC + Wikipedia + BNC). The
Italian language vectors for the cross-lingual ex-
periments of Section 6 were trained on 1.6 bil-
lion tokens from itWaC.
5
A word token is a word-
form + POS-tag string. We extract both word vec-
tors and the observed phrase vectors which are
3
Available at https://code.google.com/p/
word2vec/
4
The parameters of both models have been chosen without
specific tuning, based on their observed stable performance in
previous independent experiments.
5
Corpus sources: http://wacky.sslmit.unibo.
it, http://www.natcorp.ox.ac.uk
required for the training procedures. We sanity-
check the two spaces on MEN (Bruni et al, 2012),
a 3,000 items word similarity data set. cbow sig-
nificantly outperforms count (0.80 vs. 0.72 Spear-
man correlations with human judgments). count
performance is consistent with previously reported
results.
6
(De)composition function training The train-
ing data sets consist of the 50K most frequent
?u, v, p? tuples for each phrase type, for example,
?red, car, red.car? or ?in, car, in.car?.
7
We con-
catenate ~u and ~v vectors to obtain the [U ;V ] ma-
trix and we use the observed ~p vectors (e.g., the
corpus vector of the red.car bigram) to obtain the
phrase matrix P . We use these data sets to solve
the least squares regression problems in eqs. (1)
and (2), obtaining estimates of the composition
and decomposition matrices, respectively. For the
decomposition function in eq. (3), we replace the
observed phrase vectors with those composed with
f
comp
R
(~u,~v), where f
comp
R
is the previously esti-
mated composition function for relation R.
Composition function performance Since the
experiments below also use composed vectors as
input to the generation process, it is important to
provide independent evidence that the composi-
tion model is of high quality. This is indeed the
case: We tested our composition approach on the
task of retrieving observed AN and PN vectors,
based on their composed vectors (similarly to Ba-
roni and Zamparelli (2010), we want to retrieve the
observed red.car vector using f
comp
AN
(red, car)).
We obtain excellent results, with minimum accu-
racy of 0.23 (chance level <0.0001). We also test
on the AN-N paraphrasing test set used in Dinu
et al (2013) (in turn adapting Turney (2012)).
The dataset contains 620 ANs, each paired with
a single-noun paraphrase (e.g., false belief/fallacy,
personal appeal/charisma). The task is to rank
all nouns in the lexicon by their similarity to the
phrase, and return the rank of the correct para-
phrase. Results are reported in the first row of Ta-
ble 1. To facilitate comparison, we search, like
Dinu et al, through a vocabulary containing the
20K most frequent nouns. The count vectors re-
sults are similar to those reported by Dinu and col-
leagues for the same model, and with cbow vec-
6
See Baroni et al (2014) for an extensive comparison of
the two types of vector representations.
7
For PNs, we ignore determiners and we collapse, for ex-
ample, in.the.car and in.car occurrences.
627
Input Output cbow count
A?N N 11 171
N A, N 67,29 204,168
Table 1: Median rank on the AN-N set of Dinu et
al. (2013) (e.g., personal appeal/charisma). First
row: the A and N are composed and the closest
N is returned as a paraphrase. Second row: the
N vector is decomposed into A and N vectors and
their nearest (POS-tag consistent) neighbours are
returned.
tors we obtain a median rank that is considerably
higher than that of the methods they test.
5 Noun phrase generation
5.1 One-step decomposition
We start with testing one-step decomposition by
generating two-word phrases. A first straightfor-
ward evaluation consists in decomposing a phrase
vector into the correct constituent words. For this
purpose, we randomly select (and consequently re-
move) from the training sets 200 phrases of each
type (AN and PN) and apply decomposition op-
erations to 1) their corpus-observed vectors and
2) their composed representations. We generate
two words by returning the nearest neighbours
(with appropriate POS tags) of the two vectors
produced by the decomposition functions. Ta-
ble 2 reports generation accuracy, i.e., the pro-
portion of times in which we retrieved the cor-
rect constituents. The search space consists of
the top most frequent 20K nouns, 20K adjec-
tives and 25 prepositions respectively, leading to
chance accuracy <0.0001 for nouns and adjectives
and <0.05 for prepositions. We obtain relatively
high accuracy, with cbow vectors consistently out-
performing count ones. Decomposing composed
rather than observed phrase representations is eas-
ier, which is to be expected given that composed
representations are obtained with a simpler, lin-
ear model. Most of the errors consist in generat-
ing synonyms (hard case?difficult case, true cost
? actual cost) or related phrases (stereo speak-
ers?omni-directional sound).
Next, we use the AN-N dataset of Dinu and
colleagues for a more interesting evaluation of
one-step decomposition. In particular, we reverse
the original paraphrasing direction by attempting
to generate, for example, personal charm from
charisma. It is worth stressing the nature of the
Input Output cbow count
A.N A, N 0.36,0.61 0.20,0.41
P.N P, N 0.93,0.79 0.60,0.57
A?N A, N 1.00,1.00 0.86,0.99
P?N P, N 1.00,1.00 1.00,1.00
Table 2: Accuracy of generation models at re-
trieving (at rank 1) the constituent words of
adjective-noun (AN) and preposition-noun (PN)
phrases. Observed (A.N) and composed repre-
sentations (A?N) are decomposed with observed-
(eq. 2) and composed-trained (eq. 3) functions re-
spectively.
paraphrase-by-generation task we tackle here and
in the next experiments. Compositional distri-
butional semantic systems are often evaluated on
phrase and sentence paraphrasing data sets (Bla-
coe and Lapata, 2012; Mitchell and Lapata, 2010;
Socher et al, 2011; Turney, 2012). However,
these experiments assume a pre-compiled list of
candidate paraphrases, and the task is to rank
correct paraphrases above foils (paraphrase rank-
ing) or to decide, for a given pair, if the two
phrases/sentences are mutual paraphrases (para-
phrase detection). Here, instead, we do not as-
sume a given set of candidates: For example, in
N?AN paraphrasing, any of 20K
2
possible com-
binations of adjectives and nouns from the lexicon
could be generated. This is a much more challeng-
ing task and it paves the way to more realistic ap-
plications of distributional semantics in generation
scenarios.
The median ranks of the gold A and N of the
Dinu set are shown in the second row of Table
1. As the top-generated noun is almost always,
uninterestingly, the input one, we return the next
noun. Here we report results for the more moti-
vated corpus-observed training of eq. (2) (unsur-
prisingly, using composed-phrase training for the
task of decomposing single nouns leads to lower
performance).
Although considerably more difficult than the
previous task, the results are still very good, with
median ranks under 100 for the cbow vectors (ran-
dom median rank at 10K). Also, the dataset pro-
vides only one AN paraphrase for each noun, out
of many acceptable ones. Examples of generated
phrases are given in Table 3. In addition to gen-
erating topically related ANs, we also see nouns
disambiguated in different ways than intended in
628
Input Output Gold
reasoning deductive thinking abstract thought
jurisdiction legal authority legal power
thunderstorm thundery storm electrical storm
folk local music common people
superstition old-fashioned religion superstitious notion
vitriol political bitterness sulfuric acid
zoom fantastic camera rapid growth
religion religious religion religious belief
Table 3: Examples of generating ANs from Ns us-
ing the data set of Dinu et al (2013).
the gold standard (for example vitriol and folk in
Table 3). Other interesting errors consist of de-
composing a noun into two words which both have
the same meaning as the noun, generating for ex-
ample religion? religious religions. We observe
moreover that sometimes the decomposition re-
flects selectional preference effects, by generat-
ing adjectives that denote typical properties of the
noun to be paraphrased (e.g., animosity is a (po-
litical, personal,...) hostility or a fridge is a (big,
large, small,...) refrigerator). This effect could be
exploited for tasks such as property-based concept
description (Kelly et al, 2012).
5.2 Recursive decomposition
We continue by testing generation through recur-
sive decomposition on the task of generating noun-
preposition-noun (NPN) paraphrases of adjective-
nouns (AN) phrases. We introduce a dataset con-
taining 192 AN-NPN pairs (such as pre-election
promises? promises before election), which was
created by the second author and additionally cor-
rected by an English native speaker. The data set
was created by analyzing a list of randomly se-
lected frequent ANs. 49 further ANs (with adjec-
tives such as amazing and great) were judged not
NPN-paraphrasable and were used for the experi-
ment reported in Section 7. The paraphrased sub-
set focuses on preposition diversity and on includ-
ing prepositions which are rich in semantic content
and relevant to paraphrasing the AN. This has led
to excluding of, which in most cases has the purely
syntactic function of connecting the two nouns.
The data set contains the following 14 preposi-
tions: after, against, at, before, between, by, for,
from, in, on, per, under, with, without.
8
NPN phrase generation involves the applica-
tion of two decomposition functions. In the first
8
This dataset is available at http://clic.cimec.
unitn.it/composes
step we decompose using the modifier-noun rule
(f
decomp
AN
). We generate a noun from the head
slot vector and the ?adjective? vector is further de-
composed using f
decomp
PN
(returning the top noun
which is not identical to the previously generated
one). The results, in terms of top 1 accuracy and
median rank, are shown in Table 4. Examples are
given in Table 5.
For observed phrase vector training, accuracy
and rank are well above chance for all constituents
(random accuracy 0.00005 for nouns and 0.04 for
prepositions, corresponding median ranks: 10K,
12). Preposition generation is clearly a more diffi-
cult task. This is due at least in part to their highly
ambiguous and broad semantics, and the way in
which they interact with the nouns. For exam-
ple, cable through ocean in Table 5 is a reason-
able paraphrase of undersea cable despite the gold
preposition being under. Other than several cases
which are acceptable paraphrases but not in the
gold standard, phrases related in meaning but not
synonymous are the most common error (overcast
skies ? skies in sunshine). We also observe that
often the A and N meanings are not fully separated
when decomposing and ?traces? of the adjective
or of the original noun meaning can be found in
both generated nouns (for example nearby school
? schools after school). To a lesser degree, this
might be desirable as a disambiguation-in-context
effect as, for example, in underground cavern, in
secret would not be a context-appropriate para-
phrase of underground.
6 Noun phrase translation
This section describes preliminary experiments
performed in a cross-lingual setting on the task
of composing English AN phrases and generating
Italian translations.
Creation of cross-lingual vector spaces A
common semantic space is required in order to
map words and phrases across languages. This
problem has been extensively addressed in the
bilingual lexicon acquisition literature (Haghighi
et al, 2008; Koehn and Knight, 2002). We opt for
a very simple yet accurate method (Klementiev et
al., 2012; Rapp, 1999) in which a bilingual dictio-
nary is used to identify a set of shared dimensions
across spaces and the vectors of both languages are
projected into the subspace defined by these (Sub-
space Projection - SP). This method is applicable
to count-type vector spaces, for which the dimen-
629
Input Output Training cbow count
A?N N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5)
A?N N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5)
Table 4: Top 1 accuracy (median rank) on the AN?NPN paraphrasing data set. AN phrases are com-
posed and then recursively decomposed into N, (P, N). Comma-delimited scores reported for first noun,
preposition, second noun in this order. Training is performed on observed (eq. 2) and composed (eq. 3)
phrase representations.
Input Output Gold
mountainous region region in highlands region with mountains
undersea cable cable through ocean cable under sea
underground cavern cavern through rock cavern under ground
interdisciplinary field field into research field between disciplines
inter-war years years during 1930s years between wars
post-operative pain pain through patient pain after operation
pre-war days days after wartime days before war
intergroup differences differences between intergroup differences between minorities
superficial level level between levels level on surface
Table 5: Examples of generating NPN phrases from composed ANs.
sions correspond to actual words. As the cbow di-
mensions do not correspond to words, we align the
cbow spaces by using a small dictionary to learn
a linear map which transforms the English vectors
into Italian ones as done in Mikolov et al (2013b).
This method (Translation Matrix - TM) is applica-
ble to both cbow and count spaces. We tune the pa-
rameters (TM or SP for count and dictionary size
5K or 25K for both spaces) on a standard task of
translating English words into Italian. We obtain
TM-5K for cbow and SP-25K for count as opti-
mal settings. The two methods perform similarly
for low frequency words while cbow-TM-5K sig-
nificantly outperforms count-SP-25K for high fre-
quency words. Our results for the cbow-TM-5K
setting are similar to those reported by Mikolov et
al. (2013b).
Cross-lingual decomposition training Train-
ing proceeds as in the monolingual case, this time
concatenating the training data sets and estimating
a single (de)-composition function for the two lan-
guages in the shared semantic space. We train both
on observed phrase representations (eq. 2) and on
composed phrase representations (eq. 3).
Adjective-noun translation dataset We ran-
domly extract 1,000 AN-AN En-It phrase pairs
from a phrase table built from parallel movie sub-
titles, available at http://opus.lingfil.
uu.se/ (OpenSubtitles2012, en-it) (Tiedemann,
2012).
Input Output cbow count
A?N(En) A,N (It) 0.31,0.59 0.24,0.54
A?N (It) A,N(En) 0.50,0.62 0.28,0.48
Table 6: Accuracy of En?It and It?En phrase
translation: phrases are composed in source lan-
guage and decomposed in target language. Train-
ing on composed phrase representations (eq. (3))
(with observed phrase training (eq. 2) results are
?50% lower).
Results are presented in Table 6. While in
these preliminary experiments we lack a proper
term of comparison, the performance is very good
both quantitatively (random < 0.0001) and quali-
tatively. The En?It examples in Table 7 are repre-
sentative. In many cases (e.g., vicious killer, rough
neighborhood) we generate translations that are
arguably more natural than those in the gold stan-
dard. Again, some differences can be explained
by different disambiguations (chest as breast, as
in the generated translation, or box, as in the gold).
Translation into related but not equivalent phrases
and generating the same meaning in both con-
stituents (stellar star) are again the most signifi-
cant errors. We also see cases in which this has the
desired effect of disambiguating the constituents,
such as in the examples in Table 8, showing the
nearest neighbours when translating black tie and
indissoluble tie.
630
Input Output Gold
vicious killer assassino feroce (ferocious killer) killer pericoloso
spectacular woman donna affascinante (fascinating woman) donna eccezionale
huge chest petto grande (big chest) scrigno immenso
rough neighborhood zona malfamata (ill-repute zone) quartiere difficile
mortal sin peccato eterno (eternal sin) pecato mortale
canine star stella stellare (stellar star) star canina
Table 7: En?It translation examples (back-translations of generated phrases in parenthesis).
black tie
cravatta (tie) nero (black)
velluto (velvet) bianco (white)
giacca (jacket) giallo (yellow)
indissoluble tie
alleanza (alliance) indissolubile (indissoluble)
legame (bond) sacramentale (sacramental)
amicizia (friendship) inscindibile (inseparable)
Table 8: Top 3 translations of black tie and indis-
soluble tie, showing correct disambiguation of tie.
7 Generation confidence and generation
quality
In Section 3.2 we have defined a search function
s returning a list of lexical nearest neighbours for
a constituent vector produced by decomposition.
Together with the neighbours, this function can
naturally return their similarity score (in our case,
the cosine). We call the score associated to the
top neighbour the generation confidence: if this
score is low, the vector has no good match in the
lexicon. We observe significant Spearman cor-
relations between the generation confidence of a
constituent and its quality (e.g., accuracy, inverse
rank) in all the experiments. For example, for the
AN(En)?AN(It) experiment, the correlations be-
tween the confidence scores and the inverse ranks
for As and Ns, for both cbow and count vectors,
range between 0.34 (p < 1e
?28
) and 0.42. In
the translation experiments, we can use this to au-
tomatically determine a subset on which we can
translate with very high accuracy. Table 9 shows
AN-AN accuracies and coverage when translating
only if confidence is above a certain threshold.
Throughout this paper we have assumed that the
syntactic structure of the phrase to be generated is
given. In future work we will exploit the corre-
lation between confidence and quality for the pur-
pose of eliminating this assumption. As a concrete
example, we can use confidence scores to distin-
guish the two subsets of the AN-NPN dataset in-
troduced in Section 5: the ANs which are para-
phrasable with an NPN from those that do not
En?It It?En
Thr. Accuracy Cov. Accuracy Cov.
0.00 0.21 100% 0.32 100%
0.55 0.25 70% 0.40 63%
0.60 0.31 32% 0.45 37%
0.65 0.45 9% 0.52 16%
Table 9: AN-AN translation accuracy (both A and
N correct) when imposing a confidence threshold
(random: 1/20K
2
).
Figure 1: ROC of distinguishing ANs para-
phrasable as NPNs from non-paraphrasable ones.
have this property. We assign an AN to the NPN-
paraphrasable class if the mean confidence of the
PN expansion in its attempted N(PN) decomposi-
tion is above a certain threshold. We plot the ROC
curve in Figure 1. We obtain a significant AUC of
0.71.
8 Conclusion
In this paper we have outlined a framework for
the task of generation with distributional semantic
models. We proposed a simple but effective ap-
proach to reverting the composition process to ob-
tain meaningful reformulations of phrases through
a synthesis-generation process.
For future work we would like to experiment
with more complex models for (de-)composition
in order to improve the performance on the tasks
we used in this paper. Following this, we
631
would like to extend the framework to handle
arbitrary phrases, including making (confidence-
based) choices on the syntactic structure of the
phrase to be generated, which we have assumed
to be given throughout this paper.
In terms of applications, we believe that the line
of research in machine translation that is currently
focusing on replacing parallel resources with large
amounts of monolingual text provides an inter-
esting setup to test our methods. For example,
Klementiev et al (2012) reconstruct phrase ta-
bles based on phrase similarity scores in seman-
tic space. However, they resort to scoring phrase
pairs extracted from an aligned parallel corpus, as
they do not have a method to freely generate these.
Similarly, in the recent work on common vector
spaces for the representation of images and text,
the current emphasis is on retrieving existing cap-
tions (Socher et al, 2014) and not actual genera-
tion of image descriptions.
From a more theoretical point of view, our work
fills an important gap in distributional semantics,
making it a bidirectional theory of the connec-
tion between language and meaning. We can now
translate linguistic strings into vector ?thoughts?,
and the latter into their most appropriate linguis-
tic expression. Several neuroscientific studies sug-
gest that thoughts are represented in the brain by
patterns of activation over broad neural areas, and
vectors are a natural way to encode such patterns
(Haxby et al, 2001; Huth et al, 2012). Some
research has already established a connection be-
tween neural and distributional semantic vector
spaces (Mitchell et al, 2008; Murphy et al, 2012).
Generation might be the missing link to power-
ful computational models that take the neural foot-
print of a thought as input and produce its linguis-
tic expression.
Acknowledgments
We thank Kevin Knight, Andrew Anderson,
Roberto Zamparelli, Angeliki Lazaridou, Nghia
The Pham, Germ?an Kruszewski and Peter Tur-
ney for helpful discussions and the anonymous re-
viewers for their useful comments. We acknowl-
edge the ERC 2011 Starting Independent Research
Grant n. 283554 (COMPOSES).
References
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neigh-
bor in high dimensions. Commun. ACM, 51(1):117?
122, January.
Jacob Andreas and Zoubin Ghahramani. 2013. A
generative model of vector space semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 91?
99, Sofia, Bulgaria.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of ACL, To appear, Baltimore, MD.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511?522.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779, Columbus, OH, USA, June.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces
and objects in ventral temporal cortex. Science,
293:2425?2430.
632
Alexander Huth, Shinji Nishimoto, An Vu, and Jack
Gallant. 2012. A continuous semantic space de-
scribes the representation of thousands of object and
action categories across the human brain. Neuron,
76(6):1210?1224.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, Seattle, October. Associa-
tion for Computational Linguistics.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11?20, Montreal, Canada.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of EACL, pages 130?140, Avignon,
France.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16, Philadelphia, PA,
USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for Ma-
chine Translation. http://arxiv.org/abs/
1309.4168.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason,
and Marcel Just. 2008. Predicting human brain ac-
tivity associated with the meanings of nouns. Sci-
ence, 320:1191?1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Selecting corpus-semantic models for neu-
rolinguistic decoding. In Proceedings of *SEM,
pages 114?123, Montreal, Canada.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Richard Socher, Quoc Le, Christopher Manning, and
Andrew Ng. 2014. Grounded compositional se-
mantics for finding and describing images with sen-
tences. Transactions of the Association for Compu-
tational Linguistics. In press.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
633
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403?1414,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Is this a wampimuk?
Cross-modal mapping between distributional semantics
and the visual world
Angeliki Lazaridou and Elia Bruni and Marco Baroni
Center for Mind/Brain Sciences
University of Trento
{angeliki.lazaridou|elia.bruni|marco.baroni}@unitn.it
Abstract
Following up on recent work on estab-
lishing a mapping between vector-based
semantic embeddings of words and the
visual representations of the correspond-
ing objects from natural images, we first
present a simple approach to cross-modal
vector-based semantics for the task of
zero-shot learning, in which an image
of a previously unseen object is mapped
to a linguistic representation denoting its
word. We then introduce fast mapping, a
challenging and more cognitively plausi-
ble variant of the zero-shot task, in which
the learner is exposed to new objects and
the corresponding words in very limited
linguistic contexts. By combining prior
linguistic and visual knowledge acquired
about words and their objects, as well as
exploiting the limited new evidence avail-
able, the learner must learn to associate
new objects with words. Our results on
this task pave the way to realistic simula-
tions of how children or robots could use
existing knowledge to bootstrap grounded
semantic knowledge about new concepts.
1 Introduction
Computational models of meaning that rely on
corpus-extracted context vectors, such as LSA
(Landauer and Dumais, 1997), HAL (Lund and
Burgess, 1996), Topic Models (Griffiths et al,
2007) and more recent neural-network approaches
(Collobert and Weston, 2008; Mikolov et al,
2013b) have successfully tackled a number of lex-
ical semantics tasks, where context vector sim-
ilarity highly correlates with various indices of
semantic relatedness (Turney and Pantel, 2010).
Given that these models are learned from natu-
rally occurring data using simple associative tech-
niques, various authors have advanced the claim
that they might be also capturing some crucial as-
pects of how humans acquire and use language
(Landauer and Dumais, 1997; Lenci, 2008).
However, the models induce the meaning of
words entirely from their co-occurrence with other
words, without links to the external world. This
constitutes a serious blow to claims of cogni-
tive plausibility in at least two respects. One
is the grounding problem (Harnad, 1990; Searle,
1984). Irrespective of their relatively high per-
formance on various semantic tasks, it is debat-
able whether models that have no access to visual
and perceptual information can capture the holis-
tic, grounded knowledge that humans have about
concepts. However, a possibly even more serious
pitfall of vector models is lack of reference: natu-
ral language is, fundamentally, a means to commu-
nicate, and thus our words must be able to refer to
objects, properties and events in the outside world
(Abbott, 2010). Current vector models are purely
language-internal, solipsistic models of meaning.
Consider the very simple scenario in which visual
information is being provided to an agent about
the current state of the world, and the agent?s task
is to determine the truth of a statement similar to
There is a dog in the room. Although the agent
is equipped with a powerful context vector model,
this will not suffice to successfully complete the
task. The model might suggest that the concepts
of dog and cat are semantically related, but it has
no means to determine the visual appearance of
dogs, and consequently no way to verify the truth
of such a simple statement.
Mapping words to the objects they denote is
such a core function of language that humans are
highly optimized for it, as shown by the so-called
fast mapping phenomenon, whereby children can
learn to associate a word to an object or prop-
erty by a single exposure to it (Bloom, 2000;
Carey, 1978; Carey and Bartlett, 1978; Heibeck
and Markman, 1987). But lack of reference is not
1403
only a theoretical weakness: Without the ability to
refer to the outside world, context vectors are ar-
guably useless for practical goals such as learning
to execute natural language instructions (Brana-
van et al, 2009; Chen and Mooney, 2011), that
could greatly benefit from the rich network of lex-
ical meaning such vectors encode, in order to scale
up to real-life challenges.
Very recently, a number of papers have ex-
ploited advances in automated feature extraction
form images and videos to enrich context vectors
with visual information (Bruni et al, 2014; Feng
and Lapata, 2010; Leong and Mihalcea, 2011;
Regneri et al, 2013; Silberer et al, 2013). This
line of research tackles the grounding problem:
Word representations are no longer limited to their
linguistic contexts but also encode visual informa-
tion present in images associated with the corre-
sponding objects. In this paper, we rely on the
same image analysis techniques but instead focus
on the reference problem: We do not aim at en-
riching word representations with visual informa-
tion, although this might be a side effect of our
approach, but we address the issue of automati-
cally mapping objects, as depicted in images, to
the context vectors representing the correspond-
ing words. This is achieved by means of a simple
neural network trained to project image-extracted
feature vectors to text-based vectors through a hid-
den layer that can be interpreted as a cross-modal
semantic space.
We first test the effectiveness of our cross-
modal semantic space on the so-called zero-shot
learning task (Palatucci et al, 2009), which has re-
cently been explored in the machine learning com-
munity (Frome et al, 2013; Socher et al, 2013). In
this setting, we assume that our system possesses
linguistic and visual information for a set of con-
cepts in the form of text-based representations of
words and image-based vectors of the correspond-
ing objects, used for vision-to-language-mapping
training. The system is then provided with visual
information for a previously unseen object, and the
task is to associate it with a word by cross-modal
mapping. Our approach is competitive with re-
spect to the recently proposed alternatives, while
being overall simpler.
The aforementioned task is very demanding and
interesting from an engineering point of view.
However, from a cognitive angle, it relies on
strong, unrealistic assumptions: The learner is
asked to establish a link between a new object and
a word for which they possess a full-fledged text-
based vector extracted from a billion-word cor-
pus. On the contrary, the first time a learner is
exposed to a new object, the linguistic informa-
tion available is likely also very limited. Thus, in
order to consider vision-to-language mapping un-
der more plausible conditions, similar to the ones
that children or robots in a new environment are
faced with, we next simulate a scenario akin to fast
mapping. We show that the induced cross-modal
semantic space is powerful enough that sensible
guesses about the correct word denoting an object
can be made, even when the linguistic context vec-
tor representing the word has been created from as
little as 1 sentence containing it.
The contributions of this work are three-fold.
First, we conduct experiments with simple image-
and text-based vector representations and compare
alternative methods to perform cross-modal map-
ping. Then, we complement recent work (Frome
et al, 2013) and show that zero-shot learning
scales to a large and noisy dataset. Finally, we pro-
vide preliminary evidence that cross-modal pro-
jections can be used effectively to simulate a fast
mapping scenario, thus strengthening the claims
of this approach as a full-fledged, fully inductive
theory of meaning acquisition.
2 Related Work
The problem of establishing word reference has
been extensively explored in computational sim-
ulations of cross-situational learning (see Fazly et
al. (2010) for a recent proposal and extended re-
view of previous work). This line of research has
traditionally assumed artificial models of the ex-
ternal world, typically a set of linguistic or logi-
cal labels for objects, actions and possibly other
aspects of a scene (Siskind, 1996). Recently,
Yu and Siskind (2013) presented a system that
induces word-object mappings from features ex-
tracted from short videos paired with sentences.
Our work complements theirs in two ways. First,
unlike Yu and Siskind (2013) who considered a
limited lexicon of 15 items with only 4 nouns, we
conduct experiments in a large search space con-
taining a highly ambiguous set of potential target
words for every object (see Section 4.1). Most im-
portantly, by projecting visual representations of
objects into a shared semantic space, we do not
limit ourselves to establishing a link between ob-
1404
jects and words. We induce a rich semantic rep-
resentation of the multimodal concept, that can
lead, among other things, to the discovery of im-
portant properties of an object even when we lack
its linguistic label. Nevertheless, Yu and Siskind?s
system could in principle be used to initialize the
vision-language mapping that we rely upon.
Closer to the spirit of our work are two very
recent studies coming from the machine learning
community. Socher et al (2013) and Frome et al
(2013) focus on zero-shot learning in the vision-
language domain by exploiting a shared visual-
linguistic semantic space. Socher et al (2013)
learn to project unsupervised vector-based image
representations onto a word-based semantic space
using a neural network architecture. Unlike us,
Socher and colleagues train an outlier detector
to decide whether a test image should receive a
known-word label by means of a standard super-
vised object classifier, or be assigned an unseen
label by vision-to-language mapping. In our zero-
shot experiments, we assume no access to an out-
lier detector, and thus, the search for the correct
label is performed in the full concept space. Fur-
thermore, Socher and colleagues present a much
more constrained evaluation setup, where only 10
concepts are considered, compared to our experi-
ments with hundreds or thousands of concepts.
Frome et al (2013) use linear regression to
transform vector-based image representations onto
vectors representing the same concepts in linguis-
tic semantic space. Unlike Socher et al (2013) and
the current study that adopt simple unsupervised
techniques for constructing image representations,
Frome et al (2013) rely on a supervised state-of-
the-art method: They feed low-level features to a
deep neural network trained on a supervised object
recognition task (Krizhevsky et al, 2012). Fur-
thermore, their text-based vectors encode very rich
information, such as
~
king ? ~man + ~woman =
~queen (Mikolov et al, 2013c). A natural ques-
tion we aim to answer is whether the success of
cross-modal mapping is due to the high-quality
embeddings or to the general algorithmic design.
If the latter is the case, then these results could be
extended to traditional distributional vectors bear-
ing other desirable properties, such as high inter-
pretability of dimensions.
(a) (b)
Figure 1: A potential wampimuk (a) together with
its projection onto the linguistic space (b).
3 Zero-shot learning and fast mapping
?We found a cute, hairy wampimuk sleeping be-
hind the tree.? Even though the previous state-
ment is certainly the first time one hears about
wampimuks, the linguistic context already creates
some visual expectations: Wampimuks probably
resemble small animals (Figure 1a). This is the
scenario of zero-shot learning. Moreover, if this is
also the first linguistic encounter of that concept,
then we refer to the task as fast mapping.
Concretely, we assume that concepts, denoted
for convenience by word labels, are represented in
linguistic terms by vectors in a text-based distri-
butional semantic space (see Section 4.3). Objects
corresponding to concepts are represented in vi-
sual terms by vectors in an image-based semantic
space (Section 4.2). For a subset of concepts (e.g.,
a set of animals, a set of vehicles), we possess in-
formation related to both their linguistic and visual
representations. During training, this cross-modal
vocabulary is used to induce a projection func-
tion (Section 4.4), which ? intuitively ? represents
a mapping between visual and linguistic dimen-
sions. Thus, this function, given a visual vector,
returns its corresponding linguistic representation.
At test time, the system is presented with a previ-
ously unseen object (e.g., wampimuk). This object
is projected onto the linguistic space and associ-
ated with the word label of the nearest neighbor in
that space (degus in Figure 1b).
The fast mapping setting can be seen as a spe-
cial case of the zero-shot task. Whereas for the lat-
ter our system assumes that all concepts have rich
linguistic representations (i.e., representations es-
timated from a large corpus), in the case of the for-
mer, new concepts are assumed to be encounted in
a limited linguistic context and therefore lacking
rich linguistic representations. This is operational-
ized by constructing the text-based vector for these
1405
Figure 2: Images of chair as extracted from
CIFAR-100 (left) and ESP (right).
concepts from a context of just a few occurrences.
In this way, we simulate the first encounter of a
learner with a concept that is new in both visual
and linguistic terms.
4 Experimental Setup
4.1 Visual Datasets
CIFAR-100 The CIFAR-100 dataset
(Krizhevsky, 2009) consists of 60,000 32x32
colour images (note the extremely small size)
representing 100 distinct concepts, with 600
images per concept. The dataset covers a wide
range of concrete domains and is organized into
20 broader categories. Table 1 lists the concepts
used in our experiments organized by category.
ESP Our second dataset consists of 100K im-
ages from the ESP-Game data set, labeled through
a ?game with a purpose? (Von Ahn, 2006).
1
The
ESP image tags form a vocabulary of 20,515
unique words. Unlike other datasets used for zero-
shot learning, it covers adjectives and verbs in ad-
dition to nouns. On average, an image has 14
tags and a word appears as a tag for 70 images.
Unlike the CIFAR-100 images, which were cho-
sen specifically for image object recognition tasks
(i.e., each image is clearly depicting a single ob-
ject in the foreground), ESP contains a random se-
lection of images from the Web. Consequently,
objects do not appear in most images in their pro-
totypical display, but rather as elements of com-
plex scenes (see Figure 2). Thus, ESP constitutes
a more realistic, and at the same time more chal-
lenging, simulation of how things are encountered
in real life, testing the potentials of cross-modal
mapping in dealing with the complex scenes that
one would encounter in event recognition and cap-
tion generation tasks.
1
http://www.cs.cmu.edu/
?
biglou/
resources/
4.2 Visual Semantic Spaces
Image-based vectors are extracted using the unsu-
pervised bag-of-visual-words (BoVW) represen-
tational architecture (Sivic and Zisserman, 2003;
Csurka et al, 2004), that has been widely and suc-
cessfully applied to computer vision tasks such as
object recognition and image retrieval (Yang et al,
2007). First, low-level visual features (Szeliski,
2010) are extracted from a large collection of im-
ages and clustered into a set of ?visual words?.
The low-level features of a specific image are then
mapped to the corresponding visual words, and the
image is represented by a count vector recording
the number of occurrences of each visual word in
it. We do not attempt any parameter tuning of the
pipeline.
As low-level features, we use Scale Invariant
Feature Transform (SIFT) features (Lowe, 2004).
SIFT features are tailored to capture object parts
and to be invariant to several image transfor-
mations such as rotation, illumination and scale
change. These features are clustered into vocab-
ularies of 5,000 (ESP) and 4,096 (CIFAR-100) vi-
sual words.
2
To preserve spatial information in the
BoVW representation, we use the spatial pyramid
technique (Lazebnik et al, 2006), which consists
in dividing the image into several regions, comput-
ing BoVW vectors for each region and concatenat-
ing them. In particular, we divide ESP images into
16 regions and the smaller CIFAR-100 images into
4. The vectors resulting from region concatenation
have dimensionality 5000 ? 16 = 80, 000 (ESP)
and 4, 096 ? 4 = 16, 384 (CIFAR-100), respec-
tively. We apply Local Mutual Information (LMI,
(Evert, 2005)) as weighting scheme and reduce the
full co-occurrence space to 300 dimensions using
the Singular Value Decomposition.
For CIFAR-100, we extract distinct visual vec-
tors for single images. For ESP, given the size
and amount of noise in this dataset, we build vec-
tors for visual concepts, by normalizing and sum-
ming the BoVW vectors of all the images that have
the relevant concept as a tag. Note that relevant
literature (Pereira et al, 2010) has emphasized
the importance of learners self-generating multi-
ple views when faced with new objects. Thus, our
multiple-image assumption should not be consid-
ered as problematic in the current setup.
2
For selecting the size of the vocabulary size, we relied on
standard settings found in the relevant literature (Bruni et al,
2014; Chatfield et al, 2011).
1406
Category Seen Concepts Unseen (Test) Concepts
aquatic mammals beaver, otter, seal, whale dolphin
fish ray, trout shark
flowers orchid, poppy, sunflower, tulip rose
food containers bottle, bowl, can ,plate cup
fruit vegetable apple, mushroom, pear orange
household electrical devices keyboard, lamp, telephone, television clock
household furniture chair, couch, table, wardrobe bed
insects bee, beetle, caterpillar, cockroach butterfly
large carnivores bear, leopard, lion, wolf tiger
large man-made outdoor things bridge, castle, house, road skyscraper
large natural outdoor scenes cloud, mountain, plain, sea forest
large omnivores and herbivores camel, cattle, chimpanzee, kangaroo elephant
medium-sized mammals fox, porcupine, possum, skunk raccoon
non-insect invertebrates crab, snail, spider, worm lobster
people baby, girl, man, woman boy
reptiles crocodile, dinosaur, snake, turtle lizard
small mammals hamster, mouse, rabbit, shrew squirrel
vehicles 1 bicycle, motorcycle, train bus
vehicles 2 rocket, tank, tractor streetcar
Table 1: Concepts in our version of the CIFAR-100 data set
We implement the entire visual pipeline with
VSEM, an open library for visual seman-
tics (Bruni et al, 2013).
3
4.3 Linguistic Semantic Spaces
For constructing the text-based vectors, we fol-
low a standard pipeline in distributional semantics
(Turney and Pantel, 2010) without tuning its pa-
rameters and collect co-occurrence statistics from
the concatenation of ukWaC
4
and the Wikipedia,
amounting to 2.7 billion tokens in total. Seman-
tic vectors are constructed for a set of 30K target
words (lemmas), namely the top 20K most fre-
quent nouns, 5K most frequent adjectives and 5K
most frequent verbs, and the same 30K lemmas are
also employed as contextual elements. We collect
co-occurrences in a symmetric context window of
20 elements around a target word. Finally, simi-
larly to the visual semantic space, raw counts are
transformed by applying LMI and then reduced to
300 dimensions with SVD.
5
4.4 Cross-modal Mapping
The process of learning to map objects to the their
word label is implemented by training a projec-
tion function f
proj
v?w
from the visual onto the lin-
guistic semantic space. For the learning, we use
a set of N
s
seen concepts for which we have both
image-based visual representations V
s
? R
N
s
?d
v
3
http://clic.cimec.unitn.it/vsem/
4
http://wacky.sslmit.unibo.it
5
We also experimented with the image- and text-based
vectors of Socher et al (2013), but achieved better perfor-
mance with the reported setup.
and text-based linguistic representations W
s
?
R
N
s
?d
w
. The projection function is subject to
an objective that aims at minimizing some cost
function between the induced text-based represen-
tations
?
W
s
? R
N
s
?d
w
and the gold ones W
s
.
The induced f
proj
v?w
is then applied to the image-
based representations V
u
? R
N
u
?d
v
of N
u
un-
seen objects to transform them into text-based rep-
resentations
?
W
u
? R
N
u
?d
w
. We implement 4
alternative learning algorithms for inducing the
cross-modal projection function f
proj
v?w
.
Linear Regression (lin) Our first model is a very
simple linear mapping between the two modali-
ties estimated by solving a least-squares problem.
This method is similar to the one introduced by
Mikolov et al (2013a) for estimating a translation
matrix, only solved analytically. In our setup, we
can see the two different modalities as if they were
different languages. By using least-squares regres-
sion, the projection function f
proj
v?w
can be de-
rived as
f
proj
v?w
= (V
T
s
V
s
)
?1
V
T
s
W
s
(1)
Canonical Correlation Analysis (CCA)
CCA (Hardoon et al, 2004; Hotelling, 1936)
and variations thereof have been successfully used
in the past for annotation of regions (Socher and
Fei-Fei, 2010) and complete images (Hardoon et
al., 2006; Hodosh et al, 2013). Given two paired
observation matrices, in our case V
s
and W
s
,
CCA aims at capturing the linear relationship
that exists between these variables. This is
achieved by finding a pair of matrices, in our
1407
case C
V
? R
d
v
?d
and C
W
? R
d
w
?d
, such that
the correlation between the projections of the
two multidimensional variables into a common,
lower-rank space is maximized. The resulting
multimodal space has been shown to provide a
good approximation to human concept similarity
judgments (Silberer and Lapata, 2012). In our
setup, after applying CCA on the two spaces V
s
and W
s
, we obtain the two projection mappings
onto the common space and thus our projection
function can be derived as:
f
proj
v?w
= C
V
C
W
?1
(2)
Singular Value Decomposition (SVD) SVD is
the most widely used dimensionality reduction
technique in distributional semantics (Turney and
Pantel, 2010), and it has recently been exploited
to combine visual and linguistic dimensions in
the multimodal distributional semantic model of
Bruni et al (2014). SVD smoothing is also a way
to infer values of unseen dimensions in partially
incomplete matrices, a technique that has been ap-
plied to the task of inferring word tags of unanno-
tated images (Hare et al, 2008). Assuming that the
concept-representing rows of V
s
and W
s
are or-
dered in the same way, we apply the (k-truncated)
SVD to the concatenated matrix [V
s
W
s
], such
that [
?
V
s
?
W
s
] = U
k
?
k
Z
T
k
is a k-rank approxima-
tion of the original matrix.
6
The projection func-
tion is then:
f
proj
v?w
= Z
k
Z
T
k
(3)
where the input is appropriately padded with 0s
([V
u
0
Nu?W
]) and we discard the visual block of
the output matrix [
?
V
u
?
W
u
].
Neural Network (NNet) The last model that we
introduce is a neural network with one hidden
layer. The projection function in this model can
be described as:
f
proj
v?w
= ?
v?w
(4)
where ?
v?w
consists of the model weights ?
(1)
?
R
d
v
?h
and ?
(2)
? R
h?d
w
that map the in-
put image-based vectors V
s
first to the hid-
den layer and then to the output layer in or-
der to obtain text-based vectors, i.e.,
?
W
s
=
?
(2)
(?
(1)
(Vs?
(1)
)?
(2)
), where ?
(1)
and ?
(2)
are
6
We denote the right singular vectors matrix by Z instead
of the customaryV to avoid confusion with the visual matrix.
the non-linear activation functions. We experi-
mented with sigmoid, hyperbolic tangent and lin-
ear; hyperbolic tangent yielded the highest perfor-
mance. The weights are estimated by minimizing
the objective function
J(?
v?w
) =
1
2
(1? sim(W
s
,
?
W
s
)) (5)
where sim is some similarity function. In our ex-
periments we used cosine as similarity function,
so that sim(A,B) =
AB
?A??B?
, thus penalizing pa-
rameter settings leading to a low cosine between
the target linguistic representations W
s
and those
produced by the projection function
?
W
s
. The co-
sine has been widely used in the distributional se-
mantic literature, and it has been shown to out-
perform Euclidean distance (Bullinaria and Levy,
2007).
7
Parameters were estimated with standard
backpropagation and L-BFGS.
5 Results
Our experiments focus on the tasks of zero-shot
learning (Sections 5.1 and 5.2) and fast mapping
(Section 5.3). In both tasks, the projected vector of
the unseen concept is labeled with the word asso-
ciated to its cosine-based nearest neighbor vector
in the corresponding semantic space.
For the zero-shot task we report the accuracy
of retrieving the correct label among the top k
neighbors from a semantic space populated with
the union of seen and unseen concepts. For fast
mapping, we report the mean rank of the correct
concept among fast mapping candidates.
5.1 Zero-shot Learning in CIFAR-100
For this experiment, we use the intersection of
our linguistic space with the concepts present in
CIFAR-100, containing a total of 90 concepts. For
each concept category, we treat all concepts but
one as seen concepts (Table 1). The 71 seen con-
cepts correspond to 42,600 distinct visual vectors
and are used to induce the projection function. Ta-
ble 2 reports results obtained by averaging the per-
formance on the 11,400 distinct vectors of the 19
unseen concepts.
Our 4 models introduced in Section 4.4 are
compared to a theoretically derived baseline
Chance simulating selecting a label at random. For
the neural network NN, we use prior knowledge
7
We also experimented with the same objective func-
tion as Socher et al (2013), however, our objective function
yielded consistently better results in all experimental settings.
1408
PP
P
P
P
P
Model
k
1 2 3 5 10 20
Chance 1.1 2.2 3.3 5.5 11.0 22.0
SVD 1.9 5.0 8.1 14.5 29.0 48.6
CCA 3.0 6.9 10.7 17.9 31.7 51.7
lin 2.4 6.4 10.5 18.7 33.0 55.0
NN 3.9 6.6 10.6 21.9 37.9 58.2
Table 2: Percentage accuracy among top k nearest
neighbors on CIFAR-100.
about the number of concept categories to set the
number of hidden units to 20 in order to avoid
tuning of this parameter. For the SVD model, we
set the number of dimensions to 300, a common
choice in distributional semantics, coherent with
the settings we used for the visual and linguistic
spaces.
First and foremost, all 4 models outperform
Chance by a large margin. Surprisingly, the very
simple lin method outperforms both CCA and SVD.
However, NN, an architecture that can capture
more complex, non-linear relations in features
across modalities, emerges as the best performing
model, confirming on a larger scale the recent find-
ings of Socher et al (2013).
5.1.1 Concept Categorization
In order to gain qualitative insights into the perfor-
mance of the projection process of NN, we attempt
to investigate the role and interpretability of the
hidden layer. We achieve this by looking at which
visual concepts result in the highest hidden unit
activation.
8
This is inspired by analogous quali-
tative analysis conducted in Topic Models (Grif-
fiths et al, 2007), where ?topics? are interpreted
in terms of the words with the highest probability
under each of them.
Table 3 presents both seen and unseen con-
cepts corresponding to visual vectors that trigger
the highest activation for a subset of hidden units.
The table further reports, for each hidden unit, the
?correct? unseen concept for the category of the
top seen concepts, together with its rank in terms
of activation of the unit. The analysis demon-
strates that, although prior knowledge about cat-
egories was not explicitly used to train the net-
work, the latter induced an organization of con-
cepts into superordinate categories in which the
8
For this post-hoc analysis, we include a sparsity param-
eter in the objective function of Equation 5 in order to get
more interpretable results; hidden units are therefore maxi-
mally activated by a only few concepts.
Unseen Concept Nearest Neighbors
tiger cat, microchip, kitten, vet, pet
bike spoke, wheel, brake, tyre, motorcycle
blossom bud, leaf, jasmine, petal, dandelion
bakery quiche, bread, pie, bagel, curry
Table 4: Top 5 neighbors in linguistic space after
visual vector projection of 4 unseen concepts.
hidden layer acts as a cross-modal concept cate-
gorization/organization system. When the induced
projection function maps an object onto the lin-
guistic space, the derived text vector will inherit
a mixture of textual features from the concepts
that activated the same hidden unit as the object.
This suggests a bias towards seen concepts. Fur-
thermore, in many cases of miscategorization, the
concepts are still semantically coherent with the
induced category, confirming that the projection
function is indeed capturing a latent, cross-modal
semantic space. A squirrel, although not a ?large
omnivore?, is still an animal, while butterflies are
not flowers but often feed on their nectar.
5.2 Zero-shot Learning in ESP
For this experiment, we focus on NN, the best per-
forming model in the previous experiment. We
use a set of approximately 9,500 concepts, the in-
tersection of the ESP-based visual semantic space
with the linguistic space. For tuning the number
of hidden units of NN, we use the MEN-concrete
dataset of Bruni et al (2014). Finally, we ran-
domly pick 70% of the concepts to induce the pro-
jection function f
proj
v?w
and report results on the
remaining 30%. Note that the search space for the
correct label in this experiment is approximately
95 times larger than the one used for the experi-
ment presented in Section 5.1.
Although our experimental setup differs from
the one of Frome et al (2013), thus preventing a
direct comparison, the results reported in Table 5
are on a comparable scale to theirs. We note that
previous work on zero-shot learning has used stan-
dard object recognition benchmarks. To the best
of our knowledge, this is the first time this task has
been performed on a dataset as noisy as ESP. Over-
all, the results suggest that cross-modal mapping
could be applied in tasks where images exhibit a
more complex structure, e.g., caption generation
and event recognition.
1409
Seen Concepts Unseen Concept Rank of Correct CIFAR-100 Category
Unseen Concept
Unit 1 sunflower, tulip, pear butterfly 2 (rose) flowers
Unit 2 cattle, camel, bear squirrel 2 (elephant) large omnivores and herbivores
Unit 3 castle, bridge, house bus 4 (skyscraper) large man-made outdoor things
Unit 4 man, girl, baby boy 1 people
Unit 5 motorcycle, bicycle, tractor streetcar 2 (bus) vehicles 1
Unit 6 sea, plain, cloud forest 1 large natural outdoor scenes
Unit 7 chair, couch, table bed 1 household furniture
Unit 8 plate, bowl, can clock 3 (cup) food containers
Unit 9 apple, pear, mushroom orange 1 fruit and vegetables
Table 3: Categorization induced by the hidden layer of the NN; concepts belonging in the same CIFAR-
100 categories, reported in the last column, are marked in bold. Example: Unit 1 receives the highest
activation during training by the category flowers and at test time by butterfly, belonging to insects. The
same unit receives the second highest activation by the ?correct? test concept, the flower rose.
P
P
P
P
P
P
Model
k
1 2 5 10 50
Chance 0.01 0.02 0.05 0.10 0.5
NN 0.8 1.9 5.6 9.7 30.9
Table 5: Percentage accuracy among top k nearest
neighbors on ESP.
5.3 Fast Mapping in ESP
In this section, we aim at simulating a fast map-
ping scenario in which the learner has been just
exposed to a new concept, and thus has limited lin-
guistic evidence for that concept. We operational-
ize this by considering the 34 concrete concepts
introduced by Frassinelli and Keller (2012), and
deriving their text-based representations from just
a few sentences randomly picked from the corpus.
Concretely, we implement 5 models: context 1, con-
text 5, context 10, context 20 and context full, where
the name of the model denotes the number of sen-
tences used to construct the text-based representa-
tions. The derived vectors were reduced with the
same SVD projection induced from the complete
corpus. Cross-modal mapping is done via NN.
The zero-shot framework leads us to frame fast
mapping as the task of projecting visual represen-
tations of new objects onto language space for re-
trieving their word labels (v? w). This mapping
from visual to textual representations is arguably
a more plausible task than vice versa. If we think
about how linguistic reference is acquired, a sce-
nario in which a learner first encounters a new ob-
ject and then seeks its reference in the language of
the surrounding environment (e.g., adults having a
conversation, the text of a book with an illustration
of an unknown object) is very natural. Further-
more, since not all new concepts in the linguistic
environment refer to new objects (they might de-
note abstract concepts or out-of-scene objects), it
seems more reasonable for the learner to be more
alerted to linguistic cues about a recently-spotted
new object than vice versa. Moreover, once the
learner observes a new object, she can easily con-
struct a full visual representation for it (and the
acquisition literature has shown that humans are
wired for good object segmentation and recogni-
tion (Spelke, 1994)) ? the more challenging task is
to scan the ongoing and very ambiguous linguistic
communication for contexts that might be relevant
and informative about the new object. However,
fast mapping is often described in the psycholog-
ical literature as the opposite task: The learner
is exposed to a new word in context and has to
search for the right object referring to it. We im-
plement this second setup (w? v) by training the
projection function f
proj
w?v
which maps linguis-
tic vectors to visual ones. The adaptation of NN is
straightforward; the new objective function is de-
rived as
J(?
w?v
) =
1
2
(1? sim(V
s
,
?
V
s
)) (6)
where
?
V
s
= ?
(2)
(?
(1)
(Ws?
(1)
)?
(2)
), ?
(1)
?
R
d
w
?h
and ?
(2)
? R
h?d
v
.
Table 7 presents the results. Not surprisingly,
performance increases with the number of sen-
tences that are used to construct the textual repre-
sentations. Furthermore, all models perform bet-
ter than Chance, including those that are based on
just 1 or 5 sentences. This suggests that the system
can make reasonable inferences about object-word
connections even when linguistic evidence is very
scarce.
Regarding the sources of error, a qualitative
analysis of predicted word labels and objects as
1410
v?w w?v
cooker?potato dishwasher? corkscrew
clarinet? drum potato? corn
gorilla? elephant guitar? violin
scooter? car scarf? trouser
Table 6: Top-ranked concepts in cases where the
gold concepts received numerically high ranks.
X
X
X
X
X
X
X
X
Context
Mapping
v? w w? v
Chance 17 17
context 1 12.6 14.5
context 5 8.08 13.29
context 10 7.29 13.44
context 20 6.02 12.17
context full 5.52 5.88
Table 7: Mean rank results averaged across 34
concepts when mapping an image-based vector
and retrieving its linguistic neighbors (v? w) as
well as when mapping a text-based vector and
retrieving its visual neighbors (w? v). Lower
numbers cue better performance.
presented in Table 6 suggests that both textual
and visual representations, although capturing rel-
evant ?topical? or ?domain? information, are not
enough to single out the properties of the target
concept. As an example, the textual vector of dish-
washer contains kitchen-related dimensions such
as ?fridge, oven, gas, hob, ..., sink?. After projecting
onto the visual space, its nearest visual neighbours
are the visual ones of the same-domain concepts
corkscrew and kettle. The latter is shown in Figure
3a, with a gas hob well in evidence. As a further
example, the visual vector for cooker is extracted
from pictures such as the one in Figure 3b. Not
surprisingly, when projecting it onto the linguis-
tic space, the nearest neighbours are other kitchen-
related terms, i.e., potato and dishwasher.
6 Conclusion
At the outset of this work, we considered the
problem of linking purely language-based distri-
(a) A kettle
(b) A cooker
Figure 3: Two images from ESP.
butional semantic spaces with objects in the vi-
sual world by means of cross-modal mapping. We
compared recent models for this task both on a
benchmark object recognition dataset and on a
more realistic and noisier dataset covering a wide
range of concepts. The neural network architec-
ture emerged as the best performing approach, and
our qualitative analysis revealed that it induced a
categorical organization of concepts. Most impor-
tantly, our results suggest the viability of cross-
modal mapping for grounded word-meaning ac-
quisition in a simulation of fast mapping.
Given the success of NN, we plan to experi-
ment in the future with more sophisticated neural
network architectures inspired by recent work in
machine translation (Gao et al, 2013) and mul-
timodal deep learning (Srivastava and Salakhut-
dinov, 2012). Furthermore, we intend to adopt
visual attributes (Farhadi et al, 2009; Silberer
et al, 2013) as visual representations, since they
should allow a better understanding of how cross-
modal mapping works, thanks to their linguistic
interpretability. The error analysis in Section 5.3
suggests that automated localization techniques
(van de Sande et al, 2011), distinguishing an ob-
ject from its surroundings, might drastically im-
prove mapping accuracy. Similarly, in the textual
domain, models that extract collocates of a word
that are more likely to denote conceptual proper-
ties (Kelly et al, 2012) might lead to more infor-
mative and discriminative linguistic vectors. Fi-
nally, the lack of large child-directed speech cor-
pora constrained the experimental design of fast
mapping simulations; we plan to run more realis-
tic experiments with true nonce words and using
source corpora (e.g., the Simple Wikipedia, child
stories, portions of CHILDES) that contain sen-
tences more akin to those a child might effectively
hear or read in her word-learning years.
Acknowledgments
We thank Adam Li?ska for helpful discussions and
the 3 anonymous reviewers for useful comments.
This work was supported by ERC 2011 Starting
Independent Research Grant n. 283554 (COM-
POSES).
References
Barbara Abbott. 2010. Reference. Oxford University
Press, Oxford, UK.
1411
Paul Bloom. 2000. How Children Learn the Meanings
of Words. MIT Press, Cambridge, MA.
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL/IJCNLP, pages 82?90.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open
library for visual semantics representation. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word. Papers and Reports on Child Lan-
guage Development, 15:17?29.
Susan Carey. 1978. The child as a word learner. In
M. Halle, J. Bresnan, and G. Miller, editors, Linguis-
tics Theory and Psychological Reality. MIT Press,
Cambridge, MA.
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and
Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding meth-
ods. In Proceedings of BMVC, Dundee, UK.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of AAAI, pages
859?865, San Francisco, CA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167, Helsinki, Fin-
land.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and C?edric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop
on Statistical Learning in Computer Vision, ECCV,
pages 1?22, Prague, Czech Republic.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778?
1785, Miami Beach, FL.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science,
34:1017?1063.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of HLT-NAACL, pages 91?99, Los Angeles, CA.
Diego Frassinelli and Frank Keller. 2012. The plausi-
bility of semantic properties generated by a distribu-
tional model: Evidence from a visual world experi-
ment. In Proceedings of CogSci, pages 1560?1565.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639?2664.
David R Hardoon, Craig Saunders, Sandor Szedmak,
and John Shawe-Taylor. 2006. A correlation ap-
proach for automatic image annotation. In Ad-
vanced Data Mining and Applications, pages 681?
692. Springer.
Jonathon Hare, Sina Samangooei, Paul Lewis, and
Mark Nixon. 2008. Semantic spaces revisited: In-
vestigating the performance of auto-annotation and
semantic retrieval using semantic spaces. In Pro-
ceedings of CIVR, pages 359?368, Niagara Falls,
Canada.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335?
346.
Tracy Heibeck and Ellen Markman. 1987. Word learn-
ing in children: an examination of fast mapping.
Child Development, 58:1021?1024.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853?899.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11?20, Montreal, Canada.
1412
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Proceedings of NIPS,
pages 1106?1114.
Alex Krizhevsky. 2009. Learning multiple layers of
features from tiny images. Master?s thesis.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In Proceedings of CVPR, pages 2169?2178, Wash-
ington, DC.
Alessandro Lenci. 2008. Distributional approaches in
linguistic and cognitive research. Italian Journal of
Linguistics, 20(1):1?31.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?
208.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111?3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746?751, Atlanta, Georgia.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Proceedings of NIPS,
pages 1410?1418, Vancouver, Canada.
Alfredo F Pereira, Karin H James, Susan S Jones,
and Linda B Smith. 2010. Early biases and de-
velopmental changes in self-generated object views.
Journal of vision, 10(11).
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics, 1:25?36.
John Searle. 1984. Minds, Brains and Science. Har-
vard University Press, Cambridge, MA.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423?1433, Jeju, Korea.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of ACL, pages 572?582,
Sofia, Bulgaria.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61:39?91.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470?
1477, Nice, France.
Richard Socher and Li Fei-Fei. 2010. Connecting
modalities: Semi-supervised segmentation and an-
notation of images using unaligned text corpora. In
Proceedings of CVPR, pages 966?973.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Elizabeth Spelke. 1994. Initial knowledge: Six sug-
gestions. Cognition, 50:431?445.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In Proceedings of NIPS, pages 2231?2239.
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer, Berlin.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Koen van de Sande, Jasper Uijlings, Theo Gevers, and
Arnold Smeulders. 2011. Segmentation as selec-
tive search for object recognition. In Proceedings of
ICCV, pages 1879?1886, Barcelona, Spain.
Luis Von Ahn. 2006. Games with a purpose. Com-
puter, 29(6):92?94.
Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, and
Chong-Wah Ngo. 2007. Evaluating bag-of-visual-
words representations in scene classification. In
James Ze Wang, Nozha Boujemaa, Alberto Del
Bimbo, and Jia Li, editors, Multimedia Information
Retrieval, pages 197?206. ACM.
1413
Haonan Yu and Jeffrey Siskind. 2013. Grounded lan-
guage learning from video described with sentences.
In Proceedings of ACL, pages 53?63, Sofia, Bul-
garia.
1414
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171?181,
Dublin, Ireland, August 23-24 2014.
Dead parrots make bad pets:
Exploring modifier effects in noun phrases
Germ
?
an Kruszewski and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(german.kruszewski|marco.baroni)@unitn.it
Abstract
Sometimes modifiers have a strong effect
on core aspects of the meaning of the
nouns they are attached to: A parrot is
a desirable pet, but a dead parrot is, at
the very least, a rather unusual household
companion. In order to stimulate compu-
tational research into the impact of mod-
ification on phrase meaning, we collected
and made available a large dataset contain-
ing subject ratings for a variety of noun
phrases and the categories they might be-
long to. We propose to use compositional
distributional semantics to model these
data, experimenting with numerous distri-
butional semantic spaces, phrase compo-
sition methods and asymmetric similarity
measures. Our models capture a statis-
tically significant portion of the data, al-
though much work is still needed before
we achieve a full computational account of
modification effects.
1 Introduction
Not all modifiers are created equal. Green parrots
have all essential qualities of parrots, but dead par-
rots don?t. For example, as vocally argued by the
disgruntled costumer in Monty Python?s famous
Dead Parrot Sketch,
1
dead parrots make rather
poor pet birds. In modifier-head constructions
(that, for the purpose of this article, we restrict to
right-headed adjective-noun and noun-noun con-
structions), modifiers are not simply picking a sub-
set of the denotation of the head they modify, but
they are often distorting the properties of the head
in a radical manner.
These modifier effects on phrase meaning have
been studied extensively by theoretical linguists,
1
http://en.wikipedia.org/wiki/Dead_
Parrot_sketch
who have focused primarily on the extreme case
of intensional modifiers such as fake, alleged and
toy, where the phrase denotes something that is
no longer (or is not necessarily) a head (a toy
gun is not a gun). See McNally (2013) for a re-
cent review of the linguistic literature. Cognitive
scientists have looked at modification phenomena
within the general study of conceptual combina-
tion (see Chapter 12 of Murphy (2002) for an ex-
tensive review). The cognitive tradition has fo-
cused on how modification affects prototypicality:
a guppy is the prototypical pet fish, but it is neither
a typical pet nor a typical fish (Smith and Osher-
son, 1984). This line of research has highlighted
how strong modification effects might be the rule,
rather than the exception: Wisniewski (1997) re-
ports that, when subjects were asked to provide
the meaning for more than 200 novel modifier-
head constructions, ?70% [of the answers] in-
volved the construal of a noun?s referent as some-
thing other than the typical category named by the
noun [head].? Indeed, recent research suggests
that even the most stereotypical modifiers affect
prototypicality, so that subjects are less willing
to attribute to quacking ducks such obvious duck
properties as having webbed feet (Connolly et al.,
2007).
The impact of modification on phrase mean-
ing is not only very interesting from a linguistic
and cognitive perspective, but also important from
a practical point of view, as it might affect ex-
pected entailment patterns: If parrot entails pet,
then lively parrot also entails pet. However, as we
saw above, dead parrot doesn?t necessarily entail
pet (at least not from the point of view of a dis-
gruntled costumer who was just sold the corpse).
Being able to track the impact that modifiers have
on heads should thus have a positive effect on im-
portant tasks such as recognizing textual entail-
ment, paraphrasing and anaphora resolution (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
171
al., 2009; Poesio et al., 2010).
Despite their theoretical and practical import,
modification effects have been largely overlooked
in computational linguistics, with the notable ex-
ception of Boleda et al. (2012; 2013), who only
focused on the extreme case of intensional adjec-
tives, studied a limited number of modifiers, and
did not attempt to capture the graded nature of
modification (a dead parrot is not a prototypical
animal, but a toy parrot is not an animal at all).
This paper aims to stimulate computational re-
search into modifier effects on phrase meaning in
two ways. First, we introduce a new, large, pub-
licly available data set of modifier-head phrases
annotated with four kinds of modification-related
subject ratings: whether the concept denoted by
the phrase is an instance of the concept denoted by
its head (is a dead parrot still a parrot?), to what
extent it is a member of one of the larger categories
the head belongs to (is it still a pet?), and typical-
ity ratings for the same questions (how typical is a
dead parrot as a parrot? and as a pet?).
Second, we present a first attempt to model the
collected judgments computationally. We choose
distributional semantics (Erk, 2012) as our frame
of reference, as it produces continuous similarity
scores, in line with the graded nature of the mod-
ification effects we are investigating. In partic-
ular, we look at the compositional extension of
distributional semantics (Baroni, 2013), because
we need representations not only for words, but
also phrases, and we adopt the asymmetric simi-
larity measures developed in the literature on lex-
ical entailment (Kotlerman et al., 2010; Lenci and
Benotto, 2012), because we are interested in an
asymmetric relation (to what extent the concept
denoted by the phrase is a good instance of the tar-
get class, and not vice versa). As far as we know,
this is the first time these asymmetric measures
are applied to composed representations (Baroni
et al. (2012) experimented with entailment mea-
sures applied to phrase representations directly
harvested from corpora, and not derived composi-
tionally). We are thus also providing a novel eval-
uation of compositional models and asymmetric
measures on a challenging task where they could
potentially be very useful.
2
2
Connell and Ramscar (2001) showed good correlation of
similarity scores produced by the LSA distributional seman-
tic model with human category typicality judgments, how-
ever they did not consider phrases nor adopted an asymmetric
measure to take directionality into account.
2 The Norwegian Blue Parrot data set
We introduce Norwegian Blue Parrot (NBP),
3
a
new, large data set to explore modification effects.
Given a head noun h and a modifier adjective or
noun m, NBP contains average membership and
typicality ratings for the phrase mh both as an
instance of h and as an instance of c (a broader
category h belongs to). As a control, we also
present ratings for unmodified h as an instance
of c (we will use them below to test similarity
measures on their ability to capture the direction
of the membership relation, and to zero in on the
effect of modification vs. more general member-
ship/typicality effects). We include, and indeed fo-
cus on, relations with broader categories because
they are more prone to modification effects: In-
tuitively, a dead parrot is still a parrot, but it is,
at the very least, an atypical pet. The statistics
in Table 1, discussed below, confirm our intuition
that subjects are more likely to assign lower scores
with respect to a broader category than to the head
category itself (although this is, no doubt, in part
by construction, since we started constructing the
dataset by mining examples where mh is atypi-
cal of c, not h). We collect both membership and
typicality ratings because we expect them to have
different implications for sound entailment. If x
is not a member of class y, then x obviously does
not entail y. However, if x is an atypical y, en-
tailment still holds, but some typical properties of
y might not carry over (e.g., in an anaphora reso-
lution setting, we might still consider co-indexing
dead parrot with animal, but not with breathing
creature, despite the fact that breathing is a highly
characteristic property of animals).
In order to make sure that NBP would contain a
fair number of examples affected by strong mod-
ification effects, we first came up with a set of
?m,h, c? tuples where, according to our own in-
tuition, m makes h fairly atypical as an instance
of c. For example, a bottle is a piece of drinkware.
If we add the modifier perfume, we expect that,
while subjects might still agree that a perfume bot-
tle is a bottle, they should generally disagree on
the statement that a perfume bottle belongs to the
drinkware category. We refer to tuples of this
sort (e.g., ?perfume, bottle, drinkware?) as dis-
torted tuples in what follows.
4
3
Available from http://clic.cimec.unitn.it/
composes/
4
When creating the tuples, we also used some adjectives
172
We then constructed a number of tuples that
should not display a strong modification effect. In
particular, in order to insure that any atypical rat-
ing we obtained on the distorted tuples could not
be explained away by characteristics of m or h
alone (rather than by their combination), for each
distorted tuple we constructed a few more tuples
with the same h and c but a different m, that
we did not expect to be strongly distorting (e.g.,
?plastic, bottle, drinkware?). Similarly, for each
distorted tuple we generated a few more with the
same m, but combined with (the same or differ-
ent) h and c on which the m should not exert a
strong effect (?perfume, bottle, container?). In
total, NBP is based on 489 distorted tuples and
1938 more matching tuples.
We constructed NBP to insure that it would
contain many tuples displaying strong modifica-
tion effects, and highly comparable tuples that do
not feature such effects. An alternative approach
would have been to rate phrases that were ran-
domly selected from a corpus. This would have
led to a dataset reflecting a more realistic distribu-
tion of modification effects, but it would not have
guaranteed, for the same number of pairs, a fair
amount of distorted tuples and comparable con-
trols. We leave the study of the natural distribution
of modification strength in text to further work.
To find inspiration for the tuples, we looked into
various databases containing concepts organized
by category, namely BLESS (Baroni and Lenci,
2011), ConceptNet (Speer and Havasi, 2013) and
WordNet (Fellbaum, 1998). We insured that all
words in our tuples occurred at least 200 times in
the large corpus we describe below (phrases were
not filtered by frequency, due to data sparseness).
Finally, when looking for tuples matching the dis-
torted ones, we made sure that the mh phrases in
the new tuples have similar Pointwise Mutual In-
formation to the corresponding phrases in the dis-
torted tuple (or, where the latter were not attested
in the corpus, similar m and h frequencies). Find-
ing meaningful combinations among unattested or
infrequent phrases was not an easy task and there
was not always a perfect candidate. However, the
phrases selected in this way yielded challenging
items for which there is little or no direct cor-
pus evidence, so that compositional models are re-
quired to account for them.
that have been traditionally labeled as intensional by seman-
ticists: artificial, toy, former.
From each source tuple (e.g.,
?plastic, bottle, drinkware?), we generated 3
instance-class combinations to be rated: mh ? c
(plastic bottle ? drinkware), mh ? h (plastic
bottle? bottle), h? c (bottle? drinkware), for
a total of 5,849 pairs, that constitute the final NBP
data set (2,417 mh ? c pairs, 2,115 mh ? h
pairs and 1,317 h? c pairs).
5
For each of these pairs, we collected both mem-
bership and typicality ratings through two surveys
on the CrowdFlower platform.
6
Subjects came
exclusively from English speaking countries and
no special qualifications were required from them.
Membership ratings were collected by asking sub-
jects whether the instance is a member of the class
(formulated as a yes/no question). In a separate
study, we asked subjects to rate how typical the in-
stance is as member of the class on a 7-point scale.
For both questions, we collected 10 judgments per
pair and report their averages in NBP. For both sur-
veys, we added 48 control pairs with an expected
answer (yes/no for membership, high/low range
for typicality), that the subjects had to provide in
order for their ratings to be included in the final
set (?gold standard? items in crowd-sourcing par-
lance). These controls included highly prototypi-
cal pairs (dog? animal), possibly with stereotyp-
ical modifiers (beautiful rose? flower), and unre-
lated pairs (biology? dance), also possibly under
modification (popular magazine? animal).
We asked for binary rather than graded member-
ship judgments because these are more in line with
commonsense intuitions about category member-
ship (we might naturally speak of sparrows being
more typical birds than penguins, but it is strange
to say that they are ?more birds?). The standard
view in the psychology of concepts (Hampton,
1991) is that membership judgments are the prod-
uct of a hard threshold we impose on the typicality
scale (x is not y if the typicality of x as y is below
a certain, subject-dependent threshold), although
under certain experimental conditions subjects can
also conceptualize membership as a graded prop-
erty (Kalish, 1995).
Membership and typicality ratings, especially
in borderline cases such as those we constructed,
are the output of complex cognitive processes
where large inter-subject differences are expected,
5
There is a larger number of mh ? c pairs because dif-
ferent tuples can lead to the same mh? h or h? c combi-
nations.
6
http://crowdflower.com/
173
measure mh? c mh? h h? c tot.
memb. 0.84 (0.2) 0.97 (0.1) 0.88 (0.2) 0.89 (0.2)
typ. 5.45 (1.1) 6.29 (0.6) 5.81 (1.0) 5.84 (1.0)
Table 1: NBP summary statistics: Mean average
ratings and their standard deviations across pairs,
itemized by instance-class type and in total. Mem-
bership values range from 0 to 1, typicality values
from 1 to 7.
so it doesn?t make sense to worry about ?inter-
annotator agreement? in this context. Still, several
sanity checks indicate that, overall, our subjects
understood our questions as we meant them, and
behaved in a reasonably coherent manner. First,
both average membership and typicality, ratings
are significantly lower (p < 0.001) for the mh ?
c pairs deriving from those tuples that we manu-
ally labeled as distorted than for the non-distorted
ones. Moreover, for membership, in 86% of the
cases at least 8 over 10 subjects gave the same re-
sponse. For typicality, the observed average rat-
ing standard deviation across pairs (1.2) is signifi-
cantly below what expected by chance (p < 0.05),
based on a simulated random rating distribution.
Membership and typicality ratings are highly cor-
related, but not identical (r = 0.76)
Table 1 reports mean membership and typicality
scores in NBP. Both ratings are negatively skewed,
that is, subjects had the tendency to respond as-
sertively to the membership question and to give
high typicality scores. This is not surprising: Be-
cause of the way NBP was constructed, there are
about 4 tuples with no expected strong modifica-
tion effect for each distorted tuple. Furthermore,
except for the negative control items (not entered
in NBP), our questions did not feature cases where
a negative/low response would be entirely straight-
forward (of the ?is a cat a building?? kind). We
observe moreover that, in accordance with the in-
tuition we discussed at the beginning of this sec-
tion, the ratings are extremely high when the class
is identical to the phrase head. On the other hand,
the mh ? c condition displays, as expected, the
lowest averages, suggesting that this will be the
most interesting type to model experimentally.
Table 2 presents a few example entries from
NBP. The first block of the table illustrates cases
with the highest possible membership and typical-
ity scores. At the other extreme, the second block
contains examples with very low membership and
typicality. Interestingly, there are also cases, such
instance class memb. typ.
top membership, top typicality
gourmet soup food 1.00 7.00
huge tiger predator 1.00 7.00
sugared soda drink 1.00 7.00
live fish animal 1.00 7.00
Thai rice rice 1.00 7.00
silver spoon spoon 1.00 7.00
low membership, low typicality
fatal shooting sport 0.20 1.40
human egg food 0.40 1.50
perfume bottle drinkware 0.10 1.30
explosive vest commodity 0.30 1.90
lemon water chemical 0.20 1.60
creamy rice bean 0.20 1.30
top membership, (relatively) low typicality
sick tuna tuna 1.00 3.20
explosive vest vest 1.00 3.50
perforated sieve tool 1.00 4.20
bottled oxygen substance 1.00 4.30
grilled trout creature 1.00 4.40
educational toy amusement 1.00 4.50
Table 2: Instance-class pairs illustrating various
combinations of membership and typicality rat-
ings in NBP.
as the ones in the third block of the table, where all
subjects agreed on class membership, but the typ-
icality scores are relatively low (we did not find
clear cases of the opposite pattern, and indeed we
would have been surprised to find highly typical
instances of a class not being treated as members
of the class).
Some examples in Table 2 illustrate an impor-
tant design choice we made in constructing NBP,
namely, to ignore the issue of whether potential
modification effects are actually due to the modi-
fier and the category pertaining to different word
senses of the head term. One might argue, for
example, that egg has a food sense and a repro-
ductive vessel sense. The human modifier picks
the second sense, and so, obviously, human eggs
are judged as bad instances of food. While we
see the point of this objection, we think it?s im-
possible to draw a clear-cut distinction between
discrete word senses (even in the rather extreme
egg case, the eggs we eat are reproductive ves-
sels from a chicken point of view!). This has
been long recognized in the linguistic and cog-
nitive literature (Kilgarriff, 1997; Murphy, 2002),
174
and even by the computational word sense disam-
biguation community, that is currently addressing
the continuous nature of polysemy by shifting to
the lexical-substitution-in-context task (McCarthy
and Navigli, 2009). Context provides fundamen-
tal cues to disambiguating polysemous words, and
noun modifiers typically act as important disam-
biguating contexts for the nouns. Thus, we think
that it is more productive for computational sys-
tems to handle modifier-triggered disambiguation
as a special case of the more general class of mod-
ification effects, than to engage in the quixotic
pursuit to determine, a priori, what?s the bound-
ary between a word-sense and a ?pure? modifi-
cation effect. Note in Table 2 that grilled trout
was unanimously rated by subjects as an instance
of the creature category, despite the fact that the
cooking-related grilled modifier cues a classic
shift from an animal (and thus creature) sense to
food (Copestake and Briscoe, 1995). Examples
like this suggest that our agnosticism is warranted.
3 Methods
3.1 Composition models
We experiment with many ways to derive a phrase
vector by combining the vectors of its constituents.
Mitchell and Lapata (2010) proposed a set of sim-
ple models in which each component of the phrase
vector is a function of the corresponding compo-
nents of the constituent vectors. Given vectors ~a
and
~
b, the weighted additive model (wadd) returns
their weighted sum: ~p = w
1
~a + w
2
~
b. In the dila-
tion model (dil), the output vector is obtained by
decomposing one of the input vectors, say
~
b, into
a vector parallel to ~a and its orthogonal counter-
part, and then dilating only the parallel vector by a
factor ? before re-combining. The corresponding
formula is: (~a ?~a)
~
b + (? ? 1)(~a ?
~
b)~a. In our ex-
periments, we stretch the head vector in the direc-
tion of the modifier (i.e., ~a is the modifier,
~
b is the
head). In the multiplicative model (mult), vectors
are combined by component-wise multiplication,
such that each phrase component p
i
is given by:
p
i
= a
i
b
i
.
Guevara (2010) and Zanzotto et al. (2010) pro-
pose a full form of the additive model (fulladd),
where the two constituent vectors are multiplied
by weight matrices before being added, so that
each phrase component is a weighted sum of all
constituent components: ~p = W
1
~a+W
2
~
b.
Finally, the lexical function (lexfunc) model of
Baroni and Zamparelli (2010) and Coecke et al.
(2010) takes inspiration from formal semantics
to characterize composition as function applica-
tion. In particular, in modifier-head phrases, the
modifier is treated as a linear function operating
on the head vector. Given that linear functions
can be expressed by matrices and their application
by matrix-by-vector multiplication, the modifier is
represented by a matrix A to be multiplied with
the modifier vector
~
b, so that: ~p = A
~
b.
We use the DISSECT toolkit
7
to estimate the
parameters of the composition methods and de-
rive phrase vectors. In particular, DISSECT finds
optimal parameter settings by learning to approx-
imate corpus-extracted phrase vector examples
with least-squares methods (Dinu et al., 2013).
We use as training examples all the modifier-head
phrases that contain a modifier of interest and oc-
cur at least 50 times in our source corpus (see Sec-
tion 3.3 below).
3.2 Asymmetric similarity measures
Several measures to identify word pairs that stand
in an instance-class relationship by comparing
their vectors have been proposed in the recent dis-
tributional semantics literature (Kotlerman et al.,
2010; Lenci and Benotto, 2012; Weeds et al.,
2004).
8
While the task of deciding if u is in class v
is typically framed (also by distributional semanti-
cists) in binary, yes-or-no terms, all proposed mea-
sures return a continuous numerical score.
9
Con-
sequently, we conjecture that they might be well-
suited to capture the graded notions of class mem-
bership and typicality we recorded in NBP.
10
In what follows, we use w
x
(f) to denote the
weight (value) of feature (dimension) f in the dis-
tributional vector of term x. F
x
denotes the set of
features (dimensions) in the vector of x such that
w
x
(f) > t, where t is a predefined threshold to
decide whether a feature is active.
11
Importantly,
7
http://clic.cimec.unitn.it/composes/
toolkit/
8
We speak of ?instance-class relations? in a very broad
and loose sense, to encompass classic relations such as hy-
ponymy but also the fuzzier notion of lexical entailment.
9
SVM classifiers have also been shown by Baroni et al.
(2012) to be well-suited for entailment detection, but they do
not naturally return continuous scores.
10
Subjects had to answer a yes/no question concerning
class membership, but by averaging their response we derive
continuous membership scores.
11
The obvious choice for t is 0. However, when work-
ing with the low-rank spaces described in Section 3.3 below,
we set t to 0.1, since after SVD/NMF smoothing we observe
175
all measures assume non-negative values.
Most asymmetric measures proposed in the lit-
erature build upon the distributional inclusion hy-
pothesis, stating that ?if u is a semantically nar-
rower term than v, then a significant number
of salient distributional features of u is included
in the feature vector of v as well? (Lenci and
Benotto, 2012). In our terminology, u is the poten-
tial instance, and v is the class. We re-implement
all the measures adopted by Lenci and Benotto,
namely weedsprec, cosweeds, clarkede and invcl
(see their paper for the original references):
weedsprec(u, v) =
?
f?F
u
?F
v
w
u
(f)
?
f?F
u
w
u
(f)
cosweeds(u, v) =
?
weedsprec(u, v)? cosine(u, v)
clarkede(u, v) =
?
f?F
u
?F
v
min(w
u
(f), w
v
(f))
?
f?F
u
w
u
(f)
invcl(u, v) =
?
clarkede(u, v)? (1? clarkede(u, v))
The cosweeds formula combines weedsprec
with the widely used symmetric cosine measure:
cosine(u, v) =
?
f?F
u
?F
v
w
u
(f)? w
v
(f)
?
?
f?F
u
w
u
(f)
2
?
?
?
f?F
v
w
v
(f)
2
Finally, we experiment with the carefully
crafted balapinc measure of Kotlerman et al.
(2010):
balapinc(u, v) =
?
lin(u, v) ? apinc(u, v)
where the lin term is computed as follows:
lin(u, v) =
?
f?F
u
?F
v
w
u
(f) + w
v
(f)
?
f?F
u
w
u
(f) +
?
f?F
v
w
v
(f)
The balapinc score is the geometric average
of a symmetric similarity measure (lin) and the
strongly asymmetric apinc measure, that takes
large values when dimensions with high values in
the vector of the more specific term are also high
in the vector of the more general term (refer to
Kotlerman et al. (2010) for the apinc formula).
widespread low-frequency noise.
3.3 Distributional semantic spaces
We extract co-occurrence information from a cor-
pus of about 2.8 billion words obtained by con-
catenating ukWaC,
12
Wikipedia
13
and the British
National Corpus.
14
With DISSECT, we build co-
occurrence vectors for the top 20K most frequent
lemmas in the source corpus (plus any NBP term
missing from this list). We treat the top 10K
most frequent lemmas as context elements. We
consider context windows of 2 and 20 words on
the two sides of the targets. We weight the vec-
tors by non-negative Pointwise Mutual Informa-
tion and Local Mutual Information (Evert, 2005).
We experiment with vectors in the resulting full-
rank (10K-dimensional) semantic spaces as well
as with vectors in spaces of ranks 100 and 300.
Rank reduction is performed by applying the Sin-
gular Value Decomposition (Golub and Van Loan,
1996) or Non-negative Matrix Factorization (Lee
and Seung, 2000). It is customary to represent the
output of these operations directly in a dense low-
dimensional space. However, the asymmetric sim-
ilarity measures we use assume sparse vectors (or
the ?inclusion? criterion would be meaningless),
so we project back the outcome of SVD and NMF
to sparse 10K-dimensional but low-rank spaces. In
total, we explore 20 distinct semantic spaces.
We also collect co-occurrence vectors for
the phrases needed to estimate the composi-
tion method parameters (see Section 3.1 above).
We use DISSECT?s ?peripheral space? option to
project the phrase raw count vectors into the vari-
ous spaces without affecting their structure.
Due to memory constraints, we restrict evalua-
tion in the full-rank spaces to the wadd and mult
models.
4 Experiments
Given the methods described above, the main
question we want to answer is: Which combina-
tion of compositional model and asymmetric sim-
ilarity measure yields a better fit for the data in the
NBP dataset?
We start however with a sanity check on the
ability of the measures to capture the direction of
the instance-class membership relation. Even a
measure that is good at capturing degrees of mem-
bership/typicality won?t be of much practical use
12
http://wacky.sslmit.unibo.it
13
http://en.wikipedia.org
14
http://www.natcorp.ox.ac.uk
176
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
10 8 11 8 7
Full-rank spaces
2 4 4 4 2
Table 3: Number of spaces (over totals of 16 low-
rank and 4 full-rank spaces) in which each mea-
sure was able to predict class membership direc-
tion significantly above chance.
if it is not able to tell us which item in a pair is the
instance and which is the class.
Detecting membership direction As described
in Section 2 above, NBP also contains single-
word h? c pairs (parrot? pet). We extracted
the subset of those that all judges considered to
be in the category membership relation, and we
checked them manually to make sure that the di-
rection was one-way only. This resulted in a set
of 639 pairs where the membership relation holds
unidirectionally. We tested all combination of se-
mantic spaces (Section 3.3) and asymmetric sim-
ilarity measures (Section 3.2) on the task of as-
signing a higher score to the pairs in the h ? c
(vs. c ? h) direction (e.g., (score(parrot ?
pet) > score(pet ? parrot)). Table 3 reports,
for each measure, the number of spaces in which
the measure was able to predict membership di-
rection significantly better than chance (binomial
test, p < 0.05). We report results on full- and
low-rank (SVD, NMF) spaces separately since, as
discussed above, for most composition models we
can only use the latter. We observe that all mea-
sures are able to significantly detect directionality
in at least some spaces. For all the analyses below,
we exclude from further testing the space-measure
combinations that failed to pass this sanity check,
since they are clearly failing to capture properties
pertaining to the instance-class relation (if a com-
bination is not able to tell that it is a parrot that is
a pet, and not vice versa, there is no point in ask-
ing if the same combination is able to model how
typical a dead parrot is as a pet).
Modeling typicality ratings of mh ? c pairs
Next, for each of the remaining spaces, we first
performed composition as described in Section 3.1
above to build the representations for the nominal
phrases in the NBP dataset, and then computed
asymmetric similarity scores for pairs made of a
phrase and the corresponding potential class.
We computed the correlations between mean
human membership or typicality ratings and the
scores produced with each combination of com-
position model, similarity measure and space.
The resulting performance profiles for member-
ship and typicality are very highly correlated (r =
.99), and we thus report only the latter. We leave
it to further work to devise measures that are more
specifically tuned to capture membership or typi-
cality.
Table 4 reports the top correlation coefficients
between typicality judgments and scores of each
mh ? c pair (dead parrot? pet) across spaces,
organized by measures and composition meth-
ods. The best correlation is achieved with the
weedsprec measure using the mult composition
model in a full-rank space (precisely that of con-
text window size 2 and ppmi weighting). Recall
that mult returns the component-wise product of
the vectors it combines. Thus, modification un-
der mult is carried out by picking only those fea-
tures of the head that are also present in the mod-
ifier, and enhancing them by a factor given by the
modifier?s feature value. The weedsprec measure
is then given by the weighted proportion of active
features in mh that are also active in c. Therefore,
the more the modifier shares features with the par-
ent category, the higher weedsprec will be. This
might explain why weedsprec is a good fit for the
mult model in measuring degrees of category typ-
icality.
Looking at composition methods, there is no ev-
idence that the more complex, matrix-based ful-
ladd and lexfunc approaches are performing any
better than the simple multiplicative and additive
methods. Indeed, mult shows the most consistent
overall performance, confirming the conclusion of
Blacoe and Lapata (2012) that, at the present time,
when it comes to composition, ?simpler is better?.
A related point emerges from the comparison of
the low- and full-rank results for mult and wadd.
The smoothing process due to dimensionality re-
duction is quite disruptive for the current asym-
metric measures, that are based on feature inclu-
sion. This is a further reason to stick to simpler
composition methods, that can be applied directly
in the full-rank spaces.
Regarding the measures themselves, we see that
cosweeds, that balances weedsprec with the clas-
sic cosine score, is the most robust, returning good
177
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 9* 15* 16* 19* 8*
fulladd 17* 16* 12* 24* ?3
lexfunc 17* 12* 12* 27* ?2
mult 13* 19* 19* 29* 12*
wadd 14* 14* 16* 27* ?2
Full-rank spaces
mult 9* 39* 33* 36* 15*
wadd 30* 34* 31* 35* 14*
Table 4: Percentage Pearson r between asymmet-
ric similarity measures andmh? c typicality rat-
ings. *p < 0.001
results across all composition methods. On the
other hand, the related clarkede and invcl mea-
sures turn out to be quite brittle.
The highly significant correlations show that the
measures do capture to some extent the patterns of
variance in the data. However, when considering
potential practical applications, even the highest
reported correlation (.39) is certainly not impres-
sive, indicating that there is plenty of room for fur-
ther research into developing better composition
methods and/or membership/typicality measures.
Focusing on the modifier effect for mh? c
pairs The typicality judgment for dead parrot as
a pet is influenced by two factors: how typical par-
rots are as pets, and how much more or less typical
dead parrots are as pets, as opposed to parrots in
general. A good model must be able to capture
both factors (and this is what we tested above).
However, we are also interested in assessing to
what extent the models are capturing the modifi-
cation effectproper, as opposed to the overall de-
gree of typicality of the h concept as member of
the c category. To focus on the modification fac-
tor, we partialed out the h?c (parrot?pet) rat-
ings from the mh?c (dead parrot?pet) ratings
and from the corresponding model scores (that is,
we correlated the residuals of mh?c ratings and
model-produced scores after regressing the h?c
ratings on both). The results are shown in Table
5. Correlations are lower overall, but the general
picture from the previous analysis still holds, con-
firming that the computational models are (also)
capturing modifier effects. Interestingly, wadd, dil
and fulladd generally undergo larger performance
drops than mult and lexfunc. Evidently, models
like the latter, in which the modifier selects the
relevant features from the head, are better suited
to explain modification than the former, in which
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 5 ?1 ?1 ?2 7*
fulladd 10* 7* 5+ 7+ ?2
lexfunc 15* 9* 10* 18* ?2
mult 4+ 14* 13* 15* 9*
wadd 7+ 7* 9* 12+ ?2
Full-rank spaces
mult 1 25* 21* 24* 5+
wadd 11* 18* 13* 20* 2
Table 5: Percentage Pearson r between asymmet-
ric similarity measures andmh? c typicality rat-
ings where h ? c scores have been partialed out.
*p < 0.001, +p < 0.05
the modifier features are just added to those of the
head by means of a linear combination.
Modeling typicality ratings of mh ? h pairs
We repeated the first analysis for pairs of the type
mh ? h (dead parrot? parrot). The results,
shown in Table 6, are lower than in the previous
analysis. This is probably due to the fact that, as
discussed in Section 2, when the very same con-
cept is used as phrase head and category, judg-
ments are subject to a strong ceiling effect, and
none of our measures is designed to flatten out
above a certain threshold. Indeed, if we measure
the skewness of the typicality ratings,
15
we obtain
that, while for h? c and mh? c the skewness is
of?1.9 and?1.5, respectively, formh?h it gets
to ?3.9.
In any case, the results confirm the brittleness of
the clarkede and invcl measures. The linguistically
motivated lexfunc model emerges here as a com-
petitive alternative to the simpler models. Still, the
best results are obtained with mult and cosweeds
(on the full-rank, context window size 20, ppmi
weighted space). Notably, weedsprec applied to a
pair of the type mh? h, where the phrase is con-
structed using the mult model, results in a constant
value of 1, whatever the modifier and the head
noun is. This is due to the fact that the features of
a phrase composed using mult are a subset of the
features of the head,
16
and in this case the head is
the same as the category. Therefore, by definition,
weedsprec yields a score of 1 for every pair, the
variance is null and hence the correlation is unde-
15
A skewness factor of 0 means that the distribution is bal-
anced around the mean, while the more negative the coeffi-
cient is, the more the left tail is longer and the distribution is
concentrated to the right (toward high typicality values in our
case).
16
In set notation: F
u
? F
v
= F
u
since F
u
? F
v
178
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 2 ?1 ?2 ?3 4
fulladd 5+ 5+ 2 1 ?1
lexfunc 14* 8* 14* 17* ?1
mult 3 - 13* 15* 5+
wadd 6+ 8* 7+ 6 ?3
Full-rank spaces
mult ?2 - 18* 19* ?2
wadd 7* 13* 7* 12* ?2
Table 6: Percentage Pearson r between asymmet-
ric similarity measures andmh? h typicality rat-
ings. *p < 0.001, +p < 0.05
fined. As a consequence, in this case cosweeds,
which is the geometric mean between weedsprec
and cosine, reduces to cosine similarity! The latter
might be effective in capturing the degree of simi-
larity between the phrase and its potential category
but, as a symmetric measure, it cannot, alone, pro-
vide a full account of category typicality effects.
5 Conclusion
We introduced the challenge of quantifying the
impact of modification on the meaning of noun
phrases to the computational linguistics commu-
nity. We presented a new dataset that collects
membership and typicality ratings for modifier-
head phrases with respect to the category repre-
sented by the head as well as a broader category.
Since accounting for modifier distortion requires
semantic representations of phrases and model-
ing graded judgments, we consider this an ideal
testbed for compositional distributional semantics.
In the interaction between compositional mod-
els and directional similarity measures, we have
observed that simpler models yield better results.
Specifically, mult and wadd are economical com-
position models than can be applied on full-rank
spaces, which in turn work best with our similar-
ity measures.
Psychologists studying modification effects in
concept combination have proposed models that
are usually quite complex, relying on hand-crafted
feature definitions and making very strong as-
sumptions about the combination process (see for
example Cohen and Murphy (1984), Smith et al.
(1988)). Some of these assumptions have led other
researchers to argue that prototypes do not com-
pose at all (Connolly et al., 2007). In contrast,
the approach we borrow from distributional se-
mantics, while only mildly successful for now, has
the advantage of being very simple both in its con-
struction and application, and in the assumptions
that it makes.
Also notable is that we are putting under the
same umbrella tasks that have been traditionally
tackled separately. For example, among the ef-
fects present in the dataset, we can find both word
sense disambiguation (see discussion at the end of
Section 2) and what Murphy (2002) calls ?knowl-
edge effects? (e.g., a plane makes a very good ma-
chine, but a paper plane doesn?t). Moreover, these
effects can also interact (people know that a hu-
man egg is actually a single, small cell, and hence
not even cannibals would consider it satisfactory
food). We can thus explore the empirical ques-
tion of whether all these related phenomena can
be tackled together, with a single model account-
ing for all of them.
In conclusion, the challenge that we intro-
duced brings together concept combination and
non-subsective modification phenomena studied
in psychology and theoretical linguistics, and tries
to handle them with the standard machinery of
computational linguistics. This challenge has
proved quite difficult for current tools, but this is
exactly what we expected in the first place. Our
goal, from the outset, was to create a task that
could help us delimiting the boundaries of com-
putational methods for characterizing human con-
cepts, while delimiting, at the same time, the no-
tion of human concepts itself.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search, 38:135?187.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP GEMS Workshop, pages
1?10, Edinburgh, UK.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
179
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23?32, Avignon, France.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511?522.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First order vs. higher
order modification in distributional semantics. In
Proceedings of EMNLP, pages 1223?1233, Jeju Is-
land, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia The Pham. 2013. Intensionality was only
alleged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of IWCS, pages
35?46, Potsdam, Germany.
Graham Chapman. 1989. The complete Monty
Python?s flying circus : all the words. Pantheon
Books, New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Benjamin Cohen and Gregory L Murphy. 1984. Mod-
els of concepts. Cognitive Science, 8(1):27?58.
Louise Connell and Michael Ramscar. 2001. Using
distributional measures to model typicality in cate-
gorization. In Proceedings of CogSci, pages 226?
231, Edinburgh, UK.
Andrew Connolly, Jerry Fodor, Lila Gleitman, and
Henry Gleitman. 2007. Why stereotypes don?t even
make good defaults. Cognition, 103(1):1?22.
Ann Copestake and Ted Briscoe. 1995. Semi-
productive polysemy and sense extension. Journal
of Semantics, 12:15?67.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15:459?476.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
James Hampton. 1991. The combination of prototype
concepts. In Paula Schwanenflugel, editor, The psy-
chology of word meanings, pages 91?116. Erlbaum,
Hillsdale, NJ.
Charles Kalish. 1995. Essentialism and graded mem-
bership in animal and artifact categories. Memory
and Cognition, 23(3):335?353.
Adam Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31:91?113.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Alessandro Lenci and Giulia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of *SEM, pages 75?79, Montreal,
Canada.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Gregory Murphy. 2002. The Big Book of Concepts.
MIT Press, Cambridge, MA.
Massimo Poesio, Simone Ponzetto, and Yan-
nick Versley. 2010. Computational models
of anaphora resolution: A survey. http:
//clic.cimec.unitn.it/massimo/
Publications/lilt.pdf.
Edward Smith and Daniel Osherson. 1984. Concep-
tual combination with prototype concepts. Cogni-
tive Science, 8(4):337?361.
Edward E Smith, Daniel N Osherson, Lance J Rips,
and Margaret Keane. 1988. Combining prototypes:
A selective modification model. Cognitive Science,
12(4):485?527.
180
Robert Speer and Catherine Havasi. 2013. Con-
ceptNet 5: A large semantic network for relational
knowledge. In Iryna Gurevych and Jungi Kim, edi-
tors, The People?s Web Meets NLP, pages 161?176.
Springer, Berlin.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015?
1021, Geneva, Switzerland.
Edward Wisniewski. 1997. When concepts combine.
Psychonomic Bulletin & Review, 4(2):167?183.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
181
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1?8,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 1: Evaluation of Compositional Distributional
Semantic Models on Full Sentences through Semantic Relatedness and
Textual Entailment
Marco Marelli
(1)
Luisa Bentivogli
(2)
Marco Baroni
(1)
Raffaella Bernardi
(1)
Stefano Menini
(1,2)
Roberto Zamparelli
(1)
(1)
University of Trento, Italy
(2)
FBK - Fondazione Bruno Kessler, Trento, Italy
{name.surname}@unitn.it, {bentivo,menini}@fbk.eu
Abstract
This paper presents the task on the evalu-
ation of Compositional Distributional Se-
mantics Models on full sentences orga-
nized for the first time within SemEval-
2014. Participation was open to systems
based on any approach. Systems were pre-
sented with pairs of sentences and were
evaluated on their ability to predict hu-
man judgments on (i) semantic relatedness
and (ii) entailment. The task attracted 21
teams, most of which participated in both
subtasks. We received 17 submissions in
the relatedness subtask (for a total of 66
runs) and 18 in the entailment subtask (65
runs).
1 Introduction
Distributional Semantic Models (DSMs) approx-
imate the meaning of words with vectors sum-
marizing their patterns of co-occurrence in cor-
pora. Recently, several compositional extensions
of DSMs (CDSMs) have been proposed, with the
purpose of representing the meaning of phrases
and sentences by composing the distributional rep-
resentations of the words they contain (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Despite the ever increasing interest in the
field, the development of adequate benchmarks for
CDSMs, especially at the sentence level, is still
lagging. Existing data sets, such as those intro-
duced by Mitchell and Lapata (2008) and Grefen-
stette and Sadrzadeh (2011), are limited to a few
hundred instances of very short sentences with a
fixed structure. In the last ten years, several large
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
data sets have been developed for various com-
putational semantics tasks, such as Semantic Text
Similarity (STS)(Agirre et al., 2012) or Recogniz-
ing Textual Entailment (RTE) (Dagan et al., 2006).
Working with such data sets, however, requires
dealing with issues, such as identifying multiword
expressions, recognizing named entities or access-
ing encyclopedic knowledge, which have little to
do with compositionality per se. CDSMs should
instead be evaluated on data that are challenging
for reasons due to semantic compositionality (e.g.
context-cued synonymy resolution and other lexi-
cal variation phenomena, active/passive and other
syntactic alternations, impact of negation at vari-
ous levels, operator scope, and other effects linked
to the functional lexicon). These issues do not oc-
cur frequently in, e.g., the STS and RTE data sets.
With these considerations in mind, we devel-
oped SICK (Sentences Involving Compositional
Knowledge), a data set aimed at filling the void,
including a large number of sentence pairs that
are rich in the lexical, syntactic and semantic phe-
nomena that CDSMs are expected to account for,
but do not require dealing with other aspects of
existing sentential data sets that are not within
the scope of compositional distributional seman-
tics. Moreover, we distinguished between generic
semantic knowledge about general concept cate-
gories (such as knowledge that a couple is formed
by a bride and a groom) and encyclopedic knowl-
edge about specific instances of concepts (e.g.,
knowing the fact that the current president of the
US is Barack Obama). The SICK data set contains
many examples of the former, but none of the lat-
ter.
2 The Task
The Task involved two subtasks. (i) Relatedness:
predicting the degree of semantic similarity be-
tween two sentences, and (ii) Entailment: detect-
ing the entailment relation holding between them
1
(see below for the exact definition). Sentence re-
latedness scores provide a direct way to evalu-
ate CDSMs, insofar as their outputs are able to
quantify the degree of semantic similarity between
sentences. On the other hand, starting from the
assumption that understanding a sentence means
knowing when it is true, being able to verify
whether an entailment is valid is a crucial chal-
lenge for semantic systems.
In the semantic relatedness subtask, given two
sentences, systems were required to produce a re-
latedness score (on a continuous scale) indicating
the extent to which the sentences were expressing
a related meaning. Table 1 shows examples of sen-
tence pairs with different degrees of semantic re-
latedness; gold relatedness scores are expressed on
a 5-point rating scale.
In the entailment subtask, given two sentences
A and B, systems had to determine whether the
meaning of B was entailed by A. In particular, sys-
tems were required to assign to each pair either
the ENTAILMENT label (when A entails B, viz.,
B cannot be false when A is true), the CONTRA-
DICTION label (when A contradicted B, viz. B is
false whenever A is true), or the NEUTRAL label
(when the truth of B could not be determined on
the basis of A). Table 2 shows examples of sen-
tence pairs holding different entailment relations.
Participants were invited to submit up to five
system runs for one or both subtasks. Developers
of CDSMs were especially encouraged to partic-
ipate, but developers of other systems that could
tackle sentence relatedness or entailment tasks
were also welcome. Besides being of intrinsic in-
terest, the latter systems? performance will serve
to situate CDSM performance within the broader
landscape of computational semantics.
3 The SICK Data Set
The SICK data set, consisting of about 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment, was used to evaluate the
systems participating in the task. The data set
creation methodology is outlined in the following
subsections, while all the details about data gen-
eration and annotation, quality control, and inter-
annotator agreement can be found in Marelli et al.
(2014).
3.1 Data Set Creation
SICK was built starting from two existing data
sets: the 8K ImageFlickr data set
1
and the
SemEval-2012 STS MSR-Video Descriptions data
set.
2
The 8K ImageFlickr dataset is a dataset of
images, where each image is associated with five
descriptions. To derive SICK sentence pairs we
randomly chose 750 images and we sampled two
descriptions from each of them. The SemEval-
2012 STS MSR-Video Descriptions data set is a
collection of sentence pairs sampled from the short
video snippets which compose the Microsoft Re-
search Video Description Corpus. A subset of 750
sentence pairs were randomly chosen from this
data set to be used in SICK.
In order to generate SICK data from the 1,500
sentence pairs taken from the source data sets, a 3-
step process was applied to each sentence compos-
ing the pair, namely (i) normalization, (ii) expan-
sion and (iii) pairing. Table 3 presents an example
of the output of each step in the process.
The normalization step was carried out on the
original sentences (S0) to exclude or simplify in-
stances that contained lexical, syntactic or seman-
tic phenomena (e.g., named entities, dates, num-
bers, multiword expressions) that CDSMs are cur-
rently not expected to account for.
The expansion step was applied to each of the
normalized sentences (S1) in order to create up to
three new sentences with specific characteristics
suitable to CDSM evaluation. In this step syntac-
tic and lexical transformations with predictable ef-
fects were applied to each normalized sentence, in
order to obtain (i) a sentence with a similar mean-
ing (S2), (ii) a sentence with a logically contradic-
tory or at least highly contrasting meaning (S3),
and (iii) a sentence that contains most of the same
lexical items, but has a different meaning (S4) (this
last step was carried out only where it could yield
a meaningful sentence; as a result, not all normal-
ized sentences have an (S4) expansion).
Finally, in the pairing step each normalized
sentence in the pair was combined with all the
sentences resulting from the expansion phase and
with the other normalized sentence in the pair.
Considering the example in Table 3, S1a and S1b
were paired. Then, S1a and S1b were each com-
bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-
1
http://nlp.cs.illinois.edu/HockenmaierGroup/data.html
2
http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=data
2
Relatedness score Example
1.6
A: ?A man is jumping into an empty pool?
B: ?There is no biker jumping in the air?
2.9
A: ?Two children are lying in the snow and are making snow angels?
B: ?Two angels are making snow on the lying children?
3.6
A: ?The young boys are playing outdoors and the man is smiling nearby?
B: ?There is no boy playing outdoors and there is no man smiling?
4.9
A: ?A person in a black jacket is doing tricks on a motorbike?
B: ?A man in a black jacket is doing tricks on a motorbike?
Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).
Entailment label Example
ENTAILMENT
A: ?Two teams are competing in a football match?
B: ?Two groups of people are playing football?
CONTRADICTION
A: ?The brown horse is near a red barrel at the rodeo?
B: ?The brown horse is far from a red barrel at the rodeo?
NEUTRAL
A: ?A man in a black jacket is doing tricks on a motorbike?
B: ?A person is riding the bicycle on one wheel?
Table 2: Examples of sentence pairs with their gold entailment labels.
ing to a total of 13 different sentence pairs.
Furthermore, a number of pairs composed of
completely unrelated sentences were added to the
data set by randomly taking two sentences from
two different pairs.
The result is a set of about 10,000 new sen-
tence pairs, in which each sentence is contrasted
with either a (near) paraphrase, a contradictory or
strongly contrasting statement, another sentence
with very high lexical overlap but different mean-
ing, or a completely unrelated sentence. The ra-
tionale behind this approach was that of building
a data set which encouraged the use of a com-
positional semantics step in understanding when
two sentences have close meanings or entail each
other, hindering methods based on individual lex-
ical items, on the syntactic complexity of the two
sentences or on pure world knowledge.
3.2 Relatedness and Entailment Annotation
Each pair in the SICK dataset was annotated to
mark (i) the degree to which the two sentence
meanings are related (on a 5-point scale), and (ii)
whether one entails or contradicts the other (con-
sidering both directions). The ratings were col-
lected through a large crowdsourcing study, where
each pair was evaluated by 10 different subjects,
and the order of presentation of the sentences was
counterbalanced (i.e., 5 judgments were collected
for each presentation order). Swapping the order
of the sentences within each pair served a two-
fold purpose: (i) evaluating the entailment rela-
tion in both directions and (ii) controlling pos-
sible bias due to priming effects in the related-
ness task. Once all the annotations were collected,
the relatedness gold score was computed for each
pair as the average of the ten ratings assigned by
participants, whereas a majority vote scheme was
adopted for the entailment gold labels.
3.3 Data Set Statistics
For the purpose of the task, the data set was ran-
domly split into training and test set (50% and
50%), ensuring that each relatedness range and en-
tailment category was equally represented in both
sets. Table 4 shows the distribution of sentence
pairs considering the combination of relatedness
ranges and entailment labels. The ?total? column
3
Original pair
S0a: A sea turtle is hunting for fish S0b: The turtle followed the fish
Normalized pair
S1a: A sea turtle is hunting for fish S1b: The turtle is following the fish
Expanded pairs
S2a: A sea turtle is hunting for food S2b: The turtle is following the red fish
S3a: A sea turtle is not hunting for fish S3b: The turtle isn?t following the fish
S4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtle
Table 3: Data set creation process.
indicates the total number of pairs in each range
of relatedness, while the ?total? row contains the
total number of pairs in each entailment class.
SICK Training Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 0 (0%) 471 (10%) 471
2-3 range 59 (1%) 2 (0%) 638 (13%) 699
3-4 range 498 (10%) 71 (1%) 1344 (27%) 1913
4-5 range 155 (3%) 1344 (27%) 352 (7%) 1851
TOTAL 712 1417 2805 4934
SICK Test Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 1 (0%) 451 (9%) 452
2-3 range 59 (1%) 0 (0%) 615(13%) 674
3-4 range 496 (10%) 65 (1%) 1398 (28%) 1959
4-5 range 157 (3%) 1338 (27%) 326 (7%) 1821
TOTAL 712 1404 2790 4906
Table 4: Distribution of sentence pairs across the
Training and Test Sets.
4 Evaluation Metrics and Baselines
Both subtasks were evaluated using standard met-
rics. In particular, the results on entailment were
evaluated using accuracy, whereas the outputs on
relatedness were evaluated using Pearson correla-
tion, Spearman correlation, and Mean Squared Er-
ror (MSE). Pearson correlation was chosen as the
official measure to rank the participating systems.
Table 5 presents the performance of 4 base-
lines. The Majority baseline always assigns
the most common label in the training data
(NEUTRAL), whereas the Probability baseline
assigns labels randomly according to their rela-
tive frequency in the training set. The Overlap
baseline measures word overlap, again with
parameters (number of stop words and EN-
TAILMENT/NEUTRAL/CONTRADICTION
thresholds) estimated on the training part of the
data.
Baseline Relatedness Entailment
Chance 0 33.3%
Majority NA 56.7%
Probability NA 41.8%
Overlap 0.63 56.2%
Table 5: Performance of baselines. Figure of merit
is Pearson correlation for relatedness and accuracy
for entailment. NA = Not Applicable
5 Submitted Runs and Results
Overall, 21 teams participated in the task. Partici-
pants were allowed to submit up to 5 runs for each
subtask and had to choose the primary run to be in-
cluded in the comparative evaluation. We received
17 submissions to the relatedness subtask (for a
total of 66 runs) and 18 for the entailment subtask
(65 runs).
We asked participants to pre-specify a pri-
mary run to encourage commitment to a
theoretically-motivated approach, rather than
post-hoc performance-based assessment. Inter-
estingly, some participants used the non-primary
runs to explore the performance one could reach
by exploiting weaknesses in the data that are not
likely to hold in future tasks of the same kind
(for instance, run 3 submitted by The Meaning
Factory exploited sentence ID ordering informa-
tion, but it was not presented as a primary run).
Participants could also use non-primary runs to
test smart baselines. In the relatedness subtask
six non-primary runs slightly outperformed the
official winning primary entry,
3
while in the
entailment task all ECNU?s runs but run 4 were
better than ECNU?s primary run. Interestingly,
the differences between the ECNU?s runs were
3
They were: The Meaning Factory?s run3 (Pearson
0.84170) ECNU?s runs2 (0.83893) run5 (0.83500) and Stan-
fordNLP?s run4 (0.83462) and run2 (0.83103).
4
due to the learning methods used.
We present the results achieved by primary runs
against the Entailment and Relatedness subtasks in
Table 6 and Table 7, respectively.
4
We witnessed
a very close finish in both subtasks, with 4 more
systems within 3 percentage points of the winner
in both cases. 4 of these 5 top systems were the
same across the two subtasks. Most systems per-
formed well above the best baselines from Table
5.
The overall performance pattern suggests that,
owing perhaps to the more controlled nature of
the sentences, as well as to the purely linguistic
nature of the challenges it presents, SICK entail-
ment is ?easier? than RTE. Considering the first
five RTE challenges (Bentivogli et al., 2009), the
median values ranged from 56.20% to 61.75%,
whereas the average values ranged from 56.45%
to 61.97%. The entailment scores obtained on
the SICK data set are considerably higher, being
77.06% for the median system and 75.36% for
the average system. On the other hand, the re-
latedness task is more challenging than the one
run on MSRvid (one of our data sources) at STS
2012, where the top Pearson correlation was 0.88
(Agirre et al., 2012).
6 Approaches
A summary of the approaches used by the sys-
tems to address the task is presented in Table 8.
In the table, systems in bold are those for which
the authors submitted a paper (Ferrone and Zan-
zotto, 2014; Bjerva et al., 2014; Beltagy et al.,
2014; Lai and Hockenmaier, 2014; Alves et al.,
2014; Le?on et al., 2014; Bestgen, 2014; Zhao et
al., 2014; Vo et al., 2014; Bic?ici and Way, 2014;
Lien and Kouylekov, 2014; Jimenez et al., 2014;
Proisl and Evert, 2014; Gupta et al., 2014). For the
others, we used the brief description sent with the
system?s results, double-checking the information
with the authors. In the table, ?E? and ?R? refer
to the entailment and relatedness task respectively,
and ?B? to both.
Almost all systems combine several kinds of
features. To highlight the role played by com-
position, we draw a distinction between compo-
sitional and non-compositional features, and di-
vide the former into ?fully compositional? (sys-
4
ITTK?s primary run could not be evaluated due to tech-
nical problems with the submission. The best ITTK?s non-
primary run scored 78,2% accuracy in the entailment task and
0.76 r in the relatedness task.
ID Compose ACCURACY
Illinois-LH run1 P/S 84.6
ECNU run1 S 83.6
UNAL-NLP run1 83.1
SemantiKLUE run1 82.3
The Meaning Factory run1 S 81.6
CECL ALL run1 80.0
BUAP run1 P 79.7
UoW run1 78.5
Uedinburgh run1 S 77.1
UIO-Lien run1 77.0
FBK-TR run3 P 75.4
StanfordNLP run5 S 74.5
UTexas run1 P/S 73.2
Yamraj run1 70.7
asjai run5 S 69.8
haLF run2 S 69.4
RTM-DCU run1 67.2
UANLPCourse run2 S 48.7
Table 6: Primary run results for the entailment
subtask. The table also shows whether a sys-
tem exploits composition information at either the
phrase (P) or sentence (S) level.
tems that compositionally computed the meaning
of the full sentences, though not necessarily by as-
signing meanings to intermediate syntactic con-
stituents) and ?partially compositional? (systems
that stop the composition at the level of phrases).
As the table shows, thirteen systems used compo-
sition in at least one of the tasks; ten used compo-
sition for full sentences and six for phrases, only.
The best systems are among these thirteen sys-
tems.
Let us focus on such compositional methods.
Concerning the relatedness task, the fine-grained
analyses reported for several systems (Illinois-
LH, The Meaning Factory and ECNU) shows that
purely compositional systems currently reach per-
formance above 0.7 r. In particular, ECNU?s
compositional feature gives 0.75 r, The Meaning
Factory?s logic-based composition model 0.73 r,
and Illinois-LH compositional features combined
with Word Overlap 0.75 r. While competitive,
these scores are lower than the one of the best
5
ID Compose r ? MSE
ECNU run1 S 0.828 0.769 0.325
StanfordNLP run5 S 0.827 0.756 0.323
The Meaning Factory run1 S 0.827 0.772 0.322
UNAL-NLP run1 0.804 0.746 0.359
Illinois-LH run1 P/S 0.799 0.754 0.369
CECL ALL run1 0.780 0.732 0.398
SemantiKLUE run1 0.780 0.736 0.403
RTM-DCU run1 0.764 0.688 0.429
UTexas run1 P/S 0.714 0.674 0.499
UoW run1 0.711 0.679 0.511
FBK-TR run3 P 0.709 0.644 0.591
BUAP run1 P 0.697 0.645 0.528
UANLPCourse run2 S 0.693 0.603 0.542
UQeResearch run1 0.642 0.626 0.822
ASAP run1 P 0.628 0.597 0.662
Yamraj run1 0.535 0.536 2.665
asjai run5 S 0.479 0.461 1.104
Table 7: Primary run results for the relatedness
subtask (r for Pearson and ? for Spearman corre-
lation). The table also shows whether a system ex-
ploits composition information at either the phrase
(P) or sentence (S) level.
purely non-compositional system (UNAL-NLP)
which reaches the 4th position (0.80 r UNAL-NLP
vs. 0.82 r obtained by the best system). UNAL-
NLP however exploits an ad-hoc ?negation? fea-
ture discussed below.
In the entailment task, the best non-
compositional model (again UNAL-NLP)
reaches the 3rd position, within close reach of the
best system (83% UNAL-NLP vs. 84.5% obtained
by the best system). Again, purely compositional
models have lower performance. haLF CDSM
reaches 69.42% accuracy, Illinois-LH Word
Overlap combined with a compositional feature
reaches 71.8%. The fine-grained analysis reported
by Illinois-LH (Lai and Hockenmaier, 2014)
shows that a full compositional system (based
on point-wise multiplication) fails to capture
contradiction. It is better than partial phrase-based
compositional models in recognizing entailment
pairs, but worse than them on recognizing neutral
pairs.
Given our more general interest in the distri-
butional approaches, in Table 8 we also classify
the different DSMs used as ?Vector Space Mod-
els?, ?Topic Models? and ?Neural Language Mod-
els?. Due to the impact shown by learning methods
(see ECNU?s results), we also report the different
learning approaches used.
Several participating systems deliberately ex-
ploit ad-hoc features that, while not helping a true
understanding of sentence meaning, exploit some
systematic characteristics of SICK that should be
controlled for in future releases of the data set.
In particular, the Textual Entailment subtask has
been shown to rely too much on negative words
and antonyms. The Illinois-LH team reports that,
just by checking the presence of negative words
(the Negation Feature in the table), one can detect
86.4% of the contradiction pairs, and by combin-
ing Word Overlap and antonyms one can detect
83.6% of neutral pairs and 82.6% of entailment
pairs. This approach, however, is obviously very
brittle (it would not have been successful, for in-
stance, if negation had been optionally combined
with word-rearranging in the creation of S4 sen-
tences, see Section 3.1 above).
Finally, Table 8 reports about the use of external
resources in the task. One of the reasons we cre-
ated SICK was to have a compositional semantics
benchmark that would not require too many ex-
ternal tools and resources (e.g., named-entity rec-
ognizers, gazetteers, ontologies). By looking at
what the participants chose to use, we think we
succeeded, as only standard NLP pre-processing
tools (tokenizers, PoS taggers and parsers) and rel-
atively few knowledge resources (mostly, Word-
Net and paraphrase corpora) were used.
7 Conclusion
We presented the results of the first task on the
evaluation of compositional distributional seman-
tic models and other semantic systems on full sen-
tences, organized within SemEval-2014. Two sub-
tasks were offered: (i) predicting the degree of re-
latedness between two sentences, and (ii) detect-
ing the entailment relation holding between them.
The task has raised noticeable attention in the
community: 17 and 18 submissions for the relat-
edness and entailment subtasks, respectively, for a
total of 21 participating teams. Participation was
not limited to compositional models but the major-
ity of systems (13/21) used composition in at least
one of the subtasks. Moreover, the top-ranking
systems in both tasks use compositional features.
However, it must be noted that all systems also ex-
6
Participant ID Non composition features Comp features Learning Methods External Resources 
Ve
cto
r S
em
an
tic
s M
od
el 
To
pic
 M
od
el 
Ne
ura
l L
an
gu
ag
e M
od
el 
De
no
tat
ion
al 
Mo
de
l 
Wo
rd 
Ov
erl
ap
 
Wo
rd 
Sim
ila
rit
y 
Sy
nta
cti
c F
ea
tur
es
 
Se
nte
nc
e d
iffe
ren
ce
 
Ne
ga
tio
n F
ea
tur
es
 
Se
nte
nc
e C
om
po
sit
ion
 
Ph
ras
e c
om
po
sit
ion
  
SV
M 
an
d K
ern
el 
me
tho
ds
 
K-
Ne
are
st 
Ne
igh
bo
urs
 
Cla
ssi
fie
r C
om
bin
ati
on
 
Ra
nd
om
 Fo
res
t 
Fo
L/P
rob
ab
ilis
tic
 Fo
L 
Cu
rri
cu
lum
 ba
se
d l
ea
rni
ng
 
Ot
he
r 
Wo
rdN
et 
Pa
rap
hra
se
s D
B 
Ot
he
r C
orp
ora
 
Im
ag
eF
lick
er 
 ST
S M
SR
-V
ide
o 
De
scr
ipt
ion
 
ASAP R R R R R R R R R 
ASJAI B B B B B B B B E B R B 
BUAP B B B B E B E B 
UEdinburgh B B B B B E R B 
CECL B B B B B B 
ECNU B B B B B B B B B B B B B 
FBK-TR R R R E B E E B R E R R E 
haLF E E E E 
IITK B B B B B B B B B 
Illinois-LH B B B B B B B B B B B B 
RTM-DCU B B B B B 
SemantiKLUE B B B B B B B B 
StandfordNLP B B R R R B E 
The Meaning Factory R R R R R R B E R E B B R 
UANLPCourse B B B B B 
UIO-Lien E E 
UNAL-NLP B B B B R B B 
UoW B B B B B B 
UQeRsearch R R R R R R R 
UTexas B B B B B B B 
Yamarj B B B B 
Table 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)
or B(oth)
ploit non-compositional features and most of them
use external resources, especially WordNet. Al-
most all the participating systems outperformed
the proposed baselines in both tasks. Further anal-
yses carried out by some participants in the task
show that purely compositional approaches reach
accuracy above 70% in entailment and 0.70 r for
relatedness. These scores are comparable with the
average results obtained in the task.
Acknowledgments
We thank the creators of the ImageFlickr, MSR-
Video, and SemEval-2012 STS data sets for grant-
ing us permission to use their data for the task. The
University of Trento authors were supported by
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), volume 2.
Ana O. Alves, Adirana Ferrugento, Mariana Lorenc?o,
and Filipe Rodrigues. 2014. ASAP: Automatica se-
mantic alignment for phrases. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymon J. Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probablisitc logic. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
7
Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In The Text Analysis Conference (TAC 2009).
Yves Bestgen. 2014. CECL: a new baseline and a non-
compositional approach for the Sick benchmark. In
Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similar-
ity. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal Semantics for Recognizing Textual Entailment
and Determining Semantic Similarity. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising textual entailment, pages 177?
190. Springer.
Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
haLF:comparing a pure CDSM approach and a stan-
dard ML system for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Rohit Gupta, Ismail El Maarouf Hannah Bechara, and
Costantin Oras?an. 2014. UoW: NLP techniques de-
veloped at the University of Wolverhampton for Se-
mantic Similarity and Textual Entailment. In Pro-
ceedings of SemEval 2014: International Workshop
on Semantic Evaluation.
Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of SemEval 2014: International Workshop on Se-
mantic Evaluation.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Sa?ul Le?on, Darnes Vilarino, David Pinto, Mireya To-
var, and Beatrice Beltr?an. 2014. BUAP:evaluating
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of SemEval 2014: Inter-
national Workshop on Semantic Evaluation.
Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Thomas Proisl and Stefan Evert. 2014. SemantiK-
LUE: Robust semantic similarity at multiple levels
using maximum weight matching. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
An N. P. Vo, Octavian Popescu, and Tommaso Caselli.
2014. FBK-TR: SVM for Semantic Relatedness and
Corpus Patterns for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One Stone Two Birds: Ensemble of Het-
erogenous Measures for Semantic Relatedness and
Textual Entailment. In Proceedings of SemEval
2014: International Workshop on Semantic Evalu-
ation.
8
EACL-2006   11th Conference  of the European Chapter of the  Association for Computational Linguistics   Proceedings of the 2nd International Workshop on   Web as Corpus    Chairs: Adam Kilgarriff Marco Baroni        April 2006 Trento, Italy 
The conference, the workshop and the tutorials are sponsored by: 
 
 
 
 
 
 
Celct 
c/o BIC, Via dei Solteri, 38 
38100 Trento, Italy 
http://www.celct.it 
 
 
 
   
 
Xerox Research Centre Europe 
6 Chemin de Maupertuis 
38240 Meylan, France 
http://www.xrce.xerox.com 
 
 
 
 
  
Thales 
45 rue de Villiers 
92526 Neuilly-sur-Seine Cedex, France 
http://www.thalesgroup.com 
 
 
EACL-2006  is supported by 
Trentino S.p.a.   and Metalsistem Group  
 
 
 
 
 
? April 2006, Association for Computational Linguistics 
 
Order copies of ACL proceedings from:  
Priscilla Rasmussen,  
Association for Computational Linguistics (ACL),  
3 Landmark Center, 
East Stroudsburg, PA 18301  USA 
 
Phone  +1-570-476-8006 
Fax  +1-570-476-0860 
E-mail:  acl@aclweb.org  
On-line order form:  http://www.aclweb.org/ 
 
 
 
 
 
 
 
  
 
  
CELI s.r.l.  
Corso Moncalieri, 21 
10131 Torino, Italy 
http://www.celi.it 
WAC2: Programme 
 
9.00-9.30 Marco Baroni and Adam Kilgarriff 
Introduction 
 
9.30-10.00 Andr?s Kornai, P?ter Hal?csy, Viktor Nagy, Csaba Oravecz, Viktor Tr?n and 
D?niel Varga 
   Web-based frequency dictionaries for medium density languages 
 
10.00-10.30 Mike Cafarella and Oren Etzioni 
  BE: a search engine for NLP research 
 
 Break 
 
11.00-11.30 Masatsugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro Sasaki, 
Takehito Utsuro and Satoshi Sato 
A comparative study on compositional translation estimation using a 
domain/topic-specific corpus collected from the web 
 
11.30-12.00 Gemma Boleda, Stefan Bott, Rodrigo Meza, Carlos Castillo, Toni Badia and 
Vicente L?pez 
   CUCWeb: a Catalan corpus built from the web 
 
12.00-12.30 Paul Rayson, James Walkerdine, William H. Fletcher and Adam Kilgarriff 
  Annotated web as corpus 
 
 Lunch 
 
2.30-3.00 Arno Scharl and Albert Weichselbraun 
  Web coverage of the 2004 US presidential election 
 
3.00-3.30 C?drick Fairon 
  Corporator: A tool for creating RSS-based specialized corpora 
 
3.30-4.00 Demos, part 1 
 
 Break 
 
4.30-4.50 Demos, part 2 
 
4.50-5.20 Davide Fossati, Gabriele Ghidoni, Barbara Di Eugenio, Isabel Cruz, Huiyong 
Xiao and Rajen Subba 
   The problem of ontology alignment on the web: a first report 
 
5.20-5.50 Kie Zuraw 
  Using the web as a phonological corpus: a case study from Tagalog 
 
5.50-6.00 Organization, next meeting, closing 
 
Reserve paper 
 
R?diger Gleim, Alexander Mehler and Matthias Dehmer  
  Web corpus mining by instance of Wikipedia 
 
  
 
 
 
iii
Programme Committee 
 
Toni Badia 
Marco Baroni (co-chair) 
Silvia Bernardini 
Massimiliano Ciaramita 
Barbara Di Eugenio 
Roger Evans 
Stefan Evert 
William Fletcher 
R?diger Gleim 
Gregory Grefenstette 
P?ter Hal?csy 
Frank Keller 
Adam Kilgarriff (co-chair) 
Rob Koeling 
Mirella Lapata 
Anke L?deling 
Alexander Mehler 
Drago Radev 
Philip Resnik 
German Rigau 
Serge Sharoff 
David Weir 
 
 
iv
Preface 
 
What is the role of a workshop series on web as corpus? 
 
We argue, first, that attention to the web is critical to the health of non-corporate NLP, since 
the academic community runs the risk of being sidelined by corporate NLP if it does not 
address the issues involved in using very-large-scale web resources; second, that text type 
comes to the fore when we study the web, and the workshops provide a venue for nurturing 
this under-explored dimension of language; and thirdly that the WWW community is an 
important academic neighbour for CL, and the workshops will contribute to contact between 
CL and WWW. 
 
High-performance NLP needs web-scale resources 
 
The most talked-about presentation of the ACL 2005 was Franz-Josef Och?s, in which he 
presented statistical MT results based on a 200 billion word English corpus.  His results led 
the field.  He was in a privileged position to have access to a corpus of that size.  He works at 
Google.   
 
With enormous data, you get better results. (See e.g. Banko and Brill 2001.)  It seems to us 
there are two possible responses for the academic NLP community.  The first is to accept 
defeat: ?we will never have resources on the scale Google has, so we should accept that our 
systems will not really compete, that they will be proofs-of-concept or deal with niche 
problems, but will be out of the mainstream of high-performance HLT system development.? 
The second is to say: we too need to make resources on this scale available, and they should 
be available to researchers in universities as well as behind corporate firewalls: and we can do 
it, because resources of the right scale are available, for free, on the web.  We shall of course 
have to acquire new expertise along the way ? at, inter alia, WAC workshops. 
 
Text type 
 
The most interesting question that the use of web corpora raises is text type.  (We use ?text 
type? as a cover-all term to include domain, genre, style etc.)  The first question about web 
corpora from an outsider is usually ?how do you know that your web corpus is 
representative?? to which the fitting response is ?how do you know whether any corpus is 
representative (of what?)?.   These questions will only receive satisfactory answers when we 
have a fuller account of how to identify and distinguish different kinds of text. 
 
While text type is not centre-stage in this volume, we suspect it will be prominent in 
discussions at the workshop and will be the focus of papers in future workshops. 
 
The WWW community: links, web-as-graph, and linguistics 
 
One of CL?s academic neighbours is the WWW community (as represented by, eg, the 
WWW conference series).  Many of their key questions concern the nature of the web, 
viewing it as a large set of domains, or as a graph, or as a bag of bags of words.  The web is 
substantially a linguistic object, and there is potential for these views of the web contributing 
to our linguistic understanding. For example, the graph structure of the web has been used to 
identify highly connected areas which are ?web communities?.  How does that graph-
theoretical connectedness relate to the linguistic properties one would associate with a 
discourse community?  To date the links between the communities have been not been strong.  
(Few WWW papers are referenced in CL papers, and vice versa.)  The workshops will 
provide a venue where WWW and CL interests intersect. 
 
v
Recent work by co-chairs and colleagues 
 
At risk of abusing chairs? privilege, we briefly mention two pieces of our own work.  In the 
first we have created web corpora of over 1 billion words for German and Italian.  The text 
has been de-duplicated, passed through a range of filters, part-of-speech tagged, lemmatized, 
and loaded into a web-accessible corpus query tool supporting a wide range of linguists? 
queries.  It offers one model of how to use the web as a corpus. The corpora will be 
demonstrated in the main EACL conference (Baroni and Kilgarriff 2006).   
 
In the second, WebBootCaT (work with Jan Pomikalek and Pavel Rychl? of Masaryk 
University, Brno), we have prepared a version of the BootCaT tools (Baroni and Bernardini 
2004) as a web service. Users fill in a web form with the target language and some ?seed 
terms? to specify the domain of the target corpus, and press the ?Build Corpus? button.  A 
corpus is built.  Thus, people without any programming or software-installation skills can 
create corpora to their own specification.  The system will be demonstrated in the ?demos? 
session of the workshop.   
 
The workshop series to date 
 
This is the second international workshop, the first being held in July 2005 in Birmingham, 
UK (in association with Corpus Linguistics 2005).  There was an earlier Italian event in Forl?, 
in January 2005.   All three have attracted high levels of interest. The papers in this volume 
were selected following a highly competitive review process, and we would like to thank all 
those who submitted, all those on the programme committee who contributed to the review 
process, and the additional reviewers who helped us to get through the large number of 
submissions. Special thanks to Stefan Evert for help with assembling the proceedings. 
(Cafarella and Etzioni have an abstract rather than a full paper to avoid duplicate publication: 
we felt their presentation would make an important contribution to the workshop, which was a 
distinct issue to them not having a new text available.)  
 
We are confident that there will be much of interest for anyone engaged with NLP and the 
web. 
 
References 
 
Banko, M. and E. Brill. 2001. ?Mitigating the Paucity-of-Data Problem: Exploring the Effect of 
Training Corpus Size on Classifier Performance for Natural Language Processing.?  In Proc. 
Human Language Technology Conference (HLT 2001) 
Baroni, M and S. Bernardini 2004. BootCaT: Bootstrapping corpora and terms from the web. Proc. 
LREC 2004, Lisbon: ELDA. 1313-1316.  
Baroni, M. and A. Kilgarriff 2006.  ?Large linguistically-processed web corpora for multiple 
languages.? Proc EACL, Trento, Italy. 
M?rquez, L. and D. Klein 2006. Announcement and Call for Papers for the Tenth Conference on 
Computational Natural Language Learning. http://www.cnts.ua.ac.be/conll/cfp.html  
Och, F-J. 2005. ?Statistical Machine Translation: The Fabulous Present and Future? Invited talk at 
ACL Workshop on Building and Using Parallel Texts, Ann Arbor. 
 
 
 
Adam Kilgarriff and Marco Baroni, February 2006 
vi
Table of Contents
Web-based frequency dictionaries for medium density languages
Andra?s Kornai, Pe?ter Hala?csy, Viktor Nagy, Csaba Oravecz, Viktor Tro?n and Da?niel Varga . . . . . . . . . . . . . 1
BE: A search engine for NLP research
Mike Cafarella and Oren Etzioni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
A comparative study on compositional translation estimation using a domain/topic-specific corpus collected from
the Web
Masatsugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro Sasaki, Takehito Utsuro and S. Sato . . .11
CUCWeb: A Catalan corpus built from the Web
Gemma Boleda, Stefan Bott, Rodrigo Meza, Carlos Castillo, Toni Badia and Vicente Lo?pez . . . . . . . . . . . 19
Annotated Web as corpus
Paul Rayson, James Walkerdine, William H. Fletcher and Adam Kilgarriff . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Web coverage of the 2004 US Presidential election
Arno Scharl and Albert Weichselbraun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Corporator: A tool for creating RSS-based specialized corpora
Ce?drick Fairon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
The problem of ontology alignment on the Web: A first report
Davide Fossati, Gabriele Ghidoni, Barbara Di Eugenio, Isabel Cruz, Huiyong Xiao and Rajen Subba . . . 51
Using the Web as a phonological corpus: A case study from Tagalog
Kie Zuraw. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59
Web corpus mining by instance of Wikipedia
Ru?diger Gleim, Alexander Mehler and Matthias Dehmer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
vii
 viii
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 94?101
Manchester, August 2008
Cognitively Salient Relations
for Multilingual Lexicography
Gerhard Kremer
CIMeC
University of Trento
gerhard.kremer@unitn.it
Andrea Abel
EURAC
Bolzano
aabel@eurac.edu
Marco Baroni
CIMeC
University of Trento
marco.baroni@unitn.it
Abstract
Providing sets of semantically related
words in the lexical entries of an electronic
dictionary should help language learners
quickly understand the meaning of the tar-
get words. Relational information might
also improve memorisation, by allowing
the generation of structured vocabulary
study lists. However, an open issue is
which semantic relations are cognitively
most salient, and should therefore be used
for dictionary construction. In this paper,
we present a concept description elicita-
tion experiment conducted with German
and Italian speakers. The analysis of the
experimental data suggests that there is a
small set of concept-class?dependent rela-
tion types that are stable across languages
and robust enough to allow discrimination
across broad concept domains. Our further
research will focus on harvesting instantia-
tions of these classes from corpora.
1 Introduction
In electronic dictionaries, lexical entries can be
enriched with hyperlinks to semantically related
words. In particular, we focus here on those re-
lated words that can be seen as systematic prop-
erties of the target entry, i. e., the basic concepts
that would be used to define the entry in relation to
its superordinate category and coordinate concepts.
So, for example, for animals the most salient rela-
tions would be notions such as ?parts? and ?typical
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
behaviour?. For a horse, salient properties will in-
clude the mane and hooves as parts, and neighing
as behaviour.
Sets of relevant and salient properties allow the
user to collocate a word within its so-called ?word
field? and to distinguish it more clearly from neigh-
bour concepts, since the meaning of a word is
not defined in isolation, but in contrast to related
words in its word field (Geckeler, 2002). More-
over, knowing the typical relations of concepts in
different domains might help pedagogical lexicog-
raphy to produce structured networks where, from
each word, the learner can naturally access entries
for other words that represent properties which are
salient and distinctive for the target concept class
(parts of animals, functions of tools, etc.). We
envisage a natural application of this in the au-
tomated creation of structured vocabulary study
lists. Finally, this knowledge might be used as
a basis to populate lexical networks by building
models of concepts in terms of ?relation sketches?
based on salient typed properties (when an animal
is added to our lexicon, we know that we will have
to search a corpus to extract its parts, behaviour,
etc., whereas for a tool the function would be the
most important property to mine).
This paper provides a first step in the direction of
dictionaries enriched with cognitively salient prop-
erty descriptions by eliciting concept descriptions
from subjects speaking different languages, and
analysing the general patterns emerging from these
data.
It is worth distinguishing our approach to enrich-
ing connections in a lexical resource from the one
based on free association, such as has been recently
pursued, e. g., within the WordNet project (Boyd-
Graber et al, 2006). While we do not dispute the
usefulness of free associates, they are irrelevant to
94
our purposes, since we want to generate system-
atic, structured descriptions of concepts, in terms
of the relation types that are most salient for their
semantic fields. Knowing that the word Holland
is ?evoked? by the word tulip might be useful for
other reasons, but it does not allow us to harvest
systematic properties of flowers in order to popu-
late their relation sketch: we rather want to find
out that tulips, being flowers, will have colour as
a salient property type. As a location property of
tulips, we would prefer something like garden in-
stead of the name of a country or individual asso-
ciations. To minimise free association, we asked
participants in our experiments to produce concept
descriptions in terms of characteristic properties
of the target concepts (although we are not aware
of systematic studies comparing free associates to
concept description tasks, the latter methodology
is fairly standard in cognitive science: see sec-
tion 2.2 below).
To our knowledge, this sort of approach has
not been proposed in lexicography, yet. Cognitive
scientists focus on ?concepts?, glossing over the
fact that what subjects will produce are (strings
of) words, and as such they will be, at least to
a certain extent, language-dependent. For lexico-
graphic applications, this aspect cannot, of course,
be ignored, in particular if the goal is to produce
lexical entries for language learners (so that both
their first and their second languages should be
taken into account).
We face this issue directly in the elicitation ex-
periment we present here, in which salient rela-
tions for a set of 50 concepts from 10 different
categories are collected from comparable groups
of German and Italian speakers. In particular, we
collected data from high school students in South
Tyrol, a region situated in Northern Italy, inhabited
by both German and Italian speakers. Both Ger-
man and Italian schools exist, where the respective
non-native language is taught. It is important to
stress that the two communities are relatively sep-
arated, and most speakers are not from bilingual
families or bilingual social environments: They
study the other language as an intensively taught
L2 in school. Thus, we move in an ideal sce-
nario to test possible language-driven differences
in property descriptions, among speakers that have
a very similar cultural background.
South Tyrol also provides the concrete applica-
tive goal of our project. In public administration
and service, employees need to master both lan-
guages up to a certain standardised level (they have
to pass a ?bilingual? proficiency exam). Therefore,
there is a big need for language learning materi-
als. The practical outcome of our research will be
an extension of ELDIT1, an electronic learner?s dic-
tionary for German and Italian (Abel and Weber,
2000).
2 Related Work
Lexicographic projects providing semantic rela-
tions and experimental research on property gen-
eration are the basis for our research.
2.1 Dictionaries
In most paper-based general and learners? dictio-
naries only some information about synonyms and
sometimes antonyms is presented. Newer dictio-
naries, such as the ?Longman Language Activa-
tor? (Summers, 1999), are providing lists of related
words. While these will be useful to learners, infor-
mation about the kind of semantic relation is usu-
ally missing.
Semantic relations are often available in elec-
tronic resources, most famously in WordNet (Fell-
baum, 1998) and related projects like Kirrkirr
(Jansz et al, 1999), ALEXIA (Chanier and Selva,
1998), or as described in Fontenelle (1997). How-
ever, these resources tend to include few relation
types (hypernymy, meronymy, antonymy, etc.).
The salience of the relations chosen is not veri-
fied experimentally, and the same set of relation
types is used for all words that share the same part-
of-speech. Our results below, as well as work by
Vinson et al (2008), indicate that different concept
classes should, instead, be characterised by differ-
ent relation types (e. g., function is very salient for
tools, but not at all for animals).
2.2 Work in Cognitive Sciences
Several projects addressed the collection of prop-
erty generation data to provide the community
with feature norms to be used in different psy-
cholinguistic experiments and other analyses: Gar-
rard et al (2001) instructed subjects to complete
phrases (?concept is/has/can. . . ?), thus restricting
the set of producible feature types. McRae et
al. (2005) instructed their subjects to list concept
properties without such restrictions, but providing
them with some examples. Vinson et al (2008)
1URL http://www.eurac.edu/eldit
95
gave similar instructions, but explicitly asked sub-
jects not to freely associate.
However, these norms have been collected for
the English language. It remains to be explored
if concept representations in general and seman-
tic relations for our specific investigations have the
same properties across languages.
3 Data Collection
After choosing the concept classes and appropri-
ate concepts for the production experiment, con-
cept descriptions were collected from participants.
These were transcribed, normalised, and annotated
with semantic relation types.
3.1 Stimuli
The stimuli for the experiment consisted of 50 con-
crete concepts from 10 different classes (i. e., 5
concepts for each of the classes): mammal (dog,
horse, rabbit, bear, monkey), bird (seagull, spar-
row, woodpecker, owl, goose), fruit (apple, orange,
pear, pineapple, cherry), vegetable (corn, onion,
spinach, peas, potato), body part (eye, finger, head,
leg, hand), clothing (chemise, jacket, sweater,
shoes, socks), manipulable tool (comb, broom,
sword, paintbrush, tongs), vehicle (bus, ship, air-
plane, train, truck), furniture (table, bed, chair,
closet, armchair), and building (garage, bridge,
skyscraper, church, tower). They were mainly
taken from Garrard et al (2001) and McRae et
al. (2005). The concepts were chosen so that they
had unambiguous, reasonably monosemic lexical
realizations in both target languages.
The words representing these concepts were
translated into the two target languages, German
and Italian. A statistical analysis (using Tukey?s
honestly significant difference test as implemented
in the R toolkit2) of word length distributions
(within and across categories) showed no signif-
icant differences in either language. There were
instead significant differences in the frequency of
target words, as collected from the German, Italian
and English WaCky corpora3. In particular, words
of the class body part had significantly larger fre-
quencies across languages than the words of the
other classes (not surprisingly, the words eye, head
and hand appear much more often in corpora than
the other words in the stimuli list).
2URL http://www.r-project.org/
3URL http://wacky.sslmit.unibo.it/
3.2 Experimental Procedure
The participants in the concept description exper-
iment were students attending the last 3 years of
a German or Italian high school and reported to
be native speakers of the respective languages. 73
German and 69 Italian students participated in the
experiment, with ages ranging between 15 and 19.
The average age was 16.7 (standard deviation 0.92)
for Germans and 16.8 (s.d. 0.70) for Italians.
The experiment was conducted group-wise in
schools. Each participant was provided with a ran-
dom set of 25 concepts, each presented on a sep-
arate sheet of paper. To have an equal number of
participants describing each concept, for each ran-
domly matched subject pair the whole set of con-
cepts was randomised and divided into 2 subsets.
Each subject saw the target stimuli in his/her sub-
set in a different random order (due to technical
problems, the split was not always different across
subject pairs).
Short instructions were provided orally before
the experiment, and repeated in written format on
the front cover of the questionnaire booklet dis-
tributed to each subject. To make the concept de-
scription task more natural, we suggested that par-
ticipants should imagine a group of alien visitors,
to each of which a particular word for a concrete
object was unknown and thus had to be described.
Participants should assume that each alien visitor
knew all other words of the language apart from
the unknown (target) word.
Participants were asked to enter a descriptive
phrase per line (not necessarily a whole sentence)
and to try and write at least 4 phrases per word.
They were given a maximum of one minute per
concept, and they were not allowed to go back to
the previous pages.
Before the real experiment, subjects were pre-
sented an example concept (not in the target list)
and were encouraged to describe it while asking
clarifications about the task.
All subjects returned the questionnaire so that
for a concept we obtained, on average, descriptions
by 36.48 German subjects (s.d. 1.24) and 34.34
Italian subjects (s.d. 1.72).
3.3 Transcription and Normalisation
The collected data were digitally transcribed and
responses were manually checked to make sure
that phrases denoting different properties had been
properly split. We tried to systematically apply the
96
criterion that, if at least one participant produced
2 properties on separate lines, then the properties
would always be split in the rest of the data set.
However, this approach was not always equally
applicable in both languages. For example, Trans-
portmittel (German) and mezzo di trasporto (Ital-
ian) both are compounds used as hypernyms for
what English speakers would probably rather clas-
sify as vehicles. In contrast to Transportmittel,
mezzo di trasporto is splittable as mezzo, that can
also be used on its own to refer to a kind of vehi-
cle (and is defined more specifically by adding the
fact that it is used for transportation). The German
compound word also refers to the function of trans-
portation, but -mittel has a rather general meaning,
and would not be used alone to refer to a vehicle.
Hence, Transportmittel was kept as a whole and
the Italian quasi-equivalent was split, possibly cre-
ating a bias between the two data sets (if the Italian
string is split into mezzo and trasporto, these will
be later classified as hypernym and functional fea-
tures, respectively; if the German word is not split,
it will only receive one of these type labels). More
in general, note that in German compounds are
written as single orthographic words, whereas in
Italian the equivalent concepts are often expressed
by several words. This could also create further
bias in the data annotation and hence in the analy-
sis.
Data were then normalised and transcribed into
English, before annotating the type of semantic re-
lation. Normalisation was done in accordance with
McRae et al (2005), using their feature norms as
guidelines, and it included leaving habitual words
like ?normally,?, ?often?, ?most? etc. out, as they
just express the typicality of the concept descrip-
tion, which is the implicit task.
3.4 Mapping to Relation Types
Normalised and translated phrases were sub-
sequently labelled for relation types following
McRae et al?s criteria and using a subset of the se-
mantic relation types described in Wu and Barsa-
lou (2004): see section 4.1 below for the list of
relations used in the current analysis.
Trying to adapt the annotation style to that of
McRae et al, we encountered some dubious cases.
For example, in the McRae et al?s norms, carni-
vore is classified as a hypernym, but eats meat as
a behaviour, whereas they seem to us to convey es-
sentially the same information. In this case, we
decided to map both to eats meat (behaviour).
Among other surprising choices, the normalised
phrase used for cargo is seen by McRae et al as
a function, but used by passengers is classified as
denoting the participants in a situation. In this case,
we followed their policy.
While we tried to be consistent in relation la-
belling within and across languages, it is likely
that our own normalisation and type mapping also
include a number of inconsistencies, and our re-
sults must be interpreted by keeping this important
caveat in mind.
The average number of normalised phrases ob-
tained for a concept presented is 5.24 (s.d. 1.82) for
the German participants and 4.96 (s.d. 1.86) for the
Italian participants; in total, for a concept in our set,
the following number of phrases was obtained on
average: 191.28 (German, s.d. 25.96) and 170.42
(Italian, s.d. 25.49).
4 Results
The distribution of property types is analysed both
class-independently and within each class (sepa-
rately for German and Italian), and an unsuper-
vised clustering analysis based on property types
is conducted.
4.1 Distributional Analysis
We first look at the issue of how comparable the
German and Italian data are, starting with a check
of the overlap at the level of specific properties.
There are 226 concept?property pairs that were
produced by at least 10 German subjects; 260 pairs
were produced by at least 10 Italians. Among these
common pairs, 156 (i. e., 69% of the total Ger-
man pairs, and 60% of the Italian pairs) are shared
across the 2 languages. This suggests that the two
sets are quite similar, since the overlap of specific
pairs is strongly affected by small differences in
normalisation (e. g., has a fur, has fur and is hairy
count as completely different properties).
Of greater interest to us is to check to what
extent property types vary across languages and
across concept classes. In order to focus on the
main patterns emerging from the data, we limit our
analysis to the 6 most common property types in
the whole data set (that are also the top 6 types in
the two languages separately), accounting for 69%
of the overall responses. These types are:
? category (Wu/Barsalou code: ch;
?pear is a fruit?)
97
? (external) part (WB code: ece;
?dog has 4 legs?)
? (external) quality (WB code: ese;
?apple is green?)
? behaviour (WB code: eb;
?dog barks?)
? function (WB code: sf ;
?broom is for sweeping?)
? location (WB code: sl;
?skyscraper is found in cities?)
Figure 1 compares the distribution of property
types in the two languages via a mosaic plot
(Meyer et al, 2006), where rectangles have areas
proportional to observed frequencies in the corre-
sponding cells. The overall distribution is very
similar. The only significant differences pertain to
category and location types: Both differences are
significant at the level p < 0.0001, according to a
Pearson residual test (Zeileis et al, 2005).
For the difference in location, no clear pattern
emerges from a qualitative analysis of German and
Italian location properties. Regarding the differ-
ence in (superordinate) categories, we find, inter-
estingly, a small set of more or less abstract hy-
pernyms that are frequently produced by Italians,
but never by Germans: construction (72), object
(36), structure (16). In the these cases, the Ital-
ian translations have subtle shades of meaning that
make them more likely to be used than their Ger-
man counterparts. For example, the Italian word
oggetto (?object?) is used somewhat more con-
cretely than the extremely abstract German word
Objekt (or English ?object?, for that matter) ? in
Italian, the word might carry more of an ?arti-
fact, man-made item? meaning. At the same time,
oggetto is less colloquial than German Sache, and
thus more amenable to be entered in a written def-
inition. In addition, among others, the category ve-
hicle was more frequent in the Italian than in the
German data set (for which one reason could be the
difference between the German and Italian equiva-
lents, which was discussed in section 3.3). Differ-
ences of this sort remind us that property elicita-
tion is first and foremost a verbal task, and as such
it is constrained by language-specific usages. It is
left to future research to test to what extent linguis-
tic constraints also affect deeper conceptual repre-
sentations (would Italians be faster than Germans
type
lang
uag
e
Italian
German
cate
gory part quali
ty
beha
viour funct
ion
locat
ion
Figure 1: Cross-language distribution of property
types
at recognising superordinate properties of concepts
when they are expressed non-verbally?).
Despite the differences we just discussed, the
main trend emerging from figure 1 is one of es-
sential agreement between the two languages, and
indicates that, with some caveats, salient property
types may be cross-linguistically robust. We, thus,
turn to the issue of how such types are distributed
across concepts of different classes. This question
is visually answered by the association plots in fig-
ure 2 on the following page.
Each plot illustrates, through rectangle heights,
how much each cell deviates from the value ex-
pected given the overall contingency tables (in
our case, the reference contingency tables are the
language-specific distributions of figure 1). The
sign of the deviation is coded by direction with re-
spect to the baseline. For example, the first row
of the left plot tells us, among other things, that
in German behaviour properties are strongly over-
represented in mammals, whereas function proper-
ties are under-represented within this class. Like in
figure 1, shades of grey cue degrees of significance
of the deviation (Meyer et al, 2003).
The first observation we can make about figure 2
is how, for both languages, a large proportion of
cells show a significant departure from the overall
distribution. This confirms what has already been
observed and reported in the literature on English
norms ? see, in particular, Vinson et. al. (2008):
98
German
type
clas
s
building
furniture
vehicle
tool
clothing
body
vegetable
fruit
bird
mammal
category part qualitybehaviourfunction location
Italian
type
clas
s
building
furniture
vehicle
tool
clothing
body
vegetable
fruit
bird
mammal
category part qualitybehaviourfunction location
Figure 2: Distribution of property types across classes
property types are highly distinctive characteristics
of concept classes.
The class-specific distributions are extremely
similar in German and Italian. There is no sin-
gle case in which the same cell is deviating sig-
nificantly but in opposite directions in the two lan-
guages; and the most common pattern by far is the
one in which the two languages show the same de-
viation profile across cells, often with very simi-
lar effect sizes (compare, e. g., the behaviour and
function columns). These results suggest that prop-
erty types are not much affected by linguistic fac-
tors, an intrinsically interesting finding that also
supports our idea of structuring relation-based nav-
igation in a multi-lingual dictionary using concept-
class?specific property types.
The type patterns associated with specific con-
cept classes are not particularly surprising, and
they have been already observed in previous stud-
ies (Vinson and Vigliocco, 2008; Baroni and Lenci,
2008). In particular, living things (animals and
plants) are characterised by paucity of functional
features, that instead characterise all man-made
concepts. Within the living things, animals are
characterised by typical behaviours (they bark, fly,
etc.) and, to a lesser extent, parts (they have legs,
wings, etc.), whereas plants are characterised by
a wealth of qualities (they are sweet, yellow, etc.)
Differences are less pronounced within man-made
objects, but we can observe parts as typical of
tool and furniture descriptions. Finally, location is
a more typical definitional characteristic of build-
ings (for clothing, nothing stands out, if not, per-
haps, the pronounced lack of association with typ-
ical locations). Body parts, interestingly, have a
type profile that is very similar to the one of (ma-
nipulable) tools ? manipulable objects are, after all,
extensions of our bodies.
4.2 Clustering by Property Types
The distributional analysis presented in the previ-
ous section confirmed our main hypotheses ? that
property types are salient properties of concepts
that differ from a concept class to the other, but are
robust across languages. However, we did not take
skewing effects associated to specific concepts into
account (e. g., it could be that, say, the property
profile we observe for body parts in figure 2 is
really a deceiving average of completely oppo-
site patterns associated to, say, heads and hands).
Moreover, our analysis already assumed a division
into classes ? but the type patterns, e. g., of mam-
mals and birds are very similar, suggesting that a
higher-level ?animal? class would be more appro-
priate when structuring concepts in terms of type
profiles. We tackled both issues in an unsupervised
clustering analysis of our 50 target concepts based
on their property types. If the postulated classes
are not internally coherent, they will not form co-
herent clusters. If some classes should be merged,
they will cluster together.
Concepts were represented as 6-dimensional
vectors, with each dimension corresponding to one
99
of the 6 common types discussed above, and the
value on a dimension given by the number of times
that concept triggered a response of the relevant
type. We used the CLUTO toolkit4, selecting the
rbr method and setting all other clustering param-
eters to their default values. We explored partitions
into 2 to 10 clusters, manually evaluating the out-
put of each solution.
Both in Italian and in German, the best results
were obtained with a 3-way partition, neatly cor-
responding to the division into animals (mammals
and birds), plants (vegetables and fruits) and ob-
jects plus body parts (that, as we observed above,
have a distribution of types very similar to the one
of tools). The 2-way solution resulted in merging
two of the classes animals and plants both in Ger-
man and in Italian. The 4-way solution led to an
arbitrary partition among objects and body parts
(and not, as one could have expected, in separat-
ing objects from body parts). Similarly, the 5-
to 10-way solutions involve increasingly granular
but still arbitrary partitions within the objects/body
parts class. However, one notable aspect is that in
most cases almost all concepts of mammals and
birds, and vegetables and fruits are clustered to-
gether (both in German and Italian), expressing
their strong similarity in terms of property types
as compared to the other classes as defined here.
Looking at the 3-way solution in more detail,
in Italian, the concept horse is in the same clus-
ter with objects and body parts (as opposed to Ger-
man, where the solution is perfect). The misclassi-
fication results mainly from the fact that for horse
a lot of functional properties were obtained (which
is a feature of objects), but none of them for the
other animals in the Italian data. In German, some
functional properties were assigned to both horse
and dog, which might explain why it was not mis-
classified there.
To conclude, the type profiles associated with
animals, vegetables and objects/body parts have
enough internal coherence that they robustly iden-
tify these macro-classes in both languages. Inter-
estingly, a 3-way distinction of this sort ? exclud-
ing body parts ? is seen as fundamental on the ba-
sis of neuro-cognitive data by Caramazza and Shel-
ton (1998). On the other hand, we did not find
evidence that more granular distinctions could be
made based on the few (6) and very general types
4URL http://glaros.dtc.umn.edu/gkhome/
cluto/cluto/overview
we used. We plan to explore the distribution across
the remaining types in the future (preliminary clus-
tering experiments show that much more nuanced
discriminations, even among all 10 categories, can
be made if we use all types). However, for our ap-
plied purposes, it is sensible to focus on relatively
coarse but well-defined classes, and on just a few
common relation types (alternatively, we plan to
combine types into superordinate ones, e. g. exter-
nal and internal quality). This should simplify both
the automatic harvesting of corpus-based proper-
ties of the target types and the structuring of the
dictionary relational interface.
Finally, the peculiar object-like behaviour of
body parts on the one hand, and the special na-
ture of horse, on the other, should remind us of
how concept classification is not a trivial task, once
we try to go beyond the most obvious categories
typically studied by cognitive scientists ? animals,
plants, manipulable tools. In a lexicographic per-
spective, this problem cannot be avoided, and, in-
deed, the proposed approach should scale in diffi-
cultiese to even trickier domains, such as those of
actions or emotions.
5 Conclusion
This research is part of a project that aims to inves-
tigate the cognitive salience of semantic relations
for (pedagogical) lexicographic purposes. The re-
sulting most salient relations are to be used for re-
vising and adding to the word field entries of a mul-
tilingual electronic dictionary in a language learn-
ing environment.
We presented a multi-lingual concept descrip-
tion experiment. Participants produced differ-
ent semantic relation type patterns across concept
classes. Moreover, these patterns were robust
across the two native languages studied in the ex-
periment ? even though a closer look at the data
suggested that linguistic constraints might affect
(verbalisations of) conceptual representations (and
thus, to a certain extent, which properties are pro-
duced). This is a promising result to be used for au-
tomatically harvesting semantically related words
for a given lexical entry of a concept class.
However, the granularity of concept classes has
to be defined. In addition, to yield a larger number
of usable data for the analysis, a re-mapping of the
rare semantic relation types occurring in the actual
data set should be conducted. Moreover, the stim-
uli set will have to be expanded to include, e. g., ab-
100
stract concepts ? although we hope to mine some
abstract concept classes on the basis of the proper-
ties of our concept set (colours, for example, could
be characterised by the concrete objects of which
they are typical).
To complement the production experiment re-
sults, we aim to conduct an experiment which in-
vestigates the perceptual salience of the produced
semantic relations (and possibly additional ones),
in order to detect inconsistencies between genera-
tion and retrieval of salient properties. If, as we
hope, we will find that essentially the same proper-
ties are salient for each class across languages and
both in production and perception, we will then
have a pretty strong argument to suggest that these
are the relations one should focus on when popu-
lating multi-lingual dictionaries.
Of course, the ultimate test of our approach will
come from empirical evidence of the usefulness of
our relation links to the language learner. This is,
however, beyond the scope of the current project.
References
[Abel and Weber2000] Abel, Andrea and Vanessa We-
ber. 2000. ELDIT?A Prototype of an Innovative
Dictionary. In Heid, Ulrich, Stefan Evert, Egbert
Lehmann, and Christian Rohrer, editors, EURALEX
Proceedings, volume 2, pages 807?818, Stuttgart.
[Baroni and Lenci2008] Baroni, Marco and Alessandro
Lenci. 2008. Concepts and Properties in Word
Spaces. Italian Journal of Linguistics. To appear.
[Boyd-Graber et al2006] Boyd-Graber, Jordan, Chris-
taine Fellbaum, Daniel Osherson, and Robert
Schapire. 2006. Adding Dense, Weighted Connec-
tions to WordNet. In Proceedings of the Thirds Inter-
national WordNet Conference. Masaryk University
Brno.
[Caramazza and Shelton1998] Caramazza, Alfonso and
Jennifer R. Shelton. 1998. Domain?Specific Knowl-
edge Systems in the Brain: The Animate?Inanimate
Distinction. Journal of Cognitive Neuroscience,
10:1?34.
[Chanier and Selva1998] Chanier, Thierry and Thierry
Selva. 1998. The ALEXIA system: The Use of Vi-
sual Representations to Enhance Vocabulary Learn-
ing. In Computer Assisted Language Learning, vol-
ume 11, pages 489?522.
[Fellbaum1998] Fellbaum, Christiane, editor. 1998.
WordNet: An Electronic Lexical Database. Lan-
guage, Speech, and Communication. MIT Press,
Cambridge, MA.
[Fontenelle1997] Fontenelle, Thierry. 1997. Using a
Bilingual Dictionary to Create Semantic Networks.
International Journal of Lexicography, 10(4):275?
303.
[Garrard et al2001] Garrard, Peter, Matthew A. Lam-
bon Ralph, John R. Hodges, and Karalyn Patterson.
2001. Prototypicality, Distinctiveness, and Intercor-
relation: Analyses of the Semantic Attributes of Liv-
ing and Nonliving Concepts. Cognitive Neuropsy-
chology, 18(2):125?174.
[Geckeler2002] Geckeler, Horst. 2002. Anfa?nge und
Ausbau des Wortfeldgedankens. In Cruse, D. Alan,
Franz Hundsnurscher, Michael Job, and Peter Rolf
Lutzeier, editors, Lexikologie. Ein internationales
Handbuch zur Natur und Struktur von Wo?rtern und
Wortscha?tzen, volume 21 of Handbu?cher zur Sprach-
und Kommunikationswissenschaft, pages 713?728.
de Gruyter, Berlin ? New York.
[Jansz et al1999] Jansz, Kevin, Christopher Manning,
and Nitin Indurkha. 1999. Kirrkirr: Interactive Visu-
alisation and Multimedia From a Structured Warlpiri
Dictionary. In Proceedings of the 5th Australian
World Wide Web Conference (AusWeb?99), pages
302?316.
[McRae et al2005] McRae, Ken, George S. Cree,
Mark S. Seidenberg, and Chris McNorgan. 2005.
Semantic Feature Production Norms for a Large
Set of Living and Nonliving Things. Be-
haviour Research Methods, Instruments & Comput-
ers, 37(4):547?559.
[Meyer et al2003] Meyer, David, Achim Zeileis, and
Kurt Hornik. 2003. Visualizing Independence Using
Extended Association Plots. In Proceedings of DSC
2003. Online at URL http://www.ci.tuwien.
ac.at/Conferences/DSC-2003/.
[Meyer et al2006] Meyer, David, Achim Zeileis, and
Kurt Hornik. 2006. The Strucplot Framework: Vi-
sualizing Multi?Way Contingency Tables With vcd.
Journal of Statistical Software, 17(3):1?48.
[Summers1999] Summers, Della, editor. 1999. Long-
man Language Activator. The World?s First Produc-
tion Dictionary. Longman, Harlow.
[Vinson and Vigliocco2008] Vinson, David P. and
Gabriella Vigliocco. 2008. Semantic Feature Pro-
duction Norms for a Large Set of Objects and Events.
Behaviour Research Methods, 40(1):183?190.
[Wu and Barsalou2004] Wu, Ling?ling and
Lawrence W. Barsalou. 2004. Grounding Con-
cepts in Perceptual Simulation: I. Evidence From
Property Generation. Unpublished manuscript.
[Zeileis et al2005] Zeileis, Achim, David Meyer, and
Kurt Hornik. 2005. Residual?Based Shadings for
Visualizing (Conditional) Independence. Technical
Report 20, Department of Statistics and Mathemat-
ics, Wirtschaftsuniversita?t, Vienna.
101
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 54?62,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Predicting Cognitively Salient Modifiers
of the Constitutive Parts of Concepts
Gerhard Kremer and Marco Baroni
CIMeC, University of Trento, Italy
(gerhard.kremer|marco.baroni)@unitn.it
Abstract
When subjects describe concepts in terms
of their characteristic properties, they often
produce composite properties, e. g., rabbits
are said to have long ears, not just ears. We
present a set of simple methods to extract
the modifiers of composite properties (in
particular: parts) from corpora. We achieve
our best performance by combining evi-
dence about the association between the
modifier and the part both within the con-
text of the target concept and independently
of it. We show that this performance is rel-
atively stable across languages (Italian and
German) and for production vs. perception
of properties.
1 Introduction
Subject-generated concept descriptions in terms of
properties of different kinds (category: rabbits are
mammals, parts: they have long ears, behaviour:
they jump, . . . ) are widely used in cognitive sci-
ence as proxies to feature-based representations of
concepts in the mind (Garrard et al, 2001; McRae
et al, 2005; Vinson and Vigliocco, 2008). These
feature norms (as collections of subject-elicited
properties are called in the relevant literature) are
used in simulations of cognitive tasks and experi-
mental design. Moreover, vector spaces that have
subject-generated properties as dimensions have
been shown to be a good complement or alternative
to traditional semantic models based on corpus col-
locates (Andrews et al, 2009; Baroni et al, 2010).
Since the concept?property pairs in feature
norms resemble the tuples that relation extraction
algorithms extract from corpora (Hearst, 1992; Pan-
tel and Pennacchiotti, 2006), recent research has
attempted to extract feature-norm-like concept de-
scriptions from corpora (Almuhareb, 2006; Baroni
et al, 2010; Shaoul and Westbury, 2008). From
a practical point of view, the success of this en-
terprise would mean being able to produce much
larger norms without the need to resort to expensive
and time-consuming elicitation experiments, lead-
ing to wider cognitive simulations and possibly bet-
ter vector space models of semantics. From a the-
oretical point of view, a corpus-based system that
produces human-like concept descriptions might
provide cues of how humans themselves come up
with such descriptions.
However, the corpus-based models proposed for
this task up to this point overlook the fact that sub-
jects very often produce composite properties: Sub-
jects state that rabbits have long ears, not just ears;
cars have four wheels; a calf is a baby cow, etc.
Composite properties are not multi-word expres-
sions in the usual sense. There is nothing special
or idiomatic about long ears. It is just that we
find it to be a remarkable fact about rabbits, worth
stating in their description, that their ears are long.
In the norms described in section 3, around one
third of the part descriptions are composite. Note
that while our focus is on feature norms, a similar
point about the importance of composite properties
could be made for other knowledge repositories of
importance to computational linguistics, such as
WordNet (Fellbaum, 1998) and ConceptNet (Liu
and Singh, 2004), approximately 68,000 (36%) of
the entries and 1,300 (32%) of the part entries being
composites, respectively.
In this paper, we tackle the problem of gener-
ating composite properties from corpus data by
simplifying it in various ways. First, we focus
on part properties only, because they are com-
monly encountered in feature norms, and because
they are are commonly composite (cf. section 3).
Second, we assume that an early step in the pro-
cess of property extraction has already generated
a list of simple parts, perhaps using an existing
whole?part relation extraction algorithm (Girju et
al., 2006). Finally, we focus on composite parts
54
with an adjective?noun structure ? together with
numeral?noun cases, these constitute the near to-
tality of composite parts in the norms described
in section 3. Having thus delimited the scope of
our exploration, we will adopt the following ter-
minology: concept for the target nominal concept
(rabbit), part for the (nominal) part property (ear)
and modifier for the adjective that makes the part
composite (long).
We present simple methods that, given a list of
concept?part pairs and a POS-tagged and lemma-
tised corpus, rank and extract candidate modifiers
for the parts when predicated of the concepts. We
exploit the co-occurrence patterns of the part with
the modifier both near the concept and in other con-
texts (both kinds of co-occurrences turn out to be
helpful). We first test our methods on German fea-
ture norms, and then we show that they generalise
well by applying them to similar data in Italian, and
to the same set of German concept?part pairs when
evaluated by asking new subjects to rate the top
ranked modifiers generated by the ranking meth-
ods. This also leads to a more general discussion
of differences between modifiers produced by sub-
jects in the elicitation experiment and those that are
rated acceptable in perception, and the significance
of this for corpus-based property generation.
The paper is structured as follows. After shortly
reviewing some related work in section 2, in sec-
tion 3, we describe our feature norms focusing in
particular on composite properties. In section 4,
we describe our methods to harvest modifiers from
a corpus and report the extraction experiments,
whereas section 5 concludes by discussing direc-
tions for further work.
2 Related Work
We are not aware of other attempts to extract
concept-dependent modifiers of properties. We
review instead related work in feature norm col-
lection and prediction, and mention some rele-
vant literature on the extraction of significant co-
occurrences from corpora.
Feature-based concept description norms have
been collected in psychology for decades. Among
the more recent publicly available norms of this
sort, there are those collected by Garrard et al
(2001), Vinson and Vigliocco (2008) and McRae
et al (2005). The latter was the main methodologi-
cal inspiration for the bilingual norms we rely on
(see section 3 below). The norms of McRae and
colleagues include descriptions of 541 concrete
concepts corresponding to English nouns. The 725
subjects that rated these concepts had to list their
features on a paper questionnaire. The produced
features were then normalised and classified into
categories such as part and function by the exper-
imenters. The published norms include, among
other kinds of information, the frequency of pro-
duction of each feature for a concept by the sub-
jects.
Almuhareb (2006) was the first to attempt to
reproduce subject-generated features with text min-
ing techniques. He computed precision and re-
call measures of various pattern-based feature ex-
traction methods using Vinson and Vigliocco?s
norms for 35 concepts as a gold standard. The
best precision was around 16% at about 11% re-
call; maximum recall was around 60% with less
than 2% precision, confirming how difficult the
task is. Importantly for our purposes, Almuhareb
removed the modifier from composite features be-
fore running the experiments (1 wheel converted
to wheel), thus eschewing the main characteris-
tic of subject-generated concept descriptions that
we tackle here. Shaoul and Westbury (2008) and
Baroni et al (2010) used corpus-based semantic
space models to predict the top 10 features of 44
concepts from the McRae norms. The best model
(Baroni et al?s Strudel) guesses on average 24% of
the human-produced features, again confirming the
difficulty of the task. And, again, the test set was
pre-processed to remove modifiers of composite
features, thus sidestepping the problem we want
to deal with. It is worth remarking that, by remov-
ing modifiers, previous authors are making the task
easier in terms of feature extraction procedure (be-
cause the algorithms only need to look for single
words), but they also create artificial ?salient? fea-
tures that, once the modifier has been stripped of,
are not that salient anymore (what distinguishes a
monocycle from a tricycle is that one has 1 wheel,
the other 3, not simply having wheels). It is con-
ceivable that a method to assign sensible modifiers
to features might actually improve the overall qual-
ity of feature extraction algorithms.
Following a very long tradition in computational
linguistics (Church and Hanks, 1990), we use co-
occurrence statistics for words in certain contexts
to hypothesise a meaningful connection between
the words. In this respect, what we propose is not
different from common methods to extract and rank
55
collocations, multi-word expressions or semanti-
cally related terms (Evert, 2008). From a technical
point of view, the innovative aspect of our task is
that we do not just look for co-occurrences between
two items, but for co-occurrences in the context of
a third element, i. e., we are interested in modifier?
part pairs that are related when predicated of a
certain concept. The method we apply to the ex-
traction of modifier?part pairs when they co-occur
with the target concept in a large window is similar
to the idea of looking for partially untethered con-
textual patterns proposed by Garera and Yarowsky
(2009), that extract name?pattern?property tuples
where the pattern and the property must be adja-
cent, but the target name is only required to occur
in the same sentence.
3 Composite Parts in Feature Norms
Our empirical starting point are the feature norms
collected in parallel from 73 German and 69 Ital-
ian subjects by Kremer et al (2008), following a
methodology similar to that of McRae et al (2005).
The norms pertain to 50 concrete concepts from 10
classes such as mammals (e. g., dog), manipulable
tools (e. g., comb), etc. The concept?part pairs in
these norms served on the one hand as input to our
algorithm ? on the other hand, its output (the set of
selected modifiers from the corpus) could be evalu-
ated against those modifiers that were produced by
the subjects. Furthermore, the bilingual nature of
the norms allows us to tune our algorithm on one
language (German), and evaluate its performance
on the other (Italian), to assess its cross-lingual
generalisation capability.
To confirm that speakers actually frequently pro-
duce properties composed of part and modifier, ob-
serve that in the German data (10,010 descriptive
phrases in total), of the 1,667 parts produced, 625
(more than one third) were composite parts, and
404 were composed of an adjective and a noun, the
target of this research work. Looking at the distinct
parts that were elicited, 92 were always produced
with a modifier, 280 only without modifier, and 122
both with and without modifier. That is, for about
43% of the parts at least some speakers used a com-
posite expression of adjective and noun. This high
proportion motivates our work and is not surpris-
ing, given that, for describing a specific concept,
one will tend to come up with whatever makes this
concept special and distinguishes it from other con-
cepts ? which (considering parts) sometimes is the
part itself (elephant: trunk) and sometimes some-
thing special about the shape, colour, size, or other
attributes of the part (elephant: big ears).
The data set for modifier extraction and subse-
quent method evaluation comprises all the concept?
modifier?part triples (e. g., onion: brown peel) pro-
duced by at least one subject, taken from the Ger-
man and the Italian norms. The German (Italian)
speakers described 41 (30) different concepts by
using at least one out of 80 (45) different parts in
combination with one out of 62 (50) different mod-
ifiers, totalling to 229 (127) differently combined
triples.
4 Experiments
This section describes the approach we explored for
ranking and extracting modifiers of composite parts
and evaluates the performance of 6 different extrac-
tion methods in terms of the production norms.
Acceptance rate data from a follow-up judgement
experiment complete the evaluation.
4.1 Ranked Modifier Lists
Based on the idea that the co-occurrence of words
in a text corpus reflects to some extent how strong
these words are associated in speakers? minds
(Spence and Owens, 1990), our extraction approach
works on the lemmatised and POS-tagged German
WaCky1 web corpus of about 1.2 billion tokens.
Modifier?Part Frequencies
Using the CQP2 tool, corpus frequencies were col-
lected for all co-occurrences of adjectives with
those part nouns that were produced in the exper-
iment described above. A possible gap of up to
3 tokens between the pair of adjective and noun
allowed to extract also adjectives that are not di-
rectly adjacent to the nouns in the corpus (but in a
sequence of adjectives, for example). For each part
noun, the 5 most frequent adjective modifiers from
the ranked modifier?part list were selected under
the assumption that the preferred usage of these
modifiers with the specific part indicates the most
common attributes which that part typically has.
1See the WaCky project at http://wacky.sslmit.
unibo.it
2Corpus Query Processor (part of the IMS Open Corpus
Workbench, see http://cwb.sourceforge.net)
56
Log-Likelihood Values of Frequencies
An attempt to improve the performance of the first
method is to calculate3 the log-likelihood associ-
ation value for each modifier?part pair instead of
keeping the raw co-occurrence frequency, and se-
lect the 5 highest ranked modifiers for each part
from this list. Log-likelihood weighting should
account for typical modifiers which have a low fre-
quency but do generally not occur often in the cor-
pus, and with not many other parts ? their log-likeli-
hood value will be higher, and so will be their rank
(e. g., two-sided blade in contrast to long blade).
Modifier?Part Frequencies in Concept Context
However, both of these methods do not necessarily
yield generally atypical modifiers that are however
typical of a part when it is attributed to a specific
concept. For example, birds? beaks are typically
brown, orange or yellow, but aiming to extract mod-
ifiers for a crow?s beak, black would be one of the
desired modifiers ? which does not appear at a high
frequency rank as a generic beak modifier. The
methods described so far did not take the concept
into account when generating the modifier?part
pairs, i. e., for all concepts with a specific part the
same set of modifiers would be extracted.
To address this issue, a second frequency rank
list was prepared in the same manner ? with the
only difference that the part noun had to appear
within the context of the concept noun. That way,
also modifiers for specific concepts? parts that devi-
ate from the most typical part modifiers appear at a
high rank. However, these data are sparser, which
is why we used a wide context of 40 sentences (20
sentences before and after the part) within which
the concept had to occur (i. e., a paragraph-like con-
text size in which the topic, presumably, comprises
the concept). We refer to ranked lists of modifier?
part pairs that do not take the target concept into
account as contextless lists, and to lists within the
span of a context as in-context lists.
Due to the already mentioned data sparseness
problem, not all modifiers used for a part noun in
the production norms could be extracted with the
latter method, as some of the obvious modifiers for
specific parts are just not written about. For these,
there is a higher chance that they appear, if at all, in
the contextless rank list. For example, thin bristles
does not appear in the context of broom. In the in-
3Using the UCS toolkit, described at http://www.
collocations.de/software.html#UCS
contextless concept context
rank freq modifier freq modifier
1 507 thick 16 thick
2 209 dense 14 white
3 204 soft 11 small
4 185 black 11 soft
5 175 long 9 dense
Table 1: Top 5 modifiers from frequency rank lists
for part fur and concept bear
context list, 33% of the 229 triples extracted from
the German norms were not found (in the context-
less list, only 9% modifier?part pairs are missing).
Additionally, particular concepts, parts, or concept?
part pairs (within the 40 sentence span) might be
missing from the corpus, as well. From the Ger-
man norms collection, all concepts appeared in the
corpus, but one part (a noun?noun compound), and
6 concept?part pairs (rare, colloquial part nouns)
were missing. In the evaluation to follow, all the
modifiers pertaining to these missing data from the
corpus will be counted as positives not found by
the algorithm.
The example excerpt in table 1 shows modifiers
that were selected for bear and fur, using the two
frequency rank lists described above. Although in
this example most of the modifiers (thick, dense,
soft) are found in both lists, two arguably reason-
able modifiers are just in the contextless set (black,
long), and one only in the in-context set (white).
A disadvantage of selecting modifiers from the in-
context rank list is that many modifiers have the
same low frequency, but they should nevertheless
have differing ranks. In such cases, we assigned
ranks according to alphabetic order of modifiers.
Summed Log-Rescaled Frequencies
Next, to improve performance and profit from both
information sources the above methods provide,
the in-context and contextless rank lists were com-
bined. In one variant, the scaled frequencies for
the concept?modifier?part triples appearing in both
lists were added. Scaling was necessary because
the frequencies in the contextless list are in general
much higher than in the in-context list. Further-
more, to account for the fact that at high ranks
the difference in frequency between subsequent
ranks is much higher than at lower ranks, scaling
was done by using the logarithmic values of the fre-
57
quencies: For each concept?modifier?part triple, its
logarithmic frequency value was divided by the log-
arithmic value of the maximum corpus frequency
of all parts in the corpus (in the contextless list)
or of all concept?part pairs co-occurring within 40
sentences (in the case of the in-context list).
Productwise Combination of Frequencies
As an alternative back-off approach, the raw fre-
quencies were combined productwise into a new
list (for those modifier?part pairs missing in the in-
context list, the frequency of the pair in the context-
less list was taken alone, instead of multiplying it
by zero; i. e., the in-context term was max(freq, 1)).
This achieves a sort of ?intersective? effect, where
modifiers that are both commonly attributed to the
part and predicated of it in the context of the tar-
get concept are boosted up in the list, according to
the intuition that a good modifier should be both
plausible for the part in general, and typical for the
concept at hand.
Cosine-Based Re-Ranking
An attempt to further improve performance is based
on the idea that parts are described by some spe-
cific types of attributes. For example, a leaf would
be characterised by its shape or consistency (e. g.,
long, stiff ), whereas for fur rather colour should be
considered (e. g., white, brown). If we are able to
cluster modifiers for their attribute type and find
out which attribute types are in particular important
for a specific part, those could get a preference in
the rank list and be moved towards the top. To
approach this in a simple way, a re-ranking method
is used which is supposed to cluster and choose the
right cluster of modifiers implicitly: The modifiers
in the (productwise-) combined list were tested for
their similarity by looking if they co-occur with
the same relative frequency with the same set of
nouns. In case of high similarity (in this respect)
of a modifier to a single other modifier, or if the
modifier was similar to a lot of modifiers, it should
be re-ranked to a higher position. In more detail,
a vector was created for each modifier, denoting
its co-occurrence frequencies with each noun in
the corpus within a window of 4 tokens (on the
left side of the noun). Random indexing helped to
reduce the vector dimensionality from 27,345 to
3,000 elements (Sahlgren, 2005). These vectors
served for calculating the cosine distances between
modifiers. Then, for each of the top 200 modifiers
in the combined frequency rank list (covering 84%
of the triples from the German norms), the cosine
distance was calculated to each of the top 100 mod-
ifiers in the contextless rank list. A constant of 1
was added to each of the computed cosines, thus
obtaining a quantity between 1 and 2. The original
combined frequency value was multiplied by this
quantity (thus leaving it unchanged when the orig-
inal cosine was 0, increasing it otherwise). From
the re-ranked list resulting from this operation, we
selected, again, the top 5 modifiers of each concept?
part pair. For example, suppose that black is among
the modifiers of a crow?s beak in the combined list.
We compute the cosine similarity of black with the
top 100 modifiers of beak (in any context), and,
for each of these cosines, we multiply the original
combined value of black by cosine+1. Since the
colour is a common attribute of beaks, the presence
of modifiers like yellow and brown, high on the con-
textless beak list, helps re-ranking black high in the
crow-specific beak list. We hope that this method
helps out concept-specific values (e. g., black for
crow) of attributes that are in general typical of a
part (colour for beak).
4.2 Performance on Composite Parts From
the Production Norms
The feature norms data represented the gold stan-
dard for the evaluation of all sets of modifiers cho-
sen by each of the described methods for the given
concept?part pairs. Note that, even if a modifier?
part pair was produced only once in the fea-
ture production norms, the corresponding concept?
modifier?part triple was included in the gold stan-
dard ? which contains 41 different concepts, 80
different parts, and 62 different modifiers, totalling
to 229 concept?modifier?part triples. As in the Ger-
man corpus there are 154,935 adjective?part-noun
pairs, the random baseline (random guessing) for
finding these 229 pairs is approaching 0 (similarly
for Italian and the judgement dataset).
Figure 1 displays the performance of the meth-
ods on German in the form of a recall?precision
graph. For each rank (1?5), overall recall and inter-
polated precision values are given for all modifier?
part pairs up to this rank ? note that precision at
1% recall is overrated as it is based on an arbitrary
fraction of rank 1 pairs. As expected, extracting
modifiers of parts within a concept context (the in-
context list) achieves low recall. In contrast, modi-
fiers that were extracted by querying the corpus for
parts without considering the concept context have
58
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Method Performance
recall
prec
ision
parts in concept context (freq)contextless (freq)combination productwise (freqs)cosine re?ranking (freq product)parts in context (log?likelihood)combination by sum (log rescaled freqs)baseline: averaged random guessing
Figure 1: Evaluation on German norms
a higher recall. But this method has a lower preci-
sion in general. The performance for the method
combining frequencies productwise and for the one
that re-ranks this combined list via cosine-based
smoothing are substantially better. Not only the pre-
cision is much higher at all recall levels, but also
their maximum recall values are higher than those
of the contextless lists, i. e., it was worth combin-
ing the complementing information in the two lists.
However, the performance of the cosine-based re-
ranked list compared to the productwise-combined
list is not considerably higher, as we might have
hoped. The remaining two alternative methods per-
formed much worse: the one using log-likelihood
values as ranking criterion had in general a low pre-
cision and a low recall, and the method combining
the in-context and the contextless rank list by sum-
ming up the rescaled logarithmic frequency values
performs as bad as the contextless rank list. Never-
theless, note that all methods perform distinctively
well above the baseline.
Qualitatively analysing the data collected with
the described methods did not give definite clues
about why some performed not as good as expected.
As a comprehensible example, the modifier short
for legs is at rank 5 in the contextless list, but be-
cause of the frequent co-occurrence with monkey it
rises to rank 2 in the productwise combination of
these lists, and even to rank 1 in the cosine-based
re-ranked list. An understandable bad performing
example is the modifier yellow for the eyes of an
owl: Although it appears in the in-context list at
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Method Performance
recall
prec
ision
parts in concept context (freq)contextless (freq)combination productwise (freqs)cosine re?ranking (freq product)
Figure 2: Evaluation on Italian norms
rank 2, it is a quite infrequent modifier for eyes
in general (i. e., low in the contextless list), and
thus it is not contained in the top 5 modifiers in
the productwise combined rank list. On the other
hand, it is not perfectly clear to us why, e. g., flat for
the roof of a skyscraper, which is at rank 5 in the
contextless list and at rank 6 in the combined list,
is lowered to rank 9 in the cosine-based re-ranked
list (in the in-context list, it does not appear at all).
For all methods, collected modifiers include such
of undesired attributes not describing the part, but
other, rather situational aspects, e. g., own, left, new,
protecting, and famous. Furthermore, we observed
that some modifiers are reasonable for the respec-
tive concept?part pair, but they are counted as false
because they did not occur in the production experi-
ment (that we took as the evaluation basis), e. g., for
the blade of a sword, not only large is acceptable,
but also long and wide, essentially making the same
assertion about the size of the blade. This issue is
addressed further below by creating a new evalua-
tion standard based on plausibility judgements.
To evaluate the cross-lingual performance of
the extraction approach, the Italian norms were
explored similarly to the German norms for com-
posite parts. The gold standard here comprised
127 triples (from combinations of 30 different con-
cepts, 45 parts, and 50 different modifiers). The
same methods described above were used to ex-
tract modifiers from the Italian WaCky web corpus
(more than 1.5 billion tokens), with one difference
regarding the query for adjectives near nouns: As
59
in the Italian language adjectives in a noun phrase
can be used both before and after the noun (with
differences in their meaning), and given that most
of them were produced after the noun, we collected
all adjectives occurring up to 2 words from the left
of the noun and up to 4 words to the right.
Figure 2 shows the performance curves of the
methods for the Italian data. In this evaluation, the
method using log-likelihood values and the method
combining lists via addition of logarithmic rescaled
frequencies are omitted as their performance was
not promising at all in the German data, and they
are conceptually similar to the contextless and
productwise-combination approaches, respectively.
Like in German, the in-context method yields a
low recall, in contrast to the method not consid-
ering the presence of concepts in context. Again,
cosine-based re-ranking performs very similarly to
the method using the productwise-combined list.
For the performance on the Italian data, their differ-
ence from the simple frequency rank lists is not as
large as it is for the German data, but it is clearly
visible, especially at higher recall values.
Summarising, our comparison of various corpus-
based ranking methods to the feature production
norms, both in German and Italian, suggests that
composite parts produced by subjects are best
mined in corpora by making use of both general in-
formation about typical modifiers of the parts (the
contextless rank) and more specific information
about modifiers that co-occur with the part near the
target concept. Moreover, it is better to combine
the two information sources productwise, which
suggests an intersecting effect (the most likely mod-
ifiers are both well-attested out of context and seen
near the target concept). For both languages, there
is no strong evidence that re-ranking by cosine sim-
ilarity (a method that should favour modifiers that
are values of common attributes of a part) is im-
proving on the plain combination method (although
re-ranking is not hurting, either).
By looking at the overall performance, the re-
sults are somewhat underwhelming, with precision
around 20% at around 30% recall for the best mod-
els in both languages. A natural question at this
point is whether the modifiers ranked at the top
by the best methods and treated as false positives
because they are not in the norms are nevertheless
sensible modifiers for the parts, or whether they are
truly noise. In order to explore this issue we turn
now to our next experiment.
4.3 Performance Evaluation Based on
Plausibility Judgements
The purpose of this judgement experiment was to
see which concept?modifier?part triples the ma-
jority of participants would rate as acceptable. It
allows us to investigate two topics: (i) the compari-
son of what people produce and what they perceive
as being a prominent modifier for a concept?part
pair (our algorithm might actually provide good
candidates which were just not produced, as we just
said) and (ii) a re-evaluation of the cosine-based
re-ranking method (it could be in fact better than
we thought because we only evaluated what was
produced, but did not have a definite plausibility
rating of the candidates missing in the norms).
The tested set contained the triples yielded by
our two best performing methods (productwise
combination and cosine-based re-ranking), which
were applied to the German feature norms (692
triples, comprising 41 concepts and 71 parts). From
this set, a set of triples was chosen randomly for
each of the 46 participants (recruited by e-mail
among acquaintances of the first author). The
triples were presented to participants embedded
into a natural-sounding sentence of the form ?The
[part] of a [concept] is [modifier]?. Each partic-
ipant rated 333 sentences, presented on separate
lines of a text file (this set of sentences presented
comprised additional triples which were intended
for other purposes ? for the current evaluation, we
used a subset of 110 of these from each partici-
pant, on the average). Participants were instructed
to read the sentences as general statements about
a concept?s part and mark them by typing a let-
ter (?w? for wonderful and ?d? for dubious ? to
facilitate one-handed typing and easy memorisa-
tion) at the beginning of the line, if they thought it
plausible/unlikely that someone used the sentence
to explain an aspect of the relevant part. In total,
5,525 judgements were collected; each sentence in
the set was judged on the average by 8 persons.
The performance evaluation is based on the ac-
ceptance rate of the participants: Modifiers ac-
cepted by at least 75% of the raters are consid-
ered plausible. Figure 3 shows the recall?precision
graph for the methods tested on the concept?part
pairs from the German norms. From the 692 triples
judged, around 13% were accepted by the majority
of speakers. The precision rate is comparable with
the evaluation on the basis of the modifiers pro-
duced by participants (highest recall is 1, of course,
60
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Method Performance
recall
prec
ision
combination productwise (freqs)cosine re?ranking (freq product)
Figure 3: Evaluation on judgements (German)
because all modifiers to be judged were exclusively
from the data set selected by our methods).
Again, the performance of the cosine-based re-
ranking method is similar to the performance of the
productwise-combination method. For a more ex-
act evaluation of the difference between these two,
a last test was conducted: Instead of measuring the
performance in the form of counts of modifiers that
were accepted by the majority of participants, we
used the acceptance rates of all modifiers: The ac-
ceptance rates of all judged triples were summed up
if they contained the same concept?part pair. This
means that each concept?part pair received a score
reflecting the overall acceptance of the set of modi-
fiers for that pair (e. g., for bear: fur, all acceptance
rates for bear: brown fur, bear: soft fur, . . . were
summed up). Then, the score of each concept?part
pair in the productwise-combined list was com-
pared against the score of the same pair for the
cosine-based re-ranking method, using a pairwise
t-test (this procedure is sound because the modifiers
per pair are the same for the two methods). The
test showed a significant difference (p = 0.008), but
in favour of the productwise-combination method
(score means were slightly higher). That is, cosine-
based re-ranking in the current form brings no ad-
vantage over the simpler productwise combination
of the frequency lists.
Finally, turning to the qualitative comparison of
production and perception, there was a relatively
small overlap of triples (46) contrasting with modi-
fiers only produced but not accepted (53), and mod-
ifiers accepted but not produced (42). Intuitively,
we would have expected that what was produced
will be also accepted by the majority of people.
Possibly, some participants in the judgement ex-
periment found a few of the triples produced ques-
tionable (goose: long beak) ? such triples were in
our gold standard because we deliberately did not
want to exclude composite parts even if produced
by only one speaker ? whereas participants produc-
ing parts for given concepts probably just did not
think of specific parts or modifiers (e. g., aeroplane:
small windows and bear: dense fur). The important
fact regarding this difference is, however, that our
method captures both kinds of modifiers.
5 Discussion
We presented several corpus-based methods that
provide a set of adjective modifiers for each con-
crete concept?part pair, to be compared to those
modifiers that are salient to human subjects. The
general approach was to generate ranked lists, and
select the 5 candidates at the top of the ranks.
The best of our methods works on the simple
(productwise-) combination of frequency informa-
tion of co-occurring adjective?noun pairs with and
without considering a wide ?concept context? in
which the part noun has to occur. This method per-
formed better than the one based on co-occurrence
frequency not in concept context (generic modi-
fiers, not appropriate for every concept) and the
one based on co-occurrence frequencies in concept
context, only (low recall because of sparse data).
We evaluated the methods on feature production
norms and on plausibility judgements of generated
concept?modifier?part triples to compare produc-
tion and perception of modifiers. The performance
was similar in precision ? although the qualitative
analysis showed that modifiers produced and modi-
fiers perceived did not have a large overlap. This
means our algorithm is capable of collecting both
with the same performance.
After tuning the algorithm on German norms, we
evaluated its generalisation capability to a different
language (Italian). Performance was similar. Less
satisfying at first glance is the precision value of
just around 20% at the maximum recall level (how-
ever, when compared to the baseline of below 1%
precision, this is an essentially better value) ? as
well as the fact that our implementation of the intu-
itive idea to re-rank modifiers that are similar (and
should instantiate the same attribute) did not have
61
a performance advantage. This is subject to further
work. Moreover, using a machine-learning method
(building a binary classifier) could be tried. An-
other idea was to crawl the web and select concept-
specific text passages to build a specialised corpus.
Possibly, we could draw then from a richer infor-
mation source. A rough attempt to do this did not
seem to yield promising results.
So far, we included only adjectives as permis-
sible modifiers. A future extension could be also
aiming for numerals (e. g., four wheels). Then, for
the simulation of human-like behaviour we imag-
ine as part of the possible future work to enable
the algorithm to decide if a part noun should be
paired with a modifier, at all ? or if the part itself is
sufficient to describe a concept (big ears vs. trunk).
Regarding the evaluation, a more exact perfor-
mance measure would probably be achieved by
either having more participants producing concept
descriptions and then only selecting those modi-
fiers for the gold standard that were produced by a
majority ? or letting participants in a judgement ex-
periment also judge modifiers that were produced,
to filter out the unlikely ones.
A next step in the project will be extracting
salient parts for concepts (which we assumed to
have done already for the purpose of this paper),
possibly by integrating the information we already
collected by extracting modifiers. In the end, we
would like to come up with an adaptable method
that extracts not only parts but also other types
of relations (e. g., category, behaviour, function,
etc.), which have been already addressed in re-
lated works, though. The issue we presented in
this paper, however, is new and, we think, worth
exploring.
References
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463?498.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Kenneth Church and Peter Hanks. 1990. Word associ-
ation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Stefan Evert. 2008. Corpora and collocations. In
A. Lu?deling and M. Kyto?, editors, Corpus Linguis-
tics: An International Handbook, pages 1212?1248.
Mouton de Gruyter, Berlin, Germany.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Nikesh Garera and David Yarowsky. 2009. Structural,
transitive and latent models for biographic fact ex-
traction. In Proceedings of EACL, pages 300?308,
Athens, Greece.
Peter Garrard, Matthew Lambon Ralph, John Hodges,
and Karalyn Patterson. 2001. Prototypicality, dis-
tinctiveness, and intercorrelation: Analyses of the
semantic attributes of living and nonliving concepts.
Cognitive Neuropsychology, 18(2):25?174.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539?545, Nantes, France.
Gerhard Kremer, Andrea Abel, and Marco Baroni.
2008. Cognitively salient relations for multilingual
lexicography. In Proceedings of the COGALEX
Workshop at COLING08, pages 94?101.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
tical commonsense reasoning toolkit. BT Technol-
ogy Journal, pages 211?226.
Ken McRae, George Cree, Mark Seidenberg, and Chris
McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of COLING-ACL, pages 113?120, Sydney, Aus-
tralia.
Magnus Sahlgren. 2005. An introduction to random
indexing. http://www.sics.se/?mange/
papers/RI_intro.pdf.
Cyrus Shaoul and Chris Westbury. 2008. Performance
of HAL-like word space models on semantic cluster-
ing. In Proceedings of the ESSLLI Workshop on Dis-
tributional Lexical Semantics, pages 42?46, Ham-
burg, Germany.
Donald Spence and Kimberly Owens. 1990. Lexical
co-occurrence and association strength. Journal of
Psycholinguistic Research, 19(5):317?330.
David Vinson and Gabriella Vigliocco. 2008. Seman-
tic feature production norms for a large set of objects
and events. Behavior Research Methods, 40(1):183?
190.
62
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 1?9,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
(Linear) Maps of the Impossible:
Capturing semantic anomalies in distributional space
Eva Maria Vecchi and Marco Baroni and Roberto Zamparelli
Center for Mind/Brain Sciences, University of Trento
Rovereto (TN), Italy
{evamaria.vecchi-1,marco.baroni,roberto.zamparelli}@unitn.it
Abstract
In this paper, we present a first attempt to
characterize the semantic deviance of com-
posite expressions in distributional seman-
tics. Specifically, we look for properties of
adjective-noun combinations within a vector-
based semantic space that might cue their lack
of meaning. We evaluate four different com-
positionality models shown to have various
levels of success in representing the mean-
ing of AN pairs: the simple additive and
multiplicative models of Mitchell and Lap-
ata (2008), and the linear-map-based models
of Guevara (2010) and Baroni and Zamparelli
(2010). For each model, we generate com-
posite vectors for a set of AN combinations
unattested in the source corpus and which
have been deemed either acceptable or seman-
tically deviant. We then compute measures
that might cue semantic anomaly, and com-
pare each model?s results for the two classes of
ANs. Our study shows that simple, unsuper-
vised cues can indeed significantly tell unat-
tested but acceptable ANs apart from impos-
sible, or deviant, ANs, and that the simple ad-
ditive and multiplicative models are the most
effective in this task.
1 Introduction
Statistical approaches to describe, represent and un-
derstand natural language have been criticized as
failing to account for linguistic ?creativity?, a prop-
erty which has been accredited to the compositional
nature of natural language. Specifically, criticisms
against statistical methods were based on the ar-
gument that a corpus cannot significantly sample a
natural language because natural language is infi-
nite (Chomsky, 1957). This cricticism also applies
to distributional semantic models that build seman-
tic representations of words or phrases in terms of
vectors recording their distributional co-occurrence
patterns in a corpus (Turney and Pantel, 2010), but
have no obvious way to generalize to word combi-
nations that have not been observed in the corpus.
To address this problem, there have been several re-
cent attempts to incorporate into distributional se-
mantic models a component that generates vectors
for unseen linguistic structures by compositional op-
erations in the vector space (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010).
The ability to work with unattested data leads to
the question of why a linguistic expression might
not be attested in even an extremely large and well-
balanced corpus. Its absence might be motivated
by a number of factors: pure chance, the fact that
the expression is ungrammatical, uses a rare struc-
ture, describes false facts, or, finally, is nonsensi-
cal. One criticism from generative linguists is pre-
cisely that statistical methods could not distinguish
between these various possibilities.
The difficulty of solving this problem can be il-
lustrated by the difference in semantics between the
adjective-noun pairs in (1a) and (1b):
(1) a. blue rose
b. residential steak
Although it may be the case that you have never ac-
1
tually seen a blue rose, the concept is not inconceiv-
able. On the other hand, the concept of a residen-
tial steak is rather unimaginable, and intuitively its
absence in a corpus is motivated by more than just
chance or data sparseness.
The present paper is a first attempt to use com-
positionality and distributional measures to distin-
guish nonsensical, or semantically deviant, linguis-
tic expression from other types of unattested struc-
tures. The task of distinguishing between unattested
but acceptable and unattested but semantically de-
viant linguistic expressions is not only a way to ad-
dress the criticism about the meaning of ?unattest-
edness?, but also a task that could have a large im-
pact on the (computational) linguistic community as
a whole (see Section 2.1).
Our specific goal is to automatically detect se-
mantic deviance in attributive Adjective-Noun (AN)
expressions, using a small number of simple cues in
the vectorial representation of an AN as it is gener-
ated from the distributional vectors of its component
A and N by four compositional models found in the
literature. The choice of AN as our testbed is moti-
vated by two facts: first of all, ANs are common,
small constituents containing no functional mate-
rial, and secondly, ANs have already been studied in
compositional distributional semantics (Baroni and
Zamparelli, 2010; Guevara, 2010; Mitchell and La-
pata, 2010).
It is important to note that in this research we talk
about ?semantically deviant? expressions, but we do
not exclude the possibility that such expressions are
interpreted as metaphors, via a chain of associations.
In fact, distributional measures are desirable models
to account for this, since they naturally lead to a gra-
dient notion of semantic anomaly.
The rest of this paper is structured as follows.
Section 2 discusses relevant earlier work, introduc-
ing the literature on semantic deviance as well as
compositional methods in distributional semantics.
Section 3 presents some hypotheses about cues of
semantic deviance in distributional space. Our ex-
perimental setup and procedure are detailed in Sec-
tion 4, whereas the experiments? results are pre-
sented and analyzed in Section 5. We conclude by
summarizing and proposing future directions in Sec-
tion 6.
2 Related work
2.1 Semantic deviance
As far as we know, we are the first to try to model
semantic deviance using distributional methods, but
the issue of when a complex linguistic expression is
semantically deviant has been addressed since the
1950?s in various areas of linguistics. In compu-
tational linguistics, the possibility of detecting se-
mantic deviance has been seen as a prerequisite to
access metaphorical/non-literal semantic interpreta-
tions (Fass and Wilks, 1983; Zhou et al, 2007). In
psycholinguistics, it has been part of a wide debate
on the point at which context can make us perceive a
?literal? vs. a ?figurative? meaning (Giora, 2002). In
theoretical generative linguistics, the issue was orig-
inally part of a discussion on the boundaries between
syntax and semantics. Cases like Chomsky?s clas-
sic ?Colorless green ideas sleep furiously? can actu-
ally be regarded as violations of very fine-grained
syntactic selectional restrictions on the arguments
of verbs or modifiers, on the model of *much com-
puter (arguably a failure of much to combine with a
noun +COUNT). By 1977, even Chomsky doubted
that speakers could in general have intuitions about
whether ill-formedness was syntactic or semantic
(Chomsky, 1977, p. 4). The spirit of the selectional
approach persists in Asher (2011), who proposes a
detailed system of semantic types plus a theory of
type coercion, designed to account for the shift in
meaning seen in, e.g., (2) (lunch as food or as an
event).
(2) Lunch was delicious but took forever.
A practical problem with this approach is that a
full handmade specification of the features that de-
termine semantic compatibility is a very expensive
and time-consuming enterprise, and it should be
done consistently across the whole content lexicon.
Moreover, it is unclear how to model the intuition
that naval fraction, musical North or institutional
acid sound odd, in the absence of very particular
contexts, while (2) sounds quite natural. Whatever
the nature of coercion, we do not want it to run so
smoothly that any combination of A and N (or V and
its arguments) becomes meaningful and completely
acceptable.
2
2.2 Distributional approaches to meaning
composition
Although the issue of how to compose meaning has
attracted interest since the early days of distribu-
tional semantics (Landauer and Dumais, 1997), re-
cently a very general framework for modeling com-
positionality has been proposed by Mitchell and La-
pata (Mitchell and Lapata, 2008; Mitchell and La-
pata, 2009; Mitchell and Lapata, 2010). Given two
vectors u and v, they identify two general classes of
composition models, (linear) additive models:
p = Au + Bv (1)
where A and B are weight matrices, and multiplica-
tive models:
p = Cuv
where C is a weight tensor projecting the uv ten-
sor product onto the space of p. Mitchell and La-
pata derive two simplified models from these gen-
eral forms: The simplified additive model given by
p = ?u + ?v, and a simplified multiplicative ap-
proach that reduces to component-wise multiplica-
tion, where the i-th component of the composed vec-
tor is given by: pi = uivi. Mitchell and Lapata
evaluate the simplified models on a wide range of
tasks ranging from paraphrasing to statistical lan-
guage modeling to predicting similarity intuitions.
Both simple models fare quite well across tasks
and alternative semantic representations, also when
compared to more complex methods derived from
the equations above. Given their overall simplic-
ity, good performance and the fact that they have
also been extensively tested in other studies (Baroni
and Zamparelli, 2010; Erk and Pado?, 2008; Guevara,
2010; Kintsch, 2001; Landauer and Dumais, 1997),
we re-implement here both the simplified additive
and simplified multiplicative methods (we do not,
however, attempt to tune the weights of the additive
model, although we do apply a scalar normalization
constant to the adjective and noun vectors).
Mitchell and Lapata (as well as earlier re-
searchers) do not exploit corpus evidence about
the p vectors that result from composition, despite
the fact that it is straightforward (at least for short
constructions) to extract direct distributional evi-
dence about the composite items from the corpus
(just collect co-occurrence information for the com-
posite item from windows around the contexts in
which it occurs). The main innovation of Guevara
(2010), who focuses on adjective-noun combina-
tions (AN), is to use the co-occurrence vectors of
corpus-observed ANs to train a supervised compo-
sition model. Guevara, whose approach we also re-
implement here, adopts the full additive composi-
tion form from Equation (1) and he estimates the
A and B weights (concatenated into a single ma-
trix, that acts as a linear map from the space of con-
catenated adjective and noun vectors onto the AN
vector space) using partial least squares regression.
The training data are pairs of adjective-noun vec-
tor concatenations, as input, and corpus-derived AN
vectors, as output. Guevara compares his model
to the simplified additive and multiplicative models
of Mitchell and Lapata. Corpus-observed ANs are
nearer, in the space of observed and predicted test
set ANs, to the ANs generated by his model than
to those from the alternative approaches. The addi-
tive model, on the other hand, is best in terms of
shared neighbor count between observed and pre-
dicted ANs.
The final approach we re-implement is the one
proposed by Baroni and Zamparelli (2010), who
treat attributive adjectives as functions from noun
meanings to noun meanings. This is a standard ap-
proach in Montague semantics (Thomason, 1974),
except noun meanings here are distributional vec-
tors, not denotations, and adjectives are (linear)
functions learned from a large corpus. Unlike in
Guevara?s approach, a separate matrix is generated
for each adjective using only examples of ANs con-
taining that adjective, and no adjective vector is
used: the adjective is represented entirely by the ma-
trix mapping nouns to ANs. In terms of Mitchell
and Lapata?s general framework, this approach de-
rives from the additive form in Equation (1) with the
matrix multiplying the adjective vector (say, A) set
to 0, the other matrix (B) representing the adjective
at hand, and v a noun vector. Baroni and Zamparelli
(2010) show that their model significantly outper-
forms other vector composition methods, including
addition, multiplication and Guevara?s approach, in
the task of approximating the correct vectors for pre-
viously unseen (but corpus-attested) ANs. Simple
addition emerges as the second best model.
3
See Section 4.3 below for details on our re-
implementations. Note that they follow very closely
the procedure of Baroni and Zamparelli (2010), in-
cluding choices of source corpus and parameter val-
ues, so that we expect their results on the quality of
the various models in predicting ANs to also hold
for our re-implementations.
3 Simple indices of semantic deviance
We consider here a few simple, unsupervised mea-
sures to help us distinguish the representation that a
distributional composition model generates for a se-
mantically anomalous AN from the one it generates
for a semantically acceptable AN. In both cases, we
assume that the AN is not already part of the model
semantic space, just like you can distinguish be-
tween parliamentary tomato (odd) and marble iPad
(OK), although you probably never heard either ex-
pression.
We hypothesize that, since the values in the di-
mensions of a semantic space are a distributional
proxy to the meaning of an expression, a mean-
ingless expression should in general have low val-
ues across the semantic space dimensions. For ex-
ample, a parliamentary tomato, no longer being a
vegetable but being an unlikely parliamentary event,
might have low values on both dimensions char-
acterizing vegetables and dimensions characterizing
events. Thus, our first simple measure of seman-
tic anomaly is the length of the model-generated
AN. We hypothesize that anomalous AN vectors are
shorter than acceptable ANs.
Second, if deviant composition destroys or ran-
domizes the meaning of a noun, as a side effect we
might expect the resulting AN to be more distant, in
the semantic space, from the component noun. Al-
though even a marble iPad might have lost some es-
sential properties of iPads (it could for example be
an iPad statue you cannot use as a tablet), to the ex-
tent that we can make sense of it, it must retain at
least some characteristics of iPads (at the very least,
it will be shaped like an iPad). On the other hand, we
cannot imagine what a parliamentary tomato should
be, and thus cannot attribute even a subset of the reg-
ular tomato properties to it. We thus hypothesize that
model-generated vectors of deviant ANs will form
a wider angle (equivalently, will have a lower co-
sine) with the corresponding N vectors than accept-
able ANs.
Finally, if an AN makes no sense, its model-
generated vector should not have many neighbours
in the semantic space, since our semantic space is
populated by nouns, adjectives and ANs that are
commonly encountered in the corpus, and should
thus be meaningful. We expect deviant ANs to
be ?semantically isolated?, a notion that we opera-
tionalize in terms of a (neighborhood) density mea-
sure, namely the average cosine with the (top 10)
nearest neighbours. We hypothesize that model-
generated vectors of deviant ANs will have lower
density than model-generated acceptable ANs.
4 Experimental setup
4.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row vector represents an adjective, noun or AN. We
first introduce the source corpus, then the vocabulary
of words and ANs that we represent in the space,
and finally the procedure adopted to build the vec-
tors representing the vocabulary items from corpus
statistics, in order to obtain the semantic space ma-
trix. We work here with a ?vanilla? semantic space
(essentially, we follow the steps of Baroni and Zam-
parelli (2010)), since our focus is on the effect of
different composition methods given a common se-
mantic space. We leave it to further work to study
how choices in semantic space construction affect
composition operations.
4.1.1 Source corpus
We use as our source corpus the concate-
nation of the Web-derived ukWaC corpus
(http://wacky.sslmit.unibo.it/),
a mid-2009 dump of the English Wikipedia
(http://en.wikipedia.org) and the British
National Corpus (http://www.natcorp.
ox.ac.uk/). The corpus has been tokenized,
POS-tagged and lemmatized with the TreeTagger
(Schmid, 1995), and it contains about 2.8 billion
tokens. We extract all statistics at the lemma level,
ignoring inflectional information.
4
4.1.2 Semantic space vocabulary
The words/ANs in the semantic space must of
course include the items that we need for our exper-
iments (adjectives, nouns and ANs used for model
training and as input to composition). Moreover, in
order to study the behaviour of the test items we are
interested in (that is, model-generated AN vectors)
within a large and less ad-hoc space, we also include
many more adjectives, nouns and ANs in our vocab-
ulary not directly relevant to our experimental ma-
nipulations.
We populate our semantic space with the 8K most
frequent nouns and 4K most frequent adjectives
from the corpus (excluding, in both cases, the top
50 most frequent elements). We extended this vo-
cabulary to include two sets of ANs (33K ANs cu-
mulatively), for a total of 45K vocabulary items in
the semantic space.
To create the ANs needed to run and evaluate the
experiments described below, we focused on a set
of adjectives which are very frequent in the corpus
so that they will be in general able to combine with
wide classes of nouns, making the unattested cases
more interesting, but not so frequent as to have such
a general meaning that would permit a free combi-
nation with nearly any noun. The ANs were there-
fore generated by crossing a selected set of 200 very
frequent adjectives (adjectives attested in the corpus
at least 47K times, and at most 740K) and the set
of the 8K nouns in our semantic space vocabulary,
producing a set of 4.92M generated ANs.
The first set of ANs included in the semantic
space vocabulary is a randomly sampled set of 30K
ANs from the generated set which are attested in
the corpus at least 200 times (to avoid noise and fo-
cus on ANs for which we can extract reasonably ro-
bust distributional data). We also extracted any unat-
tested ANs from the set of generated set (about 3.5M
unattested ANs), putting them aside to later assem-
ble our evaluation material, described in Section 4.2.
To add further variety to the semantic space, we
included a less controlled second set of 3K ANs ran-
domly picked among those that are attested and are
formed by the combination of any of the 4K adjec-
tives and 8K nouns in the vocabulary.
4.1.3 Semantic space construction
For each of the items in our vocabulary, we first
build 10K-dimensional vectors by recording their
sentence-internal co-occurrence with the top 10K
most frequent content words (nouns, adjectives or
verbs) in the corpus. The raw co-occurrence counts
are then transformed into Local Mutual Information
scores (Local Mutual Information is an association
measure that closely approximates the commonly
used Log-Likelihood Ratio while being simpler to
compute (Baroni and Lenci, 2010; Evert, 2005)).
Next, we reduce the full co-occurrence matrix
applying the Singular Value Decomposition (SVD)
operation, like in LSA and related distributional
semantic methods (Landauer and Dumais, 1997;
Rapp, 2003; Schu?tze, 1997). The original 45K-by-
10K-dimensional matrix is reduced in this way to a
45K-by-300 matrix, where vocabulary items are rep-
resented by their coordinates in the space spanned
by the first 300 right singular vectors of the SVD
solution. This step is motivated by the fact that we
will estimate linear models to predict the values of
each dimension of an AN from the dimensions of the
components. We thus prefer to work in a smaller and
denser space. As a sanity check, we verify that we
obtain state-of-the-art-range results on various se-
mantic tasks using this reduced semantic space (not
reported here for space reason).
4.2 Evaluation materials
Our goal is to study what happens when composi-
tional methods are used to construct a distributional
representation for ANs that are semantically deviant,
compared to the AN representations they generate
for ANs they have not encountered before, but that
are semantically acceptable.
In order to assemble these lists, we started from
the set of 3.5M unattested ANs described in Sec-
tion 4.1.2 above, focusing on 30 randomly chosen
adjectives. For each of these, we randomly picked
100 ANs for manual inspection (3K ANs in total).
Two authors went through this list, marking those
ANs that they found semantically highly anomalous,
no matter how much effort one would put in con-
structing metaphorical or context-dependent inter-
pretations, as well as those they found completely
acceptable (so, rating was on a 3-way scale: deviant,
5
intermediate, acceptable). The rating exercise re-
sulted in rather low agreement (Cohen?s ?=0.32),
but we reasoned that those relatively few cases (456
over 3K) where both judges agreed the AN was odd
should indeed be odd, and similarly for the even
rarer cases in which they agreed an AN was com-
pletely acceptable (334 over 3K). We thus used the
agreed deviant and acceptable ANs as test data.
Of 30 adjectives, 5 were discarded for either tech-
nical reasons or for having less than 5 agreed de-
viant or acceptable ANs. This left us with a de-
viant AN test set comprising of 413 ANs, on av-
erage 16 for each of the 25 remaining adjectives.
Some examples of ANs in this set are: academic
bladder, blind pronunciation, parliamentary potato
and sharp glue. The acceptable (but unattested) AN
test set contains 280 ANs, on average 11 for each of
the 25 studied adjectives. Examples of ANs in this
set include: vulnerable gunman, huge joystick, aca-
demic crusade and blind cook. The evaluation sets
can be downloaded from http://www.vecchi.
com/eva/resources.html.
There is no significant difference between the
length of the vectors of the component nouns in the
acceptable vs. deviant AN sets (two-tailed Welch?s t
test; t=?0.25; p>0.8). This is important, since at
least one of the potential cues to deviance we con-
sider (AN vector length) is length-dependent, and
we do not want a trivial result that can simply be
explained by systematic differences in the length of
the input vectors.
4.3 Composition methods
As discussed in Section 2.2, the experiment was car-
ried out across four compositional methods.
Additive AN vectors (add method) are simply
obtained by summing the corresponding adjective
and noun vectors after normalizing them. Multi-
plicative vectors (mult method) were obtained by
component-wise multiplication of the adjective and
noun vectors, also after normalization. Confirm-
ing the results of Baroni and Zamparelli (2010),
non-normalized versions of add and mult were also
tested, but did not produce significant results (in
the case of multiplication, normalization amounts to
multiplying the composite vector by a scalar, so it
only affects the length-dependent vector length mea-
sure). It is important to note that, as reported in
Baroni and Zamparelli (2010), the mult method can
be expected to perform better in the original, non-
reduced semantic space because the SVD dimen-
sions can have negative values, leading to counter-
intuitive results with component-wise multiplication
(multiplying large opposite-sign values results in
large negative values instead of being cancelled out).
The tests of Section 5, however, are each run in the
SVD-reduced space to remain consistent across all
models. We leave it to future work to explore the
effect on the performance of using the non-reduced
space for the models for which this option is com-
putationally viable.
In the linear map (lm) approach proposed by
Guevara (2010), a composite AN vector is obtained
by multiplying a weight matrix by the concatenation
of the adjective and noun vectors, so that each di-
mension of the generated AN vector is a linear com-
bination of dimensions of the corresponding adjec-
tive and noun vectors. That is, the 600 weights in
each of the 300 rows of the weight matrix are the
coefficients of a linear equation predicting the val-
ues of a single dimension in the AN vector as a lin-
ear combination (weighted sum) of the 300 adjective
and 300 noun dimensions. Following Guevara, we
estimate the coefficients of the equation using (mul-
tivariate) partial least squares regression (PLSR) as
implemented in the R pls package (Mevik and
Wehrens, 2007), with the latent dimension param-
eter of PLSR set to 50, the same value used by Ba-
roni and Zamparelli (2010). Coefficient matrix es-
timation is performed by feeding the PLSR a set
of input-output examples, where the input is given
by concatenated adjective and noun vectors, and the
output is the vector of the corresponding AN directly
extracted from our semantic space (i.e., the AN vec-
tors used in training are not model-generated, but
directly derived from corpus evidence about their
distribution). The matrix is estimated using a ran-
dom sample of 2K adjective-noun-AN tuples where
the AN belongs to the set of 30K frequently attested
ANs in our vocabulary.
Finally, in the adjective-specific linear map
(alm) method of Baroni and Zamparelli (2010), an
AN is generated by multiplying an adjective weight
matrix with a noun vector. The weights of each of
the 300 rows of the weight matrix are the coefficients
of a linear equation predicting the values of one of
6
the dimensions of the AN vector as a linear com-
bination of the 300 dimensions of the component
noun. The linear equation coefficients are estimated
separately for each of the 25 tested adjectives from
the attested noun-AN pairs containing that adjective
(observed adjective vectors are not used), again us-
ing PLSR with the same parameter as above. For
each adjective, the training N-AN vector pairs cho-
sen are those available in the semantic space for each
test set adjective, and range from 100 to more than
500 items across the 25 adjectives.
4.4 Experimental procedure
Using each composition method, we generate com-
posite vectors for all the ANs in the two (acceptable
and deviant) evaluation sets (see Section 4.2 above).
We then compute the measures that might cue se-
mantic deviance discussed in Section 3 above, and
compare their values between the two AN sets. In
order to smooth out adjective-specific effects, we z-
normalize the values of each measure across all the
ANs sharing an adjective before computing global
statistics (i.e., the values for all ANs sharing an ad-
jective from the two sets are transformed by sub-
tracting their mean and dividing by their variance).
We then compare the two sets, for each composition
method and deviance cue, by means of two-tailed
Welch?s t tests. We report the estimated t score,
that is, the standardized difference between the mean
acceptable and deviant AN values, with the corre-
sponding significance level. For all our cues, we
predict t to be significantly larger than 0: Accept-
able AN vectors should be longer than deviant ones,
they should be nearer ? that is, have a higher cosine
with ? the component N vectors and their neighbour-
hood should be denser ? that is, the average cosines
with their top neighbours should be higher than the
ones of deviant ANs with their top neighbours.
5 Results
The results of our experiments are summarized in
Table 1. We see that add and mult provide signif-
icant results in the expected direction for 2 over 3
cues, only failing the cosine test. With the lm model,
acceptable and deviant ANs are indistinguishable
across the board, whereas alm captures the distinc-
tion in terms of density.
LENGTH COSINE DENSITY
method t sig. t sig. t sig.
add 7.89 * 0.31 2.63 *
mult 3.16 * -0.56 2.68 *
lm 0.16 0.55 -0.23
alm 0.48 1.37 3.12 *
Table 1: t scores for difference between acceptable and
deviant ANs with respect to 3 cues of deviance: length
of the AN vector, cosine of the AN vector with the com-
ponent noun vector and density, measured as the average
cosine of an AN vector with its nearest 10 neighbours in
semantic space. For all significant results, p<0.01.
The high scores in the vector length analyses of
both the addition and the multiplication models are
an indication that semantically acceptable ANs tend
to be composed of similar adjectives and nouns, i.e.,
those which occur in similar contexts and we can as-
sume are likely to belong to the same domain, which
sounds plausible.
In Baroni and Zamparelli (2010), the alm model
performed far better than add and mult in approxi-
mating the correct vectors for unseen ANs, while on
this (in a sense, more metalinguistic) task add and
mult work better, while alm is successful only in the
more sophisticated measure of neighbor density.
The lack of significant results for the cosine mea-
sure is disappointing, but not entirely surprising. A
large angle between N and AN might be a feature of
impossible ANs common to various types of pos-
sible ANs: idioms (a red herring is probably far
from herring in semantic space), non-subsective ad-
jectives (stone lion vs. lion; fake butterfly vs. but-
terfly), plus some metaphorical constructions (aca-
demic crusade vs. crusade?one of several ANs
judged acceptable in our study, which can only be
taken as metaphors). Recall, finally, that the vector
for the base N collapses together all the meanings
of an ambiguous N. The adjective might have a dis-
ambiguating effect which would increase the cosine
distance.
To gain a better understanding of the neighbor-
hood density test we performed a detailed analysis
of the nearest neighbors of the AN vectors generated
by the three models in which the difference in neigh-
bor distance was significant across deviant and ac-
ceptable ANs: alm, multiplication and addition. For
7
each of the ANs, we looked at the top 10 semantic-
space neighbors generated by each of the three mod-
els, focusing on two aspects: whether the neighbor
was a single A or N, rather than AN, and whether
the neighbor contained the same A or N as the AN
is was the neighbor of (as in blind regatta / blind
athlete or biological derivative / partial derivative).
The results are summarized in Table 2.
method status A N A1= N1=
only only A2 N2
add
accept 11.9 8.7 14.6 2.4
deviant 12.5 6.8 14.6 2.3
mult
accept 6.9 8.0 0.7 0.1
deviant 2.7 7.3 0.5 0.1
alm
accept 4.9 17.7 7.0 0.0
deviant 7.1 19.6 6.2 0.0
Table 2: Percentage distributions of various properties of
the top 10 neighbours of ANs in the acceptable (2800)
and deviant (4130) sets for add, mult and alm. The last
two columns express whether the neighbor contains the
same Adjective or Noun as the target AN.
In terms of the properties we measured, neighbor
distributions are quite similar across acceptable and
deviant ANs. One interesting finding is that the sys-
tem is quite ?adjective-driven?: particularly for the
additive model (where we can imagine that some Ns
with low dimensional values do not shift much the
adjective position in the multidimensional space),
less so in the alm method, and not at all for mult. To
put the third and forth columns in context, the subset
of the semantic space used to generate the SVD from
which the neighbors are drawn contained 2.69% ad-
jectives, 5.24% nouns and 92.07% ANs. With re-
spect to the last two columns, it is interesting to ob-
serve that matching As are frequent for deviant ANs
even in alm, a model which has never seen A-vectors
during training. Further qualitative evaluations show
that in many deviant AN cases the similarity is be-
tween the A in the target AN and the N of the neigh-
bor (e.g. academic bladder / honorary lectureship),
while the opposite effect seems to be much harder to
find.
6 Conclusion and future work
The main aim of this paper was to propose a new
challenge to the computational distributional seman-
tics community, namely that of characterizing what
happens, distributionally, when composition leads
to semantically anomalous composite expressions.
The hope is, on the one hand, to bring further sup-
port to the distributional approach by showing that it
can be both productive and constrained; and on the
other, to provide a more general characterization of
the somewhat elusive notion of semantic deviance ?
a notion that the field of formal semantics acknowl-
edges but might lack the right tools to model.
Our results are very preliminary, but also very en-
couraging, suggesting that simple unsupervised cues
can significantly tell unattested but acceptable ANs
apart from impossible, or at least deviant, ones. Al-
though, somewhat disappointingly, the model that
has been shown in a previous study (Baroni and
Zamparelli, 2010) to be the best at capturing the se-
mantics of well-formed ANs turns out to be worse
than simple addition and multiplication.
Future avenues of research must include, first of
all, an exploration on the effect on each model when
tested in the non-reduced space where computation-
ally possible, or using different dimensionality re-
duction methods. A preliminary study demonstrates
an enhanced performance of the mult method in the
full space.
Second, we hope to provide a larger benchmark
of acceptable and deviant ANs, beyond the few hun-
dreds we used here, and sampling a larger typology
of ANs across frequency ranges and adjective and
noun classes. To this extent, we are implementing
a crowd-sourcing study to collect human judgments
from a large pool of speakers on a much larger set of
ANs unattested in the corpus. Averaging over mul-
tiple judgments, we will also be able to characterize
semantic deviance as a gradient property, probably
more accurately.
Next, the range of cues we used was quite limited,
and we intend to extend the range to include more
sophisticated methods such as 1) combining multi-
ple cues in a single score; 2) training a supervised
classifier from labeled acceptable and deviant ANs,
and studying the most distinctive features discov-
ered by the classifier; 3) trying more complex unsu-
pervised techniques, such as using graph-theoretical
methods to characterize the semantic neighborhood
of ANs beyond our simple density measure.
Finally, we are currently not attempting a typol-
8
ogy of deviant ANs. We do not distinguish cases
such as parliamentary tomato, where the adjective
does not apply to the conceptual semantic type of
the noun (or at least, where it is completely undeter-
mined which relation could bridge the two objects),
from oxymorons such as dry water, or vacuously
redundant ANs (liquid water) and so on. We real-
ize that, at a more advanced stage of the analysis,
some of these categories might need to be explicitly
distinguished (for example, liquid water is odd but
perfectly meaningful), leading to a multi-way task.
Similarly, among acceptable ANs, there are spe-
cial classes of expressions, such as idiomatic con-
structions, metaphors or other rhetorical figures, that
might be particularly difficult to distinguish from
deviant ANs. Again, more cogent tasks involving
such well-formed but non-literal constructions (be-
yond the examples that ended up by chance in our
acceptable set) are left to future work.
Acknowledgments
We thank Raffaella Bernardi, Gemma Boleda,
Louise McNally and the anonymous reviewers for
their advice and comments.
References
Nicholas Asher. 2011. Lexical Meaning in Context: A
Web of Words. Cambridge University Press.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Noam Chomsky. 1957. Syntactic Structures. Mouton.
Noam Chomsky. 1977. Essays on Form and Interpreta-
tion. North Holland, New York.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP, pages 897?906, Honolulu, HI,
USA.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Dan Fass and Yorick Wilks. 1983. Preference seman-
tics, ill-formedness, and metaphor. Computational
Linguistics, 9:178?187.
Rachel Giora. 2002. Literal vs. figurative language: Dif-
ferent or equal? Journal of Pragmatics, 34:487?506.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Bjo?rn-Helge Mevik and Ron Wehrens. 2007. The
pls package: Principal component and partial least
squares regression in R. Journal of Statistical Soft-
ware, 18(2). Published online: http://www.
jstatsoft.org/v18/i02/.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH, USA.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA,
USA.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Richmond H Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Chang-Le Zhou, Yun Yang, and Xiao-Xi Huang. 2007.
Computational mechanisms for metaphor in lan-
guages: a survey. Journal of Computer Science and
Technology, 22:308?319.
9
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 1?10,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
How we BLESSed distributional semantic evaluation
Marco Baroni
University of Trento
Trento, Italy
marco.baroni@unitn.it
Alessandro Lenci
University of Pisa
Pisa, Italy
alessandro.lenci@ling.unipi.it
Abstract
We introduce BLESS, a data set specifically
designed for the evaluation of distributional
semantic models. BLESS contains a set of tu-
ples instantiating different, explicitly typed se-
mantic relations, plus a number of controlled
random tuples. It is thus possible to assess the
ability of a model to detect truly related word
pairs, as well as to perform in-depth analy-
ses of the types of semantic relations that a
model favors. We discuss the motivations for
BLESS, describe its construction and struc-
ture, and present examples of its usage in the
evaluation of distributional semantic models.
1 Introduction
In NLP, it is customary to distinguish between in-
trinsic evaluations, testing a system in itself, and
extrinsic evaluations, measuring its performance in
some task or application (Sparck Jones and Galliers,
1996). For instance, the intrinsic evaluation of a de-
pendency parser will measure its accuracy in identi-
fying specific syntactic relations, while its extrinsic
evaluation will focus on the impact of the parser on
tasks such as question answering or machine trans-
lation. Current approaches to the evaluation of Dis-
tributional Semantic Models (DSMs, also known
as semantic spaces, vector-space models, etc.; see
Turney and Pantel (2010) for a survey) are task-
oriented. Model performance is evaluated in ?se-
mantic tasks?, such as detecting synonyms, recog-
nizing analogies, modeling verb selectional prefer-
ences, ranking paraphrases, etc. Measuring the per-
formance of DSMs on such tasks represents an in-
direct test of their ability to capture lexical mean-
ing. The task-oriented benchmarks adopted in dis-
tributional semantics have not specifically been de-
signed to evaluate DSMs. For instance, the widely
used TOEFL synonym detection task was designed
to test the learners? proficiency in English as a sec-
ond language, and not to investigate the structure of
their semantic representations (cf. Section 2).
To gain a real insight into the abilities of DSMs to
address lexical semantics, existing benchmarks must
be complemented with a more intrinsically oriented
approach, to perform direct tests on the specific as-
pects of lexical knowledge captured by the models.
In order to achieve this goal, three conditions must
be met: (i) to single out the particular aspects of
meaning that we want to focus on in the evaluation
of DSMs; (ii) to design a data set that is able to ex-
plicitly and reliably encode the target semantic infor-
mation; (iii) to specify the evaluation criteria of the
system performance on the data set, in order to get
an estimate of the intrinsic ability of DSMs to cope
with the selected semantic aspects. In this paper, we
address these three conditions by presenting BLESS
(Baroni and Lenci Evaluation of Semantic Spaces),
a new data set specifically geared towards the in-
trinsic evaluation of DSMs, downloadable from:
http://clic.cimec.unitn.it/distsem.
2 Distributional semantics benchmarks
There are several benchmarks that have been widely
adopted for the evaluation of DSMs, all of them cap-
turing interesting challenges a DSM should meet.
We briefly review here some commonly used and
representative benchmarks, and discuss why we felt
1
the need to add BLESS to the set. We notice at the
outset of this discussion that we want to carve out a
space for BLESS, and not to detract from the impor-
tance and usefulness of other data sets. We further
remark that we focus on data sets that, like BLESS,
are monolingual English and, while task-oriented,
not aimed at a specific application setting (such as
machine translation or ontology population).
Probably the most commonly used benchmark in
distributional semantics is the TOEFL synonym de-
tection task introduced to computational linguis-
tics by Landauer and Dumais (1997). It consists of
80 multiple-choice questions, each made of a target
word (a noun, verb, adjective or adverb) and 4 re-
sponse words, 1 of them a synonym of the target.
For example, given the target levied, the matched
words are imposed, believed, requested, correlated,
the first one being the correct choice. The task for
a system is then to pick the true synonym among
the responses. The TOEFL task focuses on a single
semantic relation, namely synonymy. Synonymy is
actually not a common semantic relation and one of
the hardest to define, to the point that many lexi-
cal semanticists have concluded that true synonymy
does not exist (Cruse, 1986). Just looking at a few
examples of synonym pairs from the TOEFL set will
illustrate the problem: discrepancy/difference, pro-
lific/productive, percentage/proportion, to market/to
sell, color/hue. Moreover, the criteria adopted to
choose the distractors (probably motivated by the
language proficiency testing purposes of TOEFL)
are not known. By looking at the set, it is hard
to discern a coherent pattern. In certain cases, the
distractors are semantically close to the target word
(volume, sample and profit for percentage), whereas
in other cases they are not (home, trail, and song for
annals). It it thus not clear whether we are asking the
models to distinguish a semantically related word
(the synonym) from random elements, or a more
tightly related word (the synonym, again) from other
related words. The TOEFL task, finally, is based on
a discrete choice (either you get the right word, or
you don?t), with the result that evaluation is ?quan-
tized?, leading to large accuracy gains for small ac-
tual differences (one model that guesses one more
synonym right than another gets 1.25% more points
in percentage accuracy).
The WordSim 353 data set (Finkelstein et al,
2002) is a widely used example of semantic simi-
larity rating set (see also Rubenstein and Goode-
nough (1965) and Miller and Charles (1991)). Sub-
jects were asked to rate a set of 353 word pairs on a
?similarity? scale and average ratings for each pair
were computed. Models are then evaluated in terms
of correlation of their similarity scores with aver-
age ratings across pairs. From the point of view
of assessing the performance of a DSM, the Word-
Sim (and related) similarity ratings are a mixed bag,
in two senses. First, the data set contains a vari-
ety of different semantic relations. In a recent se-
mantic annotation of the WordSim performed by
Agirre et al (2009) we find that, among the 174
pairs with above-median score (and thus presum-
ably related), there is 1 identical pair, 17 synonym
pairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,
6 holo-/meronym pairs and 92 (more than half) pairs
that are ?topically related, but none of the above?.
Second, the scores are a mixture of intuitions about
which of these relations are more semantically tight
and intuitions about more or less connected pairs
within each of the relations. For example, among
the top-rated scores we find synonyms such as jour-
ney/voyage and coordinate concepts (king/queen).
If we look at the relations characterizing pairs
around the median rating, we find both less ?per-
fect? synonyms (monk/brother, that are synonymous
only under an unusual sense of brother) and less
close coordinates (skin/eye), as well as pairs in-
stantiating other, less taxonomically tight relations,
such as many syntagmatically connected items (fam-
ily/planning, disaster/area, bread/butter). Appar-
ently, a single scale is merging intuitions about se-
mantic similarity of specific pairs and semantic sim-
ilarity of different relations.
A perhaps more principled way to evaluate DSMs
that has recently gained some popularity is the con-
cept categorization task, where a DSM has to clus-
ter a set of nouns expressing basic-level concepts
into gold standard categories. A particularly care-
fully constructed example is the Almuhareb-Poesio
(AP) set of 402 concepts introduced in Almuhareb
(2006). Concept categorization sets also include the
Battig (Baroni et al, 2010) and ESSLLI 2008 (Ba-
roni et al, 2008) lists. The AP concepts must be
clustered into 21 classes, each represented by be-
tween 13 and 21 nouns. Examples include the ve-
2
hicle class (helicopter, motorcycle. . . ), the motiva-
tion class (ethics, incitement, . . . ), and the social
unit class (platoon, branch). The concepts are bal-
anced in terms of frequency and ambiguity, so that,
e.g., the tree class contains a common concept such
as pine but also the casuarina tree, as well as the
samba tree, that is not only an ambiguous term, but
one where the non-arboreal sense dominates.
Concept categorization data sets, while interest-
ing to simulate one of the basic aspects of human
cognition, are limited to one kind of semantic re-
lation (discovering coordinates). More importantly,
the quality of the results will depend not only on the
underlying DSMs, but also on the clustering algo-
rithm being used (and on how this interacts with the
overall structure of the DSM), thus making it hard
to interpret the performance of DSMs. The forced
?hard? category choice is also problematic, and ex-
aggerates performance differences between models
especially in the presence of ambiguous terms (a
model that puts samba in the occasion class with
dance and ball might be penalized as much as a
model that puts it in the monetary currency class).
A more general issue with all benchmarks is that
tasks are based on comparing a single quality score
for each considered model (accuracy for TOEFL,
correlation for WordSim, a clustering quality mea-
sure for AP, etc.). This gives little insight into how
and why the models differ. Moreover, there is no
well-established statistical procedure to assess sig-
nificance of differences for most commonly used
measures. Finally, either because the data sets were
not originally intended as standard benchmarks, or
even on purpose, they all are likely to cause coverage
problems even for DSMs trained on very large cor-
pora. Think of the presence of extremely rare nouns
like casuarina in AP, of proper nouns inWordSim (it
is not clear to us that DSMs are adequate semantic
models for referring expressions ? at the very least
they should not be mixed up lightly with common
nouns), or multi-word expressions in other data sets.
3 How we intend to BLESS distributional
semantic evaluation
DSMs measure the distributional similarity between
words, under the assumption that proximity in distri-
butional space models semantic relatedness, includ-
ing, as a special case, semantic similarity (Budanit-
sky and Hirst, 2006). However, semantically related
words in turn differ for the type of relation hold-
ing between them: e.g., dog is strongly related to
both animal and tail, but with different types of re-
lations. Therefore, evaluating the intrinsic ability of
DSMs to represent the semantic space of a word en-
tails both (i) determining to what extent words close
in semantic space are actually semantically related,
and (ii) analyzing, among related words, which type
of semantic relation they tend to instantiate. Two
models can be equally very good in identifying se-
mantically related words, while greatly differing for
the type of related pairs they favor.
The BLESS data set complies with both these
constraints. The set is populated with tuples ex-
pressing a relation between a target concept (hence-
forth referred to as concept) and a relatum concept
(henceforth referred to as relatum). For instance, in
the BLESS tuple coyote-hyper-animal, the concept
coyote is linked to the relatum animal via the hy-
pernymy relation (the relatum is a hypernym of the
concept). BLESS focuses on a coherent set of basic-
level nominal concrete concepts and a small but ex-
plicit set of semantic relations, each instantiated by
multiple relata. Depending on the type of relation,
relata can be nouns, verbs or adjectives. Moreover,
BLESS also contains, for each concept, a number of
random ?relatum? words that are not semantically
related to the concept. Thus, it also allows to evalu-
ate a model in terms of its ability to harvest related
words given a concept (by comparing true and ran-
dom relata), and to identify specific types of relata,
both in terms of semantic relation and part of speech.
A data set intending to represent a gold standard
for evaluation should include tests items that are as
little controversial as possible. The choice of re-
stricting BLESS to concrete concepts is motivated
by the fact that they are by far the most studied ones,
and there is better agreement about the relations that
characterize them (Murphy, 2002; Rogers and Mc-
Clelland, 2004).
As for the types of relation to include, we are
faced with a dilemma. On the one hand, there is
wide evidence that taxonomic relations, the best un-
derstood type, only represent a tiny portion of the
rich spectrum covered by semantic relatedness. On
the other hand, most of these wider semantic rela-
3
tions are also highly controversial, and may easily
lead to questionable classifications. For instance,
concepts are related to events, but often it is not clear
how to distinguish the events expressing a typical
function of nominal concepts (e.g., car and trans-
port), from those events that are also strongly re-
lated to them but without representing their typical
function sensu stricto (e.g., car and fix). As will be
shown in Section 4, the BLESS data set tries to over-
come this dilemma by attempting a difficult com-
promise: Semantic relations are not limited to tax-
onomic types and also include attributes and events
strongly related to a concept, but in these cases we
have resorted to underspecification, rather than com-
mitting ourselves to questionable granular relations.
BLESS strives to capture those differences and
similarities among DSMs that do not depend on
coverage, processing choices or lexical preferences.
BLESS has been constructed using a publicly avail-
able collection of corpora for reference (see Section
4.4 below), which means that anybody can train a
DSM on the same data and be sure to have perfect
coverage (but this is not strictly necessary). For each
concept and relation, we pick a variety of relata (see
next section) in order to abstract away from inciden-
tal gaps of models or different lexical/topical prefer-
ences. For example, the concept robin has 7 hyper-
nyms including the very general and non-technical
animal and bird and the more specific and techni-
cal passerine. A model more geared toward techni-
cal terminology might assign a high similarity score
to the latter, whereas a commonsense-knowledge-
oriented DSM might pick bird. Both models have
captured similarity with a hypernym, and we have
no reason, in general semantic terms, to penalize one
or the other. To maximize coverage, we also make
sure that, for each concept and relation, a reason-
able number of relata are frequently attested in our
reference corpora (see statistics below), we only in-
clude single-word relata and, where appropriate, we
include multiple forms for the same relatum (both
sock and socks as coordinates of scarf ? as discussed
in Section 4.1, we avoided similar ambiguous items
as target concepts).
Currently, distributional models for attributional
similarity and relational similarity (Turney, 2006)
are tested on different data sets, e.g., TOEFL and
SAT respectively (briefly, attributional similarity
pertains to similarity between a pair of concepts in
terms of shared properties, whereas relational sim-
ilarity measures the similarity of the relations in-
stantiated by couples of concept pairs). Conversely,
BLESS is not biased towards any particular type of
semantic similarity and thus allows both families of
models to be evaluated on the same data set. Given
a concept, we can analyze the types of relata that are
selected by a model as more attributionally similar
to the target. Alternatively, given a concept-relatum
pair instantiating a specific semantic relation (e.g.,
hypernymy) we can evaluate a model ability to iden-
tify analogically similar pairs, i.e., others concept-
relatum pairs instantiating the same relation (we do
not illustrate this possibility here).
Finally, by collecting distributions of 200 similar-
ity values for each relation, BLESS allows reliable
statistical testing of the significance of differences
in similarity within a DSM (for example, using the
procedure we present in Section 5 below), as well
as across DSMs (for example, via a linear/ANOVA
model with relations and DSMs as factors ? not il-
lustrated here).
4 Construction
4.1 Concepts
BLESS includes 200 distinct English concrete
nouns as target concepts, equally divided be-
tween living and non-living entities. Concepts
have been grouped into 17 broader classes: AM-
PHIBIAN REPTILE (including amphibians and rep-
tiles: alligator), APPLIANCE (toaster), BIRD
(crow), BUILDING (cottage), CLOTHING (sweater),
CONTAINER (bottle), FRUIT (banana), FURNI-
TURE (chair), GROUND MAMMAL (beaver), IN-
SECT (cockroach), MUSICAL INSTRUMENT (vio-
lin), TOOL (i.e., manipulable tools or devices: ham-
mer), TREE (birch), VEGETABLE (cabbage), VEHI-
CLE (bus), WATER ANIMAL (including fish and sea
mammals: herring), WEAPON (dagger).
All 200 BLESS concepts are single-word nouns
in the singular form (we avoided concepts such as
socks whose surface form might change depending
on lemmatization choices). The major source we
used to select the concepts were the McRae Norms
(McRae et al, 2005), a collection of living and non-
living basic-level concepts described by 725 sub-
4
jects with semantic features, each tagged with its
property type. As further constraints guiding our
selection, we wanted concepts with a reasonably
high frequency (cf. Section 4.4), we avoided am-
biguous or highly polysemous concepts and we bal-
anced inter- and intra-class composition. Classes in-
clude both prototypical and atypical instances (e.g.,
robin and penguin for BIRD), and have a wide spec-
trum of internal variation (e.g., the class VEHICLE
contains wheeled, air and sea vehicles). 175 BLESS
concepts are attested in the McRae Norms, while the
remnants were selected by the authors according to
the above constraints. The average number of con-
cepts per class is 11.76 (median 11; min. 5 AMPHIB-
IAN REPTILE; max. 21 GROUND MAMMAL).
4.2 Relations
For each concept noun, BLESS includes several
relatum words, linked to the concept by one of
the following 5 relations. COORD: the relatum
is a noun that is a co-hyponym (coordinate) of
the concept, i.e., they belong to the same (nar-
rowly or broadly defined) semantic class: alligator-
coord-lizard; HYPER: the relatum is a noun that
is a hypernym of the concept: alligator-hyper-
animal; MERO: the relatum is a noun referring
to a part/component/organ/member of the concept,
or something that the concept contains or is made
of: alligator-mero-mouth; ATTRI: the relatum is
an adjective expressing an attribute of the concept:
alligator-attri-aquatic; EVENT: the relatum is a
verb referring to an action/activity/happening/event
the concept is involved in or is performed by/with
the concept: alligator-event-swim. BLESS also
includes the relations RAN.N, RAN.J and RAN.V,
which relate the target concepts to control tuples
with random noun, adjective and verb relata, respec-
tively.
The BLESS relations cover a wide spectrum of
information useful to describe a target concept and
to qualify the notion of semantic relatedness: taxo-
nomically related entities (hyper and coord), typical
attributes (attri), components (mero), and associated
events (event). However, except for hyper and co-
ord (corresponding to the standard relations of class
inclusion and co-hyponymy respectively), the other
BLESS relations are highly underspecified. For in-
stance, mero corresponds to a very broad notion of
meronymy, including not only parts (dog-tail), but
also the material (table-wood) as well as the mem-
bers (hospital-patient) of the entity the target con-
cept refers to (Winston et al, 1987); event is used to
represent the behaviors of animals (dog-bark), typi-
cal functions of instruments (violin-play), and events
that are simply associated with the target concept
(car-park); attri captures a large range of attributes,
from physical (elephant-big) to evaluative ones (car-
expensive). As we said in section 3, we did not at-
tempt to further specify these relations to avoid any
commitment to controversial ontologies of property
types. Note that we exclude synonymy both because
of the inherent problems in this very notion (Cruse,
1986), and because it is impossible to find convinc-
ing synonyms for 200 concrete concepts.
In BLESS, we have adopted the simplifying as-
sumption that each relation type has relata belonging
to the same part of speech: nouns for hyper, coord
and mero, verbs for event, and adjectives for attri.
Therefore, we abstract away from the fact that the
same semantic relation can be realized with different
parts of speech, e.g., a related event can be expressed
by a verb (transport) or by a noun (transportation).
4.3 Relata
The relata of the non-random relations are English
nouns, verbs and adjectives selected and validated
by both authors using two types of sources: se-
mantic sources (the McRae Norms (McRae et al,
2005), WordNet (Fellbaum, 1998) and ConceptNet
(Liu and Singh, 2004)) and text sources (Wikipedia
and the Web-derived ukWaC corpus, see Section 4.4
below). These resources greatly differ in dimension,
origin and content and therefore provide comple-
mentary views on relata. Their relative contribution
to BLESS also depends on the type of relation and
the target concept. For instance, the rich taxonomic
structure of WordNet has been the main source of in-
formation for many technical hypernyms (e.g. gym-
nosperm, oscine), which instead are missing from
more commonsense-oriented resources such as the
McRae Norms and ConceptNet. Meronyms are
rarer in WordNet, and were collected mainly from
the latter two resources, with many technical terms
(e.g., parts of ships, weapons) harvested from the
Wikipedia entries for the target concepts.
Attributes and events were collected from McRae
5
Norms, ConceptNet and ukWaC. In the McRae
Norms, the number of features per concept is fairly
limited, but they correspond to highly distinctive,
prototypical and cognitively salient properties. Con-
ceptNet instead provides a much wider array of as-
sociated events and attributes that are part of our
commonsense knowledge about the target concepts
(e.g., the events park, steal and break, etc. for car).
ConceptNet relations such as Created by, Used for,
Capable of etc. have been analyzed to identify po-
tential event relata, while the Has property relation
has been inspected to look for attributes. The most
salient adjectival and verbal collocates of the tar-
get nouns in the ukWaC corpus were also used to
identify associated attributes and events. For in-
stance, the target concept elephant is not attested in
the McRae Norms and has few properties in Con-
ceptNet. Thus, many of its related events have been
harvested from ukWaC. They include verbs such as
hunt, kill, etc. which are quite salient and frequent
with respect to elephants, although they can hardly
be defined as prototypical properties of this animal.
As a result of the combined use of such different
types of sources, the BLESS relata are representative
of a wide spectrum of semantic information about
the target concepts: they include domain-specific
terms side by side to commonsense ones, very dis-
tinctive features of a concept (e.g., hoot for owl)
together with attributes and events that are instead
shared by a whole class of concepts (e.g., all animals
have relata such as eat, feed, and live), prototypical
features as well as events and attributes that are sta-
tistically salient for the target, etc.
In many cases, the concept properties contained
in semantic sources are expressed with phrases, e.g.,
lay eggs, eat grass, live in Africa, etc. We decided,
however, to keep only single-word relata in BLESS,
because DSMs are typically populated with single
words, and, when they are not, they differ in the
kinds of multi-word elements they store. There-
fore, phrasal relata have always been reduced to
their head: a verb for properties expressed by a verb
phrase, and a noun for properties expressed by a
noun phrase. For instance, from the property lay
eggs, we derived the event relatum lay.
To extract the random relata, we adopted the fol-
lowing procedure. For each relatum that instantiates
a true relation with the concept, we also randomly
picked from our combined corpus (cf. Section 4.4)
another lemma with the same part of speech, and
frequency within 1 absolute logarithmic unit from
the frequency of the corresponding true relatum.
Since picking a random term does not guarantee
that it will not be related to the concept, we filtered
the extracted list by crowdsourcing, using the Ama-
zon Mechanical Turk via the CrowdFlower interface
(CF).1 We presented CF workers with the list of
about 15K concept+random-term pairs selected with
the procedure we just described, plus a manually
checked validation set (a ?gold set? in CF terminol-
ogy) comprised of 500 concept+true-relatum pairs
and 500 concept+random-term pairs (these elements
are used by CF to determine the reliability of work-
ers, and discard the ratings of unreliable ones), plus a
further set of 1.5K manually checked concept+true-
relatum pairs to make the random-true distribution
less skewed. The workers? task was, for each pair,
to check a YES radio button if they thought there is
a relation between the words, NO otherwise. The
words were annotated with their part of speech, and
workers were instructed to pay attention to this in-
formation when making their choices. Extensive
commented examples of both related pairs and un-
related ones were also provided in the instruction
page. A minimum of 2 CF workers rated each pair,
and, conservatively, we preserved only those items
(about 12K) that were unanimously rated as unre-
lated to their concept by the judges. See Table 1 for
summary statistics about the preserved random sets
(nouns: RAND.N, adjectives: RAN.J, verbs:RAN.V).
4.4 BLESS statistics
For frequency information, we rely on the combi-
nation of the freely available ukWaC and Wackype-
dia corpora (size: 1.915B and 820M tokens, respec-
tively).2 The data set contains 200 concepts that
have a mean corpus frequency of 53K occurrences
(min. 1416 chisel, max. 793K car). The relata of
these concepts (26,554 in total) are distributed as re-
ported in Table 1.
Note that the distributions reflect certain ?natural?
differences between relations (hypernyms tend to be
more frequent words than coordinates, but there are
1http://crowdflower.com/
2http://wacky.sslmit.unibo.it/
6
frequency cardinality
relation min avg max min avg max
COORD 0 37K 1.7M 6 17.1 35
HYPER 31 138K 1.9M 2 6.7 15
MERO 0 133K 2M 2 14.7 53
ATTRI 0 501K 3.7M 4 13.6 27
EVENT 0 517K 5.4M 6 19.1 40
RAN.N 0 92K 2.4M 16 32.9 67
RAN.J 1 472K 4.5M 3 10.9 24
RAN.V 1 508K 7.7M 4 16.3 34
Table 1: Distribution (minimum, mean and maximum) of
the relata of all BLESS concepts: the frequency columns
report summary statistics for corpus counts across relata
instantiating a relation; the cardinality columns report
summary statistics for number of relata instantiating a
relation across the 200 concepts, only considering relata
with corpus frequency ? 100.
more coordinates than hypernyms, etc.). Instead of
trying to artificially control for these differences, we
assess their impact in Section 5 by looking at the
behavior of baselines that exploit the frequency and
cardinality of relations as proxies to semantic simi-
larity (such factors could also be entered as regres-
sors in a linear model).
5 Evaluation
This section illustrates one possible way to use
BLESS to explore and evaluate DSMs. Given the
similarity scores provided by a model for a concept
with all its relata across all relations, we pick the re-
latum with the highest score (nearest neighbour) for
each relation (see discussion in Section 3 above on
why we allow models to pick their favorite from a
set of relata instantiating the same relation). In this
way, for each of the 200 BLESS concepts, we obtain
8 similarity scores, one per relation. In order to fac-
tor out concept-specific effects that might add to the
overall score variance (for example, a frequent con-
cept might have a denser neighborhood than a rarer
one, and consequently the nearest relatum scores of
the former are trivially higher than those of the lat-
ter), we transform the 8 similarity scores of each
concept onto standardized z scores (mean: 0; s.d: 1)
by subtracting from each their mean, and dividing by
their standard deviation. After this transformation,
we produce a boxplot summarizing the distribution
of scores per relation across the 200 concepts (i.e.,
each box of the plot summarizes the distribution of
the 200 standardized scores picked for each rela-
tion). Our boxplots (see examples in Fig. 1 below)
display the median of a distribution as a thick hori-
zontal line within a box extending from the first to
the third quartile, with whiskers covering 1.5 of the
interquartile range in each direction from the box,
and values outside this extended range ? extreme
outliers ? plotted as circles (these are the default
boxplotting option of the R statistical package).3
While the boxplots are extremely informative about
the relation types that are best captured by models,
we expect some degree of overlap among the distri-
butions of different relations, and in such cases we
might want to ask whether a certain model assigns
significantly higher scores to one relation rather than
another (for example, to coordinates rather than ran-
dom nouns). It is difficult to decide a priori which
pairwise statistical comparisons will be interesting.
We thus take a conservative approach in which we
perform all pairwise comparisons using the Tukey
Honestly Significant Difference test, that is simi-
lar to the standard t test, but accounts for the greater
likelihood of Type I errors when multiple compar-
isons are performed (Abdi and Williams, 2010). We
only report the Tukey test results for those com-
parisons that are of interest in the analysis of the
boxplots, using the standard ? = 0.05 significance
threshold.
5.1 Models
Occurrence and co-occurrence statistics for all mod-
els are extracted from the combined ukWaC and
Wackypedia corpora (see Section 4.4 above). We ex-
ploit the automated morphosyntactic annotation of
the corpora by building our DSMs out of lemmas
(instead of inflected words), and relying on part of
speech information.
Baselines. The RelatumFrequency baseline uses
the frequency of occurrence of a relatum as a sur-
rogate of its cosine with the concept. With this ap-
proach, we want to verify that the unequal frequency
distribution across relations (see Table 1 above) is
not trivially sufficient to differentiate relation classes
in a semantically interesting way. For our second
baseline, we assign a random number as cosine sur-
3http://www.r-project.org/
7
rogate to each relatum (to smooth these random val-
ues, we generate them by first sampling, for each
relatum, 10K random variates from a uniform distri-
bution, and then averaging them). If the set of relata
instantiating a certain relation is larger, it is more
likely that it will contain the highest random value.
Thus, this RelationCardinality baseline will favor
relations that tend to have large relata set across con-
cepts, controlling for effects due to different cardi-
nalities across semantic relations (again, see Table 1
above).
DSMs. We choose a few ways to construct DSMs
for illustrative purposes only. All the models contain
vector representations for the same words, namely,
approximately, the top 20K most frequent nouns, 5K
most frequent adjectives and 5K most frequent verbs
in the combined corpora. All the models use Local
Mutual Information (Evert, 2005; Baroni and Lenci,
2010) to weight raw co-occurrence counts (this asso-
ciation measure is obtained by multiplying the raw
count by Pointwise Mutual Information, and it is a
close approximation to the Log-Likelihood Ratio).
Three DSMs are based on counting co-occurrences
with collocates within a window of fixed width,
in the tradition of HAL (Lund and Burgess, 1996)
and many later models. The ContentWindow2
model records sentence-internal co-occurrence with
the nearest 2 content words to the left and right
of each target concept (the same 30K target nouns,
verbs and adjectives are also employed as context
content words). ContentWindow20 is like Con-
tentWindow2, but considers a larger window of 20
words to the left and right of the target. AllWin-
dow2 adopts the same window of ContentWindow2,
but considers all co-occurrences, not only those with
content words. The Document model, finally, is
based on a (Local-Mutual-Information transformed)
word-by-document matrix, recording the distribu-
tion of the 30K target words across the documents in
the concatenated corpus. This DSM is thus akin to
traditional Latent Semantic Analysis (Landauer and
Dumais, 1997), without dimensionality reduction.
The content-window-based models have, by con-
struction, about 30K dimensions. The other models
are much larger, and for practical reasons we only
keep 1 million dimensions (those that account, cu-
mulatively, for the largest proportion of the overall
Local Mutual Information mass).
5.2 Results
The concept-by-concept z-normalized distributions
of cosines of relata instantiating each of our rela-
tions are presented, for each of the example mod-
els, in Fig. 1. The RelatumFrequency baseline
shows a preference for adjectives and verbs in gen-
eral, independently of whether they are meaningful
(attributes, events) or not (random adjectives and
verbs), reflecting the higher frequencies of adjec-
tives and verbs in BLESS (Table 1). The Relation-
Cardinality baseline produces even less interesting
results, with a strong preference for random nouns,
followed by coordinates, events and random verbs
(as predicted by the distribution in Table 1). We can
conclude that the semantically meaningful patterns
produced by the other models cannot be explained
by trivial differences in relatum frequency or rela-
tion cardinality in the BLESS data set.
Moving then to the real DSMs, ContentWindow2
essentially partitions the relations into 3 groups: co-
ordinates are the closest relata, which makes sense
since they are, taxonomically, the most similar en-
tities to target concepts. They are followed by (but
significantly closer to the concept than) events, hy-
pernyms and meronyms (events and hypernyms sig-
nificantly above meronyms). Next come the at-
tributes (significantly lower cosines than all relation
types above). All the meaningful relata are signif-
icantly closer to the concepts than the random re-
lata. Similar patterns can be observed in the Con-
tentWindow20 distribution, however in this case the
events, while still significantly below the coordi-
nates, are significantly above the (statistically in-
distinguishable) hypernym, meronym and attribute
set. Again, all meaningful relata are above the ran-
dom ones. Both content-window-based models pro-
vide reasonable results, with ContentWindow2 be-
ing probably closer to our ?ontological? intuitions.
The high ranking of events is probably explained
by the fact that a nominal concept will often ap-
pear as subject or object of verbs expressing asso-
ciated events (dog barks, fishing tuna), and thus the
corresponding verbs will share even relatively nar-
row context windows with the concept noun. The
AllWindow2 distribution probably reflects the fact
that many contexts picked by this DSM are function
8
?
??
?
?
?
??
?
??
?
?
?
?
?
?
?? ?
?
?
?
??
?
?
?
?
????
??
??
?
?
?
?
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
RelatumFrequency
?
?
?
??
??
??
????
????
???
?
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2 RelationCardinality
???
?
?
?
?
?
??
?
?
?
?
?
?
???
?
?
?
?? ?
?
???
?
?
?
??
?
?
?
?
???
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
ContentWindow2
?
???
?
??
?
??
?
??
?
?
??
?
?
?
?
?
??
??
?
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
ContentWindow20
??
???
?
?
?
?
?
?
?
?
?
??
?
?
?
??
??
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
AllWindow2
?
?
?
?
?
?
?
??
?
?
?
?
?
??
?? ?
?
??
?
??
?
?
?
?
?
?
??
?
?
???
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
Document
Figure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after concept-by-concept z-
normalization).
words, and thus they capture syntactic, rather than
semantic distributional properties. As a result, ran-
dom nouns are as high (statistically indistinguish-
able from) hypernyms and meronyms. Interestingly,
attributes also belong to this subset of relations ?
probably due to the effect of determiners, quantifiers
and other DP-initial function words, that will often
occur both before nouns and before adjectives. In-
deed, even random adjectives, although significantly
below the other relations we discussed, are signif-
icantly above both random and meaningful verbs
(i.e., events). For the Document model, all mean-
ingful relations are significantly above the random
ones. However, coordinates, while still the nearest
neighbours (significantly closer than all other rela-
tions) are much less distinct than in the window-
based models. Note that we cannot say a priori that
ContentWindow2 is better than Document because
it favors coordinates. However, while they are both
able to sort out true and random relata, the latter
shows a weaker ability to discriminate among differ-
ent types of semantic relations (co-occurring within
a document is indeed a much looser cue to similarity
than specifically co-occurring within a narrow win-
dow). Traditional DSM tests, based on a single qual-
ity measure, would not have given us this broad view
of how models are behaving.
6 Conclusion
We introduced BLESS, the first data set specifically
designed for the intrinsic evaluation of DSMs. The
data set contains tuples instantiating different, ex-
plicitly typed semantic relations, plus a number of
controlled random tuples. Thus, BLESS can be used
to evaluate both the ability of DSMs to discriminate
truly related word pairs, and to perform in-depth
analyses of the types of semantic relata that different
models tend to favor among the nearest neighbors of
a target concept. Even a simple comparison of the
performance of a few DSMs on BLESS - like the
one we have shown here - is able to highlight inter-
esting differences in the semantic spaces produced
by the various models. The success of BLESS will
obviously depend on whether it will become a refer-
ence model for the evaluation of DSMs, something
that can not be foreseen a priori. Whatever its des-
tiny, we believe that the BLESS approach can boost
and innovate evaluation in distributional semantics,
as a key condition to get at a deeper understanding
of its potentialities as a viable model for meaning.
9
References
Herv Abdi and Lynne Williams. 2010. Newman-Keuls
and Tukey test. In N.J. Salkind, D.M. Dougherty, and
B. Frey, editors, Encyclopedia of Research Design.
Sage, Thousand Oaks, CA.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, Boulder, CO.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. Bridging the Gap between Semantic The-
ory and Computational Simulations: Proceedings of
the ESSLLI Workshop on Distributional Lexical Se-
mantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13?47.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
tical commonsense reasoning toolkit. BT Technology
Journal, pages 211?226.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
McNorgan. 2005. Semantic feature production norms
for a large set of living and nonliving things. Behavior
Research Methods, 37(4):547?559.
GeorgeMiller andWalter Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Gregory Murphy. 2002. The Big Book of Concepts. MIT
Press, Cambridge, MA.
Timothy Rogers and James McClelland. 2004. Seman-
tic Cognition: A Parallel Distributed Processing Ap-
proach. MIT Press, Cambridge, MA.
Herbert Rubenstein and John Goodenough. 1965. Con-
textual correlates of synonymy. Communications of
the ACM, 8(10):627?633.
Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
ing Natural Language Processing Systems: An Analy-
sis and Review. Springer Verlag, Berlin.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Morton E. Winston, Roger Chaffin, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11:417?444.
10
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 22?32,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Distributional semantics from text and images
Elia Bruni
CIMeC, University of Trento
elia.bruni@unitn.it
Giang Binh Tran
EMLCT, Free University of Bolzano &
CIMeC, University of Trento
Giang.Tran@stud-inf.unibz.it
Marco Baroni
CIMeC, University of Trento
marco.baroni@unitn.it
Abstract
We present a distributional semantic model
combining text- and image-based features. We
evaluate this multimodal semantic model on
simulating similarity judgments, concept clus-
tering and the BLESS benchmark. When inte-
grated with the same core text-based model,
image-based features are at least as good as
further text-based features, and they capture
different qualitative aspects of the tasks, sug-
gesting that the two sources of information are
complementary.
1 Introduction
Distributional semantic models use large text cor-
pora to derive estimates of semantic similarities be-
tween words. The basis of these procedures lies in
the hypothesis that semantically similar words tend
to appear in similar contexts (Miller and Charles,
1991; Wittgenstein, 1953). For example, the mean-
ing of spinach (primarily) becomes the result of sta-
tistical computations based on the association be-
tween spinach and words like plant, green, iron,
Popeye, muscles. Alongside their applications in
NLP areas such as information retrieval or word
sense disambiguation (Turney and Pantel, 2010), a
strong debate has arisen on whether distributional
semantic models are also reflecting human cogni-
tive processes (Griffiths et al, 2007; Baroni et al,
2010). Many cognitive scientists have however ob-
served that these techniques relegate the process of
meaning extraction solely to linguistic regularities,
forgetting that humans can also rely on non-verbal
experience, and comprehension also involves the ac-
tivation of non-linguistic representations (Barsalou
et al, 2008; Glenberg, 1997; Zwaan, 2004). They
argue that, without grounding words to bodily ac-
tions and perceptions in the environment, we can
never get past defining a symbol by simply pointing
to covariation of amodal symbolic patterns (Harnad,
1990). Going back to our example, the meaning of
spinach should come (at least partially) from our ex-
perience with spinach, its colors, smell and the oc-
casions in which we tend to encounter it.
We can thus distinguish two different views of
how meaning emerges, one stating that it emerges
from association between linguistic units reflected
by statistical computations on large bodies of text,
the other stating that meaning is still the result of an
association process, but one that concerns the asso-
ciation between words and perceptual information.
In our work, we try to make these two appar-
ently mutually exclusive accounts communicate, to
construct a richer and more human-like notion of
meaning. In particular, we concentrate on percep-
tual information coming from images, and we cre-
ate a multimodal distributional semantic model ex-
tracted from texts and images, putting side by side
techniques from NLP and computer vision. In a nut-
shell, our technique is based on using a collection
of labeled pictures to build vectors recording the co-
occurrences of words with image-based features, ex-
actly as we would do with textual co-occurrences.
We then concatenate the image-based vector with
a standard text-based distributional vector, to ob-
tain our multimodal representation. The prelimi-
nary results reported in this paper indicate that en-
22
riching a text-based model with image-based fea-
tures is at least not damaging, with respect to en-
larging the purely textual component, and it leads to
qualitatively different results, indicating that the two
sources of information are not redundant.
The rest of the paper is structured as follows. Sec-
tion 2 reviews relevant work including distributional
semantic models, computer vision techniques suit-
able to our purpose and systems combining text and
image information, including the only work we are
aware of that attempts something similar to what we
try here. We introduce our multimodal distributional
semantic model in Section 3, and our experimental
setup and procedure in Section 4. Our experiments?
results are discussed in Section 5. Section 6 con-
cludes summarizing current achievements and dis-
cussing next directions.
2 Related Work
2.1 Text-based distributional semantic models
Traditional corpus-based models of semantic repre-
sentation base their analysis on textual input alone
(Turney and Pantel, 2010). Assuming the distribu-
tional hypothesis (Miller and Charles, 1991), they
represent semantic similarity between words as a
function of the degree of overlap among their lin-
guistic contexts. Similarity is computed in a seman-
tic space represented as a matrix, with words as rows
and contextual elements as columns/dimensions.
Thanks to the geometrical nature of the represen-
tation, words are compared using a distance met-
ric, such as the cosine of the angle between vectors
(Landauer and Dumais, 1997).
2.2 Bag of visual words
In NLP, ?bag of words? (BoW) is a dictionary-based
method in which a document is represented as a
?bag? (i.e., order is not considered), which contains
words from the dictionary. In computer vision, ?bag
of visual words? (BoVW) is a similar idea for image
representation (Sivic and Zisserman, 2003; Csurka
et al, 2004; Nister and Stewenius, 2006; Bosch et
al., 2007; Yang et al, 2007).
Here, an image is treated as a document, and fea-
tures from a dictionary of visual elements extracted
from the image are considered as the ?words? repre-
senting the image. The following pipeline is typ-
ically adopted in order to group the local interest
points into types (visual words) within and across
images, so that then an image can be represented
by the number of occurrences of each visual word
type in it, analogously to BoW. From every image
of a data set, keypoints are automatically detected
and represented as vectors of various descriptors.
Keypoint vectors are then projected into a common
space and grouped into a number of clusters. Each
cluster is treated as a discrete visual word (this tech-
nique is generally known as vector quantization).
With its keypoints mapped onto visual words, each
image can then be represented as a BoVW feature
vector according to the count of each visual word. In
this way, we move from representing the image by
a varying number of high-dimensional keypoint de-
scriptor vectors to a representation in terms of a sin-
gle sparse vector of fixed dimensionality across all
images. What kind of image content a visual word
captures exactly depends on a number of factors, in-
cluding the descriptors used to identify and represent
local interest points, the quantization algorithm and
the number of target visual words selected. In gen-
eral, local interest points assigned to the same visual
word tend to be patches with similar low-level ap-
pearance; but these common types of local patterns
need not be correlated with object-level parts present
in the images. Figure 1 illustrates the procedure to
form bags of visual words. Importantly for our pur-
poses, the BoVW representation, despite its unre-
lated origin in computer vision, is entirely analogous
to the BoW representation, making the integration of
text- and image-based features very straightforward.
2.3 Integrating textual and perceptual
information
Louwerse (2011), contributing to the debate on sym-
bol grounding in cognitive science, theorizes the in-
terdependency account, which suggests a conver-
gence of symbolic theories (such as distributional
semantics) and perceptual theories of meaning, but
lacks of a concrete way to harvest perceptual infor-
mation computationally. Andrews et al (2009) com-
plement text-based models with experiential infor-
mation, by combining corpus-based statistics with
speaker-generated feature norms as a proxy of per-
ceptual experience. However, the latter are an un-
satisfactory proxy, since they are still verbally pro-
23
Figure 1: Illustration of bag of visual words procedure: (a) detect and represent local interest points as descriptor
vectors (b) quantize vectors (c) histogram computation to form BoVW vector for the image
duced descriptions, and they are expensive to collect
from subjects via elicitation techniques.
Taking inspiration from methods originally used
in text processing, algorithms for image labeling,
search and retrieval have been built upon the connec-
tion between text and visual features. Such models
learn the statistical models which characterize the
joint statistical distribution of observed visual fea-
tures and verbal image tags (Hofmann, 2001; Hare
et al, 2008). This line of research is pursuing the re-
verse of what we are interested in: using text to im-
prove the semantic description of images, whereas
we want to exploit images to improve our approxi-
mation to word meaning.
Feng and Lapata are the first trying to integrate
authentic visual information in a text-based distribu-
tional model (Feng and Lapata, 2010). Using a col-
lection of BBC news with pictures as corpus, they
train a Topic model where text and visual words are
represented in terms of the same shared latent di-
mensions (topics). In this framework, word meaning
is modeled as a probability distribution over a set of
latent multimodal topics and the similarity between
two words can be estimated by measuring the topics
they have in common. A better correlation with se-
mantic intuitions is obtainable when visual modality
is taken into account, in comparison to estimating
the topic structure from text only.
Although Feng and Lapata?s work is very promis-
ing and the main inspiration for our own, their
method requires the extraction of a single distribu-
tional model from the same mixed-media corpus.
This has two important drawbacks: First, the tex-
tual model must be extracted from the same corpus
images are taken from, and the text context extrac-
tion methods must be compatible with the overall
multimodal approach. Thus, image features can-
not be added to a state-of-the-art text-based distri-
butional model ? e.g., a model computed on the
whole Wikipedia or larger corpora using syntactic
dependency information ? to assess whether visual
information is helping even when purely textual fea-
tures are already very good. Second, by training a
joint model with latent dimensions that mix textual
and visual information, it becomes hard to assess,
quantitatively and qualitatively, the separate effect
of image-based features on the overall performance.
In order to overcome these issues, we propose a
somewhat simpler approach, in which the text- and
image-based models are independently constructed
from different sources, and then concatenated.
3 Proposed method
Figure 2 presents a diagram of our overall sys-
tem. The main idea is to construct text-based and
image-based co-occurrence models separately and
then combine them. We first describe our proce-
dure to build both text-based and image-based mod-
els. However, we stress the latter since it is the
more novel part of the procedure. Then, we describe
our simple combination technique to integrate both
models and create a multimodal distributional se-
mantic space. Our implementation of the proposed
method is open-source1.
1https://github.com/s2m
24
Image Data
Visual feature extraction
Bag of visual words
Image-based distributional vectorText-based distributional vector
Text feature extraction
Normalize and concatenate
Multimodal distributional semantic vector
Tag modeling
Text corpus
Figure 2: Overview of our system architecture
3.1 Text-based distributional model
Instead of proposing yet another model, we pick one
that is publicly available off-the-shelf and has been
shown to be at the state of the art on a number of
benchmarks. The picked model (DM)2 is encoded
in a matrix in which each target word is represented
by a row vector of weights representing its associa-
tion with collocates in a corpus. See Section 4.1 for
details about the text-based model.
3.2 Image-based distributional model
We assume image data where each image is associ-
ated with word labels (somehow related to the im-
age) that we call tags.
The primary approach to form the image-based
vector space is to use the BoVW method to rep-
resent images. Having represented each image in
our data set in terms of the frequency of occurrence
of each visual word in it, we construct the image-
based distributional vector of each tag as follows.
Each tag (textual word) is associated to the list of
images which are tagged with it; we then sum visual
word occurrences across that list of images to ob-
tain the co-occurrence counts associated with each
tag. For uniformity with the treatment of textual
co-occurrences (see Section 4.1), the raw counts are
transformed into Local Mutual Information scores
computed between each tag and visual word. Lo-
cal Mutual Information is an association measure
that closely approximates the commonly used Log-
Likelihood Ratio while being simpler to compute
(Evert, 2005).
In this way, we obtain an image-based distribu-
2http://clic.cimec.unitn.it/dm
tional semantic model, that is, a matrix where each
row corresponds to a tag vector, summarizing the
distributional history of the tag in the image collec-
tion in terms of its association with the visual words.
3.3 Integrating distributional models
We assemble the two distributional vectors to con-
struct the multimodal semantic space. Given a word
that is present both in the text-based model and
(as a tag) in the image-based model, we separately
normalize the two vectors representing the word to
length 1 (so that the text and image components will
have equal weight), and we concatenate them to ob-
tain the multimodal distributional semantic vector
representing the word. The matrix of concatenated
text- and image-based vectors is our multimodal dis-
tributional semantic model. We leave it to future
work to consider more sophisticated combination
techniques (preliminary experiments on differential
weighting of the text and image components did not
lead to promising results).
4 Experimental setup
4.1 The DM text-based model
DM has been shown to be near or at the state of
the art in a great variety of semantic tasks, ranging
from modeling similarity judgments to concept cat-
egorization, predicting selectional preferences, rela-
tion classification and more.
The DM model is described in detail by Baroni
and Lenci (2010), where it is referred to as TypeDM.
In brief, the model is trained on a large corpus
of about 2.8 billion tokens that include Web docu-
ments, the Wikipedia and the BNC. DM is a struc-
tured model, where the collocates are labeled with
the link that connect them to the target words. The
links are determined by a mixture of dependency
parse information and lexico-syntactic patterns, re-
sulting in distributional features (the dimensions of
the semantic space) such as subject kill, with gun or
as sharp as. The score of a target word with a fea-
ture is not based on the absolute number of times
they co-occur in the corpus, but on the variety of
different surface realizations of the feature the word
co-occurs with. For example, for the word fat and
the feature of animal, the raw score is 9 because fat
co-occurs with 9 different forms of the feature (a
25
fat of the animal, the fat of the animal, fats of an-
imal. . . ). Refer to Baroni and Lenci (2010) for how
the surface realizations of a feature are determined.
Raw scores are then transformed into Local Mutual
Information values.
The DM semantic space is a matrix with 30K
rows (target words) represented in a space of more
than 700M dimensions. Since our visual dimension
extraction algorithms are maximally producing 32K
dimensions (see Section 4.2 below), we make the
impact of text features on the combined model di-
rectly comparable to the one of visual features by
selecting only the top n DM dimensions (with n
varying as explained below). The top dimensions
are picked based on their cumulative Local Mutual
Information mass. We show in the experiments be-
low that trimming DM in this way does not have a
negative impact on its performance, so that we are
justified in claiming that we are adding visual in-
formation to a state-of-the-art text-based semantic
space.
4.2 Visual Information Extraction
For our experiments, we use the ESP-Game data
set.3 It contains 50K images, labeled through the
famous ?game with a purpose? developed by Louis
von Ahn (von Ahn and Dabbish, 2004). The tags
of images in the data set form a vocabulary of 11K
distinct word types. Image labels contain 6.686 tags
on average (2.357 s.d.). The ESP-Game corpus is
an interesting data set from our point of view since,
on the one hand, it is rather large and we know that
the tags it contains are related to the images. On
the other hand, it is not the product of experts la-
belling representative images, but of a noisy anno-
tation process of often poor-quality or uninteresting
images (e.g., logos) randomly downloaded from the
Web. Thus, analogously to the characteristics of a
textual corpus, our algorithms must be able to ex-
ploit large-scale statistical information, while being
robust to noise.
Following what has become an increasingly stan-
dard procedure in computer vision, we use the Dif-
ference of Gaussian (DoG) detector to automatically
detect keypoints from images and consequently map
them to visual words (Lowe, 1999; Lowe, 2004). We
3http://www.espgame.org
use the Scale-Invariant Feature Transform (SIFT) to
depict the keypoints in terms of a 128-dimensional
real-valued descriptor vector. Color version SIFT
descriptors are extracted on a regular grid with five
pixels spacing, at four multiple scales (10, 15, 20,
25 pixel radii), zeroing the low contrast ones. We
chose SIFT for its invariance to image scale, ori-
entation, noise, distortion and partial invariance to
illumination changes. To map the descriptors to vi-
sual words, we cluster the keypoints in their 128-
dimensional space using the K-means clustering al-
gorithm, and encode each keypoint by the index of
the cluster (visual word) to which it belongs. We
varied the number of visual words between 250 and
2000 in steps of 250. We then computed a one-level
4x4 pyramid of spatial histograms (Grauman and
Darrell, 2005), consequently increasing the features
dimensions 16 times, for a number that varies be-
tween 4K and 32K, in steps of 4K. From the point of
view of our distributional semantic model construc-
tion, the important point to keep in mind is that stan-
dard parameter choices such as the ones we adopted
lead to distributional vectors with 4K, 8K, . . . , 32K
dimensions, where a higher number of features cor-
responds, roughly, to a more granular analysis of an
image. We used the VLFeat implementation for the
entire pipeline (Vedaldi and Fulkerson, 2008). See
the references in Section 2.2 above for technical de-
tails.
4.3 Model integration
We remarked above that the visual word extraction
procedure naturally leads to 8 kinds of image-based
vectors of dimensionalities from 4K to 32K in steps
of 4K. To balance text and image information, we
use DM vectors made of top n features ranging from
4K to 32K in the same 4K steps. By combining,
we obtain 64 combined models (4K text and 4K im-
age dimensions, 4K text and 8K image dimensions,
etc.). Since in the experiments on WordSim (Section
5.1 below) we observe best performance with 32K
text-based features, we report here only experiments
with (at least) 32K dimensions. Similar patterns to
the ones we report are observed when adding image-
based dimensions to text-based vectors of different
dimensionalities.
For a thoroughly fair comparison, if we add n vi-
sual features to the text-based model and we notice
26
an improvement, we must ask whether the same im-
provement could also be obtained by adding more
text-based features. To control for this possibility,
we also consider a set of purely text-based mod-
els that have the same number of dimensions of the
combined models, that is, the top 32K DM features
plus 8K, . . . , 32K further DM features (the next top
features in the cumulative Local Mutual Information
score ranking). In the experiments below, we refer to
the purely textual model as text (always 32K dimen-
sions), to the purely image-based model as image, to
the combined models as combined, and to the con-
trol in which further text dimensions are added for
comparability with combined as text+.
4.4 Evaluation benchmarks
We conduct our most extensive evaluation on the
WordSim353 data set (Finkelstein et al, 2002),
a widely used benchmark constructed by asking
16 subjects to rate a set of word pairs on a 10-
point similarity scale and averaging the ratings (dol-
lar/buck receive a high 9.22 average rating, profes-
sor/cucumber a low 0.31). We cover 260 Word-
Sim (mostly noun/noun) pairs. We evaluate models
in terms of the Spearman correlation of the cosines
they produce for the WordSim pairs with the average
human ratings for the same pairs (here and below,
we do not report comparisons with the state of the
art in the literature, because we have reduced cov-
erage of the data sets, making the comparison not
meaningful).
To verify if the conclusions reached on WordSim
extend to different semantic tasks, we use two con-
cept categorization benchmarks, where the goal is
to cluster a set of (nominal) concepts into broader
categories. The Almuhareb-Poesio (AP) concept set
(Almuhareb, 2006), in the version we cover, con-
tains 230 concepts to be clustered into 21 classes
such as vehicle (airplane, car. . . ), time (aeon, fu-
ture. . . ) or social unit (brigade, nation). The Battig
set (Baroni et al, 2010), in the version we cover,
contains 72 concepts to be clustered into 10 classes.
Unlike AP, Battig only contains concrete basic-level
concepts belonging to categories such as bird (ea-
gle, owl. . . ), kitchenware (bowl, spoon. . . ) or veg-
etable (broccoli, potato. . . ). For both sets, follow-
ing the original proponents and others, we clus-
ter the words based on their pairwise cosines in
the semantic space defined by a model using the
CLUTO toolkit (Karypis, 2003). We use CLUTO?s
built-in repeated bisections with global optimiza-
tion method, accepting all of CLUTO?s default val-
ues. Cluster quality is evaluated by percentage pu-
rity (Zhao and Karypis, 2003). If nir is the num-
ber of items from the i-th true (gold standard) class
that were assigned to the r-th cluster, n is the total
number of items and k the number of clusters, then:
Purity = 1n
?k
r=1 maxi
(nir). In the best case (per-
fect clusters), purity is 100% and as cluster quality
deteriorates, purity approaches 0.
Finally, we use the Baroni-Lenci Evaluation of
Semantic Similarity (BLESS) data set made avail-
able by the GEMS 2011 organizers.4 In the ver-
sion we cover, the data set contains 174 concrete
nominal concepts, each paired with a set of words
that instantiate the following 6 relations: hyper-
nymy (spear/weapon), coordination (tiger/coyote),
meronymy (castle/hall), typical attribute (an ad-
jective: grapefruit/tart) and typical event (a verb:
cat/hiss). Concepts are moreover matched with 3
sets of randomly picked unrelated words (nouns, ad-
jectives and verbs). For each true and random rela-
tion, the data set contains at least one word per con-
cept, typically more. Following the GEMS guide-
lines, we apply a model to BLESS as follows. Given
the similarity scores provided by the model for a
concept with all associated words within a relation,
we pick the term with the highest score. We then z-
standardize the 8 scores we obtain for each concept
(one per relation), and we produce a boxplot summa-
rizing the distribution of z scores per relation across
the concepts (i.e., each box of the plot summarizes
the distribution of the 174 scores picked for each re-
lation, standardized as we just described). Boxplots
are produced accepting the default boxplotting op-
tion of the R statistical package5 (boxes extend from
first to third quartile, median is horizontal line inside
the box).
4http://sites.google.com/site/geometricalmodels/shared-
evaluation
5http://www.r-project.org/
27
5 Results
5.1 WordSim
The WordSim results for our models across dimen-
sionalities as well as for the full DM are summarized
in Figure 3.
+4K +8K +12K +16K +20K +24K +28K +32K25
30
35
40
45
50
55
Adding more features to top 32K DM
Sp
ear
ma
n c
oef
fici
ent
(%
)
Performance of distributional models on WordSim
 
 
DM
combined
text
image
text+
Figure 3: Performance of distributional models on Word-
Sim
The purely image-based model is having the
worst performance in all settings, although even the
lowest image-based Spearman score (0.29) is signif-
icantly above chance (p. < 0.05), suggesting that
the model does capture some semantic information.
Contrarily, adding image-based dimensions to a tex-
tual model (combined) consistently reaches the best
performance, also better ? for all choices of dimen-
sionality ? than adding an equal number of text fea-
tures (text+) or using the full DM matrix. Inter-
estingly, the same overall result pattern is observed
if we limit evaluation to the WordSim subsets that
Agirre et al (2009) have identified as semantically
similar (e.g., synonyms or coordinate terms) and se-
mantically related (e.g., meronyms or topically re-
lated concepts).
Based on the results reported in Figure 3, fur-
ther analyses will focus on the combined model with
+20K image-based features, since performance of
combined does not seem to be greatly affected by the
dimensionality parameter, and performance around
this value looks quite stable (it is better only at the
boundary +4K value, and with +28K, where, how-
ever, there is a dip for the image model). The text+
performance is not essentially affected by the di-
mensionality parameter, and we pick the +20K ver-
sion for maximum comparability with combined.
The difference between combined and text+, al-
though consistent, is not statistically significant
according to a two-tailed paired permutation test
(Moore and McCabe, 2005) conducted on the re-
sults for the +20K versions of the models. Still, very
interesting qualitative differences emerge. Table 1
reports those WordSim pairs (among the ones with
above-median human-judged similarity) that have
the highest and lowest combined-to-text+ cosine ra-
tios, i.e., pairs that are correctly treated as similar by
combined but not by text+, and vice versa. Strik-
ingly, the pairs characterizing the image-feature-
enriched combined are all made of concrete, highly
imageable concepts, whereas the text+ pairs refer to
very abstract notions. We thus see here the first ev-
idence of the complementary nature of visual and
textual information.
combined text+
tennis/racket physics/proton
planet/sun championship/tournament
closet/clothes profit/loss
king/rook registration/arrangement
cell/phone mile/kilometer
Table 1: WordSim pairs with highest (first column) and
lowest (second column) combined-to-text+ cosine ratios
5.2 Concept categorization
Table 2 reports percentage purities in the AP and
Battig clustering tasks for full DM and the represen-
tative models discussed above.
model AP Battig
DM 81 96
text 79 83
text+ 80 86
image 25 36
combined 78 96
Table 2: Percentage AP and Battig purities of distribu-
tional models
Once more, we see that the image model alone
is not at the level of the text models, although both
its AP and Battig purities are significantly above
28
chance (p < 0.05 based on simulated distributions
for random cluster assignment). Thus, even alone,
image-based vectors do capture aspects of meaning.
For AP, adding image features does not improve per-
formance, although it does not significantly worsen
it either (a two-tailed paired permutation test con-
firms that the difference between text+ and com-
bined is far from significance). For Battig, adding
visual features improves on the purely text-based
models based on a comparable number of features
(although the difference between text+ and com-
bined is not significant), reaching the same perfor-
mance obtained with the full DM model (that in
these categorization tests is slightly above that of the
trimmed models). Intriguingly, the Battig test is en-
tirely composed of concrete concepts, so the differ-
ence in performance for combined might be related
to its preference for concrete things we already ob-
served for WordSim.
5.3 BLESS
The BLESS distributions of text-based models (in-
cluding combined) are very similar, so we use here
the full DM model as representative of the text-based
set ? its histogram is compared to the one of the
purely image-based model in Figure 4.
We see that purely text-based DM cosines capture
a reasonable scale of taxonomic similarity among
nominal neighbours (coordinates then hypernyms
then meronyms then random nouns), whereas verbs
and adjectives are uniformly very distant, whether
they are related or not. This is not surprising be-
cause the DM links mostly reflect syntactic patterns,
that will be disjoint across parts of speech (e.g., a
feature like subject kill will only apply to nouns,
save for parsing errors). Looking at the image-
only model, we first observe that it can capture dif-
ferences between related attributes/events and ran-
dom adjectives/verbs (according to a Tukey HSD
test for all pairwise comparisons, these differences
are highly significant, whereas DM only signifi-
cantly distinguishes attributes from random verbs).
In this respect, image is arguably the ?best? model
on BLESS. However, perhaps more interestingly,
the image model also shows a bias for nouns, cap-
turing the same taxonomic hierarchy found for DM.
This suggests that image analysis is providing a de-
composition of concepts into attributes shared by
similar entities, that capture ontological similarity
beyond mere syntagmatic co-occurrence in an im-
age description.
To support this latter claim, we counted the av-
erage number of times that the related terms picked
by the image model directly co-occur with the target
concepts in an ESP-Game label. It turns out that this
count is higher for both attributes (10.6) and hyper-
nyms (7.5) than for coordinates (6.5). So, the higher
similarity of coordinates in the image model demon-
strates that its features do generalize across images,
allowing us to capture ?attributional? or ?paradig-
matic? similarity in visual space. More in general,
we find that, among all the related terms picked by
the image model that have an above-average cosine
with the target concept, almost half (41%) never co-
occur with the concept in the image set, again sup-
porting the claim that, by our featural analysis, we
are capturing visual properties of similar concepts
beyond their co-occurrence as descriptions of the
same image.
A final interesting point pertains to the specific in-
stances of each (non-random) relation picked by the
textual and visual models: of 870 related term pairs
in total, almost half (418) differ between DM and
image, suggesting that the boxplots in Figure 4 hide
larger differences in what the models are doing. The
randomly picked examples of mismatches in top at-
tributes from Table 3 clearly illustrate the qualitative
difference between the models, and, once more, the
tendency of image-based representations to favour
(not surprisingly!) highly visual properties such as
colours and shapes, vs. the well-known tendency of
text-based models to extract systemic or functional
characteristics such as powerful or elegant (Baroni
et al, 2010). By combining the two sources of infor-
mation, we should be able to develop distributional
models that come with more well-rounded charac-
terizations of the concepts they describe.
6 Conclusion
We proposed a simple method to augment a state-
of-the-art text-based distributional semantic model
with information extracted from image analysis.
The method is based on the standard bag-of-visual-
words representation of images in computer vision.
The image-based distributional profile of a word is
29
ll
l
llll
l
l
l
ll
l
l
l
l
l
ll
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?
1.0
?
0.5
0.0
0.5
1.0
1.5
2.0
DM
ll
ll
l
ll
ll
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
ll
l
l
l
l
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?
1
0
1
2
Image
Figure 4: Distribution of z-normalized cosines of words instantiating various relations across BLESS concepts.
concept DM image concept DM image
ant small black potato edible red
axe powerful old rifle short black
cathedral ancient dark scooter cheap white
cottage little old shirt fancy black
dresser new square sparrow wild brown
fighter fast old squirrel fluffy brown
fork dangerous shiny sweater elegant old
goose white old truck new heavy
jet fast old villa new cosy
pistol dangerous black whale large gray
Table 3: Randomly selected cases where nearest at-
tributes picked by DM and image differ.
encoded in a vector of co-occurrences with ?visual
words?, that we concatenate with a text-based co-
occurrence vector. A cautious interpretation of our
results is that adding image-based features is at least
not damaging, when compared to adding further
text-based features, and possibly beneficial. Impor-
tantly, in all experiments we find that image-based
features lead to interesting qualitative differences in
performance: Models including image-based infor-
mation are more oriented towards capturing similar-
ities between concrete concepts, and focus on their
more imageable properties, whereas the text-based
features are more geared towards abstract concepts
and properties. Coming back to the discussion of
symbol grounding at the beginning of the paper, we
consider this (very!) preliminary evidence for an in-
tegrated view of semantics where the more concrete
aspects of meaning derive from perceptual experi-
ence, whereas verbal associations mostly account
for abstraction.
In future work, we plan first of all to improve per-
formance, by focusing on visual word extraction and
on how the text- and image-based vectors are com-
bined (possibly using supervision to optimize both
feature extraction and integration with respect to se-
mantic tasks). However, the most exciting direction
we intend to follow next will concern evaluation,
and in particular devising new benchmarks that ad-
dress the special properties of image-enhanced mod-
els directly. For example, Baroni and Lenci (2008)
observe that text-based distributional models are se-
riously lacking when it comes to characterize phys-
ical properties of concepts such as their colors or
parts. These are exactly the aspects of conceptual
knowledge where image-based information should
help most, and we will devise new test sets that will
focus specifically on verifying this hypothesis.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, Boulder, CO.
30
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Lawrence Barsalou, Ava Santos, Kyle Simmons, and
Christine Wilson, 2008. Language and Simulation in
Conceptual Processing, chapter 13, pages 245?283.
Oxford University Press, USA, 1 edition.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image Classification using Random Forests and
Ferns. In Computer Vision, 2007. ICCV 2007. IEEE
11th International Conference on, pages 1?8.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91?99, Los Angeles,
California. Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Arthur Glenberg. 1997. What memory is for. Behav
Brain Sci, 20(1), March.
Kristen Grauman and Trevor Darrell. 2005. The pyramid
match kernel: Discriminative classification with sets
of image features. In In ICCV, pages 1458?1465.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psychologi-
cal Review, 114:211?244.
Jonathon Hare, Sina Samangooei, Paul Lewis, and Mark
Nixon. 2008. Semantic spaces revisited: investigat-
ing the performance of auto-annotation and semantic
retrieval using semantic spaces. In Proceedings of the
2008 international conference on Content-based im-
age and video retrieval, CIVR ?08, pages 359?368,
New York, NY, USA. ACM.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335?346,
June.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42(1-2):177?196, January.
George Karypis. 2003. CLUTO: A clustering toolkit.
Technical Report 02-017, University of Minnesota De-
partment of Computer Science.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David Lowe. 1999. Object Recognition from Local
Scale-Invariant Features. Computer Vision, IEEE In-
ternational Conference on, 2:1150?1157 vol.2, Au-
gust.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2), November.
George Miller and Walter Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
David Moore and George McCabe. 2005. Introduction
to the Practice of Statistics. Freeman, New York, 5
edition.
David Nister and Henrik Stewenius. 2006. Scalable
recognition with a vocabulary tree. In Proceedings
of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition - Volume 2,
CVPR ?06, pages 2161?2168.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of the International Conference on
Computer Vision, volume 2, pages 1470?1477, Octo-
ber.
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer-Verlag New York Inc.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Andrea Vedaldi and Brian Fulkerson. 2008. VLFeat:
An open and portable library of computer vision algo-
rithms. http://www.vlfeat.org/.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
31
SIGCHI conference on Human factors in computing
systems, CHI ?04, pages 319?326, New York, NY,
USA. ACM.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwell, Oxford. Translated by G.E.M.
Anscombe.
Jun Yang, Yu-Gang Jiang, Alexander G. Hauptmann,
and Chong-Wah Ngo. 2007. Evaluating bag-of-
visual-words representations in scene classification.
In James Ze Wang, Nozha Boujemaa, Alberto Del
Bimbo, and Jia Li, editors, Multimedia Information
Retrieval, pages 197?206. ACM.
Ying Zhao and George Karypis. 2003. Criterion func-
tions for document clustering: Experiments and analy-
sis. Technical Report 01-40, University of Minnesota
Department of Computer Science.
Rolf Zwaan. 2004. The immersed experiencer: Toward
an embodied theory of language comprehension. Psy-
chology of Learning and Motivation: Advances in Re-
search and Theory, Vol 44, 44.
32
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 67?71,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A distributional similarity approach to the detection of semantic change in
the Google Books Ngram corpus
Kristina Gulordava
DISI, University of Trento
Trento, Italy
kgulordava@gmail.com
Marco Baroni
CIMeC, University of Trento
Trento, Italy
marco.baroni@unitn.it
Abstract
This paper presents a novel approach for auto-
matic detection of semantic change of words
based on distributional similarity models. We
show that the method obtains good results
with respect to a reference ranking produced
by human raters. The evaluation also analyzes
the performance of frequency-based methods,
comparing them to the similarity method pro-
posed.
1 Introduction
Recently a large corpus of digitized books was made
publicly available by Google (Mitchel et al, 2010).
It contains more than 5 millions of books published
between the sixteenth century and today. Computa-
tional analysis of such representative diachronic data
made it possible to trace different cultural trends
in the last centuries. Mitchel et al (2010) exploit
the change in word frequency as the main measure
for the quantitative investigation of cultural and lin-
guistic phenomena; in this paper, we extend this ap-
proach by measuring the semantic similarity of the
word occurrences in two different time points using
distributional semantics model (Turney and Pantel,
2010).
Semantic change, defined as a change of one
or more meanings of the word in time (Lehmann,
1992), is of interest to historical linguistics and is
related to the natural language processing task of
unknown word sense detection (Erk, 2006). Devel-
oping automatic methods for identifying changes in
word meaning can therefore be useful for both theo-
retical linguistics and a variety of NLP applications
which depend on lexical information.
Some first automatic approaches to the seman-
tic change detection task were recently proposed by
Sagi et al (2009) and Cook and Stevenson (2010).
These works focus on specific types of semantic
change, i.e., Sagi et al (2009) aim to identify widen-
ing and narrowing of meaning, while Cook and
Stevenson (2010) concentrate on amelioration and
pejoration cases. Their evaluation of the proposed
methods is rather qualitative, concerning just a few
examples.
In present work we address the task of auto-
matic detection of the semantic change of words in
quantitative way, comparing our novel distributional
similarity approach to a relative-frequency-based
method. For the evaluation, we used the Google
Books Ngram data from the 1960s and 1990s, tak-
ing as a reference standard a ranking produced by
human raters. We present the results of the method
proposed, which highly correlate with the human
judgements on a test set, and show the underlying
relations with relative frequency.
2 Google Books Ngram corpus
The overall data published online by Google repre-
sent a collection of digitized books with over 500
billion words in 7 different languages distributed in
n-gram format due to copyright limitations (Mitchel
et al, 2010). An n-gram is a sequence of n words di-
vided by space character; for each n-gram it is spec-
ified in which year it occurred and how many times.
For our diachronic investigation we used the
American English 2-grams corpus (with over 150
millions 2-grams) and extracted two time slices from
the 1960s and 1990s time periods. More precisely,
we automatically selected 2-grams with year of oc-
currence between 1960 and 1964 for the 1960s slice,
67
and between 1995 and 1999 for the 1990s slice, and
summed up the number of occurrences of each 2-
gram for both corpora. After preprocessing, we ob-
tained well-balanced 60s and 90s corpora containing
around 25 and 28 millions of 2-grams, respectively.
We consider the 60s and 90s to be interesting time
frames for the evaluation, having in mind that a lot
of words underwent semantic change between these
decades due to many significant technological and
social movements. At the same time, the 60s are
close enough so that non-experts should have good
intuitions about semantic change between then and
now, which, in turn, makes it possible to collect ref-
erence judgments from human raters.
3 Measuring semantic change
3.1 Relative frequency
Many previous diachronic studies in corpus linguis-
tics focused on changes of relative frequency of
the words to detect different kinds of phenomena
(Hilpert and Gries, 2009; Mitchel et al, 2010). In-
tuitively, such approach can also be applied to de-
tect semantic change, as one would expect that many
words that are more popular nowadays with respect
to the past (in our case: the 60s) have changed their
meaning or gained an alternative one. Semantic
change could explain a significant growth of the rel-
ative frequency of the word.
Therefore we decided to take as a competing mea-
sure for evaluation the logarithmic ratio between fre-
quency of word occurrence in the 60s and frequency
of word occurrence in the 90s1.
3.2 Distributional similarity
In the distributional semantics approach (see for ex-
ample Turney and Pantel, 2010), the similarity be-
tween words can be quantified by how frequently
they appear within the same context in large cor-
pora. These distributional properties of the words
are described by a vector space model where each
word is associated with its context vector. The way a
context is defined can vary in different applications.
The one we use here is the most common approach
1The logarithmic ratio helps intuition (terms more popular in
the 60s get negative scores, terms more popular in the 90s have
similarly scaled positive scores), but omitting the logarithmic
transform produced similar results in evaluation.
which considers contexts of a word as a set of all
other words with which it co-occurs. In our case we
decided to use 2-grams, that is, only words that oc-
cur right next to the given word are considered as
part of its context. The window of length 2 was cho-
sen for practical reasons given the huge size or the
Google Ngram corpus, but it has been shown to pro-
duce good results in previous studies (e.g. Bullinaria
and Levy, 2007). The words and their context vec-
tors create a so called co-occurrence matrix, where
row elements are target words and column elements
are context terms.
The scores of the constructed co-occurrence ma-
trix are given by local mutual information (LMI)
scores (Evert, 2008) computed on the frequency
counts of corresponding 2-grams2. If words w1 and
w2 occurred C(w1, w2) times together and C(w1)
and C(w2) times overall in corpus then local mutual
information score is defined as follows:
LMI = C(w1, w2) ? log2
C(w1, w2)N
C(w1)C(w2)
,
where N is the overall number of 2-gram in the cor-
pus.
Given the words w1, w2 their distributional simi-
larity is then measured as the cosine product of their
context vectors v1,v2: sim(w1, w2) = cos(v1,v2).
We apply this model to measure similarity of a
word occurrences in two corpora of different time
periods in the following way. The set of context el-
ements is fixed and remains the same for both cor-
pora; for each corpus, a context vector for a word is
extracted independently, using counts in this corpus
as discussed above. In this way, each word will have
a 60s vector and a 90s vector, with the same dimen-
sions (context elements), but different co-occurrence
counts. The vectors can be compared by computing
the cosine of their angle. Since the context vectors
are computed in the same vector space, the proce-
dure is completely equivalent to calculating similar-
ity between two different words in the same corpora;
the context vectors can be considered as belong-
ing to one co-occurrence matrix and correspond-
ing to two different row elements word 60s and
word 90s.
2LMI proved to be a good measure for different semantic
tasks, see for example the work of Baroni and Lenci, 2010.
68
group examples sim freq
more frequent users 0.29 -0.94
in 90s sleep 0.23 -0.32
disease 0.87 -0.3
card 0.17 -0.1
more frequent dealers 0.16 0.04
in 60s coach 0.25 0.12
energy 0.79 0.14
cent 0.99 1.13
Table 1: Examples illustrating word selection with simi-
larity (sim) and log-frequency (freq) metric values.
We use the described procedure to measure se-
mantic change of a word in two corpora of interest,
and hence between two time periods. High similar-
ity value (close to 1) would suggest that a word has
not undergone semantic change, while obtaining low
similarity (close to 0) should indicate a noticeable
change in the meaning and the use of the word.
4 Experiments
4.1 Distributional space construction
To be able to compute distributional similarity for
the words in the 60s and 90s corpora, we randomly
chose 250,000 mid-frequency words as the context
elements of the vector space. We calculated 60s-to-
90s similarity values for a list of 10,000 randomly
picked mid-frequency words. Among these words,
48.4% had very high similarity values (> 0.8), 50%
average similarity (from 0.2 to 0.8) and only 1.6%
had very low similarity (< 0.2). According to our
prediction, this last group of words would be the
ones that underwent semantic change.
To test such hypothesis in a quantitative way some
reference standard must be available. Since for our
task there was no appropriate database containing
words classified for semantic change, we decided to
create a reference categorization using human judge-
ments.
4.2 Human evaluation
From the list of 10,000 words we chose 100 as a
representative random subset containing words with
different similarities from the whole scale from 0
to 1 and taken from different frequency range, i.e.,
words that became more frequent in 90s (60%) and
words that became less frequent (40%) (see Table
sim-HR freq-HR sim-freq
all words 0.386?? 0.301?? 0.380??
frequent in 90s 0.445?? 0.184 0.278?
frequent in 60s 0.163 0.310 0.406?
Table 2: Correlation between similarity (sim), frequency
(freq) and human ranking (HR) values for all words,
words more frequent in 60s and more frequent in 90s.
Values statistically significant for p = 0.01(0.05) in one-
sample t-test are marked with ??(?).
1 for examples). Human raters were asked to rank
the resulting list according to their intuitions about
change in last 40 years on a 4-point scale (0: no
change; 1: almost no change; 2: somewhat change;
3: changed significantly). We took the average of
judgments as the reference value with which distri-
butional similarity scores were compared. For the
5 participants, the inter-rater agreement, computed
as an average of pair-wise Pearson correlations, was
0.51 (p < 0.01). It shows that the collected judge-
ments were highly correlated and the average judge-
ment can be considered an enough reliable reference
for semantic change measurements evaluation.
5 Results and discussion
To assess the performance of our similarity-based
measure, we computed the correlations between the
values it produced for our list of words and the av-
erage human judgements (Table 2). The Pearson
correlation value obtained was equal to 0.38, which
is reasonably high given 0.51 inter-rater agreement.
The frequency measure had a lower correlation
(0.3), though close to the similarity measure perfor-
mance. Yet, the correlation of 0.38 between the two
measures in question suggests that, even if they per-
form similarly, their predictions could be quite dif-
ferent.
In fact, if we consider separately two groups of
words: the ones whose frequency increased in the
90s (log-freq < 0), that is, the ones that are more
popular nowadays, and those whose frequency in-
stead decreased in the 90s (log-freq > 0), that is,
the ones that were more popular in the 60s, we can
make some interesting observations (see Table 2).
Remarkably, similarity performs better for the words
that are popular nowadays while the frequency-
based measure performs better for the words that
69
were popular in the 60s.
We can see the origin of this peculiar asymme-
try in behavior of similarity and frequency measures
in the following phenomenon. As we already men-
tioned, if a word became popular, the reason can be
a new sense it acquired (a lot of technological terms
are of this kind: ?disk?, ?address?, etc). The change
in such words, that are characterized by a significant
growth in frequency (log-freq  0), is detected by
the human judges, as well as by the similarity mea-
sure. However, other cases such as ?spine?, ?smok-
ing? are also characterized by a significant growth
in frequency, but no semantic change was reported
by raters (nor by the similarity measure). If word
frequency instead decreases, intuitively, a change
in word meaning is less probable. These intuitions
together can explain the behavior of the frequency
measure: for the test set as a whole its performance
is quite high, as it captures this asymmetrical dis-
tribution of words that change meanings, despite its
failure to reliably indicate semantic change for in-
dependent words. A strong evidence for this inter-
pretation is also that, if the frequency measure is
made symmetric, that is, equal for the words that
decreased and the ones that increased in frequency,
it dramatically drops in performance, showing a cor-
relation of just 0.04 with human ranking.
Some interesting observation regarding the per-
formance of the similarity measure can be made af-
ter accurate investigation of ?false-positive? exam-
ples ? the ones that have low similarity but were
ranked as ?not changed? by raters ? like ?sleep?
and ?parent?. It is enough to have a look at their
highest weighted co-occurrences to admit that the
context of their usage has indeed changed (Table
3). These examples show the difference between
the phenomenon of semantic change in linguistics
and the case of context change. It is well known
that the different contexts that distributional seman-
tics catches do not always directly refer to what lin-
guists would consider distinct senses (Reisinger and
Mooney, 2010). Most people would agree that the
word ?parent? has the same meaning now as it had
40 years before, still the social context in which it
is used has evidently changed, reflected by the more
frequent ?single parent family(ies)? collocate found
in the 90s. The same is true for ?sleep?, whose usage
context did not change radically, but might have a
?parent? ?sleep?
60s p. company 2643 deep s. 3803
p. education 1905 s. well 1403
p. corporation 1617 cannot s. 1124
p. material 1337 long s. 1102
p. body 1082 sound s. 1101
p. compound 818 dreamless s. 844
common p. 816 much s. 770
90s p. families 17710 REM s. 20150
single p. 10724 s. apnea 14768
p. company 8367 deep s. 8482
p. education 5884 s. disorders 8427
p. training 5847 s. deprivation 6108
p. involvement 5591 s. disturbances 5973
p. family 5042 s. disturbance 5251
Table 3: Examples of the top weighted 2-grams contain-
ing ?sleep? and ?parent?.
more prominent negative orientation.
The distributional similarity measure captures
therefore two kinds of phenomena: the semantic
change in its linguistic definition, that is, change of
meaning or acquiring a new sense (e.g., ?virus?, ?vir-
tual?), but also the change in the main context in
which the word is used. The latter, in turn, can be
an important preliminary evidence of the onset of
meaning change in its traditional sense, according
to recent studies on language change (Traugott and
Dasher, 2002). Moreover, context changes have cul-
tural and social origins, and therefore the similarity
measure can also be used for collecting evidence of
interest to the humanities and social sciences.
6 Conclusions
In this paper we introduced and evaluated a
novel automatic approach for measuring semantic
change with a distributional similarity model. The
similarity-based measure produces good results, ob-
taining high correlation with human judgements on
test data. The study also suggests that the method
can be suitable to detect both ?proper? semantic
change of words, and cases of major diachronic con-
text change. Therefore, it can be useful for historical
linguistic studies as well as for NLP tasks such as
novel sense detection. Some interesting phenomena
related to changes in relative frequency were also
discovered, and will be the object of further investi-
gations.
70
References
Marco Baroni, Alessandro Lenci. 2010. Distributional
memory: A general framework for corpus-based se-
mantics. Computational Linguistics, 36(4):673?721.
MIT Press, Cambridge, MA, USA.
John A. Bullinaria, Joseph P. Levy. 2007. Extracting
Semantic Representations from Word Co-occurrence
Statistics: A Computational Study. Behavior Research
Methods, 39: 510-526.
Paul Cook, Suzanne Stevenson. 2010. Automatically
identifying changes in the semantic orientation of
words. Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation. Valletta,
Malta: 28?34.
Katrin Erk. 2006. Unknown word sense detection as out-
lier detection. Proceedings of the Human Language
Technology of the North American Chapter of the ACL.
New York, USA: 128?135.
Stefan Evert. 2008. Corpora and collocations. In A.
Ldeling and M. Kyt (eds.), Corpus Linguistics. An In-
ternational Handbook, article 58. Mouton de Gruyter,
Berlin.
Hilpert Martin, Stefan Th. Gries. 2009. Assessing
frequency changes in multi-stage diachronic corpora:
applications for historical corpus linguistics and the
study of language acquisition. Literary and Linguis-
tic Computing, 34(4): 385-40.
Winfred P. Lehmann. 1992. Historical linguistics: an in-
troduction. (3. ed.) Routledge & Kegan Paul, London.
Jean-Baptiste Michel*, Yuan Kui Shen, Aviva
Presser Aiden, Adrian Veres, Matthew K. Gray,
William Brockman, The Google Books Team,
Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Pe-
ter Norvig, Jon Orwant, Steven Pinker, Martin A.
Nowak, and Erez Lieberman Aiden*. 2010. Quanti-
tative Analysis of Culture Using Millions of Digitized
Books. Science (Published online ahead of print:
12/16/2010).
Joseph Reisinger, Raymond Mooney. 2010. A Mixture
Model with Sharing for Lexical Semantics. Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing. MIT, Massachusetts, USA:
1173?1182.
Eyal Sagi, Stefan Kaufmann, Brady Clark. 2009. Se-
mantic Density Analysis: Comparing Word Meaning
across Time and Phonetic Space. Proceedings of the
EACL 2009 Workshop on GEMS: Geometrical Mod-
els of Natural Language Semantics. Athens, Greece:
104?111.
Elizabeth C. Traugott, Richard B. Dasher. 2002. Reg-
ularity in Semantic Change. Cambridge University
Press.
Peter Turney, Patrick Pantel. 2010. From Frequency to
Meaning: Vector Space Models of Semantics. Journal
of Artificial Intelligence Research (JAIR), 37(1):141-
188. AI Access Foundation.
71
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, page 32,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Unseen features.
Collecting semantic data from congenital blind subjects
Alessandro Lenci?, Marco Baroni?, Giovanna Marotta?
?University of Pisa (Italy)
?University of Trento (Italy)
alessandro.lenci@ling.unipi.it, marco.baroni@unitn.it, gmarotta@ling.unipi.it
Congenital blind subjects are able to learn how to use color terms and other types of vision-related
words in a way that is de facto undistinguishable from sighted people. It has actually been proposed
that language provides a rich source of information that blind subjects can exploit to acquire aspects
of word meaning that are related to visual experience, such as the color of fruits or animals. Despite
this, whether and how sensory deprivation affects the structure of semantic representations is still an
open question. In this talk, we present a new, freely available collection of feature norms produced by
congenital blind subjects and normal sighted people. Subjects were asked to produce semantic features
describing the meaning of concrete and abstract nouns and verbs. Data were collected from Italian
subjects, translated into English, and categorized with respect to their semantic type (e.g. hypernym,
meronym, physical property, etc.). First analyses of the feature norms highlight important differences
between blind and sighted subjects, for instance for the role of color and other visual features in the
produced semantic descriptions. This resource can provide new evidence on the role of perceptual
experience in shaping concepts, as well as on its interplay with information extracted from linguistic
data. The norms will also be used to carry out computational experiments with distributional semantic
models to simulate blind and sighted semantic spaces.
32
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50?58,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
General estimation and evaluation
of compositional distributional semantic models
Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
Abstract
In recent years, there has been widespread
interest in compositional distributional
semantic models (cDSMs), that derive
meaning representations for phrases from
their parts. We present an evaluation of al-
ternative cDSMs under truly comparable
conditions. In particular, we extend the
idea of Baroni and Zamparelli (2010) and
Guevara (2010) to use corpus-extracted
examples of the target phrases for param-
eter estimation to the other models pro-
posed in the literature, so that all models
can be tested under the same training con-
ditions. The linguistically motivated func-
tional model of Baroni and Zamparelli
(2010) and Coecke et al (2010) emerges
as the winner in all our tests.
1 Introduction
The need to assess similarity in meaning is cen-
tral to many language technology applications,
and distributional methods are the most robust ap-
proach to the task. These methods measure word
similarity based on patterns of occurrence in large
corpora, following the intuition that similar words
occur in similar contexts. More precisely, vector
space models, the most widely used distributional
models, represent words as high-dimensional vec-
tors, where the dimensions represent (functions
of) context features, such as co-occurring context
words. The relatedness of two words is assessed
by comparing their vector representations.
The question of assessing meaning similarity
above the word level within the distributional
paradigm has received a lot of attention in re-
cent years. A number of compositional frame-
works have been proposed in the literature, each
of these defining operations to combine word vec-
tors into representations for phrases or even en-
tire sentences. These range from simple but ro-
bust methods such as vector addition to more ad-
vanced methods, such as learning function words
as tensors and composing constituents through in-
ner product operations. Empirical evaluations in
which alternative methods are tested in compara-
ble settings are thus called for. This is compli-
cated by the fact that the proposed compositional
frameworks package together a number of choices
that are conceptually distinct, but difficult to disen-
tangle. Broadly, these concern (i) the input repre-
sentations fed to composition; (ii) the composition
operation proper; (iii) the method to estimate the
parameters of the composition operation.
For example, Mitchell and Lapata in their clas-
sic 2010 study propose a set of composition op-
erations (multiplicative, additive, etc.), but they
also experiment with two different kinds of input
representations (vectors recording co-occurrence
with words vs. distributions over latent topics) and
use supervised training via a grid search over pa-
rameter settings to estimate their models. Gue-
vara (2010), to give just one further example, is
not only proposing a different composition method
with respect to Mitchell and Lapata, but he is
also adopting different input vectors (word co-
occurrences compressed via SVD) and an unsu-
pervised estimation method based on minimizing
the distance of composed vectors to their equiva-
lents directly extracted from the source corpus.
Blacoe and Lapata (2012) have recently high-
lighted the importance of teasing apart the differ-
ent aspects of a composition framework, present-
ing an evaluation in which different input vector
representations are crossed with different compo-
sition methods. However, two out of three com-
position methods they evaluate are parameter-free,
so that they can side-step the issue of fixing the pa-
rameter estimation method.
In this work, we evaluate all composition meth-
ods we know of, excluding a few that lag be-
50
hind the state of the art or are special cases of
those we consider, while keeping the estimation
method constant. This evaluation is made pos-
sible by our extension to all target composition
models of the corpus-extracted phrase approxima-
tion method originally proposed in ad-hoc settings
by Baroni and Zamparelli (2010) and Guevara
(2010). For the models for which it is feasible,
we compare the phrase approximation approach
to supervised estimation with crossvalidation, and
show that phrase approximation is competitive,
thus confirming that we are not comparing mod-
els under poor training conditions. Our tests are
conducted over three tasks that involve different
syntactic constructions and evaluation setups. Fi-
nally, we consider a range of parameter settings for
the input vector representations, to insure that our
results are not too brittle or parameter-dependent.1
2 Composition frameworks
Distributional semantic models (DSMs) approxi-
mate word meanings with vectors recording their
patterns of co-occurrence with corpus contexts
(e.g., other words). There is an extensive literature
on how to develop such models and on their eval-
uation (see, e.g., Clark (2012), Erk (2012), Tur-
ney and Pantel (2010)). We focus here on compo-
sitional DSMs (cDSMs). After discussing some
options pertaining to the input vectors, we review
all the composition operations we are aware of
(excluding only the tensor-product-based models
shown by Mitchell and Lapata (2010) to be much
worse than simpler models),2 and then methods to
estimate their parameters.
Input vectors Different studies have assumed
different distributional inputs to composition.
These include bag-of-words co-occurrence vec-
tors, possibly mapped to lower dimensionality
with SVD or other techniques (Mitchell and La-
pata (2010) and many others), vectors whose di-
1We made the software we used to construct seman-
tic models and estimate and test composition methods
available online at http://clic.cimec.unitn.it/
composes/toolkit/
2Erk and Pado? (2008) and Thater et al (2010) use in-
put vectors that have been adapted to their phrasal contexts,
but then apply straightforward composition operations such
as addition and multiplication to these contextualized vec-
tors. Their approaches are thus not alternative cDSMs, but
special ways to construct the input vectors. Grefenstette and
Sadrzadeh (2011a; 2011b) and Kartsaklis et al (2012) pro-
pose estimation techniques for the tensors in the functional
model of Coecke et al (2010). Turney (2012) does not com-
pose representations but similarity scores.
Model Composition function Parameters
Add w1~u + w2~v w1, w2
Mult ~uw1  ~vw2 w1, w2
Dil ||~u||22~v + (? ? 1)?~u,~v?~u ?
Fulladd W1~u + W2~v W1,W2 ? Rm?m
Lexfunc Au~v Au ? Rm?m
Fulllex tanh([W1,W2]
h
Au~v
Av~u
i
) W1,W2,
Au, Av ? Rm?m
Table 1: Composition functions of inputs (u, v).
mensions record the syntactic link between targets
and collocates (Erk and Pado?, 2008; Thater et al,
2010), and most recently vectors based on neural
language models (Socher et al, 2011; Socher et
al., 2012). Blacoe and Lapata (2012) compared
the three representations on phrase similarity and
paraphrase detection, concluding that ?simple is
best?, that is, the bag-of-words approach performs
at least as good or better than either syntax-based
or neural representations across the board. Here,
we take their message home and we focus on bag-
of-words representations, exploring the impact of
various parameters within this approach.
Most frameworks assume that word vectors
constitute rigid inputs fixed before composition,
often using a separate word-similarity task inde-
pendent of composition. The only exception is
Socher et al (2012), where the values in the in-
put vectors are re-estimated during composition
parameter optimization. Our re-implementation of
their method assumes rigid input vectors instead.
Composition operations Mitchell and Lapata
(2008; 2010) present a set of simple but effec-
tive models in which each component of the output
vector is a function of the corresponding compo-
nents of the inputs. Given input vectors ~u and ~v,
the weighted additive model (Add) returns their
weighted sum: ~p = w1~u + w2~v. In the dilation
model (Dil), the output vector is obtained by de-
composing one of the input vectors, say ~v, into
a vector parallel to ~u and an orthogonal vector,
and then dilating only the parallel vector by a fac-
tor ? before re-combining (formula in Table 1).
Mitchell and Lapata also propose a simple mul-
tiplicative model in which the output components
are obtained by component-wise multiplication of
the corresponding input components. We intro-
duce here its natural weighted extension (Mult),
that takes w1 and w2 powers of the components
before multiplying, such that each phrase compo-
nent pi is given by: pi = u
w1
i v
w2
i .
51
Guevara (2010) and Zanzotto et al (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: ~p =
W1~u + W2~v.
Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a function from nouns onto (modified) nouns.
Given that linear functions can be expressed by
matrices and their application by matrix-by-vector
multiplication, a functor (such as the adjective) is
represented by a matrix Au to be composed with
the argument vector ~v (e.g., the noun) by multi-
plication, returning the lexical function (Lexfunc)
representation of the phrase: ~p = Au~v.
The method proposed by Socher et al (2012)
(see Socher et al (2011) for an earlier proposal
from the same team) can be seen as a combination
and non-linear extension of Fulladd and Lexfunc
(that we thus call Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (~u,Au) and (~v,Av), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products Au~v
and Av~u (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.
Estimating composition parameters If we
have manually labeled example data for a target
task, we can use supervised machine learning to
optimize parameters. Mitchell and Lapata (2008;
2010), since their models have just a few param-
eters to optimize, use a direct grid search for the
parameter setting that performs best on the train-
ing data. Socher et al (2012) train their models
using multinomial softmax classifiers.
If our goal is to develop a cDSM optimized for
a specific task, supervised methods are undoubt-
edly the most promising approach. However, ev-
ery time we face a new task, parameters must be
re-estimated from scratch, which goes against the
idea of distributional semantics as a general sim-
ilarity resource (Baroni and Lenci, 2010). More-
over, supervised methods are highly composition-
model-dependent, and for models such as Fulladd
and Lexfunc we are not aware of proposals about
how to estimate them in a supervised manner.
Socher et al (2011) propose an autoencoding
strategy. Given a decomposition function that re-
constructs the constituent vectors from a phrase
vector (e.g., it re-generates green and jacket vec-
tors from the composed green jacket vector), the
composition parameters minimize the distance be-
tween the original and reconstructed input vectors.
This method does not require hand-labeled train-
ing data, but it is restricted to cDSMs for which
an appropriate decomposition function can be de-
fined, and even in this case the learning problem
might lack a closed-form solution.
Guevara (2010) and Baroni and Zamparelli
(2010) optimize parameters using examples of
how the output vectors should look like that are
directly extracted from the corpus. To learn, say, a
Lexfunc matrix representing the adjective green,
we extract from the corpus example vectors of
?N, green N? pairs that occur with sufficient fre-
quency (?car, green car?, ?jacket, green jacket?,
?politician, green politician?, . . . ). We then use
least-squares methods to find weights for the green
matrix that minimize the distance between the
green N vectors generated by the model given the
input N and the corresponding corpus-observed
phrase vectors. This is a very general approach, it
does not require hand-labeled data, and it has the
nice property that corpus-harvested phrase vec-
tors provide direct evidence of the polysemous be-
haviour of functors (the green jacket vs. politician
contexts, for example, will be very different). In
the next section, we extend the corpus-extracted
phrase approximation method to all cDSMs de-
scribed above, with closed-form solutions for all
but the Fulllex model, for which we propose a
rapidly converging iterative estimation method.
3 Least-squares model estimation using
corpus-extracted phrase vectors3
Notation Given two matricesX,Y ? Rm?n we
denote their inner product by ?X,Y ?, (?X,Y ? =
?m
i=1
?n
j=1 xijyij). Similarly we denote by
?u, v? the dot product of two vectors u, v ? Rm?1
and by ||u|| the Euclidean norm of a vector:
3Proofs omitted due to space constraints.
52
||u|| = ?u, u?1/2. We use the following Frobe-
nius norm notation: ||X||F = ?X,X?
1/2. Vectors
are assumed to be column vectors and we use xi
to stand for the i-th (m ? 1)-dimensional column
of matrix X . We use [X,Y ] ? Rm?2n to denote
the horizontal concatenation of two matrices while[
X
Y
]
? R2m?n is their vertical concatenation.
General problem statement We assume vocab-
ularies of constituents U , V and that of resulting
phrases P . The training data consist of a set of
tuples (u, v, p) where p stands for the phrase asso-
ciated to the constituents u and v:
T = {(ui, vi, pi)|(ui, vi, pi) ? U?V?P, 1 ? i ? k}
We build the matrices U, V, P ? Rm?k by con-
catenating the vectors associated to the training
data elements as columns.4
Given the training data matrices, the general
problem can be stated as:
?? = arg min
?
||P ? fcomp?(U, V )||F
where fcomp? is a composition function and ?
stands for a list of parameters that this composition
function is associated to. The composition func-
tions are defined: fcomp? : Rm?1 ? Rm?1 ?
Rm?1 and fcomp?(U, V ) stands for their natural
extension when applied on the individual columns
of the U and V matrices.
Add The weighted additive model returns the
sum of the composing vectors which have been
re-weighted by some scalars w1 and w2: ~p =
w1~u + w2~v. The problem becomes:
w?1, w
?
2 = arg min
w1,w2?R
||P ? w1U ? w2V ||F
The optimal w1 and w2 are given by:
w?1 =
||V ||2F ?U,P ? ? ?U, V ??V, P ?
||U ||2F ||V ||
2
F ? ?U, V ?
2
(1)
w?2 =
||U ||2F ?V, P ? ? ?U, V ??U,P ?
||U ||2F ||V ||
2
F ? ?U, V ?
2
(2)
4In reality, not all composition models require u, v and p
to have the same dimensionality.
Dil Given two vectors ~u and ~v, the dilation
model computes the phrase vector ~p = ||~u||2~v +
(? ? 1)?~u,~v?~u where the parameter ? is a scalar.
The problem becomes:
?? = arg min
??R
||P ?V D||ui||2 ?UD(??1)?ui,vi?||F
where by D||ui||2 and D(??1)?ui,vi? we denote
diagonal matrices with diagonal elements (i, i)
given by ||ui||2 and (? ? 1)?ui, vi? respectively.
The solution is:
?? = 1?
?k
i=1?ui, (||ui||
2vi ? pi)??ui, vi?
?k
i=1?ui, vi?
2||ui||2
Mult Given two vectors ~u and ~v, the weighted
multiplicative model computes the phrase vector
~p = ~uw1  ~vw2 where  stands for component-
wise multiplication. We assume for this model that
U, V, P ? Rm?n++ , i.e. that the entries are strictly
larger than 0: in practice we add a small smooth-
ing constant to all elements to achieve this (Mult
performs badly on negative entries, such as those
produced by SVD). We use the w1 and w2 weights
obtained when solving the much simpler related
problem:5
w?1, w
?
2 = arg min
w1,w2?R
||log(P )?log(U.?w1V.
?w2)||F
where .? stands for the component-wise power op-
eration. The solution is the same as that for Add,
given in equations (1) and (2), with U ? log(U),
V ? log(V ) and P ? log(P ).
Fulladd The full additive model assumes the
composition of two vectors to be ~p = W1~u+W2~v
where W1,W2 ? Rm?m. The problem is:
[W1,W2]
? = arg min
[W1,W2]?Rm?2m
||P?[W1W2]
[
U
V
]
||
This is a multivariate linear regression prob-
lem (Hastie et al, 2009) for which the least
squares estimate is given by: [W1,W2] =
((XTX)?1XTY )T where we use X = [UT , V T ]
and Y = P T .
Lexfunc The lexical function composition
method learns a matrix representation for each
functor (given by U here) and defines composition
as matrix-vector multiplication. More precisely:
5In practice training Mult this way achieves similar or
lower errors in comparison to Add.
53
~p = Au~v where Au is a matrix associated to each
functor u ? U . We denote by Tu the training
data subset associated to an element u, which
contains only tuples which have u as first element.
Learning the matrix representations amounts to
solving the set of problems:
Au = arg min
Au?Rm?m
||Pu ?AuVu||
for each u ? U where Pu, Vu ? Rm?|Tu|
are the matrices corresponding to the Tu train-
ing subset. The solutions are given by: Au =
((VuV Tu )
?1VuP Tu )
T . This composition function
does not use the functor vectors.
Fulllex This model can be seen as a generaliza-
tion of Lexfunc which makes no assumption on
which of the constituents is a functor, so that both
words get a matrix and a vector representation.
The composition function is:
~p = tanh([W1,W2]
[
Au~v
Av~u
]
)
where Au and Av are the matrices associated to
constituents u and v and [W1,W2] ? Rm?2m.
The estimation problem is given in Figure 1.
This is the only composition model which does
not have a closed-form solution. We use a block
coordinate descent method, in which we fix each
of the matrix variables but one and solve the corre-
sponding least-squares linear regression problem,
for which we can use the closed-form solution.
Fixing everything but [W1,W2]:
[W ?1 ,W
?
2 ] = ((X
TX)?1XTY )T
X =
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]
]T
Y = atanh(P T )
Fixing everything but Au for some element u,
the objective function becomes:
||atanh(Pu)?W1AuVu?W2[Av1~u, ..., Avk?~u]||F
where v1...vk? ? V are the elements occurring
with u in the training data and Vu the matrix result-
ing from their concatenation. The update formula
for the Au matrices becomes:
A?u = W
?1
1 ((X
TX)?1XTY )T
X = V Tu
Y = (atanh(Pu)?W2[Av1~u, ..., Avk?~u])
T
In all our experiments, Fulllex estimation con-
verges after very few passes though the matrices.
Despite the very large number of parameters of
this model, when evaluating on the test data we ob-
serve that using a higher dimensional space (such
as 200 dimensions) still performs better than a
lower dimensional one (e.g., 50 dimensions).
4 Evaluation setup and implementation
4.1 Datasets
We evaluate the composition methods on three
phrase-based benchmarks that test the models on
a variety of composition processes and similarity-
based tasks.
Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2008), focuses on
simple sentences consisting of intransitive verbs
and their noun subjects. It contains a total of
120 sentence pairs together with human similar-
ity judgments on a 7-point scale. For exam-
ple, conflict erupts/conflict bursts is scored 7, skin
glows/skin burns is scored 1. On average, each
pair is rated by 30 participants. Rather than eval-
uating against mean scores, we use each rating as
a separate data point, as done by Mitchell and La-
pata. We report Spearman correlations between
human-assigned scores and model cosine scores.
Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We
focus on the latter, and we frame the task dif-
ferently from Turney?s original definition due to
data sparsity issues.6 In our version, the dataset
contains 620 ANs, each paired with a single-
noun paraphrase. Examples include: archaeolog-
ical site/dig, spousal relationship/marriage and
dangerous undertaking/adventure. We evaluate a
model by computing the cosine of all 20K nouns in
our semantic space with the target AN, and look-
ing at the rank of the correct paraphrase in this list.
The lower the rank, the better the model. We re-
port median rank across the test items.
Determiner phrases The last dataset, intro-
duced in Bernardi et al (2013), focuses on a
class of grammatical terms (rather than content
6Turney used a corpus of about 50 billion words, almost
20 times larger than ours, and we have very poor or no cov-
erage of many original items, making the ?multiple-choice?
evaluation proposed by Turney meaningless in our case.
54
W ?1 ,W
?
2 , A
?
u1 , ..., A
?
v1 , ... =arg min
Rm?m
||atanh(P T )? [W1,W2]
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]
]
||F
=arg min
Rm?m
||atanh(P T )?W1[Au1 ~v1, ..., Auk ~vk]?W2[Av1 ~u1, ..., Avk ~uk]||F
Figure 1: Fulllex estimation problem.
words), namely determiners. It is a multiple-
choice test where target nouns (e.g., amnesia)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., no mem-
ory). The task differs from the previous one also
because here the targets are single words, and the
related items are composite. There are 173 tar-
get nouns in total, each paired with one correct
DP response, as well as 5 foils, namely the de-
terminer (no) and noun (memory) from the correct
response and three more DPs, two of which con-
tain the same noun as the correct phrase (less mem-
ory, all memory), the third the same determiner
(no repertoire). Other examples of targets/related-
phrases are polysemy/several senses and tril-
ogy/three books. The models compute cosines be-
tween target noun and responses and are scored
based on their accuracy at ranking the correct
phrase first.
4.2 Input vectors
We extracted distributional semantic vectors us-
ing as source corpus the concatenation of ukWaC,
Wikipedia (2009 dump) and BNC, 2.8 billion to-
kens in total.7 We use a bag-of-words approach
and we count co-occurrences within sentences and
with a limit of maximally 50 words surrounding
the target word. By tuning on the MEN lexical
relatedness dataset,8 we decided to use the top
10K most frequent content lemmas as context fea-
tures (vs. top 10K inflected forms), and we experi-
mented with positive Pointwise and Local Mutual
Information (Evert, 2005) as association measures
(vs. raw counts, log transform and a probability
ratio measure) and dimensionality reduction by
Non-negative Matrix Factorization (NMF, Lee and
Seung (2000)) and Singular Value Decomposition
(SVD, Golub and Van Loan (1996)) (both outper-
forming full dimensionality vectors on MEN). For
7http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk
8http://clic.cimec.unitn.it/?elia.
bruni/MEN
both reduction techniques, we varied the number
of dimensions to be preserved from 50 to 300 in
50-unit intervals. As Local Mutual Information
performed very poorly across composition exper-
iments and other parameter choices, we dropped
it. We will thus report, for each experiment and
composition method, the distribution of the rele-
vant performance measure across 12 input settings
(NMF vs. SVD times 6 dimensionalities). How-
ever, since the Mult model, as expected, worked
very poorly when the input vectors contained neg-
ative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.
4.3 Composition model estimation
Training by approximating the corpus-extracted
phrase vectors requires corpus-based examples of
input (constituent word) and output (phrase) vec-
tors for the composition processes to be learned.
In all cases, training examples are simply selected
based on corpus frequency. For the first experi-
ment, we have 42 distinct target verbs and a total
of ?20K training instances, that is, ??noun, verb?,
noun-verb? tuples (505 per verb on average). For
the second experiment, we have 479 adjectives and
?1 million ??adjective, noun?, adjective-noun?
training tuples (2K per adjective on average). In
the third, 50 determiners and 50K ??determiner,
noun?, determiner-noun? tuples (1K per deter-
miner). For all models except Lexfunc and Ful-
llex, training examples are pooled across target el-
ements to learn a single set of parameters. The
Lexfunc model takes only argument word vectors
as inputs (the functors in the three datasets are
verbs, adjectives and determiners, respectively). A
separate weight matrix is learned for each func-
tor, using the corresponding training data.9 The
Fulllex method jointly learns distinct matrix rep-
resentations for both left- and right-hand side con-
9For the Lexfunc model we have experimented with least
squeares regression with and without regularization, obtain-
ing similar results.
55
stituents. For this reason, we must train this model
on balanced datasets. More precisely, for the in-
transitive verb experiments, we use training data
containing noun-verb phrases in which the verbs
and the nouns are present in the lists of 1,500
most frequent verbs/nouns respectively, adding to
these the verbs and nouns present in our dataset.
We obtain 400K training tuples. We create the
training data similarity for the other datasets ob-
taining 440K adjective-noun and 50K determiner
phrase training tuples, respectively (we also exper-
imented with Fulllex trained on the same tuples
used for the other models, obtaining considerably
worse results than those reported). Finally, for Dil
we treat direction of stretching as a further param-
eter to be optimized, and find that for intransitives
it is better to stretch verbs, in the other datasets
nouns.
For the simple composition models for which
parameters consist of one or two scalars, namely
Add, Mult and Dil, we also tune the parame-
ters through 5-fold crossvalidation on the datasets,
directly optimizing the parameters on the target
tasks. For Add and Mult, we search w1, w2
through the crossproduct of the interval [0 : 5] in
0.2-sized steps. For Dil we use ? ? [0 : 20], again
in 0.2-sized steps.
5 Evaluation results
We begin with some remarks pertaining to the
overall quality of and motivation for corpus-
phrase-based estimation. In seven out of nine
comparisons of this unsupervised technique with
fully supervised crossvalidation (3 ?simple? mod-
els ?Add, Dil and Mult? times 3 test sets), there
was no significant difference between the two esti-
mation methods.10 Supervised estimation outper-
formed the corpus-phrase-based method only for
Dil on the intransitive sentence and AN bench-
marks, but crossvalidated Dil was outperformed
by at least one phrase-estimated simple model on
both benchmarks.
The rightmost boxes in the panels of Fig-
ure 2 depict the performance distribution for us-
ing phrase vectors directly extracted from the
corpus to tackle the various tasks. This non-
compositional approach outperforms all composi-
tional methods in two tasks over three, and it is
one of the best approaches in the third, although
10Significance assessed through Tukey Honestly Signifi-
cant Difference tests (Abdi and Williams, 2010), ? = 0.05.
in all cases even its top scores are far from the
theoretical ceiling. Still, performance is impres-
sive, especially in light of the fact that the non-
compositional approach suffers of serious data-
sparseness problems. Performance on the intran-
sitive task is above state-of-the-art despite the fact
that for almost half of the cases one test phrase
is not in the corpus, resulting in 0 vectors and
consequently 0 similarity pairs. The other bench-
marks have better corpus-phrase coverage (nearly
perfect AN coverage; for DPs, about 90% correct
phrase responses are in the corpus), but many tar-
get phrases occur only rarely, leading to unreliable
distributional vectors. We interpret these results as
a goodmotivation for corpus-phrase-based estima-
tion. On the one hand they show how good these
vectors are, and thus that they are sensible targets
of learning. On the other hand, they do not suffice,
since natural language is infinitely productive and
thus no corpus can provide full phrase coverage,
justifying the whole compositional enterprise.
The other boxes in Figure 2 report the perfor-
mance of the composition methods trained by cor-
pus phrase approximation. Nearly all models are
significantly above chance in all tasks, except for
Fulladd on intransitive sentences. To put AN me-
dian ranks into perspective, consider that a median
rank as high as 8,300 has near-0 probability to oc-
cur by chance. For DP accuracy, random guessing
gets 0.17% accuracy.
Lexfunc emerges consistently as the best model.
On intransitive constructions, it significantly out-
performs all other models except Mult, but the dif-
ference approaches significance even with respect
to the latter (p = 0.071). On this task, Lexfunc?s
median correlation (0.26) is nearly equivalent to
the best correlation across a wide range of parame-
ters reported by Erk and Pado? (2008) (0.27). In the
AN task, Lexfunc significantly outperforms Ful-
llex and Dil and, visually, its distribution is slightly
more skewed towards lower (better) ranks than any
other model. In the DP task, Lexfunc significantly
outperforms Add and Mult and, visually, most of
its distribution lies above that of the other mod-
els. Most importantly, Lexfunc is the only model
that is consistent across the three tasks, with all
other models displaying instead a brittle perfor-
mance pattern.11
Still, the top-performance range of all models
11No systematic trend emerged pertaining to the input vec-
tor parameters (SVD vs. NMF and retained dimension num-
ber).
56
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us
0.00
0.05
0.10
0.15
0.20
0.25
0.30 Intransitive sentences
l
l
l
l
l
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us1
000
800
600
400
200
ANs
l
l
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us
0.15
0.20
0.25
0.30
0.35
DPs
Figure 2: Boxplots displaying composition model performance distribution on three benchmarks, across
input vector settings (6 datapoints for Mult, 12 for all other models). For intransitive sentences, figure of
merit is Spearman correlation, for ANs median rank of correct paraphrase, and for DPs correct response
accuracy. The boxplots display the distribution median as a thick horizontal line within a box extending
from first to third quartile. Whiskers cover 1.5 of interquartile range in each direction from the box, and
extreme outliers outside this extended range are plotted as circles.
on the three tasks is underwhelming, and none of
them succeeds in exploiting compositionality to
do significantly better than using whatever phrase
vectors can be extracted from the corpus directly.
Clearly, much work is still needed to develop truly
successful cDSMs.
The AN results might look particularly worry-
ing, considering that even the top (lowest) median
ranks are above 100. A qualitative analysis, how-
ever, suggests that the actual performance is not
as bad as the numerical scores suggest, since of-
ten the nearest neighbours of the ANs to be para-
phrased are nouns that are as strongly related to
the ANs as the gold standard response (although
not necessarily proper paraphrases). For example,
the gold response to colorimetric analysis is col-
orimetry, whereas the Lexfunc (NMF, 300 dimen-
sions) nearest neighbour is chromatography; the
gold response to heavy particle is baryon, whereas
Lexfunc proposes muon; for melodic phrase the
gold is tune and Lexfunc has appoggiatura; for in-
door garden, the gold is hothouse but Lexfunc pro-
poses glasshouse (followed by the more sophisti-
cated orangery!), and so on and so forth.
6 Conclusion
We extended the unsupervised corpus-extracted
phrase approximation method of Guevara (2010)
and Baroni and Zamparelli (2010) to estimate
all known state-of-the-art cDSMs, using closed-
form solutions or simple iterative procedures in
all cases. Equipped with a general estimation ap-
proach, we thoroughly evaluated the cDSMs in
a comparable setting. The linguistically moti-
vated Lexfunc model of Baroni and Zamparelli
(2010) and Coecke et al (2010) was the win-
ner across three composition tasks, also outper-
forming the more complex Fulllex model, our re-
implementation of Socher et al?s (2012) compo-
sition method (of course, the composition method
is only one aspect of Socher et al?s architecture).
All other composition methods behaved inconsis-
tently.
In the near future, we want to focus on improv-
ing estimation itself. In particular, we want to
explore ways to automatically select good phrase
examples for training, beyond simple frequency
thresholds. We tested composition methods on
two-word phrase benchmarks. Another natural
next step is to apply the composition rules recur-
sively, to obtain representations of larger chunks,
up to full sentences, coming, in this way, nearer to
the ultimate goal of compositional distributional
semantics.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
57
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), Sofia, Bulgaria. In press.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1394?1404, Edinburgh,
UK.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of GEMS, pages 62?66, Ed-
inburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING: Posters, pages 549?558, Mumbai, India.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
58
