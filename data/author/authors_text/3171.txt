A New Probabilistic Model for Title Generation 
 
Rong Jin 
Language Technology Institute 
Carnegie Mellon University  
5000 Forbes Ave. 
Pittsburgh, PA15213, U. S. A. 
rong+@cs.cmu.edu 
Alexander G. Hauptmann 
Department of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA15213, U. S. A. 
alex+@cs.cmu.edu 
 
Abstract  
Title generation is a complex task involving 
both natural language understanding and 
natural language synthesis. In this paper, we 
propose a new probabilistic model for title 
generation. Different from the previous 
statistical models for title generation, which 
treat title generation as a generation process 
that converts the ?document representation? 
of information directly into a ?title 
representation? of the same information, this 
model introduces a hidden state called 
?information source? and divides title 
generation into two steps, namely the step of 
distilling the ?information source? from the 
observation of a document and the step of 
generating a title from the estimated 
?information source?. In our experiment, the 
new probabilistic model outperforms the 
previous model for title generation in terms 
of both automatic evaluations and human 
judgments. 
Introduction 
Compared with a document, a title provides a 
compact representation of the information and 
therefore helps people quickly capture the main 
idea of a document without spending time on the 
details. Automatic title generation is a complex 
task, which not only requires finding the title 
words that reflects the document content but also 
demands ordering the selected title words into 
human readable sequence. Therefore, it involves 
in both nature language understanding and 
nature language synthesis, which distinguishes 
title generation from other seemingly similar 
tasks such as key phrase extraction or automatic 
text summarization where the main concern of 
tasks is identify important information units 
from documents (Mani & Maybury., 1999). 
 
The statistical approach toward title generation 
has been proposed and studied in the recent 
publications (Witbrock & Mittal, 1999; Kennedy 
& Hauptmann, 2000; Jin & Hauptmann, 2001). 
The basic idea is to first learn the correlation 
between the words in titles (title words) and the 
words in the corresponding documents 
(document words) from a given training corpus 
consisting of document-title pairs, and then 
apply the learned title-word-document-word 
correlations to generate titles for unseen 
documents.  
 
Witbrock and Mittal (1999) proposed a 
statistical framework for title generation where 
the task of title generation is decomposed into 
two phases, namely the title word selection 
phase and the title word ordering phase. In the 
phase of title word selection, each title word is 
scored based on its indication of the document 
content. During the title word ordering phase, 
the ?appropriateness? of the word order in a title 
is scored using ngram statistical language model. 
The sequence of title words with highest score in 
both title word selection phase and title word 
ordering phase is chosen as the title for the 
document. The follow-ups within this 
framework mainly focus on applying different 
approaches to the title word selection phase (Jin 
& Hauptmann, 2001; Kennedy & Hauptmann, 
2000). 
 
However, there are two problems with this 
framework for title generation. They are: 
? A problem with the title word ordering 
phase. The goal of title word selection phase is 
to find the appropriate title words for document 
and the goal of title word ordering phase is to 
find the appropriate word order for the selected 
title words. In the framework proposed by 
Witbrock and Mittal (1999), the title word 
ordering phase is accomplished by using ngram 
language model (Clarkson & Rosenfeld, 1997) 
to predict the probability P(T), i.e. how 
frequently the word sequence T is used as a title 
for a document. Of course, the probability for 
the word sequence T to be used as a title for any 
document is definitely influenced by the 
correctness of the word order in T. However, the 
factor whether the words in the sequence T are 
common words or not will also have great 
influence on the chance of seeing the sequence T 
as a title. Word sequence T with many rare 
words, even with a perfect word order, will be 
difficult to match with the content of most 
documents and has small chance to be used as a 
title. As the result, using probability P(T) for the 
purpose of ordering title words can cause the 
generated titles to include unrelated common 
title words. The obvious solution to this problem 
is to somehow eliminate the bias of favouring 
common title words from probability P(T) and 
leave it only with the task of the word ordering.  
? A problem with the title word selection 
phase. The title word selection phase is 
responsible for coming up with a set of title 
words that reflect the meaning of the document. 
In the framework proposed by Witbrock and 
Mittal (1999), every document word has an 
equal vote for title words. However, title only 
needs to reflect the main content of a document 
not every single detail of that document. 
Therefore, letting all the words in the document 
participate equally in the selection of title words 
can cause a large variance in choosing title 
words. For example, common words usually 
have little to do with the content of documents. 
Therefore, allowing common words of a 
document equally compete with the content 
words in the same document in choosing title 
words can seriously degrade the quality of 
generated titles. 
 
The solution we proposed to this problem is to 
introduce a hidden state called ?information 
source?. This ?information source? will sample 
the important content word out of a document 
and a title will be computed based on the 
sampled ?information source? instead of the 
original document. By striping off the common 
words through the ?information source? state, we 
are able to reduce the noise introduced by 
common words to the documents in selecting 
title words. The schematic diagram for the idea 
is shown in Figure 1, together with the 
schematic diagram for the framework by 
Witbrock and Mittal. As indicated by Figure 1, 
the old framework for title generation has only a 
single ?channel? connecting the document words 
to the title words while the new model contains 
two ?channels? with one connecting the 
document words to the ?information source? 
state and the other connecting the ?information 
source? state to the title words.  
 
T itle  W o rd s 
{ T W }  
D o c u m e n t W o rd s  
{ D W }  
P (T W |D W ) 
O ld  M o d e l 
T itle  W ord s 
{ T W }  
D o c u m e n t W o rd s  
{ D W }  
In fo rm a tio n  S o u rc e  
{ D W ?: c o n te n t w o rd }  
N e w  M o d e l 
P (D W ?|D W ) 
P (T W |D W ) 
 
Fig. 1: Graphic representation for previous title generation 
model and new model for title generation. 
1 Probabilistic Title Generation Model 
In the language of probabilistic theory, the goal 
of creating a title T for a document D can be 
formalized as the search of the word sequence T 
that can be best generated by the document D, or 
)|?(maxarg
?
DTPT
T
=  (1) 
Therefore, the key of a probabilistic model for 
title generation is how to estimate the probability 
P(T|D). i.e. the probability of having a word 
sequence T as the title for the document D.  
 
In this section, we will first describe the old 
framework using probability theory and 
associate the two problems of the old framework 
with the flaw in estimation of the probability 
P(T|D). Then a solution to each of the two 
problems will be presented and the new model 
based on the old framework for title generation 
with the adaptation of the solutions will be 
described at the end of this section. 
1.1 Formal Description of Old 
Framework for Title Generation 
In terms of probability theory, the old 
framework can be interpreted as approximating 
the probability P(T|D) as a product of two terms 
with term P({tw?T}|D) responsible for the title 
word selection and term P(T) responsible for the 
title word ordering and the probability P(T|D) 
can be written as: 
)()|}({)|( TPDTtwPDTP ??
 
(2) 
where {tw?T} stands for the set of words in the 
title T. Since P({tw?T}|D) stands for the 
probability of using the set of  words tw in word 
sequence T as title words given the observation 
of the document D, it corresponds to the title 
word selection phase. P(T) stands for the 
probability of using word sequence T as a title 
for any document. Since word sequences with 
wrong word orders are rarely seen as titles for 
any document, the word order in word sequence 
T is an important factor in determining the 
frequency of seeing word sequence T as a title 
for documents and therefore it can be associated 
with the title word ordering phase. 
1.2 Problem with the title word ordering 
phase 
In the old framework for title generation, term 
P(T) is used for ordering title words into a 
correct sequence. However, term P(T) is not 
only influenced by the word order in T, but also 
whether words in T are common words. A word 
sequence T with a set of rare words will have 
small chance to be used as a title for any 
document even if the word order in T is 
perfectly correct. On the other side, a title T with 
a set of common words can have a good chance 
to be a title for some documents even its word 
order is problematic. Therefore, the probability 
for a word sequence T to be used as a title, i.e. 
P(T), is determined by both the 
?appropriateness? of the word order of T and the 
?rareness? of the words in T and doesn?t 
appropriately represent the process of title word 
ordering whose only goal is to identify a correct 
word order with the given words.  
In terms of formal analysis, the problem with the 
title word selection phase can be attributed to the 
oversimplified approximation for probability 
P(T|D). According to the chain rule in 
probability theory, the approximation for P(T|D) 
in Equation (2) is quite problematic and a more 
reasonable expansion for probability P(T|D) 
should be following: 
})({/)()|}({
})?{|()|}({)|(
TtwPTPDTtwP
TtwTPDTtwPDTP
??=
???
 (3) 
where P({tw?T}) stands for the probability of 
using the set of word {tw?T} in titles without 
considering the word order. The difference 
between Equations (3) and (2) is that, Equation 
(2) uses term P(T) directly for title word 
ordering phase while Equation (3) divides term 
P(T) by term P({tw?T}) and uses the result of 
division for title word ordering process. Because 
term P({tw?T}) concerns only with the 
popularity of the words tw in sequence T, 
dividing P(T) by P({tw?T}) has the effect of 
removing the bias of favouring popular title 
words from term P(T). Therefore, term 
P(T)/P({tw?T}) is determined mainly by the 
word order in T and not influenced by the 
popularity of title words in T.  
1.3 Problem with title word selection 
phase 
As already discussed in the introduction section, 
the old framework for title generation allows all 
the words in the document equally participate in 
selecting title words and therefore, the final 
choice of title words may be influenced 
significantly by the common words in the 
document which have nothing to do with the 
content of the document. Thus, we suggest a 
solution to this problem by introducing a hidden 
state called ?information source? which is able to 
sampled the important content words from the 
original document. To find an optimal title for a 
document, we will create the title from the 
?distilled information source? instead of the 
original document.  
 
To allow titles being generated from the 
?distilled information source? instead of the 
original document, we can expand the 
probability P(T|D) as the sum of the 
probabilities P(T| ?information source? S) over 
all the possible ?information sources? S, where 
probability P(T|S) stands for the probability of 
using the word sequence T as the title for the 
?information source? S. Formally, this idea can 
be expressed as: 
?=
S
DSPSTPDTP )|()|()|(  (4) 
where symbol S stands for a possible 
?information source? S for the document D. In 
Equation (4), term P(T|S)P(S|D) represents the 
idea of two noisy channels, with term P(S|D) 
corresponding to the first channel that samples 
?information source? S out of the original 
document D and term P(T|S) corresponding to 
the second noisy channel that creates title T 
from the ?distilled information source? S. Since 
the first noisy channel, i.e. P(S|D), is new to the 
old framework for title generation, we will focus 
on the discussion of the noisy channel P(S|D). 
 
Since the motivation of introducing the hidden 
state ?information source? S is to strip off the 
common words and have important content 
words kept, we want the noisy channel P(S|D) to 
be a sampling process where important content 
words have higher chances to be selected than 
common words. Let function g(dw,D) stands for 
the importance of the word dw related to the 
document D. Then, the word sampling 
distribution should be proportional to the word 
importance function g(dw,D). Therefore, we can 
write the probability P(S|D) 
?
?
?
Sdw
DdwgDSP ),()|(  (5) 
As indicated by Equation (5), the probability for 
?information source? S to represent the content 
of the document D, i.e. P(S|D), is proportional to 
the product of the importance function values for 
all the words selected by ?information source? S. 
1.4 A New Model for Title Generation 
The new model is based on the old framework 
with the proposed solutions to the problems of 
the old framework. As the summary of 
discussions in the previous two subsections, the 
essential idea of this new model is in two 
aspects: 
? Creating titles from the distilled 
?information source?. To prevent the 
common words in the document from voting 
for title words, in the new model, titles will 
be created from the estimated ?information 
source? which has common document words 
stripped off.  
? Subtract the influence of the 
?commonness? of title words from P(T). In 
the old framework for title generation, term 
P(T) is associated with the title word 
ordering phase. Since both the word order 
and the word ?commonness? can influence 
the occurring probability of the sequence T, 
i.e. P(T), we need to subtract the factor of 
word ?commonness? from term P(T), which 
results in term P(T)/P({tw?T}) for the title 
word ordering phase. 
 
T itle  W ords 
{T W 1, T W 2, ? , T W m } 
D ocum ent W ords 
{D W 1, D W 2, ? , D W n} 
Info rm ation So urce 
{D W ?1 , D W ?2 , ? , D W ?m  } 
Sam ple content w ords D W ? o ut o f 
all the w ords D W  using g(D W ,D ) 
C rea te  title  w ord T W  from  D W ?s  
using P (T W |D W ?) 
W ord  Sequence T  
O rder selected  title  w ord  in  a  sequence  
using P (T )/P ({tw ? T }) 
 
Fig. 2: Representation of the title generation scheme used 
by the new model. n is the number of words in the 
document and m is the number of words in the title. 
Therefore, by putting Equations (2), (4) and (5) 
together, our new model for title generation can 
be expressed as 
? ??
?
?
S dw
DdwgSTtwP
TtwP
TPDTP ),()|}({})({
)()|?(  (6) 
By further assuming that the number of words in 
any ?information source? S is equal to the 
number of words in the title T and, words in title 
T are created from the ?information source? S by 
first aligning every title word with a different 
word in the ?information source? S and then 
generating every title word tw from its aligned 
document word dw according to the probability 
distribution P(tw|dw), Equation (5) can be 
simplified as 
? ?
? ??
?
Ttw Ddw
DdwgdwtwP
TtwP
TPDTP ),()|(})({
)()|(  (7) 
Equation (7) is the center of the new 
probabilistic model for title generation. There 
are three components in Equation (7). They are 
word importance function g(dw,D), title-word-
document-word translation probability P(tw|dw) 
and the word ordering component P(T)/ 
P({tw?T}). A schematic diagram in Figure 2 
shows how a title is generated from a document 
in the new model through the three components. 
As shown in Figure 2, a sampling process based 
on the word importance function g(dw,D) will 
be applied to the original document to generate 
the ?information source? set containing most 
content words. Then, a set of title words will be 
scored according to probability P(tw|dw?) based 
on the words dw? selected by the ?information 
source?. Finally, the word ordering process is 
applied to the chosen title words tw using 
P(T)/P({tw?T}). 
1.5 Estimation of Components 
To implement the new model for title 
generation, we need to know how to estimate 
each of the three components.  
? The word importance function g(dw,D). In 
information retrieval, normalized tf.idf value has 
been used as the measurement of the importance 
of a term to a document (Salton & Buckley, 
1988). Therefore, we can adapt normalized tf.idf 
value as the word importance function g(dw,D). 
Therefore, function g(dw,D) can be written as 
?= dw dwidfDdwtfdwidfDdwtfDdwg )(),(/)(),(),(  (8) 
? The title-word-document-word ?translation? 
probability P(tw|dw). The title-word-document-
word ?translation? probability can be estimated 
using statistical translation model. Similar to the 
work of Kennedy and Hauptmann (2000), we 
can treat a document and its title as a 
?translation? pair with the document as in 
?verbose? language and the title as in ?concise? 
language. Therefore, title-word-document-word 
?translation? probability P(tw|dw) can be learned 
from the training corpus using statistical 
translation model (Brown et al, 1990). 
? Word ordering component P(T)/P({tw?T}). 
There are two terms in this component, namely 
P(T) and P({tw?T}). As already used by the old 
framework for title generation, P(T) can be 
estimated using a ngram statistical language 
model (Clarkson & Rosenfeld, 1997). The term 
P({tw?T}), by assuming the independence 
between words tw, can be written as the product 
of the occurring probability of each tw in T, i.e. 
? ??? Ttw twPTtwP )(})({ .  
 
With the expressions for g(dw,D) and 
P({tw?T}) substituted into Equation (6), we 
have the final expression for our model, i.e 
???
?
???
?
?
??
??
??
?
?
??
??
??
?
?
???
?
???
??
? ?
??
? ?
??
Ttw Ddw
Ttw
T
Ddw
dwidfDdwtfdwtwP
twPdwidfDdwtf
TPDTP
)(),()|(
)()(),(
)()|( ||
 (9) 
2 Evaluation 
In this experiment, we introduce two 
different of evaluations, i.e. a F1 metric for 
automatic evaluation and human judgments 
to evaluate the quality of machine-generated 
titles.  
F1 metric is a common evaluation metric 
that has been widely used in information 
retrieval and automatic text summarization. 
Witbrock and Mittal (1999) used the F1 
measurement (Rjiesbergen, 1979) as their 
performance metric. For an automatically 
generated title Tauto, F1 is measured against 
the correspondent human assigned title 
Thuman as follows: 
recallprecision
recallprecision2
  F1
+
??
=  (10) 
Here, precision and recall is measured as the 
number of identical words shared by title 
Tauto and Thuman over the number of words in 
title Tauto and the number of words in title 
Thuman respectively. 
Unfortunately, this metric ignores syntax and 
human readability. In this paper, we also asked 
people to judge the quality of machine-generated 
titles. There are five different quality categories, 
namely ?very good?, ?good?, ?ok?, ?bad?, 
?extremely bad?. A simple score scheme is 
developed with score 5 for the category ?very 
good?, score 4 for ?good?, score 3 for ?ok?, score 
2 for ?bad? and score 1 for ?extremely bad?. The 
average score of human judgment is used as 
another evaluation metric. 
3 Experiment 
3.1 Experiment Design 
The experimental dataset comes from a CD of 
1997 broadcast news transcriptions published by 
Primary Source Media [PrimarySourceMedia, 
1997]. There were a total of 50,000 documents 
and corresponding titles in the dataset. The 
training dataset was formed by randomly 
picking four documents-title pairs from every 
five pairs in the original dataset. Thus, the size 
of training corpus was 40,000 documents with 
corresponding titles. Only 1000 documents 
randomly selected from the remaining 10,000 
documents are used as test collection because of 
computation expensiveness of applying 
language model to sequentialize the title words.  
 
To see the effectiveness of our new model for 
title generation, we implemented the framework 
proposed by Witbrock and Mittal (1999) and 
conducted a contrastive experiment. The length 
of generated titles was fixed to be 6 for both 
methods and all the stop words in the title are 
removed.  
3.2 Examples of Machine-Generated 
Titles 
Table 1 and 2 give 5 examples of the titles 
generated by the old framework and the new 
probabilistic model, respectively. The true titles 
are also listed in Table 1 and 2 for the purpose of 
comparison.  
 
As shown in Table 1, one common problem with 
this set of machine-generated titles is that 
common title words are highly favoured. For 
example, the phrase ?president clinton? is a 
common title phrase and appears in 3 out of 5 
titles and frequently is not necessary. As already 
discussed in previous sections, the problem of 
over-favouring common title words in the old 
framework can be attributed to the use of term 
P(T) for the title word ordering phase. The other 
problem with the set of generated titles in Table 
1 is that, sometimes machine-generated titles 
contain words that have nothing to do with the 
content of the document. For example, the third 
machine-generated title in Table 1 is ?president 
clinton budget tax tobacco settlement? while the 
original corresponding title is ?senate funds fight 
against underage smoking?. By the comparison 
of the two titles, we can see that the word 
?budget? has little to do with the content of the 
story and shouldn?t be selected as title words. 
We think this problem is due to the fact that in 
the old framework for title generation, all the 
words in the document have an equal chance to 
vote for their favourite title words and the votes 
of common words in the document can cause 
unrelated title words to be selected.  
Table 1: Examples of titles generated by the old 
framework. Stopwords are removed 
Original Titles Machine-generated Titles 
bill lann lee president clinton affirmative action 
supreme court 
researchers say stress can 
cause heart disease 
stress heart disease medical news 
day 
senate funds fight against 
underage smoking 
president clinton budget tax 
tobacco settlement 
reaction to john f. 
kennedy jr. speaking out 
about his family 
joe kennedy family reaction 
entertainment news 
clinton?s fast track quest 
and other stories 
vice president clinton gore 
campaign fundraising 
As shown in Table 2, the titles generated by the 
new model appear to be more relevant to the 
content of the document by comparison to the 
original titles. Furthermore, the titles in Table 2 
appear to ?smoother? than the titles listed in 
Table 1 and don?t have unnecessary common 
words in titles. We believe it is due to the effects 
of both modified process for the title word 
ordering and dual noisy channel model. By 
replacing term P(T)/P({tw?T}) with term P(T), 
we make the title word selection phase 
concentrate on finding the correct word order 
and therefore avoid the problem of overly 
favouring common title words. With the 
introduction of the hidden state ?information 
source?, the title words will be selected based on 
the sampled important content words and 
therefore the noise introduced by common 
words in the document is reduced dramatically. 
Table 2: Examples of titles generated by new 
probabilistic model. Stopwords are removed 
Original Titles Machine-generated Titles 
bill lann lee civil rights nominee bill lann lee 
researchers say stress can 
cause heart disease 
study links everyday stress heart 
disease 
senate funds fight against 
underage smoking 
companies settlement tobacco 
deal tax laws 
reaction to john f. kennedy 
jr. speaking out about his 
family 
george magazine discusses joe 
kennedy family 
clinton?s fast track quest 
and other stories 
senate vote fast track trade 
authority 
3.3 Results and Discussions 
The F1 score of each method is computed based 
on the comparison of the 1000 generated titles to 
their original titles using Equation (10). To 
collect human judgments for machine-generated 
titles, we randomly chose 100 documents out of 
the 1000 test documents and sent the machine-
generated titles by both methods to the assessor 
for the quality judgment. The F1 scores and the 
average scores of human judgments for the old 
framework and the new probabilistic model are 
listed in Table 3. 
 
Table 3: Evaluation results of the old framework 
and the new probabilistic model 
 F1 Human Judg. 
Old model 0.21 2.09 
New model 0.26 3.07 
 
 As seen from Table 1, the F1 score for the new 
probabilistic model is better than the score for 
the old model with 0.26 for the new model and 
0.21 for the old model. Since the F1 metric 
basically measures the word overlapping 
between machine-generated titles and the 
original titles, the fact that the new model is 
better than the old model in terms of F1 metric 
indicates that the new model does a better job 
than the old model in terms of finding title 
words appropriate for documents. More 
important, in terms of human judgments, the 
new model also outperforms the old model 
significantly, which implies that titles generated 
by the new model is more readable than the titles 
generated by the old model. Based on these two 
observations, we can conclude that the new 
probabilistic model for title generation is 
effective in generating human readable titles. 
Conclusion 
In this paper, we propose a new probabilistic 
model for title generation. The advantages of the 
new model over the old framework are on the 
modification of the title word ordering phase and 
the introduction of the hidden state ?information 
source?. In the contrastive experiment, the new 
model outperforms the old model significantly 
in terms of both the automatic evaluation metric 
and the human judgments of the qualities of the 
generated titles. Therefore, we conclude that our 
new probabilistic model is effective in creating 
human readable titles. 
Acknowledgements 
The authors are grateful to the anonymous 
reviewers for their comments, which have 
helped improve the quality of the paper. This 
material is based in part on work supported by 
National Science Foundation under Cooperative 
Agreement No. IRI-9817496. Partial support for 
this work was provided by the National Science 
Foundation's National Science, Mathematics, 
Engineering, and Technology Education Digital 
Library Program under grant DUE-0085834. 
This work was also supported in part by the 
Advanced Research and Development Activity 
(ARDA) under contract number MDA908-00-C-
0037. Any opinions, findings, and conclusions 
or recommendations expressed in this material 
are those of the authors and do not necessarily 
reflect the views of the National Science 
Foundation or ARDA. 
References  
I. Mani and M. T. Maybury (1999) Advances in 
Automatic Text. MIT press, pp 51?53. 
M. Witbrock and V. Mittal (1999) Ultra-
Summarization: A Statistical Approach to 
Generating Highly Condensed Non-Extractive 
Summaries, Proceedings of SIGIR 99, Berkeley, 
CA 
R. Jin and A. G. Hauptmann (2001) Learn to Select 
Good Title Word: A New Approach based on 
Reverse Information Retrieval, ICML 2001. 
P. Kennedy and A. G. Hauptmann (2000) Automatic 
Title Generation for the Informedia Multimedia 
Digital Library, ACM Digital Libraries, DL-2000, 
San Antonio Texas 
P. R. Clarkson and R. Rosenfeld (1997) Statistical 
Language Modeling Using the CMU-Cambridge 
Toolkit. Proceedings ESCA Eurospeech. 
G. Salton and C. Buckeley (1988) Term-weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24, 513?523. 
P. Brown, S. Cocke, S. Della Pietra, Della Pietra, F. 
Jelinek, J. Lafferty, R. Mercer, and Roossin (1990) 
A Statistical Approach to Machine Translation. 
Computational Linguistics V. 16, No. 2. 
V. Rjiesbergen (1979) Information Retrieval. Chapter 
7. Butterworths, London. 
Automatic Title Generation for Spoken Broadcast News
Rong Jin
Language Technology Institute
Carnegie Mellon University
Pittsburgh, PA 15213
412-268-7003
rong+@cs.cmu.edu
Alexander G. Hauptmann
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
412-268-1448
alex+@cs.cmu.edu
ABSTRACT
In this paper, we implemented a set of title generation methods
using training set of 21190 news stories and evaluated them on an
independent test corpus of 1006 broadcast news documents,
comparing the results over manual transcription to the results over
automatically recognized speech. We use both F1 and the average
number of correct title words in the correct order as metric.
Overall, the results show that title generation for speech
recognized news documents is possible at a level approaching the
accuracy of titles generated for perfect text transcriptions.
Keywords
Machine learning, title generation
1. INTRODUCTION
To create a title for a document is a complex task. To generate a
title for a spoken document becomes even more challenging
because we have to deal with word errors generated by speech
recognition.
Historically, the title generation task is strongly connected to
traditional summarization because it can be thought of extremely
short summarization. Traditional summarization has emphasized
the extractive approach, using selected sentences or paragraphs
from the document to provide a summary. The weaknesses of this
approach are inability of taking advantage of the training corpus
and producing summarization with small ratio. Thus, it will not be
suitable for title generation tasks.
More recently, some researchers have moved toward ?learning
approaches? that take advantage of training data. Witbrock and
Mittal [1] have used Na?ve Bayesian approach for learning the
document word and title word correlation. However they limited
their statistics to the case that the document word and the title
word are same surface string. Hauptmann and Jin [2] extended
this approach by relaxing the restriction. Treating title generation
problem as a variant of Machine translation problem, Kennedy
and Hauptmann [3] tried the iterative Expectation-Maximization
algorithm. To avoid struggling with organizing selected title
words into human readable sentence, Hauptmann [2] used K
nearest neighbour method for generating titles. In this paper, we
put all those methods together and compare their performance
over 1000 speech recognition documents.
We decompose the title generation problem into two parts:
learning and analysis from the training corpus and generating a
sequence of title words to form the title.
For learning and analysis of training corpus, we present five
different learning methods for comparison: Na?ve Bayesian
approach with limited vocabulary, Na?ve Bayesian approach with
full vocabulary, K nearest neighbors, Iterative Expectation-
Maximization approach, Term frequency and inverse document
frequency method. More details of each approach will be
presented in Section 2.
For the generating part, we decompose the issues involved as
follows: choosing appropriate title words, deciding how many title
words are appropriate for this document title, and finding the
correct sequence of title words that forms a readable title
?sentence?.
The outline of this paper is as follows: Section 1 gave an
introduction to the title generation problem. The details of the
experiment and analysis of results are presented in Section 2.
Section 3 discusses our conclusions drawn from the experiment
and suggests possible improvements.
2. THE CONTRASTIVE TITLE
GENERATION EXPERIMENT
In this section we describe the experiment and present the results.
Section 2.1 describes the data. Section 2.2 discusses the
evaluation method. Section 2.3 gives a detailed description of all
the methods, which were compared. Results and analysis are
presented in section 2.4.
2.1 Data Description
In our experiment, the training set, consisting of 21190 perfectly
transcribed documents, are obtain from CNN web site during
1999. Included with each training document text was a human
assigned title. The test set, consisting of 1006 CNN TV news
story documents for the same year (1999), are randomly selected
from the Informedia Digital Video Library. Each document has a
closed captioned transcript, an alternative transcript generated
with CMU Sphinx speech recognition system with a 64000-word
broadcast news language model and a human assigned title.
2.2 Evaluation
First, we evaluate title generation by different approaches using
the F1 metric. For an automatically generated title Tauto, F1 is
measured against corresponding human assigned title Thuman as
follows:
F1 = 2?precision?recall / (precision + recall)
Here, precision and recall is measured respectively as the number
of identical words in Tauto and Thuman over the number of
words in Tauto and the number of words in Thuman. Obviously
the sequential word order of the generated title words is ignored
by this metric.
To measure how well a generated title compared to the original
human generated title in terms of word order, we also measured
the number of correct title words in the hypothesis titles that were
in the same order as in the reference titles.
We restrict all approaches to generate only 6 title words, which is
the average number of title words in the training corpus. Stop
words were removed throughout the training and testing
documents and also removed from the titles.
2.3 Description of the Compared Title
Generation Approaches
The five different title generation methods are:
1. Na?ve Bayesian approach with limited vocabulary (NBL).
It tries to capture the correlation between the words in the
document and the words in the title. For each document word
DW, it counts the occurrence of title word same as DW and
apply the statistics to the test documents for generating titles.
2. Na?ve Bayesian approach with full vocabulary (NBF). It
relaxes the constraint in the previous approach and counts all
the document-word-title-word pairs. Then this full statistics
will be applied on generating titles for the test documents.
3. Term frequency and inverse document frequency
approach (TF.IDF). TF is the frequency of words occurring
in the document and IDF is logarithm of the total number of
documents divided by the number of documents containing
this word. The document words with highest TF.IDF were
chosen for the title word candidates.
4. K nearest neighbor approach (KNN). This algorithm is
similar to the KNN algorithm applied to topic classification.
It searches the training document set for the closest related
document and assign the training document title to the new
document as title.
5. Iterative Expectation-Maximization approach (EM). It
views documents as written in a ?verbal? language and their
titles as written a ?concise? language. It builds the translation
model between the ?verbal? language and the ?concise?
language from the documents and titles in the training corpus
and ?translate? each testing document into title.
2.4 The sequentializing process for title word
candidates
To generate an ordered set of candidates, equivalent to what we
would expect to read from left to right, we built a statistical
trigram language model using the SLM tool-kit (Clarkson, 1997)
and the 40,000 titles in the training set. This language model was
used to determine the most likely order of the title word
candidates generated by the NBL, NBF, EM and TF.IDF methods.
3. RESULTS AND OBSERVATIONS
The experiment was conducted both on the closed caption
transcripts and automatic speech recognized transcripts. The F1
results and the average number of correct title word in correct
order are shown in Figure 1 and 2 respectively.
KNN works surprisingly well.  KNN generates titles for a new
document by choosing from the titles in the training corpus. This
works fairly well because both the training set and test set come
from CNN news of the same year. Compared to other methods,
KNN degrades much less with speech-recognized transcripts.
Meanwhile, even though KNN performance not as well as TF.IDF
and NBL in terms of F1 metric, it performances best in terms of
the average number of correct title words in the correct order. If
consideration of human readability matters, we would expect
KNN to outperform considerately all the other approaches since it
is guaranteed to generate human readable title.
Comparison of F1 
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
KN
N
TF
IDF NB
L
NB
F EM
Methods
F1
original
documents
spoken
documents
Figure 1: Comparison of Title Generation Approaches on a
test corpus of 1006 documents with either perfect transcript or
speech recognized transcripts using the F1 score.
NBF performs much worse than NBL. NBF performances much
worse than NBL in both metrics. The difference between NBF and
NBL is that NBL assumes a document word can only generate a
title word with the same surface string. Though it appears that
NBL loses information with this very strong assumption, the
results tell us that some information can safely be ignored. In
NBF, nothing distinguishes between important words and trivial
words. This lets frequent, but unimportant words dominate the
document-word-title-word correlation.
Light learning approach TF.IDF performances considerably
well compared with heavy learning approaches. Surprisingly,
heavy learning approaches, NBL, NBF and EM algorithm didn?t
out performance the light learning approach TF.IDF. We think
learning the association between document words and title words
by inspecting directly the document and its title is very
problematic since many words in the document don?t reflect its
content. The better strategy should be distilling the document first
before learning the correlation between document words and title
words.
Comparison of # of Correct Words in 
Correct Order
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
KN
N
TF
IDF NB
L
NB
F EM
Methods
# 
of
 
Co
rr
ec
t W
o
rd
s 
in
 
th
e 
Co
rr
ec
t O
rd
er
original
documents
spoken
documents
Figure 1: Comparison of Title Generation Approaches on a
test corpus of 1006 documents with either perfect transcript or
speech recognized transcripts using the average number of
correct words in the correct order.
4. CONCLUSION
From the analysis discussed in previous section, we draw the
following conclusions:
1. The KNN approach works well for title generation especially
when overlap in content between training dataset and test
collection is large.
2. The fact that NBL out performances NBF and TF.IDF out
performance NBL and suggests that we need to distinguish
important document words from those trivial words.
5. ACKNOWLEDGMENTS
This material is based in part on work supported by National
Science Foundation under Cooperative Agreement No. IRI-
9817496. Partial support for this work was provided by the
National Science Foundation?s National Science, Mathematics,
Engineering, and Technology Education Digital Library Program
under grant DUE-0085834. This work was also supported in part
by the Advanced Research and Development Activity (ARDA)
under contract number MDA908-00-C-0037. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect the
views of the National Science Foundation or ARDA.
6. REFERENCES
[1] Michael Witbrock and Vibhu Mittal. Ultra-Summarization:
A Statistical Approach to Generating Highly Condensed
Non-Extractive Summaries. Proceedings of SIGIR 99,
Berkeley, CA, August 1999.
[2] R. Jin and A.G. Hauptmann. Title Generation for Spoken
Broadcast News using a Training Corpus. Proceedings of 6th
Internal Conference on Language Processing (ICSLP 2000),
Beijing China. 2000.
[3] P. Kennedy and A.G. Hauptmann. Automatic Title
Generation for the Informedia Multimedia Digital Library.
ACM Digital Libraries, DL-2000, San Antonio Texas, May
2000.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1057?1064,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Are These Documents Written from Different Perspectives? A Test of
Different Perspectives Based On Statistical Distribution Divergence
Wei-Hao Lin
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 U.S.A.
whlin@cs.cmu.edu
Alexander Hauptmann
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 U.S.A.
alex@cs.cmu.edu
Abstract
In this paper we investigate how to auto-
matically determine if two document col-
lections are written from different per-
spectives. By perspectives we mean a
point of view, for example, from the per-
spective of Democrats or Republicans. We
propose a test of different perspectives
based on distribution divergence between
the statistical models of two collections.
Experimental results show that the test can
successfully distinguish document collec-
tions of different perspectives from other
types of collections.
1 Introduction
Conflicts arise when two groups of people take
very different perspectives on political, socio-
economical, or cultural issues. For example, here
are the answers that two presidential candidates,
John Kerry and George Bush, gave during the third
presidential debate in 2004 in response to a ques-
tion on abortion:
(1) Kerry: What is an article of faith for me is
not something that I can legislate on some-
body who doesn?t share that article of faith. I
believe that choice is a woman?s choice. It?s
between a woman, God and her doctor. And
that?s why I support that.
(2) Bush: I believe the ideal world is one in
which every child is protected in law and wel-
comed to life. I understand there?s great dif-
ferences on this issue of abortion, but I be-
lieve reasonable people can come together
and put good law in place that will help re-
duce the number of abortions.
After reading the above transcripts some readers
may conclude that one takes a ?pro-choice? per-
spective while the other takes a ?pro-life? perspec-
tive, the two dominant perspectives in the abortion
controversy.
Perspectives, however, are not always mani-
fested when two pieces of text together are put to-
gether. For example, the following two sentences
are from Reuters newswire:
(3) Gold output in the northeast China province
of Heilongjiang rose 22.7 pct in 1986 from
1985?s level, the New China News Agency
said.
(4) Exco Chairman Richard Lacy told Reuters
the acquisition was being made from Bank
of New York Co Inc, which currently holds
a 50.1 pct, and from RMJ partners who hold
the remainder.
A reader would not from this pair of examples per-
ceive as strongly contrasting perspectives as the
Kerry-Bush answers. Instead, as the Reuters an-
notators did, one would label Example 3 as ?gold?
and Example 4 as ?acquisition?, that is, as two top-
ics instead of two perspectives.
Why does the contrast between Example 1 and
Example 2 convey different perspectives, but the
contrast between Example 3 and Example 4 result
in different topics? How can we define the impal-
pable ?different perspectives? anyway? The defi-
nition of ?perspective? in the dictionary is ?subjec-
tive evaluation of relative significance,?1 but can
we have a computable definition to test the exis-
tence of different perspectives?
1The American Heritage Dictionary of the English Lan-
guage, 4th ed. We are interested in identifying ?ideologi-
cal perspectives? (Verdonk, 2002), not first-person or second-
person ?perspective? in narrative.
1057
The research question about the definition of
different perspectives is not only scientifically in-
triguing, it also enables us to develop important
natural language processing applications. Such
a computational definition can be used to detect
the emergence of contrasting perspectives. Me-
dia and political analysts regularly monitor broad-
cast news, magazines, newspapers, and blogs to
see if there are public opinion splitting. The huge
number of documents, however, make the task ex-
tremely daunting. Therefore an automated test of
different perspectives will be very valuable to in-
formation analysts.
We first review the relevant work in Section 2.
We take a model-based approach to develop a
computational definition of different perspectives.
We first develop statistical models for the two doc-
ument collections, A and B, and then measure the
degree of contrast by calculating the ?distance?
between A and B. How document collections are
statistically modeled and how distribution differ-
ence is estimated are described in Section 3. The
document corpora are described in Section 4. In
Section 5, we evaluate how effective the proposed
test of difference perspectives based on statistical
distribution. The experimental results show that
the distribution divergence can successfully sepa-
rate document collections of different perspectives
from other kinds of collection pairs. We also in-
vestigate if the pattern of distribution difference is
due to personal writing or speaking styles.
2 Related Work
There has been interest in understanding how be-
liefs and ideologies can be represented in comput-
ers since mid-sixties of the last century (Abelson
and Carroll, 1965; Schank and Abelson, 1977).
The Ideology Machine (Abelson, 1973) can simu-
late a right-wing ideologue, and POLITICS (Car-
bonell, 1978) can interpret a text from conserva-
tive or liberal ideologies. In this paper we take
a statistics-based approach, which is very differ-
ent from previous work that rely very much on
manually-constructed knowledge base.
Note that what we are interested in is to deter-
mine if two document collections are written from
different perspectives, not to model individual per-
spectives. We aim to capture the characteristics,
specifically the statistical regularities of any pairs
of document collections with opposing perspec-
tives. Given a pair of document collections A and
B, our goal is not to construct classifiers that can
predict if a document was written from the per-
spective of A or B (Lin et al, 2006), but to deter-
mine if the document collection pair (A,B) con-
vey opposing perspectives.
There has been growing interest in subjectivity
and sentiment analysis. There are studies on learn-
ing subjective language (Wiebe et al, 2004), iden-
tifying opinionated documents (Yu and Hatzivas-
siloglou, 2003) and sentences (Riloff et al, 2003;
Riloff and Wiebe, 2003), and discriminating be-
tween positive and negative language (Turney and
Littman, 2003; Pang et al, 2002; Dave et al,
2003; Nasukawa and Yi, 2003; Morinaga et al,
2002). There are also research work on automati-
cally classifying movie or product reviews as pos-
itive or negative (Nasukawa and Yi, 2003; Mullen
and Collier, 2004; Beineke et al, 2004; Pang and
Lee, 2004; Hu and Liu, 2004).
Although we expect by its very nature much of
the language used when expressing a perspective
to be subjective and opinionated, the task of la-
beling a document or a sentence as subjective is
orthogonal to the test of different perspectives. A
subjectivity classifier may successfully identify all
subjective sentences in the document collection
pair A and B, but knowing the number of sub-
jective sentences in A and B does not necessarily
tell us if they convey opposing perspectives. We
utilize the subjectivity patterns automatically ex-
tracted from foreign news documents (Riloff and
Wiebe, 2003), and find that the percentages of
the subjective sentences in the bitterlemons corpus
(see Section 4) are similar (65.6% in the Pales-
tinian documents and 66.2% in the Israeli docu-
ments). The high but almost equivalent number of
subjective sentences in two perspectives suggests
that perspective is largely expressed in subjective
language but subjectivity ratio is not enough to tell
if two document collections are written from the
same (Palestinian v.s. Palestinian) or different per-
spectives (Palestinian v.s. Israeli)2.
3 Statistical Distribution Divergence
We take a model-based approach to measure to
what degree, if any, two document collections are
different. A document is represented as a point
2However, the close subjectivity ratio doesn?t mean that
subjectivity can never help identify document collections of
opposing perspectives. For example, the accuracy of the test
of different perspectives may be improved by focusing on
only subjective sentences.
1058
in a V -dimensional space, where V is vocabulary
size. Each coordinate is the frequency of a word
in a document, i.e., term frequency. Although vec-
tor representation, commonly known as a bag of
words, is oversimplified and ignores rich syntactic
and semantic structures, more sophisticated rep-
resentation requires more data to obtain reliable
models. Practically, bag-of-word representation
has been very effective in many tasks, including
text categorization (Sebastiani, 2002) and infor-
mation retrieval (Lewis, 1998).
We assume that a collection of N documents,
y1, y2, . . . , yN are sampled from the following
process,
? ? Dirichlet(?)
yi ? Multinomial(ni, ?).
We first sample a V -dimensional vector ? from a
Dirichlet prior distribution with a hyperparameter
?, and then sample a document yi repeatedly from
a Multinomial distribution conditioned on the pa-
rameter ?, where ni is the document length of the
ith document in the collection and assumed to be
known and fixed.
We are interested in comparing the parameter ?
after observing document collections A and B:
p(?|A) = p(A|?)p(?)p(A)
= Dirichlet(?|?+
?
yi?A
yi).
The posterior distribution p(?|?) is a Dirichlet dis-
tribution since a Dirichlet distribution is a conju-
gate prior for a Multinomial distribution.
How should we measure the difference between
two posterior distributions p(?|A) and p(?|B)?
One common way to measure the difference be-
tween two distributions is Kullback-Leibler (KL)
divergence (Kullback and Leibler, 1951), defined
as follows,
D(p(?|A)||p(?|B))
=
?
p(?|A) log p(?|A)p(?|B) d?. (5)
Directly calculating KL divergence according to
(5) involves a difficult high-dimensional integral.
As an alternative, we approximate KL divergence
using Monte Carlo methods as follows,
1. Sample ?1, ?2, . . . , ?M from Dirichlet(?|?+
?
yi?A yi).
2. Return D? = 1M
?M
i=1 log p(?i|A)p(?i|B) as a Monte
Carlo estimate of D(p(?|A)||p(?|B)).
Algorithms of sampling from Dirichlet distribu-
tion can be found in (Ripley, 1987). As M ? ?,
the Monte Carlo estimate will converge to true KL
divergence by the Law of Large Numbers.
4 Corpora
To evaluate how well KL divergence between pos-
terior distributions can discern a document collec-
tion pair of different perspectives, we collect two
corpora of documents that were written or spoken
from different perspectives and one newswire cor-
pus that covers various topics, as summarized in
Table 1. No stemming algorithms is performed;
no stopwords are removed.
Corpus Subset |D| ?|d| V
bitterlemons
Palestinian 290 748.7 10309
Israeli 303 822.4 11668
Pal. Editor 144 636.2 6294
Pal. Guest 146 859.6 8661
Isr. Editor 152 819.4 8512
Isr. Guest 151 825.5 8812
2004
Presiden-
tial
Debate
Kerry 178 124.7 2554
Bush 176 107.8 2393
1st Kerry 33 216.3 1274
1st Bush 41 155.3 1195
2nd Kerry 73 103.8 1472
2nd Bush 75 89.0 1333
3rd Kerry 72 104.0 1408
3rd Bush 60 98.8 1281
Reuters-
21578
ACQ 2448 124.7 14293
CRUDE 634 214.7 9009
EARN 3987 81.0 12430
GRAIN 628 183.0 8236
INTEREST 513 176.3 6056
MONEY-FX 801 197.9 8162
TRADE 551 255.3 8175
Table 1: The number of documents |D|, average
document length ?|d| , and vocabulary size V of
the three corpora.
The first perspective corpus consists of arti-
cles published on the bitterlemons website3 from
late 2001 to early 2005. The website is set up
to ?contribute to mutual understanding [between
Palestinians and Israelis] through the open ex-
change of ideas?4. Every week an issue about the
Israeli-Palestinian conflict is selected for discus-
sion (e.g., ?Disengagement: unilateral or coordi-
nated??), and a Palestinian editor and an Israeli
editor each contribute one article addressing the
3http://www.bitterlemons.org/
4http://www.bitterlemons.org/about/
about.html
1059
issue. In addition, the Israeli and Palestinian ed-
itors interview a guest to express their views on
the issue, resulting in a total of four articles in a
weekly edition. The perspective from which each
article is written is labeled as either Palestinian or
Israeli by the editors.
The second perspective corpus consists of the
transcripts of the three Bush-Kerry presidential de-
bates in 2004. The transcripts are from the website
of the Commission on Presidential Debates5. Each
spoken document is roughly an answer to a ques-
tion or a rebuttal. The transcript are segmented
by the speaker tags already in the transcripts. All
words from moderators are discarded.
The topical corpus contains newswire from
Reuters in 1987. Reuters-215786 is one of the
most common testbeds for text categorization.
Each document belongs to none, one, or more of
the 135 categories (e.g., ?Mergers? and ?U.S. Dol-
lars?.) The number of documents in each category
is not evenly distributed (median 9.0, mean 105.9).
To estimate statistics reliably, we only consider
categories with more than 500 documents, result-
ing in a total of seven categories (ACQ, CRUDE,
EARN, GRAIN, INTEREST, MONEY-FX, and
TRADE).
5 Experiments
A test of different perspectives is acute when it
can draw distinctions between document collec-
tion pairs of different perspectives and document
collection pairs of the same perspective and others.
We thus evaluate the proposed test of different per-
spectives in the following four types of document
collection pairs (A,B):
Different Perspectives (DP) A and B are writ-
ten from different perspectives. For example,
A is written from the Palestinian perspective
and B is written from the Israeli perspective
in the bitterlemons corpus.
Same Perspective (SP) A and B are written from
the same perspective. For example, A and B
consist of the words spoken by Kerry.
Different Topics (DT) A and B are written on
different topics. For example, A is about
5http://www.debates.org/pages/
debtrans.html
6http://www.ics.uci.edu/?kdd/
databases/reuters21578/reuters21578.html
acquisition (ACQ) and B is about crude oil
(CRUDE).
Same Topic (ST) A and B are written on the
same topic. For example, A and B are both
about earnings (EARN).
The effectiveness of the proposed test of differ-
ent perspectives can thus be measured by how the
distribution divergence of DP document collection
pairs is separated from the distribution divergence
of SP, DT, and ST document collection pairs. The
little the overlap of the range of distribution di-
vergence, the sharper the test of different perspec-
tives.
To account for large variation in the number of
words and vocabulary size across corpora, we nor-
malize the total number of words in a document
collection to be the same K, and consider only the
top C% frequent words in the document collection
pair. We vary the values of K and C , and find that
K changes the absolute scale of KL divergence
but does not change the rankings of four condi-
tions. Rankings among four conditions is consis-
tent when C is small. We only report results of
K = 1000, C = 10 in the paper due to space limit.
There are two kinds of variances in the estima-
tion of divergence between two posterior distribu-
tion and should be carefully checked. The first
kind of variance is due to Monte Carlo methods.
We assess the Monte Carlo variance by calculat-
ing a 100? percent confidence interval as follows,
[D? ? ??1(?2 )
???
M
, D? + ??1(1? ?2 )
???
M
]
where ??2 is the sample variance of ?1, ?2, . . . , ?M ,
and ?(?)?1 is the inverse of the standard normal
cumulative density function. The second kind of
variance is due to the intrinsic uncertainties of data
generating processes. We assess the second kind
of variance by collecting 1000 bootstrapped sam-
ples, that is, sampling with replacement, from each
document collection pair.
5.1 Quality of Monte Carlo Estimates
The Monte Carlo estimates of the KL divergence
from several document collection pair are listed in
Table 2. A complete list of the results is omit-
ted due to the space limit. We can see that the
95% confidence interval captures well the Monte
Carlo estimates of KL divergence. Note that KL
divergence is not symmetric. The KL divergence
1060
A B D? 95% CI
ACQ ACQ 2.76 [2.62, 2.89]
Palestinian Palestinian 3.00 [3.54, 3.85]
Palestinian Israeli 27.11 [26.64, 27.58]
Israeli Palestinian 28.44 [27.97, 28.91]
Kerry Bush 58.93 [58.22, 59.64]
ACQ EARN 615.75 [610.85, 620.65]
Table 2: The Monte Carlo estimate D? and 95%
confidence interval (CI) of the Kullback-Leibler
divergence of several document collection pairs
(A,B) with the number of Monte Carlo samples
M = 1000.
of the pair (Israeli, Palestinian) is not necessarily
the same as (Palestinian, Israeli). KL divergence is
greater than zero (Cover and Thomas, 1991) and
equal to zero only when document collections A
and B are exactly the same. Here (ACQ, ACQ) is
close to but not exactly zero because they are dif-
ferent samples of documents in the ACQ category.
Since the CIs of Monte Carlo estimates are reason-
ably tight, we assume them to be exact and ignore
the errors from Monte Carlo methods.
5.2 Test of Different Perspectives
We now present the main result of the paper.
We calculate the KL divergence between poste-
rior distributions of document collection pairs in
four conditions using Monte Carlo methods, and
plot the results in Figure 1. The test of different
perspectives based on statistical distribution diver-
gence is shown to be very acute. The KL diver-
gence of the document collection pairs in the DP
condition fall mostly in the middle range, and is
well separated from the high KL divergence of the
pairs in DT condition and from the low KL diver-
gence of the pairs in SP and ST conditions. There-
fore, by simply calculating the KL divergence of
a document collection pair, we can reliably pre-
dict that they are written from different perspec-
tives if the value of KL divergence falls in the
middle range, from different topics if the value is
very large, from the same topic or perspective if
the value is very small.
5.3 Personal Writing Styles or Perspectives?
One may suspect that the mid-range distribution
divergence is attributed to personal speaking or
writing styles and has nothing to do with differ-
ent perspectives. The doubt is expected because
half of the bitterlemons corpus are written by one
Palestinian editor and one Israeli editor (see Ta-
ble 1), and the debate transcripts come from only
two candidates.
We test the hypothesis by computing the dis-
tribution divergence of the document collection
pair (Israeli Guest, Palestinian Guest), that is, a
Different Perspectives (DP) pair. There are more
than 200 different authors in the Israeli Guest and
Palestinian Guest collection. If the distribution di-
vergence of the pair with diverse authors falls out
of the middle range, it will support that mid-range
divergence is due to writing styles. On the other
hand, if the distribution divergence still fall in the
middle range, we are more confident the effect
is attributed to different perspectives. We com-
pare the distribution divergence of the pair (Israeli
Guest, Palestinian Guest) with others in Figure 2.
ST SP DP Guest DT
KL
 D
iv
er
ge
nc
e
1
2
5
10
20
50
20
0
50
0
Figure 2: The average KL divergence of document
collection pairs in the bitterlemons Guest subset
(Israeli Guest vs. Palestinian Guest), ST ,SP, DP,
DT conditions. The horizontal lines are the same
as those in Figure 1.
The results show that the distribution diver-
gence of the (Israeli Guest, Palestinian Guest) pair,
as other pairs in the DP condition, still falls in the
middle range, and is well separated from SP and
ST in the low range and DT in the high range. The
decrease in KL divergence due to writing or speak-
ing styles is noticeable, and the overall effect due
to different perspectives is strong enough to make
the test robust. We thus conclude that the test of
different perspectives based on distribution diver-
gence indeed captures different perspectives, not
personal writing or speaking styles.
5.4 Origins of Differences
While the effectiveness of the test of different per-
spectives is demonstrated in Figure 1, one may
1061
2 5 10 20 50 100 200 500 1000
0.
00
0.
05
0.
10
0.
15
KL Divergence
D
en
si
ty
SP
ST
DP
DT
Figure 1: The KL divergence of the document collection pairs in four conditions: Different Perspectives
(DP), Same Perspective (SP), Different Topics (DT), and Same Topic (ST). Note that the x axis is in log
scale. The Monte Carlo estimates D? of the pairs in DP condition are plotted as rugs. D? of the pairs in
other conditions are omitted to avoid clutter and summarized in one-dimensional density using Kernel
Density Estimation. The vertical lines are drawn at the points with equivalent densities.
wonder why the distribution divergence of the
document collection pair with different perspec-
tives falls in the middle range and what causes the
large and small divergence of the document collec-
tion pairs with different topics (DT) and the same
topic (ST) or perspective (SP), respectively. In
other words where do the differences result from?
We answer the question by taking a closer look
at the causes of the distribution divergence in our
model. We compare the expected marginal dif-
ference of ? between two posterior distributions
p(?|A) and p(?|B). The marginal distribution of
the i-th coordinate of ?, that is, the i-th word in the
vocabulary, is a Beta distribution, and thus the ex-
pected value can be easily calculated. We plot the
?? = E[?i|A]? E[?i|B] against E[?i|A] for each
condition in Figure 3.
How ?? is deviated from zero partially explains
different patterns of distribution divergence in Fig-
ure 1. In Figure 3d we see that the ?? increases
as ? increases, and the deviance from zero is much
greater than those in the Same Perspective (Fig-
ure 3b) and Same Topic (Figure 3a) conditions.
The large ?? not only accounts for large distribu-
tion divergence of the document pairs in DT con-
ditions, but also shows that words in different top-
ics that is frequent in one topic are less likely to be
frequent in the other topic. At the other extreme,
document collection pairs of the Same Perspective
(SP) or Same Topic (ST) show very little differ-
ence in ?, which matches our intuition that docu-
ments of the same perspective or the same topic
use the same vocabulary in a very similar way.
The manner in which ?? is varied with the
value of ? in the Different Perspective (DP) con-
dition is very unique. The ?? in Figure 3c is not
as small as those in the SP and ST conditions,
but at the same time not as large as those in DT
conditions, resulting in mid-range distribution di-
vergence in Figure 1. Why do document collec-
tions of different perspectives distribute this way?
Partly because articles from different perspectives
focus on the closely related issues (the Palestinian-
Israeli conflict in the bitterlemons corpus, or the
political and economical issues in the debate cor-
pus), the authors of different perspectives write or
speak in a similar vocabulary, but with emphasis
on different words.
6 Conclusions
In this paper we develop a computational test of
different perspectives based on statistical distri-
bution divergence between the statistical models
of document collections. We show that the pro-
1062
0.00 0.01 0.02 0.03 0.04 0.05 0.06
?
0.
04
?
0.
02
0.
00
0.
02
0.
04
(a) Same Topic (ST)
0.00 0.01 0.02 0.03 0.04 0.05 0.06
?
0.
04
?
0.
02
0.
00
0.
02
0.
04
(b) Same Topic (SP)
0.00 0.01 0.02 0.03 0.04 0.05 0.06
?
0.
04
?
0.
02
0.
00
0.
02
0.
04
0.00 0.01 0.02 0.03 0.04 0.05 0.06
?
0.
04
?
0.
02
0.
00
0.
02
0.
04
(c) Two examples of Different Perspective (DP)
Figure 3: The ?? vs. ? plots of the typical docu-
ment collection pairs in four conditions. The hori-
zontal line is ?? = 0.
0.00 0.01 0.02 0.03 0.04 0.05 0.06
?
0.
04
?
0.
02
0.
00
0.
02
0.
04
0.00 0.01 0.02 0.03 0.04 0.05 0.06
?
0.
04
?
0.
02
0.
00
0.
02
0.
04
(d) Two examples of Different Topics (DT)
Figure 3: Cont?d
posed test can successfully separate document col-
lections of different perspectives from other types
of document collection pairs. The distribution di-
vergence falling in the middle range can not sim-
ply be attributed to personal writing or speaking
styles. From the plot of multinomial parameter
difference we offer insights into where the differ-
ent patterns of distribution divergence come from.
Although we validate the test of different per-
spectives by comparing the DP condition with DT,
SP, and ST conditions, the comparisons are by
no means exhaustive, and the distribution diver-
gence of some document collection pairs may also
fall in the middle range. We plan to investigate
more types of document collections pairs, e.g., the
document collections from different text genres
(Kessler et al, 1997).
Acknowledgment
We would like thank the anonymous reviewers for
useful comments and suggestions. This material
is based on work supported by the Advanced Re-
search and Development Activity (ARDA) under
contract number NBCHC040037.
1063
References
Robert P. Abelson and J. Douglas Carroll. 1965. Com-
puter simulation of individual belief systems. The
American Behavioral Scientist, 8:24?30, May.
Robert P. Abelson, 1973. Computer Models of Thought
and Language, chapter The Structure of Belief Sys-
tems, pages 287?339. W. H. Freeman and Company.
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor: Im-
proving review classification via human-provided
information. In Proceedings of the Association for
Computational Linguistics (ACL-2004).
Jaime G. Carbonell. 1978. POLITICS: Automated
ideological reasoning. Cognitive Science, 2(1):27?
51.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. Wiley-Interscience.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th International World Wide
Web Conference (WWW2003).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Brett Kessler, Geoffrey Nunberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Pro-
ceedings of the 35th Conference on Association for
Computational Linguistics, pages 32?38.
S. Kullback and R. A. Leibler. 1951. On information
and sufficiency. The Annals of Mathematical Statis-
tics, 22(1):79?86, March.
David D. Lewis. 1998. Naive (Bayes) at forty: The in-
dependence assumption in information retrieval. In
Proceedings of the 9th European Conference on Ma-
chine Learning (ECML).
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? identifying perspectives at the document and
sentence levels. In Proceedings of Tenth Conference
on Natural Language Learning (CoNLL).
S. Morinaga, K. Yamanishi, K. Tateishi, and
T. Fukushima. 2002. Mining product reputations on
the web. In Proceedings of the 2002 ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-2004).
T. Nasukawa and J. Yi. 2003. Sentiment analysis:
Capturing favorability using natural language pro-
cessing. In Proceedings of the 2nd International
Conference on Knowledge Capture (K-CAP 2003).
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL-2004).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002).
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003).
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the 7th Conference
on Natural Language Learning (CoNLL-2003).
B. D. Ripley. 1987. Stochastic Simulation. Wiley.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: an inquiry into hu-
man knowledge structures. Lawrene Erlbaum Asso-
ciates.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1?47, March.
Peter Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems (TOIS), 21(4):315?346.
Peter Verdonk. 2002. Stylistics. Oxford University
Press.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3).
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003).
1064
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 109?116, New York City, June 2006. c?2006 Association for Computational Linguistics
Which Side are You on? Identifying Perspectives at the Document and
Sentence Levels
Wei-Hao Lin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
whlin@cs.cmu.edu
Theresa Wilson, Janyce Wiebe
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
{twilson,wiebe}@cs.pitt.edu
Alexander Hauptmann
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
alex@cs.cmu.edu
Abstract
In this paper we investigate a new problem
of identifying the perspective from which
a document is written. By perspective we
mean a point of view, for example, from
the perspective of Democrats or Repub-
licans. Can computers learn to identify
the perspective of a document? Not every
sentence is written strongly from a per-
spective. Can computers learn to identify
which sentences strongly convey a partic-
ular perspective? We develop statistical
models to capture how perspectives are
expressed at the document and sentence
levels, and evaluate the proposed mod-
els on articles about the Israeli-Palestinian
conflict. The results show that the pro-
posed models successfully learn how per-
spectives are reflected in word usage and
can identify the perspective of a document
with high accuracy.
1 Introduction
In this paper we investigate a new problem of au-
tomatically identifying the perspective from which
a document is written. By perspective we mean
a ?subjective evaluation of relative significance, a
point-of-view.?1 For example, documents about the
Palestinian-Israeli conflict may appear to be about
the same topic but reveal different perspectives:
1The American Heritage Dictionary of the English Lan-
guage, 4th ed.
(1) The inadvertent killing by Israeli forces of
Palestinian civilians ? usually in the course of
shooting at Palestinian terrorists ? is
considered no different at the moral and ethical
level than the deliberate targeting of Israeli
civilians by Palestinian suicide bombers.
(2) In the first weeks of the Intifada, for example,
Palestinian public protests and civilian
demonstrations were answered brutally by
Israel, which killed tens of unarmed protesters.
Example 1 is written from an Israeli perspective;
Example 2 is written from a Palestinian perspec-
tive. Anyone knowledgeable about the issues of
the Israeli-Palestinian conflict can easily identify the
perspectives from which the above examples were
written. However, can computers learn to identify
the perspective of a document given a training cor-
pus?
When an issue is discussed from different per-
spectives, not every sentence strongly reflects the
perspective of the author. For example, the follow-
ing sentences were written by a Palestinian and an
Israeli.
(3) The Rhodes agreements of 1949 set them as
the ceasefire lines between Israel and the Arab
states.
(4) The green line was drawn up at the Rhodes
Armistice talks in 1948-49.
Examples 3 and 4 both factually introduce the back-
ground of the issue of the ?green line? without ex-
pressing explicit perspectives. Can we develop a
109
system to automatically discriminate between sen-
tences that strongly indicate a perspective and sen-
tences that only reflect shared background informa-
tion?
A system that can automatically identify the per-
spective from which a document is written will be
a valuable tool for people analyzing huge collec-
tions of documents from different perspectives. Po-
litical analysts regularly monitor the positions that
countries take on international and domestic issues.
Media analysts frequently survey broadcast news,
newspapers, and weblogs for differing viewpoints.
Without the assistance of computers, analysts have
no choice but to read each document in order to iden-
tify those from a perspective of interest, which is ex-
tremely time-consuming. What these analysts need
is to find strong statements from different perspec-
tives and to ignore statements that reflect little or no
perspective.
In this paper we approach the problem of learning
individual perspectives in a statistical framework.
We develop statistical models to learn how perspec-
tives are reflected in word usage, and we treat the
problem of identifying perspectives as a classifica-
tion task. Although our corpus contains document-
level perspective annotations, it lacks sentence-level
annotations, creating a challenge for learning the
perspective of sentences. We propose a novel sta-
tistical model to overcome this problem. The ex-
perimental results show that the proposed statisti-
cal models can successfully identify the perspective
from which a document is written with high accu-
racy.
2 Related Work
Identifying the perspective from which a document
is written is a subtask in the growing area of au-
tomatic opinion recognition and extraction. Sub-
jective language is used to express opinions, emo-
tions, and sentiments. So far, research in automatic
opinion recognition has primarily addressed learn-
ing subjective language (Wiebe et al, 2004; Riloff
et al, 2003), identifying opinionated documents (Yu
and Hatzivassiloglou, 2003) and sentences (Yu and
Hatzivassiloglou, 2003; Riloff et al, 2003), and dis-
criminating between positive and negative language
(Pang et al, 2002; Morinaga et al, 2002; Yu and
Hatzivassiloglou, 2003; Turney and Littman, 2003;
Dave et al, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005; Wilson et al, 2005). While by its
very nature we expect much of the language that is
used when presenting a perspective or point-of-view
to be subjective, labeling a document or a sentence
as subjective is not enough to identify the perspec-
tive from which it is written. Moreover, the ideol-
ogy and beliefs authors possess are often expressed
in ways other than positive or negative language to-
ward specific targets.
Research on the automatic classification of movie
or product reviews as positive or negative (e.g.,
(Pang et al, 2002; Morinaga et al, 2002; Turney
and Littman, 2003; Nasukawa and Yi, 2003; Mullen
and Collier, 2004; Beineke et al, 2004; Hu and Liu,
2004)) is perhaps the most similar to our work. As
with review classification, we treat perspective iden-
tification as a document-level classification task, dis-
criminating, in a sense, between different types of
opinions. However, there is a key difference. A pos-
itive or negative opinion toward a particular movie
or product is fundamentally different from an overall
perspective. One?s opinion will change from movie
to movie, whereas one?s perspective can be seen as
more static, often underpinned by one?s ideology or
beliefs about the world.
There has been research in discourse analysis that
examines how different perspectives are expressed
in political discourse (van Dijk, 1988; Pan et al,
1999; Geis, 1987). Although their research may
have some similar goals, they do not take a compu-
tational approach to analyzing large collections of
documents. To the best of our knowledge, our ap-
proach to automatically identifying perspectives in
discourse is unique.
3 Corpus
Our corpus consists of articles published on the
bitterlemonswebsite2. The website is set up to
?contribute to mutual understanding [between Pales-
tinians and Israelis] through the open exchange of
ideas.?3 Every week an issue about the Israeli-
Palestinian conflict is selected for discussion (e.g.,
2http://www.bitterlemons.org
3http://www.bitterlemons.org/about/
about.html
110
?Disengagement: unilateral or coordinated??), and
a Palestinian editor and an Israeli editor each con-
tribute one article addressing the issue. In addition,
the Israeli and Palestinian editors invite one Israeli
and one Palestinian to express their views on the
issue (sometimes in the form of an interview), re-
sulting in a total of four articles in a weekly edi-
tion. We choose the bitterlemons website for
two reasons. First, each article is already labeled
as either Palestinian or Israeli by the editors, allow-
ing us to exploit existing annotations. Second, the
bitterlemons corpus enables us to test the gen-
eralizability of the proposed models in a very real-
istic setting: training on articles written by a small
number of writers (two editors) and testing on arti-
cles from a much larger group of writers (more than
200 different guests).
We collected a total of 594 articles published on
the website from late 2001 to early 2005. The dis-
tribution of documents and sentences are listed in
Table 1. We removed metadata from all articles, in-
Palestinian Israeli
Written by editors 148 149
Written by guests 149 148
Total number of documents 297 297
Average document length 740.4 816.1
Number of sentences 8963 9640
Table 1: The basic statistics of the corpus
cluding edition numbers, publication dates, topics,
titles, author names and biographic information. We
used OpenNLP Tools4 to automatically extract sen-
tence boundaries, and reduced word variants using
the Porter stemming algorithm.
We evaluated the subjectivity of each sentence us-
ing the automatic subjective sentence classifier from
(Riloff and Wiebe, 2003), and find that 65.6% of
Palestinian sentences and 66.2% of Israeli sentences
are classified as subjective. The high but almost
equivalent percentages of subjective sentences in the
two perspectives support our observation in Sec-
tion 2 that a perspective is largely expressed using
subjective language, but that the amount of subjec-
tivity in a document is not necessarily indicative of
4http://sourceforge.net/projects/
opennlp/
its perspective.
4 Statistical Modeling of Perspectives
We develop algorithms for learning perspectives us-
ing a statistical framework. Denote a training corpus
as a set of documents Wn and their perspectives la-
bels Dn, n = 1, . . . ,N , where N is the total number
of documents in the corpus. Given a new document
W? with a unknown document perspective, the per-
spective D? is calculated based on the following con-
ditional probability.
P (D?|W? , {Dn,Wn}Nn=1) (5)
We are also interested in how strongly each sen-
tence in a document conveys perspective informa-
tion. Denote the intensity of the m-th sentence of
the n-th document as a binary random variable Sm,n.
To evaluate Sm,n, how strongly a sentence reflects
a particular perspective, we calculate the following
conditional probability.
P (Sm,n|{Dn,Wn}Nn=1) (6)
4.1 Na??ve Bayes Model
We model the process of generating documents from
a particular perspective as follows:
pi ? Beta(?pi, ?pi)
? ? Dirichlet(??)
Dn ? Binomial(1, pi)
Wn ? Multinomial(Ln, ?d)
First, the parameters pi and ? are sampled once from
prior distributions for the whole corpus. Beta and
Dirichlet are chosen because they are conjugate pri-
ors for binomial and multinomial distributions, re-
spectively. We set the hyperparameters ?pi, ?pi, and
?? to one, resulting in non-informative priors. A
document perspective Dn is then sampled from a bi-
nomial distribution with the parameter pi. The value
of Dn is either d0 (Israeli) or d1 (Palestinian). Words
in the document are then sampled from a multino-
mial distribution, where Ln is the length of the doc-
ument. A graphical representation of the model is
shown in Figure 1.
111
pi ?
Dn Wn
N
Figure 1: Na??ve Bayes Model
The model described above is commonly known
as a na??ve Bayes (NB) model. NB models have
been widely used for various classification tasks,
including text categorization (Lewis, 1998). The
NB model is also a building block for the model
described later that incorporates sentence-level per-
spective information.
To predict the perspective of an unseen document
using na??ve Bayes , we calculate the posterior distri-
bution of D? in (5) by integrating out the parameters,
? ?
P (D?, pi, ?|{(Dn,Wn)}Nn=1, W? )dpid? (7)
However, the above integral is difficult to compute.
As an alternative, we use Markov Chain Monte
Carlo (MCMC) methods to obtain samples from the
posterior distribution. Details about MCMC meth-
ods can be found in Appendix A.
4.2 Latent Sentence Perspective Model
We introduce a new binary random variable, S, to
model how strongly a perspective is reflected at the
sentence level. The value of S is either s1 or s0,
where s1 indicates a sentence is written strongly
from a perspective while s0 indicates it is not. The
whole generative process is modeled as follows:
pi ? Beta(?pi, ?pi)
? ? Beta(?? , ?? )
? ? Dirichlet(??)
Dn ? Binomial(1, pi)
Sm,n ? Binomial(1, ?)
Wm,n ? Multinomial(Lm,n, ?)
The parameters pi and ? have the same semantics as
in the na??ve Bayes model. S is naturally modeled as
a binomial variable, where ? is the parameter of S.
S represents how likely it is that a sentence strongly
conveys a perspective. We call this model the La-
tent Sentence Perspective Model (LSPM) because S
is not directly observed. The graphical model repre-
sentation of LSPM is shown in Figure 2.
pi ? ?
Dn
Sm,n Wm,n
N
Mn
Figure 2: Latent Sentence Perspective Model
To use LSPM to identify the perspective of a new
document D? with unknown sentence perspectives S?,
we calculate posterior probabilities by summing out
possible combinations of sentence perspective in the
document and parameters.
? ? ?
?
Sm,n
?
S?
P (D?, Sm,n, S?, pi, ?, ?| (8)
{(Dn,Wn)}Nn=1, W? )dpid?d?
As before, we resort to MCMC methods to sample
from the posterior distributions, given in Equations
(5) and (6).
As is often encountered in mixture models, there
is an identifiability issue in LSPM. Because the val-
ues of S can be permuted without changing the like-
lihood function, the meanings of s0 and s1 are am-
biguous. In Figure 3a, four ? values are used to rep-
resent the four possible combinations of document
perspective d and sentence perspective intensity s. If
we do not impose any constraints, s1 and s0 are ex-
changeable, and we can no longer strictly interpret
s1 as indicating a strong sentence-level perspective
and s0 as indicating that a sentence carries little or
no perspective information. The other problem of
this parameterization is that any improvement from
LSPM over the na??ve Bayes model is not necessarily
112
d0
?d0,s0
s0
?d0,s1
s1
d1
?d1,s0
s0
?d0,s0
s1
(a) s0 and s1 are not identifiable
s1
?d0,s1
d0
?d1,s1
d1 ?s0
s0
(b) sharing ?d1,s0 and
?d0,s0
Figure 3: Two different parameterization of ?
due to the explicit modeling of sentence-level per-
spective. S may capture aspects of the document
collection that we never intended to model. For ex-
ample, s0 may capture the editors? writing styles and
s1 the guests? writing styles in the bitterlemons
corpus.
We solve the identifiability problem by forcing
?d1,s0 and ?d0,s0 to be identical and reducing the
number of ? parameters to three. As shown in Fig-
ure 3b, there are separate ? parameters conditioned
on the document perspective (left branch of the tree,
d0 is Israeli and d1 is Palestinian), but there is single
? parameter when S = s0 shared by both document-
level perspectives (right branch of the tree). We as-
sume that the sentences with little or no perspective
information, i.e., S = s0, are generated indepen-
dently of the perspective of a document. In other
words, sentences that are presenting common back-
ground information or introducing an issue and that
do not strongly convey any perspective should look
similar whether they are in Palestinian or Israeli doc-
uments. By forcing this constraint, we become more
confident that s0 represents sentences of little per-
spectives and s1 represents sentences of strong per-
spectives from d1 and d0 documents.
5 Experiments
5.1 Identifying Perspective at the Document
Level
We evaluate three different models for the task
of identifying perspective at the document level:
two na??ve Bayes models (NB) with different infer-
ence methods and Support Vector Machines (SVM)
(Cristianini and Shawe-Taylor, 2000). NB-B uses
full Bayesian inference and NB-M uses Maximum
a posteriori (MAP). We compare NB with SVM not
only because SVM has been very effective for clas-
sifying topical documents (Joachims, 1998), but also
to contrast generative models like NB with discrimi-
native models like SVM. For training SVM, we rep-
resent each document as a V -dimensional feature
vector, where V is the vocabulary size and each co-
ordinate is the normalized term frequency within the
document. We use a linear kernel for SVM and
search for the best parameters using grid methods.
To evaluate the statistical models, we train them
on the documents in the bitterlemons corpus
and calculate how accurately each model predicts
document perspective in ten-fold cross-validation
experiments. Table 2 reports the average classi-
fication accuracy across the the 10 folds for each
model. The accuracy of a baseline classifier, which
randomly assigns the perspective of a document as
Palestinian or Israeli, is 0.5, because there are equiv-
alent numbers of documents from the two perspec-
tives.
Model Data Set Accuracy Reduction
Baseline 0.5
SVM Editors 0.9724
NB-M Editors 0.9895 61%
NB-B Editors 0.9909 67%
SVM Guests 0.8621
NB-M Guests 0.8789 12%
NB-B Guests 0.8859 17%
Table 2: Results for Identifying Perspectives at the
Document Level
The last column of Table 2 is error reduction
relative to SVM. The results show that the na??ve
Bayes models and SVM perform surprisingly well
on both the Editors and Guests subsets of the
bitterlemons corpus. The na??ve Bayes mod-
els perform slightly better than SVM, possibly be-
cause generative models (i.e., na??ve Bayes models)
achieve optimal performance with a smaller num-
ber of training examples than discriminative models
(i.e., SVM) (Ng and Jordan, 2002), and the size of
the bitterlemons corpus is indeed small. NB-B,
which performs full Bayesian inference, improves
113
on NB-M, which only performs point estimation.
The results suggest that the choice of words made
by the authors, either consciously or subconsciously,
reflects much of their political perspectives. Statis-
tical models can capture word usage well and can
identify the perspective of documents with high ac-
curacy.
Given the performance gap between Editors and
Guests, one may argue that there exist distinct edit-
ing artifacts or writing styles of the editors and
guests, and that the statistical models are capturing
these things rather than ?perspectives.? To test if the
statistical models truly are learning perspectives, we
conduct experiments in which the training and test-
ing data are mismatched, i.e., from different subsets
of the corpus. If what the SVM and na??ve Bayes
models learn are writing styles or editing artifacts,
the classification performance under the mismatched
conditions will be considerably degraded.
Model Training Testing Accuracy
Baseline 0.5
SVM Guests Editors 0.8822
NB-M Guests Editors 0.9327 43%
NB-B Guests Editors 0.9346 44%
SVM Editors Guests 0.8148
NB-M Editors Guests 0.8485 18%
NB-B Editors Guests 0.8585 24%
Table 3: Identifying Document-Level Perspectives
with Different Training and Testing Sets
The results on the mismatched training and test-
ing experiments are shown in Table 3. Both SVM
and the two variants of na??ve Bayes perform well
on the different combinations of training and testing
data. As in Table 2, the na??ve Bayes models per-
form better than SVM with larger error reductions,
and NB-B slightly outperforms NB-M. The high ac-
curacy on the mismatched experiments suggests that
statistical models are not learning writing styles or
editing artifacts. This reaffirms that document per-
spective is reflected in the words that are chosen by
the writers.
We list the most frequent words (excluding stop-
words) learned by the the NB-M model in Ta-
ble 4. The frequent words overlap greatly be-
tween the Palestinian and Israeli perspectives, in-
cluding ?state,? ?peace,? ?process,? ?secure? (?se-
curity?), and ?govern? (?government?). This is in
contrast to what we expect from topical text classi-
fication (e.g., ?Sports? vs. ?Politics?), in which fre-
quent words seldom overlap. Authors from differ-
ent perspectives often choose words from a simi-
lar vocabulary but emphasize them differently. For
example, in documents that are written from the
Palestinian perspective, the word ?palestinian? is
mentioned more frequently than the word ?israel.?
It is, however, the reverse for documents that are
written from the Israeli perspective. Perspectives
are also expressed in how frequently certain people
(?sharon? v.s. ?arafat?), countries (?international?
v.s. ?america?), and actions (?occupation? v.s. ?set-
tle?) are mentioned. While one might solicit these
contrasting word pairs from domain experts, our re-
sults show that statistical models such as SVM and
na??ve Bayes can automatically acquire them.
5.2 Identifying Perspectives at the Sentence
Level
In addition to identifying the perspective of a docu-
ment, we are interested in knowing which sentences
of the document strongly conveys perspective in-
formation. Sentence-level perspective annotations
do not exist in the bitterlemons corpus, which
makes estimating parameters for the proposed La-
tent Sentence Perspective Model (LSPM) difficult.
The posterior probability that a sentence strongly
covey a perspective (Example (6)) is of the most in-
terest, but we can not directly evaluate this model
without gold standard annotations. As an alterna-
tive, we evaluate how accurately LSPM predicts the
perspective of a document, again using 10-fold cross
validation. Although LSPM predicts the perspec-
tive of both documents and sentences, we will doubt
the quality of the sentence-level predictions if the
document-level predictions are incorrect.
The experimental results are shown in Table 5.
We include the results for the na??ve Bayes models
from Table 3 for easy comparison. The accuracy of
LSPM is comparable or even slightly better than that
of the na??ve Bayes models. This is very encouraging
and suggests that the proposed LSPM closely cap-
tures how perspectives are reflected at both the doc-
ument and sentence levels. Examples 1 and 2 from
the introduction were predicted by LSPM as likely to
114
Palestinian palestinian, israel, state, politics, peace, international, people, settle, occupation, sharon,
right, govern, two, secure, end, conflict, process, side, negotiate
Israeli israel, palestinian, state, settle, sharon, peace, arafat, arab, politics, two, process, secure,
conflict, lead, america, agree, right, gaza, govern
Table 4: The top twenty most frequent stems learned by the NB-M model, sorted by P (w|d)
Model Training Testing Accuracy
Baseline 0.5
NB-M Guests Editors 0.9327
NB-B Guests Editors 0.9346
LSPM Guests Editors 0.9493
NB-M Editors Guests 0.8485
NB-B Editors Guests 0.8585
LSPM Editors Guests 0.8699
Table 5: Results for Perspective Identification at the
Document and Sentence Levels
contain strong perspectives, i.e., large Pr(S? = s1).
Examples 3 and 4 from the introduction were pre-
dicted by LSPM as likely to contain little or no per-
spective information, i.e., high Pr(S? = s0).
The comparable performance between the na??ve
Bayes models and LSPM is in fact surprising. We
can train a na??ve Bayes model directly on the sen-
tences and attempt to classify a sentence as reflect-
ing either a Palestinian or Israeli perspective. A sen-
tence is correctly classified if the predicted perspec-
tive for the sentence is the same as the perspective
of the document from which it was extracted. Us-
ing this model, we obtain a classification accuracy of
only 0.7529, which is much lower than the accuracy
previously achieved at the document level. Identify-
ing perspectives at the sentence level is thus more
difficult than identifying perspectives at the docu-
ment level. The high accuracy at the document level
shows that LSPM is very effective in pooling evi-
dence from sentences that individually contain little
perspective information.
6 Conclusions
In this paper we study a new problem of learning to
identify the perspective from which a text is written
at the document and sentence levels. We show that
much of a document?s perspective is expressed in
word usage, and statistical learning algorithms such
as SVM and na??ve Bayes models can successfully
uncover the word patterns that reflect author per-
spective with high accuracy. In addition, we develop
a novel statistical model to estimate how strongly
a sentence conveys perspective, in the absence of
sentence-level annotations. By introducing latent
variables and sharing parameters, the Latent Sen-
tence Perspective Model is shown to capture well
how perspectives are reflected at the document and
sentence levels. The small but positive improvement
due to sentence-level modeling in LSPM is encour-
aging. In the future, we plan to investigate how con-
sistently LSPM sentence-level predictions are with
human annotations.
Acknowledgment
This material is based on work supported by
the Advanced Research and Development Activity
(ARDA) under contract number NBCHC040037.
A Gibbs Samplers
Based the model specification described in Sec-
tion 4.2 we derive the Gibbs samplers (Chen et al,
2000) for the Latent Sentence Perspective Model as
follows,
pi(t+1) ? Beta(?pi +
N
?
n=1
dn + d?(t+1),
?pi + N ?
N
?
n=1
dn + 1 ? d?(t+1))
? (t+1) ? Beta(?? +
N
?
n=1
Mn
?
m=1
sm,n +
M?
?
m=1
s?m,
?? +
N
?
n=1
Mn ?
N
?
n=1
Mn
?
m=1
sm,n + M? ?
M?
?
m=1
s?m)
115
?(t+1) ? Dirichlet(?? +
N
?
n=1
Mn
?
m=1
wm,n)
Pr(S(t+1)n,m = s1) ? P (Wm,n|Sm,n = 1, ?(t))
Pr(S(t+1)m,n = 1|?,Dn)
Pr(D?(t+1) = d1) ?
M?
?
m=1
dbinom(? (t+1)d )
M?
?
m=1
dmultinom(?d,m?(t))dbinom(pi(t))
where dbinom and dmultinom are the density func-
tions of binomial and multinomial distributions, re-
spectively. The superscript t indicates that a sample
is from the t-th iteration. We run three chains and
collect 5000 samples. The first half of burn-in sam-
ples are discarded.
References
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor:
Improving review classification via human-provided
information. In Proceedings of ACL-2004.
Ming-Hui Chen, Qi-Man Shao, and Joseph G. Ibrahim.
2000. Monte Carlo Methods in Bayesian Computa-
tion. Springer-Verlag.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW-2003.
Michael L. Geis. 1987. The Language of Politics.
Springer.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD-2004.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of ECML-1998.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-1998.
S. Morinaga, K. Yamanishi, K. Tateishi, and
T. Fukushima. 2002. Mining product reputations on
the web. In Proceedings of KDD-2002.
Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In Proceedings of EMNLP-2004.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Cap-
turing favorability using natural language processing.
In Proceedings of K-CAP 2003.
Andrew Y. Ng and Michael Jordan. 2002. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. In NIPS-2002, vol-
ume 15.
Zhongdang Pan, Chin-Chuan Lee, Joseph Man Chen, and
Clement Y.K. So. 1999. One event, three stories: Me-
dia narratives of the handover of hong kong in cultural
china. Gazette, 61(2):99?112.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP-2005, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of CoNLL-2003.
Peter Turney and Michael L. Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM TOIS, 21(4):315?346.
T.A. van Dijk. 1988. News as Discourse. Lawrence
Erlbaum, Hillsdale, NJ.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003.
116
