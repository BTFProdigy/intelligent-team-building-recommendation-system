Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1161?1165,
Prague, June 2007. c?2007 Association for Computational Linguistics
Pro3Gres Parser in the CoNLL Domain Adaptation Shared Task
Gerold Schneider and Kaarel Kaljurand and Fabio Rinaldi and Tobias Kuhn
Institute of Computational Linguistics, University of Zurich
Binzmu?hlestrasse 14
CH - 8050 Zurich, Switzerland
{gschneid,kalju,rinaldi,tkuhn}@ifi.uzh.ch
Abstract
We present Pro3Gres, a deep-syntactic, fast
dependency parser that combines a hand-
written competence grammar with proba-
bilistic performance disambiguation and that
has been used in the biomedical domain. We
discuss its performance in the domain adap-
tation open submission. We achieve aver-
age results, which is partly due to difficulties
in mapping to the dependency representation
used for the shared task.
1 Introduction
The Pro3Gres parser is a dependency parser that
combines a hand-written grammar with probabilis-
tic disambiguation. It is described in detail in
(Schneider, 2007). It uses tagger and chunker
pre-processors ? parsing proper happens only be-
tween heads of chunks ? and a post-processor graph
converter to capture long-distance dependencies.
Pro3Gres is embedded in a flexible XML pipeline.
It has been applied to many tasks, such as parsing
biomedical literature (Rinaldi et al, 2006; Rinaldi
et al, 2007) and the whole British National Cor-
pus, and has been evaluated in several ways. We
have achieved average results in the CoNLL do-
main adaptation track open submission (Marcus et
al., 1993; Johansson and Nugues, 2007; Kulick et
al., 2004; MacWhinney, 2000; Brown, 1973). The
performance of the parser is seriously affected by
mapping problems to the particular dependency rep-
resentation used in the shared task.
The paper is structured as follows. We give a brief
overview of the parser and its design policy in sec-
tion 2, we describe the domain adaptations that we
have used in section 3, comment on the results ob-
tained in section 4 and conclude in section 5.
2 Pro3Gres and its Design Policy
There has been growing interest in exploring the
space between Treebank-trained probabilistic gram-
mars (e.g. (Collins, 1999; Nivre, 2006)) and formal
grammar-based parsers integrating statistics (e.g.
(Miyao et al, 2005; Riezler et al, 2002)). We
have developed a parsing system that explores this
space, in the vein of systems like (Kaplan et al,
2004), using a linguistic competence grammar and
a probabilistic performance disambiguation allow-
ing us to explore interactions between lexicon and
grammar (Sinclair, 1996). The parser has been ex-
plicitly designed to be deep-syntactic like a formal
grammar-based parser, by using a dependency rep-
resentation that is close to LFG f-structure, but at
the same time mostly context-free and integrating
shallow approaches and aggressive pruning in or-
der to keep search-spaces small, without permitting
compromise on performance or linguistic adequacy.
(Abney, 1995) establishes the chunks and dependen-
cies model as a well-motivated linguistic theory. The
non-local linguistic constraints that a hand-written
grammar allows us to formulate, e.g. expressing
X-bar principles or barring very marked construc-
tions, further reduce parsing time by at least an order
of magnitude. Since the grammar is on Penn tags
(except for few closed classed words, e.g. allow-
ing including to function as preposition) the effort
for writing it manually is manageable. It has been
developed from scratch in about a person month,
1161
Figure 1: Pro3Gres parser flowchart
using traditional grammar engineering development
cycles. It contains about 1000 rules, the number is
largely so high due to tag combinatorics: for ex-
ample, the various subject attachment rules combin-
ing a subject ( NN, NNS, NNP, NNPS) and a verb
( VBZ, VBP, VBG, VBN, VBD) are all very simi-
lar.
The parser is fast enough for large-scale appli-
cation to unrestricted texts, and it delivers depen-
dency relations which are a suitable base for a
range of applications. We have used it to parse the
entire 100 million words British National Corpus
(http://www.natcorp.ox.ac.uk) and similar amounts
of biomedical texts. Its parsing speed is about
500,000 words per hour. The flowchart of the parser
can be seen in figure 1.
Pro3Gres (PRObabilistic PROlog-implemented
RObust Grammatical Role Extraction System) uses
a dependency representation that is close to LFG
f-structure, in order to give it an established lin-
guistic background. It uses post-processing graph
structure conversions and mild context-sensitivity to
capture long-distance dependencies. We have ar-
gued in (Schneider, 2005) that LFG f-structures can
be parsed for in a completely context-free fashion,
except for embedded WH-questions, where a de-
vice such as functional uncertainty (Kaplan and Za-
enen, 1989) or the equivalent Tree-Adjoining Gram-
mar Adjoining operation (Joshi and Vijay-Shanker,
1989) is used. In Dependency Grammar, this device
is also known as lifting (Kahane et al, 1998; Nivre
and Nilsson, 2005).
We use a hand-written competence grammar,
combined with performance-driven disambiguation
obtained from the Penn Treebank (Marcus et
al., 1993). The Maximum-Likelihood Estimation
(MLE) probability of generating a dependency re-
lation R given lexical heads (a and b) at distance (in
chunks) ? is calculated as follows.
p(R, ?|a, b) ?= p(R|a, b) ? p(?|R) =
#(R, a, b)
?n
i=1#(Ri, a, b)
?
#(R, ?)
#R
The counts are backed off (Collins, 1999; Merlo
and Esteve Ferrer, 2006). The backoff levels include
semantic classes from WordNet (Fellbaum, 1998):
we back off to the lexicographer file ID of the most
frequent word sense. An example output of the
parser is shown in figure 2.
3 Domain Adaptation
Based on our experience with parsing texts form the
biomedical domain, we have used the following two
adaptations to the domain of chemistry.
(Hindle and Rooth, 1993) exploit the fact that in
sentence-initial NP PP sequences the PP unambigu-
ously attaches to the noun. We have observed that in
sentence-initial NP PP PP sequences, also the sec-
ond PP frequently attaches to the noun, the noun
itself often being a relational noun. We have thus
used such sequences to learn relational nouns from
the unlabelled domain texts. Relational nouns are
allowed to attach several argument PPs in the gram-
mar, all other nouns are not.
Multi-word terms, adjective-preposition construc-
tions and frequent PP-arguments have strong collo-
cational force. We have thus used the collocation
extraction tool XTRACT (Smadja, 2003) to discover
collocations from large domain corpora. The prob-
ability of generating a dependency relation is aug-
mented for collocations above a certain threshold.
Since the tagging quality of the Chemistry testset
is high, the impact of multi-word term recognition
was lower than the biomedical domain when using a
standard tagger, as we have shown in (Rinaldi et al,
2007).
For the CHILDES domain, we have not used any
adaptation. The hand-written grammar fares quite
well on most types of questions, which are very fre-
quent in this domain. In the spirit of the shared
task, we have not attempted to correct tagging errors,
which were frequent in the CHILDES domain. We
have restricted the use of external resources to the
hand-written, domain-independent grammar, and to
WordNet. Due to serious problems in mapping our
1162
Figure 2: Example of original parser output
LFG f-structure based dependencies to the CoNLL
representation, much less time than expected was
available for the domain adaptation.
4 Our Results
We have achieved average results: Labeled attach-
ment score: 3151 / 5001 * 100 = 63.01, unlabeled at-
tachment score: 3327 / 5001 * 100 = 66.53, label ac-
curacy score: 3832 / 5001 * 100 = 76.62. These re-
sults are about 10 % below what we typically obtain
when using our own dependency representation or
GREVAL (Carroll et al, 2003), a deep-syntactic an-
notation scheme that is close to ours. Detailed eval-
uations are reported in (Schneider, 2007). Our map-
ping was quite poor, especially when conjunctions
are involved. Also punctuation is attached poorly.
5.7 % of all dependencies remained unmapped (un-
known in the figure). We give an overview of the the
relation-dependent results in figures 1 and 2.
Mapping problems include the following exam-
ples. First, headedness is handled very differently:
while we assume auxiliaries, prepositions and co-
ordinations to be dependents, the CoNNL repre-
sentation assumes the opposite, which leads to in-
correct mapping under complex interactions. Sec-
ond, the semantics of parentheticals (PRN) partly
remains unclear. In Quinidine elimination was
capacity limited with apparent Michaelis constant
(appKM) of 2.6 microM (about 1.2 mg/L) the gold
standard annotates the second parenthesis as paren-
thetical, but the first as nominal modification, al-
though both may be said to have appositional char-
acter. Third, we seem to have misinterpreted the
roles of ADV and AMOD, as they are often mutu-
ally exchanged. Fourth, the logical subject (LGS)
is sometimes marked on the by-PP (... are strongly
inhibited by-LGS carbon monoxide) and sometimes
on the participle (... are increased-LGS by pre-
deprel gold correct system recall (%) prec. (%)
ADV 366 212 302 57.92 70.20
AMOD 87 8 87 9.20 9.20
CC 11 0 0 0.00 NaN
COORD 402 233 342 57.96 68.13
DEP 9 0 0 0.00 NaN
EXP 2 0 0 0.00 NaN
GAP 14 0 0 0.00 NaN
IOBJ 3 0 0 0.00 NaN
LGS 37 0 0 0.00 NaN
NMOD 1813 1576 1763 86.93 89.39
OBJ 185 146 208 78.92 70.19
P 587 524 525 89.27 99.81
PMOD 681 533 648 78.27 82.25
PRN 34 13 68 38.24 19.12
ROOT 195 138 190 70.77 72.63
SBJ 279 217 296 77.78 73.31
VC 129 116 136 89.92 85.29
VMOD 167 116 149 69.46 77.85
unknown 0 0 287 NaN 0.00
Table 1: Prec.&recall of DEPREL
treatment) in the gold standard. Relations between
heads of chunks, which are central for predicate-
argument structures which Pro3Gres aims to re-
cover, such as SBJ, NMOD, ROOT, perform better
than those for which Pro3Gres was not originally
designed, particularly ADV, AMOD, PRN, P. Perfor-
mance on COORD was particularly disappointing.
Generally, mapping problems between different rep-
resentations would be smaller if one used a depen-
dency representation that maximally abstracts away
from form to function, for example (Carroll et al,
2003).
We have obtained results slightly above average
on the CHILDES domain, although we did not adapt
the parser to this domain in any way (unlabeled at-
tachment score: 3013 / 4999 * 100 = 60.27 %).
The hand-written grammar, which includes rules for
most types of questions, fares relatively well on this
domain since questions are rare in the Penn Tree-
bank (see (Hermjakob, 2001)). Pro3Gres has been
employed for question parsing at a TREC confer-
ence (Burger and Bayer, 2005).
1163
deprel gold correct system recall (%) prec. (%)
ADV 366 161 302 43.99 53.31
AMOD 87 5 87 5.75 5.75
CC 11 0 0 0.00 NaN
COORD 402 170 342 42.29 49.71
DEP 9 0 0 0.00 NaN
EXP 2 0 0 0.00 NaN
GAP 14 0 0 0.00 NaN
IOBJ 3 0 0 0.00 NaN
LGS 37 0 0 0.00 NaN
NMOD 1813 1392 1763 76.78 78.96
OBJ 185 140 208 75.68 67.31
P 587 221 525 37.65 42.10
PMOD 681 521 648 76.51 80.40
PRN 34 12 68 35.29 17.65
ROOT 195 138 190 70.77 72.63
SBJ 279 190 296 68.10 64.19
VC 129 116 136 89.92 85.29
VMOD 167 85 149 50.90 57.05
unknown 0 0 287 NaN 0.00
Table 2: Prec.&recall of DEPREL+ATTACHMENT
5 Conclusion
We have described the Pro3Gres parser. We have
achieved average results in the shared task with rel-
atively little adaptation. Mapping to different repre-
sentations is an often underestimated task. Our per-
formance on the CHILDES task, where we did not
adapt the parser, indicates that hand-written, care-
fully engineered competence grammars may be rel-
atively domain-independent while performance dis-
ambiguation is more domain-dependent. We will
adapt the parser to further domains and include more
unsupervised learning methods.
References
Steven Abney. 1995. Chunks and dependencies: Bring-
ing processing evidence to bear on syntax. In Jennifer
Cole, Georgia Green, and Jerry Morgan, editors, Com-
putational Linguistics and the Foundations of Linguis-
tic Theory, pages 145?164. CSLI.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
John D. Burger and Sam Bayer. 2005. MITRE?s Qanda
at TREC-14. In E. M. Voorhees and Lori P. Buck-
land, editors, The Fourteenth Text REtrieval Confer-
ence (TREC 2005) Notebook.
John Carroll, Guido Minnen, and Edward Briscoe. 2003.
Parser evaluation: using a grammatical relation anno-
tation scheme. In Anne Abeille?, editor, Treebanks:
Building and Using Parsed Corpora, pages 299?316.
Kluwer, Dordrecht.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
ACL 2001 Workshop on Open-Domain Question An-
swering, Toulouse, France.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19:103?120.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Aravind K. Joshi and K. Vijay-Shanker. 1989. Treat-
ment of long-distance dependencies in LFG and TAG:
Functional uncertainty in LFG is a corollary in TAG.
In Proceedings of ACL ?89.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In Proceedings of
COLINGACL, volume 1, pages 646?652, Montreal.
Ronald Kaplan and Annie Zaenen. 1989. Long-distance
dependencies, constituent structure, and functional un-
certainty. In Mark Baltin and Anthony Kroch, editors,
Alternative Concepts of Phrase Structrue, pages 17 ?
42. Chicago University Press.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of HLT/NAACL
2004, Boston, MA.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Paola Merlo and Eva Esteve Ferrer. 2006. The notion of
argument in PP attachment. Computational Linguis-
tics, 32(2):341 ? 378.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2005. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
1164
the Penn Treebank. In Keh-Yih Su, Jun?ichi Tsujii,
Jong-Hyeok Lee, and Oi Yee Kwong, editors, Natural
Language Processing - IJCNLP 2004, pages 684?693.
Springer.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 99?106, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency parsing. In Proceedings of the European
Chapter of the Association of Computational Linguis-
tics (EACL) 2006, pages 73 ? 80, Trento, Italy. Asso-
ciation for Computational Linguistics.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), Philadephia, PA.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
Michael Hess, and Martin Romacker. 2006. . an en-
vironment for relation mining over richly annotated
corpora: the case of GENIA. BMC Bioinformatics,
7(Suppl 3):S3.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
Michael Hess, Christos Andronis, Ourania Konstanti,
and Andreas Persidis. 2007. Mining of functional
relations between genes and proteins over biomedical
scientific literature using a deep-linguistic approach.
Journal of Artificial Intelligence in Medicine, 39:127
? 136.
Gerold Schneider. 2005. A broad-coverage, representa-
tionally minimal LFG parser: chunks and F-structures
are sufficient. In Mriram Butt and Traci Holloway
King, editors, The 10th international LFG Conference
(LFG 2005), Bergen, Norway. CSLI.
Gerold Schneider. 2007. Hybrid Long-Distance Func-
tional Dependency Parsing. Doctoral Thesis, Institute
of Computational Linguistics, University of Zurich.
accepted for publication.
John Sinclair. 1996. The empty lexicon. International
Journal of Corpus Linguistics, 1, 1996.
Frank Smadja. 2003. Retrieving collocations from text:
Xtract. Computational Linguistics, 19:1, Special issue
on using large corpora:143?177.
1165
Exploiting Paraphrases in a Question Answering System
Fabio Rinaldi, James Dowdall,
Kaarel Kaljurand, Michael Hess
Institute of Computational Linguistics,
University of Zu?rich
Winterthurerstrasse 190
CH-8057 Zu?rich, Switzerland
{rinaldi,dowdall,kalju,hess}
@ifi.unizh.ch
Diego Molla?
Centre for Language Technology,
Macquarie University,
Sydney NSW 2109, Australia
{diego}@ics.mq.edu.au
Abstract
We present a Question Answering system
for technical domains which makes an in-
telligent use of paraphrases to increase the
likelihood of finding the answer to the user?s
question. The system implements a simple
and efficient logic representation of ques-
tions and answers that maps paraphrases
to the same underlying semantic represen-
tation. Further, paraphrases of technical
terminology are dealt with by a separate
process that detects surface variants.
1 Introduction
The problem of paraphrases conceals a number of
different linguistic problems, which in our opinion
need to be treated in separate ways. In fact, para-
phrases can happen at various levels in language. Us-
ing the examples provided in the call for papers for
this workshop, we would like to attempt a simple
classification, without any pretense of being exhaus-
tive:
1. Lexical synonymy.
Example: article, paper, publication
2. Morpho-syntactic variants.
a) Oswald killed Kennedy. / Kennedy was killed
by Oswald.
b) Edison invented the light bulb. / Edison?s
invention of the light bulb.
while (a) is purely syntactical (active vs pas-
sive), (b) involves a nominalisation.
3. PP-attachment.
a plant in Alabama / the Alabama plant
4. Comparatives vs superlatives.
be better than anybody else / be the best
5. Subordinate clauses vs separate sentences linked
by anaphoric pronouns.
The tree healed its wounds by growing new bark.
/ The tree healed its wounds. It grew new bark.
6. Inference.
The stapler costs $10. / The price of the stapler
is $10.
Where is Thimphu located? / Thimphu is capi-
tal of what country?
Of course combinations of the different types are
possible, e.g. Oswald killed Kennedy / Kennedy
was assassinated by Oswald is a combination of (1)
and (2).
Different types of knowledge and different linguis-
tic resources are needed to deal with each of the
above types. While type (1) can be dealt with us-
ing a resource such as WordNet (Fellbaum, 1998),
type (2) needs effective parsing and mapping of syn-
tactic structures into a common deeper structure,
possibly using a repository of nominalisations like
NOMLEX (Meyers et al, 1998). More complex
approaches are needed for the other types, up to
type (6) where generic world knowledge is required,
for instance to know that being a capital of a country
implies being located in that country. 1 Such world
knowledge could be expressed in the form of axioms,
like the following:
(X costs Y) iff (the price of X is Y)
In this paper we focus on the role of paraphrases
in a Question Answering (QA) system targeted at
1Note that the reverse is not true, and therefore this
is not a perfect paraphrase.
technical manuals. Technical documentation is char-
acterised by vast amounts of domain-specific termi-
nology, which needs to be exploited for providing in-
telligent access to the information contained in the
manuals (Rinaldi et al, 2002b). The approach taken
by QA systems is to allow a user to ask a query (for-
mulated in natural language) and have the system
search a background collection of documents in order
to locate an answer. The field of Question Answer-
ing has flourished in recent years2, in part, due to
the QA track of the TREC competitions (Voorhees
and Harman, 2001). These competitions evaluate
systems over a common data set alowing develop-
ers to benchmark performance in relation to other
competitors.
It is a common assumption that technical termi-
nology is subject to strict controls and cannot vary
within a given editing process. However this assump-
tion proves all too often to be incorrect. Unless edi-
tors are making use of a terminology control system
that forces them to use a specific version of a term,
they will naturally tend to use various paraphrases
to refer to the intended domain concept. Besides in
a query a user could use an arbitrary paraphrases of
the target term, which might happen to be one of
those used in the manual itself or might happen to
be a novel one.
We describe some potential solutions to this prob-
lem, taking our Question Answering system as an ex-
ample. We show which benefits our approach based
on paraphrases bring to the system. So far two dif-
ferent domains have been targeted by the system.
An initial application aims at answering questions
about the Unix man pages (Molla? et al, 2000a; Molla?
et al, 2000b). A more complex application targets
the Aircraft Maintenance Manual (AMM) of the Air-
bus A320 (Rinaldi et al, 2002b). Recently we have
started new work, using the Linux HOWTOs as a
new target domain.
In dealing with these domains we have identified
two major obstacles for a QA system, which we can
summarise as follows:
? The Parsing Problem
? The Paraphrase Problem
The Parsing Problem consists in the increased
difficulty of parsing text in a technical domain due to
domain-specific sublanguage. Various types of multi
word expressions characterise these domains, in par-
ticular referring to specific concepts like tools, parts
or procedures. These multi word expressions might
2Although early work in AI already touched upon the
topic, e.g. (Woods, 1977).
include lexical items which are either unknown to
a generic lexicon (e.g. coax cable) or have a spe-
cific meaning unique to this domain. Abbreviations
and acronyms are another common source of incon-
sistencies. In such cases the parser might either
fail to identify the compound as a phrase and con-
sequently fail to parse the sentence including such
items. Alternatively the parser might attempt to
?guess? their lexical category (in the set of open class
categories), leading to an exponential growth of the
number of possible syntactic parses. Not only the in-
ternal structure of the compound can be multi-way
ambiguous, even the boundaries of the compounds
might be difficult to detect and the parsers might
try odd combinations of the tokens belonging to the
compounds with neighbouring tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who can-
not be expected to be completely familiar with the
domain terminology. Even experienced users, who
know very well the domain, might not remember the
exact wording of a compound and use a paraphrase
to refer to the underlying domain concept. Besides
even in the manual itself, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identified as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphrases might be cre-
ated by the users each time they query the system.
In the rest of this paper we describe first our Ques-
tion Answering System (in Section 2) and briefly
show how we solved the first of the two problems
described above. Then, in Section 3 we show in de-
tail how the system is capable of coping with the
Paraphrase Problem. Finally in Section 4 we discuss
some related work.
2 A Question Answering System for
Technical Domains
Over the past few years our research group has devel-
oped an Answer Extraction system (ExtrAns) that
works by transforming documents and queries into a
semantic representation called Minimal Logical Form
(MLF) (Molla? et al, 2000a) and derives the answers
by logical proof from the documents. A full linguis-
tic (syntactic and semantic) analysis, complete with
lexical alternations (synonyms and hyponyms) is per-
formed. While documents are processed in an off-line
stage, the query is processed on-line.
Two real world applications have so far been im-
plemented with the same underlying technology. The
original ExtrAns system (Molla? et al, 2000b) is used
///// a.d electrical coax cable.n4 connects.v062 the.d external antenna.n1 to.o the.d ANT connection.n1 /////
-Wd
ff Dsu ff Ss
-
MVp
-Os
ff Ds
-Js
ff Ds
RW
Figure 1: An Example of LG Output
to extract answers to arbitrary user queries over the
Unix documentation files (?man pages?). A set of
500+ unedited man pages has been used for this ap-
plication. An on-line demo of ExtrAns can be found
at the project web page.3
 Knowledge 
Base
Document
Linguistic
Analysis
Term
processing
Figure 2: Off-line
Processing of Docu-
ments
More recently we tackled
a different domain, the Air-
plane Maintenance Manu-
als (AMM) of the Air-
bus A320 (Rinaldi et al,
2002b), which offered the
additional challenges of an
SGML-based format and a
much larger size (120MB).4
Despite being developed
initially for a specific do-
main, ExtrAns has demon-
strated a high level of do-
main independence.
As we work on relatively
small volumes of data we
can afford to process (in
an off-line stage) all the
documents in our collection
rather than just a few se-
lected paragraphs (see Fig-
ure 2). Clearly in some sit-
uations (e.g. processing in-
coming news) such an ap-
proach might not be fea-
sible and paragraph index-
ing techniques would need
to be used. Our current ap-
proach is particularly tar-
geted to small and medium sized collections.
In an initial phase all multi-word expressions
from the domain are collected and structured in
an external resource, which we will refer to as the
TermBase (Rinaldi et al, 2003; Dowdall et al, 2003).
The document sentences (and user queries) are syn-
tactically processed with the Link Grammar (LG)
parser (Sleator and Temperley, 1993) which uses a
3http://www.ifi.unizh.ch/cl/extrans/
4Still considerably smaller than the size of the docu-
ment collections used for TREC
grammar with a wide coverage of English and has
a robust treatment of ungrammatical sentences and
unknown words. The multi-word terms from the the-
saurus are identified and passed to the parser as sin-
gle tokens. This prevents (futile) analysis of the in-
ternal structure of terms (see Figure 1), simplifying
parsing by 46%. This solves the first of the problems
that we have identified in the introduction (?The
Parsing Problem?).
In later stages of processing, a corpus-based ap-
proach (Brill and Resnik, 1994) is used to deal with
ambiguities that cannot be solved with syntactic in-
formation only, in particular attachments of preposi-
tional phrases, gerunds and infinitive constructions.
ExtrAns adopts an anaphora resolution algorithm
(Molla? et al, 2003) that is based on Lappin and Le-
ass? approach (Lappin and Leass, 1994). The original
algorithm, which was applied to the syntactic struc-
tures generated by McCord?s Slot Grammar (Mc-
Cord et al, 1992), has been ported to the output of
Link Grammar. So far the resolution is restricted to
sentence-internal pronouns but the same algorithm
can be applied to sentence-external pronouns too.
A lexicon of nominalisations based on NOMLEX
(Meyers et al, 1998) is used for the most important
cases. The main problem here is that the semantic
relationship between the base words (mostly, but not
exclusively, verbs) and the derived words (mostly,
but not exclusively, nouns) is not sufficiently sys-
tematic to allow a derivation lexicon to be compiled
automatically. Only in relatively rare cases is the
relationship as simple as with to edit <a text> ?
editor of <a text> / <text> editor, as the effort
that went into building resources such as NOMLEX
also shows.
User queries are processed on-line and converted
into MLFs (possibly expanded by synonyms) and
proved by refutation over the document knowledge
base (see Figure 3). Pointers to the original text at-
tached to the retrieved logical forms allow the system
to identify and highlight those words in the retrieved
sentence that contribute most to that particular an-
swer. When the user clicks on one of the answers
provided, the corresponding document will be dis-
played with the relevant passages highlighted.
 Knowledge 
Base
ANSWERSQuery
Document
Linguistic
Analysis
Paraphrase
Identification
Figure 3: On-line Processing of Queries
The meaning of the documents and of the queries
produced by ExtrAns is expressed by means of Mini-
mal Logical Forms (MLFs). The MLFs are designed
so that they can be found for any sentence (using
robust approaches to treat very complex or ungram-
matical sentences), and they are optimized for NLP
tasks that involve the semantic comparison of sen-
tences, such as Answer Extraction.
The expressivity of the MLFs is minimal in the
sense that the main syntactic dependencies between
the words are used to express verb-argument rela-
tions, and modifier and adjunct relations. However,
complex quantification, tense and aspect, temporal
relations, plurality, and modality are not expressed.
One of the effects of this kind of underspecification
is that several natural language queries, although
slightly different in meaning, produce the same logi-
cal form.
The main feature of the MLFs is the use of reifi-
cation (the expression of abstract concepts as con-
crete objects) to achieve flat expressions (Molla? et
al., 2000b). The MLFs are expressed as conjunc-
tions of predicates with all the variables existentially
bound with wide scope. For example, the MLF of
the sentence ?cp will quickly copy the files? is:
(1) holds(e4), object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type cp and of type command,
there is an entity x6 (a file), there is an entity e4,
which represents a copying event where the first ar-
gument is x1 and the second argument is x6, there
is an entity p3 which states that e4 is done quickly,
and the event e4, that is, the copying, holds. The
entities o1, o2, o3, e4, and p3 are the result of reifi-
cation. The reification of the event, e4, has been used
to express that the event is done quickly. The other
entities are not used in this MLF, but other more
complex sentences may need to refer to the reifica-
tion of properties (adjective-modifying adverbs) or
object predicates (non-intersective adjectives such as
the alleged suspect).
ExtrAns finds the answers to the questions by
forming the MLFs of the questions and then run-
ning Prolog?s default resolution mechanism to find
those MLFs that can prove the question. When no
direct proof for the user query is found, the system
is capable of relaxing the proof criteria in a stepwise
manner. First, hyponyms of the query terms will be
added as disjunctions in the logical form of the ques-
tion, thus making it more general but still logically
correct. If that fails, the system will attempt approx-
imate matching, in which the sentence (or sentences)
with the highest overlap of predicates with the query
is retrieved. The (partially) matching sentences are
scored and the best fits are returned. In the case
that this method finds too many answers because
the overlap is too low, the system will attempt key-
word matching, in which syntactic criteria are aban-
doned and only information about word classes is
used. This last step corresponds approximately to a
traditional passage-retrieval methodology with con-
sideration of the POS tags.
3 Dealing with Paraphrases
The system is capable of dealing with paraphrases
at two different levels. On the phrase level, differ-
ent surface realizations (terms) which refer to the
same domain concept will be mapped into a com-
mon identifier (synset identifier). On the sentence
level, paraphrases which involve a (simple) syntactic
transformation will be dealt with by mapping them
into the same logical form. In this section we will
describe these two approaches and discuss ways to
cope with complex types of parapharases.
3.1 Identifying Terminological Paraphrases
During the construction of the MLFs, thesaurus
terms are replaced by their synset identifiers. This
results in an implicit ?terminological normalization?
for the domain. The benefit to the QA process is
an assurance that a query and answer need not in-
volve exactly the same surface realization of a term.
Utilizing the synsets in the semantic representation
means that when the query includes a term, ExtrAns
returns sentences that logically answer the query, in-
Fastr
Term
Extraction
Hyponymy
Thesaurus ExtrAns
Document
Figure 4: Term Processing
volving any known paraphrase of that term.
For example, the logical form of the query Where
are the stowage compartments installed? is trans-
lated internally into the Horn query (2).
(2) evt(install,A,[B,C]),
object(D,E,[B]),
object(s stowage compartment,G,[C])
This means that a term (belonging to the same
synset as stowage compartment) is involved in an in-
stall event with an anonymous object. If there is
an MLF from the document that can match exam-
ple (2), then it is selected as a candidate answer and
the sentence it originates from is shown to the user.
The process of terminological variation is well
investigated (Ibekwe-SanJuan and Dubois, 2002;
Daille et al, 1996; Ibekwe-Sanjuan, 1998). The
primary focus has been to use linguistically based
variation to expand existing term sets through cor-
pus investigation or to produce domain representa-
tions. A subset of such variations identifies terms
which are strictly synonymous. ExtrAns gathers
these morpho-syntactic variations into synsets. The
sets are augmented with terms exhibiting three
weaker synonymy relations described by Hamon &
Nazarenko (2001). These synsets are organized into
a hyponymy (isa) hierarchy, a small example of which
can be seen in Figure 5. Figure 4 shows a schematic
representation of this process.
The first stage is to normalize any terms that con-
tain punctuation by creating a punctuation free ver-
sion and recording the fact that that the two are
strictly synonymous. Further processing is involved
in terms containing brackets to determine if the
bracketed token is an acronym or simply optional. In
the former case an acronym-free term is created and
the acronym is stored as a synonym of the remain-
ing tokens which contain it as a regular expression.
So evac is synonymous with evacuation and ohsc is
synonymous with overhead stowage compartment. In
cases such as emergency (hard landings) the brack-
eted tokens can not be interpreted as acronyms and
so are not removed.
The synonymy relations are identified using the
terminology tool Fastr (Jacquemin, 2001). Every to-
ken of each term is associated with its part-of-speech,
its morphological root, and its synonyms. Phrasal
rules represent the manner in which tokens combine
to form multi-token terms, and feature-value pairs
carry the token specific information. Metarules li-
cense the relation between two terms by constrain-
ing their phrase structures in conjunction with the
morphological and semantic information on the indi-
vidual tokens.
The metarules can identify simple paraphrases
that result from morpho-syntactic variation (cargo
compartment door ?? doors of the cargo compart-
ment), terms with synonymous heads (electrical ca-
ble ?? electrical line), terms with synonymous mod-
ifiers (fastener strip ?? attachment strip) and both
(functional test ?? operational check). For a de-
scription of the frequency and range of types of vari-
ation present in the AMM see Rinaldi et al (2002a).
3.2 Identifying Syntactic Paraphrases
An important effect of using a simplified semantic-
based representation such as the Minimal Logical
Forms is that various types of syntactic variations
are automatically captured by a common representa-
tion. This ensures that many potential paraphrases
in a user query can map to the same answer into the
manual.
For example the question shown in Figure 6 can
be answered thanks to the combination of two fac-
tors. On the lexical level ExtrAns knows that APU
is an abbreviation of Auxiliary Power Unit, while on
the syntactic level the active and passive voices (sup-
plies vs supplied with) map into the same underlying
representation (the same MLF).
Another type of paraphrase which can be detected
at this level is the kind that was classified as type (3)
in the introduction. For example the question: Is
the sensor connected to the APU ECB?, can locate
the answer This sensor is connected to the Elec-
tronic Control Box (ECB) of the APU. This has been
achieved by introducing meaning postulates that op-
erate at the level of the MLFs (such as ?any predicate
that affects an object will also affect the of -modifiers
of that object?).
3.3 Weaker Types of Paraphrases
When the thesaurus definition of terminological syn-
onymy fails to locate an answer from the docu-
ment collection, ExtrAns explores weaker types of
paraphrases, where the equivalence between the two
terms might not be complete.
TERM
doors of the cargo compartment
cargo compartment door
cargo comparment doors
cargo-compartment door
emergency ( hard landings )
emergency hard landings
emergency hard landing
emergency evacuation (evac)
emergency evacuation
evacuation
evac
electrical cable
electrical line
fastner strip
attachment strip
functional test
operational check
door functional test
stowage compartment
overhead stowage compartment
OHSC
1
2
3
5
6
7
10
9
8
11
Figure 5: A Sample of the TermBase
Figure 6: Active vs Passive Voice
First, ExtrAns makes use of the hyponymy rela-
tions, which can be considered as sort of unidirec-
tional paraphrases. Instead of looking for synset
members, the query is reformulated to included hy-
ponyms and hyperonyms of the terms:
(3) (object(s stowage compartment,A,[B]);
object(s overhead stowage compartment,A,[B])),
evt(install,C,[D,B]),
object(E,F,[D|G])
Now the alternative objects are in a logical OR rela-
tion. This query finds the answer in Figure 7 (where
stowage compartment is a hyperonym of overhead
stowage compartment).
We have implemented a very simple ad-hoc algo-
rithm to determine lexical hyponymy between terms.
Term A is a hyponym of term B if (i) A has more to-
kens than B, (ii) all the tokens of B are present in A,
and (iii) both terms have the same head. There are
three provisions. First, ignore terms with dashes and
brackets as cargo compartment is not a hyponym of
cargo - compartment and this relation (synonymy) is
already known from the normalisation process. Sec-
ond, compare lemmatised versions of the terms to
capture that stowage compartment is a hyperonym
of overhead stowage compartments. Finally, the head
of a term is the rightmost non-symbol token (i.e. a
word) which can be determined from the part-of-
speech tags. This hyponymy relation is compara-
ble to the insertion variations defined by Daille et
al. (1996).
The expressivity of the MLF can further be ex-
panded through the use of meaning postulates of the
type: ?If x is installed in y, then x is in y?. This
ensures that the query Where are the equipment and
furnishings? extracts the answer The equipment and
furnishings are installed in the cockpit.
4 Related Work
The importance of detecting paraphrasing in Ques-
tion Answering has been shown dramatically in
TREC9 by the Falcon system (Harabagiu et al,
2001), which made use of an ad-hoc module capable
of caching answers and detecting question similar-
ity. As in that particular evaluation the organisers
deliberately used a set of paraphrases of the same
questions, such approach certainly helped in boost-
ing the performance of the system. In an environ-
ment where the same question (in different formula-
tions) is likely to be repeated a number of times, a
module capable of detecting paraphrases can signif-
icantly improve the performance of a Question An-
Figure 7: Overhead stowage compartment is a Hyponym of Stowage compartment
swering system.
Another example of application of paraphrases for
Question Answering is given in (Murata and Isahara,
2001), which further argues for the importance of
paraphrases for other applications such Summarisa-
tion, error correction and speech generation.
Our approach for the acquisition of terminological
paraphrases might have some points in common with
the approach described in (Terada and Tokunaga,
2001). The motivation that they bring forward for
the necessity of identifying abbreviations is related to
the problem that we have called ?the Parsing Prob-
lem?.
A very different approach to paraphrases is taken
in (Takahashi et al, 2001) where they formulate the
problem as a special case of Machine Translation,
where the source and target language are the same
but special rules, based on different parameters, li-
cense different types of surface realizations.
Hamon & Nazarenko (2001) explore the termino-
logical needs of consulting systems. This type of IR
guides the user in query/keyword expansion or pro-
poses various levels of access into the document base
on the original query. A method of generating three
types of synonymy relations is investigated using gen-
eral language and domain specific dictionaries.
5 Conclusion
Automatic recognition of paraphrases is an effec-
tive technique to ease the information access bur-
den in a technical domain. We have presented some
techniques that we have adopted in a Question An-
swering system for dealing with paraphrases. These
techniques range from the detection of lexical para-
phrases and terminology variants, to the use of a
simplified logical form that provides the same repre-
sentation for morpho-syntactic paraphrases, and the
use of meaning postulates for paraphrases that re-
quire inferences.
References
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
ambiguation. In Proc. COLING ?94, volume 2,
pages 998?1004, Kyoto, Japan.
Beatrice Daille, Benot Habert, Christian Jacquemin,
and Jean Royaute?. 1996. Empirical observation of
term variations and principles for their description.
Terminology, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
SanJuan, and Eric SanJuan. 2003. Complex
structuring of term variants for Question Answer-
ing. In Proc. ACL-2003 Workshop on Multiword
Expressions, Sapporo, Japan.
Christiane Fellbaum 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Thierry Hamon and Adeline Nazarenko. 2001. De-
tection of synonymy links between terms: Experi-
ment and results. In Didier Bourigault, Christian
Jacquemin, and Marie-Claude L?Homme, editors,
Recent Advances in Computational Terminology,
pages 185?208. John Benjamins Publishing Com-
pany.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana G??rju, Vasile Rus, and Paul Morarescu.
2001. Falcon: Boosting knowledge for answer
engines. In Voorhees and Harman (Voorhees and
Harman, 2001).
Fidelia Ibekwe-SanJuan and Cyrille Dubois. 2002.
Can Syntactic Variations Highlight Semantic
Links Between Domain Topics? In Proceedings
of the 6th International Conference on Terminol-
ogy and Knowledge Engineering (TKE02), pages
57?64, Nancy, August.
Fidelia Ibekwe-Sanjuan. 1998. Terminological Vari-
ation, a Means of Identifying Research Topics from
Texts. In Proceedings of COLING-ACL, pages
571?577, Quebec,Canada, August.
Christian Jacquemin. 2001. Spotting and Discover-
ing Terms through Natural Language Processing.
MIT Press.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535?561.
Michael McCord, Arendse Bernth, Shalom Lap-
pin, and Wlodek Zadrozny. 1992. Natural lan-
guage processing within a slot grammar frame-
work. International Journal on Artificial Intelli-
gence Tools, 1(2):229?277.
Adam Meyers, Catherine Macleod, Roman Yangar-
ber, Ralph Grishman, Leslie Barrett, and Ruth
Reeves. 1998. Using NOMLEX to produce
nominalization patterns for information extrac-
tion. In Proceedings: the Computational Treat-
ment of Nominals, Montreal, Canada, (Coling-
ACL98 workshop), August.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000a. Answer Extraction using
a Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000b. Extrans, an answer ex-
traction system. T.A.L. special issue on Informa-
tion Retrieval oriented Natural Language Process-
ing.
Diego Molla?, Rolf Schwitter, Fabio Rinaldi, James
Dowdall, and Michael Hess. 2003. Anaphora res-
olution in ExtrAns. In Proceedings of the Interna-
tional Symposium on Reference Resolution and Its
Applications to Question Answering and Summa-
rization, 23?25 June, Venice, Italy.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal model for paraphrasing - using transformation
based on a defined criteria. In Proceedings of the
NLPRS2001 Workshop on Automatic Paraphras-
ing: Theories and Applications.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002a. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Interna-
tional Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?
30 August.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002b. Towards An-
swer Extraction: an application to Technical Do-
mains. In ECAI2002, European Conference on Ar-
tificial Intelligence, Lyon, 21?26 July.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, and Magnus Karlsson. 2003. The Role
of Technical Terminology in Question Answering.
In Proceedings of TIA-2003, Terminologie et In-
telligence Artificielle, Strasbourg, April.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, and
Kentaro Inui. 2001. Kura: A revision-based
lexico-structural paraphrasing engine. In Proceed-
ings of the NLPRS2001 Workshop on Automatic
Paraphrasing: Theories and Applications.
Akira Terada and Takenobu Tokunaga. 2001. Au-
tomatic disabbreviation by using context informa-
tion. In Proceedings of the NLPRS2001 Workshop
on Automatic Paraphrasing: Theories and Appli-
cations.
Ellen M. Voorhees and Donna Harman, editors.
2001. Proceedings of the Ninth Text REtrieval
Conference (TREC-9), Gaithersburg, Maryland,
November 13-16, 2000.
W.A. Woods. 1977. Lunar rocks in natural English:
Explorations in Natural Language Question An-
swering. In A. Zampolli, editor, Linguistic Struc-
tures Processing, volume 5 of Fundamental Studies
in Computer Science, pages 521?569. North Hol-
land.
Parmenides: an opportunity for ISO TC37 SC4?
Fabio Rinaldi
1
, James Dowdall
1
, Michael Hess
1
, Kaarel Kaljurand
1
, Andreas Persidis
2
,
Babis Theodoulidis
3
, Bill Black
3
, John McNaught
3
, Haralampos Karanikas
3
, Argyris Vasilakopoulos
3
,
Kelly Zervanou
3
, Luc Bernard
3
, Gian Piero Zarri
4
, Hilbert Bruins Slot
5
, Chris van der Touw
5
,
Margaret Daniel-King
6
, Nancy Underwood
6
, Agnes Lisowska
6
, Lonneke van der Plas
6
,
Veronique Sauron
6
, Myra Spiliopoulou
7
, Marko Brunzel
7
, Jeremy Ellman
8
,
Giorgos Orphanos
9
, Thomas Mavroudakis
10
, Spiros Taraviras
10
.
Abstract
Despite the many initiatives in recent years
aimed at creating Language Engineering
standards, it is often the case that dierent
projects use dierent approaches and often
dene their own standards. Even within the
same project it often happens that dierent
tools will require dierent ways to represent
their linguistic data.
In a recently started EU project focusing
on the integration of Information Extrac-
tion and Data Mining techniques, we aim
at avoiding the problem of incompatibility
among dierent tools by dening a Com-
mon Annotation Scheme internal to the
project. However, when the project was
started (Sep 2002) we were unaware of the
standardization eort of ISO TC37/SC4,
and so we commenced once again trying to
dene our own schema. Fortunately, as this
work is still at an early stage (the project
will last till 2005) it is still possible to redi-
rect it in a way that it will be compati-
ble with the standardization work of ISO.
In this paper we describe the status of the
work in the project and explore possible
synergies with the work in ISO TC37 SC4.
1 1
Institute of Computational Linguistics, Uni-
versity of Zurich, Switzerland;
2
Biovista, Athens,
Greece;
3
Centre for Research in Information Manage-
ment, UMIST, Manchester, UK;
4
CNRS, Paris, France;
5
Unilever Research and Development, Vlaardingen,
The Netherlands;
6
TIM/ISSCO, University of Geneva,
Switzerland;
7
Uni Magdeburg, Germany;
8
Wordmap
Ltd., Bath, UK;
9
Neurosoft, Athens, Greece;
10
The
Greek Ministry of National Defense, Athens, Greece
1 Introduction
It is by now widely accepted that some W3C stan-
dards (such as XML and RDF) provide a con-
venient and practical framework for the creation
of eld-specic markup languages (e.g. MathML,
VoiceXML). However XML provides only a common
\alphabet" for interchange among tools, the steps
that need to be taken before there is any real shar-
ing are still many (just as many human languages
share the same alphabets, that does not mean that
they can be mutually intelligible). The necessary
step to achieve mutual understanding in Language
Resources is to create a common data model.
The existence of a standard brings many other
advantages, like the ability to automatically com-
pare the results of dierent tools which provide the
same functionality, from the very basic (e.g. tok-
enization) to the most complex (e.g. discourse rep-
resentation). Some of the NIST-supported competi-
tive evaluations (e.g. MUC) greatly beneted by the
existence of scoring tools, which could automatically
compare the results of each participant against a gold
standard. The creation of such tools (and their ef-
fectiveness) was possible only because the organizing
institute had pre-dened and \imposed" upon the
participants the annotation scheme. However, that
sort of \brute force" approach might not always pro-
duce the best results. It is important to involve the
community in the denition of such standards at an
early stage, so that all the possible concerns can be
met and a wider acceptance can be achieved.
Another clear benet of agreed standards is that
they will increase interoperability among dierent
tools. It is not enough to have publicly available
APIs to ensure that dierent tools can be integrated.
In fact, if their representation languages (their \data
vocabulary") are too divergent, no integration will
be possible (or at least it will require a considerable
mapping eort). For all the above reasons we enthu-
siastically support any concertation work, aimed at
establishing common foundations for the eld.
In a recently started EU project (\Parmenides")
focusing on the integration of Information Extrac-
tion and Data Mining techniques (for Text Mining)
we aim at avoiding the problem of incompatibility
among dierent tools by dening a Common Annota-
tion Scheme internal to the project. However, when
the project was started (Sep 2002) we were unaware
of the standardization eort of ISO TC37 SC4, and
so we commenced once again trying to dene our own
schema. Fortunately, as this work is still at an early
stage (the project will last till 2005) it is still possible
to redirect it in a way that it will be compatible with
the standardization work of ISO.
In this paper we will describe the approach fol-
lowed so far in the denition of the Parmenides Com-
mon Annotation Scheme, even if its relation with ISO
is still only supercial. In the forthcoming months
our intention is to explore possible synergies between
our work and the current initiatives in ISO TC37
SC4, with the aim to get at a Parmenides annota-
tion scheme which is conformant to the approach cur-
rently discussed in the standardization committee.
2 The Parmenides Lingua Franca
In this section we will describe the XML-based anno-
tation scheme proposed for the Parmenides project.
In general terms the project is concerned with or-
ganisational knowledge management, specically, by
developing an ontology driven systematic approach
to integrating the entire process of information gath-
ering, processing and analysis.
The annotation scheme is intended to work as the
projects' lingua franca: all the modules will be re-
quired to be able to accept as input and generate
as output documents conformant to the (agreed) an-
notation scheme. The specication will be used to
create data-level compatibility among all the tools
involved in the project.
Each tool might choose to use or ignore part of
the information dened by the markup: some infor-
mation might not yet be available at a given stage
of processing or might not be required by the next
module. Facilities will need to be provided for lter-
ing annotations according to a simple conguration
le. This is in fact one of the advantages of using
XML: many readily available o-the-shelf tools can
be used for parsing and ltering the XML annota-
tions, according to the needs of each module.
The annotation scheme will be formally dened by
a DTD and an equivalent XML schema denition.
Ideally the schema should remain exible enough to
allow later additional entities when and if they are
needed. However the present document has only an
illustrative purpose, in particular the set of anno-
tation elements introduced needs to be further ex-
panded and the attributes of all elements need to be
veried.
There are a number of simplications which have
been taken in this document with the purpose of
keeping the annotation scheme as simple as possible,
however they might be put into question and more
complex approaches might be required. For instance
we assume that we will be able to identify a unique
set of tags, suitable for all the applications. If this
proves to be incorrect, a possible way to deal with
the problem is the use of XML namespaces. Our
assumptions allow us (for the moment) to keep all
XML elements in the same namespace (and there-
fore ignore the issue altogether).
2.1 Corpus Development
The annotation scheme will be used to create a de-
velopment corpus - a representative sample of the
domain, provided by the users as typical of the doc-
uments they manually process daily. In this phase,
the documents are annotated by domain experts for
the information of interest. This provides the bench-
mark against which algorithms can be developed and
tested to automate extraction as far as possible.
Of primary importance to the annotation process
is the consolidation of the \information of interest",
the text determined as the target of the Information
Extraction modules. Given the projects' goals, this
target will be both diverse and complex necessitating
clarity and consensus.
2.2 Sources Used for this Document
Parmenides aims at using consolidated Information
Extraction techniques, such as Named Entity Ex-
traction, and therefore this work builds upon well-
known approaches, such as the Named Entity anno-
tation scheme from MUC7 (Chinchor, 1997). Cru-
cially, attention will be paid to temporal annota-
tions, with the aim of using extracted temporal in-
formation for detection of trends (using Data Min-
ing techniques). Therefore we have investigated all
the recently developed approaches to such a problem,
and have decided for the adoption of the TERQAS
tagset (Ingria and Pustejovsky, 2002; Pustejovsky et
al., 2002).
Other sources that have been considered include
the GENIA tagset (GENIA, 2003), TEI (TEI Con-
sortium, 2003) and the GDA
1
tagset. The list of
entities introduced so far is by no means complete
1
http://www.i-content.org/GDA/tagset.html
but serves as the starting point, upon which to build
a picture of the domains from information types they
contain. The domain of interests (e.g. Biotechnol-
ogy) are also expected to be terminology-rich and
therefore require proper treatment of terminology.
To supplement the examples presented, a com-
plete document has been annotated according to the
outlined specication.
2
There are currently three
methods of viewing the document which oer dif-
fering ways to visualize the annotations. These
are all based on transformation of the same XML
source document, using XSLT and CSS (and some
Javascript for visualization of attributes). For exam-
ple, the basic view can be seen in gure (1).
3 Levels of Annotation
The set of Parmenides annotations is organized into
three levels:
 Structural Annotations
Used to dene the physical structure of the doc-
ument, it's organization into head and body,
into sections, paragraphs and sentences.
3
 Lexical Annotations
Associated to a short span of text (smaller than
a sentence), and identify lexical units that have
some relevance for the Parmenides project.
 Semantic Annotations
Not associated with any specic piece of text
and as such could be free-oating within the
document, however for the sake of clarity, they
will be grouped into a special unit at the end
of the document. They refer to lexical anno-
tations via co-referential Ids. They (partially)
correspond to what in MUC7 was termed `Tem-
plate Elements' and `Template Relations'.
Structural annotations apply to large text spans,
lexical annotations to smaller text spans (sub-
sentence). Semantic annotations are not directly
linked to a specic text span, however, they are
linked to text units by co-referential identiers.
All annotations are required to have an unique ID
and thus will be individually addressable, this allows
semantic annotations to point to the lexical annota-
tions to which they correspond. Semantic Annota-
tions themselves are given a unique ID, and therefore
can be elements of more complex annotations (\Sce-
nario Template" in MUC parlance).
2
available at http://www.ifi.unizh.ch/Parmenides
3
Apparently the term 'structure' is used with a dif-
ferent meaning in the ISO documentation, referring
to morpho-syntactical structure rather than document
structure.
Structural Annotations The structure of the
documents will be marked using an intuitively appro-
priate scheme which may require further adaptations
to specic documents. For the moment, the root
node is <ParDoc> (Parmenides Document) which
can contain <docinfo>, <body>, <ParAnn>. The
<docinfo> might include a title, abstract or sum-
mary of the documents contents, author informa-
tion and creation/release time. The main body
of the documents (<body>) will be split into sec-
tions (<sec>) which can themselves contain sec-
tions as well as paragraphs (<para>). Within the
paragraphs all sentences will be identied by the
<sentence> tag. The Lexical Annotations will
(normally) be contained within sentences. The -
nal section of all documents will be <ParAnn> (Par-
menides Annotations) where all of the semantic an-
notations that subsume no text are placed. Figure
(2) demonstrates the annotation visualization tool
displaying the documents structure (using nested
boxes).
Lexical Annotations Lexical Annotations are
used to mark any text unit (smaller than a sentence),
which can be of interest in Parmenides. They include
(but are not limited to):
1. Named Entities in the classical MUC sense
2. New domain-specic Named Entities
3. Terms
4. Temporal Expressions
5. Events
6. Descriptive phrases (chunks)
The set of Lexical Annotations described in this
document will need to be further expanded to cover
all the requirements of the project, e.g. names of
products (Acme Arms International's KryoZap (TM)
tear gas riot control gun), including e.g. names of
drugs (Glycocortex's Siderocephalos).
When visualizating the set of Lexical Tags in a
given annotated document, clicking on specic tags
displays the attribute values (see gure (3)).
Semantic Annotations The relations that exist
between lexical entities are expressed through the
semantic annotations. So lexically identied peo-
ple can be linked to their organisation and job ti-
tle, if this information is contained in the document
(see gure (4)). In terms of temporal annotations, it
is the explicit time references and events which are
identied lexically, the temporal relations are then
captured through the range of semantic tags.
Figure 1: Basic Annotation Viewing
3.1 Example
While the structural annotations and lexical annota-
tions should be easy to grasp as they correspond to
accepted notions of document structure and of con-
ventional span-based annotations, an example might
help to illustrate the role of semantic annotations.
(1) The recent ATP award is
<ENAMEX id="e8" type="ORGANIZATION">
Dyax
</ENAMEX>
's second, and follows a
<NUMEX id="n5" type="MONEY">
$4.3 million
</NUMEX>
<ENAMEX id="e9" type="ORGANIZATION">
NIST
</ENAMEX>
grant to
<ENAMEX id="e10" type="ORGANIZATION">
Dyax
</ENAMEX>
and
<ENAMEX id="e11" type="ORGANIZATION">
CropTech Development Corporation
</ENAMEX>
in
<TIMEX3 tid="t4" type="DATE" value="1997">
1997
</TMEX3>
There are two occurrences of Dyax in this short
text: the two Lexical Entities e8 and e10, but clearly
they correspond to the same Semantic Entity. To
capture this equivalence, we could use the syntactic
notion of co-reference (i.e. Identify the two as co-
referent). Another possible approach is to make a
step towards the conceptual level, and create a se-
mantic entity, of which both e8 and e10 are lexical
expressions (which could be dierent, e.g. \Dyax",
\Dyax Corp.", \The Dyax Corporation"). The sec-
ond approach can be implemented using an empty
XML element, created whenever a new entity is men-
tioned in text. For instance, in (2) we can use the tag
Figure 2: Visualization of Structural Annotations
<PEntity> (which stands for Parmenides Entity).
(2) <PEntity peid="obj1" type="ORGANIZATION"
mnem="Dyax" refid="e1 e3 e6 e8 e10 e12"/>
The new element is assigned (as usual) a unique
identication number and a type. The attribute mnem
contains just one of the possible ways to refer to the
semantic entity (a mnemonic name, possibly chosen
randomly). However, it also takes as the value of
the refid attribute as many coreferent ids as are
warranted by the document. In this way all lexical
manifestations of a single entity are identied. All
the lexical entities which refer to this semantic entity,
are possible ways to `name' it (see also g. 4).
Notice that the value of the `type' attribute has
been represented here as a string for readability pur-
poses, in the actual specication it will be a pointer
to a concept in a domain-specic Ontology.
Other semantic entities from (1) are:
(3) <PEntity peid="obj2" type="ORGANIZATION"
mnem="NIST" refid="e2 e4 e7 e9"/>
<PEntity peid="obj3" type="ORGANIZATION"
mnem="CropTech" refid="e11"/>
The newly introduced semantic entities can then
be used to tie together people, titles and organiza-
tions on the semantic level. Consider for example
the text fragment (4), which contains only Lexical
Annotations.
(4) ... said
<ENAMEX id="e17" type="PERSON">
Charles R. Wescott
</ENAMEX>
, Ph.D.,
<ROLE type='x' id="x5">
Senior Scientist
</ROLE>
at
<ENAMEX id="e60" type="ORGANIZATION">
Dyax Corp
</ENAMEX>
The Lexical Entity e17 requires the introduction
of a new semantic entity, which is given the arbitrary
identier `obj5':
(5) <PEntity peid="obj5" type="PERSON"
mnem="Charles R. Wescott" refid="e17"/>
Figure 3: Visualization of Lexical Annotations and their attributes
In turn, this entity is linked to the entity obj1
from (1) by a relation of type `workFor' (PRelation
stands for Parmenides Relation):
(6) <PRelation prid="rel2" source="obj5"
target="obj1" type="worksFor" role="Senior
Scientist" evidence="x5"/>
4 Discussion
As the status of the Parmenides annotation scheme
is still preliminary, we aim in this section to pro-
vide some justication for the choices done so far
and some comparison with existing alternatives.
4.1 Named Entities
One of the purposes of Named Entities is to instanti-
ate frames or templates representing facts involving
these elements. A minor reason to preserve the clas-
sic named entities is so that we can test an IE system
against the MUC evaluation suites and know how
it is doing compared to the competition and where
there may be lacunae. As such, the MUC-7 speci-
cation (Chinchor, 1997) is adopted with the minor
extension of a non-optional identication attribute
on each tag.
4.2 Terminology
A term is a means of referring to a concept of a spe-
cial subject language; it can be a single wordform,
a multiword form or a phrase, this does not matter.
The only thing that matters is that it has special
reference: the term is restricted to refer to its con-
cept of the special domain. The act of (analytically)
dening xes the special reference of a term to a con-
cept. Thus, it makes no sense to talk of a term not
having a denition. A concept is described by den-
ing it (using other certain specialised linguistic forms
(terms) and ordinary words), by relating it to other
concepts, and by assigning a linguistic form (term)
to it.
If we are interested in fact extraction from densely
terminological texts with few named entities apart
from perhaps names of authors, names of laborato-
ries, and probably many instances of amounts and
measures, then we would need to rely much more on
prior identication of terms in the texts, especially
where these are made up of several word forms.
A term can have many variants: even standard-
ised terms have variants e.g. singular, plural forms
of a noun. Thus we should perhaps more correctly
refer to a termform, at least when dealing with text.
Among variants one can also include acronyms and
reduced forms. You therefore nd a set of variants,
typically, all referring to the same concept in a special
domain: they are all terms (or termforms). Again
this problem pinpoints the need for a separation of
the lexical annotations (the surface variants within
the document) and semantic annotations (pointing
abstractly to the underlying concept).
4.3 Approaches to Temporal Annotations
TIDES (Ferro et al, 2001) is a temporal annota-
tion scheme that was developed at the MITRE Cor-
poration and it can be considered as an extension
of the MUC7 Named Entity Recognition (Tempo-
ral Entity Recognition - TIMEX Recognition) (Chin-
chor, 1997). It aims at annotating and normalizing
explicit temporal references. STAG (Setzer, 2001)
is an annotation scheme developed at the University
of She?eld. It has a wider focus than TIDES in
the sense that it combines explicit time annotation,
event annotation and the ability to annotate tempo-
ral relations between events and times.
TimeML (Ingria and Pustejovsky, 2002) stands for
\Time Markup Language" and represents the inte-
gration and consolidation of both TIDES and STAG.
It was created at the TERQAS Workshop
4
and is
designed to combine the advantages of the previous
temporal annotations schemes. It contains a set of
tags which are used to annotate events, time expres-
sions and various types of event-event, event-time
and time-time relations. TimeML is specically tar-
geted at the temporal attributes of events (time of
occurrence, duration etc.).
As the most complete and recent, TimeML should
be adopted for the temporal annotations. Broadly,
its organization follows the Parmenides distinction
between lexical/semantic annotations. Explicit tem-
poral expressions and events receive an appropriate
(text subsuming) lexical tag. The temporal rela-
tions existing between these entities are then cap-
tured through a range of semantic (non-text subsum-
ing) tags.
For example, each event introduces a correspond-
ing semantic tag. There is a distinction be-
tween event \tokens" and event \instances" moti-
vated by predicates that represent more than one
event. Accordingly, each event creates a semantic
<MAKEINSTANCE> tag that subsumes no text. Ei-
ther, one tag for each realised event or a single tag
with the number of events expressed as the value of
the cardinality attribute. The tag is introduced and
the event or to which it refers is determined by the
attributes eventID.
5 Conclusion
We believe that ISO TC37/SC4 provides a very in-
teresting framework within which specic research
concerns can be addressed without the risk of rein-
venting the wheel or creating another totally new
4
http://www.cs.brandeis.edu/~jamesp/arda/time
and incompatible annotation format. The set of an-
notations that we have been targeting so far in Par-
menides is probably a small subset of what is tar-
geted by ISO TC37/SC4. Although we had only lim-
ited access to the documentation available, we think
our approach is compatible with the work being done
in ISO.
It is, we believe, extremely important for a project
like ours, to be involved directly in the ongoing dis-
cussion. Moreover we are at precisely the right stage
for a more direct `exposure' to the ISO TC37/SC4
discussion, as we have completed the exploratory
work but no irrevocable modeling commitment has
so far been taken. Therefore we would hope to be-
come more involved in order to make our proposal
t exactly into that framework. The end result of
this process might be that Parmenides could become
a sort of \Guinea Pig" for at least a subset of ISO
TC37 SC4.
Acknowledgments
The Parmenides project is funded by the European
Commission (contract No. IST-2001-39023) and
by the Swiss Federal O?ce for Education and Sci-
ence (BBW/OFES). All the authors listed have con-
tributed to the (ongoing) work described in this pa-
per. Any remaining errors are the sole responsibility
of the rst author.
References
Nancy Chinchor. 1997. MUC-7 Named Entity Task Denition, Version
3.5. http://www.itl.nist.gov/iaui/894.02/
related projects/muc/proceedings/ne task.html.
Lisa Ferro, Inderjeet Mani, Beth Sundheim, and George Wilson. 2001.
Tides temporal annotation guidelines, version 1.0.2. Technical re-
port, The MITRE Corporation.
GENIA. 2003. Genia project home page. http://www-tsujii.is.s.u-
tokyo.ac.jp/~genia.
Bob Ingria and James Pustejovsky. 2002. TimeML
Specication 1.0 (internal version 3.0.9), July.
http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
James Pustejovsky, Roser Sauri, Andrea Setzer, Rob Giazauskas, and
Bob Ingria. 2002. TimeML Annotation Guideline 1.00 (internal
version 0.4.0), July. http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
Andrea Setzer. 2001. Temporal Information in Newswire Articles: An
Annotation Scheme and Corpus Study. Ph.D. thesis, University of
She?eld.
TEI Consortium. 2003. The text encoding initiative. http://www.tei-
c.org/.
Figure 4: Visualization of Semantic Annotations
Proceedings of the Workshop on BioNLP, pages 80?88,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TX Task:
Automatic Detection of Focus Organisms in Biomedical Publications
Thomas Kappeler, Kaarel Kaljurand, Fabio Rinaldi?
Institute of Computational Linguistics, University of Zurich
kappeler@bluewin.ch, kalju@cl.uzh.ch, rinaldi@cl.uzh.ch
Abstract
In biomedical information extraction (IE), a
central problem is the disambiguation of am-
biguous names for domain specific entities,
such as proteins, genes, etc. One important
dimension of ambiguity is the organism to
which the entities belong: in order to disam-
biguate an ambiguous entity name (e.g. a pro-
tein), it is often necessary to identify the spe-
cific organism to which it refers.
In this paper we present an approach to the
detection and disambiguation of the focus or-
ganism(s), i.e. the organism(s) which are the
subject of the research described in scientific
papers, which can then be used for the disam-
biguation of other entities.
The results are evaluated against a gold stan-
dard derived from IntAct annotations. The
evaluation suggests that the results may al-
ready be useful within a curation environment
and are certainly a baseline for more complex
approaches.
1 Introduction
The task of identifying the organisms which are in-
volved in research described in biomedical articles
is extremely important for the field of biomedical in-
formation extraction (IE), both in itself and in con-
nection with other tasks. In itself, because the con-
cept of biological taxonomy is basic for every re-
searcher: organisms and their taxonomic classifica-
tion can be used very effectively in various contexts,
for example to restrict searches, a classical infor-
mation retrieval (IR) task. At the same time, any
biomedical text mining system would be incomplete
without the possibility to use organisms as concepts,
e.g. in finding (statistical) associations, which can
?Corresponding author
then be used to form hypotheses about causal rela-
tions.
The necessity of identifying organisms is even
more evident as part of other important entity recog-
nition tasks in biomedical information extraction
(IE), e.g. identification and disambiguation of pro-
teins mentioned in the literature. For example,
within the PPI task (identification of protein-protein
interactions) of Biocreative II (Krallinger et al,
2008), the identification of the focus organism was
seen by many participants as an essential subtask in
order to properly disambiguate protein names. Pro-
tein interactions are fundamental for most biological
processes, therefore they are at the focus of a huge
and fast growing number of biomedical papers. As
these cannot all be read or even inspected by the re-
searchers, databases such as IntAct (Kerrien et al,
2006) or MINT (Zanzoni et al, 2002) try to create a
reliable catalogue of experimentally detected inter-
actions by extracting them ?manually? from the lit-
erature through the usage of human experts. This is
known as ?curation?, a costly and time-consuming
process, which could be speeded up much by effi-
cient, robust and precise extraction tools.
One of the most important obstacles for efficient
automatic identification of proteins is the extreme
ambiguity of the commonly used protein names in
the literature. The fragmentation of the biomedical
scientific community into lots of extremely special-
ized sub-communities seems to be the main reason
for this ambiguity. In most cases, the ambiguity is
between homologous proteins of different species.
Any human reader belonging to the sub-community
concerned can, in general, disambiguate an ambigu-
ous protein name like ?goat? (which can refer to
proteins found in four different organisms: human,
rat, mouse and zebrafish), as the species is obvious
to them from the context. However, this ambiguity
80
remains problematic for IE systems (and even for
curators in some cases) and needs to be solved be-
fore more complex tasks, such as protein interaction
detection, can be effectively tackled (Rinaldi et al,
2008).
Our goal is to be able to identify automatically
the focus organisms, i.e. the organisms that are
mentioned in the paper as the hosts of the exper-
iments described, or as the sources of the entities
involved. This information can then be used for tag-
ging papers for more efficient organism-based infor-
mation retrieval, or, more commonly, for the dis-
ambiguation of other entities mentioned in the same
paper. Since organism recognition is normally per-
formed with reference to a taxonomical organization
(of Linnean origin) of all known organisms (in our
case, the NCBI taxonomy) this task is often referred
to as ?TX task?.
In the rest of this paper we describe in section 2
the resources used and the approach followed in or-
der to extract and rank candidate organisms. In sec-
tion 3 we present our results and propose a more fine
grained interpretation of the task, which we again
evaluate. Finally in section 4 we compare our ap-
proach to previous work and discuss its limitations.
2 Methods
Our approach can be described briefly as (1) find all
explicit mentions of organisms either by their scien-
tific or ?common? names; (2) count these mentions
and combine the resulting numbers with a simple
use of statistics to arrive at a ranked list or a sim-
ple set of organisms which can be used, among other
things, to disambiguate ambiguous protein names in
the article under investigation.
2.1 Resources Used
The first step for this approach was to choose a
widely accepted taxonomy which not just includes
unambiguous identifiers for all known organisms,
but also provides a sufficiently large list of names
for them. The taxonomy selected for this was the
NCBI Taxonomy1.
1Available as archive taxdmp.zip from
ftp://ftp.ncbi.nih.gov/pub/taxonomy/. We worked with a
version downloaded on July 10th 2008. The file nodes.dmp
contains the taxonomy as a set of 443,299 nodes for the taxa
and immediate-dominance-relations between them. The file
As most of these organism are unlikely to ever oc-
cur in biomedical literature, we decided to restrict
our interest to the organisms for which a UniProt
organism mnemonic identifier exists. UniProt
(UniProt Consortium, 2007) is a database containing
detailed information about known proteins, obtained
by a process of curation of the biomedical literature.
For every protein, a ?mnemonic? identifier is de-
fined (e.g. HBA HUMAN for ?Human Hemoglobin
A?) which is composed by a shorthand for the pro-
tein name and a simple unique identifier for the or-
ganism. Within the UniProt entry for the protein,
the organism is also referred to by its NCBI iden-
tifier, allowing the construction of a mapping from
the mnemonic identifiers for the organisms used by
UniProt to their equivalent NCBI identifiers.
The set of organism that have a UniProt
mnemonic identifier (11,444 organisms) probably
covers the near totality of organisms that have been
subject to research in molecular biology. In the
NCBI taxonomy 31,733 names are defined for that
subset of organisms. Although several classes of
names are defined by NCBI, for the purpose of
this work we distinguish only between ?scientific
names? and the other classes (pooled together as
?common names?).2
As an additional source of information, we used
the IntAct database of protein interactions3 for two
different purposes:
? to derive statistical measures used later by the
program, most importantly the frequency of
each focus organism in papers curated by Int-
Act (using the IntAct annotations as the sources
of the ?focus?).
? to derive a gold standard against which our pro-
grams could be tested
IntAct provides an annotated set of protein in-
teractions. Each interaction is enriched with de-
tailed information about the two proteins involved
names.dmp connects one or several names (619,325) of differ-
ent nameclasses (such as ?scientific? or ?common?) to each
node. The nodes (taxa) are referred to by numeric identifiers.
2While there are no ambiguous ?scientific names? in this
taxonomy, there are several ambiguous ?common names?, but
only very few of these occurred in our sample, e.g. ?mink?,
?barley?, ?green monkey?, and they are very rare.
3Version of May 2008, downloaded from
http://www.ebi.ac.uk/intact/site/contents/downloads.jsf
81
(from which the reference organisms can be recov-
ered), and with the identifier of the paper from which
the interaction was originally derived in the curation
process. This allows to build a gold standard by as-
sociating each paper to its focus organisms.
The sample used in our experiments is a set of 621
PubMed-indexed full text articles, dating from 1995
to 2007, for which IntAct annotations are available.4
2.2 First Experiments and Normalization
As an initial experiment, we performed a simple
lexical lookup of the names of the 11,444 organ-
isms under consideration. In previous applications
of IE techniques for biomedical literature (Kappeler
et al, 2008; Rinaldi et al, 2008) we found that
simple techniques for the generation of variants of
the known names significantly benefited the recall
of the application. For example, multiword protein
names can be subject to a number of minor variants,
such as the introduction of hyphens or the separation
of compound words, which make automatic recog-
nition more challenging. In the case of organism
names, although our initial expectations were sim-
ilar, we found the benefit (in terms of additional re-
call) of such variants to be extremely limited, possi-
bly because names of species are used more consis-
tently than the names of proteins or genes.
Therefore it was possible to implement a simpler
approach to recognition of organism names, based
on lexical lookup against a database containing all
names of interest, coupled with a simple normaliza-
tion step which removes trivial orthographic differ-
ences (such as hyphens) between the key word in
the database and the lookup word from the docu-
ment (for details see (Kaljurand et al, 2009)). The
inclusion of other biomedical NE?s (such as pro-
tein names, method names, cell line names) in the
database together with a strict implementation of
the ?longest match? principle leads to better preci-
sion by eliminating false positives caused by match-
ing organism names with a fragment of a multiword
term for another entity (such as the method ?yeast
two-hybrid?).
As mentioned, the names provided by the NCBI
4The reason of this particular choice is that the same subset
was used for experiments related to the automatic detection of
experimental methods, also using IntAct annotations as a gold
standard, described in (Kappeler et al, 2008).
taxonomy have been classified into ?scientific
names? or ?common names?. Using only ?scientific
names? appeared as an effective way to obtain better
precision, but we soon discovered that precision of
the common names suffered most by a few very bad
names, such as ?Li?, which is a ?common name? for
LIV (Louping ill virus) in the taxonomy, but appears
only (and very frequently) as Chinese surname in the
texts. By eliminating about 25 of similar misleading
?common names? the results of this class rose to the
same level as the ?scientific names?, so there was
no reason to exclude the whole class (as that would
have harmed recall).
Since the bibliography might contain spurious
mentions of other organisms, we automatically re-
moved it from the main text. However, contrary to
expectations, this did not lead to better results for
this task (at least after the elimination of the mislead-
ing ?common names? mentioned above), but was
not reversed because of its effects on other tasks. An
intuition from other tasks was to use the abstracts
instead of the full text of the articles, because that
would tend to exclude accidental mentions of organ-
isms leading to false positives. But a main problem
of this approach is that many abstracts do not yield
any organism mentions. Whenever they do though,
their precision is high. So there is a strong case for
giving the mentions there a higher weight, but obvi-
ously the rest of the article plays an important role
as well. We experimentally found that counting an
?abstract mention? as equivalent to 25 ?fulltext men-
tions? worked best.
2.3 Measures Improving Recall
An experiment using all names provided by NCBI
and considering all mentions of those names in the
fulltext version of each article led to a recall of 83%,
leading us to conclude that either the taxonomy does
not contain all names used, or some organisms are
suggested to the human reader by the context and/or
his anticipations. The first of these problems was
adressed by adding some generated names to the
termbase, the second by the use of a default.
Several possible ways of generating new names
automatically from the names in the database were
considered, but only two were applied successfully,
as described below. One of them was the automatic
generation of additional names from the nameclass
82
?scientific name? (for organisms of species or sub-
species level) by the process of replacing the first
word (which would be the genus name in the classi-
cal Linnean binomial nomenclature) by its first letter
and a dot. The resulting names, such as ?E. coli?, are
widely used, but not included in the taxonomy. A
seemingly large disadvantage of this approach is its
potential for ambiguity: 338 of the resulting names
refer to more than one organism. But the test on
our sample showed that of these only 4 occurred at
all, only 1 more than once: ?C. elegans? (potentially
referring to the organisms identified in UniProt as
CAEEL, CENEL, CESEL and CUNEL) which al-
ways stood for CAEEL, i.e. ?Caenorhabditis ele-
gans?. So excluding the other options for ?C. ele-
gans? eliminated the ambiguity (at least in our sam-
ple). We observed that this type of name is in fre-
quent use only for few species and in this case the
unabbreviated name is often used first, so the addi-
tion of this generated nameclass added little to re-
call.
The other type of name missing from the taxon-
omy is the use of the (Linnean) genus name for a
very frequent species, e.g. ?Arabidopsis? used for
?Arabidopsis thaliana?. Experiments showed that
this type could not be reliably generated automat-
ically from the ?scientific names?, as this name-
class includes many names which do not follow
the rules of Linnean binomial nomenclature, mostly
virus names such as ?Human papillomavirus type
me180? where the first word is generally not a
genus name, but a host name. So the problem of
(potentially huge) ambiguity in this type of names
was not even researched, instead the names of this
type for the most frequent organisms were gener-
ated manually and those which improved the results
were included into the termbase (Saccharomyces,
Arabidopsis, Drosophila, Escherichia, Xenopus and
Synechocystis). The addition of this generated
nameclass did not add much to recall for the same
reason as for the first group: in most cases the un-
abbreviated name appears in the paper as well. To-
gether both groups improved recall by about 3.4%.
As HUMAN is the most frequent organism in
this context, it was obvious that a default HUMAN
would take care of many cases where human readers
disambiguate ambiguous protein names even with-
out any explicit mentions of this species. As there
Table 1: Most frequent organisms in IntAct (derived from
interactor proteins and host organisms)
ORG freq
HUMAN 0.281
YEAST 0.272
MOUSE 0.091
ARATH 0.056
CERAE 0.037
RAT 0.033
DROME 0.028
SCHPO 0.023
ECOLX 0.020
ECOLI 0.013
are no cases (with the current termbase and sample)
of articles with no organism mentions in the full text,
we chose to have a default triggered by no findings
in the abstract. Experiments showed that ? contrary
to intuition ? a weight of the default proportional to
the total number of mentions (just adding a percent-
age to HUMAN) would lead to worse results than an
absolute value for the default.5
2.4 Measures Improving Precision
The simple approach of considering every mention
of each organism (after excluding the misleading
common names, as described above), leads to a pre-
cision of only 27.6%, therefore the list of organism
identifiers obtained in this way has to be considered
as a ?candidates list? from which a selection has to
be made.
Candidates can be of course ranked according to
number of mentions in each article. A ranking based
on the mention counts, taking into account the cor-
rection factor of 25 for mentions in the abstract (as
described in section 2.2), was still far from opti-
mal, so we multiplied the mentions with the relative
frequencies of the organisms in a micro-averaged
frequency table (table 1) computed over all of Int-
Act (not just our sample, to avoid overfitting) and
smoothed roughly by attributing 1% of the probabil-
ity mass to all unseen organisms (over 11,000). This
ranking did far better than expected and after nor-
5 A tentative explanation: In a small paper, the effect of ac-
cidental mentions of ?wrong? organisms is much larger than in
big papers (where the important organisms are mentioned again
and again). This detrimental effect may be counterbalanced by
a relatively stronger default.
83
malizing the whole list to 1, a minimal threshold for
the score could be set up to maximize the f-score by
improving precision at the cost of recall. The actual
value of the threshold (currently 0.04) is of course
arbitrary, depending on what measure one wants to
maximize.
Another problem to be tackled is that different pa-
pers will have different numbers of focus organisms,
ranging from one (in about 70% of the cases), to sev-
eral hundreds (in a few very infrequent cases). It
could be assumed that being able to correctly guess
the number of focus organisms would lead to im-
provement in the TX task, as we could pick only as
many candidate organisms (in their ranking order)
as the expected number for the paper. However, an
experiment using the gold standard as an oracle to
predict the number of organisms to be returned as
a result, instead of using a threshold in the ranking,
did not perform much better (recall was about 1.7%
higher), so we decided not to spend any energy on
exploring ways to predict the number of organisms
as the effect would be minimal, even with perfect
prediction.
Further experiments, such as giving different
weights to mentions of names of different name-
classes, did not lead to better results. Including in-
formation about the precision or recall of the names
encountered in our test set (or the organisms pre-
dicted by them) in the formula for the weights6 did
not lead to better results either.
3 Evaluation and analysis of results
So finally the program in its current form considers
all organism mentions, as delivered by the termbase
search, eliminates the problematic common names,
counts the mentions for each organism in fulltext
and abstracts, multiplies the latter by 25 and adds
them to the fulltext mentions. In case of no abstract
mentions, a default of 28 fulltext mentions is added
to HUMAN (equivalent to about one abstract men-
tion).
The result for each organism is multiplied by the
relative frequency of the organism in IntAct and di-
vided by the sum of the results over all organisms to
6An idea suggested by its successful use in the detection of
experimental methods in (Kappeler et al, 2008) and (Rinaldi et
al., 2008).
Table 2: Most frequent false positives for the best results
with our sample
ORG freq
HUMAN 121
YEAST 104
MOUSE 68
ECOLX 18
DROME 13
ARATH 11
RAT 9
Table 3: Most frequent false negatives for the best results
with our sample
ORG freq
CERAE 73
MOUSE 59
RAT 40
YEAST 21
BOVIN 14
ECOLI 13
ECOLX 13
normalize the sum of the values to 1 (100%). All or-
ganisms under the threshold of 0.04 (or 4%) are then
eliminated from the list.
Our best results (max. f-score) for the task of find-
ing all organisms in the gold standard combining or-
ganisms of interacting proteins and host organisms
are: precision: 0.742; recall: 0.738; f-score: 0.740.
An analysis of the most frequent false positives
is reported in table 2. The ranking is more or less
identical with the frequency table (table 1), which is
what we would expect. Manual inspection of some
of the papers causing these false positives gave the
following results:
? Some names of experimental methods contain-
ing organism names (which could avoid false
positives if recognized as methods) were not
yet included in the termbase.
? Some organisms (or their proteins respectively)
are discussed in the paper, but not as results of
the authors own experiments, so they do not ap-
pear in the gold standard. Obviously the cura-
tors consider only the novel findings reported
in the paper, and all background information is
ignored.
84
Table 4: Most frequent organisms in IntAct (derived from
interactor proteins only)
ORG freq
HUMAN 0.380
MOUSE 0.123
YEAST 0.108
ARATH 0.080
RAT 0.047
DROME 0.040
SCHPO 0.032
ECOLI 0.019
BOVIN 0.016
CAEEL 0.014
? While in some cases the annotators seem to de-
cide that an organism is just used as part of
the method and does not merit an inclusion, in
other cases the annotators do not seem to treat
the problem the same way.
An analysis of the most frequent false negatives
is reported in table 3. The ranking is certainly not
identical with the frequency table (table 1), which
was unexpected. Manual inspection of some of the
papers causing these false negatives gave the follow-
ing results:
? Some common names such as ?mice?, and ad-
jectives such as ?murine?, were absent from the
taxonomy (while ?transgenic mice? e.g. was
present).
? There are probably more hints to recognize
ECOLI (Escherichia coli K12) than just the
presence of the string ?K12? (or ?K-12?). Our
program tends to attribute all mentions of ?Es-
cherichia coli? without this string to ECOLX,
generating false negatives for ECOLI and false
positives for ECOLX.
? The extremely high false negative rate for
CERAE (Chlorocebus aethiops, also known as
Cercopithecus aethiops) is a consequence of its
very different frequencies as source of interac-
tor proteins and as a host organism.
The problem with CERAE suggests that it might
be necessary to consider separately organisms in
their roles as sources of the interactor proteins and as
hosts for the experiments. CERAE is only frequent
as a host organism, but in this role it does not appear
in the papers by any of the organism names given
by the taxonomy (such as ?Chlorocebus aethiops?,
?Cercopithecus aethiops?, ?African green monkey?,
?grivet?, ?savanah monkey? or ?vervet monkey?).
The reason is that often only the names of cell lines
(e.g. ?Vero?) derived from the organism appear in
the paper.7 To a lesser degree, this is true as well for
papers where YEAST appears in this role.
A first step to deal with this problem consisted
in creating different frequency tables for organisms
as source of interactor proteins and as hosts of the
experiment (tables 4 and 5). As these frequency ta-
bles are very different from each other and from the
combined one (table 1) and as the combined task
of identifying ?protein organisms? and ?host organ-
isms? seems to be artificial in any case, we decided
to split the problem accordingly: (a) identify organ-
isms from which interacting proteins are derived; (b)
identify host organisms. The results for each of these
new tasks are not yet as good as the result for the
combined task we described above, but as the infor-
mation we are looking for now is more specific, this
was to be expected.
3.1 Identification of ?Interactor Organisms?
In order to obtain a solution for this more specific
task, we just kept the formula as for the original task,
but replaced the frequency table for ?interactor and
host organisms? (table 1) by a new one for ?interac-
tors only? (table 4). At the same time we raised the
threshold to 18%: as the new freqency tables tended
to nearly eliminate several typical host organisms,
the remaining candidates for ?interactor organisms?
profited by this, so the threshold had to be raised
to maximize f-score. The rest of the parameters re-
mained identical.
Obviously, a new gold standard for ?interactors
only? had to be derived from IntAct. Our best results
for this new task are: precision: 0.697; recall: 0.693;
f-score: 0.695.
3.2 Identification of ?Host Organisms?
For this alternative task we also had to improve the
input, not just the formula, as we noticed that of-
7 The Vero lineage is a very popular cell line isolated from
kidney epithelial cells extracted from an African green monkey
(?Cercopithecus aethiops?).
85
Table 5: Most frequent organisms in IntAct (host organ-
isms only), freq* is computed excluding ?in vitro?
ORG freq freq*
?in vitro? 0.363 -
YEAST 0.262 0.412
HUMAN 0.167 0.264
CERAE 0.036 0.057
MOUSE 0.035 0.055
ARATH 0.021 0.034
DROME 0.021 0.034
SCHPO 0.020 0.031
ECOLX 0.017 0.027
RAT 0.010 0.015
ten species which were given as hosts by IntAct
were not mentioned by any of their names (most
importantly CERAE). So we decided to include an-
other category of biological named entities in our
termbase, namely cell line names. These were de-
rived from one of the largest collections of cell
lines information: the Cell Lines Knowledge Base
(CLKB, (Sarntivijai et al, 2008)). However, a few
cell line names which are type-ambiguous with other
types of NE?s in our termbase (normally proteins)
had to be ignored to avoid conflicts. Another new
input to the formula was the mention of ?in vitro?,
contained in our termbase as a method, but used by
the IntAct annotators as annotation for the ?host or-
ganism?.
The following adaptations to the ranking formula
were necessary. The frequency table for ?interactor
and host organisms? (table 1) was replaced by a new
one for ?hosts only?, including ?in vitro? (table 5).
At the same time the default had to be changed to
?in vitro? and was given a nearly identical weight
of 30 fulltext mentions (instead of 28), the thresh-
old remained at 4% and the abstract mentions were
given a weight of 35 fulltext mentions. The new cell
line mentions were given a weight of 3 fulltext men-
tions for their respective organisms. Of course, a
new gold standard for ?interactors only? was derived
from IntAct also in this case. Our best results yet for
this new task are: precision: 0.689; recall: 0.737;
f-score: 0.712.
4 Related Work and Discussion
The task of organism recognition is only recently
starting to emerge as an independent subtask in
biomedical IE. For example, the latest BioCreative
competitive evaluation of text mining system for bi-
ology8 included a task of protein-protein interaction
detection (Krallinger et al, 2008). Although organ-
ism recognition was not officially evaluated, many
participants found that it was an indispensable step
in order to perform accurate protein recognition and
disambiguation. As a consequence, the BioCreative
meta-server (Leitner et al, 2008), offers organism
recognition as one of its services (called ?TX task?).
(Wang and Matthews, 2008) is perhaps the most
comprehensive study to date dealing with species
disambiguation for term disambiguation. They com-
bine a rule-based species disambiguation approach
with a maximum entropy classifier based on con-
textual features of the term to be disambiguated.
They evaluate in detail the contribution of both ap-
proaches over two separate corpora. While previous
work has shown the benefits of using species infor-
mation for term disambiguation (Alex et al, 2008;
Rinaldi et al, 2008), this is perhaps the first study
which also provides a separate evaluation of species
disambiguation in itself. Since their purpose is to
use the organism mentions to disambiguate entities,
they evaluate how far their system can identify the
organisms associated with each entity mention in
the document. They report a level of accuracy that
reaches 74.24% on one of their test corpora.
Since our results are for whole articles, not single
entity mentions, they are not directly comparable.
The advantage of our approach resides in its simplic-
ity, since it does not require a specifically designed
training set, being based only on publicly available
standard databases. This reduces not only the cost
compared to building own resources, but also en-
sures that their quality is monitored.
In this paper we have not discussed how our re-
sults can be used in the disambiguation of entities.
As long as only one organism is selected as the fo-
cus of a given research publication, this is a rather
trivial task. However, as mentioned already in sec-
tion 2.4, it is often the case that multiple organisms
are considered within the same publication. In that
8http://www.biocreative.org/
86
case, organism mentions would need to be ?local-
ized? within the article in order to serve for disam-
biguation purposes, as done in (Wang and Matthews,
2008). Our own approach to this problem is pre-
sented and discussed in (Kaljurand et al, 2009).
One important limitation of our approach is its
reliance on explicit mentions of organisms by their
names as stored in the termbase (or minor variants
thereof). Using all the names available to us (in-
cluding cell lines) and their variants we could so far
achieve only a maximal value of 88% recall, which
means that 12% of the organisms are not referred to
by any name in our resources. This may be due to
either missing names in the termbase (the organisms
are mentioned, but by different names) or because
they are identified by human readers through other
contextual hints which may consist of any sort of in-
formation,9 and may presuppose massive amounts
of background knowledge. The first problem might
be adressed by adding other sources of names to our
termbase. The second problem might be adressed
by using a machine learning approach, which how-
ever brings with it a whole set of new problems, such
as selection and representation of the features rele-
vant for training, as well as the fact that a sufficiently
large training corpus needs to be available.
Another limitation of our approach is the fact that
its development and testing rests on its application
to the identification of either organisms of protein
interactors or host organisms. The original formu-
lation of the goal that motivated this work was ?to
identify automatically the organisms forming part of
the subject matter of scientific papers?. This leaves
open the question of the application of the results,
and is deliberately vague in the wording ?part of the
subject matter?, which includes but is not confined
to the cases mentioned above. This formulation was
motivated by a desire to keep the task as generic as
possible, so that the resulting application could not
only be used as a module for the protein disambigua-
tion task, but also for other tasks of NE disambigua-
tion with respect to organisms, as well as for organ-
ism identification as an independent task. Addition-
ally, the ranked list of candidate organisms delivered
by our program could also be presented to human
9A trivial example would be a publication in a journal which
specializes in research on a single organism.
users, who might want to use them in novel ways,
for example in an assisted curation environment.
However, the gold standard by which we test our
results is tailored to its application as a protein dis-
ambiguation module, just as the frequency tables we
use. Even apart from this, the appropriateness of the
gold standard is partly questionable, as it does not
only prefer organisms involved in protein interac-
tions to those that are not, but also ?new? knowledge
to ?old? knowledge, etc. Our approach, based on
?correcting? simple counts of organism mentions us-
ing frequency tables, can only be successful as long
as there is a gold standard for the specific applica-
tion that is being pursued. We can derive from Int-
Act useful gold standards for organisms from which
protein interactors are derived or host organisms, but
we have no gold standard for ?organism identifica-
tion? as an independent task.
5 Conclusion
In this paper we discussed an approach to the prob-
lem of ?organism identification? as an independent
task, based only on standard resources. While
the initial results were interesting, the experimental
setup led us to identify more specific aspects of the
problem, and in particular to distinguish organisms
mentioned in their roles as sources of the interact-
ing proteins and as hosts of the experiments. We
have shown that a clear identification of the different
functional roles played by organism mentions can
lead to more accurate results.
Although a fully automated disambiguation pro-
cess based on organism mentions is not within im-
mediate reach, the results described in this paper
appear already potentially useful for protein name
disambiguation in a curation environment. An-
other possible application would be in biomedi-
cal curation-based databases, for the semi-automatic
tagging of publications with their focus organisms.
Acknowledgements
This research is partially funded by the Swiss National
Science Foundation (grant 100014-118396/1). Addi-
tional support is provided by Novartis Pharma AG, NI-
TAS, Text Mining Services, CH-4002, Basel, Switzer-
land. We thank the anonymous reviewers for their in-
sightful comments.
87
References
[Alex et al2008] Beatrice Alex, Claire Grover, Barry
Haddow, Mijail Kabadjov, Ewan Klein, Michael
Matthews, Richard Tobin, and Xinglong Wang. 2008.
Automating curation using a natural language process-
ing pipeline. Genome Biology, 9(Suppl 2):S10.
[Kaljurand et al2009] Kaarel Kaljurand, Fabio Rinaldi,
Thomas Kappeler, and Gerold Schneider. 2009. Us-
ing existing biomedical resources to detect and ground
terms in biomedical literature. In 12th Conference on
Artificial Intelligence in Medicine (AIME?09), Verona,
Italy, 18?22 July.
[Kappeler et al2008] Thomas Kappeler, Simon
Clematide, Kaarel Kaljurand, Gerold Schneider,
and Fabio Rinaldi. 2008. Towards automatic de-
tection of experimental methods from biomedical
literature. In Third International Symposium on
Semantic Mining in Biomedicine (SMBM).
[Kerrien et al2006] S. Kerrien, Y. Alam-Faruque,
B. Aranda, I. Bancarz, A. Bridge, C. Derow, E. Dim-
mer, M. Feuermann, A. Friedrichsen, R. Huntley,
C. Kohler, J. Khadake, C. Leroy, A. Liban, C. Lieftink,
L. Montecchi-Palazzi, S. Orchard, J. Risse, K. Robbe,
B. Roechert, D. Thorneycroft, Y. Zhang, R. Apweiler,
and H. Hermjakob. 2006. IntAct ? Open Source
Resource for Molecular Interaction Data. Nucleic
Acids Research.
[Krallinger et al2008] Martin Krallinger, Florian Leit-
ner, Carlos Rodriguez-Penagos, and Alfonso Valencia.
2008. Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Genome Bi-
ology, 9(Suppl 2):S4.
[Leitner et al2008] Florian Leitner, Martin Krallinger,
Carlos Rodriguez-Penagos, Jo?rg Hakenberg, Con-
rad Plake, Cheng-Ju Kuo, Chun-Nan Hsu, Richard
Tzong-Han Tsai, Hsi-Chuan Hung, William W. Lau,
Calvin A. Johnson, Rune Saetre, Kazuhiro Yoshida,
Yan Hua Chen, Sun Kim, Soo-Yong Shin, Byoung-Tak
Zhang, William A. Baumgartner, Lawrence Hunter,
Barry Haddow, Michael Matthews, Xinglong Wang,
Patrick Ruch, Fre?de?ric Ehrler, Arzucan O?zgu?r, Gu?nes
Erkan, Dragomir R. Radev, Michael Krauthammer,
ThaiBinh Luong, Robert Hoffmann, Chris Sander, and
Alfonso Valencia. 2008. Introducing meta-services
for biomedical information extraction. Genome Biol-
ogy, 9(Suppl 2):S6.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc von
Allmen, Pierre Parisot, Martin Romacker, and Therese
Vachon. 2008. OntoGene in BioCreative II. Genome
Biology, 9(Suppl 2):S13.
[Sarntivijai et al2008] Sirarat Sarntivijai, Alexander S.
Ade, Brian D. Athey, and David J. States. 2008. A
bioinformatics analysis of the cell line nomenclature.
Bioinformatics, 24(23):2760?2766.
[UniProt Consortium2007] UniProt Consortium. 2007.
The universal protein resource (UniProt). Nucleic
Acids Research, 35:D193?7.
[Wang and Matthews2008] Xinglong Wang and Michael
Matthews. 2008. Distinguishing the species of
biomedical named entities for term identification.
BMC Bioinformatics, 9(Suppl 11):S6.
[Zanzoni et al2002] A. Zanzoni, L. Montecchi-Palazzi,
M. Quondam, G. Ausiello, M. Helmer-Citterich, and
G. Cesareni. 2002. MINT: a Molecular INTeraction
database. FEBS Letters, 513(1):135?140.
88
Proceedings of the Workshop on BioNLP: Shared Task, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
UZurich in the BioNLP 2009 Shared Task
Kaarel Kaljurand
Institute of
Computational Linguistics
University of Zurich
Switzerland
kalju@cl.uzh.ch
Gerold Schneider
Institute of
Computational Linguistics
University of Zurich
Switzerland
gschneid@cl.uzh.ch
Fabio Rinaldi?
Institute of
Computational Linguistics
University of Zurich
Switzerland
rinaldi@cl.uzh.ch
Abstract
We describe a biological event detection
method implemented for the BioNLP 2009
Shared Task 1. The method relies entirely on
the chunk and syntactic dependency relations
provided by a general NLP pipeline which was
not adapted in any way for the purposes of
the shared task. The method maps the syn-
tactic relations to event structures while be-
ing guided by the probabilities of the syntactic
features of events which were automatically
learned from the training data. Our method
achieved a recall of 26% and a precision of
44% in the official test run, under ?strict equal-
ity? of events.
1 Introduction
This paper describes the adaptation of an existing
text mining system to the BioNLP shared task. The
system has been originally created for participation
in the BioCreative1 protein-protein interaction task
(Rinaldi et al, 2008) and further developed for an
internal project based on the IntAct dataset of pro-
tein interactions (Kerrien et al, 2006). We decided
to participate only in Task 1 of the BioNLP shared
task, mainly because of lack of time and resources.
Our event annotation method relied on various
preprocessing steps and an existing state of the art
dependency parser, which provided the input to the
event annotator. As all the linguistic processing was
performed by the preprocessor and the parser, the
ideas implemented for the event annotator could re-
main simple while still producing reasonable results.
?Corresponding author
1http://www.biocreative.org/
Thus, the event annotator performed a straightfor-
ward rewriting of syntactic structures to event struc-
tures, guided by the information on the syntactic
nature of events that we obtained from the train-
ing data. In this sense our system can be used as
a reference for a comparison to other systems that
rely completely on a dependency parser delivered
analysis that is rewritten into event structures using
knowledge gained from the training data.
Our system consists of a preprocessing phase that
uses a pipeline of NLP tools, described in section 2
of this paper. Linguistic resources are learned auto-
matically from the preprocessed training data (sec-
tion 3). A Prolog-implemented event generator is
applied directly to the preprocessing results and is
guided by the relative frequencies of syntactic fea-
tures provided in the resources (section 4). This
is followed by a postprocessing step that removes
some unlikely event structures, makes sure that all
events that violate the well-formedness rules are fil-
tered out, and finally serializes the event structures
into the requested output format. In section 5 we
present an illustrative example of the events gener-
ated by this approach and discuss some implications
of the event model adopted in the shared task. In
section 6, we describe the evaluation that we per-
formed during the training period, the final official
results on the test data, and some alternative evalu-
ations performed in parallel to the official one. In
section 7 we draw conclusions and describe future
work.
2 Preprocessing
Aside from a format conversion step necessary to
deal with the data provided by the shared task, the
28
preprocessing phase is largely based on an existing
pipeline of NLP tools, that we have developed in the
OntoGene project2 (Rinaldi et al, 2006; Rinaldi et
al., 2008).
2.1 Tokenization, sentence splitting,
part-of-speech tagging
For tokenization, sentence splitting, and part-of-
speech (POS) tagging we used LingPipe3. Ling-
Pipe produces very granular tokens by default, e.g.
a character sequence from abstract 10395645
caspase-3-like (CPP32/Yama/apopain)
which contains multiple hyphens and slashes (as
usual for biomedical texts) is split into 12 (rather
than just 4) tokens
caspase, -, 3, -, like, (, CPP32, /, Yama, /,
apopain, )
allowing a more detailed detection of terms
(shown in boldface in the examples) and trigger-
words which would stay token-internal if a less gran-
ular tokenization was used.
The models used for sentence splitting and POS-
tagging come with the LingPipe distribution and are
trained on the GENIA corpus (Kim et al, 2003),
thus providing a biomedical text aware sentence
splitting and POS-tagging.
2.2 Term annotation
Correctly detecting multi-word terms in the text can
substantially improve the parsing results, because
long noun sequences would be grouped together and
the parser can only focus on the heads of the groups
and ignore the rest. In this task, however, we de-
cided to keep things simple and rely on chunking as
the only means of noun grouping.
Thus, we only annotated the terms provided by
the task organizers in the a1-files (i.e. protein men-
tions). We made the assumption that terms are se-
quences of tokens as defined by the LingPipe tok-
enizer. Whereas in the vast majority of cases this co-
incides with the tokenization used by the organizers,
there are 10 cases in the training data where this as-
sumption is violated (e.g. ?IkappaB-alphaS32/36A?
2http://www.ontogene.org/
3http://alias-i.com/lingpipe/
contains the term ?IkappaB-alpha? but according to
LingPipe, the tokens are ?IkappaB?, ?-?, ?alphaS32?,
?/?, ?36A?).
As the last step of term annotation, we recon-
nected tokens which were separated by hyphens and
slashes, unless the tokens were part of terms. This
allowed for a more reliable processing with tools
which are not optimized to deal with symbols like
hyphens and slashes if these are padded with white-
space.
2.3 Lemmatization using Morpha
Lemmatization was performed using Morpha (Min-
nen et al, 2001), which provides an accurate lemma-
tization given that the input contains part-of-speech
information. We used the lemma information even-
tually only as part of the input to the dependency
parser, i.e. for the other aspects of event annotation
lemmas were ignored.
2.4 Chunking using LTCHUNK
Chunking can considerably reduce parsing com-
plexity, while hardly affecting performance (Prins,
2005). In order to group contiguous sequences of
nouns and verbs, we used LTCHUNK (Mikheev,
1997). LTCHUNK annotates all noun and verb
groups in the sentences. A chunk is an important
unit in the analysis of biomedical texts. Consider an
NP chunk like
T cell-receptor-induced FasL upregula-
tion
which contains two event triggers, amounting to a
mention of a complex event.
After applying LTCHUNK, we also detected
chunk heads, with a simple algorithm ? select last
noun in noun groups, select last verb in verb groups.
This selection is done on the basis of POS-tags.
2.5 Dependency parsing using Pro3Gres
Pro3Gres (Schneider, 2008) is a robust, deep-
syntactic, broad-coverage probabilistic dependency
parser, which identifies grammatical relations be-
tween the heads of chunks, including the majority
of long-distance dependencies. The output is a hi-
erarchical structure of relations (represented as the
directed arrows in the example shown in figure 1).
29
Figure 1: Dependency-syntax tree of the title of abstract 9360945: ?Transcription factor NF-kappaB regulates in-
ducible Oct-2 gene expression in precursor B lymphocytes.? The dependency relations link together the heads of the
5 chunks.
The parser uses a hand-written grammar express-
ing linguistic competence, and a statistical language
model that calculates lexicalized attachment proba-
bilities, thus expressing linguistic performance. The
parser expresses distinctions that are especially im-
portant for a predicate-argument based deep syntac-
tic representation, as far as they are expressed in
the training data generated from the Penn Treebank
(Marcus et al, 1993). This includes prepositional
phrase attachments, control structures, appositions,
relative clause anaphora, participles, gerunds, and
argument/adjunct distinctions. The dependency la-
bel set is similar to the one used in the Stanford
scheme, the parser achieves state-of-the-art perfor-
mance (Haverinen et al, 2008).
We have slightly adapted Pro3Gres to the biomed-
ical domain. A class of nouns that varies consider-
ably in the biomedical domain are relational nouns.
They are syntactically marked because they can have
several prepositional phrase arguments. Biomedical
relational nouns like ?overexpression? or ?transcrip-
tion? are absent from the Penn Treebank or rare. We
have used an unsupervised approach based on (Hin-
dle, D and Rooth, M, 1991) to learn relational nouns
from Medline.
A new relation type, hyph, has been added to con-
nect tokens to hyphens and slashes, and thus better
deal with these characters in biomedical texts.
2.6 Preprocessor output
The preprocessor produces 5 Prolog-formatted files
for each abstract. Each of these files is token-
centered and affiliates a token ID with a group (ei-
ther sentence, chunk, or term) that contains this to-
ken, or maps it to a syntactically related (either as
the head or the dependent) token.
? Tokens maps each token to its lemma, POS-
tag, and character offsets
? Chunks maps each token to its containing
chunk, chunk?s type (noun or verb group), and
chunk?s head
? Terms maps each token to its containing term,
term?s type, term?s ID (assigned by the a1-file,
or the a2-file in case of processing the training
data)
? Sentences maps each sentence ID to the list of
IDs of the tokens in the sentence
? Dependencies maps each token to its imme-
diate head and dependent, and to the types of
these dependency relations
These files are the input to the resource generator
described below, and later (together with the gener-
ated resources), the input to the event annotator.
3 Resources
The 800 abstracts of the training data were used
during development for the generation of three re-
sources which are described in this section. For the
official testing we used the concatenation of training
and development data (i.e. 950 abstracts). The re-
sources were generated automatically from the a1-
and a2-files; and from the preprocessed version of
txt-, a1- and a2-files. The resulting data files include
frequencies of the total occurrence of an item (e.g.
word, syntactic configuration) and the frequency of
its occurrence in an event.
All the words in the resources were lowercased
but not lemmatized. Resources were stored as
Prolog-formatted files.
30
Frequency Event type Event arguments
149 Gene expression Theme(T)
28 Transcription Theme(T)
2 Localization Theme(T), AtLoc(T)
1 Positive regulation Theme(T)
1 Positive regulation Theme(E)
Table 1: Frequency distribution of the event structures
that are triggered by the word form ?expressed? which in
total triggered an event 181 times in the training data. ?T?
means that the argument is filled by a term, ?E? means
that the argument is filled by an event.
3.1 Words
The word frequencies file provides a simple prob-
abilistic model for excluding stopwords, as we ob-
served that many different function words some-
times triggered events in the training data. We
wanted to exclude such words to obtain a better pre-
cision. The words-resource can be queried using a
simple interface
word_to_freq(+Word, -F)
which maps every word to its frequency.
3.2 Event types and arguments
Using the training data, we created a mapping from
each candidate trigger-word to the possible event
types and the permissible event frames. A sample of
this mapping is illustrated in table 1. The arguments
have a type (e.g. Theme) but their filler is abstracted
to be either ?T? (for terms) or ?E? (for events).
This resource can be queried via the interface
eword_to_event(+EventWord,
-EventType, -EventArgs, -F1, -F2)
which maps every trigger-word to its possible
event type and arguments. The returned frequencies
show how often the event structure was triggered
by the trigger-word, and how often the trigger-word
triggered an event in total.
3.3 Domination paths between terms
The most sophisticated of the resources that we
generated recorded the syntactic paths between the
terms (from a1- and a2-files) observed in the train-
ing data, and counted how often these paths were
present in events, connecting triggers with event ar-
gument fillers. With each term, also its type (e.g.
Positive regulation, Protein) was recorded.
For the syntactic paths, we only considered dom-
ination paths where one of the terms is the head and
the other the dependent, defined as follows.
Definition 1 (Domination between chunks)
Term t1 dominates term t2 if t1 ? c1 and
t2 ? c2 and there exists a directed syntactic path
h(c1) ? . . . ? h(c2), where h(?) is the head of the
given chunk.
For example, in figure 1, the term ?regulates?
dominates all the other tokens, among them the term
?expression? (which is the head of its chunk), and the
Protein-term ?Oct-2?. Note that this definition does
not require the terms to be in the chunk head posi-
tion. However, this decision did not affect the results
significantly.
The chunk-internal domination relation is defined
for terms which are chunk-internal and thus ?invisi-
ble? to the dependency parser because the parser ig-
nores everything but the head of the chunk. This re-
lation captures the default syntactic dependency be-
tween nouns in noun groups where the head noun
usually follows its dependents.
Definition 2 (Chunk-internal domination) Term
t1 dominates term t2 if t1, t2 ? c and i(t1) > i(t2),
where i(?) is the sequential index of the given term
in the chunk.
For example, in figure 1, in the 3rd chunk, the
term ?expression? dominates the terms ?Oct-2? and
?inducible?; and furthermore, ?Oct-2? dominates ?in-
ducible?.
The stored syntactic path is a list of dependency
relations from the dependent to the head, or an
empty list if both terms are in the same chunk.
Instead of domination, we also considered using
the asymmetric relation of ?connectedness?, where
two terms are connected if either of the terms dom-
inates the other, or if both are dominated by some
token in the tree. This relation, however, seemed to
decrease precision much more than increase recall.
In order to query the domination resource we
designed a simple query interface that allows for
partially instantiated input. For example the query
(where the underscores denote uninstantiated parts)
?- find_path_freq(bind, ?Binding?,
_, ?Protein?,
[modpp | _ ],
F1, F2).
31
asks how often there is a domination relation be-
tween the head term ?bind? if it has the type Bind-
ing and some dependent term with type Protein,
such that the dependency path starts with the rela-
tion modpp. The frequency counts resulting from
this query tell the frequency of this configuration in
events (F1), and in total (F2). This information al-
lows the computation of the conditional probability
of an argument of an event given the event type, the
trigger-word, the argument word, the argument type,
and the syntactic path between trigger and argument.
4 Event generation
The event generation relied fully on the syntax tree
and chunk information that was delivered by the pre-
processing module. No fall-back to a surface co-
occurrence of words was used. We only considered
words and structures seen in the training data as pos-
sible parts of events. Such a design entails relatively
good precision at lower recall.
For each of the generation steps described below,
a probability threshold decided whether to continue
the ?building? of the event given the trigger-word,
the event arguments template or the argument in-
stantiation. The thresholds were set manually after
some experimentation. We did not try to automat-
ically decide the best performing thresholds. Deci-
sions are taken locally, possibly cutting some local
minima. A simple maximum-likelihood estimation
(MLE) approach was used.
4.1 Trigger generation
Trigger candidates were generated from the token
list of each sentence in the analyzed abstract. Fig-
ure 2 shows a browser-based visualization approach
that we created as a support in our work. In the case
of the training data, the annotations come the a1-
and a2-files provided by the organizers. In the case
of the development and test data, the annotations for
the triggers are those generated by the system.
We only considered one-token trigger-words be-
cause multi-token triggers were less frequent in the
training data, where only about 8% of the trigger-
word forms contained a space character. Also, many
of these multiword triggers contain a token that ex-
ists as a trigger on its own (e.g. ?transcriptional reg-
ulation? triggers the Regulation-event in the training
data, as does ?regulation?), allowing us to generate a
sensible event structure even if it does not match a
gold standard event under the ?strict equality?. To-
kens that had been seen to trigger an event in the
training data with probability higher than 0.12 were
considered further.
In MLE terms, we calculate the probability of a
given token to be a trigger as follows:
p(Trigger |Token) = f(Token ? (Token = Trigger))f(Token)
(1)
4.2 Event type and arguments template
generation
Next, trigger-words were mapped to event type and
argument template structures. In MLE terms, we
calculated the probability of an event structure (i.e.
the combination of event type and arguments tem-
plate) given the trigger-word.
p(EventStruct |Trigger) = f(Trigger ? EventStruct)f(Trigger)
(2)
Again, only high probability structures were con-
sidered further. We used the probability threshold of
0.25 for simple event structures (i.e. not containing
nested events), and 0.1 for complex event structures
(only regulation events in the shared task).
4.3 Event argument filling
The inclusion of a protein as an argument of an event
was based on the syntactic domination of the trigger
of the event over the term of the protein. We at-
tempted to generate simple events of all types seen
in the training data.
For complex events, the trigger-words of the main
and the embedded events had to be in a domination
relationship. We generated regulation-events with
only 1-level embedding. Although more complex
embeddings are possible (see example below), these
are not very frequent.
prevents T cell-receptor-induced FasL
upregulation
In order to flexibly deal with sparse data, we per-
formed a sequence of queries, one less instantiated
32
Figure 2: Example of an annotated sentence from abstract 10080948 in the training data.
than the previous one, weighted the results accord-
ingly and calculated the weighted mean to be the fi-
nal probability for including the argument.
find_path_freq(HWord, HType, DWord, DType, Path,
C1_1, C2_1),
find_path_freq(_, HType, _, DType, Path,
C1_2, C2_2),
find_path_freq(_, HType, _, DType, _,
C1_3, C2_3)
In MLE terms, we calculate the probability that
a syntactic configuration fills an argument slot.
Syntactic configurations consist of the head word
HWord, the head event type HType, the dependent
word DWord, the dependent event type DType, and
the syntactic path Path between them.
p(Arg |HWord, HType, DWord, DType, Path) =
1
w1+w2+w3 ? (
w1 ? f(HWord, HType, DWord, DType, Path?Arg)f(HWord, HType, DWord, DType, Path) +
w2 ? f(HType, DType, Path?Arg)f(HType, DType, Path) +
w3 ? f(HType, DType?Arg)f(HType, DType) ) (3)
The weigths were set as w1 = 3, w2 = 2 and
w3 = 1.2. The fact that the weights decrease ap-
proximates a back-off model. Only if the final prob-
ability was higher than 0.3 the event was further con-
sidered. For complex events, we used formula 3 as
given, but for simple events, where DWord is a pro-
tein, DWord was always left uninstantiated.
4.4 Postprocessing
During the postprocessing step some unlikely event
structures were filtered out. This filtering is delayed
until all the events have been generated, because ex-
cluding the unwanted events is difficult during cre-
ation time as sometimes extrospection is required.
Also, the postprocessing step acts as a safety net
that filters out well-formedness errors (e.g. argu-
ment sharing violations), thus making sure that the
submission to the evaluation system is not rejected
by the system. Finally, the set of generated events is
serialized into the BioNLP a2-format.
5 Example and discussion
As an example of application of our approach, con-
sider again the syntactic tree shown in figure 1.
Our approach results in the generation of the events
shown in figure 3, given that ?regulates?, ?inducible?,
and ?expression? are trigger-words, and ?Oct-2? is an
a1-annotated protein.
Figure 3: Visualization of two simple event structures
regulates(Oct-2) and expression(Oct-2), and a complex
structure regulates(expression(Oct-2)).
We call events like regulates(Oct-2) ?shortcut
events?, as there exists an alternative and longer
path ? regulates(expression) and expression(Oct-2)
? that connects the trigger to its event argument.
These ?shortcut events? are filtered out in the post-
processing step as unlikely events.
It is useful to observe that the particular view of
event structures defined by the BioNLP shared task
is by no means unchallenged. Whether nested events
are necessary in a representation of biological rele-
vant relations is a question which is open to debate.
While from the linguistic perspective they do offer a
more adequate representation of the content matter
of the text, from the biological point of view these
structures are redundant in many cases. The exam-
ple used in this section is illustrative.
From the biologist?s perspective, ?A regulates the
expression of B? is a way to express that A regu-
lates B. Obviously such a short-circuit is not in all
cases possible, but the point is that the biologist
33
might be interested only in the direct biological in-
teractions, and be inclined to ignore the linguistic
representation of that interaction. This is the point
of view taken for example in the Protein-Protein
Interaction task of the latest BioCreative competi-
tion (Krallinger et al, 2008). In that case, all lin-
guistic structures used to better characterize the in-
teraction are purposefully ignored, and only the bare
interaction is preserved.
Since BioCreative aimed at simulating the pro-
cess of database curation, and was based on datasets
provided by real-word interaction databases such as
IntAct (Kerrien et al, 2006) and MINT (Zanzoni et
al., 2002), there is reasonable motivation for taking
this alternative view into consideration. At the very
least, a mapping from complex events to simple in-
teractions should always be provided.
The difference in the approach towards interpre-
tation of literature fragments has a direct impact on
the resources used and the success of each approach.
Our own development in the past couple of years has
been driven by the BioCreative model (Rinaldi et al,
2008), and therefore we tended to ignore intermedi-
ate structures in protein interactions. For example,
in (Schneider et al, 2009) we present a lexical re-
source that aims at capturing ?transparent? relations,
i.e. words that express a relation that from the bio-
logical point of view can be ignored because of its
transitivity properties, such as ?expression of Oct-
2? in the example above. This resource, although
certainly useful from the biological point of view,
proved to be useless in the shared task, due to the
different level of granularity in the representation of
events.
6 Official evaluation and additional
experiments
We mainly trained and evaluated using the ?strict
equality? evaluation criteria as our reference. The
results on the development data are shown in table
2. With more relaxed equality definitions, the results
were always a few percentage points better. Our re-
sults in the official testrun are shown in table 3.
Good results for some event structures (notably
Phosphorylation) are due to the simple textual repre-
sentation of these events. For example, Phosphory-
lation is always triggered by a form or derivation of
?phosphorylate?, and these forms rarely trigger any
other types of events. Furthermore, according to the
parsed training data, the probability of a Phospho-
rylation-event, given a syntactic domination relation
between a Phosphorylation-trigger and a protein is
0.92. Also, 56% of these domination paths are ei-
ther chunk-internal or over a single modpp depen-
dency relation, making them easy to detect.
In parallel to the approach used in our official sub-
mission we considered some variants, aimed at max-
imizing either recall or precision, as well as an alter-
native approach based on machine learning.
A high recall baseline method, which generates
all possible event structures in a given sentence,
achieves 81% recall on simple events, with preci-
sion dropping to 11%. One of the reasons why this
method does not reach 100% recall is the fact that
it only annotates event candidates with single-token
triggers that have been seen in the training data.
The filter described in section 4.3 has a major ef-
fect on precision. If it is removed, precision drops
by 11%, while the gain in recall is only 3% ? re-
call 35.10%, precision 37.88%, F-score 36.44%. In-
stead, if we keep w1 but set w2 = w3 = 0 in formula
3, precision increases to 56%, while recall drops to
27%. Increasing the probability thresholds to further
improve precision results in the precision of 60% but
this remains the ceiling in our experiments.
Additionally, we performed separate experiments
with a machine-learning approach which considers
a more varied set of features, including surface in-
formation and syntax coming from an ensemble of
parsers. However, the limited time and resources
available to us during the competition did not al-
low us to go beyond the results achieved using the
approach described in detail in this paper. Since
our best score on the development data was 27%
(about 10% inferior to our consolidated approach),
we opted for not considering this approach in our
official submission.
The fact that this approach was based on a de-
composition of events into their arguments led us to
realize some fundamental limitations in the official
evaluation measures. In particular, none of the orig-
inally implemented measures would give credit to
the partial recognition of an event (i.e. correct trig-
ger word and at least one correct argument, but not
all). We contend that such partial recognition can be
34
Event class Precision Recall F-Score True pos. False pos. False neg.
Simple events 56.71 48.20 52.11 389 297 418
Complex events 38.03 19.25 25.56 189 308 793
All events 48.86 32.31 38.90 578 605 1211
Table 2: Results on the development data of 150 abstracts, measured using ?strict equality?.
Event class gold (match) answer (match) Recall Precision F-Score
Localization 174 (31) 34 (31) 17.82 91.18 29.81
Binding 347 (102) 287 (102) 29.39 35.54 32.18
Gene expression 722 (370) 515 (370) 51.25 71.84 59.82
Transcription 137 (28) 148 (28) 20.44 18.92 19.65
Protein catabolism 14 (8) 16 (8) 57.14 50.00 53.33
Phosphorylation 135 (78) 84 (78) 57.78 92.86 71.23
Simple events total 1529 (617) 1084 (617) 40.35 56.92 47.23
Regulation 291 (29) 120 (29) 9.97 24.17 14.11
Positive regulation 983 (138) 533 (138) 14.04 25.89 18.21
Negative regulation 379 (55) 158 (55) 14.51 34.81 20.48
Complex events total 1653 (222) 811 (222) 13.43 27.37 18.02
All events total 3182 (839) 1895 (839) 26.37 44.27 33.05
Table 3: Results on the test data of 260 abstracts, measured using ?strict equality?, as reported by the BioNLP 2009
online evaluation system.
useful in a practical annotation task, and yet the of-
ficial scores doubly punish such an outcome (once
as a FP and once as a FN). This is a problem already
observed in previous evaluation challenges, however
we believe that a simple solution in this case consists
in decomposing the events (for evaluation purposes)
in their constituent roles and arguments. In other
words, each event is given as much ?weight? as its
number of roles. The correct recognition of an event
with two roles would therefore lead to two TP, but its
partial recognition (one argument) would still lead
to one TP, which we think is a more fair evaluation
in case of partial recognition. Our suggestion was
later implemented by the organizers as an additional
scoring criteria.
7 Conclusions and future work
We have described a biological event detection
method that relies on the chunk and syntactic de-
pendency relations obtained during the preprocess-
ing stage. No fall-back strategy that is based on e.g.
surface patterns was designed for this task. This is
consistent with our approach to biomedical event de-
tection ? relation extraction is entirely based on ex-
isting syntactic information about the sentences, and
can be ported easily if the definition of relations and
events is changed, as in the case of other competi-
tions which use a different notion of relations (e.g.
BioCreative).
As the chunker and the dependency parser form
a core of the described system, their limitations and
improvements have a fundamental effect on the fur-
ther processing. In parallel to a thorough error anal-
ysis which can drive further development of our con-
solidated approach, we intend to further explore the
enhanced flexibility provided by the machine learn-
ing approach briefly mentioned in section 6. In both
cases, we intend to use the BioNLP shared task eval-
uation site as a reference in order to compare them,
not only against each other, but also against the re-
sults of other participants.
Acknowledgements
This research is partially funded by the Swiss Na-
tional Science Foundation (grant 100014-118396/1).
Additional support is provided by Novartis Pharma
AG, NITAS, Text Mining Services, CH-4002, Basel,
Switzerland. The authors would like to thank the
two anonymous reviewers of BioNLP 2009 for their
valuable feedback.
35
References
[Haverinen et al2008] Katri Haverinen, Filip Ginter,
Sampo Pyysalo, and Tapio Salakoski. 2008. Accu-
rate conversion of dependency parses: targeting the
stanford scheme. In Proceedings of Third Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM 2008), Turku, Finland.
[Hindle, D and Rooth, M1991] Hindle, D and Rooth, M.
1991. Structural Ambiguity and Lexical Relations.
Meeting of the Association for Computational Linguis-
tics, pages 229?236.
[Kerrien et al2006] S. Kerrien, Y. Alam-Faruque,
B. Aranda, I. Bancarz, A. Bridge, C. Derow, E. Dim-
mer, M. Feuermann, A. Friedrichsen, R. Huntley,
C. Kohler, J. Khadake, C. Leroy, A. Liban, C. Lieftink,
L. Montecchi-Palazzi, S. Orchard, J. Risse, K. Robbe,
B. Roechert, D. Thorneycroft, Y. Zhang, R. Apweiler,
and H. Hermjakob. 2006. IntAct ? Open Source
Resource for Molecular Interaction Data. Nucleic
Acids Research.
[Kim et al2003] J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsu-
jii. 2003. GENIA corpus ? a semantically annotated
corpus for bio-textmining. Bioinformatics, 19(1):180?
182.
[Krallinger et al2008] Martin Krallinger, Florian Leit-
ner, Carlos Rodriguez-Penagos, and Alfonso Valencia.
2008. Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Genome Bi-
ology, 9(Suppl 2):S4.
[Marcus et al1993] M Marcus, B Santorini, and
M Marcinkiewicz. 1993. Building a Large An-
notated Corpus of English: the Penn Treebank.
Computational Linguistics, 19:313?330.
[Mikheev1997] A Mikheev. 1997. Automatic rule induc-
tion for unknown word guessing. Computational Lin-
guistics, 23(3):405?423.
[Minnen et al2001] G Minnen, J Carroll, and D Pearce.
2001. Applied morphological processing of English.
Natural Language Engineering, 7(3):207?223.
[Prins2005] Robbert Prins. 2005. Finite-State Pre-
Processing for Natural Language Analysis. Ph.D. the-
sis, Behavioral and Cognitive Neurosciences (BCN)
research school, University of Groningen.
[Rinaldi et al2006] Fabio Rinaldi, Gerold Schneider,
Kaarel Kaljurand, Michael Hess, and Martin Ro-
macker. 2006. An Environment for Relation Mining
over Richly Annotated Corpora: the case of GENIA.
BMC Bioinformatics, 7(Suppl 3):S3.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc von
Allmen, Pierre Parisot, Martin Romacker, and Therese
Vachon. 2008. OntoGene in BioCreative II. Genome
Biology, 9(Suppl 2):S13.
[Schneider et al2009] Gerold Schneider, Kaarel Kalju-
rand, Thomas Kappeler, and Fabio Rinaldi. 2009.
Detecting protein-protein interactions in biomedical
texts using a parser and linguistic resources. In CI-
CLing 2009, 10th International Conference on Intel-
ligent Text Processing and Computational Linguistics,
Mexico City, Mexico.
[Schneider2008] Gerold Schneider. 2008. Hybrid Long-
Distance Functional Dependency Parsing. Ph.D. the-
sis, Faculty of Arts, University of Zurich.
[Zanzoni et al2002] A. Zanzoni, L. Montecchi-Palazzi,
M. Quondam, G. Ausiello, M. Helmer-Citterich, and
G. Cesareni. 2002. MINT: a Molecular INTeraction
database. FEBS Letters, 513(1):135?140.
36
