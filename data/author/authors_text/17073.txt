Proceedings of NAACL-HLT 2013, pages 617?626,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Down-stream effects of tree-to-dependency conversions
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi?,
Hector Martinez, Anders S?gaard
Center for Language Technology, University of Copenhagen
?Institute for Informatics, University of Oslo
Abstract
Dependency analysis relies on morphosyntac-
tic evidence, as well as semantic evidence.
In some cases, however, morphosyntactic ev-
idence seems to be in conflict with seman-
tic evidence. For this reason dependency
grammar theories, annotation guidelines and
tree-to-dependency conversion schemes often
differ in how they analyze various syntactic
constructions. Most experiments for which
constituent-based treebanks such as the Penn
Treebank are converted into dependency tree-
banks rely blindly on one of four-five widely
used tree-to-dependency conversion schemes.
This paper evaluates the down-stream effect of
choice of conversion scheme, showing that it
has dramatic impact on end results.
1 Introduction
Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conversion
schemes for converting constituent-based treebanks
into dependency treebanks are typically based on
a specific dependency grammar theory, such as the
Prague School?s Functional Generative Description,
Meaning-Text Theory, or Hudson?s Word Grammar.
In practice most parsers constrain dependency struc-
tures to be tree-like structures such that each word
has a single syntactic head, limiting diversity be-
tween annotation a bit; but while many dependency
treebanks taking this format agree on how to an-
alyze many syntactic constructions, there are still
many constructions these treebanks analyze differ-
ently. See Figure 1 for a standard overview of clear
and more difficult cases.
The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases morphosyn-
tactic and semantic evidence cohere. Verbs gov-
ern subjects morpho-syntactically and seem seman-
tically more important. In the difficult cases, how-
ever, morpho-syntactic evidence is in conflict with
the semantic evidence. While auxiliary verbs have
the same distribution as finite verbs in head position
and share morpho-syntactic properties with them,
and govern the infinite main verbs, main verbs seem
semantically superior, expressing the main predi-
cate. There may be distributional evidence that com-
plementizers head verbs syntactically, but the verbs
seem more important from a semantic point of view.
Tree-to-dependency conversion schemes used
to convert constituent-based treebanks into
dependency-based ones also take different stands on
the difficult cases. In this paper we consider four dif-
ferent conversion schemes: the Yamada-Matsumoto
conversion scheme yamada,1 the CoNLL 2007
format conll07,2 the conversion scheme ewt used in
the English Web Treebank (Petrov and McDonald,
2012),3 and the lth conversion scheme (Johansson
1The Yamada-Matsumoto scheme can be
replicated by running penn2malt.jar available at
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html. We
used Malt dependency labels (see website). The Yamada-
Matsumoto scheme is an elaboration of the Collins scheme
(Collins, 1999), which is not included in our experiments.
2The CoNLL 2007 conversion scheme can be
obtained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/with the
?conll07? flag set.
3The EWT conversion scheme can be repli-
cated using the Stanford converter available at
http://nlp.stanford.edu/software/stanford-dependencies.shtml
617
Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal
Punctuation
Figure 1: Clear and difficult cases in dependency annotation.
and Nugues, 2007).4 We list the differences in
Figure 2. An example of differences in analysis is
presented in Figure 3.
In order to access the impact of these conversion
schemes on down-stream performance, we need ex-
trinsic rather than intrinsic evaluation. In general
it is important to remember that while researchers
developing learning algorithms for part-of-speech
(POS) tagging and dependency parsing seem ob-
sessed with accuracies, POS sequences or depen-
dency structures have no interest on their own. The
accuracies reported in the literature are only inter-
esting insofar they correlate with the usefulness of
the structures predicted by our systems. Fortunately,
POS sequences and dependency structures are use-
ful in many applications. When we consider tree-to-
dependency conversion schemes, down-stream eval-
uation becomes particularly important since some
schemes are more fine-grained than others, leading
to lower performance as measured by intrinsic eval-
uation metrics.
Approach in this work
In our experiments below we apply a state-of-the-art
parser to five different natural language processing
(NLP) tasks where syntactic features are known to
be effective: negation resolution, semantic role la-
beling (SRL), statistical machine translation (SMT),
sentence compression and perspective classification.
In all five tasks we use the four tree-to-dependency
conversion schemes mentioned above and evaluate
them in terms of down-stream performance. We also
compare our systems to baseline systems not rely-
4The LTH conversion scheme can be ob-
tained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/ with the
?oldLTH? flag set.
ing on syntactic features, when possible, and to re-
sults in the literature, when comparable results exist.
Note that negation resolution and SRL are not end
applications. It is not easy to generalize across five
very different tasks, but the tasks will serve to show
that the choice of conversion scheme has significant
impact on down-stream performance.
We used the most recent release of the Mate parser
first described in Bohnet (2010),5 trained on Sec-
tions 2?21 of the Wall Street Journal section of the
English Treebank (Marcus et al, 1993). The graph-
based parser is similar to, except much faster, and
performs slightly better than the MSTParser (Mc-
Donald et al, 2005), which is known to perform
well on long-distance dependencies often important
for down-stream applications (McDonald and Nivre,
2007; Galley and Manning, 2009; Bender et al,
2011). This choice may of course have an effect on
what conversion schemes seem superior (Johansson
and Nugues, 2007). Sentence splitting was done us-
ing splitta,6, and the sentences were then tokenized
using PTB-style tokenization7 and tagged using the
in-built Mate POS tagger.
Previous work
There has been considerable work on down-stream
evaluation of syntactic parsers in the literature, but
most previous work has focused on evaluating pars-
ing models rather than linguistic theories. No one
has, to the best of our knowledge, compared the
impact of choice of tree-to-dependency conversion
scheme across several NLP tasks.
Johansson and Nugues (2007) compare the im-
pact of yamada and lth on semantic role labeling
5http://code.google.com/p/mate-tools/
6http://code.google.com/p/splitta/
7http://www.cis.upenn.edu/?treebank/tokenizer.sed
618
FORM1 FORM2 yamada conll07 ewt lth
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2
Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names.
Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.
performance, showing that lth leads to superior per-
formance.
Miyao et al (2008) measure the impact of syntac-
tic parsers in an information extraction system iden-
tifying protein-protein interactions in biomedical re-
search articles. They evaluate dependency parsers,
constituent-based parsers and deep parsers.
Miwa et al (2010) evaluate down-stream per-
formance of linguistic representations and parsing
models in biomedical event extraction, but do not
evaluate linguistic representations directly, evaluat-
ing representations and models jointly.
Bender et al (2011) compare several parsers
across linguistic representations on a carefully de-
signed evaluation set of hard, but relatively frequent
syntactic constructions. They compare dependency
parsers, constituent-based parsers and deep parsers.
The authors argue in favor of evaluating parsers on
diverse and richly annotated data. Others have dis-
cussed various ways of evaluating across annotation
guidelines or translating structures to a common for-
mat (Schwartz et al, 2011; Tsarfaty et al, 2012).
Hall et al (2011) discuss optimizing parsers for
specific down-stream applications, but consider only
a single annotation scheme.
Yuret et al (2012) present an overview of the
SemEval-2010 Evaluation Exercises on Semantic
Evaluation track on recognition textual entailment
using dependency parsing. They also compare sev-
eral parsers using the heuristics of the winning sys-
tem for inference. While the shared task is an
example of down-stream evaluation of dependency
parsers, the evaluation examples only cover a subset
of the textual entailments relevant for practical ap-
plications, and the heuristics used in the experiments
assume a fixed set of dependency labels (ewt labels).
Finally, Schwartz et al (2012) compare the
above conversion schemes and several combinations
thereof in terms of learnability. This is very different
from what is done here. While learnability may be
a theoretically motivated parameter, our results indi-
cate that learnability and downstream performance
do not correlate well.
2 Applications
Dependency parsing has proven useful for a wide
range of NLP applications, including statistical ma-
chine translation (Galley and Manning, 2009; Xu et
al., 2009; Elming and Haulrich, 2011) and sentiment
analysis (Joshi and Penstein-Rose, 2009; Johansson
and Moschitti, 2010). This section describes the ap-
plications and experimental set-ups included in this
study.
In the five applications considered below we
619
use syntactic features in slightly different ways.
While our statistical machine translation and sen-
tence compression systems use dependency rela-
tions as additional information about words and on
a par with POS, our negation resolution system uses
dependency paths, conditioning decisions on both
dependency arcs and labels. In perspective classifi-
cation, we use dependency triples (e.g. SUBJ(John,
snore)) as features, while the semantic role labeling
system conditions on a lot of information, including
the word form of the head, the dependent and the ar-
gument candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled depen-
dency relations between predicate and its head, its
arguments, dependents or siblings.
2.1 Negation resolution
Negation resolution (NR) is the task of finding nega-
tion cues, e.g. the word not, and determining their
scope, i.e. the tokens they affect. NR has recently
seen considerable interest in the NLP community
(Morante and Sporleder, 2012; Velldal et al, 2012)
and was the topic of the 2012 *SEM shared task
(Morante and Blanco, 2012).
The data set used in this work, the Conan Doyle
corpus (CD),8 was released in conjunction with the
*SEM shared task. The annotations in CD extend
on cues and scopes by introducing annotations for
in-scope events that are negated in factual contexts.
The following is an example from the corpus show-
ing the annotations for cues (bold), scopes (under-
lined) and negated events (italicized):
(1) Since we have been so
unfortunate as to miss him [. . . ]
CD-style scopes can be discontinuous and overlap-
ping. Events are a portion of the scope that is se-
mantically negated, with its truth value reversed by
the negation cue.
The NR system used in this work (Lapponi et al,
2012), one of the best performing systems in the
*SEM shared task, is a CRF model for scope resolu-
tion that relies heavily on features extracted from de-
pendency graphs. The feature model contains token
distance, direction, n-grams of word forms, lemmas,
POS and combinations thereof, as well as the syntac-
tic features presented in Figure 4. The results in our
8http://www.clips.ua.ac.be/sem2012-st-neg/data.html
Syntactic
constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation
Cue-dependent
directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path
Figure 4: Features used to train the conditional random
field models
experiments are obtained from configurations that
differ only in terms of tree-to-dependency conver-
sions, and are trained on the training set and tested
on the development set of CD. Since the negation
cue classification component of the system does not
rely on dependency features at all, the models are
tested using gold cues.
Table 1 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.
2.2 Semantic role labeling
Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-
bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).9
The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool10 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conversion
schemes. We rely on gold standard argument identi-
fication and focus solely on the performance metric
semantic labeled F1.
9http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames
10http://code.google.com/p/clearparser/
620
2.3 Statistical machine translation
The effect of the different conversion schemes was
also evaluated on SMT. We used the reordering
by parsing framework described by Elming and
Haulrich (2011). This approach integrates a syn-
tactically informed reordering model into a phrase-
based SMT system. The model learns to predict the
word order of the translation based on source sen-
tence information such as syntactic dependency re-
lations. Syntax-informed SMT is known to be use-
ful for translating between languages with different
word orders (Galley and Manning, 2009; Xu et al,
2009), e.g. English and German.
The baseline SMT system is created as described
in the guidelines from the original shared task.11
Only modifications are that we use truecasing in-
stead of lowercasing and recasing, and allow train-
ing sentences of up to 80 words. We used data
from the English-German restricted task: ?3M par-
allel words of news, ?46M parallel words of Eu-
roparl, and ?309M words of monolingual Europarl
and news. We use newstest2008 for tuning, new-
stest2009 for development, and newstest2010 for
testing. Distortion limit was set to 10, which is
also where the baseline system performed best. The
phrase table and the lexical reordering model is
trained on the union of all parallel data with a max
phrase length of 7, and the 5-gram language model
is trained on the entire monolingual data set.
We test four different experimental systems that
only differ with the baseline in the addition of a syn-
tactically informed reordering model. The baseline
system was one of the tied best performing system
in the WMT 2011 shared task on this dataset. The
four experimental systems have reordering models
that are trained on the first 25,000 sentences of the
parallel news data that have been parsed with each
of the tree-to-dependency conversion schemes. The
reordering models condition reordering on the word
forms, POS, and syntactic dependency relations of
the words to be reordered, as described in Elming
and Haulrich (2011). The paper shows that while
reordering by parsing leads to significant improve-
ments in standard metrics such as BLEU (Papineni
et al, 2002) and METEOR (Lavie and Agarwal,
2007), improvements are more spelled out with hu-
11 http://www.statmt.org/wmt11/translation-task.html
man judgements. All SMT results reported below
are averages based on 5 MERT runs following Clark
et al (2011).
2.4 Sentence compression
Sentence compression is a restricted form of sen-
tence simplification with numerous usages, includ-
ing text simplification, summarization and recogniz-
ing textual entailment. The most commonly used
dataset in the literature is the Ziff-Davis corpus.12 A
widely used baseline for sentence compression ex-
periments is Knight and Marcu (2002), who intro-
duce two models: the noisy-channel model and a de-
cision tree-based model. Both are tree-based meth-
ods that find the most likely compressed syntactic
tree and outputs the yield of this tree. McDonald et
al. (2006) instead use syntactic features to directly
find the most likely compressed sentence.
Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), comparable to
previously explored models of noun phrase chunk-
ing. Our model is thus neither tree-based nor
sentence-based. Instead we think of sentence com-
pression as a sequence labeling problem. We com-
pare a model informed by word forms and predicted
POS with models also informed by predicted depen-
dency labels. The baseline feature model conditions
emission probabilities on word forms and POS us-
ing a ?2 window and combinations thereoff. The
augmented syntactic feature model simply adds de-
pendency labels within the same window.
2.5 Perspective classification
Finally, we include a document classification dataset
from Lin and Hauptmann (2006).13 The dataset con-
sists of blog posts posted at bitterlemons.org by Is-
raelis and Palestinians. The bitterlemons.org web-
site is set up to ?contribute to mutual understanding
through the open exchange of ideas.? In the dataset,
each blog post is labeled as either Israeli or Pales-
tinian. Our baseline model is just a standard bag-
of-words model, and the system adds dependency
triplets to the bag-of-words model in a way similar
to Joshi and Penstein-Rose (2009). We do not re-
move stop words, since perspective classification is
12LDC Catalog No.: LDC93T3A.
13https://sites.google.com/site/weihaolinatcmu/data
621
bl yamada conll07 ewt lth
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36? 87.52
PTB-23 (UAS) - 90.21 90.12 84.22? 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57
Table 1: Results. ?: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the
Ontonotes 4.0 release of the English Treebank.
similar to authorship attribution, where stop words
are known to be informative. We evaluate perfor-
mance doing cross-validation over the official train-
ing data, setting the parameters of our learning algo-
rithm for each fold doing cross-validation over the
actual training data. We used soft-margin support
vector machine learning (Cortes and Vapnik, 1995),
tuning the kernel (linear or polynomial with degree
3) and C = {0.1, 1, 5, 10}.
3 Results and discussion
Our results are presented in Table 1. The parsing
results are obtained relying on predicted POS rather
than, as often done in the dependency parsing liter-
ature, relying on gold-standard POS. Note that they
comply with the result in Schwartz et al (2012) that
Yamada-Matsumoto-style annotation is more easily
learnable.
The negation resolution results are significantly
better using syntactic features in yamada annota-
tion. It is not surprising that a syntactically ori-
ented conversion scheme performs well in this task.
Since Lapponi et al (2012) used Maltparser (Nivre
et al, 2007) with the freely available pre-trained
parsing model for English,14 we decided to also
run that parser with the gold-standard cues, in ad-
14http://www.maltparser.org/mco/english parser/engmalt.html
dition to Mate. The pre-trained model was trained
on Sections 2?21 of the Wall Street Journal sec-
tion of the English Treebank (Marcus et al, 1993),
augmented with 4000 sentences from the Question-
Bank,15 which was converted using the Stanford
converter and thus similar to the ewt annotations
used here. The results were better than using ewt
with Mate trained on Sections 2?21 alone, but worse
than the results obtained here with yamada conver-
sion scheme. F1 score on full negation was 66.92%.
The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall perfor-
mance. The difference to the baseline system is
significant (p < 0.01), showing that the reorder-
ing model leads to improvement using any of the
schemes. However, the conversion schemes lead to
very different translations. This can be seen, for
example, by the fact that the relative tree edit dis-
tance between translations of different syntactically
informed SMT systems is 12% higher than within
each system (across different MERT optimizations).
The reordering approach puts a lot of weight on
the syntactic dependency relations. As a conse-
quence, the number of relation types used in the
conversion schemes proves important. Consider the
15http://www.computing.dcu.ie/?jjudge/qtreebank/
622
REFERENCE: Zum Glu?ck kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
yamada: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
conll07: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
ewt: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
lth: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Glu?ck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .
Figure 5: Examples of SMT output.
ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .
BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
conll07 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler .
ewt 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
lth 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
Figure 6: Examples of sentence compression output.
example in Figure 5. German requires the verb in
second position, which is obeyed in the much bet-
ter translations produced by the ewt and lth sys-
tems. Interestingly, the four schemes produce virtu-
ally identical structures for the source sentence, but
they differ in their labeling. Where conll07 and ya-
mada use the same relation for the first two con-
stituents (ADV and vMOD, respectively), ewt and
lth distinguish between them (ADVMOD/PREP and
ADV/LOC). This distinction may be what enables
the better translation, since the model may learn to
move the verb after the sentence adverbial. In the
other schemes, sentence adverbials are not distin-
guished from locational adverbials. Generally, ewt
and lth have more than twice as many relation types
as the other schemes.
The schemes ewt and lth lead to better SRL
performance than conll07 and yamada when re-
lying on gold-standard syntactic dependency trees.
This supports the claims put forward in Johansson
and Nugues (2007). These annotations also hap-
pen to use a larger set of dependency labels, how-
ever, and syntactic structures may be harder to re-
construct, as reflected by labeled attachment scores
(LAS) in syntactic parsing. The biggest drop in
SRL performance going from gold-standard to pre-
dicted syntactic trees is clearly for the lth scheme,
at an average 17.8% absolute loss (yamada 5.8%;
conll07 6.8%; ewt 5.5%; lth 17.8%).
The ewt scheme resembles lth in most respects,
but in preposition-noun dependencies it marks the
preposition as the head rather than the noun. This
is an important difference for SRL, because seman-
tic arguments are often nouns embedded in preposi-
tional phrases, like agents in passive constructions.
It may also be that the difference in performance is
simply explained by the syntactic analysis of prepo-
sitional phrases being easier to reconstruct.
The sentence compression results are generally
much better than the models proposed in Knight and
Marcu (2002). Their noisy channel model obtains
an F1 compression score of 14.58%, whereas the
decision tree-based model obtains an F1 compres-
sion score of 31.71%. While F1 scores should be
complemented by human judgements, as there are
typically many good sentence compressions of any
source sentence, we believe that error reductions of
more than 50% indicate that the models used here
623
ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
L
a
b
e
l
s
srl
neg
Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme
(though previously unexplored in the literature) are
fully competitive with state-of-the-art models.
We also see that the models using syntactic fea-
tures perform better than our baseline model, except
for the model using conll07 dependency annotation.
This may be surprising to some, since distributional
information is often considered important in sen-
tence compression (Knight and Marcu, 2002). Some
output examples are presented in Figure 6. Un-
surprisingly, it is seen that the baseline model pro-
duces grammatically incorrect output, and that most
of our syntactic models correct the error leading to
ungrammaticality. The model using ewt annotation
is an exception. We also see that conll07 introduces
another error. We believe that this is due to the way
the conll07 tree-to-dependency conversion scheme
handles coordination. While the word Sweden is not
coordinated, it occurs in a context, surrounded by
commas, that is very similar to coordinated items.
In perspective classification we see that syntactic
features based on yamada and lth annotations lead
to improvements, with yamada leading to slightly
better results than lth. The fact that a syntactically
oriented conversion scheme leads to the best results
may reflect that perspective classification, like au-
thorship attribution, is less about content than stylis-
tics.
While lth seems to lead to the overall best re-
sults, we stress the fact that the five tasks considered
here are incommensurable. What is more interest-
ing is that, task to task, results are so different. The
semantically oriented conversion schemes, ewt and
lth, lead to the best results in SRL, but with a signif-
icant drop for lth when relying on predicted parses,
while the yamada scheme is competitive in the other
four tasks. This may be because distributional infor-
mation is more important in these tasks than in SRL.
The distribution of dependency labels seems rel-
atively stable across applications, but differences in
data may of course also affect the usefulness of dif-
ferent annotations. Note that conll07 leads to very
good results for negation resolution, but bad results
for SRL. See Figure 7 for the distribution of labels
in the conll07 conversion scheme on the SRL and
negation scope resolution data. Many differences
relate to differences in sentence length. The nega-
tion resolution data is literary text with shorter sen-
tences, which therefore uses more punctuation and
has more root dependencies than newspaper articles.
On the other hand we do see very few predicate de-
pendencies in the SRL data. This may affect down-
stream results when classifying verbal predicates in
SRL. We also note that the number of dependency
labels have less impact on results in general than we
would have expected. The number of dependency
labels and the lack of support for some of them may
explain the drop with predicted syntactic parses in
our SRL results, but generally we obtain our best re-
sults with yamada and lth annotations, which have
12 and 41 dependency labels, respectively.
4 Conclusions
We evaluated four different tree-to-dependency con-
version schemes, putting more or less emphasis on
syntactic or semantic evidence, in five down-stream
applications, including SMT and negation resolu-
tion. Our results show why it is important to be
precise about exactly what tree-to-dependency con-
version scheme is used. Tools like pennconverter.jar
gives us a wide range of options when converting
constituent-based treebanks, and even small differ-
ences may have significant impact on down-stream
performance. The small differences are also impor-
tant for more linguistic comparisons that also tend to
gloss over exactly what conversion scheme is used,
e.g. Ivanova et al (2012).
Acknowledgements
Hector Martinez is funded by the ERC grant
CLARA No. 238405, and Anders S?gaard is
funded by the ERC Starting Grant LOWLANDS
No. 313695.
624
References
Emily Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local dependencies in a large corpus. In EMNLP.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In COLING.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL.
Mike Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive algorithms for multiclass problems. In JMLR.
Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings of International Workshop
on Using Linguistic Information for Hybrid Machine
Translation (LIHMT-2011).
Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom? a con-
trastive study of syntactico-semantic dependencies. In
LAW.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoNLL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling nega-
tion using dependency features. In *SEM.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
In COLING-ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsers. In
EMNLP-CoNLL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
2005, pages 523?530, Vancouver, British Columbia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In COLING.
Yusuke Miyao, Rune S? tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational linguistics, 38(2):223?260.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Roy Schwartz, and Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency pars-
ing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.
625
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In EACL.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of synta. Computational linguis-
tics, 38(2):369?410.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In NAACL-
HLT, Boulder, Colorado.
Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser
evaluation using textual entailments. Language Re-
sources and Evaluation, Published online 31 October
2012.
626
Proceedings of the ACL Student Research Workshop, pages 142?149,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Simple, readable sub-sentences
Sigrid Klerke
Centre for Language Technology
University of Copenhagen
sigridklerke@gmail.com
Anders S?gaard
Centre for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
We present experiments using a new unsu-
pervised approach to automatic text sim-
plification, which builds on sampling and
ranking via a loss function informed by
readability research. The main idea is
that a loss function can distinguish good
simplification candidates among randomly
sampled sub-sentences of the input sen-
tence. Our approach is rated as equally
grammatical and beginner reader appro-
priate as a supervised SMT-based baseline
system by native speakers, but our setup
performs more radical changes that better
resembles the variation observed in human
generated simplifications.
1 Introduction
As a field of research in NLP, text simplification
(TS) has gained increasing attention recently, pri-
marily for English text, but also for Brazilian Por-
tuguese (Specia, 2010; Alu?sio et al, 2008), Dutch
(Daelemans et al, 2004), Spanish (Drndarevic
and Saggion, 2012), Danish (Klerke and S?gaard,
2012), French (Seretan, 2012) and Swedish (Ry-
bing and Smith, 2009; Decker, 2003). Our experi-
ments use Danish text which is similar to English
in that it has a deep orthography making it hard
to map between letters and sounds. Danish has a
relatively free word order and sparse morfology.
TS can help readers with below average reading
skills access information and may supply relevant
training material, which is crucial for developing
reading skills. However, manual TS is as expen-
sive as translation, which is a key limiting factor
on the availability of easy-to-read material. One of
the persistent chalenges of TS is that different in-
terventions are called for depending on the target
reader population. Automatic TS is an effective
way to counter these limitations.
2 Approach
Definitions of TS typically reflect varying target
reader populations and the methods studied. For
our purposes we define TS to include any oper-
ation on the linguistic structure and content of a
text, intended to produce new text, which
1. has semantic content similar to (a part of) the
original text
2. requires less cognitive effort to decode and
understand by a target reader, compared to
the original text.
Operations on linguistic content may include
deletion, reordering and insertion of content,
paraphrasing concepts, resolving references, etc.,
while typography and layout are excluded as non-
linguistic properties.
We cast the problem of generating a more read-
able sentence from an input as a problem of choos-
ing a reasonable sub-sentence from the words
present in the original. The corpus-example below
illustrates how a simplified sentence can be em-
bedded as scattered parts of a non-simplified sen-
tence. The words in bold are the common parts
which make up almost the entire human generated
simplification and constitutes a suitable simplifi-
cation on its own.
Original : Der er m?lt hvad der bliver betegnet som abnormt store
m?ngder af radioaktivt materiale i havvand n?r det jordsk?lvsramte
atomkraftv?rk i Japan .
What has been termed an abnormally large amount of radioactivity
has been measured in sea water near the nuclear power plant that
was hit by earthquakes in Japan
Simplified : Der er m?lt en stor m?ngde radioaktivt materiale i havet
n?r atom-kraftv?rket Fukushima i Japan .
A large amount of radioactivity has been measured in the sea near
the nuclear power plant Fukushima in Japan
To generate candidate sub-sentences we use a
random deletion procedure in combination with
142
general dependency-based heuristics for conserv-
ing main sentence constituents, and then introduce
a loss-function for choosing between candidates.
Since we avoid relying on a specialized parallel
corpus or a simplification grammar, which can be
expensive to create, the method is especially rel-
evant for under-resourced languages and organi-
zations. Although we limit rewriting to deletions,
the space of possible candidates grows exponen-
tially with the length of the input sentence, pro-
hibiting exhaustive candidate generation, which is
why we chose to sample the deletions randomly.
However, to increase the chance of sampling good
candidates, we restrict the search space under
the assumption that some general patterns apply,
namely, that the main verb and subject should al-
ways be kept, negations should be kept and that if
something is kept that originally had objects, those
objects should also be kept. Another way in which
we restrict the candidate space is by splitting long
sentences. Some clauses are simple to identify
and extract, like relative clauses, and doing so can
dramatically reduce sentence length. Both sim-
ple deletions and extraction of clauses can be ob-
served in professionally simplified text. (Medero,
2011; Klerke, 2012)
The next section positions this research in the
context of related work. Section 4 presents the ex-
perimental setup including generation and evalu-
ation. In Section 5, the results are presented and
discussed and, finally, concluding remarks and fu-
ture perspectives are presented in the last section.
3 Related work
Approaches for automatic TS traditionally focus
on lexical substitution (De Belder and Moens,
2012; Specia et al, 2012; Yatskar et al, 2010), on
identifying re-write rules at sentence level either
manually (Chandrasekar et al, 1996; Carroll et al,
1999; Canning et al, 2000; Siddharthan, 2010;
Siddharthan, 2011; Seretan, 2012) or automati-
cally from parallel corpora (Woodsend and Lap-
ata, 2011; Coster and Kauchak, 2011; Zhu et al,
2010) and possibly learning cues for when to ap-
ply such changes (Petersen and Ostendorf, 2007;
Medero, 2011; Bott et al, 2012).
Chandrasekar et al (1996) propose a structural
approach, which uses syntactic cues to recover rel-
ative clauses and appositives. Sentence level syn-
tactic re-writing has since seen a variety of man-
ually constructed general sentence splitting rules,
designed to operate both on dependencies and
phrase structure trees, and typically including lex-
ical cues (Siddharthan, 2011; Heilman and Smith,
2010; Canning et al, 2000). Similar rules have
been created from direct inspection of simplifica-
tion corpora (Decker, 2003; Seretan, 2012) and
discovered automatically from large scale aligned
corpora (Woodsend and Lapata, 2011; Zhu et al,
2010).
In our experiment we apply few basic sentence
splitting rules as a pre-processing technique be-
fore using an over-generating random deletion ap-
proach.
Carroll et al (1999) perform lexical substitution
from frequency counts and eliminate anaphora by
resolving and replacing the referring expressions
with the entity referred to. Their system further
include compound sentence splitting and rewrit-
ing of passive sentences to active ones (Canning
et al, 2000). Research into lexical simplification
remains an active topic. De Belder and Moens
(2012; Specia et al (2012) are both recent pub-
lications of new resources for evaluating lexical
simplification in English consisting of lists of syn-
onyms ranked by human judges. Another type
of resource is graded word-lists as described in
Brooke et al (2012). Annotator agreement and
comparisons so far shows that it is easy to over-
fit to reflect individual annotator and domain dif-
ferences that are not of relevance to generalized
systems.
In a minimally supervised setup, our TS ap-
proach can be modified to include lexical simpli-
fications as part of the random generation process.
This would require a broad coverage list of words
and simpler synonyms, which could for instance
be extracted from a parallel corpus like the DSim
corpus.
For the majority of research in automatic TS
the question of what constitutes cognitive load is
not discussed. An exception is Siddharthan and
Katsos (2012), who seek to isolate the psycho-
linguistically motivated notions of sentence com-
prehension from sentence acceptability by actually
measuring the effect of TS on cognition on a small
scale.
Readability research is a line of research that is
more directly concerned with the nature of cogni-
tive load in reading building on insights from psy-
cholinguistics. One goal is to develop techniques
and metrics for assessing the readability of unseen
143
text. Such metrics are used as a tool for teachers
and publishers, but existing standard metrics (like
Flesch-Kincaid (Flesch, 1948) and LIX (Bjorns-
son, 1983)) were designed and optimized for easy
manual application to human written text, requir-
ing thehuman reader to assess that the text is
congruent and coherent. More recent methods
promise to be applicable to unassessed text. Lan-
guage modeling in particular has shown to be a
robust and informative component of systems for
assessing text readability (Schwarm and Osten-
dorf, 2005; Vajjala and Meurers, 2012) as it is bet-
ter suited to evaluate grammaticality than standard
metrics. We use language modeling alongside tra-
ditional metrics for selecting good simplification
candidates.
4 Experiments
4.1 Baseline Systems
We used the original input text and the human sim-
plified text from the sentence aligned DSim corpus
which consist of 48k original and manually sim-
plified sentences of Danish news wire text (Klerke
and S?gaard, 2012) as reference in the evaluations.
In addition we trained a statistical machine trans-
lation (SMT) simplification system, in effect trans-
lating from normal news wire text to simplified
news. To train an SMT system, a large resource
of aligned parallel text and a language model of
the target language are needed. We combined the
25 million words Danish Korpus 20001 with the
entire 1.75 million words unaligned DSim cor-
pus (Klerke and S?gaard, 2012) to build the lan-
guage model2. Including both corpora gives bet-
ter coverage and assigns lower average ppl and a
simlar difference in average ppl between the two
sides of a held out part of the DSim corpus com-
pared to using only the simplified part of DSim
for the language model. Following Coster and
Kauchak (2011), we used the phrase-based SMT
Moses (Koehn et al, 2007), with GIZA++ word-
alignment (Och and Ney, 2000) and phrase tables
learned from the sentence aligned portion of the
DSim corpus.
1http://korpus.dsl.dk/korpus2000/
engelsk_hovedside
2The LM was a 5-gram Knesser-Ney smoothed lowercase
model, built using IRSTLM (Federico et al, 2008)
4.2 Experimental setup
Three system variants were set up to generate
simplified output from the original news wire of
the development and test partitions of the DSim
corpus. The texts were dependency-parsed us-
ing Bohnet?s parser (Bohnet, 2010) trained on the
Danish Treebank3 (Kromann, 2003) with default
settings4.
1. Split only performed simple sentence split-
ting.
2. Sample over-generated candidates by sam-
pling the heuristically restricted space of ran-
dom lexical deletions and ranking candidates
with a loss function.
3. Combined is a combination of the two, ap-
plying the sampling procedure of Sample to
the split sentences from Split.
Sentence Splitting We implemented sentence
splitting to extract relative clauses, as marked by
the dependency relation rel, coordinated clauses,
coord, and conjuncts, conj, when at least a verb
and a noun is left in each part of the split. Only
splits resulting in sentences of more than three
words were considered. Where applicable, re-
ferred entities were included in the extracted sen-
tence by using the dependency analysis to extract
the subtree of the former head of the new sen-
tence5. In case of more than one possibility, the
split resulting in the most balanced division of the
sentence was chosen and the rules were re-applied
if a new sentence was still longer than ten tokens.
Structural Heuristics To preserve nodes from
later deletion we applied heuristics using simple
structural cues from the dependency structures.
We favored nodes headed by a subject relation,
subj, and object relations, *obj, and negating
modifiers (the Danish word ikke) under the as-
sumption that these were most likely to be impor-
tant for preserving semantics and generating well-
formed candidates under the sampling procedure
described below. The heuristics were applied both
to trees, acting by preserving entire subtrees and
applied to words, only preserving single tokens.
3http://ilk.uvt.nl/conll/post_task_
data.html
4Performance of the parser on the treebank test set La-
beled attatchment score (LAS) = 85.65 and Unlabeled at-
tatchment score (UAS) = 90.29
5For a formal description see (Klerke, 2012)
144
This serves as a way of avoiding relying heavily
on possibly faulty dependency analyses and also
avoid the risk of insisting on keeping long, com-
plex or superfluous modifiers.
Sampling Candidates for scoring were over-
generated by randomly selecting parts of a (pos-
sibly split) input sentence. Either the selected
nodes with their full sub-tree or the single tokens
from the flat list of tokens were eliminated, unless
they were previously selected for preservation by
a heuristic. Some additional interaction between
heuristics and sampling happened when the dele-
tions were performed on trees: deletion of subtrees
allow non-continuous deletions when the parses
are non-projective, and nodes that were otherwise
selected for keeping may nevertheless be removed
if they are part of a subtree of a node selected for
deletion. After pruning, all nodes that used to have
outgoing obj-relations had the first child node of
these relations restored.
4.3 Scoring
We rank candidates according to a loss function
incorporating both readability score (the lower,
the more readable) and language model perplexity
(the lower, the less perplexing) as described below.
The loss function assigns values to the candidates
such that the best simplification candidate receives
the lowest score.
The loss function is a weighted combination of
three scores: perplexity (PPL), LIX and word-
class distribution (WCD). The PPL scores were
obtained from a 5-gram language model of Dan-
ish6 We used the standard readability metric for
Danish, LIX (Bjornsson, 1983)7. Finally, the
WCD measured the variation in universal pos-
tag-distribution 8 compared to the observed tag-
variation in the entire simplified corpus. For PPL
and LIX we calculated the difference between the
score of the input sentence and the candidate.
Development data was used for tuning the
weights of the loss function. Because the
candidate-generation is free to produce extremely
short candidates, we have to deal with candidates
6The LM was Knesser-Ney smoothed, using the same cor-
pora as the baseline system, without punctuation and built us-
ing SRILM (Stolcke, 2002).
7LIX is similar to the English Flesch-Kincaid grade level
in favoring short sentences with short words. The formula
is LIX = average sentence length + % long words , with
long words being of more than 6 characters. (Anderson,
1983) calculated a conversion from LIX to grade levels.
8suggested by(Petrov et al, 2011)
receiving extremely low scores. Those scores
never arise in the professionally simplified text,
so we eliminate extreme candidates by introduc-
ing filters on all scores. The lower limit was tuned
experimentally and fixed approximately two times
below the average difference observed between
the two parts of the aligned DSim corpus, thus lim-
iting the reduction in PPL and LIX to 60% of the
input?s PPL and LIX. The upper limit was fixed
at the input-level plus 20% to allow more varied
candidates through the filters. The WCD-filter ac-
cepted all candidates with a tag-variance that fell
below the 75-percentile observed variance in the
simplified training part of the DSim corpus. The
resulting loss was calculated as the sum of three
weighted scores.
Below is the loss function we minimized over
the filtered candidates t ? Ts for each input sen-
tence, s. The notation var() denotes the range al-
lowed through a hard filter. Using development
data we set the values of the term weights to
? = 1, ? = 6 and ? = 2.
t? = argmin
t?Ts
loss(s, t)
loss(s, t) = ? ?LIX(s, t)var(LIX(s)) + ?
?PPL(s, t)
var(PPL(s))
+ ??WCD(.75, t)WCD(.75)
If no candidates passed through the filters, the
input sentence was kept.
4.4 Evaluation
Evaluation was performed by a group of proficient
Danish speaking volunteers who received written
instructions and responded anonymously via an
online form. 240 sentences were evaluated: six
versions of each of 40 test set sentences. 48
sentences were evaluated by four judges, and
the remaining by one judge each. The judges
were asked to rate each sentence in terms of
grammaticality and in terms of perceived beginner
reader appropriateness, both on a 5-point scale,
with one signifying very good and five signifying
very bad. The evaluators had to rate six versions
of each sentence: original news wire, a human
simplified version, the baseline system, a split
sentence version (Split), a sampled only version
(Sample), and a version combining the Split and
Sample techniques (Combined). The presentation
was randomized. Below are example outputs
145
for the baseline and the other three automatic
systems:
BL: Der er hvad der bliver betegnet som abnormt store m?ngder
radioaktivt materiale i havvand n?r frygter atomkraftv?rk .
Split : Der er m?lt hvad. Hvad bliver betegnet som abnormt
store m?ngder af radioaktivt materiale i havvand n?r det
jordsk?lvsramte atomkraftv?rk i Japan .
Sample: Der er m?lt hvad der bliver betegnet som store m?ngder
af radioaktivt materiale i havvand japan .
Comb.: Der er m?lt hvad. Hvad bliver betegnet som store m?ngder
af radioaktivt materiale det atomkraftv?rk i japan .
5 Results
The ranking of the systems in terms of begin-
ner reader appropriateness and grammaticality, are
shown in Figure 1. From the test set of the DSim
corpus, 15 news wire texts were arbitrarily se-
lected for evaluation. For these texts we calcu-
lated median LIX and PPL. The results are shown
in Table 1. The sentences for human evaluation
were drawn arbitrarily from this collection. As
expected, the filtering of candidates and the loss
function force the systems Sample and Combined
to choose simplifications with LIX and PPL scores
close to the ones observed in the human simpli-
fied version. Split sentences only reduce LIX as
a result of shorter sentences, however PPL is the
highest, indicating a loss of grammaticality. Most
often this was caused by tagger and parser errors.
The baseline reduces PPL slightly, while LIX is
unchanged. This reflects the importance of the
language model in the SMT system.
In the analyses below, the rating were collapsed
to three levels. For texts ranked by more than
one judge, we calculated agreement as Krippen-
dorff?s ?. The results are shown in Table 2. In
addition to sentence-wise agreement, the system-
wise evaluation agreement was calculated as all
judges were evaluating the same 6 systems 8 times
each. We calculated ? of the most frequent score
(mode) assigned by each judge to each system.
As shown in Table 2 this system score agreement
was only about half of the single sentence agree-
ment, which reflect a notable instability in output
quality of all computer generated systems. The
same tendency is visible in both histograms in Fig-
ure 1a and 1b. While grammaticality is mostly
agreed upon when the scores are collapsed into
three bins (? = 0.650), proficient speakers do not
agree to the same extent on what constitutes be-
ginner reader appropriate text (? = 0.338). The
average, mean and most frequent assigned ranks
are recorded in Table 3. Significant differences at
p < 0.05 are reported in Table 4.
1 very good
2 3 4 5 very poor
0
10
20
30
40
50
60
70
Beginning reader appropriatenessvotes for each system
OriginalSimplifiedBaselineSplitSampleCombined
(a) Sentence ? Beginner
1 very good
2 3 4 5 very poor
0
10
20
30
40
50
60
70
Grammaticalityvotes for each system
OriginalSimplifiedBaselineSplitSampleCombined
(b) Sentence ? Grammar.
Figure 1: Distribution of all rankings on systems
before collapsing rankings.
Orig. Simpl. Base Split Sample Comb.
PPL 222 174 214 234 164 177
LIX 45 (10) 39 (8) 45 (10) 41(9) 36 (8) 32 (7)
Table 1: LIX and PPL scores for reference texts
and system generated output. Medians are re-
ported, because distributions are very skewed,
which makes the mean a bad estimator of central
tendency. LIX grade levels in parenthesis.
Reflecting the fair agreement on grammatical-
ity, all comparisons come out significant except
the human generated versions that are judged as
equally grammatical and the Combined and Base-
line systems that are indistinguishable in gram-
maticality. Beginner reader appropriateness is sig-
nificantly better in the human simplified version
146
Systems Sentences
Beginner reader 0.168 0.338
Grammaticality 0.354 0.650
Table 2: Krippendorff?s ? agreement for full-text
and sentence evaluation. Agreement on system
ranks was calculated from the most frequent score
per judge per system.
compared to all other versions, and the original
version is significantly better than the Sample and
Split systems. The remaining observed differences
are not significant due to the great variation in
quality as expressed in Figure 1a.
We found that our Combined system produced
sentences that were as grammatical as the base-
line and also frequently judged to be appropriate
for beginner readers. The main source of error
affecting both Combined and Split is faulty sen-
tence splitting as a result of errors in tagging and
parsing. One way to avoid this in future develop-
ment is to propagate several split variants to the
final sampling and scoring. In addition, the sys-
tems Combined and Sample are prone to omitting
important information that is perceived as missing
when compared directly to the original, although
those two systems are the ones that score the clos-
est to the human generated simplifications. As can
be expected in a system operating exclusively at
sentence level, coherence across sentence bound-
aries remains a weak point.
Another important point is that while the base-
line system performs well in the evaluation, this
is likely due to its conservativeness: choosing
simplifications resembling the original input very
closely. This is evident both in our automatic mea-
sures (see Table 1) and from manual inspection.
Our systems Sample and Combine, on the other
hand, have been tuned to perform much more radi-
cal changes and in this respect more closely model
the changes we see in the human simplification.
Combined is thus evaluated to be at level with
the baseline in grammaticality and beginner reader
appropriateness, despite the fact that the baseline
system is supervised.
Conclusion and perspectives
We have shown promising results for simplifica-
tion of Danish sentences. We have also shown
that using restricted over-generation and scoring
can be a feasible way for simplifying text with-
out relying directly on large scale parallel corpora,
Sent. ? Beginner Sent. ? Grammar
x? x? mode x? x? mode
Human Simp. 1.44 1 1 1.29 1 1
Orig. 2.14 1 1 1.32 1 1
Base 2.58 3 1 1.88 2 1
Split 3.31 3 5 2.44 3 3
Sample 3.22 3 5 2.39 3 3
Comb. 2.72 1 1 1.93 2 1
Table 3: Human evaluation. Mean (x?), median (x?)
and most frequent (mode) of assigned ranks by be-
ginner reader appropriateness and grammaticality
as assessed by proficient Danish speakers.
Comb. Sample Split Base Orig.
Human Simp. b, g b, g b, g b, g b
Orig. g b, g b, g g
Base g g
Split g
Sample g
Table 4: Significant differences between systems
in experiment b: Beginner reader appropriate-
ness and g: Grammaticality. Bonferroni-corrected
Mann-Whitney?s U for 15 comparisons, two-tailed
test. A letter indicate significant difference at cor-
rected p < 0.05 level.
which for many languages do not exist. To inte-
grate language modeling and readability metrics in
scoring is a first step towards applying results from
readability research to the simplification frame-
work. Our error analysis showed that many errors
come from pre-processing and thus more robust
NLP-tools for Danish are needed. Future perspec-
tives include combining supervised and unsuper-
vised methods to exploit the radical unsupervised
deletion approach and the knowledge obtainable
from observable structural changes and potential
lexical simplifications. We plan to focus on refin-
ing the reliability of sentence splitting in the pres-
ence of parser errors as well as on developing a
loss function that incorporates more of the insights
from readability research, and to apply machine
learning techniques to the weighting of features.
Specifically we would like to investigate the use-
fulness of discourse features and transition proba-
bilities (Pitler and Nenkova, 2008) for performing
and evaluating full-text simplifications.
Acknowledgements
Thanks to Mirella Lapata and Kristian Woodsend
for their feedback and comments early in the pro-
cess of this work and to the Emnlp@Cph group
and reviewers for their helpful comments.
147
References
S.M. Alu?sio, Lucia Specia, T.A.S. Pardo, E.G.
Maziero, H.M. Caseli, and R.P.M. Fortes. 2008. A
corpus analysis of simple account texts and the pro-
posal of simplification strategies: first steps towards
text simplification systems. In Proceedings of the
26th annual ACM international conference on De-
sign of communication, pages 15?22. ACM.
Jonathan Anderson. 1983. LIX and RIX: Variations on
a little-known readability index. Journal of Reading,
26(6):490?496.
C. H. Bjornsson. 1983. Readability of Newspapers
in 11 Languages. Reading Research Quarterly,
18(4):480?497.
B Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97. Association for
Computational Linguistics.
S. Bott, H. Saggion, and D. Figueroa. 2012. A hy-
brid system for spanish text simplification. In Third
Workshop on Speech and Language Processing for
Assistive Technologies (SLPAT), Montreal, Canada.
Julian Brooke, Vivian Tsang, David Jacob, Fraser
Shein, and Graeme Hirst. 2012. Building Read-
ability Lexicons with Unannotated Corpora. In Pro-
ceedings of the First Workshop on Predicting and
Improving Text Readability for target reader popula-
tions, pages 33?39, Montr{?}al, Canada, June. As-
sociation for Computational Linguistics.
Y. Canning, J. Tait, J. Archibald, and R. Crawley.
2000. Cohesive generation of syntactically simpli-
fied newspaper text. Springer.
John Carroll, G. Minnen, D. Pearce, Yvonne Canning,
S. Devlin, and J. Tait. 1999. Simplifying text
for language-impaired readers. In Proceedings of
EACL, volume 99, pages 269?270. Citeseer.
R. Chandrasekar, Christine Doran, and B Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 1041?1044.
Association for Computational Linguistics.
William Coster and David Kauchak. 2011. Simple En-
glish Wikipedia: a new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, vol-
ume 2, pages 665?669. Association for Computa-
tional Linguistics.
W. Daelemans, A. H?thker, and E.T.K. Sang. 2004.
Automatic sentence simplification for subtitling in
dutch and english. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 1045?1048.
A. Davison and R.N. Kantor. 1982. On the failure of
readability formulas to define readable texts: A case
study from adaptations. Reading Research Quar-
terly, pages 187?209.
J. De Belder and M.F. Moens. 2012. A dataset for the
evaluation of lexical simplification. Computational
Linguistics and Intelligent Text Processing, pages
426?437.
Anna Decker. 2003. Towards automatic grammati-
cal simplification of Swedish text. Master?s thesis,
Stockholm University.
Biljana Drndarevic and Horacio Saggion. 2012. To-
wards Automatic Lexical Simplification in Spanish:
An Empirical Study. In Proceedings of the First
Workshop on Predicting and Improving Text Read-
ability for target reader populations, pages 8?16,
Montr{?}al, Canada, June. Association for Compu-
tational Linguistics.
M Federico, N Bertoldi, and M Cettolo. 2008.
IRSTLM: an open source toolkit for handling large
scale language models. In Ninth Annual Conference
of the International Speech Communication Associ-
ation.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Michael Heilman and Noah A Smith. 2010. Extract-
ing simplified statements for factual question gen-
eration. In Proceedings of the Third Workshop on
Question Generation.
Sigrid Klerke and Anders S?gaard. 2012. DSim , a
Danish Parallel Corpus for Text Simplification. In
Proceedings of Language Resources and Evaluation
(LREC 2012), pages 4015?4018.
Sigrid Klerke. 2012. Automatic text simplification in
danish. sampling a restricted space of rewrites to op-
timize readability using lexical substitutions and de-
pendency analyses. Master?s thesis, University of
Copenhagen.
P Koehn, H Hoang, A Birch, C Callison-Burch, M Fed-
erico, N Bertoldi, B Cowan, W Shen, C Moran,
R Zens, and Others. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
M T Kromann. 2003. The Danish Dependency Tree-
bank and the DTAG treebank tool. In Proceedings
of the Second Workshop on Treebanks and Linguistic
Theories (TLT), page 217.
Julie Medero. 2011. Identifying Targets for Syntactic
Simplification. In Proceedings of Speech and Lan-
guage Technology in Education.
148
F.J. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 1086?1090. Association
for Computational Linguistics.
S.E. E Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analy-
sis. In the Proceedings of the Speech and Language
Technology for Education Workshop, pages 69?72.
Citeseer.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. Arxiv preprint
ArXiv:1104.2086.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Jonas Rybing and Christian Smith. 2009. CogFLUX
Grunden till ett automatiskt textf?renklingssystem
f?r svenska. Master?s thesis, Link?pings Univer-
sitet.
Sarah E Schwarm and Mari Ostendorf. 2005. Reading
Level Assessment Using Support Vector Machines
and Statistical Language Models. In Proceedings
of the 43rd Annual Meeting of the ACL, pages 523?
530.
V. Seretan. 2012. Acquisition of syntactic simplifica-
tion rules for french. In Proceedings of Language
Resources and Evaluation (LREC 2012).
Advaith Siddharthan and Napoleon Katsos. 2012.
Offline Sentence Processing Measures for testing
Readability with Users. In Proceedings of the First
Workshop on Predicting and Improving Text Read-
ability for target reader populations, pages 17?24,
Montr{?}al, Canada, June. Association for Compu-
tational Linguistics.
Advaith Siddharthan. 2010. Complex lexico-syntactic
reformulation of sentences using typed dependency
representations. Proceedings of the 6th Interna-
tional Natural Language Generation Conference.
Advaith Siddharthan. 2011. Text Simplification us-
ing Typed Dependencies: A Comparison of the Ro-
bustness of Different Generation Strategies. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, pages 2?11.
L. Specia, S.K. Jauhar, and R. Mihalcea. 2012.
Semeval-2012 task 1: English lexical simplification.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), pages 347?
355.
L. Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language, pages 30?39.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing.
S. Vajjala and D. Meurers. 2012. On improving the
accuracy of readability classification using insights
from second language acquisition. In Proceedings
of the 7th Workshop on Innovative Use of NLP for
Building Educational Applications (BEA7), pages
163?173.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to Simplify Sentences with Quasi-Synchronous
Grammar and Integer Programming. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (2011), pages 409?
420.
Mark Yatskar, Bo Pang, C. Danescu-Niculescu-Mizil,
and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifica-
tions from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368. Association for
Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and I. Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics, pages 1353?1361. Association for Com-
putational Linguistics.
149
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 408?412,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
EMNLP@CPH: Is frequency all there is to simplicity?
Anders Johannsen, H?ctor Mart?nez, Sigrid Klerke?, Anders S?gaard
Centre for Language Technology
University of Copenhagen
{ajohannsen|alonso|soegaard}@hum.ku.dk
sigridklerke@gmail.com?
Abstract
Our system breaks down the problem of rank-
ing a list of lexical substitutions according to
how simple they are in a given context into a
series of pairwise comparisons between can-
didates. For this we learn a binary classifier.
As only very little training data is provided,
we describe a procedure for generating artifi-
cial unlabeled data from Wordnet and a corpus
and approach the classification task as a semi-
supervised machine learning problem. We use
a co-training procedure that lets each classi-
fier increase the other classifier?s training set
with selected instances from an unlabeled data
set. Our features include n-gram probabilities
of candidate and context in a web corpus, dis-
tributional differences of candidate in a cor-
pus of ?easy? sentences and a corpus of normal
sentences, syntactic complexity of documents
that are similar to the given context, candidate
length, and letter-wise recognizability of can-
didate as measured by a trigram character lan-
guage model.
1 Introduction
This paper describes a system for the SemEval 2012
English Lexical Simplification shared task. The
task description uses a loose definition of simplic-
ity, defining ?simple words? as ?words that can be
understood by a wide variety of people, including for
example people with low literacy levels or some cog-
nitive disability, children, and non-native speakers of
English? (Specia et al, 2012).
Feature r
N????sf 0.33
N????sf+1 0.27
N????sf?1 0.27
L??sf -0.26
L??max -0.26
RIproto(l) -0.18
S??cn -0.17
S??w -0.17
S??cp -0.17
Feature r
RIproto(f) -0.15
C???max -0.14
RIorig(l) -0.11
L??tokens -0.10
C???min 0.10
SWfreq 0.08
SWLLR 0.07
C???avg -0.04
Table 1: Pearson?s r correlations. The table shows
the three highest correlated features per group, all of
which are significant at the p < 0.01 level
2 Features
We model simplicity with a range of features divided
into six groups. Five of these groups make use of
the distributional hypothesis and rely on external cor-
pora. We measure a candidate?s distribution in terms
of its lexical associations (RI), participation in syn-
tactic structures (S??), or corpus presence in order to
assess its simplicity (N????, SW, C???). A single
group, L??, measures intrinsic aspects of the substi-
tution candidate, such as its length.
The substitution candidate is either an adjective,
an adverb, a noun, or a verb, and all candidates within
a list share the same part of speech. Because word
class might influence simplicity, we allow our model
to fit parameters specific to the candidate?s part of
speech by making a copy of the features for each part
of speech which is active only when the candidate is
in the given part of speech.
408
Simple Wikipedia (SW) These two features con-
tain relative frequency counts of the substitution
form in Simple English Wikipedia (SWfreq), and the
log likelihood ratio of finding the word in the simple
corpus to finding it in regular Wikipedia (SWLLR)1.
Word length (L??) This set of three features de-
scribes the length of the substitution form in char-
acters (L??sf ), the length of the longest token
(L??max), and the length of the substitution form in
tokens (L??tokens). Word length is an integral part
of common measures of text complexity, e.g in the
English Flesch?Kincaid (Kincaid et al, 1975) in the
form of syllable count, and in the Scandinavian LIX
(Bjornsson, 1983).
Character trigram model (C???) These three
features approximate the reading difficulty of a word
in terms of the probabilities of its forming character
trigrams, with special characters to mark word be-
ginning and end. A word with an unusual combi-
nation of characters takes longer to read and is per-
ceived as less simple (Ehri, 2005).
We calculate the minimum, average, and maxi-
mum trigram probability (C???min, C???avg, and
C???max).2
Web corpus N-gram (N????) These 12 features
were obtained from a pre-built web-scale language
model3. Features of the form N????sf?i, where
0 < i < 4, express the probability of seeing the
substitution form together with the following (or pre-
vious) unigram, bigram, or trigram. N????sf is
the probability of substitution form itself, a feature
which also is the backbone of our frequency base-
line.
Random Indexing (RI) These four features are
obtained from measures taken from a word-to-word
distributional semantic model. Random Indexing
(RI) was chosen for efficiency reasons (Sahlgren,
2005). We include features describing the seman-
tic distances between the candidate and the original
1Wikipedia dump obtained March 27, 2012. Date on the
Simple Wikipedia dump is March 22, 2012.
2Trigram probabilities derived from Google T1 unigram
counts.
3The ?jun09/body? trigram model from Microsoft Web N-
gram Services.
form (RIorig), and between the candidate and a proto-
type vector (RIproto). For the distance between can-
didate and original, we hypothesize that annotators
would prefer a synonym closer to the original form.
A prototype distributional vector of a set of words is
built by summing the individual word vectors, thus
obtaining a representation that approximates the be-
havior of that class overall (Turney and Pantel, 2010).
Longer distances indicate that the currently exam-
ined substitution is far from the shared meaning of
all the synonyms, making it a less likely candidate.
The features are included for both lemma and surface
forms of the words.
Syntactic complexity (S??) These 23 features
measure the syntactic complexity of documents
where the substitution candidate occurs. We used
measures from (Lu, 2010) in which they describe 14
automatic measures of syntactic complexity calcu-
lated from frequency counts of 9 types of syntactic
structures. This group of syntax-metric scores builds
on two ideas.
First, syntactic complexity and word difficulty go
together. A sentence with a complicated syntax is
more likely to be made up of difficult words, and
conversely, the probability that a word in a sentence
is simple goes up when we know that the syntax of
the sentence is uncomplicated. To model this we
search for instances of the substitution candidates in
the UKWAC corpus4 and measure the syntactic com-
plexity of the documents where they occur.
Second, the perceived simplicity of a word may
change depending on the context. Consider the ad-
jective ?frigid?, which may be judged to be sim-
pler than ?gelid? if referring to temperature, but per-
haps less simple than ?ice-cold? when characterizing
someone?s personality. These differences in word
sense are taken into account by measuring the sim-
ilarity between corpus documents and substitution
contexts and use these values to provide a weighted
average of the syntactic complexity measures.
3 Unlabeled data
The unlabeled data set was generated by a three-
step procedure involving synonyms extracted from
Wordnet5 and sentences from the UKWAC corpus.
4http://wacky.sslmit.unibo.it/
5http://wordnet.princeton.edu/
409
1) Collection: Find synsets for unambigious lem-
mas in Wordnet. The synsets must have more than
three synonyms. Search for the lemmas in the cor-
pus. Generate unlabeled instances by replacing the
lemma with each of its synonyms. 2) Sampling: In
the unlabeled corpus, reduce the number of ranking
problems per lemma to a maximum of 10. Sample
from this pool while maintaining a distribution of
part of speech similar to that of the trial and test set.
3) Filtering: Remove instances for which there are
missing values in our features.
The unlabeled part of our final data set contains
n = 1783 problems.
4 Ranking
We are given a number of ranking problems (n =
300 in the trial set and n = 1710 for the test data).
Each of these consists of a text extract with a posi-
tion marked for substitution, and a set of candidate
substitutions.
4.1 Linear order
Let X (i) be the substitution set for the i-th problem.
We can then formalize the ranking problem by as-
suming that we have access to a set of (weighted)
preference judgments, w(a ? b) for all a, b ? X (i)
such that w(a ? b) is the value of ranking item a
ahead of b. The values are the confidence-weighted
pair-wise decisions from our binary classifier. Our
goal is then to establish a total order on X (i) that
maximizes the value of the non-violated judgments.
This is an instance of the Linear Ordering Problem
(Mart? and Reinelt, 2011), which is known to be NP-
hard. However, with problems of our size (maximum
ten items in each ranking), we escape these complex-
ity issues by a very narrow margin?10! ? 3.6 mil-
lion means that the number of possible orderings is
small enough to make it feasible to find the optimal
one by exhaustive enumeration of all possibilities.
4.2 Binary classication
In order to turn our ranking problem into binary clas-
sification, we generate a new data set by enumerat-
ing all point-wise comparisons within a problem and
for each apply a transformation function ?(a,b) =
a ? b. Thus each data point in the new set is the
difference between the feature values of two candi-
dates. This enables us to learn a binary classifier for
the relation ?ranks ahead of?.
We use the trial set for labeled training data L and,
in a transductive manner, treat the test set as unla-
beled data Utest. Further, we supplement the pool of
unlabeled data with artificially generated instances
Ugen, such that U = Utest ? Ugen.
Using a co-training setup (Blum and Mitchell,
1998), we divide our features in two independent sets
and train a large margin classifier6 on each split. The
classifiers then provide labels for data in the unla-
beled set, adding the k most confidently labeled in-
stances to the training data for the other classifier, an
iterative process which continues until there is no un-
labeled data left. At the end of the training we have
two classifiers. The classification result is a mixture-
of-experts: the most confident prediction of the two
classifiers. Furthermore, as an upper-bound of the
co-training procedure, we define an oracle that re-
turns the correct answer whenever it is given by at
least one classifier.
4.3 Ties
In many cases we have items a and b that tie?in
which case both a ? b and b ? a are violated. We
deal with these instances by omitting them from the
training set and setting w(a ? b) = 0. For the fi-
nal ranking, our system makes no attempt to produce
ties.
5 Experiments
In our experiments we vary feature-split, size of un-
labeled data, and number of iterations. The first fea-
ture split, S???SW, pooled all syntactic complexity
features and Wikipedia-based features in one view,
with the remaining feature groups in another view.
Our second feature split, S???C????L??, combined
the syntactic complexity features with the character
trigram language model features and the basic word
length features. Both splits produced a pair of classi-
fiers with similar performance?each had an F-score
of around .73 and an oracle score of .87 on the trial
set on the binary decision problem, and both splits
performed equally on the ranking task.
6Liblinear with L1 penalty and L2 loss. Parameter settings
were default. http://www.csie.ntu.edu.tw/?cjlin/liblinear/
410
System All N V R A
M????????F??? 0.449 0.367 0.456 0.487 0.493
S???SWf 0.377 0.283 0.269 0.271 0.421
S???SWl 0.425 0.355 0.497 0.408 0.425
S???C????L??f 0.377 0.284 0.469 0.270 0.421
S???C????L??l 0.435 0.362 0.481 0.465 0.439
Table 2: Performance on part of speech. Unlabeled
set was Utest. Subscripts tell whether the scores are
from the first or last iteration
With a large unlabeled data set available, the clas-
sifiers can avoid picking and labeling data points
with a low certainty, at least initially. The assump-
tion is that this will give us a higher quality training
set. However, as can be seen in Figure 1, none of our
systems are benefitting from the additional data. In
fact, the systems learn more when the pool of unla-
beled data is restricted to the test set.
Our submitted systems, O??1 and O??2 scored
0.405 and 0.393 on the test set, and 0.494 and 0.500
on the trial set. Following submission we adjusted
a parameter7 and re-ran each split with both U and
Utest.
We analyzed the performance by part of speech
and compared them to the frequency baseline as
shown in Table 2. For the frequency baseline, per-
formance is better on adverbs and adjectives alone,
and somewhat worse on nouns. Both our sys-
tems benefit from co-training on all word classes.
S???C????L??, our best performing system, no-
tably has a score reduction (compared to the base-
line) of only 5% on adverbs, eliminates the score re-
duction on nouns, and effectively beats the baseline
score on verbs with a 6% increase.
6 Discussion
The frequency baseline has proven very strong, and,
as witnessed by the correlations in Table 1, frequency
is by far the most powerful signal for ?simplicity?.
But is that all there is to simplicity? Perhaps it is.
For a person with normal reading ability, a sim-
ple word may be just a word with which the per-
son is well-acquainted?one that he has seen be-
fore enough times to have a good idea about what
it means and in which contexts it is typically used.
7In particular, we selected a larger value for the C parameter
in the liblinear classifier.
0 5000 10000 15000 20000 25000
Unlabeled datapoints
0.38
0.40
0.42
0.44
0.46
0.48
Sc
ore
SYN-SW(Utest)
SYN-CHAR-LEN(Utest)
SYN-CHAR-LEN(U)
Figure 1: Test set kappa score vs. number of data
points labeled during co-training
And so an n-gram model might be a fair approxi-
mation. However, lexical simplicity in English may
still be something very different to readers with low
literacy. For instance, the highly complex letter-to-
sound mapping rules are likely to prevent such read-
ers from arriving at the correct pronunciation of un-
seen words and thus frequent words with exceptional
spelling patterns may not seem simple at all.
A source of misclassifications discovered in our
error analysis is the fact that substituting candidates
into the given contexts in a straight-forward manner
can introduce syntactic errors. Fixing these can re-
quire significant revisions of the sentence, and yet
the substitutions resulting in an ungrammatical sen-
tence are sometimes still preferred to grammatical al-
ternatives.8 Here, scoring the substitution and the
immediate context in a language model is of little
use. Moreover, while these odd grammatical errors
may be preferable to many non-native English speak-
ers with adequate reading skills, such errors can be
more obstructing to reading impaired users and be-
ginning language learners.
Acknowledgments
This research is partially funded by the European Commission?s
7th Framework Program under grant agreement n? 238405
(CLARA).
8For example sentence 1528: ?However, it appears they in-
tend to pull out all stops to get what they want.? Gold: {try ev-
erything} {do everything it takes} {pull} {stop at nothing} {go
to any length} {yank}.
411
References
C. H. Bjornsson. 1983. Readability of Newspa-
pers in 11 Languages. Reading Research Quarterly,
18(4):480?497.
A Blum and T Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning
theory, pages 92?100. ACM.
Linnea C. Ehri. 2005. Learning to read words: The-
ory, findings, and issues. Scientific Studies of Reading,
9(2):167?188.
J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom.
1975. Derivation of New Readability Formulas (Auto-
mated Readability Index, Fog Count and Flesch Read-
ing Ease Formula) for Navy Enlisted Personnel.
Xiaofei Lu. 2010. Automatic analysis of syntactic com-
plexity in second language writing. International Jour-
nal of Corpus Linguistics, 15(4):474?496.
Rafael Mart? and Gerhard Reinelt. 2011. The Lin-
ear Ordering Problem: Exact and Heuristic Methods
in Combinatorial Optimization (Applied Mathematical
Sciences). Springer.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
P. D Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
412
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213?217,
Dublin, Ireland, August 23-24, 2014.
Copenhagen-Malm
?
o: Tree Approximations of Semantic Parsing Problems
Natalie Schluter
?
, Jakob Elming, Sigrid Klerke, H
?
ector Mart??nez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders S?gaard
?
Dpt. of Computer Science Center for Language Technology
Malm?o University University of Copenhagen
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
Abstract
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
1 Introduction
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.
1
At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1
For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B?ohmov?a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
2 Related work
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.
2
2
We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
213
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-2014
3
also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
3 Tree approximations
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of ?good?
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the ?top? fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
?root?. (Note that the ?root? of each non-singleton
connected component is marked as a ?top? node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3
http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
3.1 Graph pruning
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge?s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
3.2 Graph packing
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
214
a)
b)
c)
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
be represented if the head or arguments are ?fac-
tored out?. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges? labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p
1
and
p
2
in the graph, where p
1
= (v
1
, l, v
n
) and p
2
=
(v
1
, l
1
, v
2
), (v
2
, l
2
, v
3
), . . . , (v
n?1
, l
n?1
, v
n
), we
remove the last edge of p
2
and augment p
1
?s label
with the representation l
1
: l
2
: ? ? ? : l
n?1
of p
2
. p
1
becomes (v
1
, l and l
1
: l
2
: ? ? ? : l
n?1
, v
n
), indi-
cating that v
n
is also the child (with dependency
label l
n?1
) of the node found by travelling (from
v
1
) down an l
1
labelled edge, followed by an l
2
labelled edge, and so on until the child of the l
n?2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove ?worst? re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
4 Experiments
4.1 Data
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
215
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
4.2 Model
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)
4
with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
4.3 Baselines
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj?orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
Approach Cl DM PAS PCEDT Av
Systems
PRUNING
NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING
NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
4.4 Results
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
4
https://code.google.com/p/mate-tools/
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
F1 84.4 88.0 69.9 80.8
?
PREC 85.4 87.9 70.8 81.4
(W/O TOP) REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
F1 83.6 86.0 67.4 79.0
?
PREC 87.2 91.3 72.8 83.8
(W/O TOP) REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
(W/O TOP) REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
(W/O TOP) REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
4.5 Error Analysis
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
216
5 Conclusions
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
References
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43?48, Boulder, CO,
USA.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill?e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103?127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89?97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281?332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79?
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044?1050,
Prague, Czech Republic.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753?760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166?173, Odense, Denmark.
217
