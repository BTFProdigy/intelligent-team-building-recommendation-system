Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 688?697,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Simple Unsupervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon
Qiuye Zhao Mitch Marcus
Dept. of Computer & Information Science
University of Pennsylvania
qiuye, mitch@cis.upenn.edu
Abstract
We propose a new model for unsupervised
POS tagging based on linguistic distinc-
tions between open and closed-class items.
Exploiting notions from current linguis-
tic theory, the system uses far less infor-
mation than previous systems, far simpler
computational methods, and far sparser
descriptions in learning contexts. By ap-
plying simple language acquisition tech-
niques based on counting, the system is
given the closed-class lexicon, acquires a
large open-class lexicon and then acquires
disambiguation rules for both. This sys-
tem achieves a 20% error reduction for
POS tagging over state-of-the-art unsuper-
vised systems tested under the same con-
ditions, and achieves comparable accuracy
when trained with much less prior infor-
mation.
1 Introduction
All recent research on unsupervised tagging, as
well as the majority of work on supervised tag-
gers, views POS tagging as a sequential labeling
problem and treats all POS tags, both closed- and
open-class, as roughly equivalent. In this work we
explore a different understanding of the tagging
problem, viewing it as a process of first identifying
functional syntactic contexts, which are flagged
by closed-class items, and then using these func-
tional contexts to determine the POS labels. This
disambiguation model differs from most previous
work in three ways: 1) it uses different encod-
ings over two distinct domains (roughly open- and
closed-class words) with complementary distribu-
tion (and so decodes separately); 2) it is determin-
istic and 3) it is non-lexicalized. By learning dis-
ambiguation models for open- and closed- classes
separately, we found that the deterministic, rule-
based model can be learned from unannotated data
by a simple strategy of selecting a rule in each ap-
propriate context with the highest count.
In contrast to this, most previous work on un-
supervised tagging (especially for English) con-
centrates on improving the parameter estima-
tion techniques for training statistical disambigua-
tion models from unannotated data. For exam-
ple, (Smith&Eisner, 2005) proposes contrastive
estimation (CE) for log-linear models (CRF),
achieving the current state-of-the-art performance
of 90.4%; (Goldwater&Griffiths, 2007) applies
a Bayesian approach to improve maximum-
likelihood estimation (MLE) for training genera-
tive models (HMM). In the main experiments of
both of these papers, the disambiguation model
is learned, but the algorithms assume a complete
knowledge of the lexicon with all possible tags for
each word. In this work, we propose making such
a large lexicon unnecessary by learning the bulk
of the lexicon along with learning a disambigua-
tion model.
Little previous work has been done on this nat-
ural and simple idea because the clusters found by
previous induction schemes are not in line with the
lexical categories that we care about. (Chan, 2008)
is perhaps the first with the intention of generat-
ing ?a discrete set of clusters.? By applying simi-
lar techniques to (Chan, 2008), which we discuss
later, we can generate clusters that closely approx-
imate the central open-class lexical categories, a
major advance, but we still require a closed-class
lexicon specifying possible tags for these words.
This asymmetry in our lexicon acquisition model
conforms with our understanding of natural lan-
guage as structured data over two distinct domains
with complementary distribution: open-class (lex-
ical) and closed-class (functional).
Provided with only a closed-class lexicon of
288 words, about 0.6% of the full lexicon, the sys-
tem acquires a large open-class lexicon and then
acquires disambiguation rules for both closed- and
688
open-class words, achieving a tagging accuracy of
90.6% for a 24k dataset, as high as the current
state-of-the-art (90.4%) achieved with a complete
dictionary. In the test condition where both algo-
rithms are provided with a full lexicon, and are
trained and evaluated over the same 96k dataset,
we reduce the tagging error by up to 20%.
In Section 2 we explain our understanding of the
POS tagging problem in detail and define the no-
tions of functional context and open- and closed-
class elements. Then we will introduce our meth-
ods for acquiring the lexicon (Section 3) and learn-
ing disambiguation models (Section 4, 5 and 6)
step by step. Results are reported in Section 7 fol-
lowed by Section 8 which discusses the linguistic
motivation behind this work and the simplicity and
efficiency of our model.
2 The Tagging Problem
In most work on both unsupervised and supervised
problem, tagging is viewed as a sequential label-
ing problem. In this work, however, we would like
to explore another view on tagging especially con-
sidering language as structured data.
The engineering concept of POS tags derives
from the linguistic notion of syntactic category
which specifies the combinatorial properties of a
word in an underlying (syntactic) structure. Given
the parse structure for a given word sequence
which breaks the input into recursive functional
domains such as IP, VP and NP, the POS tag of
each word can be directly inferred. Of course, as-
suming a pre-parsed structure as input to POS tag-
ging is somewhat ridiculous, but it strongly mo-
tivates us to highlight the features of structural
information for POS tagging. Without resorting
to any intermediate representations richer than the
input string, we propose for engineering purposes
to capture the features of interest for POS tagging
by the functional items in language themselves.
Then tagging is considered to be a process of iden-
tifying the functional contexts (functional items in
context) in which the categorical property of the
target item can be inferred.
Following ideas in current linguistic theory dis-
cussed in Section 8, we observe that the functional
categories and some morphological endings serve
as markers of the functional domains themselves
(discussed above) and sit abstractly at the edge of
those domains; the open-class (lexical) items must
sit within appropriate functional domains. More
specifically, although long distance dependencies
are not at all rare, for a token in sequence, we
only consider adjacent closed-class words and the
verbal categorical feature (but not morphology) as
functional contexts, the core concept in our disam-
biguation model.
Our system uses five open-class categories:
three basic lexical categories verb, noun and ad-
verb, and two derived Nominal categories (the two
kinds of participles in English); and consider all
other words not included in those categories to be
closed-class items.
Overall, for the task of unsupervised tagging,
we use a rule-based disambiguation model con-
taining disambiguation rules conditioned on func-
tional contexts, and the model is learned from
unannotated data constrained by much less lexi-
cal knowledge than most previous work, namely
the closed-class lexicon as introduced below.
2.1 Closed-class Lexicon
A dictionary containing all possible tags for each
word is very useful to constrain the unsupervised
learning of a POS disambiguation model, and in
most previous work, a full lexicon computed from
the WSJ corpus (the source of both training and
test datasets) is used for both learning and tagging.
Since a full lexicon is not a reasonable resource,
we aim to limit the required knowledge to func-
tional (closed-class) words only.
It is hard to define functional words in a lin-
guistically strict sense, but this category is close
to the notion within the engineering field of NLP
of closed-class words, classes of words that are
not open for new members. From the engineer-
ing point of view, this implies that a closed class
has a finite and static number of members, so its
members can be listed once and for all.
For English, lists of closed-class categories such
as preposition, pronoun or even degree adverb, are
obtainable resources, but this is not necessarily the
case for other languages. In this paper, we leave
the automatic acquisition of a closed-class lexicon
for future work. For experiments in this work, we
automatically compute a closed-class lexicon from
the WSJ treebank 00-24 sections by picking out
those words that are labeled predominantly with
closed-class tags
1
. For each word selected as a
closed-class word, all possible tags encountered
1
For each word, if the number of instances labeled by
closed-class tags is greater than by open-class tags, we select
it as a closed-class word.
689
more than twice in the WSJ corpus are reserved
in the closed-class lexicon, so closed-class words
may also have open-class tags in our data set, a
source of noise in our results. As a core part of
language, this closed-class lexicon containing 288
entries, about 0.6% of the full lexicon by types,
should be invariant over various genres, which is
confirmed in experiments on both WSJ and Brown
corpus
2
.
2.2 Tagset
The 45 tags in the Penn Tagset (Marcus et al,
2003) contain more information than just basic
lexical categories. In recent work on unsupervised
learning of POS taggers following (Smith&Eisner,
2005), the Penn tagset is reduced to 17 tags which
nicely improves the tagging performance.
Based on our view of POS tags as local mark-
ers of underlying syntactic structure, we derive 27
tags from a feature-based analysis of the original
Penn tagset. The main principle for reduction is
that we collapse any two tags which are not distin-
guishable by structural features; such features in-
clude +/-N, +/-V for predication and +/-wh, +/-en
for movement
3
. For example, under our analysis,
the tag ?VBG? has the features [+V, +N, -tense, -
en], tag ?VBD? [+V, +tense(past), -en], and ?VB?
[+V, -tense(finite), -en]. However, since we do not
consider the tense feature to be a structural feature,
we do not distinguish ?VBD? from ?VB?; since
N(ominal) is a structural feature, ?VBG? remains
distinct from both ?VBD? and ?VB?. The 27 tags
do not cover all cases of ambiguities of closed-
class words in the original Penn tagset. Most no-
tably, adjectives are not separated from nouns.
This reduction naturally follows the crucial
properties of our disambiguation model. First of
all, our model is not lexicalized, so it can only
capture basic interactive relations between cate-
gories but cannot capture lexical dependencies,
which are heavily required to disambiguate ?RP?
2
There are two special classes of words worthy of dis-
cussion with respect to being closed or open. 1. While the
morphological ending ?-ly? freely introduces adverbs, this
category is otherwise essentially closed class; and 2. There
are obviously unboundedly many numbers(CD), but all these
match some regular pattern. So we include adverbs without
explicit morphological marking in the closed-class lexicon
(we frankly doubt adverbs can be acquired by distributional
clustering); and as for numbers, we embed exactly such a
regular pattern in our model.
3
Not all features of tags are listed here, and further dis-
cussion of the feature-based analysis of the tagset is to be
reported in other work. This analysis of tags is motivated by
Chomsky.
Tagset #tags #closed #open amb./token
Smith&Eisner 17 7 6 2.07
ThisWork 27 12 6 1.83
Penn 45 15 15 2.74
Table 1: Comparison of tagsets
Category Open tags Closed tags
Verbal VB ...
Nominal NN, VBN, VBG DT, CD, PRP($), WDT, WP($)
None RB CC, EX, IN, MD, POS, TO
Table 2: N/V categories of 27 POS tags
with ?IN? or ?PDT? with ?DT? (so these two pairs
are collapsed). More importantly, the structural
information carried by the closed-class items is
the key feature of our disambiguation model, but
nouns and adjectives are not distinguishable by
their structural positions (in NP), so they are not
to be distinguished in our tagset
4
.
We use this new reduced tagset with 27 tags in
our experiments
5
. For the purposes of comparison,
we map the results using our 27 tag tagset to the
commonly-used 17 tag tagset
6
, and evaluate our
algorithms for both tagsets. Table 1 compare the
three tagsets, and the ambiguity column shows the
average number of ambiguous tags per token in
WSJ corpus section 00-24.
2.3 NV category
By using the reduced 27 tags, we found in this
work that the heart of the disambiguation task
for open-class words is to distinguish them in the
Nominal vs. Verbal domains; and for the closed-
class words, the Nominal vs. Verbal property of
the adjacent context words is also very helpful for
4
Due to the indistinguishable roles of adjectives and
nouns in Noun Phrase, it is also hard to extract the adjectives
from nouns for lexicon acquisition.
5
For open-class categories, we keep VB (for VB*), NN
for (NN*), VBG, VBN and RB (for RB*), and we reduce the
JJ* tags to the tag NN and for closed-class tags, we keep al-
most all the original distinctions, except for two pairs: ?PDT?
and ?DT?; ?RP? and ?IN?. Also ?WRB? is reduced to ?RB?.
6
In our tagset, there are two coarser tags which stand for
more than one tag in the 17 tags: ?NN? stands for both ?N? and
?ADJ? and ?IN? for both ?RP? and ?IN?. So to map the output
of coarser tags to the finer ones, we need to look up the full-
lexicon, since adjectives are not extracted from nouns in the
lexicon acquisition process. For a word tagged as ?NN? with
a possible tag of ?JJ?, if the following word is also tagged as
?NN?, then the current ?NN? is mapped to ?JJ?. On the other
hand, no action is done for mapping ?IN?, so gold ?RP? is
always mis-tagged as ?IN? after mapping. If our tagging sys-
tem outputs a finer tag (e.g. WDT) then it is reduced to the
corresponding coarser one (e.g. ?W?) in mapping to 17 tags.
690
disambiguation. The Nominal vs. Verbal property
is defined through N/V categories of POS tags, and
we list each category containing both closed-class
and open-class tags in table 2.
3 Acquiring the open-class lexicon
Not being equipped with a full lexicon, our system
takes the closed-class lexicon as given, and auto-
matically computes possible tags, which must be
open class, for all other words in the acquisition
process as described below. There are five open-
class tags in our reduced tagset, as we describe
above: ?VBG? and ?VBN? represent two kinds
of derived Nominal elements, with correspond-
ing morphological endings attached to the verbal
roots; and ?RB? represents the adverbial class into
which new words can only be introduced if affixed
with the special ending ?-ly?. Taking into account
this special morphology, we divide our construc-
tion of the open-class lexicon into two steps: N/V-
Clustering and Morphing. At the N/V-clustering
step, we classify the base-forms (roots) of open-
class words into two clusters in a sparse feature
space. At the Morphing step, we count on the em-
bedded functional elements (i.e. morphology) to
derive specific tags for words in each cluster.
3.1 Clustering
Inducing syntactic categories is a language ac-
quisition task on which there has been ex-
tensive research, e.g. (Clark, 2003) and
(Sch?utze, 1993), based largely on variants
of distributional clustering. In a standard
setup of POS clustering, each target word to
be clustered, w
i
, is represented as a vector,
<count(w
i
,C
1
), count(w
i
,C
2
),...,count(w
i
,C
m
)>,
collecting counts of occurrences ofw
i
in each con-
text, C
j
. Then the chosen algorithm clusters the
feature vectors according to similarity.
In previous work, the contextual features are
lexical, so the length of a feature vector varies
from hundreds to thousands of features. The
clustering algorithm then runs over this high-
dimensional space, which is computationally quite
intensive. Unlike previous work, our system only
employs seven features, all functional, to represent
target words, and we are paid back by a substantial
improvement in efficiency. Each open-class word
is represented in the feature space by the following
seven component vector: <left:DT, left:MD, mid:-
?, mid:-ed, mid:-ing, right:DT, right:MD>. The
first two values in this vector represent the counts
of modal verbs (MD) and determiners (DT) occur-
ring to the left of all forms of a base form; the three
values in the middle represent the counts of three
possible morphological forms of a word; and the
last two values represent the counts of an immedi-
ately followingMD and DT. This radical reduction
of the feature space eliminates any need for so-
phisticated clustering techniques. For the purpose
of convenience, we use a basic k-means clustering
algorithm which allows us to specify the number
of output clusters (Maffi, 2007).
As is well known, clustering all words in a cor-
pus using distributional clustering results in a high
number of clusters. For example, (Sch?utze, 1993)
induces 200 clusters and (Clark, 2003) chooses
between 16-128; and most of these induced cate-
gories are difficult to associate with a specific POS
tag. Chan?s recent thesis work (Chan, 2008) pro-
vides us with a solution to this problem. In the first
pass of Chan?s model for unsupervised lexical cat-
egory induction, verbs are separated from all other
categories with a high level of purity; the second
pass separates adjectives from nouns by using the
categorical results from the first pass as an addi-
tional feature
7
. His experiments for a wide range
of languages show that the ?restriction to clus-
ter base forms only
8
? is crucial to induce clusters
more in line with the definition of the open-class
syntactic categories we care about here.
Here, we follow a variant of Chan?s approach,
grouping words with their base-forms for cluster-
ing. For example, we group all occurrences of the
transformed (morphological) forms, (start, starts,
starting and started), in a particular context, C
j
,
together with the base form start to form a single
count for (start, C
j
), in forming the correspond-
ing feature vectors. Given this, since all inflections
of one base form share the same feature vector, all
inflections enter into the same class of their base-
form. In (Chan, 2008), morphological base forms
are the output of a new morphology induction al-
gorithm he develops. Here, we simply extract the
base form of a word by stripping three possible
forms of endings: -s, -ing and -ed
9
.
7
For simplicity, we don?t run a second pass but reduce
adjectives to noun.
8
See p.139 in (Chan, 2008)
9
This simple strategy, as well as more complex morpho-
logical analyzers, cannot deal with irregular verbs, so we list
in memory the corresponding ?regular? ending of each irreg-
ular verb. For example, we know that the ending of ran is
?-ed?, but we DO NOT know that ran is only the past tense
691
3.2 Morphing
After the clustering step, which we intend to sep-
arate the Nominal and Verbal classes, two clusters
as desired are induced, but we still need a method
to automatically decide which one is which. A
trick that works well in practice is simply to pick
the smaller class as the Verbal class. These two
classes reflect the basic categories of the roots;
by a generative mechanism observed in most lan-
guages, roots (base-forms) are transformed into
derived categories by fusing with functional el-
ements, which surface as the few morphological
endings in English.
For all words in the Nominal class, except for
those with the ending -ly, the only possible tag for
each is ?NN?, since no finer categories of ?NN? ex-
ist in our reduced tagset. On the other hand, for a
word with ending -ly falling into the N class, we
simply assume that its tag must be ?RB?, although
this assumption may have a few exceptions.
The Verbal class contains all words with ver-
bal roots. There are two specific endings in En-
glish serving as morphological markers of derived
Nominal categories, -ed and -ing, correspond-
ing to derived categories ?VBN? and ?VBG? re-
spectively. So for each word ending with -ed,
we assign two possible tags to it, ?VB?(our re-
duced form of ?VBD?) and ?VBN?; and for each
word ending with -ing we assume only one pos-
sible tag, ?VBG?, although this assumption may
systematically introduce tagging error confusing
?VBG? and ?NN?. For example, if the feature vec-
tor representing the base-form group start, starts,
started,starting is classified into the verbal class,
then both starts and start will receive one possi-
ble tag ?VB?; starting will receive one possible tag
?VBG?; but started will receive two possible tags
?VBN? and ?VB?.
As one may notice, start and starts should have
two senses, noun and verb, but the Nominal sense
is lost in the Morphing step. For such cases, we
introduce a simple supplemental process to com-
pensate for the missing Nominal sense. For a word
with the possible tag ?VB? (not ?VBG? or ?VBN?)
as determined in the Morphing step, if it is ever
seen following a determiner in context, another
possible tag ?NN? will be assigned to it.
Remember that, as introduced in Sect 2.2,
form of run, because the ending ?-ed? is ambiguous for both
past tense and past participle. The list of irregular verbs is
obtained from http://www.englishpage.com.
?VBN?, ?VBG? and ?NN? are of category N and
?VB? is of category V. Then for each word in the
resulting lexicon, there is maximally one possible
tag of it falling in either category N or V, so the
category information (N or V) is enough for the
disambiguation task, as specified in Section 6.
4 Unsupervised Tagging
Taking a dictionary as input, the task of unsuper-
vised tagging is to learn a disambiguation model
from unannotated data and apply this model for
disambiguating the occurrences of words in con-
text. In this section, we are going to introduce the
representation of our disambiguation model first,
and then discuss how it affects the system design.
In the following two sections, we will describe the
algorithms for learning and decoding the language
model respectively.
4.1 Disambiguation Model
Again, we view tagging as a process of identifying
functional context, from which the proper tagging
simply follows. Given this, we represent the lan-
guage model as a set of disambiguation rules con-
ditioned on functional contexts that predict cate-
gorical information, with each rule of the form of
r = (con : cat) with con and cat the functional
context and categorical information respectively.
In both open- and closed-class domains, given
a pair of words (W
l
,W
r
), the disambiguation
rules check the functional property of W
l
and pre-
dicts the N/V category of W
r
. However, in the
open-class disambiguation model, con represents
closed-class items as well as verbal feature, but in
the closed-class disambiguation model, con rep-
resents closed-class categories (closed-class POS
tags). In disambiguating an open-class word, con
is checked against the preceding closed-class word
or verbal feature (if any), and cat of the follow-
ing open-class word is predicted. In disambiguat-
ing a closed-class word cw, each possible tag of
cw may invoke a rule and each rule will predict a
N/V category of the following item; if some rule
makes the right prediction, the corresponding tag
is assigned to cw. For example, he:V, a disam-
biguation rule for open-class words, says that if an
open-class token follows the closed-class item he,
then the Verbal tag should be assigned to this to-
ken. On the other hand, IN:N, a disambiguation
rule for closed-class words, says that if a closed-
class token precedes a Nominal word (open- or
692
closed- class) in context and has a possible tag of
?IN?, then tag it with ?IN?.
This rule-based disambiguation model is deter-
ministic in the sense that for each token in context
there is maximally one tag that can be predicted.
Not being statistically parameterized, this greedy
prediction requires that 1) each rule is determin-
istic and 2) in each context, only one rule is in-
voked (which is guaranteed by the selection step
introduced in Section 5.2). Moreover, this disam-
biguation model is non-lexicalized in that it is only
conditioned on the functional items in context but
not the target word itself.
4.2 System Design
Ideally, we should use closed-class tags in con-
text for disambiguating open-class words because
closed-class words are potentially ambiguous; but
this would cause a chicken-egg problem. If we
did this, then the learning of disambiguation rules
for closed-class words requires category informa-
tion for open-class items and vice versa, but none
of the required category information is available
from the unannotated data
10
. Thanks to how lan-
guage works (including principally the low de-
gree of ambiguity of closed-class words), it is
good enough practically, as shown by our exper-
iments, to encode the disambiguation model for
open-class words using closed-class items without
categorical information.
In this way, we can learn the disambiguation
model of open-class items from raw data; how-
ever, closed-class disambiguation model is better
learned after open-class words are disambiguated.
Then there are four models in the system for learn-
ing and tagging over two distinct domains: Model-
LC and Model-LO for learning the disambigua-
tion model of closed- and open-class words re-
spectively; Model-DC and Model-DO for disam-
biguating closed- and open-class words respec-
tively; and they must be executed in a strict order
as follows: Model-LO ? Model-DO ? Model-
LC ? Model-DC, as illustrated in Figure 1.
5 Learning Disambiguation Rules
In this section, we describe the learning algorithm
used in both Model-LO and Model-LC. Although
there is no annotated data available for learning,
10
Our disambiguation model is not statistically parameter-
ized, so this problem can not be resolved by any kind of pa-
rameter estimation technique as in previous work on unsuper-
vised tagging.
Disamb.
LO
Learning
LC
DO
DC
Open
Closed
Figure 1: The order of the four models in system.
we can use the unambiguous events in data to
establish the disambiguation rules and apply the
rules to ambiguous events. The only difference in
implementation of the two models lies in the ?rule-
extraction?, corresponding to different interpreta-
tions of unambiguous events for learning open-
and closed-class disambiguation models. After
being extracted from pairs of adjacent words in the
input sequence, the rules are counted and selected
using the same algorithm in both models.
5.1 Rule-extraction
For open-class words, disambiguation rules are
extracted from raw data. A pair of adjacent words
(W
l
,W
r
) is considered unambiguous if it satis-
fies the following two conditions: 1. W
l
is in
the closed class or an unambiguous type with only
possible tag of ?VB?; and 2. all possible tags ofW
r
fall in the same N/V category (Nominal or Verbal
but not mixed). If (W
l
,W
r
) is unambiguous in this
sense, then extract rule r = (con : cat), where
con is W
l
(for closed-class words) or ?V? (for un-
ambiguous verbal words), and cat is the N/V cat-
egory of W
r
. For example, in the sequence (...he
has claimed..), the pair (he, has) is unambiguous
in that he is a closed-class item and has has only
one possible tag, ?VB?, so a rule ((he : V ) is
extracted; but (has, claimed) is not usable since
claimed has two possible tags: ?VB? of category
V and ?VBN? of category N.
Disambiguation rules for closed-class words are
extracted after open-class disambiguation. A pair
of adjacent words (W
l
,W
r
) is considered unam-
biguous if it satisfies the following two conditions:
1. W
l
is in the closed class and has only one
possible tag in the closed-class lexicon; 2. W
r
is either disambiguated or all possible tags of W
r
fall in the same N/V category. If (W
l
,W
r
) is un-
ambiguous in the above sense, then extract rule
r = (con : cat), where con is the single tag of
W
l
, and cat is the N/V category of W
r
. For ex-
ample, in the sequence ?...for his stepping...?, the
pair (for his) is unambiguous in that for has only
693
one possible tag ?IN? and both possible tags of his,
?PRP? and ?PRP$?, fall into the Nominal category,
then a rule (IN : N) is extracted; but (his about)
is not usable since his has more than one possi-
ble tag and about has two possible tags, ?RB? and
?IN?, which are neither both ?N? nor both ?V?.
5.2 Counting and Selecting
In the counting step, a set of rules R is first initial-
ized to be empty, and then, as each disambigua-
tion rule r is generated while passing through the
data, if not already in R, it is added with an ini-
tial count of one; otherwise, N
r
, the count of r,
is incremented by one. Note that we know that
for a rule, (con : cat), the prediction cat can
only be either N or V; then for each context con,
there are two forms of rules counted, (con : N)
or (con : V ). By selecting the rule with a greater
count for each context, we guarantee that the re-
sulting disambiguation model is deterministic.
6 Tagging
Given our rule-based, deterministic language
model, tagging is a straightforward process of
decoding the disambiguation rules. Recall that
there are two separate tagging models in the sys-
tem, Model-DO and Model-DC for disambiguat-
ing open- and closed-class respectively.
The inputs to Model-DO are the open-class lex-
icon, the disambiguation rules learned in Model-
LO and raw data in sequence. For each ambiguous
open-class word w in sequence if the preceding
closed-class word (if any) invokes a disambigua-
tion rule, r = (con : cat), then pick the possible
tag of w that falls in the category of cat (N or V),
as discussed in Section 3.2. If no rule is triggered
our default choice is ?NN?; but if ?NN? is not a pos-
sible tag, we assume the default domain is Verbal
(so the ?VB? tag is favored).
The application of disambiguation rules in
Model-DC is a little more complex. For each
ambiguous closed-class word cw in sequence fol-
lowed by a token of category cat, N or V, pick a
possible tag of cw, con, such that (con : cat) is
a rule learned in Model-LC. If no tag is picked,
a random choice is made. While there are resid-
ual cases that no functional context can help with
tagging, the disambiguation model proposed here
combined with random choice results in a good
overall performance, as shown in section 7.3.
dict. with words of count > d
d 1 2 3 ? #tag
(percent lex.) (100%) (55%) (41%) (0.6%) -
BHMM2 87.3 79.6 65.0 - 17
CRF/CE 90.4 77.0 71.7 - 17
model-17 91.8 ... ... 90.6 17
model-27 93.2 ... ... 92.1 27
LDA+AC 93.4 91.2 89.7 - 17
Table 3: Tagging accuracy with partial dictionaries over
24k dataset; our closed-class lexicon is the closest approxi-
mation to the? column .
7 Results
Our unsupervised tagging system is com-
pared to the following models As reported in
(Banko&Moore, 2004), ?the quality of the lexicon
made available to unsupervised learner made the
greatest difference to tagging accuracy?. So we
only compare our experiments to recent work
built over the same dataset and a full lexicon
automatically extracted from the Penn Treebank.
As described in section 2.1, the closed-class
lexicon, special in our experiments, is also auto-
matically constructed from the WSJ corpus, and
will be used in experiments on both WSJ and
Brown corpora below
11
. CRF/CE (Smith&Eisner,
2005) and BHMM2 (Goldwater&Griffiths, 2007)
have been discussed briefly in the introduction.
LDA+AC (Toutanova&Johnson, 2007) is actually
a semi-unsupervised model given the prior on
p(t|w); despite this additional information, our
model outperforms it in experiments with partial
dictionaries. For the purpose of comparison,
our experiments use the same dataset as in these
previous work, varying in sizes from 12K to 96K.
In addition to reporting on our own tagset with 27
tags, we also map the results onto the 17 tags used
in other models as explained above.
7.1 Unsupervised Tagging over Partial
Dictionaries
As shown in Table 3, reducing the dictionary by
filtering rare words (with count<= d) has not been
a promising track to follow for accomplishing the
task with as little information as possible. How-
ever, by introducing a lexicon acquisition step, we
achieve a tagging accuracy of 90.6% for the 24K
test data with no prior open-class lexicon, pro-
vided with only a minimal lexicon of closed-class
items (about 0.6% of the full lexicon), as high as
11
If we control the quality of the closed-class lexicon (but
still leave the full-lexicon untouched) by filtering out errors
in the Treebank, the performance is considerably higher.
694
size 12K 24k 48k 96K #tag lex.
BHMM2 85.8 84.4 85.7 85.8 17 full
CRF/CE 86.2 88.6 88.4 89.4 17 full
Model-17 91.0 91.6 91.6 91.5 17 full
Model-27 93.1 93.6 93.5 93.4 27 full
model-17 88.9 89.3 90.2 90.4 17 closed
model-27 90.9 91.2 92.0 92.2 27 closed
Table 4: Tagging Accuracy of models trained over dataset
varying in sizes with full/closed-class lexicon
the best previous performance of 90.4 given a full
lexicon (CRF/CE with d = 1)
12
.
One other work that investigates the use of a
limited lexicon is (Haghighi&Klein, 2006), which
develops a prototype-drive approach to propagate
the categorical property using distributional simi-
larity features; using only three exemplars of each
tag, they achieve a tagging accuracy of 80.5% us-
ing a somewhat larger dataset but also the full
Penn tagset, which is much larger.
7.2 Varying in sizes
As shown in Table 4, our new algorithm reduces
tagging error by up to 20% over the state-of-the-
art given a full lexicon, from 89.4% to 91.5% over
the 96k dataset
13
.
To better understand the learning property of
our system and to get an estimate of the vari-
ance of our results above, we repeated the exper-
iments above, starting with either the full lexicon
or just the closed-class lexicon, with datasets vary-
ing from 0.5K to 96K in size, and repeated each
experiment 60 times on different sequences, with
four samples randomly selected from the Brown
corpus, one from the training data reported above
and the others from the WSJ corpus. As shown in
Figure 2, for the closed-class lexicon experiments,
the standard deviation of tagging accuracy over the
dataset of each size sharply decreases as the size of
the data increases, as expected. It is also clear that
12
Since we are facing an unsupervised task, the training set
is unannotated, and hence there is no reason not to use it as the
test set as well. For the sake of comparison, we use the same
split of the dataset for training as previous work. In Table 3
the tagging model is trained over 96k and evaluated on 24k,
but in Table 4, the tagging model is trained and evaluated over
test and training sets of the same size.
13
With a full lexicon, we need to disambiguate between
open-class tags which fall into the same N/V category, which
is beyond the ability of our disambiguation rules which pre-
dict N or V only. When more than one possible tag in the
same category predicted by the disambiguation rule, we sim-
ply make a random choice. Although not as constrained as
the acquired lexicon, a full lexicon does improve the tagging
performance, since the automatic lexicon acquisition is far
from perfect.
 
88
 
89
 
90
 
91
 
92
 
93
 
94
 
95
 
96
0.5k
1k
3k
6k
12k
24k
48k
96k
 
0.2
 
0.4
 
0.6
 
0.8
 
1
 
1.2
 
1.4
 
1.6
 
1.8
 
2
Accuracy (%) (tagging all words)
standard deviation of accuracy over same size
Size
 of d
ata (k
)
60 s
amp
les p
er s
ize
stan
dard
 dev
iatio
n
train
ing d
ata
brow
n co
rpus
Figure 2: Standard Deviation of Tagging Accuracy with
closed-class lexicon; 60 samples for each size, randomly se-
lected from both Brown and WSJ corpus.
system with system with
closed-class lexicon full lexicon
sub-model #errors accuracy #errors accuracy
Model-DO 1089 87.3% 3546 78.9%
Model-DC 1694 89.6% 1709 89.7%
random 1148 44.2% 981 44.9%
recall 3650 - 75 -
total 7581 75.2% 6311 82.1%
#ambiguous 30563 35229
Table 5: The number of errors and percent ambiguous to-
kens tagged correctly in the 96k dataset with 27 tags. For ei-
ther system built upon closed-class lexicon or full lexicon, the
table shows the disambiguation accuracy and number of er-
rors for each sub-model in the system: Model-DO for disam-
biguating open-class, Model-DC for disambiguating Closed-
class and random choice. The numbers of recall errors (gold
tag not in dictionary) and total errors for each system are also
shown.
the performance of our algorithm on the Brown
corpus is as strong as on the WSJ corpus. Results
for the full-lexicon are similar.
7.3 Error Analysis
There are certainly cases that no functional context
can help with tagging, since our disambiguation
models are encoded by functional context only.
Thus it is worth a closer look to how often the
system resorts to random choice, as well as to the
disambiguation accuracy of either disambiguation
model for open- and closed- class learned from
unannotated data. We show the disambiguation
accuracy of ambiguous words only for each model
in Table 5, and also the number of errors due to
imperfect lexicons or random choice.
8 Discussion and Future Work
In this work on unsupervised tagging, we com-
bine lexicon acquisition with the learning of a
695
POS disambiguation model. Moreover, the dis-
ambiguation model we used is deterministic, non-
lexicalized and defined over two distinct do-
mains with complementary distribution (open- and
closed-class).
Building a lexicon based on induced clusters
requires our morphological knowledge of three
special endings in English: -ing, -ed and -s; on
the other hand, to reduce the feature space used
for category induction, we utilize vectors of func-
tional features only, exploiting our knowledge of
the role of determiners and modal verbs. How-
ever, the above information is restricted to the lex-
icon acquisition model. Taking a lexicon as in-
put, which either consists of a known closed-class
lexicon together with an acquired open-class lexi-
con or is composed by automatic extraction from
the Penn Treebank, we need NO language-specific
knowledge for learning the disambiguation model.
We would like to point the reader to (Chan,
2008) for more discussion on Category induc-
tion
14
; and discussions below will concentrate on
the proposed disambiguation model.
Current Chomskian theory, developed in the
Minimalist Program (MP) (Chomsky, 2006), ar-
gues (very roughly speaking) that the syntactic
structure of a sentence is built around a scaffold-
ing provided by a set of functional elements
15
.
Each of these provides a large tree fragment
(roughly corresponding to what Chomsky calls a
phase) that provide the piece parts for full utter-
ances. Chomsky observes that when these frag-
ments combine, only the very edge of the frag-
ments can change and that the internal structure of
these fragments is rigid (he labels this observation
the Phase Impenetrability Condition, PIC). With
the belief in PIC, we propose the concept of func-
tional context, in which category property can be
determined; also we notice the distinct distribution
of the elements (functional) on the edge of phase
and those (lexical) assembled within the phase.
Instead of chasing the highest possible perfor-
mance by using the strongest method possible, we
wanted to explore how well a deterministic, non-
lexicalized model, following certain linguistic in-
tuitions, can approach the NLP problem. For the
14
In our experiment, using the base-forms and adding a
compensation process improves the coverage rate of the ac-
quired lexicon from 79% to 93%.
15
Such as determiners (for NPs), complementizers like that
(for clauses), and case assigning elements associated with
transitive verbs (for propositions).
unsupervised tagging task, this simple model, with
less than two hundred rules learned, even outper-
forms non-deterministic generative models with
ten of thousands of parameters.
Another motivation for our pursuit of this deter-
ministic, non-lexicalized model is computational
efficiency
16
. It takes less than 3 minutes total for
our model to acquire the lexicon, learn the disam-
biguation model, tag raw data and evaluate the out-
put for a 96k dataset on a small laptop
17
. And a
model using only counting and selecting is com-
mon in the research field of language acquisition
and perhaps more compatible to the way humans
process language.
We are certainly aware that our work does not
yet address two problems: 1). How the system
can be adapted to work for other languages and
2) How to automatically obtain the knowledge of
functional elements. We believe that, given the
proper understanding of functional elements, our
system will be easily adapted to other languages,
but we clearly need to test this hypothesis. Also,
we are highly interested in completing our system
by incorporating the acquisition of functional el-
ements. (Chan, 2008) presents an extensive dis-
cussion of his work on morphological induction
and (Mintz et al, 2002) presents interesting psy-
chological experiments we can build on to acquire
closed-class words.
9 Acknowledgments
We thank the National Science Foundation for its
support of this work under grant IIS-0415138. We
greatly appreciate the comments of the anony-
mous reviewers; section 7.3 is newly added and
two more paragraphs are added to section 2.2 in
response to their comments. Also, we would like
to thank an anonymous reviewer of a earlier ver-
sion of this paper, whose thoughtful suggestion led
to a restructuring of the current version. We bene-
fited greatly from our discussions with Dr. Charles
Yang. Noah Smith provided the data sets and de-
tails of the 17 tag tagset used in previous work.
Finally, we thank Constantine Lignos for his care-
ful editing of earlier versions.
16
In some sense, the Minimalist Program was proposed to
explore the idea that the existence of Syntax is especially mo-
tivated by efficient language processing.
17
On a Intel Core 2 Duo P8600 2.40 GHz CPU.
696
References
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING, 2004.
Erwin Chan. 2008. Structures and distributions in
morphological learning. Ph.D. dissertation, Dept.
of Computer and Information Science, UPenn.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the 10th Meeting of the
EACL.
Chomsky, N. 2006. Approaching UG from below.
MIT.
Frank, Robert. 2006. Phase theory and Tree Adjoining
Grammar. Lingua.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised Part-of-
Speech tagging. In Proceedings of ACL.
Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
Kroch, A. and Joshi, A. K. 1985. Linguistic Relevance
of Tree Adjoining Grammars. Technical Report MS-
CIS-85-18, Department of Computer and Informa-
tion Science, University of Pennsylvania.
Charles N. Li, Sandra A. Thompson. Mandarin Chi-
nese: A Functional Reference Grammar University
of California Press, 1989
Hrafn Loftsson. Tagging Icelandic text: A linguistic
rule-based approach Nordic Journal of Linguistics
(2008), 31:47-72 Cambridge University Press
Leonardo Maffi. Implementation of K-means cluster-
ing in Python.
http://www.fantascienza.net/leonardo/so/kmeans/kmeans.html
Mitchell P. Marcus , Mary Ann Marcinkiewicz , Beat-
rice Santorini, 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, v.19 n.2, June 1993.
T.H. Mintz, E.L. Newport and T.G. Bever. 2002. The
distributional structure of grammatical categories in
speech to young children. Cognitive Science 26
(2002), pp. 393C424.
Hinrich Sch?utze. 1993. Part-of-speech induction from
scratch. In Proceedings of the 31st Meeting of the
ACL.
Noah A. Smith. Novel Estimation Methods for Un-
supervised Discovery of Latent Structure in Natural
Language Text. Ph.D. thesis, Johns Hopkins Uni-
versity Department of Computer Science, Baltimore,
MD, October 2006.
L Shen, G Satta and A Joshi. 2007. Guided Learning
for Bidirectional Sequence Classification In Pro-
ceedings of ACL.
Noah Smith and Jason Eisner. 2005. Contrastive es-
timation: Training log-linear models on unlabeled
data. In Proceedings of the 43rdMeeting of the ACL.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In NIPS2007.
Charles Yang. 2002. Knowledge and learning in natu-
ral language. Oxford University Press. (Chapter 3).
697
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1054?1062,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploring Deterministic Constraints: From a Constrained English POS
Tagger to an Efficient ILP Solution to Chinese Word Segmentation
Qiuye Zhao Mitch Marcus
Dept. of Computer & Information Science
University of Pennsylvania
qiuye, mitch@cis.upenn.edu
Abstract
We show for both English POS tagging and
Chinese word segmentation that with proper
representation, large number of deterministic
constraints can be learned from training exam-
ples, and these are useful in constraining prob-
abilistic inference. For tagging, learned con-
straints are directly used to constrain Viterbi
decoding. For segmentation, character-based
tagging constraints can be learned with the
same templates. However, they are better ap-
plied to a word-based model, thus an integer
linear programming (ILP) formulation is pro-
posed. For both problems, the corresponding
constrained solutions have advantages in both
efficiency and accuracy.
1 introduction
In recent work, interesting results are reported for
applications of integer linear programming (ILP)
such as semantic role labeling (SRL) (Roth and Yih,
2005), dependency parsing (Martins et al, 2009)
and so on. In an ILP formulation, ?non-local? de-
terministic constraints on output structures can be
naturally incorporated, such as ?a verb cannot take
two subject arguments? for SRL, and the projectiv-
ity constraint for dependency parsing. In contrast
to probabilistic constraints that are estimated from
training examples, this type of constraint is usually
hand-written reflecting one?s linguistic knowledge.
Dynamic programming techniques based on
Markov assumptions, such as Viterbi decoding, can-
not handle those ?non-local? constraints as discussed
above. However, it is possible to constrain Viterbi
decoding by ?local? constraints, e.g. ?assign label t
to word w? for POS tagging. This type of constraint
may come from human input solicited in interactive
inference procedure (Kristjansson et al, 2004).
In this work, we explore deterministic constraints
for two fundamental NLP problems, English POS
tagging and Chinese word segmentation. We show
by experiments that, with proper representation,
large number of deterministic constraints can be
learned automatically from training data, which can
then be used to constrain probabilistic inference.
For POS tagging, the learned constraints are di-
rectly used to constrain Viterbi decoding. The cor-
responding constrained tagger is 10 times faster than
searching in a raw space pruned with beam-width 5.
Tagging accuracy is moderately improved as well.
For Chinese word segmentation (CWS), which
can be formulated as character tagging, analogous
constraints can be learned with the same templates
as English POS tagging. High-quality constraints
can be learned with respect to a special tagset, how-
ever, with this tagset, the best segmentation accuracy
is hard to achieve. Therefore, these character-based
constraints are not directly used for determining pre-
dictions as in English POS tagging. We propose an
ILP formulation of the CWS problem. By adopt-
ing this ILP formulation, segmentation F-measure
is increased from 0.968 to 0.974, as compared to
Viterbi decoding with the same feature set. More-
over, the learned constraints can be applied to reduce
the number of possible words over a character se-
quence, i.e. to reduce the number of variables to set.
This reduction of problem size immediately speeds
up an ILP solver by more than 100 times.
1054
2 English POS tagging
2.1 Explore deterministic constraints
Suppose that, following (Chomsky, 1970), we dis-
tinguish major lexical categories (Noun, Verb, Ad-
jective and Preposition) by two binary features:
+|? N and +|? V. Let (+N, ?V)=Noun, (?N,
+V)=Verb, (+N, +V)=Adjective, and (?N,
?V)=preposition. A word occurring in between a
preceding word the and a following word of always
bears the feature +N. On the other hand, consider
the annotation guideline of English Treebank (Mar-
cus et al, 1993) instead. Part-of-speech (POS) tags
are used to categorize words, for example, the POS
tag VBG tags verbal gerunds, NNS tags nominal plu-
rals, DT tags determiners and so on. Following this
POS representation, there are as many as 10 possi-
ble POS tags that may occur in between the?of, as
estimated from the WSJ corpus of Penn Treebank.
2.1.1 Templates of deterministic constraints
To explore determinacy in the distribution of POS
tags in Penn Treebank, we need to consider that
a POS tag marks the basic syntactic category of a
word as well as its morphological inflection. A con-
straint that may determine the POS category should
reflect both the context and the morphological fea-
ture of the corresponding word.
The practical difficulty in representing such de-
terministic constraints is that we do not have a per-
fect mechanism to analyze morphological features
of a word. Endings or prefixes of English words do
not deterministically mark their morphological in-
flections. We propose to compute the morph feature
of a word as the set of all of its possible tags, i.e.
all tag types that are assigned to the word in training
data. Furthermore, we approximate unknown words
in testing data by rare words in training data. For
a word that occurs less than 5 times in the training
corpus, we compute its morph feature as its last two
characters, which is also conjoined with binary fea-
tures indicating whether the rare word contains dig-
its, hyphens or upper-case characters respectively.
See examples of morph features in Table 1.
We consider bigram and trigram templates for
generating potentially deterministic constraints. Let
wi denote the ith word relative to the current word
w0; and mi denote the morph feature of wi. A
(frequent) (set of possible tags of the word)
w0=trades m0={NNS, VBZ}
(rare) (the last two characters...)
w0=time-shares m0={-es, HYPHEN}
Table 1: Morph features of frequent words and rare words
as computed from the WSJ Corpus of Penn Treebank.
bi- w?1w0, w0w1, m?1w0, w0m1
-gram w?1m0, m0w1, m?1m0, m0m1
tri- w?1w0w1, m?1w0w1, w?1m0w1, m?1m0w1
-gram w?1w0m1, m?1w0m1, w?1m0m1, m?1m0m1
Table 2: The templates for generating potentially deter-
ministic constraints of English POS tagging.
bigram constraint includes one contextual word
(w?1|w1) or the corresponding morph feature; and
a trigram constraint includes both contextual words
or their morph features. Each constraint is also con-
joined with w0 or m0, as described in Table 2.
2.1.2 Learning of deterministic constraints
In the above section, we explore templates for
potentially deterministic constraints that may deter-
mine POS category. With respect to a training cor-
pus, if a constraint C relative to w0 ?always? assigns
a certain POS category t? to w0 in its context, i.e.
count(C?t0=t?)
count(C) > thr, and this constraint occurs
more than a cutoff number, we consider it as a de-
terministic constraint. The threshold thr is a real
number just under 1.0 and the cutoff number is em-
pirically set to 5 in our experiments.
2.1.3 Decoding of deterministic constraints
By the above definition, the constraint of w?1 =
the,m0 = {NNS, VBZ} andw1 = of is determinis-
tic. It determines the POS category of w0 to be NNS.
There are at least two ways of decoding these con-
straints during POS tagging. Take the word trades
for example, whose morph feature is {NNS, VBZ}.
One alternative is that as long as trades occurs be-
tween the-of, it is tagged with NNS. The second al-
ternative is that the tag decision is made only if all
deterministic constraints relative to this occurrence
of trades agree on the same tag. Both ways of de-
coding are purely rule-based and involve no proba-
bilistic inference. In favor of a higher precision, we
adopt the latter one in our experiments.
1055
raw input O(nT 2) n = 23
The complex financing plan in the S&L bailout law includes...
constrained input O(m1T + m2T 2) m1 = 2,m2 = 1
The/DT complex/? financing/? plan/NN in/IN
the/DT S&L/? bailout/NN law/NN includes/VBZ ...
Table 3: Comparison of raw input and constrained input.
2.2 Search in a constrained space
Following most previous work, we consider POS
tagging as a sequence classification problem and de-
compose the overall sequence score over the linear
structure, i.e. t? = argmax
t?tagGEN(w)
n?
i=1
score(ti) where
function tagGEN maps input sentence w = w1...wn
to the set of all tag sequences that are of length n.
If a POS tagger takes raw input only, i.e. for every
word, the number of possible tags is a constant T ,
the space of tagGEN is as large as Tn. On the other
hand, if we decode deterministic constraints first be-
fore a probabilistic search, i.e. for some words, the
number of possible tags is reduced to 1, the search
space is reduced to Tm, where m is the number of
(unconstrained) words that are not subject to any de-
terministic constraints.
Viterbi algorithm is widely used for tagging, and
runs in O(nT 2) when searching in an unconstrained
space. On the other hand, consider searching in a
constrained space. Suppose that among the m un-
constrained words, m1 of them follow a word that
has been tagged by deterministic constraints and
m2 (=m-m1) of them follow another unconstrained
word. Viterbi decoder runs in O(m1T + m2T 2)
while searching in such a constrained space. The
example in Table 3 shows raw and constrained input
with respect to a typical input sentence.
Lookahead features
The score of tag predictions are usually computed
in a high-dimensional feature space. We adopt the
basic feature set used in (Ratnaparkhi, 1996) and
(Collins, 2002). Moreover, when deterministic con-
straints have applied to contextual words of w0, it
is also possible to include some lookahead feature
templates, such as:
t0&t1, t0&t1&t2, and t?1&t0&t1
where ti represents the tag of the ith word relative
to the current word w0. As discussed in (Shen et
al., 2007), categorical information of neighbouring
words on both sides of w0 help resolve POS ambi-
guity of w0. In (Shen et al, 2007), lookahead fea-
tures may be available for use during decoding since
searching is bidirectional instead of left-to-right as
in Viterbi decoding. In this work, deterministic con-
straints are decoded before the application of prob-
abilistic models, therefore lookahead features are
made available during Viterbi decoding.
3 Chinese Word Segmentation (CWS)
3.1 Word segmentation as character tagging
Considering the ambiguity problem that a Chinese
character may appear in any relative position in a
word and the out-of-vocabulary (OOV) problem that
it is impossible to observe all words in training data,
CWS is widely formulated as a character tagging
problem (Xue, 2003). A character-based CWS de-
coder is to find the highest scoring tag sequence t?
over the input character sequence c, i.e.
t? = argmax
t?tagGEN(c)
n?
i=1
score(ti) .
This is the same formulation as POS tagging. The
Viterbi algorithm is also widely used for decoding.
The tag of each character represents its relative
position in a word. Two popular tagsets include 1)
IB: where B tags the beginning of a word and I
all other positions; and 2) BMES: where B, M and E
represent the beginning, middle and end of a multi-
character word respectively, and S tags a single-
character word. For example, after decoding with
BMES, 4 consecutive characters associated with the
tag sequence BMME compose a word. However, after
decoding with IB, characters associated with BIII
may compose a word if the following tag is B or only
form part of a word if the following tag is I. Even
though character tagging accuracy is higher with
tagset IB, tagset BMES is more popular in use since
better performance of the original problem CWS can
be achieved by this tagset.
Character-based feature templates
We adopt the ?non-lexical-target? feature tem-
plates in (Jiang et al, 2008a). Let ci denote the ith
character relative to the current character c0 and t0
1056
denote the tag assigned to c0. The following tem-
plates are used:
ci&t0 (i=-2...2), cici+1&t0 (i=-2...1) and c?1c1&t0.
Character-based deterministic constraints
We can use the same templates as described in
Table 2 to generate potentially deterministic con-
straints for CWS character tagging, except that there
are no morph features computed for Chinese char-
acters. As we will show with experimental results
in Section 5.2, useful deterministic constraints for
CWS can be learned with tagset IB but not with
tagset BMES. It is interesting but not surprising to no-
tice, again, that the determinacy of a problem is sen-
sitive to its representation. Since it is hard to achieve
the best segmentations with tagset IB, we propose
an indirect way to use these constraints in the fol-
lowing section, instead of applying these constraints
as straightforwardly as in English POS tagging.
3.2 Word-based word segmentation
A word-based CWS decoder finds the highest scor-
ing segmentation sequence w? that is composed by
the input character sequence c, i.e.
w? = argmax
w?segGEN(c)
|w|?
i=1
score(wi) .
where function segGEN maps character sequence c
to the set of all possible segmentations of c. For
example, w = (c1..cl1)...(cn?lk+1...cn) represents a
segmentation of k words and the lengths of the first
and last word are l1 and lk respectively.
In early work, rule-based models find words one
by one based on heuristics such as forward maxi-
mum match (Sproat et al, 1996). Exact search is
possible with a Viterbi-style algorithm, but beam-
search decoding is more popular as used in (Zhang
and Clark, 2007) and (Jiang et al, 2008a).
We propose an Integer Linear Programming (ILP)
formulation of word segmentation, which is nat-
urally viewed as a word-based model for CWS.
Character-based deterministic constraints, as dis-
cussed in Section 3.1, can be easily applied.
3.3 ILP formulation of CWS
Given a character sequence c=c1...cn, there are s(=
n(n+1)/2) possible words that are contiguous sub-
sets of c, i.e. w1, ..., ws ? c. Our goal is to find
Table 4: Comparison of raw input and constrained input.
an optimal solution x = x1...xs that maximizes
s?
i=1
score(wi) ? xi, subject to
(1)
?
i:c?wi
xi = 1, ?c ? c;
(2) xi ? {0, 1}, 1 ? i ? s
The boolean value of xi, as guaranteed by constraint
(2), indicates whether wi is selected in the segmen-
tation solution or not. Constraint (1) requires ev-
ery character to be included in exactly one selected
word, thus guarantees a proper segmentation of the
whole sequence. This resembles the ILP formula-
tion of the set cover problem, though the first con-
straint is different. Take n = 2 for example, i.e.
c = c1c2, the set of possible words is {c1, c2, c1c2},
i.e. s = |x| = 3. There are only two possible so-
lutions subject to constraints (1) and (2), x = 110
giving an output set {c1, c2}, or x = 001 giving an
output set {c1c2}.
The efficiency of solving this problem depends on
the number of possible words (contiguous subsets)
over a character sequence, i.e. the number of vari-
ables in x. So as to reduce |x|, we apply determin-
istic constraints predicting IB tags first, which are
learned as described in Section 3.1. Possible words
are generated with respect to the partially tagged
character sequence. A character tagged with B al-
ways occurs at the beginning of a possible word. Ta-
ble 4 illustrates the constrained and raw input with
respect to a typical character sequence.
3.4 Character- and word-based features
As studied in previous work, word-based feature
templates usually include the word itself, sub-words
contained in the word, contextual characters/words
and so on. It has been shown that combining the
use of character- and word-based features helps im-
prove performance. However, in the character tag-
ging formulation, word-based features are non-local.
1057
To incorporate these non-local features and make the
search tractable, various efforts have been made. For
example, Jiang et al (2008a) combine different lev-
els of knowledge in an outside linear model of a two-
layer cascaded model; Jiang et al (2008b) uses the
forest re-ranking technique (Huang, 2008); and in
(Kruengkrai et al, 2009), only known words in vo-
cabulary are included in the hybrid lattice consisting
of both character- and word-level nodes.
We propose to incorporate character-based fea-
tures in word-based models. Consider a character-
based feature function ?(c, t, c) that maps a
character-tag pair to a high-dimensional feature
space, with respect to an input character sequence
c. For a possible word over c of length l , wi =
ci0 ...ci0+l?1, tag each character cij in this word with
a character-based tag tij . Character-based features
of wi can be computed as {?(cij , tij , c)|0 ? j < l}.
The first row of Table 5 illustrates character-based
features of a word of length 3, which is tagged with
tagset BMES. From this view, the character-based
feature templates defined in Section 3.1 are naturally
used in a word-based model.
When character-based features are incorporated
into word-based CWS models, some word-based
features are no longer of interest, such as the start-
ing character of a word, sub-words contained in
the word, contextual characters and so on. We
consider word counting features as a complemen-
tary to character-based features, following the idea
of using web-scale features in previous work, e.g.
(Bansal and Klein, 2011). For a possible word w, let
count(w) return the count of times that w occurs as
a legal word in training data. The word count num-
ber is further processed following (Bansal and Klein,
2011), wc(w) = floor(log(count(w)) ? 5)/5. In
addition to wc(wi), we also use corresponding word
count features of possible words that are composed
of the boundary and contextual characters ofwi. The
specific word-based feature templates are illustrated
in the second row of Table 5.
4 Training
We use the following linear model for scoring pre-
dictions: score(y)=?T?(x, y), where ?(y) is a high-
dimensional binary feature representation of y over
input x and ? contains weights of these features. For
character-
?(ci0 , B, c), ?(ci1 , M, c), ?(ci2 , E, c)-based
word-
wc(ci0ci1ci2), wc(clci0), wc(ci2cr)-based
Table 5: Character- and word-based features of a possi-
ble wordwi over the input character sequence c. Suppose
thatwi = ci0ci1ci2 , and its preceding and following char-
acters are cl and cr respectively.
parameter estimation of ?, we use the averaged per-
ceptron as described in (Collins, 2002). This train-
ing algorithm relies on the choice of decoding algo-
rithm. When we experiment with different decoders,
by default, the parameter weights in use are trained
with the corresponding decoding algorithm.
Especially, for experiments with lookahead fea-
tures of English POS tagging, we prepare training
data with the stacked learning technique, in order to
alleviate overfitting. More specifically, we divide the
training data into k folds, and tag each fold with the
deterministic model learned over the other k-1 folds.
The predicted tags of all folds are then merged into
the gold training data and used (only) as lookahead
features. Sun (2011) uses this technique to merge
different levels of predictors for word segmentation.
5 Experiments
5.1 Data set
We run experiments on English POS tagging on the
WSJ corpus in the Penn Treebank. Following most
previous work, e.g. (Collins, 2002) and (Shen et al,
2007), we divide this corpus into training set (sec-
tions 0-18), development set (sections 19-21) and
the final test set (sections 22-24).
We run experiments on Chinese word segmenta-
tion on the Penn Chinese Treebank 5.0. Following
(Jiang et al, 2008a), we divide this corpus into train-
ing set (chapters 1-260), development set (chapters
271-300) and the final test set (chapters 301-325).
5.2 Deterministic constraints
Experiments in this section are carried out on the de-
velopment set. The cutoff number and threshold as
defined in 2.1.2, are fixed as 5 and 0.99 respectively.
1058
precision recall F1
bigram 0.993 0.841 0.911
trigram 0.996 0.608 0.755
bi+trigram 0.992 0.857 0.920
Table 6: POS tagging with deterministic constraints.
The maximum in each column is bold.
m0={VBN, VBZ} & m1={JJ, VBD, VBN} ? VBN
w0=also & m1={VBD, VBN} ? RB
m0=?es & m?1={IN, RB, RP} ? NNS
w0=last & w?1= the? JJ
Table 7: Deterministic constraints for POS tagging.
Deterministic constraints for POS tagging
For English POS tagging, we evaluate the deter-
ministic constraints generated by the templates de-
scribed in Section 2.1.1. Since these deterministic
constraints are only applied to words that occur in
a constrained context, we report F-measure as the
accuracy measure. Precision p is defined as the per-
centage of correct predictions out of all predictions,
and recall r is defined as the percentage of gold pre-
dictions that are correctly predicted. F-measure F1
is computed by 2pr/(p+ r).
As shown in Table 6, deterministic constraints
learned with both bigram and trigram templates are
all very accurate in predicting POS tags of words
in their context. Constraints generated by bigram
template alone can already cover 84.1% of the input
words with a high precision of 0.993. By adding the
constraints generated by trigram template, recall is
increased to 0.857 with little loss in precision. Since
these deterministic constraints are applied before the
decoding of probabilistic models, reliably high pre-
cision of their predictions is crucial.
There are 114589 bigram deterministic con-
straints and 130647 trigram constraints learned from
the training data. We show a couple of examples of
bigram deterministic constraints in Table 7. As de-
fined in Section 2.2, we use the set of all possible
POS tags for a word, e.g. {VBN, VBZ}, as its morph
feature if the word is frequent (occurring more than
5 times in training data). For a rare word, the last two
characters are used as its morph feature, e.g. ?es. A
constraint is composed of w?1, w0 and w1, as well
as the morph features m?1, m0 and m1. For ex-
tagset precision recall F1
BMES 0.989 0.566 0.720
IB 0.996 0.686 0.812
Table 8: Character tagging with deterministic constraints.
ample, the first constraint in Table 7 determines the
tag VBN of w0. A deterministic constraint is aware
of neither the likelihood of each possible tag or the
relative rank of their likelihoods.
Deterministic constraints for character tagging
For the character tagging formulation of Chinese
word segmentation, we discussed two tagsets IB and
BMES in Section 3.1. With respect to either tagset,
we use both bigram and trigram templates to gen-
erate deterministic constraints for the corresponding
tagging problem. These constraints are also evalu-
ated by F-measure as defined above. As shown in
Table 8, when tagset IB is used for character tag-
ging, high precision predictions can be made by the
deterministic constraints that are learned with re-
spect to this tagset. However, when tagset BMES is
used, the learned constraints don?t always make reli-
able predictions, and the overall precision is not high
enough to constrain a probabilistic model. There-
fore, we will only use the deterministic constraints
that predict IB tags in following CWS experiments.
5.3 English POS tagging
For English POS tagging, as well as the CWS prob-
lem that will be discussed in the next section, we use
the development set to choose training iterations (=
5), set beam width etc. The following experiments
are done on the final test set.
As introduced in Section 2.2, we adopt a very
compact feature set used in (Ratnaparkhi, 1996)1.
While searching in a constrained space, we can also
extend this feature set with some basic lookahead
features as defined in Section 2.2. This replicates
the feature set B used in (Shen et al, 2007).
In this work, our main interest in the POS tag-
ging problem is on its efficiency. A well-known
technique to speed up Viterbi decoding is to con-
duct beam search. Based on experiments carried out
1Our implementation of this feature set is basically the same
as the version used in (Collins, 2002).
1059
Ratnaparkhi (1996)?s feature
Beam=1 Beam=5
raw 96.46%/3? 97.16/1?
constrained 96.80%/14? 97.20/10?
Feature B in (Shen et al, 2007)
(Shen et al, 2007) 97.15% (Beam=3)
constrained 97.03%/11? 97.20/8?
Table 9: POS tagging accuracy and speed. The maximum
in each column is bold. The baseline for speed in all cases
is the unconstrained tagger using (Ratnaparkhi, 1996)?s
feature and conducting a beam (=5) search.
on the development set, we set beam-width of our
baseline model as 5. Our baseline model, which
uses Ratnaparkhi (1996)?s feature set and conducts
a beam (=5) search in the unconstrained space,
achieves a tagging accuracy of 97.16%. Tagging
accuracy is measured by the percentage of correct
predictions out of all gold predictions. We consider
the speed of our baseline model as 1?, and compare
other taggers with this one. The speed of a POS tag-
ger is measured by the number of input words pro-
cessed per second.
As shown in Table 9, when the beam-width is re-
duced from 5 to 1, the tagger (beam=1) is 3 times
faster but tagging accuracy is badly hurt. In contrast,
when searching in a constrained space rather than
the raw space, the constrained tagger (beam=5) is 10
times fast as the baseline and the tagging accuracy
is even moderately improved, increasing to 97.20%.
When we evaluate the speed of a constrained tag-
ger, the time of decoding deterministic constraints
is included. These constraints make more accurate
predictions than probabilistic models, thus besides
improving the overall tagging speed as we expect,
tagging accuracy also improves by a little.
In Viterbi decoding, all possible transitions be-
tween two neighbour states are evaluated, so the ad-
dition of locally lookahead features may have NO
impact on performance. When beam-width is set to
5, tagging accuracy is not improved by the use of
Feature B in (Shen et al, 2007); and because the
size of the feature model grows, efficiency is hurt.
On the other hand, when lookahead features are
used, Viterbi-style decoding is less affected by the
reduction of beam-width. As compared to the con-
strained greedy tagger using Ratnaparkhi (1996)?s
feature set, with the additional use of three locally
lookahead feature templates, tagging accuracy is in-
creased from 96.80% to 97.02%.
When no further data is used other than training
data, the bidirectional tagger described in (Shen et
al., 2007) achives an accuracy of 97.33%, using a
much richer feature set (E) than feature set B, the
one we compare with here. As noted above, the
addition of three feature templates already has a
notable negative impact on efficiency, thus the use
of feature set E will hurt tagging efficiency much
worse. Rich feature sets are also widely used in
other work that pursue state-of-art tagging accuracy,
e.g. (Toutanova et al, 2003). In this work, we fo-
cus on the most compact feature sets, since tagging
efficiency is our main consideration in our work on
POS taging. The proposed constrained taggers as
described above can achieve near state-of-art POS
tagging accuracy in a much more efficient manner.
5.4 Chinese word segmentation
Like other tagging problems, Viterbi-style decoding
is widely used for character tagging for CWS. We
transform tagged character sequences to word seg-
mentations first, and then evaluate word segmenta-
tions by F-measure, as defined in Section 5.2.
We proposed an ILP formulation of the CWS
problem in Section 3.3, where we present a word-
based model. In Section 3.4, we describe a way of
mapping words to a character-based feature space.
From this view, the highest scoring tagging sequence
is computed subject to structural constraints, giving
us an inference alternative to Viterbi decoding. For
example, recall the example of input character se-
quence c = c1c2 discussed in Section 3.3. The two
possible ILP solutions give two possible segmenta-
tions {c1, c2} and {c1c2}, thus there are 2 tag se-
quences evaluated by ILP, BB and BI. On the other
hand, there are 4 tag sequences evaluated by Viterbi
decoding: BI, BB, IB and II.
With the same feature templates as described in
Section 3.1, we now compare these two decoding
methods. Tagset BMES is used for character tagging
as well as for mapping words to character-based fea-
ture space. We use the same Viterbi decoder as im-
plemented for English POS tagging and use a non-
commercial ILP solver included in GNU Linear Pro-
1060
precision recall F-measure
Viterbi 0.971 0.966 0.968
ILP 0.970 0.977 0.974
(Jiang et al, 2008a), POS- 0.971
(Jiang et al, 2008a), POS+ 0.973
Table 10: F-measure on Chinese word segmentation.
Only character-based features are used. POS-/+: percep-
tron trained without/with POS.
gramming Kit (GLPK), version 4.3. 2 As shown
in Table 10, optimal solutions returned by an ILP
solver are more accurate than optimal solutions re-
turned by a Viterbi decoder. The F-measure is im-
proved by a relative error reduction of 18.8%, from
0.968 to 0.974. These results are compared to the
core perceptron trained without POS in (Jiang et al,
2008a). They only report results with ?lexical-target?
features, a richer feature set than the one we use
here. As shown in Table 10, we achieve higher per-
formance even with more compact features.
Joint inference of CWS and Chinese POS tagging
is popularly studied in recent work, e.g. (Ng and
Low, 2004), (Jiang et al, 2008a), and (Kruengkrai et
al., 2009). It has been shown that better performance
can be achieved with joint inference, e.g. F-measure
0.978 by the cascaded model in (Jiang et al, 2008a).
We focus on the task of word segmentation only in
this work and show that a comparable F-measure is
achievable in a much more efficient manner. Sun
(2011) uses the stacked learning technique to merge
different levels of predictors, obtaining a combined
system that beats individual ones.
Word-based features can be easily incorporated,
since the ILP formulation is more naturally viewed
as a word-based model. We extend character-based
features with the word count features as described
in Section 3.4. Currently, we only use word counts
computed from training data, i.e. still a closed test.
The addition of these features makes a moderate im-
provement on the F-measure, from 0.974 to 0.975.
As discussed in Section 3.3, if we are able to
determine that some characters always start new
words, the number of possible words is reduced,
i.e. the number of variables in an ILP solution is
reduced. As shown in Table 11, when character se-
2http://www.gnu.org/software/glpk
F-measure avg. |x| #char per sec.
raw 0.974 1290.4 113 (1?)
constrained 0.974 83.75 12190 (107?)
Table 11: ILP problem size and segmentation speed.
quences are partially tagged by deterministic con-
straints, the number of possible words per sentence,
i.e. avg. |x|, is reduced from 1290.4 to 83.7. This re-
duction of ILP problem size has a very important im-
pact on the efficiency. As shown in Table 11, when
taking constrained input, the segmentation speed is
increased by 107 times over taking raw input, from
113 characters per second to 12,190 characters per
second on a dual-core 3.0HZ CPU.
Deterministic constraints predicting IB tags are
only used here for constraining possible words.
They are very accurate as shown in Section 5.2. Few
gold predictions are missed from the constrained set
of possible words. As shown in Table 11, F-measure
is not affected by applying these constraints, while
the efficiency is significantly improved.
6 Conclusion and future work
We have shown by experiments that large number of
deterministic constraints can be learned from train-
ing examples, as long as the proper representation is
used. These deterministic constraints are very use-
ful in constraining probabilistic search, for example,
they may be directly used for determining predic-
tions as in English POS tagging, or used for reduc-
ing the number of variables in an ILP solution as in
Chinese word segmentation. The most notable ad-
vantage in using these constraints is the increased ef-
ficiency. The two applications are both well-studied;
there isn?t much space for improving accuracy. Even
so, we have shown that as tested with the same fea-
ture set for CWS, the proposed ILP formulation sig-
nificantly improves the F-measure as compared to
Viterbi decoding.
These two simple applications suggest that it is
of interest to explore data-driven deterministic con-
straints learnt from training examples. There are
more interesting ways in applying these constraints,
which we are going to study in future work.
1061
References
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, pages
693?702.
Noam Chomsky. 1970. Remarks on nominalization.
In R Jacobs and P Rosenbaum, editors, Readings in
English Transformational Grammar, pages 184?221.
Ginn.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, EMNLP ?02, pages 1?8.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics.
W. Jiang, L. Huang, Q. Liu, and Y. Lu?. 2008a. A cas-
caded linear model for joint chinese word segmenta-
tion and part-of-speech tagging. In In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics.
W. Jiang, H. Mi, and Q. Liu. 2008b. Word lattice rerank-
ing for chinese word segmentation and part-of-speech
tagging. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 385?392.
T. Kristjansson, A. Culotta, and P. Viola. 2004. Inter-
active information extraction with constrained condi-
tional random fields. In In AAAI, pages 412?418.
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang,
K. Torisawa, and H. Isahara. 2009. An error-driven
word-character hybrid model for joint chinese word
segmentation and pos tagging. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, ACL ?09,
pages 513?521.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational lin-
guistics, 19(2):313?330.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP), pages
342?350, Singapore.
H. T. Ng and J. K. Low. 2004. Chinese partof-speech
tagging: One-at-a-time or all-at-once? word-based or
character-based? In In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), page 277C284.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In In Proceedings of the Em-
pirical Methods in Natural Language Processing Con-
ference (EMNLP).
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proc. ACL.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 737?744.
L. Shen, G. Satta, and A. K. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics.
R. Sproat, W. Gale, C. Shih, and N. Chang. 1996.
A stochastic finite-state word-segmentation algorithm
for chinese. Comput. Linguist., 22(3):377?404.
W. Sun. 2011. A stacked sub-word model for joint chi-
nese word segmentation and part-of-speech tagging.
In Proceedings of the ACL-HLT 2011.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL-2003.
N. Xue. 2003. Chinese word segmentation as character
tagging. International Journal of Computational Lin-
guistics and Chinese Language Processing, 9(1):29?
48.
Y. Zhang and S. Clark. 2007. Chinese Segmentation with
a Word-Based Perceptron Algorithm. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 840?847.
1062
