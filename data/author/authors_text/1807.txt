Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96?103,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Phonological Constraints and Morphological Preprocessing for
Grapheme-to-Phoneme Conversion
Vera Demberg
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
v.demberg@sms.ed.ac.uk
Helmut Schmid
IMS
University of Stuttgart
D-70174 Stuttgart
schmid@ims.uni-stuttgart.de
Gregor Mo?hler
Speech Technologies
IBM Deutschland Entwicklung
D-71072 Bo?blingen
moehler@de.ibm.com
Abstract
Grapheme-to-phoneme conversion (g2p) is a
core component of any text-to-speech sys-
tem. We show that adding simple syllab-
ification and stress assignment constraints,
namely ?one nucleus per syllable? and ?one
main stress per word?, to a joint n-gram
model for g2p conversion leads to a dramatic
improvement in conversion accuracy.
Secondly, we assessed morphological pre-
processing for g2p conversion. While mor-
phological information has been incorpo-
rated in some past systems, its contribution
has never been quantitatively assessed for
German. We compare the relevance of mor-
phological preprocessing with respect to the
morphological segmentation method, train-
ing set size, the g2p conversion algorithm,
and two languages, English and German.
1 Introduction
Grapheme-to-Phoneme conversion (g2p) is the task
of converting a word from its spelling (e.g. ?Stern-
aniso?l?, Engl: star-anise oil) to its pronunciation
(/"StERnPani:sP?:l/). Speech synthesis modules with
a g2p component are used in text-to-speech (TTS)
systems and can be be applied in spoken dialogue
systems or speech-to-speech translation systems.
1.1 Syllabification and Stress in g2p conversion
In order to correctly synthesize a word, it is not only
necessary to convert the letters into phonemes, but
also to syllabify the word and to assign word stress.
The problems of word phonemization, syllabifica-
tion and word stress assignment are inter-dependent.
Information about the position of a syllable bound-
ary helps grapheme-to-phoneme conversion. (Marc-
hand and Damper, 2005) report a word error rate
(WER) reduction of approx. 5 percentage points for
English when the letter string is augmented with syl-
labification information. The same holds vice-versa:
we found that WER was reduced by 50% when run-
ning our syllabifier on phonemes instead of letters
(see Table 4). Finally, word stress is usually defined
on syllables; in languages where word stress is as-
sumed1 to partly depend on syllable weight (such as
German or Dutch), it is important to know where ex-
actly the syllable boundaries are in order to correctly
calculate syllable weight. For German, (Mu?ller,
2001) show that information about stress assignment
and the position of a syllable within a word improve
g2p conversion.
1.2 Morphological Preprocessing
It has been argued that using morphological in-
formation is important for languages where mor-
phology has an important influence on pronuncia-
tion, syllabification and word stress such as Ger-
man, Dutch, Swedish or, to a smaller extent, also
English (Sproat, 1996; Mo?bius, 2001; Pounder and
Kommenda, 1986; Black et al, 1998; Taylor, 2005).
Unfortunately, these papers do not quantify the con-
tribution of morphological preprocessing in the task.
Important questions when considering the inte-
gration of a morphological component into a speech
1This issue is controversial among linguists; for an overview
see (Jessen, 1998).
96
synthesis system are 1) How large are the im-
provements to be gained from morphological pre-
processing? 2) Must the morphological system be
perfect or can performance improvements also be
reached with relatively simple morphological com-
ponents? and 3) How much does the benefit to
be expected from explicit morphological informa-
tion depend on the g2p algorithm? To determine
these factors, we compared morphological segmen-
tations based on manual morphological annotation
from CELEX to two rule-based systems and several
unsupervised data-based approaches. We also anal-
ysed the role of explicit morphological preprocess-
ing on data sets of different sizes and compared its
relevance with respect to a decision tree and a joint
n-gram model for g2p conversion.
The paper is structured as follows: We introduce
the g2p conversion model we used in section 2 and
explain how we implemented the phonological con-
straints in section 3. Section 4 is concerned with
the relation between morphology, word pronuncia-
tion, syllabification and word stress in German, and
presents different sources for morphological seg-
mentation. In section 5, we evaluate the contribution
of each of the components and compare our meth-
ods to state-of-the-art systems. Section 6 summa-
rizes our results.
2 Methods
We used a joint n-gram model for the grapheme-
to-phoneme conversion task. Models of this type
have previously been shown to yield very good g2p
conversion results (Bisani and Ney, 2002; Galescu
and Allen, 2001; Chen, 2003). Models that do not
use joint letter-phoneme states, and therefore are not
conditional on the preceding letters, but only on the
actual letter and the preceding phonemes, achieved
inferior results. Examples of such approaches using
Hidden Markov Models are (Rentzepopoulos and
Kokkinakis, 1991) (who applied the HMM to the
related task of phoneme-to-grapheme conversion),
(Taylor, 2005) and (Minker, 1996).
The g2p task is formulated as searching for the
most probable sequence of phonemes given the or-
thographic form of a word. One can think of it as a
tagging problem where each letter is tagged with a
(possibly empty) phoneme-sequence p. In our par-
ticular implementation, the model is defined as a
higher-order Hidden Markov Model, where the hid-
den states are a letter?phoneme-sequence pair ?l; p?,
and the observed symbols are the letters l. The out-
put probability of a hidden state is then equal to one,
since all hidden states that do not contain the ob-
served letter are pruned directly.
The model for grapheme-to-phoneme conver-
sion uses the Viterbi algorithm to efficiently com-
pute the most probable sequence p?n1 of phonemes
p?1, p?2, ..., p?n for a given letter sequence ln1 . The
probability of a letter?phon-seq pair depends on the
k preceding letter?phon-seq pairs. Dummy states ?#?
are appended at both ends of each word to indicate
the word boundary and to ensure that all conditional
probabilities are well-defined.
p?n1 = argmax
pn1
n+1?
i=1
P (?l; p?i | ?l; p?
i?1
i?k)
In an integrated model where g2p conversion, syl-
labification and word stress assignment are all per-
formed at the same time, a state additionally con-
tains a syllable boundary flag b and a stress flag a,
yielding ?l; p; b; a?i.
As an alternative architecture, we also designed a
modular system that comprises one component for
syllabification and one for word stress assignment.
The model for syllabification computes the most
probable sequence b?n1 of syllable boundary-tags b?1,
b?2, ..., b?n for a given letter sequence ln1 .
b?n1 = argmax
bn1
n+1?
i=1
P (?l; b?i | ?l; b?
i?1
i?k)
The stress assignment model works on syllables.
It computes the most probable sequence a?n1 of word
accent-tags a?1, a?2, ..., a?n for a given syllable se-
quence syln1 .
a?n1 = argmax
an1
n+1?
i=1
P (?syl; a?i | ?syl; a?
i?1
i?k)
2.1 Smoothing
Because of major data sparseness problems, smooth-
ing is an important issue, in particular for the stress
model which is based on syllable?stress-tag pairs.
Performance varied by up to 20% in function of the
smoothing algorithm chosen. Best results were ob-
tained when using a variant of Modified Kneser-Ney
Smoothing2 (Chen and Goodman, 1996).
2For a formal definition, see(Demberg, 2006).
97
2.2 Pruning
In the g2p-model, each letter can on average map
onto one of 12 alternative phoneme-sequences.
When working with 5-grams3, there are about 125 =
250,000 state sequences. To improve time and space
efficiency, we implemented a simple pruning strat-
egy that only considers the t best states at any mo-
ment in time. With a threshold of t = 15, about 120
words are processed per minute on a 1.5GHz ma-
chine. Conversion quality is only marginally worse
than when the whole search space is calculated.
Running time for English is faster, because the av-
erage number of candidate phonemes for each let-
ter is lower. We measured running time (including
training and the actual g2p conversion in 10-fold
cross validation) for a Perl implementation of our
algorithm on the English NetTalk corpus (20,008
words) on an Intel Pentium 4, 3.0 GHz machine.
Running time was less than 1h for each of the fol-
lowing three test conditions: c1) g2p conversion
only, c2) syllabification first, then g2p conversion,
c3) simultaneous g2p conversion and syllabification,
given perfect syllable boundary input, c4) simulta-
neous g2p conversion and syllabification when cor-
rect syllabification is not available beforehand. This
is much faster than the times for Pronunciation by
Analogy (PbA) (Marchand and Damper, 2005) on
the same corpus. Marchand and Damper reported a
processing time of several hours for c4), two days
for c2) and several days for c3).
2.3 Alignment
Our current implementation of the joint n-gram
model is not integrated with an automatic alignment
procedure. We therefore first aligned letters and
phonemes in a separate, semi-automatic step. Each
letter was aligned with zero to two phonemes and,
in the integrated model, zero or one syllable bound-
aries and stress markers.
3 Integration of Phonological Constraints
When analysing the results from the model that does
g2p conversion, syllabification and stress assign-
3There is a trade-off between long context windows which
capture the context accurately and data sparseness issues. The
optimal value k for the context window size depends on the
source language (existence of multiletter graphemes, complex-
ity of syllables etc.).
ment in a single step, we found that a large propor-
tion of the errors was due to the violation of basic
phonological constraints.
Some syllables had no syllable nucleus, while
others contained several vowels. The reason for the
errors is that German syllables can be very long and
therefore sparse, often causing the model to back-
off to smaller contexts. If the context is too small to
cover the syllable, the model cannot decide whether
the current syllable contains a nucleus.
In stress assignment, this problem is even worse:
the context window rarely covers the whole word.
The algorithm does not know whether it already as-
signed a word stress outside the context window.
This leads to a high error rate with 15-20% of in-
correctly stressed words. Thereof, 37% have more
than one main stress, about 27% are not assigned any
stress and 36% are stressed in the wrong position.
This means that we can hope to reduce the errors by
almost 2/3 by using phonological constraints.
Word stress assignment is a difficult problem in
German because the underlying processes involve
some deeper morphological knowledge which is not
available to the simple model. In complex words,
stress mainly depends on morphological structure
(i.e. on the compositionality of compounds and
on the stressing status of affixes). Word stress in
simplex words is assumed to depend on the sylla-
ble position within the word stem and on syllable
weight. The current language-independent approach
does not model these processes, but only captures
some of its statistics.
Simple constraints can help to overcome the prob-
lem of lacking context by explicitly requiring that
every syllable must have exactly one syllable nu-
cleus and that every word must have exactly one syl-
lable receiving primary stress.
3.1 Implementation
Our goal is to find the most probable syllabified
and stressed phonemization of a word that does not
violate the constraints. We tried two different ap-
proaches to enforce the constraints.
In the first variant (v1), we modified the proba-
bility model to enforce the constraints. Each state
now corresponds to a sequence of 4-tuples consist-
ing of a letter l, a phoneme sequence p, a syllable
boundary tag b, an accent tag a (as before) plus two
98
new flags A and N which indicate whether an ac-
cent/nucleus precedes or not. The A and N flags of
the new state are a function of its accent and syllable
boundary tag and the A and N flag of the preceding
state. They split each state into four new states. The
new transition probabilities are defined as:
P (?l; p; b; a?i | ?l; p; b; a?
i?1
i?k , A,N)
The probability is 0 if the transition violates a con-
straint, e.g., when the A flag is set and ai indicates
another accent.
A positive side effect of the syllable flag is that it
stores separate phonemization probabilities for con-
sonants in the syllable onset vs. consonants in the
coda. The flag in the onset is 0 since the nucleus has
not yet been encountered, whereas it is set to 1 in the
coda. In German, this can e.g. help in for syllable-
final devoicing of voiced stops and fricatives.
The increase in the number of states aggravates
sparse-data problems. Therefore, we implemented
another variant (v2) which uses the same set of states
(with A and N flags), but with the transition proba-
bilities of the original model, which did not enforce
the constraints. Instead, we modified the Viterbi al-
gorithm to eliminate the invalid transitions: For ex-
ample, a transition from a state with the A flag set
to a state where ai introduces a second stress, is al-
ways ignored. On small data sets, better results were
achieved with v2 (see Table 5).
4 Morphological Preprocessing
In German, information about morphological
boundaries is needed to correctly insert glottal stops
[P] in complex words, to determine irregular pro-
nunciation of affixes (v is pronounced [v] in ver-
tikal but [f] in ver+ticker+n, and the suffix syllable
heit is not stressed although superheavy and word
final) and to disambiguate letters (e.g. e is always
pronounced /@/ when occurring in inflectional suf-
fixes). Vowel length and quality has been argued
to also depend on morphological structure (Pounder
and Kommenda, 1986). Furthermore, morphologi-
cal boundaries overrun default syllabification rules,
such as the maximum onset principle.
Applying default syllabification to the word
?Sternaniso?l? would result in a syllabification into
Ster-na-ni-so?l (and subsequent phonemiza-
tion to something like /StE?"na:niz?:l/) instead of
Stern-a-nis-o?l (/"StE?nPani:sP?:l/). Syllabifi-
cation in turn affects phonemization since voiced
fricatives and stops are devoiced in syllable-final po-
sition. Morphological information also helps for
graphemic parsing of words such as ?Ro?schen?
(Engl: little rose) where the morphological bound-
ary between Ro?s and chen causes the string sch to
be transcribed to /s?/ instead of /S/. Similar ambigui-
ties can arise for all other sounds that are represented
by several letters in orthography (e.g. doubled con-
sonants, diphtongs, ie, ph, th), and is also valid for
English. Finally, morphological information is also
crucial to determine word stress in morphologically
complex words.
4.1 Methods for Morphological Segmentation
Good segmentation performance on arbitrary words
is hard to achieve. We compared several approaches
with different amounts of built-in knowledge. The
morphological information is encoded in the let-
ter string, where different digits represent different
kinds of morphological boundaries (prefixes, stems,
derivational and inflectional suffixes).
Manual Annotation from CELEX
To determine the upper bound of what can be
achieved when exploiting perfect morphological in-
formation, we extracted morphological boundaries
and boundary types from the CELEX database.
The manual annotation is not perfect as it con-
tains some errors and many cases where words are
not decomposed entirely. The words tagged [F] for
?lexicalized inflection?, e.g. gedra?ngt (past partici-
ple of dra?ngen, Engl: push) were decomposed semi-
automatically for the purpose of this evaluation. As
expected, annotating words with CELEX morpho-
logical segmentation yielded the best g2p conver-
sion results. Manual annotation is only available for
a small number of words. Therefore, only automati-
cally annotated morphological information can scale
up to real applications.
Rule-based Systems
The traditional approach is to use large morpheme
lexica and a set of rules that segment words into af-
fixes and stems. Drawbacks of using such a system
are the high development costs, limited coverage
99
and problems with ambiguity resolution between al-
ternative analyses of a word.
The two rule-based systems we evaluated, the
ETI4 morphological system and SMOR5 (Schmid et
al., 2004), are both high-quality systems with large
lexica that have been developed over several years.
Their performance results can help to estimate what
can realistically be expected from an automatic seg-
mentation system. Both of the rule-based systems
achieved an F-score of approx. 80% morphological
boundaries correct with respect to CELEX manual
annotation.
Unsupervised Morphological Systems
Most attractive among automatic systems are
methods that use unsupervised learning, because
these require neither an expert linguist to build large
rule-sets and lexica nor large manually annotated
word lists, but only large amounts of tokenized
text, which can be acquired e.g. from the internet.
Unsupervised methods are in principle6 language-
independent, and can therefore easily be applied to
other languages.
We compared four different state-of-the-art unsu-
pervised systems for morphological decomposition
(cf. (Demberg, 2006; Demberg, 2007)). The algo-
rithms were trained on a German newspaper cor-
pus (taz), containing about 240 million words. The
same algorithms have previously been shown to help
a speech recognition task (Kurimo et al, 2006).
5 Experimental Evaluations
5.1 Training Set and Test Set Design
The German corpus used in these experiments is
CELEX (German Linguistic User Guide, 1995).
CELEX contains a phonemic representation of each
4Eloquent Technology, Inc. (ETI) TTS system.
http://www.mindspring.com/?ssshp/ssshp_cd/
ss_eloq.htm
5The lexicon used by SMOR, IMSLEX, contains morpho-
logically complex entries, which leads to high precision and low
recall. The results reported here refer to a version of SMOR,
where the lexicon entries were decomposed using a rather na??ve
high-recall segmentation method. SMOR itself does not disam-
biguate morphological analyses of a word. Our version used
transition weights learnt from CELEX morphological annota-
tion. For more details refer to (Demberg, 2006).
6Most systems make some assumptions about the underly-
ing morphological system, for instance that morphology is a
concatenative process, that stems have a certain minimal length
or that prefixing and suffixing are the most relevant phenomena.
word, syllable boundaries and word stress infor-
mation. Furthermore, it contains manually verified
morphological boundaries.
Our training set contains approx. 240,000 words
and the test set consists of 12,326 words. The test
set is designed such that word stems in training and
test sets are disjoint, i.e. the inflections of a certain
stem are either all in the training set or all in the test
set. Stem overlap between training and test set only
occurs in compounds and derivations. If a simple
random splitting (90% for training set, 10% for test
set) is used on inflected corpora, results are much
better: Word error rates (WER) are about 60% lower
when the set of stems in training and test set are not
disjoint. The same effect can also be observed for
the syllabification task (see Table 4).
5.2 Results for the Joint n-gram Model
The joint n-gram model is language-independent.
An aligned corpus with words and their pronuncia-
tions is needed, but no further adaptation is required.
Table 1 shows the performance of our model in
comparison to alternative approaches on the German
and English versions of the CELEX corpus, the En-
glish NetTalk corpus, the English Teacher?s Word
Book (TWB) corpus, the English beep corpus and
the French Brulex corpus. The joint n-gram model
performs significantly better than the decision tree
(essentially based on (Lucassen and Mercer, 1984)),
and achieves scores comparable to the Pronuncia-
tion by Analogy (PbA) algorithm (Marchand and
Damper, 2005). For the Nettalk data, we also com-
pared the influence of syllable boundary annotation
from a) automatically learnt and b) manually anno-
tated syllabification information on phoneme accu-
racy. Automatic syllabification for our model in-
tegrated phonological constraints (as described in
section 3.1), and therefore led to an improvement
in phoneme accuracy, while the word error rate in-
creased for the PbA approach, which does not incor-
porate such constraints.
(Chen, 2003) also used a joint n-gram model.
The two approaches differ in that Chen uses small
chunks (?(l : |0..1|) : (p : |0..1|)? pairs only) and it-
eratively optimizes letter-phoneme alignment during
training. Chen smoothes higher-order Markov Mod-
els with Gaussian Priors and implements additional
language modelling such as consonant doubling.
100
corpus size jnt n-gr PbA Chen dec.tree
G - CELEX 230k 7.5% 15.0%
E - Nettalk 20k 35.4% 34.65% 34.6%
a) auto.syll 35.3% 35.2%
b) man.syll 29.4% 28.3%
E - TWB 18k 28.5% 28.2%
E - beep 200k 14.3% 13.3%
E - CELEX 100k 23.7% 31.7%
F - Brulex 27k 10.9%
Table 1: Word error rates for different g2p conver-
sion algorithms. Constraints were only used in the
E-Nettalk auto. syll condition.
5.3 Benefit of Integrating Constraints
The accuracy improvements achieved by integrat-
ing the constraints (see Table 2) are highly statis-
tically significant. The numbers for conditions ?G-
syllab.+stress+g2p? and ?E-syllab.+g2p? in Table 2
differ from the numbers for ?G-CELEX? and ?E-
Nettalk? in Table 1 because phoneme conversion
errors, syllabification errors and stress assignment
errors are all counted towards word error rates re-
ported in Table 2.
Word error rate in the combined g2p-syllable-
stress model was reduced from 21.5% to 13.7%. For
the separate tasks, we observed similar effects: The
word error rate for inserting syllable boundaries was
reduced from 3.48% to 3.1% on letters and from
1.84% to 1.53% on phonemes. Most significantly,
word error rate was decreased from 30.9% to 9.9%
for word stress assignment on graphemes.
We also found similarly important improvements
when applying the syllabification constraint to En-
glish grapheme-to-phoneme conversion and syllabi-
fication. This suggests that our findings are not spe-
cific to German but that this kind of general con-
straints can be beneficial for a range of languages.
no constr. constraint(s)
G - syllab.+stress+g2p 21.5% 13.7%
G - syllab. on letters 3.5% 3.1%
G - syllab. on phonemes 1.84% 1.53%
G - stress assignm. on letters 30.9% 9.9%
E - syllab.+g2p 40.5% 37.5%
E - syllab. on phonemes 12.7% 8.8%
Table 2: Improving performance on g2p conver-
sion, syllabification and stress assignment through
the introduction of constraints. The table shows
word error rates for German CELEX (G) and En-
glish NetTalk (E).
5.4 Modularity
Modularity is an advantage if the individual compo-
nents are more specialized to their task (e.g. by ap-
plying a particular level of description of the prob-
lem, or by incorporating some additional source of
knowledge).In a modular system, one component
can easily be substituted by another ? for example,
if a better way of doing stress assignment in German
was found. On the other hand, keeping everything in
one module for strongly inter-dependent tasks (such
as determining word stress and phonemization) al-
lows us to simultaneously optimize for the best com-
bination of phonemes and stress.
Best results were obtained from the joint n-gram
model that does syllabification, stress assignment
and g2p conversion all in a single step and inte-
grates phonological constraints for syllabification
and word stress (WER = 14.4% using method v1,
WER = 13.7% using method v2). If the modular ar-
chitecture is chosen, best results are obtained when
g2p conversion is done before syllabification and
stress assignment (15.2% WER), whereas doing syl-
labification and stress assignment first and then g2p
conversion leads to a WER of 16.6%. We can con-
clude from this finding that an integrated approach is
superior to a pipeline architecture for strongly inter-
dependent tasks such as these.
5.5 The Contribution of Morphological
Preprocessing
A statistically significant (according to a two-tailed
t-test) improvement in g2p conversion accuracy
(from 13.7% WER to 13.2% WER) was obtained
with the manually annotated morphological bound-
aries from CELEX. The segmentation from both of
the rule-based systems (ETI and SMOR) also re-
sulted in an accuracy increase with respect to the
baseline (13.6% WER), which is not annotated with
morphological boundaries.
Among the unsupervised systems, best results7 on
the g2p task with morphological annotation were ob-
tained with the RePortS system (Keshava and Pitler,
2006). But none of the segmentations led to an er-
ror reduction when compared to a baseline that used
no morphological information (see Table 3). Word
error rate even increased when the quality of the
7For all results refer to (Demberg, 2006).
101
Precis. Recall F-Meas. WER
RePortS (unsuperv.) 71.1% 50.7% 59.2% 15.1%
no morphology 13.7%
SMOR (rule-based) 87.1% 80.4% 83.6%
ETI (rule-based) 75.4% 84.1% 79.5% 13.6%
CELEX (manual) 100% 100% 100% 13.2%
Table 3: Systems evaluation on German CELEX
manual annotation and on the g2p task using a joint
n-gram model. WERs refer to implementation v2.
morphological segmentation was too low (the unsu-
pervised algorithms achieved 52%-62% F-measure
with respect to CELEX manual annotation).
Table 4 shows that high-quality morphological
information can also significantly improve perfor-
mance on a syllabification task for German. We used
the syllabifier described in (Schmid et al, 2005),
which works similar to the joint n-gram model used
for g2p conversion. Just as for g2p conversion, we
found a significant accuracy improvement when us-
ing the manually annotated data, a smaller improve-
ment for using data from the rule-based morpholog-
ical system, and no improvement when using seg-
mentations from an unsupervised algorithm. Syllab-
ification works best when performed on phonemes,
because syllables are phonological units and there-
fore can be determined most easily in terms of
phonological entities such as phonemes.
Whether morphological segmentation is worth the
effort depends on many factors such as training set
size, the g2p algorithm and the language considered.
disj. stems random
RePortS (unsupervised morph.) 4.95%
no morphology 3.10% 0.72%
ETI (rule-based morph.) 2.63%
CELEX (manual annot.) 1.91% 0.53%
on phonemes 1.53% 0.18%
Table 4: Word error rates (WER) for syllabification
with a joint n-gram model for two different training
and test set designs (see Section 5.1).
Morphology for Data Sparseness Reduction
Probably the most important aspect of morpho-
logical segmentation information is that it can help
to resolve data sparseness issues. Because of the ad-
ditional knowledge given to the system through the
morphological information, similarly-behaving let-
ter sequences can be grouped more effectively.
Therefore, we hypothesized that morphological
information is most beneficial in situations where
the training corpus is rather small. Our findings con-
firm this expectation, as the relative error reduction
through morphological annotation for a training cor-
pus of 9,600 words is 6.67%, while it is only 3.65%
for a 240,000-word training corpus.
In our implementation, the stress flags and sylla-
ble flags we use to enforce the phonological con-
straints increase data sparseness. We found v2 (the
implementation that uses the states without stress
and syllable flags and enforces the constraints by
eliminating invalid transitions, cf. section 3.1) to
outperform the integrated version, v1, and more sig-
nificantly in the case of more severe data sparseness.
The only condition when we found v1 to perform
better than v2 was with a large data set and addi-
tional data sparseness reduction through morpholog-
ical annotation, as in section 4 (see Table 5).
WER: designs v1 v2
data set size 240k 9.6k 240k 9.6k
no morph. 14.4% 32.3% 13.7% 25.5%
CELEX 12.5% 29% 13.2% 23.8%
Table 5: The interactions of constraints in training
and different levels of data sparseness.
g2p Conversion Algorithms
The benefit of using morphological preprocessing
is also affected by the algorithm that is used for g2p
conversion. Therefore, we also evaluated the relative
improvement of morphological annotation when us-
ing a decision tree for g2p conversion.
Decision trees were one of the first data-based ap-
proaches to g2p and are still widely used (Kienappel
and Kneser, 2001; Black et al, 1998). The tree?s
efficiency and ability for generalization largely de-
pends on pruning and the choice of possible ques-
tions. In our implementation, the decision tree can
ask about letters within a context window of five
back and five ahead, about five phonemes back and
groups of letters (e.g. consonants vs. vowels).
Both the decision tree and the joint n-gram model
convert graphemes to phonemes, insert syllable
boundaries and assign word stress in a single step
(marked as ?WER-ss? in Table 6. The imple-
mentation of the joint n-gram model incorporates
the phonological constraints described in section 3
(?WER-ss+?). Our main finding is that the joint
n-gram model profits less from morphological an-
notation. Without the constraints, the performance
102
difference is smaller: the joint n-gram model then
achieves a word error rate of 21.5% on the no-
morphology-condition.
In very recent work, (Demberg, 2007) developed
an unsupervised algorithm (f-meas: 68%; an exten-
sion of RePortS) whose segmentations improve g2p
when using a the decision tree (PER: 3.45%).
decision tree joint n-gram
PER WER-ss PER WER-ss+
RePortS 3.83% 28.3% 15.1%
no morph. 3.63% 26.59% 2.52% 13.7%
ETI 2.8% 21.13% 2.53% 13.6%
CELEX 2.64% 21.64% 2.36% 13.2%
Table 6: The effect of morphological preprocessing
on phoneme error rates (PER) and word error rates
(WER) in grapheme-to-phoneme conversion.
Morphology for other Languages
We also investigated the effect of morphological
information on g2p conversion and syllabification
in English, using manually annotated morphological
boundaries from CELEX and the automatic unsuper-
vised RePortS system which achieves an F-score of
about 77% for English. The cases where morpho-
logical information affects word pronunciation are
relatively few in comparison to German, therefore
the overall effect is rather weak and we did not even
find improvements with perfect boundaries.
6 Conclusions
Our results confirm that the integration of phonolog-
ical constraints ?one nucleus per syllable? and ?one
main stress per word? can significantly boost ac-
curacy for g2p conversion in German and English.
We implemented the constraints using a joint n-
gram model for g2p conversion, which is language-
independent and well-suited to the g2p task.
We systematically evaluated the benefit to be
gained from morphological preprocessing on g2p
conversion and syllabification. We found that mor-
phological segmentations from rule-based systems
led to some improvement. But the magnitude of
the accuracy improvement strongly depends on the
g2p algorithm and on training set size. State-of-
the-art unsupervised morphological systems do not
yet yield sufficiently good segmentations to help the
task, if a good conversion algorithm is used: Low
quality segmentation even led to higher error rates.
Acknowledgments
We would like to thank Hinrich Schu?tze, Frank Keller and the
ACL reviewers for valuable comments and discussion.
The first author was supported by Evangelisches Studienwerk
e.V. Villigst.
References
M. Bisani and H. Ney. 2002. Investigations on joint multigram
models for grapheme-to-phoneme conversion. In ICSLP.
A. Black, K. Lenzo, and V. Pagel. 1998. Issues in building gen-
eral letter to sound rules. In 3. ESCA on Speech Synthesis.
SF Chen and J Goodman. 1996. An empirical study of smooth-
ing techniques for language modeling. In Proc. of ACL.
S. F. Chen. 2003. Conditional and joint models for grapheme-
to-phoneme conversion. In Eurospeech.
V. Demberg. 2006. Letter-to-phoneme conversion for a Ger-
man TTS-System. Master?s thesis. IMS, Univ. of Stuttgart.
V. Demberg. 2007. A language-independent unsupervised
model for morphological segmentation. In Proc. of ACL-07.
L. Galescu and J. Allen. 2001. Bi-directional conversion be-
tween graphemes and phonemes using a joint n-gram model.
In Proc. of the 4th ISCA Workshop on Speech Synthesis.
CELEX German Linguistic User Guide, 1995. Center for Lex-
ical Information. Max-Planck-Institut for Psycholinguistics,
Nijmegen.
M. Jessen, 1998. Word Prosodic Systems in the Languages of
Europe. Mouton de Gruyter: Berlin.
S. Keshava and E. Pitler. 2006. A simpler, intuitive approach
to morpheme induction. In Proceedings of 2nd Pascal Chal-
lenges Workshop, pages 31?35, Venice, Italy.
A. K. Kienappel and R. Kneser. 2001. Designing very com-
pact decision trees for grapheme-to-phoneme transcription.
In Eurospeech, Scandinavia.
M. Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Sar-
aclar. 2006. Unsupervsied segmentation of words into mor-
phemes ? Challenge 2005: An introduction and evaluation
report. In Proc. of 2nd Pascal Challenges Workshop, Italy.
J. Lucassen and R. Mercer. 1984. An information theoretic
approach to the automatic determination of phonemic base-
forms. In ICASSP 9.
Y. Marchand and R. I. Damper. 2005. Can syllabification im-
prove pronunciation by analogy of English? Natural Lan-
guage Engineering.
W. Minker. 1996. Grapheme-to-phoneme conversion - an ap-
proach based on hidden markov models.
B. Mo?bius. 2001. German and Multilingual Speech Synthesis.
phonetic AIMS, Arbeitspapiere des Instituts fu?r Maschinelle
Spachverarbeitung.
K. Mu?ller. 2001. Automatic detection of syllable boundaries
combining the advantages of treebank and bracketed corpora
training. In Proceedings of ACL, pages 402?409.
A. Pounder and M. Kommenda. 1986. Morphological analysis
for a German text-to-speech system. In COLING 1986.
P.A. Rentzepopoulos and G.K. Kokkinakis. 1991. Phoneme to
grapheme conversion using HMM. In Eurospeech.
H. Schmid, A. Fitschen, and U. Heid. 2004. SMOR: A German
computational morphology covering derivation, composition
and inflection. In Proc. of LREC.
H. Schmid, B. Mo?bius, and J. Weidenkaff. 2005. Tagging syl-
lable boundaries with hidden Markov models. IMS, unpub.
R. Sproat. 1996. Multilingual text analysis for text-to-speech
synthesis. In Proc. ICSLP ?96, Philadelphia, PA.
P. Taylor. 2005. Hidden Markov models for grapheme to
phoneme conversion. In INTERSPEECH.
103
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 920?927,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Language-Independent Unsupervised Model
for Morphological Segmentation
Vera Demberg
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
v.demberg@sms.ed.ac.uk
Abstract
Morphological segmentation has been
shown to be beneficial to a range of NLP
tasks such as machine translation, speech
recognition, speech synthesis and infor-
mation retrieval. Recently, a number of
approaches to unsupervised morphological
segmentation have been proposed. This
paper describes an algorithm that draws
from previous approaches and combines
them into a simple model for morpholog-
ical segmentation that outperforms other
approaches on English and German, and
also yields good results on agglutinative
languages such as Finnish and Turkish.
We also propose a method for detecting
variation within stems in an unsupervised
fashion. The segmentation quality reached
with the new algorithm is good enough to
improve grapheme-to-phoneme conversion.
1 Introduction
Morphological segmentation has been shown to be
beneficial to a number of NLP tasks such as ma-
chine translation (Goldwater and McClosky, 2005),
speech recognition (Kurimo et al, 2006), informa-
tion retrieval (Monz and de Rijke, 2002) and ques-
tion answering. Segmenting a word into meaning-
bearing units is particularly interesting for morpho-
logically complex languages where words can be
composed of several morphemes through inflection,
derivation and composition. Data sparseness for
such languages can be significantly decreased when
words are decomposed morphologically. There ex-
ist a number of rule-based morphological segmen-
tation systems for a range of languages. However,
expert knowledge and labour are expensive, and the
analyzers must be updated on a regular basis in or-
der to cope with language change (the emergence of
new words and their inflections). One might argue
that unsupervised algorithms are not an interesting
option from the engineering point of view, because
rule-based systems usually lead to better results.
However, segmentations from an unsupervised algo-
rithm that is language-independent are ?cheap?, be-
cause the only resource needed is unannotated text.
If such an unsupervised system reaches a perfor-
mance level that is good enough to help another task,
it can constitute an attractive additional component.
Recently, a number of approaches to unsupervised
morphological segmentation have been proposed.
These algorithms autonomously discover morpheme
segmentations in unannotated text corpora. Here we
describe a modification of one such unsupervised al-
gorithm, RePortS (Keshava and Pitler, 2006). The
RePortS algorithm performed best on English in a
recent competition on unsupervised morphological
segmentation (Kurimo et al, 2006), but had very low
recall on morphologically more complex languages
like German, Finnish or Turkish. We add a new
step designed to achieve higher recall on morpho-
logically complex languages and propose a method
for identifying related stems that underwent regular
non-concatenative morphological processes such as
umlauting or ablauting, as well as morphological al-
ternations along morpheme boundaries.
The paper is structured as follows: Section
920
2 discusses the relationship between language-
dependency and the level of supervision of a learn-
ing algorithm. We then give an outline of the main
steps of the RePortS algorithm in section 3 and ex-
plain the modifications to the original algorithm in
section 4. Section 5 compares results for different
languages, quantifies the gains from the modifica-
tions on the algorithm and evaluates the algorithm
on a grapheme-to-phoneme conversion task. We fi-
nally summarize our results in section 6.
2 Previous Work
The world?s languages can be classified according
to their morphology into isolating languages (little
or no morphology, e.g. Chinese), agglutinative lan-
guages (where a word can be decomposed into a
large number of morphemes, e.g. Turkish) and in-
flectional languages (morphemes are fused together,
e.g. Latin).
Phenomena that are difficult to cope with for
many of the unsupervised algorithms are non-
concatenative processes such as vowel harmoniza-
tion, ablauting and umlauting, or modifications at
the boundaries of morphemes, as well as infixation
(e.g. in Tagalog: sulat ?write?, s-um-ulat ?wrote?, s-
in-ulat ?was written?), circumfixation (e.g. in Ger-
man: mach-en ?do?, ge-mach-t ?done?), the Ara-
bic broken plural or reduplications (e.g. in Pinge-
lapese: mejr ?to sleep?, mejmejr ?sleeping?, mejme-
jmejr ?still sleeping?). For words that are subject to
one of the above processes it is not trivial to automat-
ically group related words and detect regular trans-
formational patterns.
A range of automated algorithms for morpholog-
ical analysis cope with concatenative phenomena,
and base their mechanics on statistics about hypoth-
esized stems and affixes. These approaches can be
further categorized into ones that use conditional
entropy between letters to detect segment bound-
aries (Harris, 1955; Hafer and Weiss, 1974; De?jean,
1998; Monson et al, 2004; Bernhard, 2006; Ke-
shava and Pitler, 2006; Bordag, 2006), approaches
that use minimal description length and thereby min-
imize the size of the lexicon as measured in en-
tries and links between the entries to constitute a
word form (Goldsmith, 2001; Creutz and Lagus,
2006). These two types of approaches very closely
tie the orthographic form of the word to the mor-
phemes. They are thus not well-suited for coping
with stem changes or modifications at the edges of
morphemes. Only very few approaches have ad-
dressed word internal variations (Yarowski and Wi-
centowski, 2000; Neuvel and Fulop, 2002).
A popular and effective approach for detecting in-
flectional paradigms and filter affix lists is to cluster
together affixes or regular transformational patterns
that occur with the same stem (Monson et al, 2004;
Goldsmith, 2001; Gaussier, 1999; Schone and Juraf-
sky, 2000; Yarowski and Wicentowski, 2000; Neu-
vel and Fulop, 2002; Jacquemin, 1997). We draw
from this idea of clustering in order to detect ortho-
graphic variants of stems; see Section 4.3.
A few approaches also take into account syntac-
tic and semantic information from the context the
word occurs (Schone and Jurafsky, 2000; Bordag,
2006; Yarowski and Wicentowski, 2000; Jacquemin,
1997). Exploiting semantic and syntactic informa-
tion is very attractive because it adds an additional
dimension, but these approaches have to cope with
more severe data sparseness issues than approaches
that emphasize word-internal cues, and they can
be computationally expensive, especially when they
use LSA.
The original RePortS algorithm assumes mor-
phology to be concatenative, and specializes on pre-
fixation and suffixation, like most of the above ap-
proaches, which were developed and implemented
for English (Goldsmith, 2001; Schone and Jurafsky,
2000; Neuvel and Fulop, 2002; Yarowski and Wi-
centowski, 2000; Gaussier, 1999). However, many
languages are morphologically more complex. For
example in German, an algorithm also needs to cope
with compounding, and in Turkish words can be
very long and complex. We therefore extended the
original RePortS algorithm to be better adapted to
complex morphology and suggest a method for cop-
ing with stem variation. These modifications ren-
der the algorithm more language-independent and
thereby make it attractive for applying to other lan-
guages as well.
3 The RePortS Algorithm
On English, the RePortS algorithm clearly out-
performed all other systems in Morpho Challenge
921
20051 (Kurimo et al, 2006), obtaining an F-measure
of 76.8% (76.2% prec., 77.4% recall). The next best
system obtained an F-score of 69%. However, the
algorithm does not perform as well on other lan-
guages (Turkish, Finnish, German) due to low re-
call (see (Keshava and Pitler, 2006) and (Demberg,
2006), p. 47).
There are three main steps in the algorithm. First,
the data is structured in two trees, which provide the
basis for efficient calculation of transitional proba-
bilities of a letter given its context. The second step
is the affix acquisition step, during which a set of
morphemes is identified from the corpus data. The
third step uses these morphemes to segment words.
3.1 Data Structure
The data is stored in two trees, the forward tree and
the backward tree. Branches correspond to letters,
and nodes are annotated with the total corpus fre-
quency of the letter sequence from the root of the
tree up to the node. During the affix identification
process, the forward tree is used for discovering suf-
fixes by calculating the probability of seeing a cer-
tain letter given the previous letters of the word. The
backward tree is used to determine the probability
of a letter given the following letters of a word in
order to find prefixes. If the transitional probabil-
ity is high, the word should not be split, whereas
low probability is a good indicator of a morpheme
boundary. In such a tree, stems tend to stay together
in long unary branches, while the branching factor is
high in places where morpheme boundaries occur.
The underlying idea of exploiting ?Letter Succes-
sor Variety? was first proposed in (Harris, 1955), and
has since been used in a number of morphemic seg-
mentation algorithms (Hafer and Weiss, 1974; Bern-
hard, 2006; Bordag, 2006).
3.2 Finding Affixes
The second step is concerned with finding good af-
fixes. The procedure is quite simple and can be di-
vided into two subtasks. (1) generating all possible
affixes and (2) validating them. The validation step
is necessary to exclude bad affix candidates (e.g. let-
ter sequences that occur together frequently such as
sch, spr or ch in German or sh, th, qu in English).
1www.cis.hut.fi/morphochallenge2005/
An affix is validated if all three criteria are satisfied
for at least 5% of its occurrences:
1. The substring that remains after peeling off an
affix is also a word in the lexicon.
2. The transitional probability between the
second-last and the last stem letter is ? 1.
3. The transitional probability of the affix letter
next to the stem is <1 (tolerance 0.02).
Finally, all affixes that are concatenations of two or
more other suffixes (e.g., -ungen can be split up in
-ung and -en in German) are removed. This step re-
turns two lists of morphological segments. The pre-
fix list contains prefixes as well as stems that usually
occur at the beginning of words, while the suffix list
contains suffixes and stems that occur at the end of
words. In the remainder of the paper, we will refer
to the content of these lists as ?prefixes? and ?suf-
fixes?, although they also include stems. There are
several assumptions encoded in this procedure that
are specific to English, and cause recall to be low for
other languages: 1) all stems are valid words in the
lexicon; 2) affixes occur at the beginning or end of
words only; and 3) affixation does not change stems.
In section 4, we propose ways of relaxing these as-
sumptions to make this step less language-specific.
3.3 Segmenting Words
The final step is the complete segmentation of words
given the list of affixes acquired in the previous step.
The original RePortS algorithm uses a very simple
method that peels off the most probable suffix that
has a transitional probability smaller than 1, until no
more affixes match or until less than half of the word
remains. This last condition is problematic since it
does not scale up well to languages with complex
morphology. The same peeling-off process is exe-
cuted for prefixes.
Although this method is better than using a
heuristic such as ?always peel off the longest pos-
sible affix?, because it takes into account probable
sites of fractures in words, it is not sensitive to
the affix context or the morphotactics of the lan-
guage. Typical mistakes that arise from this con-
dition are that inflectional suffixes, which can only
occur word-finally, might be split off in the middle
of a word after previously having peeled off a num-
ber of other suffixes.
922
4 Modifications and Extensions
4.1 Morpheme Acquisition
When we ran the original algorithm on a German
data set, no suffixes were validated but reasonable
prefix lists were found. The algorithm works fine
for English suffixes ? why does it fail on German?
The algorithm?s failure to detect German suffixes is
caused by the invalid assumption that a stem must
be a word in the corpus. German verb stems do
not occur on their own (except for certain impera-
tive forms). After stripping off the suffix of the verb
abholst ?fetch?, the remaining string abhol cannot be
found in the lexicon. However, words like abholen,
abholt, abhole or Abholung are part of the corpus.
The same problem also occurs for German nouns.
Therefore, this first condition of the affix acqui-
sition step needs to be replaced. We therefore intro-
duced an additional step for building an intermediate
stem candidate list into the affix acquisition process.
The first condition is replaced by a condition that
checks whether a stem is in the stem candidate list.
This new stem candidate acquisition procedure com-
prises three steps:
Step 1: Creation of stem candidate list
All substrings that satisfy conditions 2 and 3 but
not condition 1, are stored together with the set of
affixes they occur with. This process is similar to
the idea of registering signatures (Goldsmith, 2001;
Neuvel and Fulop, 2002). For example, let us as-
sume our corpus contains the words Auffu?hrender,
Auffu?hrung, auffu?hrt and Auffu?hrlaune but not the
stem itself, since auffu?hr ?act? is not a valid Ger-
man word. Conditions 2 and 3 are met, because
the transitional probability between auffu?hr and the
next letter is low (there are a lot of different pos-
sible continuations) and the transitional probability
P (r|auffu?h) ? 1. The stem candidate auffu?hr is then
stored together with the suffix candidates {ender,
ung, en, t, laune}.
Step 2: Ranking candidate stems
There are two types of affix candidates: type-1 affix
candidates are words that are contained in the data
base as full words (those are due to compounding);
type-2 affix candidates are inflectional and deriva-
tional suffixes. When ranking the stem candidates,
we take into account the number of type-1 affix can-
didates and the average frequency of tpye-2 affix
Figure 1: Determining the threshold for validating
the best candidates from the stem candidate list.
candidates.
The first condition has very good precision, sim-
ilar to the original method. The morphemes found
with this method are predominantly stem forms that
occur in compounding or derivation (Komposition-
ssta?mme and Derivationssta?mme). The second con-
dition enables us to differentiate between stems that
occur with common suffixes (and therefore have
high average frequencies), and pseudostems such
as runtersch whose affix list contains many non-
morphemes (e.g. lucken, iebt, aute). These non-
morphemes are very rare since they are not gener-
ated by a regular process.
Step 3: Pruning
All stem candidates that occur less than three times
are removed from the list. The remaining stem can-
didates are ordered according to the average fre-
quency of their non-word suffixes. This criterion
puts the high quality stem candidates (that occur
with very common suffixes) to the top of the list.
In order to obtain a high-precision stem list, it is
necessary to cut the list of candidates at some point.
The threshold for this is determined by the data: we
choose the point at which the function of list-rank
vs. score changes steepness (see Figure 1). This
visual change of steepness corresponds to the point
where potential stems found get more noisy because
the strings with which they occur are not common
affixes. We found the performance of the result-
ing morphological system to be quite stable (?1%
f-score) for any cutting point on the slope between
20% and 50% of the list (for the German data set
ranks 4000 and 12000), but importantly before the
function tails off. The threshold was also robust
across the other languages and data sets.
923
4.2 Morphological Segmentation
As discussed in section 3.3, the original implemen-
tation of the algorithm iteratively chops off the most
probable affixes at both edges of the word without
taking into account the context of the affix. In mor-
phologically complex languages, this context-blind
approach often leads to suboptimal results, and also
allows segmentations that are morphotactically im-
possible, such as inflectional suffixes in the middle
of words. Another risk is that the letter sequence that
is left after removing potential prefixes and suffixes
from both ends is not a proper stem itself but just a
single letter or vowel-less letter-sequence.
These problems can be solved by using a bi-gram
language model to capture the morphotactic proper-
ties of a particular language. Instead of simply peel-
ing off the most probable affixes from both ends of
the word, all possible segmentations of the word are
generated and ranked using the language model. The
probabilities for the language model are learnt from
a set of words that were segmented with the origi-
nal simple approach. This bootstrapping allows us
to ensure that the approach remains fully unsuper-
vised. At the beginning and end of each word, an
edge marker ?#? is attached to the word. The model
can then also acquire probabilities about which af-
fixes occur most often at the edges of words.
Table 2 shows that filtering the segmentation re-
sults with the n-gram language model caused a sig-
nificant improvement on the overall F-score for most
languages, and led to significant changes in pre-
cision and recall. Whereas the original segmen-
tation yielded balanced precision and recall (both
68%), the new filtering boosts precision to over
73%, with 64% recall. Which method is preferable
(i.e. whether precision or recall is more important)
is task-dependent.
In future work, we plan to draw on (Creutz and
Lagus, 2006), who use a HMM with morphemic cat-
egories to impose morphotactic constraints. In such
an approach, each element from the affix list is as-
signed with a certain probability to the underlying
categories of ?stem?, ?prefix? or ?suffix?, depend-
ing on the left and right perplexity of morphemes, as
well as morpheme length and frequency. The tran-
sitional probabilities from one category to the next
model the morphotactic rules of a language, which
can thus be learnt automatically.
4.3 Learning Stem Variation
Stem variation through ablauting and umlauting
(an English example is run?ran) is an interest-
ing problem that cannot be captured by the algo-
rithm outlined above, as variations take place within
the morphemes. Stem variations can be context-
dependent and do not constitute a morpheme in
themselves. German umlauting and ablauting leads
to data sparseness problems in morphological seg-
mentation and affix acquisition. One problem is that
affixes which usually cause ablauting or umlauting
are very difficult to find. Typically, ablauted or um-
lauted stems are only seen with a very small number
of different affixes, which means that the affix sets
of such stems are divided into several unrelated sub-
sets, causing the stem to be pruned from the stem
candidate list. Secondly, ablauting and umlauting
lead to low transitional probabilities at the positions
in stems where these phenomena occur. Consider
for example the affix set for the stem candidate bock-
spr, which contains the pseudoaffixes ung, u?nge and
ingen. The morphemes sprung, spru?ng and sprin-
gen are derived from the root spring ?to jump?. In
the segmentation step this low transitional probabil-
ity thus leads to oversegmentation.
We therefore investigated whether we can learn
these regular stem variations automatically. A sim-
ple way to acquire the stem variations is to look at
the suffix clusters which are calculated during the
stem-acquisition step. When looking at the sets of
substrings that are clustered together by having the
same prefix, we found that they are often inflections
of one another, because lexicalized compounds are
used frequently in different inflectional variants. For
example, we find Trainingssprung as well as Train-
ingsspru?nge in the corpus. The affix list of the stem
candidate trainings thus contains the words sprung
and spru?nge. Edit distance can then be used to
find differences between all words in a certain affix
list. Pairs with small edit distances are stored and
ranked by frequency. Regular transformation rules
(e.g. ablauting and umlauting, u ? u?..e) occur at
the top of the list and are automatically accepted as
rules (see Table 1). This method allows us to not
only find the relation between two words in the lex-
icon (Sprung and Spru?nge) but also to automatically
learn rules that can be applied to unknown words to
check whether their variant is a word in the lexicon.
924
freq. diff. examples
1682 a a?..e sack-sa?cke, brach-bra?che, stark-sta?rke
344 a a? sahen-sa?hen, garten-ga?rten
321 u u?..e flug-flu?ge, bund-bu?nde
289 a? a..s vertra?ge-vertrages, pa?sse-passes
189 o o?..e chor-cho?re, strom-stro?me, ?ro?hre-rohr
175 t en setzt-setzen, bringt-bringen
168 a u laden-luden, *damm-dumm
160 ? ss la??t-la?sst, mi?brauch-missbrauch
[. . .]
136 a en firma-firmen, thema-themen
[. . .]
2 ? g *flie?en-fliegen, *la?t-lagt
2 um o *studiums-studios
Table 1: Excerpts from the stem variation detection
algorithm results. Morphologically unrelated word
pairs are marked with an asterisk.
We integrated information about stem variation
from the regular stem transformation rules (those
with the highest frequencies) into the segmentation
step by creating equivalence sets of letters. For ex-
ample, the rule u ? u?..e generates an equivalence
set {u?, u}. These two letters then count as the same
letter when calculating transitional probabilities. We
evaluated the benefit of integrating stem variation
information for German on the German CELEX
data set, and achieved an improvement of 2% in
recall, without any loss in precision (F-measure:
69.4%, Precision: 68.1%, Recall: 70.8%; values for
RePortS-stems). For better comparability to other
systems and languages, results reported in the next
section refer to the system version that does not in-
corporate stem variation.
5 Evaluation
For evaluating the different versions of the algorithm
on English, Turkish and Finnish, we used the train-
ing and test sets from MorphoChallenge to enable
comparison with other systems. Performance of the
algorithm on German was evaluated on 244k manu-
ally annotated words from CELEX because German
was not included in the MorphoChallenge data.
Table 2 shows that the introduction of the stem
candidate acquisition step led to much higher recall
on German, Finnish and Turkish, but caused some
losses in precision. For English, adding both com-
ponents did not have a large effect on either preci-
sion or recall. This means that this component is
well behaved, i.e. it improves performance on lan-
guages where the intermediate stem-acquisition step
Lang. alg.version F-Meas. Prec. Recall
Eng1 original 76.8% 76.2% 77.4%
stems 67.6% 62.9% 73.1%
n-gram seg. 75.1% 74.4% 75.9%
Ger2 original 59.2% 71.1% 50.7%
stems 68.4% 68.1% 68.6%
n-gram seg. 68.9% 73.7% 64.6%
Tur1 original 54.2% 72.9% 43.1%
stems 61.8% 65.9% 58.2%
n-gram seg. 64.2% 65.2% 63.3%
Fin1 original 47.1% 84.5% 32.6%
stems 56.6% 74.1% 45.8%
n-gram seg. 58.9% 76.1% 48.1%
max-split* 61.3% 66.3% 56.9%
Table 2: Performance of the algorithm with the mod-
ifications on different languages.
1MorphoChallenge Data, 2German CELEX
is needed, but does not impair results on other lan-
guages. Recall for Finnish is still very low. It can be
improved (at the expense of precision) by selecting
the analysis with the largest number of segments in
the segmentation step. The results for this heuris-
tic was only evaluated on a smaller test set (ca. 700
wds), hence marked with an asterisk in Table 2.
The algorithm is very efficient: When trained on
the 240m tokens of the German TAZ corpus, it takes
up less than 1 GB of memory. The training phase
takes approx. 5 min on a 2.4GHz machine, and the
segmentation of the 250k test words takes 3 min for
the version that does the simple segmentation and
about 8 min for the version that generates all possi-
ble segmentations and uses the language model.
5.1 Comparison to other systems
This modified version of the algorithm performs sec-
ond best for English (after original RePortS) and
ranks third for Turkish (after Bernhards algorithm
with 65.3% F-measure and Morfessor-Categories-
MAP with 70.7%). On German, our method sig-
nificantly outperformed the other unsupervised al-
gorithms, see Table 3. While most of the systems
compared here were developed for languages other
than German, (Bordag, 2006) describes a system ini-
tially built for German. When trained on the ?Pro-
jekt Deutscher Wortschatz? corpus which comprises
24 million sentences, it achieves an F-score of 61%
(precision 60%, recall 62%2) when evaluated on the
full CELEX corpus.
2Data from personal communication.
925
morphology F-Meas. Prec. Recall
SMOR-disamb2 83.6% 87.1% 80.4%
ETI 79.5% 75.4% 84.1%
SMOR-disamb1 71.8% 95.4% 57.6%
RePortS-lm 68.8% 73.7% 64.6%
RePortS-stems 68.4% 68.1% 68.6%
best Bernhard 63.5% 64.9% 62.1%
Bordag 61.4% 60.6% 62.3%
orig. RePortS 59.2% 71.1% 50.7%
best Morfessor 1.0 52.6% 70.9% 41.8%
Table 3: Evaluating rule-based and data-based sys-
tems for morphological segmentation with respect to
CELEX manual morphological annotation.
Rule-based systems are currently the most com-
mon approach to morphological decomposition and
perform better at segmenting words than state-of-
the-art unsupervised algorithms (see Table 3 for per-
formance of state-of-the-art rule-based systems eval-
uated on the same data). Both the ETI3 and the
SMOR (Schmid et al, 2004) systems rely on a large
lexicon and a set of rules. The SMOR system re-
turns a set of analyses that can be disambiguated in
different ways. For details refer to pp. 29?33 in
(Demberg, 2006).
5.2 Evaluation on Grapheme-to-Phoneme
Conversion
Morphological segmentation is not of value in itself
? the question is whether it can help improve results
on an application. Performance improvements due
to morphological information have been reported for
example in MT, information retrieval, and speech
recognition. For the latter task, morphological seg-
mentations from the unsupervised systems presented
here have been shown to improve accuracy (Kurimo
et al, 2006).
Another motivation for evaluating the system on
a task rather than on manually annotated data is
that linguistically motivated morphological segmen-
tation is not necessarily the best possible segmenta-
tion for a certain task. Evaluation against a manu-
ally annotated corpus prefers segmentations that are
closest to linguistically motivated analyses. Further-
more, it might be important for a certain task to
find a particular type of morpheme boundaries (e.g.
boundaries between stems), but for another task it
3Eloquent Technology, Inc. (ETI) TTS system.
www.mindspring.com/?ssshp/ssshp_cd/ss_
eloq.htm
morphology F-Meas. (CELEX) PER (dt)
CELEX 100% 2.64%
ETI 79.5% 2.78%
SMOR-disamb2 83.0% 3.00%
SMOR-disamb1 71.8% 3.28%
RePortS-lm 68.8% 3.45%
no morphology 3.63%
orig. RePortS 59.2% 3.83%
Bernhard 63.5% 3.88%
RePortS-stem 68.4% 3.98%
Morfessor 1.0 52.6% 4.10%
Bordag 64.1% 4.38%
Table 4: F-measure for evaluation on manually an-
notated CELEX and phoneme error rate (PER) from
g2p conversion using a decision tree (dt).
might be very important to find boundaries between
stems and suffixes. The standard evaluation proce-
dure does not differentiate between the types of mis-
takes made. Finally, only evaluation on a task can
provide information as to whether high precision or
high recall is more important, therefore, the decision
as to which version of the algorithm should be cho-
sen can only be taken given a specific task.
For these reasons we decided to evaluate the seg-
mentation from the new versions of the RePortS al-
gorithm on a German grapheme-to-phoneme (g2p)
conversion task. The evaluation on this task is moti-
vated by the fact that (Demberg, 2007) showed that
good-quality morphological preprocessing can im-
prove g2p conversion results. We here compare the
effect of using our system?s segmentations to a range
of different morphological segmentations from other
systems. We ran each of the rule-based systems
(ETI, SMOR-disamb1, SMOR-disamb2) and the
unsupervised algorithms (original RePortS, Bern-
hard, Morfessor 1.0, Bordag) on the CELEX data
set and retrained our decision tree (an implementa-
tion based on (Lucassen and Mercer, 1984)) on the
different morphological segmentations.
Table 4 shows the F-score of the different systems
when evaluated on the manually annotated CELEX
data (full data set) and the phoneme error rate (PER)
for the g2p conversion algorithm when annotated
with morphological boundaries (smaller test set,
since the decision tree is a supervised method and
needs training data). As we can see from the results,
the distribution of precision and recall (see Table 3)
has an important impact on the conversion quality:
the RePortS version with higher precision signifi-
926
cantly outperforms the other version on the task, al-
though their F-measures are almost identical. Re-
markably, the RePortS version that uses the filter-
ing step is the only unsupervised system that beats
the no-morphology baseline (p < 0.0001). While
all other unsupervised systems tested here make the
system perform worse than it would without mor-
phological information, this new version improves
accuracy on g2p conversion.
6 Conclusions
A significant improvement in F-score was achieved
by three simple modifications to the RePortS al-
gorithm: generating an intermediary high-precision
stem candidate list, using a language model to dis-
ambiguate between alternative segmentations, and
learning patterns for regular stem variation, which
can then also be exploited for segmentation. These
modifications improved results on four different lan-
guages considered: English, German, Turkish and
Finnish, and achieved the best results reported so far
for an unsupervised system for morphological seg-
mentation on German. We showed that the new ver-
sion of the algorithm is the only unsupervised sys-
tem among the systems evaluated here that achieves
sufficient quality to improve transcription perfor-
mance on a grapheme-to-phoneme conversion task.
Acknowledgments
I would like to thank Emily Pitler and Samarth Ke-
shava for making available the code of the RePortS
algorithm, and Stefan Bordag and Delphine Bern-
hard for running their algorithms on the German
data. Many thanks also to Matti Varjokallio for eval-
uating the data on the MorphoChallenge test sets
for Finnish, Turkish and English. Furthermore, I
am very grateful to Christoph Zwirello and Gregor
Mo?hler for training the decision tree on the new mor-
phological segmentation. I also want to thank Frank
Keller and the ACL reviewers for valuable and in-
sightful comments.
References
Delphine Bernhard. 2006. Unsupervised morphological seg-
mentation based on segment predictability and word seg-
ments alignment. In Proceedings of 2nd Pascal Challenges
Workshop, pages 19?24, Venice, Italy.
Stefan Bordag. 2006. Two-step approach to unsupervised mor-
pheme segmentation. In Proceedings of 2nd Pascal Chal-
lenges Workshop, pages 25?29, Venice, Italy.
Mathias Creutz and Krista Lagus. 2006. Unsupervised models
for morpheme segmentation and morphology learning. In
ACM Transaction on Speech and Language Processing.
H. De?jean. 1998. Morphemes as necessary concepts for struc-
tures: Discovery from untagged corpora. In Workshop on
paradigms and Grounding in Natural Language Learning,
pages 295?299, Adelaide, Australia.
Vera Demberg. 2006. Letter-to-phoneme conversion for a Ger-
man TTS-System. Master?s thesis. IMS, Univ. of Stuttgart.
Vera Demberg. 2007. Phonological constraints and morpho-
logical preprocessing for grapheme-to-phoneme conversion.
In Proc. of ACL-2007.
Eric Gaussier. 1999. Unsupervised learning of derivational
morphology from inflectional lexicons. In ACL ?99 Work-
shop Proceedings, University of Maryland.
CELEX German Linguistic User Guide, 1995. Center for Lex-
ical Information. Max-Planck-Institut for Psycholinguistics,
Nijmegen.
John Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. computational Linguistics,
27(2):153?198, June.
S. Goldwater and D. McClosky. 2005. Improving statistical mt
through morphological analysis. In Proc. of EMNLP.
Margaret A. Hafer and Stephen F. Weiss. 1974. Word segmen-
tation by letter successor varieties. Information Storage and
Retrieval 10, pages 371?385.
Zellig Harris. 1955. From phoneme to morpheme. Language
31, pages 190?222.
Christian Jacquemin. 1997. Guessing morphology from terms
and corpora. In Research and Development in Information
Retrieval, pages 156?165.
S. Keshava and E. Pitler. 2006. A simpler, intuitive approach
to morpheme induction. In Proceedings of 2nd Pascal Chal-
lenges Workshop, pages 31?35, Venice, Italy.
M. Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Sar-
aclar. 2006. Unsupervsied segmentation of words into mor-
phemes ? Challenge 2005: An introduction and evaluation
report. In Proc. of 2nd Pascal Challenges Workshop, Italy.
J. Lucassen and R. Mercer. 1984. An information theoretic
approach to the automatic determination of phonemic base-
forms. In ICASSP 9.
C. Monson, A. Lavie, J. Carbonell, and L. Levin. 2004. Un-
supervised induction of natural language morphology inflec-
tion classes. In Proceedings of the Seventh Meeting of ACL-
SIGPHON, pages 52?61, Barcelona, Spain.
C. Monz and M. de Rijke. 2002. Shallow morphological analy-
sis in monolingual information retrieval for Dutch, German,
and Italian. In Proceedings CLEF 2001, LNCS 2406.
Sylvain Neuvel and Sean Fulop. 2002. Unsupervised learning
of morphology without morphemes. In Proc. of the Wshp on
Morphological and Phonological Learning, ACL Pub.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proc. of LREC.
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-free
induction of morphology using latent semantic analysis. In
Proc. of CoNLL-2000 and LLL-2000, Lisbon, Portugal.
Tageszeitung (TAZ) Corpus. Contrapress Media GmbH.
https://www.taz.de/pt/.etc/nf/dvd.
David Yarowski and Richard Wicentowski. 2000. Minimally
supervised morphological analysis by multimodal align-
ment. In Proceedings of ACL 2000, Hong Kong.
927
Information Presentation in Spoken Dialogue Systems
Vera Demberg
Institute for Natural Language Processing (IMS)
University of Stuttgart
D-70174 Stuttgart
V.Demberg@gmx.de
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
J.Moore@ed.ac.uk
Abstract
To tackle the problem of presenting a
large number of options in spoken dia-
logue systems, we identify compelling op-
tions based on a model of user preferences,
and present tradeoffs between alternative
options explicitly. Multiple attractive op-
tions are structured such that the user can
gradually refine her request to find the
optimal tradeoff. We show that our ap-
proach presents complex tradeoffs under-
standably, increases overall user satisfac-
tion, and significantly improves the user?s
overview of the available options. More-
over, our results suggest that presenting
users with a brief summary of the irrele-
vant options increases users? confidence in
having heard about all relevant options.
1 Introduction
The goal of spoken dialogue systems (SDS) is
to offer efficient and natural access to applica-
tions and services, such as email and calendars,
travel and entertainment booking, and product rec-
ommendation. In evaluating nine SDS in the
DARPA Communicator domain (flight, hotel, and
rental car hire), Walker et al (2001) found that (1)
shorter task duration correlates with higher user
satisfaction, and (2) the information presentation
phase of dialogues is the primary contributor to
dialogue duration. During this phase, the typical
system enumerates the set of options that match
the user?s constraints, as shown in Figure 1. The
user can then refine these options by offering new
constraints. When the number of options to be
presented is large, this process can be painstaking,
leading to reduced user satisfaction. Moreover, as
S: I found 28 round trips from Edinburgh to Brussels on
October 31st and returning Edinburgh on November
6th. For option 1, I have an outbound flight with KLM
departing at 7 am and arriving at 8:49 am non-stop.
A return flight is with KLM departing at 10:30 am and
arriving at 11:28 am with 1 stopover. Ticket price is
?165. Please say ?next option?, or, ?flight details?, or,
?I?ll take it?.
U: Next option
S: For option 2, I have an outbound flight with KLM de-
parting at 3:30 pm and arriving at 5:19 pm nonstop.
A return flight is with KLM departing at 8:25 pm and
arriving at 12:28 pm with 1 stopover. Ticket price is
?165 please say ?next option?, or, ?flight details?, or,
?I?ll take it.?
U: Next option
S: . . .
Figure 1: Typical Information Presentation Phase
of a Communicator Dialogue
Walker et al (2004) observe, having to access the
set of available options sequentially makes it diffi-
cult for the user to remember the various aspects of
multiple options and to compare them in memory.
Clearly, alternative strategies to sequential pre-
sentation of information in SDS are needed. Re-
cently, two approaches have been proposed. In
the user-model (UM) based approach, the sys-
tem identifies a small number of options that best
match the user?s preferences (Moore et al, 2004;
Walker et al, 2004). In the summarize and re-
fine (SR) approach, the system structures the large
number of options into a small number of clus-
ters that share attributes. The system summa-
rizes the clusters based on their attributes and then
prompts the user to provide additional constraints
(Polifroni et al, 2003; Chung, 2004).
In this paper, we present an algorithm that com-
bines the benefits of these two approaches in an
approach to information presentation that inte-
grates user modelling with automated clustering.
65
Thus, the system provides detail only about those
options that are of some relevance to the user,
where relevance is determined by the user model.
If there are multiple relevant options, a cluster-
based tree structure orders these options to allow
for stepwise refinement. The effectiveness of the
tree structure, which directs the dialogue flow, is
optimized by taking the user?s preferences into ac-
count. Complex tradeoffs between alternative op-
tions are presented explicitly to allow for a bet-
ter overview and a more informed choice. In ad-
dition, we address the issue of giving the user a
good overview of the option space, despite select-
ing only the relevant options, by briefly accounting
for the remaining (irrelevant) options.
In the remainder of this paper, we describe the
prior approaches in more detail, and discuss their
limitations (Section 2). In section 3, we describe
our approach, which integrates user preferences
with automated clustering and summarization in
an attempt to overcome the problems of the origi-
nal approaches. Section 4 presents our clustering
and content structuring algorithms and addresses
issues in information presentation. In Section 5,
we describe an evaluation of our approach and dis-
cuss its implications.
2 Previous Work in Information
Presentation
2.1 Tailoring to a User Model
Previous work in natural language generation
showed how a multi-attribute decision-theoretic
model of user preferences could be used to deter-
mine the attributes that are most relevant to men-
tion when generating recommendations tailored to
a particular user (Carenini and Moore, 2001). In
the MATCH system, Walker et al (2004) applied
this approach to information presentation in SDS,
and extended it to generate summaries and com-
parisons among options, thus showing how the
model can be used to determine which options to
mention, as well as the attributes that the user will
find most relevant to choosing among them. Eval-
uation showed that tailoring recommendations and
comparisons to the user increases argument effec-
tiveness and improves user satisfaction (Stent et
al., 2002).
MATCH included content planning algorithms
to determine what options and attributes to men-
tion, but used a simple template based approach
to realization. In the FLIGHTS system, Moore
et al (2004) focussed on organizing and express-
ing the descriptions of the selected options and at-
tributes, in ways that are both easy to understand
and memorable. For example, Figure 2 shows a
description of options that is tailored to a user who
prefers flying business class, on direct flights, and
on KLM, in that order. In FLIGHTS, coherence
and naturalness of descriptions were increased by
reasoning about information structure (Steedman,
2000) to control intonation, using referring expres-
sions that highlight attributes relevant to the user
(e.g., ?the cheapest flight? vs. ?a KLM flight? ),
and signalling discourse relations (e.g., contrast)
with appropriate intonational and discourse cues.
S: You can fly business class on KLM, arriving at four
twenty p.m., but you?d need to connect in London. There
is a direct flight on BMI, arriving at four ten p.m., but it
has no availability in business class.
Figure 2: Tailored description by FLIGHTS
This prior work demonstrated that the user
model-based approach can concisely present a rel-
atively small number of options, pointing out the
ways in which those options satisfy user prefer-
ences. It is an appropriate strategy for SDS when
there are a small number of options to present, ei-
ther because the number of options is limited or
because users can supply sufficient constraints to
winnow down a large set before querying the data-
base of options.
However, there are several limitations to this ap-
proach. First, it does not scale up to presenting
a large number of options. When there are hun-
dreds of options to consider (e.g., when choosing
among consumer products, hotels, or restaurants)
there may be many options that are close in score.
In addition, users may not be able to provide con-
straints until they hear more information about the
space of options. This brings up a second problem
with the UM-based approach, namely that it does
not provide the user with an overview of the op-
tion space, because options scoring below a speci-
fied threshold are not mentioned. This is related to
the third problem, which is that users might miss
out on options they would have chosen if they had
heard about them. These last two problems may
reduce user confidence in the system, if users have
the perception that the system is not telling them
about all of the available options. This may ulti-
mately lead to a decrease in user satisfaction.
66
2.2 Stepwise Refinement through Clustering
and Summarization
Polifroni et al (2003) developed an approach that
structures large datasets for summarization and
successive refinement. It supports the user in nar-
rowing in on a suitable option by clustering the
options in the database, summarizing the option
clusters and suggesting further constraints. In this
approach, the prompts presented to the user, and
the order in which they appear are determined at
run-time based on an algorithm that computes the
most useful set of attributes, as dictated by the cur-
rent data subset. For large data sets, attributes that
partition the data into the minimal number of clus-
ters are chosen, so that a concise summary can be
presented to the user to refine, see Figure 3.
M: What restaurants do you have?
S: I have found 983 restaurants. Most of them are located
in Boston and Cambridge. There are 32 choices for cui-
sine. I also have information about price range.
M: Okay tell me about the ones in Boston.
S: I have found 401 restaurants in Boston. There are 29
choices for cuisine.
M: Do you have any that serve seafood?
S: I have found 19 seafood restaurants. They are predomi-
nantly in Back Bay, the North End, South Boston and the
South End.
M: In the South End.
S: I have found 3 restaurants. Grill Fish on Columbus Av-
enue, McCormick & Schmick?s on Columbus Avenue and
Joseph?s on High Street.
Figure 3: Dialogue between simulator (M) and Po-
lifroni system (S)
Polifroni et al?s approach was extended by
Chung (2004), who proposed a constraint relax-
ation strategy for coping with queries that are too
restrictive to be satisfied by any option. Qu and
Beale (2003) had previously addressed the prob-
lem of responding to user queries with several
constraints and used linguistic cues to determine
which constraints had to be relaxed. Our discus-
sion and evaluation of the SR approach is based
on Chung?s version.
Although the SR approach provides a solution
to the problem of presenting information when
there are large numbers of options in a way that is
suitable for SDS, it has several limitations. First,
there may be long paths in the dialogue struc-
ture. Because the system does not know about the
user?s preferences, the option clusters may contain
many irrelevant entities which must be filtered out
successively with each refinement step. In addi-
tion, the difficulty of summarizing options typi-
cally increases with their number, because values
are more likely to be very diverse, to the point
that a summary about them gets uninformative (?I
found flights on 9 airlines.?).
A second problem with the SR approach is that
exploration of tradeoffs is difficult when there is
no optimal option. If at least one option satis-
fies all requirements, this option can be found effi-
ciently with the SR strategy. But the system does
not point out alternative tradeoffs if no ?optimal?
option exists. For example, in the flight book-
ing domain, suppose the user wants a flight that is
cheap and direct, but there are only expensive di-
rect and cheap indirect flights. In the SR approach,
as described by Polifroni, the user has to ask for
cheap flights and direct flights separately and thus
has to explore different refinement paths.
Finally, the attribute that suggests the next user
constraint may be suboptimal. The procedure for
computing the attribute to use in suggesting the
next restriction to the user is based on the con-
siderations for efficient summarization, that is, the
attribute that will partition the data set into the
smallest number of clusters. If the attribute that
is best for summarization is not of interest to this
particular user, dialogue duration is unnecessarily
increased, and the user may be less satisfied with
the system, as the results of our evaluation suggest
(see section 5.2).
3 Our Approach
Our work combines techniques from the UM and
SR approaches. We exploit information from a
user model to reduce dialogue duration by (1) se-
lecting all options that are relevant to the user,
and (2) introducing a content structuring algorithm
that supports stepwise refinement based on the
ranking of attributes in the user model. In this
way, we keep the benefits of user tailoring, while
extending the approach to handle presentation of
large numbers of options in an order that reflects
user preferences. To address the problem of user
confidence, we also briefly summarize options that
the user model determines to be irrelevant (see
section 4.3). Thus, we give users an overview of
the whole option space, and thereby reduce the
risk of leaving out options the user may wish to
choose in a given situation.
The integration of a user model with the cluster-
ing and structuring also alleviates the three prob-
lems we identified for the SR approach. When a
67
user model is available, it enables the system to
determine which options and which attributes of
options are likely to be of interest to the particu-
lar user. The system can then identify compelling
options, and delete irrelevant options from the re-
finement structure, leading to shorter refinement
paths. Furthermore, the user model allows the
system to determine the tradeoffs among options.
These tradeoffs can then be presented explicitly.
The user model also allows the identification of the
attribute that is most relevant at each stage in the
refinement process. Finally, the problem of sum-
marizing a large number of diverse attribute values
can be tackled by adapting the cluster criterion to
the user?s interest.
In our approach, information presentation is
driven by the user model, the actual dialogue con-
text and the available data. We allow for an arbi-
trarily large number of alternative options. These
are structured so that the user can narrow in on one
of them in successive steps. For this purpose, a
static option tree is built. Because the structure of
the option tree takes the user model into account,
it allows the system to ask the user to make the
most relevant decisions first. Moreover, the option
tree is pruned using an algorithm that takes advan-
tage of the tree structure, to avoid wasting time
by suggesting irrelevant options to the user. The
tradeoffs (e.g., cheap but indirect flights vs. direct
but expensive flights) are presented to the user ex-
plicitly, so that the user won?t have to ?guess? or
try out paths to find out what tradeoffs exist. Our
hypothesis was that explicit presentation of trade-
offs would lead to a more informed choice and de-
crease the risk that the user does not find the opti-
mal option.
4 Implementation
Our approach was implemented within a spoken
dialogue system for flight booking. While the con-
tent selection step is a new design, the content pre-
sentation part of the system is an adaptation and
extension of the work on generating natural sound-
ing tailored descriptions reported in (Moore et al,
2004).
4.1 Clustering
The clustering algorithm in our implementation is
based on that reported in (Polifroni et al, 2003).
The algorithm can be applied to any numerically
ordered dataset. It sorts the data into bins that
roughly correspond to small, medium and large
values in the following way. The values of each at-
tribute of the objects in the database (e.g., flights)
are clustered using agglomerative group-average
clustering. The algorithm begins by assigning
each unique attribute value to its own bin, and suc-
cessively merging adjacent bins whenever the dif-
ference between the means of the bins falls below
a varying threshold. This continues until a stop-
ping criterion (a target number of no more than
three clusters in our current implementation) is
met. The bins are then assigned predefined labels,
e.g., cheap, average-price, expensive
for the price attribute.
Clustering attribute values with the above algo-
rithm allows for database-dependent labelling. A
?300 flight gets the label cheap if it is a flight
from Edinburgh to Los Angeles (because most
other flights in the database are more costly) but
expensive if it is from Edinburgh to Stuttgart
(for which there are a lot of cheaper flights in the
data base). Clustering also allows the construc-
tion of user valuation-sensitive clusters for cat-
egorial values, such as the attribute airline:
They are clustered to a group of preferred air-
lines, dispreferred airlines and airlines the
user does not-care about.
4.2 Building up a Tree Structure
The tree building algorithm works on the clusters
produced by the clustering algorithm instead of the
original values. Options are arranged in a refine-
ment tree structure, where the nodes of an option
tree correspond to sets of options. The root of
the tree contains all options and its children con-
tain complementary subsets of these options. Each
child is homogeneous for a given attribute (e.g., if
the parent set includes all direct flights, one child
might include all direct cheap flights whereas an-
other child includes all direct expensive flights).
Leaf-nodes correspond either to a single option or
to a set of options with very similar values for all
attributes.
This tree structure determines the dialogue flow.
To minimize the need to explore several branches
of the tree, the user is asked for the most essential
criteria first, leaving less relevant criteria for later
in the dialogue. Thus, the branching criterion for
the first level of the tree is the attribute that has the
highest weight according to the user model. For
example, Figure 5 shows an option tree structure
68
rank attributes
1 fare class (preferred value: business)
2 arrival time, # of legs, departure time, travel time
6 airline (preferred value: KLM)
7 price, layover airport
Figure 4: Attribute ranking for business user
Figure 5: Option tree for business user
for our ?business? user model (Figure 4).
The advantage of this ordering is that it mini-
mizes the probability that the user needs to back-
track. If an irrelevant criterion had to be decided
on first, interesting tradeoffs would risk being scat-
tered across the different branches of the tree.
A special case occurs when an attribute is ho-
mogeneous for all options in an option set. Then a
unary node is inserted regardless of its importance.
This special case allows for more efficient summa-
rization, e.g., ?There are no business class flights
on KLM.? In the example of Figure 5, the attribute
airline is inserted far up in the tree despite its
low rank.
The user is not forced to impose a to-
tal ordering on the attributes but may specify
that two attributes, e.g., arrival-time and
number-of-legs, are equally important to her.
This partial ordering leads to several attributes
having the same ranking. For equally ranked at-
tributes, we follow the approach taken by Polifroni
et al (2003). The algorithm selects the attribute
that partitions the data into the smallest number
of sub-clusters. For example, in the tree in Fig-
ure 5, number-of-legs, which creates two
sub-clusters for the data set (direct and indirect),
comes before arrival-time, which splits the
set of economy class flights into three subsets.
The tree building algorithm introduces one of
the main differences between our structuring and
Polifroni?s refinement process. Polifroni et al?s
system chooses the attribute that partitions the data
into the smallest set of unique groups for sum-
marization, whereas in our system, the algorithm
takes the ranking of attributes in the user model
into account.
4.3 Pruning the Tree Structure
To determine the relevance of options, we did not
use the notion of compellingness (as was done in
(Moore et al, 2004; Carenini and Moore, 2001)),
but instead defined the weaker criterion of ?dom-
inance?. Dominant options are those for which
there is no other option in the data set that is better
on all attributes. A dominated option is in all re-
spects equal to or worse than some other option in
the relevant partition of the data base; it should not
be of interest for any rational user. All dominant
options represent some tradeoff, but depending on
the user?s interest, some of them are more interest-
ing tradeoffs than others.
Pruning dominated options is crucial to our
structuring process. The algorithm uses informa-
tion from the user model to prune all but the dom-
inant options. Paths from the root to a given op-
tion are thereby shortened considerably, leading to
a smaller average number of turns in our system
compared to Polifroni et al?s system.
An important by-product of the pruning al-
gorithm is the determination of attributes which
make an option cluster compelling with respect
to alternative clusters (e.g., for a cluster con-
taining direct flights, as opposed to flights that
require a connection, the justification would be
#-of-legs). We call such an attribute the ?jus-
tification? for a cluster, as it justifies its existence,
i.e., is the reason it is not pruned from the tree. Jus-
tifications are used by the generation algorithm to
present the tradeoffs between alternative options
explicitly.
Additionally, the reasons why options have
been pruned from the tree are registered and pro-
vide information for the summarization of bad op-
tions in order to give the user a better overview of
the option space (e.g., ?All other flights are either
indirect or arrive too late.?). To keep summaries
about irrelevant options short, we back off to a de-
fault statement ?or are undesirable in some other
way.? if these options are very heterogeneous.
69
4.4 Presenting Clusters
4.4.1 Turn Length
In a spoken dialogue system, it is important not
to mention too many facts in one turn in order to
keep the memory load on the user manageable.
Obviously, it is not possible to present all of the
options and tradeoffs represented in the tree in a
single turn. Therefore, it is necessary to split the
tree into several smaller trees that can then be pre-
sented over several turns. In the current implemen-
tation, a heuristic cut-off point (no deeper than two
branching nodes and their children, which corre-
sponds to the nodes shown in Figure 5) is used.
This procedure produces a small set of options to
present in a turn and includes the most relevant ad-
vantages and disadvantages of an option. The next
turn is determined by the user?s choice indicating
which of the options she would like to hear more
about (for illustration see Figure 6).
4.4.2 Identifying Clusters
The identification of an option set is based on
its justification. If an option is justified by several
attributes, only one of them is chosen for identi-
fication. If one of the justifications is a contex-
tually salient attribute, this one is preferred, lead-
ing to constructions like: ?. . . you?d have to make
a connection in Brussels. If you want to fly di-
rect,. . . ?). Otherwise, the cluster is identified by
the highest ranked attribute e.g.,?There are four
flights with availability in business class.?. If an
option cluster has no compelling homogeneous at-
tribute, but only a common negative homogeneous
attribute, this situation is acknowledged: e.g., ?If
you?re willing to travel economy / arrive later / ac-
cept a longer travel time, . . . ?.
4.4.3 Summarizing Clusters
After the identification of a cluster, more in-
formation is given about the cluster. All positive
homogeneous attributes are mentioned and con-
trasted against all average or negative attributes.
An attribute that was used for identification of
an option is not mentioned again in the elabora-
tion. In opposition to a single flight, attributes may
have different values for the entities within a set of
flights. In that case, these attribute values need to
be summarized.
There are three main cases to be distinguished:
1. The continuous values for the attributes
price, arrival-time etc. need to be
summarized, as they may differ in their val-
ues even if they are in the same cluster. One
way to summarize them is to use an ex-
pression that reflects their value range, e.g.
?between x and y?. Another solution is to
mention only the evaluation value, leading to
sentences like ?The two flights with shortest
travel time? or ?The cheapest flights.?
2. For discrete-valued attributes with a
small number of possible values, e.g.,
number-of-legs and fare-class,
summarization is not an issue, because when
homogeneous for a cluster, the attribute
values of its options are identical.
3. The third group are attributes with catego-
rial values, e.g., ?airline?. If there are no
more than three different values, we summa-
rize using quantifications like ?none/all/both
of them?, as done in (Polifroni et al, 2003).
If the values are more diverse, the user model
comes back into play to produce a tailored
summary based on user preferences (e.g., lik-
ing KLM). For example, we would generate
?None are on KLM.?, which takes into ac-
count the user?s preference and is shorter than
mentioning all airlines the flights are on.
An issue arising from summarization with nega-
tion is that the negated value has to be salient, oth-
erwise the utterance might be irritating. For exam-
ple, it would be better to say ?These flights are not
direct.? in a neutral context, but ?You would not
need to connect in London Heathrow.? if London
Heathrow had already been mentioned.
A sample dialogue produced by our system,
when given the business user model (see Figure 4),
is shown in Figure 6.
5 Evaluation
A within-participants laboratory experiment was
conducted in order to determine whether user
model-based clustering leads to increased overall
user satisfaction, a better overview of the avail-
able options, quicker accessibility to the optimal
option and higher confidence of having heard all
relevant options. The experiment furthermore as-
sessed whether the options were presented in a
way that users found understandable and recorded
the time users took to read a dialogue turn.
70
U: Hello. I?d like to book a flight from Edinburgh to Brus-
sels for tomorrow arriving at around 3pm.
S: There are four flights with availability in business class.
None are on KLM. The only direct flight arrives at 5:30
pm, which is later than you requested. To arrive earlier,
you?ll have to make a connection.
If you?re willing to travel economy, there is a direct flight
on BMI, arriving at 1:10 pm. There?s also a KLM flight
arriving at 1:50 pm, but it requires a connection in Am-
sterdam. All other economy flights take longer or arrive
after 4 pm.
U: Can you tell me more about the business class flights that
arrive earlier?
S: The two flights with shortest travel time are both on
British Airways and cost 308 pounds. The first takes 4
hours and 20 minutes and arrives at 1:20 pm. The other
takes 4 hours and 30 minutes and arrives at 3:30pm.
There?s a cheaper flight on Lufthansa. It arrives at
3:10pm and costs just 270 pounds, but it takes 5 hours
and 35 minutes. All other business class flights arrive
very early or too late.
Figure 6: Example Dialogue with our System
5.1 Experimental Design
Each of the 38 subjects who completed the exper-
iment was presented with six dialogue pairs, the
first of which was used for training and was thus
not included in the analysis. Each dialogue pair
consisted of one dialogue between a user and our
system and one dialogue between the same user
and a system designed as described in (Polifroni
et al, 2003; Chung, 2004) (cf. Section 2.2). Some
of the dialogues with our system were constructed
manually based on the content selection and struc-
turing step, because the generation component did
not cover all linguistic constructions needed. The
dialogues with the Chung system were designed
manually, as this system is implemented for an-
other domain. The order of the dialogues in a pair
was randomized. The dialogues were provided as
transcripts.
After reading each dialogue transcript, partici-
pants were asked four questions about the system?s
responses. They provided their answers using Lik-
ert scales.
1. Did the system give the information in a way that was
easy to understand?
1: very hard to understand
7: very easy to understand
2. Did the system give you a good overview of the avail-
able options?
1: very poor overview
7: very good overview
3. Do you think there may be flights that are better options
for X1 that the system did not tell X1 about?
1X was instantiated by name of our example users.
1: I think that is very possible
7: I feel the system gave a good overview of all options
that are relevant for X1.
4. How quickly did the system allow X1 to find the opti-
mal flight?
1: slowly
3: quickly
After reading each pair of dialogues, the partic-
ipants were also asked the forced choice question:
?Which of the two systems would you recommend
to a friend?? to assess user satisfaction.
5.2 Results
A significant preference for our system was ob-
served. (In the diagrams, our system which com-
bines user modelling and stepwise refinement is
called UMSR, whereas the system based on Po-
lifroni?s approach is called SR.) There were a total
of 190 forced choices in the experiment (38 par-
ticipants * 5 dialogue pairs). UMSR was preferred
120 times (? 0.63%), whereas SR was preferred
only 70 times (? 0.37%). This difference is highly
significant (p < 0.001) using a two-tailed bino-
mial test. Thus, the null-hypothesis that both sys-
tems are preferred equally often can be rejected
with high confidence.
The evaluation results for the Likert scale ques-
tions confirmed our expectations. The SR dia-
logues received on average slightly higher scores
for understandability (question 1), which can be
explained by the shorter length of the system turns
for that system. However, the difference is not
statistically significant (p = 0.97 using a two-
tailed paired t-test). The differences in results
for the other questions are all highly statistically
significant, especially for question 2, assessing
the quality of overview of the options given by
the system responses, and question 3, assessing
the confidence that all relevant options were men-
tioned by the system. Both were significant at
p < 0.0001. These results confirm our hypothe-
sis that our strategy of presenting tradeoffs explic-
itly and summarizing irrelevant options improves
users? overview of the option space and also in-
creases their confidence in having heard about all
relevant options, and thus their confidence in the
system. The difference for question 4 (accessibil-
ity of the optimal option) is also statistically sig-
nificant (p < 0.001). Quite surprisingly, subjects
reported that they felt they could access options
more quickly even though the dialogues were usu-
ally longer. The average scores (based on 190 val-
71
Figure 7: Results for all Questions
ues) are shown in Figure 7.
To get a feel for whether the content given by
our system is too complex for oral presentation
and requires participants to read system turns sev-
eral times, we recorded reading times and corre-
lated them to the number of characters in a system
turn. We found a linear relation, which indicates
that participants did not re-read passages and is a
promising sign for the use of our strategy in SDS.
6 Conclusions and Future Work
In this paper, we have shown that information pre-
sentation in SDS can be improved by an approach
that combines a user model with structuring of
options through clustering of attributes and suc-
cessive refinement. In particular, when presented
with dialogues generated by a system that com-
bines user modelling with successive refinement
(UMSR) and one that uses refinement without ref-
erence to a user model (SR), participants reported
that the combined system provided them with a
better overview of the available options and that
they felt more certain to have been presented with
all relevant options. Although the presentation of
complex tradeoffs usually requires relatively long
system turns, participants were still able to cope
with the amount of information presented. For
some dialogues, subjects even felt they could ac-
cess relevant options more quickly despite longer
system turn length.
In future work, we would like to extend the clus-
tering algorithm to not use a fixed number of tar-
get clusters but to depend on the number of natural
clusters the data falls into. We would also like to
extend it to be more sensitive to the user model
when forming clusters (e.g., to be more sensitive
at lower price levels for a user for whom price is
very important than for a user who does not care
about price).
The explicit presentation of tradeoffs made by
the UMSR system in many cases leads to dialogue
turns that are more complex than typical dialogue
turns in the SR system. Even though participants
did not report that our system was harder to under-
stand, it would be interesting to investigate how
well users can understand and remember informa-
tion from the system when part of their concentra-
tion is absorbed by another task, for example when
using the system while driving a car.
Acknowledgments
We would like to thank the anonymous review-
ers for their comments. The research is supported
by the TALK project (European Community IST
project no. 507802), http://www.talk-project.org.
The first author was supported by Evangelisches
Studienwerk e.V. Villigst.
References
G. Carenini and J.D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument ef-
fectiveness. In Proc. of IJCAI 2001.
G. Chung. 2004. Developing a flexible spoken dialog system
using simulation. In Proc. of ACL ?04.
V. Demberg. 2005. Information presentation in spoken di-
alogue systems. Master?s thesis, School of Informatics,
University of Edinburgh.
J.D. Moore, M.E. Foster, O. Lemon, and M. White. 2004.
Generating tailored, comparative descriptions in spoken
dialogue. In Proc. of the 17th International Florida Artifi-
cial Intelligence Research Sociey Conference, AAAI Press.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards au-
tomatic generation of mixed-initiative dialogue systems
from web content. In Proc. of Eurospeech ?03, Geneva,
Switzerland, pp. 193?196.
Y. Qu and S. Beale. 1999. A constraint-based model for
cooperative response generation in information dialogues.
In AAAI/IAAI 1999 pp. 148?155.
M. Steedman 2000. Information structure and the syntax-
phonology interface. In Linguistic Inquiry, 31(4): 649?
689.
A. Stent, M.A. Walker, S. Whittaker, and P. Maloor. 2002.
User-tailored generation for spoken dialogue: an experi-
ment. In Proc. of ICSLP-02.
M.A. Walker, S. Whittaker, A. Stent, P. Maloor, J.D. Moore,
M. Johnston, and G. Vasireddy. 2004. Generation and
evaluation of user tailored responses in dialogue. In Cog-
nitive Science 28: 811-840.
M.A.Walker, R. Passonneau, and J.E. Boland. 2001. Quanti-
tative and qualitative evaluation of DARPA communicator
spoken dialogue systems. In Proc of ACL-01.
72
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 356?367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Syntactic surprisal affects spoken word duration in conversational contexts
Vera Demberg, Asad B. Sayeed, Philip J. Gorinski, and Nikolaos Engonopoulos
M2CI Cluster of Excellence and
Department of Computational Linguistics and Phonetics
Saarland University
66143 Saarbru?cken, Germany
{vera,asayeed,philipg,nikolaos}@coli.uni-saarland.de
Abstract
We present results of a novel experiment to in-
vestigate speech production in conversational
data that links speech rate to information den-
sity. We provide the first evidence for an asso-
ciation between syntactic surprisal and word
duration in recorded speech. Using the AMI
corpus which contains transcriptions of focus
group meetings with precise word durations,
we show that word durations correlate with
syntactic surprisal estimated from the incre-
mental Roark parser over and above simpler
measures, such as word duration estimated
from a state-of-the-art text-to-speech system
and word frequencies, and that the syntac-
tic surprisal estimates are better predictors of
word durations than a simpler version of sur-
prisal based on trigram probabilities. This re-
sult supports the uniform information density
(UID) hypothesis and points a way to more re-
alistic artificial speech generation.
1 Introduction
The uniform information density (UID) hypothesis
suggests that speakers try to distribute information
uniformly across their utterances (Frank and Jaeger,
2008). Information density can be measured in
terms of the surprisal incurred at each word, where
surprisal is defined as the negative log-probability
of an event. This paper sets out to test whether UID
holds across different linguistic levels, i.e. whether
speakers adapt word duration during production to
syntactic surprisal, such that words with higher sur-
prisal have longer durations than words with lower
surprisal. We investigate this question in a corpus
of transcribed speech from a mix of native and non-
native English speakers, a population that is a non-
trivial component of the user base for language tech-
nologies developed for English. This data reflects a
casual, uncontrolled conversational environment.
Using linear mixed-effects modeling, we found
that syntactic surprisal as calculated from a top-
down incremental PCFG parser accounts for a sig-
nificant amount of variation in spoken word dura-
tion, using an HMM-trained text-to-speech system
as a baseline. The findings of this paper provide ad-
ditional support the uniform information density hy-
pothesis and furthermore have implications for the
design of text-to-speech systems, which currently
do not take into account higher-level linguistic in-
formation such as syntactic surprisal (or even word
frequencies) for their word duration models.
1.1 Related work
The use of word-level surprisal as a predictor of pro-
cessing difficulty is based on the notion that pro-
cessing difficulty results when a word is encountered
that is unexpected given its preceding context. The
amount of surprisal on a word wi can be formal-
ized as the log of the inverse conditional probabil-
ity of wi given the preceding words in the sentence
w1 . . . wi?1, or ? logP (wi|w1...i?1). If this proba-
bility is low, then the word is unexpected, and sur-
prisal is high. Surprisal can be estimated in different
ways, e.g. from word sequences (n-grams) or with
respect to the possible syntactic structures covering
a sentence prefix (see Section 4).
Hale (2001) showed that surprisal calculated from
a probabilistic Earley parser correctly predicts well-
356
known processing phenomena that were believed
to emerge from structural ambiguities (e.g., garden
paths) and Levy (2008) further demonstrated the rel-
evance of surprisal to human sentence processing
difficulty on a range of syntactic processing diffi-
culty phenomena.
There is existing work in correlating information-
theoretic measures of linguistic redundancy to the
observed duration of speech units. Aylett and Turk
(2006) demonstrate that the contextual predictability
of a syllable (n-gram log probability) has an inverse
relationship to syllable duration in speech. Their ex-
periments were performed using a carefully articu-
lated speech synthesis training corpus.
This type of work fits into a larger programme of
understanding how speakers schedule utterances to
avoid high variation in the transmission of linguis-
tic information over time, also known as the Uni-
form Information Density (UID) hypothesis (Flo-
rian Jaeger, 2010). Levy and Jaeger (2007) show
that the reduction of optional that-complementizers
in English is related to trigram surprisal; low sur-
prisal predicts a high likelihood of reduction. Flo-
rian Jaeger (2010) shows the same result of in-
creased reduction when the complementizer is more
predictable according to information density calcu-
lated in terms of the main verb?s subcategorization
frequency.
Frank and Jaeger (2008) provide evidence that a
UID account can predict the use of reduced forms
of ?be?, ?have?, and ?not? in English. They use the
surprisal of the candidate word itself as well as sur-
prisals of the word before and after, computing bi-
gram and trigram estimates directly from the corpus
without smoothing or backoff.
Jurafsky et al2001) report a corpus study sim-
ilar to ours, showing that words that are more pre-
dictable from context are reduced. As measures
of word predictability, they use bigram and trigram
models, as well as joint probabilities, but not syntac-
tic surprisal.
Within the same theme of utterance duration
vs. information content, Piantadosi et al2011)
performed a study using Google-derived n-gram
datasets on the lexica of multiple languages, includ-
ing English, Portuguese, and Czech. For every word
in a given language?s lexicon, they calculated 2-, 3-,
and 4-gram surprisal values using the Google dataset
for every occurrence of the word, and then they
took the mean surprisal for that word over all oc-
currences. The 3-gram surprisal values in particular
were a better predictor of orthographic length than
unigram frequency, providing evidence for the use
of information content and contextual predictability
as improvement over a Zipf?s Law view of commu-
nicative efficiency. This is an n-gram approach to
supporting the UID hypothesis.
However, there is some counter-evidence for the
UID-based view. Kuperman et al2007) analyzed
the relationship between linguistic unit predictabil-
ity and syllable duration in read-aloud speech in
Dutch. Dutch makes use of interfix morphemes
-s- and -e(n)- in certain contexts to make com-
pound nouns, preferring a null interfix in most
cases. For example, the Dutch noun kandidaatsex-
amen (?Bachelor?s examination?) is composed of
kandidaat-, -s-, and -examen.
Kuperman et alind that the greater the pre-
dictability of the interfix from the morphological
context (i.e., the surrounding members of the com-
pound), the longer the duration of the pronuncia-
tion of the interfix. To illustrate, if -s- is more ex-
pected after kandidaat or if kandidaatsexamen is a
frequent compound, we would therefore expect the
-s- to be pronounced longer, given the correlations
they found. Their finding runs counter to a strong
view of UID?s fine-grained control over speech rate,
but it is focused on the morphological level. They
hypothesize that this counter-intuitive result may be
driven by complex paradigmatic constraints in the
choice of morpheme.
Our work, however, focuses on the syntactic level
rather than the paradigmatic. What we seek to an-
swer in our work is the extent to which an infor-
mation density-based analysis can not only be ap-
plied to real speech data in context but also be de-
rived from higher-level syntactic analyses, a com-
bination hitherto little explored. Existing broad-
coverage work on syntactic surprisal has largely fo-
cused on comprehension phenomena, such as Dem-
berg and Keller (2008), Roark et al2009), and
Frank (2010). We provide a production study in a
vein similar to that of Kuperman et albut show that
frequency effects work in the expected direction at
the syntactic level. This in turn expands upon the
view supported by n-gram-based work such as that
357
of Piantadosi et al2011); Levy and Jaeger (2007);
Jurafsky et al2001), showing that information con-
tent above the n-gram level is important in guiding
spoken language production in humans.
1.2 Implications for Potential Applications
Spoken dialogue systems are of increasing eco-
nomic and technological importance in recent times,
particularly as it is now feasible to include this tech-
nology in everything from small consumer devices
to industrial equipment. With this increase in impor-
tance, there is also unsurprisingly growing scientific
emphasis in understanding its usability and safety
characteristics. Recent work (Fang et al2009;
Taube-Schiff and Segalowitz, 2005) has shown that
linguistic information presentation has an effect on
user behaviour, but the overall granularity of this be-
haviour is still not well-understood.
Other potential applications exist in any place
where text-to-speech technologies can be applied,
such as in real-time spoken machine translation and
communications systems for the disabled.
In demonstrating that we can observe speakers be-
having in the manner predicted by the UID hypoth-
esis in conversational contexts, we provide evidence
for a finer-level of granularity necessary for control-
ling the rate of information presentation in artificial
systems.
1.3 AMI corpus
The Augmented Multi-Party Interaction (AMI) cor-
pus is a collection of recorded, transcribed con-
versations spanning 100 hours of simulated meet-
ings. The corpus contains a number of data streams
including speech, video, and whiteboard writing.
Transcription of the meetings was performed man-
ually, and the transcripts contain word-level time
bounds that were produced by an automatic speech
recognition system.
The freely-available AMI corpus is one of a very
small number of efforts that contain orthographic
transcriptions that are time-aligned at a word level.
We chose it for the realism of the setting in which
it was recorded; the physical presence of multiple
speakers in an unstructured discussion reflects a po-
tentially high level of noise in which we would be
looking for surprisal correspondences, potentially
increasing the application value of the correspon-
dences we find.
1.4 Organization
The remainder of this paper proceeds as follows. In
section 2, we describe at a high level the procedure
we used to test our hypothesis that parser-derived
surprisal values can partly account for utterance-
duration variation. Then (section 3.2) we discuss the
MARY text-to-speech system, from which we derive
?canonical? word utterance durations. We describe
the way we process and filter the AMI meeting cor-
pus in section 3.1. In section 4, we describe in detail
our predictors, frequency counts, trigram surprisal,
and Roark parser surprisal. Sections 5 and 6 de-
scribe how we use linear mixed effects modeling to
find significant correlations between our predictors
and the response variable, and we finally make some
concluding remarks in section 7.
2 Design
The overall design of our experiment is schemati-
cally depicted in Figure 1. We extract the words
and the word-by-word timings from the AMI corpus,
keeping track of each word?s position in the corpus
by conversation ID, speaker turn, and chronological
order. As we describe in the next section, we filter
the words for anomalies.
After pre-processing, for each word in the cor-
pus, we extract the following predictors: canoni-
cal speech durations from the MARY text-to-speech
system, logarithmic word frequencies, n-gram sur-
prisal, and surprisal values produced by the Roark
(2001a); Roark et al2009) parser (see Section 4).
The next sections describe how and from where
these values are obtained1.
Finally, we run mixed effects regression model
analyses (Baayen et al2008) with the observed
durations as a response variable and the predictors
mentioned above in order to detect whether syntac-
tic surprisal is a significant positive predictor of spo-
ken word durations above and beyond the more ba-
sic effects of canonical word duration and word fre-
quency.
1We will make this data widely available upon publication.
358
AMIcorpus Wordfiltrationandselection
Gigaword CMUtoolkit
AMIwordfreq. AMI n-gramsurprisal
Roarkparser
Roarksyntacticsurprisal
Observations
MARY
ComputedtimingsObservedtimings
Regressionanalysis
Relativesignificance
PennTreebank
Gigawordfreq. PTBn-gramsurprisal Gigawordn-gramsurprisal
Figure 1: Schematic overview of experiment.
3 Experimental materials
3.1 Corpus preparation
The AMI corpus is provided in the NITE XML
Toolkit (NXT) format. We developed a custom inter-
preter to assemble the relevant data streams: words,
meeting IDs, speaker IDs, speaker turns, and ob-
served word durations.
In addition to grouping and re-ordering the infor-
mation found in the original XML corpus, two more
steps were taken to eliminate confounding noise
from the data. Non-words (e.g. ?uhm?, ?uh-hmm?,
etc.) were filtered out, as were incomplete words or
incorrectly transcribed words (e.g. ?recogn?, ?some-
thi?, etc); the criterion for rejection was presence in
the English Gigaword corpus with subsequent mi-
nor corrections by hand, e.g., mapping unseen verbs
back into the corpus and correcting obvious com-
mon misspellings.2
Finally, turns that did not make for complete sen-
tences, e.g., utterances that were interrupted in mid-
2A reviewer asks about the extent to which our Gigaword fil-
tering process may remove words we might want to keep but ad-
mit words we want to reject. As Gigaword is mostly newswire
text, we do not expect the latter case to hold often. AMI is
hand-transcribed and uses consistent spellings for non-word in-
terjections (easy to remove), and any spelling mistakes would
have to coincide exactly with a Gigaword mistake.
The other way around (rejecting what should be allowed) is
easier to check, and we find that of 13K word types in AMI,
about 7.2% are rejected for non-appearance in Gigaword, after
filtering for interjections like ?mm-hmm?. However, we man-
ually checked them and returned all but 2.9% of word types to
the corpus. These tend to be very low-frequency types. The
manual check suggests that ultimately there would be few false
rejections.
359
sentence, were filtered out in order to maximize the
proportion of complete parses in surprisal calcula-
tion.
3.2 Word duration model
In order to investigate whether there is an association
between high/low surprisal and increased/decreased
word duration, one needs to have a baseline mea-
sure of what constitutes the ?canonical? duration of
each word?in other words, to account for the fact
that some words have longer pronunciations than
others. As one reviewer notes, one way of estimat-
ing word durations would be to calculate the aver-
age duration of each word in the corpus. However,
this approach would be insensitive to the phonolog-
ical, syllabic and phrasal context that a word oc-
curs in, which can have a large effect on word du-
ration. Therefore, we use word duration estimates
from the state-of-the-art open-source text-to-speech
system MARY (Schro?der et al2008, version 4.3.1),
with the default voice package included in this ver-
sion (cmu-slt-hsmm).
The cmu-slt-hsmm voice package uses
a Hidden Markov model, trained on the fe-
male US English section of the CMU ARCTIC
database (Kominek and Black, 2003), to predict
prosodic attributes of each individual synthesized
phone, including duration. Training was carried
out using a version of the HTS system (Zen et al
2007), modified for using the MARY context
features (Schro?der et al2008) for estimating the
parameters of the model and for decoding. Those
features include3:
? phonological features of the current and neigh-
boring phonemes
? syllabic and lexical features (e.g. syllable
stress, (estimated) part-of-speech, position of
syllable in word)
? phrasal / sentential features (e.g. sen-
tence/phrase boundaries, neighboring pauses
and punctuation)
For each word in the AMI corpus, we ob-
tained two alternative estimates of word duration:
3For further information about how HMM-based voices for
MARY TTS are trained, see http://mary.opendfki.
de/wiki/HMMVoiceCreation
one version which is independent of a word?s
sentential context, and a second version which
does take into account the sentential context (such
as phrasal/sentential and across-word-boundaries
phonological features) the word occurs in. In other
words, we obtain MARY word duration estimates
in the second version by running individual whole
sentences through MARY, segmented by standard
punctuation marks used in the AMI corpus transcrip-
tions. For each version, we obtained phone dura-
tions using MARY and calculate the total duration of
a word as the sum of the estimated phone durations
for that word. These durations serve as the ?canoni-
cal? baselines to which the observed durations of the
words in the AMI corpus are compared.
3.3 Word frequency baselines
In order to account for the effects of simple word
frequency on utterance duration, we extracted two
types of frequency counts. One was taken di-
rectly from the AMI corpus alone. The other was
taken from a 151 million-word (4.3 million full-
paragraph) sample of the English Gigaword cor-
pus. These came from the following newswire
sources: Agence France Press, Associated Press
Worldstream, New York Times Newswire, and the
Xinhua News Agency English Service. These
sources are organized by month-of-year. We se-
lected the subset of Gigaword by randomly select-
ing month-of-year files from those sources with uni-
form probability. Punctuation was stripped from the
beginnings and ends of words before taking the fre-
quency counts.
4 Surprisal models
For predicting the surprisal of utterances in context,
two different types of models were used? n-gram
probabilities models, as well as Roark?s 2001 incre-
mental top-down parser capable of calculating pre-
fix probabilities. We also estimated word frequen-
cies to account for words being spoken more quickly
due to their higher frequency which is independent
of structural surprisal.
The n-gram probabilities models, while being fast
in both training and application, inherently capture
very limited contextual influences on surprisal. The
full-fledged parser, on the other hand, quantifies sur-
360
prisal based in the prefix probability of the complete
sentence prefix and captures long-distance effects
by conditioning on c-commanding lexical items as
well as non-local node labels such as parents, grand-
parents and siblings from the left context.
CMU n-grams We used the CMU Statistical Nat-
ural Language Modeling Toolkit to provide a con-
venient way to calculate n-grams probabilities. For
the prediction of surprisal, we calculated 3-gram
models, 4-gram models and 5-gram models with
Witten-Bell smoothing. Different n-gram models
were trained on the full Gigaword corpus, as well
as the AMI corpus.
To avoid overfitting, the AMI text corpus was split
into 10 sub-corpora of equal word counts, preserv-
ing coherence of meetings. N-gram probabilities
were then calculated for each of the sub-corpora us-
ing models trained on the 9 others.
We also produced a trigram model using the text
of chapter 2?21 of the Penn Treebank?s (PTB) un-
derlying Wall Street Journal corpus. This consists
of approximately one million tokens. We generated
this model because it is the underlying training data
for the Roark parser, described below.
Syntactic Surprisal from Roark parser In order
to capture the effect of syntactically expected vs. un-
expected events, we can calculate the syntactic sur-
prisal of each word in a sentence. The syntactic sur-
prisal at word Swi is defined as the difference be-
tween the prefix probability at word wi and the pre-
fix probability at word wi?1. The prefix probability
at word wi is the sum of the probabilities of all trees
T spanning words w1 . . . wi; see also (Levy, 2008;
Demberg and Keller, 2008).
Swi = log
?
T
P (T,w1..wi?1)? log
?
T
P (T,w1..wi)
The top-down incremental Roark parser (Roark,
2001a) has the characteristic that all partial left-to-
right parses are rooted: they form a single tree with
one root. A set of heuristics ensures that rule appli-
cation occurs only through node expansion within
the connected structure.4 The grammar-derived pre-
fix probabilities of a given sentence prefix can there-
4The formulae for the calculation of the prefix probabilities
from the PCFG rules can be found in Roark et al2009).
S
NP
DT
A
3.989
NN
puppy
4.570
VP
AUX
is
3.089
S
VP
TO
to
3.873
PP
TO
to
3.873
NP
DT
a
5.973
Figure 2: Top-ranked partial parse of A puppy is to a dog
what a kitten is to a cat., stopping at the second a and
providing the Roark parser surprisal values by word. The
branch with dashed lines and struck-out symbols repre-
sents an analysis abandoned at the appearance of the a.
fore be calculated directly by multiplying the prob-
abilities of all rules used to generate the prefix tree.
The Roark parser shares this characteristic of gener-
ating fully connected structures with Earley parsers
(Earley, 1970) and left corner parsers (Rosenkrantz
and II, 1970).
The Roark parser uses a beam search. As the
amount of probability mass lost has been shown
to be small (Roark, 2001b), the surprisal estimates
can be assumed to be a good approximation. The
beam width of the parser search is controlled by a
?base parsing threshold?, which defines the distance
in terms of natural log-probability between the most
probable parse and the least probable parse within
the beam. For the experiments reported here, the
parsing beam was set to 21 (default setting is 12). A
wider beam also reduces the effects of pruning.
The parser was trained on Wall Street Journal sec-
tions 2?21 and applied to parse the full sentences
of the AMI corpus, collecting predicted surprisal at
each word (see Figure 2 for an example).
The syntactic surprisal can be furthermore be de-
composed into a structural and a lexical part: some-
times, high surprisal might be due to a word be-
ing incompatible with the high-probability syntactic
structures, other times high surprisal might just be
due to a lexical item being unexpected. It is inter-
361
esting to evaluate these two aspects of syntactic sur-
prisal separately, and the Roark parser conveniently
outputs both surprisal estimates. Structural surprisal
is estimated from the occurrence counts of the appli-
cation of syntactic rules during the parse discount-
ing the effect of lexical probabilities, while lexical
surprisal is calculated from the probabilities of the
derivational step from the POS-tag to lexical item.
5 Linear mixed effects modelling
In order to test whether surprisal estimates correlate
with speech durations, we use linear mixed effects
models (LME, Pinheiro and Bates (2000)). This
type of model can be thought of as a generalization
of linear regression that allows the inclusion of ran-
dom factors as well as fixed factors.We treat speak-
ers as a random factor, which means that our mod-
els contain an intercept term for each speaker, rep-
resenting the individual differences in speech rates.
Furthermore, we include a random slope for the
predictors (e.g. frequency, canonical duration, sur-
prisal), essentially accounting for idiosyncrasies of
a participant with respect to the predictor, such that
only the part of the variance that is common to all
participants and is attributed to that predictor.
In a first step, we fit a baseline model with all pre-
dictors related to a word?s canonical duration and its
frequency as well as their random slopes to the ob-
served word durations. Models with more than two
random slopes generally did not converge. We there-
fore included in the baseline model only the two best
random slopes (in terms of model fit). We then cal-
culated the residuals of that model, the part of the
observed word durations that cannot be accounted
for through canonical word durations or word fre-
quency.
For each of our predictors of interest (n-gram sur-
prisal, syntactic surprisal), we then fit another lin-
ear mixed-effects model with random slopes to the
residuals of the baseline model. This two-step pro-
cedure allows us to make sure to avoid problems
of collinearity between e.g. surprisal and word fre-
quency or canonical duration. A simpler (but less
conservative) method is to directly add the predic-
tors of interest to the baseline model. Results for
both modelling variants lead to the same conclusions
for our model, so we here report the more conserva-
tive two-step model. We compare models based on
the Akaike Information Criterion (AIC).
6 Results
Our baseline model uses speech durations from the
AMI corpus as the response variable and canoni-
cal duration estimates from the MARY TTS system
and log word frequencies as predictors. We exclude
from the analysis all data points with zero duration
(effectively, punctuation) or a real duration longer
than 2 seconds. Furthermore, we exclude all words
which were never seen in Gigaword and any words
for which syntactic surprisal couldn?t be estimated.
This leaves us with 771,234 out of the 799,997 data
points with positive duration.
MARY duration models As mentioned in the
earlier sections, we have calculated different ver-
sions of the MARY estimated word durations: one
model without the sentential context and one model
with the sentential context. In our regression analy-
ses, we find, as expected, that the model which in-
cludes sentential context achieves a much better fit
with the actually measured word durations from the
AMI corpus (AIC = 32167) than the model without
context (AIC = 70917).
Word frequency estimates We estimated word
frequencies from several different resources, from
the AMI corpus to have a spoken domain frequency
and from Gigaword as a very large resource. We
find that both frequency estimates significantly im-
prove model fit over a model that does not contain
frequency estimates. Including both frequency esti-
mates improves model fit with respect to a model
that includes just one of the predictors (all p <
0.0001).
Furthermore, including into the regression an in-
teraction of estimated word duration and word fre-
quency also significantly increases model fit (p <
0.0001). This means that words which are short and
frequent have longer duration than would be esti-
mated by adding up their length and frequency ef-
fects.
Baseline model Fixed effects of the fitted model
are shown in Table 2. We see a highly significant ef-
fect in the expected direction for both the canonical
duration estimate and word frequency. The positive
362
coefficient for MARY CONTEXT means that TTS
duration estimates are positively correlated with the
measured word durations. The negative coefficient
for WORDFREQUENCY means that more frequent
words are spoken faster than less frequent words.
Finally, the negative coefficient for the interaction
between word durations and frequencies means that
the duration estimate for short frequent and long in-
frequent words is less extreme than otherwise pre-
dicted by the main effects of duration and frequency.
Ami Mary Mary Giga PTB AMI AMI Giga
Dur Word Cntxt Freq Freq Freq 3grm 4grm
Mary Word .36 1
Mary Cntxt .42 .72 1
GigaFreq -.35 -.52 -.65 1
PTBFreq -.33 -.48 -.62 .98 1
AMIFreq -.33 -.61 -.57 .65 .62 1
AMI3gram .21 .40 .41 -.41 -.39 -.68 1
Giga4gram .24 .33 .44 -.59 -.59 -.44 .61 1
Srprsl .29 .40 .48 -.71 -.73 -.50 .50 .73
Table 1: Correlations (pearson) of model predictors.
Note though that the predictors are also correlated
(for correlations of the main predictors used in these
analyses, see Table 1), so there is some collinearity
in the below model. Since we are less interested in
the exact coefficients and significance sizes for these
baseline predictors, this does not have to bother us
too much. What is more important, is that we re-
move any collinearity between the baseline predic-
tors and our predictors of interest, i.e. the surprisal
estimates from the ngram models and parser. There-
fore, we run separate regression models for these
predictors on the residuals of the baseline model.
N-gram estimates We estimated 3-gram, 4-gram
and 5-gram models on the AMI corpus (9-fold-
Predictor Coef t-value Sig
INTERCEPT 0.3098 212.11 ***
MARY CONTEXT 0.4987 95.48 ***
AMIWORDFREQUENCY -0.0282 -32.28 ***
GIGAWORDFREQUENCY -0.0275 -62.44 ***
MARY CNTXT:GIGAFREQ -0.0922 -45.41 ***
Table 2: Baseline linear mixed effects model of speech
durations on the AMI corpus data for MARY CONTEXT
(including the sentential context), WORDFREQUENCY
under speaker with random intercept for speaker and ran-
dom slopes under speaker. Predictors are centered.
Predictor Coef t-value Sig
INTERCEPT 0.3099 212.94 ***
MARY CONTEXT 0.4970 94.60 ***
AMIWORDFREQUENCY -0.0279 -31.98 ***
GIGAWORDFREQUENCY -0.0254 -53.68 ***
GIGA4GRAMSURPRISAL 0.0027 11.81 ***
MARY CNTXT:GIGAFREQ -0.0912 -44.87 ***
Table 3: Linear mixed effects model of speech durations
including 4-gram surprisal trained on gigaword as a pre-
dictor.
cross), the Penn Treebank and the Gigaword Cor-
pus. We found that coefficient estimates and signif-
icance levels of the resulting models were compara-
ble. This is not surprising, given that 4-gram and 5-
gram models were backing of to 3-grams or smaller
contexts for more than 95% of cases on the AMI and
PTB corpora (both ca. 1m words), and thus were
correlated at p > .98. On the Gigaword Corpus,
the larger contexts were seen more often (5-grams:
11%, 4-grams: 36%), but still correlation with 3-
grams were high at (p > .96).
N-gram model surprisal estimated on newspaper
texts from PTB or Gigaword were statistically sig-
nificant positive predictors of spoken word durations
beyond simple word frequencies (but PTB ngram
surprisal did not improve fit over models containing
Gigaword frequency estimates). Counter-intuitively
however, ngram models estimated based on the AMI
corpus have a small negative coefficient in models
that already include word frequency as a predictor
? residuals of an AMI-estimated ngram model with
respect to word frequency are very noisy and do not
show a clear correlation anymore with word dura-
tions.
Surprisal Surprisal effects were found to have a
robust significant positive coefficient, meaning that
words with higher surprisal are spoken more slowly /
clearly than expected when taking into account only
canonical word duration and word frequency. Sur-
prisal achieves a better model fit than any of the
n-gram models, based on a comparsion of AICs,
and Surprisal significantly improved model fit over
a model including frequencies and ngram models
based on AMI and Gigaword. Table 4 shows the es-
timate for SURPRISAL on the residuals of the model
in Table 2.
363
Predictor Coef t-value Sig
INTERCEPT -0.0154 -23.45 ***
SURPRISAL 0.0024 26.09 ***
Table 4: Linear mixed effects model of surprisal (based
on Roark parser) with random intercept for speaker and
random slope. The response variable is residual word du-
rations from the model shown in Table 3.
Surprisal estimated from the Roark parser also
remains a significant positive predictor when re-
gressed against the residuals of a baseline model in-
cluding both 3-gram surprisal from the AMI corpus
and 4-gram surprisal from the Gigaword corpus. In
order to make really sure that the observed surprisal
effect has indeed to do with syntax and can not be
explained away as a frequency effect, we also cal-
culated frequency estimates for the corpus based on
the Penn Treebank. The significant positive surprisal
effect remains stable, also when run on the residuals
of a model which includes PTB trigrams and PTB
frequencies.
It is difficult from these regression models to in-
tuitively grasp the size of the effect of a particular
predictor on reading times, since one would have to
know the exact range and distribution of each pre-
dictor. To provide some intuition, we calculate the
estimated effect size of Roark surprisal on speech
durations. Per Roark surprisal ?unit?, the model es-
timates a 7 msec difference5. The range of Roark
surprisal in our data set is roughly from 0 to 25,
with most values between 2 and 15. For a word
like ?thing? which in one instance in the AMI cor-
pus was estimated with a surprisal of 2.179 and in
another instance as 16.277, the estimated difference
in duration between these instances would thus be
104msec, which is certainly an audible difference.
(Full range for Roark surprisal: 174msec, whereas
full range for gigaword 4gram surprisal is 35 msec.)
When analysing the surprisal effect in more detail,
we find that both the syntactic component of sur-
prisal and its lexical component are significant pos-
itive predictors of word durations, as well as the in-
teraction between them, which has a negative slope.
A model with the separate components and their in-
52.4msec for a unit of residualized Roark surprisal, but it
is even less intuitive what that means, hence we calculate with
non-residualized surprisal here.
Predictor Coef t-value Sig
INTERCEPT -0.0219 -18.77 ***
STRUCTSURPRISAL 0.0009 2.71 **
LEXICALSURPRISAL 0.0044 24.00 ***
STRUCT:LEXICAL -0.0004 - 6.83 ***
Table 5: Linear mixed effects model of residual speech
durations wrt. baseline model from Table 3, with random
intercept for speaker and random slope for structural and
lexical component of surprisal, estimated using the Roark
parser.
teraction achieves a better model fit (in AIC and BIC
scores) than a model with only the full surprisal ef-
fect. The detailed model is shown in Table 5.
To summarize, the positive coefficient of surprisal
means that words which carry a lot of information
from a structural point of view are spoken more
slowly than words that carry less such information.
These results thus provide good evidence for our
hypothesis that the predictability of syntactic struc-
ture affects phonetic realization and that speakers
use speech rate to achieve more uniform information
density.
Native vs. non-native speakers Finally, we also
compared effects in our native vs. non-native
speaker populations, see Table 6. Both populations
show the same effects and tell the same story (note
that significance values can?t be compared as the
sample sizes are different). It might be possible to
interpret the findings in the sense that native speak-
ers are more proficient at adapting their speech rate
to (syntactic) complexity to achieve more uniform
information density, given the slightly higher coeffi-
cient and significance for Surprisal for native speak-
ers. Since the effects are statistically significant
for both groups, we don?t want to make too strong
claims about differences between the groups.
7 Conclusions and future work
We have shown evidence in this work that syntac-
tic surprisal effects in transcribed speech data can
be detected through word utterance duration in both
native and non-native speech, and we did so using
a meeting corpus not specifically designed to iso-
late these effects. This result is the potential foun-
dation for futher work in applied, experimental, and
364
Native English Non-native
Predictor Coef t-value Sig Coef t-value Sig
INTERCEPT 0.2947 149.74 *** 0.3221 175.38 ***
MARY CONTEXT 0.5304 69.27 *** 0.4699 67.77 ***
AMIWORDFREQUENCY -0.0226 -18.10 *** -0.0321 -28.00 ***
GIGAWORDFREQUENCY -0.0264 -41.19 *** -0.0248 -39.58 ***
GIGAWORD4-GRAMS 0.0018 5.36 *** 0.0033 10.85 ***
MARY CONTEXT:GIGAFREQ -0.0810 -27.20 *** -0.0993 -35.71 ***
SURPRISAL 0.0033 24.21 *** 0.0018 15.09 ***
no of data points 320,592 391,106
*p < 0.05, **p < 0.01, ***p < 0.001
Table 6: Native speakers are possibly slightly better at adapting their speech rate to syntactic surprisal than non-native
speakers. Surprisal value is for model with residuals of other predictors as dependent variable.
theoretical psycholinguistics. It provides additional
direct support for approaches based on the UID hy-
pothesis.
From an applied perspective, the fact that fre-
quency and syntactic surprisal have a significant ef-
fect beyond what a HMM-trained TTS model would
predict for individual words is a case for further
research into incorporating syntactic models into
speech production systems. Our methodology im-
mediately provides a framework for estimating the
word-by-word effect on duration for increased nat-
uralness in TTS output. This is relevant to spo-
ken dialogue systems because it appears that syn-
thesized speech requires a greater level of attention
from the dialogue system users when compared to
the same words delivered in natural speech (Delogu
et al1998). Some of this effect may be attributable
to peaks in information density which are caused by
current generation systems not compensating for ar-
eas of high information density through speech rate,
lexical and structural choice.
Furthermore, syntax and semantics have been ob-
served to interact with the mode of speech deliv-
ery. Eye-tracking experiments by Swift et al2002)
showed that there was a synthetic vs. natural speech
difference in the time required to pay attention to
an object referred to using definite articles, but not
indefinite articles. Our result points a way towards
a direction for explaining of this phenomenon by
demonstrating that the differences between current-
technology artificial speech and natural speech can
be partially explained through higher-level syntactic
features.
However, further experimentation is required on
other measures of syntactic complexity (e.g. DLT,
Gibson (2000)) as well as other levels of representa-
tion such as the semantic level. From a theoretical
and neuroanatomical perspective, the finding that a
measure of syntactic ambiguity reduction has an ef-
fect on the phonological layer of production has ad-
ditional implications for the organization of the hu-
man language production system.
References
Aylett, M. and Turk, A. (2006). Language redun-
dancy predicts syllabic duration and the spec-
tral characteristics of vocalic syllable nuclei.
Journal of the acoustical society of America,
119(5):3048?3059.
Baayen, R., Davidson, D., and Bates, D. (2008).
Mixed-effects modeling with crossed random ef-
fects for subjects and items. Journal of memory
and language, 59(4):390?412.
Delogu, C., Conte, S., and Sementina, C. (1998).
Cognitive factors in the evaluation of synthetic
speech. Speech Communication, 24(2):153?168.
Demberg, V. and Keller, F. (2008). Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
109:193?210.
Earley, J. (1970). An efficient context-free parsing
algorithm. Commun. ACM, 13(2):94?102.
Fang, R., Chai, J. Y., and Ferreira, F. (2009). Be-
365
tween linguistic attention and gaze fixations in-
multimodal conversational interfaces. In Inter-
national Conference on Multimodal Interfaces,
pages 143?150.
Florian Jaeger, T. (2010). Redundancy and reduc-
tion: Speakers manage syntactic information den-
sity. Cognitive Psychology, 61(1):23?62.
Frank, A. and Jaeger, T. F. (2008). Speaking ra-
tionally: uniform information density as an opti-
mal strategy for language production. In The 30th
annual meeting of the Cognitive Science Society,
pages 939?944.
Frank, S. (2010). Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceed-
ings of the 2010 Workshop on Cognitive Model-
ing and Computational Linguistics, pages 81?89,
Uppsala, Sweden.
Gibson, E. (2000). Dependency locality theory: A
distance-dased theory of linguistic complexity. In
Marantz, A., Miyashita, Y., and O?Neil, W., ed-
itors, Image, Language, Brain: Papers from the
First Mind Articulation Project Symposium, pages
95?126. MIT Press, Cambridge, MA.
Hale, J. (2001). A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter
of the Association for Computational Linguistics,
volume 2, pages 159?166, Pittsburgh, PA.
Jurafsky, D., Bell, A., Gregory, M., and Raymond,
W. (2001). Evidence from reduction in lexical
production. Frequency and the emergence of lin-
guistic structure, 45:229.
Kominek, J. and Black, A. (2003). The cmu
arctic speech databases for speech synthesis
research. Language Technologies Institute,
Carnegie Mellon University, Pittsburgh, PA, Tech.
Rep. CMULTI-03-177 http://festvox. org/cmu arc-
tic.
Kuperman, V., Pluymaekers, M., Ernestus, M., and
Baayen, H. (2007). Morphological predictability
and acoustic duration of interfixes in dutch com-
pounds. The Journal of the Acoustical Society of
America, 121(4):2261?2271.
Levy, R. (2008). Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Levy, R. and Jaeger, T. F. (2007). Speakers opti-
mize information density through syntactic reduc-
tion. In Advances in Neural Information Process-
ing Systems.
Piantadosi, S., Tily, H., and Gibson, E. (2011). Word
lengths are optimized for efficient communica-
tion. Proceedings of the National Academy of Sci-
ences, 108(9).
Pinheiro, J. C. and Bates, D. M. (2000). Mixed-
effects models in S and S-PLUS. Statistics and
computing series. Springer-Verlag.
Roark, B. (2001a). Probabilistic top-down parsing
and language modeling. Computational linguis-
tics, 27(2):249?276.
Roark, B. (2001b). Robust probabilistic predictive
syntactic processing: motivations, models, and
applications. PhD thesis, Brown University.
Roark, B., Bachrach, A., Cardenas, C., and Pal-
lier, C. (2009). Deriving lexical and syntactic
expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
324?333, Singapore. Association for Computa-
tional Linguistics.
Rosenkrantz, D. J. and II, P. M. L. (1970). Deter-
ministic left corner parsing (extended abstract). In
SWAT (FOCS), pages 139?152.
Schro?der, M., Charfuelan, M., Pammi, S., and Tu?rk,
O. (2008). The MARY TTS entry in the Bliz-
zard Challenge 2008. In Proc. Blizzard Chal-
lenge. Citeseer.
Swift, M. D., Campana, E., Allen, J. F., and Tanen-
haus, M. K. (2002). Monitoring eye movements
as an evaluation of synthesized speech. In Pro-
ceedings of the IEEE 2002 Workshop on Speech
Synthesis.
Taube-Schiff, M. and Segalowitz, N. (2005). Lin-
guistic attention control: attention shifting gov-
erned by grammaticized elements of language.
Journal of experimental psychology Learning
memory and cognition, 31(3):508?519.
Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko,
T., Black, A., and Tokuda, K. (2007). The HMM-
based speech synthesis system (HTS) version 2.0.
366
In Proc. of Sixth ISCA Workshop on Speech Syn-
thesis, pages 294?299.
367
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301?312,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incremental Semantic Role Labeling with Tree Adjoining Grammar
Ioannis Konstas
?
, Frank Keller
?
, Vera Demberg
?
and Mirella Lapata
?
?: Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
{ikonstas,keller,mlap}@inf.ed.ac.uk
?: Cluster of Excellence Multimodal Computing and Interaction,
Saarland University
vera@coli.uni-saarland.de
Abstract
We introduce the task of incremental se-
mantic role labeling (iSRL), in which se-
mantic roles are assigned to incomplete
input (sentence prefixes). iSRL is the
semantic equivalent of incremental pars-
ing, and is useful for language model-
ing, sentence completion, machine trans-
lation, and psycholinguistic modeling. We
propose an iSRL system that combines
an incremental TAG parser with a seman-
tically enriched lexicon, a role propaga-
tion algorithm, and a cascade of classi-
fiers. Our approach achieves an SRL F-
score of 78.38% on the standard CoNLL
2009 dataset. It substantially outper-
forms a strong baseline that combines
gold-standard syntactic dependencies with
heuristic role assignment, as well as a
baseline based on Nivre?s incremental de-
pendency parser.
1 Introduction
Humans are able to assign semantic roles such as
agent, patient, and theme to an incoming sentence
before it is complete, i.e., they incrementally build
up a partial semantic representation of a sentence
prefix. As an example, consider:
(1) The athlete realized [her
goals]
PATIENT/THEME
were out of reach.
When reaching the noun phrase her goals, the hu-
man language processor is faced with a semantic
role ambiguity: her goals can either be the PA-
TIENT of the verb realize, or it can be the THEME
of a subsequent verb that has not been encoun-
tered yet. Experimental evidence shows that the
human language processor initially prefers the PA-
TIENT role, but switches its preference to the
theme role when it reaches the subordinate verb
were. Such semantic garden paths occur because
human language processing occurs word-by-word,
and are well attested in the psycholinguistic litera-
ture (e.g., Pickering et al., 2000).
Computational systems for performing seman-
tic role labeling (SRL), on the other hand, proceed
non-incrementally. They require the whole sen-
tence (typically together with its complete syntac-
tic structure) as input and assign all semantic roles
at once. The reason for this is that most features
used by current SRL systems are defined globally,
and cannot be computed on sentence prefixes.
In this paper, we propose incremental SRL
(iSRL) as a new computational task that mimics
human semantic role assignment. The aim of an
iSRL system is to determine semantic roles while
the input unfolds: given a sentence prefix and its
partial syntactic structure (typically generated by
an incremental parser), we need to (a) identify
which words in the input participate in the seman-
tic roles as arguments and predicates (the task of
role identification), and (b) assign correct seman-
tic labels to these predicate/argument pairs (the
task of role labeling). Performing these two tasks
incrementally is substantially harder than doing it
non-incrementally, as the processor needs to com-
mit to a role assignment on the basis of incom-
plete syntactic and semantic information. As an
example, take (1): on reaching athlete, the proces-
sor should assign this word the AGENT role, even
though it has not seen the corresponding predicate
yet. Similarly, upon reaching realized, the pro-
cessor can complete the AGENT role, but it should
also predict that this verb also has a PATIENT role,
even though it has not yet encountered the argu-
ment that fills this role. A system that performs
SRL in a fully incremental fashion therefore needs
to be able to assign incomplete semantic roles,
unlike existing full-sentence SRL models.
The uses of incremental SRL mirror the applica-
tions of incremental parsing: iSRL models can be
used in language modeling to assign better string
probabilities, in sentence completion systems to
301
provide semantically informed completions, in
any real time application systems, such as dia-
log processing, and to incrementalize applications
such as machine translation (e.g., in speech-to-
speech MT). Crucially, any comprehensive model
of human language understanding needs to com-
bine an incremental parser with an incremental se-
mantic processor (Pad?o et al., 2009; Keller, 2010).
The present work takes inspiration from the
psycholinguistic modeling literature by proposing
an iSRL system that is built on top of a cogni-
tively motivated incremental parser, viz., the Psy-
cholinguistically Motivated Tree Adjoining Gram-
mar parser of Demberg et al. (2013). This parser
includes a predictive component, i.e., it predicts
syntactic structure for upcoming input during in-
cremental processing. This makes PLTAG par-
ticularly suitable for iSRL, allowing it to predict
incomplete semantic roles as the input string un-
folds. Competing approaches, such as iSRL based
on an incremental dependency parser, do not share
this advantage, as we will discuss in Section 4.3.
2 Related Work
Most SRL systems to date conceptualize seman-
tic role labeling as a supervised learning prob-
lem and rely on role-annotated data for model
training. Existing models often implement a
two-stage architecture in which role identification
and role labeling are performed in sequence. Su-
pervised methods deliver reasonably good perfor-
mance with F-scores in the low eighties on stan-
dard test collections for English (M`arquez et al.,
2008; Bj?orkelund et al., 2009).
Current approaches rely primarily on syntactic
features (such as path features) in order to iden-
tify and label roles. This has been a mixed bless-
ing as the path from an argument to the predi-
cate can be very informative but is often quite
complicated, and depends on the syntactic formal-
ism used. Many paths through the parse tree are
likely to occur infrequently (or not at all), result-
ing in very sparse information for the classifier to
learn from. Moreover, as we will discuss in Sec-
tion 4.4, such path information is not always avail-
able when the input is processed incrementally.
There is previous SRL work employing Tree Ad-
joining Grammar, albeit in a non-incremental set-
ting, as a means to reduce the sparsity of syntax-
based features. Liu and Sarkar (2007) extract a
rich feature set from TAG derivations and demon-
strate that this improves SRL performance.
In contrast to incremental parsing, incremental
semantic role labeling is a novel task. Our model
builds on an incremental Tree Adjoining Gram-
mar parser (Demberg et al., 2013) which predicts
the syntactic structure of upcoming input. This al-
lows us to perform incremental parsing and incre-
mental SRL in tandem, exploiting the predictive
component of the parser to assign (potentially in-
complete) semantic roles on a word-by-word ba-
sis. Similar to work on incremental parsing that
evaluates incomplete trees (Sangati and Keller,
2013), we evaluate the incomplete semantic struc-
tures produced by our model.
3 Psycholinguistically Motivated TAG
Demberg et al. (2013) introduce Psycholin-
guistically Motivated Tree Adjoining Grammar
(PLTAG), a grammar formalism that extends stan-
dard TAG (Joshi and Schabes, 1992) in order to
enable incremental parsing. Standard TAG as-
sumes a lexicon of elementary trees, each of
which contains at least one lexical item as an an-
chor and at most one leaf node as a foot node,
marked with A?. All other leaves are marked with
A? and are called substitution nodes. Elementary
trees that contain a foot node are called auxiliary
trees; those that do not are called initial trees. Ex-
amples for TAG elementary trees are given in Fig-
ure 1a?c.
To derive a TAG parse for a sentence, we start
with the elementary tree of the head of the sen-
tence and integrate the elementary trees of the
other lexical items of the sentence using two oper-
ations: adjunction at an internal node and substi-
tution at a substitution node (the node at which the
operation applies is the integration point). Stan-
dard TAG derivations are not guaranteed to be in-
cremental, as adjunction can happen anywhere in
a sentence, possibly violating left-to-right process-
ing order. PLTAG addresses this limitation by in-
troducing prediction trees, elementary trees with-
out a lexical anchor. These can be used to predict
syntactic structure anchored by words that appear
later in an incremental derivation. The use of pre-
diction trees ensures that fully connected prefix
trees can be built for every prefix of the input sen-
tence.
Each node in a prediction tree carries mark-
ers to indicate that this node was predicted, rather
than being anchored by the current sentence pre-
fix. An example is Figure 1d, which contains a
prediction tree with marker ?1?. In PLTAG, mark-
ers are eliminated through a new operation called
verification, which matches them with the nodes
302
(a) NP
NNS
Banks
(b) S
VP
VB
open
NP?
(c) VP
VP*AP
RB
rarely
(d) S
1
VP
1
1
NP
1
?
Figure 1: PLTAG lexicon entries: (a) and (b) ini-
tial trees, (c) auxiliary tree, (d) prediction tree.
a
S
 B? 
 C? 
a
S
B
 C? 
b
a
S
 B? 
C
c
(a) valid (b) invalid
Figure 3: The current fringe (dashed line) indi-
cates where valid substitutions can occur. Other
substitutions result in an invalid prefix tree.
of non-predictive elementary trees. An example
of a PLTAG derivation is given in Figure 2. In
step 1, a prediction tree is introduced through sub-
stitution, which then allows the adjunction of an
adverb in step 2. Step 3 involves the verification
of the marker introduced by the prediction tree
against the elementary tree for open.
In order to efficiently parse PLTAG, Demberg
et al. (2013) introduce the concept of fringes.
Fringes capture the fact that in an incremental
derivation, a prefix tree can only be combined with
an elementary tree at a limited set of nodes. For
instance, the prefix tree in Figure 3 has two substi-
tution nodes, for B and C. However, only substi-
tution into B leads to a valid new prefix tree; if we
substitute into C, we obtain the tree in Figure 3b,
which is not a valid prefix tree (i.e., it represents a
non-incremental derivation).
The parsing algorithm proposed by Demberg
et al. (2013) exploits fringes to tabulate interme-
diate results. It manipulates a chart in which each
cell (i, f ) contains all the prefix trees whose first
i leaves are the first i words and whose current
fringe is f . To extend the prefix trees for i to
the prefix trees for i+ 1, the algorithm retrieves
all current fringes f such that the chart has entries
in the cell (i, f ). For each such fringe, it needs
to determine the elementary trees in the lexicon
that can be combined with f using substitution or
adjunction. In spite of the large size of a typi-
cal TAG lexicon, this can be done efficiently, as
it only requires matching the current fringes. For
each match, the parser then computes the new pre-
Banks refused to open today
A0
A1
A1
AM-TMP
nsbj aux
xcomp
tmod
?A0,Banks,refused?
?A1,to,refused?
?A1,Banks,open?
?AM-TMP,today,open?
Figure 4: Syntactic dependency graph with se-
mantic role annotation and the accompanying se-
mantic triples, for Banks refused to open today.
fix trees and its new current fringe f
?
and enters it
into cell (i+1, f
?
).
Demberg et al. (2013) convert the Penn Tree-
bank (Marcus et al., 1993) into TAG for-
mat by enriching it with head information and
argument/modifier information from Propbank
(Palmer et al., 2005). This makes it possible
to decompose the Treebank trees into elementary
trees as proposed by Xia et al. (2000). Predic-
tion trees can be learned from the converted Tree-
bank by calculating the connection path (Mazzei
et al., 2007) at each word in a tree. Intuitively,
a prediction tree for word w
n
contains the struc-
ture that is necessary to connect w
n
to the prefix
tree w
1
. . .w
n?1
, but is not part of any of the ele-
mentary trees of w
1
. . .w
n?1
. Using this lexicon, a
probabilistic model over PLTAG operations can be
estimated following Chiang (2000).
4 Model
4.1 Problem Formulation
In a typical semantic role labeling scenario, the
goal is to first identify words that are predicates
in the sentence and then identify and label all the
arguments for each predicate. This translates into
spotting specific words in a sentence that repre-
sent the predicate?s arguments, and assigning pre-
defined semantic role labels to them. Note that in
this work we focus on verb predicates only. The
output of a semantic role labeler is a set of seman-
tic dependency triples ?l,a, p?, with l ? R , and
a, p ? w, where R is a set of semantic role labels
denoting a specific relationship between a predi-
cate and an argument (e.g., ARG0, ARG1, ARGM
in Propbank), w is the list of words in the sentence,
l denotes a specific role label, a the argument, and
p the predicate. An example is shown in Figure 4.
As discussed in the introduction, standard se-
mantic role labelers make their decisions based on
evidence from the whole sentence. In contrast, our
aim is to assign semantic roles incrementally, i.e.,
303
NP
NNS
Banks
S
1
VP
1
1
NP
NNS
Banks
S
1
VP
1
VP
1
AP
RB
rarely
NP
NNS
Banks
S
VP
VP
VB
open
AP
RB
rarely
NP
NNS
Banks
1. subst
2. adj
3. verif
Figure 2: Incremental parse for Banks rarely open using the operations substitution (with a prediction
tree), adjunction, and verification.
we want to produce a set of (potentially incom-
plete) semantic dependency triples for each prefix
of the input sentence. Note that not every word
is an argument to a predicate, therefore the set of
triples will not necessarily change at every input
word. Furthermore, the triples themselves may
be incomplete, as either the predicate or the argu-
ment may not have been observed yet (predicate-
incomplete or argument-incomplete triples).
Our iSRL system relies on PLTAG, using a se-
mantically augmented lexicon. We parse an in-
put sentence incrementally, applying a novel in-
cremental role propagation algorithm (IRPA) that
creates or updates existing semantic triple candi-
dates whenever an elementary (or prediction) tree
containing role information is attached to the ex-
isting prefix tree. As soon as a triple is completed
we apply a two-stage classification process, that
first identifies whether the predicate/argument pair
is a good candidate, and then disambiguates role
labels in case there is more than one candidate.
4.2 Semantic Role Lexicon
Recall that Propbank is used to construct the
PLTAG treebank, in order to distinguish between
arguments and modifiers, which result in elemen-
tary trees with substitution nodes, and auxiliary
trees, i.e., trees with a foot node, respectively (see
Figure 1). Conveniently, we can use the same in-
formation to also enrich the extracted lexicon with
the semantic role annotations, following the pro-
cess described by Sayeed and Demberg (2013).
1
For arguments, annotations are retained on the
substitution node in the parental tree, while for
modifiers, the role annotation is displayed on the
foot node of the auxiliary tree. Note that we dis-
play role annotation on traces that are leaf nodes,
1
Contrary to Sayeed and Demberg (2013) we put role la-
bel annotations for PPs on the preposition rather than their
NP child, following of the CoNLL 2005 shared task (Carreras
and M`arquez, 2005).
which enables us to recover long-range dependen-
cies (third and fifth tree in Figure 5a). Likewise,
we annotate prediction trees with semantic roles,
which enables our system to predict upcoming in-
complete triples.
Our annotation procedure unavoidably intro-
duces some role ambiguity, especially for fre-
quently occurring trees. This can give rise to two
problems when we generate semantic triples incre-
mentally: IRPA tends to create many spurious can-
didate semantic triples for elementary trees that
correspond to high frequency words (e.g., preposi-
tions or modals). Secondly, a semantic triple may
be identified correctly but is assigned several role
labels. (See the elementary tree for refuse in Fig-
ure 5a.) We address these issues by applying clas-
sifiers for role label disambiguation at every pars-
ing operation (substitution, adjunction, or verifica-
tion), as detailed in Section 4.4.
4.3 Incremental Role Propagation Algorithm
The main idea behind IRPA is to create or up-
date existing semantic triples as soon as there is
available role information during parsing. Our al-
gorithm (lines 1?6 in Algorithm 1) is applied af-
ter every PLTAG parsing operation, i.e., when an
elementary or prediction tree T is adjoined to a
particular integration point node pi
ip
of the prefix
tree of the sentence, via substitution or adjunction
(lines 3?4).
2
In case an elementary tree T
v
verifies
a prediction tree T
pr
(lines 5?6), the same method-
ology applies, the only difference being that we
have to tackle multiple integration point nodes
T
pr,ip
, one for each prediction marker of T
pr
that
matches the corresponding nodes in T
v
.
For simplicity of presentation, we will use a
concrete example, see Figure 5. Figure 5a shows
the lexicon entries for the words of the sentence
2
Prediction tree T
pr
in our algorithm is only used during
verification, so it set to nil for substitution and adjunction op-
erations.
304
Banks refused to open. Naturally, some nodes in
the lexicon trees might have multiple candidate
role labels. For example, the substitution NP node
of the second tree takes two labels, namely A0
and A1. These stem from different role signatures
when the same elementary tree occurs in differ-
ent contexts during training (A1 only on the NP;
A0 on the NP and A1 on S). For simplicity?s sake,
we collapse different signatures, and let a classi-
fier labeller to disambiguate such cases (see Sec-
tion 4.4).
Algorithm 1 Incremental Role Propagation Alg.
1: procedure IRPA(pi
ip
, T , T
pr
)
2: ??? . ? is a dictionary of (pi
ip
, ?l,a, p?) pairs
3: if parser operation is substitution or adjunction then
4: CREATE-TRIPLES(pi
ip
, T )
5: else if parser operation is verification then
6: CREATE-TRIPLES-VERIF(pi
ip
, T , T
pr
)
return set of triples ?l,a, p? for prefix tree pi
7: procedure CREATE-TRIPLES(pi
ip
, T )
8: if HAS-ROLES(pi
ip
) then
9: UPDATE-TRIPLE(pi
ip
, T )
10: else if HAS-ROLES(T ) then
11: T
ip
? substitution or foot node of T
12: ADD-TRIPLE(pi
ip
, T
ip
, T )
13: for all remaining nodes n ? T with roles do
14: ADD-TRIPLE(pi
ip
, n, T ) . incomplete triples
15: procedure CREATE-TRIPLES-VERIF(pi
ip
, T
v
, T
pr
)
16: if HAS-ROLES(T
v
) then
17: anchor? lexeme of T
v
18: for all T
ip
? node in T
v
with role do
19: T
pr,ip
? matching node of T
ip
in T
pr
20: CREATE-TRIPLES(T
pr,ip
, T
v
)
. Process the rest of covered nodes in T
pr
with roles
21: for all remaining T
pr,ip
? node in T
pr
with role do
22: UPDATE-TRIPLE(T
pr,ip
, T
pr
)
23: function UPDATE-TRIPLE(pi
ip
, T )
24: dep? FIND-INCOMPLETE(?, T
ip
)
25: anchor? lexeme of T
26: if anchor of T is predicate then
27: SET-PREDICATE(dep, anchor)
28: else if anchor of T is argument then
29: SET-ARGUMENT(dep, anchor)
return dep
30: procedure ADD-TRIPLE(pi
ip
, T
ip
, T )
31: dep? ?[roles of T
ip
], nil, nil?
32: anchor? lexeme of T
33: if anchor of T is predicate then
34: SET-PREDICATE(dep, anchor)
35: SET-ARGUMENT(dep, head of pi
ip
)
36: else if anchor of T is argument then
37: if T is auxiliary then . adjunction
38: SET-ARGUMENT(dep, anchor)
39: else . substitution: arg is head of prefix tree
40: SET-ARGUMENT(dep, head of T
ip
)
41: pred? find dep ? ? with matching pi
ip
42: SET-PREDICATE(dep, pred)
43: ?? (pi
ip
, dep)
Once we process Banks, the prefix tree becomes
the lexical entry for this word, see the first col-
umn of Figure 5b. Next, we process refused:
the parser substitutes the prefix tree into the ele-
mentary tree T of refused;
3
the integration point
pi
ip
on the prefix tree is the topmost NP. Since
the operation is a substitution (line 3), we create
triples between T and pi
ip
via CREATE-TRIPLES
(lines 7?12). pi
ip
does not have any role infor-
mation (line 8), so we proceed to add a new se-
mantic triple between the role-labeled integration
point T
ip
, i.e., substitution NP node of T , and pi
ip
,
via ADD-TRIPLE (lines 30?43). First, we create
an incomplete semantic triple with all roles from
T
ip
(line 31). Then we set the predicate to the an-
chor of T to be the word refused, and the argu-
ment to be the head word of the prefix tree, Banks
(lines 34?35). Note that predicate identification is
a trivial task based on part-of-speech information
in the elementary tree.
4
Then, we add the pair (NP? ?{A0,A1},Banks,
refused?) to a dictionary (line 43). Storing the in-
tegration point along with the semantic triple is
essential, to be able to recover incomplete triples
in later stages of the algorithm. Finally, we re-
peat this process for all remaining nodes on T that
have roles, in our example the substitution node S
(lines 13?14). This outputs an incomplete triple,
?{A1},nil,refused?.
Next, the parser decides to substitute a predic-
tion tree (third tree in Figure 5a) into the substitu-
tion node S of the prefix tree. Since the integration
point is on the prefix tree and has role information
(line 8), the corresponding triple should already be
present in our dictionary. Upon retrieving it, we
set the nil argument to the anchor of the incoming
tree. Since it is a prediction tree, we set it to the
root of the tree, namely S
2
(phrase labels in triples
are denoted by italics), but mark the triple as yet
incomplete. This distinction allows us to fill in the
correct lexical information once it becomes avail-
able, i.e, when the tree gets verified. We also add
an incomplete triple for the trace t in the subject
position of the prediction tree, as described above.
Note that this triple contains multiple roles; this is
expected given that prediction trees are unlexical-
ized and occur in a wide variety of contexts.
When the next verb arrives, the parser success-
fully verifies it against the embedded prediction
3
PLTAG parsing operations can occur in two ways: An
elementary tree can be substituted into the substitution node
of the prefix tree, or the prefix tree can be substituted into a
node of an elementary tree. The same holds for adjunction.
4
Most predicates can be identified as anchors of non-
modifier auxiliary trees. However, there are exceptions to
this rule, i.e., modifier auxiliary trees and non-modifier non-
auxiliary trees being also verbs in our lexicon, hence the use
of the more reliable POS tags.
305
IRPA MaltParser
Banks ? ?
refused ?{A0,A1},Banks,refused?,
?A1,S
2
,refused?,
?{A0,A1,A2},t,nil?
?A0,Banks,refused?
to ? ?
open ?A1,to,refused?,
?A1,Banks,open?
?A1,to,refused?,
?A0,Banks,open?
today ?AM-TMP,today,open? ?AM-TMP,today,open?
Table 1: Complete and incomplete semantic triple
generation, comparing IRPA and a system that
maps gold-standard role labels onto MaltParser in-
cremental dependencies for Figure 4.
tree within the prefix tree (last step of Figure 5b).
Our algorithm first cycles through all nodes that
match between the verification tree T
v
and the pre-
diction tree T
pr
and will complete or create new
triples via CREATE-TRIPLES (lines 18?20). In
our example, the second semantic triple gets com-
pleted by replacing S
2
with the head of the sub-
tree rooted in S. Normally, this would be the verb
open, but in this case the verb is followed by the
infinitive marker to, hence we heuristically set it
to be the argument of the triple instead, following
Carreras and M`arquez (2005). For the last triple,
we set the predicate to the anchor of T
v
open, and
now are able to remove the excess role labels A0
and A2. This illustrated how the lexicalized veri-
fication tree disambiguates the semantic informa-
tion stored in the prediction tree. Finally, trace t is
set to the closest NP head that is below the same
phrase subtree, in this case Banks. Note that Banks
is part of two triples as shown in the last tree of
Figure 5b: it is either an A0 or an A1 for refused
and an A1 for open.
We are able to create incomplete semantic
triples after the prediction of the upcoming verb at
step 2, as shown in Figure 5b. This is not possible
using an incremental dependency parser such as
MaltParser (Nivre et al., 2007) that lacks a predic-
tive component. Table 1 illustrates this by compar-
ing the output of IRPA for Figure 5b with the out-
put of a baseline system that maps role labels onto
the syntactic dependencies in Figure 4, generated
incrementally by MaltParser (see Section 5.3 for
a description of the MaltParser baseline). Malt-
Parser has to wait for the verb open before out-
putting the relevant semantic triples. In contrast,
IRPA outputs incomplete triples as soon as the in-
formation is available, and later on updates its de-
cision. (MaltParser also incorrectly assigns A0 for
the Banks?open pair.)
4.4 Argument Identification and Role Label
Disambiguation
IRPA produces semantic triples for every role an-
notation present in the lexicon entries, which will
often overgenerate role information. Furthermore,
some triples have more than one role label at-
tached to them. During verification, we are able to
filter out the majority of labels in the correspond-
ing prediction trees; However, most triples are cre-
ated via substitution and adjunction.
In order to address these problems we adhere to
the following classification and ranking strategy:
after each semantic triple gets completed, we per-
form a binary classification that evaluates its suit-
ability as a whole, given bilexical and syntactic in-
formation. If the triple is identified as a good can-
didate, then we perform multi-class classification
over role labels: we feed the same bilexical and
syntactic information to a logistic classifier, and
get a ranked list of labels. We then use this list to
re-rank the existing ambiguous role labels in the
semantic triple, and output the top scoring ones.
The identifier is a binary L2-loss support vec-
tor classifier, and the role disambiguator an L2-
regularized logistic regression classifier, both im-
plemented using the efficient LIBLINEAR frame-
work of Fan et al. (2008). The features used are
based on Bj?orkelund et al. (2009) and Liu and
Sarkar (2007), and are listed in Table 2.
The bilexical features are: predicate POS tag,
predicate lemma, argument word form, argument
POS tag, and position. The latter indicates the po-
sition of the argument relative to the predicate, i.e.,
before, on, or after. The syntactic features are:
the predicate and argument elementary trees with-
out the anchors (to avoid sparsity), the category of
the integration point node on the prefix tree where
the elementary tree of the argument attaches to,
an alphabetically ordered set of the categories of
the fringe nodes of the prefix tree after attaching
the argument tree, and the path of PLTAG opera-
tions applied between the argument and the pred-
icate. Note that most of the original features used
by Bj?orkelund et al. (2009) and others are not ap-
plicable in our context, as they exploit information
that is not accessible incrementally. For example,
sibling information to the right of the word is not
available. Furthermore, our PLTAG parser does
not compute syntactic dependencies, hence these
cannot serve as features (and in any case not all
dependencies are available incrementally, see Fig-
ure 4). To counterbalance this, we use local syn-
tactic information stored in the fringe of the pre-
306
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP?
{A0,A1}
S
2
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VP?TO
to
S
VP
VB
open
NP
t
{A1}
(a) Lexicon entries
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
VP
2
VB
2
2
TO
to
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
VP
VP
VB
open
TO
to
{A1}
NP
t
VP
VBD
refused
NP
NNS
Banks
{A0,A1}/{A1}
1. subst 2. subst
3. adj
4. verif
1. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,nil,refused?
2. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,S
2
,refused?
NP ? ?{A0,A1,A2},t,nil?
3. ?
4. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,to,refused?
NP ? ?A1,Banks,open?
(b) Incremental parsing using PLTAG and incremental propagation of roles
Figure 5: Incremental Role Propagation Algorithm application for the sentence Banks refused to open.
Bilexical Syntactic
PredPOS PredElemTree
PredLemma ArgElemTree
ArgWord IntegrationPoint
ArgPOS PrefixFringe
Position OperationPath
Table 2: Features for argument identification and
role label disambiguation.
fix tree. We also store the series of operations ap-
plied by our parser between argument and predi-
cate, in an effort to emulate the effect of recover-
ing longer-range patterns.
5 Experimental Design
5.1 PLTAG and Classifier Training
We extracted the semantically-enriched lexicon
and trained the PLTAG parser by converting the
Wall Street Journal part of Penn Treebank to
PLTAG format. We used Propbank to retrieve
semantic role annotation, as described in Sec-
tion 4.2. We trained the PLTAG parser according
to Demberg et al. (2013) and evaluated the parser
on section 23, on sentences with 40 words or less,
given gold POS tags for each word, and achieved
a labeled bracket F
1
score of 79.41.
In order to train the argument identification and
role label disambiguation classifiers, we used the
English portion of the CoNLL 2009 Shared Task
(Haji?c et al., 2009; Surdeanu et al., 2008). It
consists of the Penn Treebank, automatically con-
verted to dependencies following Johansson and
307
Nugues (2007), accompanied by semantic role la-
bel annotation for every argument pair. The latter
is converted from Propbank based on Carreras and
M`arquez (2005). We extracted the bilexical fea-
tures for the classifiers directly from the gold stan-
dard annotation of the training set. The syntactic
features were obtained as follows: for every sen-
tence in the training set we applied IRPA using the
trained PLTAG parser, with gold standard lexicon
entries for each word of the input sentence. This
ensures near perfect parsing accuracy. Then for
each semantic triple predicted incrementally, we
extracted the relevant syntactic information in or-
der to construct training vectors. If the identified
predicate-argument pair was in the gold standard
then we assigned a positive label for the identifi-
cation classifier, otherwise we flagged it as nega-
tive. For those pairs that are not identified by IRPA
but exist in the gold standard (false negatives), we
extracted syntactic information from already iden-
tified similar triples, as follows: We first look for
correctly identified arguments, wrongly attached
to a different predicate and re-create the triple with
correct predicate/argument information. If no ar-
gument is found, we then pick the argument in the
list of identified arguments for a correct predicate
with the same POS-tag as the gold-standard argu-
ment. In the case of the role label disambigua-
tion classifier we just assign the gold label for ev-
ery correctly identified pair, and ignore the (possi-
bly ambiguous) predicted one. After tuning on the
development set, the argument identifier achieved
an accuracy of 92.18%, and the role label disam-
biguation classifier, 82.37%.
5.2 Evaluation
The focus of this paper is to build a system that is
able to output semantic role labels for predicate-
argument pairs incrementally, as soon as they be-
come available. In order to properly evaluate such
a system, we need to measure its performance in-
crementally. We propose two different cumulative
scores for assessing the (possibly incomplete) se-
mantic triples that have been created so far, as the
input is processed from left to right, per word. The
first metric is called Unlabeled Prediction Score
(UPS) and gets updated for every identified argu-
ment or predicate, even if the corresponding se-
mantic triple is incomplete. Note that UPS does
not take into account the role label, it only mea-
sures predicate and argument identification. In this
respect it is analogous to unlabeled dependency
accuracy reported in the parsing literature. We ex-
pect a model that is able to predict semantic roles
to achieve an improved UPS result compared to a
system that does not do prediction, as illustrated in
Table 1. Our second score, Combined Incremental
SRL Score (CISS), measures the identification of
complete semantic role triples (i.e., correct predi-
cate, predicate sense, argument, and role label) per
word; by the end of the sentence, CISS coincides
with standard combined SRL accuracy, as reported
in CoNLL 2009 SRL-only task. This score is anal-
ogous to labeled dependency accuracy in parsing.
Note that conventional SRL systems such as
Bj?orkelund et al. (2009) typically assume gold
standard syntactic information. In order to emu-
late this, we give our parser gold standard lexicon
entries for each word in the test set; these contain
all possible roles observed in the training set for
a given elementary tree (and all possible senses
for each predicate). This way the parser achieves
a syntactic parsing F
1
score of 94.24, thus ensur-
ing the errors of our system can be attributed to
IRPA and the classifiers. Also note that we evalu-
ate on verb predicates only, therefore trivially re-
ducing the task of predicate identification to the
simple heuristic of looking for words in the sen-
tence with a verb-related POS tag and excluding
auxiliaries and modals. Likewise, predicate sense
disambiguation on verbs presumably is trivial, as
we observed almost no ambiguity of senses among
lexicon entries of the same verb (we adhered to a
simple majority baseline, by picking the most fre-
quent sense, given the lexeme of the verb, in the
few ambiguous cases). It seems that the syntactic
information held in the elementary trees discrimi-
nates well among different senses.
5.3 System Comparison
We evaluated three configurations of our system.
The first configuration (iSRL) uses all seman-
tic roles for each PLTAG lexicon entry, applies
the PLTAG parser, IRPA, and both classifiers to
perform identification and disambiguation, as de-
scribed in Section 4. The second one (Majority-
Baseline), solves the problem of argument identifi-
cation and role disambiguation without the classi-
fiers. For the former we employ a set of heuristics
according to Lang and Lapata (2014), that rely on
gold syntactic dependency information, sourced
from CoNLL input. For the latter, we choose the
most frequent role given the gold standard depen-
dency relation label for the particular argument.
Note that dependencies have been produced in
view of the whole sentence and not incrementally.
308
System Prec Rec F1
iSRL-Oracle 91.00 80.26 85.29
iSRL 81.48 75.51 78.38
Majority-Baseline 71.05 58.10 63.92
Malt-Baseline 60.90 46.14 52.50
Table 3: Full-sentence combined SRL score
This gives the baseline a considerable advantage
especially in case of longer range dependencies.
The third configuration (iSRL-Oracle), is identical
to iSRL, but uses the gold standard roles for each
PLTAG lexicon entry, and thus provides an upper-
bound for our methodology. Finally, we evalu-
ated against Malt-Baseline, a variant of Majority-
Baseline that uses the MaltParser of Nivre et al.
(2007) to provide labeled syntactic dependencies
MaltParser is a state-of-the-art shift-reduce depen-
dency parser which uses an incremental algorithm.
Following Beuck et al. (2011), we modified the
parser to provide intermediate output at each word
by emitting the current state of the dependency
graph before each shift step. We trained Malt-
Parser using the arc-eager algorithm (which out-
performed the other parsing algorithms available
with MaltParser) on the CoNLL dataset, achiev-
ing a labeled dependency accuracy of 89.66% on
section 23.
6 Results
Figures 6 and 7 show the results on the incremen-
tal SRL task. We plot the F
1
for Unlabeled Predic-
tion Score (UPS) and Combined Incremental SRL
Score (CISS) per word, separately for sentences
of lengths 10, 20, 30, and 40 words. The task gets
harder with increasing sentence length, hence we
can only meaningfully compare the average scores
for sentence of the same length. (This approach
was proposed by Sangati and Keller 2013 for eval-
uating the performance of incremental parsers.)
The UPS results in Figure 6 clearly show that
our system (iSRL) outperforms both baselines
on unlabeled argument and predicate prediction,
across all four sentence lengths. Furthermore,
we note that the iSRL system achieves a near-
constant performance for all sentence prefixes.
Our PLTAG-based prediction/verification archi-
tecture allows us to correctly predict incomplete
semantic role triples, even at the beginning of the
sentence. Both baselines perform worse than the
iSRL system in general. Moreover, the Malt-
Baseline performs badly on the initial sentence
prefixes (up to word 10), presumably as it does
not benefit from syntactic prediction, and thus can-
not generate incomplete triples early in the sen-
tence, as illustrated in Table 1. The Majority-
Baseline also does not do prediction, but it has ac-
cess to gold-standard syntactic dependencies, and
thus outperforms the Malt-Baseline on initial sen-
tence prefixes. Note that due to prediction, our
system tends to over-generate incomplete triples
in the beginning of sentences, compared to non-
incremental output, which may inflate UPS for
the first words. However, this cancels out later
in the sentence if triples are correctly completed;
failure to do so would decrease UPS. The near-
constant performance of our output illustrates this
phenomenon. Finally, the iSRL-Oracle outper-
forms all other systems, as it benefits from correct
role labels and correct PLTAG syntax, thus provid-
ing an upper limit on performance.
The CISS results in Figure 7 present a simi-
lar picture. Again, the iSRL system outperforms
both baselines at all sentence lengths. In addition,
it shows particularly strong performance (almost
at the level of the iSRL-Oracle) at the beginning
of the sentence. This presumably is due to the
fact that our system uses prediction and is able to
identify correct semantic role triples earlier in the
sentence. The baselines also show higher perfor-
mance early in the sentence, but to a lesser degree.
Table 3 reports traditional combined SRL scores
for full sentences over all sentence lengths, as
defined for the CoNLL task. Our iSRL system
outperforms the Majority-Baseline by almost 15
points, and the Malt-Baseline by 25 points. It re-
mains seven points below the iSRL-Oracle upper
limit.
Finally, in order to test the effect of syntactic
parsing on our system, we also experimented with
a variant of our iSRL system that utilizes all lex-
icon entries for each word in the test set. This is
similar to performing the CoNLL 2009 joint task,
which is designed for systems that carry out both
syntactic parsing and semantic role labeling. This
variant achieved a full sentence F-score of 68.0%,
i.e., around 10 points lower than our iSRL system.
This drop in score correlates with the difference
in syntactic parsing F-score between the two ver-
sions of PLTAG parser (94.24 versus 79.41), and
is expected given the high ambiguity of the lex-
icon entries for each word. Note, however, that
the full-parsing version of our system still outper-
forms Malt-Baseline by 15 points.
309
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 6: Unlabeled Prediction Score (UPS)
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 7: Combined iSRL Score (CISS)
7 Conclusions
In this paper, we introduced the new task of incre-
mental semantic role labeling and proposed a sys-
tem that solves this task by combining an incre-
mental TAG parser with a semantically enriched
lexicon, a role propagation algorithm, and a cas-
cade of classifiers. This system achieved a full-
sentence SRL F-score of 78.38% on the standard
CoNLL dataset. Not only is the full-sentence
score considerably higher than the Majority-
Baseline (which is a strong baseline, as it uses
gold-standard syntactic dependencies), but we
also observe that our iSRL system performs well
incrementally, i.e., it predicts both complete and
incomplete semantic role triples correctly early on
in the sentence. We attributed this to the fact that
our TAG-based architecture makes it possible to
predict upcoming syntactic structure together with
the corresponding semantic roles.
Acknowledgments
EPSRC support through grant EP/I032916/1 ?An
integrated model of syntactic and semantic predic-
tion in human language processing? to FK and ML
is gratefully acknowledged.
References
Beuck, Niels, Arne Khn, and Wolfgang Menzel.
2011. Incremental parsing and the evaluation
of partial dependency analyses. In Proceedings
of the 1st International Conference on Depen-
dency Linguistics. Depling 2011.
Bj?orkelund, Anders, Love Hafdell, and Pierre
Nugues. 2009. Multilingual semantic role la-
beling. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning: Shared Task. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
CoNLL ?09, pages 43?48.
Carreras, Xavier and Llu??s M`arquez. 2005. Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Con-
ference on Computational Natural Language
Learning. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, CONLL ?05,
pages 152?164.
Chiang, David. 2000. Statistical parsing with
an automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics.
pages 456?463.
Demberg, Vera, Frank Keller, and Alexander
Koller. 2013. Incremental, predictive pars-
ing with psycholinguistically motivated tree-
adjoining grammar. Computational Linguistics
39(4):1025?1066.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
310
blinear: A library for large linear classification.
Journal of Machine Learning Research 9:1871?
1874.
Haji?c, Jan, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Ant`onia
Mart??, Llu??s M`arquez, Adam Meyers, Joakim
Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel
Stra?n?ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning (CoNLL-2009), June 4-5. Boulder,
Colorado, USA.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion
for english. In Joakim Nivre, Heiki-Jaan
Kalep, Kadri Muischnek, and Mare Koit, edi-
tors, NODALIDA 2007 Proceedings. University
of Tartu, pages 105?112.
Joshi, Aravind K. and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized grammars.
In Maurice Nivat and Andreas Podelski, editors,
Tree Automata and Languages, North-Holland,
Amsterdam, pages 409?432.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, Companion Vol-
ume: Short Papers. Uppsala, pages 60?67.
Lang, Joel and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph par-
titioning. Computational Linguistics Accepted
pages 1?62. To appear.
Liu, Yudong and Anoop Sarkar. 2007. Experimen-
tal evaluation of LTAG-based features for se-
mantic role labeling. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Prague, Czech Republic, pages 590?599.
Marcus, Mitchell P., Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: The penn treebank.
Computational Linguistics 19(2):313?330.
M`arquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic Role Labeling: An Introduction to
the Special Issue. Computational Linguistics
34(2):145?159.
Mazzei, Alessandro, Vincenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5:309?332.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Sve-
toslav Marinov, and Erwin Marsi. 2007. Malt-
parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13:95?135.
Pad?o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. The proposition bank: An anno-
tated corpus of semantic roles. Computational
Linguistics 31(1):71?106.
Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-
lution in sentence processing: Evidence against
frequency-based accounts. Journal of Memory
and Language 43(3):447?475.
Sangati, Federico and Frank Keller. 2013. In-
cremental tree substitution grammar for pars-
ing and word prediction. Transactions of
the Association for Computational Linguistics
1(May):111?124.
Sayeed, Asad and Vera Demberg. 2013. The se-
mantic augmentation of a psycholinguistically-
motivated syntactic formalism. In Proceed-
ings of the Fourth Annual Workshop on Cog-
nitive Modeling and Computational Linguistics
(CMCL). Association for Computational Lin-
guistics, Sofia, Bulgaria, pages 57?65.
Surdeanu, Mihai, Richard Johansson, Adam Mey-
ers, Llu??s M`arquez, and Joakim Nivre. 2008.
The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008).
Witten, Ian H. and Timothy C. Bell. 1991. The
zero-frequency problem: estimating the proba-
bilities of novel events in adaptive text compres-
sion. Information Theory, IEEE Transactions
on 37(4):1085?1094.
Xia, Fei, Martha Palmer, and Aravind Joshi. 2000.
A uniform method of grammar extraction and
its applications. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
311
Natural Language Processing and Very Large
Corpora. pages 53?62.
312
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196?206,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Syntactic and Semantic Factors in Processing Difficulty:
An Integrated Measure
Jeff Mitchell, Mirella Lapata, Vera Demberg and Frank Keller
University of Edinburgh
Edinburgh, United Kingdom
jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk,
v.demberg@ed.ac.uk, keller@inf.ed.ac.uk
Abstract
The analysis of reading times can pro-
vide insights into the processes that under-
lie language comprehension, with longer
reading times indicating greater cognitive
load. There is evidence that the language
processor is highly predictive, such that
prior context allows upcoming linguistic
material to be anticipated. Previous work
has investigated the contributions of se-
mantic and syntactic contexts in isolation,
essentially treating them as independent
factors. In this paper we analyze reading
times in terms of a single predictive mea-
sure which integrates a model of seman-
tic composition with an incremental parser
and a language model.
1 Introduction
Psycholinguists have long realized that language
comprehension is highly incremental, with readers
and listeners continuously extracting the meaning
of utterances on a word-by-word basis. As soon
as they encounter a word in a sentence, they inte-
grate it as fully as possible into a representation
of the sentence thus far (Marslen-Wilson 1973;
Konieczny 2000; Tanenhaus et al 1995; Sturt and
Lombardo 2005). Recent research suggests that
language comprehension can also be highly pre-
dictive, i.e., comprehenders are able to anticipate
upcoming linguistic material. This is beneficial as
it gives them more time to keep up with the in-
put, and predictions can be used to compensate for
problems with noise or ambiguity.
Two types of prediction have been observed in
the literature. The first type is semantic predic-
tion, as evidenced in semantic priming: a word
that is preceded by a semantically related prime
or a semantically congruous sentence fragment is
processed faster (Stanovich and West 1981; van
Berkum et al 1999; Clifton et al 2007). Another
example is argument prediction: listeners are able
to launch eye-movements to the predicted argu-
ment of a verb before having encountered it, e.g.,
they will fixate an edible object as soon as they
hear the word eat (Altmann and Kamide 1999).
The second type of prediction is syntactic predic-
tion. Comprehenders are faster at naming words
that are syntactically compatible with prior con-
text, even when they bear no semantic relationship
to the context (Wright and Garrett 1984). Another
instance of syntactic prediction has been reported
by Staub and Clifton (2006): following the word
either, readers predict or and the complement that
follows it, and process it faster compared to a con-
trol condition without either.
Thus, human language processing takes advan-
tage of the constraints imposed by the preceding
semantic and syntactic context to derive expecta-
tions about the upcoming input. Much recent work
has focused on developing computational mea-
sures of these constraints and expectations. Again,
the literature is split into syntactic and semantic
models. Probably the best known measure of syn-
tactic expectation is surprisal (Hale 2001) which
can be coarsely defined as the negative log proba-
bility of word wt given the preceding words, typ-
ically computed using a probabilistic context-free
grammar.
Modeling work on semantic constraint focuses
on the degree to which a word is related to its
preceding context. Pynte et al (2008) use La-
tent Semantic Analysis (LSA, Landauer and Du-
mais 1997) to assess the degree of contextual con-
straint exerted on a word by its context. In this
framework, word meanings are represented as vec-
tors in a high dimensional space and distance in
this space is interpreted as an index of process-
ing difficulty. Other work (McDonald and Brew
2004) models contextual constraint in information
theoretic terms. The assumption is that words
carry prior semantic expectations which are up-
dated upon seeing the next word. Expectations are
represented by a vector of probabilities which re-
flects the likely location in semantic space of the
upcoming word.
The measures discussed above are typically
computed automatically on real-language corpora
using data-driven methods and their predictions
are verified through analysis of eye-movements
that people make while reading. Ample evidence
196
(Rayner 1998) demonstrates that eye-movements
are related to the moment-to-moment cognitive ac-
tivities of readers. They also provide an accurate
temporal record of the on-line processing of nat-
ural language, and through the analysis of eye-
movement measurements (e.g., the amount of time
spent looking at a word) can give insight into the
processing difficulty involved in reading.
In this paper, we investigate a model of predic-
tion that is incremental and takes into account syn-
tactic as well as semantic constraint. The model
essentially integrates the predictions of an incre-
mental parser (Roark 2001) together with those
of a semantic space model (Mitchell and Lap-
ata 2009). The latter creates meaning representa-
tions compositionally, and therefore builds seman-
tic expectations for word sequences (e.g., phrases,
sentences, even documents) rather than isolated
words. Some existing models of sentence process-
ing integrate semantic information into a prob-
abilistic parser (Narayanan and Jurafsky 2002;
Pado? et al 2009); however, the semantic compo-
nent of these models is limited to semantic role in-
formation, rather than attempting to build a full se-
mantic representation for a sentence. Furthermore,
the models of Narayanan and Jurafsky (2002) and
Pado? et al (2009) do not explicitly model pre-
diction, but rather focus on accounting for garden
path effects. The proposed model simultaneously
captures semantic and syntactic effects in a sin-
gle measure which we empirically show is predic-
tive of processing difficulty as manifested in eye-
movements.
2 Models of Processing Difficulty
As described in Section 1, reading times provide
an insight into the various cognitive activities that
contribute to the overall processing difficulty in-
volved in comprehending a written text. To quan-
tify and understand the overall cognitive load asso-
ciated with processing a word in context, we will
break that load down into a sum of terms repre-
senting distinct computational costs (semantic and
syntactic). For example, surprisal can be thought
of as measuring the cost of dealing with unex-
pected input. When a word conforms to the lan-
guage processor?s expectations, surprisal is low,
and the cognitive load associated with processing
that input will also be low. In contrast, unexpected
words will have a high surprisal and a high cogni-
tive cost.
However, high-level syntactic and semantic fac-
tors are only one source of cognitive costs. A siz-
able proportion of the variance in reading times is
accounted for by costs associated with low-level
features of the stimuli, e.g.. relating to orthography
and eye-movement control (Rayner 1998). In ad-
dition, there may also be costs associated with the
integration of new input into an incremental rep-
resentation. Dependency Locality Theory (DLT,
Gibson 2000) is essentially a distance-based mea-
sure of the amount of processing effort required
when the head of a phrase is integrated with its
syntactic dependents. We do not consider integra-
tion costs here (as they have not been shown to
correlate reliably with reading times; see Demberg
and Keller 2008 for details) and instead focus on
the costs associated with semantic and syntactic
constraint and low-level features, which appear to
make the most substantial contributions.
In the following subsections we describe the
various features which contribute to the process-
ing costs of a word in context. We begin by look-
ing at the low-level costs and move on to con-
sider the costs associated with syntactic and se-
mantic constraint. For readers unfamiliar with the
methodology involved in modeling eye-tracking
data, we note that regression analysis (or the more
general mixed effects models) is typically used to
study the relationship between dependent and in-
dependent variables. The independent variables
are the various costs of processing effort and
the dependent variables are measurements of eye-
movements, three of which are routinely used in
the literature: first fixation duration (the duration
of the first fixation on a word regardless of whether
it is the first fixation on a word or the first of mul-
tiple fixations on the same word), first pass dura-
tion, also known as gaze duration, (the sum of all
fixations made on a word prior to looking at an-
other word), and total reading time (the sum of
all fixations on a word including refixations after
moving on to other words).
2.1 Low-level Costs
Low-level features include word frequency (more
frequent words are read faster), word length
(shorter words are read faster), and the position
of the word in the sentence (later words are read
faster). Oculomotor variables have also been
found to influence reading times. These include
previous fixation (indicating whether or not the
previous word has been fixated), launch distance
(how many characters intervene between the cur-
rent fixation and the previous fixation), and land-
ing position (which letter in the word the fixation
landed on).
Information about the sequential context of a
word can also influence reading times. Mc-
197
Donald and Shillcock (2003) show that forward
and backward transitional probabilities are pre-
dictive of first fixation and first pass durations:
the higher the transitional probability, the shorter
the fixation time. Backward transitional prob-
ability is essentially the conditional probabil-
ity of a word given its immediately preceding
word, P(wk|wk?1). Analogously, forward proba-
bility is the conditional probability of the current
word given the next word, P(wk|wk+1).
2.2 Syntactic Constraint
As mentioned earlier, surprisal (Hale 2001; Levy
2008) is one of the best known models of process-
ing difficulty associated with syntactic constraint,
and has been previously applied to the modeling of
reading times (Demberg and Keller 2008; Ferrara
Boston et al 2008; Roark et al 2009; Frank 2009).
The basic idea is that the processing costs relating
to the expectations of the language processor can
be expressed in terms of the probabilities assigned
by some form of language model to the input.
These processing costs are assumed to arise from
the change in the expectations of the language pro-
cessor as new input arrives. If we express these ex-
pectations in terms of a distribution over all possi-
ble continuations of the input seen so far, then we
can measure the magnitude of this change in terms
of the Kullback-Leibler divergence of the old dis-
tribution to the updated distribution. This measure
of processing cost for an input word, wk+1, given
the previous context, w1 . . .wk, can be expressed
straightforwardly in terms of its conditional prob-
ability as:
S =? logP(wk+1|w1 . . .wk) (1)
That is, the processing cost for a word decreases as
its probability increases, with zero processing cost
incurred for words which must appear in a given
context, as these do not result in any change in the
expectations of the language processor.
The original formulation of surprisal (Hale
2001) used a probabilistic parser to calculate these
probabilities, as the emphasis was on the process-
ing costs incurred when parsing structurally am-
biguous garden path sentences.1 Several variants
of calculating surprisal have been developed in the
literature since using different parsing strategies
1While hearing a sentence like The horse raced past the
barn fell (Bever 1970), English speakers are inclined to in-
terpreted horse as the subject of raced expecting the sentence
to end at the word barn. So upon hearing the word fell they
are forced to revise their analysis of the sentence thus far and
adopt a reduced relative reading.
(e.g., left-to-right vs. top-down, PCFGs vs de-
pendency parsing) and different degrees of lexi-
calization (see Roark et al 2009 for an overview) .
For instance, unlexicalized surprisal can be easily
derived by substituting the words in Equation (1)
with parts of speech (Demberg and Keller 2008).
Surprisal could be also defined using a vanilla
language model that does not take any structural
or grammatical information into account (Frank
2009).
2.3 Semantic Constraint
Distributional models of meaning have been com-
monly used to quantify the semantic relation be-
tween a word and its context in computational
studies of lexical processing. These models are
based on the idea that words with similar mean-
ings will be found in similar contexts. In putting
this idea into practice, the meaning of a word is
then represented as a vector in a high dimensional
space, with the vector components relating to the
strength on occurrence of that word in various
types of context. Semantic similarities are then
modeled in terms of geometric similarities within
the space.
To give a concrete example, Latent Semantic
Analysis (LSA, Landauer and Dumais 1997) cre-
ates a meaning representation for words by con-
structing a word-document co-occurrence matrix
from a large collection of documents. Each row in
the matrix represents a word, each column a doc-
ument, and each entry the frequency with which
the word appeared within that document. Because
this matrix tends to be quite large it is often trans-
formed via a singular value decomposition (Berry
et al 1995) into three component matrices: a ma-
trix of word vectors, a matrix of document vectors,
and a diagonal matrix containing singular values.
Re-multiplying these matrices together using only
the initial portions of each (corresponding to the
use of a lower dimensional spatial representation)
produces a tractable approximation to the original
matrix. In this framework, the similarity between
two words can be easily quantified, e.g., by mea-
suring the cosine of the angle of the vectors repre-
senting them.
As LSA is one the best known semantic space
models it comes as no surprise that it has been
used to analyze semantic constraint. Pynte et al
(2008) measure the similarity between the next
word and its preceding context under the assump-
tion that high similarity indicates high semantic
constraint (i.e., the word was expected) and analo-
gously low similarity indicates low semantic con-
straint (i.e., the word was unexpected). They oper-
198
ationalize preceding contexts in two ways, either
as the word immediately preceding the next word
as the sentence fragment preceding it. Sentence
fragments are represented as the average of the
words they contain independently of their order.
The model takes into account only content words,
function words are of little interest here as they can
be found in any context.
Pynte et al (2008) analyze reading times on the
French part of the Dundee corpus (Kennedy and
Pynte 2005) and find that word-level LSA similar-
ities are predictive of first fixation and first pass
durations, whereas sentence-level LSA is only
predictive of first pass duration (i.e., for a mea-
sure that includes refixation). This latter finding
is somewhat counterintuitive, one would expect
longer contexts to have an immediate effect as
they are presumably more constraining. One rea-
son why sentence-level influences are only visible
on first pass duration may be due to LSA itself,
which is syntax-blind. Another reason relates to
the way sentential context was modeled as vec-
tor addition (or averaging). The idea of averag-
ing is not very attractive from a linguistic perspec-
tive as it blends the meanings of individual words
together. Ideally, the combination of simple el-
ements onto more complex ones must allow the
construction of novel meanings which go beyond
those of the individual elements (Pinker 1994).
The only other model of semantic constraint we
are aware of is Incremental Contextual Distinc-
tiveness (ICD, McDonald 2000; McDonald and
Brew 2004). ICD assumes that words carry prior
semantic expectations which are updated upon
seeing the next word. Context is represented by
a vector of probabilities which reflects the likely
location in semantic space of the upcoming word.
When the latter is observed, the prior expecta-
tion is updated using a Bayesian inference mecha-
nism to reflect the newly arrived information. Like
LSA, ICD is based on word co-occurrence vectors,
however it does not employ singular value decom-
position, and constructs a word-word rather than a
word-document co-occurrence matrix. Although
this model has been shown to successfully simu-
late single- and multiple-word priming (McDon-
ald and Brew 2004), it failed to predict processing
costs in the Embra eye-tracking corpus (McDon-
ald and Shillcock 2003).
In this work we model semantic constraint us-
ing the representational framework put forward in
Mitchell and Lapata (2008). Their aim is not so
much to model processing difficulty, but to con-
struct vector-based meaning representations that
go beyond individual words. They introduce a
general framework for studying vector composi-
tion, which they formulate as a function f of two
vectors u and v:
h = f (u,v) (2)
where h denotes the composition of u and v. Dif-
ferent composition models arise, depending on
how f is chosen. Assuming that h is a linear func-
tion of the Cartesian product of u and v allows to
specify additive models which are by far the most
common method of vector combination in the lit-
erature:
hi = ui + vi (3)
Alternatively, we can assume that h is a linear
function of the tensor product of u and v, and thus
derive models based on multiplication:
hi = ui ? vi (4)
Mitchell and Lapata (2008) show that several ad-
ditive and multiplicative models can be formu-
lated under this framework, including the well-
known tensor products (Smolensky 1990) and cir-
cular convolution (Plate 1995). Importantly, com-
position models are not defined with a specific se-
mantic space in mind, they could easily be adapted
to LSA, or simple co-occurrence vectors, or more
sophisticated semantic representations (e.g., Grif-
fiths et al 2007), although admittedly some com-
position functions may be better suited for partic-
ular semantic spaces.
Composition models can be straightforwardly
used as predictors of processing difficulty, again
via measuring the cosine of the angle between a
vector w representing the upcoming word and a
vector h representing the words preceding it:
sim(w,h) =
w ?h
|w||h|
(5)
where h is created compositionally, via some (ad-
ditive or multiplicative) function f .
In this paper we evaluate additive and compo-
sitional models in their ability to capture seman-
tic prediction. We also examine the influence of
the underlying meaning representations by com-
paring a simple semantic space similar to Mc-
Donald (2000) against Latent Dirichlet Allocation
(Blei et al 2003; Griffiths et al 2007). Specif-
ically, the simpler space is based on word co-
occurrence counts; it constructs the vector repre-
senting a given target word, t, by identifying all the
tokens of t in a corpus and recording the counts of
context words, ci (within a specific window). The
context words, ci, are limited to a set of the n most
199
common content words and each vector compo-
nent is given by the ratio of the probability of a ci
given t to the overall probability of ci.
vi =
p(ci|t)
p(ci)
(6)
Despite its simplicity, the above semantic space
(and variants thereof) has been used to success-
fully simulate lexical priming (e.g., McDonald
2000), human judgments of semantic similarity
(Bullinaria and Levy 2007), and synonymy tests
(Pado? and Lapata 2007) such as those included in
the Test of English as Foreign Language (TOEFL).
LDA is a probabilistic topic model offering an
alternative to spatial semantic representations. It
is similar in spirit to LSA, it also operates on a
word-document co-occurrence matrix and derives
a reduced dimensionality description of words and
documents. Whereas in LSA words are repre-
sented as points in a multi-dimensional space,
LDA represents words using topics. Specifically,
each document in a corpus is modeled as a distri-
bution over K topics, which are themselves char-
acterized as distribution over words. The individ-
ual words in a document are generated by repeat-
edly sampling a topic according to the topic distri-
bution and then sampling a single word from the
chosen topic. Under this framework, word mean-
ing is represented as a probability distribution over
a set of latent topics, essentially a vector whose
dimensions correspond to topics and values to the
probability of the word given these topics. Topic
models have been recently gaining ground as a
more structured representation of word meaning
(Griffiths et al 2007; Steyvers and Griffiths 2007).
In contrast to more standard semantic space mod-
els where word senses are conflated into a single
representation, topics have an intuitive correspon-
dence to coarse-grained sense distinctions.
3 Integrating Semantic Constraint into
Surprisal
The treatment of semantic and syntactic constraint
in models of processing difficulty has been some-
what inconsistent. While surprisal is a theo-
retically well-motivated measure, formalizing the
idea of linguistic processing being highly predic-
tive in terms of probabilistic language models, the
measurement of semantic constraint in terms of
vector similarities lacks a clear motivation. More-
over, the two approaches, surprisal and similarity,
produce mathematically different types of mea-
sures. Formally, it would be preferable to have
a single approach to capturing constraint and the
obvious solution is to derive some form of seman-
tic surprisal rather than sticking with similarity.
This can be achieved by turning a vector model
of semantic similarity into a probabilistic language
model.
There are in fact a number of approaches to de-
riving language models from distributional mod-
els of semantics (e.g., Bellegarda 2000; Coccaro
and Jurafsky 1998; Gildea and Hofmann 1999).
We focus here on the model of Mitchell and La-
pata (2009) which tackles the issue of the compo-
sition of semantic vectors and also integrates the
output of an incremental parser. The core of their
model is based on the product of a trigram model
p(wn|w
n?1
n?2) and a semantic component ?(wn,h)
which determines the factor by which this proba-
bility should be scaled up or down given the prior
semantic context h:
p(wn) = p(wn|w
n?1
n?2) ??(wn,h) (7)
The factor ?(wn,h) is essentially based on a com-
parison between the vector representing the cur-
rent word wn and the vector representing the prior
history h. Varying the method for constructing
word vectors (e.g., using LDA or a simpler seman-
tic space model) and for combining them into a
representation of the prior context h (e.g., using
additive or multiplicative functions) produces dis-
tinct models of semantic composition.
The calculation of ? is then based on a weighted
dot product of the vector representing the upcom-
ing word w, with the vector representing the prior
context h:
?(w,h) =?
i
wihi p(ci) (8)
As shown in Equation (7) this semantic factor then
modulates the trigram probabilities, to take ac-
count of the effect of the semantic content outside
the n-gram window.
Mitchell and Lapata (2009) show that a com-
bined semantic-trigram language model derived
from this approach and trained on the Wall Street
Journal outperforms a baseline trigram model in
terms of perplexity on a held out set. They also
linearly interpolate this semantic language model
with the output of an incremental parser, which
computes the following probability:
p(w|h) = ?p1(w|h)+(1??)p2(w|h) (9)
where p1(w|h) is computed as in Equation (7)
and p2(w|h) is computed by the parser. Their im-
plementation uses Roark?s (2001) top-down incre-
mental parser which estimates the probability of
200
the next word based upon the previous words of
the sentence. These prefix probabilities are calcu-
lated from a grammar, by considering the likeli-
hood of seeing the next word given the possible
grammatical relations representing the prior con-
text.
Equation (9) essentially defines a language
model which combines semantic, syntactic and
n-gram structure, and Mitchell and Lapata (2009)
demonstrate that it improves further upon a se-
mantic language model in terms of perplexity. We
argue that the probabilities from this model give
us a means to model the incrementally and predic-
tivity of the language processor in a manner that
integrates both syntactic and semantic constraints.
Converting these probabilities to surprisal should
result in a single measure of the processing cost as-
sociated with semantic and syntactic expectations.
4 Method
Data The models discussed in the previous sec-
tion were evaluated against an eye-tracking cor-
pus. Specifically, we used the English portion
of the Dundee Corpus (Kennedy and Pynte 2005)
which contains 20 texts taken from The Indepen-
dent newspaper. The corpus consists of 51,502
tokens and 9,776 types in total. It is annotated
with the eye-movement records of 10 English na-
tive speakers, who each read the whole corpus.
The eye-tracking data was preprocessed following
the methodology described in Demberg and Keller
(2008). From this data, we computed total reading
time for each word in the corpus. Our statistical
analyses were based on actual reading times, and
so we only included words that were not skipped.
We also excluded words for which the previous
word had been skipped, and words on which the
normal left-to-right movement of gaze had been
interrupted, i.e., by blinks, regressions, etc. Fi-
nally, because our focus is the influence of seman-
tic context, we selected only content words whose
prior sentential context contained at least two fur-
ther content words. The resulting data set con-
sisted of 53,704 data points, which is about 10%
of the theoretically possible total.2
2The total of all words read by all subjects is 515,020.
The pre-processing recommended by Demberg and Keller?s
(2008) results in a data sets containing 436,000 data points.
Removing non-content words leaves 205,922 data points. It
only makes sense to consider words that were actually fixated
(the eye-tracking measures used are not defined on skipped
words), which leaves 162,129 data points. Following Pynte
et al (2008), we require that the previous word was fixated,
with 70,051 data points remaining. We exclude words on
which the normal left to right movement of gaze had been
interrupted, e.g., by blinks and regressions, which results in
the final total to 53,704 data points.
Model Implementation All elements of our
model were trained on the BLLIP corpus, a col-
lection of texts from the Wall Street Journal
(years 1987?89). The training corpus consisted of
38,521,346 words. We used a development cor-
pus of 50,006 words and a test corpus of similar
size. All words were converted to lowercase and
numbers were replaced with the symbol ?num?. A
vocabulary of 20,000 words was chosen and the
remaining tokens were replaced with ?unk?.
Following Mitchell and Lapata (2009), we con-
structed a simple semantic space based on co-
occurrence statistics from the BLLIP training set.
We used the 2,000 most frequent word types as
contexts and a symmetric five word window. Vec-
tor components were defined as in Equation (6).
We also trained the LDA model on BLLIP, using
the Gibb?s sampling procedure discussed in Grif-
fiths et al (2007). We experimented with different
numbers of topics on the development set (from 10
to 1,000) and report results on the test set with 100
topics. In our experiments, the hyperparameter ?
was initialized to .5, and the ? word probabilities
were initialized randomly.
We integrated our compositional models with a
trigram model which we also trained on BLLIP.
The model was built using the SRILM toolkit
(Stolcke 2002) with backoff and Kneser-Ney
smoothing. As our incremental parser we used
Roark?s (2001) parser trained on sections 2?21 of
the Penn Treebank containing 936,017 words. The
parser produces prefix probabilities for each word
of a sentence which we converted to conditional
probabilities by dividing each current probability
by the previous one.
Statistical Analysis The statistical analyses in
this paper were carried out using linear mixed
effects models (LME, Pinheiro and Bates 2000).
The latter can be thought of as generalization of
linear regression that allows the inclusion of ran-
dom factors (such as participants or items) as well
as fixed factors (e.g., word frequency). In our
analyses, we treat participant as a random factor,
which means that our models contain an intercept
term for each participant, representing the individ-
ual differences in the rates at which they read.3
We evaluated the effect of adding a factor to a
model by comparing the likelihoods of the mod-
els with and without that factor. If a ?2 test on the
3Other random factors that are appropriate for our anal-
yses are word and sentence; however, due to the large num-
ber of instances for these factors (given that the Dundee cor-
pus contains 51,502 tokens), we were not able to include
them: the model fitting algorithm we used (implemented in
the R package lme4) does not converge for such large models.
201
Factor Coefficient
Intercept ?.011
Word Length .264
Launch Distance .109
Landing Position .612
Word Frequency ?.010
Reading Time of Last Word .151
Table 1: Coefficients of the baseline LME model
for total reading time
likelihood ratio is significant, then this indicates
that the new factor significantly improves model
fit. We also experimented with adding random
slopes for participant to the model (in addition to
the random intercept); however, this either led to
non-convergence of the model fitting procedure, or
failed to result in an increase in model fit accord-
ing to the likelihood ratio test. Therefore, all mod-
els reported in the rest of this paper contain ran-
dom intercept of participants as the sole random
factor.
Rather than model raw reading times, we model
times on the log scale. This is desirable for a
number of reasons. Firstly, the raw reading times
tend to have a skew distribution and taking logs
produces something closer to normal, which is
preferable for modeling. Secondly, the regres-
sion equation makes more sense on the log scale
as the contribution of each term to raw reading
time is multiplicative rather than additive. That is,
log(t) = ?i?ixi implies t = ?i e
?ixi . In particular,
the intercept term for each participant now repre-
sents a multiplicative factor by which that partici-
pant is slower or faster.
5 Results
We computed separate mixed effects models for
three dependent variables, namely first fixation du-
ration, first pass duration, and total reading time.
We report results for total times throughout, as
the results of the other two dependent variables
are broadly similar. Our strategy was to first con-
struct a baseline model of low-level factors influ-
encing reading time, and then to take the resid-
uals from that model as the dependent variable
in subsequent analyses. In this way we removed
the effects of low-level factors before investigating
the factors associated with syntactic and semantic
constraint. This avoids problems with collinear-
ity between low-level factors and the factors we
are interested in (e.g., trigram probability is highly
correlated with word frequency). The baseline
model contained the factors word length, word fre-
Model Composition Coefficient
SSS
Additive ?.03820???
Multiplicative ?.00895???
LDA
Additive ?.02500???
Multiplicative ?.00262???
Table 2: Coefficients of LME models including
simple semantic space (SSS) or Latent Dirichlet
Allocation (LDA) as factors; ???p < .001
quency, launch distance, landing position, and the
reading time for the last fixated word, and its pa-
rameter estimates are given in Table 1. To further
reduce collinearity, we also centered all fixed fac-
tors, both in the baseline model, and in the models
fitted on the residuals that we report in the follow-
ing. Note that some intercorrelations remain be-
tween the factors, which we will discuss at the end
of Section 5.
Before investigating whether an integrated
model of semantic and syntactic constraint im-
proves the goodness of fit over the baseline, we ex-
amined the influence of semantic constraint alone.
This was necessary as compositional models have
not been previously used to model processing
difficulty. Besides, replicating Pynte et al?s
(2008) finding, we were also interested in assess-
ing whether the underlying semantic representa-
tion (simple semantic space or LDA) and com-
position function (additive versus multiplicative)
modulate reading times differentially.
We built an LME model that predicted the resid-
ual reading times of the baseline model using the
similarity scores from our composition models as
factors. We then carried out a ?2 test on the like-
lihood ratio of a model only containing the ran-
dom factor and the intercept, and a model also
containing the semantic factor (cosine similarity).
The addition of the semantic factor significantly
improves model fit for both the simple semantic
space and LDA. This result is observed for both
additive and multiplicative composition functions.
Our results are summarized in Table 2 which re-
ports the coefficients of the four LME models fit-
ted against the residuals of the baseline model, to-
gether with the p-values of the ?2 test.
Before evaluating our integrated surprisal mea-
sure, we evaluated its components individually in
order to tease their contributions apart. For ex-
ample, it may be the case that syntactic surprisal
is an overwhelmingly better predictor of reading
time than semantic surprisal, however we would
not be able to detect this by simply adding a factor
based on Equation (9) to the baseline model. The
202
Factor SSS Coef LDA Coef
? log(p) .00760??? .00760???
A
dd
? log(?) .03810??? .00622???
log(?+(1??) p2p1 ) .00953
??? .00943???
M
ul
t ? log(?) .01110??? ?.00033
log(?+(1??) p2p1 ) .00882
??? .00133
Table 3: Coefficients of nested LME models with
the components of SSS or LDA surprisal as fac-
tors; only the coefficient of the additional factor at
each step are shown
integrated surprisal measure can be written as:
S =? log(?p1 +(1??)p2) (10)
Where p2 is the incremental parser probability and
p1 is the product of the semantic component, ?,
and the trigram probability, p. This can be broken
down into the sum of two terms:
S =? log(p1)? log(?+(1??)
p2
p1
) (11)
Since the first term, ? log(p1) is itself a product it
can also be broken down further:
S =? log(p)? log(?)? log(?+(1??)
p2
p1
) (12)
Thus, to evaluate the contribution of the three
components to the integrated surprisal measure we
fitted nested LME models, i.e., we entered these
terms one at a time into a mixed effects model
and tested the significance of the improvement in
model fit for each additional term.
We again start with an LME model that only
contains the random factor and the intercept, with
the residuals of the baseline models as the depen-
dent variable. Considering the trigram model first,
we find that adding this factor to the model gives a
significant improvement in fit. Also adding the se-
mantic component (? log(?)) improves fit further,
both for additive and multiplicative composition
functions using a simple semantic space. Finally,
the addition of the parser probabilities (log(?+
(1??) p2p1 )) again improves model fit significantly.
As far as LDA is concerned, the additive model
significantly improves model fit, whereas the mul-
tiplicative one does not. These results mirror
the findings of Mitchell and Lapata (2009), who
report that a multiplicative composition function
produced the lowest perplexity for the simple se-
mantic space model, whereas an additive function
gave the best perplexity for the LDA space. Ta-
ble 3 lists the coefficients for the nested models for
Model Composition Coefficient
SSS
Additive .00804???
Multiplicative .00819???
LDA
Additive .00817???
Multiplicative .00640???
Table 4: Coefficients of LME models with inte-
grated surprisal measure (based on SSS or LDA)
as factor
all four variants of our semantic constraint mea-
sure.
Finally, we built a separate LME model where
we added the integrated surprisal measure (see
Equation (9)) to the model only containing the ran-
dom factor and the intercept (see Table 4). We
did this separately for all four versions of the in-
tegrated surprisal measure (SSS, LDA; additive,
multiplicative). We find that model fit improved
significantly all versions of integrated surprisal.
One technical issue that remains to be discussed
is collinearity, i.e., intercorrelations between the
factors in a model. The presence of collinearity
is problematic, as it can render the model fitting
procedure unstable; it can also affect the signifi-
cance of individual factors. As mentioned in Sec-
tion 4 we used two techniques to reduce collinear-
ity: residualizing and centering. Table 5 gives
an overview of the correlation coefficients for all
pairs of factors. It becomes clear that collinear-
ity has mostly been removed; there is a remaining
relationship between word length and word fre-
quency, which is expected as shorter words tend to
be more frequent. This correlation is not a prob-
lem for our analysis, as it is confined to the base-
line model. Furthermore, word frequency and tri-
gram probability are highly correlated. Again this
is expected, given that the frequencies of unigrams
and higher-level n-grams tend to be related. This
correlation is taken care of by residualizing, which
isolates the two factors: word frequency is part
of the baseline model, while trigram probability is
part of the separate models that we fit on the resid-
uals. All other correlations are small (with coeffi-
cients of .27 or less), with one exception: there is
a high correlation between the ? log(?) term and
the log(?+ (1? ?) p2p1 ) term in the multiplicative
LDA model. This collinearity issue may explain
the absence of a significant improvement in model
fit when these two terms are added to the baseline
(see Table 3).
203
Factor Len Freq ?l(p)?l(?)
Frequency ?.310
? log(p) .230?.700
S
S
S
A
dd
? log(?) .016?.120 .025
log(?+(1??) p2p1 ) .024 .036?.270 .065
S
S
S
M
ul
t ? log(?) ?.015?.110 .035
log(?+(1??) p2p1 ) .020 .028?.260 .160
L
D
A
A
dd
? log(?) ?.024?.130 .046
log(?+(1??) p2p1 ) .005 .014?.250 .030
L
D
A
M
ul
t ? log(?) ?.120 .006?.046
log(?+(1??) p2p1 )?.089?.005?.180 .740
Table 5: Intercorrelations between model factors
6 Discussion
In this paper we investigated the contributions of
syntactic and semantic constraint in modeling pro-
cessing difficulty. Our work departs from previ-
ous approaches in that we propose a single mea-
sure which integrates syntactic and semantic fac-
tors. Evaluation on an eye-tracking corpus shows
that our measure predicts reading time better than
a baseline model that captures low-level factors
in reading (word length, landing position, etc.).
Crucially, we were able to show that the semantic
component of our measure improves reading time
predictions over and above a model that includes
syntactic measures (based on a trigram model and
incremental parser). This means that semantic
costs are a significant predictor of reading time in
addition to the well-known syntactic surprisal.
An open issue is whether a single, integrated
measure (as evaluated in Table 4) fits the eye-
movement data significantly better than separate
measures for trigram, syntactic, and semantic sur-
prisal (as evaluated in Table 3. However, we are
not able to investigate this hypothesis: our ap-
proach to testing the significance of factors re-
quires nested models; the log-likelihood test (see
Section 4) is only able to establish whether adding
a factor to a model improves its fit; it cannot com-
pare models with disjunct sets of factors (such as
a model containing the integrated surprisal mea-
sure and one containing the three separate ones).
However, we would argue that a single, integrated
measure that captures human predictive process-
ing is preferable over a collection of separate mea-
sures. It is conceptually simpler (as it is more par-
simonious), and is also easier to use in applica-
tions (such as readability prediction). Finally, an
integrated measure requires less parameters; our
definition of surprisal in 12 is simply the sum of
the trigram, syntactic, and semantic components.
An LME model containing separate factors, on the
other hand, requires a coefficient for each of them,
and thus has more parameters.
In evaluating our model, we adopted a broad
coverage approach using the reading time data
from a naturalistic corpus rather than artificially
constructed experimental materials. In doing so,
we were able to compare different syntactic and
semantic costs on the same footing. Previous
analyses of semantic constraint have been con-
ducted on different eye-tracking corpora (Dundee
and Embra Corpus) and on different languages
(English, French). Moreover, comparisons of the
individual contributions of syntactic and semantic
factors were generally absent from the literature.
Our analysis showed that both of these factors can
be captured by our integrated surprisal measure
which is uniformly probabilistic and thus prefer-
able to modeling semantic and syntactic costs dis-
jointly using a mixture of probabilistic and non-
probabilistic measures.
An interesting question is which aspects of se-
mantics our model is able to capture, i.e., why
does the combination of LSA or LDA representa-
tions with an incremental parser yield a better fit of
the behavioral data. In the psycholinguistic liter-
ature, various types of semantic information have
been investigated: lexical semantics (word senses,
selectional restrictions, thematic roles), senten-
tial semantics (scope, binding), and discourse se-
mantics (coreference and coherence); see Keller
(2010) of a detailed discussion. We conjecture that
our model is mainly capturing lexical semantics
(through the vector space representation of words)
and sentential semantics (through the multiplica-
tion or addition of words). However, discourse
coreference effects (such as the ones reported by
Altmann and Steedman (1988) and much subse-
quent work) are probably not amenable to a treat-
ment in terms of vector space semantics; an ex-
plicit representation of discourse entities and co-
reference relations is required (see Dubey 2010
for a model of human sentence processing that can
handle coreference).
A key objective for future work will be to in-
vestigate models that integrate semantic constraint
with syntactic predictions more tightly. For ex-
ample, we could envisage a parser that uses se-
mantic representations to guide its search, e.g., by
pruning syntactic analyses that have a low seman-
tic probability. At the same time, the semantic
model should have access to syntactic informa-
tion, i.e., the composition of word representations
should take their syntactic relationships into ac-
count, rather than just linear order.
204
References
ACL. 2010. Proceedings of the 48th Annual Meet-
ing of the Association for Computational Lin-
guistics. Uppsala.
Altmann, Gerry T. M. and Yuki Kamide. 1999.
Incremental interpretation at verbs: Restricting
the domain of subsequent reference. Cognition
73:247?264.
Altmann, Gerry T. M. and Mark J. Steedman.
1988. Interaction with context during human
sentence processing. Cognition 30(3):191?238.
Bellegarda, Jerome R. 2000. Exploiting latent se-
mantic information in statistical language mod-
eling. Proceedings of the IEEE 88(8):1279?
1296.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O?Brien. 1995. Using linear algebra
for intelligent information retrieval. SIAM re-
view 37(4):573?595.
Bever, Thomas G. 1970. The cognitive basis for
linguistic strutures. In J. R. Hayes, editor, Cog-
nition and the Development of Language, Wi-
ley, New York, pages 279?362.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet alocation. Journal
of Machine Learning Research 3:993?1022.
Bullinaria, John A. and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods 39:510?526.
Clifton, Charles, Adrian Staub, and Keith Rayner.
2007. Eye movement in reading words and sen-
tences. In R V Gompel, M Fisher, W Murray,
and R L Hill, editors, Eye Movements: A Win-
dow in Mind and Brain, Elsevier, pages 341?
372.
Coccaro, Noah and Daniel Jurafsky. 1998. To-
wards better integration of semantic predictors
in satistical language modeling. In Proceedings
of the 5th International Conference on Spoken
Language Processing. Sydney, Australia, pages
2403?2406.
Demberg, Vera and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition
101(2):193?210.
Dubey, Amit. 2010. The influence of discourse on
syntax: A psycholinguistic model of sentence
processing. In ACL.
Ferrara Boston, Marisa, John Hale, Reinhold
Kliegl, Umesh Patil, and Shravan Vasishth.
2008. Parsing costs as predictors of reading dif-
ficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Re-
search 2(1):1?12.
Frank, Stefan L. 2009. Surprisal-based compar-
ison between a symbolic and a connectionist
model of sentence processing. In Proceedings
of the 31st Annual Conference of the Cognitive
Science Society. Austin, TX, pages 139?1144.
Gibson, Edward. 2000. Dependency locality the-
ory: A distance-dased theory of linguistic com-
plexity. In Alec Marantz, Yasushi Miyashita,
and Wayne O?Neil, editors, Image, Language,
Brain: Papers from the First Mind Articulation
Project Symposium, MIT Press, Cambridge,
MA, pages 95?126.
Gildea, Daniel and Thomas Hofmann. 1999.
Topic-based language models using EM. In
Proceedings of the 6th European Conference
on Speech Communiation and Technology. Bu-
dapest, Hungary, pages 2167?2170.
Griffiths, Thomas L., Mark Steyvers, and
Joshua B. Tenenbaum. 2007. Topics in se-
mantic representation. Psychological Review
114(2):211?244.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chap-
ter of the Association. Association for Compu-
tational Linguistics, Pittsburgh, PA, volume 2,
pages 159?166.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In ACL.
Kennedy, Alan and Joel Pynte. 2005. Parafoveal-
on-foveal effects in normal reading. Vision Re-
search 45:153?168.
Konieczny, Lars. 2000. Locality and parsing com-
plexity. Journal of Psycholinguistic Research
29(6):627?645.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato?s problem: the latent seman-
tic analysis theory of acquisition, induction and
representation of knowledge. Psychological Re-
view 104(2):211?240.
Levy, Roger. 2008. Expectation-based syntactic
comprehension. Cognition 106(3):1126?1177.
Marslen-Wilson, William D. 1973. Linguistic
structure and speech shadowing at very short la-
tencies. Nature 244:522?523.
McDonald, Scott. 2000. Environmental Determi-
nants of Lexical Processing Effort. Ph.D. thesis,
University of Edinburgh.
205
McDonald, Scott and Chris Brew. 2004. A dis-
tributional model of semantic context effects in
lexical processing. In Proceedings of the 42th
Annual Meeting of the Association for Com-
putational Linguistics. Barcelona, Spain, pages
17?24.
McDonald, Scott A. and Richard C. Shillcock.
2003. Low-level predictive inference in read-
ing: The influence of transitional probabilities
on eye movements. Vision Research 43:1735?
1751.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-
based models of semantic composition. In Pro-
ceedings of ACL-08: HLT . Columbus, OH,
pages 236?244.
Mitchell, Jeff and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing. Sin-
gapore, pages 430?439.
Narayanan, Srini and Daniel Jurafsky. 2002. A
Bayesian model predicts human parse prefer-
ence and reading time in sentence processing. In
Thomas G. Dietterich, Sue Becker, and Zoubin
Ghahramani, editors, Advances in Neural In-
formation Processing Systems 14. MIT Press,
Cambridge, MA, pages 59?65.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics
33(2):161?199.
Pado?, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Pinheiro, Jose C. and Douglas M. Bates.
2000. Mixed-effects Models in S and S-PLUS.
Springer, New York.
Pinker, Steven. 1994. The Language Instinct: How
the Mind Creates Language. HarperCollins,
New York.
Plate, Tony A. 1995. Holographic reduced repre-
sentations. IEEE Transactions on Neural Net-
works 6(3):623?641.
Pynte, Joel, Boris New, and Alan Kennedy. 2008.
On-line contextual influences during reading
normal text: A multiple-regression analysis. Vi-
sion Research 48:2172?2183.
Rayner, Keith. 1998. Eye movements in read-
ing and information processing: 20 years of re-
search. Psychological Bulletin 124(3):372?422.
Roark, Brian. 2001. Probabilistic top-down pars-
ing and language modeling. Computational
Linguistics 27(2):249?276.
Roark, Brian, Asaf Bachrach, Carlos Cardenas,
and Christophe Pallier. 2009. Deriving lex-
ical and syntactic expectation-based measures
for psycholinguistic modeling via incremental
top-down parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing. Association for Compu-
tational Linguistics, Singapore, pages 324?333.
Smolensky, Paul. 1990. Tensor product vari-
able binding and the representation of symbolic
structures in connectionist systems. Artificial
Intelligence 46:159?216.
Stanovich, Kieth E. and Richard F. West. 1981.
The effect of sentence context on ongoing word
recognition: Tests of a two-pricess theory. Jour-
nal of Experimental Psychology: Human Per-
ception and Performance 7:658?672.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either . . .or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425?436.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the
Internatinal Conference on Spoken Language
Processing. Denver, Colorado.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291?305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632?1634.
van Berkum, Jos J. A., Colin M. Brown, and Peter
Hagoort. 1999. Early referential context effects
in sentence processing: Evidence from event-
related brain potentials. Journal of Memory and
Language 41:147?182.
Wright, Barton and Merrill F. Garrett. 1984. Lex-
ical decision in sentences: Effects of syntactic
structure. Memory and Cognition 12:31?45.
206
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 57?65,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
The semantic augmentation of a psycholinguistically-motivated syntactic
formalism
Asad Sayeed and Vera Demberg
Computational Linguistics and Phonetics / M2CI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
{asayeed,vera}@coli.uni-saarland.de
Abstract
We augment an existing TAG-based in-
cremental syntactic formalism, PLTAG,
with a semantic component designed to
support the simultaneous modeling ef-
fects of thematic fit as well as syntactic
and semantic predictions. PLTAG is a
psycholinguistically-motivated formalism
which extends the standard TAG opera-
tions with a prediction and verification
mechanism and has experimental support
as a model of syntactic processing diffi-
culty. We focus on the problem of for-
mally modelling semantic role prediction
in the context of an incremental parse
and describe a flexible neo-Davidsonian
formalism and composition procedure to
accompany a PLTAG parse. To this
end, we also provide a means of aug-
menting the PLTAG lexicon with seman-
tic annotation. To illustrate this, we run
through an experimentally-relevant model
case, wherein the resolution of semantic
role ambiguities influences the resolution
of syntactic ambiguities and vice versa.
1 Introduction
PLTAG (PsychoLinguistically-motivated TAG,
Demberg and Keller, 2008; Demberg et al, 2014)
is a variant of Tree-Adjoining Grammar (TAG)
which is designed to allow the construction of
TAG parsers that enforce strict incrementality and
full connectedness through (1) constraints on the
order of operations, (2) a new type of unlexical-
ized tree, so-called prediction trees, and (3) a veri-
fication mechanism that matches up and extends
predicted structures with later evidence. Psy-
cholinguistic evaluation has shown that PLTAG
operations can be used to predict data from eye-
tracking experiments, lending this syntactic for-
malism greater psycholinguistic support.
Syntax, however, may not just be the skeleton of
a linguistic construction that bears semantic con-
tent: there is some evidence that syntactic struc-
ture and semantic plausibility interact with each
other. In a strongly interactive view, we would ex-
pect that semantic plausibility could directly affect
the syntactic expectations. Consider the sentences:
(1) a. The woman slid the butter to the man.
b. The woman slid the man the butter.
The ditransitive verb ?to slide? provides three
roles for participants in the predicate: agent, pa-
tient, and recipient. In both cases, ?the woman?
fills the agent role, ?the butter? the patient, and
?the man? the recipient. However, they do not gen-
erally fill all roles equally well. English-speakers
have the intuition that ?the butter? should neither
be an agent nor a recipient under normal circum-
stances. Likewise, ?the man? is not a typical pa-
tient in this situation. If there is a psycholinguis-
tic effect of semantic plausibility, we would ex-
pect that an incomplete sentence like ?The woman
slid the butter? would generate an expectation in
the listener of a PO construction (rather than DO)
with preposition ?to?, as well as an expectation of
a noun phrase and an expectation that that noun
phrase would belong to the class of entities that
are plausible recipients for entities that are slid.
If this is the case, then there is not only a syntac-
tic expectation at this point but a semantic expecta-
tion that is in turn informed by the syntactic struc-
ture and semantic content up to that point. Con-
structing a model that is formally rich, psycholin-
guistically plausible, and empirically robust re-
quires making design decisions about the specific
relationship between syntax and semantics and the
overall level of formal articulation on which the
statistical model rests. For PLTAG, we are inter-
ested in preserving as many of its syntactic charac-
teristics as are necessary to model the phenomena
that it already does (Demberg and Keller, 2009).
57
In the rest of this paper, we therefore present a
semantic augmentation of PLTAG that is based on
neo-Davidsonian event semantics and is capable
of supporting incrementality and prediction.
2 Psycholinguistic background
Does thematic fit dynamically influence the choice
of preferred syntactic structures, does it shape pre-
dictions of upcoming semantic sorts, and can we
measure this experimentally?
A classic study (Altmann and Kamide, 1999)
about the influence of thematic fit on predictions
showed that listeners can predict the complement
of a verb based on its selectional restrictions. Par-
ticipants heard sentences such as:
(2) a. The boy will eat the cake.
b. The boy will move the cake.
while viewing images that depicted sets of rele-
vant objects, in this example, a cake, a train set,
a ball, and a model car. Altmann and Kamide
(1999) monitored participants? eye-movements
while they heard the sentences and found an in-
creased number of looks to the cake during the
word eat compared the control condition, i.e., dur-
ing the word move (only the cake is edible, but
all depicted objects are movable). This indicates
that selectional preference information provided
by the verb is not only used as soon as it is avail-
able (i.e., incremental processing takes place), but
this information also triggers the prediction of up-
coming arguments of the verb. Subsequent work
has demonstrated that this is not a simple associ-
ation effect of eat and the edible item cake, but
that people assign syntactic roles rapidly based on
case marking and that missing obligatory thematic
role fillers are predicted; in a German visual world
study, Kamide et al (2003a) presented participants
with a scene containing a cabbage, a hare, a fox
and a distractor object while they heard sentences
like
(3) a. Der Hase frisst gleich den Kohl.
(The harenom will eat soon the cabbageacc.)
b. Den Hasen frisst gleich der Fuchs.?
(The hareacc will eat soon the foxnom.)
They found that, during the verb-adverb region,
people looked more to the cabbage in the first con-
dition and correctly anticipated the fox in the sec-
ond condition. This means that they were able to
correctly anticipate the filler of the missing the-
matic role. Kamide et al (2003b) furthermore
showed that role prediction is not only restricted
to the immediately-following grammatical object,
but that goals as in The woman slid the butter to
the man are also anticipated.
Thematic fit furthermore seems to interact with
syntactic structure. Consider the sentences in (4),
which are locally ambiguous with respect to a
main clause interpretation or a reduced relative
clause.
(4) a. The doctor sent for the patient arrived.
b. The flowers sent for the patient arrived.
Comprehenders incur decreased processing diffi-
culty in sentences like (4-b) compared to (4-a),
due to flowers not being a good thematic fit for
the agent role of sending (Steedman, 2000).
Taken together, the experimental evidence sug-
gests that semantic information in the form of
thematic fit can influence the syntactic structures
maintained by the comprehender and that peo-
ple do generate anticipations not only based on
the syntactic requirements of a sentence, but also
in terms of thematic roles. While there is evi-
dence that both syntactic and semantic process-
ing is rapid and incremental, there remain, how-
ever, some open questions on how closely syn-
tactic and semantic processing are integrated with
each other. The architecture suggested here mod-
els the parallel, highly incremental construction of
syntactic and semantic structure, but leaves open
to exploration the question of how quickly and
strongly they interact with each other. Note that
with the present architecture, thematic fit would
only be calculated for word pairs which stand in
a possible syntactic relation. The syntax thus ex-
erts strong constraints on which plausibilities are
considered. Our example in section 6.2 illustrates
how even a tight form of direct interaction between
syntax and semantics can be modelled.
3 Relation to previous work on joint
syntactic-semantic models
Previous attempts have been made to combine
the likelihood of syntactic structure and seman-
tic plausibility estimates into one model for pre-
dicting human processing difficulty (Pado? et al,
2009; Jurafsky, 2002). Pado? et al (2009) pre-
dict increased difficulty when the preferred syn-
tactic analysis is incompatible with the analysis
that would have the best thematic fit. They inte-
grate syntactic and semantic models as a weighted
combination of plausibility scores. The syntactic
58
and semantic models are computed to some extent
independently of one another, and then the result
is adjusted by a set of functions that take into ac-
count conflicts between the models. In relation to
the approach proposed here, it is also important
to note that the semantic components in (Pado? et
al., 2009; Jurafsky, 2002) are limited to semantic
role information, while the architecture proposed
in this paper can build complete semantic expres-
sions for a sentence. Furthermore, these models
do not model the prediction and verification pro-
cess (in particular, they do not make any seman-
tic role predictions of upcoming input) which has
been observed in human language processing.
Mitchell et al (2010) propose an integrated
measure of syntactic and semantic surprisal as a
model of processing difficulty, and show that the
semantic component improves modelling results
over a syntax-only model. However, the syntactic
and semantic surprisal components are only very
loosely integrated with one another, as the seman-
tic model is a distributional bag-of-words model
which does not take syntax into account.
Finally, the syntactic model underlying (Pado? et
al., 2009; Mitchell et al, 2010) is an incremental
top-down PCFG parser (Roark, 2001), which due
to its parsing strategy fails to predict human pro-
cessing difficulty that arises in certain cases, such
as for center embedding (Thompson et al, 1991;
Resnik, 1992). Using the PLTAG parsing model is
thus more psycholinguistically adequate.
3.1 Towards a broad-coverage integration of
syntax and semantics
The current paper does not propose a new model
of sentence processing difficulty, but rather ex-
plores the formal architecture and mechanism nec-
essary to enable the future implementation of an
integrated syntactic-semantic model. A syntax-
informed semantic surprisal component imple-
mented using distributional semantics could use
the semantic expressions generated during the
PLTAG semantics construction to determine what
words (in which relationships to the current word)
from the previous context to condition on for cal-
culating semantic surprisal.
4 PLTAG syntax
PLTAG uses the standard operations of TAG: sub-
stitution and adjunction. The order in which they
are applied during a parse is constrained by in-
crementality. This also implies that, in addition
to the standard operations, there are reverse Up
versions of these operations where the prefix tree
is substituted or adjoined into a new elementary
tree (see figure 4). In order to achieve strict incre-
mentality and full connectedness at the same time
while still using linguistically motivated elemen-
tary trees, PLTAG has an additional type of (usu-
ally) unlexicalized elementary tree called predic-
tion trees. Each node in a prediction tree is marked
with upper and/or lower indices kk to indicate its
predictive status. Examples for prediction trees
are given at the right hand side of figure 5b. The
availability of prediction trees enable a sentence
starting with ?The thief quickly? to integrate both
the NP (?The thief?) and the ADVP (?quickly?)
into the derivation even though neither type of el-
ementary tree can be substituted or adjoined to the
other?the system predicts an S tree to which both
can be attached, but no specific verb head. Pre-
diction markers can be removed from nodes via
the verification operation, which makes sure that
predicted structure is matched against actually ob-
served evidence from the input string. For the ex-
ample above, the verb ran in ?The thief quickly
ran? verifies the predicted verb structure. In fig-
ures 5c through 5e, we also provide an example of
prediction and verification as part of the demon-
stration of our semantic framework. Other foun-
dational work on PLTAG (Demberg-Winterfors,
2010) contains more detailed description.
5 Neo-Davidsonian semantics
Davidsonian semantics organizes the representa-
tion of predicates around existentially-quantified
event variables (e). Sentences are therefore treated
as descriptions of these events, leading to a less
recursive representation where predicates are not
deeply embedded inside one another. Highly
recursive representations can be incrementality-
unfriendly, potentially requiring complex infer-
ence rules to ?undo? recursive structures if rele-
vant information arrives later in the sentence.
Neo-Davidsonian semantics (Parsons, 1990;
Hunter, 2009) is an extension of Davidsonian
semantics wherein the semantic roles are also
separated out into their own first-order predi-
cates, rather than being fixed arguments of the
main predicate of the verb. This enables a sin-
gle verb predicate to correspond to multiple pos-
sible arrangements of role predicates, also an
59
incrementality-friendly characteristic1. The Neo-
Davidsonian representation allows us separate the
semantic prediction of a role from its syntactic ful-
fillment, permitting the type of flexible framework
we are proposing in this paper.
We adopt a neo-Davidsonian approach to se-
mantics by a formalism that bears similarity to ex-
isting frameworks such as (R)MRS (Robust Min-
imal Recursion Semantics) (Copestake, 2007).
However, this paper is intended to explore what
architecture is minimally required to augment the
PLTAG syntactic framework, so we do not adopt
these existing frameworks wholesale. Our ex-
amples such as figures 4, 5d, and several others
demonstrate how this looks in practice.
6 Semantics for PLTAG
6.1 Semantic augmentation for the lexicon
Constructing the lexicon for a semantically aug-
mented PLTAG uses a process based on the one
for ?purely syntactic? PLTAG. The PLTAG lex-
icon is extracted automatically from the PLTAG
treebank, which has been derived from the Penn
Treebank using heuristics for binarizing flat struc-
tures as well as additional noun phrase annotations
(Vadas and Curran, 2007), PropBank (Palmer et
al., 2003), and a slightly modified version of
the head percolation table of Magerman (1994).
PLTAG trees in the treebank are annotated with
syntactic headedness information as well as infor-
mation that allows one to distinguish arguments
and modifiers.
Given the PLTAG treebank, we extract the
canonical lexicon using well-established ap-
proaches from the LTAG literature (in particular
(Xia et al, 2000): we traverse the converted tree
from each leaf up towards the root, as long as the
parental node is the head child of its parent. If a
subtree is not the head child of its parent, we ex-
tract it as an elementary tree and proceed in this
way for each word of the converted tree. Given the
argument/modifier distinction, we then create sub-
stitution nodes in the parent tree for arguments or
a root and foot node in the child tree for modifiers.
Prediction trees are extracted automatically by cal-
culating the minimal amount of structure needed
to connect each word into a structure including all
previous words of the sentence2. The parts of this
1Consider the optionality of the agent role in passive sen-
tences, where the ?by-phrase? may or may not appear.
2The reader is referred to (Demberg-Winterfors, 2010;
S
{?e&? = e}
NP?
{Q1x1
ARG0(e, x1)}
VP
{e}
V
likes
{Like(e)}
NP?
{Q2x2
ARG1(e, x2)}
NP
{?e}
NP*
{Q1x1
ARG0(e, x1)
&? = x1
&? = Q1}
VP
{e}
V
including
{Include(e)}
NP?
{Q2x2
ARG1(e, x2)}
Figure 1: Verbal elementary trees extracted from
example sentence Pete likes sugary drinks includ-
ing alcoholic ones.
minimally-needed connecting syntactic structure
which belong to heads to the right of the current
word are stored in the lexicon as prediction trees,
c.f. right hand side of figure 5b.
Since Propbank is used in the construction pro-
cess of the PLTAG treebank, we can straightfor-
wardly display the semantic role annotation on the
tree and the extracted lexicon, with the exception
that we display role annotations for PPs on their
NP child. For arguments, annotations are retained
on the substitution node in the parental tree, while
for modifiers, the role annotation is displayed on
the foot node of the auxiliary tree, as shown for the
verbal trees extracted from the sentence Pete likes
sugary drinks including alcoholic ones in Figure
1. PropBank assigns two roles to the NP node
above sugary drinks (it is the ARG1 of likes and
the ARG0 of including), but we can correctly tease
apart these annotations in the lexical extraction
process using the syntactic annotation and argu-
ment/modifier distinction.
Using the same procedure, prediction trees are
annotated with semantic roles. It can then happen
that one form of a prediction tree is annotated with
different syntactic roles, hence introducing some
additional ambiguity into the lexicon. For exam-
ple, the NP substitution node in subject position of
the prediction tree rooted in Sk in figure 5b could
be an ARG0 for some verbs which can verify this
tree and an ARG1 for others.
PLTAG elementary trees can contain one or
more lexemes, where the first lexeme is the el-
ementary tree?s main anchor, and all further lex-
emes are predicted. In earlier PLTAG extractions,
elementary trees with several lexemes were used
for particle verbs like show up and some hand-
coded constructions in which the first part is pre-
dictive of the second part, such as either . . . or or
both . . . and. Here we extend this set of trees with
Demberg et al, 2014) for full details of the PLTAG conver-
sion and syntactic part of the lexicon extraction process.
60
more than one lexeme to verbs with subcatego-
rized PPs, as shown, for example, in the second
lexicon entry of slid in figure 5a. Note the differ-
ence to the lexicon entry of optional PPs in figure
5b as in on Sunday. Furthermore,
? All elementary trees which have a role anno-
tation in PropBank also have a correspond-
ing annotation ?e on their root node that
represents the existentially-quantified neo-
Davidsonian event variable for that predicate,
see fig. 1.
? The event variables and entity variables on an
elementary tree are available for binding on
the path from the anchor3 of the elementary
tree to the root node.
? Every role annotation on a node is in the form
of a predicate ARGn(e, x), where e is the
event variable, and x is an entity variable to
which the role is conferred.
? Every role annotation is prefixed with a vari-
able binding Qx, where Q is a higher-order
variable that represents an unknown quanti-
fier. This ensures that all variables are bound
if a role appears before its filler.
? Every elementary tree for an open-class word
has a head with corresponding predicate. For
example, ?butter? has a predicate Butter(x).
? Prediction trees for open lexical classes (such
as NPs) have a head with a (x) predicate.
? Every nominal elementary tree has a Qx at
the root node so that the entity variable that
is the argument to the predicate on the head
is bound. The Qx is on the root node so that
our semantic processing procedure for substi-
tutions and adjunctions (described in the next
section) can unify the entity variable x with
variables on higher trees.
For PPs, we obtain role annotations from Prop-
Bank and NomBank. Other closed-class syntactic
types such as pronouns have appropriately-
selected quantifier constants and predicates
(e.g. ?someone? would be represented as
?xPerson(x)&? = ?&? = x, see next paragraph
for the use of question marks). Determiners are
merely annotated with a quantifier ?constant?
symbol and no variables or predicates.
Then we require a type of additional annota-
tion to which we refer as a ?variable assignment
statement?, which we use in our syntactic com-
3Lowest node on the path to where the anchor would be
in a prediction tree which does not have a lexical anchor.
bination process. These statements are written
? = v, where v is either a quantifier variable
(Q) or constant (e.g. ?) or an entity variable (x).
These statements represent the possibility that an
incoming tree might have a variable v that could
have the same binding as one already in the pre-
fix tree. Variable assignment statements occur on
root nodes or foot nodes, except where there is a
descendent DT subsitution node, which receives
an additional ? = Q statement. The type of vari-
able assignment statement (event, entity or quan-
tifier) depends on the root node type (entity type
like NP or N vs. event type like S or VP), as shown
in figure 1. The next section describes the use of
these statements in semantic parsing. Note that
variable assignment statements need not be rep-
resented explicitly in an implementation, as reas-
signing variables can be done via references or
other data structures. We use them as a represen-
tational and illustrative convenience here.
6.2 Semantic parsing procedure
We integrate semantics into the overall process of
PLTAG parsing by the rules in figures 2 and 3. In
addition, we provide a more procedural descrip-
tion here. At the highest level, a step in an incre-
mental parse follows this pattern:
1. On scanning a new word or doing a predic-
tion step, the PLTAG statistical model selects
a tree from the lexicon, an operation (substi-
tion, adjunction, verification), and a position
in the prefix tree at which to insert the tree (or
none, if this is the first word).
2. All the nodes of the incoming tree are vis-
ited by the visit operation, and their semantic
content is appended as conjuncts to the out-
put semantic expression.
3. The operation of attaching the new tree into
the derived tree is performed (pltagOp):
(a) Variable assignment statements are
emitted and appended to the semantic
output expression according to the
rules in figure 3, as well as to the
semantic expression at the syntactic
node at which the integration occurs.
For verification, the Verify rule has to
be applied to all nodes that are verified.
(b) The syntactic integration of merging the
nodes at the substitution or adjunction
site is performed. The rules in 3 also
make sure that the semantic expressions
61
D : {?} T
PltagStep
pltagOp(D,T ) : {?&visit(T )}
D : {?}
Resolve
D : resolveEqns(?)
Figure 2: Overall rules for trees (T ) and derivations (D) and overall semantic expressions (?). PltagStep
applies when a new tree is chosen to be integrated with the prefix tree.
N1 ?: {?1, Q1,?2} N2 ?: {?3, ? = Q2,?4} D : {?}
QuantEquate
D[N1 7? nodeMerge(N1 : {?1, Q1,?2}, N2 : {?3,?4})] : {?&Q1 = Q2}
N1 ?: {?1, x1,?2} N2 ?: {?3, ? = x2,?4} D : {?}
VarEquate
D[N1 7? nodeMerge(N1 : {?1, x1,?2}, N2 : {?3,?4})] : {?&x1 = x2}
N1
p
p : {?1, 1,?2} N2 : {?3} anchor(N2):{?4, Pred(x),?5} D : {?}
Verify
D[N1 7? nodeMerge(N1 : {?1, 1,?2}, N2 : {?3})] : {?& 1 = Pred}
Figure 3: Rules for combining nodes. The nodes are attached during the derivation via the nodeMerge
operation, with N1 being the node above (?), and N2 being the node below (?). These hold for substi-
tution and adjunction (for both canonical and prediction trees). The underlying intuition is that the (?)
node will contain the variable equation, and the (?) node will contain the mention of a variable to be
equated. The Verify rule equates the variable with the predicate of the verification tree. The equation
is appended to the output expression ?. Q2 can also be ? or another quantifier. VarEquate also applies to
event variables. The ?n notation represents the prefixes and suffixes of the semantic expressions relative
to the mentioned variable or statement. The rules delete the variable assignment statement from the node
by concatenating ?3 and ?4.
from both nodes involved in the integra-
tion are included in the semantic expres-
sion of the merged node.
4. Optionally, a Resolve step is applied, which
eliminates variable assignment statements by
replacing variable mentions with their most
concrete realization.
Regarding variable assignments at the integra-
tion of two trees, the value for quantifier vari-
ables can be a constant in the form of a quanti-
fier. Entity variables can be equated with other
entity variables, and entity constants (e.g., proper
names) are a relatively simple extension to the
rules4. Verification variables can only be equated
with a constant?a predicate name.
We present an example of the processing of
a substitution step in figure 4. The S tree for
sleeps with an open NP substitution node is in
the process of having the NP ?someone? substi-
tuted into it using the substUp operation. So we
have already done step 1 of our parsing procedure.
Step 2 is visit, such that the semantic expression
of the NP is appended to the output expression
4A noun phrase like ?Peter? will have the associated se-
mantic expression peter&? = peter and will require an ad-
ditional inference rule to remove the quantifier when it is
adjoined or substituted to a node carrying a role. In other
words, substituting peter into QxARG1(e, x) should result
in ARG1(e, peter). An analogous rule for constant verifi-
cation that allows Qx (x) to be verified as peter is also
required.
?. For step 3, the variable assignment statements
are then processed by application of QuantEquate
and VarEquate. Finally in step 4, the expression is
simplified with Resolve.
The Resolve operation. From an implementa-
tion perspective, resolving variable assignment
statements does not really need a separate oper-
ation, as references can be maintained such that
the assignment is automatically performed with-
out any explicit substitution in the manner of a
Prolog inference engine?s resolution procedure.
The same holds for the variable assignment state-
ments. However, we include explicit mention of
this mechanism for ease of expression of the se-
mantic operations as well as to illustrate some de-
gree of convergence with existing formalisms such
as (R)MRS, which also has a mechanism to assert
relationships between variables post hoc.
There is only one condition under which ap-
plication of Resolve can fail, which is if there is
more than one assignment statement connecting
the same variable to different constants.
The Resolve rule is defined to be able to apply
to the entire output expression. When should it
apply? It is defined such that it can be applied at
any time; its actual execution will be controlled
by the parsing algorithm, e.g., after each parsing
operation or at the end of the parse.
There are remaining matters of quantifier scope
62
NP
{?x1Person(x1)
&? = x1&? = ?}
PRO
someone
substUp
?????? S
{Ee}
NP?
{Q0x0ARG0(e, x0)}
VP
sleeps
(Syntactic view)
? = ?eQ0x0ARG0(e, x0)
(Before substitution starts)
? = ?eQ0x0ARG0(e, x0)
&?x1Person(x1)&? = x1&? = ?
(Result of visit)
? = ?eQ0x0ARG0(e, x0)
&?x1Person(x1)&? = x1&? = ?
&Q0 = ?&x0 = x1
(Result of QuantEquate and VarEquate)
? = ?e?x0ARG0(e, x0)&Person(x0)
(Result of Resolve)
Figure 4: An example incremental step from the
semantic perspective.
and semantic well-formedness that must be han-
dled post hoc at every step. For example, univer-
sal quantifiers require a distinction to be made be-
tween the restrictor of the quantified variable and
the nuclear scope. It is possible within a neo-
Davidsonian representation to perform such rep-
resentational adjustments easily, as shown by Say-
eed and Demberg (2012).
Example Now that we have described the pro-
cedure, we provide an example of how this se-
mantic augmentation of PLTAG can represent role
labeling and prediction inside the syntactic pars-
ing system. We perform a relevant segment of the
parse of example (1-a), ?The woman slid the but-
ter to the man.? In this sentence, we expect that the
parser will already know the expected role of the
NP ?the man? before it actually receives it. That is,
it will know in advance that there is an upcoming
NP to be predicted such that it is compatible with a
recipient (ARG2) role, and this knowledge will be
represented in the incremental output expression.
The minimum lexicon required for our example
is contained in figures 5a and 5b. For our illustra-
tion, we only include the ditransitive alternation of
?slide?. Both versions of slide contain all the roles
on NP nodes. This parse involves only the predic-
tion of noun phrases, so we only have an NP pre-
diction tree. We presume for the sake of simplicity
that the determiner ?the? represents the existential
quantifier ?.
Our parse begins in figure 5c with ?The woman
slid?, since these are the same in both cases, and
it proceeds up to figure 5e with the sentence ?The
woman slid the butter to the man?. We Resolve
the assignments at every step for brevity in the ex-
amples, and we also apply it to the nodes. By fig-
ure 5d, the parser already knows that the ARG2 of
?slide? is what is sought. Finally, by figure 5e, the
appropriate NP is expected by prediction.
7 Discussion and conclusions
We demonstrated how syntactic prediction and
thematic roles can interact in our framework, but
we did so with a simple example of prediction:
a single noun phrase. Our framework is, how-
ever, able to accomodate more complex interac-
tions. In particular, we want to draw attention
to an example which can not be modelled by
other formalisms which are not fully connected
like PLTAG. Consider sentences beginning with
?The victim/criminal was violently. . . ?. Does the
semantic association between ?victim? vs. ?crimi-
nal? and ?violently? change the likelihoods of the
semantic roles that can be assigned to the subject
NP? Does it make an active or a passive voice
verb more likely after ?violently?? These are the
kinds of possible syntactic-semantic interactions
for which one will need a flexible but robust for-
malism such as we have described in this paper:
the prediction mechanism allows dependents to
jointly affect the expectation of a head even before
the head has been encountered. Note that these
interactions can also go beyond thematic roles.
In this paper, we have presented a procedure
to augment a treebank-extracted PLTAG lexicon
with semantic annotations based in a flexible neo-
Davidsonian theory of events. Then we have
provided the way to combine these representa-
tions during incremental parsing in a manner fully
synchronized with the existing PLTAG syntactic
operations. We demonstrated that we can rep-
resent thematic role prediction in a case that is
known to be relevant to an on-going stream of psy-
cholinguistic research. Ongoing and future work
includes the development of a joint syntactic-
semantic statistical model for PLTAG and experi-
mental validation of predictions made by our se-
mantic augmentation. We are also considering
higher-order semantic issues such as quantifier
scope underspecification in the context of our for-
malism (Koller et al, 2003).
63
S
{?e? = e}
NP?
{Q0x0ARG0(e, x0)}
VP
{e}
V
slid
{Slid(e)}
NP?
{Q2x2ARG2(e, x2)}
NP?
{Q1x1ARG1(e, x1)}
S
{?e? = e}
NP?
{Q0x0ARG0(e, x0)}
VP
{e}
V
slid
{Slid(e)}
NP?
{Q1x1ARG1(e, x1)}
PP
TOk
tokk
NP?
{Q2x2ARG2(e, x2)}
(a) Lexicon: ditransitive alternation of slid.
NP
{Qx? = Q&? = x}
DT?
{? = Q}
N
woman | man | butter
{Woman(x)
|Man(x)
|Butter(x)}
DT
{?}
the
TO
to
VP
VP*
{?e? = e}
PP
P
on
NP?
{ARGM-TEMP(e, x)}
Sk
{?e? = e}
NPk ?
{Q1x1ARG0(x1)}
VPkk
{ (e)}
NPk
{Qx? = Q&? = x}
DTk ?
{? = Q}
Nkk
{ (x)}
(b) Lexicalized trees and prediction trees.
S
{?e? = e}
NP?
{?x0ARG0(e, x0)}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP1
{Q2x2ARG2(e, x2)}
DT1 ?
{? = Q2}
N11
{ (x2)}
NP?
{Q1x1ARG1(e, x1)}
S
{?e? = e}
NP?
{?x0ARG0(e, x0)}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP1
{Q1x1ARG1(e, x1)}
DT1 ?
{? = Q1}
N11
{ (x1)}
PP
TOk
tokk
NP?
{Q2x2ARG2(e, x2)}
?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e) ?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)
&Q2x2ARG2(e, x2)&? = Q2& (x2)&Q1x1ARG1(e, x1) &Q1x1ARG1(e, x1)&? = Q1& (x1)&Q2x2ARG2(e, x2)
(c) Parse of ?The woman slid? with respect to the ditransitive alternation, with the syntactic prediction of an NP. Two possibilities
still remain. The semantics are identical except for the role of the predicted nominal predicate. The ? = e variable assignment
statement persists through the derivation, representing the possibility that this sentence is embedded in another.
S
{?e? = e}
NP?
{?x0ARG0(e, x0)}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP
{?x1ARG1(e, x1)}
DT
{?}
the
N
butter
{Butter(x1)}
PP
TOk
tokk
NP?
{Q2x2ARG2(e, x2)}
?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)
&?x1ARG1(e, x1)&Butter(x1)&Qx2ARG2(e, x2)
(d) Parse of ?The woman slid the butter. . . ?. The arrival of
?the butter? greatly reduces the likelihood of the recipient
role (ARG2) being the one filled at this point, effectively
abolishing the first parse.
S
{?e? = e}
NP?
{?x0 . . .}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP
{?x1 . . .}
DT
{?}
the
N
butter
{Butter(x1)}
PP
TO
to
NP2
{Q2x2 . . .}
DT2 ?
{? = Q2}
N22
{ (x2)}
?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)
&?x1ARG1(e, x1)&Butter(x1)&Qx2ARG2(e, x2)
&? = Q2& (x2)
(e) Parse of ?The woman slid the butter to. . . ?. to is verified
and the last NP is expanded via prediction. This gives us
the last predicted predicate in the semantic expression. It
shares its variable with the ARG2 role, thus thematically
restricting its possible verifications.
Figure 5: Excerpt of our example parse.
64
References
Gerry Altmann and Yuki Kamide. 1999. Incremen-
tal interpretation at verbs: Restricting the domain of
subsequent reference. Cognition, 73(3):247?264.
Ann Copestake. 2007. Semantic composition with (ro-
bust) minimal recursion semantics. In Proc. of the
Workshop on Deep Linguistic Processing.
Vera Demberg and Frank Keller. 2008. A psycholin-
guistically motivated version of tag. In Proceedings
of the 9th International Workshop on Tree Adjoin-
ing Grammars and Related Formalisms. Tu?bingen,
pages 25?32.
Vera Demberg and Frank Keller. 2009. A computa-
tional model of prediction in human parsing: Uni-
fying locality and surprisal effects. In Proceedings
of the 29th meeting of the Cognitive Science Society
(CogSci-09).
Vera Demberg, Frank Keller, and Alexander Koller.
2014. Parsing with psycholinguistically motivated
tree-adjoining grammar. Computational Linguistics,
40(1).
Vera Demberg-Winterfors. 2010. A Broad-Coverage
Model of Prediction in Human Sentence Processing.
Ph.D. thesis, University of Edinburgh.
Tim Hunter. 2009. Deriving syntactic properties of ar-
guments and adjuncts from neo-davidsonian seman-
tics. In Proc. of MOL 2009, Los Angeles, CA, USA.
Srini Narayanan Daniel Jurafsky. 2002. A bayesian
model predicts human parse preference and reading
times in sentence processing. In Advances in Neu-
ral Information Processing Systems 14: Proceed-
ings of the 2001 Neural Information Processing Sys-
tems (NIPS) Conference, volume 1, page 59. The
MIT Press.
Yuki Kamide, Gerry Altmann, and Sarah L Haywood.
2003a. The time-course of prediction in incremen-
tal sentence processing: Evidence from anticipatory
eye movements. Journal of Memory and Language,
49(1):133?156.
Yuki Kamide, Christoph Scheepers, and Gerry TM
Altmann. 2003b. Integration of syntactic and se-
mantic information in predictive processing: Cross-
linguistic evidence from german and english. Jour-
nal of Psycholinguistic Research, 32(1):37?55.
Alexander Koller, Joachim Niehren, and Stefan Thater.
2003. Bridging the gap between underspecifica-
tion formalisms: Hole semantics as dominance con-
straints. In Proc. of EACL 2003, pages 367?374.
David M Magerman. 1994. Natural language pars-
ing as statistical pattern recognition. Ph.D. thesis,
Stanford University.
Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. 2010. Syntactic and semantic factors
in processing difficulty: An integrated measure. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 196?
206. Association for Computational Linguistics.
Ulrike Pado?, Matthew W Crocker, and Frank Keller.
2009. A probabilistic model of semantic plausi-
bility in sentence processing. Cognitive Science,
33(5):794?838.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71?
106.
T. Parsons. 1990. Events in the semantics of English.
MIT Press, Cambridge, MA, USA.
Philip Resnik. 1992. Left-corner parsing and psycho-
logical plausibility. In In The Proceedings of the fif-
teenth International Conference on Computational
Linguistics, COLING-92, pages 191?197.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational linguistics,
27(2):249?276.
Asad Sayeed and Vera Demberg. 2012. Incremen-
tal neo-davidsonian semantic construction for tag.
In 11th Workshop on Tree-Adjoining Grammars and
Related Formalisms (TAG+11).
Mark Steedman. 2000. The syntactic process. MIT
Press.
Henry S. Thompson, Mike Dixon, and John Lamping.
1991. Compose-reduce parsing. In Proceedings of
the 29th annual meeting on Association for Compu-
tational Linguistics, pages 87?97, Berkeley, Califor-
nia.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 240?247,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Fei Xia, Martha Palmer, and Aravind Joshi. 2000. A
uniform method of grammar extraction and its appli-
cations. In Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 53?62.
65
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 84?93,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
On the Information Conveyed by Discourse Markers
Fatemeh Torabi Asr
MMCI Cluster of Excellence
Saarland University
Germany
fatemeh@coli.uni-saarland.de
Vera Demberg
MMCI Cluster of Excellence
Saarland University
Germany
vera@coli.uni-saarland.de
Abstract
Discourse connectives play an impor-
tant role in making a text coherent
and helping humans to infer relations
between spans of text. Using the
Penn Discourse Treebank, we investi-
gate what information relevant to in-
ferring discourse relations is conveyed
by discourse connectives, and whether
the specificity of discourse relations re-
flects general cognitive biases for estab-
lishing coherence. We also propose an
approach to measure the effect of a dis-
course marker on sense identification
according to the different levels of a re-
lation sense hierarchy. This will open a
way to the computational modeling of
discourse processing.
1 Introduction
A central question in psycholinguistic model-
ing is the development of models for human
sentence processing difficulty. An approach
that has received a lot of interest in recent
years is the information-theoretic measure of
surprisal (Hale, 2001). Recent studies have
shown that surprisal can successfully account
for a range of psycholinguistic effects (Levy,
2008), as well as account for effects in nat-
uralistic broad-coverage texts (Demberg and
Keller, 2008; Roark et al, 2009; Frank, 2009;
Mitchell et al, 2010). : what work of Roark
and Frank you mean here? Under the no-
tion of the Uniform Information Density hy-
pothesis (UID, Levy and Jaeger, 2007; Frank
and Jaeger, 2008), surprisal has also been
used to explain choices in language produc-
tion: When their language gives people the
option to choose between different linguistic
encodings, people tend to choose the encod-
ing that distributes the information more uni-
formly across the sentence (where the informa-
tion conveyed by a word is its surprisal).
When using surprisal as a cognitive model
of processing difficulty, we hypothesize that
the processing difficulty incurred by the hu-
man when processing the word is proportional
to the update of the interpretation, i.e. the in-
formation conveyed by the word (Hale, 2001;
Levy, 2008). We can try to estimate partic-
ular aspects of the information conveyed by a
word, e.g., the information conveyed about the
syntactic structure of the sentence, the seman-
tic interpretation, or about discourse relations
within the text.
This paper does not go all the way to
proposing a model of discourse relation sur-
prisal, but discusses first steps towards a
model for the information conveyed by dis-
course connectors about discourse relations,
based on available resources like the Penn Dis-
course Treebank (Prasad et al, 2008). First,
we quantify how unambiguously specific dis-
course relations are marked by their typical
connectors (Section 4.1) and test whether eas-
ily inferable relations are on average marked
more ambiguously than relations which are
less expected according to the default assump-
tion of a reader. This idea is shaped with
respect to the UID hypothesis: expected re-
lations can afford to be signaled by weaker
markers and less expected ones should be
marked by strong connectors in order to keep
the discourse-level information density smooth
throughout the text (Section 4.2). We then
investigate in more detail the types of ambi-
guity that a reader might face when process-
ing discourse relations. While some ambigui-
ties lie in discourse connectors, it also happens
that more than one relation exist at the same
time between two text spans. We show that
84
some discourse markers also signal the pres-
ence of several relations (Section 5). In com-
putational modeling as well as laboratory set-
ups, one should therefore have a strategy to
deal with the different types of ambiguities.
Finally, we ask what granularity of distinction
from other discourse relations (with respect to
the PDTB relation sense hierarchy) each En-
glish discourse connective conveys (Section 6).
2 Discourse Relations and their
Markers
A cognitive approach to discourse process-
ing emphasizes on the procedural role of the
connectives to constrain the way readers re-
late the propositions in a text (Blakemore,
1992; Blass, 1993). Experimental findings
suggest that these markers can facilitate the
inference of specific discourse relations (De-
gand and Sanders, 2002), and that discourse
connectors are processed incrementally Ko?hne
and Demberg (2013). People can however in-
fer discourse relations also in the absence of
discourse connecotrs, relying on the propo-
sitional content of the sentences and their
world-knowledge (Hobbs, 1979; Asher and
Lascarides, 1998). Asr and Demberg (2012b)
point out that similar inferences are also nec-
essary for discourse relations which are only
marked with a weak connector which can be
used for many relations, such as and. Further-
more, we know that the inference of discourse
relations is affected by a set of general cog-
nitive biases. To illuminate the role of these
factors let?s have a look at (1). While the type
of relation between the two events is clearly in-
ferable in (1-a) and (1-b) due to the discourse
connectives, in (1-c), the reader would have
to access their knowledge, e.g., about Harry
(from larger context) or the usual affairs be-
tween bosses and employees, in order to con-
struct a discourse relation.
(1) a. The boss was angry because Harry skipped
the meeting (reason).
b. The boss was angry, so Harry skipped the
meeting (result).
c. The boss was angry and Harry skipped the
meeting.
Here, not only both reason and result inter-
pretations but even an independent parallel re-
lation (simple Conjunction) between the two
events are possible to be inferred as a relatively
neutral connective, i.e., and is used. Levinson
(2000) notes in his discussion on presumptive
meanings that ?when events are conjoined they
tend to be read as temporally successive and if
at all plausible, as causally linked?. If this is
true then the result reading is most probable
for (1-c). General preferences of this kind have
been investigated via experimental approaches
(Segal et al, 1991; Murray, 1997; Sanders,
2005; Kuperberg et al, 2011). Segal et al
(1991) and Murray (1997) argue that readers
expect a sentence to be continuous with re-
spect to its preceding context (the continuity
hypothesis). Continuous discourse relations in
terms of congruency and/or temporality are
consequently easier to process than the dis-
continuous ones. Sanders (2005) proposes that
causal relatedness entails the maximum degree
of coherence in a text, therefore readers always
start by attempting to find cause-consequence
relations between neighboring sentences (the
causality-by-default hypothesis). In a similar
vein, Kuperberg et al (2011) shows that read-
ers face comprehension difficulty when sen-
tences in short text spans cannot be put into
causal relation and no marker of other rela-
tions (e.g., Concession) is available.
Taken together, these findings suggest that
world knowledge, general cognitive biases, and
linguistic features of the sentences such as the
presence of a weak or strong marker contribute
to the relational inference. With a look back
to the information theoretic approach to the
linguistic patterns, one could hypothesize that
when one factor is strongly triggering expec-
tation for a specific type of relation the other
factors could remain silent in order to keep the
information distribution uniform. With this
perspective, Asr and Demberg (2012a) tested
whether the predictability of discourse rela-
tions due to general cognitive biases (towards
causality and continuity) can explain the pres-
ence vs. absence of the discourse connectors.
They found that connectors were more likely
to be dropped in the more predictable (causal
or continuous) relations than in others. Our
investigation of the explicit relations in this
paper (the first experiment) looks into this
question in a stricter manner considering how
much information a connective delivers about
discourse relations. Since this information is
85
Figure 1: Hierarchy of senses in PDTB
(Prasad et al, 2008)
closely related to the ambiguities a connec-
tive removes (or maybe adds to the context)
in the course of reading, we dedicate a sepa-
rate section in this paper to illuminate differ-
ent types of ambiguities. Also, a more detail
question would be what types of informa-
tion a connective can convey about one or sev-
eral discourse relations. To our best of knowl-
edge there has been no corpus-based study so
far about this last point which we will try to
model in our third experiment.
3 Penn Discourse Treebank
The Penn Discourse Treebank (PDTB, Prasad
et al, 2008) is a large corpus annotated with
discourse relations, (covering the Wall Street
Journal part of the Penn Treebank). The an-
notation includes sentence connectives, spans
of their arguments and the sense of discourse
relations implied by the connectives. The rela-
tion labels are chosen according to a hierarchy
of senses (Figure 1). Annotators were asked
to find the Explicit discourse connectives and
respectively select a sense (as much specific as
possible) from the hierarchy. For neighboring
sentences where no explicit marker existed in
the original text they were asked to first insert
a suitable connective between the two argu-
ments and then annotate a relation sense, in
this case categorized as Implicit. If an expres-
sion ? not belonging to the list of constituted
connectives ? in one of the involved sentences
is already indicative of a specific relation, then
instead they marked that expression and put
the relation into the AltLex category. In all
of our experiments only the explicit relation
are considered. Some connectives were anno-
tated with two sense labels in the PDTB. In
our analyses below, we count these text spans
twice (i.e., once for each sense), resulting in a
total of 19,458 relation instances.
4 Are Unexpected Relations
Strongly Marked?
4.1 Markedness Measure
Point-wise mutual information (pmi) is an
information-theoretic measure of association
between two factors. For our purpose of mea-
suring the markedness degree of a relation r in
the corpus, we calculate the normalized pmi of
it with any of the connectives, written as c that
it co-occurs with:
npmi(r; c) =
pmi(r; c)
? log p(r, c)
=
log p(r,c)p(r)p(c)
? log p(r, c)
=
log p(r)p(c)
log p(r, c)
? 1
npmi is calculated in base 2 and ranges be-
tween ?1 and 1. For our markedness measure,
we scale it to the interval of [0, 1] and weigh it
by the probability of the connector given the
relation.
0 <
npmi(r; c) + 1
2
< 1
markedness(r) =
?
c
p(c|r)
npmi(r; c) + 1
2
Intuitively, the markedness measure tells us
whether a relation has very specific markers
(high markedness) or whether it is usually
marked by connectors that also mark many
other relations (low markedness).
4.2 Discourse Expectations and
Marker Strength
Given the markedness measure, we are now
able to test whether those relations which are
more expected given general cognitive biases
86
0.62	 ?
0.64	 ?
0.66	 ?
0.68	 ?
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
Temporal	 ?(3696)	 ? Con?gency	 ?(3741)	 ? Expansion	 ?(6431)	 ? Comparison	 ?(5590)	 ?
Markedness	 ?
Figure 2: Markedness of level-1 explicit rela-
tions in the PDTB (frequencies of the relations
given in brackets).
(expecting continuous and causal relations)
are marked less strongly than e.g. discontinu-
ous relations. Figure 2 compares the marked-
ness associated to the explicit relations of the
PDTB when the first level relation sense dis-
tinction is considered.
Figure 2 shows that COMPARISON rela-
tions exhibit higher markedness than other
relations, meaning that discontinuity is
marked with little ambiguity, i.e. markers
of COMPARISON relations are only very rarely
used in other types of discourse relations.
COMPARISON relations are exactly those rela-
tions which were classified in Asr and Demberg
(2012a) as a class of discontinuous relations.
Further experimental evidence also shows that
these relations are more likely to cause pro-
cessing difficulty than others when no connec-
tor is present (Murray, 1997), and that their
markers have a more strongly disruptive effect
than other markers when used incorrectly. Un-
der the information density view, these obser-
vations can be interpreted as markers for com-
parison relations causing a larger context up-
date. The high markedness of COMPARISON re-
lations is thus in line with the hypothesis that
unpredictable relations are marked strongly.
CONTINGENCY relations, on the other hand,
exhibit a lower score of markedness. This
indeed complies with the prediction of
the causality-by-default hypothesis (Sanders,
2005) in conjunction with the UID hypothe-
sis: causal relations can still be easily inferred
even in the presence of ambiguous connectives
because they are preferred by default.
As also discussed in Asr and Demberg
(2012a), some types of EXPANSION relations
are continuous while others are discontinuous;
finding that the level of markedness is near the
average of all relations therefore comes as no
surprise.
More interesting is the case of TEMPORAL
relations: these relations have low marked-
ness, even though this class includes contin-
uous (temporal succession) relations as well as
discontinuous (temporal precedence) relations,
and we would thus have expected a higher level
of markedness than we actually find. Even
when calculating markedness at the more fine-
grained relation distinction level, did not find
a significant difference between the marked-
ness of the temporally forward vs. backward
relations. A low level of markedness means
that the connectors used to mark temporal re-
lations are also used to mark other relations,
in particular, temporal connectives are often
used to mark CONTINGENCY relations. This
observation brings us to the question of gen-
eral patterns of ambiguity in discourse markers
and the ambiguity of discourse relations them-
selves, see Section 5.
5 Ambiguous Connective
vs. Ambiguous Relation
Some discourse connectives (e.g., since, which
can be temporal or causal, or while, which can
be temporal or contrastive) are ambiguous. In
this section, we would like to distinguish be-
tween three different types of ambiguity (all
with respect to the PDTB relation hierarchy):
1. A connector expressing different relations,
where it is possible to say that one but not
the other relation holds between the text
spans, for example since.
2. A connector expressing a class of relations
but being ambiguous with respect to the
subclasses of that relation, for example
but, which always expresses a COMPARISON
relationship but may express any subtype
of the comparison relation.
3. the ambiguity inherent in the relation be-
tween two text spans, where several rela-
87
Relation pair #R1 (total) #R2 (total) #Pair ?2
T.Synchrony?CON.Cause.reason 507 (1594) 353 (1488) 187 1.08E+00
T.Asynchronous.succession?CON.Cause.reason 189 (1101) 353 (1488) 159 2.43E+02 ***
E.Conjunction?CON.Cause.result 352 (5320) 162 (752) 140 2.22E+02 ***
T.Synchrony?EXP.Conjunction 507 (1594) 352 (5320) 123 5.43E+01 ***?
T.Synchrony?CON.Condition.reneral 507 (1594) 70 (362) 52 1.67E+01 ***
T.Synchrony?COM.Contrast.juxtaposition 507 (1594) 77 (1186) 45 1.97E+00
T.Asynchronous.precedence?E.Conjunction 66 (986) 352 (5320) 36 1.15E+01 ***
T.Synchrony?COM.Contrast 507 (1594) 37 (2380) 28 9.55E+00 ***
T.Synchrony?COM.Contrast.opposition 507 (1594) 28 (362) 21 6.78E+00 **
Table 1: Most frequent co-occurring relations in the PDTB, their frequency among multi-labels
(and in the entire corpus)
tions can be identified to hold at the same
time.
The first and second notion of ambiguity re-
fer to what we so far have been talking about:
we showed that some connectors mark can
mark differnt types of relations, and that some
connectives marking a general relation type
but not marking specific subrelations.
The third type of ambiguity is also anno-
tated in the PDTB. Relations which are am-
biguous by nature are either labeled with a
coarse-grained sense in the hierarchy (e.g.,
COMPARISON.Contrast the second most fre-
quent label in the corpus chosen by the anno-
tators when they could not agree on a more
specific relation sense), or are labelled with
two senses. Table 1 lists which two relation
senses were most often annotated to hold at
the same time in the PDTB, along with the
individual frequency (also frequency in the
entire corpus inside brackets). Sub-types of
Cause and TEMPORAL relations appear most
often together, while TEMPORAL.Synchrony is
a label that appears significantly more than
expected among the multi-label instances,
even with a higher frequency than that of
EXPANSION.Conjunction, the most frequent
label in the corpus. Such observations confirm
the existence of the third type of ambiguity in
discourse relations.
Interestingly, these inherently ambiguous
relations also have their own specific mark-
ers, such as meanwhile which occurs in about
70% of its instances with two relation senses1.
1This connective is mostly labeled with
TEMPORAL.Synchrony and EXPANSION.Conjunction.
Interestingly these two labels appear together signif-
icanly less frequently than expected (as marked in
the table with ***?) but when such a cooccurrance
happened in the corpus it has been for the connective
meanwhile.
On the other hand, other well-known ambigu-
ous connectors like since rarely mark inher-
ently ambiguous relations, and most often can
be identified as one specific relation sense by
looking at the content of the arguments. The
importance of the possibility to annotate a
second sense and hence explicitly mark the
inherently ambiguous relations has also been
pointed out by Versley (2011). In fact, a con-
nective like meanwhile can be thought of as
delivering information not only about the pos-
sible relation senses it can express, but also
about the fact that two discourse relations
hold simultaneously.
In conclusion, it is possible that more than
one discourse relation hold between two text
spans. We believe that taking into account
the different types of ambiguity in discourse
relations can also benefit automatic discourse
relation classification methods, that so far ig-
nore multiple relation senses. Relations with
two senses mostly include one temporal sense.
This also (at least partially) explains the low
level of markedness of temporal relations in
Figure 2. Of particular interest is also the find-
ing that there seem to be specific connectors
such as meanwhile which are used to mark in-
herently ambiguous relations.
6 Type of Information Conveyed
by a Discourse Connector
In this experiment, we focus on the differ-
ences among individual connectives in reflect-
ing information about discourse relations from
coarse to fine grained granularity.
6.1 Measure of Information Gain
The mutual information between two discrete
variables which is indicative of the amount of
uncertainty that one removes for inference of
88
the other, can be decomposed in the following
manner:
I(X;Y ) =
?
c
p(c)
?
r
p(r|c) log
p(r|c)
p(r)
The inner sum is known as Kullback-Leibler di-
vergence or relative entropy of the distribution
of relations p(r) independent of the connector
c and the distribution of relations p(r|c) af-
ter observing c2. The relative entropy thus
quantifies in how far knowing the connector c
changes the distribution of relations.
gain(c) = DKL(p(r|c)||p(r))
This formulation also allows us to calculate
the change in distribution for different levels of
the PDTB relation sense hierarchy and thus to
analyse which connectors convey information
about which level of the hierarchy. We define
the measure of enhancement to formalize this
notion:
enhancementxy(c) = gainy(c)? gainx(c)
The enhancementxy(c) indicates the amount
of information delivered by cue c for the
classification of the instances into finer-
grained relation subtypes. For exam-
ple, enhancement01(because) describes how
much information gain because provides
for distinguishing the level-1 relations it
marks from other relations. Similarly,
high enhancement23(because) indicates that
this connective is important for distinguish-
ing among level 3 relations (here, distin-
guishing CONTINGENCY.Cause.reason from
CONTINGENCY.Cause.result relations), while
low enhancement23(if) indicates that if does
not contribute almost any information for
distinguishing among the subtypes of the
CONTINGENCY.Condition relation.
2Note that this formulation is closely related to sur-
prisal: Levy (2008) shows that surprisal(wk+1) =
? logP (wk+1|w1..wk) is equivalent to the KL diver-
gence D(P (T |w1..j+1)||P (T |w1..j)) for ?any stochas-
tic generative process P , conditioned on some (pos-
sibly null) external context, that generates complete
structures T , each consisting at least partly of sur-
face strings to be identified with serial linguistic in-
put?. Note however that in our current formula-
tion of a discourse relation, the simplification to gen-
eral structure-independent surprisal does not hold
(DKL(p(r|c)||p(r)) 6= ? log p(c)) because our relations
(as they are defined here) do not satisfy the above con-
dition for T , in particular, P (r, c) 6= P (r).
6.2 Connective Help in Hierarchical
Classification
Figure 3 shows the amount of enhancement
for 27 frequent (> 100 occurrences) connec-
tives in the corpus in three transitions, namely
from no information to the first level classifi-
cation, from first to the second level and from
second to the third. Most of the connectives
contribute most strongly at the coarsest level
of classification, i.e., their L1-Root enhance-
ment is the highest. In particular, we find that
some of the most frequent connectives such as
but, and, and also only help distinguishing dis-
course relation meaning at the coarsest level of
the PDTB relation hierarchy, but contribute
little to distinguish among e.g. different sub-
types of COMPARISON or EXPANSION. An inter-
esting observation is also that frequent mark-
ers of comparison relations but, though, still
and however provide almost no information
about the second or third level of the hierar-
chy.
Another group of connectors, for example,
instead, indeed and or contribute significantly
more information in transition from the first
to the second level. These are specific markers
of some level-2 relation senses. Among these,
instead and or even help more for the deepest
classification3.
Temporal and causal connectives such as be-
fore, after, so, then ,when and thus have more
contribution to the deepest classification level.
This reflects the distinctions employed in the
definition of the third level senses which has
a direct correlation with the temporal order-
ing, i.e., forward vs. backward transition be-
tween the involved sentences. In other words,
regardless of whatever high-level class of rela-
tion such markers fit in, the temporal infor-
mation they hold make them beneficial for the
3rd level classification.
There are also a few connectives (if, indeed,
for example) that convey a lot of information
about the distinctions made at the first and
second level of the hierarchy, but not about the
third level. The reason for this is either that
the third level distinction can only be made
based on the propositional information in the
3Markers of EXPANSION.Alternative.conjunction
and EXPANSION.Alternative.chosen alternative re-
spectively.
89
0	 ?
0.5	 ?
1	 ?
1.5	 ?
2	 ?
2.5	 ?
3	 ? enhancement	 ?0-??>1	 ?
enhancement	 ?1-??>2	 ?
enhancement	 ?2-??>3	 ?
4.4	 ? 4.0	 ? 3.5	 ?
Figure 3: Enhancement through three levels of relation sense classification obtained by 27 most
frequent connectives in the PDTB ? ordered left to right by frequency.
arguments (this is the case for the sub-types
of conditionals), or that the connector usually
marks a relation which does not have a third
level (e.g., for example is a good marker of
the EXPANSION.Instantiation relation which
does not have any subtypes).
It is worth noting that a sum over enhance-
ments obtained in the three levels results in the
total relative entropy the distribution of dis-
course relations before vs. after encountering
the connective. As expected, ambiguous con-
nectors of the first type of ambiguity (while,
since, when) convey a little bit of information
at each level of distinction, while overall in-
formation gain is relatively small. Ambigu-
ous connectors of the second type of ambigu-
ity (e.g., but, and, if ) convey almost no infor-
mation about specific sub-types of relations.
Finally, markers of inherently ambiguous rela-
tions (meanwhile) stand out for very low in-
formation gain at all levels.
6.3 Discussion
The notion of the information conveyed by a
discourse connector about a discourse relation
can also help to explain two previous find-
ings on the relative facilitative effect of causal
and adversative connectors, that at first glance
seem contradictory.
While Murray (1997) showed a generally
more salient effect for a group of adversative
cues such as however, yet, nevertheless and
but compared with causal connectives there-
fore, so, thus and consequently, others reported
different patterns when particular pairs of con-
nectives were compared: Caron et al (1988)
found greater inference activity and recall ac-
curacy for because sentences than sentences
connected with but. Also, Millis and Just
(1994) found a faster reading time and bet-
ter response to the comprehension questions
in the case of because than that of although
sentences. Interestingly, by looking at Figure
3, we find that because is a more constrain-
ing connective than but and even although,
given that the information gain obtain by this
connective in all levels of relation classifica-
tion is greater than that of but and although.
While adversative connectives are reliable sig-
nals to distinguish comparison relations in a
high-level from the other three major types of
relations, most causal connectives deliver spe-
cific information down to the finer grains. In
particular, because is a distinguished marker of
the reason relation; hence, it should be associ-
ated with a more constraining discourse effect,
while a generally used connective such as but
can serve as the marker of a variety of adver-
sative relations, e.g., a simple Contrast vs. a
Concession relation.
The information-theoretic view can also ac-
count for the larger facilitating effect of highly
constraining causal and adversative connec-
tives on discourse comprehension compared
to additive connectives such as and, also and
moreover (Murray, 1995, 1997; Ben-Anath,
2006). We also can see from the Figure 3 that
the mentioned additive connectives show a rel-
atively lower sum of enhancement.
In summary, the broad classification of a dis-
course connector (Murray, 1997; Halliday and
Hasan, 1976) is not the only factor that deter-
mines how constraining it is, or how difficult it
will be to process. Instead, one should look at
its usage in different context (i.e., specificity
of the connective usage in the natural text).
For example, based on the measurements pre-
sented in the Figure 3 we would expect a rel-
atively high constraining effect of the connec-
tives such as for example and instead. Note
90
however that these predictions strongly de-
pend on the discourse relation sense inventory
and the discourse relation hierarchy. In partic-
ular, it is important to ask in how far compu-
tational linguistics resources, like the PDTB,
reflect the inference processes in humans ? in
how far are the sense distinction and hierar-
chical classification cognitively adequate?
7 Discussion and Conclusion
Discourse Relation Hierarchy and Fea-
ture Space Dimensions Psycholingusitic
models that need to be trained on annotated
data from computational linguistics resources
also have to be concerned about the psycholin-
guistic adequacy of the annotation. In par-
ticular, for a model of discourse relation sur-
prisal, we need to ask which discourse relations
are relevant to humans, and which distinctions
between relations are relevant to them? For
example, it may be possible that the distinc-
tion between cause and consequence (3rd level
PDTB hierarchy) is more important in the in-
ference process than the distinction between
conjunction and list (2nd level PDTB hierar-
chy). Given the fact that more than one dis-
course relation (or none) can hold between two
text segments, one should also ask whether a
hierarchy is the right way to think about the
discourse relation senses at all ? it might be
more adequate to think about discourse con-
nectives conveying information about tempo-
rality, causality, contrast etc, with each con-
nector possibly conveying information about
more than one of these aspects at the same
time.
These questions are also relevant for auto-
matic discourse relation identification: many
approaches to discourse relation identification
have simplified the task to only distinguish
between e.g. the level-1 sense distinctions, or
level-2 distinctions (Versley, 2011; Lin et al,
2011; Hernault et al, 2011; Park and Cardie,
2012), but may be missing to differentiate as-
pects that are important also for many text
interpretation tasks, such as distinguishing be-
tween causes and consequences.
Towards discourse relation surprisal A
computational model of discourse relation sur-
prisal would have to take the actual local con-
text into account, i.e. factors other than just
the connective, and model the interplay of dif-
ferent factors in the arguments of the discourse
relation. We would then be in a position to
argue about the predictability of a specific in-
stance of a discourse relation, as opposed to
arguing based on general cognitive biases such
as the causality-by-default or continuity hy-
potheses.
From the three studies in this paper, we note
that our findings so far are compatible with
a surprisal account at the discourse relation
level: The first study showed that discourse
relations that seem to cause a larger context
update are marked by less ambiguous connec-
tives than relations for which less information
needs to be conveyed in order to be inferred.
This is in line with the UID and the conti-
nuity and causality-by-default hypotheses put
forth by Murray (1997) and Sanders (2005).
The second study then went on to show that
one can distinguish several types of ambiguity
among discourse relations, in particular, more
than one relation can hold between two propo-
sitions, and there are some connectives which
express this inherent ambiguity. In the third
study, we also showed that the effect of par-
ticular discourse markers varies with respect
to their contribution in different levels of re-
lation classification. Some connectives such as
the majority of the adversative ones, simply
help to distinguish contrastive relations from
other classes, while those with a temporal di-
rectionality contribute most in the deeper level
of the PDTB hierarchical classification. The
enhancement measure introduced in this pa-
per can be employed for measuring the effect
of any discriminative feature through the hi-
erarchical classification of the relations. This
work is a first step towards the computational
modeling of the discourse processing with re-
spect to the linguistic markers of the abstract
discourse relations. In future work, we would
like to look at the contribution of different
types of relational markers including sentence
connectives, sentiment words, implicit causal-
ity verbs, negation markers, event modals etc.,
which in the laboratory setup have proven to
affect the expectation of the readers about
an upcoming discourse relation (Kehler et al,
2008; Webber, 2013).
91
References
Asher, N. and Lascarides, A. (1998). Bridging.
Journal of Semantics, 15(1):83?113.
Asr, F. T. and Demberg, V. (2012a). Implic-
itness of discourse relations. In Proceedings
of COLING, Mumbai, India.
Asr, F. T. and Demberg, V. (2012b). Mea-
suring the strength of the discourse cues.
In workshop on the Advances in Discourse
Analysis and its Computational Aspects,
Mumbai, India.
Ben-Anath, D. (2006). The role of connec-
tives in text comprehension. Teachers Col-
lege, Columbia University Working Papers
in TESOL & Applied Linguistics, 5(2).
Blakemore, D. (1992). Understanding ut-
terances: An introduction to pragmatics.
Blackwell Oxford.
Blass, R. (1993). Are there logical relations in
a text? Lingua, 90(1-2):91?110.
Caron, J., Micko, H. C., and Thuring, M.
(1988). Conjunctions and the recall of com-
posite sentences. Journal of Memory and
Language, 27(3):309?323.
Degand, L. and Sanders, T. (2002). The im-
pact of relational markers on expository text
comprehension in l1 and l2. Reading and
Writing, 15(7):739?757.
Demberg, V. and Keller, F. (2008). Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cogni-
tion, 109(2):193?210.
Frank, A. and Jaeger, T. (2008). Speaking ra-
tionally: Uniform information density as an
optimal strategy for language production.
Proceedings of the 28th meeting of the Cog-
nitive Science Society.
Frank, S. (2009). Surprisal-based compari-
son between a symbolic and a connectionist
model of sentence processing. In Proceedings
of the 31st annual conference of the cogni-
tive science society, pages 1139?1144.
Hale, J. (2001). A probabilistic earley parser
as a psycholinguistic model. In Second meet-
ing of the North American Chapter of the
Association for Computational Linguistics
on Language technologies 2001, pages 1?8.
Halliday, M. and Hasan, R. (1976). Cohesion
in English. Longman (London).
Hernault, H., Bollegala, D., and Ishizuka, M.
(2011). Semi-supervised discourse relation
classification with structural learning. Com-
putational Linguistics and Intelligent Text
Processing, pages 340?352.
Hobbs, J. R. (1979). Coherence and corefer-
ence. Cognitive science, 3(1):67?90.
Kehler, A., Kertz, L., Rohde, H., and Elman,
J. L. (2008). Coherence and coreference re-
visited. Journal of Semantics, 25(1):1?44.
Ko?hne, J. and Demberg, V. (2013). The time-
course of processing discourse connectives.
In Proceedings of the 35th Annual Meeting
of the Cognitive Science Society.
Kuperberg, G., Paczynski, M., and Ditman,
T. (2011). Establishing causal coherence
across sentences: An ERP study. Journal of
Cognitive Neuroscience, 23(5):1230?1246.
Levinson, S. (2000). Presumptive Meanings:
The Theory of Generalized Conversational
Implicature. The MIT Press.
Levy, R. (2008). Expectation-based syntac-
tic comprehension. Cognition, 106(3):1126?
1177.
Levy, R. and Jaeger, T. F. (2007). Speakers
optimize information density through syn-
tactic reduction. In Advances in Neural In-
formation Processing Systems.
Lin, Z., Ng, H., and Kan, M. (2011). Automat-
ically evaluating text coherence using dis-
course relations. In Proceedings of the 49th
Annual Meeting of the Association for Com-
putational Linguistics: Human Language
Technologies-Volume 1, pages 997?1006.
Millis, K. and Just, M. (1994). The influence
of connectives on sentence comprehension.
Journal of Memory and Language.
Mitchell, J., Lapata, M., Demberg, V., and
Keller, F. (2010). Syntactic and seman-
tic factors in processing difficulty: An inte-
grated measure. In Proceedings of the 48th
Annual Meeting of the Association for Com-
putational Linguistics, pages 196?206.
Murray, J. (1995). Logical connectives and
local coherence. Sources of Coherence in
Reading, pages 107?125.
92
Murray, J. (1997). Connectives and narrative
text: The role of continuity. Memory and
Cognition, 25(2):227?236.
Park, J. and Cardie, C. (2012). Improving im-
plicit discourse relation recognition through
feature set optimization. In Proceedings of
the 13th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue,
pages 108?112. Association for Computa-
tional Linguistics.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki,
E., Robaldo, L., Joshi, A., and Webber, B.
(2008). The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Con-
ference on Language Resources and Evalua-
tion, pages 2961?2968.
Roark, B., Bachrach, A., Cardenas, C., and
Pallier, C. (2009). Deriving lexical and syn-
tactic expectation-based measures for psy-
cholinguistic modeling via incremental top-
down parsing. In Proceedings of the 2009
Conference on Empirical Methods in Nat-
ural Language Processing, pages 324?333,
Singapore. Association for Computational
Linguistics.
Sanders, T. (2005). Coherence, causality and
cognitive complexity in discourse. In Pro-
ceedings/Actes SEM-05, First International
Symposium on the Exploration and Mod-
elling of Meaning, pages 105?114.
Segal, E., Duchan, J., and Scott, P. (1991).
The role of interclausal connectives in nar-
rative structuring: Evidence from adults? in-
terpretations of simple stories. Discourse
Processes, 14(1):27?54.
Versley, Y. (2011). Towards finer-grained
tagging of discourse connectives. In Pro-
ceedings of the Workshop Beyound Seman-
tics: Corpus-based Investigations of Prag-
matic and Discourse Phenomena.
Webber, B. (2013). What excludes an alterna-
tive in coherence relations? In Proceedings
of the IWCS.
93
