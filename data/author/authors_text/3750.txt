Information Extraction for Question Answering:
Improving Recall Through Syntactic Patterns
Valentin Jijkoun and Maarten de Rijke
Informatics Institute
University of Amsterdam
{jijkoun,mdr}@science.uva.nl
Jori Mur
Information Science
University of Groningen
mur@let.rug.nl
Abstract
We investigate the impact of the precision/recall
trade-off of information extraction on the per-
formance of an offline corpus-based question
answering (QA) system. One of our findings is
that, because of the robust final answer selection
mechanism of the QA system, recall is more im-
portant. We show that the recall of the extrac-
tion component can be improved using syntac-
tic parsing instead of more common surface text
patterns, substantially increasing the number of
factoid questions answered by the QA system.
1 Introduction
Current retrieval systems allow us to locate docu-
ments that might contain the pertinent information,
but most of them leave it to the user to extract the
useful information from a ranked list of documents.
Hence, the (often unwilling) user is left with a rel-
atively large amount of text to consume. There is
a need for tools that reduce the amount of text one
might have to read to obtain the desired informa-
tion. Corpus-based question answering is designed
to take a step closer to information retrieval rather
than document retrieval. The question answering
(QA) task is to find, in a large collection of data,
an answer to a question posed in natural language.
One particular QA strategy that has proved suc-
cessful on large collections uses surface patterns de-
rived from the question to identify answers. For ex-
ample, for questions like When was Gandhi born?,
typical phrases containing the answer are Gandhi
was born in 1869 and Gandhi (1869?1948). These
examples suggest that text patterns such as ?name
was born in birth date? and ?name (birth
year?death year)? formulated as regular ex-
pressions, can be used to select the answer phrase.
Similarly, such lexical or lexico-syntactic pat-
terns can be used to extract specific information on
semantic relations from a corpus offline, before ac-
tual questions are known, and store it in a repository
for quick and easy access. This strategy allows one
to handle some frequent question types: Who is. . . ,
Where is. . . , What is the capital of. . . etc. (Fleis-
chman et al, 2003; Jijkoun et al, 2003).
A great deal of work has addressed the problem
of extracting semantic relations from unstructured
text. Building on this, much recent work in QA
has focused on systems that extract answers from
large bodies of text using simple lexico-syntactic
patterns. These studies indicate two distinct prob-
lems associated with using patterns to extract se-
mantic information from text. First, the patterns
yield only a small subset of the information that may
be present in a text (the recall problem). Second, a
fraction of the information that the patterns yield is
unreliable (the precision problem). The precision of
the extracted information can be improved signif-
icantly by using machine learning methods to filter
out noise (Fleischman et al, 2003). The recall prob-
lem is usually addressed by increasing the amount
of text data for extraction (taking larger collections
(Fleischman et al, 2003)) or by developing more
surface patterns (Soubbotin and Soubbotin, 2002).
Some previous studies indicate that in the setting
of an end-to-end state-of-the-art QA system, with
additional answer finding strategies, sanity check-
ing, and statistical candidate answer re-ranking, re-
call is more of a problem than precision (Bernardi et
al., 2003; Jijkoun et al, 2003): it often seems use-
ful to have more data rather than better data. The
aim of this paper is to address the recall problem
by using extraction methods that are linguistically
more sophisticated than surface pattern matching.
Specifically, we use dependency parsing to extract
syntactic relations between entities in a text, which
are not necessarily adjacent on the surface level. A
small set of hand-built syntactic patterns allows us
to detect relevant semantic information. A com-
parison of the parsing-based approach to a surface-
pattern-based method on a set of TREC questions
about persons shows a substantial improvement in
the amount of the extracted information and num-
ber of correctly answered questions.
In our experiments we tried to understand
whether linguistically involved methods such as
parsing can be beneficial for information extraction,
where rather shallow techniques are traditionally
employed, and whether the abstraction from surface
to syntactic structure of the text does indeed help to
find more information, at the same time avoiding the
time-consuming manual development of increasing
numbers of surface patterns.
The remainder of the paper is organized as fol-
lows. In Section 2 we discuss related work on
extracting semantic information. We describe our
main research questions and experimental setting in
Section 3. Then, in Section 4 we provide details
on the extraction methods used (surface and syntac-
tic). Sections 5 and 6 contain a description of our
experiments and results, and an error analysis, re-
spectively. We conclude in Section 7.
2 Related Work
There is a large body of work on extracting seman-
tic information using lexical patterns. Hearst (1992)
explored the use of lexical patterns for extracting
hyponym relations, with patterns such as ?such as.?
Berland and Charniak (1999) extract ?part-of? rela-
tions. Mann (2002) describes a method for extract-
ing instances from text by means of part-of-speech
patterns involving proper nouns.
The use of lexical patterns to identify answers in
corpus-based QA received lots of attention after a
team taking part in one of the earlier QA Tracks
at TREC showed that the approach was competi-
tive at that stage (Soubbotin and Soubbotin, 2002;
Ravichandran and Hovy, 2002). Different aspects of
pattern-based methods have been investigated since.
E.g., Ravichandran et al (2003) collect surface pat-
terns automatically in an unsupervised fashion us-
ing a collection of trivia question and answer pairs
as seeds. These patterns are then used to generate
and assess answer candidates for a statistical QA
system. Fleischman et al (2003) focus on the preci-
sion of the information extracted using simple part-
of-speech patterns. They describe a machine learn-
ing method for removing noise in the collected data
and showed that the QA system based on this ap-
proach outperforms an earlier state-of-the-art sys-
tem. Similarly, Bernardi et al (2003) combine the
extraction of surface text patterns with WordNet-
based filtering of name-apposition pairs to increase
precision, but found that it hurt recall more than it
helped precision, resulting in fewer questions an-
swered correctly when the extracted information is
deployed for QA.
The application of deeper NLP methods has also
received much attention in the QA community. The
open-domain QA system by LCC (Moldovan et al,
2002) uses predicate-argument relations and lexical
chaining to actually prove that a text snippet pro-
vides an answer to a question. Katz and Lin (2003)
use syntactic dependency parsing to extract rela-
tions between words, and use these relations rather
than individual words to retrieve sentences relevant
to a question. They report a substantial improve-
ment for certain types of questions for which the
usual term-based retrieval performs quite poorly,
but argue that deeper text analysis methods should
be applied with care.
3 Experimental Setting
We set up experiments to address two related issues.
First, we wanted to understand how the usual pre-
cision/recall trade-off shows up in off-line corpus-
based QA, and specifically, whether extracting more
data of lower quality (i.e., favoring recall) gives
a QA system a better performance than extracting
smaller amounts of more accurate data (i.e., favor-
ing precision). Second, we tried to verify the hy-
pothesis that syntactic parsing for information ex-
traction does increase the extraction recall by iden-
tifying relations between entities not adjacent on the
surface layer but connected syntactically.
There are different approaches to the evaluation
of information extraction modules. The usual recall
and precision metrics (e.g., how many of the inter-
esting bits of information were detected, and how
many of the found bits were actually correct) require
either a test corpus previously annotated with the
required information, or manual evaluation (Fleis-
chman et al, 2003). Although intrinsic evaluation
of an IE module is important, we were mainly inter-
ested in measuring the performance of this module
in context, that is, working as a sub-part of a QA
system. We used the number of questions answered
correctly as our main performance indicator.
3.1 QA System
For the experiments described below we used an
open-domain corpus-based QA system QUARTZ
(Jijkoun et al, 2004). The system implements
a multi-stream approach, where several different
strategies are used in parallel to find possible an-
swers to a question. We ran the system turning on
only one stream, Table Lookup, which implements
an off-line strategy for QA.
The Table Lookup stream uses a number of
knowledge bases created by pre-processing a doc-
ument collection. Currently, QUARTZ? knowledge
bases include 14 semi-structured tables containing
various kinds of information: birth dates of persons,
dates of events, geographical locations of different
objects, capitals and currencies of countries, etc. All
this information is extracted from the corpus off-
line, before actual questions are known.
An incoming question is analyzed and assigned
to one of 37 predefined question types. Based on
the question type, the Table Lookup stream identi-
fies knowledge bases where answers to the question
can potentially be found. The stream uses keywords
from the question to identify relevant entries in the
selected knowledge bases and extracts candidate an-
swers. Finally, the QA system reranks and sanity
checks the candidates and selects the final answer.
3.2 Questions and Corpus
To get a clear picture of the impact of using dif-
ferent information extraction methods for the off-
line construction of knowledge bases, similarly to
(Fleischman et al, 2003), we focused only on
questions about persons, taken from the TREC-
8 through TREC 2003 question sets. The ques-
tions we looked at were of two different types:
person identification (e.g., 2301. What composer
wrote ?Die Go?tterda?mmerung??) and person defi-
nition (e.g., 959. Who was Abraham Lincoln?). The
knowledge base relevant for answering questions of
these types is a table with several fields containing
a person name, an information bit about the per-
son (e.g., occupation, position, activities), the con-
fidence value assigned by the extraction modules
to this information bit (based on its frequency and
the reliability of the patterns used for extraction),
and the source document identification. The Table
Lookup finds the entries whose relevant fields best
match the keywords from the question.
We performed our experiments with the 336
TREC questions about persons that are known to
have at least one answer in the collection. The
collection used at TREC 8, 9 and 10 (referred to
as TREC-8 in the rest of the paper) consists of
1,727,783 documents, with 239 of the correspond-
ing questions identified by our system as asking
about persons. The collection used at TREC 2002
and 2003 (AQUAINT) contains 1,033,461 docu-
ments and 97 of the questions for these editions of
TREC are person questions.
4 Extraction of Role Information
In this section we describe the two extraction meth-
ods we used to create knowledge bases containing
information about persons: extraction using surface
text patterns and using syntactic patterns.
Clearly, the performance of an information ex-
traction module depends on the set of language phe-
nomena or patterns covered, but this relation is not
straightforward: having more patterns allows one to
find more information, and thus increases recall, but
it might introduce additional noise that hurts preci-
sion. Since in our experiments we aimed at com-
paring extraction modules based on surface text vs.
syntactic patterns, we tried to keep these two mod-
ules parallel in terms of the phenomena covered.
First, the collections were tagged with a Named
Entity tagger based on TnT (TnT, 2003) and trained
on CoNLL data (CoNLL, 2003). The Named Entity
tagger was used mainly to identify person names as
separate entities. Although the tagging itself was
not perfect, we found it useful for restricting our
surface text patterns.
Below we describe the two extraction methods.
4.1 Extraction with Surface Text Patterns
To extract information about roles, we used the set
of surface patterns originally developed for the QA
system we used at TREC 2003 (Jijkoun et al, 2004).
The patterns are listed in Table 1.
In these patterns, person is a phrase that is
tagged as person by the Named Entity tagger, role
is a word from a list of roles extracted from the
WordNet (all hyponyms of the word ?person,? 15703
entries),1 role-verb is from a manually con-
structed list of ?important? verbs (discovered, in-
vented, etc.; 48 entries), leader is a phrase identify-
ing leadership from a manually created list of lead-
ers (president, minister, etc.; 22 entries). Finally,
superlat is the superlative form of an adjective
and location is a phrase tagged as location by
the Named Entity tagger.
4.2 Extraction with Syntactic Patterns
To use the syntactic structure of sentences for role
information extraction, the collections were parsed
with Minipar (Lin, 1998), a broad coverage depen-
dency parser for English. Minipar is reported to
achieve 88% precision and 80% recall with respect
to dependency relations when evaluated on the SU-
SANNE corpus. We found that it performed well
on the newpaper and newswire texts of our collec-
tions and was fairly robust to fragmented and not
well-formed sentences frequent in this domain. Be-
fore extraction, Minipar?s output was cleaned and
made more compact. For example, we removed
some empty nodes in the dependency parse to re-
solve non-local dependencies. While not loosing
any important information, this made parses easier
to analyse when developing patterns for extraction.
Table 2 lists the patterns that were used to ex-
tract information about persons; we show syntactic
dependencies as arrows from dependents to heads,
with Minipar?s dependency labels above the arrows.
As with the earlier surface patterns, role is one
of the nouns in the list of roles (hyponyms of person
1The list of roles is used to increase precision by filtering
out snippets that may not be about roles; in some of the experi-
ments below, we turn this filtering mechanism off.
Pattern Example
... role, person The British actress, Emma Thompson
... (superlat|first|last)..., person The first man to set foot on the moon, Armstrong
person,... role... Audrey Hepburn, goodwill ambassador for UNICEF.
person,... (superlat|first|last)... Brown, Democrats? first black chairman.
person,... role-verb... Christopher Columbus, who discovered America,
... role person District Attoney Gil Garcetti
role... person The captain of the Titanic Edward John Smith
person,... leader... location Tony Blair, the prime minister of England
location... leader, person The British foreign secretary , Jack Straw
Table 1: Surface patterns.
Pattern Example
Apposition person appo????role a major developer, Joseph Beard
Apposition person appo????role Jerry Lewis, a Republican congressman
Clause person subj????role-verb Bell invented the telephone
Person person person????role Vice President Al Gore
Nominal modifier person nn????role businessman Bill Shockley
Subject person subj????role Alvarado was chancellor from 1983 to 1984
Conjunction person conj????role Fu Wanzhong, director of the Provincial Department of Foreign Trade
(this is a frequent parsing error)
Table 2: Syntactic patterns.
in WordNet), role-verb is one of the ?important
verbs.? The only restriction for person was that it
should contain a proper noun.
When an occurence of a pattern was found in
a parsed sentence, the relation (person; info-
bit) was extracted, where info-bit is a se-
quence of all words below role or role-verb
in the dependency graph (i.e., all dependents along
with their dependents etc.), excluding the per-
son. For example, for the sentence Jane Goodall,
an expert on chimps, says that evidence for so-
phisticated mental performances by apes has be-
come ever more convincing, that matches the pat-
tern person appo????role, the extracted informa-
tion was (Jane Goodall; an expert on chimps).
5 Experiments and Results
We ran both surface pattern and syntactic pattern
extraction modules on the two collections, with a
switch for role filtering. The performance of the Ta-
ble Lookup stream of our QA system was then eval-
uated on the 336 role questions using the answer
patterns provided by the TREC organizers. An early
error analysis showed that many of the incorrect
answers were due to the table lookup process (see
Section 3) rather than the information extraction
method itself: correct answers were in the tables,
but the lookup mechanism failed to find them or
picked up other, irrelevant bits of information. Since
we were interested in evaluating the two extraction
methods rather than the lookup mechanism, we per-
formed another experiment: we reduced the sizes
of the collections to simplify the automatic lookup.
For each TREC question with an answer in the col-
lection, NIST provides a list of documents that are
known to contain an answer to this question. We put
together the document lists for all questions, which
left us with much smaller sub-collections (16.4 MB
for the questions for the TREC-8 collection and 3.2
MB for the AQUAINT collection). Then, we ran the
two extraction modules on these small collections
and evaluated the performance of the QA system on
the resulting tables. All the results reported below
were obtained with these sub-collections. Compari-
son of the extraction modules on the full TREC col-
lections gave very similar relative results.
Table 3 gives the results of the different runs for
the syntactic pattern extraction and the surface pat-
tern extraction on the TREC-8 collection: the num-
ber of correct answers (in the top one and the top
three answer candidates) for the 239 person ques-
tions. The columns labeled Roles+ show the results
for the extraction modules using the list of possible
roles from WordNet (Section 4), and the columns la-
beled Roles ? show the results when the extraction
modules consider any word as possibly denoting a
role. The results of the runs on the AQUAINT col-
lection with 97 questions are shown in Table 4.
The syntactic pattern module without role filter-
ing scored best of all, with more than a third of the
Syntactic patterns Surface patterns
Rank Roles ? Roles + Roles ? Roles +
1 80 (34%) 73 (31%) 59 (25%) 54 (23%)
1?3 90 (38%) 79 (33%) 68 (29%) 59 (25%)
Table 3: Correct answers for the TREC-8 collection
(239 questions).
Syntactic patterns Surface patterns
Rank Roles ? Roles + Roles ? Roles +
1 16 (17%) 14 (14%) 9 (9%) 6 (6%)
1?3 20 (21%) 14 (14%) 11 (11%) 6 (6%)
Table 4: Correct answers for the AQUAINT collec-
tion (97 questions).
questions answered correctly for the TREC-8 col-
lection. Another interesting observation is that in all
experiments the modules based on syntactic patterns
outperformed the surface-text-based extraction.
Furthermore, there is a striking difference be-
tween the results in Table 3 (questions from
TREC 8, 9 and 10) and the results in Table 4
(questions from TREC 2002 and 2003). The ques-
tions from the more recent editions of TREC are
known to be much harder: indeed, the Table Lookup
stream answers only 21% of the questions from
TREC 2002 and 2003, vs. 38% for earlier TRECs.
In all experiments, both for syntactic and surface
patterns, using the list of roles as a filtering mecha-
nism decreases the number of correct answers. Us-
ing lexical information from WordNet improves the
precision of the extraction modules less than it hurts
the recall. Moreover, in the context of our knowl-
edge base lookup mechanism, low precision of the
extracted information does not seem to be an ob-
stacle: the irrelevant information that gets into the
tables is either never asked for or filtered out during
the final sanity check and answer selection stage.
This confirms the conclusions of (Bernardi et al,
2003): in this specific task having more data seems
to be more useful than having better data.
To illustrate the interplay between the precision
and recall of the extraction module and the perfor-
mance of the QA system, Table 5 gives the com-
parison of the different extraction mechanisms (syn-
tactic and surface patterns, using or not using the
list of roles for filtering). The row labelled # facts
shows the size of the created knowledge base, i.e.,
the number of entries of the form (person, info), ex-
tracted by each method. The row labelled Preci-
sion shows the precision of the extracted informa-
tion (i.e., how many entries are correct, according to
a human annotator) estimated by random sampling
and manual evaluation of 1% of the data for each ta-
ble, similar to (Fleischman et al, 2003). The row la-
belled Corr. answers gives the number of questions
correctly answered using the extracted information.
Syntactic patterns Surface patterns
Roles ? Roles + Roles ? Roles +
# facts 29890 9830 28803 6028
Precision 54% 61% 23% 68%
Corr. answers 34% 31% 25% 23%
Table 5: Comparison of the tables built with differ-
ent extraction methods on the TREC-8 collection.
The results in Table 5 indicate that role filtering af-
fects the syntactic and surfaces modules quite dif-
ferently. Filtering seems almost essential for the
surface-pattern-based extraction, as it increases the
precision from 23% to 68%. This confirms the re-
sults of Fleischman et al (2003): shallow methods
may benefit significantly from the post-processing.
On the other hand, the precision improvement for
the syntactic module is modest: from 54% to 61%.
The data from the syntactic module contains
much less noise, although the sizes of the extracted
tables before role filtering are almost the same. Af-
ter filtering, the number of valid entries from the
syntactic module (i.e., the table size multiplied by
the estimated precision) is about 6000. This is sub-
stantially better than the recall of the surface module
(about 4100 valid entries).
6 Error Analysis
In theory, all relatively simple facts extracted by the
surface pattern module should also be extracted by
the syntactic pattern module. Moreover, the syn-
tactic patterns should extract more facts, especially
ones whose structure deviates from the patterns pre-
defined in the surface pattern module, e.g., where
elements adjacent in the syntactic parse tree are far
apart on the surface level. To better understand the
differences between the two extraction approaches
and to verify the conjecture that syntactic parsing
does indeed increase the recall of the extracted in-
formation, we performed a further (manual) error
analysis, identifying questions that were answered
with one extraction method but not with the other.
Tables 6 and 7 gives the breakdown of the per-
formance of the two modules, again in terms of the
questions answered correctly. We show the results
for the 239 questions on the TREC-8 collection; for
the 97 questions on the AQUAINT corpus the rela-
tive scores are similar. As Tables 6 and 7 indicate,
not all questions answered by the surface pattern
module were also answered by the syntactic pattern
module, contrary to our expectations. We took a
closer look at the questions for which the two mod-
ules performed differently.
Syntactic patterns
Su
rfa
ce
pa
tte
rn
s correct incorrect
correct 47 12
incorrect 32 148
Table 6: Performance analysis for the TREC-8 col-
lection with role filtering.
Syntactic patterns
Su
rfa
ce
pa
tte
rn
s correct incorrect
correct 51 17
incorrect 39 132
Table 7: Performance analysis for the TREC-8 col-
lection without role filtering.
6.1 Syntactic Patterns vs. Surface Patterns
There were three types of errors responsible for pro-
ducing an incorrect answer by the syntactic pattern
module for questions correctly answered with sur-
face patterns. The most frequent errors were pars-
ing errors. For 6 out of 12 questions (see Table 6)
the answer was not extracted by the syntactic pat-
tern method, because the sentences containing the
answers were not parsed correctly. The next most
frequent error was caused by the table lookup pro-
cess. For 4 questions out of the 12, the required
information was extracted but simply not selected
from the table as the answer due to a failure of the
lookup algorithm. The remaining errors (2 out of
12) were of a different type: for these 2 cases the
surface pattern extraction did perform better than
the syntactic method. In both cases this was because
of wildcards allowed in the surface patterns. E.g.,
for the sentence . . . aviator Charles Lindbergh mar-
ried Anne Spencer Morrow. . . the syntactic pattern
method extracted only the relation
(Charles Lindbergh; aviator),
whereas the surface pattern method also extracted
(Anne Spencer Morrow; aviator Charles Lindbergh
married),
because of the pattern ?role. . . person? with
role instantiated with aviator and person with
Anne Spencer Morrow. In fact, the extracted in-
formation is not even correct, because Anne is not
an aviator but Lindbergh?s wife. However, due to
the fuzzy nature of the lookup mechanism, this new
entry in the knowledge base allows the QA sys-
tem to answer correctly the question 646. Who was
Charles Lindbergh?s wife?, which is not answered
with the syntactic pattern extraction module.
To summarize, of the 12 questions where the sur-
face patterns outperformed the syntactic patterns
? 6 questions were not answered by the syntactic
method due to parsing errors,
? 4 were not answered because of the table
lookup failure and
? for 2 the surface-based method was more ap-
propriate.
6.2 Surface Patterns vs. Syntactic Patterns
We also took a closer look at the 32 questions for
which the syntactic extraction performed better than
the surface patterns (see Table 6). For the sur-
face pattern extraction module there were also three
types of errors. First, some patterns were miss-
ing, e.g., person role-verb.... The only
difference from one of the actually used patterns
(person,... role-verb...) is that there
is no comma between person and role-verb.
This type of incompleteness of the set of the surface
patterns was the cause for 16 errors out of 32.
The second class of errors was caused by the
Named Entity tagger. E.g., Abraham Lincoln was
always tagged as location, so the name never
matched any of the surface patterns. Out of 32 ques-
tions, 10 were answered incorrectly for this reason.
Finally, for 6 questions out of 32, the syntactic
extraction performed better because the information
could not be captured on the surface level. For ex-
ample, the surface pattern module did not extract
the fact that Oswald killed Kennedy from the sen-
tence . . . when Lee Harvey Oswald allegedly shot
and killed President John F. Kennedy. . . , because
none of the patterns matched. Indeed, Lee Harvey
Oswald and the potentially interesting verb killed
are quite far apart in the text, but there is an imme-
diate relation (subject) on the syntactic level.
It is worth pointing out that there were no lookup
errors for the surface pattern method, even though
it used the exact same lookup mechanism as the
approach based on syntactic patterns (that did ex-
perience various lookup errors, as we have seen).
It seems that the increased recall of the syntactic
pattern approach caused problems by making the
lookup process harder.
To summarize, out of 32 questions answered us-
ing syntactic extraction method but not by the sur-
face pattern approach
? 16 questions would have required extending
the set of surface patterns,
? 10 questions were not answered because of NE
tagging error, and
? 6 questions required syntactic analysis for ex-
traction of the relevant information.
6.3 Adding Patterns?
We briefly return to a problem noted for extrac-
tion based on surface patterns: the absence of cer-
tain surface patterns. The surface pattern person
role-verb... was not added because, we felt,
it would introduce too much noise in the knowledge
base. With dependency parsing this is not an is-
sue as we can require that person is the subject
of role-verb. So in this case the syntactic pat-
tern module has a clear advantage. More generally,
while we believe that extraction methods based on
hand-crafted patterns are necessarily incomplete (in
that they will fail to extract certain relevant facts),
these observations suggest that coping with the in-
completeness is a more serious problem for the sur-
face patterns than for the syntactic ones.
7 Conclusions
We described a set of experiments aimed at com-
paring different information extraction methods in
the context of off-line corpus-based Question An-
swering. Our main finding is that a linguistically
deeper method, based on dependency parsing and a
small number of simple syntactic patterns, allows an
off-line QA system to correctly answer substantially
more questions than a traditional method based on
surface text patterns. Although the syntactic method
showed lower precision of the extracted facts (61%
vs. 68%), in spite of parsing errors the recall was
higher than that of the surface-based method, judg-
ing by the number of correctly answered questions
(31% vs. 23%). Thus, the syntactic analysis can in
fact be considered as another, intensive way of im-
proving the recall of information extraction, in ad-
dition to successfully used extensive ways, such as
developing larger numbers of surface patterns or in-
creasing the size of the collection.
Moreover, we confirmed the claim that for a com-
plex off-line QA system, with statistical as well as
knowledge-intensive sanity checking answer selec-
tion modules, recall of the information extraction
module is more important than precision, and a sim-
ple WordNet-based method for improving precision
does not help QA. In our future work we plan to in-
vestigate the effect of more sophisticated and, prob-
ably, more accurate filtering methods (Fleischman
et al, 2003) on the QA results.
8 Acknowledgements
Valentin Jijkoun and Maarten de Rijke were sup-
ported by a grant from the Netherlands Organiza-
tion for Scientific Research (NWO) under project
number 220-80-001. De Rijke was also sup-
ported by NWO under project numbers 365-20-
005, 612.069.006, 612.000.106, 612.000.207, and
612.066.302.
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of the 37th Annual
Meeting of the ACL.
R. Bernardi, V. Jijkoun, G. Mishne, and M. de Rijke.
2003. Selectively using linguistic resources through-
out the question answering pipeline. In Proceedings
of the 2nd CoLogNET-ElsNET Symposium.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: answering
questions before they are asked. In Proceedings of the
41st Annual Meeting of the ACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
International Conference on Computational Linguis-
tics (COLING-92).
V. Jijkoun, G. Mishne, and M. de Rijke. 2003. Prepro-
cessing Documents to Answer Dutch Questions. In
Proceedings of the 15th Belgian-Dutch Conference on
Artificial Intelligence (BNAIC?03).
V. Jijkoun, G. Mishne, C. Monz, M. de Rijke,
S. Schlobach, and O. Tsur. 2004. The University of
Amsterdam at the TREC 2003 Question Answering
Track. In Proceedings of the TREC-2003 Conference.
B. Katz and J. Lin. 2003. Selectively using relations to
improve precision in question answering. In Proceed-
ings of the EACL-2003 Workshop on Natural Lan-
guage Processing for Question Answering.
D. Lin. 1998. Dependency-based evaluation of Minipar.
In Proceedings of the Workshop on the Evaluation of
Parsing Systems.
G. Mann. 2002. Fine-grained proper noun ontologies
for question answering. In SemaNet?02: Building and
Using Semantic Networks.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu,
A. Novischi F. Lacatusu, A. Badulescu, and O. Bolo-
han. 2002. LCC tools for question answering. In Pro-
ceedings of the TREC-2002.
TnT Statistical Part of Speech Tagging. 2003.
URL: http://www.coli.uni-sb.de/
?thorsten/tnt/.
CoNLL: Conference on Natural Language Learn-
ing. 2003. URL: http://cnts.uia.ac.be/
signll/shared.html.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the ACL.
D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003.
Automatic derivation of surface text patterns for a
maximum entropy based question answering system.
In Proceedings of the HLT-NAACL Conference.
M.M. Soubbotin and S.M. Soubbotin. 2002. Use of pat-
terns for detection of likely answer strings: A system-
atic approach. In Proceedings of the TREC-2002 Con-
ference.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 398?405,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Generating a Non-English Subjectivity Lexicon:
Relations That Matter
Valentin Jijkoun and Katja Hofmann
ISLA, University of Amsterdam
Amsterdam, The Netherlands
{jijkoun,k.hofmann}@uva.nl
Abstract
We describe a method for creating a non-
English subjectivity lexicon based on an
English lexicon, an online translation ser-
vice and a general purpose thesaurus:
Wordnet. We use a PageRank-like algo-
rithm to bootstrap from the translation of
the English lexicon and rank the words
in the thesaurus by polarity using the net-
work of lexical relations in Wordnet. We
apply our method to the Dutch language.
The best results are achieved when using
synonymy and antonymy relations only,
and ranking positive and negative words
simultaneously. Our method achieves an
accuracy of 0.82 at the top 3,000 negative
words, and 0.62 at the top 3,000 positive
words.
1 Introduction
One of the key tasks in subjectivity analysis is
the automatic detection of subjective (as opposed
to objective, factual) statements in written doc-
uments (Mihalcea and Liu, 2006). This task is
essential for applications such as online market-
ing research, where companies want to know what
customers say about the companies, their prod-
ucts, specific products? features, and whether com-
ments made are positive or negative. Another
application is in political research, where pub-
lic opinion could be assessed by analyzing user-
generated online data (blogs, discussion forums,
etc.).
Most current methods for subjectivity identi-
fication rely on subjectivity lexicons, which list
words that are usually associated with positive or
negative sentiments or opinions (i.e., words with
polarity). Such a lexicon can be used, e.g., to clas-
sify individual sentences or phrases as subjective
or not, and as bearing positive or negative senti-
ments (Pang et al, 2002; Kim and Hovy, 2004;
Wilson et al, 2005a). For English, manually cre-
ated subjectivity lexicons have been available for
a while, but for many other languages such re-
sources are still missing.
We describe a language-independent method
for automatically bootstrapping a subjectivity lex-
icon, and apply and evaluate it for the Dutch lan-
guage. The method starts with an English lexi-
con of positive and negative words, automatically
translated into the target language (Dutch in our
case). A PageRank-like algorithm is applied to the
Dutch wordnet in order to filter and expand the set
of words obtained through translation. The Dutch
lexicon is then created from the resulting ranking
of the wordnet nodes. Our method has several ben-
efits:
? It is applicable to any language for which a
wordnet and an automatic translation service
or a machine-readable dictionary (from En-
glish) are available. For example, the Eu-
roWordnet project (Vossen, 1998), e.g., pro-
vides wordnets for 7 languages, and free on-
line translation services such as the one we
have used in this paper are available for many
other languages as well.
? The method ranks all (or almost all) entries of
a wordnet by polarity (positive or negative),
which makes it possible to experiment with
different settings of the precision/coverage
threshold in applications that use the lexicon.
We apply our method to the most recent version
of Cornetto (Vossen et al, 2007), an extension of
the Dutch WordNet, and we experiment with vari-
ous parameters of the algorithm, in order to arrive
at a good setting for porting the method to other
languages. Specifically, we evaluate the quality of
the resulting Dutch subjectivity lexicon using dif-
ferent subsets of wordnet relations and informa-
tion in the glosses (definitions). We also examine
398
the effect of the number of iterations on the per-
formance of our method. We find that best perfor-
mance is achieved when using only synonymy and
antonymy relations and, moreover, the algorithm
converges after about 10 iterations.
The remainder of the paper is organized as fol-
lows. We summarize related work in section 2,
present our method in section 3 and describe the
manual assessment of the lexicon in section 4. We
discuss experimental results in section 5 and con-
clude in section 6.
2 Related work
Creating subjectivity lexicons for languages other
than English has only recently attracted attention
of the research community. (Mihalcea et al, 2007)
describes experiments with subjectivity classifica-
tion for Romanian. The authors start with an En-
glish subjectivity lexicon with 6,856 entries, Opin-
ionFinder (Wiebe and Riloff, 2005), and automat-
ically translate it into Romanian using two bilin-
gual dictionaries, obtaining a Romanian lexicon
with 4,983 entries. A manual evaluation of a sam-
ple of 123 entries of this lexicon showed that 50%
of the entries do indicate subjectivity.
In (Banea et al, 2008) a different approach
based on boostrapping was explored for Roma-
nian. The method starts with a small seed set of
60 words, which is iteratively (1) expanded by
adding synonyms from an online Romanian dic-
tionary, and (2) filtered by removing words which
are not similar (at a preset threshold) to the orig-
inal seed, according to an LSA-based similarity
measure computed on a half-million word cor-
pus of Romanian. The lexicon obtained after 5
iterations of the method was used for sentence-
level sentiment classification, indicating an 18%
improvement over the lexicon of (Mihalcea et al,
2007).
Both these approaches produce unordered sets
of positive and negative words. Our method,
on the other hand, assigns polarity scores to
words and produces a ranking of words by polar-
ity, which provides a more flexible experimental
framework for applications that will use the lexi-
con.
Esuli and Sebastiani (Esuli and Sebastiani,
2007) apply an algorithm based on PageRank to
rank synsets in EnglishWordNet according to pos-
itive and negativite sentiments. The authors view
WordNet as a graph where nodes are synsets and
synsets are linked with the synsets of terms used
in their glosses (definitions). The algorithm is ini-
tialized with positivity/negativity scores provided
in SentiWordNet (Esuli and Sebastiani, 2006), an
English sentiment lexicon. The weights are then
distributed through the graph using an the algo-
rithm similar to PageRank. Authors conclude that
larger initial seed sets result in a better ranking
produced by the method. The algorithm is always
run twice, once for positivity scores, and once for
negativity scores; this is different in our approach,
which ranks words from negative to positive in
one run. See section 5.4 for a more detailed com-
parison between the existing approaches outlined
above and our approach.
3 Approach
Our approach extends the techniques used in
(Esuli and Sebastiani, 2007; Banea et al, 2008)
for mining English and Romanian subjectivity lex-
icons.
3.1 Boostrapping algorithm
We hypothesize that concepts (synsets) that are
closely related in a wordnet have similar meaning
and thus similar polarity. To determine relatedness
between concepts, we view a wordnet as a graph
of lexical relations between words and synsets:
? nodes correspond to lexical units (words) and
synsets; and
? directed arcs correspond to relations between
synsets (hyponymy, meronymy, etc.) and be-
tween synsets and words they contain; in one
of our experiments, following (Esuli and Se-
bastiani, 2007), we also include relations be-
tween synsets and all words that occur in their
glosses (definitions).
Nodes and arcs of such a graph are assigned
weights, which are then propagated through the
graph by iteratively applying a PageRank-like al-
gorithm.
Initially, weights are assigned to nodes and arcs
in the graph using translations from an English po-
larity lexicon as follows:
? words that are translations of the positive
words from the English lexicon are assigned
a weight of 1, words that are translations of
the negative words are initialized to -1; in
general, weight of a word indicates its polar-
ity;
399
? All arcs are assigned a weight of 1, except
for antonymy relations which are assigned
a weight of -1; the intuition behind the arc
weights is simple: arcs with weight 1 would
usually connect synsets of the same (or simi-
lar) polarity, while arcs with weight -1 would
connect synsets with opposite polarities.
We use the following notation. Our algorithm
is iterative and k = 0, 1, . . . denotes an iteration.
Let aki be the weight of the node i at the k-th iter-
ation. Let wjm be the weight of the arc that con-
nects node j with nodem; we assume the weight is
0 if the arc does not exist. Finally, ? is a damping
factor of the PageRank algorithm, set to 0.8. This
factor balances the impact of the initial weight of
a node with the impact of weight received through
connections to other nodes.
The algorithm proceeds by updating the weights
of nodes iteratively as follows:
ak+1i = ? ?
?
j
akj ? wji
?
m |wjm|
+ (1? ?) ? a0i
Furthermore, at each iterarion, all weights ak+1i
are normalized by maxj |a
k+1
j |.
The equation above is a straightforward exten-
sion of the PageRank method for the case when
arcs of the graph are weighted. Nodes propagate
their polarity mass to neighbours through outgoing
arcs. The mass transferred depends on the weight
of the arcs. Note that for arcs with negative weight
(in our case, antonymy relation), the polarity of
transferred mass is inverted: i.e., synsets with neg-
ative polarity will enforce positive polarity in their
antonyms.
We iterate the algorithm and read off the result-
ing weight of the word nodes. We assume words
with the lowest resulting weight to have negative
polarity, and word nodes with the highest weight
positive polarity. The output of the algorithm is a
list of words ordered by polarity score.
3.2 Resources used
We use an English subjectivity lexicon of Opinion-
Finder (Wilson et al, 2005b) as the starting point
of our method. The lexicon contains 2,718 English
words with positive polarity and 4,910 words with
negative polarity. We use a free online translation
service1 to translate positive and negative polar-
ity words into Dutch, resulting in 974 and 1,523
1http://translate.google.com
Dutch words, respectively. We assumed that a
word was translated into Dutch successfully if the
translation occurred in the Dutch wordnet (there-
fore, the result of the translation is smaller than the
original English lexicon).
The Dutch wordnet we used in our experiments
is the most recent version of Cornetto (Vossen et
al., 2007). This wordnet contains 103,734 lexical
units (words), 70,192 synsets, and 157,679 rela-
tions between synsets.
4 Manual assessments
To assess the quality of our method we re-used
assessments made for earlier work on comparing
two resources in terms of their usefulness for au-
tomatically generating subjectivity lexicons (Jij-
koun and Hofmann, 2008). In this setting, the
goal was to compare two versions of the Dutch
Wordnet: the first from 2001 and the other from
2008. We applied the method described in sec-
tion 3 to both resources and generated two subjec-
tivity rankings. From each ranking, we selected
the 2000 words ranked as most negative and the
1500 words ranked as most positive, respectively.
More negative than positive words were chosen to
reflect the original distribution of positive vs. neg-
ative words. In addition, we selected words for
assessment from the remaining parts of the ranked
lists, randomly sampling chunks of 3000 words at
intervals of 10000 words with a sampling rate of
10%. The selection was made in this way because
we were mostly interested in negative and positive
words, i.e., the words near either end of the rank-
ings.
4.1 Assessment procedure
Human annotators were presented with a list of
words in random order, for each word its part-of-
speech tag was indicated. Annotators were asked
to identify positive and negative words in this list,
i.e., words that indicate positive (negative) emo-
tions, evaluations, or positions.
Annotators were asked to classify each word on
the list into one of five classes:
++ the word is positive in most contexts (strongly
positive)
+ the word is positive in some contexts (weakly
positive)
0 the word is hardly ever positive or negative
(neutral)
400
? the a word is negative in some contexts
(weakly negative)
?? the word is negative in most contexts
(strongly negative)
Cases where assessors were unable to assign a
word to one of the classes, were separately marked
as such.
For the purpose of this study we were only inter-
ested in identifying subjective words without con-
sidering subjectivity strength. Furthermore, a pi-
lot study showed assessments of the strength of
subjectivity to be a much harder task (54% inter-
annotator agreement) than distinguishing between
positive, neutral and negative words only (72%
agreement). We therefore collapsed the classes of
strongly and weakly subjective words for evalua-
tion. These results for three classes are reported
and used in the remainder of this paper.
4.2 Annotators
The data were annotated by two undergraduate
university students, both native speakers of Dutch.
Annotators were recruited through a university
mailing list. Assessment took a total of 32 work-
ing hours (annotating at approximately 450-500
words per hour) which were distributed over a to-
tal of 8 annotation sessions.
4.3 Inter-annotator Agreement
In total, 9,089 unique words were assessed, of
which 6,680 words were assessed by both anno-
tators. For 205 words, one or both assessors could
not assign an appropriate class; these words were
excluded from the subsequent study, leaving us
with 6,475 words with double assessments.
Table 1 shows the number of assessed words
and inter-annotator agreement overall and per
part-of-speech. Overall agreement is 69% (Co-
hen?s ?=0.52). The highest agreement is for ad-
jectives, at 76% (?=0.62) . This is the same
level of agreement as reported in (Kim and Hovy,
2004) for English. Agreement is lowest for verbs
(55%, ?=0.29) and adverbs (56%, ?=0.18), which
is slightly less than the 62% agreement on verbs
reported by Kim and Hovy. Overall we judge
agreement to be reasonable.
Table 2 shows the confusion matrix between the
two assessors. We see that one assessor judged
more words as subjective overall, and that more
words are judged as negative than positive (this
POS Count % agreement ?
noun 3670 70% 0.51
adjective 1697 76% 0.62
adverb 25 56% 0.18
verb 1083 55% 0.29
overall 6475 69% 0.52
Table 1: Inter-annotator agreement per part-of-
speech.
can be explained by our sampling method de-
scribed above).
? 0 + Total
? 1803 137 39 1979
0 1011 1857 649 3517
+ 81 108 790 979
Total 2895 2102 1478 6475
Table 2: Contingency table for all words assessed
by two annotators.
5 Experiments and results
We evaluated several versions of the method of
section 3 in order to find the best setting.
Our baseline is a ranking of all words in the
wordnet with the weight -1 assigned to the trans-
lations of English negative polarity words, 1 as-
signed to the translations of positive words, and
0 assigned to the remaining words. This corre-
sponds to simply translating the English subjec-
tivity lexicon.
In the run all.100 we applied our method to all
words, synsets and relations from the DutchWord-
net to create a graph with 153,386 nodes (70,192
synsets, 83,194 words) and 362,868 directed arcs
(103,734 word-to-synset, 103,734 synset-to-word,
155,400 synset-to-synset relations). We used 100
iterations of the PageRank algorihm for this run
(and all runs below, unless indicated otherwise).
In the run syn.100 we only used synset-to-
word, word-to-synset relations and 2,850 near-
synonymy relations between synsets. We added
1,459 near-antonym relations to the graph to
produce the run syn+ant.100. In the run
syn+hyp.100 we added 66,993 hyponymy and
66,993 hyperonymy relations to those used in run
syn.100.
We also experimented with the information pro-
vided in the definitions (glosses) of synset. The
glosses were available for 68,122 of the 70,192
401
synsets. Following (Esuli and Sebastiani, 2007),
we assumed that there is a semantic relationship
between a synset and each word used in its gloss.
Thus, the run gloss.100 uses a graph with 70,192
synsets, 83,194 words and 350,855 directed arcs
from synsets to lemmas of all words in their
glosses. To create these arcs, glosses were lemma-
tized and lemmas not found in the wordnet were
ignored.
To see if the information in the glosses can com-
plement the wordnet relations, we also generated
a hybrid run syn+ant+gloss.100 that used arcs de-
rived from word-to-synset, synset-to-word, syn-
onymy, antonymy relations and glosses.
Finally, we experimented with the number of
iterations of PageRank in two setting: using all
wordnet relations and using only synonyms and
antonyms.
5.1 Evaluation measures
We used several measures to evaluate the quality
of the word rankings produced by our method.
We consider the evaluation of a ranking parallel
to the evaluation for a binary classification prob-
lem, where words are classified as positive (resp.
negative) if the assigned score exceeds a certain
threshold value. We can select a specific thresh-
old and classify all words exceeding this score as
positive. There will be a certain amount of cor-
rectly classified words (true positives), and some
incorrectly classified words (false positives). As
we move the threshold to include a larger portion
of the ranking, both the number of true positives
and the number of false positives increase.
We can visualize the quality of rankings by plot-
ting their ROC curves, which show the relation be-
tween true positive rate (portion of the data cor-
rectly labeled as positive instances) and false pos-
itive rate (portion of the data incorrectly labeled
as positive instances) at all possible threshold set-
tings.
To compare rankings, we compute the area un-
der the ROC curve (AUC), a measure frequently
used to evaluate the performance of ranking clas-
sifiers. The AUC value corresponds to the proba-
bility that a randomly drawn positive instance will
be ranked higher than a randomly drawn negative
instance. Thus, an AUC of 0.5 corresponds to ran-
dom performance, a value of 1.0 corresponds to
perfect performance. When evaluating word rank-
ings, we compute AUC? and AUC+ as evalua-
Run ?k Dk AUC? AUC+
baseline 0.395 0.303 0.701 0.733
syn.10 0.641 0.180 0.829 0.837
gloss.100 0.637 0.181 0.829 0.835
all.100 0.565 0.218 0.792 0.787
syn.100 0.645 0.177 0.831 0.839
syn+ant.100 0.650 0.175 0.833 0.841
syn+ant+gloss.100 0.643 0.178 0.831 0.838
syn+hyp.100 0.594 0.203 0.807 0.810
Table 3: Evaluation results
tion measures for the tasks of identifying words
with negative (resp., positive) polarity.
Other measures commonly used to evalu-
ate rankings are Kendall?s rank correlation, or
Kendall?s tau coefficient, and Kendall?s dis-
tance (Fagin et al, 2004; Esuli and Sebastiani,
2007). When comparing rankings, Kendall?s mea-
sures look at the number of pairs of ranked items
that agree or disagree with the ordering in the gold
standard. The measures can deal with partially
ordered sets (i.e., rankings with ties): only pairs
that are ordered in the gold standard are used.
Let T = {(ai, bi)}i denote the set of pairs or-
dered in the gold standard, i.e., ai ?g bi. Let
C = {(a, b) ? T | a ?r b} be the set of con-
cordant pairs, i.e., pairs ordered the same way in
the gold standard and in the ranking. Let D =
{(a, b) ? T | b ?r a} be the set of discordant
pairs and U = T \ (C ? D) the set of pairs or-
dered in the gold standard, but tied in the rank-
ing. Kendall?s rank correlation coefficient ?k and
Kendall?s distance Dk are defined as follows:
?k =
|C| ? |D|
|T |
Dk =
|D|+ p ? |U |
|T |
where p is a penalization factor for ties, which we
set to 0.5, following (Esuli and Sebastiani, 2007).
The value of ?k ranges from -1 (perfect dis-
agreement) to 1 (perfect agreement), with 0 indi-
cating an almost random ranking. The value of
Dk ranges from 0 (perfect agreement) to 1 (per-
fect disagreement).
When applying Kendall?s measures we assume
that the gold standard defines a partial order: for
two words a and b, a ?g b holds when a ? Ng, b ?
Ug ? Pg or when a ? Ug, b ? Pg; here Ng, Ug, Pg
are sets of words judged as negative, neutral and
positive, respectively, by human assessors.
5.2 Types of wordnet relations
The results in Table 3 indicate that the method per-
forms best when only synonymy and antonymy
402
Negative polarity
False positive rate
Tru
e p
osit
ive 
rate
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
baseline
all.100
gloss.100
syn+ant.100
syn+hyp.100
Positive polarity
False positive rate
Tru
e p
osit
ive 
rate
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
baseline
all.100
gloss.100
syn+ant.100
syn+hyp.100
Figure 1: ROC curves showing the impact of using different sets of relations for negative and positive
polarity. Graphs were generated using ROCR (Sing et al, 2005).
relations are considered for ranking. Adding hy-
ponyms and hyperonyms, or adding relations be-
tween synsets and words in their glosses substan-
tially decrease the performance, according to all
four evaluation measures. With all relations, the
performance degrades even further. Our hypothe-
sis is that with many relations the polarity mass of
the seed words is distributed too broadly. This is
supported by the drop in the performance early in
the ranking at the ?negative? side of runs with all
relations and with hyponyms (Figure 1, left). An-
other possible explanation can be that words with
many incoming arcs (but without strong connec-
tions to the seed words) get substantial weights,
thereby decreasing the quality of the ranking.
Antonymy relations also prove useful, as using
them in addition to synonyms results in a small
improvement. This justifies our modification of
the PageRank algorithm, when we allow negative
node and arc weights.
In the best setting (syn+ant.100), our method
achieves an accuracy of 0.82 at top 3,000 negative
words, and 0.62 at top 3,000 positive words (esti-
mated from manual assessments of a sample, see
section 4). Moreover, Figure 1 indicates that the
accuracy of the seed set (i.e., the baseline transla-
tions of the English lexicon) is maintained at the
positive and negative ends of the ranking for most
variants of the method.
5.3 The number of iterations
In Figure 2 we plot how the AUC? measure
changes when the number of PageRank iterations
increases (for positive polarity; the plots are al-
most identical for negative polarity). Although the
absolute maximum of AUC is achieved at 110 iter-
ation (60 iterations for positive polarity), the AUC
clearly converges after 20 iterations. We conclude
that after 20 iterations all useful information has
been propagated through the graph. Moreover, our
version of PageRank reaches a stable weight dis-
tribution and, at the same time, produces the best
ranking.
5.4 Comparison to previous work
Although the values in the evaluation results are,
obviously, language-dependent, we tried to repli-
cate the methods used in the literature for Roma-
nian and English (section 2), to the degree possi-
ble.
Our baseline replicates the method of (Mihal-
cea et al, 2007): i.e., a simple translation of the
English lexicon into the target language. The
run syn.10 is similar to the iterative method used
in (Banea et al, 2008), except that we do not per-
form a corpus-based filtering. We run PageRank
for 10 iterations, so that polarity is propagated
from the seed words to all their 5-step-synonymy
neighbours. Table 3 indicates that increasing the
number of iterations in the method of (Banea et
403
0 50 100 150 200
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
Number of iterations
AU
C
all relations
synsets+antonyms
Figure 2: The number of iterations and the ranking
quality (AUC), for positive polarity. Rankings for
negative polarity behave similarly.
al., 2008) might help to generate a better subjec-
tivity lexicon.
The run gloss.100 is similar to the PageRank-
based method of (Esuli and Sebastiani, 2007).
The main difference is that Esuli and Sebastiani
used the extended English WordNet, where words
in all glosses are manually assigned to their cor-
rect synsets: the PageRank method then uses re-
lations between synsets and synsets of words in
their glosses. Since such a resource is not avail-
able for our target language (Dutch), we used rela-
tions between synsets and words in their glosses,
instead. With this simplification, the PageRank
method using glosses produces worse results than
the method using synonyms. Further experiments
with the extended English WordNet are neces-
sary to investigate whether this decrease can be at-
tributed to the lack of disambiguation for glosses.
An important difference between our method
and (Esuli and Sebastiani, 2007) is that the lat-
ter produces two independent rankings: one for
positive and one for negative words. To evalu-
ate the effect of this choice, we generated runs
gloss.100.N and gloss.100.P that used only nega-
tive (resp., only positive) seed words. We compare
these runs with the run gloss.100 (that starts with
both positive and negative seeds) in Table 4. To
allow a fair comparison of the generated rankings,
the evaluation measures in this case are calculated
separately for two binary classification problems:
words with negative polarity versus all words, and
words with positive polarity versus all.
The results in Table 4 clearly indicate that in-
Run ??k D
?
k AUC
?
gloss.100 0.669 0.166 0.829
gloss.100.N 0.562 0.219 0.782
?+k D
+
k AUC
+
gloss.100 0.665 0.167 0.835
gloss.100.P 0.580 0.210 0.795
Table 4: Comparison of separate and simultaneous
rankings of negative and positive words.
formation about words of one polarity class helps
to identify words of the other polarity: negative
words are unlikely to be also positive, and vice
versa. This supports our design choice: ranking
words from negative to positive in one run of the
method.
6 Conclusion
We have presented a PageRank-like algorithm that
bootstraps a subjectivity lexicon from a list of
initial seed examples (automatic translations of
words in an English subjectivity lexicon). The al-
gorithm views a wordnet as a graph where words
and concepts are connected by relations such as
synonymy, hyponymy, meronymy etc. We initial-
ize the algorithm by assigning high weights to pos-
itive seed examples and low weights to negative
seed examples. These weights are then propagated
through the wordnet graph via the relations. After
a number of iterations words are ranked according
to their weight. We assume that words with lower
weights are likely negative and words with high
weights are likely positive.
We evaluated several variants of the method for
the Dutch language, using the most recent version
of Cornetto, an extension of Dutch WordNet. The
evaluation was based on the manual assessment
of 9,089 words (with inter-annotator agreement
69%, ?=0.52). Best results were achieved when
the method used only synonymy and antonymy
relations, and was ranking positive and negative
words simultaneously. In this setting, the method
achieves an accuracy of 0.82 at the top 3,000 neg-
ative words, and 0.62 at the top 3,000 positive
words.
Our method is language-independent and can
easily be applied to other languages for which
wordnets exist. We plan to make the implemen-
tation of the method publicly available.
An additional important outcome of our experi-
ments is the first (to our knowledge) manually an-
notated sentiment lexicon for the Dutch language.
404
The lexicon contains 2,836 negative polarity and
1,628 positive polarity words. The lexicon will be
made publicly available as well. Our future work
will focus on using the lexicon for sentence- and
phrase-level sentiment extraction for Dutch.
Acknowledgments
This work was supported by projects DuO-
MAn and Cornetto, carried out within the
STEVIN programme which is funded by the
Dutch and Flemish Governments (http://
www.stevin-tst.org), and by the Nether-
lands Organization for Scientific Research (NWO)
under project number 612.061.814.
References
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC 2006,
pages 417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 424?431.
Ronald Fagin, Ravi Kumar, Mohammad Mahdian,
D. Sivakumar, and Erik Vee. 2004. Com-
paring and aggregating rankings with ties. In
PODS ?04: Proceedings of the twenty-third ACM
SIGMOD-SIGACT-SIGART symposium on Princi-
ples of database systems, pages 47?58, New York,
NY, USA. ACM.
Valentin Jijkoun and Katja Hofmann. 2008.
Task-based Evaluation Report: Building a
Dutch Subjectivity Lexicon. Technical report.
Technical report, University of Amsterdam.
http://ilps.science.uva.nl/biblio/
cornetto-subjectivity-lexicon.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING).
R. Mihalcea and H. Liu. 2006. A corpus-based ap-
proach to finding happiness. In Proceedings of
the AAAI Spring Symposium on Computational Ap-
proaches to Weblogs.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 976?983, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 79?86.
T. Sing, O. Sander, N. Beerenwinkel, and T. Lengauer.
2005. ROCR: visualizing classifier performance in
R. Bioinformatics, 21(20):3940?3941.
P. Vossen, K. Hofman, M. De Rijke, E. Tjong
Kim Sang, and K. Deschacht. 2007. The cornetto
database: Architecture and user-scenarios. In Pro-
ceedings of 7th Dutch-Belgian Information Retrieval
Workshop DIR2007.
Piek Vossen, editor. 1998. EuroWordNet: a mul-
tilingual database with lexical semantic networks.
Kluwer Academic Publishers, Norwell, MA, USA.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceeding of CICLing-05, In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, volume 3406 of
Lecture Notes in Computer Science, pages 475?486.
Springer-Verlag.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP 2005), pages 347?354.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005b. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLTEMNLP 2005.
405
Finding non-local dependencies: beyond pattern matching
Valentin Jijkoun
Language and Inference Technology Group,
ILLC, University of Amsterdam
jijkoun@science.uva.nl
Abstract
We describe an algorithm for recover-
ing non-local dependencies in syntac-
tic dependency structures. The pattern-
matching approach proposed by John-
son (2002) for a similar task for phrase
structure trees is extended with machine
learning techniques. The algorithm is es-
sentially a classifier that predicts a non-
local dependency given a connected frag-
ment of a dependency structure and a
set of structural features for this frag-
ment. Evaluating the algorithm on the
Penn Treebank shows an improvement of
both precision and recall, compared to the
results presented in (Johnson, 2002).
1 Introduction
Non-local dependencies (also called long-distance,
long-range or unbounded) appear in many fre-
quent linguistic phenomena, such as passive, WH-
movement, control and raising etc. Although much
current research in natural language parsing focuses
on extracting local syntactic relations from text, non-
local dependencies have recently started to attract
more attention. In (Clark et al, 2002) long-range
dependencies are included in parser?s probabilistic
model, while Johnson (2002) presents a method for
recovering non-local dependencies after parsing has
been performed.
More specifically, Johnson (2002) describes a
pattern-matching algorithm for inserting empty
nodes and identifying their antecedents in phrase
structure trees or, to put it differently, for recover-
ing non-local dependencies. From a training corpus
with annotated empty nodes Johnson?s algorithm
first extracts those local fragments of phrase trees
which connect empty nodes with their antecedents,
thus ?licensing? corresponding non-local dependen-
cies. Next, the extracted tree fragments are used as
patterns to match against previously unseen phrase
structure trees: when a pattern is matched, the algo-
rithm introduces a corresponding non-local depen-
dency, inserting an empty node and (possibly) co-
indexing it with a suitable antecedent.
In (Johnson, 2002) the author notes that the
biggest weakness of the algorithm seems to be that
it fails to robustly distinguish co-indexed and free
empty nodes and it is lexicalization that may be
needed to solve this problem. Moreover, the author
suggests that the algorithm may suffer from over-
learning, and using more abstract ?skeletal? patterns
may be helpful to avoid this.
In an attempt to overcome these problems we de-
veloped a similar approach using dependency struc-
tures rather than phrase structure trees, which, more-
over, extends bare pattern matching with machine
learning techniques. A different definition of pat-
tern allows us to significantly reduce the number
of patterns extracted from the same corpus. More-
over, the patterns we obtain are quite general and in
most cases directly correspond to specific linguistic
phenomena. This helps us to understand what in-
formation about syntactic structure is important for
the recovery of non-local dependencies and in which
cases lexicalization (or even semantic analysis) is
required. On the other hand, using these simpli-
fied patterns, we may loose some structural infor-
mation important for recovery of non-local depen-
dencies. To avoid this, we associate patterns with
certain structural features and use statistical classifi-
NP
NP
the asbestos
VP
found NP
*
PP
in NP
schools
(a) The Penn Treebank format
asbestos
the
MOD
found
S-OBJ
in
PP
schools
NP-OBJ
NP-OBJ
(b) Derived dependency structure
Figure 1: Past participle (reduced relative clause).
cation methods on top of pattern matching.
The evaluation of our algorithm on data automat-
ically derived from the Penn Treebank shows an in-
crease in both precision and recall in recovery of
non-local dependencies by approximately 10% over
the results reported in (Johnson, 2002). However,
additional work remains to be done for our algorithm
to perform well on the output of a parser.
2 From the Penn Treebank to a
dependency treebank
This section describes the corpus of dependency
structures that we used to evaluate our algorithm.
The corpus was automatically derived from the Penn
Treebank II corpus (Marcus et al, 1993), by means
of the script chunklink.pl (Buchholz, 2002)
that we modified to fit our purposes. The script uses
a sort of head percolation table to identify heads of
constituents, and then converts the result to a de-
pendency format. We refer to (Buchholz, 2002) for
a thorough description of the conversion algorithm,
and will only emphasize the two most important
modifications that we made.
One modification of the conversion algorithm
concerns participles and reduced relative clauses
modifying NPs. Regular participles in the Penn
Treebank II are simply annotated as VPs adjoined
to the modified NPs (see Figure 1(a)). These par-
ticiples (also called reduced relative clauses, as they
lack auxiliary verbs and complementizers) are both
syntactically and semantically similar to full rela-
tive clauses, but the Penn annotation does not in-
troduce empty complementizers, thus preventing co-
indexing of a trace with any antecedent. We perform
a simple heuristic modification while converting the
Treebank to the dependency format: when we en-
counter an NP modified by a VP headed by a past
participle, an object dependency is introduced be-
tween the head of the VP and the head of the NP.
Figure 1(b) shows an example, with solid arrows de-
noting local and dotted arrows denoting non-local
dependencies. Arrows are marked with dependency
labels and go from dependents to heads.
This simple heuristics does not allow us to handle
all reduced relative clauses, because some of them
correspond to PPs or NPs rather than VPs, but the
latter are quite rare in the Treebank.
The second important change to Buchholz? script
concerns the structure of VPs. For every verb clus-
ter, we choose the main verb as the head of the clus-
ter, and leave modal and auxiliary verbs as depen-
dents of the main verb. A similar modification was
used by Eisner (1996) for the study of dependency
parsing models. As will be described below, this al-
lows us to ?factor out? tense and modality of finite
clauses from our patterns, making the patterns more
general.
3 Pattern extraction and matching
After converting the Penn Treebank to a dependency
treebank, we first extracted non-local dependency
patterns. As in (Johnson, 2002), our patterns are
minimal connected fragments containing both nodes
involved in a non-local dependency. However, in our
Henderson will become chairman, succeeding Butler. . .
become
will
AUX
chairman
NP-PRD
Henderson
NP-SBJ
succeeding
S-ADV
NP-SBJ
Butler
NP-OBJ
.
.
..
.
.
.
.
.
..
.
.
.
.
.
.
..
.
.
.
.
.
..
.
.
.
.
 
.
.
..
.
.
.
.
.
..
.
.
.
.
.
.
..
.
.
.
.
.
..
.
.
.
.

........
................
..........
......
.......
...
.
.
.
..
.
.
..
.
.
.
..
.
..
.
.
..
.
..
.
.
..
.
..
..
.
..
.
..
.

.........................
..........
...
...
...
...
....
...
...
.
....
...
...
....
...
....
.....
....
.....
......
...

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.






NP-SBJ




S-ADV

	
NP-SBJ
. . . which he declined to specify
which
declined
he specify
to
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


.
..
.
..
.
.
..
.
..
.
..
.
.
..
.
..
.
..
.

..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..

.
.
.
.
.
.
.
.
.
.
.



S
NP-SBJ S-OBJ
NP-SBJ
NP-OBJ




NP-SBJ




S-OBJ

	
NP-SBJ




S
 

 
S-OBJ  
NP-OBJ
Figure 2: Dependency graphs and extracted patterns.
case these fragments are not connected sets of local
trees, but shortest paths in local dependency graphs,
leading from heads to non-local dependents. Pat-
terns do not include POS tags of the involved words,
but only labels of the dependencies. Thus, a pat-
tern is a directed graph with labeled edges, and two
distinguished nodes: the head and the dependent of
a corresponding non-local dependency. When sev-
eral patterns intersect, as may be the case, for exam-
ple, when a word participates in more than one non-
local dependency, these patterns are handled inde-
pendently. Figure 2 shows examples of dependency
graphs (above) and extracted patterns (below, with
filled bullets corresponding to the nodes of a non-
local dependency). As before, dotted lines denote
non-local dependencies.
The definition of a structure matching a pattern,
and the algorithms for pattern matching and pat-
tern extraction from a corpus are straightforward and
similar to those described in (Johnson, 2002).
The total number of non-local dependencies
found in the Penn WSJ is 57325. The number of
different extracted patterns is 987. The 80 most fre-
quent patterns (those that we used for the evaluation
of our algorithm) cover 53700 out of all 57325 non-
local dependencies (93,7%). These patterns were
further cleaned up manually, e.g., most Penn func-
tional tags (-TMP, -CLR etc., but not -OBJ, -SBJ,
-PRD) were removed. Thus, we ended up with 16
structural patterns (covering the same 93,7% of the
Penn Treebank).
Table 1 shows some of the patterns found in the
Penn Treebank. The column Count gives the number
of times a pattern introduces non-local dependen-
cies in the corpus. The Match column is the num-
ber of times a pattern actually occurs in the corpus
(whether it introduces a non-local dependency or
not). The patterns are shown as dependency graphs
with labeled arrows from dependents to heads. The
column Dependency shows labels and directions of
introduced non-local dependencies.
Clearly, an occurrence of a pattern alone is not
enough for inserting a non-local dependency and de-
termining its label, as for many patterns Match is
significantly greater than Count. For this reason we
introduce a set of other structural features, associ-
ated with patterns. For every occurrence of a pattern
and for every word of this occurrence, we extract the
following features:
 pos, the POS tag of the word;
 class, the simplified word class (similar to (Eis-
ner, 1996));
 fin, whether the word is a verb and a head of
a finite verb cluster (as opposed to infinitives,
gerunds or participles);
 subj, whether the word has a dependent (prob-
ably not included in the pattern) with a depen-
dency label NP-SBJ; and
 obj, the same for NP-OBJ label.
Thus, an occurrence of a pattern is associated with
a sequence of symbolic features: five features for
each node in the pattern. E.g., a pattern consisting of
3 nodes will have a feature vector with 15 elements.
Id Count Match Pattern Dependency Dep. count P R f
1 
NP-SBJ
 
 7481 1.00 1.00 1.00
2 
ADVP
 
 1945 0.82 0.90 0.86
3
10527 12716 
S



NP-OBJ
 
 727 0.60 0.71 0.65
4 
NP-SBJ
 
 8562 0.84 0.95 0.89
5
8789 17911 
S-*

NP-SBJ
 


NP-OBJ
 
 227 0.83 0.71 0.77
6 8120 8446 
VP-OBJ

NP-SBJ
 
 
NP-OBJ
 
 8120 0.99 1.00 1.00
7 
NP-OBJ
 
 1013 0.73 0.84 0.79
8 
NP-SBJ
 
 836 0.60 0.96 0.74
9
2518 34808  SBAR 

ADVP
 
 669 0.56 0.16 0.25
10 1424 1442 
S-OBJ
	
VP-OBJ

NP-SBJ
 
 
NP-SBJ
 
 1424 0.99 1.00 0.99
11 
NP-SBJ
 
 1047 0.86 0.83 0.85
12
1265 28170 
S*

S*



NP-OBJ
 
 218 0.77 0.71 0.74
13 880 1699 
S-NOM
	
PP

NP-SBJ
 
 
NP-SBJ
 
 880 0.85 0.87 0.86
Table 1: Several non-local dependency patterns, frequencies of patterns and pattern-dependency pairs in
Penn Treebank, and evaluation results. The best scores are in boldface.
Id Dependency Example
1 NP-SBJ . . . sympthoms thatdep showhead up decades later. . .
2 ADVP . . . buying futures whendep future prices fallhead. . .
3 NP-OBJ . . . practices thatdep the government has identifiedhead. . .
4 NP-SBJ . . . the airlinedep had been planning to initiatehead service. . .
5 NP-OBJ . . . that its absencedep is to blamehead for the sluggish development. . .
6 NP-OBJ . . . the situationdep will get settledhead in the short term. . .
7 NP-OBJ . . . the numberdep of planes the company has soldhead. . .
8 NP-SBJ . . . one of the first countriesdep to concludehead its talks. . .
9 ADVP . . . buying sufficient optionsdep to purchasehead shares. . .
10 NP-SBJ . . . both magazinesdep are expected to announcehead their ad rates. . .
11 NP-SBJ . . . whichdep is looking to expandhead its business. . .
12 NP-OBJ . . . the programsdep we wanted to dohead. . .
13 NP-SBJ . . . youdep can?t make soap without turninghead up the flame. . .
Table 2: Examples of the patterns. The ?support? words, i.e. words that appear in a pattern but are nei-
ther heads nor non-local dependents, are in italic; they correspond to empty bullets in patterns in Table 1.
Boldfaced words correspond to filled bullets in Table 1.
4 Classification of pattern instances
Given a pattern instance and its feature vector, our
task now is to determine whether the pattern intro-
duces a non-local dependency and, if so, what the
label of this dependency is. In many cases this is not
a binary decision, since one pattern may introduce
several possible labeled dependencies (e.g., the pat-
tern   S   in Table 1). Our task is a classification
task: an instance of a pattern must be assigned to
two or more classes, corresponding to several possi-
ble dependency labels (or absence of a dependency).
We train a classifier on instances extracted from a
corpus, and then apply it to previously unseen in-
stances.
The procedure for finding non-local dependencies
now consists of the two steps:
1. given a local dependency structure, find match-
ing patterns and their feature vectors;
2. for each pattern instance found, use the clas-
sifier to identify a possible non-local depen-
dency.
5 Experiments and evaluation
In our experiments we used sections 02-22 of the
Penn Treebank as the training corpus and section 23
as the test corpus. First, we extracted all non-local
patterns from the Penn Treebank, which resulted in
987 different (pattern, non-local dependency) pairs.
As described in Section 3, after cleaning up we took
16 of the most common patterns.
For each of these 16 patterns, instances of the pat-
tern, pattern features, and a non-local dependency
label (or the special label ?no? if no dependency was
introduced by the instance) were extracted from the
training and test corpora.
We performed experiments with two statistical
classifiers: the decision tree induction system C4.5
(Quinlan, 1993) and the Tilburg Memory-Based
Learner (TiMBL) (Daelemans et al, 2002). In most
cases TiBML performed slightly better. The re-
sults described in this section were obtained using
TiMBL.
For each of the 16 structural patterns, a separate
classifier was trained on the set of (feature-vector,
label) pairs extracted from the training corpus, and
then evaluated on the pairs from the test corpus. Ta-
ble 1 shows the results for some of the most fre-
quent patterns, using conventional metrics: preci-
sion (the fraction of the correctly labeled dependen-
cies among all the dependencies found), recall (the
fraction of the correctly found dependencies among
all the dependencies with a given label) and f-score
(harmonic mean of precision and recall). The table
also shows the number of times a pattern (together
with a specific non-local dependency label) actually
occurs in the whole Penn Treebank corpus (the col-
umn Dependency count).
In order to compare our results to the results pre-
sented in (Johnson, 2002), we measured the over-
all performance of the algorithm across patterns and
non-local dependency labels. This corresponds to
the row ?Overall? of Table 4 in (Johnson, 2002), re-
peated here in Table 4. We also evaluated the pro-
cedure on NP traces across all patterns, i.e., on non-
local dependencies with NP-SBJ, NP-OBJ or NP-
PRD labels. This corresponds to rows 2, 3 and 4
of Table 4 in (Johnson, 2002). Our results are pre-
sented in Table 3. The first three columns show the
results for those non-local dependencies that are ac-
tually covered by our 16 patterns (i.e., for 93.7% of
all non-local dependencies). The last three columns
present the evaluation with respect to all non-local
dependencies, thus the precision is the same, but re-
call drops accordingly. These last columns give the
results that can be compared to Johnson?s results for
section 23 (Table 4).
On covered deps On all deps
P R f P R f
All 0.89 0.93 0.91 0.89 0.84 0.86
NPs 0.90 0.96 0.93 0.90 0.87 0.89
Table 3: Overall performance of our algorithm.
On section 23 On parser output
P R f P R f
Overall 0.80 0.70 0.75 0.73 0.63 0.68
Table 4: Results from (Johnson, 2002).
It is difficult to make a strict comparison of our
results and those in (Johnson, 2002). The two algo-
rithms are designed for slightly different purposes:
while Johnson?s approach allows one to recover free
empty nodes (without antecedents), we look for non-
local dependencies, which corresponds to identifica-
tion of co-indexed empty nodes (note, however, the
modifications we describe in Section 2, when we ac-
tually transform free empty nodes into co-indexed
empty nodes).
6 Discussion
The results presented in the previous section show
that it is possible to improve over the simple pattern
matching algorithm of (Johnson, 2002), using de-
pendency rather than phrase structure information,
more skeletal patterns, as was suggested by John-
son, and a set of features associated with instances
of patterns.
One of the reasons for this improvement is that
our approach allows us to discriminate between dif-
ferent syntactic phenomena involving non-local de-
pendencies. In most cases our patterns correspond
to linguistic phenomena. That helps to understand
why a particular construction is easy or difficult for
our approach, and in many cases to make the nec-
essary modifications to the algorithm (e.g., adding
other features to instances of patterns). For example,
for patterns 11 and 12 (see Tables 1 and 2) our classi-
fier distinguishes subject and object reasonably well,
apparently, because the feature has a local object is
explicitly present for all instances (for the examples
11 and 12 in Table 2, expand has a local object, but
do doesn?t).
Another reason is that the patterns are general
enough to factor out minor syntactic differences in
linguistic phenomena (e.g., see example 4 in Ta-
ble 2). Indeed, the most frequent 16 patterns cover
93.7% of all non-local dependencies in the corpus.
This is mainly due to our choices in the dependency
representation, such as making the main verb a head
of a verb phrase. During the conversion to a de-
pendency treebank and extraction of patterns some
important information may have been lost (e.g., the
finiteness of a verb cluster, or presence of subject
and object); for that reason we had to associate pat-
terns with additional features, encoding this infor-
mation and providing it to the classifier. In other
words, we first take an ?oversimplified? representa-
tion of the data, and then try to find what other data
features can be useful. This strategy appears to be
successful, because it allows us to identify which in-
formation is important for the recovery of non-local
dependencies.
More generally, the reasonable overall perfor-
mance of the algorithm is due to the fact that for
the most common non-local dependencies (extrac-
tion in relative clauses and reduced relative clauses,
passivization, control and raising) the structural in-
formation we extract is enough to robustly identify
non-local dependencies in a local dependency graph:
the most frequent patterns in Table 1 are also those
with best scores. However, many less frequent phe-
nomena appear to be much harder. For example, per-
formance for relative clauses with extracted objects
or adverbs is much worse than for subject relative
clauses (e.g., patterns 2 and 3 vs. 1 in Table 1). Ap-
parently, in most cases this is not due to the lack
of training data, but because structural information
alone is not enough and lexical preferences, subcat-
egorization information, or even semantic properties
should be considered. We think that the aproach al-
lows us to identify those ?hard? cases.
The natural next step in evaluating our algorithm
is to work with the output of a parser instead of
the original local structures from the Penn Tree-
bank. Obviously, because of parsing errors the per-
formance drops significantly: e.g., in the experi-
ments reported in (Johnson, 2002) the overall f-
score decreases from 0.75 to 0.68 when evaluating
on parser output (see Table 4). While experimenting
with Collins? parser (Collins, 1999), we found that
for our algorithm the accuracy drops even more dra-
matically, when we train the classifier on Penn Tree-
bank data and test it on parser output. One of the
reasons is that, since we run our algorithm not on
the parser?s output itself but on the output automat-
ically converted to dependency structures, conver-
sion errors also contribute to the performance drop.
Moreover, the conversion script is highly tailored to
the Penn Treebank annotation (with functional tags
and empty nodes) and, when run on the parser?s out-
put, produces structures with somewhat different de-
pendency labels. Since our algorithm is sensitive to
the exact labels of the dependencies, it suffers from
these systematic errors.
One possible solution to that problem could be to
extract patterns and train the classification algorithm
not on the training part of the Penn Treebank, but on
the parser output for it. This would allow us to train
and test our algorithm on data of the same nature.
7 Conclusions and future work
We have presented an algorithm for recovering long-
distance dependencies in local dependency struc-
tures. We extend the pattern matching approach
of Johnson (2002) with machine learning tech-
niques, and use dependency structures instead of
constituency trees. Evaluation on the Penn Treebank
shows an increase in accuracy.
However, we do not have yet satisfactory results
when working on a parser output. The conversion
algorithm and the dependency labels we use are
largely based on the Penn Treebank annotation, and
it seems difficult to use them with the output of a
parser.
A parsing accuracy evaluation scheme based on
grammatical relations (GR), presented in (Briscoe
et al, 2002), provides a set of dependency labels
(grammatical relations) and a manually annotated
dependency corpus. Non-local dependencies are
also annotated there, although no explicit difference
is made between local and non-local dependencies.
Since our classification algorithm does not depend
on a particular set of dependency labels, we can also
use the set of labels described by Briscoe et al if we
convert Penn Treebank to a GR-based dependency
treebank and use it as the training corpus. This will
allow us to make the patterns independent of the
Penn Treebank annotation details and simplify test-
ing the algorithm with a parser?u output. We will
also be able to use the flexible and parameterizable
scoring schemes discussed in (Briscoe et al, 2002).
We also plan to develop the approach by using
iteration of our non-local relations extraction algo-
rithm, i.e., by running the algorithm, inserting the
found non-local dependencies, running it again etc.,
until no new dependencies are found. While rais-
ing an important and interesting issue of the order in
which we examine our patterns, we believe that this
will allow us to handle very long extraction chains,
like the one in sentence ?Aichi revised its tax calcu-
lations after being challenged for allegedly failing
to report. . . ?, where Aichi is a (non-local) depen-
dent of five verbs. Iteration of the algorithm will
also help to increase the coverage (which is 93,7%
with our 16 non-iterated patterns).
Acknowledgements
This research was supported by the Netherlands
Organization for Scientific Research (NWO), un-
der project number 220-80-001. We would like to
thank Maarten de Rijke, Detlef Prescher and Khalil
Sima?an for many fruitful discussions and useful
suggestions and comments.
References
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pages 340?345.
Michael Collins. 1999. Head-Driven Statistical Models
For Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Sabine Buchholz. 2002. Memory-based grammatical re-
lation finding. Ph.D. thesis, Tilburg University.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2002. TiMBL:
Tilburg Memory Based Learner, version 4.3, Refer-
ence Guide. ILK Technical Report 02-10, Available
from http://ilk.kub.nl/downloads/pub/
papers/ilk0210.ps.gz
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann Publishers.
Michael P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ted Briscoe, John Carroll, Jonathan Graham and Ann
Copestake. 2002. Relational evaluation schemes. In
Proceedings of the Beyond PARSEVAL Workshop at
LREC 2002, pages 4?8.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures using a
wide-coverage CCG parser. In Proceedings of the 40th
Meeting of the ACL, pages 327-334.
Enriching the Output of a Parser Using Memory-Based Learning
Valentin Jijkoun and Maarten de Rijke
Informatics Institute, University of Amsterdam
  jijkoun, mdr  @science.uva.nl
Abstract
We describe a method for enriching the output of a
parser with information available in a corpus. The
method is based on graph rewriting using memory-
based learning, applied to dependency structures.
This general framework allows us to accurately re-
cover both grammatical and semantic information
as well as non-local dependencies. It also facili-
tates dependency-based evaluation of phrase struc-
ture parsers. Our method is largely independent of
the choice of parser and corpus, and shows state of
the art performance.
1 Introduction
We describe a method to automatically enrich the
output of parsers with information that is present
in existing treebanks but usually not produced by
the parsers themselves. Our motivation is two-fold.
First and most important, for applications requiring
information extraction or semantic interpretation of
text, it is desirable to have parsers produce gram-
matically and semantically rich output. Second, to
facilitate dependency-based comparison and evalu-
ation of different parsers, their outputs may need to
be transformed into specific rich dependency for-
malisms.
The method allows us to automatically trans-
form the output of a parser into structures as they
are annotated in a dependency treebank. For a
phrase structure parser, we first convert the pro-
duced phrase structures into dependency graphs
in a straightforward way, and then apply a se-
quence of graph transformations: changing depen-
dency labels, adding new nodes, and adding new
dependencies. A memory-based learner trained
on a dependency corpus is used to detect which
modifications should be performed. For a depen-
dency corpus derived from the Penn Treebank and
the parsers we considered, these transformations
correspond to adding Penn functional tags (e.g.,
-SBJ, -TMP, -LOC), empty nodes (e.g., NP PRO)
and non-local dependencies (controlled traces, WH-
extraction, etc.). For these specific sub-tasks our
method achieves state of the art performance. The
evaluation of the transformed output of the parsers
of Charniak (2000) and Collins (1999) gives 90%
unlabelled and 84% labelled accuracy with respect
to dependencies, when measured against a depen-
dency corpus derived from the Penn Treebank.
The paper is organized as follows. After provid-
ing some background and motivation in Section 2,
we give the general overview of our method in Sec-
tion 3. In Sections 4 through 8, we describe all
stages of the transformation process, providing eval-
uation results and comparing our methods to earlier
work. We discuss the results in Section 9.
2 Background and Motivation
State of the art statistical parsers, e.g., parsers
trained on the Penn Treebank, produce syntactic
parse trees with bare phrase labels, such as NP, PP,
S, although the training corpora are usually much
richer and often contain additional grammatical and
semantic information (distinguishing various modi-
fiers, complements, subjects, objects, etc.), includ-
ing non-local dependencies, i.e., relations between
phrases not adjacent in the parse tree. While this in-
formation may be explicitly annotated in a treebank,
it is rarely used or delivered by parsers.1 The rea-
son is that bringing in more information of this type
usually makes the underlying parsing model more
complicated: more parameters need to be estimated
and independence assumptions may no longer hold.
Klein and Manning (2003), for example, mention
that using functional tags of the Penn Treebank
(temporal, location, subject, predicate, etc.) with a
simple unlexicalized PCFG generally had a negative
effect on the parser?s performance. Currently, there
are no parsers trained on the Penn Treebank that use
the structure of the treebank in full and that are thus
1Some notable exceptions are the CCG parser described in
(Hockenmaier, 2003), which incorporates non-local dependen-
cies into the parser?s statistical model, and the parser of Collins
(1999), which uses WH traces and argument/modifier distinc-
tions.
capable of producing syntactic structures containing
all or nearly all of the information annotated in the
corpus.
In recent years there has been a growing inter-
est in getting more information from parsers than
just bare phrase trees. Blaheta and Charniak (2000)
presented the first method for assigning Penn func-
tional tags to constituents identified by a parser.
Pattern-matching approaches were used in (John-
son, 2002) and (Jijkoun, 2003) to recover non-local
dependencies in phrase trees. Furthermore, experi-
ments described in (Dienes and Dubey, 2003) show
that the latter task can be successfully addressed by
shallow preprocessing methods.
3 An Overview of the Method
In this section we give a high-level overview of our
method for transforming a parser?s output and de-
scribe the different steps of the process. In the ex-
periments we used the parsers described in (Char-
niak, 2000) and (Collins, 1999). For Collins? parser
the text was first POS-tagged using Ratnaparkhi?s
maximum enthropy tagger.
The training phase of the method consists in
learning which transformations need to be applied
to the output of a parser to make it as similar to the
treebank data as possible.
As a preliminary step (Step 0), we convert the
WSJ2 to a dependency corpus without losing the an-
notated information (functional tags, empty nodes,
non-local dependencies). The same conversion is
applied to the output of the parsers we consider. The
details of the conversion process are described in
Section 4 below.
The training then proceeds by comparing graphs
derived from a parser?s output with the graphs
from the dependency corpus, detecting various mis-
matches, such as incorrect arc labels and missing
nodes or arcs. Then the following steps are taken to
fix the mismatches:
Step 1: changing arc labels
Step 2: adding new nodes
Step 3: adding new arcs
Obviously, other modifications are possible, such
as deleting arcs or moving arcs from one node to
another. We leave these for future work, though,
and focus on the three transformations mentioned
above.
The dependency corpus was split into training
(WSJ sections 02?21), development (sections 00?
2Thoughout the paper WSJ refers to the Penn Treebank II
Wall Street Journal corpus.
01) and test (section 23) corpora. For each of the
steps 1, 2 and 3 we proceed as follows:
1. compare the training corpus to the output of the
parser on the strings of the corpus, after apply-
ing the transformations of the previous steps
2. identify possible beneficial transformations
(which arc labels need to be changed or where
new nodes or arcs need to be added)
3. train a memory-based classifier to predict pos-
sible transformations given their context (i.e.,
information about the local structure of the
dependency graph around possible application
sites).
While the definitions of the context and application
site and the graph modifications are different for the
three steps, the general structure of the method re-
mains the same at each stage. Sections 6, 7 and 8
describe the steps in detail.
In the application phase of the method, we pro-
ceed similarly. First, the output of the parser is con-
verted to dependency graphs, and then the learners
trained during the steps 1, 2 and 3 are applied in
sequence to perform the graph transformations.
Apart from the conversion from phrase structures
to dependency graphs and the extraction of some
linguistic features for the learning, our method does
not use any information about the details of the tree-
bank annotation or the parser?s output: it works with
arbitrary labelled directed graphs.
4 Step 0: From Constituents to
Dependencies
To convert phrase trees to dependency structures,
we followed the commonly used scheme (Collins,
1999). The conversion routine,3 described below, is
applied both to the original WSJ structures and the
output of the parsers, though the former provides
more information (e.g., traces) which is used by the
conversion routine if available.
First, for the treebank data, all traces are resolved
and corresponding empty nodes are replaced with
links to target constituents, so that syntactic trees
become directed acyclic graphs. Second, for each
constituent we detect its head daughters (more than
one in the case of conjunction) and identify lexical
heads. Then, for each constituent we output new
dependencies between its lexical head and the lex-
ical heads of its non-head daughters. The label of
every new dependency is the constituent?s phrase
3Our converter is available at http://www.science.
uva.nl/?jijkoun/software.
(a)
S
NP?SBJ VP
to seek NP
seats
*?1
directors
NP?SBJ?1
this month
NP?TMP
VP
planned
S
(b)
VP
to seek NP
seats
VP
planned
S
directors
this month
      NP
     NP  S
(c)
planned
directors
VP|S
S|NP?SBJ
to
seek
seats
VP|NPmonth
 this
VP|TO
S|NP?TMP
NP|DT
S|NP?SBJ
(d)
planned
directors
VP|SS|NP
to
seek
seats
VP|NPmonth
 this
VP|TO
S|NP
NP|DT
Figure 1: Example of (a) the Penn Treebank WSJ annotation, (b) the output of Charniak?s parser, and the
results of the conversion to dependency structures of (c) the Penn tree and of (d) the parser?s output
label, stripped of all functional tags and coindex-
ing marks, conjoined with the label of the non-head
daughter, with its functional tags but without coin-
dexing marks. Figure 1 shows an example of the
original Penn annotation (a), the output of Char-
niak?s parser (b) and the results of our conversion of
these trees to dependency structures (c and d). The
interpretation of the dependency labels is straight-
forward: e.g., the label S   NP-TMP corresponds to
a sentence (S) being modified by a temporal noun
phrase (NP-TMP).
The core of the conversion routine is the selection
of head daughters of the constituents. Following
(Collins, 1999), we used a head table, but extended
it with a set of additional rules, based on constituent
labels, POS tags or, sometimes actual words, to ac-
count for situations where the head table alone gave
unsatisfactory results. The most notable extension
is our handling of conjunctions, which are often left
relatively flat in WSJ and, as a result, in a parser?s
output: we used simple pattern-based heuristics to
detect conjuncts and mark all conjuncts as heads of
a conjunction.
After the conversion, every resulting dependency
structure is modified deterministically:
 auxiliary verbs (be, do, have) become depen-
dents of corresponding main verbs (similar to
modal verbs, which are handled by the head ta-
ble);
 to fix a WSJ inconsistency, we move the -LGS
tag (indicating logical subject of passive in a
by-phrase) from the PP to its child NP.
5 Dependency-based Evaluation of
Parsers
After the original WSJ structures and the parsers?
outputs have been converted to dependency struc-
tures, we evaluate the performance of the parsers
against the dependency corpus. We use the standard
precision/recall measures over sets of dependencies
(excluding punctuation marks, as usual) and evalu-
ate Collins? and Charniak?s parsers on WSJ section
23 in three settings:
 on unlabelled dependencies;
 on labelled dependencies with only bare labels
(all functional tags discarded);
 on labelled dependencies with functional tags.
Notice that since neither Collins? nor Charniak?s
parser outputs WSJ functional labels, all dependen-
cies with functional labels in the gold parse will be
judged incorrect in the third setting. The evaluation
results are shown in Table 1, in the row ?step 0?.4
As explained above, the low numbers for the de-
pendency evaluation with functional tags are ex-
pected, because the two parsers were not intended
to produce functional labels.
Interestingly, the ranking of the two parsers is
different for the dependency-based evaluation than
for PARSEVAL: Charniak?s parser obtains a higher
PARSEVAL score than Collins? (89.0% vs. 88.2%),
4For meaningful comparison, the Collins? tags -A and -g
are removed in this evaluation.
Evaluation Parser unlabelled labelled with func. tagsP R f P R f P R f
after conversion Charniak 89.9 83.9 86.8 85.9 80.1 82.9 68.0 63.5 65.7
(step 0, Section 4) Collins 90.4 83.7 87.0 86.7 80.3 83.4 68.4 63.4 65.8
after relabelling Charniak 89.9 83.9 86.8 86.3 80.5 83.3 83.8 78.2 80.9
(step 1, Section 6) Collins 90.4 83.7 87.0 87.0 80.6 83.7 84.6 78.4 81.4
after adding nodes Charniak 90.1 85.4 87.7 86.5 82.0 84.2 84.1 79.8 81.9
(step 2, Section 7) Collins 90.6 85.3 87.9 87.2 82.1 84.6 84.9 79.9 82.3
after adding arcs Charniak 90.0 89.7 89.8 86.5 86.2 86.4 84.2 83.9 84.0
(step 3, Section 8) Collins 90.4 89.4 89.9 87.1 86.2 86.6 84.9 83.9 84.4
Table 1: Dependency-based evaluation of the parsers after different transformation steps
but slightly lower f-score on dependencies without
functional tags (82.9% vs. 83.4%).
To summarize the evaluation scores at this stage,
both parsers perform with f-score around 87%
on unlabelled dependencies. When evaluating on
bare dependency labels (i.e., disregarding func-
tional tags) the performance drops to 83%. The
new errors that appear when taking labels into ac-
count come from different sources: incorrect POS
tags (NN vs. VBG), different degrees of flatness of
analyses in gold and test parses (JJ vs. ADJP, or
CD vs. QP) and inconsistencies in the Penn anno-
tation (VP vs. RRC). Finally, the performance goes
down to around 66% when taking into account func-
tional tags, which are not produced by the parsers at
all.
6 Step 1: Changing Dependency Labels
Intuitively, it seems that the 66% performance on
labels with functional tags is an underestimation,
because much of the missing information is easily
recoverable. E.g., one can think of simple heuris-
tics to distinguish subject NPs, temporal PPs, etc.,
thus introducing functional labels and improving
the scores. Developing such heuristics would be
a very time consuming and ad hoc process: e.g.,
Collins? -A and -g tags may give useful clues for
this labelling, but they are not available in the out-
put of other parsers. As an alternative to hard-
coded heuristics, Blaheta and Charniak (2000) pro-
posed to recover the Penn functional tags automat-
ically. On the Penn Treebank, they trained a sta-
tistical model that, given a constituent in a parsed
sentence and its context (parent, grandparent, head
words thereof etc.), predicted the functional label,
possibly empty. The method gave impressive per-
formance, with 98.64% accuracy on all constituents
and 87.28% f-score for non-empty functional la-
bels, when applied to constituents correctly identi-
fied by Charniak?s parser. If we extrapolate these re-
sults to labelled PARSEVAL with functional labels,
the method would give around 87.8% performance
(98.64% of the ?usual? 89%) for Charniak?s parser.
Adding functional labels can be viewed as a
relabelling task: we need to change the labels
produced by a parser. We considered this more
general task, and used a different approach,
taking dependency graphs as input. We first
parsed the training part of our dependency tree-
bank (sections 02?21) and identified possible
relabellings by comparing dependencies output
by a parser to dependencies from the treebank.
E.g., for Collins? parser the most frequent rela-
bellings were S   NP   S   NP-SBJ, PP   NP-A   PP   NP,
VP   NP-A   VP   NP, S   NP-A   S   NP-SBJ and
VP   PP   VP   PP-CLR. In total, around 30% of
all the parser?s dependencies had different labels
in the treebank. We then learned a mapping from
the parser?s labels to those in the dependency
corpus, using TiMBL, a memory-based classifier
(Daelemans et al, 2003). The features used for
the relabelling were similar to those used by Bla-
heta and Charniak, but redefined for dependency
structures. For each dependency we included:
 the head (  ) and dependent (  ), their POS tags;
 the leftmost dependent of  and its POS;
 the head of
 (  ), its POS and the label of the
dependency

;
 the closest left and right siblings of  (depen-
dents of
 ) and their POS tags;
 the label of the dependency (   ) as derived
from the parser?s output.
When included in feature vectors, all dependency
labels were split at ? 	 ?, e.g., the label S   NP-A resulted
in two features: S and NP-A.
Testing was done as follows. The test corpus
(section 23) was also parsed, and for each depen-
dency a feature vector was formed and given to
TiMBL to correct the dependency label. After this
transformation the outputs of the parsers were eval-
uated, as before, on dependencies in the three set-
tings. The results of the evaluation are shown in
Table 1 (the row marked ?step 1?).
Let us take a closer look at the evaluation re-
sults. Obviously, relabelling does not change the
unlabelled scores. The 1% improvement for eval-
uation on bare labels suggests that our approach
is capable not only of adding functional tags, but
can also correct the parser?s phrase labels and part-
of-speech tags: for Collins? parser the most fre-
quent correct changes not involving functional la-
bels were NP   NN

NP   JJ and NP   JJ

NP   VBN, fix-
ing POS tagging errors. A very substantial increase
of the labelled score (from 66% to 81%), which is
only 6% lower than unlabelled score, clearly indi-
cates that, although the parsers do not produce func-
tional labels, this information is to a large extent im-
plicitly present in trees and can be recovered.
6.1 Comparison to Earlier Work
One effect of the relabelling procedure described
above is the recovery of Penn functional tags. Thus,
it is informative to compare our results with those
reported in (Blaheta and Charniak, 2000) for this
same task. Blaheta and Charniak measured tag-
ging accuracy and precision/recall for functional tag
identification only for constituents correctly identi-
fied by the parser (i.e., having the correct span and
nonterminal label). Since our method uses the de-
pendency formalism, to make a meaningful com-
parison we need to model the notion of a constituent
being correctly found by a parser. For a word   we
say that the constituent corresponding to its maxi-
mal projection is correctly identified if there exists

, the head of   , and for the dependency  

the
right part of its label (e.g., NP-SBJ for S   NP-SBJ) is
a nonterminal (i.e., not a POS tag) and matches the
right part of the label in the gold dependency struc-
ture, after stripping functional tags. Thus, the con-
stituent?s label and headword should be correct, but
not necessarily the span. Moreover, 2.5% of all con-
stituents with functional labels (246 out of 9928 in
section 23) are not maximal projections. Since our
method ignores functional tags of such constituents
(these tags disappear after the conversion of phrase
structures to dependency graphs), we consider them
as errors, i.e., reducing our recall value.
Below, the tagging accuracy, precision and recall
are evaluated on constituents correctly identified by
Charniak?s parser for section 23.
Method Accuracy P R f
Blaheta 98.6 87.2 87.4 87.3
This paper 94.7 90.2 86.9 88.5
The difference in the accuracy is due to two reasons.
First, because of the different definition of a cor-
rectly identified constituent in the parser?s output,
we apply our method to a greater portion of all la-
bels produced by the parser (95% vs. 89% reported
in (Blaheta and Charniak, 2000)). This might make
the task for out system more difficult. And second,
whereas 22% of all constituents in section 23 have a
functional tag, 36% of the maximal projections have
one. Since we apply our method only to labels of
maximal projections, this means that our accuracy
baseline (i.e., never assign any tag) is lower.
7 Step 2: Adding Missing Nodes
As the row labelled ?step 1? in Table 1 indicates,
for both parsers the recall is relatively low (6%
lower than the precision): while the WSJ trees,
and hence the derived dependency structures, con-
tain non-local dependencies and empty nodes, the
parsers simply do not provide this information. To
make up for this, we considered two further tran-
formations of the output of the parsers: adding new
nodes (corresponding to empty nodes in WSJ), and
adding new labelled arcs. This section describes the
former modification and Section 8 the latter.
As described in Section 4, when converting WSJ
trees to dependency structures, traces are resolved,
their empty nodes removed and new dependencies
introduced. Of the remaining empty nodes (i.e.,
non-traces), the most frequent in WSJ are: NP PRO,
empty units, empty complementizers, empty rela-
tive pronouns. To add missing empty nodes to de-
pendency graphs, we compared the output of the
parsers on the strings of the training corpus after
steps 0 and 1 (conversion to dependencies and re-
labelling) to the structures in the corpus itself. We
trained a classifier which, for every word in the
parser?s output, had to decide whether an empty
node should be added as a new dependent of the
word, and what its symbol (?*?, ?*U*? or ?0? in
WSJ), POS tag (always -NONE- in WSJ) and the
label of the new dependency (e.g., ?S   NP-SBJ? for
NP PRO and ?VP   SBAR? for empty complementiz-
ers) should be. This decision is conditioned on the
word itself and its context. The features used were:
 the word and its POS tag, whether the word
has any subject and object dependents, and
whether it is the head of a finite verb group;
 the same information for the word?s head (if
any) and also the label of the corresponding de-
pendency;
 the same information for the rightmost and
leftmost dependents of the word (if exist) along
with their dependency labels.
In total, we extracted 23 symbolic features for ev-
ery word in the corpus. TiMBL was trained on sec-
tions 02?21 and applied to the output of the parsers
(after steps 0 and 1) on the test corpus (section
23), producing a list of empty nodes to be inserted
in the dependency graphs. After insertion of the
empty nodes, the resulting structures were evaluated
against section 23 of the gold dependency treebank.
The results are shown in Table 1 (the row ?step 2?).
For both parsers the insertion of empty nodes im-
proves the recall by 1.5%, resulting in a 1% increase
of the f-score.
7.1 Comparison to Earlier Work
A procedure for empty node recovery was first de-
scribed in (Johnson, 2002), along with an evalua-
tion criterion: an empty node is correct if its cate-
gory and position in the sentence are correct. Since
our method works with dependency structures, not
phrase trees, we adopt a different but comparable
criterion: an empty node should be attached as a
dependent to the correct word, and with the correct
dependency label. Unlike the first metric, our cor-
rectness criterion also requires that possible attach-
ment ambiguities are resolved correctly (e.g., as in
the number of reports 0 they sent, where the empty
relative pronoun may be attached either to number
or to reports).
For this task, the best published results (using
Johnson?s metric) were reported by Dienes and
Dubey (2003), who used shallow tagging to insert
empty elements. Below we give the comparison to
our method. Notice that this evaluation does not in-
clude traces (i.e., empty elements with antecedents):
recovery of traces is described in Section 8.
Type
This paper Dienes&Dubey
P R f P R f
PRO-NP 73.1 63.89 68.1 68.7 70.4 69.5
COMP-SBAR 82.6 83.1 82.8 93.8 78.6 85.5
COMP-WHNP 65.3 40.0 49.6 67.2 38.3 48.8
UNIT 95.4 91.8 93.6 99.1 92.5 95.7
For comparison we use the notation of Dienes and
Dubey: PRO-NP for uncontrolled PROs (nodes ?*?
in the WSJ), COMP-SBAR for empty complemen-
tizers (nodes ?0? with dependency label VP   SBAR),
COMP-WHNP for empty relative pronouns (nodes
?0? with dependency label X   SBAR, where X
 
 VP)
and UNIT for empty units (nodes ?*U*?).
It is interesting to see that for empty nodes ex-
cept for UNIT both methods have their advantages,
showing better precision or better recall. Yet shal-
low tagging clearly performs better for UNIT.
8 Step 3: Adding Missing Dependencies
We now get to the third and final step of our trans-
formation method: adding missing arcs to depen-
dency graphs. The parsers we considered do not
explicitly provide information about non-local de-
pendencies (control, WH-extraction) present in the
treebank. Moreover, newly inserted empty nodes
(step 2, Section 7) might also need more links to the
rest of a sentence (e.g., the inserted empty comple-
mentizers). In this section we describe the insertion
of missing dependencies.
Johnson (2002) was the first to address recovery
of non-local dependencies in a parser?s output. He
proposed a pattern-matching algorithm: first, from
the training corpus the patterns that license non-
local dependencies are extracted, and then these pat-
terns are detected in unseen trees, dependencies be-
ing added when matches are found. Building on
these ideas, Jijkoun (2003) used a machine learning
classifier to detect matches. We extended Jijkoun?s
approach by providing the classifier with lexical in-
formation and using richer patterns with labels con-
taining the Penn functional tags and empty nodes,
detected at steps 1 and 2.
First, we compared the output of the parsers on
the strings of the training corpus after steps 0, 1 and
2 to the dependency structures in the training cor-
pus. For every dependency that is missing in the
parser?s output, we find the shortest undirected path
in the dependency graph connecting the head and
the dependent. These paths, connected sequences
of labelled dependencies, define the set of possible
patterns. For our experiments we only considered
patterns occuring more than 100 times in the train-
ing corpus. E.g., for Collins? parser, 67 different
patterns were found.
Next, from the parsers? output on the strings of
the training corpus, we extracted all occurrences of
the patterns, along with information about the nodes
involved. For every node in an occurrence of a pat-
tern we extracted the following features:
 the word and its POS tag;
 whether the word has subject and object depen-
dents;
 whether the word is the head of a finite verb
cluster.
We then trained TiMBL to predict the label of the
missing dependency (or ?none?), given an occur-
rence of a pattern and the features of all the nodes
involved. We trained a separate classifier for each
pattern.
For evaluation purposes we extracted all occur-
rences of the patterns and the features of their nodes
from the parsers? outputs for section 23 after steps
0, 1 and 2 and used TiMBL to predict and insert new
dependencies. Then we compared the resulting de-
pendency structures to the gold corpus. The results
are shown in Table 1 (the row ?step 3?). As ex-
pected, adding missing dependencies substantially
improves the recall (by 4% for both parsers) and
allows both parsers to achieve an 84% f-score on
dependencies with functional tags (90% on unla-
belled dependencies). The unlabelled f-score 89.9%
for Collins? parser is close to the 90.9% reported
in (Collins, 1999) for the evaluation on unlabelled
local dependencies only (without empty nodes and
traces). Since as many as 5% of all dependencies
in WSJ involve traces or empty nodes, the results in
Table 1 are encouraging.
8.1 Comparison to Earlier Work
Recently, several methods for the recovery of non-
local dependencies have been described in the lit-
erature. Johnson (2002) and Jijkoun (2003) used
pattern-matching on local phrase or dependency
structures. Dienes and Dubey (2003) used shallow
preprocessing to insert empty elements in raw sen-
tences, making the parser itself capable of finding
non-local dependencies. Their method achieves a
considerable improvement over the results reported
in (Johnson, 2002) and gives the best evaluation re-
sults published to date. To compare our results to
Dienes and Dubey?s, we carried out the transforma-
tion steps 0?3 described above, with a single mod-
ification: when adding missing dependencies (step
3), we only considered patterns that introduce non-
local dependencies (i.e., traces: we kept the infor-
mation whether a dependency is a trace when con-
verting WSJ to a dependency corpus).
As before, a dependency is correctly found if
its head, dependent, and label are correct. For
traces, this corresponds to the evaluation using the
head-based antecedent representation described in
(Johnson, 2002), and for empty nodes without an-
tecedents (e.g., NP PRO) this is the measure used
in Section 7.1. To make the results comparable to
other methods, we strip functional tags from the
dependency labels before label comparison. Be-
low are the overall precision, recall, and f-score for
our method and the scores reported in (Dienes and
Dubey, 2003) for antecedent recovery using Collins?
parser.
Method P R f
Dienes and Dubey 81.5 68.7 74.6
This paper 82.8 67.8 74.6
Interestingly, the overall performance of our post-
processing method is very similar to that of the
pre- and in-processing methods of Dienes and
Dubey (2003). Hence, for most cases, traces and
empty nodes can be reliably identified using only
local information provided by a parser, using the
parser itself as a black box. This is important, since
making parsers aware of non-local relations need
not improve the overall performance: Dienes and
Dubey (2003) report a decrease in PARSEVAL f-
score from 88.2% to 86.4% after modifying Collins?
parser to resolve traces internally, although this al-
lowed them to achieve high accuracy for traces.
9 Discussion
The experiments described in the previous sections
indicate that although statistical parsers do not ex-
plicitly output some information available in the
corpus they were trained on (grammatical and se-
mantic tags, empty nodes, non-local dependencies),
this information can be recovered with reasonably
high accuracy, using pattern matching and machine
learning methods.
For our task, using dependency structures rather
than phrase trees has several advantages. First, af-
ter converting both the treebank trees and parsers?
outputs to graphs with head?modifier relations, our
method needs very little information about the lin-
guistic nature of the data, and thus is largely corpus-
and parser-independent. Indeed, after the conver-
sion, the only linguistically informed operation is
the straightforward extraction of features indicating
the presence of subject and object dependents, and
finiteness of verb groups.
Second, using a dependency formalism facilitates
a very straightforward evaluation of the systems that
produce structures more complex than trees. It is
not clear whether the PARSEVAL evaluation can be
easily extended to take non-local relations into ac-
count (see (Johnson, 2002) for examples of such ex-
tension).
Finally, the independence from the details of the
parser and the corpus suggests that our method can
be applied to systems based on other formalisms,
e.g., (Hockenmaier, 2003), to allow a meaning-
ful dependency-based comparison of very different
parsers. Furthermore, with the fine-grained set of
dependency labels that our system provides, it is
possible to map the resulting structures to other de-
pendency formalisms, either automatically in case
annotated corpora exist, or with a manually devel-
oped set of rules. Our preliminary experiments with
Collins? parser and the corpus annotated with gram-
matical relations (Carroll et al, 2003) are promis-
ing: the system achieves 76% precision/recall f-
score, after the parser?s output is enriched with our
method and transformed to grammatical relations
using a set of 40 simple rules. This is very close
to the performance reported by Carroll et al (2003)
for the parser specifically designed for the extrac-
tion of grammatical relations.
Despite the high-dimensional feature spaces, the
large number of lexical features, and the lack of in-
dependence between features, we achieved high ac-
curacy using a memory-based learner. TiMBL per-
formed well on tasks where structured, more com-
plicated and task-specific statistical models have
been used previously (Blaheta and Charniak, 2000).
For all subtasks we used the same settings for
TiMBL: simple feature overlap measure, 5 nearest
neighbours with majority voting. During further ex-
periments with our method on different corpora, we
found that quite different settings led to a better per-
formance. It is clear that more careful and system-
atic parameter tuning and the analysis of the contri-
bution of different features have to be addressed.
Finally, our method is not restricted to syntac-
tic structures. It has been successfully applied
to the identification of semantic relations (Ahn et
al., 2004), using FrameNet as the training corpus.
For this task, we viewed semantic relations (e.g.,
Speaker, Topic, Addressee) as dependencies be-
tween a predicate and its arguments. Adding such
semantic relations to syntactic dependency graphs
was simply an additional graph transformation step.
10 Conclusions
We presented a method to automatically enrich the
output of a parser with information that is not pro-
vided by the parser itself, but is available in a tree-
bank. Using the method with two state of the art
statistical parsers and the Penn Treebank allowed
us to recover functional tags (grammatical and se-
mantic), empty nodes and traces. Thus, we are able
to provide virtually all information available in the
corpus, without modifying the parser, viewing it, in-
deed, as a black box.
Our method allows us to perform a meaningful
dependency-based comparison of phrase structure
parsers. The evaluation on a dependency corpus
derived from the Penn Treebank showed that, after
our post-processing, two state of the art statistical
parsers achieve 84% accuracy on a fine-grained set
of dependency labels.
Finally, our method for enriching the output of a
parser is, to a large extent, independent of a specific
parser and corpus, and can be used with other syn-
tactic and semantic resources.
11 Acknowledgements
We are grateful to David Ahn and Stefan Schlobach
and to the anonymous referees for their useful
suggestions. This research was supported by
grants from the Netherlands Organization for Scien-
tific Research (NWO) under project numbers 220-
80-001, 365-20-005, 612.069.006, 612.000.106,
612.000.207 and 612.066.302.
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and Maarten
de Rijke. 2004. The University of Amsterdam at
Senseval-3: semantic roles and logic forms. In Pro-
ceedings of the ACL-2004 Workshop on Evaluation of
Systems for the Semantic Analysis of Text.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Meeting of NAACL, pages 234?240.
John Carroll, Guido Minnen, and Ted Briscoe. 2003.
Parser evaluation using a grammatical relation anno-
tation scheme. In Anne Abeille?, editor, Building and
Using Parsed Corpora, pages 299?316. Kluwer.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of NAACL,
pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch, 2003. TiMBL: Tilburg
Memory Based Learner, version 5.0, Reference
Guide. ILK Technical Report 03-10. Available from
http://ilk.kub.nl/downloads/pub/papers/ilk0310.ps.gz.
Pe?ter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proceedings
of the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 33?40.
Julia Hockenmaier. 2003. Parsing with generative mod-
els of predicate-argument structure. In Proceedings of
the 41st Meeting of ACL, pages 359?366.
Valentin Jijkoun. 2003. Finding non-local dependen-
cies: Beyond pattern matching. In Proceedings of the
ACL-2003 Student Research Workshop, pages 37?43.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th meeting of ACL,
pages 136?143.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
The University of Amsterdam at Senseval-3:
Semantic Roles and Logic Forms
David Ahn Sisay Fissaha Valentin Jijkoun Maarten de Rijke
Informatics Institute, University of Amsterdam
Kruislaan 403
1098 SJ Amsterdam
The Netherlands
{ahn,sfissaha,jijkoun,mdr}@science.uva.nl
Abstract
We describe our participation in two of the tasks or-
ganized within Senseval-3: Automatic Labeling of
Semantic Roles and Identification of Logic Forms
in English.
1 Introduction
This year (2004), Senseval, a well-established fo-
rum for the evaluation and comparison of word
sense disambiguation (WSD) systems, introduced
two tasks aimed at building semantic representa-
tions of natural language sentences. One task, Auto-
matic Labeling of Semantic Roles (SR), takes as its
theoretical foundation Frame Semantics (Fillmore,
1977) and uses FrameNet (Johnson et al, 2003) as
a data resource for evaluation and system develop-
ment. The definition of the task is simple: given
a natural language sentence and a target word in
the sentence, find other fragments (continuous word
sequences) of the sentence that correspond to ele-
ments of the semantic frame, that is, that serve as
arguments of the predicate introduced by the target
word.
For this task, the systems receive a sentence, a
target word, and a semantic frame (one target word
may belong to multiple frames; hence, for real-
world applications, a preliminary WSD step might
be needed to select an appropriate frame). The out-
put of a system is a list of frame elements, with their
names and character positions in the sentence. The
evaluation of the SR task is based on precision and
recall. For this year?s task, the organizers chose 40
frames from FrameNet 1.1, with 32,560 annnotated
sentences, 8,002 of which formed the test set.
The second task, Identification of Logic Forms
in English (LF), is based on the LF formalism de-
scribed in (Rus, 2002). The LF formalism is a sim-
ple logical form language for natural language se-
mantics with only predicates and variables; there
is no quantification or negation, and atomic predi-
cations are implicitly conjoined. Predicates corre-
spond directly to words and are composed of the
base form of the word, the part of speech tag, and a
sense number (corresponding to the WordNet sense
of the word as used). For the task, the system is
given sentences and must produce LFs. Word sense
disambiguation is not part of the task, so the pred-
icates need not specify WordNet senses. System
evaluation is based on precision and recall of pred-
icates and predicates together with all their argu-
ments as compared to a gold standard.
2 Syntactic Processing
For both tasks, SR and LF, the core of our systems
was the syntactic analysis module described in de-
tail in (Jijkoun and de Rijke, 2004). We only have
space here to give a short overview of the module.
Every sentence was part-of-speech tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
parsed using a state-of-the-art wide coverage phrase
structure parser (Collins, 1999). Both the tagger and
the parser are trained on the Penn Treebank Wall
Street Journal Corpus (WSJ in the rest of this paper)
and thus produce structures similar to those in the
Penn Treebank. Unfortunately, the parser does not
deliver some of the information available in WSJ
that is potentially useful for our two applications:
Penn functional tags (e.g., subject, temporal, closely
related, logical subject in passive) and non-local de-
pendencies (e.g., subject and object control, argu-
ment extraction in relative clauses). Our syntactic
module tries to compensate for this and make this
information explicit in the resulting syntactic analy-
ses.
As a first step, we converted phrase trees pro-
duced by the parser to dependency structures, by
detecting heads of constituents and then propagat-
ing the lexical head information up the syntactic
tree, similarly to (Collins, 1999). The resulting de-
pendency structures were labeled with dependency
labels derived from corresponding Penn phrase la-
bels: e.g., a verb phrase (VP) modified by a prepo-
sitional phrase (PP) resulted in a dependency with
label ?VP|PP?.
Then, the information available in the WSJ (func-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
VP
to seek NP
seats
VP
planned
S
directors
this month
      NP
     NP  S
planned
directors
VP|S
S|NP
S|NP
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
planned
directors
VP|SS|NP?SBJ
S|NP?TMP
S|NP?SBJ
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
(a) (b) (c)
Figure 1: Stages of the syntactic processing: (a) the parser?s output, (b) the result of conversion to a depen-
dency structure, (c) final output of our syntactic module
tional tags, non-local dependencies) was added to
dependency structures using Memory-Based Learn-
ing (Daelemans et al, 2003): we trained the learner
to change dependency labels, or add new nodes or
arcs to dependency structures. Trained and tested
on WSJ, our system achieves state-of-the-art perfor-
mance for recovery of Penn functional tags and non-
local dependencies (Jijkoun and de Rijke, 2004).
Figure 1 shows three stages of the syntactic anal-
ysis of the sentence Directors this month planned to
seek seats (a simplified actual sentence from WSJ):
(a) the phrase structure tree produced by Collins?
parser, (b) the phrase structure tree converted to a
dependency structure and (c) the transformed de-
pendency structure with added functional tags and a
non-local dependency?the final output of our syn-
tactic module. Dependencies are shown as arcs
from heads to dependents.
3 Automatic Labeling of Semantic Roles
For the SR task, we applied a method very similar to
the one used in (Jijkoun and de Rijke, 2004) for re-
covering syntactic structures and somewhat similar
to the first method for automatic semantic role iden-
tification described in (Gildea and Jurafsky, 2002).
Essentially, our method consists of extracting possi-
ble syntactic patterns (paths in syntactic dependency
structures), introducing semantic relations from a
training corpus, and then using a machine learn-
ing classifier to predict which syntactic paths cor-
respond to which frame elements.
Our main assumption was that frame elements,
as annotated in FrameNet, correspond directly to
constituents (constituents being complete subtrees
of dependency structures). Similarly to (Gildea and
Jurafsky, 2002), our own evaluation showed that
about 15% of frame elements in FrameNet 1.1 do
not correspond to constituents, even when applying
some straighforward heuristics (see below) to com-
pensate for this mismatch. This observation puts an
upper bound of around 85% on the accuracy of our
system (with strict evaluation, i.e., if frame element
boundaries must match the gold standard exactly).
Note, though, that these 15% of ?erroneous? con-
stituents also include parsing errors.
Since the core of our SR system operates on
words, constituents, and dependencies, two im-
portant steps are the conversion of FrameNet el-
ements (continuous sequences of characters) into
head words of constituents, and vice versa. The con-
version of FrameNet elements is straightforward:
we take the head of a frame element to be the word
that dominates the most words of this element in
the dependency graph of the sentence. In the other
direction, when converting a subgraph of a depen-
dency graph dominated by a word w into a contin-
uous sequence of words, we take all (i.e., not only
immediate) dependents of w, ignoring non-local de-
pendencies, unless w is the target word of the sen-
tence, in which case we take the word w alone. This
latter heuristic helps us to handle cases when a noun
target word is a semantic argument of itself. Sev-
eral other simple heristics were also found helpful:
e.g., if the result of the conversion of a constituent to
a word sequence contains the target word, we take
only the words to the right of the target word.
With this conversion between frame elements and
constituents, the rest of our system only needs to
operate on words and labeled dependencies.
3.1 Training: the major steps
First, we extract from the training corpus
(dependency-parsed FrameNet sentences, with
words marked as targets and frame elements) all
shortest undirected paths in dependency graphs that
connect target words with their semantic arguments.
In this way, we collect all ?interesting? syntactic
paths from the training corpus.
In the second step, for all extracted syntactic
paths and again for all training sentences, we extract
all occurences of the paths (i.e., paths, starting from
a target word, that actually exist in the dependency
graph), recording for each such occurrence whether
it connects a target word to one of its semantic ar-
guments. For performance reasons, we consider for
each target word only syntactic paths extracted from
sentences annotated with respect to the same frame,
and we ignore all paths of length more than 3.
For every extracted occurrence, we record the
features describing the occurrence of a path in more
detail: the frame name, the path itself, the words
along the path (including the target word and the
possible head of a frame element?first and last
node of the path, respectively), their POS tags and
semantic classes. For nouns, the semantic class
of a word is defined as the hypernym of the first
sense of the noun in WordNet, one of 19 manu-
ally selected terms (animal, person, social group,
clothes, feeling, property, phenomenon, etc.) For
lexical adverbs and prepositions, the semantic class
is one of the 6 clusters obtained automatically using
the K-mean clustering algorithm on data extracted
from FrameNet. Examples of the clusters are:
(abruptly, ironically, slowly, . . . ), (above, beneath,
inside, . . . ), (entirely, enough, better, . . . ). The list
of WordNet hypernyms and the number of clusters
were chosen experimentally. We also added features
describing the subcategorization frame of the tar-
get word; this information is straightforwardly ex-
tracted from the dependency graph. In total, the sys-
tem used 22 features.
The set of path occurrences obtained in the sec-
ond step, with all the extracted features, is a pool of
positive and negative examples of whether certain
syntactic patterns correspond to any semantic argu-
ments. The pool is used as an instance base to train
TiMBL, a memory-based learner (Daelemans et al,
2003), to predict whether the endpoint of a syntac-
tic path starting at a target word corresponds to a
semantic argument, and if so, what its name is.
We chose TiMBL for this task because we had
previously found that it deals successfully with
complex feature spaces and data sparseness (in our
case, in the presence of many lexical features) (Ji-
jkoun and de Rijke, 2004). Moreover, TiMBL is
very flexible and implements many variants of the
basic k-nearest neighbor algorithm. We found that
tuning various parameters (the number of neigh-
bors, weighting and voting schemes) made substan-
tial differences in the performance of our system.
3.2 Applying the system
Once the training is complete, the system can be
applied to new sentences (with the indicated target
word and its frame) as follows. A sentence is parsed
and its dependency structure is built, as described in
Section 2. All occurences of ?interesting? syntac-
tic paths are extracted, along with their features as
described above. The resulting feature vectors are
fed to TiMBL to determine whether the endpoints
of the syntactic paths correspond to semantic argu-
ments of the target word. For the path occurences
classified positively, the constituents of their end-
points are converted to continuous word sequences,
as described earlier; in this case the system has de-
tected a frame element.
3.3 Results
During the development of our system, we used
only the 24,558 sentences from FrameNet set aside
for training by the SR task organizers. To tune the
system, this corpus was randomly split into training
and development sets (70% and 30%, resp.), evenly
for all target words. The official test set (8002 sen-
tences) was used only once to produce the submitted
run, with the whole training set (24,558 sentences)
used for training.
We submitted one run of the system (with iden-
tification of both element boundaries and element
names). Our official scores are: precision 86.9%,
recall 75.2% and overlap 84.7%. Our own evalua-
tion of the submitted run with the strict measures,
i.e., an element is considered correct only if both its
name and boundaries match the gold standard, gave
precision 73.5% and recall 63.6%.
4 Logic Forms
4.1 Method
For the LF task, it was straightforward to turn de-
pendency structures into LFs. Since the LF for-
malism does not attempt to represent the more sub-
tle aspects of semantics, such as quantification, in-
tensionality, modality, or temporality (Rus, 2002),
the primary information encoded in a LF is based
on argument structure, which is already well cap-
tured by the dependency parses. Our LF genera-
tor traverses the dependency structure, turning POS-
tagged lexical items into LF predicates, creating ref-
erential variables for nouns and verbs, and using
dependency labels to order the arguments for each
predicate. We make one change to the dependency
graphs originally produced by the parser. Instead of
taking coordinators, such as and, to modify the con-
stituents they coordinate, we take the coordinated
constituents to be arguments of the coordinator.
Our LF generator builds a labeled directed graph
from a dependency structure and traverses this
graph depth-first. In general, a well-formed depen-
dency graph has exactly one root node, which cor-
responds to the main verb of the sentence. Sen-
tences with multiple independent clauses may have
one root per clause. The generator begins traversing
the graph at one of these root nodes; if there is more
than one, it completes traversal of the subgraph con-
nected to the first node before going on to the next
node.
The first step in processing a node?producing an
LF predicate from the node?s lexical item?is taken
care of in the graph-building stage. We use a base
form dictionary to get the base form of the lexical
item and a simple mapping of Penn Treebank tags
into ?n?, ?v?, ?a?, and ?r? to get the suffix. For words
that are not tagged as nouns, verbs, adjectives, or
adverbs, the LF predicate is simply the word itself.
As the graph is traversed, the processing of a node
depends on its type. The greatest amount of pro-
cessing is required for a node corresponding to a
verb. First, a fresh referential variable is generated
as the event argument of the verbal predication. The
out-edges are then searched for nodes to process.
Since the order of arguments in an LF predication
is important and some sentence constitutents are ig-
nored for the purposes of LF, the out-edges are cho-
sen in order by label: first particles (?VP|PRT?),
then arguments (?S|NP-SBJ?, ?VP|NP?, etc.), and
finally adjuncts. We attempt to follow the argu-
ment order implicit in the description of LF given
in (Rus, 2002), and as the formalism requires, we
ignore auxiliary verbs and negation. The processing
of each of these arguments or adjuncts is handled re-
cursively and returns a set of predications. For mod-
ifiers, the event variable also has to be passed down.
For referential arguments and adjuncts, a referen-
tial variable also is returned to serve as an argument
for the verb?s LF predicate. Once all the arguments
and adjuncts have been processed, a new predica-
tion is generated, in which the verb?s LF predicate
is applied to the event variable and the recursively
generated referential variables. This new predica-
tion, along with the recursively generated ones, is
returned.
The processing of a nominal node proceeds sim-
ilarly. A fresh referential variable is generated?
since determiners are ignored in the LF formalism,
it is simply assumed that all noun phrases corre-
spond to a (possibly composite) individual. Out-
edges are examined for modifiers and recursively
processed. Both the referential variable and the set
of new predications are returned. Noun compounds
introduce some additional complexity; each modi-
fying noun introduces two additional variables, one
for the modifying noun and one for composite indi-
vidual realizing the compound. This latter variable
then replaces the referential variable for the head
noun.
Processing of other types of nodes proceeds in a
similar fashion. For modifiers such as adjectives,
adverbs, and prepositional phrases, a variable (cor-
responding to the individual or event being modi-
fied) is passed in, and the LF predicate of the node
is applied to this variable, rather than to a fresh
variable. In the case of prepositional phrases, the
predicate is applied to this variable and to the vari-
able corresponding to the object of the preposition,
which must be processed, as well. The latter vari-
able is then returned along with the new predica-
tions. For other modifiers, just the predications are
returned.
4.2 Development and results
The rules for handling dependency labels were writ-
ten by hand. Of the roughly 1100 dependency la-
bels that the parser assigns (see Section 2), our sys-
tem handles 45 labels, all of which fall within the
most frequent 135 labels. About 50 of these 135
labels are dependencies that can be ignored in the
generation of LFs (labels involving punctuation, de-
terminers, auxiliary verbs, etc.); of the remaining
85 labels, the 45 labels handled were chosen to pro-
vide reasonable coverage over the sample corpus
provided by the task organizers. Extending the sys-
tem is straightforward; to handle a dependency label
linking two node types, a rule matching the label
and invoking the dependent node handler is added
to the head node handler.
On the sample corpus of 50 sentences to which
our system was tuned, predicate identification, com-
pared to the provided LFs, including POS-tags, was
performed with 89.1% precision and 87.1% recall.
Argument identification was performed with 78.9%
precision and 77.4% recall. On the test corpus of
300 sentences, our official results, which exclude
POS-tags, were 82.0% precision and 78.4% recall
for predicate identification and 73.0% precision and
69.1% recall for argument identification.
We did not get the gold standard for the test cor-
pus in time to perform error analysis for our official
submission, but we did examine the errors in the
LFs we generated for the trial corpus. Most could
be traced to errors in the dependency parses, which
is unsurprising, since the generation of LFs from de-
pendency parses is relatively straightforward. A few
errors resulted from the fact that our system does not
try to identify multi-word compounds.
Some discrepancies between our LFs and the LFs
provided for the trial corpus arose from apparent
inconsistencies in the provided LFs. Verbs with
particles were a particular problem. Sometimes,
as in sentences 12 and 13 of the trial corpus, a
verb-particle combination such as look forward to
is treated as a single predicate (look forward to); in
other cases, such as in sentence 35, the verb and its
particle (go out) are treated as separate predicates.
Other inconsistencies in the provided LFs include
missing arguments (direct object in sentence 24),
and verbs not reduced to base form (felt, saw, and
found in sentences 34, 48, 50).
5 Conclusions
Our main finding during the development of the sys-
tems for the two Senseval tasks was that semantic
relations are indeed very close to syntactic depen-
dencies. Using deep dependency structures helped
to keep the manual rules for the LF task simple
and made the learning for the SR task easier. Also
we found that memory-based learning can be effi-
ciently applied to complex, highly structured prob-
lems such as the identification of semantic roles.
Our future work includes more accurate fine-
tuning of the learner for the SR task, extending the
coverage of the LF generator, and experimenting
with the generated LFs for question answering.
6 Acknowledgments
Ahn and De Rijke were supported by a grant from
the Netherlands Organization for Scientific Re-
search (NWO) under project number 612.066.302.
Fissaha, Jijkoun, and De Rijke were supported by a
grant from NWO under project number 220-80-001.
De Rijke was also supported by grants from NWO,
under project numbers 365-20-005, 612.069.006,
612.000.106, and 612.000.207.
References
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch, 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10. Available from
http://ilk.kub.nl/downloads/pub/papers/ilk0310.pdf.
C. J. Fillmore. 1977. The need for a frame semantics in
linguistics. Statistical Methods in Linguistics, 12:5?
29.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Proceed-
ings of ACL 2004.
C. Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-
penhofer, and C. Fillmore. 2003. Framenet: Theory
and practice. http://www.icsi.berkeley.edu/ framenet.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference.
V. Rus. 2002. Logic Form for WordNet Glosses. Ph.D.
thesis, Southern Methodist University.
Representing and Querying Multi-dimensional Markup
for Question Answering
Wouter Alink, Valentin Jijkoun, David Ahn, Maarten de Rijke
ISLA, University of Amsterdam
alink,jijkoun,ahn,mdr@science.uva.nl
Peter Boncz, Arjen de Vries
CWI, Amsterdam, The Netherlands
boncz,arjen@cwi.nl
Abstract
This paper describes our approach to rep-
resenting and querying multi-dimensional,
possibly overlapping text annotations, as
used in our question answering (QA) sys-
tem. We use a system extending XQuery,
the W3C-standard XML query language,
with new axes that allow one to jump eas-
ily between different annotations of the
same data. The new axes are formulated in
terms of (partial) overlap and containment.
All annotations are made using stand-off
XML in a single document, which can be
efficiently queried using the XQuery ex-
tension. The system is scalable to giga-
bytes of XML annotations. We show ex-
amples of the system in QA scenarios.
1 Introduction
Corpus-based question answering is a complex
task that draws from information retrieval, infor-
mation extraction and computational linguistics to
pinpoint information users are interested in. The
flexibility of natural language means that poten-
tial answers to questions may be phrased in differ-
ent ways?lexical and syntactic variation, ambi-
guity, polysemy, and anaphoricity all contribute to
a gap between questions and answers. Typically,
QA systems rely on a range of linguistic analyses,
provided by a variety of different tools, to bridge
this gap from questions to possible answers.
In our work, we focus on how we can integrate
the analyses provided by completely independent
linguistic processing components into a uniform
QA framework. On the one hand, we would like
to be able, as much as possible, to make use of
off-the-shelf NLP tools from various sources with-
out having to worry about whether the output of
the tools are compatible, either in a strong sense
of forming a single hierarchy or even in a weaker
sense of simply sharing common tokenization. On
the other hand, we would like to be able to issue
simple and clear queries that jointly draw upon an-
notations provided by different tools.
To this end, we store annotated data as stand-
off XML and query it using an extension of
XQuery with our new StandOff axes, inspired by
(Burkowski, 1992). Key to our approach is the use
of stand-off annotation at every stage of the anno-
tation process. The source text, or character data,
is stored in a Binary Large OBject (BLOB), and all
annotations, in a single XML document. To gen-
erate and manage the annotations we have adopted
XIRAF (Alink, 2005), a framework for integrating
annotation tools which has already been success-
fully used in digital forensic investigations.
Before performing any linguistic analysis, the
source documents, which may contain XMLmeta-
data, are split into a BLOB and an XML docu-
ment, and the XML document is used as the ini-
tial annotation. Various linguistic analysis tools
are run over the data, such as a named-entity tag-
ger, a temporal expression (timex) tagger, and syn-
tactic phrase structure and dependency parsers.
The XML document will grow during this analy-
sis phase as new annotations are added by the NLP
tools, while the BLOB remains intact. In the end,
the result is a fully annotated stand-off document,
and this annotated document is the basis for our
QA system, which uses XQuery extended with the
new axes to access the annotations.
The remainder of the paper is organized as fol-
lows. In Section 2 we briefly discuss related work.
Section 3 is devoted to the issue of querying multi-
dimensional markup. Then we describe how we
coordinate the process of text annotation, in Sec-
3
tion 4, before describing the application of our
multi-dimensional approach to linguistic annota-
tion to question answering in Section 5. We con-
clude in Section 6.
2 Related Work
XML is a tree structured language and provides
very limited capabilities for representing several
annotations of the same data simultaneously, even
when each of the annotations is tree-like. In par-
ticular, in the case of inline markup, multiple an-
notation trees can be put together in a single XML
document only if elements from different annota-
tions do not cross each other?s boundaries.
Several proposals have tried to circumvent this
problem in various ways. Some approaches are
based on splitting overlapping elements into frag-
ments. Some use SGML with the CONCUR fea-
ture or even entirely different markup schemes
(such as LMNL, the Layered Markup and An-
notation Language (Piez, 2004), or GODDAGs,
generalized ordered-descendant directed acyclic
graphs (Sperberg-McQueen and Huitfeldt, 2000))
that allow arbitrary intersections of elements from
different hierarchies. Some approaches use empty
XML elements (milestones) to mark beginnings
and ends of problematic elements. We refer to
(DeRose, 2004) for an in-depth overview.
Although many approaches solve the problem
of representing possibly overlapping annotations,
they often do not address the issue of accessing
or querying the resulting representations. This
is a serious disadvantage, since standard query
languages, such as XPath and XQuery, and stan-
dard query evaluation engines cannot be used with
these representations directly.
The approach of (Sperberg-McQueen and Huit-
feldt, 2000) uses GODDAGs as a conceptual
model of multiple tree-like annotations of the
same data. Operationalizing this approach,
(Dekhtyar and Iacob, 2005) describes a system
that uses multiple inline XML annotations of the
same text to build a GODDAG structure, which
can be queried using EXPath, an extension of
XPath with new axis steps.
Our approach differs from that of Dekhtyar and
Iacob in several ways. First of all, we do not use
multiple separate documents; instead, all annota-
tion layers are woven into a single XML docu-
ment. Secondly, we use stand-off rather than in-
line annotation; each character in the original doc-
ument is referred to by a unique offset, which
means that specific regions in a document can be
denoted unambiguously with only a start and an
end offset. On the query side, our extended XPath
axes are similar to the axes of Dekhtyar and Iacob,
but less specific: e.g., we do not distinguish be-
tween left-overlapping and right-overlapping char-
acter regions.
In the setting of question answering there
are a few examples of querying and retrieving
semistructured data. Litowski (Litkowksi, 2003;
Litkowksi, 2004) has been advocating the use of
an XML-based infrastructure for question answer-
ing, with XPath-based querying at the back-end,
for a number of years. Ogilvie (2004) outlines the
possibility of using multi-dimensional markup for
question answering, with no system or experimen-
tal results yet. Jijkoun et al (2005) describe initial
experiments with XQuesta, a question answering
system based on multi-dimensional markup.
3 Querying Multi-dimensional Markup
Our approach to markup is based on stand-off
XML. Stand-off XML is already widely used, al-
though it is often not recognized as such. It can
be found in many present-day applications, es-
pecially where annotations of audio or video are
concerned. Furthermore, many existing multi-
dimensional-markup languages, such as LMNL,
can be translated into stand-off XML.
We split annotated data into two parts: the
BLOB (Binary Large OBject) and the XML anno-
tations that refer to specific regions of the BLOB.
A BLOB may be an arbitrary byte string (e.g., the
contents of a hard drive (Alink, 2005)), and the
annotations may refer to regions using positions
such as byte offsets, word offsets, points in time
or frame numbers (e.g., for audio or video appli-
cations). In text-based applications, such as de-
scribed in this paper, we use character offsets. The
advantage of such character-based references over
word- or token-based ones is that it allows us to
reconcile possibly different tokenizations by dif-
ferent text analysis tools (cf. Section 4).
In short, a multi-dimensional document consists
of a BLOB and a set of stand-off XML annota-
tions of the BLOB. Our approach to querying such
documents extends the common XML query lan-
guages XPath and XQuery by defining 4 new axes
that allow one to move from one XML tree to an-
other. Until recently, there have been very few
4
AB
C
E
D
XML tree 1
XML tree 2
BLOB
(text characters)
Figure 1: Two annotations of the same data.
approaches to querying stand-off documents. We
take the approach of (Alink, 2005), which allows
the user to relate different annotations using con-
tainment and overlap conditions. This is done us-
ing the new StandOff XPath axis steps that we add
to the XQuery language. This approach seems to
be quite general: in (Alink, 2005) it is shown that
many of the query scenarios given in (Iacob et al,
2004) can be easily handled by using these Stand-
Off axis steps.
Let us explain the axis steps by means of an
example. Figure 1 shows two annotations of the
same character string (BLOB), where the first
XML annotation is
<A start="10" end="50">
<B start="30" end="50"/>
</A>
and the second is
<E start="20" end="60">
<C start="20" end="40"/>
<D start="55" end="60">
</E>
While each annotation forms a valid XML tree and
can be queried using standard XML query lan-
guages, together they make up a more complex
structure.
StandOff axis steps, inspired by (Burkowski,
1992), allow for querying overlap and contain-
ment of regions, but otherwise behave like reg-
ular XPath steps, such as child (the step be-
tween A and B in Figure 1) or sibling (the step
between C and D). The new StandOff axes, de-
noted with select-narrow, select-wide,
reject-narrow, and reject-wide select
contained, overlapping, non-contained and non-
overlapping region elements, respectively, from
possibly distinct layers of XML annotation of the
data. Table 1 lists some examples for the annota-
tions of our example document.
In XPath, the new axis steps are used in exactly
the same way as the standard ones. For example,
Context Axis Result nodes
A select-narrow B C
A select-wide B C E
A reject-narrow E D
A reject-wide D
Table 1: Example annotations.
the XPath query:
//B/select-wide::*
returns all nodes that overlap with the span of a
B node: in our case the nodes A, B, C and E. The
query:
//*[./select-narrow::B]
returns nodes that contain the span of B: in our
case, the nodes A and E.
In implementing the new steps, one of our de-
sign decisions was to put all stand-off annotations
in a single document. For this, an XML processor
is needed that is capable of handling large amounts
of XML. We have decided to use MonetDB/X-
Query, an XQuery implementation that consists of
the Pathfinder compiler, which translates XQuery
statements into a relational algebra, and the re-
lational database MonetDB (Grust, 2002; Boncz,
2002).
The implementation of the new axis steps in
MonetDB/XQuery is quite efficient. When the
XMark benchmark documents (XMark, 2006)
are represented using stand-off notation, query-
ing with the StandOff axis steps is interactive for
document size up to 1GB. Even millions of re-
gions are handled efficiently. The reason for the
speed of the StandOff axis steps is twofold. First,
they are accelerated by keeping a database in-
dex on the region attributes, which allows fast
merge-algorithms to be used in their evaluation.
Such merge-algorithms make a single linear scan
through the index to compute each StandOff
step. The second technical innovation is ?loop-
lifting.? This is a general principle inMonetDB/X-
Query(Boncz et al, 2005) for the efficient execu-
tion of XPath steps that occur nested in XQuery
iterations (i.e., inside for-loops). A naive strategy
would invoke the StandOff algorithm for each it-
eration, leading to repeated (potentially many) se-
quential scans. Loop-lifted versions of the Stand-
Off algorithms, in contrast, handle all iterations to-
gether in one sequential scan, keeping the average
complexity of the StandOff steps linear.
5
The StandOff axis steps are part of release
0.10 of the open-source MonetDB/XQuery prod-
uct, which can be downloaded from http://
www.monetdb.nl/XQuery.
In addition to the StandOff axis steps, a key-
word search function has been added to the
XQuery system to allow queries asking for re-
gions containing specific words. This function
is called so-contains($node, $needle)
which will return a boolean specifying whether
$needle occurs in the given region represented
by the element $node.
4 Combining Annotations
In our QA application of multi-dimensional
markup, we work with corpora of newspaper arti-
cles, each of which comes with some basic anno-
tation, such as title, body, keywords, timestamp,
topic, etc. We take this initial annotation structure
and split it into raw data, which comprises all tex-
tual content, and the XML markup. The raw data
is the BLOB, and the XML annotations are con-
verted to stand-off format. To each XML element
originally containing textual data (now stored in
the BLOB), we add a start and end attribute
denoting its position in the BLOB.
We use a separate system, XIRAF, to coordi-
nate the process of automatically annotating the
text. XIRAF (Figure 2) combines multiple text
processing tools, each having an input descriptor
and a tool-specific wrapper that converts the tool
output into stand-off XML annotation. Figure 3
shows the interaction of XIRAF with an automatic
annotation tool using a wrapper.
The input descriptor associated with a tool is
used to select regions in the data that are candi-
dates for processing by that tool. The descrip-
tor may select regions on the basis of the original
metadata or annotations added by other tools. For
example, both our sentence splitter and our tempo-
ral expression tagger use original document meta-
data to select their input: both select document
text, with //TEXT. Other tools, such as syntac-
tic parsers and named-entity taggers, require sep-
arated sentences as input and thus use the output
annotations of the sentence splitter, with the input
descriptor //sentence. In general, there may
be arbitrary dependencies between text-processing
tools, which XIRAF takes into account.
In order to add the new annotations generated
by a tool to the original document, the output of
the tool must be represented using stand-off XML
annotation of the input data. Many text process-
ing tools (e.g., parsers or part-of-speech taggers)
do not produce XML annotation per se, but their
output can be easily converted to stand-off XML
annotation. More problematically, text process-
ing tools may actually modify the input text in the
course of adding annotations, so that the offsets
referenced in the new annotations do not corre-
spond to the original BLOB. Tools make a vari-
ety of modifications to their input text: some per-
form their own tokenization (i.e., inserting whites-
paces or other word separators), silently skip parts
of the input (e.g., syntactic parsers, when the pars-
ing fails), or replace special symbols (e.g., paren-
theses with -LRB- and -RRB-). For many of the
available text processing tools, such possible mod-
ifications are not fully documented.
XIRAF, then, must map the output of a process-
ing tool back to the original BLOB before adding
the new annotations to the original document. This
re-alignment of the output of the processing tools
with the original BLOB is one of the major hur-
dles in the development of our system. We ap-
proach the problems systematically. We compare
the text data in the output of a given tool with the
data that was given to it as input and re-align in-
put and output offsets of markup elements using
an edit-distance algorithm with heuristically cho-
sen weights of character edits. After re-aligning
the output with the original BLOB and adjusting
the offsets accordingly, the actual data returned by
the tool is discarded and only the stand-off markup
is added to the existing document annotations.
5 Question Answering
XQuesta, our corpus-based question-answering
system for English and Dutch, makes use of the
multi-dimensional approach to linguistic annota-
tion embodied in XIRAF. The system analyzes an
incoming question to determine the required an-
swer type and keyword queries for retrieving rel-
evant snippets from the corpus. From these snip-
pets, candidate answers are extracted, ranked, and
returned.
The system consults Dutch and English news-
paper corpora. Using XIRAF, we annotate the
corpora with named entities (including type infor-
mation), temporal expressions (normalized to ISO
values), syntactic chunks, and syntactic parses
(dependency parses for Dutch and phrase structure
6
;4XHVWD ;,5$))HDWXUH([WUDFWLRQ)UDPHZRUN;4XHU\6\VWHP
    	

 
 TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 53?60,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Learning to Transform Linguistic Graphs
Valentin Jijkoun and Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
jijkoun,mdr@science.uva.nl
Abstract
We argue in favor of the the use of la-
beled directed graph to represent various
types of linguistic structures, and illustrate
how this allows one to view NLP tasks as
graph transformations. We present a gen-
eral method for learning such transforma-
tions from an annotated corpus and de-
scribe experiments with two applications
of the method: identification of non-local
depenencies (using Penn Treebank data)
and semantic role labeling (using Propo-
sition Bank data).
1 Introduction
Availability of linguistically annotated corpora such
as the Penn Treebank (Bies et al, 1995), Proposition
Bank (Palmer et al, 2005), and FrameNet (John-
son et al, 2003) has stimulated much research on
methods for automatic syntactic and semantic anal-
ysis of text. Rich annotations of corpora has al-
lowed for the development of techniques for recov-
ering deep linguistic structures: syntactic non-local
dependencies (Johnson, 2002; Hockenmaier, 2003;
Dienes, 2004; Jijkoun and de Rijke, 2004) and se-
mantic arguments (Gildea, 2001; Pradhan et al,
2005; Toutanova et al, 2005; Giuglea andMoschitti,
2006). Most state-of-the-art methods for the latter
two tasks use a cascaded architecture: they employ
syntactic parsers and re-cast the corresponding tasks
as pattern matching (Johnson, 2002) or classifica-
tion (Pradhan et al, 2005) problems. Other meth-
ods (Jijkoun and de Rijke, 2004) use combinations
of pattern matching and classification.
The method presented in this paper belongs to
the latter category. Specifically, we propose (1) to
use a flexible and expressive graph-based represen-
tation of linguistic structures at different levels; and
(2) to view NLP tasks as graph transformation prob-
lems: namely, problems of transforming graphs of
one type into graphs of another type. An exam-
ple of such a transformation is adding a level of
the predicate argument structure or semantic argu-
ments to syntactically annotated sentences. Further-
more, we describe a general method to automati-
cally learn such transformations from annotated cor-
pora. Our method combines pattern matching on
graphs and machine learning (classification) and can
be viewed as an extension of the Transformation-
Based Learning paradigm (Brill, 1995). After de-
scribing the method for learning graph transforma-
tions we demonstrate its applicability on two tasks:
identification of non-local dependencies (using Penn
Treebank data) and semantic roles labeling (using
Proposition Bank data).
The paper is organized as follows. In Section 2
we give our motivations for using graphs to encode
linguistic data. In Section 3 we describe our method
for learning graph transformations and in Section 4
we report on experiments with applications of our
method. We conclude in Section 5.
2 Graphs for linguistic structures and
language processing tasks
Trees and graphs are natural and common ways of
encoding linguistic information, in particular, syn-
53
VP
to seek NP
seats
VP
planned
S
directors S
NP?SBJthis month
NP?TMP
*
NP?SBJ
Figure 1: Local and non-local syntantic relations.
VP
using NP
S
stoppedLorillard Inc
in
cigarette filters
NP in NP
1956
ARG0 head ARG1 ARGM
feature=TMP
pred
crocidolite
S
PP PP
NP VP
Figure 2: Syntactic structure and semantic roles.
tactic structures (phrase trees, dependency struc-
tures). In this paper we use node- and edge-labeled
directed graphs as our representational formalism.
Figures 1 and 2 give informal examples of such rep-
resentations.
Figure 1 shows a graph encoding of the Penn
Treebank annotation of the local (solid edges) and
non-local (dashed edges) syntantic structure of the
sentence directors this month planned to seek more
seats. In this example, the co-indexing-based im-
plicit annotation of the non-local dependency (sub-
ject control) in the Penn Treebank (Bies et al, 1995)
is made explicit in the graph-based encoding.
Figure 2 shows a graph encoding of linguistic
structures for the sentence Lorillard Inc stopped us-
ing crocodolite in sigarette filters in 1956. Here,
solid lines correspond to surface syntactic structure,
produced by Charniak?s parser (Charniak, 2000),
and dashed lines are an encoding of the Proposition
Bank annotation of the semantic roles with respect
to the verb stopped.
Graph-based representations allow for a uniform
view on the linguistic structures on different layers.
An advantage of such a uniform view is that ap-
parently different NLP tasks can be considered as
VP
to seek NP
seats
VP
planned
S
directors S
this month
NP
NP
Figure 3: Output of a syntactic parser.
manipulations with graphs, in other words, as graph
transformation problems.
Consider the task of recovering non-local depen-
dencies (such as control, WH-extraction, topicaliza-
tion) in the surface syntactic phrase trees produced
by the state-of-the-art parser of (Charniak, 2000).
Figure 3 shows a graph-based encoding of the output
of the parser, and the task in question would consist
in transforming the graph in Figure 3 into the graph
in Figure 1. We notice that this transformation can
be realised as a sequence of independent and rela-
tively simple graph transformations: adding nodes
and edges to the graph or changing their labels (e.g.,
from NP to NP-SBJ).
Similarly, for the example in Figure 2, adding a
semantic layer (dashed edges) to the syntactic struc-
ture can also be seen as transforming a graph.
In general, we can view NLP tasks as adding ad-
ditional linguistic information to text, based on the
information already present: e.g., syntactic pars-
ing taking part-of-speech tagged sentences as in-
put (Collins, 1999), or anaphora resolution tak-
ing sequences of syntactically analysed and named-
entity-tagged sentences. If both input and output lin-
guistic structures are encoded as graphs, such NLP
tasks become graph transformation problems.
In the next section we describe our general
method for learning graph transformations from an
annotated corpus.
3 Learning graph transformations
We start with a few basic definitions. Similar
to (Schu?rr, 1997), we define ?emphgraph as a rela-
tional structure, i.e., a set of objects and relations
between them; we represent such structures as sets
of first-order logic atomic predicates defining nodes,
54
directed edges and their attributes (labels). Con-
stants used in the predicates represent objects (nodes
and edges) of graphs, as well as attribute names and
values. Atomic predicates node(?), edge(?, ?, ?) and
attr(?, ?, ?) define nodes, edges and their attributes.
We refer to (Schu?rr, 1997; Jijkoun, 2006) for formal
definitions and only illustrate these concepts with an
example. The following set of predicates:
node(n1), node(n2), edge(e, n1, n2),
attr(n1, label, Src), attr(n2, label,Dst)
defines a graph with two nodes, n1 and n2, hav-
ing labels Src and Dst (encoded as attributes named
label), and an (unlabelled) edge e going from n1 to
n2.
A pattern is an arbitrary graph and an occurence
of a pattern P in graph G is a total injective homo-
morphism ? from P to G, i.e., a mapping that asso-
ciates each object of P with one object G and pre-
serves the graph structure (relations between nodes,
edges, attribute names and values). We will also use
the term occurence to refer to the graph?(P ), a sub-
graph of G, the image of the mapping ? on P .
A graph rewrite rule is a triple r =
?lhsr, Cr, rhsr?: the left-hand side, the constraint
and the right-hand side of r, respectively, where lhsr
and rhsr are graphs and Cr is a function that returns
0 or 1 given a graphG, pattern lhsr and its occurence
in G (i.e., Cr specifies a constraint on occurences of
a pattern in a graph).
To apply a rewrite rule r = ?lhsr, Cr, rhsr? to
a graph G means finding all occurences of lhsr in
G for which Cr evaluates to 1, and replacing such
occurences of lhsr with occurences of rhsr. Effec-
tively, objects and relations present in lhsr but not in
rhsr will be removed from G, objects and relations
in rhsr but not in lhsr will be added to G, and com-
mon objects and relations will remain intact. Again,
we refer to (Jijkoun, 2006) for formal definitions.
As will be discussed below, our method for learn-
ing graph transformations is based on the ability to
compare pairs of graphs, identifying where the two
graphs are similar and where they differ. An align-
ment of two graphs is a partial one-to-one homomor-
phism between their nodes and edges, such that if
two edges of the two graphs are aligned, their re-
spective endpoints are aligned as well. A maximal
alignment of two graphs is an alignment that maxi-
mizes the sum of (1) the number of aligned objects
(nodes and edges), and (2) the number of match-
ing attribute values of all aligned objects. In other
words, a maximal alignment identifies as many sim-
ilarities between two graphs as possible. Given an
alignment of two graphs, it is possible to extract a
list of rewrite rules that can transform one graph into
another. For a maximal alignment such a list will
consist of rules with the smallest possible left- and
right-hand sides. See (Jijkoun, 2006) for details.
As stated above, we view NLP applications as
graph transformation modules. Our supervised
method for learning graph transformation requires
two corpora: input graphs In = {Ink} and corre-
sponding output graphs Out = {Outk}, such that
Outk is the desired output of the NLP module on
the input Ink.
The result of the method is an ordered list of graph
rewrite rules R = ?r1, . . . rn?, that can be applied in
sequence to input graphs to produce the output of the
NLP module.
Our method for learning graph transforma-
tions follows the structure of Transformation-Based
Learning (Brill, 1995) and proceeds iteratively, as
shown in Figure 4. At each iteration, we compare
and align pairs of input and output graphs, identify
possible rewrite rules and select rules with the most
frequent left-hand sides. For each selected rewrite
rule r, we extract all occurences of its left-hand
side and use them to train a two-class classifier im-
plementing the constraint Cr: the classifier, given
an encoding of an occurence of the left-hand side
predicts whether this particular occurence should
be replaced with the corresponding right-hand side.
When encoding an occurence as a feature vector, we
add as features all paths and all attributes of nodes
and edges in the one-edge neighborhood from the
nodes of the occurence. For the experiments de-
scribed in this paper we used the SVM Light classi-
fier (Joachims, 1999) with a standard linear kernel.
See (Jijkoun, 2006) for details.
4 Applications
Having presented a general method for learning
graph transformations, we now illustrate the method
at work and describe two applications to concrete
55
Compare
Apply
Extract rules
Aligned graphs
Compare
Apply
Extract rules
Aligned graphs
rules rulesrules
Ideal output graphsInput graphs
...
Iteration 1 Iteration 2 Iteration N
Compare
Apply
Extract rules
Aligned graphs
...
Figure 4: Structure of our method for learning graph transformations.
NLP problems: identification of non-local depen-
dencies (with the Penn Treebank data) and semantic
role labeling (with the Proposition Bank data).
4.1 Non-local dependencies
State-of-the-art statistical phrase structure parsers,
e.g., Charniak?s and Collins? parsers trained on
the Penn Treebank, produce syntactic parse trees
with bare phrase labels, (NP, PP, S, see Figure 3),
i.e., providing surface grammatical analysis of sen-
tences, even though the training corpus, the Penn
Treebank, is richer and contains additional gram-
matical and semantic information: it distinguishes
various types of modifiers, complements, subjects,
objects and annotates non-local dependencies, i.e.,
relations between phrases not adjacent in the parse
tree (see Figure 1). The task of recovering this in-
formation in the parser?s output has received a good
deal of attention. (Campbell, 2004) presents a rule-
based algorithm for empty node identification in
syntactic trees, competitive with the machine learn-
ing methods we mention next. In (Johnson, 2002)
a simple pattern-matching algorithm was proposed
for inserting empty nodes into syntactic trees, with
patterns extracted from the Penn Treebank. (Dienes,
2004) used a preprocessor that identified surface lo-
cation of empty nodes and a syntactic parser incor-
porating non-local dependencies into its probabilis-
tic model. (Jijkoun and de Rijke, 2004) described
an extension of the pattern-matching method with a
classifier trained on the dependency graphs derived
from the Penn Treebank data.
In order to apply our graph transformation method
to the task of identifying non-local dependencies,
we need to encode the information provided in the
Penn Treebank annotations and in the output of a
syntactic parser using directed labeled graphs. We
used a straightforward encoding of syntactic trees,
with nodes representing terminals and non-terminals
and edges defining the parent-child relationship. For
each node, we used the attribute type to specify
whether it is a terminal or a non-terminal. Ter-
minals corresponding to Penn empty nodes were
marked with the attribute empty = 1. For each
terminal (i.e., each word), the values of attributes
pos, word and lemma provided the part-of-speech tag,
the actual form and the lemma of the word. For
non-terminals, the attribute label contained the la-
bel of the corresponding syntactic phrase. The co-
indexing of empty nodes and non-terminals used in
the Penn Treebank to annotate non-local dependen-
cies was encoded using explicit edges with a distinct
type attribute, connecting empty nodes with their an-
tecedents (e.g., the dashed edge in Figure 1). For
each non-terminal node, its head child was marked
by attaching attribute head with value 1 to the corre-
56
sponding parent-child edge, and the lexical head of
each non-terminal was explicitly indicated using ad-
ditional edges with the attribute type = lexhead. We
used a heuristic method of (Collins, 1999) for head
identification.
When Penn Treebank sentences and the output of
the parser are encoded as directed labeled graphs
as described above, the task of identifying non-
local dependencies can be formulated as transform-
ing phrase structure graphs produced by a parser into
graphs of the type used in Penn Treebank annota-
tions.
We parsed the strings of the Penn Treebank with
Charniak?s parser and then used the data from sec-
tions 02?21 of the Penn Treebank for training: en-
coding of the parser?s output was used as the cor-
pus of input graphs for our learning method, and
the encoding of the original Penn annotations was
used as the corpus of output graphs. Similarly, we
used the data of sections 00?01 for development and
section 23 for testing. Using the input and output
corpora, we ran the learning method as described
above, at each iteration considering 20 most frequent
left-hand sides of rewrite rules. At each iteration,
the learned rewrite rules were applied to the current
training and development corpora to create a cor-
pus of input graphs for the next iteration (see Fig-
ure 4) and to estimate the performance of the system
at the current iteration. The system was evaluated
on the development corpus with respect to non-local
dependencies using the ?strict? evaluation measure
of (Johnson, 2002): the F1 score of precision and
recall of correctly identified empty nodes and an-
tecedents. If the absolute improvement of the F1
score for the evaluation measure was smaller than
0.1, the learning cycle was terminated, otherwise a
new iteration was started.
The learning cycle terminated after 12 iterations.
The resulting sequence of 12 ? 20 = 240 graph
rewrite rules was applied to the test corpus of in-
put graphs: Charniak?s parser output on the strings
of section 23 of the Penn Treebank. The result
was evaluated against the original annotations of the
Penn Treebank.
The results of the evaluation of the system on
empty nodes and non-local dependencies and the
PARSEVAL F1 score on local syntactic phrase
structure against the test corpus at each iteration are
Stage P R F1 PARSEVAL F1
Initial 0.0 0.0 0.0 88.7
1 88.2 38.6 53.7 88.4
2 87.2 48.6 62.5 88.4
3 87.5 51.9 65.2 88.4
4 86.7 52.1 65.1 88.4
5 86.1 56.3 68.1 88.3
6 86.0 57.2 68.7 88.4
7 86.3 61.3 71.7 88.4
8 86.6 63.4 73.2 88.4
9 86.7 64.6 74.0 88.4
10 86.7 64.9 74.2 88.4
11 86.6 65.1 74.3 88.4
12 86.7 65.2 74.4 88.4
Table 1: Evaluation of our method for identification
of empty nodes and their antecedents (12 first itera-
tions).
shown in Table 1.
As one can expect, at each iteration the method
extracts graph rewrite rules that introduce empty
nodes and non-local relations into syntactic struc-
tures, increasing the recall. The performance of the
final system (P/R/F1 = 86.7/65.2/74.4) for the task
of identifying non-local dependencies is compara-
ble to the performance of the best model of (Di-
enes, 2004): P/R/F1=82.5/70.1/75.8. The PARSE-
VAL score for the present system (88.4) is, however,
higher than the 87.3 for the system of Dienes.
Another effect of the learned transformations is
changing node labels of non-terminals, specifically,
modifying labels to include Penn functional tags
(e.g., changing NP in the input graph in Figure 3 to
NP-SBJ in the output graph in Figure 1). In fact, 17%
of all learned rewrite rules involved only changing
labels of non-terminal nodes. Analysis of the results
showed that the system is capable of assigning Penn
function tags to constituents produced by Charniak?s
parser with F1 = 91.4 (we use here the evalua-
tion measure of (Blaheta, 2004): the F1 score of the
precision and recall for assigning function tags to
constituents with surface spans correctly identified
by Charniak?s parser). Comparison to the evalua-
tion results of the function tagging method presented
in (Blaheta, 2004) is shown in Table 2.
The present system outperforms the system of
Blaheta on semantic tags such as -TMP or -MNR
marking temporal and manner adjuncts, respec-
tively, but performs worse on syntactic tags such
as -SBJ or -PRD marking subjects and predicatives,
57
(Blaheta, 2004) Here
Type Count P / R / F1 P / R / F1
All tags 8480 - 93.3 / 89.6 / 91.4
Syntactic 4917 96.5 / 95.3 / 95.9 95.4 / 95.5 / 95.5
Semantic 3225 86.7 / 80.3 / 83.4 89.7 / 82.5 / 86.0
Table 2: Evaluation of adding Penn Treebank func-
tion tags.
respectively. Note that the present method was not
specifically designed to add functional tags to con-
stituent labels. The method is not even ?aware? that
functional tags exists: it simply treats NP and NP-SBJ
as different labels and tries to correct labels compar-
ing input and output graphs in the training corpora.
In general, of the 240 graph rewrite rules ex-
tracted during the 12 iterations of the method, 25%
involved only one graph node in the left-hand side,
16% two nodes, 12% three nodes, etc. The two
most complicated extracted rewrite rules involved
left-hand sides with ten nodes.
We now switch to the second application of our
graph transformation method.
4.2 Semantic role labeling
Put very broadly, the task of semantic role labeling
consists in detecting and labeling simple predicates:
Who did what to whom, where, when, how, why, etc.
There is no single definition of a universal set of
semantic roles and moreover, different NLP appli-
cations may require different specificity of role la-
bels. In this section we apply the graph transforma-
tion method to the task of identification of semantic
roles as annotated in the Proposition Bank (Palmer
et al, 2005), PropBank for short. In PropBank, for
all verbs (except copular) of the syntactically anno-
tated sentences of the Wall Street Journal section of
the Penn Treebank, semantic arguments are marked
using references to the syntactic constituents of the
Penn Treebank. For the 49,208 syntactically anno-
tated sentences of the Penn Treebank, the PropBank
annotated 112,917 verb predicates (2.3 predicates
per sentence on average), with a total of 292,815 se-
mantic arguments (2.6 arguments per predicate on
average).
PropBank does not aim at cross-verb semantically
consistent labeling of arguments, but rather at anno-
tating the different ways arguments of a verb can
be realized syntactically in the corpus, which re-
sulted in the choice of theory-neutral numbered la-
bels (e.g., Arg0, Arg1, etc.) for semantic arguments.
Figure 2 shows an example of a PropBank annota-
tion (dashed edges).
In this section we address a specific NLP task:
identifying and labeling semantic arguments in the
output of a syntactic parser. For the example in
Figure 2 this task corresponds to adding ?semantic?
nodes and edges to the syntactic tree.
As before, in order to apply our graph transfor-
mation method, we need to encode the available in-
formation using graphs. Our encoding of syntactic
phrase structure is the same as in Section 4.1 and the
encoding of the semantic annotations of PropBank
is straightforward. For each PropBank predicate, a
new node with attributes type = propbank and label =
pred is added. Another node with label = head and
nodes for all semantic arguments of the predicate
(with labels indicating PropBank argument names)
are added and connected to the predicate node. Ar-
gument nodes with label ARGM (adjunct) addition-
ally have a feature attribute with values TMP, LOC,
etc., as specified in PropBank. The head node and
all argument nodes are linked to their respective syn-
tactic constituents, as specified in the PropBank an-
notation. All introduced semantic edges are marked
with the attribute type = propbank.
As before, we used section 02?21 of the Prop-
Bank (which annotates the same text as the Penn
Treebank) to train our graph transformation system,
section 00-01 for development and section 23 for
testing. We ran three experiments, taking three dif-
ferent corpora of input graphs:
1. the original syntactic structures of the Penn
Treebank containing function tags, empty
nodes, non-local dependencies, etc.;
2. the output of Charniak?s parser (i.e., bare syn-
tactic trees) on the strings of sections 02?21;
and
3. the output of Charniak?s parser processed
with the graph transformation system described
in 4.1.
For all three experiments we used the gold stan-
dard syntactic and semantic annotations from the
58
Penn Treebank Charniak Charniak +
Iter. P R P R P R
1 90.0 70.7 79.5 58.6 79.9 59.1
2 90.7 76.5 81.2 63.9 81.0 64.2
3 90.7 78.1 81.3 65.6 81.1 65.8
4 90.6 78.9 81.4 66.5 81.2 66.7
5 90.5 80.4 81.4 67.0 81.2 68.3
6 90.4 81.2 81.4 68.3 81.1 68.8
7 90.3 81.9 81.3 68.9 81.0 69.3
8 90.3 82.2 81.3 69.3 81.0 69.8
9 90.3 82.5 81.3 69.6 81.0 70.1
10 90.3 82.8 81.4 69.8 81.0 70.3
11 90.3 83.0 81.3 69.9 81.0 70.4
12 90.3 83.2
Table 3: Evaluation of our method for semantic role
identification with Propbank: with Charniak parses
and with parses processed by the system of Sec-
tion 4.1.
Penn Treebank and PropBank as the corpora of out-
put graphs (for the experiment with bare Charniak
parses, we dropped function tags, empty nodes and
non-local dependencies from the syntactic annota-
tion of the output graphs: we did not want our sys-
tem to start recovering these annotations, but were
interested in the identification of PropBank informa-
tion alone).
For each of the experiments, we used the corpora
of input and output graphs as before, at each itera-
tion extracting 20 rewrite rules with most frequent
left-hand sides, applying the rules to the develop-
ment data to measure the current performance of the
system. We stopped the learning in case the perfor-
mance improvement was less than a threshold and,
otherwise, continued the learning loop. As our per-
formance measure we used the F1 score of precision
and recall of the correctly identified and labeled non-
empty constituents?semantic arguments.
In all experiments, the learning stopped after 11
or 12 iterations. The results of the evaluation of the
system at each iteration on the test section of Prop-
Bank are shown in Table 3.
As one may expect, the performance of our se-
mantic role labeler is substantially higher on the
gold Penn Treebank syntactic structures than on the
parser?s output. Surprisingly, however, adding extra
information to the parser?s output (i.e., processing it
with the system of Section 4.1) does not significantly
improve the performance of the resulting system.
In Table 4 we compare our system for semantic
System P R F1
(Pradhan et al, 2005) 80.9 76.8 78.8
Here 81.0 70.4 75.3
Table 4: Evaluation of our methods for semantic role
identification with Propbank (12 first iterations).
roles labeling with the output of Charniak?s parser to
the state-of-the-art system of (Pradhan et al, 2005).
While showing good precision, our system per-
forms worse than state-of-the-art with respect to re-
call. Taking into account the iterative nature of
the method and imperfect rule selection criteria (we
simply take the most frequent left-hand sides), we
believe that it is the rule selection and learning termi-
nation condition that account for the relatively low
recall values. Indeed, in all three experiments de-
scribed above the learning loop stops while the recall
is still on the rise, albeit very slowly. It seems that
a more careful rule selection mechanism and loop
termination criteria are needed to address the recall
problem.
5 Conclusions
In this paper we argued that encoding diverse and
complex linguistic structures as directed labeled
graphs allows one to view many NLP tasks as graph
transformation problems. We proposed a general
method for learning graph transformation from an-
notated corpora and described experiments with two
NLP applications.
For the task of identifying non-local dependen-
cies and for function tagging our general method
demonstrates performance similar to the state-of-
the-art systems, designed specifically for these tasks.
For the PropBank semantic role labeling the method
shows a relatively low recall, which can be explained
by our sub-optimal ?rule of thumb? heuristics (such
as selecting 20 most frequent rewrite rules at each
iteration of the learning method). We see two ways
of avoiding such heuristics. First, one can define
and fine-tune the heuristics for each specific appli-
cation. Second, one can use more informed rewrite
rule selection methods, based on graph-based rela-
tional learning and frequent subgraph detection al-
gorithms (Cook and Holder, 2000; Yan and Han,
2002). Furthermore, more experiments are required
59
to see how the details of encoding linguistic in-
formation in graphs affect the performance of the
method.
Acknowledgements
This research was supported by the Netherlands
Organization for Scientific Research (NWO) un-
der project numbers 017.001.190, 220-80-001,
264-70-050, 354-20-005, 600.065.120, 612-13-
001, 612.000.106, 612.066.302, 612.069.006,
640.001.501, 640.002.501, and by the E.U. IST
programme of the 6th FP for RTD under project
MultiMATCH contract IST-033104.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style Penn Treebank project. Technical report, Uni-
versity of Pennsylvania.
Don Blaheta. 2004. Function Tagging. Ph.D. thesis,
Brown University.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Richard Campbell. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 645?653.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of NAACL,
pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Diane J. Cook and Lawrence B. Holder. 2000.
Graph-based data mining. IEEE Intelligent Systems,
15(2):32?41.
Pe?ter Dienes. 2004. Statistical Parsing with Non-local
Dependencies. Ph.D. thesis, Universita?t des Saarlan-
des, Saarbru?cken, Germany.
Daniel Gildea. 2001. Statistical Language Understand-
ing Using Frame Semantics. Ph.D. thesis, University
of California, Berkeley.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via framenet, verbnet and prop-
bank. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 929?936.
Julia Hockenmaier. 2003. Parsing with generative mod-
els of predicate-argument structure. In Proceedings of
the 41st Meeting of ACL, pages 359?366.
Valentin Jijkoun and Maarten de Rijke. 2004. Enrich-
ing the output of a parser using memory-based learn-
ing. In Proceedings of the 42nd Meeting of the Asso-
ciation for Computational Linguistics (ACL?04), Main
Volume, pages 311?318, Barcelona, Spain, July.
Valentin Jijkoun. 2006. Graph Transformations for Nat-
ural Language Processing. Ph.D. thesis, University of
Amsterdam.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT-Press.
Christopher R. Johnson, Miriam R. L. Petruck, Collin F.
Baker, Michael Ellsworth, Josef Ruppenhofer, and
Charles J. Fillmore. 2003. FrameNet: Theory and
Practice. http://www.icsi.berkeley.edu/
?framenet.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th meeting of ACL,
pages 136?143.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1).
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JimMar-
tin, and Dan Jurafsky. 2005. Semantic role label-
ing using different syntactic views. In Proceedings of
ACL-2005.
A. Schu?rr. 1997. Programmed graph replacement sys-
tems. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Transfor-
mation, chapter 7, pages 479?546.
Kristina Toutanova, Aria Haghighi, and Chris Manning.
2005. Joint learning improves semantic role labeling.
In Proceedings of the 43rd Meeting of the Association
for Computational Linguistics (ACL).
Xifeng Yan and Jiawei Han. 2002. gspan: Graph-based
substructure pattern mining. In Proceedings of the
2002 IEEE International Conference on Data Mining
(ICDM).
60
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 585?594,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Focused Topic-specific Sentiment Lexicons
Valentin Jijkoun Maarten de Rijke Wouter Weerkamp
ISLA, University of Amsterdam, The Netherlands
jijkoun,derijke,w.weerkamp@uva.nl
Abstract
We present a method for automatically
generating focused and accurate topic-
specific subjectivity lexicons from a gen-
eral purpose polarity lexicon that allow
users to pin-point subjective on-topic in-
formation in a set of relevant documents.
We motivate the need for such lexicons
in the field of media analysis, describe
a bootstrapping method for generating a
topic-specific lexicon from a general pur-
pose polarity lexicon, and evaluate the
quality of the generated lexicons both
manually and using a TREC Blog track
test set for opinionated blog post retrieval.
Although the generated lexicons can be an
order of magnitude more selective than the
general purpose lexicon, they maintain, or
even improve, the performance of an opin-
ion retrieval system.
1 Introduction
In the area of media analysis, one of the key
tasks is collecting detailed information about opin-
ions and attitudes toward specific topics from var-
ious sources, both offline (traditional newspapers,
archives) and online (news sites, blogs, forums).
Specifically, media analysis concerns the follow-
ing system task: given a topic and list of docu-
ments (discussing the topic), find all instances of
attitudes toward the topic (e.g., positive/negative
sentiments, or, if the topic is an organization or
person, support/criticism of this entity). For every
such instance, one should identify the source of
the sentiment, the polarity and, possibly, subtopics
that this attitude relates to (e.g., specific targets
of criticism or support). Subsequently, a (hu-
man) media analyst must be able to aggregate
the extracted information by source, polarity or
subtopics, allowing him to build support/criticism
networks etc. (Altheide, 1996). Recent advances
in language technology, especially in sentiment
analysis, promise to (partially) automate this task.
Sentiment analysis is often considered in the
context of the following two tasks:
? sentiment extraction: given a set of textual
documents, identify phrases, clauses, sen-
tences or entire documents that express atti-
tudes, and determine the polarity of these at-
titudes (Kim and Hovy, 2004); and
? sentiment retrieval: given a topic (and possi-
bly, a list of documents relevant to the topic),
identify documents that express attitudes to-
ward this topic (Ounis et al, 2007).
How can technology developed for sentiment
analysis be applied to media analysis? In order
to use a sentiment extraction system for a media
analysis problem, a system would have to be able
to determine which of the extracted sentiments are
actually relevant, i.e., it would not only have to
identify specific targets of all extracted sentiments,
but also decide which of the targets are relevant
for the topic at hand. This is a difficult task, as
the relation between a topic (e.g., a movie) and
specific targets of sentiments (e.g., acting or spe-
cial effects in the movie) is not always straight-
forward, in the face of ubiquitous complex lin-
guistic phenomena such as referential expressions
(?. . . this beautifully shot documentary?) or bridg-
ing anaphora (?the director did an excellent jobs?).
In sentiment retrieval, on the other hand, the
topic is initially present in the task definition, but
it is left to the user to identify sources and targets
of sentiments, as systems typically return a list
of documents ranked by relevance and opinion-
atedness. To use a traditional sentiment retrieval
system in media analysis, one would still have to
manually go through ranked lists of documents re-
turned by the system.
585
To be able to support media analysis, we need to
combine the specificity of (phrase- or word-level)
sentiment analysis with the topicality provided by
sentiment retrieval. Moreover, we should be able
to identify sources and specific targets of opinions.
Another important issue in the media analysis
context is evidence for a system?s decision. If the
output of a system is to be used to inform actions,
the system should present evidence, e.g., high-
lighting words or phrases that indicate a specific
attitude. Most modern approaches to sentiment
analysis, however, use various flavors of classifi-
cation, where decisions (typically) come with con-
fidence scores, but without explicit support.
In order to move towards the requirements of
media analysis, in this paper we focus on two of
the problems identified above: (1) pinpointing ev-
idence for a system?s decisions about the presence
of sentiment in text, and (2) identifying specific
targets of sentiment.
We address these problems by introducing a
special type of lexical resource: a topic-specific
subjectivity lexicon that indicates specific relevant
targets for which sentiments may be expressed; for
a given topic, such a lexicon consists of pairs (syn-
tactic clue, target). We present a method for au-
tomatically generating a topic-specific lexicon for
a given topic and query-biased set of documents.
We evaluate the quality of the lexicon both manu-
ally and in the setting of an opinionated blog post
retrieval task. We demonstrate that such a lexi-
con is highly focused, allowing one to effectively
pinpoint evidence for sentiment, while being com-
petetive with traditional subjectivity lexicons con-
sisting of (a large number of) clue words.
Unlike other methods for topic-specific senti-
ment analysis, we do not expand a seed lexicon.
Instead, we make an existing lexicon more fo-
cused, so that it can be used to actually pin-point
subjectivity in documents relevant to a given topic.
2 Related Work
Much work has been done in sentiment analy-
sis. We discuss related work in four parts: sen-
timent analysis in general, domain- and target-
specific sentiment analysis, product review mining
and sentiment retrieval.
2.1 Sentiment analysis
Sentiment analysis is often seen as two separate
steps for determining subjectivity and polarity.
Most approaches first try to identify subjective
units (documents, sentences), and for each of these
determine whether it is positive or negative. Kim
and Hovy (2004) select candidate sentiment sen-
tences and use word-based sentiment classifiers
to classify unseen words into a negative or posi-
tive class. First, the lexicon is constructed from
WordNet: from several seed words, the structure
of WordNet is used to expand this seed to a full
lexicon. Next, this lexicon is used to measure the
distance between unseen words and words in the
positive and negative classes. Based on word sen-
timents, a decision is made at the sentence level.
A similar approach is taken by Wilson et al
(2005): a classifier is learnt that distinguishes be-
tween polar and neutral sentences, based on a prior
polarity lexicon and an annotated corpus. Among
the features used are syntactic features. After this
initial step, the sentiment sentences are classified
as negative or positive; again, a prior polarity lexi-
con and syntactic features are used. The authors
later explored the difference between prior and
contextual polarity (Wilson et al, 2009): words
that lose polarity in context, or whose polarity is
reversed because of context.
Riloff and Wiebe (2003) describe a bootstrap-
ping method to learn subjective extraction pat-
terns that match specific syntactic templates, using
a high-precision sentence-level subjectivity clas-
sifier and a large unannotated corpus. In our
method, we bootstrap from a subjectivity lexi-
cion rather than a classifier, and perform a topic-
specific analysis, learning indicators of subjectiv-
ity toward a specific topic.
2.2 Domain- and target-specific sentiment
The way authors express their attitudes varies
with the domain: An unpredictable movie can be
positive, but unpredictable politicians are usually
something negative. Since it is unrealistic to con-
struct sentiment lexicons, or manually annotate
text for learning, for every imaginable domain or
topic, automatic methods have been developed.
Godbole et al (2007) aim at measuring over-
all subjectivity or polarity towards a certain entity;
they identify sentiments using domain-specific
lexicons. The lexicons are generated from man-
ually selected seeds for a broad domain such as
Health or Business, following an approach simi-
lar to (Kim and Hovy, 2004). All named entites
in a sentence containing a clue from a lexicon are
586
considered targets of sentiment for counting. Be-
cause of the data volume, no expensive linguistic
processing is performed.
Choi et al (2009) advocate a joint topic-
sentiment analysis. They identify ?sentiment top-
ics,? noun phrases assumed to be linked to a sen-
timent clue in the same expression. They address
two tasks: identifying sentiment clues, and clas-
sifying sentences into positive, negative, or neu-
tral. They start by selecting initial clues from Sen-
tiWordNet, based on sentences with known polar-
ity. Next, the sentiment topics are identified, and
based on these sentiment topics and the current list
of clues, new potential clues are extracted. The
clues can be used to classifiy sentences.
Fahrni and Klenner (2008) identify potential
targets in a given domain, and create a target-
specific polarity adjective lexicon. To this end,
they find targets using Wikipedia, and associated
adjectives. Next, the target-specific polarity of ad-
jectives is detemined using Hearst-like patterns.
Kanayama and Nasukawa (2006) introduce po-
lar atoms: minimal human-understandable syn-
tactic structures that specify polarity of clauses.
The goal is to learn new domain-specific polar
atoms, but these are not target-specific. They
use manually-created syntactic patterns to identify
atoms and coherency to determine polarity.
In contrast to much of the work in the literature,
we need to specialize subjectivity lexicons not for
a domain and target, but for ?topics.?
2.3 Product features and opinions
Much work has been carried out for the task of
mining product reviews, where the goal is to iden-
tify features of specific products (such as picture,
zoom, size, weight for digital cameras) and opin-
ions about these specific features in user reviews.
Liu et al (2005) describe a system that identifies
such features via rules learned from a manually
annotated corpus of reviews; opinions on features
are extracted from the structure of reviews (which
explicitly separate positive and negative opinions).
Popescu and Etzioni (2005) present a method
that identifies product features for using corpus
statistics, WordNet relations and morphological
cues. Opinions about the features are extracted us-
ing a hand-crafted set of syntactic rules.
Targets extracted in our method for a topic are
similar to features extracted in review mining for
products. However, topics in our setting go be-
yond concrete products, and the diversity and gen-
erality of possible topics makes it difficult to ap-
ply such supervised or thesaurus-based methods to
identify opinion targets. Moreover, in our method
we directly use associations between targets and
opinions to extract both.
2.4 Sentiment retrieval
At TREC, the Text REtrieval Conference, there
has been interest in a specific type of sentiment
analysis: opinion retrieval. This interest materi-
alized in 2006 (Ounis et al, 2007), with the opin-
ionated blog post retrieval task. Finding blog posts
that are not just about a topic, but also contain an
opinion on the topic, proves to be a difficult task.
Performance on the opinion-finding task is domi-
nated by performance on the underlying document
retrieval task (the topical baseline).
Opinion finding is often approached as a two-
stage problem: (1) identify documents relevant to
the query, (2) identify opinions. In stage (2) one
commonly uses either a binary classifier to distin-
guish between opinionated and non-opinionated
documents or applies reranking of the initial result
list using some opinion score. Opinion add-ons
show only slight improvements over relevance-
only baselines.
The best performing opinion finding system at
TREC 2008 is a two-stage approach using rerank-
ing in stage (2) (Lee et al, 2008). The authors
use SentiWordNet and a corpus-derived lexicon
to construct an opinion score for each post in an
initial ranking of blog posts. This opinion score
is combined with the relevance score, and posts
are reranked according to this new score. We de-
tail this approach in Section 6. Later, the authors
use domain-specific opinion indicators (Na et al,
2009), like ?interesting story? (movie review), and
?light? (notebook review). This domain-specific
lexicon is constructed using feedback-style learn-
ing: retrieve an initial list of documents and use
the top documents as training data to learn an opin-
ion lexicon. Opinion scores per document are then
computed as an average of opinion scores over
all its words. Results show slight improvements
(+3%) on mean average precision.
3 Generating Topic-Specific Lexicons
In this section we describe how we generate a lex-
icon of subjectivity clues and targets for a given
topic and a list of relevant documents (e.g., re-
587
Extract all 
syntactic contexts 
of clue words 
Background 
corpus
Topic-independent 
subjectivity lexicon
Relevant docs
Topic
For each clue 
word, select D 
contexts with 
highest entropy
List of syntactic clues:
(clue word, syn. context)
Extract all 
occurrences 
endpoints of 
syntactic clues 
Extract all 
occurrences 
endpoints of 
syntactic clues 
Potential targets in 
background corpus
Potential targets in 
relevant doc. list
Compare frequencies 
using chi-square; 
select top T targets
List of T targets
For each target, 
find syn. clues it 
co-occurs with
Topic-specific lexicon of tuples:
(syntactic clue, target)
Step 1
Step 2
Step 3
Figure 1: Our method for learning a topic-
dependent subjectivity lexicon.
trieved by a search engine for the topic). As an ad-
ditional resource, we use a large background cor-
pus of text documents of a similar style but with
diverse subjects; we assume that the relevant doc-
uments are part of this corpus as well. As the back-
ground corpus, we used the set of documents from
the assessment pools of TREC 2006?2008 opin-
ion retrieval tasks (described in detail in section 4).
We use the Stanford lexicalized parser1 to extract
labeled dependency triples (head, label, modifier).
In the extracted triples, all words indicate their cat-
egory (noun, adjective, verb, adverb, etc.) and are
normalized to lemmas.
Figure 1 provides an overview of our method;
below we describe it in more detail.
3.1 Step 1: Extracting syntactic contexts
We start with a general domain-independent prior
polarity lexicon of 8,821 clue words (Wilson et al,
2005). First, we identify syntactic contexts in
which specific clue words can be used to express
1http://nlp.stanford.edu/software/
lex-parser.shtml
attitude: we try to find how a clue word can be syn-
tactically linked to targets of sentiments. We take a
simple definition of the syntactic context: a single
labeled directed dependency relation. For every
clue word, we extract all syntactic contexts, i.e.,
all dependencies, in which the word is involved
(as head or as modifier) in the background corpus,
along with their endpoints. Table 1 shows exam-
ples of clue words and contexts that indicate sen-
timents. For every clue, we only select those con-
texts that exhibit a high entropy among the lemmas
at the other endpoint of the dependencies. E.g.,
in our background corpus, the verb to like occurs
97,179 times with a nominal subject and 52,904
times with a direct object; however, the entropy of
lemmas of the subjects is 4.33, compared to 9.56
for the direct objects. In other words, subjects of
like are more ?predictable.? Indeed, the pronoun
I accounts for 50% of subjects, followed by you
(14%), they (4%), we (4%) and people (2%). The
most frequent objects of like are it (12%), what
(4%), idea (2%), they (2%). Thus, objects of to
like will be preferred by the method.
Our entropy-driven selection of syntactic con-
texts of a clue word is based on the following as-
sumption:
Assumption 1: In text, targets of sentiments
are more diverse than sources of sentiments
or other accompanying attributes such as lo-
cation, time, manner, etc. Therefore targets
exhibit higher entropy than other attributes.
For every clue word, we select the top D syntac-
tic contexts whose entropy is at least half of the
maximum entropy for this clue.
To summarize, at the end of Step 1 of our
method, we have extracted a list of pairs (clue
word, syntactic context) such that for occurrences
of the clue word, the words at the endpoint of the
syntactic dependency are likely to be targets of
sentiments. We call such a pair a syntactic clue.
3.2 Step 2: Selecting potential targets
Here, we use the extracted syntantic clues to iden-
tify words that are likely to serve as specific tar-
gets for opinions about the topic in the relevant
documents. In this work we only consider individ-
ual words as potential targets and leave exploring
other options (e.g., NPs and VPs as targets) for fu-
ture work. In extracting targets, we rely on the
following assumption:
588
Clue word Syntactic context Target Example
to like has direct object u2 I do still like U2 very much
to like has clausal complement criticize I don?t like to criticize our intelligence services
to like has about-modifier olympics That?s what I like about Winter Olympics
terrible is adjectival modifier of idea it?s a terrible idea to recall judges for...
terrible has nominal subject shirt And Neil, that shirt is terrible!
terrible has clausal complement can It is terrible that a small group of extremists can . . .
Table 1: Examples of subjective syntactic contexts of clue words (based on Stanford dependencies).
Assumption 2: The list of relevant documents
contains a substantial number of documents
on the topic which, moreover, contain senti-
ments about the topic.
We extract all endpoints of all occurrences of the
syntactic clues in the relevant documents, as well
as in the background corpus. To identify potential
attitude targets in the relevant documents, we com-
pare their frequency in the relevant documents to
the frequency in the background corpus using the
standard ?2 statistics. This technique is based on
the following assumption:
Assumption 3: Sentiment targets related to
the topic occur more often in subjective con-
text in the set of relevant documents, than
in the background corpus. In other words,
while the background corpus contains senti-
ments towards very diverse subjects, the rel-
evant documents tend to express attitudes re-
lated to the topic.
For every potential target, we compute the ?2-
score and select the top T highest scoring targets.
As the result of Steps 1 and 2, as candidate tar-
gets for a given topic, we only select words that oc-
cur in subjective contexts, and that do so more of-
ten than we would normally expect. Table 2 shows
examples of extracted targets for three TREC top-
ics (see below for a description of our experimen-
tal data).
3.3 Step 3: Generating topic-specific lexicons
In the last step of the method, we combine clues
and targets. For each target identified in Step 2,
we take all syntactic clues extracted in Step 1 that
co-occur with the target in the relevant documents.
The resulting list of triples (clue word, syntactic
context, target) constitute the lexicon. We conjec-
ture that an occurrence of a lexicon entry in a text
indicates, with reasonable confidence, a subjective
attitude towards the target.
Topic ?Relationship between Abramoff and Bush?
abramoff lobbyist scandal fundraiser bush fund-raiser re-
publican prosecutor tribe swirl corrupt corruption norquist
democrat lobbying investigation scanlon reid lawmaker
dealings president
Topic ?MacBook Pro?
macbook laptop powerbook connector mac processor note-
book fw800 spec firewire imac pro machine apple power-
books ibook ghz g4 ata binary keynote drive modem
Topic: ?Super Bowl ads?
ad bowl commercial fridge caveman xl endorsement adver-
tising spot advertiser game super essential celebrity payoff
marketing publicity brand advertise watch viewer tv football
venue
Table 2: Examples of targets extracted at Step 2.
4 Data and Experimental Setup
We consider two types of evaluation. In the next
section, we examine the quality of the lexicons
we generate. In the section after that we evaluate
lexicons quantitatively using the TREC Blog track
benchmark.
For extrinsic evaluation we apply our lexi-
con generation method to a collection of doc-
uments containing opinionated utterances: blog
posts. The Blogs06 collection (Macdonald and
Ounis, 2006) is a crawl of blog posts from 100,649
blogs over a period of 11 weeks (06/12/2005?
21/02/2006), with 3,215,171 posts in total. Be-
fore indexing the collection, we perform two pre-
processing steps: (i) when extracting plain text
from HTML, we only keep block-level elements
longer than 15 words (to remove boilerplate mate-
rial), and (ii) we remove non-English posts using
TextCat2 for language detection. This leaves us
with 2,574,356 posts with 506 words per post on
average. We index the collection using Indri,3 ver-
sion 2.10.
TREC 2006?2008 came with the task of opin-
ionated blog post retrieval (Ounis et al, 2007).
For each year a set of 50 topics was created, giv-
2http://odur.let.rug.nl/?vannoord/
TextCat/
3http://www.lemurproject.org/indri/
589
ing us 150 topics in total. Every topic comes with
a set of relevance judgments: Given a topic, a blog
post can be either (i) nonrelevant, (ii) relevant, but
not opinionated, or (iii) relevant and opinionated.
TREC topics consist of three fields (title, descrip-
tion, and narrative), of which we only use the title
field: a query of 1?3 keywords.
We use standard TREC evaluation measures for
opinion retrieval: MAP (mean average precision),
R-precision (precision within the top R retrieved
documents, where R is the number of known rel-
evant documents in the collection), MRR (mean
reciprocal rank), P@10 and P@100 (precision
within the top 10 and 100 retrieved documents).
In the context of media analysis, recall-oriented
measures such as MAP and R-precision are more
meaningful than the other, early precision-oriented
measures. Note that for the opinion retrieval task
a document is considered relevant if it is on topic
and contains opinions or sentiments towards the
topic.
Throughout Section 6 below, we test for signif-
icant differences using a two-tailed paired t-test,
and report on significant differences for ? = 0.01
(N and H), and ? = 0.05 (M and O).
For the quantative experiments in Section 6 we
need a topical baseline: a set of blog posts po-
tentially relevant to each topic. For this, we use
the Indri retrieval engine, and apply the Markov
Random Fields to model term dependencies in the
query (Metzler and Croft, 2005) to improve topi-
cal retrieval. We retrieve the top 1,000 posts for
each query.
5 Qualitative Analysis of Lexicons
Lexicon size (the number of entries) and selectiv-
ity (how often entries match in text) of the gen-
erated lexicons vary depending on the parame-
ters D and T introduced above. The two right-
most columns of Table 4 show the lexicon size
and the average number of matches per topic. Be-
cause our topic-specific lexicons consist of triples
(clue word, syntactic context, target), they actu-
ally contain more words than topic-independent
lexicons of the same size, but topic-specific en-
tries are more selective, which makes the lexicon
more focused. Table 3 compares the application
of topic-independent and topic-specific lexicons to
on-topic blog text.
We manually performed an explorative error
analysis on a small number of documents, anno-
There are some tragic mo-
ments like eggs freezing ,
and predators snatching the
females and little ones-you
know the whole NATURE
thing ... but this movie is
awesome
There are some tragic mo-
ments l ike eggs freezing ,
and predators snatching the
females and little ones-you
know the whole NATURE
thing ... but this movie is
awesome
Saturday was more errands,
then spent the evening with
Dad and Stepmum, and fi-
nallywas able to see March
of the Penguins, which
was wonderful. Christmas
Day was lovely, surrounded
by family, good food and
drink, and little L to play
with.
Saturday was more errands,
then spent the evening with
Dad and Stepmum, and fi-
nally was able to see March
of the Penguins, which
was wonderful. Christmas
Day was lovely, surrounded
by family, good food and
drink, and little L to play
with.
Table 3: Posts with highlighted targets (bold) and
subjectivity clues (blue) using topic-independent
(left) and topic-specific (right) lexicons.
tated using the smallest lexicon in Table 4 for the
topic ?March of the Pinguins.? We assigned 186
matches of lexicon entries in 30 documents into
four classes:
? REL: sentiment towards a relevant target;
? CONTEXT: sentiment towards a target that
is irrelevant to the topic due to context (e.g.,
opinion about a target ?film?, but refering to
a film different from the topic);
? IRREL: sentiment towards irrelevant target
(e.g., ?game? for a topic about a movie);
? NOSENT: no sentiment at all
In total only 8% of matches were manually clas-
sified as REL, with 62% classified as NOSENT,
23% as CONTEXT, and 6% as IRREL. On the
other hand, among documents assessed as opio-
nionated by TREC assessors, only 13% did not
contain matches of the lexicon entries, compared
to 27% of non-opinionated documents, which
does indicate that our lexicon does attempt to sep-
arate non-opinionated documents from opinion-
ated.
6 Quantitative Evaluation of Lexicons
In this section we assess the quality of the gen-
erated topic-specific lexicons numerically and ex-
trinsically. To this end we deploy our lexicons to
the task of opinionated blog post retrieval (Ounis
et al, 2007). A commonly used approach to this
task works in two stages: (1) identify topically rel-
evant blog posts, and (2) classify these posts as
being opinionated or not. In stage 2 the standard
590
approach is to rerank the results from stage 1, in-
stead of doing actual binary classification. We take
this approach, as it has shown good performance
in the past TREC editions (Ounis et al, 2007) and
is fairly straightforward to implement. We also ex-
plore another way of using the lexicon: as a source
for query expansion (i.e., adding new terms to the
original query) in Section 6.2. For all experiments
we use the collection described in Section 4.
Our experiments have two goals: to compare
the use of topic-independent and topic-specific
lexicons for the opinionated post retrieval task,
and to examine how different settings for the pa-
rameters of the lexicon generation affect the em-
pirical quality.
6.1 Reranking using a lexicon
To rerank a list of posts retrieved for a given topic,
we opt to use the method that showed best per-
formance at TREC 2008. The approach taken
by Lee et al (2008) linearly combines a (top-
ical) relevance score with an opinion score for
each post. For the opinion score, terms from a
(topic-independent) lexicon are matched against
the post content, and weighted with the probability
of term?s subjectivity. Finally, the sum is normal-
ized using the Okapi BM25 framework. The final
opinion score Sop is computed as in Eq. 1:
Sop(D) =
Opinion(D) ? (k1 + 1)
Opinion(D) + k1 ? (1 ? b +
b?|D|
avgdl )
, (1)
where k1, and b are Okapi parameters (set to their
default values k1 = 2.0, and b = 0.75), |D| is the
length of document D, and avgdl is the average
document length in the collection. The opinion
score Opinion(D) is calculated using Eq. 2:
Opinion(D) =
?
w?O
P (sub|w) ? n(w,D), (2)
where O is the set of terms in the sentiment lex-
icon, P (sub|w) indicates the probability of term
w being subjective, and n(w,D) is the number of
times term w occurs in document D. The opinion
scoring can weigh lexicon terms differently, using
P (sub|w); it normalizes scores to cancel out the
effect of varying document sizes.
In our experiments we use the method de-
scribed above, and plug in the MPQA polarity
lexicon.4 We compare the results of using this
4http://www.cs.pitt.edu/mpqa/
topic-independent lexicon to the topic-dependent
lexicons our method generates, which are also
plugged into the reranking of Lee et al (2008).
In addition to using Okapi BM25 for opinion
scoring, we also consider a simpler method. As
we observed in Section 5, our topic-specific lexi-
cons are more selective than the topic-independent
lexicon, and a simple number of lexicon matches
can give a good indication of opinionatedness of a
document:
Sop(D) = min(n(O,D), 10)/10, (3)
where n(O,D) is the number of matches of the
term of sentiment lexicon O in document D.
6.1.1 Results and observations
There are several parameters that we can vary
when generating a topic-specific lexicon and when
using it for reranking:
D: the number of syntactic contexts per clue
T : the number of extracted targets
Sop(D): the opinion scoring function.
?: the weight of the opinion score in the linear
combination with the relevance score.
Note that ? does not affect the lexicon creation,
but only how the lexicon is used in reranking.
Since we want to assess the quality of lexicons,
not in the opinionated retrieval performance as
such, we factor out ? by selecting the best setting
for each lexicon (including the topic-independent)
and each evaluation measure.
In Table 4 we present the results of evaluation
of several lexicons in the context of opinionated
blog post retrieval.
First, we note that reranking using all lexi-
cons in Table 4 significantly improves over the
relevance-only baseline for all evaluation mea-
sures. When comparing topic-specific lexicons to
the topic-independent one, most of the differences
are not statistically significant, which is surpris-
ing given the fact that most topic-specific lexicons
we evaluated are substantially smaller (see the two
rightmost columns in the table). The smallest lex-
icon in Table 4 is seven times more selective than
the general one, in terms of the number of lexicon
matches per document.
The only evaluation measure where the topic-
independent lexicon consistently outperforms
topic-specific ones, is Mean Reciprocal Rank that
depends on a single relevant opinionated docu-
ment high in a ranking. A possible explanation
591
Lexicon MAP R-prec MRR P@10 P@100 |lexicon| hits per doc
no reranking 0.2966 0.3556 0.6750 0.4820 0.3666 ? ?
topic-independent 0.3182 0.3776 0.7714 0.5607 0.3980 8,221 36.17
D T Sop
3 50 count 0.3191 0.3769 0.7276O 0.5547 0.3963 2,327 5.02
3 100 count 0.3191 0.3777 0.7416 0.5573 0.3971 3,977 8.58
5 50 count 0.3178 0.3775 0.7246O 0.5560 0.3931 2,784 5.73
5 100 count 0.3178 0.3784 0.7316O 0.5513 0.3961 4,910 10.06
all 50 count 0.3167 0.3753 0.7264O 0.5520 0.3957 4,505 9.34
all 100 count 0.3146 0.3761 0.7283O 0.5347O 0.3955 8,217 16.72
all 50 okapi 0.3129 0.3713 0.7247H 0.5333O 0.3833O 4,505 9.34
all 100 okapi 0.3189 0.3755 0.7162H 0.5473 0.3921 8,217 16.72
all 200 okapi 0.3229N 0.3803 0.7389 0.5547 0.3987 14,581 29.14
Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topic-
independent lexicon. The two rightmost columns show the number of lexicon entries (average per topic)
and the number of matches of lexicon entries in blog posts (average for top 1,000 posts).
is that the large general lexicon easily finds a few
?obviously subjective? posts (those with heavily
used subjective words), but is not better at detect-
ing less obvious ones, as indicated by the recall-
oriented MAP and R-precision.
Interestingly, increasing the number of syntac-
tic contexts considered for a clue word (parame-
ter D) and the number of selected targets (param-
eter T ) leads to substantially larger lexicons, but
only gives marginal improvements when lexicons
are used for opinion retrieval. This shows that our
bootstrapping method is effective at filtering out
non-relevant sentiment targets and syntactic clues.
The evaluation results also show that the choice
of opinion scoring function (Okapi or raw counts)
depends on the lexicon size: for smaller, more fo-
cused lexicons unnormalized counts are more ef-
fective. This also confirms our intuition that for
small, focused lexicons simple presence of a sen-
timent clue in text is a good indication of subjec-
tivity, while for larger lexicons an overall subjec-
tivity scoring of texts has to be used, which can be
hard to interpret for (media analysis) users.
6.2 Query expansion with lexicons
In this section we evaluate the quality of targets
extracted as part of the lexicons by using them for
query expansion. Query expansion is a commonly
used technique in information retrieval, aimed at
getting a better representation of the user?s in-
formation need by adding terms to the original
retrieval query; for user-generated content, se-
lective query expansion has proved very benefi-
cial (Weerkamp et al, 2009). We hypothesize that
if our method manages to identify targets that cor-
respond to issues, subtopics or features associated
Run MAP P@10 MRR
Topical blog post retrieval
Baseline 0.4086 0.7053 0.7984
Rel. models 0.4017O 0.6867 0.7383H
Subj. targets 0.4190M 0.7373M 0.8470M
Opinion retrieval
Baseline 0.2966 0.4820 0.6750
Rel. models 0.2841H 0.4467H 0.5479H
Subj. targets 0.3075 0.5227N 0.7196
Table 5: Query expansion using relevance mod-
els and topic-specific subjectivity targets. Signifi-
cance tested against the baseline.
with the topic, the extracted targets should be good
candidates for query expansion. The experiments
described below test this hypothesis.
For every test topic, we select the 20 top-scoring
targets as expansion terms, and use Indri to re-
turn 1,000 most relevant documents for the ex-
panded query. We evaluate the resulting ranking
using both topical retrieval and opinionated re-
trieval measures. For the sake of comparison, we
also implemented a well-known query expansion
method based on Relevance Models (Lavrenko
and Croft, 2001): this method has been shown to
work well in many settings. Table 5 shows evalu-
ation results for these two query expansion meth-
ods, compared to the baseline retrieval run.
The results show that on topical retrieval query
expansion using targets significantly improves re-
trieval performance, while using relevance mod-
els actually hurts all evaluation measures. The
failure of the latter expansion method can be at-
tributed to the relatively large amount of noise
in user-generated content, such as boilerplate
592
material, timestamps of blog posts, comments
etc. (Weerkamp and de Rijke, 2008). Our method
uses full syntactic parsing of the retrieved doc-
uments, which might substantially reduce the
amount of noise since only (relatively) well-
formed English sentences are used in lexicon gen-
eration.
For opinionated retrieval, target-based expan-
sion also improves over the baseline, although the
differences are only significant for P@10. The
consistent improvement for topical retrieval sug-
gests that a topic-specific lexicon can be used both
for query expansion (as described in this section)
and for opinion reranking (as described in Sec-
tion 6.1). We leave this combination for future
work.
7 Conclusions and Future Work
We have described a bootstrapping method for de-
riving a topic-specific lexicon from a general pur-
pose polarity lexicon. We have evaluated the qual-
ity of generated lexicons both manually and using
a TREC Blog track test set for opinionated blog
post retrieval. Although the generated lexicons
can be an order of magnitude more selective, they
maintain, or even improve, the performance of an
opinion retrieval system.
As to future work, we intend to combine our
method with known methods for topic-specific
lexicon expansion (our method is rather concerned
with lexicon ?restriction?). Existing sentence-
or phrase-level (trained) sentiment classifiers can
also be used easily: when collecting/counting tar-
gets we can weigh them by ?prior? score provided
by such classifiers. We also want to look at more
complex syntactic patterns: Choi et al (2009) re-
port that many errors are due to exclusive use of
unigrams. We would also like to extend poten-
tial opinion targets to include multi-word phrases
(NPs and VPs), in addition to individual words.
Finally, we do not identify polarity yet: this can
be partially inherited from the initial lexicon and
refined automatically via bootstrapping.
Acknowledgements
This research was supported by the European
Union?s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430, by the DuOMAn project carried out
within the STEVIN programme which is funded
by the Dutch and Flemish Governments under
project nr STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project nrs 612.066.512, 612.061.814, 612.061.-
815, 640.004.802.
References
Altheide, D. (1996). Qualitative Media Analysis. Sage.
Choi, Y., Kim, Y., and Myaeng, S.-H. (2009). Domain-
specific sentiment analysis using contextual feature gen-
eration. In TSA ?09: Proceeding of the 1st international
CIKM workshop on Topic-sentiment analysis for mass
opinion, pages 37?44, New York, NY, USA. ACM.
Fahrni, A. and Klenner, M. (2008). Old Wine or Warm
Beer: Target-Specific Sentiment Analysis of Adjectives.
In Proc.of the Symposium on Affective Language in Hu-
man and Machine, AISB 2008 Convention, 1st-2nd April
2008. University of Aberdeen, Aberdeen, Scotland, pages
60 ? 63.
Godbole, N., Srinivasaiah, M., and Skiena, S. (2007). Large-
scale sentiment analysis for news and blogs. In Proceed-
ings of the International Conference on Weblogs and So-
cial Media (ICWSM).
Kanayama, H. and Nasukawa, T. (2006). Fully automatic lex-
icon expansion for domain-oriented sentiment analysis. In
EMNLP ?06: Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing, pages
355?363, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kim, S. and Hovy, E. (2004). Determining the sentiment of
opinions. In Proceedings of COLING 2004.
Lavrenko, V. and Croft, B. (2001). Relevance-based language
models. In SIGIR ?01: Proceedings of the 24th annual
international ACM SIGIR conference on research and de-
velopment in information retrieval.
Lee, Y., Na, S.-H., Kim, J., Nam, S.-H., Jung, H.-Y., and Lee,
J.-H. (2008). KLE at TREC 2008 Blog Track: Blog Post
and Feed Retrieval. In Proceedings of TREC 2008.
Liu, B., Hu, M., and Cheng, J. (2005). Opinion observer: an-
alyzing and comparing opinions on the web. In Proceed-
ings of the 14th international conference on World Wide
Web.
Macdonald, C. and Ounis, I. (2006). The TREC Blogs06
collection: Creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Metzler, D. and Croft, W. B. (2005). A markov random feld
model for term dependencies. In SIGIR ?05: Proceed-
ings of the 28th annual international ACM SIGIR con-
ference on research and development in information re-
trieval, pages 472?479, New York, NY, USA. ACM Press.
Na, S.-H., Lee, Y., Nam, S.-H., and Lee, J.-H. (2009). Im-
proving opinion retrieval based on query-specific senti-
ment lexicon. In ECIR ?09: Proceedings of the 31th Eu-
ropean Conference on IR Research on Advances in In-
formation Retrieval, pages 734?738, Berlin, Heidelberg.
Springer-Verlag.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 blog
track. In The Fifteenth Text REtrieval Conference (TREC
2006). NIST.
Popescu, A.-M. and Etzioni, O. (2005). Extracting prod-
uct features and opinions from reviews. In Proceedings
of Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP).
Riloff, E. and Wiebe, J. (2003). Learning extraction patterns
593
for subjective expressions. In Proceedings of the 2003
Conference on Empirical methods in Natural Language
Processing (EMNLP).
Weerkamp, W., Balog, K., and de Rijke, M. (2009). A gener-
ative blog post retrieval model that uses query expansion
based on external collections. In Joint conference of the
47th Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation of
Natural Language Processing (ACL-ICNLP 2009), Singa-
pore.
Weerkamp, W. and de Rijke, M. (2008). Credibility im-
proves topical blog post retrieval. In Proceedings of ACL-
08: HLT, page 923931, Columbus, Ohio. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT ?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Morristown, NJ, USA.
Association for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2009). Recog-
nizing contextual polarity: an exploration of features for
phrase-level sentiment analysis. Computational Linguis-
tics, 35(3):399?433.
594
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 17?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Mining User Experiences from Online Forums: An Exploration?
Valentin Jijkoun Maarten de Rijke
Wouter Weerkamp
ISLA, University of Amsterdam
Science Park 107
1098 XG Amsterdam, The Netherlands
jijkoun,m.derijke,w.weerkamp@uva.nl
Paul Ackermans Gijs Geleijnse
Philips Research Europe
High Tech Campus 34
5656 AE Eindhoven, The Netherlands
paul.ackermans@philips.com
gijs.geleijnse@philips.com
1 Introduction
Recent years have shown a large increase in the
usage of content creation platforms?blogs, com-
munity QA sites, forums, etc.?aimed at the gen-
eral public.User generated data contains emotional,
opinionated, sentimental, and personal posts. This
characteristic makes it an interesting data source
for exploring new types of linguistic analysis, as is
demonstrated by research on, e.g., sentiment analy-
sis [4], opinion retrieval [3], and mood detection [1].
We introduce the task of experience mining. Here,
the goal is to gain insights into criteria that people
formulate to judge or rate a product or its usage.
These criteria can be formulated as the expectations
that people have of the product in advance (i.e., the
reasons to buy), but can also be expressed as reports
of experiences while using the product and compar-
isons with other products. We focus on the latter:
reports of experiences with products. In this paper,
we define the task, describe guidelines for manual
annotation and analyze linguistic features that can
be used in an automatic experience mining system.
2 Motivation
Our main use-case is user-centered design for prod-
uct development. User-centered design [2] is an in-
novation paradigm where users of a product are in-
volved in each step of the research and development
process. The first stage of the product design process
is to identify unmet needs and demands of users for
a specific product or a class of products. Forums,
?This research was supported by project STE-09-12 within
the STEVIN programme funded by the Dutch and Flemish gov-
ernments, and by the Netherlands Organisation for Scientific
Research (NWO) under projects 640.001.501, 640.002.501,
612.066.512, 612.061.814, 612.061.815, 640.004.802.
review sites, and mailing lists are platforms where
people share experiences about a subject they care
about. Although statements found in such platforms
may not always be representative for the general user
group, they can accelerate user-centered design.
Another use-case comes from online communi-
ties themselves. Users of online forums are often in-
terested in other people?s experiences with concrete
products and/or solutions for specific problems. To
quote one such user: [t]he polls are the only in-
formation we have, though, except for individual
[users] giving their own evaluations. With the vol-
ume of online data increasing rapidly, users need im-
proved access to previously reported experiences.
3 Experience mining
Experiences are particular instances of personally
encountering or undergoing something. We want
to identify experiences about a specific target prod-
uct, that are personal, involve an activity related to
the target and, moreover, are accompanied by judge-
ments or evaluative statements. Experience mining
is related to sentiment analysis and opinion retrieval,
in that it involves identifying attitudes; the key dif-
ference is, however, that we are looking for attitudes
towards specific experiences with products, not atti-
tudes towards the products themselves.
4 An explorative study
To assess the feasibility of automatic experience
mining, we carried out an explorative study: we
asked human assessors to find experiences in ac-
tual forum data and then examined linguistic fea-
tures likely to be useful for identifying experiences
automatically.
17
Mean and deviation in posts
Feature with exper. without exper.
subjectivity score2 0.07 ?0.23 0.17 ?0.35
polarity score2 0.87 ?0.30 0.77 ?0.38
#words per post 102.57 ?80.09 52.46 ?53.24
#sentences per post 6.00 ?4.16 3.34 ?2.33
# words per sentence 17.07 ?4.69 15.71 ?7.61
#questions per post 0.32 ?0.63 0.54 ?0.89
p(post contains question) 0.25 ?0.43 0.33 ?0.47
#I?s per post 5.76 ?4.75 2.09 ?2.88
#I?s per sentence 1.01 ?0.48 0.54 ?0.60
p(sentence in post contains I) 0.67 ?0.23 0.40 ?0.35
#non-modal verbs per post 19.62 ?15.08 9.82 ?9.57
#non-modal verbs per sent. 3.30 ?1.18 2.82 ?1.37
#modal verbs per sent. 0.22 ?0.22 0.26 ?0.36
fraction of past-tense verbs 0.26 ?0.17 0.17 ?0.19
fraction of present tense verbs 0.42 ?0.18 0.41 ?0.23
Table 1: Comparison of surface text features for posts
with and without experience; p(?) denotes probability.
We acquired data by crawling two forums on
shaving,1 with 111,268 posts written by 2,880 users.
Manual assessments Two assessors (both authors
of this paper) were asked to search for posts on five
specific target products using a standard keyword
search, and label each result post as:
? reporting no experience, or
? reporting an off-target experience, or
? reporting an on-target experience.
Moreover, posts should be marked as reporting an
experience only if (i) the author explicitly reports
his or someone else?s (a concrete person?s) use of
a product; and (ii) the author makes some conclu-
sions/judgements about the experience.
In total, 203 posts were labeled by the two asses-
sors, with 101 posts marked as reporting an experi-
ence by at least one assessor (71% of those an on-
target experience). The inter-annotator agreement
was 0.84, with Cohen?s ? = 0.71. If we merge
on- and off-target experience labels, the agreement
is 0.88, with ? = 0.76. The high level of agreement
demonstrates the validity of the task definition.
Features for experience mining We considered a
number of linguistic features and compared posts re-
porting experience (on- or off-target) to the posts
1www.shavemyface.com, www.menessentials.com/community
2Computed using LingPipe: http://alias-i.com/lingpipe
With experience Without experience
used 0.15, found 0.09,
bought 0.07, tried 0.07,
got 0.07, went 0.07, started
0.05, switched 0.04, liked
0.03, decided 0.03
got 0.09, thought 0.09,
switched 0.06, meant 0.06,
used 0.06, went 0.06, ig-
nored 0.03, quoted 0.03,
discovered 0.03, heard 0.03
Table 2: Most frequent past tense verbs following I in
posts with and without experience, with rel. frequencies.
with no experience. Table 1 lists the features and
the comparison results. Remarkably, the subjectiv-
ity score is lower for experience posts: this indicates
that our task is indeed different from sentiment re-
trieval. Experience posts are on average twice as
long as non-experience posts and contain more sen-
tences with pronoun I. They also contain more con-
tent (non-modal) verbs, especially past tense verbs.
Table 2 presents a more detailed analysis of the verb
use. Experience posts appear to contain more verbs
referring to concrete actions rather than to attitude
and perception. It is still to be seen, though, whether
this informal observation can be quantified using re-
sources such as standard semantic verb classification
(state, process, action), WordNet verb hierarchy or
FrameNet semantic frames.
5 Conclusions
We introduced the novel task of experience min-
ing. Users of products share their experiences, and
mining these could help define requirements for
next-generation products. We developed annotation
guidelines for labeling experiences, and used them
to annotate data from online forums. An initial ex-
ploration revealed multiple features that might prove
useful for automatic labeling via classification.
References
[1] K. Balog, G. Mishne, and M. de Rijke. Why are they
excited?: identifying and explaining spikes in blog
mood levels. In EACL ?06, pages 207?210, 2006.
[2] B. Buxton. Sketching User Experiences: Getting the
Design Right and the Right Design. Morgan Kauf-
mann Publishers Inc., 2007.
[3] I. Ounis, C. Macdonald, M. de Rijke, G. Mishne, and
I. Soboroff. Overview of the TREC 2006 Blog Track.
In TREC 2006, 2007.
[4] B. Pang and L. Lee. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
2008.
18
