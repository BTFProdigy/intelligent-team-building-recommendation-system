Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 25?32
Manchester, August 2008
An Improved Hierarchical Bayesian Model of Language
for Document Classification
Ben Allison
Department of Computer Science
University of Sheffield
UK
ben@dcs.shef.ac.uk
Abstract
This paper addresses the fundamental
problem of document classification, and
we focus attention on classification prob-
lems where the classes are mutually exclu-
sive. In the course of the paper we advo-
cate an approximate sampling distribution
for word counts in documents, and demon-
strate the model?s capacity to outperform
both the simple multinomial and more re-
cently proposed extensions on the classifi-
cation task. We also compare the classi-
fiers to a linear SVM, and show that pro-
vided certain conditions are met, the new
model allows performance which exceeds
that of the SVM and attains amongst the
very best published results on the News-
groups classification task.
1 Introduction
Document classification is one of the key technolo-
gies in the emerging digital world: as the amount
of textual information existing in electronic form
increases exponentially, reliable automatic meth-
ods to sift through the haystack and pluck out the
occasional needle are almost a necessity.
Previous comparative studies of different classi-
fiers (for example, (Yang and Liu, 1999; Joachims,
1998; Rennie et al, 2003; Dumais et al, 1998))
have consistently shown linear Support Vector Ma-
chines to be the most appropriate method. Gen-
erative probabilistic classifiers, often represented
by the multinomial classifier, have in these same
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
studies performed poorly, and this empirical evi-
dence has been bolstered by theoretical arguments
(Lasserre et al, 2006).
In this paper we revisit the theme of genera-
tive classifiers for mutually exclusive classification
problems, but consider classifiers employing more
complex models of language; as a starting point
we consider recent work (Madsen et al, 2005)
which relaxes some of the multinomial assump-
tions. We continue and expand upon the theme
of that work, but identify some weaknesses both
in its theoretical motivations and practical applica-
tions. We demonstrate a new approximate model
which overcomes some of these concerns, and
demonstrate substantial improvements that such a
model achieves on four classification tasks, three
of which are standard and one of which is a newly
created task. We also show the new model to be
highly competitive to an SVM where the previous
models are not.
?2 of the paper describes previous work which
has sought a probabilistic model of language and
its application to document classification. ?3 de-
scribes the models we consider in this paper, and
gives details of parameter estimation. ?4 describes
our evaluation of the models, and ?5 presents the
results of this evaluation. ?6 explores reasons for
the observed results, and finally ?7 ends with some
concluding remarks.
2 Related Work
The problem of finding an appropriate and
tractable model for language is one which has been
studied in many different areas. In many cases, the
first (and often only) model is one in which counts
of words are modelled as binomial? or Poisson?
distributed random variables. However, the use of
such distributions entails an implicit assumption
25
that the occurrence of words is the result of a fixed
number of independent trials?draws from a ?bag
of words??where on each trial the probability of
success is constant.
Several authors, among them (Church and Gale,
1995; Katz, 1996), observe empirically such mod-
els are not always accurate predictors of actual
word behaviour. This moves them to suggest dis-
tributions for word counts where the underlying
probability varies between documents; thus the ex-
pected behaviour of a word in a new document is
a combination of predictions for all possible prob-
abilities. Other authors (Jansche, 2003; Eyhera-
mendy et al, 2003; Lowe, 1999) use these same
ideas to classify documents on the basis of subsets
of vocabulary, in the first and third cases with en-
couraging results using small subsets (in the sec-
ond case, the performance of the model is shown
to be poor compared to the multinomial).
When one moves to consider counts of all words
in some vocabulary, the proper distribution of
the whole vector of word counts is multinomial.
(Madsen et al, 2005) apply the same idea as for
the single word (binomial) case to the multino-
mial, using the most convenient form of distribu-
tion to represent the way the vector of multino-
mial probabilities varies between documents, and
report encouraging results compared to the simple
multinomial. However, we show that the use of
the most mathematically convenient distribution to
describe the way the vector of probabilities varies
entails some unwarranted and undesirable assump-
tions. This paper will first describe those assump-
tions, and then describe an approximate technique
for overcoming the assumptions. We show that,
combined with some alterations to estimation, the
models lead to a classifier able to outperform both
the multinomial classifier and a linear SVM.
3 Probabilistic Models of Language for
Document Classification
In this section, we briefly describe the use of a gen-
erative model of language as applied to the prob-
lem of document classification, and also how we
estimate all relevant parameters for the work which
follows.
In terms of notation, we use c? to represent a ran-
dom variable and c to represent an outcome. We
use roman letters for observed or observable quan-
tities and greek letters for unobservables (i.e. pa-
rameters). We write c? ? ?(c) to mean that c? has
probability density (discrete or continuous) ?(c),
and write p(c) as shorthand for p(c? = c). Finally,
we make no explicit distinction in notation be-
tween univariate and multivariate quantities; how-
ever, we use ?
j
to refer to the j-th component of
the vector ?.
We consider documents to be represented as
vectors of count?valued random variables such
that d = {d
1
...d
v
}. For classification, interest
centres on the conditional distribution of the class
variable, given such a document. Where docu-
ments are to be assigned to one class only (as in
the case of this paper), this class is judged to be
the most probable class. For generative classifiers
such as those considered here, the posterior distri-
bution of interest is modelled from the joint distri-
bution of class and document; thus if c? is a variable
representing class and
?
d is a vector of word counts,
then:
p(c|d) ? p(c) ? p(d|c) (1)
For the purposes of this work we also assume a
uniform prior on c?, meaning the ultimate decision
is on the basis of the document alone.
Multinomial Sampling Model
A natural way to model the distribution of
counts is to let p(d|c) be distributed multinomially,
as proposed in (Guthrie et al, 1994; McCallum
and Nigam, 1998) amongst others. The multino-
mial model assumes that documents are the result
of repeated trials, where on each trial a word is se-
lected at random, and the probability of selecting
the j-th word from class c is ?
cj
. However, in gen-
eral we will not use the subscript c ? we estimate
one set of parameters for each possible class.
Using multinomial sampling, the term p(d|c)
has distribution:
p
multinomial
(d|?) =
(
?
j
d
j
)
!
?
j
(d
j
!)
?
j
?
d
j
j
(2)
A simple Bayes estimator for ? can be obtained
by taking the prior for ? as a Dirichlet distribution,
in which case the posterior is also Dirichlet. De-
note the total training data for the class in ques-
tion as D = {(d
11
...d
1v
) ... (d
k1
...d
kv
)} (that is,
counts of each of v words in k documents). Then
if p(?) ? Dirichlet(?
1
...?
v
), the mean of p(?|D)
for the j-th component of ? (which is the estimate
we use) is:
26
??
j
= E[?
j
|D] =
?
j
+ n
j
?
j
?
j
+ n
?
(3)
where the n
j
are the sufficient statistics
?
i
n
ij
,
and n
?
is
?
j
n
j
. We follow common practice and
use the standard reference Dirichlet prior, which is
uniform on ?, such that ?
j
= 1 for all j.
3.1 Hierarchical Sampling Models
In contrast to the model above, a hierarchical sam-
pling model assumes that
?
? varies between docu-
ments, and has distribution which depends upon
parameters ?. This allows for a more realistic
model, letting the probabilities of using words vary
between documents subject only to some general
trend.
For example, consider documents about politics:
some will discuss the current British Prime Minis-
ter, Gordon Brown. In these documents, the proba-
bility of using the word brown (assuming case nor-
malisation) may be relatively high. Other politics
articles may discuss US politics, for example, or
the UN, French elections, and so on, and these ar-
ticles may have a much lower probability of using
the word brown: perhaps just the occasional refer-
ence to the Prime Minister. A hierarchical model
attempts to model the way this probability varies
between documents in the politics class.
Starting with the joint distribution p(?, d|?) and
averaging over all possible values that ? may take
in the new document gives:
p(d|?) =
?
p(?|?)p(d|?) d? (4)
where integration is understood to be over the
entire range of possible ?. Intuitively, this allows
?
? to vary between documents subject to the restric-
tion that
?
? ? p(?|?), and the probability of observ-
ing a document is the average of its probability for
all possible ?, weighted by p(?|?). The sampling
process is 1) ? is first sampled from p(?|?) and
then 2) d is sampled from p(d|?), leading to the
hierarchical name for such models.
Dirichlet Compound Multinomial Sampling
Model
(Madsen et al, 2005) suggest a form of (4)
where p(?|?) is Dirichlet-distributed, leading to
a Dirichlet?Compound?Multinomial sampling dis-
tribution. The main benefit of this assumption is
that the integral of (4) can be obtained in closed
form. Thus p(d|?) (using the standard ? notation
for Dirichlet parameters) has distribution:
p
DCM
(d|?) =
(
?
j
d
j
)
!
?
j
(d
j
!)
?
?
(
?
j
?
j
)
?
(
?
j
d
j
+ ?
j
)
?
?
j
?(?
j
+ d
j
)
?(?
j
)
(5)
Maximum likelihood estimates for the ? are dif-
ficult to obtain, since the likelihood for ? is a func-
tion which must be maximised for all components
simultaneously, leading some authors to use ap-
proximate distributions to improve the tractability
of maximum likelihood estimation (Elkan, 2006).
In contrast, we reparameterise the Dirichlet com-
pound multinomial, and estimate some of the pa-
rameters in closed form.
We reparameterise the model in terms of ? and
? ? ? is a vector of length v, and ? is a con-
stant which reflects the variance of ?. Under this
parametrisation, ?
j
= ??
j
. The estimate we use
for ?
j
is simply:
??
j
=
n
j
n
?
(6)
where n
j
and n
?
are defined above. This simply
matches the first moment about the mean of the
distribution with the first moment about the mean
of the sample. Once again letting:
D = {d
1
...d
k
} = {(d
11
...d
1v
)...(d
k1
...d
kv
)}
denote the training data such that the d
i
are indi-
vidual document vectors and d
ij
are counts of the
j-th word in the i-th document, the likelihood for
? is:
L(?) =
?
i
?(
?
j
??
j
)
?(
?
j
d
ij
+ ??
j
)
?
j
?(??
j
+ d
ij
)
?(??
j
)
(7)
This is a one?dimensional function, and as such
is much more simple to maximise using standard
optimisation techniques, for example as in (Minka,
2000).
As before, however, simple maximum likeli-
hood estimates alone are not sufficient: if a word
fails to appear at all in D, the corresponding ?
j
will be zero, in which case the distribution is im-
proper. The theoretically sound solution would be
27
to incorporate a prior on either ? or (under our
parameterisation) ?; however, this would lead to
high computational cost as the resulting posterior
would be complicated to work with. (Madsen et
al., 2005) instead set each ??
j
as the maximum like-
lihood estimate plus some , in some ways echo-
ing the estimation of ? for the multinomial model.
Unfortunately, unlike a prior this strategy has the
same effect regardless of the amount of training
data available, whereas any true prior would have
diminishing effect as the amount of training data
increased. Instead, we supplement actual training
data with a pseudo?document in which every word
occurs once (note this is quite different to setting
 = 1); this echoes the effect of a true prior on ?,
but without the computational burden.
A Joint Beta-Binomial Sampling Model
Despite its apparent convenience and theoretical
well?foundedness, the Dirichlet compound multi-
nomial model has one serious drawback, which
is emphasised by the reparameterisation. Under
the Dirichlet, there is a functional dependence be-
tween the expected value of ?
j
, ?
j
and its variance,
where the relationship is regulated by the constant
?. Thus two words whose ?
j
are the same will also
have the same variance in the ?
j
. This is of concern
since different words have different patterns of use
? to use a popular turn of phrase, some words are
more ?bursty? than others (see (Church and Gale,
1995) for examples). In practice, we may hope
to model different words as having the same ex-
pected value, but drastically different variances ?
unfortunately, this is not possible using the Dirich-
let model.
The difficulty with switching to a different
model is the evaluation of the integral in (4). The
integral is in fact in many thousands of dimensions,
and even if it were possible to evaluate such an
integral numerically, the process would be excep-
tionally slow.
We overcome this problem by decomposing the
term p(d|?) into a product of independent terms
of the form p(d
j
|?
j
). A natural way for each of
these terms to be distributed is to let the probability
p(d
j
|?
j
) be binomial and to let p(?
j
|?
j
) be beta?
distributed. The probability p(d
j
|?
j
) (where ?
j
=
{?
j
, ?
j
}, the parameters of the beta distribution) is
then:
p
bb
(d
j
|?
j
, ?
j
) =
(
n
d
j
)
B(d
j
+ ?
j
, n? d
j
+ ?
j
)
B(?
j
, ?
j
)
(8)
where B(?) is the Beta function. The term
p(d|?) is then simply:
p
beta?binomial
(d|?) =
?
j
p(d
j
|?
j
) (9)
This allows means and variances for each of the
?
j
to be specified separately, but this comes at a
price: while the Dirichlet ensures that
?
j
?
j
= 1
for all possible ?, the model above does not. Thus
the model is only an approximation to a true model
where components of ? have independent means
and variances, and the requirements of the multi-
nomial are fulfilled. However, given the inflexibil-
ity of the Dirichlet multinomial model, we argue
that such a sacrifice is justified.
In order to estimate parameters of the Beta?
Binomial model, we take a slight departure from
both (Lowe, 1999) and (Jansche, 2003) who have
both used a similar model previously for individual
words. (Lowe, 1999) uses numerical techniques
to find maximum likelihood estimates of the ?
j
and ?
j
, which was feasible in that case because of
the highly restricted vocabulary and two-classes.
(Jansche, 2003) argues exactly this point, and uses
moment?matched estimates; our estimation is sim-
ilar to that, in that we use moment?matching, but
different in other regards.
Conventional parameter estimates are affected
(in some way or other) by the likelihood function
for a parameter, and the likelihood function is such
that longer documents exert a greater influence on
the overall likelihood for a parameter. That is, we
note that if the true binomial parameter ?
ij
for the
j-th word in the i-th document were known, then
the most sensible expected value for the distribu-
tion over ?
j
would be:
E [?
j
] =
1
k
?
k
?
i=1
?
ij
(10)
Whereas the expected value of conventional
method?of?moments estimate is:
E [?
j
] =
k
?
i=1
p (?
ij
)?
?
?
ij
(11)
That is, a weighted mean of the maximum like-
lihood estimates of each of the ?
ij
, with weights
28
given by p (?
ij
), i.e. the length of the i-th docu-
ment. Similar effects would be observed by max-
imising the likelihood function numerically. This
is to our minds undesireable, since we do note be-
lieve that longer documents are necessarily more
representative of the population of all documents
than are shorter ones (indeed, extremely long doc-
uments are likeliy to be an oddity), and in any case
the goal is to capture variation in the parameters.
This leads us to suggest estimates for parameters
such that the expected value of the distribution is
as in 10 but with the ?
ij
(which are unknown) re-
placed with their maximum likelihood estimates,
?
?
ij
. We then use these estimates to specify the de-
sired variance, leading to the simultaneous equa-
tions:
?
j
?
j
+ ?
j
=
?
i
?
?
ij
k
(12)
?
j
?
j
(?
j
+ ?
j
)
2
(?
j
+ ?
j
+ 1)
=
?
i
(
?
?
ij
? E[?
j
])
2
k
(13)
As before, we supplement actual training doc-
uments with a pseudo-document in which every
word occurs once to prevent any ?
j
being zero.
4 Evaluating the Models
This section describes evaluation of the models
above on four text classification problems.
The Newsgroups task is to classify postings into
one of twenty categories, and uses data originally
collected in (Lang, 1995). The task involves a rel-
atively large number of documents (approximately
20,000) with roughly even distribution of mes-
sages, giving a very low baseline of approximately
5%.
For the second task, we use a task derived from
the Enron mail corpus (Klimt and Yang, 2004), de-
scribed in (Allison and Guthrie, 2008). Corpus is
a nine?way email authorship attribution problem,
with 4071 emails (between 174 and 706 emails per
author)
1
. The mean length of messages in the cor-
pus is 75 words.
WebKB is a web?page classification task, where
the goal is to determine the webpage type of the
unseen document. We follow the setup of (McCal-
lum and Nigam, 1998) and many thereafter, and
1
The corpus is available for download from
www.dcs.shef.ac.uk/?ben.
use the four biggest categories, namely student,
faculty, course and project. The resulting
corpus consists of approximately 4,200 webpages.
The SpamAssassin corpus is made available for
public use as part of the open-source Apache Spa-
mAssassin Project
2
. It consists of email divided
into three categories: Easy Ham, which is email
unambiguously ham (i.e. not spam), Hard Ham
which is not spam but shares many traits with
spam, and finally Spam. The task is to apply these
labels to unseen emails. We use the latest ver-
sion of all datasets, and combine the easy ham and
easy ham 2 as well as spam and spam 2 sets to
form a corpus of just over 6,000 messages.
In all cases, we use 10?fold cross validation
to make maximal use of the data, where folds
are chosen by random assignment. We define
?words? to be contiguous whitespace?delimited
alpha?numeric strings, and perform no stemming
or stoplisting.
For the purposes of comparison, we also present
results using a linear SVM (Joachims, 1999),
which we convert to multi?class problems using
a one?versus?all strategy shown to be amongst
the best performing strategies (Rennie and Rifkin,
2001). We normalise documents to be vectors of
unit length, and resolve decision ambiguities by
sole means of distance to the hyperplane. We also
note that experimentation with non?linear kernels
showed no consistent trends, and made very little
difference to performance.
5 Results
Table 1 displays results for the three models over
the four datasets. We use the simplest measure of
classifier performance, accuracy, which is simply
the total number of correct decisions over the ten
folds, divided by the size of the corpus. In response
to a growing unease over the use of significance
tests (because they have a tendency to overstate
significance, as well as obscure effects of sample
size) we provide 95% intervals for accuracy as well
as the metric itself. To calculate these, we view ac-
curacy as an (unknown) parameter to a binomial
distribution such that the number of correctly clas-
sified documents is a binomially distributed ran-
dom variable. We then calculate the Bayesian in-
terval for the parameter, as described in (Brown et
al., 2001), which allows immediate quantification
2
The corpus is available online at
http://spamassassin.apache.org/publiccorpus/
29
of uncertainty in the true accuracy after a limited
sample.
As can be seen from the performance figures,
no one classifier is totally dominant, although there
are obvious and substantial gains in using the Beta-
Binomial model on the Newsgroups and Enron
tasks when compared to all other models. The Spa-
mAssassin corpus shows the beta?binomial model
and the SVM to be considerably better than the
other two models, but there is little to choose be-
tween them. The WebKB task, however, shows
extremely unusual results: the SVM is head and
shoulders above other methods, and of the genera-
tive approaches the multinomial is clearly superior.
In all cases, the Dirichlet model actually performs
worse than the multinomial model, in contrast to
the observations of (Madsen et al, 2005).
In terms of comparison with other work, we note
that the performance of our multinomial model
agrees with that in other work, including for exam-
ple (Rennie et al, 2003; Eyheramendy et al, 2003;
Madsen et al, 2005; Jansche, 2003). Our Dirichlet
model performs worse than that in (Madsen et al,
2005) (85% here compared to 89% in that work),
which we attribute to their experimentation with
alternate smoothing  as described in ?3.1. We
note however that the Beta-Binomial model here
still outperforms that work by some considerable
margin. Finally, we note that our beta?binomial
model outperforms that in (Jansche, 2003), which
we attribute mainly to the altered estimate, but also
to the partial vocabulary used in that work. In fact,
(Jansche, 2003) shows there to be little to sepa-
rate the beta-binomial and multinomial models for
larger vocabularies, in stark contrast to the work
here, and this is doubtless due to the parameter es-
timation.
6 Analysis
One might expect performance of a hierarchical
sampling model to eclipse that of the SVM because
of the nature of the decision boundary, provided
certain conditions are met: the SVM estimates
a linear decision boundary, and the multinomial
classifier does the same. However, the decision
boundaries for the hierarchical classifiers are non?
linear, and can represent more complex word be-
haviour, provided that sufficient data exist to pre-
dict it. However, unlike generic non?linear SVMs
(which made little difference compared to a lin-
ear SVM) the non?linear decision boundary here
arises naturally from a model of word behaviour.
For the hierarchical models, performance rests
on the ability to estimate both the rate of word oc-
currence ?
j
and also the way that this rate varies
between documents. To reliably estimate variance
(and arguably rate as well) would require words to
occur a sufficient number of times. However, this
section will demonstrate that two of the datasets
have many words which do not occur with suf-
ficient frequency to estimate parameters, and in
those two the linear SVM?s performance is more
comparable.
We present two quantifications of word reuse to
support our conclusions. The first are frequency
spectra for each of the four corpora, shown in Fig-
ure 1. The two more problematic datasets appear
in the top of the figure. To generate the charts, we
pool all documents from all classes in a each prob-
lem, and count the number of words that appear
once, twice, and so on. The x axis is the num-
ber of times a word occurs, and the y axis the total
number of words which have that count.
The WebKB corpus has the large majority of
words occurring very few times (the mass of the
distribution is concentrated towards the left of the
chart), while the SpamAssassin corpus is more rea-
sonable and the Newsgroups corpus has by far
the most words which occur with substantial fre-
quency (this correlates perfectly with the relative
performances of the classifiers on these datasets).
For the Enron corpus, it is somewhat harder to tell,
since its size means no words occur with substan-
tial frequency.
We also consider the proportion of all word pairs
in a corpus in which the first word is the same as
the second word. If a corpus has n
?
words total
with total counts n
1
...n
v
then the statistic is:
r =
1
(n
?
(n
?
? 1)) /2
?
i
(n
i
(n
i
? 1))/2. (14)
To measure differing tendencies to reuse words,
we calculate the r statistic once for each class, and
then its mean across all classes in a problem (Ta-
ble 2). We note that the two corpora on which the
hierarchical model dominates have much greater
tendency for word reuse, meaning the extra pa-
rameters can be esimated with greater accuracy.
The SpamAssassin corpus is, by this measure, a
harder task, but this is somewhat mitigated by the
more even frequency distribution evidenced in Fig-
ure 1; on the other hand, the WebKB corpus does
30
Newsgroups Enron Authors WebKB SpamAssassin
Multinomial 85.66 ? 0.5 74.55 ? 1.34 85.69 ? 1.06 95.96 ? 0.5
DCM 85.03 ? 0.51 74.43 ? 1.34 82.69 ? 1.15 91.47 ? 0.7
Beta-Bin 91.65 ? 0.4
?+
83.54 ? 1.14
?+
84.81 ? 1.08 97.35 ? 0.4
?
SVM 88.8 ? 0.45
?
80 ? 1.23
?
92.68 ? 0.79
?
97.65 ? 0.38
?
Table 1: Performance of four classifiers on four tasks. Error is 95% interval for accuracy. Bold denotes
best performance on a task.
?
denotes performance superior to multinomial which exceeds posterior
uncertainty (i.e. observed performance outside 95% interval).
+
denotes the same for the SVM
Frequency Spectrum of Words in the SpamAssassin Corpus
Frequency
log(N
umb
er of
 wor
ds w
ith fr
eque
ncy)
1
10
100
100
0
100
00
0 500 1000 1500
Frequency Spectrum of Words in the WebKB Corpus
Frequency
log(N
umb
er of
 wor
ds w
ith fr
eque
ncy)
1
10
100
100
0
100
00
0 500 1000 1500
Frequency Spectrum of Words in the Newsgroups Corpus
Frequency
log(N
umb
er of
 wor
ds w
ith fr
eque
ncy)
1
10
100
100
0
100
00
0 500 1000 1500
Frequency Spectrum of Words in the Enron Authors Corpus
Frequency
log(N
umb
er of
 wor
ds w
ith fr
eque
ncy)
1
10
100
100
0
100
00
0 500 1000 1500
Figure 1: Frequency spectra for the four datasets. y axis is on a logarithmic scale
not look promising for the hierarchical model by
either measure.
7 Conclusion
In this paper, we have advocated the use of a
joint beta?binomial distribution for word counts in
documents for the purposes of classification. We
have shown that this model outperforms classifiers
based upon both multinomial and Dirichlet Com-
pound Multinomial distributions for word counts.
We have further made the case that, where cor-
pora are sufficiently large as to warrant it, a gener-
ative classifier employing a hierarchical sampling
model outperforms a discriminative linear SVM.
We attribute this to the capacity of the proposed
model to capture aspects of word behaviour be-
yond a simpler model. However, in cases where
the data contain many infrequent words and the
tendency to reuse words is relatively low, default-
ing to a linear classifier (either the multinomial
for a generative classifier, or preferably the lin-
ear SVM) increases performance relative to a more
complex model, which cannot be fit with sufficient
precision.
References
Allison, Ben and Louise Guthrie. 2008. Authorship at-
tribution of e-mail: Comparing classifiers over a new
corpus for evaluation. In Proceedings of LREC?08.
Brown, Lawrence D., Tony Cai, and Anirban Das-
Gupta. 2001. Interval estimation for a binomial pro-
portion. Statistical Science, 16(2):101?117, may.
31
Newsgroups Enron Authors WebKB SpamAssassin
Mean r 0.0090 0.0083 0.0047 0.0037
Table 2: Mean r statistic for the four problems
Church, K. and W. Gale. 1995. Poisson mixtures. Nat-
ural Language Engineering, 1(2):163?190.
Dumais, Susan, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algo-
rithms and representations for text categorization. In
CIKM ?98, pages 148?155.
Elkan, Charles. 2006. Clustering documents with
an exponential-family approximation of the dirich-
let compound multinomial distribution. In Proceed-
ings of the Twenty-Third International Conference
on Machine Learning.
Eyheramendy, S., D. Lewis, and D. Madigan. 2003.
The naive bayes model for text categorization. Arti-
ficial Intelligence and Statistics.
Guthrie, Louise, Elbert Walker, and Joe Guthrie. 1994.
Document classification by machine: theory and
practice. In Proceedings COLING ?94, pages 1059?
1063.
Jansche, Martin. 2003. Parametric models of linguistic
count data. In ACL ?03, pages 288?295.
Joachims, Thorsten. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In N?edellec, Claire and C?eline Rou-
veirol, editors, Proceedings of ECML-98, 10th Euro-
pean Conference on Machine Learning, pages 137?
142.
Joachims, Thorsten. 1999. Making large-scale svm
learning practical. Advances in Kernel Methods -
Support Vector Learning.
Katz, Slava M. 1996. Distribution of content words
and phrases in text and language modelling. Nat.
Lang. Eng., 2(1):15?59.
Klimt, Bryan and Yiming Yang. 2004. The enron cor-
pus: A new dataset for email classification research.
In Proceedings of ECML 2004, pages 217?226.
Lang, Ken. 1995. NewsWeeder: learning to filter net-
news. In Proceedings of the 12th International Con-
ference on Machine Learning, pages 331?339.
Lasserre, Julia A., Christopher M. Bishop, and
Thomas P. Minka. 2006. Principled hybrids of gen-
erative and discriminative models. In CVPR ?06:
Proceedings of the 2006 IEEE Computer Society
Conference on Computer Vision and Pattern Recog-
nition, pages 87?94.
Lowe, S. 1999. The beta-binomial mixture model and
its application to tdt tracking and detection. In Pro-
ceedings of the DARPA Broadcast News Workshop.
Madsen, Rasmus E., David Kauchak, and Charles
Elkan. 2005. Modeling word burstiness using the
Dirichlet distribution. In ICML ?05, pages 545?552.
McCallum, A. and K. Nigam. 1998. A comparison
of event models for na??ve bayes text classification.
In Proceedings AAAI-98 Workshop on Learning for
Text Categorization.
Minka, Tom. 2000. Estimating a dirichlet distribution.
Technical report, Microsoft Research.
Rennie, Jason D. M. and Ryan Rifkin. 2001. Improv-
ing multiclass text classification with the Support
Vector Machine. Technical report, Massachusetts In-
sititute of Technology, Artificial Intelligence Labora-
tory.
Rennie, J., L. Shih, J. Teevan, and D. Karger. 2003.
Tackling the poor assumptions of naive bayes text
classifiers.
Yang, Y. and X. Liu. 1999. A re-examination of text
categorization methods. In 22nd Annual Interna-
tional SIGIR, pages 42?49, Berkley, August.
32
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 407?411,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards the Orwellian Nightmare 
Separation of Business and Personal Emails 
 
Sanaz Jabbari, Ben Allison, David Guthrie, Louise Guthrie 
Department of Computer Science 
University of Sheffield 
211 Portobello St. 
Sheffield 
S1 4DP 
{s.jabbari, b.allison, d.guthrie, l.guthrie}@dcs.shef.ac.uk 
 
Abstract 
This paper describes the largest scale annotation pro-
ject involving the Enron email corpus to date. Over 
12,500 emails were classified, by humans, into the 
categories ?Business? and ?Personal?, and then sub-
categorised by type within these categories. The paper 
quantifies how well humans perform on this task 
(evaluated by inter-annotator agreement). It presents 
the problems experienced with the separation of these 
language types. As a final section, the paper presents 
preliminary results using a machine to perform this 
classification task. 
 
1 Introduction 
Almost since it became a global phenomenon, com-
puters have been examining and reasoning about our 
email. For the most part, this intervention has been 
well natured and helpful ? computers have been try-
ing to protect us from attacks of unscrupulous blanket 
advertising mail shots. However, the use of computers 
for more nefarious surveillance of email has so far 
been limited. The sheer volume of email sent means 
even government agencies (who can legally intercept 
all mail) must either filter email by some pre-
conceived notion of what is interesting, or they must 
employ teams of people to manually sift through the 
volumes of data. For example, the NSA has had mas-
sive parallel machines filtering e-mail traffic for at 
least ten years. 
 
The task of developing such automatic filters at re-
search institutions has been almost impossible, but for 
the opposite reason. There is no shortage of willing 
researchers, but progress has been hampered by the 
lack of any data ? one?s email is often hugely private, 
and the prospect of surrendering it, in its entirety, for 
research purposes is somewhat unsavoury. 
 
Recently, a data resource has become available where 
exactly this condition (several hundred people?s entire 
email archive) has been satisfied ? the Enron dataset. 
During the legal investigation of the collapse of En-
ron, the FERC (Federal Energy Regulatory Commis-
sion) seized the emails of every employee in that 
company. As part of the process, the collection of 
emails was made public and subsequently prepared 
for research use by researchers at Carnegie Melon 
University (Klimt and Yang, 2004).Such a corpus of 
authentic data, on such a large scale, is unique, and an 
invaluable research tool. It then falls to the prospec-
tive researcher to decide which divisions in the lan-
guage of email are interesting, which are possible, and 
how the new resource might best be used.  
 
Businesses which offer employees an email system at 
work (and there are few who do not) have always 
known that they possess an invaluable resource for 
monitoring their employees? work habits. During the 
1990s, UK courts decided that that an employee?s 
email is not private ? in fact, companies can read 
them at will. However, for exactly the reasons de-
scribed above, automatic monitoring has been impos-
sible, and few businesses have ever considered it suf-
ficiently important to employ staff to monitor the 
email use of other staff. However, in monitoring staff 
productivity, few companies would decline the use of 
a system which could analyse the email habits of its 
employees, and report the percentage of time which 
each employee was spending engaged in non-work 
related email activities. 
 
The first step in understanding how this problem 
might be tackled by a computer, and if it is even fea-
sible for this to happen, is to have humans perform the 
task. This paper describes the process of having hu-
mans annotate a corpus of emails, classifying each as 
to whether they are business or personal, and then 
attempting to classify the type of business or personal 
mail being considered. 
 
A resource has been created to develop a system able 
to make these distinctions automatically. Furthermore, 
the process of subcategorising types of business and 
personal has allowed invaluable insights into the areas 
407
where confusion can occur, and how these confusions 
might be overcome. 
 
The paper presents an evolution of appropriate sub-
categories, combined with analysis of performance 
(measured by inter-annotator agreement) and reasons 
for any alterations. It addresses previous work done 
with the Enron dataset, focusing particularly on the 
work of Marti Hearst at Berkeley who attempted a 
smaller-scale annotation project of the Enron corpus, 
albeit with a different focus. It concludes by suggest-
ing that in the main part (with a few exceptions) the 
task is possible for human annotators. The project has 
produced a set of labeled messages (around 14,000, 
plus double annotations for approximately 2,500) with 
arguably sufficiently high business-personal agree-
ment that machine learning algorithms will have suf-
ficient material to attempt the task automatically. 
2 Introduction to the Corpus 
Enron?s email was made public on the Web by FERC 
(Federal Energy Regulatory Commission), during a 
legal investigation on Enron Corporation. The emails 
cover 92 percent of the staff?s emails, because some 
messages have been deleted "as part of a redaction 
effort due to requests from affected employees". The 
dataset was comprised of 619,446 messages from 158 
users in 3,500 folders. However, it turned out that the 
raw data set was suffering from various data integrity 
problems. Various attempts were made to clean and 
prepare the dataset for research purposes. The dataset 
used in this project was the March 2, 2004 version 
prepared at Carnegie Mellon University, acquired 
from http://www.cs.cmu.edu/~enron/. This version of 
the dataset was reduced to 200,399 emails by remov-
ing some folders from each user. Folders like ?discus-
sion threads? and ?all documents?, which were ma-
chine generated and contained duplicate emails, were 
removed in this version.  
 
There were on average 757 emails per each of the 158 
users. However, there are between one and 100,000 
emails per user. There are 30,091 threads present in 
123,091 emails. The dataset does not include attach-
ments. Invalid email addresses were replaced with 
?user@enron.com?.  When no recipient was specified 
the address was replaced with 
?no_address@enron.com? (Klimt and Yang, 2005). 
 
3 Previous Work with the Dataset 
The most relevant piece of work to this paper was 
performed at Berkeley. Marti Hearst ran a small-scale 
annotation project to classify emails in the corpus by 
their type and purpose (Email annotation at Berkely). 
In total, approximately 1,700 messages were anno-
tated by two distinct annotators. Annotation catego-
ries captured four dimensions, but broadly speaking 
they reflected the following qualities of the email: 
coarse genre, the topic of the email if business was 
selected, information about any forwarded or included 
text and the emotional tone of the email. However, the 
categories used at the Berkeley project were incom-
patible with our requirements for several reasons: that 
project allowed multiple labels to be assigned to each 
email; the categories were not designed to facilitate 
discrimination between business and personal emails; 
distinctions between topic, genre, source and purpose 
were present in each of the dimensions; and no effort 
was made to analyse the inter-annotator agreement 
(Email annotation at Berkely). 
 
User-defined folders are preserved in the Enron data, 
and some research efforts have used these folders to 
develop and evaluate machine-learning algorithms for 
automatically sorting emails (Klimt and Yang, 2004). 
However, as users are often inconsistent in organising 
their emails, so the training and testing data in these 
cases are questionable.  For example, many users 
have folders marked ?Personal?, and one might think 
these could be used as a basis for the characterisation 
of personal emails. However, upon closer inspection it 
becomes clear that only a tiny percentage of an indi-
vidual?s personal emails are in these folders. Simi-
larly, many users have folders containing exclusively 
personal content, but without any obvious folder 
name to reveal this. All of these problems dictate that 
for an effective system to be produced, large-scale 
manual annotation will be necessary. 
 
Researchers at Queen?s University, Canada (Keila, 
2005) recently attempted to categorise and identify 
deceptive messages in the Enron corpus. Their 
method used a hypothesis from deception theory (e.g., 
deceptive writing contains cues such as reduced fre-
quency of first-person pronouns and increased fre-
quency of ?negative emotion? words) and as to what 
constitutes deceptive language. Single value decom-
position (SVD) was applied to separate the emails, 
and a manual survey of the results allowed them to 
conclude that this classification method for detecting 
deception in email was promising. 
 
Other researchers have attempted to analyse the Enron 
emails from a network analytic perspective (Deisner, 
2005).  Their goal was to analyse the flow of commu-
nication between employees at times of crisis, and 
develop a characterisation for the state of a communi-
cation network in such difficult times, in order to 
identify looming crises in other companies from the 
state of their communication networks. They com-
pared the network flow of email in October 2000 and 
October 2001.    
 
4 Annotation Categories for this Project 
Because in many cases there is no definite line be-
tween business emails and personal emails, it was 
decided to mark emails with finer categories than 
408
Business and Personal. This subcategorising not only 
helped us to analyse the different types of email 
within business and personal emails, but it helped us 
to find the nature of the disagreements that  occurred 
later on, in inter-annotation.  In other words, this 
process allowed us to observe patterns in disagree-
ment.  
 
Obviously, the process of deciding categories in any 
annotation project is a fraught and contentious one. 
The process necessarily involves repeated cycles of 
category design, annotation, inter-annotation, analysis 
of disagreement, category refinement. While the proc-
ess described above could continue ad infinitum, the 
sensible project manager must identify were this 
process is beginning to converge on a set of well-
defined but nonetheless intuitive categories, and final-
ise them. 
 
Likewise, the annotation project described here went 
through several evolutions of categories, mediated by 
input from annotators and other researchers. The final 
categories chosen were: 
 
Business: Core Business, Routine Admin, Inter-
Employee Relations, Solicited/soliciting mailing, Im-
age. 
 
Personal: Close Personal, Forwarded, Auto generated 
emails. 
 
5 Annotation and Inter-Annotation 
Based on the categories above, approximately 12,500 
emails were single-annotated by a total of four anno-
tators. 
 
The results showed that around 83% of the emails 
were business related, while 17% were personal. The 
company received one personal email for every five 
business emails. 
 
Fig 1: Distribution of Emails in the Corpus
BUSINESS
83%
PERSONAL
17%
BUSINESS
PERSONAL
 
 
A third of the received emails were ?Core Business? 
and a third were ?Routine Admin?. All other catego-
ries comprised the remaining third of the emails. One 
could conclude that approximately one third of emails 
received at Enron were discussions of policy, strategy, 
legislation, regulations, trading, and other high-level 
business matters. The next third of received emails 
were about the peripheral, routine matters of the com-
pany. These are emails related to HR, IT administra-
tion, meeting scheduling, etc. which can be regarded 
as part of the common infrastructure of any large 
scale corporation. 
 
The rest of the emails were distributed among per-
sonal emails, emails to colleagues, company news 
letters, and emails received due to subscription. The 
biggest portion of the last third, are emails received 
due to subscription, whether the subscription be busi-
ness or personal in nature. 
 
In any annotation project consistency should be 
measured. To this end 2,200 emails were double an-
notated between four annotators. As Figure 2 below 
shows, for 82% of the emails both annotators agreed 
that the email was business email and in 12% of the 
emails, both agreed on them being personal. Six per-
cent of the emails were disagreed upon. 
 
Fig 2: Agreements and Disagreements in Inter-Annotation
Disagreement
6%
Personal 
Agreement
12%
Business 
Agreement
82%
Business Agreement
Personal Agreement
Disagreement
 
 
By analysing the disagreed categories, some patterns 
of confusion were found.  
 
Around one fourth of the confusions were solicited 
emails where it was not clear whether the employee 
was subscribed to a particular newsletter group for his 
personal interest, private business, or Enron?s busi-
ness. While some subscriptions were clearly personal 
(e.g. subscription to latest celebrity news) and some 
were clearly business related (e.g. Daily Energy re-
ports), for some it was hard to identify the intention of 
the subscription (e.g. New York Times). 
 
Eighteen percent of the confusions were due to emails 
about travel arrangements, flight and hotel booking 
confirmations, where it was not clear whether the per-
sonal was acting in a business or personal role. 
 
409
Thirteen percent of the disagreements were upon 
whether an email is written between two Enron em-
ployees as business colleagues or friends. The emails 
such as ?shall we meet for a coffee at 2:00?? If insuf-
ficient information exists in the email, it can be hard 
to draw the line between a personal relationship and a 
relationship between colleagues. The annotators were 
advised to pick the category based on the formality of 
the language used in such emails, and reading be-
tween the lines wherever possible. 
 
About eight percent of the disagreements were on 
emails which were about services that Enron provides 
for its employees. For example, the Enron?s running 
club is seeking for runners, and sending an ad to En-
ron?s employers. Or Enron?s employee?s assistance 
Program (EAP), sending an email to all employees, 
letting them know that in case of finding themselves 
in stressful situations they can use some of the ser-
vices that Enron provides for them or their families.  
 
One theme was encountered in many types of confu-
sions: namely, whether to decide an e-mail?s category 
based upon its topic or its form. For example, should 
an email be categorised because it is scheduling a 
meeting or because of the subject of the meeting be-
ing scheduled? One might consider this a distinction 
by topic or by genre. 
 
As the result, final categories were created to reflect 
topic as the only dimension to be considered in the 
annotation. ?Solicited/Soliciting mailing?, ?Solic-
ited/Auto generated mailing? and ?Forwarded? were 
removed and ?Keeping Current?, ?Soliciting? were 
added as business categories and ?Personal Mainte-
nance? and ?Personal Circulation? were added as per-
sonal categories. The inter-annotation agreement was 
measured for one hundred and fifty emails, annotated 
by five annotators. The results confirmed that these 
changes had a positive effect on the accuracy of anno-
tation. 
 
6 Preliminary Results of Automatic 
Classification 
Some preliminary experiments were performed with 
an automatic classifier to determine the feasibility of 
separating business and personal emails by machine. 
The classifier used was a probabilistic classifier based 
upon the distribution of distinguishing words. More 
information can be found in (Guthrie and Walker, 
1994). 
 
Two categories from the annotation were chosen 
which were considered to typify the broad categories 
? these were Core Business (representing business) 
and Close Personal (representing personal). The Core 
Business class contains 4,000 messages (approx 
900,000 words), while Close Personal contains ap-
proximately 1,000 messages (220,000 words). 
 
The following table summarises the performance of 
this classifier in terms of Recall, Precision and F-
Measure and accuracy: 
 
Class Recall Precision F-
Measure 
Accuracy 
Business 0.99 0.92 0.95 0.99 
Personal 0.69 0.95 0.80 0.69 
AVERAGE 0.84 0.94 0.88 0.93 
 
Based upon the results of this experiment, one can 
conclude that automatic methods are also suitable for 
classifying emails as to whether they are business or 
personal. The results indicate that the business cate-
gory is well represented by the classifier, and given 
the disproportionate distribution of emails, the classi-
fier?s tendency towards the business category is un-
derstandable. 
 
Given that our inter-annotator agreement statistic tells 
us that humans only agree on this task 94% of the 
time, preliminary results with 93% accuracy (the sta-
tistic which correlates exactly to agreement) of the 
automatic method are encouraging. While more work 
is necessary to fully evaluate the suitability of this 
task for application to a machine, the seeds of a fully 
automated system are sown. 
 
7 Conclusion 
This paper describes the process of creating an email 
corpus annotated with business or personal labels. By 
measuring inter-annotator agreement it shows that this 
process was successful. Furthermore, by analysing the 
disagreements in the fine categories, it has allowed us 
to characterise the areas where the business/personal 
decisions are difficult.  
 
In general, the separation of business and personal 
mails is a task that humans can perform. Part of the 
project has allowed the identification of the areas 
where humans cannot make this distinction (as dem-
onstrated by inter-annotator agreement scores) and 
one would not expect machines to perform the task 
under these conditions either. In all other cases, where 
the language is not ambiguous as judged by human 
annotators, the challenge has been made to automatic 
classifiers to match this performance. 
 
Some initial results were reported where machines 
attempted exactly this task. They showed that accu-
racy almost as high as human agreement was 
achieved by the system. Further work, using much 
larger sets and incorporating all types of business and 
personal emails, is the next logical step. 
 
410
Any annotation project will encounter its problems in 
deciding appropriate categories. This paper described 
the various stages of evolving these categories to a 
stage where they are both intuitive and logical and 
also, produce respectable inter-annotator agreement 
scores. The work is still in progress in ensuring 
maximal consistency within the data set and refining 
the precise definitions of the categories to avoid pos-
sible overlaps. 
References 
Brian Klimt and Yiming Yang. 2004. Introducing the 
Enron Email Corpus, Carnegie Mellon University.  
 
Brian Klimt and Yiming Yang. 2004. The Enron Cor-
pus: A New Data Set for Email Classification Re-
search. Carnegie Mellon University. 
 
Email Annotation at Berkely   
http://bailando.sims.berkeley.edu/enron_email.html
 
Jana Diesner and Kathleen M. Karley. 2005. Explora-
tion of Communication Networks from the Enron 
Email Corpus, Carnegie Mellon University  
 
Louise Guthrie,  Elbert Walker and Joe Guthrie. 1994 
Document classification by machine: Theory and 
practice. Proc. of COLING'94 
 
Parambir S. Keila and David B. Skillcorn. 2005.  De-
tecting Unusual and Deceptive Communication in 
Email. Queen?s University, CA 
 
 
 
 
 
411
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 71?81, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generative Goal-Driven User Simulation for Dialog Management
Aciel Eshky
ILCC
School of Informatics
University of Edinburgh
a.eshky@sms.ed.ac.uk
Ben Allison
ILCC
School of Informatics
University of Edinburgh
{ballison, steedman}@inf.ed.ac.uk
Mark Steedman
ILCC
School of Informatics
University of Edinburgh
Abstract
User simulation is frequently used to train
statistical dialog managers for task-oriented
domains. At present, goal-driven simula-
tors (those that have a persistent notion of
what they wish to achieve in the dialog) re-
quire some task-specific engineering, making
them impossible to evaluate intrinsically. In-
stead, they have been evaluated extrinsically
by means of the dialog managers they are in-
tended to train, leading to circularity of argu-
ment. In this paper, we propose the first fully
generative goal-driven simulator that is fully
induced from data, without hand-crafting or
goal annotation. Our goals are latent, and take
the form of topics in a topic model, clustering
together semantically equivalent and phoneti-
cally confusable strings, implicitly modelling
synonymy and speech recognition noise. We
evaluate on two standard dialog resources,
the Communicator and Let?s Go datasets, and
demonstrate that our model has substantially
better fit to held out data than competing ap-
proaches. We also show that features derived
from our model allow significantly greater im-
provement over a baseline at distinguishing
real from randomly permuted dialogs.
1 Introduction
Automatically simulating user behaviour in human-
machine dialogs has become vital for training sta-
tistical dialog managers in task-oriented domains.
These managers are often trained with some vari-
ant of reinforcement learning (Sutton and Barto,
1998), where optimal behaviour is sought or learnt
through the exploration of the space of possible di-
alogs. Although learning by interacting with human
subjects is a possibility (Gas?ic? et al2011), it has
been argued that user simulation avoids the expen-
sive, labour intensive, and error-prone experience of
exposing real humans to fledgling dialog systems
(Eckert et al1997).
Training effective dialog managers should benefit
from exposure to properties exhibited by real users.
Table 1 shows an example dialog in a domain such
as we consider, where the objective is to simulate at
the semantic level. In such task oriented domains,
the user has a goal (in this case, to book a flight
from New York to Osaka), and the machine is tasked
with fulfilling it. Notice that the user is consistent
with this goal throughout the dialog, in that they do
not provide contradictory information (although an
ASR error is present), but that every mention of their
destination city uses a different string. This moti-
vates our first desideratum: that simulation be con-
sistent over the course of a dialog. Furthermore, one
can imagine users not always responding identically
in identical situations: we thus additionally require
variability. In this paper we demonstrate a fully gen-
erative, latent variable probability model exhibiting
both of these properties.
Thus far, consistent simulators have been par-
tially deterministic and have required some hand-
engineering. As a result, it has only been possible to
evaluate them extrinsically using dialog managers.
This is circular because we need simulators to train
managers, but need managers to evaluate simulators.
The issue is that judgements of quality of each de-
pend on the specifics of the other and that a proper
evaluation of one depends on the correct function-
ing of the other. Furthermore, there is little reason to
assume that because a simulator performs well with
a certain dialog manager, it would perform similarly
71
Speech Semantic Representation
M: Hello, How Can I help? M: GREETING
M: META REQUEST INFO
U: A trip from New York City to Osaka, U: PROVIDE orig city New York City
please. U: PROVIDE dest city Salt Lake City
M: Leaving from New York City to Salt Lake M: IMPLICIT CONFIRM orig dest city
City. What day would you like to travel? M: REQUEST depart date
U: No, no. Leaving from New York to Osaka U: NO ANSWER null no
in Japan. U: PROVIDE orig city New York
U: PROVIDE dest city Osaka Japan
M: Leaving from New York to Osaka Japan, M: EXPLICIT CONFIRM orig city
correct? M: EXPLICIT CONFIRM dest city
U: Yes. U: YES ANSWER null yes
Table 1: An example of a dialog in speech and its semantic equivalent. M and U denote machine and user utterances
respectively. Note how a single speech utterance is split by the semantic parser into multiple logical utterances, each
of which is broken down to an ACT, slot, and value. We consider resources where gold standard transcriptions are not
available; thus there will be speech recognition noise, e.g. Osaka rendered as Salt Lake City, something our model
is able to capture.
well with other managers. In contrast, a probabilistic
formulation such as we propose allows us to evalu-
ate our models intrinsically using standard machine
learning metrics, and without reference to a specific
manager, thus breaking the circularity, and guarding
against such experimental biases.
We demonstrate the efficacy of our model on
two tasks, and compare it to two other approaches.
Firstly we use a standard bigram model as conceived
by Eckert et al1997) and Levin and Pieraccini
(2000); secondly we compare to a probabilistic goal-
based simulator where the goals are string literals,
as envisaged by Scheffler and Young (2002) and
Schatzmann et al2007b). We demonstrate sub-
stantial improvement over these models in terms of
predicting heldout data on two standard dialog re-
sources: DARPA Communicator (Levin et al2000;
Georgila et al2005b) and Let?s Go (Black and Es-
kenazi, 2009).
2 Related Work
2.1 Related Work on User Simulation
User simulation as a stochastic process was first en-
visioned by Eckert et al1997): their Bigram model
conditions user utterances exclusively on the pre-
ceding machine utterance. This was extended by
Levin and Pieraccini (2000), who manually restrict
the model to estimating ?sensible? pairs of user and
machine utterances by assigning all others probabil-
ity zero.
Bigram models ensure that a locally sensible
response to a machine utterance is provided by
the simulator; however, they do not ensure that
it provides responses consistent with one another
throughout the dialog. Several approaches have at-
tempted to overcome this problem. Pietquin (2004),
for example, explicitly models a user goal as a set
of slot-value pairs randomly generated once per dia-
log. He then hand selects parameters to ensure that
the user?s actions are in accordance with their goal.
Jung et al2009) use large amounts of dialog
state annotations (e.g. what information has been
provided so far) to learn Conditional Random Fields
over the user utterances, and assume that those fea-
tures ensure user consistency. Georgila et al2005a)
instead consider only act-slot pairs, and thus incon-
sistency is not a factor.
Scheffler and Young (2002) simulate user be-
haviour by introducing rules for actions that depend
on the user goal, and probabilistic modelling for ac-
tions that are not goal-dependent. They then map
out a decision network that determines user actions
at every node prior to the start of the dialog. Agenda-
based user simulation, another approach from the lit-
erature, assumes a probability distribution over the
72
user goal which is either induced from data (Schatz-
mann et al2007b), or is manually set when no data
is available (Schatzmann et al2007a). An agenda,
which is a stack-like structure of utterances to be
produced given the goal, is then devised determin-
istically. Keizer et al2010) combine the decision
network with the agenda and goal to allow for some
variability for some actions. These models ensure
consistency but restrict the variability in user be-
haviour that can be accommodated. Furthermore,
because these approaches do not define a complete
probability distribution over user behaviour, they re-
strict possibilities for their evaluation, a point to
which we now turn.
2.2 Related Work on Simulator Evaluation
No standardised metric of evaluation has been estab-
lished for user simulators largely because they have
been so inextricably linked to dialog managers. The
most popular method of evaluation relies on gener-
ating synthetic dialogs through the interaction of the
user simulator with some dialog manager. Schatz-
mann et al2005) hand-craft a simple determin-
istic dialog manager based on finite automata, and
compute similarity measures between these synthet-
ically produced dialogs and real dialogs. Georgila
et al2006) use a scoring function to evaluate syn-
thetic dialogs using accuracy, precision, recall, and
perplexity, while Schatzmann et al2007b) rely
on dialog completion rates. Williams (2008) use
a Cramer?von Mises test, a hypothesis test to de-
termine whether simulated and real dialogs are sig-
nificantly different, while Janarthanam and Lemon
(2009) use Kullback Leibler Divergence between the
empirical distributions over acts in real and simu-
lated dialogs. Singh et al2000) and Ai and Lit-
man (2008) judge the consistency of human quality
ranked synthetic dialogs generated by different sim-
ulators interacting with the IT-SPOKE dialog sys-
tem.
Schatzmann et al2007b) use a simulator to train
a statistical dialog manager and then evaluate the
learned policy. Because this only indirectly evalu-
ates the simulator, it is inappropriate as a sole mea-
sure of quality.
There has been far less evaluation of simulators
without a dialog manager. The main approach is
to compute precision and recall on an utterance ba-
sis, which is intended to measure the similarity be-
tween real user responses in the corpora and simu-
lated user responses produced under similar circum-
stances (Schatzmann et al2005; Georgila et al
2006). However, this is a harsh evaluation as it as-
sumes a correct or ?best? answer, and penalises valid
variability in user behaviour.
3 Dialog as a Statistical Process
We consider a dialog to be a series of turns, com-
prised of multiple utterances. Each Utterance con-
sists of an ACT, a slot, and a value, as shown in Ta-
ble 1. Dialogs proceed by the user and the machine
alternating turns. Because the dialogs are of mixed
initiative, there is no restriction on the number of
contiguous machine or user utterances.
Our aim is to model the user, and are interested
in the conditional distribution of the user utterances
given the dialog up to that point. In other words, we
are interested in the distribution p (ui|d1 . . . di?1),
where dn is either a machine utterance mn or a user
utterance un.
4 Models of Users in Dialogs
This section describes several models of increas-
ing complexity: a Bigram model, which serves as
a baseline; an upper-bound on String-Goal models,
which we design to mimic the behaviour of previous
goal-based approaches, but with a probabilistic for-
mulation; and finally our approach, the Topic-Goal
model.
4.1 Bigram Model
The simplest model we define over dialogs is the bi-
gram model of Eckert et al1997):
p (ui|m) = p (ui|mi?1) (1)
p (u|m) =
?
i
p (ui|m) (2)
The probability of each user utterance ui (the com-
plete {ACT, slot, value} triple) is dependent only on
the machine utterance immediately preceding it (the
slight abuse of notationmi?1 here does not mean the
utterance at i?1 in the machine utterance list, but the
utterance immediately preceding the i-th), and utter-
ances in the dialog are conditionally independent of
73
one another. (Georgila et al2006) found no bene-
fit from increasing the Markov horizon). Since each
utterance is generated independently of others in the
dialog with the same context, there is no enforced
consistency between utterances.
Since we require a distribution over all possible
utterances, assigning non?zero probability to cases
outside of the training data, our bigram model is in-
terpolated with a unigram model, which itself is in-
terpolated with a smoothing model which assumes
independence between the act, slot, and value el-
ements of the utterance. Interpolation weights are
set to maximise probability of a development set of
dialogs. Each sub-model uses the maximum like-
lihood estimator (the relative frequency of the ut-
terance), and unseen machine utterances place full
weight on the unigram/smoothed model (ignoring
the bigram probability since it has no meaning if
mi?1 is unobserved). We label this model the Bi-
gram model in subsequent experiments.
4.2 Goal-Based Models
One way to ensure consistency and more realistic
behaviour is to have a goal for the user in the dia-
log, which corresponds to values for slots required in
the problem. For instance, they might be the origin
and destination cities in a flight booking domain. In
standard machine learning terms, the goal becomes
a latent variable g in a probability model. We can
then define a distribution over utterances as:
p (ui|m, g) = p (ui|mi?1, g) (3)
p (u|m) =
?
g
p (g)
?
i
p (ui|mi?1, g) (4)
4.3 An Upper-Bound on String-Goal Models
The simplest variant of g has string values for each
of the slots the user is required to provide in order
for the dialog to succeed. Thus we may have:
g = [orig city: New York; dest city: Osaka]
as presented in Schatzmann et al2005) and Schatz-
mann et al2007b). However, in these simulators,
while the goal is probabilistic, there is no distribu-
tion over utterances given the goal because utter-
ances are assembled deterministically from a series
of rule applications. There is also no marginalisation
over the goal as in (4) above.
The issue with a model of user goals as strings
in this fashion is that users describe the same val-
ues in multiple ways (Osaka Japan, Osaka), and
speech recognition errors corrupt consistent user
input (Osaka mis-recognised as Salt Lake City).
Users also might legitimately switch their goals mid-
dialog. Inference in the model would have allow
for these possibilities: we would have to marginalise
over all possible goal switches.
For the sake of comparison, we compute an upper-
bound on string-goal models, which gives a flavour
for how such models would perform optimistically.
The upper-bound assigns probability to dialogs as
follows: for each utterance ui if the corresponding
value vi has been seen before in the dialog, the prob-
ability used for that utterance is just p (ai, si|mi?1),
that is, the probability of the act ai and slot si only;
there is no penalty for repetition of the value. If the
value is unseen in the dialog, we use the full proba-
bility of the utterance from the bigram model as de-
scribed above. This is optimistic because there is no
penalty for repeated goal changes besides that im-
posed by the bigram model itself, and no penalty is
imposed for choosing between previously sampled
goals as would be necessary in a probability model.
Any string-based model necessarily assigns lower
probabilities to data than the upper bound, because
it would penalise goal changes (in a probabilistic
sense; that is, there would be a term to reflect the
probability of some new goal given the old) to al-
low for the discrepancy in values present in dialogs.
In contrast, our upper bound does not include such
a term. Furthermore, once multiple goal values had
been uttered in the dialog, we would have to sample
one to use for the next utterance, which would again
incur some cost: again, we do not have such a cost
in our upper bound.
We could in theory use an external model of noise
to account for these value discrepancies (and the
ASR errors we model in the next section). However,
this would further decrease the probability, as some
probability mass currently assigned to the heldout
data would have to be reserved for the possibility of
string renderings other than those we observe.
It bears reiterating that our upper bound on string-
goals is not a generative model: however, it allows
us to assign probabilities to unseen data (albeit op-
timistically), and thus provides us with a point of
74
comparison. Although not technically a model, we
refer to this as the String-Goal model for the remain-
der of the paper.
4.4 Topic-Goal Model
To motivate our proposal, consider that over the
course of a dialog one could look at the set of all
values used for some slot, for example the destina-
tion city, as a count vector:
vdest city = Salt Lake:1; Osaka:2; Osaka Japan:1
The above vector may arise because the user actu-
ally wants to go to Osaka, but the destination is
initially mis-recognised as Salt Lake, and the user
finally disambiguates with the addition of the coun-
try. Such situations are common in the noisy dia-
log resources from which simulators are induced?
however, any string-based goal will necessarily con-
sider these different renderings to be different goals,
and will require resampling or smoothing terms to
deal with them.
Our approach instead treats the count vector as
samples from a topic model; that is, a mixture over
multinomial distributions. Whilst by far the most
popular topic model is LDA (Blei et al2003), it
provides too flexible a distribution over count vec-
tors to be used with such small samples (we con-
firmed the poor suitability of this model in pre-
liminary experiments). Instead we use the simpler
Mixture-of-Multinomials model, where the latent
topic is sampled once per dialog instead of once per
value uttered. We describe below how parameters to
this model are estimated, and focus for now on how
the resulting model assigns probability to dialogs.
In this formulation, the latent goal for each slot,
which was previously a string, now becomes an in-
dicator for a topic in a topic model. Each topic can
in theory generate any string (so the model is inher-
ently smoothed), but most strings in most topics will
have only the smoothing weight and most probabil-
ity mass will be on a small number of highly corre-
lated strings. We treat the slots as being independent
of one another in the goal, and thus:
p(g) =
?
s
p (zs) (5)
Where zs is the topic indicator for some slot s. If slot
s has associated with it a count vector of values vs,
each looking like the example above, then the distri-
bution over the values used for each slot becomes:
p (vs) =
?
zs
p (zs) p (vs|zs) (6)
We then define a bigram-based Act model to de-
scribe the probabilities of the {ACT, slot} pairs to
which these values belong, so that:
p (u|m) =
?
s
p (zs) ?
?
i
p (ai, si|mi?1) p (vi|zsi)
(7)
In reality, some slots will not have corresponding
values, or will be slots whose values are not appro-
priate to model in the above way. Dates and times,
for example, have ordinal and structural relations be-
tween them, and a model which treats them as dis-
connected entities is inappropriate. For utterances
defined over such slots we use a standard bigram
model as in (1), and for appropriate utterances we
use a topic-goal model as in (7). This constitutes
the only domain knowledge necessary to adapt the
model for a new resource. We refer to this model as
the Topic-Goal model.
4.4.1 Topic Model Parameter Estimation
Our topic model is a Bayesian version of the
Mixture-of-Multinomials model. Under this model,
each dialog has associated with it a latent variable
zs for each slot s in the goal, which indicates which
topic is used to draw the values for that slot. Con-
ditioned on z, independent samples are drawn from
the distribution over words to which that value of
z corresponds?however, the effect in the marginal
distribution over words is to strongly prefer sets
which have co-occurred in training as these are as-
signed to the same topic.
Bayesian inference in mixture models has been
described in detail in Neal (1991) and Griffiths and
Steyvers (2004), so we give only a brief account here
for our particular model. We take r appropriately-
spaced samples from a Gibbs? sampler over the pos-
terior mixture parameters ?, ?: ? are the word-topic
parameters and ? are the mixture proportions. We
assume a uniform Dirichlet prior on ? and ?, lead-
ing to Dirichlet posteriors which we integrate out in
the predictive distribution over v using the standard
Dirichlet integral. For each of our r samples we have
75
components z parameterised by ?rz (the Dirichlet
parameter for the z-th mixture component in the r-
th sample) and ?rzj for each word j in the z-th topic
for the r-th sample. The ? notation indicates a sum
over the corresponding index, i.e. ?r? =
?
z ?rz .
Then:
p (v) =
1
|r|
?
r
?
z
?rz
?r?
p (v|?rz) (8)
p (v|?) =
? (??)
? (?? + v?)
?
j
? (vj + ?j)
? (?j)
(9)
This states that each of the r samples has topics z
which are multinomial distributions with posteriors
governed by parameters ?rz . For any of these top-
ics, the distribution over v is as given in Equation
(9) (we suppress the subscripting of ? here for the
different samples and topics, since this holds what-
ever its value). The final predictive probability given
in Equation (8) averages over the samples r and the
topics z (with topics weighted by their parameters
?rz).
5 Experimental Setup
Our experiments use two standard corpora, the
first of which is DARPA Communicator (DC), a
flight booking domain collected between 2000-2001
through the interaction of real users with 10 different
systems (Levin et al2000). It was later automati-
cally annotated by Georgila et al2005b) to include
semantic information. The second corpora is Let?s
Go (LG), years 2007, 2008, and 2009, distributed as
part of the Spoken Dialog Challenge (Black and Es-
kenazi, 2009). Let?s Go is a bus routing domain in
Pittsburgh collected by having the general public in-
teract with the CMU dialog system to find their way
through the city. The dialogs in both corpora are of
mixed-initiative, having a free number of contiguous
system and user responses.
We preprocessed the corpora, converting Com-
municator XML-tagged files and Let?s Go system
log files into sequences of ACT, slot, and value ut-
terances. Table 2 gives examples. We then divided
the corpora into training, development and test sets
as follows: Communicator contains 2285 dialogs in
total, and Let?s Go contains 17992, and in each case
we selected 80% of dialogs at random for training,
10% for development, and 10% for testing.
DC: PROVIDE INFO orig city Boston
LG: INFORM place [departure place CMU,
arrival place airport]
Table 2: Example utterances from the two corpora. Note
how in addition to the value, the Let?s Go utterances con-
tain properties (departure place and arrival place).
Let?s Go is a noisy corpus that contains far more
speech recognition errors than Communicator. In
addition, users tend to be more flexible with their
bus routes than they are with their flight destinations,
and so values are a lot more varied throughout the
course of Let?s Go dialogs than Communicator ones.
Furthermore, Let?s Go semantic parses contain am-
biguity not present in Communicator; the parser
fails to distinguish departure from arrival places over
90% of the time, and instead assigns them a generic
Single Place property. Our current model assumes
the decisions made by the semantic parser are cor-
rect. In reality however, a better model would in-
corporate potential noise in the semantic parse in a
joint model. We defer this more complex treatment
for future work.
Free model parameters are set by a simple search
on the development set, where the objective is
likelihood?for the bigram model the parameters are
the interpolation weights, and for the topic model we
search for the number of topics and smoothing con-
stant for the topic distributions. For Let?s Go, since
we can have multiple places provided in a single act,
we treat each utterance as containing a set of values
and build the count vector for the topic model as the
union of these sets over the whole dialog. The slots
over which the topic model is defined for Commu-
nicator are dest city and orig city (this takes into ac-
count PROVIDE and REPROVIDE acts). For Let?s Go
we derive the model over the three properties: sin-
gle place, arrival place and departure place, as op-
posed to the less informative slot place.
6 Evaluating the Simulators
We evaluate each of the models in terms of the prob-
ability they assign to the test data. This metric is
more suitable than the precision and recall metrics
which have been previously used, because it ac-
knowledges that, rather than each user response be-
ing ?correct? at the point which it is observed, there
76
Model DC(A) DC(P) LG(A) LG(P)
Topic 252.78 860.2 113.45 1417.06
String 270.09 1286.03 169.87 4578.23
Bigram 347.88 5979.53 223.23 10125.87
Act 9.56 5.2 2.77 2.34
Table 3: The mean per-utterance perplexity on heldout
data. DC-A is all acts for Communicator, while DC-P is
the calculated on PROVIDE acts alone (the acts on which
our model is designed to improve prediction). LG-A and
LG-P have the same meaning for Let?s Go.
is a distribution over possible responses. Because
the models we define are full probability models, we
are able to compute this metric and do not need to
use an arbitrarily selected dialog manager for evalu-
ation.
The heldout probability metric should be under-
stood as a means of comparing the relative viabil-
ity of different models of the same data. Note that
we are reporting the probability of unobserved data,
rather than data from which the models were in-
duced, and are thus measuring the generalisability
of the models (in contrast, maximising the proba-
bility of the training data would simply encourage
overfitting). The absolute numbers are hard to inter-
pret, as there is no hard upper bound; while it may
be appealing to think of an upper bound of 1, this is
incorrect as it would imply that there was no vari-
ability in the data. However, it should be understood
that assigning particular behaviour higher probabil-
ity means that the model is more likely to exhibit
it when run in simulation mode?and since the user
behaviour in question has not been seen at training
time, this measures the extent to which the models
have generalised beyond the training data relative to
one another.
We report the mean per-utterance log probability
of unseen data, that is, the probability of the whole
heldout corpus divided by the number of user utter-
ances.
6.1 Results
Figure 1 shows the results of our evaluation. We see
that the Bigram model is weak on both resources.
The results of the String Goal model suggest that,
even using the generous evaluation we do here, there
20 40 60 80 100?
8.0
?
7.5
?
7.0
?
6.5
?
6.0
?
5.5
?
5.0
?
4.5
Percent of Training Examples
Mea
n Pe
r?U
ttera
nce
 Log
 Pro
babi
lity
Topic (DC)String (DC)Bigram (DC)Topic (LG)String (LG)Bigram (LG)
Figure 1: Heldout probability of the two resources for
varying percentages of training dialogs. Note that while
the percentages match across resources, Let?s Go is much
larger and thus the absolute numbers of dialogs are differ-
ent, which explains the better performance on Let?s Go.
is much variability due to synonymy and recognition
errors which string goals are unable to capture (in
contrast to our Topic Goal model). The Topic Goal
model explains this much more easily by grouping
commonly co?occurring values into the same topic.
Table 3 shows the perplexities corresponding to the
performances with 100% training data for all acts
and just PROVIDE acts (perplexity is 2?lp where lp is
the log probability). Improvements are more appar-
ent when we compute the probability over PROVIDE
acts alone, which the models are designed to handle.
And since perplexity is not on a log scale, the differ-
ences are more pronounced. The Act model, which
is a bigram model over {ACT, slot} pairs alone ex-
cluding the values, demonstrates the vast discrep-
ancy in uncertainty between the full problem and the
valueless prediction problem. We note that the per-
plexity of our Act model on Communicator is com-
parable to that of Georgila et al2006).
6.2 Example Simulator Behaviour
In this section we give examples of our Topic
Goal model simulator in generation mode, which
corresponds to sampling from the induced model.
77
d zdest city [probability] proportion user utterance given topic zdest city and
of samples machine utterance REQUEST INFO dest city
Norfolk Virginia [0.562] 0.264 PROVIDE INFO dest city Norfolk Virginia
Norfolk [0.234] 0.111 PROVIDE INFO dest city Norfolk
1 Newark Virginia [0.088] 0.039 PROVIDE INFO dest city Newark Virginia
Virginia Beach [0.0412] 0.028 PROVIDE INFO orig city Las Vegas Nevada
Newark [0.040] 0.028 NO ANSWER null no
0.025 COMMAND start over start over
Chicago [0.350] 0.164 PROVIDE INFO dest city Chicago
Chicago Illinois [0.182] 0.082 PROVIDE INFO dest city Chicago Illinois
2 Duluth Minnesota [0.124] 0.057 PROVIDE INFO dest city New Orleans
New Orleans [0.122] 0.055 PROVIDE INFO dest city Duluth Minnesota
New Orleans Louisiana [0.085] 0.039 PROVIDE INFO dest city New Orleans Louisiana
0.028 NO ANSWER null no
Anchorage [0.539] 0.252 PROVIDE INFO dest city Anchorage
Anchorage Alaska [0.148] 0.072 PROVIDE INFO dest city Anchorage Alaska
3 Jacksonville Florida [0.124] 0.056 PROVIDE INFO dest city Jacksonville Florida
Great Anchorage Alaska [0.098] 0.048 PROVIDE INFO dest city Great Anchorage Alaska
Duluth Minnesota [0.057] 0.047 PROVIDE INFO orig city Hartford Connecticut
0.026 PROVIDE INFO dest city Duluth Minnesota
Table 4: Examples of sampling from the topic goal model. Left: top 5 strings (with probabilities) sampled from topics
for three different dialogs d. Right: top 6 utterances (plus fraction of samples in 10,000) generated in response to the
machine utterance ?REQUEST INFO dest city? and conditioned on the topic zdest city .
Our examples are drawn from the model induced
for the Communicator data. Sampling from stan-
dard distributions can be implemented following
the algorithms in Bishop (2006) and other statisti-
cal resources. Utterances are sampled by sampling
ACT, slot pairs from the distribution p (ai, si|mi?1)
(drawing a value from a multinomial distribution). If
we sample a PROVIDE INFO act, we check whether
we have sampled a topic for the corresponding slot
thus far in the dialog. If not, we sample one by
drawing a topic indicator from p(zs) =
?rz
???
and then
drawing a multinomial distribution over strings from
the Dirichlet posterior corresponding to z. Once the
topic for the slot is set, we sample values as draws
from the fixed multinomial and add these to the ACT,
slot pair.
Table 4 shows some examples drawn from the
model. For each row in the table (corresponding to a
new dialog d), we sample a topic for the dest city
and orig city as needed, and sample 10000 utter-
ances given that topic. The left hand side of the ta-
ble shows the top five strings in the sampled topic,
while the right hand side shows the top six utter-
ances in response to REQUEST INFO dest city. Note
that the proportion of utterances on the right does
not match the probability of the values on the left
because of the presence of other user acts besides
PROVIDE dest city.
7 Evaluating Model Consistency
Having shown in the previous section that our Topic
Goal model is a much better predictor of heldout
data than the String Goal model or Bigram model,
we now turn to a demonstration of the model?s cap-
turing of consistency.
In the face of value synonymy and ASR errors,
we define inconsistent dialogs to be ones that are lo-
cally coherent but lack the structure of a real dialog
from one turn to the next. We then suggest that an
appropriate task for consistent models is distinguish-
ing between consistent and inconsistent dialogs.
To test this hypothesis, we devise the following
classification problem: can we discriminate between
78
Baseline Dialog length (turns)
Mean, standard deviation, min and max acts per turn
Presence of special machine acts (flight offer and confirm)
Presence of user acts (provide a dest city and arrival city)
Proportion of acts which were provides
String Consistency Did the user provide inconsistent information about dest city?
Did the user provide inconsistent information about orig city?
Topic Model Ranked list of posterior probabilities of top 50 topics
Normalised probability of dialog for topic model
Table 5: Feature sets for consistency experiments
real dialogs and those generated by randomly sam-
pling turns from different dialogs? In this section we
induce classifiers over various feature sets to demon-
strate that we can, and that the Topic Goal model
contains far more useful information in this regard
than string-based consistency features. (The bigram
model by definition provides no help here, since the
units of which dialogs consist contain the entire win-
dow of context used for the bigram model).
We take our training and development data from
the Communicator corpus in the previous section,
and create a classification problem as follows: real
dialogs form positive examples in the classification
problem. To create negative examples, we sample
{machine, user} turns at random from the appropri-
ate resource. We keep a histogram over real dia-
log lengths, and sample a number of turns for our
?fake? dialogs proportional to this histogram. We
then sample this many turns from the frequency dis-
tribution over turns in the real data, and create ex-
actly as many dialogs in this fashion as real dialogs
in the data. The result is an equal number of dialogs
comprised of real turns, of (expected) real length,
but where the sequence of turns is highly unlikely to
be coherent given the random sampling. The classi-
fication problem is thus far from trivial. We do this
from our training data to produce data with which to
train the classifier, and from our development data
to provide test instances. This gives rise to 2500
training instances, and 500 test instances.
We learn linear SVMs with various features de-
scribed in Table 6. These feature sets are designed
to capture different aspects of consistency: the base-
line features are intended to capture surface level
features of the dialogs, inspired by (Schatzmann et
al., 2005) where they provide trivial separation of
real from simulated dialogs. However, our setting
is different: we do not seek to tell real dialogs from
fully simulated ones, but real dialogs from scram-
bled versions of real dialogs. In addition to length-
based features, we add binary presence indicator for
several user and machine acts highly correlated with
the completion of dialogs, as well as for acts which
indicate the provision of information and the propor-
tion of all acts occupied by these. The table gives a
complete list of these Baseline (B) features.
We derive a second set of features intended to
replicate the utility of string-based goals: we set up
binary features to fire if contradictory information is
provided for the slots over the course of the dialog.
These are our String Consistency (SC) features.
Finally, we use our topic-model simulator to de-
rive consistency features. Our features are the pos-
terior distribution over topics for each slot given that
dialog. Our topics are induced from the real training
dialogs, and their posterior probabilities computed
for all dialogs relative to this model. We take poste-
rior probabilities of the fifty most probable topics for
each of the dest city and orig city slots as features,
as well as the normalised log probability of the di-
alog (the log probability divided by the number of
user utterances). These form our Topic Model (TM)
features.
Our classifiers are linear SVMs, and we use lib-
svm (Chang and Lin, 2011), scaling features to the
range [0 ? 1]. All other parameters are left at their
defaults.
79
Feature Set Accuracy
Baseline (B) 74.34 ?3.77
String Consistency (SC) 63.60 ?4.27
B + SC 77.63 ?3.58
Topic Model (TM) 79.61 ?3.44
B + SC + TM 85.96 ?2.89
Table 6: Performances for the classifiers. Errors are 95%
intervals to the accuracies assuming they are parameters
to a binomial distribution
7.1 Results
The results of the classifiers are shown in Table 5.
Since we have an equally balanced binary classifi-
cation task, accuracy is the most appropriate metric.
Here we see that the baseline and string consistency
features have roughly the same discriminatory po-
tential, and their union produces a slight improve-
ment. The topic model features are far superior to
this, and the union of all three sets gives a further
improvement.
These results demonstrate that our model encodes
notions of consistency which go substantially be-
yond those defined at the level of strings. Features
defined over the latent topic goal space substantially
improve performance in a difficult discrimination
task, demonstrating that our model captures an im-
portant notion of how real dialogs appear that is not
shared by the other models we consider.
8 Concluding Remarks and Future Work
This paper presents a fully generative goal driven
user simulator, the first to merge both consistency
and variability within a fully probabilistic frame-
work. We evaluate our model on two task-based di-
alog domains, Let?s Go and Communicator, and find
it to outperform both a simple bigram model and an
upper bound on probability models where the strings
are represented as goals, in terms of the probability
the model assigns to heldout dialogs.
We then move on to show that features derived
from the model lead to substantial improvement in
detecting real dialogs from those where the turns
have been selected at random from all turns in the
training data: this is a fairly difficult task, but our
model allows significant improvement over strong
and sensible baselines.
Our model could be extended in a number of
ways. It could be improved to incorporate noise
resulting from the decisions made by the semantic
parser. Another possible improvement is to explore
the effects of introducing dependency between the
slots in the user goal, which would enforce more
plausible values pairings and would potentially im-
prove the simulator?s performance. The effects of a
dependence assumption between the different utter-
ances occurring in a single user turn under the act
model can also be explored. We would also like to
use our simulator to train a POMDP-based dialog
manager using a form of reinforcement learning.
References
Hua Ai and Diane J. Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using human
judges. In Proceedings of ACL-08: HLT.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer-Verlag New York, Inc.
Alan W. Black and Maxine Eskenazi. 2009. The Spo-
ken Dialogue Challenge. In Proceedings of SIGDIAL
2009, SIGDIAL ?09.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alation. Journal of Machine
Learning Research, 3:993?1022, March.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proceedings of IEEE Workshop on Auto-
matic Speech Recognition and Understanding.
M. Gas?ic?, F. Jurcicek, B. Thomson, K. Yu, and S. Young.
2011. On-line policy optimisation of spoken dia-
logue systems via live interaction with human subjects.
In Automatic Speech Recognition and Understanding,
2011 IEEE Workshop on, Hawaii, December.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2005a. Learning user simulations for information state
update dialogue systems. In Proceedings InterSpeech
2005.
Kallirroi Georgila, Oliver Lemon, and James Henderson.
2005b. Automatic annotation of communicator dia-
logue data for learning dialogue strategies and user
simulations. In Proceedings Ninth Workshop on the
Semantics and Pragmatics of Dialogue.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User Simulation for Spoken Dialogue Systems:
80
Learning and Evaluation. In Proceedings InterSpeech
2006.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Srinivasan Janarthanam and Oliver Lemon. 2009. A two-
tier user simulation model for reinforcement learn-
ing of adaptive referring expression generation poli-
cies. In Proceedings of SIGDIAL 2009, SIGDIAL ?09,
pages 120?123.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer Speech & Lan-
guage, 23(4):479?509.
Simon Keizer, Milica Gas?ic?, Filip Jurc???c?ek, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve Young.
2010. Parameter estimation for agenda-based user
simulation. In Proceedings of SIGDIAL 2010.
Esther Levin and Roberto Pieraccini. 2000. A stochastic
model of human-machine interaction for learning di-
alog strategies. In IEEE Transactions on Speech and
Audio Processing.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialog system. In In ICSLP.
Radford Neal. 1991. Bayesian Mixture Modeling by
Monte Carlo Simulation. Technical report, University
of Toronto.
Olivier Pietquin. 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Faculte?
Polytechnique de Mons, TCTS Lab (Belgique), apr.
Jost Schatzmann, Kallirroi Georgila, and Steve Young.
2005. Quantitative evaluation of user simulation tech-
niques for spoken dialogue systems. In Proceeings of
6th SIGDIAL Workshop.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007a. Agenda-based user
simulation for bootstrapping a POMDP dialogue sys-
tem. In HLT-NAACL (Short Papers), NAACL-Short
?07.
Jost Schatzmann, Blaise Thomson, and Steve Young.
2007b. Statistical User Simulation with a Hidden
Agenda. In Proceedings 8th SIDdial Workshop on
Discourse and Dialogue, September.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT 2002.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman,
and Marilyn A. Walker. 2000. Empirical evaluation of
a reinforcement learning spoken dialogue system. In
Proceedings of the Seventeenth National Conference
on Artificial Intelligence and Twelfth Conference on
Innovative Applications of Artificial Intelligence.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Jason D. Williams. 2008. Evaluating user simulations
with the Cramer-von Mises divergence. Speech Com-
munication, 50(10):829?846, October.
81
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 626?635,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Generative Model for User Simulation in a Spatial Navigation Domain
Aciel Eshky
1
, Ben Allison
2
, Subramanian Ramamoorthy
1
, and Mark Steedman
1
1
School of Informatics, University of Edinburgh, UK
2
Actual Analytics Ltd., Edinburgh, UK
{a.eshky,s.ramamoorthy,steedman}@ed.ac.uk
ballison@actualanalytics.com
Abstract
We propose the use of a generative model
to simulate user behaviour in a novel task-
oriented dialog domain, where user goals
are spatial routes across artificial land-
scapes. We show how to derive an effi-
cient feature-based representation of spa-
tial goals, admitting exact inference and
generalising to new routes. The use of
a generative model allows us to capture
a range of plausible behaviour given the
same underlying goal. We evaluate intrin-
sically using held-out probability and per-
plexity, and find a substantial reduction in
uncertainty brought by our spatial repre-
sentation. We evaluate extrinsically in a
human judgement task and find that our
model?s behaviour does not differ signif-
icantly from the behaviour of real users.
1 Introduction
Automated dialog management is an area of re-
search that has undergone rapid advancement in
the last decade. The driving force of this innova-
tion has been the rise of the statistical paradigm
for monitoring dialog state, reasoning about the
effects of possible dialog moves, and planning fu-
ture actions (Young et al., 2013). Statistical di-
alog management treats conversations as Markov
Decision Processes, where dialog moves are as-
sociated with a utility, estimated online by inter-
acting with a simulated user (Levin et al., 1998;
Roy et al., 2000; Singh et al., 2002; Williams and
Young, 2007; Henderson and Lemon, 2008). Slot-
filling domains have been the subject of most of
this research, with the exception of work on trou-
bleshooting domains (Williams, 2007) and rela-
tional domains (Lison, 2013).
Although navigational dialogs have received
much attention in studies of human conversational
behaviour (Anderson et al., 1991; Thompson et
al., 1993; Reitter and Moore, 2007), they have not
been the subject of statistical dialog management
research, and existing systems addressing naviga-
tional domains remain largely hand crafted (Ja-
narthanam et al., 2013). Navigational domains
present an interesting challenge, due to the dispar-
ity between the spatial goals and their grounding
as utterances. This disparity renders much of the
statistical management literature inapplicable. In
this paper, we address this deficiency.
We focus on the task of simulating user be-
haviour, both because of the important role sim-
ulators plays in the induction of dialog managers,
and because it provides a self-contained means of
developing the domain representations which fa-
cilitate dialog reasoning. We show how a genera-
tive model of user behaviour can be induced from
data, alleviating the manual effort typically in-
volved in the development of simulators, and pro-
viding an elegant mechanism for reproducing the
natural variability observed in human behaviour.
1.1 Spatial Goals of Users
Users in task-oriented domains are goal-directed,
with a persistent notion of what they wish to ac-
complish from the dialog. In slot-filling domains,
goals are comprised of a group of categorical en-
tities, represented as slot-value pairs. These en-
tities can be placed directly into the user?s utter-
ance. For example, in a flight booking domain, if
a user?s goal is to fly to London from New York
on the 3
rd
of November, then the goal takes the
form: {origin=?New York?, dest=?London?, de-
part date=?03-11-13?}, and expressing the desti-
nation takes the form: Provide dest=?London?.
In contrast, consider the task of navigating
somebody across a landscape. Figure 1 shows a
pair of maps taken from a spatial navigation do-
main, the Map Task. Because the Giver aims to
communicate their route, one can view the route
626
Natural Language Semantic Representation
G: you are above the camera shop Instruct POSITION(ABOVE, LM)
F : yeah Acknowledge
G: go left jus? just to the side of the paper, ? Instruct MOVE(TO, PAGE LEFT) ?
then south, Instruct MOVE(TOWARDS, ABSOLUTE SOUTH)
under the parked van  Instruct MOVE(UNDER, LM) 
you have a parked van? Query-yn
F : a parked van no Reply-n
G: you go? you just go west, ? Clarify MOVE(TOWARDS, ABSOLUTE WEST) ?
and down, Clarify MOVE(TOWARDS, ABSOLUTE SOUTH)
and then you go along to the? you go east  Clarify MOVE(TOWARDS, ABSOLUTE EAST) 
F : south then east Check
G: yeah Reply-y
Table 1: A Giver (G) and a Follower (F ) alternating turns in a dialog concerning the maps in Figure 1.
The utterances are shown in natural language (left), and the semantic equivalent (right), which is com-
posed of Dialog Acts and SEMANTIC UNITS. Utterances marked ? demonstrate a plausible variability in
expressing the same part of the route on the Giver?s map, and similarly those marked . We model the
Giver?s behaviour, conditioned on the Follower?s, at the semantic level.
as the Giver?s goal for the dialog. However, unlike
goals in slot-filling domains, it is unclear whether
the route can be represented categorically in a
form that would allow the giver to communicate
it by placing it directly into an utterance. As raw
data, a specific route is represented numerically as
a series of pixel coordinates. Before modelling in-
terlocutors in this domain, we must derive a mean-
ingful representation for the spatial goals, and then
devise a mechanism that takes us from the spatial
goals to the utterances which express them.
1.2 Utterance Variability for the Same Goal
In addition to making sensible utterances, a con-
cern for user simulation is providing plausible
variability in utterances, to provide dialog man-
agers with realistic training scenarios. Consider
the dialog in Table 1, resulting from the maps in
Figure 1. Utterances marked ? (and similarly those
marked ) illustrate how the same route can be
described in different ways, not only at the natu-
ral language level, but also at the semantic level
1
.
A model providing a 1-to-1 mapping from spatial
routes to semantic utterances would fail to capture
this phenomenon. Instead, we need to be able to
account for plausible variability in expressing the
underlying spatial route as semantic utterances.
1
Route descriptor TOWARDS indicates a movement in the
direction of the referent ABS WEST, whereas TO indicates a
movement until the referent is reached.
Giver Follower
Figure 1: In the Map Task, the instruction Giver?s
task is to communicate a route to a Follower,
whose map may differ. The route can be seen as
the Giver?s goal which the Follower tries to infer.
A corresponding dialog is shown in Table 1.
1.3 Overview of Approach
In order to perform efficient reasoning, we pro-
pose a new feature-based representation of spatial
goals, transforming them from coordinate space to
a low-dimensional feature space. This groups sim-
ilar routes together intelligently, permitting exact
inference, and generalising to new routes. To ad-
dress the problem of variability of utterances given
the same underlying route, we learn a distribution
over possible utterances given the feature vector
derived from a route, with probability proportional
to the plausibility of the utterance.
Because this domain has not been previously
addressed in the context of dialog management or
user simulation, there is no directly comparable
prior work. We thus conduct several novel evalu-
627
ations to validate our model. We first use intrinsic
information theoretic measures, which compute
the extent of the reduction in uncertainty brought
by our feature-based representation of the spatial
goals. We then evaluate extrinsically by gener-
ating utterances from our model, and comparing
them to held-out utterance of real humans in the
test data. We also utilise human judgements for
the task, where the judges score the output of the
different models and the human utterances based
on their suitability to a particular route.
2 Related Work
2.1 Related Work on the Map Task
To our knowledge, there are no attempts to model
instruction Givers as users in the Map Task do-
main. Two studies model the Follower, in the con-
text of understanding natural language instructions
and interpreting them by drawing a route (Levit
and Roy, 2007; Vogel and Jurafsky, 2010). Both
studies exclude dialog from their modelling. Al-
though their work is not directly comparable to
ours, they provide a corpus suitable for our task.
2.2 Related Work on User Simulation
Early user simulation techniques are based on N-
grams (Eckert et al., 1997; Levin and Pieraccini,
2000; Georgila et al., 2005; Georgila et al., 2006),
ensuring that simulator responses to a machine ut-
terance are sensible locally. However, they do not
enforce user consistency throughout the dialog.
Deterministic simulators with trainable parame-
ters mitigate the lack of consistency using rules in
conjunction with explicit goals or agendas (Schef-
fler and Young, 2002; Rieser and Lemon, 2006;
Pietquin, 2006; Ai and Litman, 2007; Schatzmann
and Young, 2009). However, they require large
amounts of hand crafting and restrict the variabil-
ity in user responses, which by extension restricts
the access of the dialog manager to potentially in-
teresting states. An alternative approach to dealing
with the lack of consistency is to extend N-grams
to explicitly model user goals and condition utter-
ances on them (Pietquin, 2004; Cuay?ahuitl et al.,
2005; Pietquin and Dutoit, 2006; Rossignol et al.,
2010; Rossignol et al., 2011; Eshky et al., 2012).
3 The Model
Our task is to model the Giver?s utterances in re-
sponse to the Follower?s, at the semantic level. A
Giver?s utterance takes the form:
g = Instruct, u = MOVE (UNDER, LM)
consisting of a dialog act g and a semantic unit u
2
.
Aligned with u, is an ordered set of waypoints W ,
corresponding only to part of the route u describes.
Figure 2(a) shows an example of such a sub-route.
The point-setW can be seen as the Giver?s current
goal on which they base their behaviour. Because
the routes are drawn on the Giver?s maps, we treat
W as observed.
To model some of the interaction between the
Giver and the Follower, we additionally consider
in our model the previous dialog act of the Fol-
lower, which could for example be:
f = Acknowledge
Given point-set W and preceding Follower act
f , as the giver, we need to determine a procedure
for choosing which dialog act g and semantic unit
u to produce. In other words, we are interested in
the following distribution:
p (g, u|f,W ) (1)
which says that, as the Giver, we select our utter-
ances on the basis of what the Follower says, and
on the set of waypoints we next wish to describe.
To formalise this idea into a generative model,
we assume that the Giver act g depends only on
the Follower act f . We further assume that the se-
mantic unit u depends on the set of waypoints W
which it describes, and on the Giver?s choice of
dialog act g. Thus, u and f are conditionally in-
dependent given g. This provides a simple way of
incorporating the different sources of information
into a complete generative model
3
. Using Bayes?
theorem, we can rewrite Equation (1) as:
p (g, u|f,W ) =
p(u) p(g|u) p(f |g) p(W |u)
?
g
?
u
?
p(u
?
) p(g
?
|u
?
) p(f |g
?
) p(W |u
?
)
(2)
requiring four distributions: p(u), p(g|u), p(f |g),
and p(W |u). The first three become the seman-
tic component of our model, to which we dedi-
cate Section 3.1. The fourth is the spatio-semantic
component, to which we dedicate Sections 3.2?
3.4.
2
We align g and u in a preprocessing step, and store the
names of landmarks which the units abstract away from.
3
Further advancements to this work would investigate the
effects of relaxing the conditional independence assumption.
628
3.1 The Semantic Component
The semantic component concerns only the cate-
gorical variables, f , g, and u, and addresses how
the Giver selects their semantic utterances based
on what the Follower says. We model the distri-
butions u, g|u, and f |g from Equation (2) as cate-
gorical distributions with uniform Dirichlet priors:
u ? Cat(?) ? ? Dir() (3a)
g|u ? Cat(?) ? ? Dir(?) (3b)
f |g ? Cat(?) ? ? Dir(?) (3c)
We use point estimates for ?, ? and ?, fixing them
at their posterior means in the following manner:
?
?
gu
= p(g|u) =
Count(g, u) + 1
?
g
?
Count(g
?
, u) + L
(4)
and similarly for ?? and ?? (L = size of vector ?).
3.2 Spatial Goal Abstraction
Each ordered point-set W on some given map can
be seen as the Giver?s current goal, on which they
base their behaviour. Let W = {w
i
; 0 ? i < n},
where w
i
= (x
i
, y
i
) is a waypoint, and x
i
, y
i
are
pixel coordinates on the map, typically obtained
through a vision processing step.
Given this goal formulation, from Equation (2)
we require p(W |u), i.e. the probability of a set
of waypoints given a semantic unit. However,
there are two problems with deriving a generative
model directly over W . Firstly, the length of W
varies from one point-set to the next, making it
hard to compare probabilities with different num-
bers of observations. Secondly, deriving a model
directly over x, y coordinates introduces sparsity
problems, as we are highly unlikely to encounter
the same set of coordinates multiple times. We
thus require an abstraction away from the space of
pixel coordinates.
Our approach is to extract feature vectors of
fixed length from the point-sets, and then derive a
generative model over the feature vectors instead
of the point-sets. Feature extraction allows point-
sets with similar characteristics, rather than exact
pixel values, to give rise to similar distributions
over units, thus enabling the model to reason given
previously unseen point-sets. The features we ex-
tract are detailed in Section 3.4.
3.3 The Multivariate Normal Distribution
Let M be an unordered point-set describing map
elements, such as landmark locations and map
boundary information. M = {m
j
; 0 ? j < k},
where m
j
= (x
j
, y
j
) is a map element with pixel
coordinates x
j
and y
j
. We define a spatial feature
function ? : W,M ? R
n
which captures, as fea-
ture values, the characteristics of the point-set W
in relation to elements in M . Let the spatial fea-
ture vector, extracted from the point-setW and the
map elements M , be:
v = ?(W,M) (5)
Figure 2(b) illustrates the feature extraction pro-
cess. We now define a distribution over the feature
vector v given the semantic unit u. We model v|u
as a multivariate normal distribution (recall that v
is in R
n
):
v|u ? N (?
u
,?
u
) (6)
where ? and ? are the mean vectors and covari-
ance matrices respectively. Subscript
u
indicates
that there is one such parameter for each unit u.
Since the alignments between units u and point-
sets W are fully observed, parameter estimation
is a question of estimating the mean vectors ?
u
?
and the covariance matrices ?
u
?
from the point-
sets co-occuring with unit u
?
. We use maximum
likelihood estimators. To avoid issues with de-
generate covariance matrices resulting from small
amounts of data, we consider diagonal covariance
matrices. Because v|u is normally distributed, in-
ference, both for parameters and conditional distri-
butions over units, can be performed exactly, and
so the model is exceptionally quick to learn and
perform inference.
3.4 The Spatial Feature Sets
We derive four feature sets from the ordered point-
set W , while considering the map elements in the
unordered point-set M :
1. Absolute features capture directions and dis-
tances of movement. We compute the distance
between the first and last points inW , and com-
pute the angle between unit vector <0,-1> and
the line connecting first and last points in W
2. Polynomial features capture shapes of move-
ments as straight lines or curves. We compute
the mean residual of a degree one polynomial
fit to the points in W (linear), and a degree two
polynomial (quadratic)
4
4
These features are computed quickly and efficiently, re-
quiring only the solution to a least squares problem.
629
ug
v
f
?
u
?
u
?
?
?
?
?
?
u = MOVE(UNDER, LM)
w
6
w
0
w
6
w
0
M=
W=
v =
w
0
w
1
:
w
n
m
0
m
1
:
m
k
(a) sub-route aligned with u (c) the model(b) spatial feature extraction
Figure 2: (a) At training time, a Giver?s semantic unit u is aligned with an ordered point-set W , repre-
senting a sub-route. (b) We extract a spatial feature vector v of fixed length, from point-sets W and M
of varying lengths. (c) We define a generative model of the Giver, over Giver act g and semantic unit u,
preceding Follower act f , and spatial feature vector v. Latent parameters and priors are shown.
3. Landmark features capture how close the
route takes the Follower to the nearest land-
mark. We compute the distance between the
end-point inW and the nearest landmark inM ,
and compute the angle between the route taken
in W and the line connecting the start point in
to the nearest landmark
4. Edge features capture the relationship between
the movement and the map edges. We compute
the distance from the start-point in W to the
nearest edge and corner inM , and similarly for
the end-point in W
3.5 The Complete Generative Model
Our complete generative model of the Giver is a
distribution over Giver act g and semantic unit u,
given the preceding Follower act f and the spatial
feature vector v. Vector v is the result of apply-
ing the feature extraction function ? over W and
M , where W is the ordered point-set describing
the sub-route aligned with u, and M is the point-
set describing landmark locations and map edge
information. We rewrite Equation (2) as:
p (g, u|f, v) =
p(u) p(g|u) p(f |g) p(v|u)
?
g
?
u
?
p(u
?
) p(g
?
|u
?
) p(f |g
?
) p(v|u
?
)
(7)
We call our model the Spatio-Semantic Model,
SSM, and depict it in Figure 2(c).
4 Corpus Statistics and State Space
We conduct our experiments on the Map Task cor-
pus (Anderson et al., 1991), a collection of cooper-
ative human-human dialogs arising from the task
explained in Figure 1 and Table 1. The original
corpus was labelled with dialog acts, such as Ac-
knowledge and Instruct. The semantic units can
be obtained through a semantic parse of the nat-
ural language utterances, while the spatial infor-
mation can be obtained through vision processing
of the maps. We use an existing extension of the
corpus by Levit and Roy (2007), which is seman-
tically and spatially annotated. The spatial anno-
tation are x, y pixel coordinates of landmark loca-
tions and evenly spaces points on the routes. All
15 maps were annotated. The semantic units take
the predicates MOVE, TURN, POSITION, or ORI-
ENTATION, and two arguments: a route descrip-
tor and a referent. The semantic annotations were
restricted to the Giver?s Instruct, Clarify, and Ex-
plain acts. Out of the original 128 dialogs, 25 were
semantically annotated.
For our experiments, we use all 15 pairs of
maps, and all 25 semantically annotated dialogs.
A dialog on average contain 57.5 instances, where
an instance is an occurrence of f , g, u, and W .
We find 87 unique semantic units u in our data,
however, according to the semantic representation,
there can be 456 distinct possible values for u
5
.
As for the rest of the variables, f takes 15 values,
g takes 4, and v is a real-valued vector of length
10, extracted from the real valued sets W and M
of varying lengths. We thus reason in a semantic
state space of 87? 15? 4 = 5220, and an infinite
spatial state space.
5 Intrinsic Evaluation
Our first evaluation metric is an information theo-
retic one, based on notion that better models find
new instances of data (not used to train them) to be
more predictable. One such metric is the probabil-
ity a model assigns to the data, (higher is better). A
5
20?2 for TURN and ORIENTATION, + 208?2 for
MOVE and POSITION.
630
second metric is perplexity, which computes how
surprising a model finds the data (lower is better).
Both metrics have been used to evaluate user simu-
lators in the literature (Georgila et al., 2005; Eshky
et al., 2012; Pietquin and Hastie, 2013). We com-
pute the per-utterance probability of held-out data,
instead of the per-dialog probability, since the lat-
ter was deemed incompatible across dialogs of dif-
ferent lengths by Pietquin and Hastie (2013). Per-
plexity is 2
?log
2
(d)
where d is the probability of the
instance in question. We evaluate using leave-one-
out validation, which estimates the model from all
but one dialog, then evaluates the probability of
that dialog. We repeat this process until all dialogs
have been evaluated as the unseen dialog.
Because we evaluate on held-out dialogs, we
need to be able to assign probabilities to pre-
viously unseen instances. We therefore smooth
our models (at training time) by learning a back-
ground model which we estimate from all the
training data. This results in high variance in the
distribution over features and a flat overall dis-
tribution. Where no model can be estimated for
a particular semantic unit, we use that semantic
unit?s smoothed prior probability combined with
the background model for its likelihood.
We first consider the suitability of the differ-
ent feature sets for predicting utterances. Fig-
ure 4 shows the mean per-utterance probability our
model assigns to held-out data when using differ-
ent sets. The more predictable the model finds the
data, the higher the probability. Note that the tar-
get metric here is not 1, as there is no single cor-
rect answer. It can be seen that the most success-
ful features in order of predictiveness are: Abso-
lute, then Polynomial, then Landmark, and finally
Edge. The combination of all buys us further im-
provement. Perplexity is shown in Table 2.
Secondly, we consider two baselines inspired by
similar approaches of comparison in the literature
(Eckert et al., 1997; Levin and Pieraccini, 2000;
Georgila et al., 2005). Both are variants of our
model that lack the spatial component, i.e. they are
not goal-based. Although the baselines are weak,
they allow us to measure the reduction in uncer-
tainty brought by the introduction of the spatial
componenet to our model, which is the purpose
of this comparison. Baseline 1 is p(g, u) while
Baseline 2 is (g, u|f). The first tells us how pre-
dictable giver utterances are (in the held-out data),
based only on the normalised frequencies. The
A P L E A.P A.L A.E All0.0
0
0.08
0.16
0.24
0.32
Figure 3: Mean per-utterance probability, as-
signed to held-out data by our model, when de-
fined over the four feature sets and their combina-
tions, estimated through leave-one-out validation.
A=Absolute, P=Polynomial, L=Landmark, and
E=Edge. Error bars are standard deviations.
Feature Set Perplexity
Absolute (A) 7.26? 4.08
Polynomial (P) 12.86? 8.39
Landmark (L) 15.16? 6.27
Edge (E) 17.92? 8.47
All 4.66? 2.22
Table 2: Perplexity scores (and standard devia-
tions) of our model, computed over the four fea-
ture sets and their combination, estimated through
leave-one-out validation. (A) outperforms all indi-
vidual sets, while the combination performs best.
second tells us how predictable they become when
we condition on the previous follower act. Details
of the baselines are similar to Section 3.1.
Figure 4 shows the mean per-utterance prob-
ability our model assigns to held-out data when
compared to the two baselines. Baseline 2 slightly
improves our predictions over Baseline 1, al-
though not reliably so, when considering the small
increase in perplexity in Table 3. SSM demon-
strates a much larger relative improvement across
both metrics. The results demonstrate that our
spatial component enables substantial reduction in
uncertainty, brought by the transfer of information
from the maps to the utterances.
Intrinsic metrics, such as the probability of
held-out data and perplexity, provide us with an el-
egant way of evaluating probabilistic models in a
setting where there is no single correct answer, but
a range of plausible answers, because they exploit
the model?s inherit ability to assign probability to
behaviour. However, the metrics can be hard to in-
terpret in an absolute sense, providing much better
631
Baseline1 Baseline2 SSM0.0
0
0.08
0.16
0.24
0.32
Figure 4: Mean per-utterance probability, assigned
to held-out data by our model (SSM), compared to
two baselines which lack the spatial componenet,
estimated through leave-one-out validation. Error
bars are standard deviations.
Model Perplexity
Baseline1 24.95? 4.05
Baseline2 25.06? 12.02
SSM 4.66? 2.22
Table 3: Perplexity scores of our model (SSM),
compared to the two baselines, estimated through
leave-one-out validation. SSM finds the held-out
data to be least surprising.
information about the relative strengths of differ-
ent models rather than their absolute utility. In the
next section, we explore methods for determining
the utility of the models when applied to tasks.
6 Extrinsic Evaluation
In this section, we undertake a task-based evalu-
ation of model output. We train on 22 of the di-
alogs, holding out 3 at random for testing. The
task is to then generate, for each sub-route in the
test dialogs, the most probable unit to describe it
6
.
Figure 5 shows some examples of sub-routes taken
from the test dialogs, and shows the the most prob-
able unit to describe each under our model, SSM.
We first explore a naive notion of accuracy:
the percentage of model-generated units matching
Real Giver units observed in the test dialogs. We
compute the same for Baseline 1 from Section 5
as a lower bound. A quick glance at the results in
Table 4 might suggest that both models have lit-
tle utility: SSM is ?correct? only 33% of the time.
However, the extent to which this conclusion fol-
lows depends on the suitability of accuracy as a
6
The models can generate 1 of the 87 units observed in
the training set, but are made to output the most probable in
this experiment.
Baseline SSM
Match to Real Giver 7.69% 33.08%
Table 4: Percentage of model-generated units that
match Real Giver units in the test set. The models
output the most probable unit to describe a given
sub-route. We argue that this metric is unsuitable
as it assumes one correct answer.
Mismatch Baseline SSM Real Giver
1.45 3.04 5.27 5.11
Table 5: Average scores assigned by human judges
to model-generated units on a 7-level Likert scale.
Mismatch is judged to be the worst, followed by
Baseline. SSM and Real Giver are scored well,
and are judged to be of similar quality.
means of evaluating dialog. In most situations,
there is not a single correct description and a host
of incorrect ones, but rather a gradient of descrip-
tions from the highly informative and appropriate
to the nonsensical and confusing. Such subtleties
are not captured by an accuracy test (or the closely
related recall and precision). In demonstration of
this point, we next conduct qualitative evaluation
of model output.
We ask humans to rate, on a Likert scale of 7,
the degree to which a given unit provides a suitable
description of a given sub-route. Sub-routes are
taken from the test dialogs, and are marked simi-
larly to Figure 5 but on the complete map. Units
are generated from SSM, Baseline, Real Giver,
and a control condition: a deliberate Mismatch
to the sub-route. The Mismatch is generated au-
tomatically by taking the least probable unit under
SSM, of the form MOVE(TOWARD, x) where x is
one of the four compass directions. We collect 5
judgements for each sub-route-unit combinations
on Mechanical Turk, and randomise so that no
judge sees the same order of pairs. Test dialogs
contained 94 distinct sub-routes.
We analyse the results with a two-way ANOVA,
with the first factor being model, and the second
being the sub-route, for a 4?94 design. The means
of the ?model? factor are shown in Table 5. It
can be seen that Mismatch and Baseline are scored
sensibly poorly, while SSM and Real Giver are
scored reasonably well, and are judged to be of a
similar quality. We thus proceed with a more rig-
orous analysis. The ANOVA summary is shown
in Table 6. A significant effect of the model fac-
632
(c) (d) (e)(a) (b)
Figure 5: Given a sub-route marked with start-point ? and end-point ? (in red), SSM generates the
following u: (a) MOVE(TOWARDS, ABS NORTHEAST) (b) TURN(ABS WEST) (c) MOVE(FOLLOW-
BOUNDARY, LM) (d) MOVE(AROUND, LM) (e) MOVE(TOWARDS, ABS SOUTHWEST)
Factor S Sq Df F Pr(>F)
Model 4845.3 3 783.93 <0.001
Sub-route 1140.0 93 5.95 <0.001
M:S 2208.7 279 3.84 <0.001
Residuals 3263.5 1584
Table 6: Two way ANOVA with factors model (4
possibilities), and sub-route (94 possibilities). Re-
sults show a model effect accounting for most of
the variance. Meaning that the scores assigned to
the units by human judges are significantly influ-
enced by the model used to generate the units.
Model Comparison t value Pr(>|t|)
Mismatch : Baseline -16.974 <0.001
SSM : Baseline 23.882 <0.001
Real Giver : Baseline 23.192 <0.001
SSM : Mismatch 40.857 <0.001
Real Giver : Mismatch 40.507 <0.001
SSM : Real Giver 1.171 0.646
Table 7: Tukey HSD shows that all models are
assigned significantly different scores by judges,
apart from SSM and Real Giver. This asserts that,
although only 33% of SSM units match Real Giver
units (as shown in Table 4), the quality of the units
are not judged to be significantly different.
tor is present, meaning that the scores assigned by
human judges to the units are significantly influ-
enced by which model was used to generate the
units. Additionally, a significant effect for the sub-
route factor can be seen, which is due to some sub-
routes being harder to describe than others. An in-
teraction effect is also present, which is expected
given such a large number of examples. Note how
the model factor accounts for the largest amount
of variance of all the factors.
Having confirmed the presence of a model ef-
fect, we conduct a post-hoc analysis of the model
factors. Table 7 shows a Tukey HSD test, demon-
strating that all models are significantly different
from one another, except Real Giver and SSM. Re-
sults show that, despite the large number of judge-
ments collected, we are unable to separate the
quality of our model?s unit from that in the origi-
nal data, against which accuracy was being judged
in Table 4. This demonstrates that when many an-
swers are feasible, scoring correctness against the
original human units is unsuitable. It also firmly
demonstrates the suitability of our spatial repre-
sentation, and the strength of the generative model
we have induced for the task.
7 Conclusion and Discussion
We have shown how to represent spatial goals in a
navigational domain, and have validated our rep-
resentation by inducing (fully from data) a gen-
erative model of the Giver?s semantic utterances
conditioned on the spatial goal and the previous
Follower act. Intrinsic and extrinsic evaluation
demonstrate the strength of our model.
A direct application of this work is robot guid-
ance, by using the Giver?s simulator to induce an
optimal Follower: an MDP-based dialog manager
that interprets and follows navigational instruc-
tions. Another variation would be to learn a gen-
erative model of the Follower, by extracting fea-
tures from Follower maps (labelled with routes
drawn by real Followers). Finally, this work has
broader applications beyond simulation, in partic-
ular for systems that describe routes to users (spa-
tial goal representation and model dependencies
would hold). Decisions about which part of the
route to describe next is one extension to that end.
Acknowledgements
We thank Ioannis Konstas, Johanna Moore, Robin
Hill, S. M. Ali Eslami, and the anonymous review-
ers for valuable feedback. This work is funded
by King Saud University. Mark Steedman is sup-
ported by EC-PF7-270273 Xperience and ERC
Advanced Fellowship 249520 GRAMPLUS.
633
References
Hua Ai and Diane J. Litman. 2007. Knowledge con-
sistent user simulations for dialog systems. In Inter-
Speech 2007, pages 2697?2700.
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The hcrc map task cor-
pus. Language and Speech, 34(4):351?366.
Heriberto Cuay?ahuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
ASRU 2005, pages 290?295.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system
evaluation. In Proceedings of IEEE Workshop on
Automatic Speech Recognition and Understanding.
Aciel Eshky, Ben Allison, and Mark Steedman. 2012.
Generative goal-driven user simulation for dialog
management. In EMNLP-CoNLL 2012, pages 71?
81, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for in-
formation state update dialogue systems. In Inter-
Speech 2005.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User Simulation for Spoken Dia-
logue Systems: Learning and Evaluation. In Inter-
Speech 2006.
James Henderson and Oliver Lemon. 2008. Mixture
model pomdps for efficient handling of uncertainty
in dialogue management. In ACL, HLT-Short ?08,
pages 73?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Srinivasan Janarthanam, Oliver Lemon, Phil Bartie,
Tiphaine Dalmas, Anna Dickinson, Xingkun Liu,
William Mackaness, and Bonnie Webber. 2013.
Evaluating a city exploration dialogue system with
integrated question-answering and pedestrian navi-
gation. In ACL.
Esther Levin and Roberto Pieraccini. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. In IEEE Transactions on Speech
and Audio Processing.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision process for learning
dialogue strategies. In Proc. ICASSP, pages 201?
204.
M. Levit and D. Roy. 2007. Interpretation of spatial
language in a map navigation task. IEEE Trans-
actions on Systems, Man, and Cybernetics, Part A,
37(3):667?679.
Pierre Lison. 2013. Model-based bayesian reinforce-
ment learning for dialogue management. In Inter-
speech 2013.
Olivier Pietquin and Thierry Dutoit. 2006. A prob-
abilistic framework for dialog simulation and opti-
mal strategy learning. Audio, Speech, and Language
Processing, IEEE Transactions on, 14(2):589?599,
march.
Olivier Pietquin and Helen Hastie. 2013. A survey
on metrics for the evaluation of user simulations.
Knowledge Eng. Review, 28(1):59?73.
Olivier Pietquin. 2004. A Framework for Unsuper-
vised Learning of Dialogue Strategies. Ph.D. thesis,
Facult?e Polytechnique de Mons, TCTS Lab (Bel-
gique), apr.
Olivier Pietquin. 2006. Consistent goal-directed user
model for realisitc man-machine task-oriented spo-
ken dialogue simulation. In Multimedia and Expo,
2006 IEEE International Conference on, pages 425?
428. IEEE.
David Reitter and Johanna D. Moore. 2007. Predicting
success in dialogue. In ACL.
Verena Rieser and Oliver Lemon. 2006. Cluster-
based user simulations for learning dialogue strate-
gies. In INTERSPEECH 2006 - ICSLP, Ninth Inter-
national Conference on Spoken Language Process-
ing, September.
St?ephane Rossignol, Olivier Pietquin, and Michel Ian-
otto. 2010. Simulation of the grounding process in
spoken dialog systems with bayesian networks. In
IWSDS, pages 110?121.
St?ephane Rossignol, Olivier Pietquin, and Michel Ian-
otto. 2011. Training a bn-based user model for di-
alogue simulation with missing data. In IJCNLP,
pages 598?604.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?00, pages 93?100, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jost Schatzmann and Steve Young. 2009. The hid-
den agenda user simulation model. Audio, Speech,
and Language Processing, IEEE Transactions on,
17(4):733?747.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT 2002.
Satinder Singh, Diane J. Litman, Michael Kearns, and
Marilyn A. Walker. 2002. Optimizing dialogue
management with reinforcement learning: Experi-
ments with the njfun system. Journal of Artificial
Intelligence Research, 16:105?133.
634
Henry S. Thompson, Anne Anderson, Ellen G. Bard,
Gwyneth D. Sneddon, Alison Newlands, and Cathy
Sotillo. 1993. The HCRC Map Task corpus: natu-
ral dialogue for speech recognition. In Proceedings
of the workshop on Human Language Technology,
HLT ?93, pages 25?30, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In ACL 2010, Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, July 11-16,
2010, Uppsala, Sweden, pages 806?814. The Asso-
ciation for Computer Linguistics.
Jason D. Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams. 2007. Applying pomdps to dia-
log systems in the troubleshooting domain. In Pro-
ceedings of the Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technolo-
gies, NAACL-HLT ?07, pages 1?8, Morristown, NJ,
USA. Association for Computational Linguistics.
Steve Young, Milica Gasic, Blaise Thomson, and Ja-
son D. Williams. 2013. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160?1179.
635
