Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 497?507,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
)HDWXUH5LFK/DQJXDJH,QGHSHQGHQW6\QWD[%DVHG$OLJQPHQWIRU
6WDWLVWLFDO0DFKLQH7UDQVODWLRQ
-DVRQ5LHVD1 $QQ,UYLQH2 'DQLHO0DUFX1
1,QIRUPDWLRQ6FLHQFHV,QVWLWXWH
8QLYHUVLW\RI6RXWKHUQ&DOLIRUQLD
0DULQDGHO5H\ &$ 
{ULHVD PDUFX}#LVLHGX
2'HSDUWPHQWRI&RPSXWHU6FLHQFH
-RKQV+RSNLQV8QLYHUVLW\
%DOWLPRUH 0' 
DQQL#MKXHGX
$EVWUDFW
:HSUHVHQWDQDFFXUDWHZRUGDOLJQPHQWDOJR
ULWKPWKDWKHDYLO\H[SORLWVVRXUFHDQGWDUJHW
ODQJXDJHV\QWD[ 8VLQJDGLVFULPLQDWLYHIUDPH
ZRUNDQGDQHI?FLHQWERWWRPXSVHDUFKDOJR
ULWKP ZHWUDLQDPRGHORIKXQGUHGVRIWKRX
VDQGVRIV\QWDFWLFIHDWXUHV 2XUQHZPRGHO
KHOSVXVWRYHU\DFFXUDWHO\PRGHOV\QWDF
WLFWUDQVIRUPDWLRQVEHWZHHQODQJXDJHV LV
ODQJXDJHLQGHSHQGHQW DQGZLWKDXWRPDWLF
IHDWXUH  H[WUDFWLRQ DVVLVWV  V\VWHP GHYHORSHUV
LQREWDLQLQJJRRGZRUGDOLJQPHQWSHUIRUPDQFH
RIIWKHVKHOIZKHQWDFNOLQJQHZODQJXDJHSDLUV
:HDQDO\]HWKHLPSDFWRIRXUIHDWXUHV GHVFULEH
LQIHUHQFH XQGHU  WKH PRGHO DQG GHPRQVWUDWH
VLJQL?FDQW  DOLJQPHQW  DQG  WUDQVODWLRQ  TXDOLW\
LPSURYHPHQWVRYHUDOUHDG\SRZHUIXOEDVHOLQHV
WUDLQHG RQ YHU\  ODUJH  FRUSRUD :H REVHUYH
WUDQVODWLRQTXDOLW\LPSURYHPHQWVFRUUHVSRQG
LQJWRDQG%/(8 IRU$UDELF(QJOLVKDQG
&KLQHVH(QJOLVK UHVSHFWLYHO\
 ,QWURGXFWLRQ
,QUHFHQW\HDUV VHYHUDOVWDWHRIWKHDUWVWDWLVWLFDOPD
FKLQHWUDQVODWLRQ07 V\VWHPVKDYHLQFRUSRUDWHG
ERWKVRXUFHDQGWDUJHWV\QWD[LQWRWKHJUDPPDUVWKDW
WKH\JHQHUDWHDQGXVHWRWUDQVODWH :KLOHVRPHWUHH
WRWUHH V\VWHPVSDUVH VRXUFH DQG WDUJHW  VHQWHQFHV
VHSDUDWHO\ *DOOH\HWDO  =ROOPDQDQG9HQX
JRSDO  +XDQJDQG0L  RWKHUVSURMHFW
V\QWDFWLFSDUVHVDFURVVZRUGDOLJQPHQWV/LHWDO
 ,QERWKDSSURDFKHV DVLQODUJHO\DOOVWDWLVWLFDO
07WKHTXDOLW\RIWKHDOLJQPHQWVXVHGWRJHQHUDWH
WKHUXOHVRIWKHJUDPPDUDUHFULWLFDOWRWKHVXFFHVV
RIWKHV\VWHP +RZHYHU WRGDWH PRVWZRUGDOLJQ
PHQWV\VWHPVKDYHQRWFRQVLGHUHGWKHVDPHGHJUHH
RIV\QWDFWLFLQIRUPDWLRQWKDW07 V\VWHPVKDYH
([WHQGLQJ  XQVXSHUYLVHG PRGHOV OLNH  WKH  ,%0
PRGHOV %URZQ  HW  DO  JHQHUDOO\  UHTXLUHV
FKDQJLQJWKHHQWLUHJHQHUDWLYHVWRU\ 7KHDGGLWLRQDO
FRPSOH[LW\ZRXOGOLNHO\PDNHWUDLQLQJVXFKPRG
HOVTXLWHH[SHQVLYH $OUHDG\ ZLWKXELTXLWRXVWRROV
OLNH*,=$ 2FKDQG1H\  WUDLQLQJDFFXUDWH
PRGHOVRQODUJHFRUSRUDWDNHVXSZDUGVRIGD\V
5HFHQWZRUNLQGLVFULPLQDWLYHDOLJQPHQWKDVIR
FXVHGRQLQFRUSRUDWLQJIHDWXUHVWKDWDUHXQDYDLODEOH
RUGLI?FXOWWRLQFRUSRUDWHZLWKLQRWKHUPRGHOV HJ
0RRUH  ,WW\FKHULDKDQG5RXNRV  /LX
HW  DO  7DVNDU HW  DO E %OXQVRPDQG
&RKQ  /DFRVWH-XOLHQHWDO  0RRUHHWDO
 (YHQPRUHUHFHQWO\ PRWLYDWHGE\WKHULVHRI
V\QWD[EDVHGWUDQVODWLRQPRGHOV RWKHUVKDYHVRXJKW
WRLQIRUPDOLJQPHQWGHFLVLRQVZLWKV\QWDFWLFLQIRU
PDWLRQ)UDVHUDQG0DUFX Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1077?1088,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Monolingual Marginal Matching for Translation Model Adaptation
Ann Irvine
Johns Hopkins University
anni@jhu.edu
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Hal Daume? III
University of Maryland
me@hal3.name
Abstract
When using a machine translation (MT)
model trained on OLD-domain parallel data to
translate NEW-domain text, one major chal-
lenge is the large number of out-of-vocabulary
(OOV) and new-translation-sense words. We
present a method to identify new translations
of both known and unknown source language
words that uses NEW-domain comparable doc-
ument pairs. Starting with a joint distribution
of source-target word pairs derived from the
OLD-domain parallel corpus, our method re-
covers a new joint distribution that matches
the marginal distributions of the NEW-domain
comparable document pairs, while minimiz-
ing the divergence from the OLD-domain dis-
tribution. Adding learned translations to our
French-English MT model results in gains of
about 2 BLEU points over strong baselines.
1 Introduction
When a statistical machine translation (SMT) model
trained on OLD-domain (e.g. parliamentary proceed-
ings) parallel text is used to translate text in a NEW-
domain (e.g. medical or scientific), performance de-
grades drastically. One of the major causes is the
large number of NEW-domain words that are out-of-
vocabulary (OOV) with respect to the OLD-domain
text. Figure 1 shows the OOV rate for text in several
NEW-domains, with respect to OLD-domain parlia-
mentary proceedings. Even more challenging are
the difficult-to-detect new-translation-sense (NTS)
words: French words that are present in both the
OLD and NEW domains but that are translated dif-
ferently in each domain. For example, the French
Parliament Subtitles Medical Science
Perc
ent 
of W
ord 
Typ
es t
hat 
are 
OOV
0
10
20
30
40
50
60
3.95
27.03
41.42
50.93
Figure 1: Percent of test set word types by domain that
are OOV with respect to five million tokens of OLD-
domain French parliamentary proceedings data.
word enceinte is mostly translated in parliamentary
proceedings as place, house, or chamber; in medical
text, the translation is mostly pregnant; in scientific
text, enclosures.
One potential remedy is to collect parallel data in
the NEW-domain, from which we can train a new
SMT model. Smith et al (2010), for example, mine
parallel text from comparable corpora. Parallel sen-
tences are informative but also rare: in the data re-
leased by Smith et al (2010), only 21% of the for-
eign sentences have a near-parallel counterpart in the
English article.1 Furthermore, these sentences do
not capture all terms. In that same dataset, we find
that on average only 20% of foreign and 28% of En-
glish word types in a given article are represented in
the parallel sentence pairs.
In this work, we seek to learn a joint distribu-
1Only 12% of sentences from generally longer English arti-
cles have a near-parallel counterpart in the foreign language.
1077
tion of translation probabilities over all source and
target word pairs in the NEW-domain. We begin
with a maximum likelihood estimate of the joint
based on a word aligned OLD-domain corpus and
update this distribution using NEW-domain compa-
rable data. We define a model based on a single com-
parable corpus and then extend it to learn from doc-
ument aligned comparable corpora with any number
of comparable document pairs. This approach al-
lows us to identify translations for OOV words in the
OLD-domain (e.g. French cisaillement and perc?age,
which translate as shear and drilling, in the scien-
tific domain) as well as new translations for previ-
ously observed NTS words (e.g. enceinte translates
as enclosures, not place, in the scientific domain).
In our MT experiments, we use the learned NEW-
domain joint distribution to update our SMT model
with translations of OOV and low frequency words;
we leave the integration of new translations for NTS
words to future work.
Our approach crucially depends on finding com-
parable document pairs relevant to the NEW-domain.
Such pairs could be derived from a number of
sources, with document pairings inferred from
timestamps (e.g. news articles) or topics (inferred or
manually labeled). We use Wikipedia2 as a source
of comparable pairs. So-called ?interwiki links?
(which link Wikipedia articles written on the same
topic but in different languages) act as rough guid-
ance that pages may contain similar information.
Our approach does not exploit any Wikipedia struc-
ture beyond this signal, and thus is portable to alter-
nate sources of comparable articles, such as multi-
lingual news articles covering the same event.
Our model also relies on the assumption that each
comparable document pair describes generally the
same concepts, though the order and structure of
presentation may differ significantly. The efficacy
of this method likely depends on the degree of com-
parability of the data; exploring the correlation be-
tween comparability and MT performance is an in-
teresting question for future work.
2 Previous Work
In prior work (Irvine et al, 2013), we presented a
systematic analysis of errors that occur when shift-
2www.wikipedia.org
ing domains in machine translation. That work con-
cludes that errors resulting from unseen (OOV) and
new translation sense words cause the majority of
the degradation in translation performance that oc-
curs when an MT model trained on OLD-domain
data is used to translate data in a NEW-domain. Here,
we target OOV errors, though our marginal match-
ing method is also applicable to learning translations
for NTS words.
A plethora of prior work learns bilingual lex-
icons from monolingual and comparable corpora
with many signals including distributional, tempo-
ral, and topic similarity (Rapp, 1995; Fung and Yee,
1998; Rapp, 1999; Schafer and Yarowsky, 2002;
Schafer, 2006; Klementiev and Roth, 2006; Koehn
and Knight, 2002; Haghighi et al, 2008; Mimno
et al, 2009; Mausam et al, 2010; Prochasson
and Fung, 2011; Irvine and Callison-Burch, 2013).
However, this prior work stops short of using these
lexicons in translation. We augment a baseline MT
system with learned translations.
Our approach bears some similarity to Ravi and
Knight (2011), Dou and Knight (2012), and Nuhn
et al (2012); we learn a translation distribution de-
spite a lack of parallel data. However, we focus on
the domain adaptation setting. Parallel data in an
OLD-domain acts as a starting point (prior) for this
translation distribution. It is reasonable to assume an
initial bilingual dictionary can be obtained even in
low resource settings, for example by crowdsourc-
ing (Callison-Burch and Dredze, 2010) or pivoting
through related languages (Schafer and Yarowsky,
2002; Nakov and Ng, 2009).
Daume? III and Jagarlamudi (2011) mine trans-
lations for high frequency OOV words in NEW-
domain text in order to do domain adaptation. Al-
though that work shows significant MT improve-
ments, it is based primarily on distributional simi-
larity, thus making it difficult to learn translations for
low frequency source words with sparse word con-
text counts. Additionally, that work reports results
using artificially created monolingual corpora taken
from separate source and target halves of a NEW-
domain parallel corpus, which may have more lexi-
cal overlap with the corresponding test set than we
could expect from true monolingual corpora. Our
work mines NEW-domain-like document pairs from
Wikipedia. In this work, we show that, keeping
1078
data resources constant, our model drastically out-
performs this previous approach. Razmara et al
(2013) take a fundamentally different approach and
construct a graph using source language monolin-
gual text and identify translations for source lan-
guage OOV words by pivoting through paraphrases.
Della Pietra et al (1992) and Federico (1999) ex-
plore models for combining foreground and back-
ground distributions for the purpose of language
modeling, and their approaches are somewhat simi-
lar to ours. However, our focus is on translation.
3 Model
Our goal is to recover a probabilistic translation dic-
tionary in a NEW-domain, represented as a joint
probability distribution pnew(s, t) over source/target
word pairs. At our disposal, we have access to a joint
distribution pold(s, t) from the OLD-domain (com-
puted from word alignments), plus comparable doc-
ument pairs in the NEW-domain. From these com-
parable documents, we can extract raw word fre-
quencies on both the source and target side, repre-
sented as marginal distributions q(s) and q(t). The
key idea is to estimate this NEW-domain joint dis-
tribution to be as similar to the OLD-domain distri-
bution as possible, subject to the constraint that its
marginals match those of q.
To illustrate our goal, consider an example. Imag-
ine in the OLD-domain parallel data we find that ac-
corder translates as grant 10 times and as tune 1
time. In the NEW-domain comparable data, we find
that accorder occurs 5 times, but grant occurs only
once, and tune occurs 4 times. Clearly accorder no
longer translates as grant most of the time; perhaps
we should shift much of its mass onto the translation
tune instead. Figure 2 shows the intuition.
First, we present an objective function and set of
constraints over joint distributions to minimize the
divergence from the OLD-domain distribution while
matching both the source and target NEW-domain
marginal distributions. Next, we augment the objec-
tive with information about word string similarity,
which is particularly useful for the French-English
language pair. Optimizing this objective with a sin-
gle pair of source and target marginals can be per-
formed using an off-the-shelf solver. In practice,
though, we have a large set of document pairs, each
of which can induce a pair of marginals. Using
these per-document marginals provides additional
information to the learning function but would over-
whelm a common solver. Therefore, we present a se-
quential learning method for approximately match-
ing the large set of document pair marginal distribu-
tions. Finally, we describe how we identify compa-
rable document pairs relevant to the NEW-domain.
3.1 Marginal Matching Objective
Given word-aligned parallel data in the OLD-domain
and source and target comparable corpora in the
NEW-domain, we first estimate a joint distribution
pold(s, t) over word pairs (s, t) in the OLD-domain,
where s and t range over source and target lan-
guage words, respectively. For the OLD-domain
joint distribution, we use a simple maximum like-
lihood estimate based on non-null automatic word
alignments (using grow-diag-final GIZA++ align-
ments (Och and Ney, 2003)). Next, we find source
and target marginal distributions, q(s) and q(t), by
relative frequency estimates over the source and tar-
get comparable corpora. Our goal is to recover a
joint distribution pnew(s, t) for the new domain that
matches the marginals, q(s) and q(t), but is mini-
mally different from the original joint distribution,
pold(s, t).
We cast this as a linear programming problem:
pnew = arg min
p
?
?
?p? pold
?
?
?
1
(1)
subject to:
?
s,t
p(s, t) = 1, p(s, t) ? 0
?
s
p(s, t) = q(t),
?
t
p(s, t) = q(s)
In the objective function, the joint probability matri-
ces p and pold are interpreted as large vectors over
all word pairs (s, t). The first two constraints force
the result to be a well-formed distribution, and the
final two force the marginals to match.
Following prior work (Ravi and Knight, 2011),
we would like the matrix to remain as sparse as pos-
sible; that is, introduce the smallest number of new
translation pairs necessary. A regularization term
captures this goal:
?(p) =
?
s,t:
pold(s,t)=0
?r ? p(s, t) (2)
1079
house place pregnant dress
q
old
(s)
enceinte 0.30 0.40 0.10 0
0.80
habiller 0 0 0 0.20
0.20
q
old
(t)
0.30 0.40 0.10 0.20
(a) OLD-Domain Joint (b) NEW-Domain Marginals
house place pregnant dress girl
q(s)
enceinte
?
0.60
habiller
0.20
fille
0.20
q(t)
0.12 0.08 0.40 0.20 0.20
(c) Inferred NEW-Domain Joint
house place pregnant dress girl q
new
(s)
enceinte
0.12 0.08 0.40 0 0
0.60
habiller 0 0 0 0.20 0 0.20
fille 0 0 0 0 0.20 0.20
q
new
(t)
0.12 0.08 0.40 0.20 0.20
=
Matched 
Marginals
Figure 2: Starting with a joint distribution derived from OLD-domain data, we infer a NEW-domain joint distribution
based on the intuition that the new joint should match the marginals that we observe in NEW-domain comparable
corpora. In this example, a translation is learned for the previously OOV word fille, and pregnant becomes a preferred
translation for enceinte.
If the old domain joint probability pold(s, t) was
nonzero, there is no penalty. Otherwise, the penalty
is ?r times the new joint probability p(s, t). To dis-
courage the addition of translation pairs that are un-
necessary in the new domain, we use a value of ?r
greater than one. Thus, the benefit of a more sparse
matrix overwhelms the desire for preventing change.
Any value greater than one seems to suffice; we use
?r = 1.1 in our experiments.
Inspired by the preference for sparse matrices
captured by ?(p), we include another orthogonal
cue that words are translations of one another: their
string similarity. In prior work, string similarity was
a valuable signal for inducing translations, particu-
larly for closely related languages such as French
and English (Daume? III and Jagarlamudi, 2011). We
define a penalty function f(p) as follows: if the nor-
malized Levenshtein edit distance between swithout
accents and t is less than 0.2, no penalty is applied;
a penalty of 1 is applied otherwise. We chose the
0.2 threshold manually by inspecting results on our
development sets.
f(p) =
?
s,t
p(s, t) ?
{
0 if lev(t,strip(s))len(s)+len(t) < 0.2
1 otherwise
The objective function including this penalty is:
pnew = arg min
p
?
?
?p? pold
?
?
?
1
+ ?(p) + f(p)
In principle, additional penalties could be encoded
in a similar way.3 This objective can be optimized
by any standard LP solver; we use the Gurobi pack-
age (Gurobi Optimization Inc., 2013).
3.2 Document Pair Modification
The above formulation applies whenever we have
access to comparable corpora. However, often we
have access to comparable documents, such as those
given by Wikipedia inter-language links. We modify
our approach to take advantage of the document cor-
respondences within our comparable corpus. In par-
ticular, we would like to match the marginals for all
document pairs.4 By maintaining separate marginal
distributions, our algorithm is presented with more
3We experimented with penalties measuring document-pair
co-occurrence and monolingual frequency differences but did
not see gains on our development sets.
4This situation is not unique to our application; multiple
marginals are likely to exist in many cases.
1080
information. For example, imagine that one doc-
ument pair uses ?dog? and ?chien?, where another
document pair uses ?cat? and ?chat?, each with sim-
ilar frequency. If we sum these marginals to produce
a single marginal distribution, it is now difficult to
identify that ?dog? should correspond to ?chien? and
not ?chat.? Document pair alignments add informa-
tion at the cost of additional constraints.
An initial formulation of our problem with mul-
tiple comparable document pairs might require
the pnew marginals to match all of the document
marginals. In general, this constraint set is likely
to result in an infeasible problem. Instead, we take
an incremental, online solution, considering a sin-
gle comparable document pair at a time. For docu-
ment pair k, we solve the optimization problem in
Eq (1) to find the joint distribution minimally dif-
ferent from pk-1, while matching the marginals of
this pair only. This gives a new joint distribution,
tuned specifically for this pair. We then update our
current guess of the new domain joint toward this
document-pair-specific distribution, much like a step
in stochastic gradient ascent.
More formally, suppose that before processing the
kth document we have a guess at the NEW-domain
joint distribution, pnew1:k?1 (the subscript indicates that
it includes all document pairs up to and including
document k ? 1). We first solve Eq (1) solely on
the basis of this document pair, finding a joint dis-
tribution pnewk that matches the marginals of the kth
document pair only and is minimally different from
pnew1:k?1. Finally, we form a new estimate of the joint
distribution by moving pnew1:k?1 in the direction of
pnewk , via:
pnew1:k = p
new
1:k?1 + ?u
[
pnewk ? p
new
1:k?1
]
The learning rate ?u is set to 0.001.5
This incremental update of parameters is simi-
lar to the margin infused relaxed algorithm (MIRA)
(Crammer et al, 2006). Like MIRA and the percep-
tron, there is not an overall ?objective? function that
we are attempting to optimize (as one would in many
stochastic gradient steps). Instead, we?re aiming for
5We tuned ?u on semi-extrinsic results on the development
set. Note that although 0.001 seems small, the values we are
moving are joint probabilities, which are tiny and so small
learning rates make sense.
a solution that makes a small amount of progress
on each example, in such a way if it received that
example again, it would ?do better? (in this case:
have a closer match of marginals). Also like MIRA,
our learning rate is constant. We parallelize learning
with mini-batches for increased speed. Eight paral-
lel learners update an initial joint distribution based
on 100 document pairs (i.e. each learner makes 100
incremental updates), and then we merge results us-
ing an average over the 8 learned joint distributions.
3.3 Comparable Data Selection
It remains to select comparable document pairs. We
assume that we have enough monolingual NEW-
domain data in one language to rank comparable
document pairs (here, Wikipedia pages) according
to how NEW-domain-like they are. In particular, we
estimate the similarity to a source language (here,
French) corpus in the NEW domain. For our experi-
ments, we use the French side of a NEW-domain par-
allel corpus.6 We could have targeted our learning
even more by using our NEW-domain MT test sets.
Doing so would increase the chances that our source
language words of interest appear in the comparable
corpus. However, to avoid overfitting any particular
test set, we use the French side of the training data.
For each Wikipedia document pair, we com-
pute the percent of French phrases up to length
four that are observed in the French monolingual
NEW-domain corpus and rank document pairs by
the geometric mean of the four overlap measures.
More sophisticated ways to identify NEW-domain-
like Wikipedia pages (e.g. Moore and Lewis (2010))
may yield additional performance gains, but, quali-
tatively, the ranked Wikipedia pages seemed reason-
able to the authors.
4 Experimental setup
4.1 Data
We use French-English Hansard parliamentary pro-
ceedings7 as our OLD-domain parallel corpus. With
over 8 million parallel lines of text, it is one of the
largest freely available parallel corpora for any lan-
6We could have, analogously, used the target language (En-
glish) side of the parallel corpus and measure overlap with the
English Wikipedia documents, or even used both.
7http://www.parl.gc.ca
1081
guage pair. In order to simulate more typical data
settings, we sample every 32nd line, using the result-
ing parallel corpus of 253, 387 lines and 5, 051, 016
tokens to train our baseline model.
We test our model using three NEW-domain cor-
pora: (1) the EMEA medical corpus (Tiedemann,
2009), (2) a corpus of scientific abstracts (Carpuat
et al, 2013a), and (3) a corpus of translated movie
subtitles (Tiedemann, 2009). We use development
and test sets to tune and evaluate our MT mod-
els. We use the NEW-domain parallel training cor-
pora only for language modeling and for identifying
NEW-domain-like comparable documents.
4.2 Machine translation
We use the Moses MT framework (Koehn et al,
2007) to build a standard statistical phrase-based
MT model using our OLD-domain training data. Us-
ing Moses, we extract a phrase table with a phrase
limit of five words and estimate the standard set of
five feature functions (phrase and lexical translation
probabilities in each direction and a constant phrase
penalty feature). We also use a standard lexicalized
reordering model and two language models based on
the English side of the Hansard data and the given
NEW-domain training corpora. Features are com-
bined using a log-linear model optimized for BLEU,
using the n-best batch MIRA algorithm (Cherry and
Foster, 2012). We call this the ?simple baseline.? In
Section 5.2 we describe several other baseline ap-
proaches.
4.3 Experiments
For each domain, we use the marginal matching
method described in Section 3 to learn a new,
domain-adapted joint distribution, pnewk (s, t), over
all French and English words. We use the learned
joint to compute conditional probabilities, pnewk (t|s),
for each French word s and rank English translations
t accordingly. First, we evaluate the learned joint
directly using the distribution based on the word-
aligned NEW-domain development set as a gold
standard. Then, we perform end-to-end MT exper-
iments. We supplement phrase tables with transla-
tions for OOV and low frequency words (we ex-
periment with training data frequencies less than
101, 11, and 1) and include pnewk (t|s) and p
new
k (s|t)
as new translation features for those supplemental
translations. For these new phrase pairs, we use the
average lexicalized reordering values from the ex-
isting reordering tables. For phrase pairs extracted
bilingually, we use the bilingually estimated trans-
lation probabilities and uniform scores for the new
translation features. We experimented with using
pnewk (t|s) and p
new
k (s|t) to estimate additional lex-
ical translation probabilities for the bilingually ex-
tracted phrase pairs but did not observe any gains
(experimental details omitted due to space con-
straints). We re-run tuning in all experiments.
We also perform oracle experiments in which
we identify translations for French words in word-
aligned development and test sets and append these
translations to baseline phrase tables.
5 Results
5.1 Semi-extrinsic evaluation
Before doing end-to-end MT experiments, we eval-
uate our learned joint distribution, pnewk (s, t), by
comparing it to the joint distribution taken from a
word aligned NEW-domain parallel development set,
pgold(s, t). We call this evaluation semi-extrinsic
because it involves neither end-to-end MT (our ex-
trinsic task) nor an intrinsic evaluation based on our
training objective (L1 norm). We find it informa-
tive to evaluate the models using bilingual lexicon
induction metrics before integrating our output into
full MT. That is, we do not compare the full joint dis-
tributions, but, rather, for a given French word, how
our learned model ranks the word?s most probable
translation under the gold distribution. In particular,
because we are primarily concerned with learning
translations for previously unseen words, we eval-
uate over OOV French word types. In some cases,
the correct translation for OOV words is the identi-
cal string (e.g. na+, lycium). Because it is trivial
to produce these translations,8 we evaluate over the
subset of OOV development set French words for
which the correct translation is not the same string.
Figure 3 shows the mean reciprocal rank for the
learned distribution, pnewk (s, t), for each domains as
a function of the number of comparable document
pairs used in learning. In all domains, the compara-
ble document pairs are sorted according to their sim-
8And, indeed, by default our decoder copies OOV strings
into its output directly.
1082
0 10000 20000 30000 40000 50000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Number of Document Pairs
Mea
n Re
cipro
cal R
ank
0 10000 20000 30000 40000 50000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Number of Document Pairs
Mea
n Re
cipro
cal R
ank
0 10000 20000 30000 40000 50000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
?
Number of Document Pairs
Mea
n Re
cipro
cal R
ank ?
Full modelEdit distance baselineCCA BaselineModel without ED penalty 0.38
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
0.22
0.14
0.11
0.08
0.26
0.39
0.07
0.05
0.03
(a) Science (b) EMEA (c) Subtitles
Figure 3: Semi-extrinsic bilingual lexicon induction results. Mean reciprocal rank is computed over all OOV devel-
opment set words for which identity is not the correct translation.
ilarity with the NEW-domain. Figure 3 also shows
the performance of baseline models and our learner
without the edit distance penalty. For each source
word s, the edit distance (ED) baseline ranks all En-
glish words t in our monolingual data by their edit
distance with s.9 The Canonical Correlation Analy-
sis (CCA) baseline uses the approach of Daume? III
and Jagarlamudi (2011) and the top 25, 000 ranked
document pairs as a comparable corpus. That model
performs poorly largely because of sparse word con-
text counts. Interestingly, for Science and EMEA,
the performance of our full model at 50, 000 doc-
ument pairs is higher than the sum of the edit dis-
tance baseline and the model without the edit dis-
tance penalty, indicating that our approach effec-
tively combines the marginal matching and edit dis-
tance signals.
The learning curves for the three domains vary
substantially. For Science, learning is gradual and
it appears that additional gains could be made by
iterating over even more document pairs. In con-
trast, the model learns quickly for the EMEA do-
main; performance is stable after 20, 000 document
pairs. Given these results and our experience with
the two domains, we hypothesize that the difference
is due to the fact that the Science data is much more
heterogenous than the EMEA data. The Science data
9In particular, for each domain and each OOV French word,
we ranked the set of all English words that appeared at least five
times in the set of 50,000 most NEW-domain like Wikipedia
pages. Using a frequency threshold of five helped eliminate
French words and improperly tokenized English words from the
set of candidates.
includes physics, chemistry, and biology abstracts,
among others. The drug labels that make up most of
the EMEA data are more homogeneous. In Section
6 we comment on the poor Subtitles performance,
which persists in our MT experiments.
We experimented with making multiple learning
passes over the document pairs and observed rela-
tively small gains from doing so. In all experiments,
learning from some number of additional new doc-
ument pairs resulted in higher semi-extrinsic per-
formance gains than passing over document pairs
which were already observed.
In the case of OOV words, it?s clear that learning
something about how to translate a previously un-
observed French word is beneficial. However, our
learning method also learns domain-specific new-
translation senses (NTS). Table 1 shows some exam-
ples of what the marginal matching method learns
for different types of source words (OOVs, low fre-
quency, and NTS).
5.2 MT evaluation
By default, the Moses decoder copies OOV words
directly into its translated output. In some cases,
this is correct (e.g. ensembles, blumeria, google).
In other cases, French words can be translated into
English correctly by simply stripping accent marks
off of the OOV word and then copying it to the out-
put (e.g. came?ra, e?le?ments, mole?cules). In the Sci-
ence and EMEA domains, we found that our base-
line BLEU scores improved from 21.91 to 22.20
and 23.67 to 24.45, respectively, when we changed
the default handling of OOVs to strip accents before
1083
French OLD top pold(t|s) NEW top pgold(t|s) MM-learned top pnew(t|s)
OOV words
cisaillement - shear strength shearing shear viscous newtonian
courbure - curvature bending curvatures curvature curved manifold
Low frequency words
line?aires linear linear nonlinear non-linear linear linearly nonlinear
re?cepteur receiver receptor receiver y1 receptor receiver receptors
New translation sense words
champ field jurisdiction scope field magnetic near-field field magnetic fields
marche working march work walk step walking march walk walking
Table 1: Hand-picked examples of Science-domain French words and their top English translations in the OLD-
domain, NEW-domain, and marginal matching distributions. The first two are OOVs. The next two only appeared four
and one time, respectively, in the training data and only aligned to a single English word. The last two are NTS French
words: words that appeared frequently in the training data but for which the word?s sense in the new domain shifts.
copying into the output. Interestingly, performance
on the Subtitles domain text did not change at all
with this baseline modification. This is likely due
to the fact that there are fewer technical OOVs (the
terms typically captured by this accent-stripping pat-
tern) in the subtitles domain.
Throughout our experiments, we found it criti-
cal to retain correct ?freebie? OOV translations. In
the results presented below, including the baselines,
we supplement phrase tables with a new candidate
translation but also include accent-stripped identity,
or ?freebie,? translations in the table for all OOV
words. We experimented with classifying French
words as freebies or needing a new translation, but
oracle experiments showed very little improvement
(about 0.2 BLEU improvement in the Science do-
main), so instead we simply include both types of
translations in the phrase tables.
In addition to the strip-accents baseline, we com-
pare results with four other baselines. First, we
drop OOVs from the output translations. Second,
like our semi-extrinsic baseline, we rank English
words by their edit distance away from each French
OOV word (ED baseline). Third, we rank En-
glish words by their document-pair co-occurrence
score with each French OOV word. That is, for
all words w, we compute D(w), the vector indicat-
ing the document pairs in which w occurs, over the
set of 50,000 document-pairs which are most NEW-
domain-like. For French and English words s and t,
ifD(s) andD(t) are dissimilar, it is less likely (s, t)
is a valid translation pair. We weight D(w) entries
with BM25 (Robertson et al, 1994). For all French
OOVs, we rank all English translations according to
the cosine similarity between the pair of D(w) vec-
tors. The fourth baseline uses the CCA model de-
scribed in Daume? III and Jagarlamudi (2011) to rank
English words according to their distributional sim-
ilarity with each French word. For the CCA base-
line comparison, we only learned translations using
25,000 Science-domain document pairs, rather than
the full 50,000 and for all domains. However, it?s
unlikely that learning over more data would over-
come the low performance observed so far. For the
final three baselines, we append French OOV words
and their highest ranked English translation to the
phrase table. Along with each new translation pair,
we include one new phrase table feature with the
relevant translation score (edit distance, document
similarity, or CCA distributional similarity). For all
baselines other than drop-OOVs, we also include
accent-stripped translation pairs with an additional
indicator feature.
Table 3 shows results appending the top ranked
English translation for each OOV French word using
each baseline method. None of the alternate base-
lines outperform the simplest baseline on the subti-
tles data. Using document pair co-occurrences is the
strongest baseline for the Science and EMEA do-
mains. This confirms our intuition that taking ad-
vantage of document pair alignments is worthwhile.
For Science and EMEA, supplementing a model
with OOV translations learned through our marginal
matching method drastically outperforms all base-
1084
OOVs translated correctly and incorrectly
Input les re?sistances au cisaillement par poinc?onnement ...
Ref the punching shear strengths...
Baseline the resistances in cisaillement by poinconnement ...
MM the resistances in shear reinforcement...
OOV translated incorrectly
Input pre?sentation d? un logiciel permettant de ge?rer les donne?es temporelles .
Ref presentation of software which makes it possible to manage temporal data .
Baseline introduction of a software to manage temporelles data .
MM introduction of a software to manage data plugged .
Low frequency French words
Input ...limite est lie?e a` la de?croissance tre`s rapide du couplage e?lectron-phonon avec la tempe?rature .
Ref ...limit is linked to the rapid decrease of the electron-phonon coupling with temperature .
Baseline ...limit is linked to the decline very rapid electron-phonon linkage with the temperature .
MM ...limit is linked to the linear very rapid electron-phonon coupling with the temperature .
Table 2: Example MT outputs for Science domain. The baseline strips accents (Table 3). In the first example, the
previously OOV word cisaillement is translated correctly by an MM-supplemented model. The OOV poinc?onnement
is translated as reinforcement instead of strengths, which is incorrect with respect to the reference but arguably not
bad. In the second example, temporelles is not translated correctly in the MM output. In the third example, the MM-
hypothesized correct translation of low frequency word couplage, coupling, is chosen instead of incorrect linkage. Also
in the third example, the low frequency word de?croissance is translated as the MM-hypothesized incorrect translation
linear. In the case of de?croissance, the baseline?s translation, decline, is much better than the MM translation linear.
lines. Using our model to translate OOV words
yields scores of 23.62 and 26.97 in the Science and
EMEA domains, or 1.19 and 1.94 BLEU points, re-
spectively, above the strongest baseline. We observe
additional gains by also supplementing the model
with translations for low frequency French words.
For example, when we use our approach to translate
source words in the Science domain which appear
ten or fewer times in our OLD-domain training data,
the BLEU score increases to 24.28.
We tried appending top-k translations, varying k.
However, we found that for the baselines as well as
our MM translations, using only the top-1 English
translations outperformed using more.
Table 3 also shows the result of supplementing a
baseline phrase table with oracle OOV translations.
Using the marginal matching learned OOV transla-
tions takes us 30% and 40% of the way from the
baseline to the oracle upper bound for Science and
EMEA, respectively.
We have focused on supplementing an SMT
model trained on a sample of the Hansard parallel
corpus in order to mimic typical data conditions, but
we have also performed experiments supplementing
Science EMEA Subs
Simple Baseline 21.91 23.67 13.18
Drop OOVs 20.22 18.95 11.86
Accent-Stripped 22.20 24.45 13.13
ED Baseline 22.10 24.35 12.95
Doc Sim Baseline. 22.43 25.03 13.02
CCA Baseline 21.41 - -
MM Freq<1 (OOV) 23.62 26.97 13.07
MM Freq<11 24.28 27.26 12.97
MM Freq<101 23.96 26.82 12.92
Oracle OOV 26.38 29.99 15.06
Table 3: BLEU results using: (1) baselines, (2) phrase
tables augmented with top-1 translations for French
words with indicated OLD training data frequencies, (3)
phrase tables augmented with OOV oracle translations.
a model trained on the full dataset.10 Beginning with
the larger model, we observe performance gains of
0.8 BLEU points for both the EMEA and the Sci-
ence domains over the strongest baselines, which are
based on document similarity, when we add OOV
10We still use the joint that was learned starting with the one
estimated over the sample; we may observe greater gains over
the full Hansard baseline with a stronger initial joint.
1085
translations. As expected, these gains are less than
what we observe when our baseline model is esti-
mated over less data, but they are still substantial.
In all experiments, we have assumed that we have
no NEW-domain parallel training data, which is the
case for the vast majority of language pairs and do-
mains. However, In the case that we do have some
NEW-domain parallel data, OOV rates will be some-
what lower, but our method is still applicable. For
example, we would need 2.3 million words of Sci-
ence (NEW-domain) parallel data to cover just 50%
of the OOVs in our Science test set, and 4.3 million
words to cover 70%.
6 Discussion
BLEU score performance gains are substantial for
the Science and EMEA domains, but we don?t ob-
serve gains on the subtitles text. We believe this dif-
ference relates to the difference between a corpus
domain and a corpus register. As Lee (2002) ex-
plains, a text?s domain is most related to its topic,
while a text?s register is related to its type and pur-
pose. For example, religious, scientific, and dia-
logue texts may be classified as separate registers,
while political and scientific expositions may have a
single register but different domains. Our science
and EMEA corpora are certainly different in do-
main from the OLD-domain parliamentary proceed-
ings, and our success in boosting MT performance
with our methods indicates that the Wikipedia com-
parable corpora that we mined match those domains
well. In contrast, the subtitles data differs from the
OLD-domain parliamentary proceedings in both do-
main and register. Although the Wikipedia data that
we mined may be closer in domain to the subtitles
data than the parliamentary proceedings,11 its regis-
ter is certainly not film dialogues.
Although the use of marginal matching is, to the
best of our knowledge, novel in MT, there are related
threads of research that might inspire future work.
The intuition that we should match marginal distri-
butions is similar to work using no example labels
but only label proportions to estimate labels, for ex-
ample in Quadrianto et al (2008). Unlike that work,
11In fact, we believe that it is. Wikipedia pages that ranked
very high in our subtitles-like list included, for example, the
movie The Other Side of Heaven and actor Frank Sutton.
our label set corresponds to entire vocabularies, and
we have multiple observed label proportions. Also,
while the marginal matching objective seems effec-
tive in practice, it is difficult to optimize. A number
of recently developed approximate inference meth-
ods use a decomposition that bears a strong resem-
blance to this objective function. Considering the
marginal distributions from each document pair to
be a separate subproblem, we could approach the
global objective of satisfying all subproblems as an
instance of dual decomposition (Sontag et al, 2010)
or ADMM (Gabay and Mercier, 1976; Glowinski
and Marrocco, 1975).
We experiment with French-English because tun-
ing and test sets are available in several domains for
that language pair. However, our techniques are di-
rectly applicable to other language pairs, including
those that are less related. We have observed that
many domain-specific terms, particularly in medi-
cal and science domains, are borrowed across lan-
guages, whether or not the languages are related.
Even for languages with different character sets,
one could do transliteration before measuring ortho-
graphical similarity.
Although we were able to identify translations for
some NTS words (Table 1), we did not make use of
them in our MT experiments. Recent work has iden-
tified NTS words in NEW-domain corpora (Carpuat
et al, 2013b), and in future work we plan to incorpo-
rate discovered translations for such words into MT.
7 Conclusions
We proposed a model for learning a joint distribu-
tion of source-target word pairs based on the idea
that its marginals should match those observed in
NEW-domain comparable corpora. Supplementing
a baseline phrase-based SMT model with learned
translations results in BLEU score gains of about
two points in the medical and science domains.
Acknowledgments
We gratefully acknowledge the support of the
2012 JHU Summer Workshop and NSF Grant No
1005411. We would like to thank the entire DAMT
team (http://hal3.name/damt/) and San-
jeev Khudanpur for their help and suggestions.
We also acknowledge partial support from DARPA
1086
CSSG Grant D11AP00279 and DARPA BOLT Con-
tract HR0011-12-C-0015 for Hal Daume? III and
support from the Johns Hopkins University Human
Language Technology Center of Excellence for Ann
Irvine. The views and conclusions contained in this
publication are those of the authors and should not
be interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk.
Marine Carpuat, Hal Daume? III, Alexander Fraser, Chris
Quirk, Fabienne Braune, Ann Clifton, Ann Irvine,
Jagadeesh Jagarlamudi, John Morgan, Majid Raz-
mara, Ales? Tamchyna, Katharine Henry, and Rachel
Rudinger. 2013a. Domain adaptation in machine
translation: Final report. In 2012 Johns Hopkins Sum-
mer Workshop Final Report.
Marine Carpuat, Hal Daume? III, Katharine Henry, Ann
Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger.
2013b. Sensespotting: Never let your parallel data tie
you to an old domain. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
S. Della Pietra, V. Della Pietra, R. L. Mercer, and
S. Roukos. 1992. Adaptive language modeling us-
ing minimum discriminant estimation. Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Marcello Federico. 1999. Efficient language model
adaptation through mdi estimation. In Proceedings of
EUROSPEECH.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the Conference of the As-
sociation for Computational Linguistics (ACL).
Daniel Gabay and Bertrand Mercier. 1976. A dual al-
gorithm for the solution of nonlinear variational prob-
lems via finite element approximation. Computers and
Mathematics with Applications, 2(1):17 ? 40.
Roland Glowinski and A. Marrocco. 1975. Sur
l?approximation, par e?le?ments finis d?ordre un, et la
re?solution, par pe?nalisation-dualite?, d?une classe de
proble`mes de dirichlet non line?aires. Rev. Franc. Au-
tomat. Inform. Rech. Operat., 140:41?76.
Gurobi Optimization Inc. 2013. Gurobi optimizer refer-
ence manual.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Ann Irvine and Chris Callison-Burch. 2013. Supervised
bilingual lexicon induction with multiple monolingual
signals. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Ann Irvine, John Morgan, Marine Carpuat, Hal Daume?
III, and Dragos Munteanu. 2013. Measuring machine
translation errors in new domains. Transactions of the
Association for Computational Linguistics (TACL).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
David Lee. 2002. Genres, registers, text types, domains
and styles: Clarifying the concepts and navigating a
path through the bnc jungle. Language and Comput-
ers, 42(1):247?292.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,
1087
and Jeff Bilmes. 2010. Panlingual lexical transla-
tion via probabilistic inference. Artificial Intelligence,
174:619?637, June.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL).
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Novi Quadrianto, Alex J. Smola, Tiberio S. Caetano, and
Quoc V. Le. 2008. Estimating labels from label pro-
portions. In Proceedings of the International Confer-
ence on Machine Learning (ICML).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the Conference of the Associ-
ation for Computational Linguistics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Majid Razmara, Maryam Siahbani, Gholamreza Haffari,
and Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In Proceedings of the Text REtrieval Conference.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using Di-
verse Similarity Measures. Ph.D. thesis, Johns Hop-
kins University.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
David Sontag, A. Globerson, and Tommi Jaakola, 2010.
Introduction to dual decomposition for inference,
chapter 1. MIT Press.
Jo?rg Tiedemann. 2009. News from OPUS - A collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing (RANLP).
1088
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130?140,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Toward Statistical Machine Translation without Parallel Corpora
Alexandre Klementiev Ann Irvine Chris Callison-Burch David Yarowsky
Center for Language and Speech Processing
Johns Hopkins University
Abstract
We estimate the parameters of a phrase-
based statistical machine translation sys-
tem from monolingual corpora instead of a
bilingual parallel corpus. We extend exist-
ing research on bilingual lexicon induction
to estimate both lexical and phrasal trans-
lation probabilities for MT-scale phrase-
tables. We propose a novel algorithm to es-
timate reordering probabilities from mono-
lingual data. We report translation results
for an end-to-end translation system us-
ing these monolingual features alone. Our
method only requires monolingual corpora
in source and target languages, a small
bilingual dictionary, and a small bitext for
tuning feature weights. In this paper, we ex-
amine an idealization where a phrase-table
is given. We examine the degradation in
translation performance when bilingually
estimated translation probabilities are re-
moved and show that 80%+ of the loss can
be recovered with monolingually estimated
features alone. We further show that our
monolingual features add 1.5 BLEU points
when combined with standard bilingually
estimated phrase table features.
1 Introduction
The parameters of statistical models of transla-
tion are typically estimated from large bilingual
parallel corpora (Brown et al 1993). However,
these resources are not available for most lan-
guage pairs, and they are expensive to produce in
quantities sufficient for building a good transla-
tion system (Germann, 2001). We attempt an en-
tirely different approach; we use cheap and plen-
tiful monolingual resources to induce an end-to-
end statistical machine translation system. In par-
ticular, we extend the long line of work on in-
ducing translation lexicons (beginning with Rapp
(1995)) and propose to use multiple independent
cues present in monolingual texts to estimate lex-
ical and phrasal translation probabilities for large,
MT-scale phrase-tables. We then introduce a
novel algorithm to estimate reordering features
from monolingual data alone, and we report the
performance of a phrase-based statistical model
(Koehn et al 2003) estimated using these mono-
lingual features.
Most of the prior work on lexicon induction
is motivated by the idea that it could be applied
to machine translation but stops short of actu-
ally doing so. Lexicon induction holds the po-
tential to create machine translation systems for
languages which do not have extensive parallel
corpora. Training would only require two large
monolingual corpora and a small bilingual dictio-
nary, if one is available. The idea is that intrin-
sic properties of monolingual data (possibly along
with a handful of bilingual pairs to act as exam-
ple mappings) can provide independent but infor-
mative cues to learn translations because words
(and phrases) behave similarly across languages.
This work is the first attempt to extend and apply
these ideas to an end-to-end machine translation
pipeline. While we make an explicit assumption
that a table of phrasal translations is given a priori,
we induce every other parameter of a full phrase-
based translation system from monolingual data
alone. The contributions of this work are:
? In Section 2.2 we analyze the challenges
of using bilingual lexicon induction for sta-
tistical MT (performance on low frequency
items, and moving from words to phrases).
? In Sections 3.1 and 3.2 we use multiple cues
present in monolingual data to estimate lexi-
cal and phrasal translation scores.
? In Section 3.3 we propose a novel algo-
rithm for estimating phrase reordering fea-
tures from monolingual texts.
? Finally, in Section 5 we systematically drop
feature functions from a phrase table and
then replace them with monolingually es-
timated equivalents, reporting end-to-end
translation quality.
130
2 Background
We begin with a brief overview of the stan-
dard phrase-based statistical machine translation
model. Here, we define the parameters which
we later replace with monolingual alternatives.
We continue with a discussion of bilingual lex-
icon induction; we extend these methods to es-
timate the monolingual parameters in Section 3.
This approach allows us to replace expensive/rare
bilingual parallel training data with two large
monolingual corpora, a small bilingual dictionary,
and ?2,000 sentence bilingual development set,
which are comparatively plentiful/inexpensive.
2.1 Parameters of phrase-based SMT
Statistical machine translation (SMT) was first
formulated as a series of probabilistic mod-
els that learn word-to-word correspondences
from sentence-aligned bilingual parallel corpora
(Brown et al 1993). Current methods, includ-
ing phrase-based (Och, 2002; Koehn et al 2003)
and hierarchical models (Chiang, 2005), typically
start by word-aligning a bilingual parallel cor-
pus (Och and Ney, 2003). They extract multi-
word phrases that are consistent with the Viterbi
word alignments and use these phrases to build
new translations. A variety of parameters are es-
timated using the bitexts. Here we review the pa-
rameters of the standard phrase-based translation
model (Koehn et al 2007). Later we will show
how to estimate them using monolingual texts in-
stead. These parameters are:
? Phrase pairs. Phrase extraction heuristics
(Venugopal et al 2003; Tillmann, 2003;
Och and Ney, 2004) produce a set of phrase
pairs (e, f) that are consistent with the word
alignments. In this paper we assume that the
phrase pairs are given (without any scores),
and we induce every other parameter of the
phrase-based model from monolingual data.
? Phrase translation probabilities. Each
phrase pair has a list of associated fea-
ture functions (FFs). These include phrase
translation probabilities, ?(e|f) and ?(f |e),
which are typically calculated via maximum
likelihood estimation.
? Lexical weighting. Since MLE overestimates
? for phrase pairs with sparse counts, lexi-
cal weighting FFs are used to smooth. Aver-
How
much
should
you
charge
for
your
W
i
e
v
i
e
l
s
o
l
l
t
e
m
a
n
a
u
f
r
g
u
n
d
s
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
v
e
r
d
i
e
n
e
n
Facebook
profile
s
d
m
m
m
d
d
Figure 1: The reordering probabilities from the phrase-
based models are estimated from bilingual data by cal-
culating how often in the parallel corpus a phrase pair
(f, e) is orientated with the preceding phrase pair in
the 3 types of orientations (monotone, swapped, and
discontinuous).
age word translation probabilities, w(ei|fj),
are calculated via phrase-pair-internal word
alignments.
? Reordering model. Each phrase pair (e, f)
also has associated reordering parameters,
po(orientation|f, e), which indicate the dis-
tribution of its orientation with respect to the
previously translated phrase. Orientations
are monotone, swap, discontinuous (Tillman,
2004; Kumar and Byrne, 2004), see Figure 1.
? Other features. Other typical features are
n-gram language model scores and a phrase
penalty, which governs whether to use fewer
longer phrases or more shorter phrases.
These are not bilingually estimated, so we
can re-use them directly without modifica-
tion.
The features are combined in a log linear model,
and their weights are set through minimum error
rate training (Och, 2003). We use the same log
linear formulation and MERT but propose alterna-
tives derived directly from monolingual data for
all parameters except for the phrase pairs them-
selves. Our pipeline still requires a small bitext of
approximately 2,000 sentences to use as a devel-
opment set for MERT parameter tuning.
131
2.2 Bilingual lexicon induction for SMT
Bilingual lexicon induction describes the class of
algorithms that attempt to learn translations from
monolingual corpora. Rapp (1995) was the first
to propose using non-parallel texts to learn the
translations of words. Using large, unrelated En-
glish and German corpora (with 163m and 135m
words) and a small German-English bilingual dic-
tionary (with 22k entires), Rapp (1999) demon-
strated that reasonably accurate translations could
be learned for 100 German nouns that were not
contained in the seed bilingual dictionary. His al-
gorithm worked by (1) building a context vector
representing an unknown German word by count-
ing its co-occurrence with all the other words
in the German monolingual corpus, (2) project-
ing this German vector onto the vector space of
English using the seed bilingual dictionary, (3)
calculating the similarity of this sparse projected
vector to vectors for English words that were con-
structed using the English monolingual corpus,
and (4) outputting the English words with the
highest similarity as the most likely translations.
A variety of subsequent work has extended the
original idea either by exploring different mea-
sures of vector similarity (Fung and Yee, 1998)
or by proposing other ways of measuring simi-
larity beyond co-occurence within a context win-
dow. For instance, Schafer and Yarowsky (2002)
demonstrated that word translations tend to co-
occur in time across languages. Koehn and Knight
(2002) used similarity in spelling as another kind
of cue that a pair of words may be translations of
one another. Garera et al(2009) defined context
vectors using dependency relations rather than ad-
jacent words. Bergsma and Van Durme (2011)
used the visual similarity of labeled web images
to learn translations of nouns. Additional related
work on learning translations from monolingual
corpora is discussed in Section 6.
In this paper, we apply bilingual lexicon in-
duction methods to statistical machine translation.
Given the obvious benefits of not having to rely
on scarce bilingual parallel training data, it is sur-
prising that bilingual lexicon induction has not
been used for SMT before now. There are sev-
eral open questions that make its applicability to
SMT uncertain. Previous research on bilingual
lexicon induction learned translations only for a
small number of high frequency words (e.g. 100
l
llll
lll
l
0 100 200 300 400 500 600
0
10
20
30
40
Accu
racy
, %
Corpus Frequency
l Top 1Top 10
Figure 2: Accuracy of single-word translations in-
duced using contextual similarity as a function of the
source word corpus frequency. Accuracy is the pro-
portion of the source words with at least one correct
(bilingual dictionary) translation in the top 1 and top
10 candidate lists.
nouns in Rapp (1995), 1,000 most frequent words
in Koehn and Knight (2002), or 2,000 most fre-
quent nouns in Haghighi et al(2008)). Although
previous work reported high translation accuracy,
it may be misleading to extrapolate the results to
SMT, where it is necessary to translate a much
larger set of words and phrases, including many
low frequency items.
In a preliminary study, we plotted the accuracy
of translations against the frequency of the source
words in the monolingual corpus. Figure 2 shows
the result for translations induced using contex-
tual similarity (defined in Section 3.1). Unsur-
prisingly, frequent terms have a substantially bet-
ter chance of being paired with a correct transla-
tion, with words that only occur once having a low
chance of being translated accurately.1 This prob-
lem is exacerbated when we move to multi-token
phrases. As with phrase translation features esti-
mated from parallel data, longer phrases are more
sparse, making similarity scores less reliable than
for single words.
Another impediment (not addressed in this
paper) for using lexicon induction for SMT is
the number of translations that must be learned.
Learning translations for all words in the source
language requires n2 vector comparisons, since
each word in the source language vocabulary must
1For a description of the experimental setup used to pro-
duce these translations, see Experiment 8 in Section 5.2.
132
s1
s
2
s
3
s
N-1
s
N
?
?
?
t
1
t
2
t
3
t
M-1
t
M
?
?
dict.
project
?
?
?
?
?
?
c
o
m
p
a
r
e
para crecer
to expand
activity of
economic
activity
policy
growth
foreign
economico
tasa
planeta
empleo
extranjero
policy
para crecer
(projected)
ES Context
Vector
Projected ES
Context Vector
EN Context
Vectors
Figure 3: Scoring contextual similarity of phrases:
first, contextual vectors are projected using a small
seed dictionary and then compared with the target lan-
guage candidates.
be compared against the vectors for all words in
the target language vocabulary. The size of the n2
comparisons hugely increases if we compare vec-
tors for multi-word phrases instead of just words.
In this work, we avoid this problem by assuming
that a limited set of phrase pairs is given a pri-
ori (but without scores). By limiting ourselves
to phrases in a phrase table, we vastly limit the
search space of possible translations. This is an
idealization because high quality translations are
guaranteed to be present. However, as our lesion
experiments in Section 5.1 show, a phrase table
without accurate translation probability estimates
is insufficient to produce high quality translations.
We show that lexicon induction methods can be
used to replace bilingual estimation of phrase- and
lexical-translation probabilities, making a signifi-
cant step towards SMT without parallel corpora.
3 Monolingual Parameter Estimation
We use bilingual lexicon induction methods to es-
timate the parameters of a phrase-based transla-
tion model from monolingual data. Instead of
scores estimated from bilingual parallel data, we
make use of cues present in monolingual data to
provide multiple orthogonal estimates of similar-
ity between a pair of phrases.
3.1 Phrasal similarity features
Contextual similarity. We extend the vector
space approach of Rapp (1999) to compute sim-
ilarity between phrases in the source and tar-
get languages. More formally, assume that
(s1, s2, . . . sN ) and (t1, t2, . . . tM ) are (arbitrarily
indexed) source and target vocabularies, respec-
tively. A source phrase f is represented with an
terrorist (en)terrorista (es)
Occ
urre
nces
terrorist (en)riqueza (es)
Occ
urre
nces
Time
Figure 4: Temporal histograms of the English phrase
terrorist, its Spanish translation terrorista, and riqueza
(wealth) collected from monolingual texts spanning a
13 year period. While the correct translation has a
good temporal match, the non-translation riqueza has
a distinctly different signature.
N - and target phrase e with an M -dimensional
vector (see Figure 3). The component values of
the vector representing a phrase correspond to
how often each of the words in that vocabulary
appear within a two word window on either side
of the phrase. These counts are collected using
monolingual corpora. After the values have been
computed, a contextual vector f is projected onto
the English vector space using translations in a
seed bilingual dictionary to map the component
values into their appropriate English vector posi-
tions. This sparse projected vector is compared
to the vectors representing all English phrases e.
Each phrase pair in the phrase table is assigned
a contextual similarity score c(f, e) based on the
similarity between e and the projection of f .
Various means of computing the component
values and vector similarity measures have been
proposed in literature (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of f ?s
contextual vector as follows:
wk = nf,k ? (log(n/nk) + 1)
where nf,k and nk are the number of times sk ap-
pears in the context of f and in the entire corpus,
and n is the maximum number of occurrences of
any word in the data. Intuitively, the more fre-
quently sk appears with f and the less common
it is in the corpus in general, the higher its com-
ponent value. Similarity between two vectors is
measured as the cosine of the angle between them.
Temporal similarity. In addition to contex-
tual similarity, phrases in two languages may
133
be scored in terms of their temporal similarity
(Schafer and Yarowsky, 2002; Klementiev and
Roth, 2006; Alfonseca et al 2009). The intu-
ition is that news stories in different languages
will tend to discuss the same world events on the
same day. The frequencies of translated phrases
over time give them particular signatures that will
tend to spike on the same dates. For instance, if
the phrase asian tsunami is used frequently dur-
ing a particular time span, the Spanish transla-
tion maremoto asia?tico is likely to also be used
frequently during that time. Figure 4 illustrates
how the temporal distribution of terrorist is more
similar to Spanish terrorista than to other Span-
ish phrases. We calculate the temporal similar-
ity between a pair of phrases t(f, e) using the
method defined by Klementiev and Roth (2006).
We generate a temporal signature for each phrase
by sorting the set of (time-stamped) documents in
the monolingual corpus into a sequence of equally
sized temporal bins and then counting the number
of phrase occurrences in each bin. In our exper-
iments, we set the window size to 1 day, so the
size of temporal signatures is equal to the num-
ber of days spanned by our corpus. We use cosine
distance to compare the normalized temporal sig-
natures for a pair of phrases (f, e).
Topic similarity. Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. Thus, topic or cat-
egory information associated with monolingual
data can also be used to indicate similarity be-
tween a phrase and its candidate translation. In
order to score a pair of phrases, we collect their
topic signatures by counting their occurrences in
each topic and then comparing the resulting vec-
tors. We again use the cosine similarity mea-
sure on the normalized topic signatures. In our
experiments, we use interlingual links between
Wikipedia articles to estimate topic similarity. We
treat each linked article pair as a topic and collect
counts for each phrase across all articles in its cor-
responding language. Thus, the size of a phrase
topic signature is the number of article pairs with
interlingual links in Wikipedia, and each compo-
nent contains the number of times the phrase ap-
pears in (the appropriate side of) the correspond-
ing pair. Our Wikipedia-based topic similarity
feature, w(f, e), is similar in spirit to polylingual
topic models (Mimno et al 2009), but it is scal-
able to full bilingual lexicon induction.
3.2 Lexical similarity features
In addition to the three phrase similarity features
used in our model ? c(f, e), t(f, e) and w(f, e) ?
we include four additional lexical similarity fea-
tures for each of phrase pair. The first three lex-
ical features clex(f, e), tlex(f, e) and wlex(f, e)
are the lexical equivalents of the phrase-level con-
textual, temporal and wikipedia topic similarity
scores. They score the similarity of individual
words within the phrases. To compute these
lexical similarity features, we average similarity
scores over all possible word alignments across
the two phrases. Because individual words are
more frequent than multiword phrases, the accu-
racy of clex, tlex, and wlex tends to be higher than
their phrasal equivalents (this is similar to the ef-
fect observed in Figure 2).
Orthographic / phonetic similarity. The final
lexical similarity feature that we incorporate is
o(f, e), which measures the orthographic similar-
ity between words in a phrase pair. Etymolog-
ically related words often retain similar spelling
across languages with the same writing system,
and low string edit distance sometimes signals
translation equivalency. Berg-Kirkpatrick and
Klein (2011) present methods for learning cor-
respondences between the alphabets of two lan-
guages. We can also extend this idea to language
pairs not sharing the same writing system since
many cognates, borrowed words, and names re-
main phonetically similar. Transliterations can be
generated for tokens in a source phrase (Knight
and Graehl, 1997), with o(f, e) calculating pho-
netic similarity rather than orthographic.
The three phrasal and four lexical similarity
scores are incorporated into the log linear trans-
lation model as feature functions, replacing the
bilingually estimated phrase translation probabil-
ities ? and lexical weighting probabilities w. Our
seven similarity scores are not the only ones that
could be incorporated into the translation model.
Various other similarity scores can be computed
depending on the available monolingual data and
its associated metadata (see, e.g. Schafer and
Yarowsky (2002)).
3.3 Reordering
The remaining component of the phrase-based
SMT model is the reordering model. We
introduce a novel algorithm for estimating
134
Input: Source and target phrases f and e,
Source and target monolingual corpora Cf and Ce,
Phrase table pairs T = {(f (i), e(i))}Ni=1.
Output: Orientation features (pm, ps, pd).
Sf ? sentences containing f in Cf ;
Se ? sentences containing e in Ce;
(Bf ,?,?)? CollectOccurs(f,?Ni=1f
(i), Sf );
(Be, Ae, De)? CollectOccurs(e,?Ni=1e
(i), Se);
cm = cs = cd = 0;
foreach unique f ? in Bf do
foreach translation e? of f ? in T do
cm = cm + #Be (e
?);
cs = cs + #Ae (e
?);
cd = cd + #De (e
?);
c? cm + cs + cd;
return ( cmc ,
cs
c ,
cd
c )
CollectOccurs(r, R, S)
B ? (); A? (); D ? ();
foreach sentence s ? S do
foreach occurrence of phrase r in s do
B ? B + (longest preceding r and in R);
A? A + (longest following r and in R);
D ? D + (longest discontinuous w/ r and in
R);
return (B, A, D);
Figure 5: Algorithm for estimating reordering
probabilities from monolingual data.
po(orientation|f, e) from two monolingual cor-
pora instead a bitext.
Figure 1 illustrates how the phrase pair orienta-
tion statistics are estimated in the standard phrase-
based SMT pipeline. For a phrase pair like (f =
?Profils?, e = ?profile?), we count its orien-
tation with the previously translated phrase pair
(f ? = ?in Facebook?, e? = ?Facebook?) across
all translated sentence pairs in the bitext.
In our pipeline we do not have translated sen-
tence pairs. Instead, we look for monolingual
sentences in the source corpus which contain
the source phrase that we are interested in, like
f = ?Profils?, and at least one other phrase
that we have a translation for, like f ? = ?in
Facebook?. We then look for all target lan-
guage sentences in the target monolingual cor-
pus that contain the translation of f (here e =
?profile?) and any translation of f ?. Figure 6 il-
lustrates that it is possible to find evidence for
po(swapped|Profils, profile), even from the non-
parallel, non-translated sentences drawn from two
independent monolingual corpora. By looking for
foreign sentences containing pairs of adjacent for-
eign phrases (f, f ?) and English sentences con-
D
a
s
A
n
l
e
g
e
n
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
i
s
t
e
i
n
f
a
c
h
s
What
does
your
Facebook
profile
reveal
Figure 6: Collecting phrase orientation statistics for
a English-German phrase pair (?profile?, ?Profils?)
from non-parallel sentences (the German sentence
translates as ?Creating a Facebook profile is easy?).
taining their corresponding translations (e, e?), we
are able to increment orientation counts for (f, e)
by looking at whether e and e? are adjacent,
swapped, or discontinuous. The orientations cor-
respond directly to those shown in Figure 1.
One subtly of our method is that shorter and
more frequent phrases (e.g. punctuation) are more
likely to appear in multiple orientations with a
given phrase, and therefore provide poor evi-
dence of reordering. Therefore, we (a) collect
the longest contextual phrases (which also appear
in the phrase table) for reordering feature estima-
tion, and (b) prune the set of sentences so that
we only keep a small set of least frequent contex-
tual phrases (this has the effect of dropping many
function words and punctuation marks and and re-
lying more heavily on multi-word content phrases
to estimate the reordering).2
Our algorithm for learning the reordering pa-
rameters is given in Figure 5. The algorithm
estimates a probability distribution over mono-
tone, swap, and discontinuous orientations (pm,
ps, pd) for a phrase pair (f, e) from two mono-
lingual corpora Cf and Ce. It begins by calling
CollectOccurs to collect the longest match-
ing phrase table phrases that precede f in source
monolingual data (Bf ), as well as those that pre-
cede (Be), follow (Ae), and are discontinuous
(De) with e in the target language data. For each
unique phrase f ? preceding f , we look up transla-
tions in the phrase table T. Next, we count3 how
2The pruning step has an additional benefit of minimizing
the memory needed for orientation feature estimations.
3#L(x) returns the count of object x in list L.
135
Monolingual training corpora
Europarl Gigaword Wikipedia
date range 4/96-10/09 5/94-12/08 n/a
uniq shared dates 829 5,249 n/a
Spanish articles n/a 3,727,954 59,463
English articles n/a 4,862,876 59,463
Spanish lines 1,307,339 22,862,835 2,598,269
English lines 1,307,339 67,341,030 3,630,041
Spanish words 28,248,930 774,813,847 39,738,084
English words 27,335,006 1,827,065,374 61,656,646
Spanish-English phrase table
Phrase pairs 3,093,228
Spanish phrases 89,386
English phrases 926,138
Spanish unigrams 13,216
Avg # translations 98.7
Spanish bigrams 41,426
Avg # translations 31.9
Spanish trigrams 34,744
Avg # translations 13.5
Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
many translations e? of f ? appeared before, after
or were discontinuous with e in the target lan-
guage data. Finally, the counts are normalized and
returned. These normalized counts are the values
we use as estimates of po(orientation|f, e).
4 Experimental Setup
We use the Spanish-English language pair to test
our method for estimating the parameters of an
SMT system from monolingual corpora. This al-
lows us to compare our method against the nor-
mal bilingual training procedure. We expect bilin-
gual training to result in higher translation qual-
ity because it is a more direct method for learn-
ing translation probabilities. We systematically
remove different parameters from the standard
phrase-based model, and then replace them with
our monolingual equivalents. Our goal is to re-
cover as much of the loss as possible for each of
the deleted bilingual components.
The standard phrase-based model that we use
as our top-line is the Moses system (Koehn et
al., 2007) trained over the full Europarl v5 par-
allel corpus (Koehn, 2005). With the exception
of maximum phrase length (set to 3 in our ex-
periments), we used default values for all of the
parameters. All experiments use a trigram lan-
guage model trained on the English side of the
Europarl corpus using SRILM with Kneser-Ney
smoothing. To tune feature weights in minimum
error rate training, we use a development bitext
of 2,553 sentence pairs, and we evaluate per-
formance on a test set of 2,525 single-reference
translated newswire articles. These development
and test datasets were distributed in the WMT
shared task (Callison-Burch et al 2010).4 MERT
4Specifcially, news-test2008 plus news-syscomb2009 for
dev and newstest2009 for test.
was re-run for every experiment.
We estimate the parameters of our model from
two sets of monolingual data, detailed in Table 1:
? First, we treat the two sides of the Europarl
parallel corpus as independent, monolingual
corpora. Haghighi et al(2008) also used
this method to show how well translations
could be learned from monolingual corpora
under ideal conditions, where the contextual
and temporal distribution of words in the two
monolingual corpora are nearly identical.
? Next, we estimate the features from truly
monolingual corpora. To estimate the con-
textual and temporal similarity features, we
use the Spanish and English Gigaword cor-
pora.5 These corpora are substantially larger
than the Europarl corpora, providing 27x as
much Spanish and 67x as much English for
contextual similarity, and 6x as many paired
dates for temporal similarity. Topical simi-
larity is estimated using Spanish and English
Wikipedia articles that are paired with inter-
language links.
To project context vectors from Spanish to En-
glish, we use a bilingual dictionary containing en-
tries for 49,795 Spanish words. Note that end-to-
end translation quality is robust to substantially
reducing dictionary size, but we omit these ex-
periments due to space constraints. The con-
text vectors for words and phrases incorporate co-
occurrence counts using a two-word window on
either side.
The title of our paper uses the word towards be-
cause we assume that an inventory of phrase pairs
is given. Future work will explore inducing the
5We use the afp, apw and xin sections of the corpora.
136
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 7: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-
English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equiva-
lents estimated from monolingual Eur parl data (experiments 5-10). The labels indicate how the different types
of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 8: Performance of monolingual features de-
rived from truly monolingual corpora. Over 82% of
the BLEU score loss can be recovered.
phrase table itself from monolingual texts. Across
all of our experiments, we use the phrase table
that the bilingual model learned from the Europarl
parallel corpus. We keep its phrase pairs, but we
drop all of its scores. Table 1 gives details of the
phrase pairs. In our experiments, we estimated
similarity and reordering scores for more than 3
million phrase pairs. For each source phrase, the
set of possible translations was constrained and
likely to contain good translations. However, the
average number of possible translations was high
(ranging from nearly 100 translations for each un-
igram to 14 for each trigram). These contain a
lot of noise and result in low end-to-end transla-
tion quality without good estimates of translation
quality, as the experiments in Section 5.1 show.
Software. Because many details of our estima-
tion procedures must be omitted for space, we dis-
tribute our full set of code along with scripts for
running our experiments and output translations.
These may be downed from http://www.cs.
jhu.edu/?anni/papers/lowresmt/
5 Experimental Results
Figures 7 and 8 give experimental results. Figure
7 shows the performance of the standard phrase-
based model when each of the bilingually esti-
mated features are removed. It shows how much
of the performance loss can be recovered using
our monolingual features when they are estimated
from the Europarl training corpus but treating
each side as an independent, monolingual cor-
pus. Figure 8 shows the recovery when using truly
monolingual corpora to estimate the parameters.
5.1 Lesion experiments
Experiments 1-4 remove bilingually estimated pa-
rameters from the standard model. For Spanish-
English, the relative contribution of the phrase-
table features (which include the phrase transla-
tion probabilities ? and the lexical weights w) is
greater than the reordering probabilities. When
the reordering probability po(orientation|f, e) is
eliminated and replaced with a simple distance-
based distortion feature that does not require a
bitext to estimate, the score dips only marginally
since word order in English and Spanish is simi-
lar. However, when both the reordering and the
phrase table features are dropped, leaving only
the LM feature and the phrase penalty, the result-
ing translation quality is abysmal, with the score
dropping a total of over 17 BLEU points.
5.2 Adding equivalent monolingual features
estimated using Europarl
Experiments 5-10 show how much our monolin-
gual equivalents could recover when the monolin-
gual corpora are drawn from the two sides of the
bitext. For instance, our algorithm for estimating
137
reordering probabilities from monolingual data (?
/M) adds 5 BLEU points, which is 73% of the po-
tential recovery going from the model (?/?) to the
model with bilingual reordering features (?/B).
Of the temporal, orthographic, and contextual
monolingual features the temporal feature per-
forms the best. Together (M/?), they recover
more than each individually. Combining mono-
lingually estimated reordering and phrase table
features (M/M) yields a total gain of 13.5 BLEU
points, or over 75% of the BLEU score loss that
occurred when we dropped all features from the
phrase table. However, these results use ?mono-
lingual? corpora which have practically identical
phrasal and temporal distributions.
5.3 Estimating features using truly
monolingual corpora
Experiments 12-18 estimate all of the features
from truly monolingual corpora. Our novel al-
gorithm for estimating reordering holds up well
and recovers 69% of the loss, only 0.4 BLEU
points less than when estimated from the Europarl
monolingual texts. The temporal similarity fea-
ture does not perform as well as when it was esti-
mated using Europarl data, but the contextual fea-
ture does. The topic similarity using Wikipedia
performs the strongest of the individual features.
Combining the monolingually estimated re-
ordering features with the monolingually esti-
mated similarity features (M/M) yields a total
gain of 14.8 BLEU points, or over 82% of the
BLEU point loss that occurred when we dropped
all features from the phrase table. This is equiv-
alent to training the standard system on a bi-
text with roughly 60,000 lines or nearly 2 million
words (learning curve omitted for space).
Finally, we supplement the standard bilingually
estimated model parameters with our monolin-
gual features (BM/B), and we see a 1.5 BLEU
point increase over the standard model. There-
fore, our monolingually estimated scores capture
some novel information not contained in the stan-
dard feature set.
6 Additional Related Work
Carbonell et al(2006) described a data-driven
MT system that used no parallel text. It produced
translation lattices using a bilingual dictionary
and scored them using an n-gram language model.
Their method has no notion of translation similar-
ity aside from a bilingual dictionary. Similarly,
Sa?nchez-Cartagena et al(2011) supplement an
SMT phrase table with translation pairs extracted
from a bilingual dictionary and give each a fre-
quency of one for computing translation scores.
Ravi and Knight (2011) treat MT without paral-
lel training data as a decipherment task and learn
a translation model from monolingual text. They
translate corpora of Spanish time expressions and
subtitles, which both have a limited vocabulary,
into English. Their method has not been applied
to broader domains of text.
Most work on learning translations from mono-
lingual texts only examine small numbers of fre-
quent words. Huang et al(2005) and Daume? and
Jagarlamudi (2011) are exceptions that improve
MT by mining translations for OOV items.
A variety of past research has focused on min-
ing parallel or comparable corpora from the web
(Munteanu and Marcu, 2006; Smith et al 2010;
Uszkoreit et al 2010). Others use an existing
SMT system to discover parallel sentences within
independent monolingual texts, and use them to
re-train and enhance the system (Schwenk, 2008;
Chen et al 2008; Schwenk and Senellart, 2009;
Rauf and Schwenk, 2009; Lambert et al 2011).
These are complementary but orthogonal to our
research goals.
7 Conclusion
This paper has demonstrated a novel set of tech-
niques for successfully estimating phrase-based
SMT parameters from monolingual corpora, po-
tentially circumventing the need for large bitexts,
which are expensive to obtain for new languages
and domains. We evaluated the performance of
our algorithms in a full end-to-end translation sys-
tem. Assuming that a bilingual-corpus-derived
phrase table is available, we were able utilize our
monolingually-estimated features to recover over
82% of BLEU loss that resulted from removing
the bilingual-corpus-derived phrase-table proba-
bilities. We also showed that our monolingual fea-
tures add 1.5 BLEU points when combined with
standard bilingually estimated features. Thus our
techniques have stand-alone efficacy when large
bilingual corpora are not available and also make
a significant contribution to combined ensemble
performance when they are.
138
References
Enrique Alfonseca, Massimiliano Ciaramita, and
Keith Hall. 2009. Gazpacho and summer rash:
lexical relationships from temporal patterns of web
search queries. In Proceedings of EMNLP.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-2011), Edinburgh, Scotland, UK.
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual simi-
larity of labeled web images. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence.
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Robert Mercer,
and Paul Poossin. 1988. A statistical approach to
language translation. In 12th International Confer-
ence on Computational Linguistics (CoLing-1988).
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation.
Jaime Carbonell, Steve Klein, David Miller, Michael
Steinbaum, Tomer Grassiany, and Jochen Frey.
2006. Context-based machine translation. In Pro-
ceedings of AMTA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In Proceedings of ACL/HLT, pages
157?160.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Hal Daume? and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of ACL/HLT.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of ACL/CoLing.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Thir-
teenth Conference On Computational Natural Lan-
guage Learning (CoNLL-2009), Boulder, Colorado.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In ACL 2001 Workshop
on Data-Driven Machine Translation, Toulouse,
France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web corpora.
In Proceedings of EMNLP.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the ACL/Coling.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007 Demo
and Poster Sessions.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Machine Translation Summit.
Shankar Kumar and William Byrne. 2004. Local
phrase reordering models for statistical machine
translation. In Proceedings of HLT/NAACL.
Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011. Investigations
on translation model adaptation using monolingual
data. In Proceedings of the Workshop on Statistical
Machine Translation, pages 284?293, Edinburgh,
Scotland, UK.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of
EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the
ACL/Coling.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
139
Franz Joseph Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of ACL.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of ACL.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of EACL.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL/HLT.
Vctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-
Martnez, and Juan Antonio Pe?rez-Ortiz. 2011.
Integrating shallow-transfer rules into phrase-based
statistical machine translation. In Proceedings of
the XIII Machine Translation Summit.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL.
Holger Schwenk and Jean Senellart. 2009. Transla-
tion model adaptation for an Arabic/French news
translation system by lightly-supervised training. In
MT Summit.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In Proceedings of IWSLT.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of HLT/NAACL.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Christoph Tillmann. 2003. A projection extension al-
gorithm for statistical machine translation. In Pro-
ceedings of EMNLP.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of CoLing.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
140
Proceedings of NAACL-HLT 2013, pages 518?523,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supervised Bilingual Lexicon Induction with Multiple Monolingual SignalsAnn Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch?
Computer and Information Science Dept.
University of PennsylvaniaAbstract
Prior research into learning translations from
source and target language monolingual texts
has treated the task as an unsupervised learn-
ing problem. Although many techniques take
advantage of a seed bilingual lexicon, this
work is the first to use that data for super-
vised learning to combine a diverse set of sig-
nals derived from a pair of monolingual cor-
pora into a single discriminative model. Even
in a low resource machine translation setting,
where induced translations have the potential
to improve performance substantially, it is rea-
sonable to assume access to some amount of
data to perform this kind of optimization. Our
work shows that only a few hundred transla-
tion pairs are needed to achieve strong per-
formance on the bilingual lexicon induction
task, and our approach yields an average rel-
ative gain in accuracy of nearly 50% over an
unsupervised baseline. Large gains in accu-
racy hold for all 22 languages (low and high
resource) that we investigate.1 Introduction
Bilingual lexicon induction is the task of identifying
word translation pairs using source and target mono-
lingual corpora, which are often comparable. Most
approaches to the task are based on the idea that
words that are translations of one another have sim-
ilar distributional properties across languages. Prior
research has shown that contextual similarity (Rapp,
1995), temporal similarity (Schafer and Yarowsky,
2002), and topical information (Mimno et al, 2009)
?Performed while faculty at Johns Hopkins University
are all good signals for learning translations from
monolingual texts.
Most prior work either makes use of only one or
two monolingual signals or uses unsupervised meth-
ods (like rank combination) to aggregate orthogonal
signals (Schafer and Yarowsky, 2002; Klementiev
and Roth, 2006). Surprisingly, no past research has
employed supervised approaches to combine diverse
monolingually-derived signals for bilingual lexicon
induction. The field of machine learning has shown
decisively that supervised models dramatically out-
perform unsupervised models, including for closely
related problems like statistical machine translation
(Och and Ney, 2002).
For the bilingual lexicon induction task, a super-
vised approach is natural, particularly because com-
puting contextual similarity typically requires a seed
bilingual dictionary (Rapp, 1995), and that same
dictionary may be used for estimating the param-
eters of a model to combine monolingual signals.
Alternatively, in a low resource machine transla-
tion (MT) setting, it is reasonable to assume a small
amount of parallel data from which a bilingual dic-
tionary can be extracted for supervision. In this set-
ting, bilingual lexicon induction is critical for trans-
lating source words which do not appear in the par-
allel data or dictionary.
We frame bilingual lexicon induction as a binary
classification problem; for a pair of source and tar-
get language words, we predict whether the two are
translations of one another or not. For a given source
language word, we score all target language can-
didates separately and then rerank them. We use
a variety of signals derived from source and target
518
monolingual corpora as features and use supervision
to estimate the strength of each. In this work we:
? Use the following similarity metrics derived
from monolingual corpora to score word pairs:
contextual, temporal, topical, orthographic, and
frequency.
? For the first time, explore using supervision to
combine monolingual signals and learn a dis-
criminative model for predicting translations.
? Present results for 22 low and high resource
languages paired with English and show large
accuracy gains over an unsupervised baseline.2 Previous Work
Prior work suggests that a wide variety of mono-
lingual signals, including distributional, temporal,
topic, and string similarity, may inform bilingual
lexicon induction (Rapp, 1995; Fung and Yee, 1998;
Rapp, 1999; Schafer and Yarowsky, 2002; Schafer,
2006; Klementiev and Roth, 2006; Koehn and
Knight, 2002; Haghighi et al, 2008; Mimno et
al., 2009; Mausam et al, 2010). Klementiev et al
(2012) use many of those signals to score an exist-
ing phrase table for end-to-end MT but do not learn
any new translations. Schafer and Yarowsky (2002)
use an unsupervised rank-combination method for
combining orthographic, contextual, temporal, and
frequency similarities into a single ranking.
Recently, Ravi and Knight (2011), Dou and
Knight (2012), and Nuhn et al (2012) have worked
toward learning a phrase-based translation model
from monolingual corpora, relying on decipherment
techniques. In contrast to that work, we use a
seed bilingual lexicon for supervision and multiple
monolingual signals proposed in prior work.
Haghighi et al (2008) and Daume? and Jagarla-
mudi (2011) use some supervision to learn how to
project contextual and orthographic features into a
low-dimensional space, with the goal of represent-
ing words which are translations of one another
as vectors which are close together in that space.
However, both of those approaches focus on only
two signals, high resource languages, and frequent
words (frequent nouns, in the case of Haghighi et
al. (2008)). In our classification framework, we can
incorporate any number of monolingual signals, in-
Language #Words Language #Words
Nepali 0.4 Somali 0.5
Uzbek 1.4 Azeri 2.6
Tamil 3.7 Albanian 6.5
Bengali 6.6 Welsh 7.5
Bosnian 12.9 Latvian 40.2
Indonesian 21.8 Romanian 24.1
Serbian 25.8 Turkish 31.2
Ukrainian 37.6 Hindi 47.4
Bulgarian 49.5 Polish 104.5
Slovak 124.3 Urdu 287.2
Farsi 710.3 Spanish 972
Table 1: Millions of monolingual web crawl and
Wikipedia word tokens
cluding contextual and string similarity, and directly
learn how to combine them.3 Monolingual Data and Signals3.1 Data
Throughout our experiments, we seek to learn how
to translate words in a given source language into
English. Table 1 lists our languages of interest,
along with the total amount of monolingual data
that we use for each. We use web crawled time-
stamped news articles to estimate temporal sim-
ilarity, Wikipedia pages which are inter-lingually
linked to English pages to estimate topic similarity,
and both datasets to estimate frequency and contex-
tual similarity. Following Irvine et al (2010), we
use pairs of Wikipedia page titles to train a simple
transliterator for languages written in a non-Roman
script, which allows us to compute orthographic
similarity for pairs of words in different scripts.3.2 Signals
Our definitions of orthographic, topic, temporal, and
contextual similarity are taken from Klementiev et
al. (2012), and the details of each may be found
there. Here, we give briefly describe them and give
our definition of a novel, frequency-based signal.Orthographic We measure orthographic similar-
ity between a pair of words as the normalized1 edit
distance between the two words. For non-Roman
script languages, we transliterate words into the Ro-
man script before measuring orthographic similarity.TopicWe use monolingual Wikipedia pages to es-
timate topical signatures for each source and target
1Normalized by the average of the lengths of the two words
519
language word. Signature vectors are the length of
the number of inter-lingually linked source and En-
glish Wikipedia pages and contain counts of how
many times the word appears on each page. We use
cosine similarity to compare pairs of signatures.Temporal We use time-stamped web crawl data
to estimate temporal signatures, which, for a given
word, are the length of the number of time-stamps
(dates) and contain counts of how many times the
word appears in news articles with the given date.
We use a sliding window of three days and use co-
sine similarity to compare signatures. We expect
that source and target language words which are
translations of one another will appear with similar
frequencies over time in monolingual data.Contextual We score monolingual contextual
similarity by first collecting context vectors for each
source and target language word. The context vector
for a given word contains counts of how many times
words appear in its context. We use bag of words
contexts in a window of size two. We gather both
source and target language contextual vectors from
our web crawl data and Wikipedia data (separately).Frequency Words that are translations of one an-
other are likely to have similar relative frequencies
in monolingual corpora. We measure the frequency
similarity of two words as the absolute value of the
difference between the logs of their relative mono-
lingual corpus frequencies.4 Supervised Bilingual Lexicon Induction4.1 Baseline
Our unsupervised baseline method is based on
ranked lists derived from each of the signals listed
above. For each source word, we generate ranked
lists of English candidates using the following six
signals: Crawls Context, Crawls Time, Wikipedia
Context, Wikipedia Topic, Edit distance, and Log
Frequency Difference. Then, for each English can-
didate we compute its mean reciprocal rank2 (MRR)
based on the six ranked lists. The baseline ranks En-
glish candidates according to the MRR scores. For
evaluation, we use the same test sets, accuracy met-
ric, and correct translations described below.
2The MRR of the jth English word, ej , is 1N
PN
i=1
1
rankij
,
where N is the number of signals and rankij is ej?s rank ac-
cording to signal i.
4.2 Supervised Approach
In addition to the monolingual resources described
in Section 3.1, we have a bilingual dictionary for
each language, which we use to project context vec-
tors and for supervision and evaluation. For each
language, we choose up to 8, 000 source language
words among those that occur in the monolingual
data at least three times and that have at least one
translation in our dictionary. We randomly divide
the source language words into three equally sized
sets for training, development, and testing. We use
the training data to train a classifier, the develop-
ment data to choose the best classification settings
and feature set, and the test set for evaluation.
For all experiments, we use a linear classifier
trained by stochastic gradient descent to minimize
squared error3 and perform 100 passes over the
training data.4 The binary classifiers predict whether
a pair of words are translations of one another or not.
The translations in our training data serve as posi-
tive supervision, and the source language words in
the training data paired with random English words5
serve as negative supervision. We used our develop-
ment data to tune the number of negative examples
to three for each positive example. At test time, af-
ter scoring all source language words in the test set
paired with all English words in our candidate set,6
we rank the English candidates by their classifica-
tion scores and evaluate accuracy in the top-k trans-
lations.4.3 Features
Our monolingual features are listed below and are
based on raw similarity scores as well as ranks:
? Crawls Context: Web crawl context similarity score
? Crawls Context RR: reciprocal rank of crawls con-
text
3We tried using logistic rather than linear regression, but
performance differences on our development set were very
small and not statistically significant.
4We use http://hunch.net/?vw/ version 6.1.4, and
run it with the following arguments that affect how updates are
made in learning: ?exact adaptive norm ?power t 0.5
5Among those that appear at least five times in our monolin-
gual data, consistent with our candidate set.
6All English words appearing at least five times in our
monolingual data. In practice, we further limit the set to those
that occur in the top-1000 ranked list according to at least one
of our signals.
520
??
?0.0
0.2
0.4
0.6
0.8
1.0
Acc
urac
y in 
Top?
10
CrawlContext EditDist CrawlTime WikiContext WikiTopic Is?Ident. DiffLg?Frq DiscrimAll
Figure 1: Each box-and-whisker plot summarizes per-
formance on the development set using the given fea-
ture(s) across all 22 languages. For each source word
in our development sets, we rank all English target words
according to the monolingual similarity metric(s) listed.
All but the last plot show the performance of individual
features. Discrim-All uses supervised data to train classi-
fiers for each language based on all of the features.
? Crawls Time: Web crawl temporal similarity score
? Crawls Time RR: reciprocal rank of crawls time
? Edit distance: normalized (by average length of
source and target word) edit distance
? Edit distance RR: reciprocal rank of edit distance
? Wiki Context: Wikipedia context similarity score
? Wiki Context RR: recip. rank of wiki context
? Wiki Topic: Wikipedia topic similarity score
? Wiki Topic RR: recip. rank of wiki topic
? Is-Identical: source and target words are the same
? Difference in log frequencies: Difference between
the logs of the source and target word monolingual
frequencies
? Log Freqs Diff RR: reciprocal rank of difference in
log frequencies
We train classifiers separately for each source lan-
guage, and the learned weights vary based on, for
example, corpora size and the relatedness of the
source language and English (e.g. edit distance is
informative if there are many cognates). In order to
use the trained classifiers to make top-k translation
predictions for a given source word, we rank candi-
dates by their classification scores.4.4 Feature Evaluation and Selection
After training initial classifiers, we use our develop-
ment data to choose the most informative subset of
features. Figure 1 shows the top-10 accuracy on the
development data when we use individual features
?
0.0
0.2
0.4
0.6
0.8
1.0
Acc
urac
y in
 Top
?10
WikiTopic WikiContext DiffLog?Freq EditDist. EditDist. RR CrawlContext AllFeatures
Figure 2: Performance on the development set goes up
as features are greedily added to the feature space. Mean
performance is slightly higher using this subset of six fea-
tures (second to last bar) than using all features (last bar).
to predict translations. Top-10 accuracy refers to the
percent of source language words for which a correct
English translation appears in the top-10 ranked En-
glish candidates. Each box-and-whisker plot sum-
marizes performance over the 22 languages. We
don?t display reciprocal rank features, as their per-
formance is very similar to that of the correspond-
ing raw similarity score. It?s easy to see that features
based on the Wikipedia topic signal are the most in-
formative. It is also clear that training a supervised
model to combine all of the features (the last plot)
yields performance that is dramatically higher than
using any individual feature alone.
Figure 2, from left to right, shows a greedy search
for the best subset of features among those listed
above. Again, the Wikipedia topic score is the most
informative stand-alone feature, and the Wikipedia
context score is the most informative second feature.
Adding features to the model beyond the six shown
in the figure does not yield additional performance
gains over our set of languages.4.5 Learning Curve Analysis
Figure 3 shows learning curves over the number of
positive training instances. In all cases, the number
of randomly generated negative training instances
is three times the number of positive. For all lan-
guages, performance is stable after about 300 cor-
rect translations are used for training. This shows
that our supervised method for combining signals
requires only a small training dictionary.
521
??
??
? ?
? ?
0 200 400 600 800 1000 1200
0.0
0.2
0.4
0.6
0.8
1.0
?
?
?
?
??
? ?
? ? ?
???
??
?
?
? ? ? ?
?
?
?
?
? ?
? ?
? ?
?
?
?
?
? ?
? ?
????
?
?
? ?
?
??
???
? ? ?
?
???
? ?
?
????
?
?
?
?
?
?
?
?
?
?
?
SpanishRomanianPolishBulgarianIndonesianWelshSlovakBosnianLatvianAlbanianUkrainianTurkishAzeriSerbianHindiBengaliUzbekFarsiSomaliTamilUrduNepali
Positive training data instances
Acc
urac
y in
 Top
?10
Figure 3: Learning curves over number of positive train-
ing instances, up to 1250. For some languages, 1250
positive training instances are not available. In all cases,
evaluation is on the development data and the number of
negative training instances is three times the number of
positive. For all languages, performance is fairly stable
after about 300 positive training instances.5 Results
We use a model based on the six features shown
in Figure 2 to score and rank English translation
candidates for the test set words in each language.
Table 2 gives the result for each language for the
MRR baseline and our supervised technique. Across
languages, the average top-10 accuracy using the
MRR baseline is 30.4, and the average using our
technique is 43.9, a relative improvement of about
44%. We did not attempt a comparison with more
sophisticated unsupervised rank aggregation meth-
ods. However, we believe the improvements we
observe drastically outweigh the expected perfor-
mance differences between different rank aggrega-
tion methods. Figure 4 plots the accuracies yielded
by our supervised technique versus the total amount
of monolingual data for each language. An increase
in monolingual data tends to improve accuracy. The
correlation isn?t perfect, however. For example, per-
formance on Urdu and Farsi is relatively poor, de-
spite the large amounts of monolingual data avail-
able for each. This may be due to the fact that we
have large web crawls for those languages, but their
Wikipedia datasets, which tend to provide a strong
topic signal, are relatively small.
AzeriBengali
BosnianWelsh
Hindi
Indonesian
Latvian
Nepali
Romanian
Slovak
Somali
Albanian
SerbianTamilUzbek
Farsi
Spanish
Urdu
TurkishBulgarianUkranian
Polish
Millions of Monolingual Word Tokens
Acc
urac
y
1e?01 1e+00 1e+01 1e+02 1e+03
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Millions of monolingual word tokens vs. Lex-
icon Induction Top-10 Accuracy
Lang MRR Supv. Lang MRR Supv.
Nepali 11.2 13.6 Somali 16.7 18.1
Uzbek 23.2 29.6 Azeri 16.1 29.4
Tamil 28.4 33.3 Albanian 32.0 45.3
Bengali 19.3 32.8 Welsh 36.1 56.4
Bosnian 32.6 52.8 Latvian 29.6 47.7
Indonesian 41.5 63.5 Romanian 53.3 71.6
Serbian 29.0 33.3 Turkish 31.4 52.1
Ukrainian 29.7 46.0 Hindi 18.2 34.6
Bulgarian 40.2 57.9 Polish 47.4 67.1
Slovak 34.6 53.5 Urdu 13.2 21.2
Farsi 10.5 21.1 Spanish 74.8 85.0
Table 2: Top-10 Accuracy on test set. Performance
increases for all languages moving from the baseline
(MRR) to discriminative training (Supv).6 Conclusions
On average, we observe relative gains of more than
44% over an unsupervised rank-combination base-
line by using a seed bilingual dictionary and a di-
verse set of monolingual signals to train a supervised
classifier. Using supervision for bilingual lexicon in-
duction makes sense. In some cases a dictionary is
already assumed for computing contextual similar-
ity, and, in the remaining cases, one could be com-
piled easy, either automatically, e.g. Haghighi et al
(2008), or through crowdsourcing, e.g. Irvine and
Klementiev (2010) and Callison-Burch and Dredze
(2010). We have shown that only a few hundred
translation pairs are needed to achieve good perfor-
mance. Our framework has the additional advantage
that any new monolingually-derived similarity met-
rics can easily be added as new features.
522
7 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and by
the Johns Hopkins University Human Language
Technology Center of Excellence. The views and
conclusions contained in this publication are those
of the authors and should not be interpreted as repre-
senting official policies or endorsements of DARPA
or the U.S. Government.References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the Conference of the As-
sociation for Computational Linguistics (ACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
frommonolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Ann Irvine and Alexandre Klementiev. 2010. Using me-
chanical turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas (AMTA).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch, and
David Yarowsky. 2012. Toward statistical machine
translation without parallel corpora. In Proceedings of
the Conference of the European Association for Com-
putational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,
and Jeff Bilmes. 2010. Panlingual lexical transla-
tion via probabilistic inference. Artificial Intelligence,
174:619?637, June.
DavidMimno, HannaWallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL).
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the Conference of the Associ-
ation for Computational Linguistics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using Di-
verse Similarity Measures. Ph.D. thesis, Johns Hop-
kins University.
523
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 54?61,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Statistical Machine Translation in Low Resource Settings
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Abstract
My thesis will explore ways to improve the
performance of statistical machine translation
(SMT) in low resource conditions. Specif-
ically, it aims to reduce the dependence of
modern SMT systems on expensive parallel
data. We define low resource settings as hav-
ing only small amounts of parallel data avail-
able, which is the case for many language
pairs. All current SMT models use parallel
data during training for extracting translation
rules and estimating translation probabilities.
The theme of our approach is the integration
of information from alternate data sources,
other than parallel corpora, into the statisti-
cal model. In particular, we focus on making
use of large monolingual and comparable cor-
pora. By augmenting components of the SMT
framework, we hope to extend its applicabil-
ity beyond the small handful of language pairs
with large amounts of available parallel text.
1 Introduction
Statistical machine translation (SMT) systems are
heavily dependent on parallel data. SMT doesn?t
work well when fewer than several million lines of
bitext are available (Kolachina et al, 2012). When
the available bitext is small, statistical models per-
form poorly due to the sparse word and phrase
counts that define their parameters. Figure 1 gives a
learning curve that shows this effect. As the amount
of bitext approaches zero, performance drops dras-
tically. In this thesis, we seek to modify the SMT
model to reduce its dependence on parallel data and,
thus, enable it to apply to new language pairs.
Specifically, we plan to address the following
challenges that arise when using SMT systems in
low resource conditions:
l l
l l
l
l l
l
l l l
l
l l
1e+02 1e+03 1e+04 1e+05 1e+060
5
10
15
20
25
Lines of Training Bitext
BLEU
 scor
e
Figure 1: Learning curve that shows how SMT per-
formance on the Spanish to English translation task in-
creases with increasing amounts of parallel data. Perfor-
mance is measured with BLEU and drops drastically as
the amount of bitext approaches zero. These results use
the Europarl corpus and the Moses phrase-based SMT
framework, but the trend shown is typical.
? Translating unknown words. In the context
of SMT, unknown words (or out-of-vocabulary,
OOV) are defined as having never appeared in
the source side of the training parallel corpus.
When the training corpus is small, the percent
of words which are unknown can be high.
? Inducing phrase translations. In high re-
source conditions, a word aligned bitext is used
to extract a list of phrase pairs or translation
rules which are used to translate new sentences.
With more parallel data, this list is increasingly
comprehensive. Using multi-word phrases in-
stead of individual words as the basic transla-
tion unit has been shown to increase translation
performance (Koehn et al, 2003). However,
when the parallel corpus is small, so is the num-
ber of phrase pairs that can be extracted.
? Estimating translation probabilities. In the
standard SMT pipeline, translation probabil-
ities are estimated using relative frequency
counts over the training bitext. However, when
the bitext counts are sparse, probability esti-
54
Language #Words Language #Words
Nepali 0.4 Somali 0.5
Uzbek 1.4 Azeri 2.6
Tamil 3.7 Albanian 6.5
Bengali 6.6 Welsh 7.5
Bosnian 12.9 Latvian 40.2
Indonesian 21.8 Romanian 24.1
Serbian 25.8 Turkish 31.2
Ukrainian 37.6 Hindi 47.4
Bulgarian 49.5 Polish 104.5
Slovak 124.3 Urdu 287.2
Farsi 710.3 Spanish 972
Table 1: Millions of monolingual web crawl and
Wikipedia word tokens
mates are likely to be noisy.
My thesis focuses on translating into English. We
assume access to a small amount of parallel data,
which is realistic, especially considering the recent
success of crowdsourcing translations (Zaidan and
Callison-Burch, 2011; Ambati, 2011; Post et al,
2012). Additionally, we assume access to larger
monolingual corpora. Table 1 lists the 22 languages
for which we plan to perform translation experi-
ments, along with the total amount of monolingual
data that we will use for each. We use web crawled
time-stamped news articles and Wikipedia for each
language. We have extracted the Wikipedia pages
which are inter-lingually linked to English pages.
2 Translating Unknown Words
OOV words are a major challenge in low resource
SMT settings. Here, we describe several approaches
to identifying translations for unknown words.
2.1 Transliteration
For non-roman script languages, in some cases,
OOV words may be transliterated rather than trans-
lated. This is often true for named entities,
where transliterated words are pronounced approxi-
mately the same across languages but have different
spellings in the source and target language alphabets
(e.g. Russian Anna translates as English Anna). In
the case of roman script languages, of course, such
words are often translated correctly without change
(e.g. French Anna translates as English Anna).
In my prior work, Irvine et al (2010a) and
Irvine et al (2010b), I have presented a language-
independent approach to gathering pairs of translit-
erated words (specifically, names) in a pair of lan-
guages, built a module to transliterate from one lan-
guage to the other, and integrated the output into an
end-to-end SMT system. In my thesis, I will use
this technique to hypothesize translations for OOV
words. Additionally, I plan to include techniques
that build upon the one described in Hermjakob et
al. (2008) in order to predict when words are likely
to be transliterated rather than translated. That work
uses features based on an Arabic named entity tag-
ger. In our low resource setting, we cannot assume
access to such off-the-shelf tools and must adapt this
existing technique accordingly.
2.2 Bilingual Lexicon Induction
Bilingual lexicon induction is the task of identify-
ing word translation pairs in source and target lan-
guage monolingual or comparable corpora. The task
is well-researched, however, in prior work, Irvine
and Callison-Burch (2013), we were the first to pro-
pose using supervised methods. Because we assume
access to some small amount of parallel data, we can
extract a bilingual dictionary from it to use for posi-
tive supervision. In my prior work and in the thesis,
we use the following signals estimated over com-
parable source and target language corpora: ortho-
graphic, topic, temporal, and contextual similarity.
Here, we give brief descriptions of each.
Orthographic We measure orthographic similar-
ity between a pair of words as the normalized1 edit
distance between the two words. For non-Roman
script languages, we transliterate words into the Ro-
man script before measuring orthographic similarity.
Topic We use monolingual Wikipedia pages to es-
timate topical signatures for each source and target
language word. Signatures contain counts of how
many times a given word appears on each interlin-
gually linked Wikipedia page, and we use cosine
similarity to compare pairs of signatures.
Temporal We use time-stamped web crawl data
to estimate temporal signatures, which, for a given
word, contain counts of how many times that word
appeared in news articles with a certain date. We ex-
pect that source and target language words which are
translations of one another will appear with similar
frequencies over time in monolingual data.
1Normalized by the average of the lengths of the two words
55
Contextual We score monolingual contextual
similarity by first collecting context vectors for each
source and target language word. The context vector
for a given word contain counts of how many times
words appear in its context. We use bag of words
contexts in a window of size two. We gather both
source and target language contextual vectors from
our web crawl data and Wikipedia data (separately).
Frequency Words that are translations of one an-
other are likely to have similar relative frequencies
in monolingual corpora. We measure the frequency
similarity of two words as the absolute value of the
difference between the log of their relative monolin-
gual corpus frequencies.
We propose using a supervised approach to learn-
ing how to combine the above signals into a sin-
gle discriminative binary classifier which predicts
whether a source and target language word are trans-
lations of one another or not. Given a classification
score for each source language word paired with all
English candidates, we rerank candidates and evalu-
ate on the top-k. We give some preliminary experi-
mental details and results here.
We have access to bilingual dictionaries for the 22
languages listed in Table 12. For each language, we
choose up to 8, 000 source language words among
those that occur in the monolingual data at least
three times and that have at least one translation in
our dictionary. We randomly divide the source lan-
guage words into three equally sized sets for train-
ing, development, and testing. We use the train-
ing data to train a classifier, the development data
to choose the best classification settings and feature
set, and the test set for evaluation.
For all experiments, we use a linear classifier
trained by stochastic gradient descent to minimize
squared error3 and perform 100 passes over the
training data.4 The binary classifiers predict whether
a pair of words are translations of one another or not.
The translations in our training data serve as posi-
tive supervision, and the source language words in
2Details about the dictionaries in work under review.
3We tried using logistic rather than linear regression, but
performance differences on our development set were very
small and not statistically significant.
4We use http://hunch.net/~vw/ version 6.1.4, and
run it with the following arguments that affect how updates are
made in learning: ?exact adaptive norm ?power t 0.5
l
0.0
0.2
0.4
0.6
0.8
1.0
Accu
racy
 in T
op?1
0
WikiTopic WikiContext DiffLog?Freq EditDist. EditDist. RR CrawlContext AllFeatures
Figure 2: Performance goes up as features are greedily
added to the feature space. Mean performance is slightly
higher using this subset of six features (second to last bar)
than using all features (last bar). Each plot represents
results over our 22 languages.
the training data paired with random English words5
serve as negative supervision. We used our develop-
ment data to tune the number of negative examples
to three for each positive example. At test time, af-
ter scoring all source language words in the test set
paired with all English words in our candidate set,6
we rank the English candidates by their classifica-
tion scores and evaluate accuracy in the top-k.
We use raw similarity scores based on the signals
enumerated above as features. Additionally, for each
source word, we rank all English candidates with
respect to each signal and include their reciprocal
ranks as another set of features. Finally, we include
a binary feature that indicates if a given source and
target word are identical strings or not.
We train classifiers separately for each of the 22
languages listed in Table 1, and the learned weights
vary based on, for example, corpora size and the re-
latedness of the source language and English (e.g.
edit distance is informative if there are many cog-
nates). When we use the trained classifier to pre-
dict which English words are translations of a given
source word, all English words appearing at least
five times in our monolingual data are candidates,
and we rank them by their classification scores.
Figure 2, from left to right, shows a greedy search
5Among those that appear at least five times in our monolin-
gual data, consistent with our candidate set.
6All English words appearing at least five times in our
monolingual data. In practice, we further limit the set to those
that occur in the top-1000 ranked list according to at least one
of our signals.
56
Lang MRR Supv. Lang MRR Supv.
Nepali 11.2 13.6 Somali 16.7 18.1
Uzbek 23.2 29.6 Azeri 16.1 29.4
Tamil 28.4 33.3 Albanian 32.0 45.3
Bengali 19.3 32.8 Welsh 36.1 56.4
Bosnian 32.6 52.8 Latvian 29.6 47.7
Indonesian 41.5 63.5 Romanian 53.3 71.6
Serbian 29.0 33.3 Turkish 31.4 52.1
Ukrainian 29.7 46.0 Hindi 18.2 34.6
Bulgarian 40.2 57.9 Polish 47.4 67.1
Slovak 34.6 53.5 Urdu 13.2 21.2
Farsi 10.5 21.1 Spanish 74.8 85.0
Table 2: Top-10 Accuracy on test set. Performance
increases for all languages moving from the baseline
(MRR) to discriminative training (Supv).
for the best subset of features. The Wikipedia topic
score is the most informative stand-alone feature,
and Wikipedia context is the most informative sec-
ond feature. Adding features to the model beyond
the six shown in the figure does not yield additional
performance gains over our set of languages.
We use a model based on the six features shown in
Figure 2 to score and rank English translation candi-
dates for the test set words in each language.
Our unsupervised baseline method is based on
ranked lists derived from each of the signals listed
above. For each source word, we generate ranked
lists of English candidates using the following six
signals: Crawls Context, Crawls Time, Wikipedia
Context, Wikipedia Topic, Edit distance, and Log
Frequency Difference. Then, for each English can-
didate we compute its mean reciprocal rank7 (MRR)
based on the six ranked lists. The baseline ranks En-
glish candidates according to the MRR scores. For
evaluation, we use the same test sets, accuracy met-
ric, and correct translations.
Table 2 gives results for the baseline and our su-
pervised technique. Across languages, the average
top-10 accuracy using the baseline is 30.4, and us-
ing our technique it is 43.9, about 44% higher.
In Section 3 we use the same features to score all
phrase pairs in a phrase-based MT model and in-
clude them as features in tuning and decoding.
7The MRR of the jth English word, ej , is 1N
?N
i=1
1
rankij ,where N is the number of signals and rankij is ej?s rank ac-
cording to signal i.
2.3 Distributed Representations
Our third method for inducing OOV translations em-
ploys a similar intuition to that of contextual simi-
larity. However, unlike standard contextual vectors
that represent words as large vectors of counts of
nearby words, we propose to use distributed rep-
resentations. These word representations are low-
dimensional and are induced iteratively using the
distributed representations of nearby words, not the
nearby words themselves. Using distributed repre-
sentations helps to alleviate data sparsity problems.
Recently, Klementiev et al (2012b) induced dis-
tributed representations for the crosslingual setting.
There, the induced embedding is learned jointly over
multiple languages so that the representations of se-
mantically similar words end up ?close? to one an-
other irrespective of language. They simultaneously
use large monolingual corpora to induce represen-
tations for words in each language and use parallel
data to bring the representations together across lan-
guages. The intuition for their approach to crosslin-
gual representation induction comes from the multi-
task learning setup of Cavallanti et al (2010). They
apply this set-up to a variant of a neural probabilistic
language model (Bengio et al, 2003).
In my thesis, I propose to use the distributed rep-
resentations proposed by Klementiev et al (2012b)
in order to induce translations for OOV words. Ad-
ditionally, I plan to learn how to compose the rep-
resentations of individual words in a phrase into a
single representation, allowing for the induction of
phrase translations in addition to single words.
3 Inducing and Scoring a Phrase Table
Although by extracting OOV word translations we
may increase the coverage of our SMT model,
inducing phrase translations may increase perfor-
mance further. In order to do so, we need to be able
to score pairs of phrases to determine which have
high translation probabilities. Furthermore, using al-
ternate sources of data to score phrase pairs directly
extracted from a small bitext may help distinguish
good translation pairs from bad ones, which could
result from incorrect word alignments, for example.
In moving from words to phrases, we make use of
many of the same techniques described in Section 2.
Here, I present several proposals for addressing the
57
major additional challenges that arise for phrases,
and Section 4 presents some experimental results.
3.1 Phrase translation induction
The difficulty in inducing a comprehensive set of
phrase translations is that the number of phrases, on
both the source and target side, is very large. In
moving from the induction of word translations to
phrase translations, the number of comparisons nec-
essary to do an exhaustive search becomes infeasi-
ble. I propose to explore several ways to speed up
that search in my thesis:
? Use distributed phrase representations.
? Use filters to limit the phrase pair search space.
Filters should be fast and could include in-
formation such as word translations, phrase
lengths, and monolingual frequencies.
? Predict when phrases should be translated as a
unit, rather than compositionally. If it is pos-
sible to accurately translate a phrase composi-
tionally from its word translations, then there is
no need to induce a translation for the phrase.
3.2 Phrase translation scoring
In our prior work, Klementiev et al (2012a), we
have started to explore scoring a phrase table us-
ing comparable corpora. Given a set of phrase pairs,
either induced or extracted from a small bitext, the
idea is to score them using the same signals derived
from comparable corpora described in the context of
bilingual lexicon induction in Section 2.2. No matter
the source of the phrase pairs, the hope is that such
scores will help an SMT model distinguish between
good and bad translations. We estimate both phrasal
and lexical similarity features over phrase pairs. We
estimate the first using contextual, temporal, and
topical signatures over entire phrases. We estimate
the latter by using the lexical contextual, temporal,
topical, and orthographic signatures of each word
in each phrase. We use phrasal word alignments
in order to compute the lexical similarity between
phrases. That is, we compute each similarity met-
ric for each pair of aligned words and then, for each
similarity metric, average over the word pairs. This
approach is analogous to the lexical weighting fea-
ture introduced by Koehn et al (2003).
Language Train Dev OOV Dev OOVWords Word Types Word Tokens
Tamil 452k 44% 25%
Bengali 272k 37% 18%
Hindi 708k 34% 11%
Table 3: Information about datasets released by Post et
al. (2012). Training data gives the number of words in the
source language training set. OOV rates give the percent
of development set word types and work tokens that do
not appear in the training data.
4 Preliminary Results
Here we show preliminary results using our methods
for translating OOV words and our methods for scor-
ing a phrase table in end-to-end low resource ma-
chine translation. Post et al (2012) used Amazon?s
Mechanical Turk to collect a small parallel corpus
for several Indian languages. In our experiments, we
use their Tamil, Bengali, and Hindi datasets. We use
the data splits given by Post et al (2012) and, fol-
lowing that work, report results on the devtest set.
Table 3 shows statistics about the datasets.
In our experiments, we use the Moses phrase-
based machine translation framework (Koehn et al,
2007). For each language, we extract a phrase ta-
ble from the training data with a phrase limit of
seven and, like Post et al (2012), use the English
side of the training data to train a language model.
Throughout our experiments, we use MIRA (Chiang
et al, 2009) for tuning the feature set.
Our experiments compare the following:
? A baseline phrase-based model, using phrase
pairs extracted from the training data and the
standard phrasal and lexical translation proba-
bilities based on the bitext.
? Baseline supplemented with word translations
induced by our baseline unsupervised bilingual
lexicon induction method (Section 2.2)
? Baseline supplemented with word translations
induced by our supervised bilingual lexicon in-
duction methods (Section 2.2).
? Baseline model supplemented with additional
features, estimated over comparable corpora
(Section 3.2).
? Baseline model supplemented with induced
word translations and also additional features.
Table 4 shows our results. Adding additional
phrase table features increased BLEU scores from
58
Tamil Bengali Hindi
Experiment K BLEU Diff. BLEU Diff. BLEU Diff.
Baseline 9.16 12.14 14.85
+ Mono. Features 9.70 +0.54 12.54 +0.40 15.16 +0.31
+ Unsupervised Word Translations 1 9.33 +0.17 12.11 -0.03 15.37 +0.52
+ Supervised Word Translations 1 9.76 +0.60 12.38 +0.24 15.64 +0.79
+ Mono. Feats. & Sup. Trans. 1 10.20 +1.04 13.01 +0.87 15.84 +0.99
+ Mono. Feats. & Sup. Trans. 5 10.41 +1.25 12.64 +0.50 16.02 +1.17
+ Mono. Feats. & Sup. Trans. 10 10.12 +0.96 12.57 +0.43 15.86 +1.01
Table 4: BLEU performance gains that target coverage and accuracy separately and together. We add the top-K
ranked translations for each OOV source word.
0.31 BLEU points for Hindi to 0.54 for Tamil.
Next, we monolingually induced translations for
all development and test set source words. We
experimented with adding translations for source
words with low training data frequencies in addition
to OOV words but did not observe BLEU improve-
ments beyond what was gained by translating OOVs
alone. Our BLEU score gains that result from im-
proving OOV coverage, +Supervised Word Transla-
tions, range from 0.24 for Bengali to 0.79 for Hindi
and outperform the unsupervised lexicon induction
baseline for all three languages.
Using comparable corpora to supplement both the
feature space and the coverage of OOVs results in
translations that are better than applying either tech-
nique alone. For all languages, the BLEU improve-
ments are approximately additive. For Tamil, the to-
tal BLEU point gain is 1.25, and it is 1.17 for Hindi
and 0.87 for Bengali. Table 4 shows results as we
add the top-k ranked translation for each OOV word
and vary k. For Tamil and Hindi, we get a slight
boost by adding the top-5 translations instead of the
single best but get no further gains with the top-10.
5 Previous Work
Prior work on bilingual lexicon induction has shown
that a variety of signals derived from monolingual
data, including distributional, temporal, topic, and
string similarity, are informative (Rapp, 1995; Fung
and Yee, 1998; Koehn and Knight, 2002; Schafer
and Yarowsky, 2002; Monz and Dorr, 2005; Huang
et al, 2005; Schafer, 2006; Klementiev and Roth,
2006; Haghighi et al, 2008; Mimno et al, 2009;
Mausam et al, 2010; Daum? and Jagarlamudi,
2011). This thesis builds upon this work and uses
a diverse set of signals for translating full sentences,
not just words. Recently, Ravi and Knight (2011),
Dou and Knight (2012), and Nuhn et al (2012) have
worked toward learning a phrase-based translation
model from monolingual corpora, relying on deci-
pherment techniques. In contrast to that research
thread, we make the realistic assumption that a small
parallel corpus is available for our low resource lan-
guages. With a small parallel corpus, we are able to
take advantage of supervised techniques, changing
the problem setting dramatically.
Since the early 2000s, the AVENUE (Carbonell
et al, 2002; Probst et al, 2002; Lavie et al, 2003)
project has researched ways to rapidly develop MT
systems for low-resource languages. In contrast
to that work, my thesis will focus on a language-
independent approach as well as integrating tech-
niques into current state-of-the-art SMT frame-
works. In her thesis, Gangadharaiah (2011) tack-
les several data sparsity issues within the example-
based machine translation (EBMT) framework. Her
work attempts to tackle some of the same data spar-
sity issues that we do including, in particular, phrase
table coverage. However, our models for doing so
are quite different and focus much more on the use
of a variety of new non-parallel data resources.
Other approaches to low resource machine trans-
lation include extracting parallel sentences from
comparable corpora (e.g. Smith et al (2010)) and
translation crowdsourcing. Our efforts are orthogo-
nal and complementary to these.
6 Conclusion
My thesis will explore using alternative data
sources, other than parallel text, to inform statisti-
cal machine translation models. In particular, I will
build upon a long thread of research on bilingual lex-
icon induction from comparable corpora. The result
of my thesis will be broadening the applicability of
current SMT frameworks to language pairs and do-
mains for which parallel data is limited.
59
7 Acknowledgements
The research presented in this paper was done in col-
laboration with my advisor, Chris Callison-Burch.
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and by
the Johns Hopkins University Human Language
Technology Center of Excellence. The views and
conclusions contained in this publication are those
of the authors and should not be interpreted as repre-
senting official policies or endorsements of DARPA
or the U.S. Government.
References
Vamshi Ambati. 2011. Active Learning for Machine
Translation in Scarce Data Scenarios. Ph.D. thesis,
Carnegie Mellon University.
Yoshua Bengio, R?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research
(JMLR), 3:1137?1155.
Jaime G. Carbonell, Katharina Probst, Erik Peterson,
Christian Monson, Alon Lavie, Ralf D. Brown, and
Lori S. Levin. 2002. Automatic rule learning for
resource-limited mt. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americas (AMTA).
Giovanni Cavallanti, Nicol? Cesa-bianchi, and Claudio
Gentile. 2010. Linear algorithms for online multitask
classification. Journal of Machine Learning Research
(JMLR), 11:2901?2934.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL).
Hal Daum?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the Conference of the As-
sociation for Computational Linguistics (ACL).
Rashmi Gangadharaiah. 2011. Coping with Data-
sparsity in Example-based Machine Translation.
Ph.D. thesis, Carnegie Mellon University.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Ulf Hermjakob, Kevin Knight, and Hal DaumO? Iii.
2008. Name translation in statistical machine transla-
tion learning when to transliterate. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Fei Huang, Ying Zhang, and Stephan Vogel. 2005. Min-
ing key phrase translations from web corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Ann Irvine and Chris Callison-Burch. 2013. Supervised
bilingual lexicon induction with multiple monolingual
signals. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010a. Transliterating from all languages.
In Proceedings of the Conference of the Association
for Machine Translation in the Americas (AMTA).
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton, and
Chris Callison-Burch. 2010b. Integrating output from
specialized modules in machine translation: translit-
erations in joshua. Prague Bulletin of Mathematical
Linguistics, pages 107?116.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch, and
David Yarowsky. 2012a. Toward statistical machine
translation without parallel corpora. In Proceedings of
the Conference of the European Association for Com-
putational Linguistics (EACL).
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012b. Inducing crosslingual distributed representa-
tions of words. In Proceedings of the International
Conference on Computational Linguistics (COLING).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
60
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Prasanth Kolachina, Nicola Cancedda, Marc Dymetman,
and Sriram Venkatapathy. 2012. Prediction of learn-
ing curves in machine translation. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Alon Lavie, Stephan Vogel, Lori Levin, Erik Peterson,
Katharina Probst, Ariadna Font, Rachel Reynolds,
Jaime Carbonelle, and Richard Cohen. 2003. Experi-
ments with a Hindi-to-English transfer-based MT sys-
tem under a miserly data scenario. ACM Transactions
on Asian Language Information Processing (TALIP),
2.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,
and Jeff Bilmes. 2010. Panlingual lexical transla-
tion via probabilistic inference. Artificial Intelligence,
174:619?637, June.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Christof Monz and Bonnie J. Dorr. 2005. Iterative trans-
lation disambiguation for cross-language information
retrieval. In Proceedings of the Conference on Re-
search and Developments in Information Retrieval (SI-
GIR).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL).
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Workshop on Statistical Machine Translation (WMT).
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,
and Jaime Carbonell. 2002. MT for minority lan-
guages using elicitation-based learning of syntactic
transfer rules. Machine Translation, 17:245?270, De-
cember.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the Conference of the Associ-
ation for Computational Linguistics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using Di-
verse Similarity Measures. Ph.D. thesis, Johns Hop-
kins University.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
61
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435?1445,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SenseSpotting: Never let your parallel data tie you to an old domain
Marine Carpuat1, Hal Daume? III2, Katharine Henry3,
Ann Irvine4, Jagadeesh Jagarlamudi5, Rachel Rudinger6
1 National Research Council Canada, marine.carpuat@nrc.gc.ca
2 CLIP, University of Maryland, me@hal3.name
3 CS, University of Chicago, kehenry@uchicago.edu
4 CLSP, Johns Hopkins University, anni@jhu.edu
5 IBM T.J. Watson Research Center, jags@us.ibm.com
6 CLSP, Johns Hopkins University, rachel.rudinger@aya.yale.edu
Abstract
Words often gain new senses in new do-
mains. Being able to automatically iden-
tify, from a corpus of monolingual text,
which word tokens are being used in a pre-
viously unseen sense has applications to
machine translation and other tasks sensi-
tive to lexical semantics. We define a task,
SENSESPOTTING, in which we build sys-
tems to spot tokens that have new senses
in new domain text. Instead of difficult
and expensive annotation, we build a gold-
standard by leveraging cheaply available
parallel corpora, targeting our approach to
the problem of domain adaptation for ma-
chine translation. Our system is able to
achieve F-measures of as much as 80%,
when applied to word types it has never
seen before. Our approach is based on
a large set of novel features that capture
varied aspects of how words change when
used in new domains.
1 Introduction
As Magnini et al (2002) observed, the domain of
the text that a word occurs in is a useful signal for
performing word sense disambiguation (e.g. in a
text about finance, bank is likely to refer to a finan-
cial institution while in a text about geography, it
is likely to refer to a river bank). However, in the
classic WSD task, ambiguous word types and a set
of possible senses are known in advance. In this
work, we focus on the setting where we observe
texts in two different domains and want to iden-
tify words in the second text that have a sense that
did not appear in the first text, without any lexical
knowledge in the new domain.
To illustrate the task, consider the French noun
rapport. In the parliament domain, this means
e?tat rapport re?gime
Govt. geo. state report (political) regime
Medical state (mind) report dietgeo. state ratio (political) regime
Science geo. state ratio (political) regimereport diet
Movies geo. state report (political) regimediet
Table 1: Examples of French words and their most
frequent senses (translations) in four domains.
(and is translated as) ?report.? However, in mov-
ing to a medical or scientific domain, the word
gains a new sense: ?ratio?, which simply does not
exist in the parliament domain. In a science do-
main, the ?report? sense exists, but it is dominated
about 12:1 by ?ratio.? In a medical domain, the
?report? sense remains dominant (about 2:1), but
the new ?ratio? sense appears frequently.
In this paper we define a new task that we call
SENSESPOTTING. The goal of this task is to iden-
tify words in a new domain monolingual text that
appeared in old domain text but which have a
new, previously unseen sense1. We operate un-
der the framework of phrase sense disambiguation
(Carpuat and Wu, 2007), in which we take au-
tomatically align parallel data in an old domain
to generate an initial old-domain sense inventory.
This sense inventory provides the set of ?known?
word senses in the form of phrasal translations.
Concrete examples are shown in Table 1. One of
our key contributions is the development of a rich
set of features based on monolingual text that are
indicative of new word senses.
This work is driven by an application need.
When machine translation (MT) systems are ap-
plied in a new domain, many errors are a result
of: (1) previously unseen (OOV) source language
words, or (2) source language words that appear
with a new sense and which require new transla-
1All features, code, data and raw results are at: github.
com/hal3/IntrinsicPSDEvaluation
1435
tions2 (Carpuat et al, 2012). Given monolingual
text in a new domain, OOVs are easy to identify,
and their translations can be acquired using dictio-
nary extraction techniques (Rapp, 1995; Fung and
Yee, 1998; Schafer and Yarowsky, 2002; Schafer,
2006; Haghighi et al, 2008; Mausam et al, 2010;
Daume? III and Jagarlamudi, 2011), or active learn-
ing (Bloodgood and Callison-Burch, 2010). How-
ever, previously seen (even frequent) words which
require new translations are harder to spot.
Because our motivation is translation, one sig-
nificant point of departure between our work and
prior related work (?3) is that we focus on word
tokens. That is, we are not interested only in the
question of ?has this known word (type) gained
a new sense??, but the much more specific ques-
tion of ?is this particular (token) occurrence of this
known word being used in a new sense?? Note
that for both the dictionary mining setting and the
active learning setting, it is important to consider
words in context when acquiring their translations.
2 Task Definition
Our task is defined by two data components. De-
tails about their creation are in ?5. First, we need
an old-domain sense dictionary, extracted from
French-English parallel text (in our case, parlia-
mentary proceedings). Next, we need new-domain
monolingual French text (we use medical text, sci-
entific text and movie subtitle text). Given these
two inputs, our challenge is to find tokens in the
new-domain text that are being used in a new sense
(w.r.t. the old-domain dictionary).
We assume that we have access to a small
amount of new domain parallel ?tuning data.?
From this data, we can extract a small new do-
main dictionary (?5). By comparing this new do-
main dictionary to the old domain dictionary, we
can identify which words have gained new senses.
In this way, we turn the SENSESPOTTING problem
into a supervised binary classification problem: an
example is a French word in context (in the new
domain monolingual text) and its label is positive
when it is being used in a sense that did not ex-
ist in the old domain dictionary. In this task, the
classifier is always making predictions on words
2Sense shifts do not always demand new translations;
some ambiguities are preserved across languages. E.g.,
fene?tre can refer to a window of a building or on a moni-
tor, but translates as ?window? either way. Our experiments
use bilingual data with an eye towards improving MT perfor-
mance: we focus on words that demand new translations.
outside this tuning data on word types it has never
seen before! From an applied perspective, the as-
sumption of a small amount of parallel data in the
new domain is reasonable: if we want an MT sys-
tem for a new domain, we will likely have some
data for system tuning and evaluation.
3 Related Work
While word senses have been studied extensively
in lexical semantics, research has focused on word
sense disambiguation, the task of disambiguating
words in context given a predefined sense inven-
tory (e.g., Agirre and Edmonds (2006)), and word
sense induction, the task of learning sense inven-
tories from text (e.g., Agirre and Soroa (2007)). In
contrast, detecting novel senses has not received as
much attention, and is typically addressed within
word sense induction, rather than as a distinct
SENSESPOTTING task. Novel sense detection
has been mostly motivated by the study of lan-
guage change over time. Most approaches model
changes in co-occurrence patterns for word types
when moving between corpora of old and modern
language (Sagi et al, 2009; Cook and Stevenson,
2010; Gulordava and Baroni, 2011).
Since these type-based models do not capture
polysemy in the new language, there have been a
few attempts at detecting new senses at the token-
level as in SENSESPOTTING. Lau et al (2012)
leverage a common framework to address sense
induction and disambiguation based on topic mod-
els (Blei et al, 2003). Sense induction is framed
as learning topic distributions for a word type,
while disambiguation consists of assigning topics
to word tokens. This model can interestingly be
used to detect newly coined senses, which might
co-exist with old senses in recent language. Bam-
man and Crane (2011) use parallel Latin-English
data to learn to disambiguate Latin words into En-
glish senses. New English translations are used as
evidence that Latin words have shifted sense. In
contrast, the SENSESPOTTING task consists of de-
tecting when senses are unknown in parallel data.
Such novel sense induction methods require
manually annotated datasets for the purpose of
evaluation. This is an expensive process and there-
fore evaluation is typically conducted on a very
small scale. In contrast, our SENSESPOTTING task
leverages automatically word-aligned parallel cor-
pora as a source of annotation for supervision dur-
ing training and evaluation.
1436
The impact of domain on novel senses has also
received some attention. Most approaches oper-
ate at the type-level, thus capturing changes in the
most frequent sense of a word when shifting do-
mains (McCarthy et al, 2004; McCarthy et al,
2007; Erk, 2006; Chan and Ng, 2007). Chan and
Ng (2007) notably show that detecting changes in
predominant sense as modeled by domain sense
priors can improve sense disambiguation, even af-
ter performing adaptation using active learning.
Finally, SENSESPOTTING has not been ad-
dressed directly in MT. There has been much inter-
est in translation mining from parallel or compara-
ble corpora for unknown words, where it is easy to
identify which words need translations. In con-
trast, SENSESPOTTING detects when words have
new senses and, thus, frequently a new translation.
Work on active learning for machine translation
has focused on collecting translations for longer
unknown segments (e.g., Bloodgood and Callison-
Burch (2010)). There has been some interest in
detecting which phrases that are hard to translate
for a given system (Mohit and Hwa, 2007), but dif-
ficulties can arise for many reasons: SENSESPOT-
TING focuses on a single problem.
4 New Sense Indicators
We define features over both word types and word
tokens. In our classification setting, each instance
consists of a French word token in context. Our
word type features ignore this context and rely on
statistics computed over our entire new domain
corpus. In contrast, our word token features con-
sider the context of the particular instance of the
word. If it were the case that only one sense ex-
isted for all word tokens of a particular type within
a single domain, we would expect our word type
features to be able to spot new senses without the
help of the word token features. However, in fact,
even within a single domain, we find that often a
word type is used with several senses, suggesting
that word token features may also be useful.
4.1 Type-level Features
Lexical Item Frequency Features A very ba-
sic property of the new domain that we hope to
capture is that word frequencies change, and such
changes might be indicative of a domain shift. As
such, we compute unigram log probabilities (via
smoothed relative frequencies) of each word un-
der consideration in the old domain and the new
domain. We then add as features these two log
probabilities as well as their difference. These are
our Type:RelFreq features.
N-gram Probability Features The goal of the
Type:NgramProb feature is to capture the fact
that ?unusual contexts? might imply new senses.
To capture this, we can look at the log probability
of the word under consideration given its N-gram
context, both according to an old-domain language
model (call this `oldng ) and a new-domain language
model (call this `newng ). However, we do not sim-
ply want to capture unusual words, but words that
are unlikely in context, so we also need to look at
the respective unigram log probabilities: `oldug and
`newug . From these four values, we compute corpus-
level (and therefore type-based) statistics of the
new domain n-gram log probability (`newng , the dif-
ference between the n-gram probabilities in each
domain (`newng ? `oldng ), the difference between the
n-gram and unigram probabilities in the new do-
main (`newng ? `newug ), and finally the combined differ-
ence: `newng ? `newug + `oldug ? `oldng ). For each of these
four values, we compute the following type-based
statistics over the monolingual text: mean, stan-
dard deviation, minimum value, maximum value
and sum. We use trigram models.
Topic Model Feature The intuition behind the
topic model feature is that if a word?s distribu-
tion over topics changes when moving into a new
domain, it is likely to also gain a new sense.
For example, suppose that in our old domain, the
French word enceinte is only used with the sense
?wall,? but in our new domain, enceinte may have
senses corresponding to either ?wall? or to ?preg-
nant.? We would expect to see this reflected in
enceinte?s distribution over topics: the topic that
places relatively high probabilities on words such
as ?be?be?? (English ?baby?) and enfant (English
?child?) will also place a high probability on en-
ceinte when trained on new domain data. In the
old domain, however, we would not expect a sim-
ilar topic (if it exists) to give a high probabil-
ity to enceinte. Based on this intuition, for all
words w, where To and Tn are the set of old
and new topics and Po and Pn are the old and
new distributions defined over them, respectively,
and cos is the cosine similarity between a pair
of topics, we define the feature Type:TopicSim:?
t?Tn,t??To Pn(t|w)Po(t?|w) cos(t, t?). For aword w, the feature value will be high if, for
each new domain topic t that places high proba-
bility on w, there is an old domain topic t? that
1437
is similar to t and also places a high probabil-
ity on w. Conversely, if no such topic exists, the
score will be low, indicating the word has gained
a new sense. We use the online LDA (Blei et
al., 2003; Hoffman et al, 2010), implemented
in http://hunch.net/?vw/ to compute topics on
the two domains separately. We use 100 topics.
Context Feature It is expected that words acquir-
ing new senses will tend to neighbor different sets
of words (e.g. different arguments, prepositions,
parts of speech, etc.). Thus, we define an addi-
tional type level feature to be the ratio of the num-
ber of new domain n-grams (up to length three)
that contain word w and which do not appear in
the old domain to the total number of new domain
n-grams containing w. With Nw indicating the set
of n-grams in the new domain which contain w,
Ow indicating the set of n-grams in the old domain
which contain w, and |Nw ? Ow| indicating the
n-grams which contain w and appear in the new
but not the old domain, we define Type:Contextas
|Nw?Ow|
|Nw| . We do not count n-grams containingOOVs, as they may simply be instances of apply-
ing the same sense of a word to a new argument
4.2 Token-level Features
N-gram Probability Features Akin to the N-
gram probability features at the type level (namely,
Token:NgramProb), we compute the same val-
ues at the token level (new/old domain and un-
igram/trigram). Instead of computing statistics
over the entire monolingual corpus, we use the in-
stantaneous values of these features for the token
under consideration. The six features we construct
are: unigram (and trigram) log probabilities in the
old domain, the new domain, and their difference.
Context Features Following the type-level n-
gram feature, we define features for a particular
word token based on its n-gram context. For token
wi, in position i in a given sentence, we consider
its context words in a five word window: wi?2,
wi?1, wi+1, and wi+2. For each of the four con-
textual words in positions p = {?2,?1, 1, 2},
relative to i, we define the following feature, To-
ken:CtxCnt: log(cwp) where cwp is the number
of times word wp appeared in position p relative
to wi in the OLD-domain data. We also define a
single feature which is the percent of the four con-
textual words which had been seen in the OLD-
domain data, Token:Ctx%.
Token-Level PSD Features These features aim
to capture generalized characteristics of a context.
Towards this end, first, we pose the problem as a
phrase sense disambiguation (PSD) problem over
the known sense inventory. Given a source word in
a context, we train a classifier to predict the most
likely target translation. The ground truth labels
(target translation for a given source word) for this
classifier are generated from the phrase table of
the old domain data. We use the same set of fea-
tures as in Carpuat and Wu (2007). Second, given
a source word s, we use this classifier to com-
pute the probability distribution of target transla-
tions (p(t|s)). Subsequently, we use this prob-
ability distribution to define new features for the
SENSESPOTTING task. The idea is that, if a word
is used in one of the known senses then its con-
text must have been seen previously and hence we
hope that the PSD classifier outputs a spiky dis-
tribution. On the other hand, if the word takes a
new sense then hopefully it is used in an unseen
context resulting in the PSD classifier outputting
an uniform distribution. Based on this intuition,
we add the following features: MaxProb is the
maximum probability of any target translation:
maxt p(t|s). Entropy is the entropy of the proba-
bility distribution: ??t p(t|s) log p(t|s). Spread
is the difference between maximum and mini-
mum probabilities of the probability distribution:(
maxt p(t|s) ? mint p(t|s)
). Confusion is the
uncertainty in the most likely prediction given the
source token: mediantp(t|s)maxt p(t|s) . The use of median inthe numerator rather than the second best is mo-
tivated by the observation that, in most cases, top
ranked translations are of the same sense but differ
in morphology.
We train the PSD classifier in two modes:
1) a single global classifier that predicts the
target translation given any source word; 2) a
local classifier for each source word. When
training the global PSD classifier, we include
some lexical features that depend on the source
word. For both modes, we use real valued
and binned features giving rise to four families
of features Token:G-PSD, Token:G-PSDBin,
Token:L-PSD and Token:L-PSDBin.
Prior vs. Posterior PSD Features When the
PSD classifier is trained in the second mode, i.e.
one classifier per word type, we can define ad-
ditional features based on the prior (with out the
word context) and posterior (given the word?s
context) probability distributions output by the
classifier, i.e. pprior(t|s) and ppost.(t|s) respec-
1438
Domain Sentences Lang Tokens Types
Hansard 8,107,356 fr 161,695,309 191,501en 144,490,268 186,827
EMEA 472,231 fr 6,544,093 34,624en 5,904,296 29,663
Science 139,215 fr 4,292,620 117,669en 3,602,799 114,217
Subs 19,239,980 fr 154,952,432 361,584en 174,430,406 293,249
Table 2: Basic characteristics of the parallel data.
tively. We compute the following set of fea-
tures referred to as Token:PSDRatio: SameMax
checks if both the prior and posterior distri-
butions have the same translation as the most
likely translation. SameMin is same as the
above feature but check if the least likely trans-
lation is same. X-OR MinMax is the exclusive-
OR of SameMax and SameMin features. KL
is the KL-divergence between the two distri-
butions. Since KL-divergence is asymmetric,
we use KL(pprior||ppost.) and KL(ppost.||pprior).
MaxNorm is the ratio of maximum probabilities
in prior and posterior distributions. SpreadNorm
is the ratio of spread of the prior and posterior dis-
tributions, where spared is the difference between
maximum and minimum probabilities of the dis-
tribution as defined earlier. ConfusionNorm is the
ratio of confusion of the prior and posterior distri-
butions, where confusion is defined as earlier.
5 Data and Gold Standard
The first component of our task is a parallel cor-
pus of old domain data, for which we use the
French-English Hansard parliamentary proceed-
ings (http://www.parl.gc.ca). From this, we
extract an old domain sense dictionary, using the
Moses MT framework (Koehn et al, 2007). This
defines our old domain sense dictionary. For new
domains, we use three sources: (1) the EMEA
medical corpus (Tiedemann, 2009), (2) a corpus of
scientific abstracts, and (3) a corpus of translated
movie subtitles (Tiedemann, 2009). Basic statis-
tics are shown in Table 2. In all parallel corpora,
we normalize the English for American spelling.
To create the gold standard truth, we followed
a lexical sample apparoach and collected a set
of 300 ?representative types? that are interest-
ing to evaluate on, because they have multiple
senses within a single domain or whose senses
are likely to change in a new domain. We used
a semi-automatic approach to identify represen-
tative types. We first used the phrase table from
Parallel Repr. Repr. % New
Sents fr-tok Types Tokens Sense
EMEA 24k 270k 399 35,266 52.0%
Science 22k 681k 425 8,355 24.3%
Subs 36k 247k 388 22,598 43.4%
Table 3: Statistics about representative words and
the size of the development sets. The columns
show: the total amount of parallel development
data (# of sentences and tokens in French), # of
representative types that appear in this corpus, the
corresponding # of tokens, and the percentage of
these tokens that correspond to ?new senses.?
the Moses output to rank phrases in each domain
using TF-IDF scores with Okapi BM25 weight-
ing. For each of the three new domains (EMEA,
Science, and Subs), we found the intersection of
phrases between the old and the new domain. We
then looked at the different translations that each
had in the phrase table and a French speaker se-
lected a subset that have multiple senses.3
In practice, we limited our set alost entirely
to source words, and included only a single multi-
word phrase, vue des enfants, which usually trans-
lates as ?for children? in the old domain but al-
most always translates as ?sight of children? in
the EMEA domain (as in ?. . . should be kept out
of the sight of children?). Nothing in the way we
have defined, approached, or evaluated the SENS-
ESPOTTING task is dependent on the use of rep-
resentative words instead of longer representative
phrases. We chose to consider mostly source lan-
guage words for simplicity and because it was eas-
ier to identify good candidate words.
In addition to the manually chosen words, we
also identified words where the translation with
the highest lexical weight varied in different do-
mains, with the intuition being that are the words
that are likely to have acquired a new sense. The
top 200 words from this were added to the man-
ually selected representative words to form a list
of 450. Table 3 shows some statistics about these
words across our three test domains.
6 Experiments
6.1 Experimental setup
Our goal in evaluation is to be able to under-
stand what our approach is realistically capa-
ble of. One challenge is that the distribution
3In order to create the evaluation data, we used both sides
of the full parallel text; we do not use the English side of the
parallel data for actually building systems.
1439
of representative words is highly skewed.4 We
present results in terms of area under the ROC
curve (AUC),5 micro-averaged precision/recall/f-
measure and macro-averaged precision/recall/f-
measure. For macro-averaging, we compute a sin-
gle confusion matrix over all the test data and
determining P/R/F from that matrix. For micro-
averaging, we compute a separate confusion ma-
trix for each word type on the French side, com-
pute P/R/F for each of these separately, and then
average the results. (Thus, micro-F is not a
function of micro-P and micro-R.) The AUC and
macro-averaged scores give a sense of how well
the system is doing on a type-level basis (es-
sentially weighted by type frequency), while the
micro-averaged scores give a sense as to how well
the system is doing on individual types, not taking
into account their frequencies.
For most of our results, we present standard
deviations to help assess significance (?2? is
roughly a 90% confidence interval). For our re-
sults, in which we use new-domain training data,
we compute these results via 16-fold cross valida-
tion. The folds are split across types so the sys-
tem is never being tested on a word type that it has
seen before. We do this because it more closely re-
sembles our application goals. We do 16-fold for
convenience, because we divide the data into bi-
nary folds recursively (thus having a power-of-two
is easier), with an attempt to roughly balance the
size of the training sets in each fold (this is tricky
because of the skewed nature of the data). This en-
tire 16-fold cross-validation procedure is repeated
10 times and averages and standard deviations are
over the 160 replicates.
We evaluate performance using our type-level
features only, TYPEONLY, our token-level fea-
tures only, TOKENONLY, and using both our type
and our token level features, ALLFEATURES.
We compare our results with two baselines:
RANDOM and CONSTANT. RANDOM predicts
new-sense or not-new-sense randomly and with
equal probability. CONSTANT always predicts
new-sense, achieving 100% recall and a macro-
level precision that is equal to the percent of repre-
sentative words which do have a new sense, mod-
ulo cross-validation splits (see Table 3). Addi-
4The most frequent (voie) appears 3881 times; there are
60 singleton words on average across the three new domains.
5AUC is the probability that the classifier will assign a
higher score to a randomly chosen positive example than to a
randomly chosen negative example (Wikipedia, 2013).
tionally, we compare our results with a type-level
oracle, TYPEORACLE. For all tokens of a given
word type, the oracle predicts the majority label
(new-sense or not-new-sense) for that word type.
These results correspond to an upper bound for the
TYPEONLY experiments.
6.2 Classification Setup
For all experiments, we use a linear classifier
trained by stochastic gradient descent to optimize
logistic loss. We also did some initial experi-
ments on development data using boosted deci-
sion trees instead and other loss functions (hinge
loss, squared loss), but they never performed as
well. In all cases, we perform 20 passes over
the training data, using development data to per-
form early stopping (considered at the end of each
pass). We also use development data to tune a
regularizer (either `1 or `2) and its regularization
weight.6 Finally, all real valued features are au-
tomatically bucketed into 10 consecutive buckets,
each with (approximately) the same number of
elements. Each learner uses a small amount of
development data to tune a threshold on scores
for predicting new-sense or not-a-new-sense, us-
ing macro F-measure as an objective.
6.3 Result Summary
Table 4 shows our results on the SENSESPOT-
TING task. Classifiers based on the features
that we defined outperform both baselines in all
macro-level evaluations for the SENSESPOTTING
task. Using AUC as an evaluation metric, the
TOKENONLY, TYPEONLY, and ALLFEATURES
models performed best on EMEA, Science, and
Subtitles data, respectively. Our token-level fea-
tures perform particularly poorly on the Science
and Subtitles data. Although the model trained on
only those features achieves reasonable precision
(72.59 and 70.00 on Science and Subs, respec-
tively), its recall is very low (20.41 and 35.15), in-
dicating that the model classifies many new-sense
words as not-new-sense. Most of our token-level
features capture the intuition that when a word to-
ken appears in new or infrequent contexts, it is
likely to have gained a new sense. Our results indi-
cate that this intuition was more fruitful for EMEA
than for Science or Subs.
In contrast, the type-only features (TYPEONLY)
6We use http://hunch.net/?vw/ version 7.1.2,and run it with the following arguments that affect learning
behavior: --exact adaptive norm --power t 0.5
1440
Macro Micro
AUC P R F P R F
EMEA
RANDOM 50.34 ? 0.60 51.24 ? 0.59 50.09 ? 1.18 50.19 ? 0.75 47.04 ? 0.60 56.07 ? 1.99 37.27 ? 0.91
CONSTANT 50.00 ? 0.00 50.99 ? 0.00 100.0 ? 0.00 67.09 ? 0.00 45.80 ? 0.00 100.0 ? 0.00 52.30 ? 0.00
TYPEONLY 55.91 ? 1.13 69.76 ? 3.45 43.13 ? 1.42 41.61 ? 1.07 77.92 ? 2.04 50.12 ? 2.35 31.26 ? 0.63
TYPEORACLE 88.73 ? 0.00 87.32 ? 0.00 86.76 ? 0.00 87.04 ? 0.00 90.01 ? 0.00 67.46 ? 0.00 59.39 ? 0.00
TOKENONLY 78.80 ? 0.52 69.83 ? 1.59 75.58 ? 2.61 69.40 ? 1.92 59.03 ? 1.70 62.53 ? 1.66 43.39 ? 0.94
ALLFEATURES 79.60 ? 1.20 68.11 ? 1.19 79.84 ? 2.27 71.64 ? 1.83 55.28 ? 1.11 71.50 ? 1.62 46.83 ? 0.62
Science
RANDOM 50.18 ? 0.78 24.48 ? 0.57 50.32 ? 1.33 32.92 ? 0.79 46.99 ? 0.51 60.32 ? 1.06 34.72 ? 1.03
CONSTANT 50.00 ? 0.00 24.34 ? 0.00 100.0 ? 0.00 39.15 ? 0.00 44.39 ? 0.00 100.0 ? 0.00 50.44 ? 0.00
TYPEONLY 77.06 ? 1.23 66.07 ? 2.80 36.28 ? 4.10 34.50 ? 4.06 84.97 ? 0.82 36.81 ? 2.33 24.22 ? 1.70
TYPEORACLE 88.76 ? 0.00 78.43 ? 0.00 69.29 ? 0.00 73.54 ? 0.00 84.19 ? 0.00 67.41 ? 0.00 52.67 ? 0.00
TOKENONLY 66.62 ? 0.47 60.50 ? 3.11 28.05 ? 2.06 30.81 ? 2.75 76.21 ? 1.78 36.57 ? 2.23 24.68 ? 1.36
ALLFEATURES 73.91 ? 0.66 50.59 ? 2.08 60.60 ? 2.04 47.54 ? 1.52 66.72 ? 1.19 62.30 ? 1.36 40.22 ? 1.03
Subs
RANDOM 50.26 ? 0.69 42.47 ? 0.60 50.17 ? 0.84 45.68 ? 0.68 52.18 ? 1.32 54.63 ? 2.01 39.87 ? 2.10
CONSTANT 50.00 ? 0.00 42.51 ? 0.00 100.0 ? 0.00 59.37 ? 0.00 50.63 ? 0.00 100.0 ? 0.00 58.67 ? 0.00
TYPEONLY 67.16 ? 0.73 76.41 ? 1.51 31.91 ? 3.15 36.37 ? 2.58 90.03 ? 0.61 34.78 ? 1.12 26.20 ? 0.61
TYPEORACLE 81.35 ? 0.00 83.12 ? 0.00 70.23 ? 0.00 76.12 ? 0.00 90.62 ? 0.00 52.37 ? 0.00 44.43 ? 0.00
TOKENONLY 63.30 ? 0.99 63.17 ? 2.31 45.38 ? 2.07 43.30 ? 1.29 76.38 ? 1.68 49.70 ? 1.76 37.92 ? 1.20
ALLFEATURES 69.26 ? 0.60 63.48 ? 1.77 56.22 ? 2.66 52.78 ? 1.96 67.55 ? 0.83 62.18 ? 1.45 43.85 ? 0.90
Table 4: Complete SENSESPOTTING results for all domains. The scores are from cross-validation on
a single domain; in all cases, higher is better. Two standard deviations of performance over the cross-
validation are shown in small type. For all domains and metrics, the highest (not necessarily statistically
significant) non-oracle results are bolded.
are relatively weak for predicting new senses on
EMEA data but stronger on Subs (TYPEONLY
AUC performance is higher than both baselines)
and even stronger on Science data (TYPEONLY
AUC and f-measure performance is higher
than both baselines as well as the ALLFEA-
TURESmodel). In our experience with the three
datasets, we know that the Science data, which
contains abstracts from a wide variety of scientific
disciplines, is the most diverse, followed by the
Subs data, and then EMEA, which mostly consists
of text from drug labels and tends to be quite repet-
itive. Thus, it makes sense that type-level features
would be the most informative for the least homo-
geneous dataset. Representative words in scien-
tific text are likely to appear in variety of contexts,
while in the EMEA data they may only appear in
a few, making it easier to contrast them with the
distributions observed in the old domain data.
For all domains, in micro-level evaluation, our
models fail to outperform the CONSTANT base-
line. Recall that the micro-level evaluation com-
putes precision, recall, and f-measure for all word
tokens of a given word type and then averages
across word types. We observe that words that are
less frequent in both the old and the new domains
are more likely to have a new sense than more fre-
quent words, which causes the CONSTANT base-
line to perform reasonably well. In contrast, it is
more difficult for our models to make good pre-
dictions for less frequent words. A low frequency
in the new domain makes type level features (esti-
mated over only a few instances) noisy and unreli-
able. Similarly, a low frequency in the old domain
makes the our token level features, which all con-
trast with old domain instances of the word type.
6.4 Feature Ablation
In the previous section, we observed that (with one
exception) both Type-level and Token-level fea-
tures are useful in our task (in some cases, essen-
tial). In this section, we look at finer-grained fea-
ture distinctions through a process of feature ab-
lation. In this setting, we begin with all features
in a model and remove one feature at a time, al-
ways removing the feature that hurts performance
least. For these experiments, we determine which
feature to remove using AUC. Note that we?re ac-
tually able to beat (by 2-4 points AUC) the scores
from Table 4 by removing features!
The results here are somewhat mixed. In EMEA
and Science, one can actually get by (accord-
ing to AUC) with very few features: just two
(Type:NgramProband Type:Context) are suffi-
cient to achieve optimal AUC scores. To get
higher Macro-F scores requires nearly all the fea-
tures, though this is partially due to the choice of
1441
EMEA AUC MacF
ALLFEATURES 79.60 71.64
?Token:L-PSDBin 77.09 70.50
?Type:RelFreq 78.43 72.19
?Token:G-PSD 79.66 72.11
?Type:Context 79.66 72.45
?Token:Ctx% 78.91 73.37
?Type:TopicSim 78.05 71.33
?Token:CtxCnt 76.90 71.72
?Token:L-PSD 76.03 73.35
?Type:NgramProb 73.32 69.54
?Token:G-PSDBin 74.41 69.76
?Token:NgramProb 69.78 68.89
?Token:PSDRatio 48.38 3.45
Science AUC MacF
ALLFEATURES 73.91 47.54
?Token:L-PSDBin 76.26 53.69
?Token:G-PSD 77.04 53.56
?Token:G-PSDBin 77.44 54.54
?Token:L-PSD 77.85 56.05
?Token:PSDRatio 77.92 57.34
?Token:CtxCnt 77.85 54.42
?Type:Context 78.17 55.45
?Token:Ctx% 78.06 55.04
?Type:TopicSim 77.83 54.57
?Token:NgramProb 76.98 51.02
?Type:RelFreq 74.25 49.57
?Type:NgramProb 50.00 0.00
Subs AUC MacF
ALLFEATURES 69.26 52.78
?Type:NgramProb 69.13 53.33
?Token:G-PSDBin 70.23 54.72
?Token:CtxCnt 71.23 58.35
?Token:L-PSDBin 72.07 57.85
?Token:G-PSD 72.17 57.33
?Type:TopicSim 72.31 58.41
?Token:Ctx% 72.17 56.17
?Token:NgramProb 71.35 59.26
?Token:PSDRatio 70.33 46.88
?Token:L-PSD 69.05 53.31
?Type:RelFreq 65.25 48.22
?Type:Context 50.00 0.00
Table 5: Feature ablation results for all three corpora. Selection criteria is AUC, but Macro-F is presented
for completeness. Feature selection is run independently on each of the three datasets. The features
toward the bottom were the first selected.
AUC Macro-F Micro-F
EMEA
TYPEONLY 71.43 ? 0.94 52.62 ? 3.41 38.67 ? 1.35
TOKENONLY 73.75 ? 1.11 67.77 ? 4.18 45.49 ? 3.96
ALLFEATURES 72.19 ? 4.07 67.26 ? 7.88 49.29 ? 3.55
XV-ALLFEATURES 79.60 ? 1.20 71.64 ? 1.83 46.83 ? 0.62
Science
TYPEONLY 75.19 ? 0.89 51.53 ? 2.55 37.14 ? 4.41
TOKENONLY 71.24 ? 1.45 47.27 ? 1.11 40.48 ? 1.84
ALLFEATURES 74.14 ? 0.93 48.86 ? 3.94 43.20 ? 3.16
XV-ALLFEATURES 73.91 ? 0.66 47.54 ? 1.52 40.22 ? 1.03
Subs
TYPEONLY 60.90 ? 1.47 39.21 ? 14.78 24.77 ? 2.78
TOKENONLY 62.00 ? 1.16 49.74 ? 6.30 42.95 ? 3.92
ALLFEATURES 60.12 ? 2.11 50.16 ? 8.63 38.56 ? 5.20
XV-ALLFEATURES 69.26 ? 0.60 52.78 ? 1.96 43.85 ? 0.90
Table 6: Cross-domain test results on the SENS-
ESPOTTING task. Two standard deviations are
shown in small type. Only AUC, Macro-F and
Micro-F are shown for brevity.
AUC as the measure on which to ablate. It?s quite
clear that for Science, all the useful information
is in the type-level features, a result that echoes
what we saw in the previous section. While for
EMEA and Subs, both type- and token-level fea-
tures play a significant role. Considering the six
most useful features in each domain, the ones that
pop out as frequently most useful are the global
PSD features, the ngram probability features (ei-
ther type- or token-based), the relative frequency
features and the context features.
6.5 Cross-Domain Training
One disadvantage to the previous method for eval-
uating the SENSESPOTTING task is that it requires
parallel data in a new domain. Suppose we have no
parallel data in the new domain at all, yet still want
to attack the SENSESPOTTING task. One option is
to train a system on domains for which we do have
parallel data, and then apply it in a new domain.
This is precisely the setting we explore in this sec-
tion. Now, instead of performing cross-validation
in a single domain (for instance, Science), we take
the union of all of the training data in the other
domains (e.g., EMEA and Subs), train a classifier,
and then apply it to Science. This classifier will al-
most certainly be worse than one trained on NEW
(Science) but does not require any parallel data in
that domain. (Hyperparameters are chosen by de-
velopment data from the OLD union.)
The results of this experiment are shown in
Table 6. We include results for TOKENONLY,
TYPEONLY and ALLFEATURES; all of these are
trained in the cross-domain setting. To ease com-
parison to the results that do not suffer from do-
main shift, we also present ?XV-ALLFEATURES?,
which are results copied from Table 4 in which
parallel data from NEW is used. Overall, there is a
drop of about 7.3% absolute in AUC, moving from
XV-ALLFEATURES to ALLFEATURES, including
a small improvement in Science (likely because
Science is markedly smaller than Subs, and ?more
difficult? than EMEA with many word types).
6.6 Detecting Most Frequent Sense Changes
We define a second, related task: MOSTFRE-
QSENSECHANGE. In this task, instead of predict-
ing if a given word token has a sense which is
brand new with respect to the old domain, we pre-
dict whether it is being used with a a sense which
is not the one that was observed most frequently
in the old domain. In our EMEA, Science, and
Subtitles data, 68.2%, 48.3%, and 69.6% of word
tokens? predominant sense changes.
1442
6 12 25 50 100.32
.40
.50
.63
Science
Macro
?F
% of data 6 12 25 50 100
.40
.50
.63
.79 EMEA
% of data 
 
6 12 25 50 100
.40
.50
.63
Subs
% of data 
 
TypeOracleRandomAllFeatures
Figure 1: Learning curves for the three domains. X-axis is percent of data used, Y-axis is Macro-F score.
Both axes are in log scale to show the fast rate of growth. A horizontal bar corresponding to random
predictions, and the TYPEORACLE results are shown for comparison.
AUC Macro-F Micro-F
EMEA
RANDOM 50.54 ? 0.41 58.23 ? 0.34 49.69 ? 0.85
CONSTANT 50.00 ? 0.00 82.15 ? 0.00 74.43 ? 0.00
TYPEONLY 55.05 ? 1.00 67.45 ? 1.35 65.72 ? 0.59
TYPEORACLE 88.36 ? 0.00 90.64 ? 0.00 77.46 ? 0.00
TOKENONLY 66.42 ? 1.07 80.27 ? 0.50 68.96 ? 0.58
ALLFEATURES 58.64 ? 3.45 80.57 ? 0.45 69.40 ? 0.51
Science
RANDOM 50.13 ? 0.78 49.05 ? 0.82 48.19 ? 1.47
CONSTANT 50.00 ? 0.00 65.21 ? 0.00 73.22 ? 0.00
TYPEONLY 68.32 ? 1.05 54.70 ? 2.35 57.04 ? 1.52
TYPEORACLE 91.41 ? 0.00 86.71 ? 0.00 74.26 ? 0.00
TOKENONLY 68.49 ? 0.59 62.76 ? 0.89 64.40 ? 1.08
ALLFEATURES 68.31 ? 0.93 64.73 ? 1.93 67.20 ? 1.65
Subs
RANDOM 50.27 ? 0.27 56.93 ? 0.29 50.93 ? 1.11
CONSTANT 50.00 ? 0.00 79.96 ? 0.00 76.26 ? 0.00
TYPEONLY 60.36 ? 0.90 67.78 ? 1.98 61.58 ? 1.78
TYPEORACLE 82.16 ? 0.00 87.96 ? 0.00 73.87 ? 0.00
TOKENONLY 59.49 ? 1.04 77.79 ? 0.82 73.51 ? 0.68
ALLFEATURES 54.97 ? 0.89 77.30 ? 1.58 72.29 ? 1.68
Table 7: Cross-validation results on the MOST-
FREQSENSECHANGE task. Two standard devia-
tions are shown in small type.
We use the same set of features and learn-
ing framework to generate and evaluate models
for this task. While the SENSESPOTTING task
has MT utility in suggesting which new domain
words demand a new translation, the MOSTFRE-
QSENSECHANGE task has utility in suggesting
which words demand a new translation proba-
bility distribution when shifting to a new do-
main. Table 7 shows the results of our MOSTFRE-
QSENSECHANGE task experiments.
Results on the MOSTFREQSENSECHANGE
task are somewhat similar to those for the SENS-
ESPOTTING task. Again, our models perform bet-
ter under a macro-level evaluation than under a
micro-level evaluation. However, in contrast to
the SENSESPOTTING results, token-level features
perform quite well on their own for all domains.
It makes sense that our token level features have a
better chance of success on this task. The impor-
tant comparison now is between a new domain to-
ken in context and the majority of the old domain
tokens of the same word type. This comparison
is likely to be more informative than when we are
equally interested in identifying overlap between
the current token and any old domain senses. Like
the SENSESPOTTING results, when doing a micro-
level evaluation, our models do not perform as
well as the CONSTANT baseline, and, as before,
we attribute this to data sparsity.
6.7 Learning Curves
All of the results presented so far use classi-
fiers trained on instances of representative types
(i.e. ?representative tokens?) extracted from fairly
large new domain parallel corpora (see Table 3),
consisting of between 22 and 36 thousand parallel
sentences, which yield between 8 and 35 thousand
representative tokens. Although we expect some
new domain parallel tuning data to be available
in most MT settings, we would like to know how
many representative types are required to achieve
good performance on the SENSESPOTTING task.
Figure 6.5 shows learning curves over the num-
ber of representative tokens that are used to train
SENSESPOTTING classifiers. In fact, only about
25-50% of the data we used is really necessary to
achieve the performance observed before.
Acknowledgments We gratefully acknowledge the support
of the JHU summer workshop program (and its funders), the
entire DAMT team (http://hal3.name/DAMT/), San-
jeev Khudanpur, support from the NRC for Marine Carpuat,
as well as DARPA CSSG Grant D11AP00279 for Hal Daume?
III and Jagadeesh Jagarlamudi.
1443
References
E. Agirre and P.G. Edmonds. 2006. Word Sense Dis-
ambiguation: Algorithms and Applications. Text,
Speech, and Language Technology Series. Springer
Science+Business Media B.V.
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12.
David Bamman and Gregory Crane. 2011. Measuring
historical word sense variation. In Proceedings of
the 2011 Joint International Conference on Digital
Libraries (JCDL 2011), pages 1?10.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research
(JMLR), 3.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 854?864,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving
Statistical Machine Translation using Word Sense
Disambiguation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), pages 61?
72, Prague, June.
Marine Carpuat, Hal Daume? III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Ales? Tamchyna, Katharine Henry, and
Rachel Rudinger. 2012. Domain adaptation in ma-
chine translation: Final report. In 2012 Johns Hop-
kins Summer Workshop Final Report.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the Association for
Computational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
pages 28?34, Valletta, Malta.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 128?135.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the google books ngram corpus. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK, July. Association for
Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Matthew Hoffman, David Blei, and Francis Bach.
2010. Online learning for latent dirichlet alocation.
In Advances in Neural Information Processing Sys-
tems (NIPS).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, Timothy Baldwin, and Lexical Computing.
2012. Word sense induction for novel sense de-
tection. In Proceedings of the 13th Conference of
the European Chapter of the Association for compu-
tational Linguistics (EACL 2012), pages 591?601.
Citeseer.
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2002. The role of domain
information in word sense disambiguation. Natural
Language Engineering, 8(04):359?373.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174:619?637, June.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 279. Association for Computational Lin-
guistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
1444
Behrang Mohit and Rebecca Hwa. 2007. Localiza-
tion of difficult-to-translate phrases. In proceedings
of the 2nd ACL Workshop on Statistical Machine
Translations.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and phonetic space. In Proceedings
of the EACL 2009 Workshop on GEMS: GEometical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece, March.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using
Diverse Similarity Measures. Ph.D. thesis, Johns
Hopkins University.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).
Wikipedia. 2013. Receiver operating characteristic.
http://en.wikipedia.org/wiki/Receiver_
operating_characteristic#Area_Under_
the_Curve, February.
1445
The Language Demographics of Amazon Mechanical Turk
Ellie Pavlick1 Matt Post2 Ann Irvine2 Dmitry Kachaev2 Chris Callison-Burch1,2
1Computer and Information Science Department, University of Pennsylvania
2Human Language Technology Center of Excellence, Johns Hopkins University
Abstract
We present a large scale study of the languages
spoken by bilingual workers on Mechanical
Turk (MTurk). We establish a methodology
for determining the language skills of anony-
mous crowd workers that is more robust than
simple surveying. We validate workers? self-
reported language skill claims by measuring
their ability to correctly translate words, and
by geolocating workers to see if they reside in
countries where the languages are likely to be
spoken. Rather than posting a one-off survey,
we posted paid tasks consisting of 1,000 as-
signments to translate a total of 10,000 words
in each of 100 languages. Our study ran
for several months, and was highly visible on
the MTurk crowdsourcing platform, increas-
ing the chances that bilingual workers would
complete it. Our study was useful both to cre-
ate bilingual dictionaries and to act as cen-
sus of the bilingual speakers on MTurk. We
use this data to recommend languages with the
largest speaker populations as good candidates
for other researchers who want to develop
crowdsourced, multilingual technologies. To
further demonstrate the value of creating data
via crowdsourcing, we hire workers to create
bilingual parallel corpora in six Indian lan-
guages, and use them to train statistical ma-
chine translation systems.
1 Overview
Crowdsourcing is a promising new mechanism for
collecting data for natural language processing re-
search. Access to a fast, cheap, and flexible work-
force allows us to collect new types of data, poten-
tially enabling new language technologies. Because
crowdsourcing platforms like Amazon Mechanical
Turk (MTurk) give researchers access to a world-
wide workforce, one obvious application of crowd-
sourcing is the creation of multilingual technologies.
With an increasing number of active crowd workers
located outside of the United States, there is even the
potential to reach fluent speakers of lower resource
languages. In this paper, we investigate the feasi-
bility of hiring language informants on MTurk by
conducting the first large-scale demographic study
of the languages spoken by workers on the platform.
There are several complicating factors when try-
ing to take a census of workers on MTurk. The
workers? identities are anonymized, and Amazon
provides no information about their countries of ori-
gin or their language abilities. Posting a simple sur-
vey to have workers report this information may be
inadequate, since (a) many workers may never see
the survey, (b) many opt not to do one-off surveys
since potential payment is low, and (c) validating the
answers of respondents is not straightforward.
Our study establishes a methodology for deter-
mining the language demographics of anonymous
crowd workers that is more robust than simple sur-
veying. We ask workers what languages they speak
and what country they live in, and validate their
claims by measuring their ability to correctly trans-
late words and by recording their geolocation. To
increase the visibility and the desirability of our
tasks, we post 1,000 assignments in each of 100 lan-
guages. These tasks each consist of translating 10
foreign words into English. Two of the 10 words
have known translations, allowing us to validate that
the workers? translations are accurate. We construct
bilingual dictionaries with up to 10,000 entries, with
the majority of entries being new.
Surveying thousands of workers allows us to ana-
lyze current speaker populations for 100 languages.
79
Transactions of the Association for Computational Linguistics, 2 (2014) 79?92. Action Editor: Mirella Lapata.
Submitted 12/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
11/26/13 turkermap.html
file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html 1/1
1 1,998
Figure 1: The number of workers per country. This map was generated based on geolocating the IP address
of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during
the study, and 238 workers who could not be geolocated. The size of the circles represents the number
of workers from each country. The two largest are India (1,998 workers) and the United States (866). To
calibrate the sizes: the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.
The data also allows us to answer questions like:
How quickly is work completed in a given language?
Are crowdsourced translations reliably good? How
often do workers misrepresent their language abili-
ties to obtain financial rewards?
2 Background and Related Work
Amazon?s Mechanical Turk (MTurk) is an on-
line marketplace for work that gives employers
and researchers access to a large, low-cost work-
force. MTurk allows employers to provide micro-
payments in return for workers completing micro-
tasks. The basic units of work on MTurk are called
?Human Intelligence Tasks? (HITs). MTurk was de-
signed to accommodate tasks that are difficult for
computers, but simple for people. This facilitates
research into human computation, where people can
be treated as a function call (von Ahn, 2005; Little et
al., 2009; Quinn and Bederson, 2011). It has appli-
cation to research areas like human-computer inter-
action (Bigham et al., 2010; Bernstein et al., 2010),
computer vision (Sorokin and Forsyth, 2008; Deng
et al., 2010; Rashtchian et al., 2010), speech pro-
cessing (Marge et al., 2010; Lane et al., 2010; Parent
and Eskenazi, 2011; Eskenazi et al., 2013), and natu-
ral language processing (Snow et al., 2008; Callison-
Burch and Dredze, 2010; Laws et al., 2011).
On MTurk, researchers who need work completed
are called ?Requesters?, and workers are often re-
ferred to as ?Turkers?. MTurk is a true market, mean-
ing that Turkers are free to choose to complete the
HITs which interest them, and Requesters can price
their tasks competitively to try to attract workers and
have their tasks done quickly (Faridani et al., 2011;
Singer and Mittal, 2011). Turkers remain anony-
mous to Requesters, and all payment occurs through
Amazon. Requesters are able to accept submitted
work or reject work that does not meet their stan-
dards. Turkers are only paid if a Requester accepts
their work.
Several reports examine Mechanical Turk as an
economic market (Ipeirotis, 2010a; Lehdonvirta and
Ernkvist, 2011). When Amazon introduced MTurk,
it first offered payment only in Amazon credits, and
later offered direct payment in US dollars. More re-
cently, it has expanded to include one foreign cur-
rency, the Indian rupee. Despite its payments be-
ing limited to two currencies or Amazon credits,
MTurk claims over half a million workers from 190
countries (Amazon, 2013). This suggests that its
worker population should represent a diverse set of
languages.
80
A demographic study by Ipeirotis (2010b) fo-
cused on age, gender, martial status, income lev-
els, motivation for working on MTurk, and whether
workers used it as a primary or supplemental form
of income. The study contrasted Indian and US
workers. Ross et al. (2010) completed a longitudi-
nal follow-on study. A number of other studies have
informally investigated Turkers? language abilities.
Munro and Tily (2011) compiled survey responses
of 2,000 Turkers, revealing that four of the six most
represented languages come from India (the top six
being Hindi, Malayalam, Tamil, Spanish, French,
and Telugu). Irvine and Klementiev (2010) had
Turkers evaluate the accuracy of translations that
had been automatically inducted from monolingual
texts. They examined translations of 100 words in
42 low-resource languages, and reported geolocated
countries for their workers (India, the US, Romania,
Pakistan, Macedonia, Latvia, Bangladesh and the
Philippines). Irvine and Klementiev discussed the
difficulty of quality control and assessing the plausi-
bility of workers? language skills for rare languages,
which we address in this paper.
Several researchers have investigated using
MTurk to build bilingual parallel corpora for ma-
chine translation, a task which stands to benefit
low cost, high volume translation on demand (Ger-
mann, 2001). Ambati et al. (2010) conducted a pilot
study by posting 25 sentences to MTurk for Span-
ish, Chinese, Hindi, Telugu, Urdu, and Haitian Cre-
ole. In a study of 2000 Urdu sentences, Zaidan
and Callison-Burch (2011) presented methods for
achieving professional-level translation quality from
Turkers by soliciting multiple English translations
of each foreign sentence. Zbib et al. (2012) used
crowdsourcing to construct a 1.5 million word par-
allel corpus of dialect Arabic and English, train-
ing a statistical machine translation system that pro-
duced higher quality translations of dialect Arabic
than a system a trained on 100 times more Mod-
ern Standard Arabic-English parallel data. Zbib et
al. (2013) conducted a systematic study that showed
that training an MT system on crowdsourced trans-
lations resulted in the same performance as training
on professional translations, at 15 the cost. Hu etal. (2010; Hu et al. (2011) performed crowdsourced
translation by having monolingual speakers collab-
orate and iteratively improve MT output.
English 689 Tamil 253 Malayalam 219
Hindi 149 Spanish 131 Telugu 87
Chinese 86 Romanian 85 Portuguese 82
Arabic 74 Kannada 72 German 66
French 63 Polish 61 Urdu 56
Tagalog 54 Marathi 48 Russian 44
Italian 43 Bengali 41 Gujarati 39
Hebrew 38 Dutch 37 Turkish 35
Vietnamese 34 Macedonian 31 Cebuano 29
Swedish 26 Bulgarian 25 Swahili 23
Hungarian 23 Catalan 22 Thai 22
Lithuanian 21 Punjabi 21 Others ? 20
Table 1: Self-reported native language of 3,216
bilingual Turkers. Not shown are 49 languages with
?20 speakers. We omit 1,801 Turkers who did not
report their native language, 243 who reported 2 na-
tive languages, and 83 with ?3 native languages.
Several researchers have examined cost optimiza-
tion using active learning techniques to select the
most useful sentences or fragments to translate (Am-
bati and Vogel, 2010; Bloodgood and Callison-
Burch, 2010; Ambati, 2012).
To contrast our research with previous work, the
main contributions of this paper are: (1) a robust
methodology for assessing the bilingual skills of
anonymous workers, (2) the largest-scale census to
date of language skills of workers on MTurk, and (3)
a detailed analysis of the data gathered in our study.
3 Experimental Design
The central task in this study was to investigate Me-
chanical Turk?s bilingual population. We accom-
plished this through self-reported surveys combined
with a HIT to translate individual words for 100
languages. We evaluate the accuracy of the work-
ers? translations against known translations. In cases
where these were not exact matches, we used a sec-
ond pass monolingual HIT, which asked English
speakers to evaluate if a worker-provided translation
was a synonym of the known translation.
Demographic questionnaire At the start of each
HIT, Turkers were asked to complete a brief survey
about their language abilities. The survey asked the
following questions:
? Is [language] your native language?
? How many years have you spoken [language]?
81
? Is English your native language?
? How many years have you spoken English?
? What country do you live in?
We automatically collected each worker?s current lo-
cation by geolocating their IP address. A total of
5,281 unique workers completed our HITs. Of these,
3,625 provided answers to our survey questions, and
we were able to geolocate 5,043. Figure 1 plots
the location of workers across 106 countries. Table
1 gives the most common self-reported native lan-
guages.
Selection of languages We drew our data from the
different language versions of Wikipedia. We se-
lected the 100 languages with the largest number of
articles 1 (Table 2). For each language, we chose
the 1,000 most viewed articles over a 1 year period,2
and extracted the 10,000 most frequent words from
them. The resulting vocabularies served as the input
to our translation HIT.
Translation HIT For the translation task, we
asked Turkers to translate individual words. We
showed each word in the context of three sentences
that were drawn from Wikipedia. Turkers were al-
lowed to mark that they were unable to translate a
word. Each task contained 10 words, 8 of which
were words with unknown translations, and 2 of
which were quality control words with known trans-
lations. We gave special instruction for translat-
ing names of people and places, giving examples
of how to handle ?Barack Obama? and ?Australia?
using their interlanguage links. For languages with
non-Latin alphabets, names were transliterated.
The task paid $0.15 for the translation of 10
words. Each set of 10 words was independently
translated by three separate workers. 5,281 workers
completed 256,604 translation assignments, totaling
more than 3 million words, over a period of three
and a half months.
Gold standard translations A set of gold stan-
dard translations were automatically harvested from
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
2http://dumps.wikimedia.org/other/
pagecounts-raw/
500K+ ARTICLES: German (de), English (en), Spanish (es), French
(fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese
(pt), Russian (ru)
100K-500K ARTICLES: Arabic (ar), Bulgarian (bg), Catalan (ca),
Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa),
Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu),
Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-
gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-
bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese
(vi), Waray-Waray (war), Chinese (zh)
10K-100K ARTICLES: Afrikaans (af) Amharic (am) Asturian (ast)
Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri
(bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki
(diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati
(gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-
gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian
(lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi
(mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal
Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-
ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili
(sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba
(yo)
<10K ARTICLES: Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo)
Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali
(so) Uzbek (uz) Wolof (wo)
Table 2: A list of the languages that were used in our
study, grouped by the number of Wikipedia articles
in the language. Each language?s code is given in
parentheses. These language codes are used in other
figures throughout this paper.
Wikipedia for every language to use as embedded
controls. We used Wikipedia?s inter-language links
to pair titles of English articles with their corre-
sponding foreign article?s title. To get a more trans-
latable set of pairs, we excluded any pairs where: (1)
the English word was not present in the WordNet
ontology (Miller, 1995), (2) either article title was
longer than a single word, (3) the English Wikipedia
page was a subcategory of person or place, or (4)
the English and the foreign titles were identical or a
substring of the other.
Manual evaluation of non-identical translations
We counted all translations that exactly matched
the gold standard translation as correct. For non-
exact matches we created a second-pass quality as-
surance HIT. Turkers were shown a pair of En-
glish words, one of which was a Turker?s transla-
tion of the foreign word used for quality control,
and the other of which was the gold-standard trans-
lation of the foreign word. Evaluators were asked
whether the two words had the same meaning, and
chose between three answers: ?Yes?, ?No?, or ?Re-
82
Figure 2: Days to complete the translation HITs for
40 of the languages. Tick marks represent the com-
pletion of individual assignments.
lated but not synonymous.? Examples of mean-
ing equivalent pairs include: <petroglyphs, rock
paintings>, <demo, show> and <loam, loam: soil
rich in decaying matter>. Non-meaning equiva-
lents included: <assorted, minutes>, and <major,
URL of image>. Related items were things like
<sky, clouds>. Misspellings like <lactation, lac-
tiation > were judged to have same meaning, and
were marked as misspelled. Three separate Turkers
judged each pair, allowing majority votes for diffi-
cult cases.
We checked Turkers who were working on this
task by embedding pairs of words which were ei-
?? ?$ %??? ( ?? + ?$ ??? %?.? ?? ?? ???? 5 ?? ?????9 ???:? ?? ??<? 
In retribution pakistan also did six nuclear tests on 28 may 1998.
On 28 May Pakistan also conducted six nuclear tests as an act
of redressal.
Retaliating on this ?Pakistan? conducted Six(6) Nuclear Tests
on 28 May, 1998.
pakistan also did 6 nuclear test in retribution on 28 may, 1998
Figure 3: An example of the Turkers? translations of
a Hindi sentence. The translations are unedited and
contain fixable spelling, capitalization and grammat-
ical errors.
ther known to be synonyms (drawn from Word-
Net) or unrelated (randomly chosen from a corpus).
Automating approval/rejections for the second-pass
evaluation allowed the whole pipeline to be run au-
tomatically. Caching judgments meant that we ulti-
mately needed only 20,952 synonym tasks to judge
all of the submitted translations (a total of 74,572
non-matching word pairs). These were completed
by an additional 1,005 workers. Each of these as-
signments included 10 word pairs and paid $0.10.
Full sentence translations To demonstrate the
feasibility of using crowdsourcing to create multi-
lingual technologies, we hire Turkers to construct
bilingual parallel corpora from scratch for six In-
dian languages. Germann (2001) attempted to build
a Tamil-English translation system from scratch by
hiring professional translators, but found the cost
prohibitive. We created parallel corpora by trans-
lating the 100 most viewed Wikipedia pages in Ben-
gali, Malyalam, Hindi, Tamil, Telugu, and Urdu into
English. We collected four translations from differ-
ent Turkers for each source sentence.
Workers were paid $0.70 per HIT to translate
10 sentences. We accepted or rejected translations
based on a manual review of each worker?s submis-
sions, which included a comparison of the transla-
tions to a monotonic gloss (produced with a dic-
tionary), and metadata such as the amount of time
the worker took to complete the HIT and their geo-
graphic location.
Figure 3 shows an example of the translations we
obtained. The lack of a professionally translated
reference sentences prevented us from doing a sys-
tematic comparison between the quality of profes-
83
pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the pro-
portion of translations which exactly matched gold standard translations, and light blue indicate translations
which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
sion and non-professional translations as Zaidan and
Callison-Burch (2011) did. Instead we evaluate the
quality of the data by using it to train SMT systems.
We present results in section 5.
4 Measuring Translation Quality
For single word translations, we calculate the qual-
ity of translations on the level of individual assign-
ments and aggregated over workers and languages.
We define an assignment?s quality as the proportion
of controls that are correct in a given assignment,
where correct means exactly correct or judged to be
synonymous.
Quality(ai) = 1ki
ki?
j=1
?(trij ? syns[gj]) (1)
where ai is the ith assignment, ki is the number of
controls in ai, trij is the Turker?s provided transla-
tion of control word j in assignment i, gj is the gold
standard translation of control word j, syns[gj] is
the set of words judged to be synonymous with gj
and includes gj , and ?(x) is Kronecker?s delta and
takes value 1 when x is true. Most assignments had
two known words embedded, so most assignments
had scores of either 0, 0.5, or 1.
Since computing overall quality for a language as
the average assignment quality score is biased to-
wards a small number of highly active Turkers, we
instead report language quality scores as the aver-
age per-Turker quality, where a Turker?s quality is
the average quality of all the assignments that she
completed:
Quality(ti) =
?
aj?assigns[i] Quality(aj)
| assigns[i] | (2)
where assigns[i] is the assignments completed
by Turker i, and Quality(a) is as above.
Quality for a language is then given by
Quality(li) =
?
tj?turkers[i] Quality(tj)
| turkers[i] | (3)
When a Turker completed assignments in more than
one language, their quality was computed separately
for each language. Figure 4 shows the transla-
tion quality for languages with contributions from
at least 50 workers.
Cheating using machine translation One obvi-
ous way for workers to cheat is to use available
online translation tools. Although we followed
best practices to deter copying-and-pasting into on-
line MT systems by rendering words and sentences
84
as images (Zaidan and Callison-Burch, 2011), this
strategy does not prevent workers from typing the
words into an MT system if they are able to type in
the language?s script.
To identify and remove workers who appeared to
be cheating by using Google Translate, we calcu-
lated each worker?s overlap with the Google transla-
tions. We used Google to translate all 10,000 words
for the 51 foreign languages that Google Trans-
late covered at the time of the study. We mea-
sured the percent of workers? translations that ex-
actly matched the translation returned from Google.
Figure 5a shows overlap between Turkers?s trans-
lations and Google Translate. When overlap is high,
it seems likely that those Turkers are cheating. It is
also reasonable to assume that honest workers will
overlap with Google some amount of the time as
Google?s translations are usually accurate. We di-
vide the workers into three groups: those with very
high overlap with Google (likely cheating by using
Google to translate words), those with reasonable
overlap, and those with no overlap (likely cheating
by other means, for instance, by submitting random
text).
Our gold-standard controls are designed to iden-
tify workers that fall into the third group (those who
are spamming or providing useless translations), but
they will not effectively flag workers who are cheat-
ing with Google Translate. We therefore remove the
500 Turkers with the highest overlap with Google.
This equates to removing all workers with greater
than 70% overlap. Figure 5b shows that removing
workers at or above the 70% threshold retains 90%
of the collected translations and over 90% of the
workers.
Quality scores reported throughout the paper re-
flect only translations from Turkers whose overlap
with Google falls below this 70% threshold.
5 Data Analysis
We performed an analysis of our data to address the
following questions:
? Do workers accurately represent their language
abilities? Should we constrain tasks by region?
? How quickly can we expect work to be com-
pleted in a particular language?
(a) Individual workers? overlap with Google Translate.
We removed the 500 workers with the highest overlap
(shaded region on the left) from our analyses, as it is rea-
sonable to assume these workers are cheating by submit-
ting translations from Google. Workers with no overlap
(shaded region on the right) are also likely to be cheating,
e.g. by submitting random text.
(b) Cumulative distribution of overlap with Google trans-
late for workers and translations. We see that eliminating
all workers with >70% overlap with google translate still
preserves 90% of translations and >90% of workers.
Figure 5
? Can Turkers? translations be used to train MT
systems?
? Do our dictionaries improve MT quality?
Language skills and location We measured the
average quality of workers who were in countries
that plausibly speak a language, versus workers from
countries that did not have large speaker populations
of that language. We used the Ethnologue (Lewis
85
Avg. Turker quality (# Ts) Primary locations Primary locations
In region Out of region of Turkers in region of Turkers out of region
Hindi 0.63 (296) 0.69 (7) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1)
Tamil 0.65 (273) ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1)
Malayalam 0.76 (234) 0.83 (2) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1)
Spanish 0.81 (191) 0.84 (18) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1)
French 0.75 (170) 0.82 (11) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1)
Chinese 0.60 (116) 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2)
German 0.82 (91) 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1)
Italian 0.86 (90) * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2)
Amharic 0.14 (16) ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5)
Kannada 0.70 (105) NA (0) India (105)
Arabic 0.74 (60) ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3)
Sindhi 0.19 (96) 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2)
Portuguese 0.87 (101) 0.96 (3) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1)
Turkish 0.76 (76) 0.80 (27) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1)
Telugu 0.80 (102) 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1)
Irish 0.74 (54) 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2)
Swedish 0.73 (54) 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2)
Czech 0.71 (45) * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5)
Russian 0.15 (67) * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3)
Breton 0.17 (3) 0.18 (89) US (3) India (83) Macedonia (2) China (1)
Table 3: Translation quality when partitioning the translations into two groups, one containing translations
submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the
other containing translations from Turkers outside those regions. In general, in-region Turkers provide
higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
et al., 2013) to compile the list of countries where
each language is spoken. Table 3 compares the av-
erage translation quality of assignments completed
within the region of each language, and compares it
to the quality of assignments completed outside that
region.
Our workers reported speaking 95 languages na-
tively. US workers alone reported 61 native lan-
guages. Overall, 4,297 workers were located in a
region likely to speak the language from which they
were translating, and 2,778 workers were located
in countries considered out of region (meaning that
about a third of our 5,281 Turkers completed HITs
in multiple languages).
Table 3 shows the differences in translation qual-
ity when computed using in-region versus out-of-
region Turkers, for the languages with the greatest
number of workers. Within region workers typi-
cally produced higher quality translations. Given the
number of Indian workers on Mechanical Turk, it
is unsurprising that they represent majority of out-
of-region workers. For the languages that had more
than 75 out of region workers (Malay, Amharic, Ice-
landic, Sicilian, Wolof, and Breton), Indian workers
represented at least 70% of the out of region workers
in each language.
A few languages stand out for having suspiciously
strong performance by out of region workers, no-
tably Irish and Swedish, for which out of region
workers account for a near equivalent volume and
quality of translations to the in region workers. This
is admittedly implausible, considering the relatively
small number of Irish speakers worldwide, and the
very low number living in the countries in which our
Turkers were based (primarily India). Such results
highlight the fact that cheating using online transla-
tion resources is a real problem, and despite our best
efforts to remove workers using Google Translate,
some cheating is still evident. Restricting to within
region workers is an effective way to reduce the
prevalence of cheating. We discuss the languages
which are best supported by true native speakers in
section 6.
Speed of translation Figure 2 gives the comple-
tion times for 40 languages. The 10 languages to
finish in the shortest amount of time were: Tamil,
Malayalam, Telugu, Hindi, Macedonian, Spanish,
Serbian, Romanian, Gujarati, and Marathi. Seven of
the ten fastest languages are from India, which is un-
86
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 6: The total volume of translations (measured
in English words) as a function of elapsed days.
sentence English + dictionary
language pairs foreign words entries
Bengali 22k 732k 22k
Hindi 40k 1,488k 22k
Malayalam 32k 863k 23k
Tamil 38k 916k 25k
Telugu 46k 1,097k 21k
Urdu 35k 1,356k 20k
Table 4: Size of parallel corpora and bilingual dic-
tionaries collected for each language.
surprising given the geographic distribution of work-
ers. Some languages follow the pattern of having a
smattering of assignments completed early, with the
rate picking up later.
Figure 6 gives the throughput of the full-sentence
translation task for the six Indian languages. The
fastest language was Malayalam, for which we col-
lected half a million words of translations in just un-
der a week. Table 4 gives the size of the data set that
we created for each of these languages.
Training SMT systems We trained statistical
translation models from the parallel corpora that we
created for the six Indian languages using the Joshua
machine translation system (Post et al., 2012). Table
5 shows the translation performance when trained
on the bitexts alone, and when incorporating the
bilingual dictionaries created in our earlier HIT. The
scores reflect the performance when tested on held
out sentences from the training data. Adding the dic-
trained on bitext + BLEU
language bitexts alone dictionaries ?
Bengali 12.03 17.29 5.26
Hindi 16.19 18.10 1.91
Malayalam 6.65 9.72 3.07
Tamil 8.08 9.66 1.58
Telugu 11.94 13.70 1.76
Urdu 19.22 21.98 2.76
Table 5: BLEU scores for translating into English
using bilingual parallel corpora by themselves, and
with the addition of single-word dictionaries. Scores
are calculated using four reference translations and
represent the mean of three MERT runs.
tionaries to the training set produces consistent per-
formance gains, ranging from 1 to 5 BLEU points.
This represents a substantial improvement. It is
worth noting, however, that while the source doc-
uments for the full sentences used for testing were
kept disjoint from those used for training, there is
overlap between the source materials for the dictio-
naries and those from the test set, since both the dic-
tionaries and the bitext source sentences were drawn
from Wikipedia.
6 Discussion
Crowdsourcing platforms like Mechanical Turk give
researchers instant access to a diverse set of bilin-
gual workers. This opens up exciting new avenues
for researchers to develop new multilingual systems.
The demographics reported in this study are likely to
shift over time. Amazon may expand its payments to
new currencies. Posting long-running HITs in other
languages may recruit more speakers of those lan-
guages. New crowdsourcing platforms may emerge.
The data presented here provides a valuable snap-
shot of the current state of MTurk, and the methods
used can be applied generally in future research.
Based on our study, we can confidently recom-
mend 13 languages as good candidates for research
now: Dutch, French, German, Gujarati, Italian, Kan-
nada, Malayalam, Portuguese, Romanian, Serbian,
Spanish, Tagalog, and Telugu. These languages
have large Turker populations who complete tasks
quickly and accurately. Table 6 summarizes the
strengths and weaknesses of all 100 languages cov-
ered in our study. Several other languages are viable
87
workers quality speed
many high fast Dutch, French, German, Gu-
jarati, Italian, Kannada, Malay-
alam, Portuguese, Romanian,
Serbian, Spanish, Tagalog, Tel-
ugu
slow Arabic, Hebrew, Irish, Punjabi,
Swedish, Turkish
low fast Hindi, Marathi, Tamil, Urdu
or
medium
slow Bengali, Bishnupriya Ma-
nipuri, Cebuano, Chinese,
Nepali, Newar, Polish, Russian,
Sindhi, Tibetan
few high fast Bosnia, Croatian, Macedonian,
Malay, Serbo-Croatian
slow Afrikaans, Albanian,
Aragonese, Asturian, Basque,
Belarusian, Bulgarian, Central
Bicolano, Czech, Danish,
Finnish, Galacian, Greek,
Haitian, Hungarian, Icelandic,
Ilokano, Indonesian, Japanese,
Javanese, Kapampangan,
Kazakh, Korean, Lithuanian,
Low Saxon, Malagasy, Nor-
wegian (Bokmal), Sicilian,
Slovak, Slovenian, Thai, UKra-
nian, Uzbek, Waray-Waray,
West Frisian, Yoruba
low fast ?
or
medium
slow Amharic, Armenian, Azer-
baijani, Breton, Catalan,
Georgian, Latvian, Luxembour-
gish, Neapolitian, Norwegian
(Nynorsk), Pashto, Pied-
montese, Somali, Sudanese,
Swahili, Tatar, Vietnamese,
Walloon, Welsh
none low or
medium
slow Esperanto, Ido, Kurdish, Per-
sian, Quechua, Wolof, Zazaki
Table 6: The green box shows the best languages to
target on MTurk. These languages have many work-
ers who generate high quality results quickly. We
defined many workers as 50 or more active in-region
workers, high quality as?70% accuracy on the gold
standard controls, and fast if all of the 10,000 words
were completed within two weeks.
candidates provided adequate quality control mech-
anisms are used to select good workers.
Since Mechanical Turk provides financial incen-
tives for participation, many workers attempt to
complete tasks even if they do not have the lan-
guage skills necessary to do so. Since MTurk does
not provide any information about workers demo-
graphics, including their language competencies, it
can be hard to exclude such workers. As a result
naive data collection on MTurk may result in noisy
data. A variety of techniques should be incorporated
into crowdsourcing pipelines to ensure high quality
data. As a best practice, we suggest: (1) restricting
workers to countries that plausibly speak the foreign
language of interest, (2) embedding gold standard
controls or administering language pretests, rather
than relying solely on self-reported language skills,
and (3) excluding workers whose translations have
high overlap with online machine translation sys-
tems like Google translate. If cheating using exter-
nal resources is likely, then also consider (4) record-
ing information like time spent on a HIT (cumulative
and on individual items), patterns in keystroke logs,
tab/window focus, etc.
Although our study targeted bilingual workers on
Mechanical Turk, and neglected monolingual work-
ers, we believe our results reliably represent the cur-
rent speaker populations, since the vast majority of
the work available on the crowdsourced platform
is currently English-only. We therefore assume the
number of non-English speakers is small. In the fu-
ture, it may be desirable to recruit monolingual for-
eign workers. In such cases, we recommend other
tests to validate their language abilities in place of
our translation test. These could include perform-
ing narrative cloze, or listening to audio files con-
taining speech in different language and identifying
their language.
7 Data release
With the publication of this paper, we are releasing
all data and code used in this study. Our data release
includes the raw data, along with bilingual dictionar-
ies that are filtered to be high quality. It will include
256,604 translation assignments from 5,281 Turkers
and 20,952 synonym assignments from 1,005 Turk-
ers, along with meta information like geolocation
88
and time submitted, plus external dictionaries used
for validation. The dictionaries will contain 1.5M
total translated words in 100 languages, along with
code to filter the dictionaries based on different cri-
teria. The data also includes parallel corpora for six
Indian languages, ranging in size between 700,000
to 1.5 million words.
8 Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing offi-
cial policies or endorsements by DARPA or the U.S.
Government. This research was supported by the
Johns Hopkins University Human Language Tech-
nology Center of Excellence and through gifts from
Microsoft and Google.
The authors would like to thank the anonymous
reviewers for their thoughtful comments, which sub-
stantially improved this paper.
References
Amazon. 2013. Service summary tour for re-
questers on Amazon Mechanical Turk. https://
requester.mturk.com/tour.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk. Association for Computational Lin-
guistics.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mellon
University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceed-
ings of the ACM Symposium on User Interface Soft-
ware and Technology (UIST).
Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
tle, Andrew Miller, Robert C. Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White, Samual White,
and Tom Yeh. 2010. VizWiz: nearly real-time an-
swers to visual questions. In Proceedings of the ACM
Symposium on User Interface Software and Technol-
ogy (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010.
What does classifying more than 10,000 image cate-
gories tell us? In Proceedings of the 12th European
Conference of Computer Vision (ECCV, pages 71?84.
Maxine Eskenazi, Gina-Anne Levow, Helen Meng,
Gabriel Parent, and David Suendermann. 2013.
Crowdsourcing for Speech Processing, Applications to
Data Collection, Transcription and Assessment. Wi-
ley.
Siamak Faridani, Bjo?rn Hartmann, and Panagiotis G.
Ipeirotis. 2011. What?s the right price? pricing tasks
for finishing on time. In Third AAAI Human Compu-
tation Workshop (HCOMP?11).
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of ACM SIGKDD
Workshop on Human Computation (HCOMP).
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency sms messages. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 399?404, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Panagiotis G. Ipeirotis. 2010a. Analyzing the mechani-
cal turk marketplace. In ACM XRDS, December.
Panagiotis G. Ipeirotis. 2010b. Demographics of
Mechanical Turk. Technical Report Working paper
89
CeDER-10-01, New York University, Stern School of
Business.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Workshop on Creating Speech and
Language Data with MTurk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora
via mechanical-turk. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, Los An-
geles.
Florian Laws, Christian Scheible, and Hinrich Schu?tze.
2011. Active learning with amazon mechanical turk.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland.
Matthew Lease, Jessica Hullman, Jeffrey P. Bigham,
Juho Kim Michael S. Bernstein and, Walter Lasecki,
Saeideh Bakhshi, Tanushree Mitra, and Robert C.
Miller. 2013. Mechanical Turk is not anony-
mous. http://dx.doi.org/10.2139/ssrn.
2228728.
Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowl-
edge map of the virtual economy: Converting
the virtual economy into development potential.
http://www.infodev.org/en/Document.
1056.pdf, April. An InfoDev Publication.
M. Paul Lewis, Gary F. Simons, and Charles D. Fennig
(eds.). 2013. Ethnologue: Languages of the world,
seventeenth edition. http://www.ethnologue.
com.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ?09), Paris.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the Amazon Mechanical Turk
to transcribe and annotate meeting speech for extrac-
tive summarization. In Workshop on Creating Speech
and Language Data with MTurk.
George A. Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Robert Munro and Hal Tily. 2011. The start of the
art: Introduction to the workshop on crowdsourcing
technologies for language and cognition studies. In
Crowdsourcing Technologies for Language and Cog-
nition Studies, Boulder.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 207?215. Association for
Computational Linguistics.
Gabriel Parent and Maxine Eskenazi. 2011. Speaking
to the crowd: looking at past achievements in using
crowdsourcing for speech and predicting future chal-
lenges. In Proceedings Interspeech 2011, Special Ses-
sion on Crowdsourcing.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 401?409, Montre?al, Canada, June. Association
for Computational Linguistics.
Alexander J. Quinn and Benjamin B. Bederson. 2011.
Human computation: A survey and taxonomy of a
growing field. In Computer Human Interaction (CHI).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Workshop on
Creating Speech and Language Data with MTurk.
Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zal-
divar, and Bill Tomlinson. 2010. Who are the crowd-
workers?: Shifting demographics in Amazon Mechan-
ical Turk. In alt.CHI session of CHI 2010 extended
abstracts on human factors in computing systems, At-
lanta, Georgia.
Yaron Singer and Manas Mittal. 2011. Pricing mecha-
nisms for online labor markets. In Third AAAI Human
Computation Workshop (HCOMP?11).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with amazon mechanical turk. In First
IEEE Workshop on Internet Vision at CVPR.
Luis von Ahn. 2005. Human Computation. Ph.D. thesis,
School of Computer Science, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229. Association for Computational Linguistics.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of Arabic dialects. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
90
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013. Sys-
tematic comparison of professional and crowdsourced
reference translations for machine translation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta,
Georgia.
91
92
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 108?113,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Mechanical Turk to Annotate Lexicons for
Less Commonly Used Languages
Ann Irvine and Alexandre Klementiev
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218
{anni,aklement}@jhu.edu
Abstract
In this work we present results from using
Amazon?s Mechanical Turk (MTurk) to an-
notate translation lexicons between English
and a large set of less commonly used lan-
guages. We generate candidate translations for
100 English words in each of 42 foreign lan-
guages using Wikipedia and a lexicon induc-
tion framework. We evaluate the MTurk an-
notations by using positive and negative con-
trol candidate translations. Additionally, we
evaluate the annotations by adding pairs to our
seed dictionaries, providing a feedback loop
into the induction system. MTurk workers are
more successful in annotating some languages
than others and are not evenly distributed
around the world or among the world?s lan-
guages. However, in general, we find that
MTurk is a valuable resource for gathering
cheap and simple annotations for most of the
languages that we explored, and these anno-
tations provide useful feedback in building a
larger, more accurate lexicon.
1 Introduction
In this work, we make use of several free and cheap
resources to create high quality lexicons for less
commonly used languages. First, we take advan-
tage of small existing dictionaries and freely avail-
able Wikipedia monolingual data to induce addi-
tional lexical translation pairs. Then, we pay Me-
chanical Turk workers a small amount to check and
correct our system output. We can then use the up-
dated lexicons to inform another iteration of lexicon
induction, gather a second set of MTurk annotations,
and so on.
Here, we provide results of one iteration of MTurk
annotation. We discuss the feasibility of using
MTurk for annotating translation lexicons between
English and 42 less commonly used languages. Our
primary goal is to enlarge and enrich the small,
noisy bilingual dictionaries that we have for each
language. Our secondary goal is to study the quality
of annotations that we can expect to obtain for our
set of low resource languages. We evaluate the anno-
tations both alone and as feedback into our lexicon
induction system.
2 Inducing Translation Candidates
Various linguistic and corpus cues are helpful for re-
lating word translations across a pair of languages.
A plethora of prior work has exploited orthographic,
topic, and contextual similarity, to name a few
(Rapp, 1999; Fung and Yee, 1998; Koehn and
Knight, 2000; Mimno et al, 2009; Schafer and
Yarowsky, 2002; Haghighi et al, 2008; Garera et
al., 2008). In this work, our aim is to induce trans-
lation candidates for further MTurk annotation for a
large number of language pairs with varying degrees
of relatedness and resource availability. Therefore,
we opt for a simple and language agnostic approach
of using contextual information to score translations
and discover a set of candidates for further anno-
tation. Table 1 shows our 42 languages of interest
and the number of Wikipedia articles with interlin-
gual links to their English counterparts. The idea
is that tokens which tend to appear in the context
of a given type in one language should be similar
to contextual tokens of its translation in the other
language. Each word can thus be represented as a
108
Tigrinya 36 Punjabi 401
Kyrgyz 492 Somali 585
Nepali 1293 Tibetan 1358
Uighur 1814 Maltese 1896
Turkmen 3137 Kazakh 3470
Mongolian 4009 Tatar 4180
Kurdish 5059 Uzbek 5875
Kapampangan 6827 Urdu 7674
Irish 9859 Azeri 12568
Tamil 13470 Albanian 13714
Afrikaans 14315 Hindi 14824
Bangla 16026 Tagalog 17757
Latvian 22737 Bosnian 23144
Welsh 25292 Latin 31195
Basque 38594 Thai 40182
Farsi 58651 Bulgarian 68446
Serbian 71018 Indonesian 73962
Slovak 76421 Korean 84385
Turkish 86277 Ukrainan 91022
Romanian 97351 Russian 295944
Spanish 371130 Polish 438053
Table 1: Our 42 languages of interest and the number of
Wikipedia pages for each that have interlanguage links
with English.
vector of contextual word indices. Following Rapp
(1999), we use a small seed dictionary to project1
the contextual vector of a source word into the tar-
get language, and score its overlap with contextual
vectors of candidate translations, see Figure 1. Top
scoring target language words obtained in this man-
ner are used as candidate translations for MTurk an-
notation. While longer lists will increase the chance
of including correct translations and their morpho-
logical variants, they require more effort on the part
of annotators. To strike a reasonable balance, we ex-
tracted relatively short candidate lists, but allowed
MTurk users to type their own translations as well.
3 Mechanical Turk Task
Following previous work on posting NLP tasks on
MTurk (Snow et al, 2008; Callison-Burch, 2009),
we use the service to gather annotations for proposed
bilingual lexicon entries. For 32 of our 42 languages
of interest, we were able to induce lexical translation
1A simple string match is used for projection. While we
expect that more sophisticated approaches (e.g. exploiting mor-
phological analyses) are likely to help, we cannot assume that
such linguistic resources are available for our languages.
e
(1)
e
(2)
e
(3)
e
(K-1)
e
(K)
e
(i)
?
?
?
f 
(1)
f 
(2)
f 
(3)
f 
(N-1)
f 
(N)
?
?
?
dict.
project
?
?
?
?
?
?
?
?
f 
(1)
f 
(2)
f 
(N)
c
o
m
p
a
r
e
Figure 1: Lexicon induction using contextual informa-
tion. First, contextual vectors are projected using small
dictionaries and then they are compared with the target
language candidates.
candidates and post them on MTurk for annotation.
We do not have dictionaries for the remaining ten,
so, for those languages, we simply posted a set of
100 English words and asked workers for manual
translations. We had three distinct workers translate
each word.
For the 32 languages for which we proposed
translation candidates, we divided our set of 100
English words into sets of ten English words to be
completed within a single HIT. MTurk defines HIT
(Human Intelligence Task) as a self-contained unit
of work that requesters can post and pay workers a
small fee for completing. We requested that three
MTurk workers complete each of the ten HITs for
each language. For each English word within a HIT,
we posted ten candidate translations in the foreign
language and asked users to check the boxes beside
any and all of the words that were translations of the
English word. We paid workers $0.10 for complet-
ing each HIT. If our seed dictionary included an en-
try for a given English word, we included that in the
candidate list as a positive control. Additionally, we
included a random word in the foreign language as
a negative control. The remaining eight or nine can-
didate translations were proposed by our induction
system. We randomized the order in which the can-
didates appeared to workers and presented the words
as images rather than text to discourage copying and
pasting into online translation systems.
In addition to gathering annotations on candidate
109
!"#$%&'"$()#&*(%()+&
,-.%"$%&
/%0$+(%"&
1%2)#-"$%& 3-")&
4%(5$%&6%"78%#)+9&
/9$8$::$")+&
;(9)<&
Figure 2: Distribution of MTurk workers around the
world
translations, we gathered the following information
in each HIT:
? Manual translations of each English word, es-
pecially for the cases where none of our pro-
posed candidate translations were accurate
? Geographical locations via IP addresses
? How the HIT was completed: knowledge of the
languages, paper dictionary, online dictionary
? Whether the workers were native speakers of
each language (English and foreign), and for
how many years they have spoken each
4 Results
Figure 2 shows the percent of HITs that were com-
pleted in different countries. More than 60% of HITs
were completed by workers in India, more than half
of which were completed in the single city of Chen-
nai. Another 18% were completed in the United
States, and roughly 2% were completed in Romania,
Pakistan, Macedonia, Latvia, Bangladesh, and the
Philippines. Of all annotations, 54% reported that
the worker used knowledge of the two languages,
while 28% and 18% reported using paper and online
dictionaries, respectively, to complete the HITs.
Ninety-three MTurk workers completed at least
one of our HITs, and 53 completed at least two.
The average number of HITs completed per worker
was 12. One worker completed HITs for 17 differ-
ent languages, and nine workers completed HITs in
more than three languages. Of the ten prolific work-
ers, one was located in the United States, one in the
United Kingdom, and eight in India. Because we
posted each HIT three times, the minimum number
of workers per language was three. Exactly three
workers completed all ten HITs posted in the fol-
lowing languages: Kurdish, Maltese, Tatar, Kapam-
pangan, Uzbek, and Latvian. We found that the av-
erage number of workers per language was 5.2. Ten
distinct workers (identified with MTurk worker IDs)
completed Tamil HITs, and nine worked on the Farsi
HITs.
4.1 Completion Time
Figure 3 shows the time that it took for our HITs
for 37 languages to be completed on MTurk. The
HITs for the following languages were posted for a
week and were never completed: Tigrinya, Uighur,
Tibetan, Kyrgyz, and Kazakh. All five of the un-
completed HIT sets required typing annotations, a
more time consuming task than checking transla-
tion candidates. Not surprisingly, languages with
many speakers (Hindi, Spanish, and Russian) and
languages spoken in and near India (Hindi, Tamil,
Urdu) were completed very quickly. The languages
for which we posted a manual translation only HIT
are marked with a * in Figure 3. The HIT type does
not seem to have affected the completion time.
4.2 Annotation Quality
Lexicon Check Agreement. Figure 4 shows the
percent of positive control candidate translations
that were checked by the majority of workers (at
least two of three). The highest amounts of agree-
ment with the controls were for Spanish and Polish,
which indicates that those workers completed the
HITs more accurately than the workers who com-
pleted, for example, the Tatar and Thai HITs. How-
ever, as already mentioned, the seed dictionaries are
very noisy, so this finding may be confounded by
discrepancies in the quality of our dictionaries. The
noisy dictionaries also explain why agreement with
the positive controls is, in general, relatively low.
We also looked at the degree to which workers
agreed upon negative controls. The average per-
cent agreement between the (majority of) workers
and the negative controls over all 32 languages is
only 0.21%. The highest amount of agreement with
negative controls is for Kapampangan and Turkmen
(1.28% and 1.26%, respectively). These are two of
110
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
120	 ?
140	 ?
160	 ?
Hin
di*	 ?
Spa
nish
	 ?
Rus
sian
	 ?
Tam
il	 ?
Urd
u*	 ?
Tur
kish
	 ?
Rom
ania
n	 ?
Ukr
aina
n*	 ?
Latv
ian	 ? Poli
sh	 ?
Alb
ania
n	 ?
Afri
kaa
ns	 ?
Bul
gar
ian	 ? Fars
i	 ?
Ind
one
sian
	 ?
We
lsh	 ?
Slov
ak	 ?
Tag
alog
	 ?
Ma
ltes
e	 ?
Ser
bian
	 ?
Uzb
ek	 ? Tha
i	 ?
Bos
nian
	 ?
Kor
ean
	 ?
Nep
ali*
	 ?
Irish
	 ?
Mo
ngo
lian
*	 ?
Aze
ri	 ?
Pun
jabi
	 ?
Ban
gla	 ? Tata
r	 ?
Kap
am
pan
gan
	 ?
Kur
dish
	 ?
La?	 ?
Tur
kme
n	 ?
Som
ali	 ?
Bas
que
	 ?
Hou
rs	 ?
Time	 ?to	 ?complete	 ?HITs	 ?
Time	 ?to	 ?first	 ?hit	 ?
Figure 3: Number of hours HITs posted on MTurk before completion; division of the time between posting and the
completion of one HIT and the time between the completion of the first and last HIT shown. HITs that required lexical
translation only (not checking candidate translations) are marked with an *.
the languages for which there was little agreement
with the positive controls, substantiating our claim
that those HITs were completed less accurately than
for other languages.
Manual Translation Agreement. For each En-
glish word, we encouraged workers to manually pro-
vide one or more translations into the foreign lan-
guage. Figure 5 shows the percent of English words
for which the MTurk workers provided and agreed
upon at least one manual translation. We defined
agreement as exact string match between at least
two of three workers, which is a conservative mea-
sure, especially for morphologically rich languages.
As shown, there was a large amount of agreement
among the manual translations for Ukrainian, Farsi,
Thai, and Korean. The MTurk workers did not pro-
vide any manual translations at all for the following
languages: Somali, Kurdish, Turkmen, Uzbek, Ka-
pampangan, and Tatar.
It?s easy to speculate that, despite discouraging
the use of online dictionaries and translation systems
by presenting text as images, users reached this high
level of agreement for manual translations by using
the same online translation systems. However, we
searched for 20 of the 57 English words for which
the workers agreed upon a manually entered Russian
translation in Google translate, and we found that the
Russian translation was the top Google translation
for only 11 of the 20 English words. Six of the Rus-
sian words did not appear at all in the list of trans-
lations for the given English word. Thus, we con-
clude that, at least for some of our languages of in-
terest, MTurk workers did provide accurate, human-
generated lexical translations.
4.3 Using MTurk Annotations in Induction
To further test the usefulness of MTurk generated
bilingual lexicons, we supplemented our dictionar-
ies for each of the 37 languages for which we gath-
ered MTurk annotations with translation pairs that
workers agreed were good (both chosen from the
candidate set and manually translated). We com-
pared seed dictionaries of size 200 with those sup-
plemented with, on average, 69 translation pairs. We
found an average relative increase in accuracy of
our output candidate set (evaluated against complete
available dictionaries) of 53%. This improvement is
further evidence that we are able to gather high qual-
ity translations from MTurk, which can assist the
lexicon induction process. Additionally, this shows
that we could iteratively produce lexical translation
candidates and have MTurk workers annotate them,
supplementing the induction dictionaries over many
iterations. This framework would allow us to gener-
111
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
Spa
nis
h	 ?
Pol
ish
	 ?
Bos
nia
n	 ?
Rom
ani
an	 ? Iris
h	 ?
Bul
gar
ian
	 ?
Tag
alo
g	 ?
Rus
sian
	 ?
Afr
ika
ans
	 ?
We
lsh
	 ?
Tam
il	 ?
Ma
ltes
e	 ?
Tur
kish
	 ?
Lat
via
n	 ?
Pun
jab
i	 ?
Ind
one
sian
	 ?
Aze
ri	 ?
Slo
vak
	 ?
La?	 ?
Alb
ani
an	 ?
Uzb
ek	 ?
Bas
que
	 ?
Ser
bia
n	 ?
Tur
km
en	 ?
Kor
ean
	 ?
Ban
gla
	 ?
Som
ali	 ?
Kur
dis
h	 ?
Far
si	 ?
Kap
am
pan
gan
	 ?
Tat
ar	 ? Tha
i	 ?
Per
cen
t	 ?ag
ree
me
nt	 ?o
n	 ?	 ?	 ?
pos
i?v
e	 ?c
ont
rols
	 ?
Figure 4: Percent of positive control candidate translations for which two or three workers checked as accurate.
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
90	 ?
100	 ?
Ukr
ain
an*
	 ?
Far
si	 ? Tha
i	 ?
Kor
ean
	 ?
Iris
h	 ?
Ma
ltes
e	 ?
Alb
ani
an	 ?
Slo
vak
	 ?
Lat
via
n	 ?
Tur
kish
	 ?
Ser
bia
n	 ?
We
lsh
	 ?
Afr
ika
ans
	 ?
Ind
one
sian
	 ?
Hin
di*
	 ?
Pol
ish
	 ?
Rus
sian
	 ?
Urd
u*	 ?
Tag
alo
g	 ?
Rom
ani
an	 ?
Mo
ngo
lian
*	 ?
Spa
nis
h	 ?
Bul
gar
ian
	 ?
Bos
nia
n	 ?
Aze
ri	 ?
Ne
pal
i*	 ?
La?	 ? Tam
il	 ?
Pun
jab
i	 ?
Ban
gla
	 ?
Bas
que
	 ?
Som
ali	 ?
Kur
dis
h	 ?
Tur
km
en	 ?
Uzb
ek	 ?
Kap
am
pan
gan
	 ?
Tat
ar	 ?Per
cen
t	 ?of
	 ?En
glis
h	 ?w
ord
s	 ?w
ith
	 ?
agr
eed
	 ?up
on	 ?
	 ?ma
nua
l	 ?tr
ans
la?
ons
	 ?
Figure 5: Percent of 100 English words for which at least two of three MTurk workers provided at least one matching
manual translation; HITs that required lexical translation only (not checking candidate translations) are marked with
an *.
ate very large and high quality dictionaries starting
with a very small set of seed translation pairs.
5 Conclusion
The goal of this work was to use Amazon?s Mechan-
ical Turk to collect and evaluate the quality of trans-
lation lexicons for a large set of low resource lan-
guages. In order to make the annotation task easier
and maximize the amount of annotation given our
budget and time constraints, we used contextual sim-
ilarity along with small bilingual dictionaries to ex-
tract a set of translation candidates for MTurk anno-
tation. For ten of our languages without dictionaries,
we asked workers to type translations directly. We
were able to get complete annotations of both types
quickly for 37 of our languages. The other five lan-
guages required annotations of the latter type, which
may explain why they remained unfinished.
We used annotator agreement with positive and
negative controls to assess the quality of generated
lexicons and provide an indication of the relative
difficulty of obtaining high quality annotations for
each language. Not surprisingly, annotation agree-
ment tends to be low for those languages which are
especially low resource, as measured by the num-
ber of Wikipedia pages. Because there are relatively
few native speakers of these languages in the on-
line community, those HITs were likely completed
by non-native speakers. Finally, we demonstrated
that augmenting small seed dictionaries with the ob-
tained lexicons substantially impacts contextual lex-
icon induction with an average relative gain of 53%
in accuracy across languages.
In sum, we found that the iterative approach of au-
112
tomatically generating noisy annotation and asking
MTurk users to correct it to be an effective means of
obtaining supervision. Our manual annotation tasks
are simple and annotation can be obtained quickly
for a large number of low resource languages.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons mechan-
ical turk. In Proceedings of EMNLP.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of ACL, pages 414?420.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2008. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In Proceedings
of CoNLL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
AAAI.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519?526.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity mea-
sures and bridge languages. In Proceedings of CoNLL,
pages 146?152.
Rion Snow, Brendan OConnor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP.
113
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 75?78,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Processing Informal, Romanized Pakistani Text Messages
Ann Irvine and Jonathan Weese and Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Abstract
Regardless of language, the standard character
set for text messages (SMS) and many other
social media platforms is the Roman alphabet.
There are romanization conventions for some
character sets, but they are used inconsistently
in informal text, such as SMS. In this work, we
convert informal, romanized Urdu messages
into the native Arabic script and normalize
non-standard SMS language. Doing so pre-
pares the messages for existing downstream
processing tools, such as machine translation,
which are typically trained on well-formed,
native script text. Our model combines infor-
mation at the word and character levels, al-
lowing it to handle out-of-vocabulary items.
Compared with a baseline deterministic ap-
proach, our system reduces both word and
character error rate by over 50%.
1 Introduction
There are many reasons why systematically process-
ing informal text, such as Twitter posts or text mes-
sages, could be useful. For example, during the Jan-
uary 2010 earthquake in Haiti, volunteers translated
Creole text messages that survivors sent to English
speaking relief workers. Machine translation (MT)
could supplement or replace such crowdsourcing ef-
forts in the future. However, working with SMS data
presents several challenges. First, messages may
have non-standard spellings and abbreviations (?text
speak?), which we need to normalize into standard
language. Second, many languages that are typically
written in a non-Roman script use a romanized ver-
sion for SMS, which we need to deromanize. Nor-
malizing and deromanizing SMS messages would
allow us to use existing MT engines, which are typ-
ically trained on well-formed sentences written in
their native-script, in order to translate the messages.
With this work, we use and release a corpus of
1 million (4, 195 annotated) anonymized text mes-
sages sent in Pakistan1. We deromanize and normal-
ize messages written in Urdu, although the general
approach is language-independent. Using Mechan-
ical Turk (MTurk), we collect normalized Arabic
script annotations of romanized messages in order to
both train and evaluate a Hidden Markov Model that
automates the conversion. Our model drastically
outperforms our baseline deterministic approach and
its performance is comparable to the agreement be-
tween annotators.
2 Related Work
There is a strong thread of research dedicated to nor-
malizing Twitter and SMS informal English (Sproat
et al, 2001). Choudhury et al (2007) use a super-
vised English SMS dataset and build a character-
level HMM to normalize individual tokens. Aw et
al. (2006) model the same task using a statistical MT
system, making the output context-sensitive at the
cost of including a character-level analysis. More
recently, Han and Baldwin (2011) use unsupervised
methods to build a pipeline that identifies ill-formed
English SMS word tokens and builds a dictionary
of their most likely normalized forms. Beaufort et
al. (2010) use a large amount of training data to su-
pervise an FST-based French SMS normalizer. Li
and Yarowsky (2008) present methods that take ad-
vantage of monolingual distributional similarities to
identify the full form of abbreviated Chinese words.
One challenge in working with SMS data is that pub-
lic data is sparse (Chen and Kan, 2011). Translit-
eration is well-studied (Knight and Graehl, 1997;
Haizhou et al, 2004; Li et al, 2010) and is usually
viewed as a subproblem of MT.
With this work, we release a corpus of SMS mes-
sages and attempt to normalize Urdu SMS texts. Do-
ing so involves the same challenges as normalizing
English SMS texts and has the added complexity
that we must also deromanize, a process similar to
the transliteration task.
1See http://www.cs.jhu.edu/?anni/papers/
urduSMS/ for details about obtaining the corpus.
75
!"#$#%&'()*++&$*! ,#-./(0&1&%($&#2(13(4&5&5('3$6(7$4&(1*(8&"1&#(13("1#(1*9(:1&'3(+1&2&+1(8&"1('39();<=>?@=!
"#$%&#%'! ()*&!
+',-./#$01#2.$! !"#$!!"#$!%&!'#(!)%*!+!#,-*!%&!'(")*+!%&!'&,!%&!+!%./! 0#1#2!-*+!%*!
3$%4056!7)#$54#2.$! 86')'!#)'!9.&!:'.:4';!5''/5!'<')9.$'!05!5=&*90$%>!.?!5=&*9!0=5!%..*!
Figure 1: Example of SMS with MTurk annotations
3 Data and Annotation
Our Pakistani SMS dataset was provided by the
Transnational Crisis Project, and it includes 1 mil-
lion (724,999 unique) text messages that were sent
in Pakistan just prior to the devastating July 2010
floods. The messages have been stripped of all
metadata including sender, receiver, and timestamp.
Messages are written in several languages, though
most are in Urdu, English, or a combination of the
two. Regardless of language, all messages are com-
posed in the Roman alphabet. The dataset contains
348,701 word types, 49.5% of which are singletons.
We posted subsets of the SMS data to MTurk to
perform language identification, followed by dero-
manization and normalization on Urdu messages.
In the deromanization and normalization task, we
asked MTurk workers to convert all romanized
words into script Urdu and use full, non-abbreviated
word forms. We applied standard techniques for
eliminating noise in the annotation set (Callison-
Burch and Dredze, 2010) and limited annotators to
those in Pakistan. We also asked annotators to in-
dicate if a message contained private, sensitive, or
offensive material, and we removed such messages
from our dataset.
We gathered deromanization and normalization
MTurk annotations for 4,195 messages. In all ex-
periments, we use 3,695 of our annotated SMS texts
for training and 500 for testing. We found that 18%
of word tokens and 44% of word types in the test
data do not appear in the training data. An example
of a fully annotated SMS is shown in Figure 1.
Figure 2 shows that, in general, productive MTurk
annotators also tend to produce high quality annota-
tions, as measured by an additional round of MTurk
annotations which asked workers to choose the best
annotation among the three we gathered. The raw
average annotator agreements as measured by char-
acter and word level edit distance are 40.5 and 66.9,
respectively. However, the average edit distances
0 100 200 300 400
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of MTurk HITs completed
P
e
r
c
e
n
t
 
o
f
 
a
n
n
o
t
a
t
i
o
n
s
 
v
o
t
e
d
 
b
e
s
t
Good Performance
Mediocre Performance
Poor Performance
Figure 2: Productivity vs. percent of annotations voted
best among three deromanizations gathered on MTurk.
!"#$%&' ()*+,$-+.),/'01#2342,"56' 7,89$/:'
'!"#' ;#+*'0<6';+#+*'0<6';+#*'0=6';+#2*'0>6' 8#+"2'
'$%&'#'' ;),/+'0?6';,'/+'0@6';,/+'0<6';),'/+'0=6' A:$":'
'()"*)+'' B4/#+'0>6'!"#$0>6'B)/+#)'0>6'B4/#),'0>6' )&:2#'%2)%92'
','-'' ;:4/:'0<=6';$'0>6';:4/'0>6' :+%%5'
''./0'' C48,)'0>6'8+,C)'0>6' D#2E5'
'123$4'' F+&2$,'0G6'F+&2,'0G6'F++&2$,'0@6'F+&+$,'0@6'F&2$,'0<6' ":$&H":+&'
Figure 3: Urdu words romanized in multiple ways. The
Urdu word for ?2? is pronounced approximately ?du.?
between ?good? MTurk workers (at least 50% of a
worker?s messages are voted best) and the deroman-
ization which was voted best (when the two are dif-
ferent) are 25.1 (character) and 53.7 (word).
We used an automatic aligner to align the words
in each Arabic script annotation to words in the orig-
inal romanized message. The alignments show an
average fertility of 1.04 script words per romanized
word. Almost all alignments are one-to-one and
monotonic. Since there is no reordering, the align-
ment is a simplified case of word alignment in MT.
Using the aligned dataset, we examine how Urdu
words are romanized. The average entropy for non-
singleton script word tokens is 1.49 bits. This means
it is common for script words to be romanized in
multiple ways (4.2 romanizations per script word on
average). Figure 3 shows some examples.
4 Deromanization and Normalization
In order to deromanize and normalize Urdu SMS
texts in a single step, we use a Hidden Markov
Model (HMM), shown in Figure 4. To estimate the
probability that one native-script word follows an-
76
???? ??? ???????
walo kia soratehal
Figure 4: Illustration of HMM with an example from
SMS data. English translation: ?What?s the situation??
other, we use a bigram language model (LM) with
add-1 smoothing (Lidstone, 1920) and compare two
sources of LM training data.
We use two sources of data to estimate the prob-
ability of a romanized word given a script word:
(1) a dictionary of candidates generated from auto-
matically aligned training data, (2) a character-based
transliteration model (Irvine et al, 2010).
If r is a romanized word and u is a script Urdu
word, the dictionary-based distribution, pDICT(r|u),
is given by relative frequency estimations over the
aligned training data, and the transliteration-based
distribution, pTRANS(r|u), is defined by the transliter-
ation model scores. We define the model?s emission
probability distribution as the linear interpolation of
these two distributions:
pe(r|u) = (1? ?)pDICT(r|u) + ?pTRANS(r|u)
When ? = 0, the model uses only the dictionary,
and when ? = 1 only the transliterations.
Intuitively, we want the dictionary-based model to
memorize patterns like abbreviations in the training
data and then let the transliterator take over when a
romanized word is out-of-vocabulary (OOV).
5 Results and discussion
In the eight experiments summarized in Table 1, we
vary the following: (1) whether we estimate HMM
emissions from the dictionary, the transliterator, or
both (i.e., we vary ?), (2) language model training
data, and (3) transliteration model training data.
Our baseline uses an Urdu-extension of the Buck-
walter Arabic deterministic transliteration map.
Even our worst-performing configuration outper-
forms this baseline by a large margin, and the best
configuration has a performance comparable to the
agreement among good MTurk workers.
LM Translit ? CER WER
1 News ? Dict 41.5 63.3
2 SMS ? Dict 38.2 57.1
3 SMS Eng Translit 33.4 76.2
4 SMS SMS Translit 33.3 74.1
5 News SMS Both 29.0 58.1
6 News Eng Both 28.4 57.2
7 SMS SMS Both 25.0 50.1
8 SMS Eng Both 24.4 49.5
Baseline: Buckwalter Determ. 64.6 99.9
Good MTurk Annotator Agreement 25.1 53.7
Table 1: Deromanization and normalization results on
500 SMS test set. Evaluation is by character (CER) and
word error rate (WER); lower scores are better. ?LM?
indicates the data used to estimate the language model
probabilities: News refers to Urdu news corpus and SMS
to deromanized side of our SMS training data. ?Translit?
column refers to the training data that was used to train
the transliterator: SMS; SMS training data; Eng; English-
Urdu transliterations. ? refers to the data used to estimate
emissions: transliterations, dictionary entries, or both.
Unsurprisingly, using the dictionary only (Exper-
iments 1-2) performs better than using translitera-
tions only (Experiments 3-4) in terms of word error
rate, and the opposite is true in terms of character
error rate. Using both the dictionary derived from
the SMS training data and the transliterator (Experi-
ments 5?8) outperforms using only one or the other
(1?4). This confirms our intuition that using translit-
eration to account for OOVs in combination with
word-level learning from the training data is a good
strategy2.
We compare results using two language model
training corpora: (1) the Urdu script side of our
SMS MTurk data, and (2) the Urdu side of an Urdu-
English parallel corpus,3 which contains news-
domain text. We see that using the SMS MTurk data
(7?8) outperforms the news text (5?6). This is due to
the fact that the news text is out of domain with re-
spect to the content of SMS texts. In future work, we
plan to mine Urdu script blog and chat data, which
may be closer in domain to the SMS texts, providing
better language modeling probabilities.
2We experimented with different ? values on held out data
and found its value did not impact system performance signifi-
cantly unless it was set to 0 or 1, ignoring the transliterations or
dictionary. We set ? = 0.5 for the rest of the experiments.
3LDC2006E110
77
Training Freq. bins Length Diff. bins
Bin CER WER Bin CER WER
100+ 9.8 14.8 0 23.5 43.3
10?99 15.2 22.1 1, 2 29.1 48.7
1?9 27.5 37.2 -1, -2 42.3 70.1
0 73.5 96.6 ?3 100.3 100.0
?-3 66.4 87.3
Table 2: Results on tokens in the test set, binned by train-
ing frequency or difference in character length with their
reference. Length differences are number of characters
in romanized token minus the number of characters in its
deromanization. ? = 0.5 for all.
We compare using a transliterator trained on ro-
manized/deromanized word pairs extracted from the
SMS text training data with a transliterator trained
on English words paired with their Urdu translitera-
tions and find that performance is nearly equivalent.
The former dataset is noisy, small, and in-domain
while the latter is clean, large, and out-of-domain.
We expect that the SMS word pairs based translit-
erator would outperform the English-Urdu trained
transliterator given more, cleaner data.
To understand in more detail when our system
does well and when it does not, we performed ad-
ditional experiments on the token level. That is, in-
stead of deromanizing and normalizing entire SMS
messages, we take a close look at the kinds of ro-
manized word tokens that the system gets right and
wrong. We bin test set word tokens by their frequen-
cies in the training data and by the difference be-
tween their length (in characters) and the length of
their reference deromanization. Results are given in
Table 2. Not surprisingly, the system performs better
on tokens that it has seen many times in the training
data than on tokens it has never seen. It does not
perform perfectly on high frequency items because
the entropy of many romanized word types is high.
The system also performs best on romanized word
types that have a similar length to their deromanized
forms. This suggests that the system is more suc-
cessful at the deromanization task than the normal-
ization task, where lengths are more likely to vary
substantially due to SMS abbreviations.
6 Summary
We have defined a new task: deromanizing and nor-
malizing SMS messages written in non-native Ro-
man script. We have introduced a unique new anno-
tated dataset that allows exploration of informal text
for a low resource language.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING/ACL.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of ACL.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In NAACL-NLT Workshop on Creating Speech
and Language Data With Mechanical Turk.
Tao Chen and Min-Yen Kan. 2011. Creating a live, pub-
lic short message service corpus: The NUS SMS cor-
pus. Computation and Language, abs/1112.2468.
Monojit Choudhury, Vijit Jain Rahul Saraf, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. In International Journal on Docu-
ment Analysis and Recognition.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of ACL.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
Proceedings of the Association for Machine Transla-
tion in the America, AMTA ?10.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Zhifei Li and David Yarowsky. 2008. Unsupervised
translation induction for Chinese abbreviations using
monolingual corpora. In Proceedings of ACL/HLT.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Report of NEWS 2010 translitera-
tion generation shared task. In Proceedings of the ACL
Named Entities WorkShop.
George James Lidstone. 1920. Note on the general case
of the Bayes-Laplace formula for inductive or a poste-
riori probabilities. Transactions of the Faculty of Ac-
tuaries, 8:182?192.
Richard Sproat, Alan W. Black, Stanley F. Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech & Language, pages 287?333.
78
Workshop on Computational Linguistics for Literature, pages 64?68,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Digitizing 18th-Century French Literature:
Comparing transcription methods for a critical edition text
Ann Irvine
Computer Science Dept.
Johns Hopkins University
Baltimore, MD
anni@jhu.edu
Laure Marcellesi
French and Italian Dept.
Dartmouth College
Hanover, NH
laure.marcellesi@dartmouth.edu
Afra Zomorodian
The D. E. Shaw Group
New York, NY
Abstract
We compare four methods for transcribing
early printed texts. Our comparison is through
a case-study of digitizing an eighteenth-
century French novel for a new critical edition:
the 1784 Lettres ta??tiennes by Jose?phine de
Monbart. We provide a detailed error analy-
sis of transcription by optical character recog-
nition (OCR), non-expert humans, and expert
humans and weigh each technique based on
accuracy, speed, cost and the need for schol-
arly overhead. Our findings are relevant to
18th-century French scholars as well as the
entire community of scholars working to pre-
serve, present, and revitalize interest in litera-
ture published before the digital age.
1 Introduction
Preparing a text for modern publication involves the
following: (1) digitizing1 a printed version of the
text, and (2) supplementing the original content with
new scholarly contributions such as a critical intro-
duction, annotations, and a thorough bibliography.
The second task requires a high degree of expertise
and academic insight and the first does not. How-
ever, scholars working on such projects often spend
large amounts of time transcribing literature from
scratch, instead of focusing on skilled contributions.
In this paper, we present an analysis of our efforts
using alternative methods, other than highly skilled
scholars themselves, to transcribe a scanned image
of a novel into a modifiable, searchable document.
We compare four different methods of transcription
with a gold standard and evaluate each for accuracy,
speed, and cost-effectiveness. Choosing an appro-
1In this work, digitizing means transcribing an image into a
modifiable, searchable file of unicode characters.
priate transcription method may save scholars time
and allow them to focus on critical contributions.
First published in 1784, Jose?phine de Monbart?s
Lettres ta??tiennes is an epistolary novel dramatiz-
ing the European colonial takeover of the newly-
encountered island of Tahiti from the fictional point
of view of a young Tahitian woman. While most
works of the time painted a fictional Tahitian par-
adise of uninhibited sexuality, this novel offers a
singular anti-colonial critique by grounding it in the
suffering of the female body. We describe our work
transcribing the second edition of the novel, which is
written in French and was published in Paris, with-
out date (probably 1786). The text is comprised of
156 pages, which are split into two volumes.
There are many off-the-shelf (OTS) natural lan-
guage processing (NLP) tools available for French,
including optical character recognition (OCR),
context-sensitive spell checking, and machine trans-
lation. Additionally, French is a widely spoken lan-
guage in the world today and it is often possible to
recruit French speakers to do transcription and an-
notation. However, the early-modern (18th-century)
form of the language varies substantially from the
modern form, which is used to train OTS French
tools and is what non-domain-expert transcribers are
familiar with. Differences between the modern and
early-modern forms of the language include orthog-
raphy, lexical choice, and morphological patterns.
An additional challenge is that our transcriptions
are based on a copied version of the bound text avail-
able at the Bibliothe`que nationale de France. This
common scenario introduces the challenge of noise,
or ink marks which are not part of the text. Scattered
dots of ink may result in punctuation and character
accenting errors, for example.
In this paper, we compare the accuracy, speed, and
64
cost of using several different methods to transcribe
Lettres tahitiennes. In Section 2 we describe the
transcription methods, and in Section 3 we present a
detailed analysis of the types of errors made by each.
We also provide a discussion of the difficulty of
post-editing the output from each transcriber. Sec-
tion 4 gives an overview of prior work in the area and
Section 5 a practical conclusion, which may inform
scholars in the beginning stages of similar projects.
2 Methods
We compare four sources of transcription for 30
pages of the novel with one gold standard:
? OTS French OCR output
? Non-expert French speakers on Amazon?s Me-
chanical Turk (MTurk)
? Non-expert undergraduate students in the hu-
manities, closely supervised by the expert
? Professional transcription service
? Gold standard: early-modern French literature
scholar and editor of the critical edition
Given PDF images of a copy of the novel, each
source transcribed the same 30 pages2. The pages
are a representative sample from each of the two vol-
umes of the text.
We used OTS Abbyy Finereader OCR software,
which is trained on modern French text and has a
fixed cost of $99.
Three MTurk workers transcribed each page of
text, and the domain expert chose the best transcrip-
tion of each page. In future work, we could have
another round of MTurk workers choose the best
transcription among several MTurk outputs, which
has been shown to be effective in other NLP tasks
(Zaidan and Callison-Burch, 2011). We paid each
MTurk worker $0.10 to transcribe a single page.
Two closely supervised undergraduate students
transcribed the novel3, including the 30 test pages.
The cost per page per student was about $0.83.
Our group also hired a professional company to
transcribe the entire novel, which charged a fixed
cost of $1000, or about $3.21 per page.
The early-modern French literature domain-
expert also transcribed the 30 test pages from
2Each page is in the original duodecimo format and contains
about 150 word tokens.
3One student transcribed volume 1, the other volume 2.
scratch, and this transcription was used as the gold
standard for measuring accuracy.
Because the critical edition text should be as faith-
ful as possible to the original text, with no alteration
to the spelling, syntax, capitalization, italicization,
and paragraph indentation, we define as errors to be:
? an incomplete transcription
? missing or added words, letters, or characters
? a word transcribed incorrectly
? capitalization, bold, italics not matching the
original text
? incorrect formatting, including missing or
added paragraph indentations and footnote dis-
tinctions
In Section 3, we present a quantitative and quali-
tative analysis of the types of errors made by each of
our transcription methods.
3 Results and Error Analysis
Table 1 lists the error rate for each transcriber.
3.1 S/F errors
One of the most common errors made by all four
transcription methods is confusing the letter ? (or
long s), which is common in early-modern French
but doesn?t appear in modern French, with the letter
f. Figure 1 shows examples of phrases in the original
document that include both characters. These ex-
amples illustrate how familiarity with the language
may impact when transcription errors are made. All
three human transcribers (MTurk workers, students,
professionals) confused an f for an ? in (b). Interest-
ingly, the phrase in (b) would never be used in mod-
ern French, so the transcribers, not recognizing the
overall meaning of the sentence and wary of ?miss-
ing? a ?, incorrectly wrote seront instead of feront.
In contrast, the phrase in (a) is rare but does exist
in modern French. The MTurk worker and profes-
sional transcriber correctly transcribed feront but the
student, who probably didn?t know the phrase, tran-
scribed the word as seront.
The OCR system trained on modern French did
not recognize ? at all. In most cases, it transcribed
the letter as an f, but it sometimes chose other letters,
such as t, i, or v, in order to output French words that
exist in its dictionary. Although it may have been
65
Wednesday, February 22, 2012
Figure 1: Correct transcription: (a) ils feront l?aumo?ne
(give alms). The student incorrectly transcribed feront as
seront. (b) ils ne se feront nul scrupule (they will have no
qualms). All four alternative transcription sources incor-
rectly transcribed feront as seront.
Figure 2: Correct transcription: Outre qu?elles me parois-
sent toutes dans la pre?miere jeunesse, elles ont des graces
qui vous ravissent avant d?avoir songe? a` examiner, si elles
e?toient belles (Besides [these women] appearing to me in
the prime of youth, they have graces that delight you be-
fore you even think of considering whether they are beau-
tiful. Transcribers made both conjugation (paraissent vs.
paroissent) and accenting (pre?miere vs. premie`re) mod-
ernization errors in this passage.
possible to train the OCR system on early-modern
French, the very slight difference between the char-
acter strokes means that disambiguating between f
and ? would likely remain a difficult task.
3.2 Modernization errors
Eighteenth-century French is understandable
by speakers of modern French, but there are
a few differences. In addition to the absence
of the letter ?, modern French conjugates
verbs with ?ai,?ais,?ait,?aient instead of
?oi,?ois,?oit,?oient and follows stricter rules
that no longer allow for variations in spelling or
accenting. Figure 2 shows examples of both. In
general, the authors of modern critical editions seek
to maintain original spellings so that future scholars
can work as close to the original text as possible,
even if the original work includes typos, which
we have seen. However, our human transcribers
incorrectly modernized and ?fixed? many original
spellings. This is likely due to the fact that it is
hard for a human transcriber who is familiar with
the language to not ?correct? a word into its modern
form. We observed this across all human tran-
scribers. For example, our professional transcriber
transcribed premie`re instead of pre?miere and one
MTurk worker transcribed chez instead of che?s. The
OCR model, which is trained on modern French,
is also biased toward modern spellings. Most of
its modernization errors were related to accents.
For example, it transcribed graces as gra?ces and
differentes as diffe?rentes.
Some modernization errors occur systematically
and, thus, are easy to automatically correct after the
initial transcription is complete. For example, all
?aient word endings could be changed to ?oient.
This is not true for all modernization errors.
3.3 Errors from page noise
Since all of our transcribers worked from a scan of
a copy of the original book held at the Bibliothe`que
nationale de France, noise in the form of small dots,
originally bits of ink, appears on the pages. These
small dots are easily confused with diacritics and
punctuation. Our human transcribers made such er-
rors very infrequently. However, this type of noise
greatly affected the output of the OCR system. In
addition to mistaking this type of noise for punctua-
tion, sometimes it affected the recognition of words.
In once instance, visages became ylfygc because of
small dots that appeared below the v and a4.
3.4 Formatting errors
We asked all transcribers to maintain the original
formatting of the text, including paragraph indenta-
tions, footnotes, and font styles. However, because
of limitations inherent to the MTurk task design in-
terface, we were unable to collect anything but plain,
unformatted text from those transcribers. In general,
our other human transcribers were able to accurately
maintain the format of the original text. The OCR
output also made formatting mistakes, particularly
bold and italicized words.
3.5 Other errors
Both humans and the OCR system made an assort-
ment of additional errors. For example, two MTurk
workers failed to turn off the English automatic spell
correctors in their text editors, which resulted in let-
tre becoming letter and dont becoming don?t.
3.6 Scholar overhead
Table 1 lists the average number of errors per page
for each transcription method. In addition to consid-
4In this example, an ? was also transcribed as an f
66
Error OCR MTurk Prof. Stud.
Modernization 26.29 2.82 0.71 0.46
Noise 7.68 0.0 0.32 0.21
Formatting 1.96 0.82 0.36 0.0
Total 35.93 3.86 1.39 0.71
Table 1: Mean number of errors per page, by error type
and transcription method. The total includes the error
types shown as well as an assortment of other errors.
ering the error rate of each, we found that it is critical
to consider (a) the effort that the scholar must ex-
ert to correct, or post-edit, a non-expert?s transcrip-
tion, and (b) the amount of overhead required by the
scholar to gather the transcriptions.
All errors are not equally serious. We found
that the expert scholar had an easier time correct-
ing some errors in post-editing than others. For ex-
ample, modernization errors may be corrected auto-
matically or in a single read through the transcrip-
tion, without constantly consulting the original text.
In contrast, correcting formatting errors is very time
consuming. Similarly, correcting errors resulting
from page noise requires the scholar to closely com-
pare punctuation in the original text with that of the
transcription and takes a lot of time.
Previous research on gathering and using non-
expert annotations using MTurk (Snow et al, 2008;
Callison-Burch and Dredze, 2010; Zaidan and
Callison-Burch, 2011) has been optimistic. How-
ever, that work has failed to account for the time and
effort required to compose, post, monitor, approve,
and parse MTurk HITs (human intelligence tasks).
In our exploration, we found that the time required
by our expert scholar to gather MTurk annotations
nearly offsets the cost savings that result from us-
ing it instead of local student or professional tran-
scribers. Similarly, the scholar had to provide some
supervision to the student transcribers. The profes-
sional transcription service, in contrast, though more
expensive than the other high quality (non-OCR)
methods, required no oversight on the part of the
scholar. After using all methods to transcribe 30
pages of Lettres ta??tiennes and critically comparing
the costs and benefits of each, we had the profes-
sional transcription service complete the project and
our expert French literature scholar has based a new
critical edition of the text on this transcription.
4 Background
Snow et al (2008) gathered annotations on MTurk in
order to supervise a variety of NLP tasks. In general,
they found a high degree of annotator agreement and
inspired a plethora of research on using non-expert
annotations for additional tasks in language process-
ing (Callison-Burch and Dredze, 2010).
OCR has been an active area of research in NLP
for decades (Arica and Yarman-Vural, 2001). Re-
cent work has acknowledged that post-editing OCR
output is an important engineering task but generally
assumes large amounts of training data and does not
attempt to maintain text format (Kolak et al, 2003).
As we described, for our application, transcribing
all content and formatting, including footnotes, ref-
erences, indentations, capitalization, etc. is crucial.
Furthermore, OCR output quality was so low that
post-editing it would have required more work than
transcribing from scratch. We did not attempt to
train the OCR since, even if it had recognized ? and
learned an appropriate language model, the format-
ting and noise errors would have remained.
5 Future Work and Conclusions
In Section 3.2, we mentioned that it may be possible
to automatically post-edit transcriptions and correct
systematic modernization errors. The same may be
true for, for example, some types of typos. This type
of post-editing could be done manually or automati-
cally. One potential automatic approach is to train a
language model on the first transcription attempt and
then use the model to identify unlikely segments of
text. We plan to pursue this in future work.
Although we hoped that using MTurk or OCR
would provide an inexpensive, high-quality first
round transcription, we found that we preferred to
use student and professional transcribers.The trade-
offs between speed and accuracy and between low
cost and overhead time were not worthwhile for our
project. If a scholar were working with a larger text
or tighter budget, investing the time and effort to use
MTurk could prove worthwhile. Using an OCR sys-
tem would demand extensive training to the text do-
main as well as post-editing. This paper enumerates
important challenges, costs, and benefits of several
transcription approaches, which are worthy of con-
sideration by scholars working on similar projects.
67
References
N. Arica and F. T. Yarman-Vural. 2001. An overview of
character recognition focused on off-line handwriting.
Systems, Man, and Cybernetics, Part C: Applications
and Reviews, IEEE Transactions on, 31(2):216?233,
May.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Jose?phine de Monbart. 1786. Lettres tahitiennes. Les
Marchands de nouveaute?s, Paris.
Okan Kolak, William Byrne, and Philip Resnik. 2003. A
generative probabilistic ocr model for nlp applications.
In Proceedings of the NAACL, pages 55?62. Associa-
tion for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 254?263, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, USA, June. Association for
Computational Linguistics.
68
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 262?270,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Combining Bilingual and Comparable Corpora
for Low Resource Machine Translation
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch?
Computer and Information Science Dept.
University of Pennsylvania
Abstract
Statistical machine translation (SMT) per-
formance suffers when models are trained
on only small amounts of parallel data.
The learned models typically have both
low accuracy (incorrect translations and
feature scores) and low coverage (high
out-of-vocabulary rates). In this work, we
use an additional data resource, compa-
rable corpora, to improve both. Begin-
ning with a small bitext and correspond-
ing phrase-based SMT model, we improve
coverage by using bilingual lexicon induc-
tion techniques to learn new translations
from comparable corpora. Then, we sup-
plement the model?s feature space with
translation scores estimated over compa-
rable corpora in order to improve accu-
racy. We observe improvements between
0.5 and 1.7 BLEU translating Tamil, Tel-
ugu, Bengali, Malayalam, Hindi, and Urdu
into English.
1 Introduction
Standard statistical machine translation (SMT)
models (Koehn et al, 2003) are trained using
large, sentence-aligned parallel corpora. Unfortu-
nately, parallel corpora are not always available in
large enough quantities to train robust models (Ko-
lachina et al, 2012). In this work, we consider the
situation in which we have access to only a small
amount of bitext for a given low resource language
pair, and we wish to supplement an SMT model
with additional translations and features estimated
using comparable corpora in the source and tar-
get languages. Assuming access to a small amount
?Performed while faculty at Johns Hopkins University
of parallel text is realistic, especially considering
the recent success of crowdsourcing translations
(Zaidan and Callison-Burch, 2011; Ambati, 2011;
Post et al, 2012).
We frame the shortcomings of SMT models
trained on limited amounts of parallel text1 in
terms of accuracy and coverage. In this con-
text, coverage refers to the number of words and
phrases that a model has any knowledge of at all,
and it is low when the training text is small, which
results in a high out-of-vocabulary (OOV) rate.
Accuracy refers to the correctness of the transla-
tion pairs and their corresponding probability fea-
tures that make up the translation model. Because
the quality of unsupervised automatic word align-
ments correlates with the amount of available par-
allel text and alignment errors result in errors in
extracted translation pairs, accuracy tends to be
low in low resource settings. Additionally, esti-
mating translation probabilities2 over sparse train-
ing sets results in inaccurate feature scores.
Given these deficiencies, we begin with a base-
line SMT model learned from a small parallel cor-
pus and supplement the model to improve its ac-
curacy and coverage. We apply techniques pre-
sented in prior work that use comparable corpora
to estimate similarities between word and phrases.
In particular, we build on prior work in bilingual
lexicon induction in order to predict translations
for OOV words, improving coverage. We then use
the same corpora to estimate additional translation
feature scores, improving model accuracy. We see
improvements in translation quality between 0.5
1We consider low resource settings to be those with par-
allel datasets of fewer than 1 million words. Most standard
MT datasets contain tens or hundreds of millions of words.
2Estimating reordering probabilities over sparse data also
leads to model inaccuracies; we do not tackle that here.
262
and 1.7 BLEU points translating the following low
resource languages into English: Tamil, Telugu,
Bengali, Malayalam, Hindi, and Urdu.
2 Previous Work
Prior work shows that a variety of signals, in-
cluding distributional, temporal, topic, and string
similarity, may inform bilingual lexicon induc-
tion (Rapp, 1995; Fung and Yee, 1998; Rapp,
1999; Schafer and Yarowsky, 2002; Koehn and
Knight, 2002; Monz and Dorr, 2005; Huang
et al, 2005; Schafer, 2006; Klementiev and
Roth, 2006; Haghighi et al, 2008; Mimno et al,
2009; Mausam et al, 2010). Other work has
used decipherment techniques to learn translations
from monolingual and comparable data (Ravi and
Knight, 2011; Dou and Knight, 2012; Nuhn et al,
2012). Daume? and Jagarlamudi (2011) use con-
textual and string similarity to mine translations
for OOV words in a high resource language do-
main adaptation for a machine translation setting.
Unlike most other prior work on bilingual lexicon
induction, Daume? and Jagarlamudi (2011) use the
translations in end-to-end SMT.
More recently, Irvine and Callison-Burch
(2013) combine a variety of the techniques for
estimating word pair similarity using source and
target language comparable corpora. That work
shows that only a small amount of supervision is
needed to learn how to effectively combine simi-
larity features into a single model for doing bilin-
gual lexicon induction. In this work, because we
assume access to a small amount of bilingual data,
it is natural to take such a supervised approach to
inducing new translations, and we directly apply
that of Irvine and Callison-Burch (2013).
Klementiev et al (2012) use comparable cor-
pora to score an existing Spanish-English phrase
table extracted from the Europarl corpus. In this
work, we directly apply their technique for scor-
ing an existing phrase table. However, unlike that
work, our initial phrase tables are estimated from
small parallel corpora for genuine low resource
languages. Additionally, we include new transla-
tions discovered in comparable corpora.
Other prior work has mined supplemental paral-
lel data from comparable corpora (Munteanu and
Marcu, 2006; AbduI-Rauf and Schwenk, 2009;
Smith et al, 2010; Uszkoreit et al, 2010; Smith et
al., 2013). Such efforts are orthogonal and com-
plementary to the approach that we take.
Language Train Words (k) Dev Types Dev TokensSent Dict % OOV % OOV
Tamil 335 77 44 25
Telugu 414 41 39 21
Bengali 240 7 37 18
Malayalam 263 151 6 3
Hindi 659 n/a 34 11
Urdu 616 116 23 6
Table 1: Information about datasets released by Post et al
(2012): thousands of words in the source language parallel
sentences and dictionaries, and percent of development set
word types (unique word tokens) and word tokens that are
OOV (do not appear in either section of the training data).
Language Web Crawls Wikipedia
Tamil 0.1 4.4
Telugu 0.4 8.6
Bengali 2.7 3.3
Malayalam 0.1 3.7
Hindi 18.1 6.4
Urdu 285 2.5
Table 2: Millions of words of time-stamped web crawls and
Wikipedia text, by language.
3 Using Comparable Corpora to
Improve Accuracy and Coverage
After describing our bilingual and comparable cor-
pora, we briefly describe the techniques proposed
by Irvine and Callison-Burch (2013) and Klemen-
tiev et al (2012). The contribution of this paper
is the application and combination of these tech-
niques in truly low resource translation conditions.
3.1 Datasets
Post et al (2012) used Mechanical Turk to col-
lect small parallel corpora for the following Indian
languages and English: Tamil, Telugu, Bengali,
Malayalam, Hindi, and Urdu. They collected both
parallel sentence pairs and a dictionary of word
translations.3 We use all six datasets, which pro-
vide real low resource data conditions for six truly
low resource language pairs. Table 1 shows statis-
tics about the datasets.
Table 2 lists the amount of comparable data
that we use for each language. Following both
Klementiev et al (2012) and Irvine and Callison-
Burch (2013), we use time-stamped web crawls
as well as interlingually linked Wikipedia docu-
ments. We use the time-stamped data to estimate
temporal similarity and the interlingual Wikipedia
links, which indicate documents about the same
topic written in different languages, to estimate
3No dictionary was provided for Hindi.
263
topic similarity. We use both datasets in combina-
tion with a dictionary derived from the small par-
allel corpora to estimate contextual similarity.
3.2 Improving Coverage
In order to improve the coverage of our low re-
source translation models, we use bilingual lexi-
con induction techniques to learn translations for
words which appear in our test sets but not in our
training data (OOVs). Bilingual lexicon induction
is the task of inducing pairs of words that are trans-
lations of one another from monolingual or com-
parable corpora. Irvine and Callison-Burch (2013)
use a diverse set of features estimated over compa-
rable corpora and a small set of known translations
as supervision for training a discriminative classi-
fier, which makes predictions (translation or not a
translation) on test set words paired with all pos-
sible translations. Possible translations are taken
from the set of all target words appearing in the
comparable corpora. Candidates are ranked ac-
cording to their classification scores. They achieve
very good performance on the induction task itself
compared with an unsupervised baseline that ag-
gregates the same similarity features uniformly. In
our setting, we have access to a small parallel cor-
pus, which makes such a supervised approach to
bilingual lexicon induction a natural choice.
We use the framework described in Irvine and
Callison-Burch (2013) directly, and further details
may be found there. In particular, we use the same
feature set, which includes the temporal, contex-
tual, topic, orthographic, and frequency similarity
between a candidate translation pair. We derive
translations to serve as positive supervision from
our automatically aligned parallel text4 and, like
the prior work, use random word pairs as nega-
tive supervision. Figure 1 shows some examples
of Bengali words, their correct translations, and
the top-3 translations that this framework induces.
In our initial experiments, we add the high-
est ranked English candidate translation for each
source language OOV to our phrase tables. Be-
cause all of the OOVs appear at least once in our
comparable corpora,5 we are able to mine transla-
tions for all of them. Adding these translations by
definition improves the coverage of our MT mod-
els. Then, in additional sets of experiments, we
4GIZA++ intersection alignments over all training data.
5The Post et al (2012) datasets are crowdsourced English
translations of source Wikipedia text. Using Wikipedia as
comparable corpora, we observe all OOVs at least once.
Source Induced Translations Correct Translation
???????????
mathematical
mathematicallyequal
ganitikovabe
????? 
function
functionfunctions
variables
?????? 
made
inauguration
goal
earned
Figure 1: Examples of OOV Bengali words, our top-3
ranked induced translations, and their correct translations.
also induce translations for source language words
which are low frequency in the training data and
supplement our SMT models with top-k transla-
tions, not just the highest ranked.
3.3 Improving Accuracy
In order to improve the accuracy of our mod-
els, we use comparable corpora to estimate ad-
ditional features over the translation pairs in our
phrase tables and include those features in tuning
and decoding. This approach follows that of Kle-
mentiev et al (2012). We compute both phrasal
features and lexically smoothed features (using
word alignments, like the Moses lexical transla-
tion probabilities) for all of the following except
orthographic similarity, for which we only use lex-
ically smoothed features,6 resulting in nine addi-
tional features: temporal similarity based on time-
stamped web crawls, contextual similarity based
on web crawls and Wikipedia (separately), ortho-
graphic similarity using normalized edit distance,
and topic similarity based on inter-lingually linked
Wikipedia pages. Our hope is that by adding a di-
verse set of similarity features to the phrase tables,
our models will better distinguish between good
and bad translation pairs, improving accuracy.
4 Experiments
4.1 Experimental setup
We use the data splits given by Post et al (2012)
and, following that work, include the dictionaries
in the training data and report results on the devtest
set using case-insensitive BLEU and four refer-
ences. We use the Moses phrase-based MT frame-
work (Koehn et al, 2007). For each language, we
extract a phrase table with a phrase limit of seven.
In order to make our results comparable to those
of Post et al (2012), we follow that work and use
6Because the words within a phrase pair are often re-
ordered, phrase-level orthographic similarity is unreliable.
264
Language Top-1 Acc. Top-10 Acc.
Tamil 4.5 10.2
Telugu 32.8 47.9
Bengali 17.9 29.8
Malayalam 12.9 23.0
Hindi 44.3 57.6
Urdu 16.1 33.8
Table 3: Percent of word types in a held out portion of the
training data which are translated correctly by our bilingual
lexicon induction technique. Evaluation is over the top-1 and
top-10 outputs in the ranked lists for each source word.
the English side of the training data to train a lan-
guage model. Using a language model trained on
a larger corpus (e.g. the English side of our com-
parable corpora) may yield better results, but such
an improvement is orthogonal to the focus of this
work. Throughout our experiments, we use the
batch version of MIRA (Cherry and Foster, 2012)
for tuning the feature set.7 We rerun tuning for
all experimental conditions and report results av-
eraged over three tuning runs (Clark et al, 2011).
Our baseline uses the bilingually extracted
phrase pairs and standard translation probability
features. We supplement it with the top ranked
translation for each OOV to improve coverage (+
OOV Trans) and with additional features to im-
prove accuracy (+Features). In Section 4.2, we
make each modification separately and then to-
gether. Then we present additional experiments
where we induce translations for low frequency
words, in addition to OOVs (4.3), append top-k
translations (4.4), vary the amount of training data
used to induce the baseline model (4.5), and vary
the amount of comparable corpora used to esti-
mate features and induce translations (4.6).
4.2 Results
Before presenting end-to-end MT results, we ex-
amine the performance of the supervised bilingual
lexicon induction technique that we use for trans-
lating OOVs. In Table 3, top-1 accuracy is the per-
cent of source language words in a held out portion
of the training data8 for which the highest ranked
English candidate is a correct translation.9 Perfor-
mance is lowest for Tamil and highest for Hindi.
For all languages, top-10 accuracy is much higher
than the top-1 accuracy. In Section 4.4, we explore
7We experimented with MERT and PRO as well but saw
consistently better baseline performance using batch MIRA.
8Described in Section 3.2. We retrain with all training
data for MT experiments.
9Post et al (2012) gathered up to six translations for each
source word, so some have multiple correct translations
appending the top-k translations for OOV words to
our model instead of just the top-1.
Table 4 shows our results adding OOV transla-
tions, adding features, and then both. Additional
translation features alone, which improve our
models? accuracy, increase BLEU scores between
0.18 (Bengali) and 0.60 (Malayalam) points.
Adding OOV translations makes a big differ-
ence for some languages, such as Bengali and
Urdu, and almost no difference for others, like
Malayalam and Tamil. The OOV rate (Table 1) is
low in the Malayalam dataset and high in the Tamil
dataset. However, as Table 3 shows, the translation
induction accuracy is low for both. Since few of
the supplemental translations are correct, we don?t
observe BLEU gains. In contrast, induction ac-
curacies for the other languages are higher, OOV
rates are substantial, and we do observe moderate
BLEU improvements by supplementing phrase ta-
bles with OOV translations.
In order to compute the potential BLEU gains
that we could realize by correctly translating all
OOV words (achieving 100% accuracy in Table
3), we perform an oracle experiment. We use au-
tomatic word alignments over the test sets to iden-
tify correct translations and append those to the
phrase tables.10 The results, in Table 4, show pos-
sible gains between 4.3 (Telugu and Bengali) and
0 (Malayalam) BLEU points above the baseline.
Not surprisingly, the possible gain for Malayalam,
which has a very low OOV rate, is very low. Our
+OOV Trans. model gains between 0% (Tamil)
and 38% (Urdu) of the potential improvement.
Using comparable corpora to improve both ac-
curacy (+Features) and coverage (+OOV Trans.)
results in translations that are better than apply-
ing either technique alone for five of the six lan-
guages. BLEU gains range from 0.48 (Bengali)
to 1.39 (Urdu). We attribute the particularly good
Urdu performance to the relatively large compa-
rable corpora (Table 2). As a result, we have al-
ready begun to expand our web crawls for all lan-
guages. In Section 4.6, we present results varying
the amount of Urdu-English comparable corpora
used to induce translations and estimate additional
features.
Table 4 also shows the Hiero (Chiang, 2005)
and SAMT (Zollmann and Venugopal, 2006) re-
sults that Post et al (2012) report for the same
10Because the automatic word alignments are noisy, this
oracle is conservative.
265
Tamil Telugu Bengali Malayalam Hindi Urdu
Experiment BLEU Diff. BLEU Diff. BLEU Diff. BLEU Diff. BLEU Diff. BLEU Diff.
Baseline 9.45 11.72 12.07 13.55 15.01 20.39
+Features 9.77 +0.32 11.96 +0.24 12.25 +0.18 14.15 +0.60 15.34 +0.33 20.97 +0.58
+OOV Trans. 9.45 0.00 12.20 +0.48 12.74 +0.67 13.65 +0.10 15.59 +0.58 21.30 +0.91
+Feats & OOV 9.98 +0.53 12.25 +0.53 12.55 +0.48 14.18 +0.63 16.08 +1.07 21.78 +1.39
OOV Oracle 12.32 +2.87 16.04 +4.32 16.41 +4.34 13.55 0.00 17.72 +2.71 22.80 2.41
Hiero 9.81 12.46 12.72 13.72 15.53 19.53
SAMT 9.85 12.61 13.53 14.28 17.29 20.99
Table 4: BLEU performance gains that target coverage (+OOV Trans.) and accuracy (+Features), and both (+Feats & OOV).
OOV oracle uses OOV translations from automatic word alignments. Hiero and SAMT results are reported in Post et al (2012).
datasets. Both syntax-based models outperform
the phrase-based MT baseline for each language
except Urdu, where the phrase-based model out-
performs Hiero. Here, we extend a phrase-based
rather than a syntax-based system because it is
simpler. However, our improvements may also ap-
ply to syntactic models (future work). Because our
efforts have focused on the accuracy and cover-
age of translation pairs and have not addressed re-
ordering or syntax, we expect that combining them
with an SAMT grammar will result in state-of-the
art performance.
4.3 Translations of Low Frequency Words
Given the positive results in Section 4.2, we hy-
pothesize that mining translations for low fre-
quency words, in addition to OOV words, may im-
prove accuracy. For source words which only ap-
pear a few times in the parallel training text, the
bilingually extracted translations in the standard
phrase table are likely to be inaccurate. There-
fore, we perform additional experiments varying
the minimum source word training data frequency
for which we induce additional translations. That
is, if freq(wsrc) ? M , we induce a new transla-
tion for it and include that translation in our phrase
table. Note that in the results presented in Table 4,
M = 0. In these experiments, we include our ad-
ditional phrase table features estimated over com-
parable corpora and hope that these scores will as-
sist the model in choosing among multiple trans-
lation options for low frequency words, one or
more of which is extracted bilingually and one of
which is induced using comparable corpora. Table
5 shows the results when we vary M . As before,
we average BLEU scores over three tuning runs.
In general, modest BLEU score gains are made
as we supplement our phrase-based models with
induced translations of low frequency words. The
highest performance is achieved when M is be-
tween 5 and 50, depending on language. The
Language Base. M : trans added for freq(wsrc) ? M0 1 5 10 25 50
Tamil 9.5 10.0 9.9 10.2 10.2 9.9 10.2
Telugu 11.7 12.3 12.2 12.3 12.4 12.3 11.9
Bengali 12.1 12.6 12.8 13.0 12.9 13.1 13.0
Malayalam 13.6 14.2 14.1 14.2 14.2 13.9 13.9
Hindi 15.0 16.1 16.1 16.2 16.2 16.0 15.8
Urdu 20.4 21.8 21.8 21.8 21.9 22.1 21.8
Table 5: Varying minimum parallel training data frequency
of source words for which new translations are induced and
included in the phrase-based model. In all cases, the top-1
induced translation is added to the phrase table and features
estimated over comparable corpora are included (i.e. +Feats
& Trans model).
largest gains are 0.5 and 0.3 BLEU points for Ben-
gali and Urdu, respectively, at M = 25. This
is not surprising; we also saw the largest rela-
tive gains for those two languages when we added
OOV translations to our baseline model. With the
addition of low frequency translations, our highest
performing Urdu model achieves a BLEU score
that is 1.7 points higher than the baseline.
In different data conditions, inducing transla-
tions for low frequency words may result in better
or worse performance. For example, the size of the
training set impacts the quality of automatic word
alignments, which in turn impacts the reliability
of translations of low frequency words. However,
the experiments detailed here suggest that includ-
ing induced translations of low frequency words
will not hurt performance and may improve it.
4.4 Appending Top-K Translations
So far we have only added the top-1 induced trans-
lation for OOV and low frequency source words to
our phrase-based model. However, the bilingual
lexicon induction results in Table 3 show that ac-
curacies in the top-10 ranked translations are, on
average, nearly twice the top-1 accuracies. Here,
we explore adding the top-k induced translations.
We hope that our additional phrase table features
estimated over comparable corpora will enable the
266
Language Base. k: top-k translations added1 3 5 10 25
Tamil 9.5 10.0 10.0 9.8 10.0 10.0
Telugu 11.7 12.3 11.7 11.9 11.7 11.6
Bengali 12.1 12.6 12.6 12.6 12.7 12.8
Malayalam 13.6 14.2 14.2 14.2 14.2 14.1
Hindi 15.0 16.1 16.0 15.9 15.9 15.9
Urdu 20.4 21.8 21.8 21.7 21.5 21.6
Table 6: Adding top-k induced translations for source lan-
guage OOV words, varying k. Features estimated over com-
parable corpora are included (i.e. +Feats & Trans model).
The highest BLEU score for each language is highlighted. In
many cases differences are less than 0.1 BLEU.
decoder to correctly choose between the k trans-
lation options. We induce translations for OOV
words only (M = 0) and include all comparable
corpora features.
Table 6 shows performance as we append the
top-k ranked translations for each OOV word and
vary k. With the exception of Bengali, using a
k greater than 1 does not increase performance.
In the case of Bengali, and additional 0.2 BLEU
is observed when the top-25 translations are ap-
pended. In contrast, we see performance decrease
substantially for other languages (0.7 BLEU for
Telugu and 0.2 for Urdu) when the top-25 trans-
lations are used. Therefore, we conclude that, in
general, the models do not sufficiently distinguish
good from bad translations when we append more
than just the top-1. Although using a k greater than
1 means that more correct translations are in the
phrase table, it also increases the number of possi-
ble outputs over which the decoder must search.
4.5 Learning Curves over Parallel Data
In the experiments above, we only evaluated our
methods for improving the accuracy and coverage
of models trained on small amounts of bitext us-
ing the full parallel training corpora released by
Post et al (2012). Here, we apply the same tech-
niques but vary the amount of parallel data in order
to generate learning curves. Figure 2 shows learn-
ing cures for all six languages. In all cases, results
are averaged over three tuning runs. We sample
both parallel sentences and dictionary entries.
All six learning curves show similar trends. In
all experimental conditions, BLEU performance
increases approximately linearly with the log of
the amount of training data. Additionally, supple-
menting the baseline with OOV translations im-
proves performance more than supplementing the
baseline with additional phrase table scores based
5 10 20 50 100 200
20.0
20.5
21.0
21.5
22.0
Comparable Corpora (Millions of Tokens)
BLE
U ? ? ? ? ?
?
?
Baseline+Trans.+Feats.+Trans. & Feats.
Figure 3: English to Urdu translation results using vary-
ing amounts of comparable corpora to estimate features and
induce translations.
on comparable corpora. However, in most cases,
supplementing the baseline with both translations
and features improves performance more than ei-
ther alone. Performance gains are greatest when
very little training data is used. The Urdu learning
curve shows the most gains as well as the clean-
est trends across training data amounts. As before,
we attribute this to the relatively large comparable
corpora available for Urdu.
4.6 Learning Curves over Comparable
Corpora
In our final experiment, we consider the effect of
the amount of comparable corpora that we use
to estimate features and induce translations. We
present learning curves for Urdu-English because
we have the largest amount of comparable corpora
for that pair. We use the full amount of paral-
lel data to train a baseline model, and then we
randomly sample varying amounts of our Urdu-
English comparable corpora. Sampling is done
separately for the web crawl and Wikipedia com-
parable corpora. Figure 3 shows the results. As
before, results are averaged over three tuning runs.
The phrase table features estimated over com-
parable corpora improve end-to-end MT perfor-
mance more with increasing amounts of compa-
rable corpora. In contrast, the amount of com-
parable corpora used to induce OOV translations
does not impact the performance of the resulting
MT system as much. The difference may be due
267
500 1000 2000 5000 10000 50000
0
5
10
15
20
Telugu
Training Data
BLE
U
l l l
l l
l l
l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(a) Telugu
500 1000 2000 5000 10000 20000
0
5
10
15
20
Bengali
Training Data
BLE
U
l l
l
l l
l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(b) Bengali
500 1000 2000 5000 20000 50000 200000
0
5
10
15
20
Malayalam
Training Data
BLE
U
l l
l l l l
l
l
l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(c) Malayalam
5 0 1000 2000 5000 10000 50000
0
5
10
15
20
Tamil
Training Data
BLE
U
l l l
l l
l
l l l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(d) Tamil
500 1000 2000 5000 10000 20000
0
5
10
15
20
Hindi
Training Data
BLE
U
l
l l
l l
l l
l
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(e) Hindi
500 1000 2000 5000 20000 50000
0
5
10
15
20
Urdu
Training Data
BLE
U
l
l l
l
l
l
l
l
ll
l Baseline+Trans.
+Feats.
+Trans. & Feats.
(f) Urdu
Figure 2: Comparison of learning curves over lines of parallel training data for four SMT systems: our
baseline phrase-based model (baseline), model that supplements the baseline with translations of OOV
words induced using our supervised bilingual lexicon induction framework (+Trans), model that supple-
ments the baseline with additional phrase table features estimated over comparable corpora (+Feats), and
a system that supplements the baseline with both OOV translations and additional features (+Trans &
Feats).
268
to the fact that data sparsity is always more of an
issue when estimating features over phrase pairs
than when estimating features over word pairs be-
cause phrases appear less frequently than words
in monolingual corpora. Our comparable cor-
pora features are estimated over phrase pairs while
translations are only induced for OOV words, not
phrases. So, it makes sense that the former would
benefit more from larger comparable corpora.
5 Conclusion
As Post et al (2012) showed, it is reasonable
to assume a small parallel corpus for training an
SMT model even in a low resource setting. We
have used comparable corpora to improve the ac-
curacy and coverage of phrase-based MT models
built using small bilingual corpora for six low re-
source languages. We have shown that our meth-
ods improve BLEU score performance indepen-
dently and that their combined impact is nearly ad-
ditive. Additionally, our results show that adding
induced translations of low frequency words im-
proves performance beyond what is achieved by
inducing translations for OOVs alone. Finally, our
results show that our techniques improve relative
performance most when very little parallel train-
ing data is available.
6 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
References
Sadaf AbduI-Rauf and Holger Schwenk. 2009. On
the use of comparable corpora to improve smt per-
formance. In Proceedings of the Conference of the
European Association for Computational Linguis-
tics (EACL).
Vamshi Ambati. 2011. Active Learning for Machine
Translation in Scarce Data Scenarios. Ph.D. thesis,
Carnegie Mellon University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web cor-
pora. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
269
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Prasanth Kolachina, Nicola Cancedda, Marc Dymet-
man, and Sriram Venkatapathy. 2012. Prediction of
learning curves in machine translation. In Proceed-
ings of the Conference of the Association for Com-
putational Linguistics (ACL).
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174:619?637, June.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Christof Monz and Bonnie J. Dorr. 2005. Iterative
translation disambiguation for cross-language infor-
mation retrieval. In Proceedings of the Conference
on Research and Developments in Information Re-
trieval (SIGIR).
Dragos Munteanu and Daniel Marcu. 2006. Extracting
parallel sub-sentential fragments from non-parallel
corpora. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL).
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the Conference
of the Association for Computational Linguistics
(ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using
Diverse Similarity Measures. Ph.D. thesis, Johns
Hopkins University.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the common crawl. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING).
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the Con-
ference of the Association for Computational Lin-
guistics (ACL).
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation (WMT).
270
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 96?101,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
The (Un)faithful Machine Translator
Ruth Jones
Dept. of French and Francophone Studies
University of California Los Angeles
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Abstract
Applying machine translation (MT) to lit-
erary texts involves the same domain shift
challenges that arise for any sublanguage
(e.g. medical or scientific). However, it
also introduces additional challenges. One
focus in the discussion of translation the-
ory in the humanities has been on the hu-
man translator?s role in staying faithful to
an original text versus adapting it to make
it more familiar to readers. In contrast
to other domains, one objective in literary
translation is to preserve the experience of
reading a text when moving to the target
language. We use existing MT systems to
translate samples of French literature into
English. We then use qualitative analy-
sis grounded in translation theory and real
example outputs in order to address what
makes literary translation particularly hard
and the potential role of the machine in it.
1 Introduction
The question of how to translate, especially when
the source text is valued for its perceived literary
merit, has been the focus of a discussion that is
nearly as old as written text itself. A key debate is
whether the translator should (1) adapt the source
language text as it is translated into the target lan-
guage to make it familiar and understandable to
the reader, or (2) stay as faithful as possible to the
original. Schleiermacher (2012) calls the former
a free translation and the latter faithful. The for-
mer has also been referred to as domesticating the
text, or bringing the text to the reader, in contrast
to foreignizing the text, or bringing the reader to
the text (Venuti, 2008; Berman, 1992).
Consider the French phrase enculer les
mouches. Staying as faithful to the original
French as possible, the first word, enculer trans-
lates as the infinitive for the French word for anal
penetration, while the second is the more banal
flies. Google translate gives to fuck flies. However,
idiomatically, it is, despite the strongly sexual first
term, a not uncommon way to say to nitpick. This
translation makes the text more understandable, at
the cost of staying faithful to the meanings of the
individual words of the original text. Stylistic el-
ements such as metaphor, alliteration, metonymy,
and rhyme likewise require the translator to make
interpretive choices beyond the literal meaning
of the original, bringing the original text to the
reader of the translation even at the expense of
losing some of the literal meaning of the source.
Often multiple equally faithful translations of
a word or phrase exist, and the translator must
choose one based on context, either local or more
broad. For example, the French il neige can be
translated as it snows or it is snowing.1 In English,
it is snowing suggests the narrative present, while
it snows suggests a habitual occurrence.
Like human translators, a statistical machine
translation (SMT) system may produce transla-
tions that are relatively free or faithful and must
constantly make translation choices in decoding.
For SMT, choices are dependent on what is ob-
served in training and language modeling data and
their frequencies. When systems are trained on
datasets that are similar to a test text, they are more
likely to make reasonable translation choices. Ad-
ditionally, if a model, either a priori or automat-
ically, knows something about what the output
should look like (e.g. poetry should rhyme or
have rhythm), features could encourage free trans-
lations to take a certain form.
How much a translation sounds like an origi-
nal text in its target language and how much it
preserves elements of its source language, which
make it sound foreign, is in part an ethical choice
made by the human translator. Still, even experi-
enced human translators have difficulty recogniz-
1There is no present progressive tense in French.
96
ing when they are being faithful and when their
cultural experiences have influenced a translation.
Current SMT models have no awareness of this
and no ability to make specific choices to bal-
ance the two tendencies in the same output. Our
work shines a light on SMT from the perspective
of translation theory based on a qualitative analy-
sis of two translated samples of French literature,
one prose and one poetry. We compare SMT and
human translations to address the following:
? What types of translation choices does the
machine make, compared with humans?
? Is there evidence for the need to encourage a
machine to translate more freely?
? Can SMT translate non-ethnocentrically?
2 Background
2.1 Translation Theory
Schleiermacher (2012) raises the issue of a trans-
lation?s approximation of its source language vs.
its fluency or resemblance to an original work in
its target language, referring to translations ?that
are faithful or free.? Berman (1992), alternatively,
outlined the need for an ethics and an analytics of
translation. For Berman, the translator has an im-
perative to avoid ?freedom? where it brings with
it a tendency to alter the foreign text by making it
resemble a work of literature created in the target
language through adjustments to the original on
the levels of style, idiom, and content (both lexi-
cal and explicative). His is an argument for what
Venuti (2008) calls ?foreignization? in translation,
preserving the distance between the language of
the original text and the language of the translation
by creating a translation that is perceptibly differ-
ent from an original work in the target language.
He opposes this to domestication, which instead
privileges fluency and readability.
Venuti (2008) uses a similar critique to address
the relative visibility or invisibility of the transla-
tor. For Venuti, part of the domestication of the
translated text comes in the form of the invisi-
bility of its translator in the finished (marketed)
product. Compare, for instance, Venuti?s exam-
ple of the translator?s invisibility in the 2002 Pen-
guin translation of the Anna Karenina, advertised
with praise for the ?transparency? of its translation
without naming the translators, to Seamus Heany?s
2000 translation of Beowulf, which includes both
original and translated texts side-by-side and fea-
tures the poet/translator?s name prominently on
the cover. In the first case, the reader is asked to
forget that she is not, in fact, reading Tolstoy in
his own words, while, in the second, Heany?s text
is open to constant comparison with its original.
2.2 MT of Non-Standard Language
Prior work applying SMT to non-standard lan-
guage focuses primarily on domain adaptation. In
that task, an MT system trained on, for example,
newswire, is used to translate text in a different
domain, such as science. Much of this work has
focused on up-weighting subsets of the training or
language modeling data that are most similar to
the new domain (Matsoukas et al, 2009; Foster et
al., 2010; Ananthakrishnan et al, 2011; Niehues
and Waibel, 2010; Foster and Kuhn, 2007; Tiede-
mann, 2010; Lavergne et al, 2011).
Other work has focused on literary texts (Reddy
and Knight, 2011; Kao and Jurafsky, 2012; Roque,
2012). Most relevant is Greene et al (2010),
which presents a model for translating Italian po-
etry into English. That work focuses on preserv-
ing meaning as well as rhythm and is an interest-
ing first attempt at integrating models of poetry
(?how to say?) and storyline (?what to say?) gen-
eration. In many cases, it is hard to do both well
at once; simultaneously maintaining the meaning
and rhythm of a poem is challenging.
3 Experiments
3.1 Data and Setup
We analyze translations of two samples of French
literature, one prose and one poem (Figures 1-
2). The prose selection is a sample of the twen-
tieth century novel L?E?tranger by Albert Camus
(Camus, 1955). We use the Camus and Ward
(1989) English translation as a reference. The po-
etry selection is a sample of the twentieth cen-
tury poem ?Jardin? by Yves Bonnefoy (Bonnefoy,
1968), from the collection De?but et fin de la neige,
translated in Bonnefoy et al (2012). We selected
the passages because they use fairly simple lan-
guage and have modern and well-known authors.
We translate the two literary selections using
two SMT systems. First, we train a phrase-based
MT model using the Hansard data.2 The corpus
contains over 8 million parallel lines of text and
is one of the largest freely available parallel cor-
pora for any language pair. It contains proceed-
ings of the Canadian parliament. Recent work has
2http://www.parl.gc.ca
97
shown that newswire corpora, the other common
bitext domain, is not very different from the par-
liamentary domain. Thus, a model trained on the
Hansard data reflects the status of a typical mod-
ern SMT system trained on freely available data.
We use the Moses SMT framework (Koehn et al,
2007), GIZA++ automatic word alignments (Och
and Ney, 2003), and the batch version of MIRA
for tuning (Cherry and Foster, 2012). For compar-
ison, we also present and analyze translations by
Google translate.3
In addition to our detailed manual analysis,
we automatically evaluated outputs using case-
insensitive BLEU and a single reference. The
Moses system achieve a slightly higher BLEU
score than Google (16.62 vs. 11.25) on the Bon-
nefoy selection and the opposite is true for the Ca-
mus selection (26.03 vs. 30.05). However, be-
cause the selections are small, we don?t interpret
these results as particularly meaningful.
3.2 Analysis
Figures 1 and 2 show the outputs. Focusing on
the differences between the machine and human
translations with respect to the originals reveals
places where the human translators? choices di-
verged from the translations considered probable
by the two SMT models. Close reading of the
source text and human translation suggests pos-
sible reasons for the translator?s choices. The
probabilities that the SMT model assigns to the
human translations relative to those assigned to
the observed MT output highlights the need for
probabilistic translation models that are specific to
the domain of the particular texts or for literary
translation more generally. While differences oc-
curred based on a variety of factors, for the sake of
brevity, we only consider lexical variation and the
question of time as an aspect of translation. We
take examples from Camus? prose and Bonnefoy?s
poem while keeping in mind the possibility of a
definable difference in domain between the two.
Ward?s translation puts the plain language of
Camus? text into a clear and conversational En-
glish, a stylistic choice for fluency and domes-
tication of the French. The focus in this pas-
sage is on the speaker, actively calming himself
after the departure of the warden, and the ?re?
of ?retrouve?? appears as ?again.? The machine
translations, looking at the words in the absence
3translate.google.com
lui parti, j' ai retrouv? le calme.
j' ?tais ?puis? et je me suis jet? sur ma couchette. 
je crois que j' ai dormi parce que je me suis r?veill? avec des 
?toiles sur le visage.
des bruits de campagne montaient jusqu' ? moi.
des odeurs de nuit , de terre et de sel rafra?chissaient mes 
tempes.
la merveilleuse paix de cet ?t? endormi entrait en moi comme 
une mar?e.
a ce moment, et ? la limite de la nuit, des sir?nes ont hurl?.
elles annon?aient des d?parts pour un monde qui maintenant 
m' ?tait ? jamais indiff?rent.
pour la premi?re fois depuis bien longtemps j' ai pens? ? 
maman.
L??tranger - Albert Camus
his party, i found the calm. 
i was exhausted and i had on my sleeper. 
i think i slept because i woke up with stars on your face. 
the campaign noise up to me. 
the smell of night of land and salt rafra?chissaient my tempes. 
the great peace of this summer asleep came into me as a tide. 
has this time, and the limit of the night of sirens screamed and 
yelled. 
they announced departures for a world that now was never 
indifferent. 
for the first time in a long time i thought about mom. 
Hansard SMT
he was gone, i found calm.
i was exhausted and i threw myself on my bunk.
i think i slept because i woke up with stars on her face.
noises campaign amounted to me.
the smell of night, earth and salt refreshed my temples.
heavenly peace this summer sleeping entered me like a tide.
at that time, and the limit of the night, sirens screamed .
they announced departures for a world that now was never 
indifferent to me.
for the first time in ages i thought mom.
Google SMT
with him gone , i was able to calm down again.
i was exhausted and threw myself on my bunk.
i must have fallen asleep, because i woke up with the stars in 
my face.
sounds of the countryside were drifting in.
smells of night, earth, and salt air were cooling my temples.
the wondrous peace of that sleeping summer flowed through 
me like a tide. 
then, in the dark hour before dawn, sirens blasted.
they were announcing departures for a world that now and 
forever meant nothing to me.
for the first time in a long time i thought about maman.
The Stranger, translation by Matthew Ward
Figure 1: The Stranger by Albert Camus
of Camus? protagonist, give ?found,? eliminating
the ?re.? Ward translates ?se calmer? exactly, ?to
calm (down).? In contrast, the machine versions
give ?found (the) calm.? It is not the passive as-
pect of Camus? phrase that is problematic (?find-
ing calm? as opposed to ?calming down?); rather,
it is the return implied by the ?re? that gives pause.
Ward?s translation gives a plainer, more informal
style than the translations offered by the SMT sys-
tems, choosing to preserve the repetition of ?re?
(in ?retrouve??) with ?again? rather than the core
meaning of ?found? in ?trouve?.?
Later in the passage (line 3), the phrase ?je
98
il neige.
sous les flocons la porte
ouvre enfin au jardin
de plus que le monde.
j' avance. mais se prend
mon ?charpe ? du fer
rouill?, et se d?chire
en moi l' ?toffe du songe.
il neige.
sous les flocons la porte ouvre enfin au jardin de plus que le 
monde.
j' avance.
mais se prend mon ?charpe ? du fer rouill?, et se d?chire en 
moi l' ?toffe du songe.
D?but et fin de la neige, Yves Bonnefoy, ? Le jardin ?
it snows. 
under the snowflakes the door 
opens finally au jardin 
more than the world. 
but is my point. 
my scarf to iron 
rusty tears, 
i think in character. 
it snows. 
under the cornflakes and opens the door au jardin de more 
than the world. 
my point. 
but does my scarf to iron rusty, that tears character in me 
thinking.
Hansard SMT
it snows.
flakes under the door
finally opens to the garden
over the world.
i advance. but takes
my scarf with iron
rusty, and tears
in me the stuff of dreams.
it snows.
finally, in the snow the door opens to the garden over the world.
i advance.
but take my scarf of rusty iron, and tears in me the stuff of 
dreams.
Google SMT
it?s snowing.
beneath the snowflakes the gate
opens at last on the garden
of more than the world.
i enter. but my scarf
catches on rusty iron,
and it tears apart in me
the fabric of the dream.
it?s snowing.
beneath the snowflakes the gate opens at last on the garden of 
more than the world.
i enter.
but my scarf catches on rusty iron, and it tears apart in me the 
fabric of the dream.
Beginning and End of the Snow, Emily Grolsholz, ?The Garden?
Figure 2: The Garden by Yves Bonnefoy
me suis re?veille? avec des e?toiles sur le visage?
is translated as ?I woke up with the stars in my
face? in Ward?s translation, whereas the Hansard
and Google translations drop the indefinite arti-
cle and assume a second person in the scene, giv-
ing ?i woke up with stars on {your, her} face.?
Later, the phrase ?des bruits de campagne? (line
4) also provides a source of linguistic confusion.
It is ?sounds of the countryside? in Ward, but
?the campaign noise? and ?noises campaign? in
Hansard and Google, respectively. Ward?s trans-
lations make two distinct choices for the indefi-
nite article ?des,? converting it to a definite article
(the) in the first instance while dropping it in the
second. Both examples again show Ward working
the text into plain-spoken English prose by choos-
ing the specific ?the stars? over the general ?stars?
for ?des e?toiles? and the more conventional con-
struction sounds of the coutryside over country-
side sounds, which would preserve the unfamiliar
(as shown by the difficulty of both MT systems in
translating this phrase) construction of ?des bruits
de campagne.? The discrepancies between the hu-
man and MT versions of Camus? text suggest that
the MT systems might, at the least, be able to iden-
tify the difficulties of translating certain stylistic
elements of the French.
The translations of Bonnefoy?s poem reveal
slightly different concerns. The translations of
?e?toffe? exemplify a lexical choice problem. Grol-
sholz?s choice of ?fabric? has a lower transla-
tion probability in the SMT models than ?stuff?
(Google translation). Both meanings are possi-
ble, but while ?stuff? is more common, the source
text suggests an association between ?e?charpe?
(scarf) and ?e?toffe? (stuff/fabric) that comes to the
fore in Grolsholz?s translation. Taken with simi-
lar choices (?gate? for ?door?, also ?snowflakes?
for ?flocons,? earlier in the poem), Grolsholz?s
translation reveals a preference for specificity over
probability that goes beyond rhythmic consistency
to effect the translated poem?s recreation of the im-
ages present in the original.
Temporality also appears as a difference be-
tween Grolsholz?s and the machine translations.
Specifically, Grolscholz translates ?il neige? (line
1) as ?it is snowing.? Neither SMT model se-
lected the present progressive. Their translation,
?it snows? has a distinctly high probability in the
Hansard model, as the parliamentary proceedings
deal most often with general conditions when dis-
cussing weather (i.e. ?it snows in the prairies?).
While this is an adequate translation of the French
phrase, Grolsholz?s choice of the progressive an-
chors the poem in a narrative present that is absent
in the general expression ?it snows.? This moment
is key to understanding the poem in the context of
the larger collection, as it gives the poet a defined
position in time that anchors the poem?s imagery.
99
The fact that neither MT system made this choice
suggests a difference between literary and nonlit-
erary texts in terms of how each treats time and the
experience of duration. Temporality functions in
subtly different ways in French and English. It is
important to narrative and literary text and is par-
ticularly difficult for the MT system.
4 Discussion
Defining the type and degree of domestication
that a literary translation should take is difficult
to express, even to a human. We can say that
Ward?s translation, with its conversational style
and choice of sense and style over language play,
is more domestic than Grolsholz?s, which tries to
reflect the syntax of the original. Indeed, if we
look back to Venuti?s complaint about the transla-
tion of Anna Karenina, Grolsholz is certainly the
more visible of the two translators, each of her
translations being accompanied by its original on
the facing page. From a technical standpoint, we
may want a translation to take into consideration
the narrative of a text in order to describe events in
the narrative present (e.g. choosing ?it is snowing?
over ?it snows?). However, defining the scope of
the relevant narrative context is difficult and may
vary substantially from text to text.
From the ethical perspective of the for-
eign/domestic debate, deciding how much the nar-
rative context needs to be explicated or altered
to be understandable in the translation is depen-
dent on variables including the translator?s stance
on this issue, the author?s wishes (if the author
is living) and the publisher?s requirements. Even
once they have been determined, specifying such
preferences precisely enough for a computational
model to follow is even harder. For example,
we could model a general preference for specific
translations of nouns over more probable transla-
tions (e.g. ?snowflakes? instead of ?flakes?), but
translation rules are typically very noisy and an
SMT system would likely be tempted by garbage
translation rules (e.g. in the Hansard system, ?flo-
cons? translates as ?cornflakes? with higher prob-
ability than ?snow?, ?flakes?, or ?snowflakes?). In
short, part of the human translator?s job is know-
ing when to make exceptions to convention for the
sake of the reader?s experience of the translated
text, and the question of the exception is difficult
for the machine to account for.
Even if the type and degree to which a text
should be domesticated could be accurately mod-
eled, some types of free/fluent/flexible translations
will be easier for a machine to produce than oth-
ers. For example, idioms may be easy to inte-
grate; if they are observed in training data, then
a machine can easily produce them. This, how-
ever, requires in-domain training data, and domain
is somewhat of a moving target in literature due
to extremely high variability. In contrast to the
ease of memorizing static idioms, computationally
choosing correct, relevant, and appropriately spe-
cific translations of individual nouns (e.g. ?porte?
as ?gate? instead of ?door?) is difficult.
We end our discussion on a note about visi-
bility. Introducing an SMT system into debates
surrounding literary translation by human transla-
tors would seem to cause the translator to disap-
pear entirely. Indeed, according to Cronin (2012),
?machine translation services would appear to ren-
der invisible the labour of translation...? How-
ever, for Venuti, visibility is crucial to the ethics
of balancing domestication and foreignization to
create non-ethnocentric translations in that it re-
minds the reader to be attentive to the translation
and to the translator as creative labourer. As a level
of domestication is to be expected in fluent trans-
lations, Venuti?s argument for visibility is also an
argument for a disruption to the reader?s experi-
ence that reinserts the distance of the foreignizing
translation in a different way, suggesting that flu-
ency, which hides the act of translation, might be
ethical under conditions of visibility. Difficulties
encountered by an SMT system can constitute a
kind of visibility, because they expose problems
in the translation, which often come in the form of
disfluencies. However, these systems cannot con-
sider translation in terms of domestication and for-
eignization; the SMT objective is to use patterns
observed in training data example translations to
produce something that has the same meaning as
the source text and looks like the target language.
There is a constant tradeoff between fluency and
faithfulness. Although SMT can deal with fluency,
it cannot handle ideas of domestic and foreign.
Therefore, if we accept that domesticating and for-
eignization is key to distinguishing visibility, then
the relationship between visibility and invisibility
for the human translator and the machine transla-
tor must be different. And this divergence, in turn,
means that current approaches to SMT could not
ensure non-ethnocentric translations.
100
References
Sankaranarayanan Ananthakrishnan, Rohit Prasad, and
Prem Natarajan. 2011. On-line language model bi-
asing for statistical machine translation. In Proceed-
ings of the Conference of the Association for Com-
putational Linguistics (ACL).
Antoine Berman. 1992. The Experience of the For-
eign: Culture and Translation in Romantic Ger-
many. State University of New York Press, New
York. Trans. by S. Heyvaert.
Y. Bonnefoy, E. Grosholz, and F. Ostovani. 2012. Be-
ginning and End of the Snow / Debut Et Fin de la
Neige: Followed by Where the Arrow Falls / Suivi
de La Ou Retombe la Fleche. Rowman & Littlefield
Publishers, Incorporated.
Yves Bonnefoy. 1968. Selected poems. Cape editions.
Cape.
A. Camus and M. Ward. 1989. The Stranger. Every-
man?s library. Knopf Doubleday Publishing Group.
A. Camus. 1955. L?e?tranger. Appleton-Century-
Crofts.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Michael Cronin. 2012. The Translation Age: Transla-
tion, Technology, and the new Instrumentalism. The
Translation Studies Reader, Third Editions. Rout-
ledge, New York.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Justine Kao and Dan Jurafsky. 2012. A computational
analysis of style, affect, and imagery in contempo-
rary poetry. In NAACL Workshop on Computational
Linguistics for Literature.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
T. Lavergne, A. Allauzen, H. Le, and F. Yvon.
2011. LIMSI?s experiments in domain adapta-
tion for IWSLT11. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Jan Niehues and Alex Waibel. 2010. Domain adapta-
tion in statistical machine translation using factored
translation models. In Proceedings of the European
Association for Machine Translation (EAMT).
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Sravana Reddy and Kevin Knight. 2011. Unsuper-
vised discovery of rhyme schemes. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL).
Antonio Roque. 2012. Towards a computational ap-
proach to literary text analysis. In NAACL Workshop
on Computational Linguistics for Literature.
Fredreich Schleiermacher. 2012. On different meth-
ods of translating. The Translation Studies Reader,
Third Editions. Routledge, New York. Trans. by Su-
san Bernofsky.
Jo?rg Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the ACL Work-
shop on Statistical Machine Translation and Metrics
(MATR).
Lawrence Venuti. 2008. The Translator?s Invisibility:
A History of Translation. Routledge, New York.
101
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 160?170,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Hallucinating Phrase Translations for Low Resource MT
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Science Dept.
University of Pennsylvania
Abstract
We demonstrate that ?hallucinating?
phrasal translations can significantly im-
prove the quality of machine translation in
low resource conditions. Our hallucinated
phrase tables consist of entries composed
from multiple unigram translations drawn
from the baseline phrase table and from
translations that are induced from mono-
lingual corpora. The hallucinated phrase
table is very noisy. Its translations are low
precision but high recall. We counter this
by introducing 30 new feature functions
(including a variety of monolingually-
estimated features) and by aggressively
pruning the phrase table. Our analysis
evaluates the intrinsic quality of our
hallucinated phrase pairs as well as their
impact in end-to-end Spanish-English and
Hindi-English MT.
1 Introduction
In this work, we augment the translation model for
a low-resource phrase-based SMT system by auto-
matically expanding its phrase table. We ?halluci-
nate? new phrase table entries by composing the
unigram translations from the baseline system?s
phrase table and translations learned from compa-
rable monolingual corpora. The composition pro-
cess yields a very large number of new phrase pair
translations, which are high recall but low preci-
sion. We filter the phrase table using a new set of
feature functions estimated from monolingual cor-
pora. We evaluate the hallucinated phrase pairs in-
trinsically as well as in end-to-end machine trans-
lation. The augmented phrase table provides more
coverage than the original phrase table, while be-
ing high quality enough to improve translation per-
formance.
We propose a four-part approach to hallucinat-
ing and using new phrase pair translations:
1. Learn potential translations for out-of-
vocabulary (OOV) words from comparable
monolingual corpora
2. ?Hallucinate? a large, noisy set of phrase
translations by composing unigram transla-
tions from the baseline model and from the
monolingually-induced bilingual dictionary
3. Use comparable monolingual corpora to
score, rank, and prune the huge number of
hallucinated translations
4. Augment the baseline phrase table with hal-
lucinated translations and new feature func-
tions estimated from monolingual corpora
We define an algorithm for generating loosely
compositional phrase pairs, which we use to hal-
lucinate new translations. In oracle experiments,
we show that such loosely compositional phrase
pairs contribute substantially to the performance
of end-to-end SMT, beyond that of component un-
igram translations. In our non-oracle experiments,
we show that adding a judiciously pruned set of
automatically hallucinated phrase pairs to an end-
to-end baseline SMT model results in a signifi-
cant improvement in translation quality for both
Spanish-English and Hindi-English.
2 Motivation
Translation models learned over small amounts
of parallel data suffer from the problem of low
coverage. That is, they do not include trans-
lations for many words and phrases. Unknown
160
words, or out-of-vocabulary (OOV) words, have
been the focus of previous work on integrating
bilingual lexicon induction and machine transla-
tion (Daum?e and Jagarlamudi, 2011; Irvine and
Callison-Burch, 2013a; Razmara et al., 2013).
Bilingual lexicon induction is the task of learning
translations from monolingual texts, and typical
approaches compare projected distributional sig-
natures of words in the source language with dis-
tributional signatures representing target language
words (Rapp, 1995; Schafer and Yarowsky, 2002;
Koehn and Knight, 2002; Haghighi et al., 2008).
If the source and target language each contain, for
example, 100, 000 words, the number of pairwise
comparisons is about 10 billion, which is signifi-
cant but computationally feasible.
In contrast to unigrams, the difficulty in induc-
ing a comprehensive set of phrase translations is
that the number of both source and target phrases
is immense. For example, there are about 83 mil-
lion unique phrases up to length three in the En-
glish Wikipedia. Pairwise comparisons of two sets
of 100 million phrases corresponds to 1 x 10
16
.
Thus, even if we limit the task to short phrases, the
number of pairwise phrase comparisons necessary
to do an exhaustive search is infeasible. However,
multi-word translation units have been shown to
improve the quality of SMT dramatically (Koehn
et al., 2003). Phrase translations allow transla-
tion models to memorize local context-dependent
translations and reordering patterns.
3 Approach
Rather than compare all source language phrases
with all target language phrases, our approach effi-
ciently proposes a smaller set of hypothesis phrase
translations for each source language phrase. Our
method builds upon the notion that many phrase
translations can be composed from the translations
of its component words and subphrases. For ex-
ample Spanish la bruja verde translates into En-
glish as the green witch. Each Spanish word cor-
responds to exactly one English word. The phrase
pair could be memorized and translated as a unit,
or the English translation could be composed from
the translations of each Spanish unigram.
Zens et al. (2012) found that only 2% of phrase
pairs in German-English, Czech-English, Spanish-
English, and French-English phrase tables consist
of multi-word source and target phrases and are
non-compositional. That is, for these languages,
the vast majority of phrase pairs in a given phrase
table could be composed from smaller units. Our
approach takes advantage of the fact that many
phrases can be translated compositionally.
We describe our approach in three parts. In Sec-
tion 3.1, we begin by inducing translations for un-
known unigrams. Then, in 3.2, we introduce our
algorithm for composing phrase translations. In
order to achieve a high recall in our set of hypoth-
esis translations, we define compositionality more
loosely than is typical. Finally, in 3.3, we use com-
parable corpora to prune the large set of hypothesis
translations for each source phrase.
3.1 Unigram Translations
In any low resource setting, many word transla-
tions are likely to be unknown. Therefore, before
moving to phrases, we use a bilingual lexicon in-
duction technique to identify translations for un-
igrams. Specifically, because we assume a set-
ting where we have some small amount of paral-
lel data, we follow our prior work on supervised
bilingual lexicon induction (Irvine and Callison-
Burch, 2013b). We take examples of good transla-
tion pairs from our word aligned training data (de-
scribed in Section 4) and use random word pairs
as negative supervision. We use this supervision
to learn a log-linear classifier that predicts whether
a given word pair is a translation or not. We pair
and score all source language unigrams in our tun-
ing and test sets with target language unigrams that
appear in our comparable corpora. Then, for each
source language unigram, we use the log-linear
model scores to rerank candidate target language
unigram translations. As in our prior work, we
include the following word pair features in our
log-linear classifier: contextual similarity, tempo-
ral similarity, topic similarity, frequency similar-
ity, and orthographic similarity.
3.2 Loosely Compositional Translations
We propose a novel technique for loosely compos-
ing phrasal translations from an existing dictio-
nary of unigram translations and stop word lists.
Given a source language phrase, our approach
considers all combinations and all permutations
of all unigram translations for each source phrase
content word. We ignore stop words in the in-
put source phrase and allow any number of stop
words anywhere in the output target phrase. In
order to make the enumeration efficient, we pre-
compute an inverted index that maps sorted target
161
casa houselinda prettylinda cutelinda handsome
la casa linda
stop words removed
casa linda
Cartesian product?of unigram translationscute, house handsome, house house, pretty
Inverted Index lookups
pretty house the pretty house a pretty house cute house house and handsome
Bilingual Dictionary:
Input Phrase:
A
B
C
D
Figure 1: Example of loosely composed translations for the
Spanish input in A, la casa linda. In B, we remove the stop
word la. Then, in C, we enumerate the cartesian product of all
unigram translations in the bilingual dictionary and sort the
words within each alphabetically. Finally, we look up each
list of words in C in the inverted index, and corresponding
target phrases are enumerated in D. The inverted index con-
tains all phrasal combinations and permutations of the word
lists in C which also appear monolingually with some fre-
quency and with, optionally, any number of stop words.
language content words to sets of phrases contain-
ing those words in any order along with, option-
ally, any number of stop words. Our algorithm for
composing candidate phrase translations is given
in Algorithm 1, and an example translation is com-
posed in Figure 1. Although in our experiments
we compose translations for source phrases up to
length three, the algorithm is generally applicable
to any set of source phrases of interest.
Algorithm 1 yields a set of target language
translations for any source language phrase for
which all content unigrams have at least one
known translation. For most phrases, the result-
ing set of hypothesis translations is very large and
the majority are incorrect. In an initial pruning
step, we add a monolingual frequency cutoff to the
composition algorithm and only add target phrases
that have a frequency of at least ?
Freq
T
to the in-
verted index. Doing so eliminates improbable tar-
get language constructions early on, for example
house handsome her or cute a house.
Input: A set of source language phrases of interest, S,
each consisting of a sequence of words
s
m
1
, s
m
2
, ...s
m
i
; A list of all target language
phrases, targetPhrases; Source and target stop
word lists, Stop
src
and Stop
trg
; Set of unigram
translations, t
s
m
i
, for all source language words
s
m
i
R Stop
src
; monolingual target language
phrase frequencies, Freq
T
; Monolingual
frequency threshold ?
Freq
T
Output: @ Sm P S, a set of candidate phrase
translations, T
m
1
, T
m
2
, ...T
m
k
Construct TargetInvertedIndex:
for T P targetPhrases do
if Freq
T
pT q ? ?
Freq
T
then
T
1 ?words t
j
P T if t
j
R Stop
trg
T
1
sorted
? sortedpT 1q
append T to TargetInvertedIndex[T
1
sorted
]
end
end
for S
m P S do
S
1 ?words sm
i
P Sm if sm
i
R Stop
src
Combs
S
1 ? t
s
1
1
?
t
s
1
2
?
...
?
t
s
1
k
T ? r s
for c
s
1 P Combs
S
1
do
c
s
1
sorted
? sortedpc
s
1q
T ? T`TargetInvertedIndexpc
s
1
sorted
q
end
T
m ? T
end
Algorithm 1: Computing a set of candidate composi-
tional phrase translations for each source phrase in the set
S. An inverted index of target phrases is constructed that
maps sorted lists of content words to phrases that contain
those content words, as well as optionally any stop words,
and have a frequency of at least ?
Freq
T
. Then, for a given
source phrase S
m
, stop words are removed from the phrase.
Next, the cartesian product of all unigram translations is
computed. Each element in the product is sorted and any
corresponding phrases in the inverted index are added to the
output.
3.3 Pruning Phrase Pairs Using Scores
Derived from Comparable Corpora
We further prune the large, noisy set of hypothe-
sized phrase translations before augmenting a seed
translation model. To do so, we use a supervised
setup very similar to that used for inducing uni-
gram translations; we estimate a variety of sig-
nals that indicate translation equivalence, includ-
ing temporal, topical, contextual, and string simi-
larity. As we showed in Klementiev et al. (2012),
such signals are effective for identifying phrase
translations as well as unigram translations. We
add ngram length, alignment, and unigram trans-
lation features to the set, listed in Appendix A.
We learn a log-linear model for combining the
features into a single score for predicting the qual-
ity of a given phrase pair. We extract training data
from the seed translation model. We rank hypoth-
esis translations for each source phrase using clas-
162
sification scores and keep the top-k. We found that
using a score threshold sometimes improves pre-
cision. However, as experiments below show, the
recall of the set of phrase pairs is more important,
and we did not observe improvements in transla-
tion quality when we used a score threshold.
4 Experimental Setup
In all of our experiments, we assume that we have
access to only a small parallel corpus. For our
Spanish experiments, we randomly sample 2, 000
sentence pairs (about 57, 000 Spanish words) from
the Spanish-English Europarl v5 parallel corpus
(Koehn, 2005). For Hindi, we use the parallel cor-
pora released by Post et al. (2012). Again, we
randomly sample 2, 000 sentence pairs from the
training corpus (about 39, 000 Hindi words). We
expect that this amount of parallel text could be
compiled for a single text domain and any pair of
modern languages. Additionally, we use approxi-
mately 2, 500 and 1, 000 single-reference parallel
sentences each for tuning and testing our Span-
ish and Hindi models, respectively. Spanish tun-
ing and test sets are newswire articles taken from
the 2010 WMT shared task (Callison-Burch et al.,
2010).
1
We use the Hindi development and testing
splits released by Post et al. (2012).
4.1 Unigram Translations
Of the 16, 269 unique unigrams in the source side
of our Spanish MT tuning and test sets, 73% are
OOV with respect to our training corpus. 21% of
unigram tokens are OOV. For Hindi, 61% of the
8, 137 unique unigrams in the tuning and test sets
are OOV with respect to our training corpus, and
18% of unigram tokens are OOV. However, be-
cause automatic word alignments estimated over
the small parallel training corpora are noisy, we
use bilingual lexicon induction to induce transla-
tions for all unigrams. We use the Wikipedia and
online news web crawls datasets that we released
in Irvine and Callison-Burch (2013b) to estimate
similarity scores. Together, the two datasets con-
tain about 900 million words of Spanish data and
about 50 million words of Hindi data. For both
languages, we limit the set of hypothesis target un-
igram translations to those that appear at least 10
times in our comparable corpora.
We use 3, 000 high probability word translation
1
news-test2008 plus news-syscomb2009 for tuning and
newstest2009 for testing.
pairs extracted from each parallel corpus as posi-
tive supervision and 9, 000 random word pairs as
negative supervision. We use Vowpal Wabbit
2
for
learning. The top-5 induced translations for each
source language word are used as both a baseline
set of new translations (Section 6.3) and for com-
posing phrase translations.
4.2 Composing and Pruning Phrase
Translations
There are about 183 and 66 thousand unique bi-
grams and trigrams in the Spanish and Hindi tun-
ing and test sets, respectively. However, many
of these phrases do not demand new hypothesis
translations. We do not translate those which con-
tain numbers or punctuation. Additionally, for
Spanish, we exclude names, which are typically
translated identically between Spanish and En-
glish.
3
We exclude phrases which are sequences of
stop words only. Additionally, we exclude phrases
that appear more than 100 times in the small train-
ing corpus because our seed phrase table likely al-
ready contains high quality translations for them.
Finally, we exclude phrases that appear fewer than
20 times in our comparable corpora as our fea-
tures are unreliable when estimated over so few
tokens. We hypothesize translations for the ap-
proximately 15 and 6 thousand Spanish and Hindi
phrases, respectively, which meet these criteria.
Our approach for inducing translations straightfor-
wardly generalizes to any set of source phrases.
In defining loosely compositional phrase trans-
lations, we use both the induced unigram dictio-
nary (Section 3.1) and the dictionary extracted
from the word aligned parallel corpus. We ex-
pand these dictionaries further by mapping uni-
grams to their five-character word prefixes. We
use monolingual corpora of Wikipedia articles
4
to
construct stop word lists, containing the most fre-
quent 300 words in each language, and indexes of
monolingual phrase frequencies. There are about
83 million unique phrases up to length three in
the English Wikipedia. However, we ignore tar-
get phrases that appear fewer than three times, re-
ducing this set to 10 million English phrases. On
2
http://hunch.net/
?
vw/, version 6.1.4. with
standard learning parameters
3
Our names list comes from page titles of Spanish
Wikipedia pages about people. We iterate through years, be-
ginning with 1AD, and extract names from Wikipedia ?born
in? category pages, e.g. ?2013 births,? or ?Nacidos en 2013.?
4
All inter-lingually linked source language and English
articles.
163
average, our Spanish model yields 7, 986 English
translations for each Spanish bigram, and 9, 231
for each trigram, or less than 0.1% of all possi-
ble candidate English phrases. Our Hindi model
yields even fewer candidate English phrases, 826
for each bigram and 1, 113 for each trigram, on
average.
We use the same comparable corpora used for
bilingual lexicon induction to estimate features
over hypothesis phrase translations. The full fea-
ture set is listed in Appendix A. We extract su-
pervision from the seed translation models by first
identifying phrase pairs with multi-word source
strings, that appear at least three times in the train-
ing corpus, and that are composeable using base-
line model unigram translations and induced dic-
tionaries. Then, for each language pair, we use
the 3, 000 that have the highest ppf |eq scores as
positive supervision. We randomly sample 9, 000
compositional phrase pairs from those not in each
phrase table as negative supervision. Again, we
use Vowpal Wabbit for learning a log linear model
to score any phrase pair.
4.3 Machine Translation
We use GIZA++ to word align each training cor-
pus. We use the Moses SMT framework (Koehn et
al., 2007) and the standard phrase-based MT fea-
ture set, including phrase and lexical translation
probabilities and a lexicalized reordering model.
When we augment our models with new transla-
tions, we use the average reordering scores over
all bilingually estimated phrase pairs. We tune
all models using batch MIRA (Cherry and Fos-
ter, 2012). We average results over three tuning
runs and use approximate randomization to mea-
sure statistical significance (Clark et al., 2011).
For Spanish, we use a 5-gram language model
trained on the English side of the complete Eu-
roparl corpus and for Hindi a 5-gram language
model trained on the English side of the com-
plete training corpus released by Post et al. (2012).
We train our language models using SRILM with
Kneser-Ney smoothing. Our baseline models use
a phrase limit of three, and we augment them with
translations of phrases up to length three in our ex-
periments.
5 Oracle Experiment
Before moving to the results of our proposed
approach for composing phrase translations, we
present an oracle experiment to answer these re-
search questions: Would a low resource transla-
tion model benefit from composing its unigram
translations into phrases? Would this be fur-
ther improved by adding unigram translations that
are learned from monolingual texts? We an-
swer these questions by starting with our low-
resource Spanish-English and Hindi-English base-
lines and augmenting each with (1) phrasal trans-
lations composed from baseline model unigram
translations, and (2) phrasal translations composed
of a mix of baseline model unigram translations
and the monolingually-induced unigrams.
Figure 2 illustrates how our hallucinated phrase-
table entries can result in improved translation
quality for Spanish to English translation. Since
the baseline model is trained from such a small
amount of data, it typically translates individual
words instead of phrases. In our augmented sys-
tem, we compose a translation of was no one from
habia nadie, since habia translates as was in the
baseline model, nadie translates as one, and no is
a stop word. We are able to monolingually-induce
translations for the OOVs centros and electorales
before composing the phrase translation polling
stations for centros electorales.
In our oracle experiments, composed transla-
tions are only added to the phrase table if they
are contained in the reference. This eliminates the
huge number of noisy translations that our com-
positional algorithm generates. We augment base-
line models with translations for the same sets of
source language phrases described in Section 4.
We use GIZA++ to word align our tuning and
test sets
5
and use a standard phrase pair extraction
heuristic
6
to identify oracle phrase translations.
We add oracle translations to each baseline model
without bilingually estimated translation scores
7
because such scores are not available for our auto-
matically induced translations. Instead, we score
the oracle phrase pairs using the 30 new phrase ta-
ble features described in Section 3.3.
Table 1 shows the results of our oracle experi-
ments. Augmenting the baselines with the subset
of oracle translations which are composed given
the unigram translations in the baseline models
themselves (i.e. in the small training sets) yields
5
For both languages, we learn an alignment over our tun-
ing and test sets and complete parallel training sets.
6
grow-diag-final
7
We use an indicator feature for distinguishing new com-
posed translations from bilingually extracted phrase pairs.
164
not having dependent on the centros electorales .
no was no one in the polling stations .
no hab?a nadie en los centros electorales .
original composeable?from original original composeable?from induced original
Baseline:
Input:
Hallucination Oracle:
Figure 2: Example output from motivating experiment: a comparison of the baseline and full oracle translations of Spanish
no hab??a nadie en los centros electorales, which translates correctly as there was nobody at the voting offices. The full oracle
is augmented with translations composed from the seed model as well as induced unigram translations. The phrase was no one
is composeable from hab??a nadie given the seed model. In contrast, the phrase polling stations is composeable from centros
electorales using induced translations. For each translation, the phrase segmentations used by the decoder are highlighted.
Experiment
BLEU
Baseline Monolingually
Features Estimated Feats.
Spanish
Low Resource Baseline 13.47 13.35
+ Composeable Oracle
14.90 15.18
from Initial Model
+ Composeable Oracle
15.47 15.94
w/ Induced Unigram Trans.
Hindi
Low Resource Baseline 8.49 8.26
+ Composeable Oracle
9.12 9.54
from Initial Model
+ Composeable Oracle
10.09 10.19
w/ Induced Unigram Trans.
Table 1: Motivating Experiment: BLEU results using the
baseline SMT model and composeable oracle translations
with and without induced unigram translations.
a BLEU score improvement of about 1.4 points
for Spanish and about 0.6 for Hindi. This find-
ing itself is noteworthy, and we investigated the
reason for it. A representative example of a com-
positional oracle translation that was added to the
Spanish model is para evitarlos, which translates
as to prevent them. In the training corpus, para
translates far more frequently as for than to. Thus,
it is useful for the translation model to know that,
in the context of evitarlos, para should translate
as to and not for. Additionally, evitarlos was ob-
served only translating as the unigram prevent.
The small model fails to align the adjoined clitic
los with its translation them. However, our loose
definition of compositionality allows the English
stop word them to appear anywhere in the target
translation.
In the first result, composeable translations do
not include those that contain new, induced word
translations. Using the baseline model and in-
duced unigram translations to compose phrase
translations results in a 2 and 1.6 BLEU point gain
for Spanish and Hindi, respectively.
The second column of Table 1 shows the results
of augmenting the baseline models with the same
oracle phrase pairs as well as the new features esti-
mated over all phrase pairs. Although the features
do not improve the performance of the baseline
models, this diverse set of scores improves perfor-
mance dramatically when new, oracle phrase pairs
are added. Adding all oracle translations and the
new feature set results in a total gain of about 2.6
BLEU points for Spanish and about 1.9 for Hindi.
These gains are the maximum that we could hope
to achieve by augmenting models with our hallu-
cinated translations and new feature set.
6 Experimental Results
6.1 Unigram Translations
Table 2 shows examples of top ranked transla-
tions for several Spanish words. Although per-
formance is generally quite good, we do observe
some instances of false cognates, for example the
top ranked translation for aburridos, which trans-
lates correctly as bored, is burritos. Using au-
tomatic word alignments as a reference, we find
that 44% of Spanish tuning set unigrams have a
correct translation in their top-10 ranked lists and
62% in the top-100. For Hindi, 31% of tuning set
unigrams have a correct translation in their top-10
ranked lists and 43% in the top-100.
6.2 Hallucinated Phrase Pairs
Before moving to end-to-end SMT experiments,
we evaluate the goodness of the hallucinated and
pruned phrase pairs themselves. In order to do so,
we use the same set of oracle phrase translations
described in Section 5.
Table 3 shows the top three English transla-
tions for several Spanish phrases along with their
model scores. Common, loose translations of
some phrases are scored higher than less common
but literal translations. For example, very obvi-
165
Spanish abdominal abejorro abril aburridos accionista aceite actriz
Top 5
English
Translations
abdominal bumblebees april burritos actionists adulterated actress
abdomen bombus march boredom actionist iooc actor
bowel xylocopa june agatean telmex olive award
appendicitis ilyitch july burrito shareholder milliliters american
acute bumble december poof antagonists canola singer
Table 2: Top five induced translations for several source words. Correct translations are bolded. aceite translates as oil.
Spanish English Score
ambos partidos
two parties 5.72
both parties 5.31
and parties 3.16
hab??a apoyado
were supported 4.80
were members 4.52
had supported 4.39
ministro neerland`es
finnish minister 4.76
finnish ministry 2.77
dutch minister 1.31
unas cuantas semanas
over a week 4.30
a few weeks 3.72
few weeks 3.22
muy evidentes
very obvious 1.88
very evident 1.87
obviously very 1.84
Table 3: Top three compositional translations for several
source phrases and their model scores. Correct translations
are bolded.
ous scores higher than very evident as a translation
of Spanish muy evidentes. Similarly, dutch minis-
ter is scored higher than netherlands minister as a
translation for ministro neerland`es.
We use model scores to rerank candidate trans-
lations for each source phrase and keep the top-
k translations. Figure 3 shows the precision and
type-based recall (the percent of source phrases
for which at least one correct translation is gen-
erated) as we vary k for each language pair. At
k ? 1, precision and recall are about 27% for
Spanish and about 25% for Hindi.
8
At k ? 200,
recall increases to 57% for Spanish and precision
drops to 2%. For Hindi, recall increases to 40%
and precision drops to 1%.
Moving from k ? 1 to k ? 200, precision
drops at about the same rate for the two source lan-
guages. However, recall increases less for Hindi
than for Spanish. We attribute this to two things.
First, Hindi and English are less related than Span-
ish and English, and fewer phrases are translated
compositionally. Our oracle experiments showed
that there is less to gain in composing phrase trans-
lations for Hindi than for Spanish. Second, the
accuracy of our induced unigram translations is
lower for Hindi than it is for Spanish. Without ac-
curate unigram translations, we are unable to com-
pose high quality phrase translations.
8
Since we are computing type-based recall, and at k=1,
we produce exactly one translation for each source phrase,
precision and recall are the same.
l
l
l
l
l
l
l
l
l
l
l l0 10 20 30 40 50 60 700
20
40
60
80
Recall
Prec
ision
13.9014.0714.30 14.50 14.57
13.47
(a) Spanish
l
l
l
l
l
l
l
0 10 20 30 40 50 60 700
10
20
30
40
50
Recall
Prec
ision 8.16
8.868.899.009.04
8.49
(b) Hindi
Figure 3: Precision Recall curve with BLEU scores for the
top-k scored hallucinated translations. k varies from 1 to 200.
Baseline model performance is shown with a red triangle.
Because we hallucinate translations for source
phrases that appear in the training data up to 100
times, our baseline models include some of the
oracle phrase translations. Not surprisingly, the
bilingually extracted phrase pairs have relatively
high precision (81% and 40% for Spanish and
Hindi, respectively) and low recall (6% and 15%
for Spanish and Hindi, respectively).
6.3 End-to-End Translation
Table 4 shows end-to-end translation BLEU score
results (Papineni et al., 2002). Our first baseline
SMT models are trained using only 2, 000 paral-
lel sentences and no new translation model fea-
tures. Our Spanish baseline achieves a BLEU
score of 13.47 and our Hindi baseline a BLEU
score of 8.49. When we add the 30 new feature
functions estimated over comparable monolingual
corpora, performance is slightly lower, 13.35 for
166
Experiment
BLEU
Spanish Hindi
Baseline 13.47 8.49
+ Mono. Scores 13.35 8.26
+ Mono. Scores & OOV Trans 14.01 8.31
+ Phrase Trans, k=1 13.90 8.16
+ Phrase Trans, k=2 14.07 8.86*
+ Phrase Trans, k=5 14.30* 8.89*
+ Phrase Trans, k=25 14.50* 9.00*
+ Phrase Trans, k=200 14.57* 9.04*
Table 4: Experimental results. First, the baseline models
are augmented with monolingual phrase table features and
then also with the top-5 induced translations for all OOV un-
igrams. Then, we append the top-k hallucinated phrase trans-
lations to the third baseline models. BLEU scores are aver-
aged over three tuning runs. We measure the statistical sig-
nificance of each +Phrase Trans model in comparison with
the highest performing (bolded) baseline for each language;
* indicates statistical significance with p ? 0.01.
Spanish and 8.26 for Hindi. Our third baselines
augment the second with unigram translations for
all OOV tuning and test set source words using the
bilingual lexicon induction techniques described
in Section 3.1. We append the top-5 translations
for each,
9
score both the original and the new
phrase pairs with the new feature set, and retune.
With these additional unigram translations, perfor-
mance increases to 14.01 for Spanish and 8.31 for
Hindi.
We append the top-k composed translations for
the source phrases described in Section 4 to the
third baseline models. Both original and new
phrase pairs are scored using the new feature set.
BLEU score results are shown at different values
of k along the precision-recall plots for each lan-
guage pair in Figure 3 as well as in Table 4. We
would expect that higher precision and higher re-
call would benefit end-to-end SMT. As usual, a
tradeoff exists between precision and recall, how-
ever, in this case, improvements in recall outweigh
the risk of a lower precision. As k increases, pre-
cision decreases but both recall and BLEU scores
increase. For both Spanish and Hindi, BLEU score
gains start to taper off at k values over 25.
In additional experiments, we found that with-
out the new features the same sets of hallucinated
phrase pairs hurt performance slightly in compar-
ison with the baseline augmented with unigram
translations, and results don?t change as we vary
k.
10
Thus, the translation models are able to ef-
fectively use the higher recall sets of new phrase
9
The same set used for composing phrase translations.
10
For all values of k between 1 and 100, without the new
features, BLEU scores are about 13.70 for Spanish
pairs because we also augmented the models with
30 new feature functions, which help them distin-
guish good from bad translations.
7 Discussion
Our results showed that including a high recall
set of ?hallucinated? translations in our augmented
phrase table successfully improved the quality of
our machine translations. The algorithm that we
proposed for hypothesizing translations is flexible,
and in future work we plan to modify it slightly
to output even more candidate translations. For
example, we could retrieve target phrases which
contain at least one source word translation instead
of all. Alternatively, we could identify candidates
using entirely different information, for example
the monolingual frequency of a source and target
word, instead of unigram translations. This type
of inverted index may improve recall in the set of
hypothesis phrase translations at the cost of gener-
ating a much bigger set for reranking.
Our new phrase table features were informa-
tive in distinguishing correct from incorrect phrase
translations, and they allowed us to make use of
noisy but high recall supplemental phrase pairs.
This is a critical result for research on identify-
ing phrase translations from non-parallel text. We
also believe that using fairly strong target (En-
glish) language models contributed to our models?
ability to discriminate between good and bad hal-
lucinated phrase pairs. We leave research on the
influence of the language model in our setting to
future work.
In this work, we experimented with two lan-
guage pairs, Spanish-English and Hindi-English.
While Spanish and English are very closely re-
lated, Hindi and English are less related. Our
oracle experiments showed potential for compos-
ing phrase translations for both language pairs,
and, indeed, in our experiments using hallucinated
phrase translations we saw significant translation
quality gains for both. We expect that improving
the quality of induced unigram translations will
yield even more performance gains.
The vast majority of prior work on low resource
MT has focused on Spanish-English (Haghighi
et al., 2008; Klementiev et al., 2012; Ravi and
Knight, 2011; Dou and Knight, 2012; Ravi, 2013;
Dou and Knight, 2013). Although such experi-
ments serve as important proofs of concept, we
found it important to also experiment with a more
167
truly low resource language pair. The success of
our approach that we have seen for Spanish and
Hindi suggests that it is worth pursuing such di-
rections for other even less related and resourced
language pairs. In addition to language pair, text
genre and the degree of looseness or literalness of
given parallel corpora may also affect the amount
of phrase translation compositionality.
8 Related Work
Phrase-based SMT models estimated over very
large parallel corpora are expensive to store and
process. Prior work has reduced the size of SMT
phrase tables in order to improve efficiency with-
out the loss of translation quality (He et al., 2009;
Johnson et al., 2007; Zens et al., 2012). Typi-
cally, the goal of pruning is to identify and re-
move phrase pairs which are likely to be inaccu-
rate, using either the scores and counts of a given
pair itself or those relative to other phrase pairs.
Our work, in contrast, focuses on low resource set-
tings, where training data is limited and provides
incomplete and unreliable scored phrase pairs. We
begin by dramatically increasing the size of our
SMT phrase table in order to expand its coverage
and then use non-parallel data to rescore and filter
the table.
In the decipherment task, translation models
are learned from comparable corpora without any
parallel text (Ravi and Knight, 2011; Dou and
Knight, 2012; Ravi, 2013). In contrast, we be-
gin with a small amount of parallel data and take
a very different approach to learning translation
models. In our prior work (Irvine and Callison-
Burch, 2013b), we showed how effective even
small amounts of bilingual data can be for learning
translations from monolingual texts.
Garera and Yarowsky (2008) pivot through
bilingual dictionaries in several language pairs to
compose translations for compound words. Zhang
and Zong (2013) construct a set of new, additional
phrase pairs for the task of domain adaptation for
machine translation. That work uses two dictio-
naries to bootstrap a set of phrase pair transla-
tions: one probabilistic dictionary extracted from
2 million words of bitext and one manually created
new-domain dictionary of 140, 000 word transla-
tions. Our approach to the construction of new
phrase pairs is somewhat similar to Zhang and
Zong (2013), but we don?t rely on a very large
manually generated dictionary. Additionally, we
focus on the low resource language pair setting,
where a large training corpus is not available.
Deng et al. (2008) work in a standard SMT set-
ting but use a discriminative framework for ex-
tracting phrase pairs from parallel corpora. That
approach yields a phrase table with higher preci-
sion and recall than the table extracted by stan-
dard world alignment based heuristics (Och and
Ney, 2003; Koehn et al., 2003). The discrimi-
native model combines features from word align-
ments and bilingual training data as well as infor-
mation theoretic features estimated over monolin-
gual data into a single log-linear model and then
the phrase pairs are filtered using a threshold on
model scores. The phrase pairs that it extracts are
limited to those that appear in pairs of sentences in
the parallel training data. Our work takes a similar
approach to that of Deng et al. (2008), however,
unlike that work, we hallucinate phrase pairs that
did not appear in training data in order to augment
the original, bilingually extracted phrase table.
Other prior work has used comparable cor-
pora to extract parallel sentences and phrases
(Munteanu and Marcu, 2006; Smith et al., 2010).
Such efforts are orthogonal to our approach. We
use parallel corpora, when available, and hallu-
cinates phrase translations without assuming any
parallel text in our comparable corpora.
9 Conclusions
We showed that ?hallucinating? phrasal transla-
tions can significantly improve machine transla-
tion performance in low resource conditions. Our
hallucinated translations are composed from uni-
gram translations. The translations are low preci-
sion but high recall. We countered this by intro-
ducing new feature functions and pruning aggres-
sively.
10 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
168
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation (WMT).
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008.
Phrase table training for precision and recall: What
makes a good phrase and a good phrase pair? In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale
decipherment for out-of-domain machine transla-
tion. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL).
Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Nikesh Garera and David Yarowsky. 2008. Translating
compounds by learning component gloss translation
models via multiple languages. In Proceedings of
the International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Zhongjun He, Yao Meng, and Hao Yu. 2009. Dis-
carding monotone composed rule for hierarchical
phrase-based statistical machine translation. In Pro-
ceedings of the 3rd International Universal Commu-
nication Symposium.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low
resource machine translation. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit.
Prasanth Kolachina, Nicola Cancedda, Marc Dymet-
man, and Sriram Venkatapathy. 2012. Prediction of
learning curves in machine translation. In Proceed-
ings of the Conference of the Association for Com-
putational Linguistics (ACL).
Dragos Munteanu and Daniel Marcu. 2006. Extracting
parallel sub-sentential fragments from non-parallel
corpora. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
169
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL).
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL).
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
Appendix A: Phrase pair filtering features
The first ten features are similar to those described
by Irvine and Callison-Burch (2013b). Stop words
are defined as the most frequent 300 words in each
language?s Wikipedia, and content words are all
non-stop words.
? Web crawl phrasal context similarity score
? Web crawl lexical context similarity score, averaged over
aligned unigrams
? Web crawl phrasal temporal similarity score
? Web crawl lexical temporal similarity score, averaged
over aligned unigrams
? Wikipedia phrasal context similarity score
? Wikipedia lexical context similarity score, averaged over
aligned unigrams
? Wikipedia phrasal topic similarity score
? Wikipedia lexical topic similarity score, averaged over
aligned unigrams
? Normalized edit distance, averaged over aligned unigrams
? Absolute value of difference between the logs of the
source and target phrase Wikipedia monolingual frequen-
cies
? Log target phrase Wikipedia monolingual frequency
? Log source phrase Wikipedia monolingual frequency
? Indicator: source phrase is longer
? Indicator: target phrase is longer
? Indicator: source and target phrases same length
? Number of source content words higher than target
? Number of target content words higher than source
? Number of source and target content words same
? Number of source stop words higher than target
? Number of target stop words higher than source
? Number of source and target stop words same
? Percent of source words aligned to at least one target word
? Percent of target words aligned to at least one source word
? Percent of source content words aligned to at least one
target word
? Percent of target content words aligned to at least one
source word
? Percent of aligned word pairs aligned in bilingual training
data
? Percent of aligned word pairs in induced dictionary
? Percent of aligned word pairs in stemmed induced dictio-
nary
170
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437?444,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Using Comparable Corpora to Adapt MT Models to New Domains
Ann Irvine
Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Science Dept.
University of Pennsylvania
Abstract
In previous work we showed that when us-
ing an SMT model trained on old-domain
data to translate text in a new-domain,
most errors are due to unseen source
words, unseen target translations, and in-
accurate translation model scores (Irvine
et al., 2013a). In this work, we target er-
rors due to inaccurate translation model
scores using new-domain comparable cor-
pora, which we mine from Wikipedia. We
assume that we have access to a large old-
domain parallel training corpus but only
enough new-domain parallel data to tune
model parameters and do evaluation. We
use the new-domain comparable corpora
to estimate additional feature scores over
the phrase pairs in our baseline models.
Augmenting models with the new features
improves the quality of machine transla-
tions in the medical and science domains
by up to 1.3 BLEU points over very strong
baselines trained on the 150 million word
Canadian Hansard dataset.
1 Introduction
Domain adaptation for machine translation is
known to be a challenging research problem that
has substantial real-world application. In this set-
ting, we have access to training data in some old-
domain of text but very little or no training data
in the domain of the text that we wish to translate.
For example, we may have a large corpus of par-
allel newswire training data but no training data in
the medical domain, resulting in low quality trans-
lations at test time due to the mismatch.
In Irvine et al. (2013a), we introduced a tax-
onomy for classifying machine translation errors
related to lexical choice. Our ?S4? taxonomy in-
cludes seen, sense, score, and search errors. Seen
errors result when a source language word or
phrase in the test set was not observed at all during
training. Sense errors occur when the source lan-
guage word or phrase was observed during train-
ing but not with the correct target language trans-
lation. If the source language word or phrase was
observed with its correct translation during train-
ing, but an incorrect alternative outweighs the cor-
rect translation, then a score error has occurred.
Search errors are due to pruning in beam search
decoding. We measured the impact of each error
type in a domain adaptation setting and concluded
that seen and sense errors are the most frequent but
that there is also room for improving errors due to
inaccurate translation model scores (Irvine et al.,
2013a). In this work, we target score errors, using
comparable corpora to reduce their frequency in a
domain adaptation setting.
We assume the setting where we have an old-
domain parallel training corpus but no new domain
training corpus.
1
We do, however, have access
to a mixed-domain comparable corpus. We iden-
tify new-domain text within our comparable cor-
pus and use that data to estimate new translation
features on the translation models extracted from
old-domain training data. Specifically, we focus
on the French-English language pair because care-
fully curated datasets exist in several domains for
tuning and evaluation. Following our prior work,
we use the Canadian Hansard parliamentary pro-
ceedings as our old-domain and adapt models to
both the medical and the science domains (Irvine
et al., 2013a). At over 8 million sentence pairs,
1
Some prior work has referred to old-domain and new-
domain corpora as out-of-domain and in-domain, respec-
tively.
437
the Canadian Hansard dataset is one of the largest
publicly available parallel corpora and provides a
very strong baseline. We give details about each
dataset in Section 4.1.
We use comparable corpora to estimate sev-
eral signals of translation equivalence. In partic-
ular, we estimate the contextual, topic, and or-
thographic similarity of each phrase pair in our
baseline old-domain translation model. In Sec-
tion 3, we describe each feature in detail. Us-
ing just 5 thousand comparable new-domain doc-
ument pairs, which we mine from Wikipedia, and
five new phrase table features, we observe perfor-
mance gains of up to 1.3 BLEU points on the sci-
ence and medical translation tasks over very strong
baselines.
2 Related Work
Recent work on machine translation domain adap-
tation has focused on either the language model-
ing component or the translation modeling com-
ponent of an SMT model. Language modeling re-
search has explored methods for subselecting new-
domain data from a large monolingual target lan-
guage corpus for use as language model training
data (Lin et al., 1997; Klakow, 2000; Gao et al.,
2002; Moore and Lewis, 2010; Mansour et al.,
2011). Translation modeling research has typi-
cally assumed that either (1) two parallel datasets
are available, one in the old domain and one in the
new, or (2) a large, mixed-domain parallel training
corpus is available. In the first setting, the goal is
to effectively make use of both the old-domain and
the new-domain parallel training corpora (Civera
and Juan, 2007; Koehn and Schroeder, 2007; Fos-
ter and Kuhn, 2007; Foster et al., 2010; Haddow
and Koehn, 2012; Haddow, 2013). In the sec-
ond setting, it has been shown that, in some cases,
training a translation model on a subset of new-
domain parallel training data within a larger train-
ing corpus can be more effective than using the
complete dataset (Mansour et al., 2011; Axelrod
et al., 2011; Sennrich, 2012; Gasc?o et al., 2012).
For many language pairs and domains, no new-
domain parallel training data is available. Wu et
al. (2008) machine translate new-domain source
language monolingual corpora and use the syn-
thetic parallel corpus as additional training data.
Daum?e and Jagarlamudi (2011), Zhang and Zong
(2013), and Irvine et al. (2013b) use new-domain
comparable corpora to mine translations for un-
seen words. That work follows a long line of re-
search on bilingual lexicon induction (e.g. Rapp
(1995), Schafer and Yarowsky (2002), Koehn and
Knight (2002), Haghighi et al. (2008), Irvine and
Callison-Burch (2013), Razmara et al. (2013)).
These efforts improve S4 seen, and, in some in-
stances, sense error types. To our knowledge,
no prior work has focused on fixing errors due
to inaccurate translation model scores in the set-
ting where no new-domain parallel training data is
available.
In Klementiev et al. (2012), we used compara-
ble corpora to estimate several features for a given
phrase pair that indicate translation equivalence,
including contextual, temporal, and topical simi-
larity. The definitions of phrasal and lexical con-
textual and topic similarity that we use here are
taken from our prior work, where we replaced
bilingually estimated phrase table features with
the new features and cited applications to low re-
source SMT. In this work we also focus on scoring
a phrase table using comparable corpora. How-
ever, here we work in a domain adaptation setting
and seek to augment, not replace, an existing set
of bilingually estimated phrase table features.
3 Phrase Table Scoring
We begin with a scored phrase table estimated us-
ing our old-domain parallel training corpus. The
phrase table contains about 201 million unique
source phrases up to length seven and about 479
million total phrase pairs. We use Wikipedia as a
source for comparable document pairs (details are
given in Section 4.1). We augment the bilingually
estimated features with the following: (1) lexical
and phrasal contextual similarity estimated over a
comparable corpus, (2) lexical and phrasal topi-
cal similarity estimated over a comparable corpus,
and (3) lexical orthographic similarity.
Contextual Similarity We estimate contextual
similarity
2
by first computing a context vector for
each source and target word and phrase in our
phrase table using the source and target sides of
our comparable corpus, respectively. We begin by
collecting vectors of counts of words that appear
in the context of each source and target phrase, p
s
and p
t
. We use a bag-of-words context consist-
ing of the two words to the left and two words to
2
Similar to distributional similarity, which is typically de-
fined monolingually.
438
the right of each occurrence of each phrase. Vari-
ous means of computing the component values of
context vectors from raw context frequency counts
have been proposed (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of p
s
?s
contextual vector, C
p
s
, as follows:
C
p
s
k
? n
p
s
,k
? plogpn{n
k
q ` 1q
where n
p
s
,k
and n
k
are the number of times the
k-th source word, s
k
, appears in the context of p
s
and in the entire corpus, and n is the maximum
number of occurrences of any word in the data.
Intuitively, the more frequently s
k
appears with p
s
and the less common it is in the corpus in general,
the higher its component value. The context vector
for p
s
, C
p
s
, is M -dimensional, where M is the
size of the source language vocabulary. Similarly,
we compute N -dimensional context vectors for all
target language words and phrases, where N is the
size of the target language vocabulary.
We identify the most probable translation t for
each of the M source language words, s, as the
target word with the highest ppt|sq under our word
aligned old-domain training corpus. Given this
dictionary of unigram translations, we then project
each M -dimensional source language context vec-
tor into the N -dimensional target language context
vector space. To compare a given pair of source
and target context vectors, C
p
s
and C
p
t
, respec-
tively, we compute their cosine similarity, or their
dot product divided by the product of their magni-
tudes:
sim
contextual
pp
s
, p
t
q ?
C
p
s
? C
p
t
||C
p
s
||||C
p
t
||
For a given phrase pair in our phrase table, we
estimate phrasal contextual similarity by directly
comparing the context vectors of the two phrases
themselves. Because context vectors for phrases,
which tend to be less frequent than words, can be
sparse, we also compute lexical contextual simi-
larity over phrase pairs. We define lexical con-
textual similarity as the average of the contextual
similarity between all word pairs within the phrase
pair.
Topic Similarity Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. We estimate topic
similarity using the distribution of words and
phrases across Wikipedia pages, for which we
have interlingual French-English links. Specif-
ically, we compute topical vectors by counting
the number of occurrences of each word and
phrase across Wikipedia pages. That is, for each
source and target phrase, p
s
and p
t
, we collect M -
dimensional topic vectors, where M is the number
of Wikipedia page pairs used (in our experiments,
M is typically 5, 000). We use Wikipedia?s inter-
lingual links to align the French and English topic
vectors and normalize each topic vector by the to-
tal count. As with contextual similarity, we com-
pare a pair of source and target topic vectors, T
p
s
and T
p
t
, respectively, using cosine similarity:
sim
topic
pp
s
, p
t
q ?
T
p
s
? T
p
t
||T
p
s
||||T
p
t
||
We estimate both phrasal and lexical topic simi-
larity for each phrase pair. As before, lexical topic
similarity is estimated by taking an average topic
similarity across all word pairs in a given phrase
pair.
Orthographic Similarity We make use of one
additional signal of translation equivalence: ortho-
graphic similarity. In this case, we do not refer-
ence comparable corpora but simply compute the
edit distance between a given pair of phrases. This
signal is often useful for identifying translations
of technical terms, which appear frequently in our
medical and science domain corpora. However,
because of word order variation, we do not mea-
sure edit distance on phrase pairs directly. For ex-
ample, French embryon humain translates as En-
glish human embryo; embryon translates as em-
bryo and humain translates as human. Although
both word pairs are cognates, the words appear
in opposite orders in the two phrases. Therefore,
directly measuring string edit distance across the
phrase pair would not effectively capture the relat-
edness of the words. Hence, we only measure lex-
ical orthographic similarity, not phrasal. We com-
pute lexical orthographic similarity by first com-
puting the edit distance between each word pair,
w
s
and w
t
, within a given phrase pair, normalized
by the lengths of the two words:
sim
orth
pw
s
, w
t
q ?
edpw
s
, w
t
q
|w
s
||w
t
|
2
We then compute the average normalized edit dis-
tance across all word pairs.
The above similarity metrics all allow for scores
of zero, which can be problematic for our log-
439
Corpus Source Words Target Words
Training
Canadian Hansard 161.7 m 144.5 m
Tune-1 / Tune-2 / Test
Medical 53k / 43k / 35k 46k / 38k / 30k
Science 92k / 120k / 120k 75k / 101k / 101k
Language Modeling and Comparable Corpus Selection
Medical - 5.9 m
Science - 3.6 m
Table 1: Summary of the size of each corpus of text used
in this work in terms of the number of source and target word
tokens.
linear translation models. We describe our ex-
periments with different minimum score cutoffs in
Section 4.2.
4 Experimental Setup
4.1 Data
We assume that the following data is available in
our translation setting:
? Large old-domain parallel corpus for training
? Small new-domain parallel corpora for tun-
ing and testing
? Large new-domain English monolingual cor-
pus for language modeling and identifying
new-domain-like comparable corpora
? Large mixed-domain comparable corpus,
which includes some text from the new-
domain
These data conditions are typical for many real-
world uses of machine translation. A summary of
the size of each corpus is given in Table 1.
Our old-domain training data is taken from
the Canadian Hansard parliamentary proceedings
dataset, which consists of manual transcriptions
and translations of meetings of the Canadian par-
liament. The dataset is substantially larger than
the commonly used Europarl corpus, containing
over 8 million sentence pairs and about 150 mil-
lion word tokens of French and English.
For tuning and evaluation, we use new-domain
medical and science parallel datasets released by
Irvine et al. (2013a). The medical texts con-
sist of documents from the European Medical
Agency (EMEA), originally released by Tiede-
mann (2009). This data is primarily taken from
prescription drug label text. The science data is
made up of translated scientific abstracts from the
fields of physics, biology, and computer science.
For both the medical and science domains, we
use three held-out parallel datasets of about 40
and 100 thousand words,
3
respectively, released
by Irvine et al. (2013a). We do tuning on dev1,
additional parameter selection on test2, and blind
testing on test1.
We use large new-domain monolingual English
corpora for language modeling and for selecting
new-domain-like comparable corpora from our
mixed domain comparable corpus. Specifically,
we use the English side of the medical and science
training datasets released by Irvine et al. (2013a).
We do not use the parallel French side of the train-
ing data at all; our data setting assumes that no
new-domain parallel data is available for training.
We use Wikipedia as a source of compara-
ble corpora. There are over half a million
pairs of inter-lingually linked French and English
Wikipedia documents.
4
We assume that we have
enough monolingual new-domain data in one lan-
guage to rank Wikipedia pages according to how
new-domain-like they are. In particular, we use
our new-domain English language modeling data
to measure new-domain-likeness. We could have
targeted our learning even more by using our new-
domain French test sets to select comparable cor-
pora. Doing so may increase the similarity be-
tween our test data and comparable corpora. How-
ever, to avoid overfitting any particular test set, we
use our large English new-domain language mod-
eling corpus instead.
For each inter-lingually linked pair of French
and English Wikipedia documents, we compute
the percent of English phrases up to length four
that are observed in the English monolingual new-
domain corpus and rank document pairs by the ge-
ometric mean of the four overlap measures. More
sophisticated ways to identify new-domain-like
Wikipedia pages (e.g. (Moore and Lewis, 2010))
may yield additional performance gains, but, qual-
itatively, the ranked Wikipedia pages seem rea-
sonable for the purposes of generating a large set
of top-k new-domain document pairs. The top-10
ranked pages for each domain are listed in Table 2.
The top ranked science domain pages are primar-
ily related to concepts from the field of physics
but also include computer science and chemistry
3
Or about 4 thousand lines each. The sentences in the
medical domain text are much shorter than those in the sci-
ence domain.
4
As of January 2014.
440
Science Medical
Diagnosis (artificial intelligence) Pregabalin
Absorption spectroscopy Cetuximab
Spectral line Fluconazole
Chemical kinetics Calcitonin
Mahalanobis distance Pregnancy category
Dynamic light scattering Trazodone
Amorphous solid Rivaroxaban
Magnetic hyperthermia Spironolactone
Photoelasticity Anakinra
Galaxy rotation curve Cladribine
Table 2: Top 10 Wikipedia articles ranked by their similar-
ity to large new-domain English monolingual corpora.
topics. The top ranked medical domain pages are
nearly all prescription drugs, which makes sense
given the content of the EMEA medical corpus.
4.2 Phrase-based Machine Translation
We word align our old-domain training corpus
using GIZA++ and use the Moses SMT toolkit
(Koehn et al., 2007) to extract a translation gram-
mar. In this work, we focus on phrase-based
SMT models, however our approach to using new-
domain comparable corpora to estimate translation
scores is theoretically applicable to any type of
translation grammar.
Our baseline models use a phrase limit of seven
and the standard phrase-based SMT feature set, in-
cluding forward and backward phrase and lexical
translation probabilities. Additionally, we use the
standard lexicalized reordering model. We exper-
iment with two 5-gram language models trained
using SRILM with Kneser-Ney smoothing on (1)
the English side of the Hansard training corpus,
and (2) the relevant new-domain monolingual En-
glish corpus. We experiment with using, first, only
the old-domain language model and, then, both the
old-domain and the new-domain language models.
Our first comparison system augments the stan-
dard feature set with the orthographic similarity
feature, which is not based on comparable cor-
pora. Our second comparison system uses both
the orthographic feature and the contextual and
topic similarity features estimated over a random
set of comparable document pairs. The third sys-
tem estimates contextual and topic similarity using
new-domain-like comparable corpora. We tune
our phrase table feature weights for each model
separately using batch MIRA (Cherry and Fos-
ter, 2012) and new-domain tuning data. Results
are averaged over three tuning runs, and we use
the implementation of approximate randomization
released by Clark et al. (2011) to measure the
statistical significance of each feature-augmented
model compared with the baseline model that uses
the same language model(s).
As noted in Section 3, the features that we
estimate from comparable corpora may be zero-
valued. We use our second tuning sets
5
to tune
a minimum threshold parameter for our new fea-
tures. We measure performance in terms of BLEU
score on the second tuning set as we vary the new
feature threshold between 1e?07 and 0.5 for each
domain. A threshold of 0.01, for example, means
that we replace all feature with values less than
0.01 with 0.01. For both new-domains, perfor-
mance drops when we use thresholds lower than
0.01 and higher than 0.25. We use a minimum
threshold of 0.1 for all experiments presented be-
low for both domains.
5 Results
Table 3 presents a summary of our results on the
test set in each domain. Using only the old-domain
language model, our baselines yield BLEU scores
of 22.70 and 21.29 on the medical and science test
sets, respectively. When we add the orthographic
similarity feature, BLEU scores increase signifi-
cantly, by about 0.4 on the medical data and 0.6 on
science. Adding the contextual and topic features
estimated over a random selection of comparable
document pairs improves BLEU scores slightly in
both domains. Finally, using the most new-domain
like document pairs to estimate the contextual and
topic features yields a 1.3 BLEU score improve-
ment over the baseline in both domains. For both
domains, this result is a statistically significant im-
provement
6
over each of the first three systems.
In both domains, the new-domain language
models contribute substantially to translation qual-
ity. Baseline BLEU scores increase by about
6 and 5 BLEU score points in the medical and
science domains, respectively, when we add the
new-domain language models. In the medical do-
main, neither the orthographic feature nor the or-
thographic feature in combination with contextual
and topic features estimated over random docu-
ment pairs results in a significant BLEU score
improvement. However, using the orthographic
feature and the contextual and topic features es-
timated over new-domain document pairs yields a
5
test2 datasets released by Irvine et al. (2013a)
6
p-value ? 0.01
441
Language Model(s) System Medical Science
Old
Baseline 22.70 21.29
+ Orthographic Feature 23.09* (`0.4) 21.86* (`0.6)
+ Orthographic & Random CC Features 23.22* (`0.5) 21.88* (`0.6)
+ Orthographic & New-domain CC Features 23.98* (`1.3) 22.55* (`1.3)
Old+New
Baseline 28.82 26.18
+ Orthographic Feature 29.02 (`0.2) 26.40* (`0.2)
+ Orthographic & Random CC Features 28.86 (`0.0) 26.52* (`0.3)
+ Orthographic & New-domain CC Features 29.16* (`0.3) 26.50* (`0.3)
Table 3: Comparison between the performance of baseline old-domain translation models and domain-adapted models in
translating science and medical domain text. We experiment with two language models: old, trained on the English side of our
Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We
use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All
results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented
with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are
statistically significant with p ? 0.01.
small but significant improvement of 0.3 BLEU.
In the science domain, in contrast, all three aug-
mented models perform statistically significantly
better than the baseline. Contextual and topic fea-
tures yield only a slight improvement above the
model that uses only the orthographic feature, but
the difference is statistically significant. For the
science domain, when we use the new domain lan-
guage model, there is no difference between esti-
mating the contextual and topic features over ran-
dom comparable document pairs and those chosen
for their similarity with new-domain data.
Differences across domains may be due to the
fact that the medical domain corpora are much
more homogenous, containing the often boiler-
plate text of prescription drug labels, than the sci-
ence domain corpora. The science domain cor-
pora, in contrast, contain abstracts from several
different scientific fields; because that data is more
diverse, a randomly chosen mixed-domain set of
comparable corpora may still be relevant and use-
ful for adapting a translation model.
We experimented with varying the number of
comparable document pairs used for estimating
contextual and topic similarity but saw no sig-
nificant gains from using more than 5, 000 in ei-
ther domain. In fact, performance dropped in the
medical domain when we used more than a few
thousand document pairs. Our proposed approach
orders comparable document pairs by how new-
domain-like they are and augments models with
new features estimated over the top-k. As a result,
using more comparable document pairs means that
there is more data from which to estimate sig-
nals, but it also means that the data is less new-
domain like overall. Using a domain similarity
threshold to choose a subset of comparable doc-
ument pairs may prove useful in future work, as
the ideal amount of comparable data will depend
on the type and size of the initial mixed-domain
comparable corpus as well as the homogeneity of
the text domain of interest.
We also experimented with using a third lan-
guage model estimated over the English side of
our comparable corpora. However, we did not see
any significant improvements in translation qual-
ity when we used this language model in combina-
tion with the old and new domain language mod-
els.
6 Conclusion
In this work, we targeted SMT errors due
to translation model scores using new-domain
comparable corpora. Our old-domain French-
English baseline model was trained on the Cana-
dian Hansard parliamentary proceedings dataset,
which, at 8 million sentence pairs, is one of the
largest publicly available parallel datasets. Our
task was to adapt this baseline to the medical and
scientific text domains using comparable corpora.
We used new-domain parallel data only to tune
model parameters and do evaluation. We mined
Wikipedia for new-domain-like comparable docu-
ment pairs, over which we estimated several addi-
tional features scores: contextual, temporal, and
orthographic similarity. Augmenting the strong
baseline with our new feature set improved the
quality of machine translations in the medical and
science domains by up to 1.3 BLEU points.
442
7 Acknowledgements
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Workshop on Sta-
tistical Machine Translation (WMT).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to sta-
tistical language modeling for chinese. ACM Trans-
actions on Asian Language Information Processing
(TALIP).
Guillem Gasc?o, Martha-Alicia Rocha, Germ?an
Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Workshop on Statistical Machine
Translation (WMT).
Barry Haddow. 2013. Applying pairwise ranked op-
timisation to improve the interpolation of transla-
tion models. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Ann Irvine, John Morgan, Marine Carpuat, Hal Daum?e
III, and Dragos Munteanu. 2013a. Measuring ma-
chine translation errors in new domains. Transac-
tions of the Association for Computational Linguis-
tics, 1(October).
Ann Irvine, Chris Quirk, and Hal Daume III. 2013b.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Workshop on Statistical
Machine Translation (WMT).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
443
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Fifth European Conference on Speech Com-
munication and Technology.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining translation and language model
scoring for domain-specific data filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT).
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the Conference
of the Association for Computational Linguistics
(ACL).
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).
J?org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING).
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
444
