  
An Agent-based Approach to Chinese Named Entity Recognition 
 
Shiren Ye Tat-Seng Chua Liu Jimin 
School of Computing, National University of Singapore, 
Singapore, 117543 
yesr@comp.nus.edu.sg chuats@comp.nus.edu.sg Liujm@comp.nus.edu.sg 
   
Abstract 
Chinese NE (Named Entity) recognition is 
a difficult problem because of the 
uncertainty in word segmentation and 
flexibility in language structure. This paper 
proposes the use of a rationality model in a 
multi-agent framework to tackle this 
problem. We employ a greedy strategy and 
use the NE rationality model to evaluate 
and detect all possible NEs in the text. We 
then treat the process of selecting the best 
possible NEs as a multi-agent negotiation 
problem. The resulting system is robust 
and is able to handle different types of NE 
effectively. Our test on the MET-2 test 
corpus indicates that our system is able to 
achieve high F1 values of above 92% on all 
NE types. 
1. Introduction 
Named entity (NE) recognition is a fundamental 
step to many language processing tasks. It was a 
basic task of the Message Understanding 
Conference (MUC) and has been studied 
intensively. Palma & Day (97) reported that 
person (PER), location (LOC) and organization 
(ORG) names are the most difficult sub-tasks as 
compared to other entities as defined in MUC. 
This paper thus focuses only on the recognition 
of PER, LOC and ORG entities. 
Recent research on NE recognition has been 
focused on the machine learning approach, such 
as the transformation-based learning (Aberdeen 
95), hidden Markov model (Bikel et al 97), 
decision tree (Sekin et al 98), collocation 
statistics (Lin 98), maximum entropy model 
(Borthwick 99), and EM bootstrapping 
(Cucerzan & Yarowsky 99). Other than English, 
several recent works examined the extraction of 
information from Spanish, Chinese, and 
Japanese (Isozaki 01). Most approaches for 
Chinese NE recognition used handcrafted rules, 
supplemented by word or character frequency 
statistics. These methods require a lot of 
resources to model the NEs. Chen et al (98) 
used 1-billion person name dictionary and 
employed mainly internal word statistics with no 
generalization. Yu et al (98) employed a 
common framework to model both the context 
and information residing within the entities, and 
performed rule generalization using POS 
(part-of-speech) and some semantic tags. A 
similar system is also reported in Luo & Song 
(01). 
Chinese NE recognition is much more difficult 
than that in English due to two major problems. 
The first is the word segmentation problem 
(Sproat et al 96, Palmer 97). In Chinese, there is 
no white space to delimit the words, where a 
word is defined as consisting of one or more 
characters representing a linguistic token. Word 
is a vague concept in Chinese, and Palmer (97) 
showed that even native speakers could only 
achieve about 75% agreement on ?correct? 
segmentation. As word segmentation is the basic 
initial step to almost all linguistic analysis tasks, 
many techniques developed in English NLP 
cannot be applied to Chinese. 
Second, there is no exterior feature (such as the 
capitalization) to help identify the NEs, which 
share many common characters with non-NE (or 
common words). For example, while ?  is 
normally associated with the country China, it 
could also mean the concepts in, at or hit; and? 
normally refers to the surname Zhang, but it also 
means the concepts open, sheet or spread. 
Moreover, proper names in Chinese may contain 
common words and vice versa. 
Because of the above problems, the use of 
statistical and heuristic rules commonly adopted 
in most existing systems is inadequate to tackle 
the Chinese NE recognition problem. In this 
paper, we consider a new approach of employing 
a rationality model in a multi-agent framework. 
 The main ideas of our approach are as follows. 
First, we use an NE rationality measure to 
evaluate the probability of a sequence of tokens 
being a specific NE type, and adopt a greedy 
approach to detect all possible NEs. Second, we 
treat the process of selecting the best NEs among 
a large set of possibilities as a multi-agent 
negotiation problem. We test our overall 
approach on the MET-2 test set and the system is 
able to achieve high F1 values of over 92% on all 
NE types. The results are significantly better 
than most reported systems on MET-2 test set. 
The rest of the paper describes the details of our 
rationality-based and multi-agent negotiation 
approach to detect and refine NEs. 
2. Rationality Model for NE Detection 
2.1 Named Entity and Its tokens Feature 
For clarity and without lost of generality, we 
focus our discussion mainly on PER entity. The 
problems and techniques discussed are 
applicable to LOC and ORG entities. We 
consider a simple PER name model comprising 
the surname followed by the first-name. Given 
the presence of a surname (as cue-word) in a 
token sequence, we compute the likelihood of 
this token playing the role of surname and the 
next token as the first-name. The pair could be 
recognized as PER only if both tokens are 
labeled as positive (or of the right types) as 
shown in Table 1. If either one of both of the 
tokens are evaluated negatively, then the pair 
will not be recognized as PER based on the 
model defined above.  
Sentence PER? Label Remarks 
?????? Y ?(+) ?(+) 
... invite Zhang Fei 
to speak ... 
?????? N ?(-) ?(-) 
? a piece of airline 
ticket ? 
?????? ? ?(+) ?(-) //Illegal PER 
????* ? ?(-) ?(+) //Illegal PER 
* Strictly, ??? and Mr. Zhang are not really person names. 
They are references to person names and should be detected via 
co-reference.  
Table 1: An example of NE and non-NE 
Although the example depicted in Table 1 is very 
simple, the same idea can be extended to the 
more complex NE Types for ORG and LOCs. 
The number of tokens in a NE may vary from 2 
in PER to about 20 for ORG. One constraint is 
that the sequencing of tokens and their labels 
must be consistent with the respective NE type. 
Also, there are grammatical rules governing the 
composition of different NE type. For example, 
LOC may consist of a sequence of LOCs; and 
ORG may include PER and/or LOC on its left. 
Thus by considering one pair of tokens at a time, 
and by extending the token sequence to the 
adjacent token one at a time, we can draw 
similar conclusion as that depicted in Table 1 for 
complex NE types. 
2.2 The Rationality Computation 
If we know the probability distribution of each 
type of token in a window, NE recognition is 
then the procedure of evaluating the rationality 
or certainty of a sequence of tokens with respect 
to a NE type. Motivated by the results in Table 1 
we view NE recognition as a special coloring 
problem. Initially, all the tokens in the corpus are 
considered as a sequence of White balls. Given a 
chain of tokens appears in a NE window, we 
want to use the probability distribution of these 
tokens to re-paint some of the white balls to 
different colors. A sequence of appropriately 
colored balls would induce an appropriate NE. 
For simplicity, we again focus on PER NE type 
with 2 tokens. The surname token will be 
colored red and first-name blue. We assume that 
the number of PER names in the corpus is N, 
and the rest of tokens is M. Because there are N 
surname and N first-name tokens in the corpus, 
the total number of tokens is M+2N. Hence the 
marginal probability of PER name is 
Pr(PER)=N/(2N+M) . 
 Red  Blue White 
 Format Pr. Format Pr. Format Pr. 
Red aRbR 0 aRbB 1 aRbW 0 
Blue aBbR 
N/(N
+M) aBbB 0 aBbW 
M/(N
+M)
White aWbR 
N/(N
+M) aWbB 0 aWbW 
M/(N
+M)
Note: Red ? Surname; Blue ? First-name; White - Others 
Table 2: Possibility combination of neighboring 
tokens within the corpus for PER 
Table 2 shows the possible relationships 
between the red and blue balls for the PER NE 
type by  considering the grammer that the 
surname must be followed by a first-name in a 
 formal PER. As we only permit the token pair 
for PER to be labeled as a red ball followed by a 
blue ball, the following sequences are not 
possible under our model: (a) a red (or blue) ball 
follows by itself; (b) a red ball follows by white 
ball; and (c) a white ball follows by the blue ball. 
Thus aRbR (a red follows by a red), aRbW, aBbB, 
and aWbB are illegal combinations. 
Given a pair of tokens a and b in the corpus, 
they are labeled as surname |aR| and |bR| times, as 
first-name |aB| and |bB| times, and as non-PER 
|aW| and |bW| times respectively. The expected 
value of a token sequence ab representing a PER 
when a is red and b is blue is: 
 | | | | | || | | | B R BR B R
b a b
a b a
N N
?= ? =  (1) 
The expected value of the cases when the token 
pair ab is not a PER name is the sum of expected 
values of four cases: aBbR, aBbW, aWbR, aWbW (see 
Table 2), which after simplification, is given by: 
 
| | | | | | | | | |
| | | | | | | | | | | || | | |
(| | | |) (| | | |)
(2)
B R B W W R W WR B
B W W R W RB R
B W R W
a b a b a b a b a b
a b a b a ba b
N M N M N M N M
a a b b
N M
= + + +
? ? ??= + + ++ + + +
+ ? += +
 
The ratio between the cases when ab is a PER 
versus when ab is not a PER is: 
| |
| |
C R BR B
ab a b
R B
a b
a b
?? = = ?? ??     (3) 
where | | | |;
(| | | |) (| | | |)
R BR B
a b
B W R W
a b
a a b b
? = ? =+ + ; 
and N M
N
? += . We call RbRacab and ??? ,,  
the rationality values of tokens ab, a and b of 
being a PER, red ball or blue ball respectively. 
On the other hand, the probabilities of a as a 
surname (red ball) and b as a first-name (blue 
ball) are: 
| | | |
,
| | (| | | |) | | (| | | |)
R BR B
a b
R B W B R W
a b
P P
a a a b b b
= =+ + + +  
Thus, ;
1 1
R B
R Ba b
a bR B
a b
P P
P P
? = ? =? ?  (4) 
The form of Equation (4) is similar to the 
concept of odds likelihood O(h), first introduced 
in Duda et al (79) as a generic term to denote 
the ratio of the probability and converse 
probability in the Prospector system, namely: 
 ( ) ( )( )
(- ) 1- ( )
P h P hO h
P h P h
= =  (5) 
Eq. (5) is used in a modified version of the 
Bayes theorem to solve the uncertainty 
reasoning problems. Surprisingly, our approach 
of rationality ? for NE with two tokens can be 
deduced as the product of their odds-likelihood. 
By linking the concept of odds-likelihood and 
rationality, we can compute the probability of a 
sequence of tokens being a specific NE type. 
Since the rationality values of tokens could 
vary from 0 to ?, it may incur overflow or 
underflow during the rationality evaluation. This 
is especially so for unknown tokens where their 
rationality values will be zero. To resolve this 
problem, we construct a piecewise function to 
map the rationality values from the range [0, ?] 
to [?min,?max]. Here we set the parameters 
?min=0.05 and ?max=50, and ensure that most 
rationality values will retain their original values 
after transformation. 
2.3 The Context of NEs 
In addition to identifying the structural 
information within the NEs, it is equally 
important to model the context around the NEs. 
Context is especially pivotal to language such as 
the Chinese or Korean where there is no white 
space and capital characters among the tokens. 
For PER type, the context tokens are likely to be 
person titles and action words. 
 
Figure 1: A NE detection window 
Thus after we have computed the rationality 
values of possible NEs, we enlarge the analysis 
window to cover both the NE candidate and its 
context. As shown in Figure 1, the window 
consists of three components: prefix, suffix and 
the NE candidate. If the NE is at the beginning 
or end of a paragraph, then the corresponding 
Prefix
Boundary of a possible 
Window
Suffix
?? ?   ??   ?? ?? 
NE 
 prefix or suffix is set to void. We can extend the 
rationality computation for an NE to the context 
window by incorporating both the prefix and 
suffix tokens separately. 
2.4 The Overall Procedure 
The overall procedure for estimating the 
likelihood of an NE among a sequence of tokens 
is as follows. 
a) Convert prior probability Pr(e) of each token 
e to rationality ?(e). A token e may have 
multiple Pr(e) values, each is dependent on 
the role token e plays in a possible NE, such 
as the probability of being a surname, 
first-name, prefix, suffix, general token or 
cue-word. 
b) At each cue-word position, compute the 
rationality of a possible NE by considering 
one pair of tokens at a time, and extending to 
the next token on the left or right depending 
on the NE type. The boundaries of PERs are 
extended forward; while that of ORGs and 
LOCs are extended backward. Each extension 
will produce a new NE candidate. The scope 
of the extension is also determined by the 
type of NE. The process terminates when the 
rationality value of the next token falls below 
a minimum threshold. 
c) For all possible NEs, construct the context 
window and compute its final rationality 
value within the context window. 
The process will result in multiple possible NEs, 
with most NEs overlapping with one another. 
3. Multi-Agent Framework for NE 
Confirmation 
3.1 Relationships between possible NEs 
Our greedy approach of identifying all possible 
NEs using the rationality model results in over 
segmentation of NEs. Figure 2 shows a list of 80 
possible NEs detected from a test article in the 
MET-2 test corpus. The number of correct NEs 
in this case is only 13. These possible NEs relate 
to each other in a complex way. The possible 
relationships between them are: 
a. Overlapping: This is the most common case 
when the tokens of multiple NEs overlap each 
other. Examples include ????????? 
and ???????????. They are both 
reasonable ORGs if considered separately. 
However, only one of them can be true. 
b. Repetition: Some possible NEs may repeat 
themselves with same or similar tokens. For 
example, the NE ???????????? 
is similar to ??????????????
???? in different part of the text. It means 
that these NEs have same beliefs and could 
cooperate to enhance each other?s belief. 
Figure 2: All possible NEs identified in a test article 
c. Unification: When the tokens of two NEs are 
adjacent to each other in a sentence, they may 
be unified to become a new NE by combining 
their tokens. For instance, the NEs ???? 
and ???? may be combined to form a new 
NE ????? ?. By the way, not all 
neighboring NEs can be unified because the 
unification must satisfy the syntactic and 
semantic specifications of the language. For 
example, two adjoining PERs cannot be 
unified, while it is possible for LOCs. 
d. Enumerated name list: This is a common 
language construct to present a list of names. 
An example of such construct is: ?????
(??)???, ?????????, and 
???????????????. 
If we knew the relationships between possible 
NEs, we can use this knowledge to modify the 
rationality values of possible NEs. The first 
relationship (overlapping) is of type competition 
while the other three are of type supporting. In a 
competition relationship, the rationality values of 
losing NEs are decremented, whereas in a 
supporting relationship, the rationality of the 
winning NE can be used to reinforce other NEs. 
??????????????????????????
???????? ??  
???????????????? 
??????????????????????? 
????????????????????? 
???????????????????????????
???????????? 
a team
 3.2 Agent-based Reasoning & Negotiation 
There is a need to modify the rationality values 
of possible NEs in order to identify the best 
possible NEs. One way to achieve this is to 
employ a decision tree (Sekine 98) to select the 
best possible candidates. However, it is difficult 
to use the decision tree to handle multiple 
relationships between conflicting NEs, and to 
perform incremental updates of rationality 
values in situations where the number, 
distribution and relationships in possible NEs are 
uncertain. In this work, we adopt a multi-agent 
approach to refine the rationality of possible NEs 
and vote the best potential NEs. 
Agents are software entities that perform some 
operations on behalf of their users or another 
programs with some degree of autonomy, and in 
so doing, employ some knowledge or 
representation of the user?s goals or desires (Don 
et al 96). In our system, we map every possible 
NE detected to an agent, which acts as the 
deputy of the NE and depicts all its attributes. 
Following the approach taken in the DBI system, 
we use the rationality of the NE as the belief, 
denoted by Br(A), of agent A. Agents are 
divided into Teams (Decker & Lesser 95) 
according to their contents and positions in the 
corpus. The division of agents into teams 
facilitates the negotiation of agents? beliefs.  
The negotiation between agents aims to 
eliminate underlying conflicts and uncertainty 
among them. The process of multi-agent 
negotiation is carried out as follows. 
a. We identify agents involved in an unification 
relationship. These agents will be unified if 
the constraints of unification are fulfilled. The 
new agents would inherit the evidences, 
including the rationality values, of its child 
agents. 
b. We divide the resulting agents into teams. 
Agents with overlapping tokens will be 
grouped into same teams, while independent 
agents will be assigned to different teams. 
c. We perform negotiation between agents based 
on the type of their relationship. For agents 
that are in competition relationship (i.e. those 
overlapping agents within the same team), we 
select the agent with the maximal belief (said 
ai) as the winner, and decrement the beliefs of 
the rest of Nt agents in the same team by ?(ai), 
i.e. 
  Br(aj) = Br(aj) - ?(ai), for j=1,.. Nt, and j?i 
 For agents involved in the supporting 
relations, we again select the agent with the 
maximal belief (of say ak) as the winner, but 
increment the rest of agents in the same set Sk 
by ?(ak), i.e. 
  Br(aj) = Br(aj) + ?(ak), for all j in Sk & j?k 
d. Repeat step c until the pre-defined rounds of 
negotiations have been reached. 
In order to ensure fairness in the negotiation 
process, we limit the amount of belief 
adjustment, ?(ai), during each round of 
negotiation. If the desired rounds of negotiation 
is NR, then the amount of adjustment in each 
round should be limited to ?(ai)/NR. NR should 
be set to allow all agents to have a fair chance to 
participate in the negotiation process. Here we 
set NR to 10. 
At the end of negotiation, only agents whose 
beliefs are greater than the threshold are selected. 
Figure 3 shows the resulting set of NEs derived 
from the list given in Figure 2. 
 
 
 
 
 
 
 
Fig. 3: NEs after agents-based modification 
4. The Overall Process of NE Recognition 
Since there is no white space between words in 
Chinese, the first essential step is to perform 
preliminary segmentation. Here, we adopt a 
greedy approach of generating all possible 
segmentation from the input text by performing 
the dictionary-based look-up using a common 
word dictionary. The common word dictionary is 
generated from the PKU corpus (Yu 99) (see 
Section 5.1). 
Second, we compute the rationality value of 
each token in the context of being a keyword, 
general word, or as boundary (prefix or suffix) 
of a specific NE type. 
????????????????????????
?????????? ?? ??????????
???????????????????????
???????????????????????
?????????????? ????????
?????????????????????
 Third, we identify all possible NE cue-words 
and use them as seeds of NE candidates. We 
construct all possible NEs from the cue-word 
positions through boundary extension and 
context inclusion. 
Forth, we modify the rationality values of all 
possible NEs using the agent-based negotiation 
methodology. The conflicts between possible 
NEs will disappear. 
Fifth, we select NEs with the labels of its 
corresponding seed if their rationality values are 
above a predefined limit ?. The value ? affects 
the balance between recall and precision. 
5. Experimental Results and Discussions 
5.1 The Datasets Used in Our Experiments 
We use a number of openly available datasets 
for our training and testing, including the 
PKU-corpus (Yu 99), Hownet (Dong & Dong 
00), MET2 Chinese resources (Chinchor 02), 
and two name lists (for foreign and ORG names) 
collected from the web by using a bootstrapping 
approach. The PKU is a manually tagged corpus 
containing one-month of news report from 
China?s People Daily. It uses over 30 POS tags 
including separate tags for surname and 
first-name. It contains about 37,000 sentences 
with over 106 tokens. From these resources, we 
generate the following dictionaries and statistics.  
a. We use the PKU corpus to build a common 
word dictionary by removing all words that 
are tagged as NE. The resulting dictionary 
contains 37,025 common words. 
b. From the PKU corpus, we compute each 
token?s distribution information based on its 
POS tags, and if it is an NE, its NE type and 
its role with respect to the NE. Altogether, we 
obtain the distribution information of about 
37,000 different tokens. 
c. We maintain a list of LOCs found in the 
MET-2 test corpus. We do not maintain the 
PER and ORG lists, because their 
re-occurrence probabilities are low. 
d. We supplement the distribution information 
derived in step (b) by incorporating tokens 
obtained from other resources stated above. 
The resources we derived are available for down 
loading at http://www.pris.nus.edu.sg/ie.html 
5.2 The Experiment and Results 
We test our resulting model on the MET-2 test 
corpus. Table 3 tabulates the results of our 
system in terms of recall (Rc), precision (Pr) and 
F1 measures. In order to demonstrate the 
effectiveness of our approach, we perform the 
tests under 3 different test configurations. 
a. We perform the baseline test by simply 
performing name-dictionary look-up. Notice 
that we do not use PER dictionary, and hence 
the performance under PER is left blank (*). 
b. We extract all possible NEs by using only the 
rationality-based approach where the 
threshold ? is set to 1.1. If there are conflicts 
between possible NEs, we simply select the 
NE with the maximal rationality. 
c. We employ the agent-based modification in 
conjunction with the rationality-based 
approach to select the best possible NEs. 
For comparison purpose, we also list in Table 3 
the corresponding results reported in Yu et al 
(98) and Chen et al (98) for the MUC-7 tests. 
Type NC NP NW NM NS Rc Pr F1
Base- 
line test 
(a) 
ORG
PER 
LOC 
79 3 0 295 0 
*   * * * * 
363 84 0 303 26 
21 98 35.0
*  * * 
54 86 66.0
Config 
(b) 
ORG
PER 
LOC 
309 5 28 35 47 
154 2 7 11 87 
618 0 29 103 112 
83 79 81.0
89 62 73.4
82 81 81.7
Config 
(c) 
ORG
PER 
LOC 
356 2 5 14 21 
167 1 2 4 9 
703 0 18 29 52 
95 93 93.7
96 93 94.7
94 91 92.3
Results 
of Chen 
et (98) 
ORG
PER 
LOC 
393 0 7 77 44 
159 0 0 25 56 
583 0 65 102 194 
78 83 81.3
91 74 81.6
78 69 73.2
Results 
of Yu et 
al. (98) 
ORG
PER 
LOC 
331 0 14 32 25 
160 0 7 7 74 
682 0 1 67 83 
88 89 88.5
92 66 76.7
91 89 0.0
where Pr = (NC + 0.5*NP)/(NC + NW + NP + NS); 
 Rc = (NC + 0.5*NP)/(NC + NW + NP + NM); 
 F1 = 2*Pr*Rc/(Pr+Rc). 
 and NC gives the number of NEs correctly recognized; 
   NP denotes the number of NEs partially recognized; 
   NW gives the number of NEs incorrectly recognized; 
   NM denotes the number of NEs missed; and finally 
   NS gives the number of NEs found by the system but not 
in the tagged list. 
Table 3: Results of MET2 under different configurations 
Table 3 shows that as we apply the rationality 
model (Config. b) followed by multi-agent 
framework (Config. c), the performance of the 
system improves steadily until it reaches a high 
performance of over 92% in F1 value. In fact 
 Config c results in significant improvements 
over Conig b in both precision and recall forall 
NE types. This shows that the agent-based 
modification could significantly reduce spurious 
and missing NEs. The performance of our 
overall system is significantly better than both 
reported systems as listed in Table 3. 
To demonstrate the effectiveness of our 
approach on general web-based documents, we 
perform another informal test to recognize NEs 
on the 100 randomly collected headline news 
articles from the well-known Chinese web sites 
(www.sina.com.cn, www.sohu.com, www. 
zaobao.com, www.Chinese times.com). The 
topics covered in these articles ranging from 
politic, economic, society to sports. The 
informal test shows that our approach could 
perform well on general web-based articles with 
F1 measures of over 90%. 
6. Conclusion 
Chinese NE recognition is a difficult problem 
because of the uncertainty in word segmentation. 
Many existing techniques that require 
knowledge of word segmentation, and syntactic 
and semantic tagging of text cannot be applied. 
In this paper, we propose a new approach of 
employing a rationality model in a multi-agent 
framework. We employ a greedy strategy and 
use the NE rationality measures to detect all 
possible NEs in the text. We then treat the 
process of selecting the best possible NEs as the 
multi-agent negotiation problem. The resulting 
system is robust and is able to handle different 
NE models. Our test on the MET-2 test corpus 
indicates that we could achieve high F1 values of 
above 92% on all NE types. 
We plan to further test our system on a 
large-scale test corpus. We will refine our 
techniques on a wide variety of text corpuses, 
and apply the bootstrapping technique to tackle 
the data sparseness problem. Finally, we will 
extend our research to perform relation and 
information extraction in multilingual text. 
References 
Bikel D.M., Schwartz R. & Weischedel R.M. (1999) 
An Algorithm that Learns What?s in a Name. 
Machine Learning, 34(1-3), 211-231 
Borthwick A. (1999) A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis, New 
York Univ.  
Chen H. H., Ding Y. W. Tsai S.C. & Bian, G.W. (1998) 
Description of the NTU System used for MET-2. In 
MUC-7 Proc. 
Chinchor N. A. (2002), http://www.itl.nist.gov/iaui/ 
894.02/related_projects/muc/. 
Cucerzan S. & Yarowsky D. D. (1999) Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proc 
of 1999 Joint SIGDAT Conference on Empirical 
Methods in NLP & Very Large Corpora, 90-99. 
Decker K., & Lesser V. (1995) Designing a Family of 
Coordination Algorithm, In Proc Of 1st Int?l Conf. 
on Multiagent Sys, 73-80, Menlo Park, CA, AAAI 
Press. 
Don Gilbert, Manny Aparicio, et al(1996) White 
paper on intelligent agents (IBM), http://activist. 
gpl.ibm.com:81/WhitePaper/ptc2.htm. 
Dong Z.D. & Dong Q. (2000) HowNet, available at 
http://www.keenage.com/zhiwang/e_zhiwang.html. 
Duda R., Gaschnig J., & Hart P. (1979) Model design 
in the prospector consultant system for mineral 
exploration. In Expert systems in the micro 
-electronic age, Michie D. Ed., Edinburgh Univ. 
Press, Edinburgh, England. 
Isozaki H. (2001) Japanese Named Entity 
Recognition Based on a Simple Rule Generator and 
Decision Tree Learning, In ACL?01, 306-313. 
Lin D. (1998) Using collocation statistics in 
information extraction. In MUC-7 Proc. 
Luo Z.Y. & Song R. (2001) An Integrated and Fast 
Approach to Chinese Proper Name Recognition in 
Chinese Word Segmentation, In Proc. of Int?l 
Chinese Computing Conf., Singapore 323-328. 
Palmer D. D. (1997) A Trainable Rule-Based 
Algorithm for Word Segmentation, In Proc of 35th 
of ACL & 8th conf. of EACL, 321-328. 
Sproat R., Shih C., et al(1996) A Stochastic 
Finite-state Word Segmentation Algorithm for 
Chinese. Computational Linguistics, 22(3), 
377-404. 
Yu S.H., Bai S.H. & Wu P. (1998) Description of the 
Kent Ridge Digital Labs System Used For MUC-7, 
1998, In MUC-7 Proc. 
Yu S.W. (1999) The Specification and Manual of 
Chinese Word Segmentation and Part of Speech 
Tagging. http:// www.icl.pku.edu.cn/ 
Sekine S. (1998) NYU: Description of The Japanese 
NE System Used for MET-2, in MUC-7 Proc. 
 
Building Semantic Perceptron Net for Topic Spotting 
 
Jimin Liu  and  Tat-Seng Chua 
School of Computing 
National University of Singapore 
SINGAPORE 117543 
  {liujm, chuats}@comp.nus.edu.sg 
 
 
 
 
Abstract 
This paper presents an approach to 
automatically build a semantic 
perceptron net (SPN) for topic spotting. 
It uses context at the lower layer to 
select the exact meaning of key words, 
and employs a combination of context, 
co-occurrence statistics and thesaurus to 
group the distributed but semantically 
related words within a topic to form 
basic semantic nodes. The semantic 
nodes are then used to infer the topic 
within an input document. Experiments 
on Reuters 21578 data set demonstrate 
that SPN is able to capture the semantics 
of topics, and it performs well on topic 
spotting task. 
 
1. Introduction 
 
Topic spotting is the problem of identifying the 
presence of a predefined topic in a text document. 
More formally, given a set of n topics together with 
a collection of documents, the task is to determine 
for each document the probability that one or more 
topics is present in the document. Topic spotting 
may be used to automatically assign subject codes 
to newswire stories, filter electronic emails and on-
line news, and pre-screen document in information 
retrieval and information extraction applications. 
Topic spotting, and its related problem of text 
categorization, has been a hot area of research for 
over a decade. A large number of techniques have 
been proposed to tackle the problem, including: 
regression model, nearest neighbor classification, 
Bayesian probabilistic model, decision tree, 
inductive rule learning, neural network, on-line 
learning, and, support vector machine (Yang & Liu, 
1999; Tzeras & Hartmann, 1993). Most of these 
methods are word-based and consider only the 
relationships between the features and topics, but 
not the relationships among features. 
It is well known that the performance of the 
word-based methods is greatly affected by the lack 
of linguistic understanding, and, in particular, the 
inability to handle synonymy and polysemy. A 
number of simple linguistic techniques has been 
developed to alleviate such problems, ranging from 
the use of stemming, lexical chain and thesaurus 
(Jing & Tzoukermann, 1999; Green, 1999), to 
word-sense disambiguation (Chen & Chang, 1998; 
Leacock et al 1998; Ide & Veronis, 1998) and 
context (Cohen & Singer, 1999; Jing & 
Tzoukermann, 1999). 
The connectionist approach has been widely 
used to extract knowledge in a wide range of 
information processing tasks including natural 
language processing, information retrieval and 
image understanding (Anderson, 1983; Lee & 
Dubin, 1999; Sarkas & Boyer, 1995; Wang & 
Terman, 1995). Because the connectionist 
approach closely resembling human cognition 
process in text processing, it seems natural to adopt 
this approach, in conjunction with linguistic 
analysis, to perform topic spotting. However, there 
have been few attempts in this direction. This is 
mainly because of difficulties in automatically 
constructing the semantic networks for the topics. 
In this paper, we propose an approach to 
automatically build a semantic perceptron net 
(SPN) for topic spotting. The SPN is a 
connectionist model with hierarchical structure. It 
uses a combination of context, co-occurrence 
statistics and thesaurus to group the distributed but 
semantically related words to form basic semantic 
nodes. The semantic nodes are then used to identify 
the topic. This paper discusses the design, 
implementation and testing of an SPN for topic 
spotting. 
The paper is organized as follows. Section 2 
discusses the topic representation, which is the 
prototype structure for SPN. Sections 3 & 4 
respectively discuss our approach to extract the 
semantic correlations between words, and build 
semantic groups and topic tree. Section 5 describes 
the building and training of SPN, while Section 6 
presents the experiment results. Finally, Section 7 
concludes the paper. 
2. Topic Representation 
 
The frame of Minsky (1975) is a well-known 
knowledge representation technique. A frame 
represents a high-level concept as a collection of 
slots, where each slot describes one aspect of the 
concept. The situation is similar in topic spotting. 
For example, the topic ?water? may have many 
aspects (or sub-topics). One sub-topic may be 
about ?water supply?, while the other is about 
?water and environment protection?, and so on. 
These sub-topics may have some common 
attributes, such as the word ?water?, and each sub-
topic may be further sub-divided into finer sub-
topics, etc. 
The above points to a hierarchical topic 
representation, which corresponds to the hierarchy 
of document classes (Figure 1). In the model, the 
contents of the topics and sub-topics (shown as 
circles) are modeled by a set of attributes, which is 
simply a group of semantically related words 
(shown as solid elliptical shaped bags or 
rectangles). The context (shown as dotted ellipses) 
is used to identify the exact meaning of a word. 
 
topic 
a word 
the context of a word 
Sub-topic  
Aspect attribute 
common attribute 
 
Figure 1. Topic representation 
Hofmann (1998) presented a word occurrence 
based cluster abstraction model that learns a 
hierarchical topic representation. However, the 
method is not suitable when the set of training 
examples is sparse. To avoid the problem of 
automatically constructing the hierarchical model, 
Tong et al(1987) required the users to supply the 
model, which is used as queries in the system. 
Most automated methods, however, avoided this 
problem by modeling the topic as a feature vector, 
rule set, or instantiated example (Yang & Liu, 
1999). These methods typically treat each word 
feature as independent, and seldom consider 
linguistic factors such as the context or lexical 
chain relations among the features. As a result, 
these methods are not good at discriminating a 
large number of documents that typically lie near 
the boundary of two or more topics. 
In order to facilitate the automatic extraction 
and modeling of the semantic aspects of topics, we 
adopt a compromise approach. We model the topic 
as a tree of concepts as shown in Figure 1. 
However, we consider only one level of hierarchy 
built from groups of semantically related words. 
These semantic groups may not correspond strictly 
to sub-topics within the domain. Figure 2 shows an 
example of an automatically constructed topic tree 
on ?water?. 
 
Contexts 
Basic Semantic 
 Nodes 
Topic 
   price 
agreement  
   water 
    ton 
  
   waste 
environment 
    bank 
   provide 
costumer 
corporation 
      plant  
 
  rain 
rainfall 
  dry  water  
water 
 
water 
river 
tourist 
  f e 
d c b a 
 
Figure 2. An example of a topic tree 
In Figure 2, node ?a? contains the common 
feature set of the topic; while nodes ?b?, ?c? and 
?d? are related to sub-topics on ?water supply?, 
?rainfall?, and ?water and environment protection? 
respectively. Node ?e? is the context of the word 
?plant?, and node ?f? is the context of the word 
?bank?. Here we use training to automatically 
resolve the corresponding relationship between a 
node and an attribute, and the context word to be 
used to select the exact meaning of a word. From 
this representation, we observe that: 
a) Nodes ?c? and ?d? are closely related and may 
not be fully separable. In fact, it is sometimes 
difficult even for human experts to decide how 
to divide them into separate topics. 
b) The same word, such as ?water?, may appear in 
both the context node and the basic semantic 
node.  
c) Some words use context to resolve their 
meanings, while many do not need context. 
3. Semantic Correlations  
 
Although there exists many methods to derive the 
semantic correlations between words (Lee, 1999; 
Lin, 1998; Karov & Edelman, 1998; Resnik, 1995; 
Dagan et al 1995), we adopt a relatively simple 
and yet practical and effective approach to derive 
three topic -oriented semantic correlations: 
thesaurus-based, co-occurrence-based and context-
based correlation.  
3.1 Thesaurus based correlation  
WordNet is an electronic thesaurus popularly used 
in many researches on lexical semantic acquisition, 
and word sense disambiguation (Green, 1999; 
Leacock et al 1998). In WordNet, the sense of a 
word is represented by a list of synonyms (synset), 
and the lexical information is represented in the 
form of a semantic network. 
However, it is well known that the granularity 
of semantic meanings of words in WordNet is often 
too fine for practical use. We thus need to enlarge 
the semantic granularity of words in practical 
applications. For example, given a topic on 
?children education?, it is highly likely that the 
word ?child? will be a key term. However, the 
concept ?child? can be expressed in many 
semantically related terms, such as ?boy?, ?girl?, 
?kid?, ?child?, ?youngster?, etc. In this case, it 
might not be necessary to distinguish the different 
meaning among these words, nor the different 
senses within each word. It is, however, important 
to group all these words into a large synset {child, 
boy, girl, kid, youngster}, and use the synset to 
model the dominant but more general meaning of 
these words in the context. 
In general, it is reasonable and often useful to 
group lexically related words together to represent 
a more general concept. Here, two words are 
considered to be lexically related if they are related 
to by the ?is_a?, ?part_of?, ?member_of?, or 
?antonym? relations, or if they belong to the same 
synset. Figure 3 lists the lexical relations that we 
considered, and the examples. 
Since in our experiment, there are many 
antonyms co-occur within the topic, we also group 
antonyms together to identify a topic. Moreover, if 
a word had two senses of, say, sense-1 and sense-2. 
And if there are two separate words that are 
lexically related to this word by sense-1 and sense-
2 respectively, we simply group these words 
together and do not attempt to distinguish the two 
different senses. The reason is because if a word is 
so important to be chosen as the keyword of a 
topic, then it should only have one dominant 
meaning in that topic. The idea that a keyword 
should have only one dominant meaning in a topic 
is also suggested in Church & Yarowsky (1992). 
 
corn 
 
maize 
metal 
zinc  
per
import 
export  
perso
synset         is_a              part_of       member_of      antonym 
tree 
leaf  
family  
son  
per  Figure 3: Examples of lexical relationship 
Based on the above discussion, we compute the 
thesaurus-based correlation between the two terms 
t1 and t2, in topic Ti, as: 
                         1    (t1 and t2 are in the same synset, or t1=t2) 
                         0.8  (t1 and t2 have ?antonym? relation) 
0..5  (t1 and t2 have relations of ?is_a?,  
          ?part_of?, or ?member_of?) 
                        0   (others) 
=),( 21)( ttR iL
 
3.2 Co-occurrence based correlation 
Co-occurrence relationship is like the global 
context of words. Using co-occurrence statistics, 
Veling & van der Weerd (1999) was able to find 
many interesting conceptual groups in the Reuters-
2178 text corpus. Examples of the conceptual 
groups found include: {water, rainfall, dry}, 
{bomb, injured, explosion, injuries}, and {cola, 
PEP, Pepsi, Pespi-cola, Pepsico}. These groups 
are meaningful, and are able to capture the 
important concepts within the corpus. 
Since in general, high co-occurrence words are 
likely to be used together to represent (or describe) 
a certain concept, it is reasonable to group them 
together to form a large semantic node. Thus for 
topic Ti, the co-occurrence-based correlation of two 
terms, t1 and t2, is computed as: 
)(/)(),( 21
)(
21
)(
21
)( ttdfttdfttR iiico ??=  (2) 
where )( 21)( ttdf i ?  ( )( 21)( ttdf i ? ) is the fraction of 
documents in Ti that contains t1 and (or) t2. 
3.3 Context based correlation 
Broadly speaking, there are three kinds of context: 
domain, topic and local contexts (Ide & Vernois, 
1998). Domain context requires extensive 
knowledge of domain and is not considered in this 
paper. Topic context can be modeled 
approximately using the co-occurrence 
(1) 
relationships between the words in the topic. In this 
section, we will define the local context explicitly. 
The local context of a word t is often defined as  
the set of non-trivial words near t. Here a word wd 
is said to be near t if their word distance is less than 
a given threshold, which is set to be 5 in our 
experiment. 
We represent the local context of term tj in topic 
Ti by a context vector cv(i)(tj). To derive cv(i)(tj), we 
first rank all candidate context words of ti by their 
density values: 
)(/)( )()()( jikij
i
jk tnwdm=r  (3) 
where )()( ji tn is the number of occurrence of tj in 
Ti, and )()( kij wdm is the number of occurrences of 
wdk near t j. We then select from the ranking, the top 
ten words as the context of tj in Ti as: 
),(),...,,(),,{()( )(10
)(
10
)(
2
)(
2
)(
1
)(
1
)( i
j
i
j
i
j
i
j
i
j
i
jj
i wdwdwdtcv rrr=  (4) 
When the training sample is sufficiently large, 
the context vector will have good statistic 
meanings. Noting again that an important word to a 
topic should have only one dominant meaning 
within that topic, and this meaning should be 
reflected by its context. We can thus draw the 
conclusion that if two words have a very high 
context similarity within a topic, it will have a high 
possibility that they are semantic related. Therefore 
it is reasonable to group them together to form a 
larger semantic node. We thus compute the 
context-based correlation between two term t1 and 
t2 in topic Ti as: 
2/12)(
2
2/12)(
1
10
1
)(
)(2
)(
1
)(
)(2
)(
1
)(
21)(
])([*])([
**),(
),(
??
?
= =
k
i
kk
i
k
k
i
km
i
k
i
km
i
k
i
co
ic
wdwdR
ttR
rr
rr
 (5) 
where  ),(maxarg)( )(2
)(
1
)( i
s
i
k
i
co
s
wdwdRkm =  
For example, in Reuters 21578 corpus, 
?company? and ?corp? are context-related words 
within the topic ?acq?. This is because they have 
very similar context of ?say, header, acquire, 
contract?. 
4. Semantic Groups & Topic Tree 
 
There are many methods that attempt to construct 
the conceptual representation of a topic from the 
original data set (Veling & van der Weerd, 1999; 
Baker & McCallum, 1998; Pereira et al 1993). In 
this Section, we will describe our semantic -based 
approach to finding basic semantic groups and 
constructing the topic tree. Given a set of training 
documents, the stages involved in finding the 
semantic groups for each topic are given below. 
A) Extract all distinct terms {t1,  t2, ..tn} from the 
training document set for topic Ti. For each term 
tj, compute its df(i)(tj) and cv(i)(t j), where df(i)(tj)  
is defined as the fraction of documents in T i that 
contain tj. In other words, df (i)(tj) gives the 
conditional probability of tj appearing in Ti. 
B)  Derive the semantic group Gj using tj as the 
main keyword. Here we use the semantic 
correlations defined in Section 3 to derive the 
semantic relationship between tj and any other 
term tk. Thus: 
 For each pair (t j,tk), k=1,..n,  set Link(tj,tk)=1 
 if )( iLR (tj,tk)>0,   or,    
  df (i)(tj)>d0  and  )(icoR (tj, tk)>d1  or 
  df (i)(tj)>d2  and  )(icR  (tj, tk)>d 3. 
 where d0, d1, d2, d3  are predefined thresholds. 
 For all tk with Link(tj,tk)=1, we form a semantic 
group centered around tj denoted by: 
 },...,,{},...,,{ 2121 njjjj ttttttG jk ?=  
(6) 
 Here tj is the main keyword of node G j and is 
denoted by  main(Gj)=t j. 
C)  Calculate the information value inf (i)(Gj) of each 
basic semantic group. First we compute the 
information value of each tj: 
  }1,0max{*)()(inf )()( Nptdft ijj
i
j
i -=  (7) 
 where     
?
=
=
N
k
ki
j
i
ij
tdf
tdfp
1
)(
)(
)(
)(  
 and N is the number of topics. Thus 1/N  denotes 
the probability that a term is in any class, and pij 
denotes the normalized conditional probability 
of tj in Ti. Only those terms whose normalized 
conditional probability is higher than 1/N will 
have a positive information value. 
 The information value of the semantic group Gj 
is simply the summation of information value of 
its constituent terms weighted by their 
maximum semantic correlation with t j as: 
 ?=
=
jk
k
ki
i
jkj
i twG
1
)()()( )](inf*[)(inf  (8) 
 where )},(),,(),,(max{ )()()()( kjiLkj
i
ckj
i
co
i
jk ttRttRttRw =  
D) Select the essential semantic groups using the 
following algorithm: 
 a) Initialize: 
  },...,,{ 11 nGGGS ? ,  F?Groups , 
b) Select the semantic group with highest 
information value: 
 ))((infmaxarg )( kiSGk
Gj
k ?
?  
 c) Terminate if inf (i)(Gj) is less than a 
predefined threshold d4. 
 d) Add Gj into the set Groups: 
  jGSS -= ,  and  }{ jGGroupsGroups ??  
 e) Eliminate those groups in S whose key terms 
appear in the selected group Gj. That is: 
  For each  SGk ? , if jk GGmain ?)( , then  
 }{ kGSS -?  
 f) Eliminate those terms in remaining groups in 
S that are found in the selected group G j. 
That is: 
  For each SGk ? ,  jkk GGG -? ,  
  and if F=kG , then  }{ kGSS -?  
 g) If F=S  then stop; else go to step (b). 
In the above grouping algorithm, the predefined 
thresholds d0,d1,d2,d3 are used to control the size of 
each group, and d4 is used to control the number of 
groups. 
The set of basic semantic groups found then 
forms the sub-topics of a 2-layered topic tree as 
illustrated in Figure 2. 
5. Building and Training of SPN 
 
The Combination of local perception and global 
arbitrator has been applied to solve perception 
problems (Wang & Terman, 1995; Liu & Shi, 
2000). Here we adopt the same strategy for topic 
spotting. For each topic, we construct a local 
perceptron net (LPN), which is designed for a 
particular topic. We use a global expert (GE) to 
arbitrate all decisions of LPNs and to model the 
relationships between topics. Here we discuss the 
design of both LPN and GE, and their training 
processes. 
5.1 Local Perceptron Net (LPN) 
We derive the LPN directly from the topic tree as 
discussed in Sectio n 2 (see Figure 2). Each LPN is 
a multi- layer feed-forward neural network with a 
typical structure as shown in Figure 4. 
In Figure 4, x ij represents the feature value of 
keyword wdi j in the ith semantic group; xijk?s (where 
k=1,?10) represent the feature values of the context 
words wdijk?s of keyword wd ij; and aij denotes the 
meaning of keyword wd i j as determined by its 
context. Ai corresponds to the ith basic semantic 
node. The weights wi, wi j, and wijk and biases ?i and 
? ij are learned from training, and y(i)(x) is the output 
of the network. 
 y(i) 
  iA  
  iw  
  ijw  
  ijkw  
  ija  
  ijkx  
Context  key term  
Semantic 
group 
Class 
  ijx  
Basic 
meaning 
q (i) 
  ijq  
 
Figure 4: The architecture of LPN for topic i 
Given a document: 
 x = {(xi j,cv i j) | i=1,2,?m, j=1,?ij} 
where m is the number of basic semantic nodes, ij 
is the number of key terms contained in the i th 
semantic node, and cv ij={xi j1,xi j2? ijijkx } is the 
context of term x ij. The output y(i) =y(i)(x) is 
calculated as follows: 
 ?==
=
m
i
ii
ii Awxyy
1
)()( )(  (9) 
where  ])*(exp[1
1*
? --+
=
? ijijk cvx
ijijkijk
ijij xw
xa
q  (10) 
and     
)exp(1
)exp(1
1
1
?-+
?--
=
=
=
j
j
i
j
iji
i
j
iji
i
aw
aw
A
 (11) 
Equation (10) expresses the fact that only if a 
key term is present in the document (i.e. xij > 0), its 
context needs to be checked. 
For each topic Ti, there is a corresponding net 
y(i) =y(i)(x) and a threshold q(i). The pair of (y(i)(x), 
q(i)) is a local binary classifier for Ti such that: 
If y(i)(x)-q(i) > 0, then Ti is present; otherwise 
  Ti is not present in document x. 
From the procedures employed to building the 
topic tree, we know that each feature is in fact an 
evidence to support the occurrence of the topic. 
This gives us the suggestion that the activation 
function for each node in the LPN should be a non-
decreasing function of the inputs. Thus we impose 
a weight constraint on the LPN as: 
 wi>0,  wi j>0,  wijk>0 (12) 
5.2 Global expert (GE) 
Since there are relations among topics, and LPNs 
do not have global information, it is inevitable that 
LPNs will make wrong decisions. In order to 
overcome this problem, we use a global expert 
(GE) to arbitrate al local decisions. Figure 5 
illustrates the use of global expert to combine the 
outputs of LPNs. 
 
)()( iiy q-
Y(i)  
)()( jjy q-
ijW  
)1()1( q-y
 )(iQ  
 
Figure 5: The architecture of global expert 
Given a document x, we first use each LPN to 
make a local decision. We then combine the 
outputs of LPNs as follows: 
])([)( )()()()()( )(
0)()(
ij
ij
iii j
jjy
ij
yWyY Q--?+-=
>-
?
qq
q
 (13) 
where Wij?s are the weights between the global 
arbitrator i and the j th LPN; and )(iQ ?s are the 
global bias. From the result of Equation (13), we 
have: 
If Y(i) > 0; then topic Ti is present; otherwise 
  Ti is not present in document x  
The use of Equation (13) implies that: 
a) If a LPN is not activated, i.e., y(i)  ? q(i), then its 
output is not used in the GE. Thus it will not 
affect the output of other LPN.  
b) The weight Wi j models the relationship or 
correlation between topic i and j. If Wi j > 0, it 
means that if document x is related to Tj, it may 
also have some contribution ( Wij) to topic T j. On 
the other hand, if Wi j < 0, it means the two 
topics are negatively correlated, and a document 
x will not be related to both Tj and Ti. 
The overall structure of SPN is as follows: 
 
Input document 
Local Perception 
Global Expert 
x 
y(i) 
Y(i) 
 
Figure 6: Overall structure of SPN 
5.3 The Training of SPN 
In order to adopt SPN for topic spotting, we 
employ the well-known BP algorithm to derive the 
optimal weights and biases in SPN. The training 
phase is divided to two stages. The first stage 
learns a LPN for each topic, while the second stage 
trains the GE. As the BP algorithm is rather 
standard, we will discuss only the error functions 
that we employ to guide the training process. 
In topic spotting, the goal is to achieve both 
high recall and precision. In particular, we want to 
allow y(x) to be as large (or as small) as possible in 
cases when there is no error, or when +W?x  and 
q>)(xy  (or -W?x  and q<)( xy ). Here +W  and -W  
denote the positive and negative training document 
sets respectively. To achieve this, we adopt a new 
error function as follows to train the LPN: 
?
W+W
W+
?
W+W
W=
-
+
W?
-
+-
+
W?
+
+-
-
x
x
iijijijk
xy
xywwwE
)),((
||||
||
)),((
||||
||),,,,(
qe
qeqq
 (14) 
where 
??
??
?
?
<-=+
)(0
)()(
2
1
),(
2
q
qqqe
x
xxx ,  and 
 ),(),( qeqe --= +- xx  
Equation (14) defines a piecewise differentiable 
error function. The coefficients 
||||
||
+-
-
W+W
W  and 
||||
||
+-
+
W+W
W  are used to ensure that the contributions 
of positive and negative examples are equal. 
After the training, we choose the node with the 
biggest wi value as the common attribute node. 
Also, we trim the topic representation by removing 
those words or context words with very small wij or 
wijk values. 
We adopt the following error function to train 
GE: 
? ? Q+? Q=Q
= W?
-
W?
+
-+
n
i x
iii
x
iiiiij
ii
xYxYWE
1
])),(()),(([),( ee  (15) 
where +W i  is the set of positive examples of Ti. 
6. Experiment and Discussion 
 
We employ the ModApte Split version of Reuters-
21578 corpus to test our method. In order to ensure 
that the training is meaningful, we select only those 
classes that have at least one document in each of 
the training and test sets. This results in 90 classes 
in both the training and test sets. After eliminating 
documents that do not belong to any of these 90 
classes, we obtain a training set of 7,770 
documents and a test set of 3,019 documents.  
From the set of training documents, we derive the 
set of semantic nodes for each topic using the 
procedures outlined in Section 4. From the training 
set, we found that the average number of semantic 
nodes for each topic is 132, and the average 
number of terms in each node is 2.4. For 
illustration, Table 1 lists some examples of the 
semantic nodes that we found. From table 1, we 
can draw the following general observations. 
Node 
ID 
Semantic Node 
(SN) 
Method used 
to find SNs 
Topic 
1 wheat  1 
2 import, export, 
output  
1,2,3 
3 farmer, production,  
mln, ton 
2 
4 disease, insect, pest 2 
 
 
Wheat 
5 fall, fell, rise, rose 3 Wpi 
Method 1 ? by looking up WordNet 
Method 2 ? by analyzing co-occurrence correlation 
Method 3 ? by analyzing context correlation  
Table 1: Examples of semantic nodes 
a) Under the topic ?wheat?, we list four semantic 
nodes. Node 1 contains the common attribute 
set of the topic. Node 2 is related to the ?buying 
and selling of wheat?. Node 3 is related to 
?wheat production?; and node 4 is related to 
?the effects of insect on wheat production?. The 
results show that the automatically extracted 
basic semantic nodes are meaningful and are 
able to capture most semantics of a topic. 
b) Node 1 originally contains two terms ?wheat? 
and ?corn? that belong to the same synset found 
by looking up WordNet. However, in the 
training stage, the weight of the word ?corn? 
was found to be very small in topic ?wheat?, 
and hence it was removed from the semantic 
group. This is similar to the discourse based 
word sense disambiguation.  
c) The granularity of information expressed by the 
semantic nodes may not be the same as what 
human expert produces. For example, it is 
possible that a human expert may divide node 2 
into two nodes {import} and {export, output}. 
d) Node 5 contains four words and is formed by 
analyzing context. Each context vector of the 
four words has the same two components: 
?price? and ?digital number?. Meanwhile, 
?rise? and ?fall? can also be grouped together 
by ?antonym? relation. ?fell? is actually the past 
tense of ?fall?. This means that by comparing 
context, it is possible to group together those 
words with grammatical variations without 
performing grammatical analysis. 
Table 2 summarizes the results of SPN in terms 
of macro and micro F1 values (see Yang & Liu 
(1999) for definitions of the macro and micro F1 
values). For comparison purpose, the Table also 
lists the results of other TC methods as reported in 
Yang & Liu (1999). From the table, it can be seen 
that the SPN method achieves the best macF1 
value. This indicates that the method performs well 
on classes with a small number of training samples. 
In terms of the micro F1 measures, SPN out-
performs NB, NNet, LSF and KNN, while posting 
a slightly lower performance than that of SVM. 
The results are encouraging as they are rather 
preliminary. We expect the results to improve 
further by tuning the system ranging from the 
initial values of various parameters, to the choice 
of error functions, context, grouping algorithm, and 
the structures of topic tree and SPN. 
Method MicR MicP micF1 macF1   
SVM 0.8120 0.9137 0.8599 0.5251 
KNN 0.8339 0.8807 0.8567 0.5242 
LSF 0.8507 0.8489 0.8498 0.5008 
NNet 0.7842 0.8785 0.8287 0.3763 
NB 0.7688 0.8245 0.7956 0.3886 
SPN 0.8402 0.8743 0.8569 0.6275 
Table 2. The performance comparison 
7. Conclusion 
 
In this paper, we proposed an approach to 
automatically build semantic perceptron net (SPN) 
for topic spotting. The SPN is a connectionist 
model in which context is used to select the exact 
meaning of a word. By analyzing the context and 
co-occurrence statistics, and by looking up 
thesaurus, it is able to group the distributed but 
semantic related words together to form basic 
semantic nodes. Experiments on Reuters 21578 
show that, to some extent, SPN is able to capture 
the semantics of topics and it performs well on 
topic spotting task. 
It is well known that human expert, whose most 
prominent characteristic is the ability to understand 
text documents, have a strong natural ability to spot 
topics in documents. We are, however, unclear 
about the nature of human cognition, and with the 
present state-of-art natural language processing 
technology, it is still difficult to get an in-depth 
understanding of a text passage. We believe that 
our proposed approach provides a promising 
compromise between full understanding and no 
understanding. 
Acknowledgment 
 
The authors would like to acknowledge the support 
of the National Science and Technology Board, and 
the Ministry of Education of Singapore for the 
provision of a research grant RP3989903 under 
which this research is carried out. 
References 
 
J.R. Anderson (1983). A Spreading Activation 
Theory of Memory. J. of Verbal Learning & 
Verbal Behavior, 22(3):261-295. 
L.D. Baker & A.K. McCallum (1998). 
Distributional Clustering of Words for Text 
Classification. SIGIR?98. 
J.N. Chen & J.S. Chang (1998). Topic Clustering 
of MRD Senses based on Information Retrieval 
Technique.  Comp Linguistic, 24(1), 62-95. 
G.W.K. Church & D. Yarowsky (1992). One Sense 
per Discourse. Proc. of 4th DARPA Speech and 
Natural Language Workshop. 233-237. 
W.W. Cohen & Y. Singer (1999). Context-
Sensitive Learning Method for Text 
Categorization. ACM Trans. on Information 
Systems, 17(2), 141-173, Apr. 
I. Dagan, S. Marcus & S. Markovitch (1995). 
Contextual Word Similarity and Estimation 
from Sparse Data. Computer speech and 
Language, 9:123-152. 
S.J. Green (1999). Building Hypertext Links by 
Computing Semantic Similarity. IEEE Trans on 
Knowledge & Data Engr, 11(5). 
T. Hofmann (1998). Learning and Representing 
Topic, a Hierarchical Mixture Model for Word 
Occurrences in Document Databases. 
Workshop on Learning from Text and the 
Web, CMU.  
N. Ide & J. Veronis (1998). Introduction to the 
Special Issue on Word Sense Disambiguation: 
the State of Art. Comp Linguistics, 24(1), 1-39. 
H. Jing & E. Tzoukermann (1999). Information 
Retrieval based on Context Distance and 
Morphology. SIGIR?99, 90-96. 
Y. Karov & S. Edelman (1998). Similarity-based 
Word Sense Disambiguation, Computational 
Linguistics, 24(1), 41-59. 
C. Leacock & M. Chodorow & G. Miller (1998). 
Using Corpus Statistics and WordNet for Sense 
Identification. Comp. Linguistic, 24(1), 147-
165. 
L. Lee (1999). Measure of Distributional 
Similarity.  Proc of 37 th Annual Meeting of 
ACL. 
J. Lee & D. Dubin (1999). Context-Sensitive 
Vocabulary Mapping with a Spreading 
Activation Network. SIGIR?99, 198-205. 
D. Lin (1998). Automatic Retrieval and Clust ering 
of Similar Words.  In COLING-ACL?98, 768-
773. 
J. Liu & Z. Shi (2000). Extracting Prominent 
Shape by Local Interactions and Global 
Optimizations. CVPRIP?2000, USA.  
M.A. Minsky (1975). A Framework for 
Representing Knowledge. In: Winston P (eds). 
?The psychology of computer vision?, 
McGraw-Hill, New York, 211-277. 
F.C.N. Pereira, N.Z. Tishby & L. Lee (1993). 
Distributional Clustering of English Words. 
ACL?93, 183-190. 
P. Resnik (1995). Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. 
Proc of IJCAI-95, 448-453. 
S. Sarkas & K.L. Boyer (1995). Using Perceptual 
Inference Network to Manage Vision 
Processes.  Computer Vision & Image 
Understanding, 62(1), 27-46. 
R. Tong, L. Appelbaum, V. Askman & J. 
Cunningham (1987). Conceptual Information  
Retrieval using RUBRIC.  SIGIR?87, 247? 253. 
K. Tzeras & S. Hartmann (1993). Automatic 
Indexing based on Bayesian Inference 
Networks. SIGIR?93, 22-34. 
A. Veling & P. van der Weerd (1999). Conceptual 
Grouping in Word Co-occurrence Networks. 
IJCAI 99: 694-701. 
D. Wang & D. Terman (1995). Locally Excitatory 
Globally Inhibitory Oscillator Networks. IEEE 
Trans. Neural Network. 6(1). 
Y. Yang & X. Liu (1999). Re-examination of Text 
Categorization. SIGIR?99, 43-49. 
 
 
Extracting Pronunciation-translated Names from Chinese  
Texts using Bootstrapping Approach 
Jing Xiao 
School of Computing, 
National University of Singapore 
xiaojing@comp.nus.edu.sg 
Jimin Liu 
School of Computing, 
National University of Singapore 
liujm@comp.nus.edu.sg 
Tat-Seng Chua 
School of Computing, 
National University of Singapore 
chuats@comp.nus.edu.sg 
 
Abstract  
Pronunciation-translated names (P-Names) 
bring more ambiguities to Chinese word 
segmentation and generic named entity 
recognition. As there are few annotated 
resources that can be used to develop a good 
P-Name extraction system, this paper 
presents a bootstrapping algorithm, called 
PN-Finder, to tackle this problem. Starting 
from a small set of P-Name characters and 
context cue-words, the algorithm iteratively 
locates more P-Names from the Internet. 
The algorithm uses a combination of 
P-Name and context word probabilities to 
identify new P-Names. Experiments show 
that our PN-Finder is able to locate a large 
number of P-Names (over 100,000) from the 
Internet with a high recognition accuracy of 
over 85%. Further tests on the MET-2 test 
set show that our PN-Finder can achieve a 
performance of over 90% in F1 value in 
locating P-Names. The results demonstrate 
that our PN-Finder is effective. 
1 Introduction 
Pronunciation-translated names (P-Names) are 
those foreign names that are translated to Chinese 
characters according to their pronunciations. A 
P-Name sometimes forms part of but not a 
complete named entity. For instance, in the place 
name ?? (Berkeley University), only 
the term ?? (Berkeley) is a P-Name, while 
?? (University) is not since it is translated 
semantically.  
The ability to recognize P-Names helps to reduce 
ambiguities in word segmentation and improve the 
performance of Chinese information retrieval 
since many unknown words are P-Names, 
especially for international Chinese news. Unlike 
English, there is no blank between words in 
Chinese, in which a word is a linguistic token 
consisting of one or more characters. In addition, the 
same characters may appear in multiple context with 
different meanings (Chua and Liu, 2002). The 
presence of P-Names brings more ambiguities to 
Chinese word segmentation since every character in 
a P-Name can be used as a common character. 
Intuitively, we can extract the P-Names based on the 
distinctive sequence of characters that they are  used 
as compared to common words. In addition, we can 
use local context around the P-Names to confirm 
and classify them into person or part of location and 
organization names. One way to perform these tasks 
effectively is to rely on statistics derived from a 
large corpus in which the P-Names are annotated. 
While some annotated corpuses with general named 
entities are available such as the PKUC (Yu, 1999) 
and MET-2 (Chinchor, 2001), there is no such 
annotated corpus for P-Names. While annotated data 
is difficult to obtain, un-annotated data is readily 
available and plentiful, especially on the Internet. To 
take advantage of that, we need to tackle two major 
problems. The first is how to gather sufficient 
distinct P-Names from the Internet, and the second is 
how to use the available resources to derive reliable 
statistical information to characterize the P-Names. 
The problem of gathering sufficient reliable 
information from a small initial set of seed resources 
has been tackled in bootstrapping research for 
information extraction (Agichtein and Gravano, 
2000; Brin, 1998; Collins and Singer, 1999; 
Mihalcea and Moldovan, 2001; Riloff and Jones, 
1999). Bootstrapping approach aims to perform 
unsupervised text processing to extract information 
from open resources such as the Internet using 
minimum manual labor. Given the lack of annotated 
training samples for P-Name extraction, this paper 
introduces a bootstrapping algorithm, called 
PN-Finder. It starts from a small set of seed samples, 
and iteratively locates, extracts and classifies the 
new and more P-Names. It works in conjunction 
with a general Chinese named entity recognizer 
(Chua and Liu, 2002) to extract general named 
entities. 
In the remaining parts of this paper, we describe 
the details of PN-Finder in Section 2 and its 
application in locating P-Names from new 
documents in Section 3. Section 4 presents the 
experimental results using the MET-2 test corpus. 
Section 5 contains our conclusion and outline for 
future work. 
2 Bootstrapping Algorithm for Locating 
P-Names 
Currently, there is no standard corpus that 
annotates all P-Names. Since annotating thousands 
of P-Names is more difficult than collecting 
thousands of P-Names from the Internet, we recur 
to using the Internet search engine to collect a 
large set of P-Names. Figure 1 illustrates our main 
components in bootstrapping process. 
 
 
 
 
Figure 1: Main components of the bootstrapping 
The inputs to the PN-Finder are:  
a) A seed P-Name character set Cs(0) consisting of 
5 characters {??, ??, ??, ?	?, ?
?}. 
b) A set of seed context cue words CW(0) 
consisting of 60 context words, such as 
{?NULL?, ??, ??, ??, ??, ??}. 
These are typical context words found around 
person, location and organization names in 
PKUC1 (the PoS Corpus of Peking University), 
which contains one month of news from the 
People Daily.  
c) A set of P-Name candidates P(0), which is null at 
the beginning. 
d) A common word dictionary extracted from 
PKUC by removing proper nouns, numbers and 
non-Chinese symbols. It contains about 37,000 
words. 
                                                      
1
 http://icl.pku.edu.cn/Introduction/corpustagging.htm 
From the initial seeds, we perform the followings: 
a) We use every two characters in Cs(i-1) as query to 
retrieve relevant web pages from the Internet 
using a commercial search engine. We then 
extract possible P-Names from the returned web 
pages to update P(i). 
b) We find a most probable new P-Name character. 
Update Cs(i-1) to Cs(i) by adding the new character. 
c) We bootstrap new context words around the new 
P-Names found to derive CW(i). We then perform 
the lexical chaining to generalize these context 
words to generate semantic classes. 
d) We repeat the process from step (a) until any of 
the following conditions is satisfied: (i) when no 
new P-Name is found; (ii) when the desired 
number of iterations is reached; or (iii) when the 
number of P-Names found exceeds a desired 
number. 
The following subsections discuss the details of  
the bootstrapping process. 
2.1 Querying and Extracting the P-Names 
from the Web 
The first step of the algorithm is to derive good 
queries from the character set Cs(m-1) to search the 
Internet to obtain new web pages. If we use all single 
characters from Cs(m-1) to perform the search, we are 
likely to get too many pages containing irrelevant 
information. As a compromise, we use every two 
characters cicj in Cs(m-1) (except those combinations 
that have been used in the previous iterations) to 
search the Internet using Google (by using its 
language tool2). We consider only up to 300 entries 
returned by Google. We divide the content of the 
web pages into text segments by using the 
non-alphanumeric characters as delimiters. We 
extract those text segments that contain the search 
characters ci, cj or both and store them in R(m). For 
example, from the web page given in Figure 2, the 
text segments extracted include: strings ?
		

? and ?
		
? from the first 
entry; and strings ?